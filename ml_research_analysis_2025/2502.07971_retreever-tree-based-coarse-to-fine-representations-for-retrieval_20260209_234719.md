---
ver: rpa2
title: 'ReTreever: Tree-based Coarse-to-Fine Representations for Retrieval'
arxiv_id: '2502.07971'
source_url: https://arxiv.org/abs/2502.07971
tags:
- ndcg
- tree
- node
- retrieval
- retreever
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RETREEVER is a tree-based document retrieval method that organizes
  documents into a binary tree with learned routing functions at each internal node.
  By optimizing directly for retrieval performance, it produces coarse-to-fine representations
  at different tree levels while preserving full representation accuracy.
---

# ReTreever: Tree-based Coarse-to-Fine Representations for Retrieval

## Quick Facts
- arXiv ID: 2502.07971
- Source URL: https://arxiv.org/abs/2502.07971
- Reference count: 40
- ReTreever is a tree-based document retrieval method that organizes documents into a binary tree with learned routing functions at each internal node. By optimizing directly for retrieval performance, it produces coarse-to-fine representations at different tree levels while preserving full representation accuracy.

## Executive Summary
ReTreever introduces a novel approach to document retrieval that organizes documents into a binary tree with learned routing functions at each internal node. Unlike traditional hierarchical methods that require multiple retrieval stages, ReTreever optimizes directly for retrieval performance by making the tree structure differentiable through probabilistic relaxations. The method jointly learns routing functions that assign query-document pairs to similar tree branches, enabling efficient coarse-to-fine representations while maintaining accuracy. By using cross-attention split functions and contrastive learning with negative Total Variation Distance, ReTreever achieves superior retrieval accuracy at lower latency compared to hierarchical baselines like HIER-KMEANS and RAPTOR.

## Method Summary
ReTreever organizes documents into a fixed-depth binary tree where each internal node contains a learned split function that routes documents probabilistically based on their encoded representations. The method uses a frozen encoder (typically BGE-Large) to convert text to embeddings, then applies parallel split functions at each tree level to compute routing probabilities. These probabilities are propagated down the tree to determine leaf node assignments, which are used for document retrieval via negative Total Variation Distance similarity. Training uses InfoNCE contrastive loss with stochastic depth scheduling to optimize retrieval performance across all tree levels simultaneously. The approach enables efficient retrieval by starting at coarse representations and refining results as needed, while providing interpretable semantic groupings of documents at different tree levels.

## Key Results
- ReTreever achieves the best retrieval accuracy at the lowest latency among hierarchical methods on multiple QA datasets
- Stochastic depth training provides superior coarse representations while maintaining fine-level accuracy
- Cross-attention split functions outperform linear and MLP alternatives across all tree levels
- The hierarchical structure enables inspectable semantic groupings with cosine similarity decreasing as tree distance increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Differentiable tree routing enables end-to-end optimization of document organization for retrieval.
- Mechanism: Hard binary routing decisions are relaxed into soft probability distributions via sigmoid functions (z_left = σ(s_θ(x)), z_right = 1 - σ(s_θ(x))), allowing gradient flow through the tree structure. Each internal node learns a split function (linear, MLP, or cross-attention) that outputs routing scores independently, then probabilities are propagated down the tree by multiplying along ancestor paths.
- Core assumption: Probabilistic relaxations preserve enough decision structure to learn meaningful semantic partitions while remaining optimizable via gradient descent.
- Evidence anchors:
  - [abstract] "Our method, called ReTreever, jointly learns a routing function per internal node of a binary tree such that query and reference documents are assigned to similar tree branches, hence directly optimizing for retrieval performance."
  - [section 3] "However, hard assignments would result in piece-wise constant functions with null gradients, making back-propagation ineffective. Therefore, we make use of probabilistic relaxations... to make the tree learning problem differentiable."
  - [corpus] Related work on differentiable trees (Zantedeschi et al., 2021) supports the viability of this approach, though specific retrieval applications are novel here.
- Break condition: If routing probabilities collapse to near-uniform or near-deterministic extremes, gradient signals weaken and tree structure degrades to trivial solutions.

### Mechanism 2
- Claim: Contrastive learning with negative Total Variation Distance (nTVD) aligns query-context pairs at shared tree locations while dispersing unrelated pairs.
- Mechanism: The InfoNCE loss optimizes similarity between leaf assignment distributions for positive pairs while pushing negative pairs apart. The similarity metric sim(a,b) = -½Σ|a_l - b_l| measures distributional overlap rather than vector dot product, directly matching the tree's probabilistic routing structure.
- Core assumption: Similar documents should concentrate probability mass in overlapping tree regions; dissimilar documents should spread across different branches.
- Evidence anchors:
  - [abstract] "Our method... jointly learns a routing function per internal node... such that query and reference documents are assigned to similar tree branches, hence directly optimizing for retrieval performance."
  - [section 3] "To avoid such a solution, we define an objective that additionally encourages maximal use of the tree, by spreading unrelated contexts and queries across different tree leaves."
  - [corpus] Hierarchical Retrieval work (corpus neighbor "Hierarchical Retrieval: The Geometry and a Pretrain-Finetune Recipe") discusses geometric constraints in dual encoders; ReTreever's tree structure provides explicit hierarchical constraints absent in flat embedding spaces.
- Break condition: If negative sampling is insufficient or batch size too small, the model may collapse to routing all inputs through a single path.

### Mechanism 3
- Claim: Cross-attention split functions with learned node embeddings capture semantic structure for routing decisions.
- Mechanism: Each node maintains learnable embeddings that attend to input token embeddings via cross-attention (Q from node embeddings, K/V from input tokens). The attention output is aggregated through a node-specific linear layer, allowing each node to specialize in detecting semantic features relevant to its position in the hierarchy.
- Core assumption: Node embeddings can encode thematic information useful for distinguishing document categories at different granularities.
- Evidence anchors:
  - [section 3] "The node embeddings and projection matrices act as different memories that store information from past query and context embeddings, useful for scoring the current inputs."
  - [section 5.1] "Cosine similarity clearly decreases as the distance increases, demonstrating that the learned embeddings reflect the tree structure."
  - [corpus] Weak corpus evidence on attention-based routing; this appears novel in retrieval contexts.
- Break condition: If attention heads are too few or node embeddings insufficiently expressive, routing decisions may fail to capture nuanced semantic distinctions.

## Foundational Learning

- Concept: **Contrastive Learning (InfoNCE)**
  - Why needed here: The entire training objective is built on contrastive loss to align positive query-context pairs in tree space while separating negatives.
  - Quick check question: Can you explain why the loss includes both query→context and context→query terms?

- Concept: **Differentiable / Soft Decision Trees**
  - Why needed here: Standard decision trees have discrete splits with zero gradients; understanding probabilistic relaxation (sigmoid-based soft routing) is essential for grasping how backpropagation works through the tree.
  - Quick check question: Why does product propagation naturally enforce that probabilities sum to 1 at any depth?

- Concept: **Negative Total Variation Distance (nTVD)**
  - Why needed here: The similarity metric for leaf assignment distributions differs from standard cosine similarity; understanding nTVD is required to interpret why certain pairs score higher.
  - Quick check question: Given two leaf assignment distributions a = [0.9, 0.1, 0.0, ...] and b = [0.8, 0.2, 0.0, ...], what is sim(a,b)?

## Architecture Onboarding

- Component map:
  - **Frozen Encoder E** (BGE-Large, BERT, ModernBERT, or LongFormer) -> **Binary Tree T** (depth D=10) -> **Split Functions s_θt** (linear/MLP/cross-attention) -> **Tree Propagation** (product or learned) -> **Leaf Assignment Distributions** -> **InfoNCE Loss with nTVD Similarity**

- Critical path:
  1. Encode query and context with frozen E → dense embeddings
  2. Pass embeddings through all split functions in parallel (not sequentially)
  3. Apply tree propagation to compute leaf assignment distributions
  4. Compute nTVD similarity between query and context distributions
  5. Optimize contrastive loss; gradients flow back to split function parameters only (encoder frozen)

- Design tradeoffs:
  - **Constant vs. Stochastic Depth Training**: Constant optimizes only final layer (best fine representations); stochastic trains all levels (better coarse representations, slight fine-level degradation).
  - **Product vs. Learned Propagation**: Product is simpler; learned propagation is more expressive and performs better at final layer (Figure 8).
  - **Tree Depth**: Deeper trees (D=10+) give more granular representations but require more parameters and may overfit on smaller corpora.

- Failure signatures:
  - **Collapsed Tree**: All documents routed to same leaf → check loss curve for near-zero gradient norms; increase batch size or negative sampling.
  - **Near-Uniform Assignments**: Routing probabilities ≈ 0.5 at all nodes → split function may be underpowered; switch from linear to cross-attention.
  - **Poor Coarse Representations**: Level 1-3 representations useless → likely trained with constant depth scheduler; switch to stochastic depth.

- First 3 experiments:
  1. **Sanity check**: Train ReTreever with depth=3 on a small subset (1K query-context pairs). Verify that positive pairs converge to similar leaf assignments and negative pairs diverge. Visualize routing probabilities at each level.
  2. **Split function ablation**: Compare linear vs. cross-attention split functions on a validation set. Expect cross-attention to outperform at all levels; if not, check attention head count and node embedding dimension.
  3. **Depth scheduler comparison**: Train two models on the same data—one with constant depth, one with stochastic depth. Plot NDCG@10 vs. representation size across all levels to confirm stochastic depth improves coarse representations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the binary tree structure be learned adaptively in terms of width and depth, or pruned dynamically?
- Basis in paper: [explicit] Section 6 states, "A natural extension of this work is to learn the tree structure including the width and depth adaptively, or pruning the tree dynamically based on retrieval needs."
- Why unresolved: The current method relies on a fixed "perfect binary tree" where all levels are completely filled to a specific depth (e.g., 10), regardless of the complexity or distribution of the specific data subset.
- What evidence would resolve it: A modified training algorithm that allows the tree to grow asymmetrically or prune specific branches, resulting in a non-uniform structure that maintains accuracy while reducing parameter count.

### Open Question 2
- Question: Can a "summary decoder" be developed to provide natural language explanations for the semantic content of tree nodes?
- Basis in paper: [explicit] Section 6 proposes "develop[ing] tools for analyzing the hierarchical structure, such as a summary decoder that explains what a node represents based on its learned embeddings."
- Why unresolved: Current interpretability relies on post-hoc analysis using external keyword extraction (Section 5.2) rather than a learned, generative explanation mechanism native to the model.
- What evidence would resolve it: The integration of a generative model that takes node embeddings as input and outputs accurate, human-readable summaries of the document clusters assigned to those nodes.

### Open Question 3
- Question: Is it possible to adapt a trained ReTreever model to new tasks or datasets via partial updates rather than full retraining?
- Basis in paper: [explicit] Section 6 identifies "adapting a learned tree to new tasks or datasets" as a challenge, asking "whether certain subtrees can be updated or new ones grown."
- Why unresolved: The paper currently trains on a fixed corpus; it does not explore the stability of the tree when facing distribution shifts or how to incorporate new knowledge efficiently.
- What evidence would resolve it: A methodology demonstrating that updating specific subtrees or appending new branches for a new domain preserves retrieval accuracy on the new data without degrading performance on the original data.

## Limitations

- Stochastic depth training scheduler details are incompletely specified, potentially affecting reproducibility and coarse representation quality
- Learned propagation mechanism lacks complete architectural specifications (hidden layer dimensions, dropout rate)
- Node embedding dimensions for cross-attention split functions are not explicitly stated
- Limited quantitative validation of tree interpretability beyond retrieval metrics

## Confidence

**High Confidence**: The core mechanism of differentiable tree routing with contrastive learning is well-established theoretically and empirically validated. The InfoNCE loss with nTVD similarity directly optimizes for retrieval performance, and the retrieval metrics (NDCG@10, Recall@K) are convincingly superior to hierarchical baselines at low latency.

**Medium Confidence**: The claims about coarse-to-fine representation quality are supported by experiments but rely on implementation details (stochastic depth, learned propagation) that are incompletely specified. The paper demonstrates that intermediate tree levels provide useful semantic groupings, but the exact conditions for optimal coarse representation quality remain unclear.

**Low Confidence**: The paper's assertions about tree interpretability and semantic organization lack quantitative validation beyond retrieval metrics. While cosine similarity between node embeddings decreases with tree distance (Section 5.1), systematic analysis of semantic coherence within tree branches or comparison to human-annotated hierarchies is absent.

## Next Checks

1. **Stochastic Depth Ablation**: Train ReTreever with three different depth schedulers - constant (final layer only), uniform random (all levels equally likely), and biased (higher levels more probable) - on the same dataset. Measure NDCG@10 across all tree levels and document whether biased sampling truly provides optimal trade-off between coarse and fine representation quality.

2. **Tree Interpretability Analysis**: Select 100 documents from 5 distinct semantic categories. For each document, extract the top-3 most probable leaf nodes across the tree. Calculate intra-category and inter-category similarity scores (using nTVD) to quantify whether semantically related documents cluster together in tree space, independent of retrieval performance.

3. **Negative Sampling Sensitivity**: Vary the number of negative samples per batch (4, 16, 64, 256) while keeping all other hyperparameters constant. Track both retrieval performance and routing probability distributions to determine if performance gains plateau or if tree collapse occurs with insufficient negative sampling.