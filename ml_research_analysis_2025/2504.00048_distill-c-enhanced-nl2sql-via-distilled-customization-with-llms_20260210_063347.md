---
ver: rpa2
title: 'Distill-C: Enhanced NL2SQL via Distilled Customization with LLMs'
arxiv_id: '2504.00048'
source_url: https://arxiv.org/abs/2504.00048
tags:
- date
- year
- sysdate
- extract
- distill-c
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents Distill-C, a framework for enhancing small NL2SQL
  models through distilled customization with LLMs. It uses large teacher LLMs to
  generate synthetic training data tailored to specific customer use cases, then fine-tunes
  smaller open-source models on this data.
---

# Distill-C: Enhanced NL2SQL via Distilled Customization with LLMs

## Quick Facts
- **arXiv ID:** 2504.00048
- **Source URL:** https://arxiv.org/abs/2504.00048
- **Reference count:** 12
- **Primary result:** 36% average improvement in execution accuracy across three LLM families on NL2SQL benchmarks

## Executive Summary
Distill-C is a framework for enhancing small NL2SQL models through distilled customization with large language models (LLMs). The approach uses large teacher LLMs to generate synthetic training data tailored to specific customer use cases, then fine-tunes smaller open-source models on this data. The framework incorporates customization scenarios (AddRef, LearnPrior, FixIt) and a multi-step filtering pipeline for data quality. Experiments show significant improvements in execution accuracy, enabling smaller models to match or outperform larger teacher models while maintaining low computational costs.

## Method Summary
Distill-C follows a three-stage pipeline: first, large LLMs generate diverse natural language queries (NL Synthesizer); second, different LLMs generate corresponding SQL queries (SQL Synthesizer); third, a multi-step filtering pipeline removes invalid or low-quality examples. The framework supports three customization scenarios: AddRef (customer examples), LearnPrior (custom instructions like DateTime handling), and FixIt (error feedback). Students are fine-tuned on the filtered synthetic data using supervised learning. The approach leverages task-specific strengths of different LLMs through decoupled NL and SQL generation.

## Key Results
- 36% average improvement in execution accuracy across three LLM families on standard benchmarks
- 22.6% additional improvement on three internal customer benchmarks
- Student models match or outperform larger teacher models on execution accuracy while requiring fewer computational resources

## Why This Works (Mechanism)

### Mechanism 1: Decoupled NL and SQL Synthesis
Separating natural language generation from SQL generation improves data diversity and leverages model-specific strengths. Different LLMs excel at different tasks—some generate realistic queries while others handle SQL dialects better. The framework first generates NL queries, then independently generates SQL completions, allowing specialization at each stage.

### Mechanism 2: Customization Signals (AddRef + LearnPrior + FixIt)
Integrating customer-specific examples, instructions, and error feedback produces training data aligned with real deployment needs. Three complementary signals—reference NL examples, prior SQL instructions like datetime handling rules, and documented failure cases—guide synthetic data generation toward the target distribution.

### Mechanism 3: Multi-Step Filtering Prevents Noise Degradation
Cascaded quality filters (syntax → execution → LLM jury) remove invalid or irrelevant training examples that would otherwise degrade student model performance. Pattern-based filtering removes dialect-incompatible SQL; execution-based filtering validates queries against real databases; LLM-based evaluation scores semantic correctness and relevance.

## Foundational Learning

- **Concept:** Knowledge Distillation (Teacher-Student Transfer)
  - Why needed here: Distill-C's core premise is transferring capabilities from large proprietary-grade LLMs to smaller deployable models via synthetic data.
  - Quick check question: Can you explain why distillation through synthetic SFT data differs from direct weight-based distillation?

- **Concept:** Execution Accuracy vs. Syntax Matching
  - Why needed here: The paper evaluates on execution accuracy (query results match ground truth), not just SQL syntax similarity—this matters for real database interactions.
  - Quick check question: Why might two syntactically different SQL queries produce identical execution results?

- **Concept:** Model Collapse and Synthetic Data Degradation
  - Why needed here: Section 2.2.3 explicitly mentions using a "bootstrapping dataset" to prevent model collapse when training on synthetic data alone.
  - Quick check question: What happens when a model is trained recursively on its own outputs without external grounding?

## Architecture Onboarding

- **Component map:**
```
[NL Synthesizer] → [SQL Synthesizer] → [Pattern Filter] → [Execution Filter] → [LLM Jury] → [SFT Dataset] → [Student Fine-tuning]
        ↑                   ↑                                                                                    ↑
   AddRef/FixIt        LearnPrior                                                                        Bootstrapping Data
```

- **Critical path:** The SQL Synthesizer output quality directly determines student model ceiling. If synthesized SQL contains dialect errors or semantic mismatches, no amount of filtering fully recovers quality.

- **Design tradeoffs:**
  - Decoupled synthesis increases pipeline complexity but improves diversity (section 2.2.1)
  - LLM-based jury evaluation is scalable but may inherit judge model biases (section 2.2.2)
  - Multiple teacher models improve coverage but require orchestration and cost management

- **Failure signatures:**
  - Low execution accuracy despite high filter pass rate → SQL generator producing semantically incorrect but executable queries
  - Student underperforms teacher on specific task types → FixIt scenario not covering those failure modes
  - Training loss plateaus early → Bootstrapping dataset too small or synthetic data too homogeneous

- **First 3 experiments:**
  1. Baseline replication: Run Setting B (AddRef only) on a single benchmark (e.g., Spider OracleSQL DateTime) to establish improvement delta.
  2. Ablation of filtering: Compare student performance trained on raw synthesized data vs. fully filtered data to quantify filter impact.
  3. Teacher-student gap analysis: Identify task categories where the student still trails the teacher significantly; these are FixIt candidates for iterative improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating preference alignment techniques (e.g., Direct Preference Optimization) further enhance the performance of smaller student models compared to standard supervised fine-tuning?
- Basis in paper: [explicit] The conclusion states, "Our future work will explore extensions to preference alignment training (Rafailov et al., 2024)..."
- Why unresolved: The current Distill-C framework relies on supervised fine-tuning (SFT) using synthetic data. While effective, SFT can lead to hallucinations or subtle logic errors that preference alignment might correct by training the model to distinguish between "good" and "bad" SQL outputs.
- What evidence would resolve it: A comparative study measuring the execution accuracy and semantic correctness of student models trained with Distill-C (SFT) versus those further refined with DPO or RLHF on the same tasks.

### Open Question 2
- Question: Is the Distill-C framework effective for tasks outside of NL2SQL, such as code generation or question answering?
- Basis in paper: [explicit] The conclusion notes future work will explore "applications to other practical tasks."
- Why unresolved: The current methodology is tailored to the structural constraints of SQL (schema linking, syntax validation via execution). It is unclear if the "AddRef," "LearnPrior," and "FixIt" customization scenarios generalize to unstructured tasks or programming languages without a ground-truth execution environment for filtering.
- What evidence would resolve it: Evaluating the Distill-C pipeline on a code generation benchmark (e.g., HumanEval) to see if the filtering and synthesis logic holds for programming syntaxes other than SQL.

### Open Question 3
- Question: How robust is the Distill-C framework when the customer-provided reference examples (AddRef) or error feedback (FixIt) contain errors or noise?
- Basis in paper: [inferred] The methodology relies on the assumption that "product and engineering teams typically have the capacity to provide a few examples... and error feedback."
- Why unresolved: The paper demonstrates success using curated reference examples and failures. In real-world scenarios, user-provided examples may be incorrect or ambiguous. If the teacher LLM synthesizes data based on flawed reference inputs ("AddRef"), the filtering pipeline might not catch logic errors that execute successfully but answer the wrong question.
- What evidence would resolve it: An ablation study introducing varying degrees of noise (incorrect SQL, irrelevant NL) into the AddRef and FixIt inputs to measure the degradation of the student model's accuracy.

## Limitations

- **Customer dataset verification:** The framework's effectiveness on three customer datasets cannot be independently verified as the data is not publicly available.
- **Filtering contribution unclear:** The multi-step filtering pipeline shows promise but lacks ablation studies to quantify the contribution of each filter stage.
- **Execution accuracy sensitivity:** Results depend on execution accuracy metrics, which may be sensitive to database configurations and ground truth definitions.

## Confidence

- **High confidence:** The basic distillation mechanism (teacher → synthetic data → student fine-tuning) is well-established and the implementation details are sufficiently specified for reproduction.
- **Medium confidence:** The decoupled NL/SQL synthesis approach is theoretically sound and supported by qualitative evidence, but lacks direct quantitative comparison against coupled synthesis baselines.
- **Low confidence:** The real-world impact on customer datasets is difficult to verify without access to the actual data and use cases.

## Next Checks

1. **Public benchmark replication:** Implement and test Setting B (AddRef only) on publicly available benchmarks like Spider to establish baseline improvement patterns independently.
2. **Filtering ablation study:** Compare student model performance trained on: raw synthesized data, data with only pattern filtering, data with full multi-step filtering, to quantify filtering contribution.
3. **Teacher-student gap analysis:** Systematically identify task categories where the student model underperforms the teacher, then verify if FixIt customization effectively closes these specific gaps.