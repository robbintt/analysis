---
ver: rpa2
title: 'DYNAMAX: Dynamic computing for Transformers and Mamba based architectures'
arxiv_id: '2504.20922'
source_url: https://arxiv.org/abs/2504.20922
tags:
- mamba
- performance
- early
- transformer
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces DYNAMAX, the first framework to apply early
  exit (EE) mechanisms to both decoder-only Transformers and Mamba architectures,
  addressing the computational inefficiency of large language models (LLMs). By integrating
  EEs into the latter half of models and training classifiers via knowledge distillation,
  DYNAMAX dynamically terminates inference when confidence thresholds are met, reducing
  computational costs.
---

# DYNAMAX: Dynamic computing for Transformers and Mamba based architectures

## Quick Facts
- arXiv ID: 2504.20922
- Source URL: https://arxiv.org/abs/2504.20922
- Reference count: 40
- Introduces DYNAMAX framework applying early exit mechanisms to both Transformers and Mamba architectures for dynamic inference termination

## Executive Summary
This work introduces DYNAMAX, the first framework to apply early exit (EE) mechanisms to both decoder-only Transformers and Mamba architectures, addressing the computational inefficiency of large language models (LLMs). By integrating EEs into the latter half of models and training classifiers via knowledge distillation, DYNAMAX dynamically terminates inference when confidence thresholds are met, reducing computational costs. The study compares Mistral 7B (Transformer) and Codestral 7B (Mamba) on tasks like TruthfulQA, CoQA, and TriviaQA, evaluating computational savings, accuracy, and consistency. Mamba-based EE classifiers demonstrate superior performance, leveraging state-space models to balance efficiency and accuracy. Results show EEs outperform static layer pruning, offering scalable and efficient inference for resource-constrained environments.

## Method Summary
DYNAMAX trains early exit classifiers via knowledge distillation on FineWeb-Edu dataset, placing four classifiers in the latter half of Mistral 7B and Codestral 7B backbones. Three classifier types are tested: CALM-style one-layer FFN, Transformer FFN-based (4x expansion), and Mamba blocks with modified output projection. Classifiers output two values through softmax for exit decisions. Inference uses confidence thresholds to dynamically terminate processing, with Transformers copying KV cache states and Mamba models optionally skipping recomputation. Evaluation on TruthfulQA, CoQA, and TriviaQA measures exact match, F1, BLEU, and Rouge metrics against computational savings compared to static layer pruning.

## Key Results
- Early exit classifiers reduce computational overhead by terminating inference when prediction confidence exceeds thresholds
- Mamba-based EE classifiers outperform stateless FFN alternatives due to state-space models maintaining generation history context
- DYNAMAX achieves better accuracy-efficiency trade-offs than static layer pruning across evaluated tasks
- Confidence threshold tuning is critical to avoid repetitive token generation while maintaining quality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Early exit (EE) classifiers reduce computational overhead by terminating inference once sufficient prediction confidence is achieved, rather than processing the full depth of the network.
- **Mechanism:** Neural network classifiers are attached to intermediate layers (specifically the latter half of the backbone). These classifiers output a binary decision (exit vs. continue) based on a confidence threshold. If the confidence exceeds the threshold, the forward pass stops, and the current hidden state is passed to the output head.
- **Core assumption:** Intermediate layers in the latter half of the model have already captured sufficient semantic information to make accurate predictions for "easier" tokens, and the overhead of the classifier itself does not negate the savings from skipped layers.
- **Evidence anchors:**
  - [abstract]: "...dynamically terminating inference when confidence thresholds are met, reducing computational costs."
  - [section IV-B]: "Each EE classifier outputs two values... to decide whether to exit at the current layer or continue."
  - [corpus]: The neighbor paper "ADEPT" supports the general viability of early-exit processes for Transformers, confirming the broader efficacy of this paradigm.
- **Break condition:** If the confidence threshold is set too low, the model may exit prematurely on complex tokens, leading to inconsistent or repetitive text generation (noted in Section V).

### Mechanism 2
- **Claim:** Using Mamba blocks as EE classifiers provides superior performance and stability compared to stateless feed-forward networks (FFN) or simple CALM-style classifiers.
- **Mechanism:** Unlike FFNs, Mamba blocks maintain an internal state (via State Space Models). When acting as a classifier, this allows the decision gate to contextually correlate with the previously generated tokens, ensuring the exit decision is consistent with the generation history.
- **Core assumption:** The recurrent state of the Mamba classifier adds informational value that justifies its parameter count, offering a better efficiency-accuracy trade-off than stateless alternatives.
- **Evidence anchors:**
  - [section V]: "Mamba shows as a better model for EE prediction, possibly because Mamba provides a state, which can keep information of the currently generated text... which does not happen with the FFN."
  - [abstract]: "...repurpose Mamba as an efficient EE classifier... highlighting the adaptability of Mamba."
  - [corpus]: Corpus evidence specifically validating Mamba-as-a-classifier is absent; current literature focuses on Mamba as a backbone replacement rather than an auxiliary component.
- **Break condition:** The benefits may diminish if the sequence length is extremely short, where the Mamba state has insufficient context to differentiate itself from an FFN.

### Mechanism 3
- **Claim:** In Transformers, computational efficiency is further enhanced by copying Key-Value (KV) cache states rather than recomputing them when an early exit occurs.
- **Mechanism:** When an early exit is triggered, subsequent layers miss the update for the current token. Instead of a "partial forward" to compute these missing states, the system copies the KV values from the last active layer. This avoids the math of attention calculation for the skipped blocks.
- **Core assumption:** The error introduced by using stale or copied projections for the skipped layers is negligible for the final output quality compared to the computational savings.
- **Evidence anchors:**
  - [section IV-D]: "...copying the cached values directly to subsequent blocks, bypassing the need for recomputation."
  - [section V]: "In the Transformer model, the recomputation does not affect too much total computation spent... [copying] may allow for more consistent results."
- **Break condition:** Accuracy may degrade in tasks requiring precise long-range dependency tracking where the specific key-value representations of deeper layers are critical.

## Foundational Learning

- **Concept:** **State Space Models (SSMs) & Mamba**
  - **Why needed here:** The paper leverages Mamba both as a backbone and as a classifier component. Understanding that SSMs compress context into a recurrent state (unlike the quadratic attention of Transformers) is essential to grasp why Mamba classifiers handle generation history better.
  - **Quick check question:** How does the computational complexity of a Mamba block scale with sequence length compared to a Transformer's self-attention?

- **Concept:** **Knowledge Distillation**
  - **Why needed here:** The EE classifiers are not trained from scratch but are "distilled" from the full model (the oracle). This ensures the intermediate classifiers mimic the behavior of the final layer.
  - **Quick check question:** In this context, does the student model (EE classifier) learn to match the hard labels or the soft probability distribution of the teacher (oracle)?

- **Concept:** **Autoregressive Decoding & KV Caching**
  - **Why needed here:** The paper proposes a modification to standard KV caching (copying vs. partial forward). One must understand that standard inference stores keys and values to avoid recomputation to see why "missing states" in skipped layers is a problem to solve.
  - **Quick check question:** What specific data is stored in the KV cache during autoregressive generation, and what happens to it if a layer is skipped?

## Architecture Onboarding

- **Component map:** Input token -> Backbone blocks (Transformer or Mamba) -> Layer Norm outputs -> EE Classifiers (CALM, FFN, or Mamba) -> Exit Head (if confidence threshold met) or Continue to next block

- **Critical path:**
  1. Forward token through backbone blocks.
  2. At designated EE layers, fork hidden state -> Layer Norm -> EE Classifier.
  3. Classifier outputs exit confidence.
  4. If confidence > threshold: Halt loop, copy/update KV cache (if Transformer) or state (if Mamba), proceed to Exit Head.
  5. If confidence < threshold: Continue to next backbone block.

- **Design tradeoffs:**
  - **Classifier Capacity vs. Overhead:** A Mamba classifier is more accurate but has higher parameter overhead than a simple FFN.
  - **Threshold Selection:** A high threshold preserves accuracy but yields lower computational savings; a low threshold risks "degeneration" (repetitive outputs).
  - **State Handling:** Recomputing states (Partial Forward) is safer for accuracy but costs compute; Copying/Skipping is faster but theoretically noisier.

- **Failure signatures:**
  - **Degenerative Looping:** The model repeats the same token infinitely. This signals the confidence threshold is too low or the classifier is insufficiently trained (Section V).
  - **Accuracy Cliff:** A sudden drop in Exact Match scores suggests the EE layers are positioned too early in the network or the "relaxation" (top-k) during training was insufficient.

- **First 3 experiments:**
  1. **Baseline Calibration:** Run the backbone (Mistral/Codestral) without EEs on the target dataset (e.g., TriviaQA) to establish the accuracy and computational cost baseline.
  2. **Classifier Ablation:** Insert a single Mamba-based EE classifier at 75% depth. Train using the relaxation method. Compare accuracy vs. FLOPs reduction against a simple FFN classifier at the same depth.
  3. **State Handling Test:** For the Transformer backbone, compare "Partial Forward" vs. "Copy State" methods on the TruthfulQA generation task to quantify the trade-off between generation quality and latency.

## Open Questions the Paper Calls Out

- **Question:** Can adaptive thresholds that dynamically respond to context complexity, latency requirements, or device constraints outperform fixed confidence thresholds?
  - **Basis in paper:** [explicit] "Future work could refine EE strategies through adaptive thresholds that respond dynamically to context, latency, or device constraints."
  - **Why unresolved:** The current implementation uses fixed confidence thresholds requiring manual tuning, limiting deployment flexibility across varying operational conditions.
  - **What evidence would resolve it:** Comparative experiments measuring efficiency-performance trade-offs using adaptive vs. fixed thresholds across diverse contexts and hardware configurations.

- **Question:** Does integrating early exit mechanisms during pretraining produce better efficiency-performance trade-offs than post-hoc knowledge distillation training?
  - **Basis in paper:** [explicit] "Applying EEs in pretraining, or integrating parameter-efficient fine-tuning methods, may foster confidence-aware models that perform effectively across diverse, high-demand tasks."
  - **Why unresolved:** Current EE classifiers are trained post-hoc, potentially limiting the model's ability to develop early-exit-friendly internal representations.
  - **What evidence would resolve it:** Controlled comparison of models pretrained with EE mechanisms versus post-hoc trained EE classifiers on identical downstream tasks.

- **Question:** How well do DYNAMAX's early exit mechanisms generalize across different model scales and architectures beyond Mistral 7B and Codestral 7B?
  - **Basis in paper:** [inferred] The study evaluates only two specific 7B-parameter models, leaving scalability and cross-architecture generalizability unexamined.
  - **Why unresolved:** Different model sizes and architectural families may exhibit distinct internal representation patterns affecting EE classifier effectiveness.
  - **What evidence would resolve it:** Systematic evaluation across multiple model families (e.g., LLaMA, Gemma, GPT) and parameter scales (1B to 70B+).

## Limitations
- Experimental scope limited to decoder-only models (Mistral 7B, Codestral 7B) on three specific tasks (TruthfulQA, CoQA, TriviaQA)
- Static placement of four EE classifiers in "latter half" without systematic exploration of optimal placement strategies
- Comparison against static layer pruning but not against other dynamic sparsity methods or quantization approaches

## Confidence

- **High Confidence:** The core mechanism of early exit via confidence thresholding is well-established and experimentally validated. The comparison showing EE classifiers outperform static pruning on accuracy and efficiency is robust within the tested configurations.
- **Medium Confidence:** The superiority of Mamba-based classifiers over FFN variants is demonstrated, but the analysis is limited to a single model pair and task set. The explanation linking Mamba's statefulness to better generation history tracking is plausible but not rigorously proven against alternatives. The claim that KV cache copying is sufficient for quality preservation is supported but lacks quantitative ablation on cache fidelity impact.
- **Low Confidence:** The assertion that DYNAMAX is "the first framework" to apply EEs to Mamba architectures is difficult to verify without exhaustive literature review. The long-term stability of low-threshold settings (prone to repetition) under diverse generation scenarios is not stress-tested.

## Next Checks
1. **Generalization Test:** Evaluate DYNAMAX on encoder-decoder architectures (e.g., LLaMA-2) and multimodal models (e.g., LLaVA) to assess cross-architecture robustness. Compare EE performance on tasks like summarization and visual question answering.

2. **State Handling Ablation:** Conduct a controlled experiment isolating the impact of KV cache copying vs. partial forward recomputation on generation quality. Measure BLEU/Rouge metrics degradation per task when skipping layers, and profile actual GPU latency to quantify the real-world efficiency gain.

3. **Hybrid Classifier Design:** Implement and test a hybrid EE classifier combining Mamba blocks with FFN layers (e.g., Mamba for state tracking, FFN for final classification). Compare accuracy and computational overhead against pure Mamba classifiers to determine if statefulness alone justifies the parameter cost.