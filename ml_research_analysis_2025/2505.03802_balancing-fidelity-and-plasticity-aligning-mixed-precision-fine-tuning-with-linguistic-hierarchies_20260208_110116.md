---
ver: rpa2
title: 'Balancing Fidelity and Plasticity: Aligning Mixed-Precision Fine-Tuning with
  Linguistic Hierarchies'
arxiv_id: '2505.03802'
source_url: https://arxiv.org/abs/2505.03802
tags:
- qr-adaptor
- rank
- adalora
- qlora
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of deploying and fine-tuning
  large language models (LLMs) on resource-constrained edge devices by navigating
  the trade-off between memory footprint and task performance. The core issue is the
  "Fidelity-Plasticity Trade-off": a layer''s capacity to adapt (plasticity) is constrained
  by the information capacity of its frozen weights (fidelity), making aggressive
  quantization of semantically critical layers ineffective regardless of adapter rank.'
---

# Balancing Fidelity and Plasticity: Aligning Mixed-Precision Fine-Tuning with Linguistic Hierarchies

## Quick Facts
- **arXiv ID:** 2505.03802
- **Source URL:** https://arxiv.org/abs/2505.03802
- **Reference count:** 34
- **Primary result:** QR-Adaptor framework achieves SOTA results under strict memory constraints, with a 4-bit model rivaling 16-bit baselines

## Executive Summary
This paper addresses the challenge of deploying and fine-tuning large language models (LLMs) on resource-constrained edge devices by navigating the trade-off between memory footprint and task performance. The core issue is the "Fidelity-Plasticity Trade-off": a layer's capacity to adapt (plasticity) is constrained by the information capacity of its frozen weights (fidelity), making aggressive quantization of semantically critical layers ineffective regardless of adapter rank. To resolve this, the authors propose QR-Adaptor, a unified framework that jointly optimizes per-layer quantization bit-width and LoRA rank using a multi-objective discrete search aligned with the model's linguistic hierarchy. Extensive experiments demonstrate that QR-Adaptor establishes a new Pareto frontier: notably, a model fine-tuned under a strict 4-bit memory budget achieves performance rivaling 16-bit baselines, showing that precise resource alignment is as critical as model size.

## Method Summary
The paper proposes QR-Adaptor, a three-stage framework for jointly optimizing per-layer quantization bit-width and LoRA rank in LLM fine-tuning. The method begins with Fidelity Sensitivity Profiling (Phase I) using KL-divergence to identify which layers are most sensitive to quantization. Next, an NSGA-II evolutionary search (Phase II) explores the discrete search space of bit-width and rank combinations, using proxy evaluations (1-epoch fine-tuning) to estimate performance. Finally, Bayesian Optimization with a Matérn-5/2 kernel (Phase III) refines the Pareto front to find the optimal configuration. The framework is validated on LLaMA-3-8B and Qwen-3 models across multiple benchmarks including Alpaca, ARC, PIQA, HellaSwag, WinoGrande, MMLU, GSM8K, WikiText-2, and C4.

## Key Results
- QR-Adaptor achieves SOTA results under strict memory constraints, with a 4-bit model rivaling 16-bit baselines
- Joint optimization of bit-width and rank outperforms fixed-precision approaches across all tested models and benchmarks
- The framework establishes a new Pareto frontier between accuracy and memory usage, demonstrating that precise resource alignment is as critical as model size

## Why This Works (Mechanism)

### Mechanism 1: Fidelity-Gated Plasticity
The adaptation potential of a trainable adapter (Plasticity) is conditionally bounded by the information capacity of the frozen quantized weights (Fidelity). Aggressive quantization introduces noise that destroys the underlying signal in pre-trained weights. If this noise exceeds a critical threshold, the gradient signal required to train the adapter becomes too corrupted to effectively modulate the output, rendering adapter rank irrelevant. This creates an "information bottleneck" that cannot be bypassed by simply increasing LoRA rank. Core assumption: required rank is a function of both quantization error and task demand. Evidence: Config C (Deep 2-bit, Rank 16) fails significantly compared to Config D (Deep 4-bit, Rank 8) despite higher total rank, validating that plasticity cannot compensate for lost fidelity in semantic layers.

### Mechanism 2: Linguistic Hierarchy Alignment
Optimal resource allocation follows the model's linguistic structure, requiring high fidelity in deep semantic layers and allowing compression in shallow syntactic layers. Transformer layers exhibit functional specialization: shallow layers encode robust surface-level syntax and can tolerate low fidelity (e.g., 2-bit), while deep layers handle complex semantic reasoning and require high fidelity (e.g., 4-bit or 8-bit). By aligning bit-widths with this hierarchy, the system preserves critical reasoning capacity while aggressively pruning redundancy. Core assumption: information entropy of a layer's output correlates with its semantic importance and sensitivity to quantization noise. Evidence: Search algorithm autonomously allocates higher bits and ranks to deep layers (20-32) and compresses shallow layers (0-10).

### Mechanism 3: Gradient-Free Discrete Search
Joint optimization of bit-width and rank is effectively navigated by a multi-stage discrete search (Evolutionary + Bayesian) rather than differentiable proxies. Quantization is a discrete operation that creates a non-smooth loss landscape. Differentiable proxies often fail to capture the sharp performance drops caused by changing bit-widths. The paper uses a Genetic Algorithm for global exploration followed by Bayesian Optimization for local refinement to directly evaluate configurations without relying on gradients. Core assumption: "Proxy Tuning" phase (few steps of training) is sufficient to estimate a configuration's potential without full convergence. Evidence: NSGA-II directly evaluates discrete configurations, avoiding the gradient mismatch problem; Matérn-5/2 Kernel specifically models rough, non-smooth landscapes characteristic of discrete quantization.

## Foundational Learning

- **Information Bottleneck in Quantization**
  - Why needed here: To understand why you cannot just "add more LoRA rank" to fix a bad quantization. The frozen weights serve as the carrier signal; if the carrier is destroyed, the modulation (adapter) is useless.
  - Quick check question: Does increasing LoRA rank always recover performance lost due to weight compression? (Answer: No, only if fidelity is above the noise threshold).

- **Non-dominated Sorting Genetic Algorithm II (NSGA-II)**
  - Why needed here: This is the core engine of Phase II. You need to understand how it balances multiple objectives (Accuracy vs. Memory) to find a "Pareto Frontier" rather than a single solution.
  - Quick check question: How does NSGA-II handle the trade-off between two competing objectives like model size and accuracy?

- **Bayesian Optimization with Matérn Kernel**
  - Why needed here: This explains the "refinement" phase. The Matérn kernel is crucial here because standard smooth kernels assume the performance curve is smooth, which is false for bit-flips.
  - Quick check question: Why is a less smooth kernel (Matérn 5/2) preferred over a Gaussian kernel for searching bit-widths?

## Architecture Onboarding

- **Component map:** Calibration Dataset → Fidelity Sensitivity Profiling → NSGA-II Evolutionary Search → Bayesian Optimization → Optimal Config (bit-width, rank tuples)
- **Critical path:** The "Proxy Evaluation" step inside Phase II/III is the bottleneck. This is where the system briefly fine-tunes a candidate configuration to see if it "collapses." If the proxy metric (loss decrease) is misaligned with the final metric (accuracy), the search fails.
- **Design tradeoffs:** Search Cost vs. Zero-Shot: QR-Adaptor requires ~0.5 epochs of compute for search, whereas QLoRA is "free" (instant start). The tradeoff pays off only if the recovered performance justifies the upfront compute or if memory constraints are strict. Discrete vs. Differentiable: The paper trades off the speed of gradient-based search for the accuracy of discrete evaluation to avoid "gradient mismatch."
- **Failure signatures:** Random Init Failure: Starting Phase II with random configurations instead of Phase I profiles leads to inefficient convergence. Deep Layer Collapse: If the search allocates 2-bit to deep layers (violating the linguistic hierarchy), the model exhibits high perplexity and low reasoning scores.
- **First 3 experiments:**
  1. Sensitivity Profile Validation: Run Phase I on a target model and plot the sensitivity scores vs. layer index to confirm that deep layers are indeed flagged as "high sensitivity."
  2. Pilot Study Reproduction: Manually configure a "Config C" (Shallow High-bit, Deep Low-bit) and "Config D" (Shallow Low-bit, Deep High-bit) on a small model to observe the fidelity-plasticity break point firsthand.
  3. Ablation on Search Budget: Run the full pipeline with reduced evolutionary generations to measure the degradation in the final Pareto frontier and determine the minimum viable search cost.

## Open Questions the Paper Calls Out
- **Can predictor-based NAS eliminate computational overhead of Fidelity Sensitivity Profiling?** The authors state future work will explore predictor-based neural architecture search to further accelerate the profiling phase, as the current three-stage pipeline requires approximately 0.5 training epochs of compute.
- **How to modify framework for rapid, one-shot adaptation for continuously changing tasks?** The methodology assumes the discovered configuration is static and reusable, but does not address dynamic environments where the "optimal" configuration might shift frequently alongside the task distribution.
- **To what extent does calibration dataset choice influence generality of discovered configuration?** The methodology relies on calibration set to measure Fidelity Sensitivity and evaluate proxy tuning performance, but if the calibration data fails to trigger specific linguistic hierarchies, the sensitivity profiling might misallocate bits.

## Limitations
- Core claims hinge on strong assumptions about universal information thresholds and linguistic hierarchy patterns that may not generalize across all model architectures and tasks
- Proxy evaluation phase uses only 1 epoch of fine-tuning to estimate final performance, which may break down for more complex tasks or different training dynamics
- Search space is vast and proxy evaluation introduces uncertainty, making the three-stage methodology computationally intensive compared to zero-cost heuristic methods

## Confidence
- **High Confidence:** Empirical demonstration that joint optimization of bit-width and rank outperforms fixed-precision approaches (QR-Adaptor achieves SOTA results under memory constraints). The failure of Config C compared to Config D provides strong evidence for the fidelity-gated plasticity mechanism.
- **Medium Confidence:** Three-stage search methodology (Profiler → Explorer → Refiner) as an effective approach for discrete mixed-precision optimization. While results are compelling, the search space is vast and proxy evaluation introduces uncertainty.
- **Low Confidence:** Universal applicability of linguistic hierarchy alignment across different model families and the assumption that KL-divergence sensitivity scores directly translate to optimal resource allocation.

## Next Checks
1. **Task-Specific Threshold Validation:** Run the pilot study (Config C vs D) across multiple downstream tasks (simple classification, reasoning, code generation) to map how the fidelity-plasticity break point shifts with task complexity.
2. **Cross-Architecture Generalization Test:** Apply QR-Adaptor to a structurally different LLM (e.g., LLaMA vs. Mixtral's MoE architecture) and verify whether the same linguistic hierarchy patterns emerge.
3. **Proxy Evaluation Correlation Analysis:** Systematically vary the proxy evaluation duration (0.5, 1, 2, 5 epochs) and measure the correlation between proxy performance and final converged accuracy across different configurations.