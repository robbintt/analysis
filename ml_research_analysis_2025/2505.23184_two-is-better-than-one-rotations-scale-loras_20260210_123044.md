---
ver: rpa2
title: 'Two Is Better Than One: Rotations Scale LoRAs'
arxiv_id: '2505.23184'
source_url: https://arxiv.org/abs/2505.23184
tags:
- lora
- radargate
- gating
- performance
- generalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes RadarGate, a novel gating method that introduces
  rotational operations of LoRA representations to boost the expressiveness and facilitate
  richer feature interactions among multiple LoRAs for scalable large language models
  (LLMs). RadarGate consists of two key components: RotationGate and StretchGate.'
---

# Two Is Better Than One: Rotations Scale LoRAs

## Quick Facts
- **arXiv ID**: 2505.23184
- **Source URL**: https://arxiv.org/abs/2505.23184
- **Reference count**: 40
- **One-line primary result**: RadarGate achieves 30%-50% higher accuracy than rule-based methods and 5%-10% improvements over learnable baselines on generalization benchmarks, with sustained superiority as LoRA modules increase from 5 to 40.

## Executive Summary
This paper proposes RadarGate, a novel geometrically inspired gating method that introduces rotational operations of LoRA representations to boost the expressiveness and facilitate richer feature interactions among multiple LoRAs for scalable large language models (LLMs). The key insight is that standard weighted-sum gating confines outputs to a fixed convex cone, limiting expressiveness. RadarGate addresses this by applying input-dependent rotational transformations followed by magnitude scaling, creating a more flexible hypothesis space for the gating function. Extensive experiments on 6 public benchmarks across 21 tasks demonstrate that RadarGate achieves better fitting capability, generalization, and scalability for LoRA-MoE compared to existing methods.

## Method Summary
RadarGate consists of two key components: RotationGate and StretchGate. RotationGate dynamically injects relative angular rotation information through a learnable parameter module, while StretchGate stretches the magnitude of each LoRA representation by assigning weights to them. The method constructs a binary relation between each LoRA module and a reference set of other LoRAs, then calculates rotation angles based on this relation. These rotations transform the output space from a static convex hull to a dynamic one, whose union over all inputs is strictly larger. The rotated representations are then scaled by StretchGate to produce the final weighted sum.

## Key Results
- RadarGate achieves 30%-50% higher accuracy than rule-based methods on generalization benchmarks
- RadarGate shows 5%-10% improvements over learnable baselines across multiple tasks
- RadarGate demonstrates near-monotonic improvement with increasing LoRA modules (5-40), achieving up to 8% maximum gain

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Introducing rotational degrees of freedom to LoRA representations may expand the expressiveness of LoRA-MoE gating beyond the fixed convex cone limitation.
- Mechanism: The RotationGate applies input-dependent rotational transformations to individual LoRA expert outputs, transforming the output space from a static convex hull (H) to a dynamic convex hull (H'), whose union over all inputs is strictly larger. This allows fitting of target outputs that lie outside the fixed convex cone.
- Core assumption: The ideal target contribution for downstream tasks often lies outside the convex cone formed by simple weighted sums of original LoRA outputs.
- Evidence anchors: [abstract] "This motivates us to propose RadarGate... to boost the expressiveness..."; [section 4.2] "According to Lemma 2, we have S_x H'(x) ⊃ H. Thus ∆ytarget ∈ S_x H'(x) \ H (outside H) can still be fitted."
- Break condition: The mechanism's benefit would diminish if optimal target outputs consistently lie inside the fixed convex cone, or if rotational parameters fail to learn meaningful angles.

### Mechanism 2
- Claim: The rotational operation may facilitate cross-LoRA synergy by learning contrastive relationships, aligning semantically similar representations and pushing dissimilar ones apart.
- Mechanism: RotationGate calculates rotation angles based on the binary relationship between a LoRA module and a reference set of others, inducing a contrastive effect where semantically similar LoRAs are rotated closer together while unrelated ones are pushed further apart.
- Core assumption: There are meaningful latent structures or task relationships among LoRA experts that can be captured through geometric transformations.
- Evidence anchors: [abstract] "We also provide valuable insights, revealing that the rotations to each pair of representations are contrastive..."; [section 5.4] "This suggests that the rotations to each pair of representations are contrastive..."
- Break condition: The benefit would fail if LoRA modules are highly specialized with minimal shared semantics, making rotation proximity irrelevant.

### Mechanism 3
- Claim: The StretchGate may mitigate underfitting by providing an extra optimization path.
- Mechanism: By separating rotation and magnitude scaling, RadarGate creates a more complex hypothesis space (Kours) for the gating function compared to standard weighted-sum methods (Kgate). This extra freedom allows capturing complex ideal weight distributions that simpler gates cannot.
- Core assumption: Underfitting in standard LoRA-MoE is due to insufficient expressiveness in the gating function, not other issues like data scarcity.
- Evidence anchors: [section 4.2] "Due to the added RotationGate module, RadarGate has an advantage for ideal mapping g*..."; [Figure 3a, Table 1] Experimental results show higher accuracy compared to learnable baselines.
- Break condition: The advantage would disappear if rotational and scaling operations interact negatively during optimization, leading to instability or overfitting.

## Foundational Learning

- **Convex Hull/Convex Cone**
  - Why needed here: The paper's central argument hinges on the idea that standard weighted-sum gating confines outputs to a fixed convex cone, limiting expressiveness.
  - Quick check question: Can a convex combination of two non-parallel vectors ever produce a result outside the line segment connecting them?

- **Rotational Degrees of Freedom**
  - Why needed here: The core innovation is introducing rotation as a second degree of freedom alongside magnitude scaling.
  - Quick check question: How does rotating a vector change its relationship to other vectors in the space without altering its magnitude?

- **Mixture of Experts (MoE) Gating**
  - Why needed here: RadarGate is a novel gating method for LoRA-MoE systems.
  - Quick check question: What is the primary role of a gating network in a Mixture of Experts model?

## Architecture Onboarding

- **Component map**: Input x → all LoRA outputs v_i → construction of relational context via Map → calculation of rotational angles α_ri → application of rotation matrix R_i to v_i → calculation of scalar weights g_i via StretchGate → final weighted sum of rotated outputs

- **Critical path**: input x → all LoRA outputs v_i → construction of relational context via Map → calculation of rotational angles α_ri using learnable θ_r → application of rotation matrix R_i to v_i → calculation of scalar weights g_i via StretchGate → final weighted sum of rotated outputs

- **Design tradeoffs**:
  - Expressiveness vs. Complexity: RotationGate adds expressiveness and scalability but increases learnable parameters and computational steps.
  - Fixed vs. Dynamic Output Space: Standard gating trades flexibility for stable output space; RadarGate gains dynamic space at cost of complex optimization.
  - Rule-based vs. Learnable: Rule-based methods are simpler but less performant than learnable ones like RadarGate.

- **Failure signatures**:
  - Performance Collapse with Few LoRAs: With very small N (<3), relational information may be too sparse, and RotationGate overhead may not justify itself.
  - Training Instability/Non-convergence: Joint optimization of rotational and scaling parameters could be unstable, leading to erratic loss curves.
  - No Gain over Stretch-Only: If ablation studies show StretchGate alone matches RadarGate, the rotational component isn't learning useful signal.

- **First 3 experiments**:
  1. **Sanity Check (Fitting)**: Train and test on same dataset (FLAN subset) with small number of LoRAs (5). Verify RadarGate achieves higher accuracy than Stretch-Only gate.
  2. **Scalability Stress Test**: Systematically increase LoRA modules (5-40) on GLUE. Plot accuracy vs. number of modules to confirm RadarGate maintains near-monotonic improvement.
  3. **Ablation Study**: Run experiments with RotationGate only, StretchGate only, and full RadarGate on multiple benchmarks. Confirm full system consistently outperforms individual components.

## Open Questions the Paper Calls Out
- The paper aims to extend RadarGate to multimodal scenarios for image and video data in future work.
- The paper believes RadarGate can bring communication efficiency benefits in distributed training environments.
- The paper does not explore how performance varies when increasing the internal LoRA rank beyond the fixed value of 8 used in experiments.

## Limitations
- The theoretical claims rely heavily on geometric intuitions, but practical benefits depend on learning meaningful rotational parameters.
- The Map(L_i) function's exact implementation details and rotation rank r_a are unspecified, creating reproducibility gaps.
- Comparisons are primarily against established approaches rather than the most recent state-of-the-art LoRA-MoE methods.

## Confidence

- **High Confidence**: The empirical results showing RadarGate's superior performance on fitting and generalization benchmarks across multiple tasks.
- **Medium Confidence**: The theoretical justification that introducing rotational degrees of freedom expands expressiveness beyond the convex cone limitation.
- **Medium Confidence**: The claim about contrastive rotational effects facilitating cross-LoRA synergy.

## Next Checks

1. **Ablation Study with Controlled Rotation Parameters**: Implement an experiment where rotation angles α are randomly initialized but frozen (no learning), to isolate contribution of rotational structure versus learnable rotational parameters.

2. **Transfer Learning Test**: Train RadarGate on one set of LoRA experts (e.g., Math and Reasoning) and evaluate on a different set (e.g., Language and Translation) to test whether rotational contrastive effects generalize across semantically distinct LoRA groups.

3. **Optimization Stability Analysis**: Conduct a hyperparameter sweep over rotation rank r_a and learning rate for θ_r, measuring training stability (loss variance) and final performance to identify the sweet spot where rotational expressiveness is maximized without causing optimization instability.