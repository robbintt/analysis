---
ver: rpa2
title: "Na Pr\xE1tica, qual IA Entende o Direito? Um Estudo Experimental com IAs Generalistas\
  \ e uma IA Jur\xEDdica"
arxiv_id: '2510.18108'
source_url: https://arxiv.org/abs/2510.18108
tags:
- encia
- para
- crit
- sistemas
- avaliac
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study developed and applied a systematic evaluation protocol\
  \ to compare the quality of legal responses from domain-specialized versus general-purpose\
  \ AI systems. Four conversational AI systems\u2014JusIA (a specialized legal AI),\
  \ ChatGPT Free, ChatGPT Plus, and Gemini\u2014were tested on 15 real-world legal\
  \ tasks across five use cases."
---

# Na Prática, qual IA Entende o Direito? Um Estudo Experimental com IAs Generalistas e uma IA Jurídica

## Quick Facts
- arXiv ID: 2510.18108
- Source URL: https://arxiv.org/abs/2510.18108
- Reference count: 0
- Primary result: JusIA consistently outperformed general-purpose AI systems on legal quality metrics across correctness, reliability, completeness, and fluency

## Executive Summary
This study developed and applied a systematic evaluation protocol to compare the quality of legal responses from domain-specialized versus general-purpose AI systems. Four conversational AI systems—JusIA (a specialized legal AI), ChatGPT Free, ChatGPT Plus, and Gemini—were tested on 15 real-world legal tasks across five use cases. Responses were evaluated by 48 legal professionals using criteria grounded in legal theory: correctness, completeness, reliability, and fluency. JusIA consistently outperformed the general-purpose systems, achieving higher scores across all criteria, with particular strength in correctness and reliability. Statistical analysis confirmed the results were significant. The findings demonstrate that domain specialization substantially improves the quality and trustworthiness of AI-generated legal content, while also validating the feasibility of using theory-based, empirical protocols for evaluating legal AI systems.

## Method Summary
The study evaluated four AI systems (JusIA, ChatGPT Free/Plus, Gemini) on 15 legal tasks across five use cases using a double-blind protocol with 48 legal professionals. The evaluation used a weighted scoring formula (2×correctness + completeness + 2×reliability + fluency) based on 10 binary rubric questions grounded in legal theory. Statistical significance was assessed via Friedman test (χ² = 28.7) and Nemenyi post-hoc analysis, with inter-rater reliability measured at Krippendorff's α = 0.6.

## Key Results
- JusIA achieved 0.86 average score vs. 0.63-0.72 for general-purpose systems
- JusIA demonstrated 88% reliability rate with 43% lower hallucination rates than general-purpose systems
- Statistical analysis confirmed performance differences were significant across all use cases

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specialized Training Data Reduces Legal Hallucinations
- Claim: Models trained on curated legal corpora produce fewer fabricated citations and normative errors than general-purpose models
- Mechanism: JusIA was "ajustado com corpora normativos, doutrinários e jurisprudenciais brasileiros" (adjusted with normative, doctrinal, and jurisprudential Brazilian corpora), grounding outputs in verifiable primary sources rather than pattern-matching across general web text
- Core assumption: Curated domain data contains accurate legal citations and valid normative reasoning that transfers during training
- Evidence anchors:
  - [abstract] "JusIA, a domain-specialized model, consistently outperformed the general-purpose systems, showing that both domain specialization and a theoretically grounded evaluation are essential for reliable legal AI outputs"
  - [section 4.1] "JusIA apresentou 88% de confiabilidade... taxa de alucinação 43% inferior à dos sistemas generalistas avaliados"
  - [corpus] Related work (KCL benchmark) similarly finds domain knowledge is separable from reasoning ability—specialized evaluation isolates this
- Break condition: If training data contains outdated or incorrect legal references, specialization amplifies errors rather than reducing them

### Mechanism 2: Theory-Grounded Evaluation Criteria Capture Legal Quality
- Claim: Translating legal theory (material correctness, systematic coherence, argumentative integrity) into weighted empirical criteria (correctness, reliability, completeness, fluency) produces evaluation protocols aligned with professional legal judgment
- Mechanism: The protocol assigns double weight to correctness and reliability because these correspond to "material correction" and "argumentative integrity" in legal theory—dimensions professionals prioritize for trustworthy outputs
- Core assumption: Legal professionals share a coherent, domain-specific notion of quality that can be operationalized through structured rubrics
- Evidence anchors:
  - [section 2.3, Table 1] Maps "Correção material → Corretude," "Coerência argumentativa → Confiabilidade," etc.
  - [section 3.3] "Os critérios de confiabilidade e corretude receberam peso interpretativo maior... por representarem o núcleo das expectativas profissionais"
  - [corpus] LexRAG benchmark similarly emphasizes domain-specific evaluation for legal RAG systems, reinforcing this approach
- Break condition: If evaluators lack sufficient domain expertise or if legal theory categories don't align with practical workflows, the weighting becomes arbitrary

### Mechanism 3: Double-Blind Expert Evaluation Controls for Platform Bias
- Claim: Blinding evaluators to model identity and randomizing response order reduces bias toward familiar interfaces or expectations about "AI-generated" style
- Mechanism: Evaluators saw responses without knowing which system produced them; order was randomized per case. This isolates content quality from brand reputation or formatting preferences
- Core assumption: Evaluators can judge legal quality independently of stylistic familiarity or prior exposure to specific AI systems
- Evidence anchors:
  - [section 3.4] "Cegamento duplo adaptado: os avaliadores não foram informados da origem dos conteúdos... Randomização da ordem de exibição"
  - [section 3.7] Krippendorff's α = 0.6 indicates "nível de consistência substancial" among 48 evaluators
  - [corpus] Insufficient direct corpus evidence on blinding efficacy in legal AI evaluation; related work focuses on automated metrics
- Break condition: If evaluators recognize distinctive output patterns from specific models (e.g., Gemini's more fluid style), blinding is compromised

## Foundational Learning

- **Legal Hermeneutics (Law as Interpretive System)**:
  - Why needed here: The paper explicitly grounds evaluation in Hart, Dworkin, and Luhmann—understanding law as argumentative reconstruction, not rule lookup. Without this, you might default to fact-verification metrics that miss structural legal reasoning errors
  - Quick check question: Can you explain why a legally "correct" answer requires more than factual accuracy?

- **Weighted Multi-Criteria Evaluation**:
  - Why needed here: The Jus IA Index weights correctness and reliability ×2 over completeness and fluency, reflecting professional priorities. Understanding why prevents misapplying equal-weight benchmarks
  - Quick check question: If fluency scores were weighted equally to correctness, how would that distort legal quality assessment?

- **Inter-Rater Reliability (Krippendorff's α)**:
  - Why needed here: With 48 evaluators judging subjective criteria, α = 0.6 validates that the protocol produces consistent judgments, not random noise
  - Quick check question: What would α < 0.4 suggest about your evaluation rubric?

## Architecture Onboarding

- **Component map**: Input cases -> AI models -> Anonymized responses -> Double-blind evaluation -> Weighted scoring -> Statistical analysis -> Inter-rater reliability check
- **Critical path**:
  1. Define use-case taxonomy (e.g., "Pesquisar Precedente," "Gerar Documento")
  2. Map legal theory → empirical criteria with domain experts
  3. Collect real queries with LGPD-compliant anonymization
  4. Deploy double-blind evaluation with randomized presentation
  5. Compute weighted scores + inter-rater reliability + statistical significance
- **Design tradeoffs**:
  - Binary rubric ("sim/não") forces decisive judgment but loses nuance vs. Likert scales
  - Small case count (15) limits generalization; larger benchmarks needed for production claims
  - Human evaluation is expensive but necessary—automated "LLM-as-judge" showed consistency issues in related work (OAB-Bench)
- **Failure signatures**:
  - High fluency + low reliability = smooth-sounding but hallucinated citations (common in general-purpose models)
  - High completeness + low correctness = thorough but legally wrong (conceptual errors like confusing "prescrição" with "decadência")
  - High variability (large standard deviation) = inconsistent behavior across similar tasks
- **First 3 experiments**:
  1. **Baseline replication**: Run the same 15 cases through your system; compare scores to JusIA benchmarks using the published rubric
  2. **Ablation by use-case**: Test performance on "Pesquisar Precedente" (where JusIA scored 0.95) vs. "Gerar Documento" (0.64) to identify weak categories
  3. **Hallucination audit**: Specifically check citation accuracy (questions 2-3 in rubric) on 50 new queries; target <12% hallucination rate to match JusIA's 88% reliability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the theoretically grounded evaluation protocol be automated using "LLM-as-a-Judge" or synthetic raters while maintaining alignment with human legal experts?
- Basis in paper: [explicit] The authors state that future work includes "the integration of automatic metrics that allow the experiment to be scaled more robustly."
- Why unresolved: The current protocol relies on 48 human professionals, which is resource-intensive and limits the frequency of evaluation
- Evidence to resolve: High correlation coefficients between automated metric scores and the Jus IA Quality Index scores derived from human experts across a new set of legal cases

### Open Question 2
- Question: Is the specific weighting of criteria (prioritizing reliability and correctness over fluency) applicable to Common Law jurisdictions or Civil Law systems outside of Brazil?
- Basis in paper: [explicit] The paper notes that "New experiments are necessary to confirm the applicability of the Jus IA protocol in other jurisdictions or legal systems of distinct tradition."
- Why unresolved: The study is restricted to the Brazilian context and Portuguese language; legal theory principles like "material correctness" may manifest differently in other legal cultures
- Evidence to resolve: Results from replicating the experimental design with legal professionals in a non-Brazilian jurisdiction (e.g., US or Europe) using local legal tasks

### Open Question 3
- Question: How do smaller, open-source legal models perform relative to large proprietary general-purpose models on the Jus IA Index?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section that "The evaluation of the performance of small models (small models) or open models for the same tasks remains future work."
- Why unresolved: The study focused on state-of-the-art proprietary models (GPT-5, Gemini 2.5 Pro); it is unknown if efficient, local models can meet the required "correctness" and "reliability" thresholds
- Evidence to resolve: A comparative benchmark study including open-source legal LLMs (e.g., 7B-13B parameter range) evaluated against the 15 existing cases

## Limitations
- JusIA's architecture and training details remain undisclosed, limiting reproducibility of the domain-specialization effect
- Small sample size (15 cases) constrains generalizability to broader legal domains
- Prompt engineering variations across models could influence output quality beyond architectural differences

## Confidence
- **High confidence**: General-purpose systems performed significantly worse than specialized AI on correctness and reliability metrics; evaluation protocol produces consistent judgments (α = 0.6)
- **Medium confidence**: Domain specialization causally improves legal AI quality—while supported by results, confounding factors (prompt differences, model versions) cannot be fully excluded
- **Low confidence**: Specific numerical performance gaps between models generalize to other legal tasks or jurisdictions

## Next Checks
1. **Ablation study**: Test performance on "Pesquisar Precedente" (where JusIA scored 0.95) vs. "Gerar Documento" (0.64) to identify use-case-specific strengths and weaknesses
2. **Hallucination audit**: Specifically measure citation accuracy on 50 new queries; target <12% hallucination rate to match JusIA's 88% reliability benchmark
3. **Cross-jurisdiction replication**: Apply the same protocol to U.S. or European legal tasks to assess whether specialization benefits transfer across legal systems