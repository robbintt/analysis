---
ver: rpa2
title: 'Evaluating Dataset Watermarking for Fine-tuning Traceability of Customized
  Diffusion Models: A Comprehensive Benchmark and Removal Approach'
arxiv_id: '2511.19316'
source_url: https://arxiv.org/abs/2511.19316
tags:
- fine-tuning
- image
- dataset
- methods
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the security and copyright challenges arising
  from fine-tuning diffusion models, where unauthorized use of datasets for model
  customization poses risks. To enable traceability of fine-tuning data, the authors
  establish a universal threat model and introduce a comprehensive evaluation framework
  with three dimensions: Universality, Transmissibility, and Robustness.'
---

# Evaluating Dataset Watermarking for Fine-tuning Traceability of Customized Diffusion Models: A Comprehensive Benchmark and Removal Approach

## Quick Facts
- arXiv ID: 2511.19316
- Source URL: https://arxiv.org/abs/2511.19316
- Authors: Xincheng Wang; Hanchi Sun; Wenjun Sun; Kejun Xue; Wangqiu Zhou; Jianbo Zhang; Wei Sun; Dandan Zhu; Xiongkuo Min; Jun Jia; Zhijun Fang
- Reference count: 40
- Key outcome: Dataset watermarking methods show good universality and transmissibility for diffusion model fine-tuning traceability but are vulnerable to targeted removal attacks.

## Executive Summary
This paper addresses copyright and security challenges in fine-tuning diffusion models by establishing a comprehensive benchmark for dataset watermarking methods. The authors evaluate four state-of-the-art watermarking techniques across three key dimensions: Universality (performance across fine-tuning methods), Transmissibility (persistence with partial watermarked datasets), and Robustness (resistance to removal attacks). While existing methods perform well in tracking unauthorized use of training data, the study reveals significant vulnerabilities to targeted degradation-restoration attacks, highlighting the need for more robust watermarking strategies that can withstand adversarial removal while preserving model utility.

## Method Summary
The authors establish a universal threat model and benchmark four dataset watermarking methods (DIAGNOSIS, DiffusionShield, SIREN, WatermarkDM) across diverse tasks and fine-tuning approaches using Stable Diffusion 1.4. They evaluate performance across three dimensions: Universality (detection accuracy across four fine-tuning methods), Transmissibility (watermark persistence with mixed datasets at various protection ratios), and Robustness (resistance to targeted removal attacks). To expose vulnerabilities, they propose DeAttack, a unified watermark removal framework combining degradation operators (Gaussian blur, noise) with restoration networks (IRNeXt, SwinIR) that successfully erases watermarks while preserving generation quality, demonstrating the need for adversary-aware watermarking strategies.

## Key Results
- DiffusionShield achieves ~100% detection accuracy across four fine-tuning methods and three datasets for universality and transmissibility
- Watermark detection remains effective even with only 20% watermarked images in the training set
- DeAttack successfully removes watermarks from vulnerable methods while preserving model performance
- DIAGNOSIS and WatermarkDM show significant accuracy drops when text encoder is trained

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dataset watermarks persist through fine-tuning and remain detectable in generated outputs, enabling traceability.
- **Mechanism:** Imperceptible watermark signals $W(x,y)$ are embedded into training images as $I_w(x,y) = I(x,y) + \alpha W(x,y)$. During fine-tuning, these signals become encoded into model weights (UNet cross-attention, text encoder, or LoRA matrices). When the fine-tuned model generates new images, the learned watermark patterns manifest in outputs, allowing extraction via trained watermark detectors.
- **Core assumption:** The watermark embedding strength $\alpha \ll 1$ is sufficient to influence model learning without degrading generation quality. Assumption: watermark signals survive the diffusion denoising process.
- **Evidence anchors:**
  - [abstract] "even after using the watermarked images to fine-tune diffusion models, the watermarks remain detectable in the generated outputs"
  - [section 4.2] Table 2 shows DiffusionShield achieving ~100% detection accuracy across four fine-tuning methods and three datasets
  - [corpus] Weak direct evidence; related work on "Radioactive Watermarks" explores persistence across model training but in autoregressive contexts
- **Break condition:** If fine-tuning significantly modifies the text encoder (w/ te configurations), some methods (DIAGNOSIS, WatermarkDM) show substantially reduced detection accuracy (Table 2, WikiArt: DIAGNOSIS drops from 90% to 16%).

### Mechanism 2
- **Claim:** Watermarks transmit through fine-tuning even when only a partial subset of training data is watermarked.
- **Mechanism:** When fine-tuning uses mixed datasets (watermarked + clean images), the model still incorporates watermark patterns into its learned representations. The watermark signal propagates through gradient updates across batches, creating traceable signatures in outputs regardless of complete dataset coverage.
- **Core assumption:** The watermark signal is sufficiently distinct from semantic content that partial exposure during training still yields detectable patterns at inference.
- **Evidence anchors:**
  - [section 4.3] "DiffusionShield remains largely unaffected" even at low protection ratios
  - [section 4.3, Table 3] At 20% protection ratio, DiffusionShield maintains ~100% accuracy across all fine-tuning methods; DIAGNOSIS drops significantly
  - [corpus] No directly comparable corpus evidence for partial-dataset transmissibility in diffusion models
- **Break condition:** Methods vary significantly—DIAGNOSIS accuracy drops from 64% to 44% when protection ratio decreases from 100% to 20% (Text-to-Image, w/o te).

### Mechanism 3
- **Claim:** Targeted degradation-restoration attacks can remove watermarks while preserving image utility for fine-tuning.
- **Mechanism:** DeAttack applies a two-stage process: (1) Degradation operator $D(x) = x + \delta$ introduces perturbations in both pixel-space (Gaussian blur, noise) and latent-space (encoder perturbations $z' = z + \epsilon$), attenuating watermark frequency components; (2) Restoration operator $R(y) = \arg\min_x \|y-x\|^2_2 + \beta\Phi(x)$ recovers perceptual quality using networks like IRNeXt and SwinIR. The degradation disrupts structured watermark patterns while restoration maintains semantic content necessary for effective fine-tuning.
- **Core assumption:** Watermark energy concentrates in frequency bands vulnerable to blur/noise attenuation, and restoration networks prioritize semantic reconstruction over watermark recovery.
- **Evidence anchors:**
  - [section 5, Eq. 8] "watermark energy decays exponentially as frequency increases" under Gaussian blur
  - [section 5, Table 5] DeAttack methods reduce DIAGNOSIS accuracy from 64-100% (baseline) to 64-84% across removal approaches; some methods still struggle against DiffusionShield
  - [corpus] "Invisible image watermarks are provably removable using generative AI" (Zhao et al.) supports the broader vulnerability claim
- **Break condition:** DiffusionShield shows strong resilience even against targeted attacks (Table 5 maintains ~100% accuracy), suggesting mechanism effectiveness varies by watermark architecture.

## Foundational Learning

- **Concept: Stable Diffusion Fine-tuning Paradigms**
  - Why needed here: The paper evaluates watermarks across four distinct fine-tuning methods (Text-to-Image, LoRA, DreamBooth, Textual Inversion), each modifying different model components (full UNet, low-rank adapters, text encoder). Understanding which parameters are trainable determines where watermark signals can persist.
  - Quick check question: If only the text encoder is trained (Textual Inversion) vs. full UNet (Text-to-Image), where would you expect watermark signals to be encoded differently?

- **Concept: Frequency-Domain Watermark Embedding**
  - Why needed here: The DeAttack analysis leverages frequency-domain properties—Gaussian blur acts as a low-pass filter $H(u,v) = e^{-2\pi^2\sigma^2(u^2+v^2)}$ attenuating high-frequency watermark components. Understanding DWT/DCT/SVD watermarking helps explain vulnerability patterns.
  - Quick check question: Why would watermarks embedded in high-frequency coefficients be more vulnerable to blur attacks than those in low-frequency bands?

- **Concept: Threat Modeling for Adversarial ML**
  - Why needed here: The paper formalizes Image Owner vs. Image User threat scenarios with three attack surfaces (unknown fine-tuning methods, mixed datasets, adversarial removal). Designing defenses requires explicit adversary capability assumptions.
  - Quick check question: What additional capabilities would a "malicious Image User" need beyond those modeled to defeat even DiffusionShield's robust detection?

## Architecture Onboarding

- **Component map:**
  Image Owner Pipeline: [Original Dataset] → [Watermark Embedder] → [Watermarked Dataset]
  Image User Pipeline: [Watermarked Images] → [Fine-tuning] → [Customized Model] → [Generated Images] → [Watermark Detector]

- **Critical path:**
  1. Watermark embedding must preserve image quality (FID/CLIP-T metrics)
  2. Fine-tuning must encode watermark into model weights regardless of method
  3. Generated outputs must contain extractable watermark signals
  4. Detector must distinguish watermark from random noise (>50% accuracy baseline)

- **Design tradeoffs:**
  - **Embedding strength vs. generation quality**: Higher $\alpha$ improves detection but degrades FID (Table 2 shows watermarked models have worse FID than clean baselines)
  - **Universality vs. specialization**: Methods strong on one fine-tuning type may fail on others (WatermarkDM excels on Textual Inversion but struggles with text encoder training)
  - **Robustness vs. removability**: Robustness to common distortions (blur, JPEG, noise) doesn't transfer to targeted attacks—designing for one threat model may create blind spots

- **Failure signatures:**
  - ~50% detection accuracy = random guessing (SIREN baseline across all conditions)
  - Sharp accuracy drop when text encoder is trained = watermark dependency on frozen text embeddings
  - FID degradation without accuracy gain = over-embedded watermarks damaging semantic learning

- **First 3 experiments:**
  1. **Reproduce universality baseline**: Take DiffusionShield on CelebA-HQ with LoRA (w/o te), verify ~99% accuracy and compare FID against clean baseline. This validates your evaluation pipeline.
  2. **Test transmissibility threshold**: Fine-tune with 10%, 30%, 50% watermarked images (below paper's 20% minimum) to find where DiffusionShield breaks. This identifies deployment constraints.
  3. **Implement minimal DeAttack variant**: Apply Gaussian blur ($\sigma=15$, kernel 71×71 as specified) + IRNeXt restoration, measure impact on DIAGNOSIS vs. DiffusionShield. This validates robustness claims and reveals which method to deploy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can dataset watermarking techniques be redesigned to withstand targeted adversarial removal attacks like DeAttack?
- Basis in paper: [explicit] The authors conclude there is an "urgent need for more robust, adversary-aware dataset watermarking strategies" and that their removal approach "reveal[s] this vulnerability."
- Why unresolved: Current state-of-the-art methods (e.g., DiffusionShield) were shown to be fragile against the proposed degradation-restoration framework, failing to persist through the attack while maintaining model utility.
- What evidence would resolve it: A watermarking method that maintains high detection accuracy (>90%) even after undergoing the DeAttack removal process, without degrading the FID or CLIP scores of the fine-tuned model.

### Open Question 2
- Question: How can watermarks be engineered to survive specifically when the user fine-tunes the text encoder?
- Basis in paper: [inferred] Section 4.2 notes that for methods like DIAGNOSIS and WatermarkDM, "The adoption of fine-tuning the text encoder also significantly decreases the detection accuracy," yet no solution is proposed.
- Why unresolved: The authors demonstrate this sensitivity as a limitation of current approaches but do not investigate mechanisms to decouple the watermark signal from the text embeddings modified during training.
- What evidence would resolve it: A watermarking scheme that achieves consistent detection accuracy across both "frozen text encoder" and "trainable text encoder" configurations in the benchmark.

### Open Question 3
- Question: Do the universality and robustness findings generalize to newer diffusion architectures such as SDXL or Diffusion Transformers?
- Basis in paper: [inferred] The experimental settings in Section 4.1 restrict all evaluations to Stable Diffusion 1.4, leaving the performance on more complex or recent architectures unexplored.
- Why unresolved: The interaction between watermark survival and architectural components (e.g., different attention mechanisms or latent spaces) varies, and the results on SD1.4 may not scale.
- What evidence would resolve it: Successful replication of the benchmark results (Universality, Transmissibility, Robustness) using the same four watermarking methods on alternative diffusion backbones.

## Limitations

- The evaluation focuses on detection accuracy thresholds (>50%) rather than practical deployment requirements for reliable watermark detection across diverse prompts
- DeAttack's effectiveness varies dramatically by watermarking method, with no clear explanation for why DiffusionShield resists attacks while others fail
- The threat model assumes complete knowledge of watermarking methods by adversaries, which may not reflect realistic attack scenarios

## Confidence

- **High Confidence**: The experimental methodology for benchmarking universality and transmissibility across multiple fine-tuning paradigms is sound and well-executed. The detection accuracy results (e.g., DiffusionShield achieving ~100% across methods) are reproducible and clearly demonstrate method-specific strengths and weaknesses.
- **Medium Confidence**: The DeAttack framework's effectiveness is demonstrated, but the variance in results across watermarking methods suggests the vulnerability is not universal. The analysis of why DiffusionShield resists these attacks while others fail requires deeper investigation.
- **Low Confidence**: The threat model assumes complete knowledge of watermarking methods by adversaries. The practical implications of the ~50% detection accuracy threshold as "failure" rather than the theoretical random-guessing baseline need clarification for real-world deployment.

## Next Checks

1. **Deployment Threshold Analysis**: Test watermark detection at various accuracy thresholds (60%, 70%, 80%) across different prompt distributions to establish practical deployment requirements beyond the >50% random-guessing baseline.

2. **Black-Box Attack Validation**: Evaluate DeAttack's effectiveness when the watermarking method is unknown to the attacker, using blind watermark detection or universal removal strategies to better reflect realistic adversary constraints.

3. **Long-Term Persistence Study**: Fine-tune watermarked models for extended iterations (500+ steps) to assess whether watermarks persist through continued training, identifying potential break conditions for robust methods like DiffusionShield.