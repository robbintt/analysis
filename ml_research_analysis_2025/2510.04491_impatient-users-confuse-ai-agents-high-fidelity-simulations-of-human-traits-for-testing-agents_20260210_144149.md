---
ver: rpa2
title: 'Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits
  for Testing Agents'
arxiv_id: '2510.04491'
source_url: https://arxiv.org/abs/2510.04491
tags:
- trait
- user
- traits
- traitbasis
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "TraitBasis is a lightweight, model-agnostic method that induces\
  \ high-fidelity user traits in AI agents by learning steerable directions in activation\
  \ space, enabling systematic stress-testing of agents under realistic user behaviors\
  \ like impatience, confusion, skepticism, and incoherence. Applied to extend \u03C4\
  -Bench into \u03C4-Trait, it dynamically generates trait-perturbed multi-turn dialogues,\
  \ revealing significant performance drops\u2014up to 46%\u2014in frontier models\
  \ such as GPT-4o, Kimi K2, and GLM-4.5 across telecom, telehealth, airline, and\
  \ retail domains."
---

# Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents

## Quick Facts
- arXiv ID: 2510.04491
- Source URL: https://arxiv.org/abs/2510.04491
- Authors: Muyu He; Anand Kumar; Tsach Mackey; Meghana Rajeev; James Zou; Nazneen Rajani
- Reference count: 40
- Primary result: TraitBasis induces user traits in AI agents via activation steering, revealing up to 46% performance drops in frontier models across four domains.

## Executive Summary
TraitBasis is a lightweight, model-agnostic method that induces high-fidelity user traits in AI agents by learning steerable directions in activation space. It enables systematic stress-testing of agents under realistic user behaviors like impatience, confusion, skepticism, and incoherence. Applied to extend τ-Bench into τ-Trait, it dynamically generates trait-perturbed multi-turn dialogues, revealing significant performance drops—up to 46%—in frontier models such as GPT-4o, Kimi K2, and GLM-4.5 across telecom, telehealth, airline, and retail domains. TraitBasis outperforms baselines (prompt-based, SFT, LoRA) in realism (+10%), fidelity (+2.5%), stability (+19.8%), and compositionality (+11%), maintaining stable trait intensities and balanced blends in long conversations. This approach bridges the robustness testing gap, enabling AI agents to be evaluated under behaviorally diverse, realistic user interactions. TraitBasis and τ-Trait are open-sourced for community use.

## Method Summary
TraitBasis extracts trait-specific direction vectors from contrastive conversation pairs by computing differences in model activations at specific layers, then applies these vectors at inference time to steer LLM outputs toward target user traits. The method is model-agnostic, requires no fine-tuning, and allows for scaling and composition of multiple traits. These trait vectors are integrated into τ-Trait, a benchmark that simulates trait-perturbed multi-turn dialogues between AI users and agents, revealing significant performance degradation in frontier models across four domains.

## Key Results
- TraitBasis induces user traits (impatience, confusion, skepticism, incoherence) in LLMs via activation steering, outperforming prompt-based, SFT, and LoRA baselines in realism (+10%), fidelity (+2.5%), stability (+19.8%), and compositionality (+11%).
- When applied to extend τ-Bench into τ-Trait, TraitBasis dynamically generates trait-perturbed multi-turn dialogues, revealing significant performance drops—up to 46%—in frontier models like GPT-4o, Kimi K2, and GLM-4.5 across telecom, telehealth, airline, and retail domains.
- TraitBasis maintains stable trait intensities and balanced blends in long conversations, addressing the persona collapse issues common in baseline methods.

## Why This Works (Mechanism)

### Mechanism 1: Activation Space Steering
- Claim: Adding computed trait vectors to model activations at specific layers induces consistent, scalable user persona traits at inference time.
- Mechanism: The paper extracts contrastive activation differences from conversation pairs (trait-present vs. trait-absent) for the same prompt, averages these to isolate a trait-specific direction vector per layer, and then adds this scaled vector to hidden states during inference. This steers output generation toward the target trait.
- Core assumption: The target trait corresponds to a linearly separable direction in the model's activation space that can be isolated via contrastive averaging and is independent of the underlying intent or task.
- Evidence anchors:
  - [abstract] "...learns directions in activation space corresponding to steerable user traits...can be controlled, scaled, composed, and applied at inference time without any fine-tuning..."
  - [section 3.2, Page 3-4] Details the formulation of TraitBasis, including `P_T^(z) := 1/n Σ (P_pos - P_neg)` and inference steering `h^(z) ← h^(z) + α P_t^(z)`.
  - [corpus] Related work (Chen et al., 2025; Turner et al., 2023) supports activation steering for traits, but corpus lacks direct replication of this specific multi-trait, multi-turn approach.
- Break condition: If the trait is not linearly-additive across diverse prompts, or if contrastive averaging fails to disentangle it from intent, effectiveness will degrade.

### Mechanism 2: Trait Vector Composition
- Claim: Multiple trait vectors can be linearly combined to generate composite user personas with blended characteristics.
- Mechanism: Individual trait vectors are computed independently. At inference, they are combined via a weighted sum, defined by a `C` vector of trait intensities, and added to the activations.
- Core assumption: Trait vectors are approximately orthogonal, so their linear combination results in a coherent blend rather than interference.
- Evidence anchors:
  - [abstract] "...steerable user traits...can be controlled, scaled, composed..."
  - [section 6.1, Page 8] Results show TraitBasis achieves higher exact-pair match accuracy (62.5%) for blended traits and avoids trait suppression/imbalances seen in baselines.
  - [corpus] Corpus evidence on composability of fine-grained, multi-trait personas with this method is weak or missing.
- Break condition: If trait vectors are highly correlated, linear combination could lead to unpredictable outputs, trait cancellation, or amplification.

### Mechanism 3: Robustness Testing via User Simulation
- Claim: Stress-testing AI agents with trait-induced simulated users reveals brittleness not captured by standard benchmarks.
- Mechanism: TraitBasis is integrated into τ-Trait. The trait-perturbed user interacts with a target agent model across multi-turn dialogues. Task success rate is compared with performance on the original τ-Bench.
- Core assumption: The trait-induced user simulator provides a sufficiently realistic and challenging interaction pattern representative of real-world edge cases.
- Evidence anchors:
  - [abstract] "...dynamically generates trait-perturbed multi-turn dialogues, revealing significant performance drops—up to 46%..."
  - [Table 3, Page 9] Shows consistent performance degradation across three frontier models and four domains on τ-Trait.
  - [Figure 3, Page 12] Qualitative example of an agent passing a task on τ-Bench but failing on τ-Trait due to a skeptical user.
  - [corpus] Corpus evidence does not directly validate this specific robustness-testing mechanism.
- Break condition: If simulated user traits are not realistic or do not cover the space of real-world user variations, observed performance drops may not generalize to deployment.

## Foundational Learning

- Concept: **Activation Steering / Representation Engineering**
  - Why needed here: This is the core technique powering TraitBasis. Understanding how directions in the activation space correspond to semantic or behavioral properties is essential.
  - Quick check question: Can you explain the difference between modifying a model's behavior via prompting vs. by adding a vector to its hidden states?

- Concept: **Contrastive Pair Extraction**
  - Why needed here: The method relies on generating and averaging differences from conversation pairs to isolate a trait vector. Understanding this process is key to replicating and debugging it.
  - Quick check question: Why is it important for the conversation pairs to share the same underlying intent and context?

- Concept: **Persona Drift and Stability**
  - Why needed here: The paper emphasizes the stability of traits over long conversations. This is a key evaluation metric and a failure mode for baselines.
  - Quick check question: What is "persona collapse" or "trait fading" in the context of a multi-turn dialogue with an AI user simulator?

## Architecture Onboarding

- Component map: Contrastive conversation pairs -> Trait vector extraction -> Inference-time steering -> τ-Trait evaluation pipeline

- Critical path:
  1. The quality of the **contrastive conversation pairs** is the foundation. This data is manually curated or generated.
  2. The **vector extraction and layer selection** process determines the final steering vectors. Poor selection leads to weak or misdirected steering.
  3. At inference, the **vector composition and scaling logic** directly impacts the user simulator's behavior.

- Design tradeoffs:
  - **Vector Extraction Complexity:** The paper uses a simple averaging of contrastive pairs. More complex methods (e.g., PCA) could find more robust directions but add complexity.
  - **Layer Selection:** Steering at different layers has different effects. The paper manually selects the most effective layer for each trait. This is not automated.
  - **Data Efficiency vs. Control:** TraitBasis is highly data-efficient (needs only a few pairs) but requires manual creation of high-quality pairs. Fine-tuning (SFT) uses more data but offers less fine-grained control and stability.

- Failure signatures:
  - **Trait Not Elicited:** The simulated user shows no sign of the target trait. Indicates a failure in vector extraction or incorrect layer/scaling.
  - **Wrong Trait Elicited:** The user shows a different behavior (e.g., aggressive instead of just impatient). Suggests the extracted vector encodes a confounding factor.
  - **Trait Collapse:** In long conversations, the user's trait fades or becomes inconsistent. This is a known failure mode for prompt-based and SFT methods; TraitBasis is designed to be more stable but is not immune.
  - **Incoherent Blend:** When composing traits, the user's behavior becomes erratic or contradictory, rather than a coherent blend.

- First 3 experiments:
  1. **Reproduce Vector Extraction:** Select one trait (e.g., "impatience"), create a small set of contrastive pairs, and follow the paper's procedure to extract the trait vector. Test its effect by generating a short dialogue with and without the vector applied.
  2. **Ablate Layer Selection:** For the extracted "impatience" vector, apply it at different layers of the model. Qualitatively assess which layer produces the most prominent and realistic effect, replicating the paper's selection process.
  3. **Compose Traits:** Extract vectors for two traits (e.g., "skepticism" and "confusion"). Generate dialogues using each vector individually, and then a dialogue using their linear combination. Evaluate whether the combined output reflects a blend of both traits.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can advanced non-linear composition strategies or orthogonal trait bases improve the blending of complex user personas compared to simple linear combinations?
- Basis in paper: [explicit] The authors state that "exploring more advanced mixing strategies, such as using PCA to find orthogonal trait bases or non-linear composition methods, is a promising direction for future work."
- Why unresolved: The current study relies on a simple weighted linear combination of vectors to compose traits, leaving the potential of more sophisticated methods untested.
- What evidence would resolve it: A comparative analysis measuring fidelity and realism in scenarios with blended traits, contrasting linear combinations against PCA-based or non-linear methods.

### Open Question 2
- Question: Can fine-tuning AI agents on trajectories generated by TraitBasis effectively mitigate the performance drops caused by realistic user behaviors?
- Basis in paper: [explicit] The paper mentions TraitBasis can power "training loops" and "opens the door to building AI agents that remain reliable," but the experiments focus on evaluation rather than training.
- Why unresolved: The current work demonstrates that TraitBasis reveals brittleness (testing), but does not provide results on whether using this data for training actually fixes the identified robustness gaps.
- What evidence would resolve it: Pre- and post-training evaluations on $\tau$-Trait showing that agents fine-tuned with TraitBasis data recover the 30%+ performance degradation observed in standard models.

### Open Question 3
- Question: Can the optimal layer selection for trait steering be automated without relying on human annotation?
- Basis in paper: [inferred] The methodology requires manual intervention where "five annotators" select the conversation with the most obvious steering result to identify the optimal layer $z^*$.
- Why unresolved: Reliance on human annotators to determine the effective layer for each trait creates a scalability bottleneck for applying the method to new models or traits.
- What evidence would resolve it: The development of an automated metric (e.g., activation strength, probe accuracy) that selects layers with performance statistically indistinguishable from human-selected layers.

## Limitations

- The core mechanism relies on the assumption that user traits correspond to linearly separable directions in activation space, but the paper does not provide systematic analysis of vector orthogonality or robustness to distribution shifts.
- The optimal layer selection is performed manually through human annotation, which introduces potential subjectivity and scalability concerns.
- While TraitBasis claims superior stability over fine-tuning methods, the paper does not extensively test against catastrophic forgetting or domain adaptation scenarios.

## Confidence

- **High confidence**: The observed performance degradation (up to 46%) across multiple frontier models and domains is directly measured and reported.
- **Medium confidence**: The superiority over baseline methods (SFT, LoRA, prompt-based) is demonstrated on specific metrics, but the experimental setup may favor TraitBasis's design choices.
- **Medium confidence**: The claim of trait stability over long conversations is supported by human evaluation, but the evaluation protocol's sensitivity to subtle persona drift is not fully characterized.

## Next Checks

1. **Ablation Study**: Systematically test TraitBasis with different numbers of contrastive pairs (n=2, 4, 8) to quantify the minimum viable dataset size for stable vector extraction.
2. **Cross-Domain Transfer**: Apply trait vectors extracted from one domain (e.g., telecom) to user-agent interactions in a different domain (e.g., healthcare) to assess generalization.
3. **Orthogonality Analysis**: Compute pairwise cosine similarities between extracted trait vectors to empirically validate the assumption of linear independence required for compositionality.