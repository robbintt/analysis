---
ver: rpa2
title: Bayesian Subspace Gradient Estimation for Zeroth-Order Optimization of Large
  Language Models
arxiv_id: '2601.01452'
source_url: https://arxiv.org/abs/2601.01452
tags:
- uni00000013
- bszo
- subspace
- adaptive
- bayesian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces BSZO, a zeroth-order optimizer that applies
  Bayesian inference and Kalman filtering to aggregate gradient information across
  multiple perturbation directions for large language model fine-tuning. The method
  treats finite-difference measurements as noisy observations of the true gradient,
  builds a posterior distribution over the projected gradient, and updates it through
  Bayesian updates with an adaptive residual-based mechanism to adjust perturbation
  scales.
---

# Bayesian Subspace Gradient Estimation for Zeroth-Order Optimization of Large Language Models

## Quick Facts
- **arXiv ID**: 2601.01452
- **Source URL**: https://arxiv.org/abs/2601.01452
- **Reference count**: 40
- **Primary result**: BSZO achieves up to 6.67% absolute improvement on OPT-13B while maintaining memory usage comparable to inference-only baselines (1.00×–1.08× of MeZO)

## Executive Summary
This paper introduces BSZO, a zeroth-order optimizer for large language model fine-tuning that leverages Bayesian inference and Kalman filtering to aggregate gradient information across multiple perturbation directions. The method treats finite-difference measurements as noisy observations of the true gradient and builds posterior distributions over projected gradients, achieving theoretical convergence improvements by a factor of k/γ compared to standard approaches. Experiments demonstrate significant performance gains on RoBERTa, Mistral, and OPT models while maintaining numerical stability in fp16/bf16 precision and memory efficiency comparable to inference-only baselines.

## Method Summary
BSZO operates by projecting gradients onto randomly selected subspaces and aggregating information from multiple perturbation directions through Bayesian updates. The method treats finite-difference measurements as noisy observations of the true gradient and builds posterior distributions over the projected gradient. A Kalman filter framework is employed to maintain and update these posterior distributions, with an adaptive residual-based mechanism to adjust perturbation scales during optimization. This approach enables efficient gradient estimation without requiring explicit gradient computation, making it suitable for memory-constrained LLM fine-tuning scenarios.

## Key Results
- Achieves up to 6.67% absolute improvement on OPT-13B model fine-tuning tasks
- Maintains memory usage comparable to inference-only baselines (1.00×–1.08× of MeZO)
- Demonstrates numerical stability in fp16/bf16 precision settings
- Shows theoretical convergence rate improvement by factor k/γ compared to standard zeroth-order methods

## Why This Works (Mechanism)
BSZO's effectiveness stems from its ability to aggregate gradient information across multiple perturbation directions using Bayesian inference, which provides more accurate gradient estimates than single-direction approaches. The Kalman filter framework enables efficient posterior updates that adapt to changing gradient landscapes during optimization. The residual-based perturbation scale adaptation mechanism ensures appropriate exploration-exploitation balance throughout training. By projecting gradients onto random subspaces and leveraging Bayesian aggregation, BSZO can estimate gradients more efficiently while maintaining numerical stability in low-precision arithmetic settings.

## Foundational Learning

### Bayesian Inference for Gradient Estimation
- **Why needed**: Provides probabilistic framework for aggregating noisy gradient measurements from multiple perturbation directions
- **Quick check**: Verify posterior distributions properly capture uncertainty in gradient estimates

### Kalman Filtering
- **Why needed**: Enables efficient sequential updating of posterior distributions during optimization
- **Quick check**: Confirm filter stability and convergence properties in dynamic optimization settings

### Subspace Projection
- **Why needed**: Reduces dimensionality of gradient estimation problem while preserving essential information
- **Quick check**: Ensure projection preserves sufficient gradient information for effective optimization

### Residual-Based Perturbation Scaling
- **Why needed**: Adapts exploration scale based on optimization progress and gradient consistency
- **Quick check**: Verify adaptive scaling maintains appropriate balance between exploration and exploitation

## Architecture Onboarding

### Component Map
Bayesian Inference Module -> Kalman Filter Update -> Subspace Projection -> Residual-Based Scale Adaptation -> Gradient Estimation

### Critical Path
Perturbation generation → Finite-difference measurement → Bayesian update → Kalman filter → Gradient estimation → Parameter update

### Design Tradeoffs
- Accuracy vs. computational overhead in Bayesian updates
- Exploration vs. exploitation balance in perturbation scaling
- Memory efficiency vs. gradient estimation precision
- Numerical stability vs. performance in low-precision arithmetic

### Failure Signatures
- Degraded performance with poorly chosen subspace dimensions
- Numerical instability in Kalman filter updates under extreme conditions
- Suboptimal convergence with inappropriate perturbation scaling
- Memory bottlenecks from inefficient posterior storage

### First Experiments
1. Verify gradient estimation accuracy on simple linear models with known gradients
2. Test numerical stability of Kalman filter updates under varying precision settings
3. Evaluate convergence behavior on small transformer models before scaling to LLMs

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical convergence guarantees assume gradient smoothness and bounded noise that may not hold in practice
- Memory usage comparisons assume specific baseline implementations that may vary
- Experimental evaluation focuses on classification tasks, not demonstrating generalization to other LLM fine-tuning scenarios

## Confidence

### High Confidence
- Memory efficiency claims (1.00×–1.08× of MeZO) based on controlled experiments
- Numerical stability in fp16/bf16 precision settings verified through multiple runs

### Medium Confidence
- Performance improvements (up to 6.67% absolute) may depend on specific hyperparameter configurations
- Results potentially sensitive to task-specific implementation details

### Low Confidence
- Theoretical improvement factor k/γ may not translate directly to practical performance across diverse scenarios
- Generalization to different LLM architectures beyond evaluated models uncertain

## Next Checks
1. Evaluate BSZO on instruction tuning and generation tasks beyond classification to verify generalizability across LLM fine-tuning scenarios
2. Conduct ablation studies isolating contributions of Bayesian updates versus residual-based perturbation scale adaptation
3. Test BSZO on additional LLM architectures including transformer variants with different attention mechanisms