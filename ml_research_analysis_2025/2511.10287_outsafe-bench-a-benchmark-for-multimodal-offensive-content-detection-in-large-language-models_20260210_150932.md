---
ver: rpa2
title: 'OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large
  Language Models'
arxiv_id: '2511.10287'
source_url: https://arxiv.org/abs/2511.10287
tags:
- safety
- evaluation
- risk
- content
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OutSafe-Bench introduces a comprehensive multimodal safety benchmark
  covering text, image, audio, and video in both Chinese and English. It features
  23,400 annotated samples across nine content risk categories.
---

# OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models

## Quick Facts
- **arXiv ID**: 2511.10287
- **Source URL**: https://arxiv.org/abs/2511.10287
- **Reference count**: 40
- **Key outcome**: OutSafe-Bench introduces a comprehensive multimodal safety benchmark covering text, image, audio, and video in both Chinese and English, with 23,400 annotated samples across nine content risk categories, achieving 0.567 Spearman correlation with human ratings.

## Executive Summary
OutSafe-Bench addresses the critical need for comprehensive safety evaluation of Multimodal Large Language Models (MLLMs) across diverse content types and languages. The benchmark introduces a Multidimensional Cross Risk Score (MCRS) that models semantic relationships between nine risk categories using sentence-BERT embeddings, and a FairScore mechanism that uses weighted model juries to reduce evaluation bias. Tested on nine leading MLLMs, the benchmark reveals significant safety vulnerabilities, particularly in video and audio modalities, while demonstrating improved alignment with human judgment through its novel scoring mechanisms.

## Method Summary
OutSafe-Bench constructs a comprehensive multimodal safety evaluation framework using 30 seed datasets covering text (18,000 samples), image (4,500), audio (450), and video (450) in both Chinese and English. Each sample is evaluated by five jury models (Claude-3.7-Sonnet, Deepseek-v3, GPT-4o, GPT-4o-mini, Ernie-4.0) on nine risk dimensions using a JSON prompt. The MCRS mechanism computes cross-risk influence weights from sentence-BERT embeddings of risk categories, while FairScore aggregates jury scores using performance-based weights derived from baseline benchmarks. All non-text modalities are transcribed to text for consistent evaluation, with quality control via keyword matching and Jaccard similarity thresholds.

## Key Results
- MCRS achieved Spearman correlation of 0.567 with human ratings, outperforming unweighted baselines (0.518) on 936 human-annotated samples
- FairScore demonstrated highest agreement with human judgments (Spearman 0.506) compared to single-model (0.559) and average scoring (0.536) methods
- Video modality showed lowest correlation with human ratings (ρ=0.306), revealing significant evaluation challenges in this domain
- Overall automated evaluation achieved ρ=0.572 (p<10⁻⁸²) with human ratings across all modalities

## Why This Works (Mechanism)

### Mechanism 1: Cross-Risk Influence Weighting via Semantic Embeddings (MCRS)
- **Claim**: Modeling risk category co-occurrence improves composite safety scoring alignment with human judgment
- **Mechanism**: Sentence-BERT encodes nine risk category labels into semantic vectors. Cosine similarity between these vectors forms a 9×9 matrix, which is row-normalized to produce the Cross-Risk Influence Matrix (γ). The final risk score for scenario k is R(j,k) = Σγ(k,q)·r̄(j,k)_q, blending individual risk scores with their semantic relatedness weights
- **Core assumption**: Semantic similarity between risk category embeddings correlates with real-world co-occurrence patterns in harmful outputs
- **Evidence anchors**: MCRS achieved Spearman ρ=0.5672 vs. 0.5179 for unweighted mean on 936 human-annotated samples

### Mechanism 2: Reliability-Weighted Jury Aggregation (FairScore)
- **Claim**: Weighted aggregation of multiple reviewer models, with weights derived from baseline performance, reduces single-model bias and improves human alignment
- **Mechanism**: Each jury model RM_l receives weight λ_l based on safety classification accuracy on held-out benchmarks (Chinese Safety Prompts, Case-Bench). Final score per dimension is r̂ = Σλ_l·r_(j,k,t,l)_i. Top five models by accuracy are selected; weights are normalized via softmax
- **Core assumption**: Performance on text-based safety benchmarks generalizes to multimodal safety evaluation
- **Evidence anchors**: FairScore achieved Spearman ρ=0.5681 and Kendall τ=0.4127, outperforming single-model (ρ=0.5589) and average (ρ=0.5364) baselines

### Mechanism 3: Unified Text-Mediated Multimodal Evaluation
- **Claim**: Transcoding all modalities to text enables consistent safety evaluation across text, image, audio, and video using a shared rubric
- **Mechanism**: Images/videos are described by MLLMs with temporal instruction tuning (frame sampling at 1 fps, max 5 minutes). Audio undergoes noise reduction (spectral subtraction) and voice activity detection before transcription. All outputs are evaluated via the same nine-dimensional prompt by jury models
- **Core assumption**: MLLM-generated descriptions preserve safety-relevant semantic content without introducing systematic distortions or omissions
- **Evidence anchors**: Automated evaluation achieved ρ=0.572 (p<10⁻⁸²) with human ratings overall; image/audio modalities showed highest consistency (ρ=0.7719, 0.7001)

## Foundational Learning

- **Sentence-BERT Embeddings**
  - **Why needed here**: MCRS depends on encoding risk categories as semantic vectors to compute similarity weights
  - **Quick check question**: Can you explain why cosine similarity between "violence and hatred" and "crime and illegal activities" would be higher than between "violence" and "copyright infringement"?

- **LLM-as-Judge Evaluation Paradigm**
  - **Why needed here**: FairScore operationalizes multi-model juries; understanding prompt design and judge calibration is essential
  - **Quick check question**: What failure mode occurs if all jury models share the same training data bias?

- **Spearman/Kendall Correlation for Ranking Agreement**
  - **Why needed here**: Paper validates MCRS and FairScore primarily through rank correlation with human judgments
  - **Quick check question**: Why use Spearman instead of Pearson for comparing safety score rankings?

## Architecture Onboarding

- **Component map**: Data Layer (30 seed datasets) → Quality Control (filtering, keyword extraction, Jaccard validation) → Scoring Layer (jury ratings, MCRS, FairScore) → Evaluation Interface (nine-dimensional JSON prompt)

- **Critical path**: 1) Input prompt → MLLM generates output 2) Output transcoded to text (if non-text) 3) Five jury models each score nine dimensions 4) Per-sample scores aggregated via FairScore weights (λ) 5) Final composite score computed via MCRS cross-risk matrix (γ)

- **Design tradeoffs**:
  - **Text unification vs. modality-specific rubrics**: Choosing text mediation simplifies evaluation but risks information loss in video/audio
  - **Fixed jury vs. adaptive selection**: Pre-selecting top 5 models on baseline benchmarks improves stability but may miss modality-specific expertise
  - **Semantic γ vs. empirical co-occurrence**: Using sentence-BERT similarity is tractable but may not reflect actual risk correlations in generated content

- **Failure signatures**:
  - **Semantic drift flag**: Jaccard similarity <0.35 between model outputs for same prompt indicates unreliable description quality
  - **Refusal clustering**: High refusal rates on specific risk categories may indicate over-conservative alignment rather than genuine safety
  - **Modality gap**: Video correlation with human ratings (ρ=0.3056) significantly lower than image/audio, suggesting transcoding quality issues

- **First 3 experiments**:
  1. **Reproduce MCRS ablation**: Compare γ-weighted vs. unweighted scoring on the 936-sample human-annotated subset to verify ρ improvement (~0.05 gain expected)
  2. **Sensitivity analysis on jury composition**: Vary jury size (3, 5, 7 models) and weight assignment (uniform vs. performance-based) to measure robustness
  3. **Cross-modal consistency check**: For samples with both image and text versions of the same prompt, compare risk scores to assess whether text-mediated evaluation introduces systematic bias

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can cross-modal alignment risks be systematically quantified when text, image, audio, and video outputs from the same prompt contain contradictory or misaligned safety violations?
- **Basis in paper**: [explicit] The limitations section states: "Another limitation lies in the lack of evaluation for cross-modal alignment, such as consistency between text and visual outputs. Although we assess each modality independently, future work should explore joint reasoning and misalignment risks across modalities"
- **Why unresolved**: OutSafe-Bench evaluates each modality independently using MCRS, but real-world MLLM outputs may present conflicting risk profiles across modalities that require joint reasoning
- **What evidence would resolve it**: Development of a cross-modal consistency metric and evaluation protocol that measures safety alignment between simultaneously generated text-image-audio-video outputs

### Open Question 2
- **Question**: What scalable evaluation methods can effectively assess safety in video inputs exceeding five minutes without compromising annotation quality or computational feasibility?
- **Basis in paper**: [explicit] "OutSafe-Bench is constrained by current model capabilities. In particular, long-form video samples (more than five minutes) were excluded due to input length limits in existing MLLMs. Future work will explore scalable methods for evaluating long-context inputs"
- **Why unresolved**: Current MLLMs have token/processing limits that prevent handling extended temporal sequences, yet many real-world harmful videos are longer-form content
- **What evidence would resolve it**: Benchmark results demonstrating reliable safety assessment on videos of 10+ minutes with acceptable correlation to human judgments and tractable computational costs

### Open Question 3
- **Question**: Does using text-based safety benchmark performance to assign jury weights in FairScore adequately capture reviewer reliability for non-text modalities?
- **Basis in paper**: [inferred] The paper states jury models are "selected based on the top five best-performing MLLMs" using text-based safety benchmarks (Chinese Safety Prompts, Case-Bench) "since all modality outputs are translated into text for evaluation." However, text translation quality and safety evaluation capability may differ across modalities
- **Why unresolved**: The correlation between text-based safety performance and multimodal evaluation reliability remains unvalidated, potentially introducing modality-specific bias
- **What evidence would resolve it**: A comparative study showing jury weights derived from modality-specific benchmarks produce equivalent or superior FairScore correlations with human judgments across image, audio, and video modalities

## Limitations
- The generalizability of MCRS cross-risk weighting depends on whether semantic similarity between risk categories (computed via sentence-BERT) actually reflects empirical co-occurrence patterns in harmful outputs
- Jury model performance weights (λ) are derived from text-only safety benchmarks, but it's unclear whether text safety performance translates to multimodal safety evaluation quality
- The 450 video samples may be insufficient to draw robust conclusions about video modality safety evaluation, given the observed low correlation with human ratings

## Confidence

- **High confidence**: The benchmark construction methodology, data quality control procedures, and overall experimental framework are well-specified and reproducible
- **Medium confidence**: The MCRS cross-risk weighting mechanism shows statistical improvement over unweighted baselines (ρ=0.567 vs 0.518), but the semantic basis for weight assignment lacks empirical validation
- **Low confidence**: The FairScore mechanism's superiority (ρ=0.568 vs 0.559 for single-model) is demonstrated, but the jury weight computation procedure is underspecified, making exact reproduction difficult

## Next Checks

1. **Validate MCRS semantic assumptions**: Compute empirical co-occurrence frequencies of risk categories in the dataset and compare with sentence-BERT similarity weights to verify alignment
2. **Cross-modal evaluation bias test**: For a subset of prompts available in both text and image modalities, compare risk scores to quantify systematic bias introduced by text-mediated evaluation
3. **Jury composition sensitivity**: Systematically vary jury size (3, 5, 7 models) and weight assignment strategies (uniform, performance-based, random) to measure stability of FairScore improvements