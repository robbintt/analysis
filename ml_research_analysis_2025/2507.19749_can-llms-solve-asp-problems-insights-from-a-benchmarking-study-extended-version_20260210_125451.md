---
ver: rpa2
title: Can LLMs Solve ASP Problems? Insights from a Benchmarking Study (Extended Version)
arxiv_id: '2507.19749'
source_url: https://arxiv.org/abs/2507.19749
tags:
- rule
- answer
- 'true'
- rules
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ASPBench, a comprehensive benchmark for
  evaluating large language models (LLMs) on Answer Set Programming (ASP) tasks. ASPBench
  includes three distinct tasks: ASP entailment, answer set verification, and answer
  set computation, and features both synthetic and real-world ASP programs with diverse
  logical operations.'
---

# Can LLMs Solve ASP Problems? Insights from a Benchmarking Study (Extended Version)

## Quick Facts
- **arXiv ID**: 2507.19749
- **Source URL**: https://arxiv.org/abs/2507.19749
- **Reference count**: 40
- **Primary result**: LLMs struggle with Answer Set Computation, achieving only 8.6% exact match on real-world ASP programs despite moderate performance on simpler entailment and verification tasks.

## Executive Summary
This paper introduces ASPBench, a comprehensive benchmark for evaluating large language models on Answer Set Programming tasks. Through experiments with 14 state-of-the-art LLMs across three distinct tasks—ASP entailment, answer set verification, and answer set computation—the study reveals significant limitations in current LLMs' ability to perform robust logical reasoning. While models perform reasonably well on classification tasks like entailment and verification, they struggle dramatically with the core computational task of generating answer sets, particularly for real-world programs and those with complex logical structures. The findings highlight the gap between current LLMs' semantic reasoning capabilities and the formal symbolic reasoning required for ASP.

## Method Summary
The study evaluates 14 state-of-the-art LLMs on ASPBench, a benchmark containing 1,000 samples per task across three tasks: ASP Entailment (3-class classification), Answer Set Verification (binary classification), and Answer Set Computation (generation task). The benchmark includes both synthetic and real-world ASP programs with diverse logical operations and supports multiple representations (symbolic/textual) and predicate styles (P-style, ConceptNet-related, random). Evaluation uses DLV2 solver for ground truth validation and macro-F1 for classification tasks, with exact match for the generation task. No forced output formats are used to avoid interfering with reasoning, and raw LLM outputs are post-processed to structured JSON for automated evaluation.

## Key Results
- LLMs achieve moderate performance on ASP entailment (60-70% macro-F1) and verification tasks but struggle significantly with answer set computation (23.2% EM on synthetic, 8.6% on real-world programs)
- Performance varies dramatically based on syntactic class membership, with sharp drops when programs violate stratification or head-cycle-free constraints
- Semantic content in predicates matters, with lexicalized predicates from ConceptNet outperforming abstract P-style identifiers
- LLMs show risk-averse behavior, over-predicting "unknown" in entailment tasks and failing to handle multi-answer-set scenarios effectively

## Why This Works (Mechanism)

### Mechanism 1: Syntactic Class Sensitivity
LLM performance on ASP tasks varies dramatically based on whether programs adhere to specific syntactic constraints (positive, stratified, head-cycle-free), with performance collapsing when constraint violations introduce complex negation patterns. The models appear to rely on pattern recognition from training data for familiar program structures and default to "unknown" predictions when encountering unfamiliar recursive negation patterns, showing a risk-averse bias in ternary evaluation contexts.

### Mechanism 2: Semantic Grounding Dependency
LLMs reason more effectively when predicates carry semantic meaning (lexicalized from ConceptNet) compared to abstract symbolic identifiers, indicating reliance on semantic priors rather than pure logical manipulation. Lexicalized predicates activate relevant knowledge graphs and reasoning patterns from pretraining, providing semantic anchors that guide inference, while abstract identifiers force models to reason purely structurally—a task for which current LLMs appear under-equipped.

### Mechanism 3: Computational Depth and Multi-Answer-Set Collapse
LLMs struggle with tasks requiring iterative computation across multiple potential answer sets, with performance degrading sharply as the solution space expands beyond single-answer-set scenarios. Answer Set Computation requires constructing complete coherent models through iterative rule application, fundamentally different from classification tasks. When programs have multiple answer sets, LLMs must explore a branching solution space requiring systematic search and backtracking capabilities that current architectures may not support.

## Foundational Learning

- **Concept: Answer Set Semantics and Gelfond-Lifschitz Transformation**
  - **Why needed here:** The entire benchmark evaluates whether LLMs can correctly compute or verify answer sets—the stable models defined through the GL reduct. Understanding how negation-as-failure is resolved by conditionally removing rules and simplifying programs is essential to interpret why LLMs fail on stratification violations.
  - **Quick check question:** Given program P = {a :- not b. b :- not a.}, what are the answer sets, and how does the GL reduct differ for candidate {a} versus candidate {a, b}?

- **Concept: Syntactic Classes (Positive, Stratified, Head-Cycle-Free)**
  - **Why needed here:** The paper's central finding is that performance tracks syntactic class membership. These constraints guarantee computational properties (unique answer sets, polynomial computation) that LLMs implicitly exploit, and their violations expose reasoning deficits.
  - **Quick check question:** Classify P1 = {p :- not q. q :- not p.} and P2 = {a | b :- c. c :- a.} as stratified/HCF/neither, and explain what computational guarantee each class provides.

- **Concept: Default Negation vs. Strong Negation**
  - **Why needed here:** ASPBench includes both negation types, and case studies show LLMs conflating their semantics. Default negation ("not p") means "p cannot be derived" (epistemic), while strong negation ("-p") means "p is explicitly false" (ontological).
  - **Quick check question:** In program {bird(tweety). fly(X) :- bird(X), not -fly(X). -fly(X) :- penguin(X).}, explain why fly(tweety) is true and how the two negation types differ in their semantic role.

## Architecture Onboarding

- **Component map:**
  ASPBench Pipeline: Graph Construction -> Rule Generation -> Benchmark Construction -> Evaluation Tasks
  Evaluation Tasks: ASE (classification) -> ASV (verification) -> ASC (generation)

- **Critical path:**
  1. Understand why ASC is the "core" ASP task (requires constructive reasoning vs. classification)
  2. Map the performance cliffs: synthetic → real-world (23.2% → 8.6% EM), positive → non-positive (56.1% → 19.4%)
  3. Trace failure modes in case studies: variable unification errors, default negation misunderstanding, incomplete derivations
  4. Recognize that test-time scaling helps but doesn't close the gap—architectural limitations persist

- **Design tradeoffs:**
  - Synthetic vs. real-world programs: Controlled experiments vs. ecological validity
  - Symbolic vs. textual representation: Slight advantage for ASC (structured output) vs. ASE/ASV (linguistic pattern matching)
  - Safety checking during generation: Guarantees validity but may exclude interesting edge cases

- **Failure signatures:**
  - Risk-averse "unknown" bias: Over-predicting "unknown" in ASE (47.2% predictions vs 26.6% ground truth)
  - Completeness blind spot: Struggling to detect missing facts in ASV
  - Derivation path commitment: Early commitment to incorrect paths in ASC, declaring premature completeness

- **First 3 experiments:**
  1. Reproduce syntactic class sensitivity analysis by filtering samples by Positive/Stratified/HCF membership
  2. Probe semantic grounding mechanism with controlled predicate pairs (abstract vs. semantically coherent vs. incoherent)
  3. Characterize multi-answer-set collapse by binning ASC samples by answer set count and measuring degradation slope

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can neuro-symbolic hybrid architectures be optimized to integrate symbolic ASP solvers with LLMs to overcome limitations in Answer Set Computation?
- **Basis in paper:** The conclusion explicitly suggests developing "hybrid architectures that integrate symbolic logic representation with neural networks" as a primary future direction.
- **Why unresolved:** The paper identifies LLMs fail at ASC (8.6% EM on real-world programs) but does not propose or evaluate specific hybrid architectures.

### Open Question 2
- **Question:** Can specific training strategies or prompting techniques be developed to enable LLMs to perform robust iterative and fixed-point reasoning required for non-stratified ASP programs?
- **Basis in paper:** Section 5.6 concludes LLMs have a "critical lack of robust iterative and fixed-point reasoning," evidenced by performance collapses when Stratification or HCF constraints are violated.
- **Why unresolved:** The paper demonstrates the failure but does not offer methodological solutions to handle recursion or cyclic dependencies.

### Open Question 3
- **Question:** Does the sensitivity to predicate naming styles (semantic vs. symbolic) persist in fine-tuned models or neuro-symbolic approaches?
- **Basis in paper:** Section 5.3 notes LLMs perform significantly better with lexicalized predicates than P-style identifiers, framing "impact of predicate semantics" as a previously unexplored key factor.
- **Why unresolved:** The benchmark evaluates this sensitivity in frozen models but doesn't determine if it's a fundamental limitation or a feature of pre-training data distribution.

## Limitations
- The study cannot definitively attribute performance gaps to architectural limitations versus insufficient training data or prompting strategies
- The underlying mechanisms for performance differences remain partially speculative without ablation studies isolating semantic content from syntactic structure
- The benchmark relies on DLV2 for ground truth validation, assuming perfect solver behavior that may not capture all edge cases

## Confidence
- **High Confidence**: Performance degradation on answer set computation versus verification tasks; correlation between syntactic class membership and performance
- **Medium Confidence**: Semantic grounding hypothesis explaining predicate naming effects (pending direct ablation studies)
- **Medium Confidence**: Multi-answer-set collapse mechanism (computational depth limitations are well-documented in broader literature)

## Next Checks
1. **Ablation Study on Predicate Semantics**: Create controlled program pairs identical in logical structure but varying predicate naming (abstract, semantically coherent, semantically incoherent) to isolate whether semantic grounding provides genuine reasoning advantages beyond lexical familiarity effects.

2. **Architecture Stress Test**: Evaluate whether performance degradation follows smooth scaling laws or exhibits sharp cliffs at syntactic boundaries using systematically perturbed programs that incrementally violate stratification/HCF constraints.

3. **Oracle Baseline Comparison**: Implement a hybrid system where a symbolic ASP solver generates candidate answer sets while an LLM verifies their correctness, then compare performance against pure LLM approaches to isolate whether the bottleneck is search space exploration versus verification logic.