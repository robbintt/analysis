---
ver: rpa2
title: 'SID: Multi-LLM Debate Driven by Self Signals'
arxiv_id: '2510.06843'
source_url: https://arxiv.org/abs/2510.06843
tags:
- correct
- wrong
- confidence
- value
- debate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces SID, a novel multi-LLM debate framework\
  \ that leverages self signals\u2014specifically model-level confidence and token-level\
  \ semantic focus\u2014to enhance both performance and efficiency. The approach employs\
  \ an early-exit mechanism based on token-wise uncertainty metrics to terminate confident\
  \ agents early, and a compression mechanism using attention-based semantic focus\
  \ to condense debate content."
---

# SID: Multi-LLM Debate Driven by Self Signals

## Quick Facts
- arXiv ID: 2510.06843
- Source URL: https://arxiv.org/abs/2510.06843
- Authors: Xuhang Chen; Zhifan Song; Deyi Ji; Shuo Gao; Lanyun Zhu
- Reference count: 40
- Primary result: Multi-LLM debate framework using self signals achieves higher accuracy and 40% token reduction

## Executive Summary
This paper introduces SID, a novel multi-LLM debate framework that leverages self signals—specifically model-level confidence and token-level semantic focus—to enhance both performance and efficiency. The approach employs an early-exit mechanism based on token-wise uncertainty metrics to terminate confident agents early, and a compression mechanism using attention-based semantic focus to condense debate content. Experiments on diverse benchmarks with various LLMs and MLLMs demonstrate that SID consistently outperforms existing debate methods while reducing token consumption by up to 40%.

## Method Summary
SID operates within a multi-agent debate framework where three agents engage in two debate rounds. The key innovations are an early-exit mechanism that uses token-wise uncertainty metrics (entropy and negative log-likelihood) to identify confident agents, and an attention-based compression mechanism that extracts semantically relevant debate spans. Agents compute confidence from their initial responses using vocabulary-adaptive thresholds (α·log|V|), with high-confidence agents exiting early. For continuing agents, disagreement-oriented prompts are prepended to other agents' responses, and attention weights are extracted to identify and compress semantically relevant tokens. The compressed content is then expanded to sentence boundaries for semantic preservation.

## Key Results
- SID outperforms standard MAD across benchmarks, improving accuracy by 2.1% on MMLUpro and 6.6% on GPQA
- Token consumption reduced by up to 40% compared to baseline MAD methods
- Early-exit mechanism alone contributes significant efficiency gains while maintaining accuracy
- Compression mechanism preserves semantic coherence while reducing debate history length

## Why This Works (Mechanism)

### Mechanism 1: Early Exit via Model-Level Confidence
Agents with high confidence in their initial answer can exit the debate early, reducing unnecessary computation while preserving or improving overall accuracy. The framework computes token-wise uncertainty metrics (entropy and negative log-likelihood) from output logits, aggregates into confidence vectors, and exits if confidence exceeds a vocabulary-adaptive threshold.

### Mechanism 2: Adaptive Compression via Token-Level Semantic Focus
Using attention patterns conditioned on disagreement-oriented prompts, the framework extracts semantically relevant spans from other agents' responses, compressing debate history while preserving key points of contention. Attention weights from prompt tokens to context tokens identify critical disagreement regions.

### Mechanism 3: Combined Efficiency–Performance Gain
Integrating early exit and attention-based compression yields both higher accuracy and reduced token consumption. Early exit skips unnecessary debate rounds for confident agents, while compressed debate histories reduce token overhead and focus reasoning on substantive disagreements.

## Foundational Learning

- **Uncertainty Quantification in LLMs**
  - Why needed here: Early-exit mechanism relies on confidence estimates; understanding how entropy and NLL relate to correctness is critical
  - Quick check question: For a given LLM output, does higher entropy necessarily indicate incorrectness? Explain why or why not.

- **Attention Mechanisms in Transformers**
  - Why needed here: Compression mechanism uses attention maps; understanding how attention reflects semantic relevance is essential
  - Quick check question: In a transformer, what does a high attention weight from token i to token j suggest about their relationship?

- **Multi-Agent Debate Dynamics**
  - Why needed here: SID operates within a MAD framework; understanding how agents exchange and refine responses helps contextualize where self-signals intervene
  - Quick check question: In standard MAD, what are two common sources of redundancy or inefficiency during multi-round debate?

## Architecture Onboarding

- **Component map:** Confidence Estimator -> Early-Exit Gate -> Debate Orchestrator (if continue) -> Attention Extractor -> Semantic Preservation Module -> Next-Round Response Generation

- **Critical path:** Initial answer generation → confidence computation → early-exit decision → (if debate continues) construct prompt-conditioned input → extract attention → compress → generate next-round responses

- **Design tradeoffs:** Threshold α (higher = more exits but risk of errors), top-p ratio ρ (higher = less compression but more context), aggregation strategy (max vs avg vs first vs penultimate)

- **Failure signatures:** Overconfident early exits on incorrect answers, over-compression removing critical disagreement points, attention sink artifacts retaining irrelevant tokens

- **First 3 experiments:**
  1. Confidence threshold sweep: Vary α on validation set and measure accuracy vs token ratio
  2. Compression ablation: Disable semantic preservation and compare accuracy/readability
  3. Baselines vs SID components: Compare full SID, only early exit, only compression, and standard MAD on challenging subset

## Open Questions the Paper Calls Out

- **Open Question 1:** Can SID be adapted for closed-source LLM APIs without internal logits/attention access?
- **Open Question 2:** Can a learned compression policy outperform the current rule-based semantic preservation heuristic?
- **Open Question 3:** Does SID maintain performance on open-ended generation tasks with ambiguous ground truth?

## Limitations
- Relies on internal model states (logits, attention) unavailable in closed-source APIs
- Attention-based compression assumes attention weights reflect semantic relevance, which may not always hold
- Vocabulary-adaptive threshold heuristic may not generalize across model families

## Confidence

- **High confidence:** SID outperforms standard MAD across multiple benchmarks (consistent accuracy gains, token reductions)
- **Medium confidence:** Theoretical justification for confidence metrics assumes well-calibrated models; attention-based compression plausible but not rigorously validated
- **Low confidence:** Claim about self-signals being more reliable than external mechanisms lacks direct empirical comparison

## Next Checks
1. Test whether entropy and NLL correlate with correctness by plotting confidence vs accuracy curves on held-out validation set
2. Evaluate compression quality by measuring retention of critical disagreement points vs introduction of noise
3. Perform ablation study by disabling early-exit mechanism to quantify marginal contribution and test for overconfidence failures