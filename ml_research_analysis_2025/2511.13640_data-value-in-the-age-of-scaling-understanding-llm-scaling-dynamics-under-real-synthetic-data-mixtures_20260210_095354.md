---
ver: rpa2
title: 'Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under
  Real-Synthetic Data Mixtures'
arxiv_id: '2511.13640'
source_url: https://arxiv.org/abs/2511.13640
tags:
- data
- synthetic
- real
- training
- valuation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the scaling dynamics of LLMs trained on real-synthetic\
  \ data mixtures, identifying a three-phase learning pattern\u2014rapid head learning,\
  \ plateau, and tail learning\u2014with two critical breakpoints. It derives a generalization\
  \ bound that accounts for real and synthetic data distributions, training dynamics\
  \ via NTK, and the mixture ratio."
---

# Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under Real-Synthetic Data Mixtures

## Quick Facts
- arXiv ID: 2511.13640
- Source URL: https://arxiv.org/abs/2511.13640
- Reference count: 40
- This paper studies LLM scaling dynamics under real-synthetic data mixtures, identifies three-phase learning patterns, derives generalization bounds, and proposes a retraining-free data valuation method.

## Executive Summary
This work addresses the challenge of data valuation in large language model training where datasets contain both real and synthetic data. The authors identify a three-phase scaling dynamic (rapid head learning, plateau, tail learning) with two critical breakpoints when training on real-synthetic mixtures. Based on theoretical analysis of these dynamics, they derive a generalization bound that enables retraining-free data valuation by combining empirical losses, distribution discrepancy, and NTK initialization terms. The method achieves high correlation with ground truth while being significantly more efficient than baseline approaches.

## Method Summary
The paper proposes a data valuation method for real-synthetic mixtures that avoids full retraining. The valuation score v(S) combines four components: empirical losses on real and synthetic subsets weighted by mixture ratio π, distribution discrepancy between subsets and test set via MK-MMD, NTK-based training dynamics term computed at initialization, and a composition term. Weights are fit via linear regression. The approach leverages theoretical analysis showing three-phase scaling dynamics with breakpoints at |S| = k^β and |S| = k^β/π, where k is truncation rank and β is tail exponent.

## Key Results
- Three-phase scaling dynamics observed across four tasks with breakpoints at predicted locations
- Up to 0.87 Spearman and 0.96 Pearson correlation with ground truth data value
- Runtime efficiency demonstrated through 1% gradient subsampling while maintaining stable rankings
- Consistent contributor rankings across 100x range in subsample sizes (100-4000 samples)

## Why This Works (Mechanism)

### Mechanism 1: Three-Phase Scaling Dynamics from Long-Tail Coverage
Models exhibit distinct learning phases governed by when head vs. tail knowledge receives sufficient coverage. Synthetic data truncates tail knowledge (p'_i = 0 for i > k due to top-p sampling), while real data provides tail coverage but at lower proportion π. Head knowledge saturates first (both sources contribute), creating a plateau until real samples accumulate enough tail observations. Breakpoints occur at |S| = k^β and |S| = k^β/π.

### Mechanism 2: Generalization Bound via NTK and Distribution Discrepancy
Test error on real distribution can be bounded by combining empirical losses, distribution discrepancy, NTK initialization, and mixture ratio. The bound L_DT(f) ≤ πL_S1(f) + (1-π)L_S2(f) + πd_H(T,S1) + (1-π)d_H(T,S2) + NTK term separates contributions from real vs. synthetic subsets. Computing these terms at initialization enables retraining-free valuation.

### Mechanism 3: Scalable Valuation via MK-MMD and Subsampling Stability
Relative contributor rankings remain stable under subsampling, enabling efficient large-scale valuation. MK-MMD captures multi-scale distribution discrepancies without retraining. The valuation score's NTK and MMD components maintain consistent contributor rankings across subsample sizes (100 to 4000 samples).

## Foundational Learning

- **Neural Tangent Kernel (NTK)**: Used to estimate training dynamics without actually training by computing gradient interactions across samples at initialization. Quick check: Can you explain why NTK remains approximately constant during training for sufficiently wide networks, and why λ_min(Θ) > 0 matters for inversion?

- **Distribution Discrepancy (H-divergence, MMD)**: Quantifies how synthetic vs. real data diverge from test distribution using d_H and MK-MMD. Quick check: Given two datasets, how would you compute MK-MMD, and what does a high value indicate about their relationship?

- **Long-Tail Distribution and Truncation Effects**: Three-phase scaling emerges from tail truncation in synthetic data. Understanding p_i ∝ i^-β and how top-p/temperature sampling cuts off rare tokens is critical. Quick check: If β = 2 and k = 100, what proportion of total probability mass is lost when synthetic data truncates beyond rank k?

## Architecture Onboarding

- **Component map**: Data partitioning -> NTK computation -> MK-MMD estimation -> Loss evaluation -> Scoring aggregation
- **Critical path**: 1) Initialize model → compute Θ_0 (O(|S|²) in naive form, use approximations for LLM scale) 2) For each contributor: compute MMD to test set, compute empirical loss, extract ȳ vector 3) Aggregate via v(S) = w₁[loss terms] + w₂[discrepancy terms] + w₃[NTK term] + w₄[scale term] 4) Rank contributors by v(S)
- **Design tradeoffs**: Exact NTK vs. approximation (exact is O(|S|²), infeasible for large datasets); MK-MMD kernel selection (multiple kernels capture different scales but add compute); Weight fitting (paper fits w₁...w₄ via linear regression to empirical loss + MMD)
- **Failure signatures**: Negative correlations with ground truth (check gradient sign conventions); Unstable rankings under subsampling (violate core assumption); Θ₀ singular or near-singular (NTK inversion fails)
- **First 3 experiments**: 1) Validate three-phase scaling on controlled long-tail data (CIFAR-100 with known long-tail class distribution, verify breakpoints at |S|≈kᵝ and |S|≈kᵝ/π) 2) Ablate valuation components (compute correlation with ground truth using only MMD term, only NTK term, only loss term, then combinations) 3) Stress test subsampling (compute valuation scores at 1%, 5%, 10%, 50%, 100% of data, report Kendall rank correlation between subsampled and full rankings)

## Open Questions the Paper Calls Out

### Open Question 1
Does the three-phase scaling behavior generalize to non-long-tail knowledge distributions or distributions with different tail heaviness parameters β? The theoretical analysis assumes p_i ∝ i^{-β} and specific truncation at rank k, but real-world knowledge may deviate from strict Zipfian distributions.

### Open Question 2
Can the weighting coefficients w₁, w₂, w₃, w₄ in the valuation function be derived theoretically rather than fitted empirically? The paper treats these as tunable hyperparameters optimized via linear regression without theoretical guidance.

### Open Question 3
How robust is the valuation method to synthetic data generated through mechanisms that do not produce hard truncation at rank k? The framework assumes synthetic data distribution is truncated: p'_i = 0 for i > k, but real generation methods may produce soft tails.

### Open Question 4
Does the NTK-based valuation component remain meaningful for modern transformer architectures where infinite-width assumptions may not hold? The theoretical analysis assumes m₁, ..., m_{L-1} → ∞ for NTK convergence, yet experiments use finite-width LLMs.

## Limitations
- Theoretical assumptions about long-tail distributions and truncation effects may not generalize to all domains
- NTK-based generalization bound relies on well-conditioned initialization and bounded gradients that could break for poorly initialized models
- MK-MMD approach may miss subtle synthetic data artifacts despite computational efficiency

## Confidence
- Three-phase scaling dynamics: High (empirically validated across four tasks with consistent breakpoints)
- NTK-based generalization bound: Medium (theoretically sound but relies on initialization assumptions)
- Subsampling stability: Medium (demonstrated on limited contributor sets, may not hold for highly heterogeneous data)
- Runtime efficiency claims: High (method avoids full retraining, subsampling approach validated)

## Next Checks
1. Test three-phase scaling on datasets with different power-law exponents (β ≠ 2) and synthetic truncation points to verify breakpoint predictions generalize
2. Perform ablation studies on NTK conditioning by intentionally using poorly initialized models or adding synthetic data with known distribution shift
3. Evaluate valuation stability on datasets with highly non-uniform contributor quality distributions to stress-test the subsampling assumption