---
ver: rpa2
title: How Far Can Pretrained LLMs Go in Symbolic Music? Controlled Comparisons of
  Supervised and Preference-based Adaptation
arxiv_id: '2601.22764'
source_url: https://arxiv.org/abs/2601.22764
tags:
- music
- symbolic
- generation
- language
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares how well pretrained LLMs adapt to symbolic
  music generation and understanding tasks using ABC notation. It evaluates supervised
  finetuning (SFT) and preference-based optimization (DPO) against an off-the-shelf
  instruction-tuned baseline and a music-specialized model.
---

# How Far Can Pretrained LLMs Go in Symbolic Music? Controlled Comparisons of Supervised and Preference-based Adaptation

## Quick Facts
- **arXiv ID**: 2601.22764
- **Source URL**: https://arxiv.org/abs/2601.22764
- **Reference count**: 10
- **Primary result**: Supervised fine-tuning best for music understanding; preference-based optimization better preserves general capabilities but degrades domain perplexity.

## Executive Summary
This study systematically evaluates how pretrained LLMs adapt to symbolic music tasks using ABC notation through supervised fine-tuning (SFT) and preference-based optimization (DPO) compared to an off-the-shelf instruction-tuned baseline and a music-specialized model. The research reveals critical tradeoffs: SFT excels at music understanding but risks general capability loss, while DPO preserves general abilities better but achieves worse domain perplexity. Music-specialized models show strong in-domain performance but lose general instruction-following ability. The work highlights limitations in current symbolic music evaluation metrics and underscores the need for better methods to balance domain gains with general capability retention.

## Method Summary
The study adapts LLaMA 3.1 Inst. 8B to symbolic music using ABC notation through two finetuning strategies: supervised fine-tuning (SFT) with cross-entropy loss and preference-based optimization (DPO) using synthetic degradation pairs. A unified instruction dataset combines ABC tunes, PDMX, DLC, Open Lieder, Open String Quartets, and MusicPile-sft, split into short (≤500 tokens) and long (>500 tokens) sequences. Models are evaluated on perplexity (token-level fit), Fréchet Music Distance (FMD) over CLAMP2 (global musical similarity), and MMLU (general capability retention). ChatMusician serves as a music-specialized baseline.

## Key Results
- SFT achieves best perplexity on music understanding tasks (36.3 on MusicPile-Short vs 693.9 baseline)
- DPO degrades in domain perplexity but retains general instruction-following capabilities better than other methods
- Music-specialized models (ChatMusician) show strongest in-domain performance but lose general capability
- Evaluation metrics capture different aspects: perplexity measures token-level fit while FMD measures global musical similarity

## Why This Works (Mechanism)

### Mechanism 1: Supervised Fine-Tuning Concentrates Probability Mass on Domain-Specific Token Patterns
Cross-entropy loss over concatenated prompt-context-target sequences forces the model to reallocate representational capacity toward ABC syntax, rhythmic patterns, and music-theoretic terminology, improving next-token prediction within the domain. The instruction-tuned backbone has sufficient residual capacity to absorb domain patterns without catastrophic interference.

### Mechanism 2: Preference-Based Optimization with Synthetic Degradations Preserves General Capabilities
The contrastive DPO objective learns relative preferences rather than exact token replication, avoiding overfitting to specific training sequences and preserving prior knowledge representations. Musically motivated degradations (key changes, pitch swaps, bar truncation) produce meaningful negative samples that teach musical quality distinctions.

### Mechanism 3: Music-Specialized Pretraining Trades Domain Performance for General Capability
Extended exposure to domain-specific token distributions shifts the model's prior toward music-specific reasoning patterns, at the cost of general language competencies encoded during initial pretraining. Domain gains compete for the same parameter space as general capabilities.

## Foundational Learning

- **ABC Notation as Text Representation**: The entire study frames symbolic music as ASCII-encoded sequences (notes, rhythms, meter, key) compatible with standard LLM tokenizers. Quick check: Can you identify the key signature, meter, and first three notes in `X:1\nT:Example\nM:4/4\nK:C\nCDEF|`?
- **Perplexity vs. Distributional Metrics (FMD)**: The paper shows these metrics capture different signal qualities—perplexity measures token-level fit, FMD measures global musical similarity. Quick check: Why might a model achieve low perplexity but poor FMD, or vice versa?
- **Domain Adaptation vs. Catastrophic Forgetting Tradeoff**: All finetuning strategies show some tradeoff between music-domain gains and general capability retention. Quick check: If MMLU scores drop significantly after SFT on music data, what does this imply about the model's representations?

## Architecture Onboarding

- **Component map**: LLaMA 3.1 Inst. 8B -> Tokenizer (standard LLaMA) -> SFT/DPO training -> Evaluation (PPL, FMD, MMLU)
- **Critical path**: 1. Curate and unify instruction data → 2. Split by sequence length (<500 tokens = short, ≥500 = long) → 3. Apply SFT or construct preference pairs via Algorithm 1 degradations → 4. Train and evaluate on both domain and general benchmarks
- **Design tradeoffs**: SFT: Better domain perplexity, higher risk of general capability loss; DPO: Worse domain perplexity, better general capability retention; Sequence length: Long-sequence models show weaker adaptation to short-sequence test sets
- **Failure signatures**: Extremely high perplexity on specific datasets (e.g., DPO shows PPL 517.3 on MusicPile-Short vs. 36.3 for SFT); FMD scores near zero indicate model output distribution doesn't match target distribution; "na" results for ChatMusician due to unreadable generation or padding issues
- **First 3 experiments**: 1. Replicate SFT on short-sequence data and measure PPL/FMD delta on MusicPile test split vs. base model; 2. Implement Algorithm 1 degradation pipeline and verify DPO preference pairs maintain ABC syntactic validity; 3. Run MMLU evaluation before and after finetuning to quantify general capability retention; compare SFT vs. DPO degradation rates

## Open Questions the Paper Calls Out

- **Vocabulary Expansion**: Does extending the LLM vocabulary with music-specific special tokens (e.g., for barlines, ornaments, or voice separators) improve symbolic music generation fidelity compared to standard tokenization?
- **Capability Preservation**: Can strategies like data mixing or multi-objective training prevent the degradation of general instruction-following capabilities during music domain adaptation?
- **Metric Refinement**: How can symbolic music evaluation metrics be refined to better correlate with human judgments of long-range structure and musicality?
- **Human Preference Data**: Do models trained with human preference data outperform those trained on synthetic degradation signals (Algorithm 1) for symbolic music alignment?

## Limitations

- **Data Quality and Representativeness**: The unified dataset combines multiple sources with varying musical styles and annotation quality, but lacks detailed statistics on musical diversity or difficulty distribution within each subset.
- **Evaluation Metric Alignment**: The study employs three distinct evaluation paradigms but does not establish clear correspondence between these metrics and human musical quality judgments, and ChatMusician baseline produces "na" results for several metrics.
- **Generalization Boundaries**: While all finetuning methods show some MMLU score degradation, the paper does not establish clear thresholds for acceptable capability retention or provide guidance on when music specialization becomes counterproductive.

## Confidence

- **High Confidence**: SFT achieves superior perplexity scores on music understanding tasks compared to off-the-shelf instruction-tuned baselines, directly supported by quantitative results (36.3 vs 693.9 PPL).
- **Medium Confidence**: DPO preserves general instruction-following capabilities better than SFT despite higher domain perplexity, supported by comparative MMLU retention but lacking full characterization of the mechanism.
- **Low Confidence**: Music-specialized pretraining models represent an optimal long-term strategy for symbolic music tasks, as the paper shows limited analysis of whether this specialization creates irreversible capability loss.

## Next Checks

1. **Ablation Study on Degradation Parameters**: Systematically vary the probability parameters (p for pitch swaps, B_max for bar truncation) in Algorithm 1 to determine which specific degradations contribute most to DPO's general capability retention, measuring both domain perplexity and MMLU scores across different degradation intensities.

2. **Human Evaluation Correlation Study**: Conduct blinded human assessments of musical quality across SFT, DPO, and ChatMusician outputs, then correlate these judgments with automated metrics (PPL, FMD, MMLU) to validate whether current evaluation paradigms capture meaningful quality distinctions.

3. **Capability Recovery Experiment**: After music-specialized finetuning, attempt to restore general instruction-following by training on mixed-domain data (music plus general instruction datasets) to measure whether MMLU scores can be recovered without completely sacrificing domain performance.