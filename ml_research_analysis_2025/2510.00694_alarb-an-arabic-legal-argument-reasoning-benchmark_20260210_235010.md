---
ver: rpa2
title: 'ALARB: An Arabic Legal Argument Reasoning Benchmark'
arxiv_id: '2510.00694'
source_url: https://arxiv.org/abs/2510.00694
tags:
- reasoning
- verdict
- facts
- legal
- case
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ALARB is a dataset and benchmark for evaluating Arabic legal reasoning
  in LLMs. It comprises over 13K structured commercial court cases from Saudi Arabia,
  each including facts, reasoning, verdict, and cited statutes.
---

# ALARB: An Arabic Legal Argument Reasoning Benchmark

## Quick Facts
- arXiv ID: 2510.00694
- Source URL: https://arxiv.org/abs/2510.00694
- Reference count: 25
- ALARB is a dataset and benchmark for evaluating Arabic legal reasoning in LLMs

## Executive Summary
ALARB introduces a comprehensive dataset of over 13K structured commercial court cases from Saudi Arabia to evaluate Arabic legal reasoning capabilities in large language models. The benchmark defines multiple tasks including verdict prediction (from facts alone, with regulations, or with reasoning) and article identification, and evaluates leading Arabic and multilingual models. Instruction-tuning a 12B parameter model on ALARB significantly improves its performance, achieving GPT-4o-level accuracy. Models show varying performance based on context provision and language of reasoning, with some performing better when reasoning in English than Arabic.

## Method Summary
ALARB was constructed by scraping over 13K structured commercial court cases from the Saudi Ministry of Justice website, with each case including facts, reasoning steps, verdict, and cited statutes. The dataset was split into training (12,012 cases) and test sets (1,329 cases). Article identification tasks were created as multiple-choice questions with distractors. A Gemma-3-12B model was fine-tuned using supervised fine-tuning with three instruction tasks: generating reasoning and verdict from facts and regulations, predicting verdict from facts, regulations, and reasoning, and generating reasoning from facts, regulations, and verdict. Models were evaluated using GPT-4o as judge with CORRECT/PARTIALLY CORRECT/INCORRECT labels for verdict tasks and exact match for article identification.

## Key Results
- Instruction-tuning a 12B parameter model on ALARB significantly improves verdict prediction accuracy, reaching GPT-4o-level performance
- Some models (AceGPT-v2, Falcon-7B) perform worse when provided with relevant regulations compared to facts-only
- Models generally perform better when reasoning in English than Arabic, suggesting potential language-dependent reasoning biases

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific instruction-tuning can elevate mid-sized models (12B parameters) to match frontier model performance on specialized legal reasoning tasks. Supervised fine-tuning on structured legal cases enables models to internalize domain-specific reasoning patterns, legal terminology, and argument structures. The fine-tuned Gemma-3-12B-SFT achieved 37.3% correct verdicts from facts alone (vs. 15.8% baseline), a +21.5 point gain.

### Mechanism 2
Providing relevant legal regulations as context improves reasoning accuracy for capable models but degrades performance for less robust reasoners. Strong models (GPT-4o, Qwen3-14B) leverage additional context to ground reasoning in specific statutory language, while weaker models experience "context confusion" where regulatory text introduces competing interpretations.

### Mechanism 3
Multilingual models may reason more effectively in English even for non-English domain tasks, suggesting English-centric internal representations. Models trained predominantly on English corpora may have more developed reasoning circuits in English representation space, leading to higher performance when reasoning in English despite Arabic output.

## Foundational Learning

- **Structured Legal Reasoning**: Understanding the decomposition of legal cases into discrete components (facts, reasoning steps, verdict, regulations) is prerequisite to designing tasks and interpreting model failures.
  - Quick check: Given a case with 8 fact statements and 6 reasoning steps, what's the minimum information needed to predict the verdict?

- **Instruction Tuning vs. In-Context Learning**: Distinguishing these paradigms clarifies when each approach is appropriate for legal reasoning tasks.
  - Quick check: If you have 100 labeled cases, should you use them for few-shot prompting or fine-tuning? What factors determine the answer?

- **LLM-as-Judge Evaluation**: Understanding meta-evaluation challenges and potential biases in using GPT-4o as judge with CORRECT/PARTIAL/INCORRECT labels.
  - Quick check: What types of verdict errors might an LLM judge systematically miss or over-penalize?

## Architecture Onboarding

- **Component map**: MoJ scraping → NER for article extraction → restructuring/anonymization → train/test split → SFT training → inference → GPT-4o evaluation
- **Critical path**: Article citation extraction (NER accuracy determines regulation grounding quality) → Reasoning step segmentation (affects argument completion task difficulty) → Judge prompt design (evaluation reliability depends on prompt clarity)
- **Design tradeoffs**: MCQ vs. open-ended (trades realism for measurability), ternary vs. finer evaluation (trades granularity for annotation cost), translation vs. native evaluation (trades evaluation purity for performance gains)
- **Failure signatures**: Context confusion (models perform worse with regulations than without), partial reasoning cascade (error rates increase ~5-10% per removed reasoning step), language mismatch (Arabic reasoning underperforms English by 9+ points for some models)
- **First 3 experiments**: 1) Replicate facts-only verdict prediction on 100-case subset using open model, 2) Compare facts-only vs facts+regulations vs facts+reasoning to identify context effects, 3) Test language-of-reasoning by prompting to reason in English while outputting verdicts in Arabic

## Open Questions the Paper Calls Out

- **Open Question 1**: Why do multilingual models perform better when prompted to reason in English rather than Arabic for Arabic legal tasks? The paper observes the performance gap but doesn't investigate whether this stems from training data dominance, tokenization efficiency, or cross-lingual alignment issues.

- **Open Question 2**: Can a finer-grained evaluation scale improve the reliability of the LLM-as-a-judge methodology? The current ternary classification may obscure nuances in legal accuracy or stylistic faithfulness.

- **Open Question 3**: Does instruction-tuning on ALARB yield performance gains for models significantly larger than 12B parameters? The authors acknowledge this was done primarily for convenience and that larger models need investigation.

- **Open Question 4**: What are the specific failure dynamics in open Arabic models that cause them to pursue incorrect reasoning paths before self-correcting? The paper identifies the phenomenon but lacks qualitative breakdown of why models fail to apply legal articles correctly in initial steps.

## Limitations

- The dataset focuses exclusively on Saudi commercial court cases, potentially limiting generalizability to other legal domains or jurisdictions
- GPT-4o as judge may introduce bias by sharing implicit reasoning patterns with evaluated models
- Performance gains from instruction-tuning were demonstrated on a single model (Gemma-3-12B) with specific hyperparameters, raising replicability questions

## Confidence

**High Confidence**: Dataset construction methodology and basic task definitions are well-documented and reproducible; pattern that some models perform worse with regulatory context is clearly demonstrated.

**Medium Confidence**: Claim that instruction-tuning achieves GPT-4o-level performance is supported by metrics but involves different model families; language-of-reasoning effects are observed but underlying mechanism remains speculative.

**Low Confidence**: Assertion that performance gains reflect learned legal reasoning rather than memorization depends on assumptions about train-test split representativeness; ternary evaluation scale may mask important performance nuances.

## Next Checks

1. **Cross-jurisdiction validation**: Evaluate ALARB-trained models on commercial court cases from a different jurisdiction (e.g., UAE or Egypt) to test generalizability beyond the Saudi legal framework.

2. **Human evaluation validation**: Have legal experts manually review a stratified sample of model predictions (especially borderline PARTIALLY CORRECT cases) to validate the LLM-as-judge reliability and identify systematic evaluation biases.

3. **Ablation study on training data**: Train models with progressive subsets of ALARB cases (100, 500, 1000, full dataset) to quantify whether performance gains are linear with data volume or exhibit diminishing returns.