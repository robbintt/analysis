---
ver: rpa2
title: 'Prediction-powered estimators for finite population statistics in highly imbalanced
  textual data: Public hate crime estimation'
arxiv_id: '2505.04643'
source_url: https://arxiv.org/abs/2505.04643
tags:
- hate
- crimes
- police
- crime
- reports
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of estimating population parameters
  in finite populations of text documents when manual annotation is costly and time-consuming,
  particularly in highly imbalanced settings such as hate crime statistics. The proposed
  solution leverages transformer-based neural network predictions as an auxiliary
  variable in well-established survey sampling estimators, enabling more efficient
  and unbiased estimation.
---

# Prediction-powered estimators for finite population statistics in highly imbalanced textual data: Public hate crime estimation

## Quick Facts
- arXiv ID: 2505.04643
- Source URL: https://arxiv.org/abs/2505.04643
- Reference count: 40
- One-line result: Transformer predictions as auxiliary variables reduce sample size from ~28,000 to 200 for hate crime estimation, achieving a design effect of 0.0068

## Executive Summary
This work addresses the challenge of estimating population parameters in finite populations of text documents when manual annotation is costly and time-consuming, particularly in highly imbalanced settings such as hate crime statistics. The proposed solution leverages transformer-based neural network predictions as an auxiliary variable in well-established survey sampling estimators, enabling more efficient and unbiased estimation. The approach involves training a transformer encoder neural network (e.g., BERT or RoBERTa) on a binary classification task, using its predictions as an auxiliary variable for sampling, and applying standard estimators like the Hansen-Hurwitz estimator, stratified random sampling, or difference estimation. This method was demonstrated using Swedish hate crime statistics, where only a small fraction of police reports contain hate crime motives.

## Method Summary
The method fine-tunes pre-trained Swedish BERT/RoBERTa models on binary classification of hate crime police reports, generating predicted probabilities ($\hat{p}$) for each document. These predictions serve as an auxiliary variable in the Hansen-Hurwitz (HH) estimator with Probability-Proportional-to-Size (PPS) sampling, where inclusion probabilities $\pi_i \propto \hat{p}_i$. The HH estimator $\hat{t}_{HH} = \frac{1}{n} \sum \frac{y_i}{\pi_i}$ uses gold-standard manual annotations $y_i$ of sampled units to produce unbiased population total estimates. Domain adaptation (optional) involves continued MLM pre-training on police reports before fine-tuning with standard hyperparameters (LR=2e-5, batch=16, AdamW, early stopping).

## Key Results
- Transformer classifier achieved >95% accuracy and >95% F1-scores on expert-annotated test data, outperforming police annotations
- Using classifier as auxiliary information reduced required sample size from ~28,000 to 200 for comparable accuracy (design effect = 0.0068)
- Estimated total number of hate crimes in Swedish police reports for 2022 was 6,051 (SE = 548), indicating police missed flagging 2,260-4,452 true hate crimes
- Method significantly improves efficiency in estimating rare events in large text corpora

## Why This Works (Mechanism)

### Mechanism 1: Variance Reduction via Correlated Auxiliary Variables
- **Claim:** Utilizing model predictions as an auxiliary variable in survey sampling can drastically reduce the variance of population total estimates compared to Simple Random Sampling (SRS).
- **Mechanism:** The transformer model predicts probability $\hat{p}_i$ for each document. In the Hansen-Hurwitz estimator, inclusion probability $\pi_i$ is set proportional to $\hat{p}_i$. Because $\hat{p}_i$ is correlated with the true label $y_i$, this sampling strategy concentrates the sample on units likely to belong to the rare class (hate crimes), reducing the number of "wasted" observations on the majority class.
- **Core assumption:** The classifier's predictions must be positively correlated with the true labels; specifically, the cross-entropy loss must be sufficiently low for variance reduction.
- **Evidence anchors:**
  - Using the classifier as auxiliary information reduced the required sample size from approximately 28,000 to 200... with a design effect of 0.0068.
  - Proposition 1 demonstrates that as classifier loss approaches zero, the variance of the HH estimator approaches zero.
- **Break condition:** If the classifier has high bias or near-random performance on the target population, the correlation between $\hat{p}$ and $y$ breaks down, and the design effect approaches 1 (no gain over SRS).

### Mechanism 2: Maintaining Unbiasedness via Design-based Inference
- **Claim:** The proposed Prediction-Powered estimators (e.g., H2P2) yield unbiased estimates of population totals even when the underlying machine learning classifier makes prediction errors.
- **Mechanism:** Unlike "plug-in" estimators that sum model predictions directly, this method uses predictions only to determine sampling probabilities $\pi_i$. The final estimator $\hat{t}_{HH} = \frac{1}{n} \sum \frac{y_i}{\pi_i}$ relies on the gold-standard manual annotation $y_i$ of the sampled units. As long as the sampling probabilities are known and non-zero, the design-based estimator remains unbiased regardless of model quality.
- **Core assumption:** The manual annotations $y_i$ in the sample are accurate (gold standard).
- **Evidence anchors:**
  - The HH estimator is unbiased... if we select an auxiliary variable that is highly correlated with the target $y$, we can reduce the variance.
  - However, the naive use of prediction models introduces bias... [our method] maintains asymptotically unbiased parameter estimates.
- **Break condition:** Unbiasedness holds for the total, but breaks for domain estimation if the stratification strategy fails to sample any positive cases in a specific stratum (e.g., the "zero-stratum" issue).

### Mechanism 3: Domain Adaptation for Low-Resource Register Text
- **Claim:** Fine-tuning transformer models on domain-specific text (police reports) improves classification performance (F1), which directly translates to lower estimator variance.
- **Mechanism:** Standard pre-trained models (e.g., BERT) may struggle with the specific jargon, structure, or brevity of police reports. Continued pre-training (Domain Adaptation) on the unlabeled report corpus aligns the model's representation space with the target domain, improving the accuracy of the auxiliary variable $\hat{p}$.
- **Core assumption:** The language distribution of the unlabeled police reports is relevant to the classification task.
- **Evidence anchors:**
  - Domain adaptation... has been shown to improve performance on the downstream task.
  - For the BERT-type models, the accuracies and F1-scores were approximately 95%... The BERT-DA model had the highest F1 score.
- **Break condition:** If the labeled training data is statistically dissimilar from the target population (distribution shift), domain adaptation may not close the gap, leading to poor $\hat{p}$ estimates.

## Foundational Learning

- **Concept: Probability Proportional to Size (PPS) Sampling & The Hansen-Hurwitz Estimator**
  - **Why needed here:** This is the statistical engine of the paper. You must understand how sampling with replacement with probabilities $\pi_i \propto \hat{p}_i$ allows us to back-calculate an unbiased population total.
  - **Quick check question:** If a unit has a high predicted probability (e.g., $\hat{p}=0.9$) and is sampled, does it contribute a larger or smaller value to the sum $\frac{y}{\pi}$ compared to a unit with low probability?

- **Concept: The "Plug-in" vs. Survey Sampling Approach**
  - **Why needed here:** To distinguish why this method is valid while summing raw model predictions is not. The paper explicitly contrasts this approach with naive "prediction-powered" uses that introduce bias.
  - **Quick check question:** If a model predicts 1000 hate crimes but has a 10% false positive rate, does the survey sampling approach simply accept this 1000, or does it correct for it? How?

- **Concept: Design Effect (DEFF)**
  - **Why needed here:** The paper cites a DEFF of 0.0068. You need to know that DEFF < 1 implies the complex design (using the model) is more efficient than simple random sampling.
  - **Quick check question:** If the DEFF is 0.01, how many times smaller a sample do you need to achieve the same standard error as a Simple Random Sample?

## Architecture Onboarding

- **Component map:** Data Ingest (Police Report Text + Historical Annotated Data) -> ML Layer (Transformer -> Predicted Probabilities $\hat{p}$) -> Sampling Layer (PPS/Stratification) -> Human Layer (Expert Annotation $y$) -> Estimation Layer (HH/Stratified Estimator)
- **Critical path:** The quality of the auxiliary variable $\hat{p}$. If the model predicts random noise, the sampling becomes inefficient (effectively random), wasting annotation budget.
- **Design tradeoffs:**
  - H2P2 (Hansen-Hurwitz): Highest efficiency (lowest variance) but produces a "thick right tail" in distribution (occasional very high estimates if false negatives are sampled)
  - Stratified-by-Prediction (SbP): Better for estimating sub-domains (e.g., specific types of hate crimes), but risky for the "zero-stratum" (predicted negatives) where variance estimation is unstable
- **Failure signatures:**
  - Thick Right Tail: The H2P2 estimator occasionally spikes. This happens when a true positive $y=1$ is found in a unit with very low predicted probability $\hat{p}$, causing the term $\frac{y}{\pi}$ to explode
  - Zero-Stratum Bimodality: In SbP, if the "predicted negative" stratum is sampled and happens to contain 0 or 1 positive cases, the variance estimate fluctuates wildly or becomes zero (underestimation of uncertainty)
- **First 3 experiments:**
  1. **Monte Carlo Simulation on Known Subset:** Replicate the "incitement against ethnic group" experiment. Compare the standard error of your implementation of H2P2 against SRS to verify the efficiency gains
  2. **Classifier Calibration Check:** Verify if the predicted probabilities $\hat{p}$ are calibrated. If the model predicts $\hat{p}=0.8$, do 80% of those documents actually contain hate crimes? (Poor calibration hurts efficiency but preserves unbiasedness)
  3. **Stratum Sensitivity Analysis:** Test the SbP method with varying classification thresholds $\tau$. Does moving the threshold reduce the "zero-stratum" variance without increasing the sample size excessively?

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the theoretical properties of prediction-powered sampling estimators be formally characterized, particularly regarding variance bounds and optimal sampling strategies under model misspecification?
- **Basis in paper:** [explicit] "One is to further study the theoretical properties of prediction-powered sampling estimators."
- **Why unresolved:** Proposition 1 shows variance approaches zero under perfect classification, but real-world performance under model errors remains uncharacterized theoretically.
- **What evidence would resolve it:** Derivation of variance bounds that account for classifier error rates and their propagation through the estimation pipeline.

### Open Question 2
- **Question:** Can the Horvitz-Thompson estimator be effectively adapted for prediction-powered finite population estimation, and how does it compare to the Hansen-Hurwitz estimator in terms of efficiency and robustness?
- **Basis in paper:** [explicit] "This would also include extending the results in this paper to the use of the Horvitz-Thompson estimator instead of the Hansen-Hurwitz estimator."
- **Why unresolved:** The current work only implements Hansen-Hurwitz with replacement sampling; without-replacement alternatives remain unexplored.
- **What evidence would resolve it:** Implementation and empirical comparison of Horvitz-Thompson-based prediction-powered estimators on the same hate crime data.

### Open Question 3
- **Question:** How can large language models (LLMs) be adapted to provide well-calibrated probability outputs suitable for the H2P2 estimator, particularly for sensitive governmental data requiring local deployment?
- **Basis in paper:** [explicit] "The use of an LLM is a promising approach... However, to use the H2P2 estimator, a numerical auxiliary variable is needed, and LLMs does not provide this by default."
- **Why unresolved:** LLMs typically output text or discrete predictions rather than calibrated probabilities; local hardware constraints limit model size.
- **What evidence would resolve it:** Development of methods to extract calibrated probabilities from LLMs, tested on the hate crime classification task with fewer training examples.

### Open Question 4
- **Question:** What classifier performance thresholds (F1, accuracy) are necessary to achieve meaningful efficiency gains in prediction-powered estimation for highly imbalanced populations?
- **Basis in paper:** [inferred] "The limitation of the method is the need for a good classifier. Without high-performance classifiers, there will be no performance gains for the estimator."
- **Why unresolved:** The paper demonstrates success with F1 > 0.80, but minimum viable performance levels for various population imbalance ratios remain undetermined.
- **What evidence would resolve it:** Systematic simulation study varying classifier performance and class imbalance to identify break-even points for efficiency gains.

## Limitations
- The method critically depends on having a high-quality classifier as an auxiliary variable; poor classifier performance eliminates efficiency gains
- Distribution shifts between training data and target population could degrade classifier performance and reduce estimator efficiency
- The zero-stratum problem in stratified sampling can lead to unstable variance estimates for domain-level parameters

## Confidence
- **High confidence:** The unbiasedness property of the Hansen-Hurwitz estimator under design-based inference; the general efficiency gains when classifier performance is strong (>95% accuracy reported); the validity of using PPS sampling with predicted probabilities
- **Medium confidence:** The specific efficiency gains (DEFF = 0.0068) and the estimated hate crime totals (6,051 Â± 548), which depend on the quality of the specific classifier trained and the particular sampling realization
- **Medium confidence:** The robustness of the method to classifier errors, as while unbiasedness is guaranteed, the variance reduction benefits require sufficiently accurate predictions

## Next Checks
1. **Monte Carlo Simulation on Known Subset:** Replicate the "incitement against ethnic group" experiment to verify efficiency gains of H2P2 over SRS by comparing standard errors
2. **Classifier Calibration Check:** Verify if predicted probabilities $\hat{p}$ are well-calibrated by checking if documents with $\hat{p}=0.8$ contain hate crimes at approximately 80% rate
3. **Stratum Sensitivity Analysis:** Test the stratified-by-prediction method with varying classification thresholds $\tau$ to assess the tradeoff between reducing zero-stratum variance and sample size requirements