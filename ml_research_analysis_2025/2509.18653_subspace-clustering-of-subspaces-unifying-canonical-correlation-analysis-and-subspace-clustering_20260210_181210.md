---
ver: rpa2
title: 'Subspace Clustering of Subspaces: Unifying Canonical Correlation Analysis
  and Subspace Clustering'
arxiv_id: '2509.18653'
source_url: https://arxiv.org/abs/2509.18653
tags:
- clustering
- proposed
- subspace
- subspaces
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Subspace Clustering of Subspaces (SCoS),
  a novel framework for clustering a collection of tall matrices based on their column
  spaces. Unlike traditional subspace clustering methods that assume vectorized data,
  SCoS directly models each data sample as a matrix and clusters them according to
  their underlying subspaces.
---

# Subspace Clustering of Subspaces: Unifying Canonical Correlation Analysis and Subspace Clustering

## Quick Facts
- **arXiv ID:** 2509.18653
- **Source URL:** https://arxiv.org/abs/2509.18653
- **Reference count:** 40
- **Primary result:** Introduces Subspace Clustering of Subspaces (SCoS), a framework for clustering tall matrices by their column spaces using Block Term Decomposition of a tensor.

## Executive Summary
This paper introduces Subspace Clustering of Subspaces (SCoS), a novel framework for clustering a collection of tall matrices based on their column spaces. Unlike traditional subspace clustering methods that assume vectorized data, SCoS directly models each data sample as a matrix and clusters them according to their underlying subspaces. The proposed approach leverages a Block Term Decomposition (BTD) of a third-order tensor constructed from the input matrices, enabling joint estimation of cluster memberships and partially shared subspaces. The paper provides the first identifiability results for this formulation and proposes scalable optimization algorithms tailored to large datasets. Experiments on real-world hyperspectral imaging datasets demonstrate that the method achieves superior clustering accuracy and robustness, especially under high noise and interference, compared to existing subspace clustering techniques.

## Method Summary
The method formulates the problem as clustering a collection of tall matrices based on their column spaces. It constructs a third-order tensor from orthonormal bases of input matrices, where each frontal slice represents a projection matrix. This tensor is modeled as a symmetric Block Term Decomposition with cluster-specific subspace terms and private noise terms. The approach solves this using an Alternating Least Squares method with either a penalty formulation or an augmented Lagrangian formulation for stability. Scalable updates are achieved through orthogonal iteration for basis updates and efficient gradient computation for assignment updates, avoiding explicit tensor construction.

## Key Results
- SCoS achieves superior clustering accuracy and robustness on hyperspectral datasets compared to existing subspace clustering techniques.
- The method provides first identifiability results for clustering tall matrices by their column spaces using BTD.
- Scalable optimization algorithms enable application to large-scale datasets without materializing large tensors.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Block Term Decomposition of a third-order tensor enables joint recovery of cluster memberships and partially shared subspaces.
- Mechanism: Each input matrix $X_k$ is converted to an orthonormal basis $U_k$, and the projection matrix $P(:,:,k) = U_kU_k^T$ forms one frontal slice of a third-order tensor $\mathcal{P}$. Under the generative model where matrices in cluster $r$ share subspace $G_r$, this tensor admits a symmetric BTD $\mathcal{P} = \sum_{r=1}^R (G_rG_r^T) \circ c_r + \text{private terms}$. Alternating optimization updates $G_r$ via eigendecomposition and $C$ via regularized nonnegative least squares.
- Core assumption: Non-overlapping subsets of matrices span partially common subspaces with identifiable structure (sufficient Kruskal rank, diverse subspaces).
- Evidence anchors:
  - [abstract]: "Our approach is based on a Block Term Decomposition (BTD) of a third-order tensor constructed from the input matrices, enabling joint estimation of cluster memberships and partially shared subspaces."
  - [section III.B, Eq. 7-9]: Constructing $\mathcal{P}$ from projection matrices and formulating recovery as constrained BTD with binary assignment matrix $C$.
  - [corpus]: Related work "ALPCAHUS" and "Deep Subspace Clustering" address subspace clustering but do not validate BTD-based joint estimation; corpus evidence for this specific mechanism is weak.
- Break condition: When cluster subspaces have high overlap or minimum cluster size $\min_r \alpha(r)$ is very small, identifiability conditions (Theorem 1) may fail, and optimization can converge to poor local minima.

### Mechanism 2
- Claim: Augmented Lagrangian with scale-invariant regularization stabilizes optimization under orthogonality constraints and overlapping subspaces.
- Mechanism: Instead of penalty methods that require large $\rho_t$ and cause ill-conditioning, the paper uses augmented Lagrangian (Eq. 11) with dual variable updates $\Lambda_{t+1} = \Lambda_t + \rho_t (\hat{C}^T\hat{C} * Q)$. For overlap-heavy settings, it further introduces $\Psi(\hat{C}) = \sqrt{\hat{C}^T_{norm}\hat{C}_{norm} + \varepsilon}$ (Eq. 12) to penalize cross-column assignments without scaling ambiguity, enabling concurrent basis updates without Stiefel-manifold constraints.
- Core assumption: Regularization on normalized $C$ can suppress multi-cluster assignments while allowing flexible basis trajectories.
- Evidence anchors:
  - [section III.B]: "To address these drawbacks, in this work we propose the adoption of the augmented Lagrangian method, which combines the benefits of penalty-based formulations with dual variable updates, thereby improving numerical stability and convergence behavior."
  - [section III.B, Eq. 12]: Definition of $\Psi(\hat{C})$ and its role in overlap-heavy scenarios.
  - [corpus]: Corpus does not provide direct validation for this regularization design; evidence is method-internal.
- Break condition: If $\rho_t$ or dual updates are poorly tuned, regularization may under-penalize (cross-cluster leakage) or over-penalize (unassigned samples); failure is signaled by rows of $C$ becoming near-zero or multiple large entries per row.

### Mechanism 3
- Claim: Scalable updates via orthogonal iteration and efficient gradient reuse avoid materializing large $N \times N$ matrices.
- Mechanism: Updating $G_{r'}$ requires top-$L_{r'}$ eigenvectors of $\hat{W}_{r'}$ (Eq. 17). Instead of explicit $\mathcal{O}(N^3)$ eigendecomposition, orthogonal iteration computes $\hat{W}_{r'}A$ without forming $\hat{W}_{r'}$, achieving complexity $\mathcal{O}(NL_{r'}(KM_{max} + RL_{max}))$. For $C$ updates, precompute $\hat{B}^T\hat{B}$ and $\mathcal{P}^{(3)}\hat{B}$ via trace tricks (Eq. 21-22) to reduce per-iteration cost to $\mathcal{O}(KR^2)$.
- Core assumption: The number of views $K$, clusters $R$, and subspace dimensions $L_r$ are modest relative to ambient dimension $N$.
- Evidence anchors:
  - [section IV.A.1]: "Adopting the Orthogonal Iteration allows us to obtain a scalable algorithm for updating each basis $\hat{G}_{r'}$, with a complexity that is linear in $K$, $R$, $M_{max}$, and quadratic in $L_r$."
  - [section IV.A.3]: Efficient computation of $\nabla f_{\mathcal{P}|\hat{G}}$ via trace-based simplifications.
  - [corpus]: "Scalable Deep Subspace Clustering Network" notes $O(n^3)$ bottlenecks in spectral clustering but does not validate this specific scalable update scheme.
- Break condition: When $L_r$ or $R$ grow with $N$, complexity may become prohibitive; numerical instability can arise if orthogonal iteration is under-iterated.

## Foundational Learning

- Concept: Block Term Decomposition (rank-$(L_r, L_r, 1)$ terms)
  - Why needed here: BTD provides the structural model for $\mathcal{P}$, separating cluster-specific subspace terms $(G_rG_r^T) \circ c_r$ from private noise terms $H_kH_k^T$. Essential uniqueness (up to permutation and block-wise transformations) is the theoretical basis for identifiability.
  - Quick check question: Given a third-order tensor with symmetric frontal slices, what constraints on $G_r$ and $C$ ensure that a symmetric BTD is essentially unique?

- Concept: Grassmann manifold and chordal distance
  - Why needed here: The cost function $\frac{1}{2}\|\mathcal{P} - \sum_r (\hat{G}_r\hat{G}_r^T) \circ \hat{c}_r\|_F^2$ aggregates squared chordal distances between each sample's column space and its assigned cluster subspace. Understanding this geometric interpretation clarifies why the Frobenius norm is appropriate.
  - Quick check question: Why is the squared chordal distance $d^2_{Gr}(p_1, p_2, n) = \frac{1}{2}\|P_{S_1} - P_{S_2}\|_F^2$ suitable for comparing subspaces of potentially different dimensions?

- Concept: Augmented Lagrangian methods vs. penalty methods
  - Why needed here: The paper transitions from penalty-based formulations (ill-conditioned for large $\rho_t$) to augmented Lagrangian with dual ascent. Understanding this optimization machinery is essential for implementing stable updates and diagnosing convergence issues.
  - Quick check question: How does dual variable update $\Lambda_{t+1} = \Lambda_t + \rho_t (\hat{C}^T\hat{C} * Q)$ improve feasibility enforcement compared to purely increasing $\rho_t$ in a penalty method?

## Architecture Onboarding

- Component map:
  - Preprocessing: Compute orthonormal bases $\{U_k\}$ from input matrices $\{X_k\}$ (e.g., via QR or SVD); construct projection tensor $\mathcal{P}(:,:,k) = U_kU_k^T$ implicitly.
  - Initialization: Initialize $\hat{G}_r$ (random orthonormal or via GCCA on subsets) and $\hat{C}$ (uniform or k-means-based).
  - Optimization loop: (a) Update each $\hat{G}_r$ via orthogonal iteration on $\hat{W}_{r'}$; (b) Update $\hat{C}$ via regularized NMLS with precomputed $\hat{B}^T\hat{B}$ and $\mathcal{P}^{(3)}\hat{B}$; (c) Update dual variables $\Lambda_t$ and penalty $\rho_t$.
  - Post-processing: Estimate subspace dimensions $L_r^*$ via eigenvalue thresholding on $T_{r'} = \sum_{k \in I_{r'}} P_{col(X_k)}$ (using AIC/MDL or similar); refine bases if needed.
  - Model selection: Sweep $R$ and monitor $\bar{\phi} = \frac{1}{K}\sum_k \phi_k$ to detect elbow; avoid overfitting by cross-checking identifiability conditions.

- Critical path:
  1. Verify identifiability conditions (Theorem 1) for your $(N, K, R, \{L_r\}, \{M_k\})$ before large-scale runs.
  2. Implement scalable $\hat{G}_r$ updates via orthogonal iteration; ensure sufficient inner iterations.
  3. For $\hat{C}$ updates, precompute and reuse $\hat{B}^T\hat{B}$ and $\mathcal{P}^{(3)}\hat{B}$; use efficient NMLS solver.
  4. Tune $\rho_t$ schedule and regularization weights; monitor orthogonality violation $\|\hat{C}^T\hat{C} * Q\|_F$ and assignment sparsity.

- Design tradeoffs:
  - Orthogonality on $G_r$ vs. unconstrained formulation: Enforcing $\hat{G}_r^T\hat{G}_r = I_{L_r}$ ensures uniqueness but may lead to poor local minima in overlap-heavy settings; unconstrained formulation (Eq. 12) offers flexibility but requires careful normalization and post-processing.
  - Penalty vs. augmented Lagrangian: Penalty methods are simpler but require large $\rho_t$; augmented Lagrangian is more stable but introduces dual variables and tuning complexity.
  - Scalable eigendecomposition vs. accuracy: Orthogonal iteration reduces complexity but may introduce approximation error; validate against full SVD on small instances.

- Failure signatures:
  - Cross-cluster leakage: Multiple large entries per row in $\hat{C}$ indicate insufficient regularization on $\Psi(\hat{C})$ or $\hat{C}^T\hat{C} * Q$.
  - Unassigned samples: Rows of $\hat{C}$ near zero suggest over-regularization or conflict between spatial homogeneity and assignment penalties.
  - Degenerate bases: Columns of $\hat{G}_r$ becoming linearly dependent or spanning incorrect subspaces may indicate dimension overestimation or identifiability violation.
  - Slow convergence: Stagnant cost or oscillating assignments may require adjusting $\rho_t$ schedule or switching to unconstrained formulation with normalization.

- First 3 experiments:
  1. Synthetic validation: Generate data from the proposed generative model (Eq. 5) with known $R$, $\{L_r\}$, and controlled noise/overlap; verify recovery against ground truth and test identifiability boundary conditions.
  2. Small hyperspectral subset: Apply SCoS to a small patch (e.g., $50 \times 50$ pixels with $3 \times 3$ neighborhoods) from Indian Pines or Pavia; compare assignments against labeled pixels and visualize segmentation maps.
  3. Regularization and scalability ablation: Compare penalty vs. augmented Lagrangian formulations, and full SVD vs. orthogonal iteration for $\hat{G}_r$ updates; measure runtime, memory, and clustering metrics (ARI, NMI, OA) on medium-sized hyperspectral subsets.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the dimensions of partially common subspaces ($L_r$) be accurately estimated in the presence of significant noise?
- **Basis in paper:** [explicit] The authors state, "In the presence of noise, more effective approaches are needed for determining the dimensions $L_r$," noting that the current method relies on an "overly idealistic assumption" of noiselessness.
- **Why unresolved:** Standard criteria (AIC/MDL) are suggested but identified as potentially insufficient for this specific tensor structure under non-ideal conditions.
- **What evidence would resolve it:** Derivation of a noise-robust estimator specifically for the Block Term Decomposition model that avoids the combinatorial search over dimensions.

### Open Question 2
- **Question:** Does the unconstrained formulation (Equation 12) offer identifiability guarantees similar to the constrained model (Theorem 1) for overlapping subspaces?
- **Basis in paper:** [inferred] The paper provides identifiability proofs only for the constrained formulation but notes that this model "may become ineffective" with overlapping subspaces, necessitating the unconstrained formulation.
- **Why unresolved:** The theoretical analysis (Theorem 1) relies on orthogonality constraints that are explicitly dropped in the alternative formulation proposed for difficult cases.
- **What evidence would resolve it:** A theoretical proof establishing essential uniqueness for the unconstrained BTD under specific overlap conditions.

### Open Question 3
- **Question:** Can the optimization hyperparameters (e.g., penalty coefficients $\rho_t$) be determined automatically without manual tuning?
- **Basis in paper:** [inferred] The experimental section notes parameters were "manually tuned to maximize performance," and the algorithm descriptions rely on heuristic update rules for penalty parameters.
- **Why unresolved:** The method's practical deployment as an unsupervised technique is limited if optimal performance requires manual parameter search.
- **What evidence would resolve it:** An adaptive parameter update scheme or a theoretical bound linking penalty parameters to data noise statistics that eliminates the need for manual selection.

## Limitations
- Identifiability conditions are conservative and may exclude many practical settings.
- Hyperparameter tuning (e.g., $\rho_t$, regularization weights, subspace dimensions) is not fully automated.
- Overlap-heavy scenarios remain challenging despite the unconstrained formulation.

## Confidence
- **High** confidence in the BTD formulation and scalable update equations.
- **Medium** confidence in the augmented Lagrangian stabilization mechanism.
- **Medium** confidence in the overall scalable implementation.

## Next Checks
1. **Synthetic identifiability boundary test**: Generate data from the generative model with controlled overlap and varying cluster sizes; verify recovery accuracy and failure points against Theorem 1 conditions.
2. **Regularization ablation on overlap-heavy data**: Compare penalty vs. augmented Lagrangian formulations and constrained vs. unconstrained updates on synthetic data with overlapping subspaces; measure assignment sparsity and convergence behavior.
3. **Scalability benchmark**: Implement full SVD vs. orthogonal iteration for basis updates on medium-scale hyperspectral subsets; measure runtime, memory usage, and clustering metrics (ARI, NMI, OA).