---
ver: rpa2
title: Geographic Transferability of Machine Learning Models for Short-Term Airport
  Fog Forecasting
arxiv_id: '2510.21819'
source_url: https://arxiv.org/abs/2510.21819
tags:
- data
- atmospheric
- across
- physical
- solar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that a coordinate-free, physics-informed
  machine learning model can successfully forecast airport fog events across diverse
  geographic locations and climates without geographic-specific features. A gradient
  boosting classifier trained on Santiago, Chile data (2002-2009) achieved strong
  zero-shot transfer performance, with AUC values of 0.923-0.947 at three distant
  validation sites (Puerto Montt, San Francisco, London) spanning distances up to
  11,650 km and different fog formation regimes.
---

# Geographic Transferability of Machine Learning Models for Short-Term Airport Fog Forecasting

## Quick Facts
- arXiv ID: 2510.21819
- Source URL: https://arxiv.org/abs/2510.21819
- Reference count: 2
- A coordinate-free, physics-informed machine learning model successfully forecasts airport fog across diverse geographic locations and climates without geographic-specific features.

## Executive Summary
This study demonstrates that a coordinate-free, physics-informed machine learning model can successfully forecast airport fog events across diverse geographic locations and climates without geographic-specific features. A gradient boosting classifier trained on Santiago, Chile data (2002-2009) achieved strong zero-shot transfer performance, with AUC values of 0.923-0.947 at three distant validation sites (Puerto Montt, San Francisco, London) spanning distances up to 11,650 km and different fog formation regimes. SHAP analysis revealed consistent feature importance rankings across sites, with visibility persistence, solar angle, and thermal gradients emerging as the dominant predictors, indicating the model learned transferable physical relationships rather than site-specific patterns. The results show that fundamental thermodynamic and radiative processes governing fog formation can be discovered from data and applied universally, providing a pathway for developing geographically transferable atmospheric forecasting tools that reconstruct physically coherent mechanisms.

## Method Summary
The FOG-Net model uses a gradient boosting classifier trained on 19 physics-informed features engineered from ERA5 reanalysis and METAR observations. Training data comes from Santiago, Chile airport (SCEL) for 2002-2009, with validation on SCEL 2010-2012 and zero-shot transfer to Puerto Montt (SCTE), San Francisco (KSFO), and London (EGLL). Features are coordinate-free, excluding geographic coordinates while capturing physical drivers of fog through moisture availability, cooling mechanisms, atmospheric stability, and temporal persistence. The model predicts T+2h fog occurrence (visibility < 1.0 km) using a strict zero-shot protocol with no adaptation at target sites.

## Key Results
- Achieved AUC values of 0.923-0.947 at zero-shot sites spanning 0-11,650 km distance
- SHAP analysis revealed consistent feature importance rankings across all sites
- Performance degrades gracefully to T+6h, maintaining AUC ~0.86
- Training on 8 years (vs 3 years) substantially improves zero-shot performance (+7.6% AUC at KSFO)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Coordinate-free feature engineering constrains the model to learn transferable physical relationships rather than location-specific patterns.
- Mechanism: By explicitly excluding latitude, longitude, and geographic identifiers from the feature set, the model cannot memorize local climatology. Instead, it must discover underlying physical signals (radiative forcing, boundary layer stability, moisture dynamics) that operate uniformly across locations.
- Core assumption: The fundamental thermodynamic and radiative processes governing fog formation are universal and can be encoded without geographic context.
- Evidence anchors: [abstract] "A coordinate-free (location-independent) feature set... explicitly excluding geographic coordinates"; [Section 2.4.1] Solar angle serves as a "coordinate-free representation" that "constrains the model to learn the physical relationship between solar forcing and fog formation, rather than memorizing latitude-specific patterns"

### Mechanism 2
- Claim: Physics-informed feature selection captures the actual causal drivers of fog, enabling cross-regime generalization.
- Mechanism: Features are engineered to represent known physical pillars: (1) moisture availability via dew point depression and humidity, (2) cooling mechanisms via solar angle and cooling rates, (3) atmospheric stability via thermal gradients and wind speed, and (4) temporal persistence via visibility lags. These encode the trajectory toward saturation.
- Core assumption: The selected 19 features sufficiently approximate the true physical state variables controlling fog formation across different regimes (radiative, advective, marine).
- Evidence anchors: [abstract] SHAP analysis revealed "visibility persistence, solar angle, and thermal gradients emerging as the dominant predictors"; [Section 2.4.2] Feature selection followed iterative refinement based on three "indispensable physical pillars": abundant moisture, effective cooling, atmospheric stability

### Mechanism 3
- Claim: Extended temporal diversity in training data improves zero-shot transfer by exposing the model to a wider range of atmospheric states.
- Mechanism: Training on 8 years (2002-2009) versus 3 years (2015-2017) captures more interannual variability, including diverse ENSO phases and rare atmospheric configurations, enabling the model to learn more robust physical representations.
- Core assumption: Rare but physically important atmospheric states in the training distribution are critical for extrapolation to new geographic and climatic contexts.
- Evidence anchors: [Section 4.2] "Training on a longer historical period (8 years vs. 3 years) substantially improves zero-shot performance (+7.6% AUC at KSFO)"; [Section 4.2] Extended training "encompassing a wider range of interannual variability... allows the model to capture rare but physically important atmospheric states"

## Foundational Learning

- Concept: **Atmospheric boundary layer thermodynamics**
  - Why needed here: Understanding saturation, radiative cooling, and temperature inversions is essential to interpret why the model's features work and to diagnose failure cases.
  - Quick check question: Can you explain why a nocturnal temperature inversion promotes radiation fog formation?

- Concept: **SHAP (SHapley Additive exPlanations) values**
  - Why needed here: The paper relies on SHAP to demonstrate that the model learned transferable physics rather than site-specific correlations; practitioners need to interpret SHAP outputs to validate physical coherence.
  - Quick check question: If SHAP feature importance changes dramatically between training and zero-shot sites, what would that suggest about transferability?

- Concept: **Zero-shot transfer learning protocol**
  - Why needed here: This is the core evaluation methodology; understanding what constitutes "strict zero-shot" (no retraining, no fine-tuning, no threshold recalibration during evaluation) is critical for reproducible validation.
  - Quick check question: In a strict zero-shot protocol, should you fit a new StandardScaler on the target site data?

## Architecture Onboarding

- Component map:
  - Data ingestion: ERA5 reanalysis (0.25° resolution) + METAR surface observations → spatiotemporal alignment
  - Feature engineering: 19 physics-informed features computed from raw variables (lags, trends, solar angle, thermal gradient)
  - Model: XGBoost Classifier (gradient boosting on decision trees)
  - Evaluation: AUC (threshold-independent), AUPRC (for class imbalance), MCC, F1 (threshold-dependent at 0.5)
  - Interpretability: SHAP analysis for feature importance and physical coherence validation

- Critical path:
  1. Compute solar angle using pvlib with timestamp + coordinates (coordinates used for computation only, not passed to model)
  2. Forward-shift target variable by 2 hours (T+2h forecast horizon) to prevent information leakage
  3. Fit StandardScaler on training data only; apply saved scaler to all test/validation sites
  4. Train XGBoost with scale_pos_weight = (negative_samples / positive_samples) to handle class imbalance
  5. Apply trained model to zero-shot sites without any adaptation

- Design tradeoffs:
  - XGBoost vs. deep learning: XGBoost chosen for interpretability and robustness; deep learning may capture finer nonlinear interactions but sacrifices SHAP-compatibility and requires more data
  - 19 features vs. larger set: Concise feature set promotes generalization; risk of omitting important physical processes
  - 2-hour horizon vs. longer: Operationally useful for airport decisions; graceful degradation to T+6h (AUC drops to ~0.86) limits medium-range utility

- Failure signatures:
  - Low AUC at zero-shot site (below ~0.80): Likely regime mismatch (fog type not represented in training) or missing physical features
  - SHAP importance radically different at target site: Model may have learned training-site artifacts rather than transferable physics
  - Very low recall with high precision at fixed threshold: Base rate mismatch; requires site-specific threshold calibration (not a model failure)
  - Performance degradation with shorter training periods: Insufficient temporal diversity; extend historical data if available

- First 3 experiments:
  1. **Reproduce SCEL → SCTE transfer**: Download ERA5 and METAR data for Santiago (2002-2012) and Puerto Montt; implement the 19 features; train on 2002-2009, test on SCEL 2010-2012 and SCTE zero-shot; verify AUC > 0.92
  2. **Ablation study on feature categories**: Remove one category at a time (persistence, cyclical drivers, vertical structure, trends) and measure zero-shot AUC degradation; hypothesis: persistence features contribute most, but all categories needed for robust transfer
  3. **Test threshold calibration at KSFO**: Apply the trained model to KSFO data; plot precision-recall curve; identify threshold achieving recall > 0.5 with precision > 0.3; compare calibrated F1 to fixed 0.5 threshold

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the coordinate-free, physics-informed framework transfer effectively to tropical, polar, or complex terrain environments?
- Basis in paper: [explicit] The authors state in the "Limitations and Future Work" section that expansion to "tropical, polar, and complex terrain environments would further test the boundaries of transferability."
- Why unresolved: The current study validates the model only at mid-latitude airports (ranging from 33°S to 51°N), leaving extreme latitudes and complex topographies untested.
- What evidence would resolve it: Successful zero-shot validation (AUC > 0.9) at airports located in tropical zones, polar regions, or mountainous terrain.

### Open Question 2
- Question: How does FOG-Net performance compare directly against operational Numerical Weather Prediction (NWP) guidance?
- Basis in paper: [explicit] The authors note that "direct quantitative comparison with operational NWP guidance... was not conducted and remains an important direction for future validation."
- Why unresolved: The study only compared the model against persistence, climatology, and simple logistic regression baselines, rather than state-of-the-art operational forecasts.
- What evidence would resolve it: A benchmarking study comparing FOG-Net's AUC and skill scores against high-resolution operational NWP models at the same validation sites.

### Open Question 3
- Question: Does integrating high-resolution topographic data improve performance at complex terrain sites without sacrificing geographic transferability?
- Basis in paper: [explicit] Future work includes "investigating the integration of high-resolution topographic information or local circulation features to enhance performance at complex terrain sites."
- Why unresolved: The current model explicitly excludes geographic coordinates and relies on 0.25° reanalysis data, which may miss microscale boundary layer processes.
- What evidence would resolve it: Improved performance metrics at high-altitude or valley airports following the inclusion of fine-scale elevation or slope features.

## Limitations

- Temporal generalization boundaries: The study demonstrates successful zero-shot transfer across 0-11,650 km distances, but the temporal scope remains narrow (training on 2002-2009, testing on 2010-2012).
- Feature completeness gaps: While the 19-feature set captures known thermodynamic and radiative processes, the model's reliance on ERA5 reanalysis at 0.25° resolution may miss critical local-scale processes.
- Operational threshold calibration: The paper reports strong AUC values (0.923-0.947) but fixed 0.5 thresholds yield recall as low as 0.19 at zero-shot sites.

## Confidence

- High confidence in the coordinate-free feature engineering approach and SHAP-based physical interpretability results. The methodology is well-specified, the training/evaluation protocol is rigorous (strict zero-shot), and the cross-site consistency in SHAP importance rankings provides strong evidence that the model learned universal physical relationships rather than geographic memorization.
- Medium confidence in the generalization claims across fog regimes. While the model succeeds at sites with different fog formation mechanisms (marine stratus at KSFO, radiation fog at EGLL), the sample size of zero-shot sites (3) and the absence of sites with extreme fog regimes (polar ice fog, tropical convective fog) limit the strength of these claims.
- Low confidence in the temporal robustness claims. The study doesn't test performance on data outside the 2002-2012 window, doesn't account for potential climate shifts, and the assertion that 8 years of training captures sufficient atmospheric diversity remains unverified against longer historical records or future projections.

## Next Checks

1. **Climate regime stress test**: Apply the trained model to airport locations with fog regimes absent from the training distribution (e.g., Arctic airports with ice fog, tropical airports with convective fog) to identify fundamental limits of transferability. Measure whether performance degradation correlates with missing physical features or fundamental thermodynamic mismatches.

2. **Temporal validation extension**: Test model performance on data from 2013-2023 to assess whether the learned physical relationships remain stable under climate variability. Compare performance across different ENSO phases and against long-term trends in fog frequency to quantify temporal robustness.

3. **Operational threshold calibration study**: Implement site-specific threshold optimization (e.g., maximizing F1 score or minimizing cost-weighted error) at each zero-shot location. Measure the trade-off between fixed 0.5 thresholds and calibrated thresholds, and assess whether physical interpretability (SHAP rankings) changes under different operational objectives.