---
ver: rpa2
title: Order Independence With Finetuning
arxiv_id: '2503.23483'
source_url: https://arxiv.org/abs/2503.23483
tags:
- prompting
- set-based
- contrastive
- answer
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces fine-tuning with Set-Based Prompting (SBP)
  to improve order invariance in large language models for multiple-choice question
  answering. The core method applies SBP formatting to answer options during training,
  combined with a margin-based contrastive loss that explicitly separates correct
  answers from distractors.
---

# Order Independence With Finetuning

## Quick Facts
- arXiv ID: 2503.23483
- Source URL: https://arxiv.org/abs/2503.23483
- Reference count: 15
- Fine-tuning with Set-Based Prompting (SBP) eliminates order dependence in multiple-choice QA

## Executive Summary
The paper introduces fine-tuning with Set-Based Prompting (SBP) to improve order invariance in large language models for multiple-choice question answering. The core method applies SBP formatting to answer options during training, combined with a margin-based contrastive loss that explicitly separates correct answers from distractors. Experiments on LLaMA-2-7b and LLaMA-2-7b-chat models show that SBP fine-tuning significantly improves robustness to answer-order permutations, with accuracy gains observed across in-distribution (MMLU) and out-of-distribution (CSQA, ARC Challenge) datasets. The contrastive loss outperforms standard cross-entropy, achieving higher SBP accuracy while preserving general language modeling capabilities (minimal perplexity change on WikiText-103). The approach eliminates order dependence without degrading performance, offering a practical path to fairer and more consistent NLP systems. Limitations include task-specific focus and fixed margin hyperparameters.

## Method Summary
The authors propose Set-Based Prompting (SBP) fine-tuning to address order dependence in multiple-choice question answering. The method reformats answer options using SBP templates during training and applies a margin-based contrastive loss that explicitly separates correct answers from distractors. This is combined with standard cross-entropy training. The approach is evaluated on LLaMA-2-7b and LLaMA-2-7b-chat models across multiple datasets including MMLU, CSQA, and ARC Challenge, demonstrating significant improvements in order invariance while maintaining general language modeling capabilities.

## Key Results
- SBP fine-tuning achieves significantly higher accuracy when answer options are randomly permuted
- Margin-based contrastive loss outperforms standard cross-entropy for SBP-formatted tasks
- Minimal impact on general language modeling (WikiText-103 perplexity) while improving task-specific robustness

## Why This Works (Mechanism)
The mechanism leverages contrastive learning principles where the model learns to create larger decision margins between correct answers and distractors. By training with SBP formatting that explicitly represents answer options as a set rather than a sequence, the model develops representations that are invariant to option ordering. The margin-based loss enforces this invariance by penalizing incorrect answers more heavily when they are close to the correct answer in the model's decision space.

## Foundational Learning
- Set-based prompting: Reformats multiple-choice options as unordered sets rather than sequences; needed to explicitly teach models that option order shouldn't matter
- Contrastive loss with margin: Creates explicit separation between correct and incorrect answers; needed to enforce decision boundaries that are robust to permutation
- Order invariance: The property that model predictions don't change when input order changes; needed as the target capability for fair and consistent QA
- Quick check: Verify that model predictions remain stable when answer options are randomly shuffled

## Architecture Onboarding
- Component map: Question + SBP-formatted options -> LLM -> Logits -> Contrastive Loss + Cross-Entropy Loss -> Parameter Updates
- Critical path: Input formatting → Model inference → Loss computation → Backpropagation → Parameter updates
- Design tradeoffs: Contrastive loss provides better order invariance but may require careful margin tuning; cross-entropy alone is simpler but less effective for SBP
- Failure signatures: High variance in predictions across different option orderings; degraded performance on permuted inputs
- First experiments: 1) Measure baseline accuracy with shuffled options, 2) Implement SBP formatting on a subset of training data, 3) Compare contrastive vs cross-entropy loss performance

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to multiple-choice tasks with predefined answer options
- Fixed margin hyperparameter (m=0.3) not systematically explored
- No evaluation on open-ended generation or ranking tasks

## Confidence
High: SBP fine-tuning effectively improves order invariance across multiple datasets
Medium: Contrastive loss consistently outperforms cross-entropy for SBP tasks
Low: Impact on broader language modeling capabilities beyond WikiText-103

## Next Checks
1. Test SBP fine-tuning on open-ended generation tasks to assess generalization beyond multiple-choice formats
2. Conduct a systematic hyperparameter sweep for the margin m across diverse datasets to identify optimal values
3. Evaluate the approach on larger models (e.g., LLaMA-2-13b or above) to determine scalability and whether performance gains persist