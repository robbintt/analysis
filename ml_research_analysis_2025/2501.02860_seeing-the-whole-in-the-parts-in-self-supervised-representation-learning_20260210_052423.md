---
ver: rpa2
title: Seeing the Whole in the Parts in Self-Supervised Representation Learning
arxiv_id: '2501.02860'
source_url: https://arxiv.org/abs/2501.02860
tags:
- representations
- co-byol
- learning
- local
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CO-SSL, a new family of self-supervised learning
  methods that model spatial co-occurrences of visual features by aligning local and
  global image representations. The authors propose two specific methods, CO-BYOL
  and CO-DINO, which extend existing instance discrimination approaches by applying
  the loss function between local representations (before pooling) and global image
  representations.
---

# Seeing the Whole in the Parts in Self-Supervised Representation Learning

## Quick Facts
- **arXiv ID:** 2501.02860
- **Source URL:** https://arxiv.org/abs/2501.02860
- **Authors:** Arthur Aubret; Céline Teulière; Jochen Triesch
- **Reference count:** 40
- **Primary result:** CO-SSL achieves 71.5% Top-1 accuracy on ImageNet-1K with 100 pre-training epochs

## Executive Summary
This paper introduces CO-SSL, a novel family of self-supervised learning methods that model spatial co-occurrences of visual features by aligning local and global image representations. The authors propose CO-BYOL and CO-DINO as specific instantiations that extend existing instance discrimination approaches by applying the loss function between local representations (before pooling) and global image representations. A key contribution is RF-ResNet, a modified ResNet architecture that extracts local patch representations with smaller receptive fields. The approach demonstrates improved robustness to various perturbations and outperforms previous methods on standard benchmarks.

## Method Summary
CO-SSL models spatial co-occurrences by aligning local and global representations in self-supervised learning. The method extends existing instance discrimination approaches (BYOL and DINO) by applying the loss function between local representations (before pooling) and global image representations. The authors introduce RF-ResNet, a modified ResNet architecture that extracts local patch representations with smaller receptive fields. This allows the model to capture fine-grained spatial relationships while maintaining the benefits of global context. The approach consists of two specific methods: CO-BYOL and CO-DINO, which inherit the core principles of their parent methods but incorporate the local-global alignment mechanism.

## Key Results
- CO-SSL achieves 71.5% Top-1 accuracy on ImageNet-1K with 100 pre-training epochs, outperforming previous methods
- The method demonstrates improved robustness to noise corruption, internal corruption, small adversarial attacks, and large training crop sizes
- Analysis reveals that CO-SSL learns highly redundant local representations, which explains its robustness properties

## Why This Works (Mechanism)
CO-SSL works by aligning local and global representations, which allows the model to capture spatial co-occurrences in images. The RF-ResNet architecture with smaller receptive fields enables extraction of fine-grained local features that retain spatial relationships. When these local representations are aligned with global representations through the loss function, the model learns to associate local patterns with their global context. This alignment process creates redundancy in local representations, which contributes to robustness against various perturbations. The method effectively combines the benefits of instance discrimination (through CO-BYOL and CO-DINO) with explicit modeling of spatial relationships.

## Foundational Learning
- **Self-supervised learning**: Training models without labeled data by creating pretext tasks. Needed to leverage large amounts of unlabeled data. Quick check: Can the model learn meaningful representations without explicit supervision?
- **Instance discrimination**: Treating each image as its own class. Needed as the foundation for CO-BYOL and CO-DINO. Quick check: Does the model learn to distinguish between different images?
- **Receptive field**: The region of the input that influences a particular neuron. Needed to control the scale of local features. Quick check: Are the local representations capturing appropriate spatial contexts?
- **Spatial co-occurrences**: Statistical relationships between visual features in different image locations. Needed to model natural image statistics. Quick check: Does the model learn meaningful spatial relationships?
- **Feature pooling**: Aggregating local features into global representations. Needed to create global context for alignment. Quick check: Does pooling preserve important information?

## Architecture Onboarding

**Component Map:** Input Images -> RF-ResNet (Encoder) -> Local Representations & Global Representations -> Loss Function (Local-Global Alignment) -> Updated Model

**Critical Path:** The critical path involves processing images through RF-ResNet to extract both local and global representations, then applying the alignment loss to update model parameters. The RF-ResNet modification is crucial as it determines the quality of local representations.

**Design Tradeoffs:** The main tradeoff is between local detail and global context. Smaller receptive fields capture more local detail but may miss broader context. The alignment loss balances these competing needs but adds computational overhead. The choice of parent method (BYOL vs DINO) also affects performance characteristics.

**Failure Signatures:** Poor local-global alignment may manifest as degraded performance on spatial reasoning tasks or reduced robustness to perturbations. Overly small receptive fields might miss important contextual information, while overly large ones might lose fine-grained spatial details.

**First Experiments:**
1. Verify that RF-ResNet produces meaningful local representations by visualizing activations
2. Test local-global alignment on a simple dataset before scaling to ImageNet
3. Compare performance with different receptive field sizes to find optimal configuration

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation is primarily limited to standard image classification benchmarks, with limited validation on diverse downstream tasks
- The optimal tradeoff between local and global information through receptive field sizing remains unclear
- The theoretical analysis of redundancy in local representations lacks rigorous mathematical grounding
- Computational overhead from maintaining local representations may limit practical applicability

## Confidence

**High confidence:** The ImageNet-1K results strongly support CO-SSL as a powerful principle for unsupervised category learning
**Medium confidence:** The effectiveness of RF-ResNet and the explanation of robustness through redundant local representations
**Medium confidence:** The claim that CO-SSL represents a general principle for unsupervised learning, pending validation on more diverse tasks

## Next Checks
1. Evaluate CO-SSL on non-classification downstream tasks (detection, segmentation, video understanding) to assess generalizability of the spatial co-occurrence principle
2. Conduct systematic ablation studies varying receptive field sizes and pooling strategies to optimize the local-global alignment trade-off
3. Perform computational efficiency analysis comparing training time, memory usage, and inference latency against baseline methods to assess practical deployment considerations