---
ver: rpa2
title: Knowledge-Guided Adaptive Mixture of Experts for Precipitation Prediction
arxiv_id: '2509.11459'
source_url: https://arxiv.org/abs/2509.11459
tags:
- data
- prediction
- precipitation
- climate
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a knowledge-guided Adaptive Mixture of Experts
  (MoE) model for precipitation prediction using multimodal climate data. The approach
  leverages domain expertise to organize 19 climate features into six physically meaningful
  groups, enabling each expert to specialize in a coherent subset.
---

# Knowledge-Guided Adaptive Mixture of Experts for Precipitation Prediction

## Quick Facts
- arXiv ID: 2509.11459
- Source URL: https://arxiv.org/abs/2509.11459
- Reference count: 40
- Primary result: MAE of 0.212, MSE of 0.057, RMSE of 0.238 on Hurricane Ian precipitation prediction

## Executive Summary
This paper introduces a knowledge-guided Adaptive Mixture of Experts (MoE) model for precipitation prediction using multimodal climate data. The approach organizes 19 climate features into six physically meaningful groups, enabling each expert to specialize in a coherent subset. A dynamic router assigns inputs to the most relevant experts, improving both accuracy and interpretability. Evaluated on a curated MoE-Climate dataset capturing Hurricane Ian (2022), the model outperforms MLP, LSTM, Transformer, and conventional MoE baselines.

## Method Summary
The method employs 16 independent MLP experts organized into six knowledge-guided feature groups (Momentum, Temperature, Moisture, Mass, Cloud, Radiation) based on domain expertise. Training occurs in two phases: first, selective training of expert pairs with diversity regularization prevents mode collapse; second, the router is trained to dynamically allocate inputs while experts remain frozen. The final prediction is a weighted sum of expert outputs, with the router learning context-dependent weighting through gradient-based optimization.

## Key Results
- Achieves MAE of 0.212, MSE of 0.057, and RMSE of 0.238 on Hurricane Ian precipitation prediction
- Outperforms MLP, LSTM, Transformer, and conventional MoE baselines
- Ablation studies show specialized experts and router learning are critical for performance

## Why This Works (Mechanism)

### Mechanism 1: Knowledge-Guided Feature Partitioning
By organizing 19 heterogeneous climate variables into 6 physically meaningful groups, the model constrains the hypothesis space and prevents learning spurious correlations between unrelated variables. This allows experts to focus on coherent representations within each physical domain.

### Mechanism 2: Selective Training with Diversity Regularization
Alternating optimization of expert pairs combined with a diversity penalty prevents mode collapse where all experts learn identical functions. The diversity loss explicitly pushes weight matrices apart, forcing experts to identify distinct sub-patterns in the climate data.

### Mechanism 3: Differentiable Routing for Context-Dependent Prediction
A weighted sum of expert outputs, trained while experts are frozen, allows the router to learn context-dependent "regime switching." This enables the model to prioritize different physical experts (e.g., moisture vs. radiation) based on current input conditions.

## Foundational Learning

- **Mixture of Experts (MoE)**: Why needed: Standard models can average out rare but critical signals in heterogeneous data. Quick check: Can you explain why a "soft" router might be safer than a "hard" router for scientific data?

- **Heterogeneous Multimodal Fusion**: Why needed: The dataset combines radar, satellite, and surface data with different units and resolutions. Quick check: Why would min-max scaling be critical before feeding disparate variables into a unified MLP?

- **Overfitting vs. Generalization in Extreme Weather**: Why needed: The model is trained on Hurricane Ian data, risking memorization of specific tracks rather than general physics. Quick check: If trained only on Hurricane Ian data, would you trust it to predict a calm day?

## Architecture Onboarding

- **Component map**: Input Layer (19 features → 6 groups) → Expert Layer (16 MLPs) → Router → Aggregator (weighted sum)

- **Critical path**: 1. Data prep: map 19 channels to 6 physical groups, 2. Phase 1: run selective training with diversity loss, 3. Phase 2: train router with frozen experts, 4. Inference: forward pass through all experts with router weights

- **Design tradeoffs**: Interpretability vs. performance (16-expert system allows analysis of physical drivers but is harder to debug); Training stability (selective training is computationally cheaper but may take longer to converge)

- **Failure signatures**: Mode collapse (all experts converge to similar weights); Router dominance (assigns ≈1.0 weight to single expert); NaN loss (check normalization in diversity loss)

- **First 3 experiments**: 1. Baseline verification: replicate MLP and standard MoE results, 2. Ablation on groups: retrain with random feature groups vs. knowledge-guided, 3. Router visualization: plot router weights over time during hurricane landfall

## Open Questions the Paper Calls Out

1. Can attention-based gating or knowledge distillation methods successfully handle missing modalities or incomplete features? [Explicit in Section 10]

2. Does incorporating physical constraints into the MoE architecture significantly improve generalizability to climate zones outside Hurricane Ian? [Explicit in Section 10]

3. How can uncertainty quantification be effectively integrated into the Adaptive MoE framework to better support high-stakes environmental decision-making? [Explicit in Section 10]

## Limitations
- Performance evaluation limited to single extreme weather event (Hurricane Ian)
- Missing specification of exact MLP architecture details and training hyperparameters
- Dataset release claimed but no public download link provided
- Input sequence length for 6-hour window not specified

## Confidence
- Knowledge-guided feature grouping improves coherence: Medium
- Selective training with diversity penalty prevents mode collapse: Medium
- Weighted router output captures optimal expert combinations: Medium
- Outperformance vs. MLP/LSTM/Transformer baselines: Medium
- Interactive visualization tool adds interpretability value: High

## Next Checks
1. Request or reconstruct exact MLP architecture and training hyperparameters from authors
2. Implement data preprocessing pipeline to create 100x100 grid from NOAA data and verify 6-group feature mapping
3. Conduct ablation: retrain model with random feature groups to quantify value of knowledge-guided organization