---
ver: rpa2
title: 'PretrainRL: Alleviating Factuality Hallucination of Large Language Models
  at the Beginning'
arxiv_id: '2602.01875'
source_url: https://arxiv.org/abs/2602.01875
tags:
- knowledge
- arxiv
- pretrainrl
- factual
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies imbalanced data distribution in pretraining
  corpora as a root cause of factual hallucinations in LLMs, where high-frequency
  (head) knowledge interferes with learning low-frequency (tail) knowledge. To address
  this, the authors propose PretrainRL, a novel framework that integrates reinforcement
  learning into the pretraining phase.
---

# PretrainRL: Alleviating Factuality Hallucination of Large Language Models at the Beginning

## Quick Facts
- **arXiv ID**: 2602.01875
- **Source URL**: https://arxiv.org/abs/2602.01875
- **Reference count**: 20
- **Primary result**: PretrainRL achieves 63.44% accuracy on POPQA with Qwen3-14B, compared to 19.41% for base model

## Executive Summary
PretrainRL addresses factual hallucinations in LLMs by integrating reinforcement learning into the pretraining phase. The framework identifies imbalanced data distribution in pretraining corpora as a root cause, where high-frequency knowledge interferes with learning low-frequency facts. The core principle is "debiasing then learning": first lowering the probability of high-probability falsehoods using negative sampling, then increasing the probability of low-probability truths. Experiments on three public benchmarks show PretrainRL significantly outperforms state-of-the-art methods and closed-source LLMs, achieving up to 63.44% accuracy on POPQA with Qwen3-14B.

## Method Summary
PretrainRL combines Direct Preference Optimization (DPO) with next-token prediction (NTP) loss to reshape probability distributions during pretraining. The method employs negative sampling via beam search to identify high-probability falsehoods, then uses DPO to down-weight these while the NTP loss up-weights true but low-probability facts. The training uses template-based formatting "{Question}{pos}<pad>{Question}{neg}" with λ-weighted combined loss. Key hyperparameters include LR=3e-5, weight decay=0.1, and 1 epoch training with dataset repetition (POPQA 3×, EntityQuestions 2×).

## Key Results
- Achieves 63.44% accuracy on POPQA with Qwen3-14B (base: 19.41%)
- Maintains strong performance on downstream tasks, showing effective knowledge consolidation
- Scales across model sizes (4B, 8B, 14B parameters) with consistent improvements
- Outperforms state-of-the-art methods and closed-source LLMs on factual accuracy benchmarks

## Why This Works (Mechanism)
The framework works by directly addressing the root cause of hallucinations: imbalanced pretraining data distribution. By first identifying and down-weighting high-probability falsehoods through negative sampling, then up-weighting true but low-probability facts, PretrainRL reshapes the model's probability distribution to favor factual accuracy. The DPO component handles preference learning while NTP loss provides regularization, preventing the artificial inflation of probability metrics seen in DPO-only approaches.

## Foundational Learning
- **Direct Preference Optimization (DPO)**: A reinforcement learning method that optimizes for preference-based objectives. Needed to learn from pairwise comparisons of correct vs. incorrect answers. Quick check: Verify DPO implementation correctly handles pairwise ranking.
- **Next-Token Prediction (NTP) Loss**: Standard language modeling objective predicting the next token. Needed to maintain general language understanding while focusing on factual accuracy. Quick check: Monitor NTP loss stability during training.
- **Beam Search Negative Sampling**: Algorithm for generating multiple candidate responses. Needed to identify high-probability falsehoods for the debiasing phase. Quick check: Verify beam search generates diverse, relevant negative examples.
- **Knowledge Consolidation**: Process of integrating factual knowledge into model parameters. Needed to improve factual accuracy without catastrophic forgetting. Quick check: Test model performance on both factual and general downstream tasks.

## Architecture Onboarding

**Component Map**: Base Model -> Negative Sampling -> DPO Loss + NTP Loss -> Trained Model

**Critical Path**: The negative sampling quality directly impacts DPO effectiveness, which combined with NTP loss drives the probability distribution reshaping that improves factual accuracy.

**Design Tradeoffs**: Using base models instead of instruct-tuned versions provides more "learnable room" but requires additional alignment steps. The 1-epoch training with dataset repetition balances knowledge consolidation with computational efficiency.

**Failure Signatures**: 
- High probability metrics with low accuracy indicates missing NTP regularization
- Minimal improvement on instruct models suggests training stage saturation
- Poor negative sampling quality leads to ineffective debiasing

**First Experiments**:
1. Implement negative sampling with beam search and validate output diversity
2. Train with combined DPO+NTP loss on small POPQA subset to verify convergence
3. Compare base vs. instruct model performance to confirm learnable room hypothesis

## Open Questions the Paper Calls Out

**Open Question 1**: How does the negative sampling strategy perform on datasets lacking distinct question categories, and does the RLVR alternative maintain efficiency? The authors suggest RLVR as a fallback but don't experimentally validate it.

**Open Question 2**: Does the "debiasing" mechanism inadvertently suppress valid but low-ranked truths during down-weighting of high-probability candidates? The evaluation focuses on top-ranked accuracy, overlooking semantic validity of rejected samples.

**Open Question 3**: Is the lower performance on instruction-tuned models due to intrinsic training stage properties or saturation from prior fine-tuning? The paper suggests rigidity but doesn't isolate the exact cause.

## Limitations
- Negative sampling strategy presupposes meaningful category distinctions in datasets
- Potential for false negatives when valid but low-ranked answers are suppressed
- Requires training on base models rather than post-trained models for optimal effectiveness

## Confidence

**High Confidence**: Core methodology of combining DPO with NTP loss during pretraining is well-specified and theoretically sound. Experimental setup with clear benchmarks and evaluation metrics is fully defined.

**Medium Confidence**: Training procedure details are largely complete but missing λ hyperparameter for loss weighting. Negative sampling strategy is described but lacks precise beam search parameters.

**Low Confidence**: Without exact λ value and precise beam search configuration, probability distribution reshaping may differ from original implementation, potentially affecting claimed accuracy improvements.

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Conduct small-scale experiments varying λ values (0.01, 0.1, 1.0) on POPQA subset to determine optimal weighting.
2. **Negative Sampling Quality Verification**: Implement beam search with multiple beam sizes (10, 20, 30) and compare output diversity using perplexity metrics.
3. **Base vs. Instruct Model Validation**: Train both base and instruct versions on same dataset to empirically verify "learnable room" hypothesis.