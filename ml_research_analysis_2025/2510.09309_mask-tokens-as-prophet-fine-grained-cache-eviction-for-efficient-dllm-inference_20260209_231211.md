---
ver: rpa2
title: 'Mask Tokens as Prophet: Fine-Grained Cache Eviction for Efficient dLLM Inference'
arxiv_id: '2510.09309'
source_url: https://arxiv.org/abs/2510.09309
tags:
- tokens
- cache
- arxiv
- budget
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient cache management
  for diffusion large language models (dLLMs), which require substantial memory due
  to bidirectional attention over the entire sequence during iterative denoising.
  Existing cache eviction strategies designed for autoregressive models fail to leverage
  the unique characteristics of dLLMs, particularly the role of masked tokens in the
  denoising process.
---

# Mask Tokens as Prophet: Fine-Grained Cache Eviction for Efficient dLLM Inference

## Quick Facts
- arXiv ID: 2510.09309
- Source URL: https://arxiv.org/abs/2510.09309
- Authors: Jianuo Huang, Yaojie Zhang, Yicun Yang, Benhao Huang, Biqing Qi, Dongrui Liu, Linfeng Zhang
- Reference count: 21
- Primary result: 31× inference speedup with 65% memory reduction while retaining 94% performance using only 256 KV pairs

## Executive Summary
This paper introduces MaskKV, a training-free cache eviction framework specifically designed for diffusion large language models (dLLMs). Unlike autoregressive models, dLLMs require bidirectional attention across the entire sequence during iterative denoising, making traditional KV cache strategies ineffective. MaskKV exploits the unique characteristics of dLLMs by using mask-token attention patterns to identify critical prompt tokens and implement hierarchical cache allocation across layers and attention heads.

The framework achieves up to 31× acceleration in inference speed and 65% reduction in peak memory usage while maintaining 94% of full-cache performance on LongBench tasks. By leveraging the observation that mask tokens consistently attend to a small set of critical prompt tokens across all denoising steps, MaskKV provides a universal importance ranking that guides fine-grained cache eviction decisions without requiring additional training or significant computational overhead.

## Method Summary
MaskKV is a training-free cache eviction framework that operates through two main components: (1) Mask-Voting, which uses mask-token attention scores to rank prompt token importance, and (2) hierarchical budget allocation that distributes cache resources across layers based on boundary-aware importance and across heads based on prompt preference. The framework employs a two-stage approach where token-level importance is first established using mask-query attention scores, then budgets are allocated using offline layer importance calibration and online head preference adaptation. Implementation optimizations include prompt-state exclusion and mask-only projection to reduce memory overhead during inference.

## Key Results
- Achieves 94% of full-cache performance while compressing KV cache to only 256 pairs (less than 5% of tokens)
- Delivers up to 31× acceleration in inference speed and 65% reduction in peak memory usage at 32K token context length
- Outperforms baselines (SnapKV, PyramidKV, SqueezeAttention, AdaKV) by 1.43% to 2.66% on LongBench with only 32 KV pairs
- Maintains effectiveness across diverse tasks including code generation, math reasoning, and general knowledge tasks

## Why This Works (Mechanism)

### Mechanism 1: Mask-Query Guided Token Importance Ranking
The framework leverages attention scores from mask tokens to provide a stable, task-aligned signal for identifying critical prompt tokens. At inference start, it computes the attention matrix from all mask queries to all keys, aggregates per-key attention scores across mask positions to produce an importance vector, and selects top-k tokens per head. This works because mask tokens perform sparse, long-range retrieval that is consistent across denoising steps and reflects task-relevant information needs.

### Mechanism 2: Layer-wise Budget Allocation via Boundary-Aware Importance
Boundary layers (first and last) require more cache budget than intermediate layers due to higher representational transformation. The framework computes layer importance as 1 minus the average cosine similarity between layer inputs and outputs, then partitions total budget into a uniform base plus importance-driven allocation, assigning more resources to boundary layer groups. This exploits the bimodal importance profile and cross-sample consistency of layer contributions.

### Mechanism 3: Head-wise Budget Redistribution via Prompt-Preference
Attention heads exhibit functional heterogeneity—information heads rely more on prompts while structure heads focus on masked regions. The framework computes prompt preference as the ratio of mask-to-prompt attention to total attention, then distributes per-layer budgets across heads using hybrid allocation that combines uniform base allocation with preference-weighted distribution. This concentrates resources on heads that are more critical for generation quality.

## Foundational Learning

- **Bidirectional Attention in Diffusion LLMs**: dLLMs allow each position to attend to all positions including mask tokens, preventing direct reuse of past KV states and demanding new caching strategies. Quick check: Why can't standard KV cache from autoregressive models be applied directly to dLLMs?

- **Iterative Denoicing Process**: dLLMs refine entire sequences over T discrete steps, requiring cache refresh policies that balance recency with computation savings. Quick check: How does multi-step refinement in dLLMs differ from single-pass autoregressive generation?

- **Attention Pattern Analysis**: MaskKV relies on distinguishing mask-query attention (sparse, long-range) from prompt-query attention (locality-biased) to identify important tokens. Quick check: What behavioral differences separate mask-query attention from prompt-query attention in dLLMs?

## Architecture Onboarding

- **Component map**: Input Prompt + Mask Tokens → Mask-Voting Engine → Token Importance Vector I → Offline Layer Budget Allocator → Per-layer budgets k_l → Online Head Budget Redistributor → Per-head budgets k_{l,h} → KV Cache Evictor → Top-k token selection per head

- **Critical path**: Mask-Voting (initial step) → Universal importance ranking → Hierarchical budget application (offline layers, online heads) → Per-head eviction. Offline layer budgeting determines the allocation skeleton; online head redistribution adapts within that skeleton per prompt.

- **Design tradeoffs**:
  - **α (head base rate)**: Low α → aggressive preference-based allocation; may starve structure heads. High α → near-uniform; dilutes preference signal. Paper finds α=0.1 optimal.
  - **β (layer base rate)**: High β → more uniform layer allocation; reduces boundary emphasis. Paper finds β=0.4 optimal.
  - **Online vs. Offline layer budgeting**: Online requires full-cache forward pass before eviction (no memory reduction). Offline assumes cross-sample consistency; reduces peak memory from 68GB to 23GB.

- **Failure signatures**:
  - Performance cliff at very low budgets (B<32): indicates importance scoring breakdown
  - High variance across prompt types: suggests preference metric instability
  - Peak memory not decreasing: verify offline mode is enabled; online mode cannot reduce peak memory

- **First 3 experiments**:
  1. Reproduce attention visualization (Fig 1): Run LLaDA on sample prompt; extract mask-query attention at steps 0/35/70/105; verify sparse long-range pattern and cross-step consistency
  2. Ablate voting strategy (Table 7): Compare Mask-Voting vs. Prompt-Voting vs. All-Voting on GSM8K (budget=128). Confirm mask-query advantage (68.08% vs. 60.35% vs. 66.64%)
  3. Budget sweep on LongBench (Fig 3): Test B∈{32, 64, 128, 256, 512}; plot performance curve; identify acceptable accuracy-efficiency operating point

## Open Questions the Paper Calls Out

- **Generalization to different model scales**: The effectiveness of methods and observed attention behavior patterns have yet to be validated on both larger-scale and smaller lightweight models. Only LLaDA-8B and Dream-7B were tested due to limited availability of open-source dLLMs.

- **Extension to multimodal dLLMs**: The evaluation is confined to text generation benchmarks, and extending the analysis to multimodal reasoning remains an important direction for future research. Multimodal attention patterns may differ substantially from text-only patterns.

- **Position-aware voting strategies**: Later positions yield more accurate votes for identifying critical tokens, suggesting position-aware voting may further improve eviction effectiveness. The current Mask-Voting aggregates all mask positions equally.

- **Offline budget allocation stability**: The cross-sample consistency assumption for offline layer budget allocation needs validation across heterogeneous task distributions and potential distributional shifts.

## Limitations

- Offline calibration assumption for layer importance presents scalability challenges and may limit applicability to new model architectures or domains with substantially different characteristics

- Hierarchical budget allocation introduces complex hyperparameter dependencies (α, β) that may require task-specific tuning and have limited sensitivity analysis across different model scales

- Prompt-state exclusion optimization may introduce subtle accuracy degradations not captured in reported metrics, making it difficult to assess the contribution of core importance ranking versus implementation optimizations

## Confidence

- **High Confidence**: Mask tokens exhibit sparse, long-range attention patterns; bimodal layer importance profile is empirically validated
- **Medium Confidence**: Mask-query attention as universal importance signal across diverse tasks; hierarchical budget allocation shows consistent improvements
- **Low Confidence**: Head-level prompt preference analysis lacks rigorous statistical validation and comparison with alternative head importance metrics

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate MaskKV on dLLM models and tasks outside LongBench domain to validate whether offline layer budget calibration generalizes or requires task-specific retraining

2. **Hyperparameter Sensitivity Analysis**: Systematically vary α (0.05-0.3) and β (0.2-0.6) across multiple model scales (3B, 13B, 30B) and prompt types to quantify robustness of hybrid allocation strategy

3. **Ablation of Implementation Optimizations**: Compare Mask-Voting performance with and without prompt-state exclusion and mask-only projection to isolate contribution of core importance ranking mechanism versus implementation-level optimizations