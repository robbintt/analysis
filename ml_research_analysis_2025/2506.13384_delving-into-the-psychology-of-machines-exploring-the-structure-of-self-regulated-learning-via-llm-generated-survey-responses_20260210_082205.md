---
ver: rpa2
title: 'Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated
  Learning via LLM-Generated Survey Responses'
arxiv_id: '2506.13384'
source_url: https://arxiv.org/abs/2506.13384
tags:
- data
- llms
- responses
- factor
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study examined whether large language models (LLMs) can simulate
  human-like responses to the Motivated Strategies for Learning Questionnaire (MSLQ),
  a widely used instrument for assessing self-regulated learning (SRL). Responses
  from 1,000 simulated students were generated using five LLMs: GPT-4o, Claude 3.7
  Sonnet, Gemini 2 Flash, LLaMA 3.1-8B, and Mistral Large.'
---

# Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses

## Quick Facts
- arXiv ID: 2506.13384
- Source URL: https://arxiv.org/abs/2506.13384
- Reference count: 30
- Primary result: LLM-generated survey responses show limited variability and systematic predictability, with Gemini 2 Flash producing the most psychometrically valid synthetic data

## Executive Summary
This study examines whether large language models can simulate human-like responses to the Motivated Strategies for Learning Questionnaire (MSLQ), a widely used instrument for assessing self-regulated learning. Using five different LLMs, the researchers generated responses from 1,000 simulated students each and analyzed the psychometric properties of the resulting data. While none of the models perfectly replicated the original MSLQ factor structure, Gemini 2 Flash demonstrated the highest variability and produced psychological networks and factor structures most closely aligned with established SRL theory. The study reveals both the potential and limitations of using LLMs for psychological survey data in educational research.

## Method Summary
The study employed five LLMs (GPT-4o, Claude 3.7 Sonnet, Gemini 2 Flash, LLaMA 3.1-8B, and Mistral Large) to generate synthetic survey responses to a 44-item version of the MSLQ. Each model produced responses for 1,000 simulated students using a prompt that included demographic fields and required CSV-formatted output. Data was collected in batches of 10 students per API call to avoid token truncation. The resulting datasets underwent validation for format compliance and missing data patterns, followed by network analysis using Gaussian graphical models and factor analysis through both confirmatory and exploratory approaches. The study compared model-generated data against established psychometric properties of the MSLQ.

## Key Results
- Gemini 2 Flash showed the highest response variability (SDs ranging 1.59-1.85) and produced factor structures most aligned with theoretical expectations
- All models exhibited systematic predictability (R² values 0.48-0.67), suggesting overfitting to training patterns rather than simulating human-like noise
- Missing data patterns revealed context window limitations, with most models showing staircase patterns in later items, though LLaMA showed unstructured missingness
- None of the models fully replicated the original MSLQ factor structure, though Gemini and Claude demonstrated the most coherent patterns

## Why This Works (Mechanism)

### Mechanism 1: Training Data Statistical Pattern Encoding
- Claim: LLMs produce plausible psychological survey responses by reproducing statistical regularities encoded from training data containing psychological instruments and their typical response patterns.
- Mechanism: Models trained on academic literature, surveys, and human responses encode both the linguistic structure of questionnaire items and aggregate response distributions. When prompted to simulate respondents, they generate outputs that satisfy linguistic coherence with the items while approximating learned response patterns.
- Core assumption: The training corpora contain sufficient representations of psychological survey instruments and their associated response distributions for the model to learn both item semantics and typical human response patterns.
- Evidence anchors:
  - LLMs are notoriously acquiescent and tend to agree with most statements, which undermines the realism and variability expected in human data.
  - Models may reproduce familiar answers rather than generate novel, persona-driven responses.
  - Related work by Bisbee et al. (2024) found synthetic responses showed less variation than real survey data and were sensitive to prompt wording changes.
- Break condition: When response variability falls below human baselines (e.g., SD < 0.7 on 7-point scales) or when predictability (R²) exceeds realistic human thresholds (>0.90 across all constructs), the mechanism is overfitting to aggregate patterns rather than simulating individual variation.

### Mechanism 2: Temperature-Controlled Sampling Variability
- Claim: Response diversity in LLM-generated survey data is primarily controlled through temperature settings and sampling parameters, which determine how much the model deviates from the highest-probability outputs.
- Mechanism: Higher temperature values (closer to 1.0) increase stochasticity in token selection, allowing greater deviation from the modal response. This creates between-respondent variability but may sacrifice internal consistency within individual profiles.
- Core assumption: Increasing temperature produces variability that approximates human individual differences rather than merely introducing noise.
- Evidence anchors:
  - Gemini shows notable response variability... whereas GPT, LLaMa, Mistral, and Claude display more stable response patterns across items.
  - Table 3 shows Gemini SDs ranging 1.59-1.85 while GPT SDs range 0.52-0.66 on the same dimensions.
  - De Winter et al. (2024) used temperature=1 for diversity and temperature=0 for consistency in their Big Five simulation study.
- Break condition: When temperature-induced variability produces internally inconsistent profiles (e.g., extreme scores on theoretically opposed constructs simultaneously) or when factor structures become uninterpretable due to noise, the mechanism has exceeded useful bounds.

### Mechanism 3: Context Window Degradation Under Sequential Prompting
- Claim: As questionnaire length increases within a single generation session, model coherence degrades systematically, producing truncated responses and formatting failures.
- Mechanism: LLMs have finite effective context windows. When generating multi-item responses sequentially, token accumulation eventually exceeds the model's ability to maintain consistent persona and output formatting, leading to "staircase" missing data patterns.
- Core assumption: Missing data is mechanistically linked to token position rather than item content or any substantive process.
- Evidence anchors:
  - Missing values, if they occur for one item, also occur in variables recorded at later time points... a distinct staircase pattern is evident.
  - We opted to use the 44-item version... instead of the full 81-item questionnaire... [which] would have significantly increased the likelihood of token truncation.
  - No direct corpus evidence on context window mechanisms in survey generation specifically.
- Break condition: When missing data patterns become unstructured rather than staircase-patterned (as with LLaMa in the study), or when early items show similar missingness rates to late items, the context window mechanism is not the primary driver.

## Foundational Learning

- Concept: **Psychometric Factor Structure (CFA/EFA)**
  - Why needed here: The paper's core evaluation compares LLM-generated data against the MSLQ's theoretical 4-5 factor structure. Understanding how latent constructs manifest through observed item responses is essential for interpreting why some models "pass" and others "fail."
  - Quick check question: If an LLM produces items Q26, Q27, Q37, Q38 (reverse-coded) with loadings in the same direction as non-reversed items, what does this indicate about the model's understanding?

- Concept: **Self-Regulated Learning (SRL) Construct Relationships**
  - Why needed here: The network analysis evaluates whether LLMs reproduce theoretically expected relationships—specifically, that Test Anxiety should negatively correlate with Self-Efficacy and Self-Regulation. Without understanding these theoretical priors, you cannot evaluate whether generated data is "plausible."
  - Quick check question: In the Gemini network, why is the positive edge (r=.079) between the TA/SE factor and the CSU/IV/SR factor actually consistent with theory?

- Concept: **LLM Sampling Parameters (Temperature, Batch Size)**
  - Why needed here: The study's pipeline relied on batch sizes of 10 students per API call and temperature settings to balance variability against coherence. Understanding these knobs is essential for reproducing or extending the methodology.
  - Quick check question: Why would increasing batch size from 10 to 100 students per request cause more formatting errors?

## Architecture Onboarding

- Component map:
Configuration Layer -> Generation Layer -> Validation Layer -> Analysis Layer
  - Model selection (5 APIs: OpenAI, Anthropic, Google, Meta, Mistral)
  - Prompt template (demographic fields + 44 items + CSV format constraints)
  - Parameter settings (temperature, max_tokens, batch_size)
      ↓
  - Chunked request loop (10 students/batch)
  - API call with error handling (retry on HTTP 429)
  - Response capture and parsing
      ↓
  - Format validation (expected columns, numeric ratings 1-7)
  - Illegal value detection (scores >7)
  - Missing data pattern logging
      ↓
  - Descriptive statistics (means, SDs, outliers)
  - Network analysis (GGM with EBICglasso)
  - Factor analysis (CFA → EFA → model comparison)

- Critical path:
  1. Prompt engineering for CSV output format (most common failure point—models add markdown headers or explanatory text)
  2. Batch size tuning to stay within token limits (batch=10 was reliable; batch=20-100 triggered truncation)
  3. Complete-case filtering for factor analysis (missing data is MCAR but requires filtering)

- Design tradeoffs:
  - **Batch size vs. API calls**: Smaller batches (10) are more reliable but require more API calls (100 batches for 1,000 students). Larger batches reduce calls but increase truncation risk.
  - **Temperature vs. coherence**: Higher temperature produces more human-like variability but risks internally inconsistent profiles. The paper found Gemini's default produced the best balance without explicit temperature tuning.
  - **Questionnaire length vs. data completeness**: 44-item version had 14-35% missingness across models; 81-item would likely be worse.

- Failure signatures:
  - **Staircase missing data**: If Q1-Q30 are complete but Q31-Q44 show progressive missingness, the context window was exceeded mid-batch.
  - **Illegal values**: LLaMa produced scores of 8-9; Mistral produced a 71. These indicate tokenization or parsing failures.
  - **Formatting contamination**: Models wrapping CSV in markdown blocks (```csv ... ```) or adding conversational filler before the data.
  - **R² values >0.90**: Indicates overfitting to training patterns rather than simulating realistic human noise.

- First 3 experiments:
  1. **Single-model baseline with controlled temperature**: Generate 200 responses from one model (recommend Gemini based on results) at temperatures [0.3, 0.7, 1.0]. Measure SDs and R² across conditions to find optimal diversity-coherence tradeoff.
  2. **Persona-conditioned generation**: Add explicit demographic constraints to the prompt (e.g., "Generate responses for a 16-year-old student with high test anxiety and low self-efficacy"). Compare factor structures against unconstrained generation to assess whether persona grounding improves validity.
  3. **Cross-questionnaire validation**: Apply the same pipeline to a shorter validated instrument (e.g., Big Five Inventory-10). If the same model produces valid factor structures on BFI-10 but not MSLQ, the issue may be questionnaire length rather than model capability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs maintain persona consistency across multi-item questionnaires measuring constructs requiring internal coherence?
- Basis in paper: "Maintaining a consistent persona across all 44 MSLQ items... requires models to simulate stable beliefs and behaviors, which may exceed the working memory of many LLMs."
- Why unresolved: The study avoided persona-based generation, assuming it would exacerbate existing limitations; the authors call this a "logical next step" but did not test it.
- What evidence would resolve it: Generate persona-conditioned MSLQ responses and evaluate internal consistency (Cronbach's alpha) and test-retest stability within personas.

### Open Question 2
- Question: How sensitive are LLM-generated survey response distributions to minor prompt wording variations?
- Basis in paper: "We did not investigate prompt sensitivity... Even minor changes in wording or parameters can lead to notable differences in responses."
- Why unresolved: The study used a fixed prompt design and did not systematically vary wording, structure, or instruction framing.
- What evidence would resolve it: Conduct a controlled experiment with systematically varied prompts (e.g., instruction phrasing, context framing) and compare resulting response distributions and factor structures.

### Open Question 3
- Question: Can LLMs reproduce known demographic subgroup differences in psychological constructs when conditioned on detailed personas?
- Basis in paper: The authors note the lack of human comparison data limits evaluation of persona-based generation, and cite Petrov et al. (2024) showing LLMs "struggle to reliably simulate individual-level human behavior, especially when based on detailed demographic personas."
- Why unresolved: No human benchmark data was collected; the study only assessed whether models produce plausible aggregate structures, not subgroup-specific patterns.
- What evidence would resolve it: Compare LLM-generated persona responses to human survey data with known demographic differences on the same instrument using measurement invariance testing.

## Limitations

- Limited transparency around sampling parameters, particularly temperature settings, making exact replication difficult
- Substantial missing data (14-35%) across all models, with patterns suggesting context window limitations rather than substantive missingness mechanisms
- Factor structure validation conducted on cleaned datasets that excluded 9-51 students per model due to outliers or missing data, potentially biasing results toward more "typical" responses

## Confidence

- **High confidence**: Gemini 2 Flash produces the most psychometrically valid synthetic data (variability SDs 1.59-1.85, R² predictability 0.49-0.67, factor structure alignment closest to theory)
- **Medium confidence**: All models exhibit response predictability (R² 0.48-0.67), suggesting systematic overfitting to training patterns rather than simulating human-like noise
- **Low confidence**: Claims about LLaMA 3.1-8B's non-staircase missing data patterns, as this contradicts the stated context window mechanism and requires further investigation

## Next Checks

1. **Temperature sensitivity validation**: Generate 200 responses from Gemini 2 Flash at temperatures [0.3, 0.7, 1.0] and measure the tradeoff between response variability (SDs) and internal consistency (R²). This will identify the optimal balance point for producing realistic human-like variation.

2. **Cross-questionnaire validation**: Apply the same generation pipeline to a shorter validated instrument (e.g., Big Five Inventory-10). If Gemini produces valid factor structures on BFI-10 but not MSLQ, this would indicate that questionnaire length—not model capability—drives the validity differences observed.

3. **Persona-conditioned generation**: Add explicit demographic constraints to the prompt (e.g., "Generate responses for a 16-year-old student with high test anxiety and low self-efficacy"). Compare resulting factor structures against unconstrained generation to assess whether persona grounding improves psychometric validity and reduces overfitting to aggregate patterns.