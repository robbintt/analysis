---
ver: rpa2
title: 'Understanding New-Knowledge-Induced Factual Hallucinations in LLMs: Analysis,
  Solution, and Interpretation'
arxiv_id: '2511.02626'
source_url: https://arxiv.org/abs/2511.02626
tags:
- knowledge
- tasks
- figure
- attention
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates factual hallucinations in LLMs caused by
  learning new knowledge during fine-tuning. The authors design a controlled synthetic
  dataset (Biography-Reasoning) to systematically analyze how different types and
  proportions of unknown knowledge affect hallucination across QA and reasoning tasks.
---

# Understanding New-Knowledge-Induced Factual Hallucinations in LLMs: Analysis, Solution, and Interpretation

## Quick Facts
- **arXiv ID**: 2511.02626
- **Source URL**: https://arxiv.org/abs/2511.02626
- **Reference count**: 40
- **Primary result**: Learning new knowledge during fine-tuning causes factual hallucinations driven by unfamiliarity of entire knowledge types, not just proportion of new data.

## Executive Summary
This paper investigates why fine-tuning large language models on new knowledge causes factual hallucinations. Through controlled experiments on synthetic biography data, the authors discover that hallucinations are driven more by complete unfamiliarity of specific knowledge types than by the overall proportion of new information. They find that learning new knowledge reduces attention to key entities in mid-to-late transformer layers, causing the model to rely on surrounding context and hallucinate. To address this, they propose KnownPatch, a method that injects known knowledge late in training to restore attention balance and significantly reduce hallucinations. The study also reveals that hallucination propagation between tasks is primarily driven by lexical similarity rather than semantic similarity.

## Method Summary
The authors design a synthetic Biography-Reasoning dataset with 3,000 individuals having four attributes (birth year, death year, major, university). They systematically replace knowledge types with unknown information during fine-tuning and measure hallucination effects. The study uses a two-stage training approach: CPT with known knowledge followed by SFT with controlled proportions of unknown knowledge. They analyze attention patterns in mid-to-late transformer layers and propose KnownPatch, which injects known samples late in training to restore attention to key entities. The method is validated across multiple reasoning tasks and compared against shuffled training baselines.

## Key Results
- High unfamiliarity of a specific knowledge type (100% unknown) causes more severe hallucinations than moderate proportions of new knowledge
- Hallucinations propagate between tasks primarily through lexical similarity rather than semantic similarity
- Learning new knowledge reduces attention to key entities in layers 12-24 of 28-layer models, increasing reliance on context
- KnownPatch injection of 5-20% known samples at late training stages significantly reduces hallucinations while maintaining learning capacity

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Type Unfamiliarity
When a model fine-tunes on a completely unfamiliar knowledge type, the internal representations destabilize, causing performance degradation that propagates to unrelated tasks. This "unfamiliarity shock" is more damaging than gradual exposure to new knowledge.

### Mechanism 2: Attention Displacement to Context
New knowledge training reduces attention to key entities (subjects) in mid-to-late transformer layers, forcing over-reliance on surrounding context. This misalignment causes the model to extract incorrect features and hallucinate.

### Mechanism 3: Lexical Propagation of Errors
Hallucinations spread to new tasks through lexical (token overlap) similarity rather than semantic relatedness. The disrupted attention pattern becomes associated with specific token patterns, triggering faulty attention regardless of meaning.

## Foundational Learning

- **Concept: Supervised Fine-Tuning (SFT) vs. Knowledge Acquisition**
  - Why needed here: Highlights conflict between SFT's goal of teaching format versus practitioners trying to teach new facts during this phase
  - Quick check question: Does the model's attention map change more when learning a new skill (reasoning) or a new fact (entity attribute)?

- **Concept: Transformer Attention Layers (Mid-to-Late)**
  - Why needed here: Analysis relies on inspecting specific layers (12-24) where different levels of abstraction are handled
  - Quick check question: In a 28-layer model, which layers are likely responsible for retrieving the "Major" of a specific person versus the grammar of the sentence?

- **Concept: Lexical vs. Semantic Similarity**
  - Why needed here: Critical for understanding why hallucinations jump between unrelated tasks
  - Quick check question: "The cat sat on the mat" vs. "The feline rested on the rug." Which pair has higher lexical similarity? Which has higher semantic similarity?

## Architecture Onboarding

- **Component map**: Synthetic Data Generator -> Knowledge Splitter -> Training Loop -> Attention Monitor
- **Critical path**: Prepare data with 100% unknown knowledge types → Run SFT and observe attention drop → Inject known samples at end (KnownPatch) to restore attention balance
- **Design tradeoffs**: Data Filtering vs. KnownPatch (stability vs. learning new facts), Shuffling vs. structured late-stage injection
- **Failure signatures**: Attention Drop (decrease in attention weights on entity tokens in layers 12+), OOD Degradation (accuracy drops on Wiki test set despite training improvement)
- **First 3 experiments**:
  1. Attention Visualization: Train on "Unknown" type and plot attention heatmap of layers 12-24 to verify shift from entity to context tokens
  2. KnownPatch Ablation: Compare "Shuffled" training vs. "KnownPatch" (20% known injected at end) to measure accuracy recovery on Wiki test set
  3. Lexical Propagation Test: Construct reasoning task semantically different but lexically similar to training set to confirm hallucination transfer

## Open Questions the Paper Calls Out

### Open Question 1
Do the mechanisms of hallucination propagation and the efficacy of KnownPatch generalize to extremely large-scale models (70B+ parameters)? The authors note this requires future work due to computational constraints.

### Open Question 2
How do the identified hallucination mechanisms function within natural, uncontrolled corpora containing significant linguistic noise and semantic complexity? The synthetic dataset may not capture real-world text complexity.

### Open Question 3
Is the reduction in attention to key entities a necessary mechanism for acquiring new knowledge, or a detrimental side effect that can be fully decoupled from learning? The paper leaves open whether enforcing attention stability impacts knowledge acquisition.

## Limitations

- Synthetic data generalizability to real-world noisy text remains uncertain
- Attention mechanism interpretation as direct cause of hallucinations is inferential
- KnownPatch efficacy boundaries for different model sizes and domains are unknown
- Lexical propagation findings may be context-dependent and not universal

## Confidence

- **High Confidence**: 100% unfamiliarity of knowledge type causes more severe hallucinations than mixed settings
- **Medium Confidence**: Attention reduction to entities drives hallucination risk
- **Medium Confidence**: Lexical similarity primarily drives hallucination propagation

## Next Checks

1. Validate RemoveKnown vs. KeepKnown effects on non-synthetic dataset with intertwined knowledge types
2. Implement attention constraint (KL loss) during unknown knowledge training to test causal relationship
3. Design cross-domain propagation test with lexically distinct but semantically related domains