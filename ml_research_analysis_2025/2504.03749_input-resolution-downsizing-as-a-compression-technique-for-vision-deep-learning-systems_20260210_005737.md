---
ver: rpa2
title: Input Resolution Downsizing as a Compression Technique for Vision Deep Learning
  Systems
arxiv_id: '2504.03749'
source_url: https://arxiv.org/abs/2504.03749
tags:
- resolution
- scaling
- size
- memory
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores reducing input image resolution as a model
  compression technique for vision tasks, focusing on both convolutional neural networks
  (ResNets) and vision transformers (ViTs). The authors propose methods to scale input
  resolution at different stages in both architectures, enabling significant reductions
  in computational cost and memory usage.
---

# Input Resolution Downsizing as a Compression Technique for Vision Deep Learning Systems

## Quick Facts
- **arXiv ID**: 2504.03749
- **Source URL**: https://arxiv.org/abs/2504.03749
- **Reference count**: 40
- **Primary result**: Input resolution scaling achieves up to 28% FLOPs reduction for ViTs and 24% memory reduction for semantic segmentation tasks while maintaining competitive performance

## Executive Summary
This paper proposes input resolution downsizing as a model compression technique for vision deep learning systems, demonstrating its effectiveness across both convolutional neural networks (ResNets) and vision transformers (ViTs). The authors systematically explore scaling input resolution at different stages of the architectures, showing that this approach can significantly reduce computational costs and memory usage while maintaining competitive performance. The technique is shown to be complementary to quantization, enabling further compression benefits. Experiments on ImageNet and Cityscapes validate the approach, with results indicating that input resolution scaling often outperforms traditional model scaling methods.

## Method Summary
The authors propose modifying input resolution scaling at different stages for both CNNs and ViTs. For ResNets, they adjust random crop sizes during training and apply resolution scaling after the first convolution layer, introducing a hyperparameter to control the degree of resolution reduction. For ViTs, they explore adjusting the number of tokens and investigate the relationship between patch size and sequence length. The approach involves systematically reducing input resolution while monitoring performance degradation, with scaling factors optimized for each architecture. The method is designed to be orthogonal to other compression techniques, particularly quantization, allowing for combined compression strategies.

## Key Results
- Input resolution scaling achieves up to 28% reduction in FLOPs for vision transformers
- Semantic segmentation tasks show 24% reduction in memory and FLOPs
- The technique is complementary to quantization, enabling additional compression benefits
- Performance remains competitive with traditional model scaling approaches across ImageNet and Cityscapes datasets

## Why This Works (Mechanism)
Input resolution downsizing works by reducing the spatial dimensions of input images, which directly decreases the number of operations required for convolution and attention computations. For CNNs, smaller inputs mean fewer pixels to process through convolutional layers, reducing both FLOPs and memory requirements. For ViTs, reducing resolution decreases the number of tokens in the sequence, directly lowering the quadratic complexity of self-attention operations. The key insight is that many vision tasks can tolerate moderate resolution reductions without significant performance loss, particularly when the downscaling is applied strategically within the architecture. This creates a favorable trade-off between computational efficiency and accuracy, with the sweet spot varying by task and architecture.

## Foundational Learning
- **Input resolution scaling**: Reducing the spatial dimensions of input images to decrease computational requirements. Why needed: Fundamental to understanding the core compression mechanism. Quick check: Verify that halving both dimensions reduces FLOPs by approximately 75%.
- **Convolutional neural networks (ResNets)**: Deep learning architectures using convolutional layers for hierarchical feature extraction. Why needed: One of the primary architectures evaluated in the study. Quick check: Confirm that early layers dominate memory usage in ResNets.
- **Vision transformers (ViTs)**: Transformer-based architectures adapted for computer vision tasks using self-attention mechanisms. Why needed: Second primary architecture evaluated, with different scaling considerations than CNNs. Quick check: Verify that ViT computational complexity scales quadratically with sequence length.
- **FLOPs (Floating Point Operations)**: Measure of computational complexity representing the number of floating-point operations required. Why needed: Primary metric for quantifying computational savings. Quick check: Calculate FLOPs reduction when input dimensions are halved.
- **Model compression**: Techniques to reduce the size and computational requirements of neural networks while maintaining performance. Why needed: Provides context for the proposed technique within the broader field. Quick check: Compare resolution scaling with other compression methods like pruning and quantization.

## Architecture Onboarding

**Component map**: Input image → Resolution scaling (CNN/Transformer-specific) → Feature extraction layers → Classification/Segmentation head

**Critical path**: For CNNs: Input → Random crop sizing → First convolution layer → Resolution scaling → Subsequent layers → Output. For ViTs: Input → Patch extraction → Token sequence length adjustment → Self-attention layers → Output.

**Design tradeoffs**: Resolution scaling offers favorable computational savings but requires careful tuning of scaling factors to balance performance and efficiency. The technique is architecture-specific, requiring different approaches for CNNs versus ViTs. While effective for many vision tasks, it may not be suitable for applications requiring fine-grained spatial detail.

**Failure signatures**: Excessive resolution reduction leads to significant performance degradation, particularly for tasks requiring spatial precision. Poor scaling factor selection can result in suboptimal compression or accuracy loss. The technique may not generalize well across all vision tasks, particularly those with unique resolution requirements.

**3 first experiments**:
1. Apply resolution scaling to a standard ResNet-50 on ImageNet with scaling factors of 0.75, 0.5, and 0.25 to establish baseline performance-accuracy tradeoffs.
2. Implement token sequence length adjustment in ViT-B/16 on ImageNet, varying patch sizes and measuring impact on FLOPs and accuracy.
3. Combine resolution scaling with 8-bit quantization on both architectures to quantify complementary benefits.

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the generalization of input resolution scaling across different vision tasks and architectures. Key questions include how the technique performs on domain-specific datasets like medical imaging or satellite imagery, where resolution requirements may differ significantly from standard benchmarks. The paper also raises questions about the optimal scaling strategies for more recent efficient architectures beyond standard ResNets and ViTs. Additionally, the extent of combined benefits with quantization across different bit-widths and hardware platforms remains unexplored.

## Limitations
- Results primarily validated on ImageNet and Cityscapes, limiting generalization to other vision tasks
- Analysis focuses on standard architectures without exploring recent efficient variants
- Memory reduction calculations for CNNs are theoretical rather than empirically measured
- Optimal scaling factors and strategies may vary significantly across different domains and tasks

## Confidence
**High confidence**: The basic premise that reducing input resolution decreases computational requirements is well-established. The applicability of this technique to both CNNs and ViTs is demonstrated through systematic experiments.

**Medium confidence**: The claimed performance retention at various scaling levels and the specific scaling factors proposed are supported by experiments but may vary across different tasks and datasets.

**Low confidence**: The generalization of these findings across diverse vision tasks, particularly those with unique resolution requirements, and the precise quantification of memory savings in practical implementations.

## Next Checks
1. Test the resolution scaling approach on domain-specific datasets (medical imaging, satellite imagery) to verify performance retention across different resolution requirements
2. Measure actual memory usage during training and inference across different hardware platforms to validate theoretical memory reduction calculations
3. Evaluate the combined effects of resolution scaling and quantization across different bit-widths (e.g., 8-bit, 4-bit, 2-bit) to quantify the additive benefits and identify optimal combinations