---
ver: rpa2
title: 'Scaling Arabic Medical Chatbots Using Synthetic Data: Enhancing Generative
  AI with Synthetic Patient Records'
arxiv_id: '2509.10108'
source_url: https://arxiv.org/abs/2509.10108
tags:
- data
- synthetic
- medical
- arabic
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of scaling Arabic medical chatbots
  by overcoming the scarcity of high-quality Arabic medical datasets. The authors
  propose a synthetic data augmentation strategy, expanding a real-world dataset of
  20,000 Arabic patient-doctor interactions to 100,000 records using generative AI
  systems ChatGPT-4o and Gemini 2.5 Pro.
---

# Scaling Arabic Medical Chatbots Using Synthetic Data: Enhancing Generative AI with Synthetic Patient Records

## Quick Facts
- arXiv ID: 2509.10108
- Source URL: https://arxiv.org/abs/2509.10108
- Reference count: 23
- 5 Arabic LLMs fine-tuned on 100K augmented dataset showed up to 13% F1-score improvement over 20K baseline

## Executive Summary
This paper addresses the challenge of scaling Arabic medical chatbots by overcoming the scarcity of high-quality Arabic medical datasets. The authors propose a synthetic data augmentation strategy, expanding a real-world dataset of 20,000 Arabic patient-doctor interactions to 100,000 records using generative AI systems ChatGPT-4o and Gemini 2.5 Pro. The synthetic records were semantically filtered, manually validated, and integrated with the original data. Five LLMs were fine-tuned and evaluated using BERTScore metrics and expert-driven qualitative assessments. An ablation study compared the effectiveness of ChatGPT-4o and Gemini-generated data, showing that ChatGPT-4o consistently led to higher F1-scores and fewer hallucinations. The approach demonstrates that synthetic augmentation is a viable solution for enhancing domain-specific language models in low-resource medical NLP settings.

## Method Summary
The method employs a three-stage pipeline: (1) synthetic generation using ChatGPT-4o and Gemini 2.5 Pro with structured prompts simulating patient complaints, (2) semantic filtering via cosine similarity and BERT embeddings, followed by manual validation of 500 samples by native Arabic speakers with medical knowledge, and (3) fine-tuning five Arabic LLMs (AraGPT2-Base, LLaMA-2-7B, Mistral-7B-Instruct-v0.2, BLOOM-560M, GPT-2 Medium) using LoRA-based fine-tuning with PEFT. Models were trained with learning rate 5e-5, cosine decay, 200-step warmup, batch size 8, 3-5 epochs, FP16 precision, and AdamW optimizer. Evaluation combined BERTScore F1 with qualitative expert review focusing on fluency, relevance, and medical plausibility.

## Key Results
- Data augmentation improved F1-scores across all models, with up to +13% improvement for Mistral-7B over the 20K baseline
- ChatGPT-4o-generated synthetic data consistently outperformed Gemini 2.5 Pro, showing higher F1-scores and fewer hallucinations across all models
- The ablation study demonstrated that synthetic augmentation is a viable solution for enhancing domain-specific language models in low-resource medical NLP settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic data augmentation can compensate for scarce real-world training data in low-resource language domains, improving model generalization without manual data collection
- Mechanism: High-capacity LLMs generate contextually grounded synthetic patient-doctor dialogues using structured prompts anchored in real data patterns. These samples are filtered for semantic coherence and medical plausibility, then merged with original data to create a 5× larger training corpus
- Core assumption: The synthetic samples preserve sufficient linguistic authenticity and medical accuracy to enhance—rather than degrade—downstream model performance
- Evidence anchors: [abstract] "The results showed that ChatGPT-4o data consistently led to higher F1-scores and fewer hallucinations across all models"; [Section VI-A] "Synthetic augmentation led to a clear increase in F1-score for every model—improving up to +13% over the 20K fine-tuned version in the case of Mistral-7B"
- Break condition: If synthetic data introduces systematic biases, medical inaccuracies, or dialectal drift not caught by filtering, model performance degrades

### Mechanism 2
- Claim: The choice of synthetic data generator materially affects downstream model quality, even with identical training configurations
- Mechanism: ChatGPT-4o generates more semantically coherent and medically grounded samples than Gemini 2.5 Pro under controlled prompting. These quality differences persist through training, yielding higher F1-scores and fewer hallucinations
- Core assumption: The observed performance gap stems from intrinsic generation quality differences, not confounding factors like prompt design or sampling variance
- Evidence anchors: [Section VII-B] "ChatGPT-4o outputs showed less hallucinations, improved adherence to medical context, and more accurate language structure. Gemini 2.5 Pro... occasionally produced verbose or overly generic responses"; [Table II] Mistral-7B: ChatGPT-4o (69.30%) vs Gemini (67.22%)
- Break condition: If prompt engineering favored ChatGPT-4o's tokenization or training distribution, the quality gap may be methodological rather than intrinsic

### Mechanism 3
- Claim: Multi-stage filtering (semantic similarity + manual review) is necessary to ensure synthetic data quality before model training
- Mechanism: Cosine similarity filters and BERT-based embeddings remove samples diverging too far from real data distribution. Manual validation by native Arabic speakers with medical knowledge catches hallucinations and incoherence missed by automated filters
- Core assumption: Proximity to real data distribution correlates with training utility and safety; human reviewers reliably detect subtle errors
- Evidence anchors: [Section IV-A-2] "Each pair of QA was passed through cosine similarity filters and BERT-based embedding analysis... A random sample of 500 synthetic QA pairs was manually reviewed"
- Break condition: If filtering is too aggressive, synthetic diversity drops; if too permissive, noise and hallucinations propagate into training

## Foundational Learning

- Concept: Low-Resource NLP Strategies (data augmentation, transfer learning, LoRA)
  - Why needed here: Arabic medical text lacks the large-scale annotated datasets common in English; practitioners must extract maximal value from limited data
  - Quick check question: Can you explain why LoRA enables fine-tuning large models (7B parameters) on modest GPU hardware?

- Concept: Semantic Similarity Metrics (BERTScore, cosine similarity on embeddings)
  - Why needed here: Traditional n-gram metrics (BLEU, ROUGE) poorly capture semantic equivalence in generative tasks, especially with dialectal variation
  - Quick check question: How does BERTScore differ from BLEU in evaluating paraphrased medical responses?

- Concept: Hallucination Detection in Medical LLMs
  - Why needed here: Medical chatbots can produce plausible-sounding but factually incorrect advice; understanding hallucination patterns is critical for safety
  - Quick check question: What are two failure modes the paper attributes to Gemini-generated data compared to ChatGPT-4o?

## Architecture Onboarding

- Component map: Seed Dataset (20K real Arabic patient-doctor dialogues) → Synthetic Generator Module (ChatGPT-4o/Gemini APIs with prompts) → Filtering Pipeline (cosine similarity + BERT embeddings) → Manual Validation Layer (500-sample spot checks) → Dataset merge → LoRA fine-tuning → BERTScore evaluation → Ablation comparison

- Critical path: Seed data → Prompt engineering → Synthetic generation → Semantic filtering → Manual validation → Dataset merge → LoRA fine-tuning → BERTScore evaluation → Ablation comparison

- Design tradeoffs:
  - Volume vs. Quality: Aggressive filtering yields cleaner data but may discard useful dialectal diversity
  - Generator selection: ChatGPT-4o produces higher-quality output but introduces dependency on a single commercial API
  - Manual review scale: 500-sample validation is statistically limited; full review is cost-prohibitive
  - Model size: Larger models (Mistral-7B, LLaMA-2-7B) perform better but require more GPU memory and LoRA tuning

- Failure signatures:
  - Hallucination cascades: Models trained on low-quality synthetic data produce confident but medically inaccurate responses
  - Overfitting to synthetic artifacts: Models repeat phrasing patterns unique to synthetic generator style
  - Dialectal collapse: Models default to Modern Standard Arabic, losing dialectal nuance from original data
  - Verbosity/generic responses: Models produce long, non-committal answers (observed with Gemini data in ablation)

- First 3 experiments:
  1. Replicate ablation with held-out test set: Train separate models on ChatGPT-4o-only and Gemini-only synthetic data; evaluate on 2K real Arabic medical QA pairs not in training. Confirm F1 gap persists
  2. Filter sensitivity analysis: Vary cosine similarity threshold (0.6, 0.7, 0.8) and measure impact on F1-score, hallucination rate, and vocabulary diversity
  3. Cross-generator augmentation: Train models on 50/50 mixed synthetic data vs. ChatGPT-4o-only; test whether blending improves robustness or dilutes quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal ratio of synthetic to real data for maximizing performance without introducing noise?
- Basis in paper: [inferred] The study utilized a fixed 5:1 ratio (80,000 synthetic to 20,000 real records) but did not test intermediate thresholds to identify diminishing returns
- Why unresolved: It remains unclear if the 100k volume was the efficiency peak or if the high proportion of synthetic data introduced subtle linguistic drift
- What evidence would resolve it: A comparative analysis fine-tuning models on varying synthetic-to-real ratios (e.g., 1:1, 2:1, 10:1) and measuring performance degradation

### Open Question 2
- Question: Does scaling synthetic data increase the rate of medical hallucinations or misinformation compared to real data?
- Basis in paper: [inferred] Section III.D warns that training on synthetic data risks "misinformation spread," yet the evaluation relies heavily on BERTScore (semantic similarity) rather than factual accuracy metrics
- Why unresolved: While qualitative analysis noted fewer hallucinations for ChatGPT-4o data, the paper lacks a quantitative benchmark for medical factuality across the full dataset
- What evidence would resolve it: Evaluation using a medical fact-checking benchmark or expert annotation of error types (e.g., diagnostic hallucinations) on the full test set

### Open Question 3
- Question: How well does the synthetic augmentation strategy generalize to low-resource Arabic dialects not present in the seed data?
- Basis in paper: [explicit] Section III.B identifies dialectal variety as a major complexity, and Section III.E lists examining "dialectal nuances" as a research objective
- Why unresolved: The results aggregate performance metrics, failing to isolate how well the synthetic models handle specific dialectal variations versus Modern Standard Arabic
- What evidence would resolve it: A stratified evaluation of model responses based on the specific dialect or informality level of the input prompt

## Limitations

- The study relies on proprietary LLMs (ChatGPT-4o, Gemini 2.5 Pro) for synthetic data generation, creating potential reproducibility barriers and dependency on commercial APIs
- The 500-sample manual validation represents only 0.5% of the synthetic dataset, raising questions about statistical robustness of quality assurance claims
- Original 20K Arabic patient-doctor dataset is not publicly available, preventing exact replication and verification of claimed performance improvements
- Dialectal diversity coverage remains unclear; the paper doesn't specify how well synthetic data represents different Arabic dialects encountered in real medical contexts

## Confidence

- **High Confidence**: Data augmentation improves F1-scores across all tested models (+13% maximum observed improvement)
- **Medium Confidence**: ChatGPT-4o generates higher quality synthetic data than Gemini 2.5 Pro
- **Medium Confidence**: Multi-stage filtering (semantic + manual) effectively removes low-quality synthetic samples

## Next Checks

1. Replicate the ablation study using a held-out test set of 2,000 real Arabic medical QA pairs not present in training data, to verify the claimed F1-score differences between ChatGPT-4o and Gemini-generated models
2. Conduct a filter sensitivity analysis by varying cosine similarity thresholds (0.6, 0.7, 0.8) and measuring impacts on F1-score, hallucination rates, and vocabulary diversity
3. Perform a cross-generator augmentation experiment training models on 50/50 mixed synthetic data versus ChatGPT-4o-only data to test whether blending improves robustness or dilutes quality