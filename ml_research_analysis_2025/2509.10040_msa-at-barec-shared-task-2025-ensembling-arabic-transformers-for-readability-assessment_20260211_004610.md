---
ver: rpa2
title: '!MSA at BAREC Shared Task 2025: Ensembling Arabic Transformers for Readability
  Assessment'
arxiv_id: '2509.10040'
source_url: https://arxiv.org/abs/2509.10040
tags:
- readability
- arabic
- barec
- data
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We present !MSA\u2019s winning system for the BAREC 2025 Shared\
  \ Task on fine-grained Arabic readability assessment, achieving first place in six\
  \ of six tracks. Our approach is a confidence-weighted ensemble of four complementary\
  \ transformer models (AraBERTv2, AraELECTRA, MARBERT, and CAMeLBERT) each fine-tuned\
  \ with distinct loss functions to capture diverse readability signals."
---

# !MSA at BAREC Shared Task 2025: Ensembling Arabic Transformers for Readability Assessment

## Quick Facts
- **arXiv ID**: 2509.10040
- **Source URL**: https://arxiv.org/abs/2509.10040
- **Reference count**: 5
- **Primary result**: First place in all six BAREC 2025 tracks (sentence/document, strict/constrained/open) with 87.5% QWK sentence-level, 87.4% document-level.

## Executive Summary
This paper presents !MSA’s winning system for the BAREC 2025 Shared Task on fine-grained Arabic readability assessment. The approach is a confidence-weighted ensemble of four transformer models (AraBERTv2, AraELECTRA, MARBERT, CAMeLBERT), each fine-tuned with distinct loss functions to capture diverse readability signals. To address severe class imbalance and data scarcity, the authors applied weighted training, SAMER corpus relabeling, and synthetic data generation via Gemini 2.5 Flash (~10K rare-level samples). A targeted post-processing step corrected prediction distribution skew, delivering a 6.3% QWK gain. The system achieved 87.5% QWK at the sentence level and 87.4% at the document level, demonstrating the power of model and loss diversity, confidence-informed fusion, and intelligent augmentation for robust Arabic readability prediction.

## Method Summary
The method combines four complementary Arabic transformer models, each fine-tuned with different loss functions (Cross-Entropy, MSE, CORAL/COR) to capture varied readability signals. A confidence-weighted ensembling strategy uses softmax probabilities (classification) or inverse variance (regression) to fuse predictions. Class imbalance is addressed through weighted training (inverse-frequency weights), advanced preprocessing (D3TOK morphological tokenization, punctuation normalization), SAMER corpus relabeling with the strongest model, and synthetic data generation (~10K rare-level samples via Gemini 2.5 Flash). Document-level predictions use max aggregation over sentence-level outputs. A post-processing step adjusts prediction distributions, particularly correcting floor/ceiling averaging and enforcing predictions of 16 or 17 when present.

## Key Results
- Achieved first place in all six BAREC 2025 tracks (sentence/document, strict/constrained/open)
- 87.5% Quadratic Weighted Kappa (QWK) at sentence level, 87.4% at document level
- 6.3% QWK improvement attributed to post-processing and synthetic data generation
- Demonstrated effectiveness of confidence-weighted ensembling and model/loss diversity

## Why This Works (Mechanism)
The ensemble’s success stems from combining four transformer architectures with diverse loss functions, capturing complementary readability signals across the 19-class ordinal scale. Confidence-weighted fusion leverages model-specific prediction certainty, while synthetic data generation and SAMER relabeling address severe class imbalance. The post-processing step corrects distribution skew that would otherwise suppress rare class predictions. This multi-pronged approach ensures robust performance across all tracks despite limited training data and extreme label imbalance.

## Foundational Learning
- **Arabic transformer architectures (AraBERTv2, AraELECTRA, MARBERT, CAMeLBERT)**: Needed to capture linguistic patterns specific to Arabic readability assessment; quick check: verify each model’s pretraining corpus and vocabulary coverage.
- **Ordinal classification with multiple loss functions**: Cross-Entropy for classification, MSE for regression, and CORAL for ordinal regression capture different aspects of the 19-class readability scale; quick check: ensure proper label encoding (0-18) for each loss variant.
- **Confidence-weighted ensembling**: Combines model predictions based on their estimated certainty (softmax probabilities or inverse variance); quick check: validate that confidence scores are normalized across models before fusion.
- **Class imbalance mitigation strategies**: Weighted training, synthetic data generation, and corpus relabeling address the severe skew toward middle classes; quick check: plot class distribution before/after each intervention to verify balance improvement.
- **Document-level aggregation via max pooling**: Ensures document predictions reflect the most challenging sentence in the text; quick check: verify that aggregation preserves rare class predictions when present.
- **Post-processing heuristics**: Floor/ceiling adjustments and label-16/17 override correct systematic prediction biases; quick check: quantify contribution of each heuristic through ablation studies.

## Architecture Onboarding
- **Component map**: Preprocessing (D3TOK tokenization, punctuation normalization) -> 4 transformer fine-tuning pipelines (AraBERTv2, AraELECTRA, MARBERT, CAMeLBERT with CE/MSE/CORAL losses) -> Confidence-weighted ensemble fusion -> Post-processing (floor/ceiling adjustment, label-16/17 override) -> Document aggregation (max over sentences)
- **Critical path**: Model fine-tuning → Confidence-weighted ensembling → Post-processing → Document aggregation
- **Design tradeoffs**: Model diversity vs. training complexity; synthetic data generation vs. potential contamination; post-processing heuristics vs. model generalization
- **Failure signatures**: Majority class clustering (labels 12, 14); near-zero predictions for rare levels (1, 18, 19); document-level aggregation artifacts (missing label 10, over-prediction of label 15)
- **Three first experiments**:
  1. Train each transformer with all three loss functions independently to establish baseline performance and identify strongest model-loss combinations
  2. Implement confidence-weighted ensembling with simple average fusion to verify improvement over individual models
  3. Apply post-processing heuristics to baseline ensemble to quantify their contribution to overall performance

## Open Questions the Paper Calls Out
None

## Limitations
- Exact ensemble configuration (which specific 6 variants formed the best run) is unspecified
- Confidence fusion mechanics (normalization across heterogeneous outputs) lack detail
- Post-processing heuristics are empirically tuned without theoretical justification
- Synthetic data generation process is not detailed, raising potential data leakage concerns
- Reliance on specific transformer models may limit generalization

## Confidence
- **High confidence**: QWK scores (87.5% sentence, 87.4% document) and primary ranking claims (1st place in all 6 tracks) are verifiable via BAREC official leaderboards; ensemble methodology is clearly described
- **Medium confidence**: Class imbalance mitigation strategies are methodologically valid, but individual contributions to the 6.3% QWK gain are unclear without ablation studies
- **Low confidence**: Exact ensemble composition and confidence fusion mechanics are underspecified, making exact replication uncertain

## Next Checks
1. **Reconstruct ensemble variants**: Train the 4 base models with all 3 loss functions (CE, MSE, CORAL), ensemble all 12 possible combinations, and compare QWK distributions to identify the top-6 configuration that matches the reported performance
2. **Isolate post-processing impact**: Run ablation studies disabling the floor/ceiling adjustments and label-16/17 override, then quantify their individual contributions to the 6.3% QWK gain
3. **Validate synthetic data quality**: Analyze the synthetic samples generated via Gemini 2.5 Flash for label distribution shifts, semantic coherence, and potential test-data contamination using embedding similarity metrics