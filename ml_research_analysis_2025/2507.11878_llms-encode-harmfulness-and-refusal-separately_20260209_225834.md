---
ver: rpa2
title: LLMs Encode Harmfulness and Refusal Separately
arxiv_id: '2507.11878'
source_url: https://arxiv.org/abs/2507.11878
tags:
- refusal
- harmfulness
- harmful
- instructions
- harmless
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether LLMs internally understand harmfulness
  separately from refusal. The authors find that LLMs encode harmfulness and refusal
  as distinct concepts at different token positions: harmfulness is primarily encoded
  at the last token of the instruction (tinst), while refusal is encoded at the last
  token of the entire sequence (tpost-inst).'
---

# LLMs Encode Harmfulness and Refusal Separately

## Quick Facts
- arXiv ID: 2507.11878
- Source URL: https://arxiv.org/abs/2507.11878
- Authors: Jiachen Zhao; Jing Huang; Zhengxuan Wu; David Bau; Weiyan Shi
- Reference count: 40
- Primary result: LLMs encode harmfulness and refusal as distinct concepts at different token positions, enabling more robust safety detection

## Executive Summary
This paper reveals that large language models (LLMs) encode harmfulness and refusal as separate concepts at different token positions: harmfulness is primarily represented at the last token of the instruction (tinst), while refusal is encoded at the last token of the entire sequence (tpost-inst). Through clustering analysis and steering experiments, the authors demonstrate that these concepts can be independently manipulated, with steering along the harmfulness direction reversing the model's judgment of harmfulness, while steering along the refusal direction elicits refusal without changing harmfulness perception. This insight led to the development of "Latent Guard," a safety mechanism that leverages the model's internal harmfulness representation, achieving comparable or better performance than existing methods while being more robust to adversarial fine-tuning attacks.

## Method Summary
The authors analyze hidden states at two key token positions (tinst and tpost-inst) across three 7B-8B models (Llama2, Llama3, Qwen2) using clustering analysis to identify where harmfulness and refusal are encoded. They compute difference vectors (harmfulness direction: μ_harmful - μ_harmless at tinst; refusal direction: μ_refuse - μ_accept at tpost-inst) and use activation addition (h'_l = h_l + v) to test causal effects on model behavior. Latent Guard classifies instructions by computing a harmfulness belief score (Δ_harmful) at tinst, using the sign of this score to detect harmful content. The method is evaluated against various jailbreak techniques and compared with Llama Guard 3 8B.

## Key Results
- Clustering analysis shows hidden states at tinst cluster by harmfulness while those at tpost-inst cluster by refusal, with opposite patterns for accepted harmful and refused harmless instructions
- Steering along harmfulness direction reverses harmfulness judgments in reply inversion tasks, while steering along refusal direction elicits refusal without reversing harmfulness perception
- Latent Guard achieves comparable or better performance than Llama Guard 3 8B across different jailbreak methods while remaining robust to adversarial fine-tuning attacks
- Finetuning with 50-400 adversarial examples increases acceptance rates but leaves harmfulness belief scores nearly unchanged on held-out harmful instructions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs encode harmfulness and refusal at different token positions, enabling their decoupling.
- Mechanism: Hidden states at tinst (last instruction token) cluster by harmfulness regardless of model behavior, while hidden states at tpost-inst (last sequence token) cluster by refusal/acceptance. The paper demonstrates this through clustering analysis where "accepted harmful instructions" fall into the harmful cluster at tinst but into the acceptance cluster at tpost-inst.
- Core assumption: Clustering patterns in hidden states reflect what concepts are encoded at each position.
- Evidence anchors:
  - [abstract] "harmfulness is primarily encoded at the last token of the instruction (tinst), while refusal is encoded at the last token of the entire sequence (tpost-inst)"
  - [Section 3.2] Figure 2 shows opposing clustering patterns at tinst vs tpost-inst across three models
  - [corpus] Limited direct support; one neighbor paper (arXiv:2502.14486) investigates jailbreak defense mechanisms but doesn't directly address harmfulness/refusal separation
- Break condition: If clustering analysis were performed on different model architectures or at different layers and showed no separation, this mechanism would not hold.

### Mechanism 2
- Claim: Steering along the harmfulness direction reverses the model's harmfulness judgment, whereas steering along the refusal direction elicits refusal without reversing that judgment.
- Mechanism: Using a reply inversion task where models must answer "Certainly" if harmful (inverted response), the paper shows that harmfulness-direction steering flips responses from "No" to "Certainly" on harmless inputs. Refusal-direction steering maintains "No" responses despite the inversion prompt.
- Core assumption: Response flips in the reply inversion task reflect genuine changes in harmfulness perception rather than surface-level output patterns.
- Evidence anchors:
  - [abstract] "steering along the harmfulness direction can lead LLMs to interpret harmless instructions as harmful, but steering along the refusal direction tends to elicit refusal responses directly without reversing the model's judgment on harmfulness"
  - [Section 3.5] Figure 5 shows opposite steering behaviors on Qwen2 in reply inversion task
  - [corpus] Corpus papers do not directly test this specific steering distinction
- Break condition: If refusal-direction steering also flipped responses in the reply inversion task, the claimed separation would collapse.

### Mechanism 3
- Claim: Latent Guard leverages the model's internal harmfulness representation for safety detection, remaining robust to finetuning attacks.
- Mechanism: The harmfulness belief score (Δ_harmful) computed at tinst is used to classify instructions. After adversarial finetuning, the model's harmfulness belief on held-out harmful instructions remains nearly unchanged despite increased acceptance rates.
- Core assumption: Internal harmfulness representations are more stable under finetuning than surface-level refusal behaviors.
- Evidence anchors:
  - [Section 5.2] Figure 7 shows Δ_harmful stays consistent across finetuning with 50-400 adversarial examples
  - [Section 5.1] Table 3 shows Latent Guard achieves comparable or better performance than Llama Guard 3 on jailbreak detection
  - [corpus] One neighbor paper (arXiv:2511.14423) addresses unified defense against jailbreak and finetuning attacks but uses different methods
- Break condition: If finetuning with larger or more diverse adversarial datasets altered the harmfulness representation, Latent Guard would lose robustness.

## Foundational Learning

- Concept: Residual stream and token positions in decoder-only Transformers
  - Why needed here: Understanding that h_l(x_t) contains information from prior tokens and encodes future token plans is essential for interpreting why different token positions encode different concepts.
  - Quick check question: Why does the hidden state at tinst contain information about the entire instruction?

- Concept: Difference-in-means for extracting steering directions
  - Why needed here: Both harmfulness and refusal directions are computed as vector differences between cluster centroids (µ_harmful - µ_harmless, µ_refuse - µ_accept).
  - Quick check question: What does the direction vector from harmless cluster centroid to harmful cluster centroid represent?

- Concept: Activation addition (steering) intervention
  - Why needed here: The paper intervenes via h'_l = h_l + v_harmful to test causal effects of directions on model behavior.
  - Quick check question: At which layers does steering with the harmfulness direction achieve maximum effect, and why might middle layers be optimal?

## Architecture Onboarding

- Component map:
  - tinst: Last token of user instruction → encodes harmfulness belief (clustering by harmful/harmless)
  - tpost-inst: Last token of post-instruction sequence → encodes refusal decision (clustering by refused/accepted)
  - Harmfulness direction: v_harmful = µ_harmful - µ_harmless at tinst
  - Refusal direction: v_refuse = µ_refuse - µ_accept at tpost-inst
  - Latent Guard classifier: sign of Δ_harmful score (Equation 3)

- Critical path:
  1. Collect training examples: refused harmful and accepted harmless instructions
  2. Extract hidden states at tinst and tpost-inst across layers
  3. Compute cluster centroids and steering directions
  4. For Latent Guard: compute Δ_harmful using cosine similarity difference to centroids

- Design tradeoffs:
  - Training set size: Paper uses 100 harmful + 100 harmless examples; smaller sets reduce cluster quality, larger sets may introduce noise
  - Layer selection: Steering peaks at middle layers (layer 9 for Llama3, layer 13 for Qwen2); early/late layers show weaker effects
  - Threshold α for clustering: Paper sets α=0 but notes oracle values may differ; incorrect thresholds increase misclassification

- Failure signatures:
  - Latent Guard performs poorly on out-of-distribution categories (e.g., ToxicChat, OpenAI Moderation) without in-domain examples (Appendix H.1)
  - Over-refusal cases: harmless instructions with trigger words (e.g., "kill the lights") may be refused at tpost-inst despite correct harmfulness belief at tinst
  - Persuasion jailbreaks can sometimes reverse harmfulness belief (Figure 6), evading Latent Guard

- First 3 experiments:
  1. Replicate clustering analysis: Extract hidden states at tinst and tpost-inst for accepted harmful and refused harmless instructions; verify they cluster by harmfulness at tinst but by refusal at tpost-inst.
  2. Reply inversion steering test: Apply harmfulness vs refusal direction steering on harmless instructions with inversion prompts; confirm opposite response patterns (Figure 5).
  3. Latent Guard robustness check: Finetune model on 100-200 adversarial (harmful instruction, acceptance response) pairs; measure whether Δ_harmful on held-out harmful examples remains stable.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the spatial separation and distinct encoding of harmfulness versus refusal generalize to larger model scales (e.g., 70B+ parameters) and different architectures?
- Basis in paper: [explicit] The authors state in the Limitations section that they focused on 7B and 8B models due to resource constraints, leaving it unclear how these features are encoded as model size increases.
- Why unresolved: The paper provides evidence for Llama2-7B, Llama3-8B, and Qwen2-7B, but the complexity of representations in much larger models may alter the clustering or steering effectiveness observed at smaller scales.
- What evidence would resolve it: Replicating the clustering analysis (t-inst vs. t-post-inst) and the reply inversion steering experiments on models like Llama-3-70B or proprietary equivalents.

### Open Question 2
- Question: What is the mechanistic role of specific layers and attention heads in transmitting the "harmfulness belief" from the instruction token to the refusal signal at the post-instruction token?
- Basis in paper: [explicit] The authors note in Section 3.2 that they leave "model-level (e.g., neurons or layers in the model) interpretations of harmfulness and refusal" and understanding the specific "role of each layer" as future work.
- Why unresolved: While the paper identifies *where* (token positions) these concepts reside, it does not fully explain the internal circuitry or attention mechanisms that link the perception of harmfulness to the execution of refusal.
- What evidence would resolve it: A circuit analysis (e.g., attention head attribution) tracing information flow from the $t_{inst}$ hidden state to the $t_{post-inst}$ hidden state during the processing of harmful inputs.

### Open Question 3
- Question: Does safety alignment (SFT/RLHF) primarily install superficial refusal behaviors, or does it fundamentally alter the model's internal semantic understanding of harmfulness?
- Basis in paper: [explicit] The authors ask in the Discussion: "Through supervised finetuning... do LLMs primarily learn superficial refusal/acceptance behaviors, or do they acquire a deeper understanding of harmfulness semantics?"
- Why unresolved: The finding that finetuning attacks leave the harmfulness belief ($\Delta_{harmful}$) intact suggests a disconnect, but the evolution of these representations during the original safety training phase was not tracked.
- What evidence would resolve it: Tracking the trajectory and separation of the harmfulness direction and refusal direction throughout the stages of pre-training, SFT, and RLHF.

### Open Question 4
- Question: How does the differentiation of harmfulness directions across specific risk categories impact the generalizability of safety mechanisms like Latent Guard to out-of-distribution inputs?
- Basis in paper: [explicit] The authors note in Appendix H.1 that the limited training set of Latent Guard may not cover broad taxonomies, and they "leave it as promising future work to study the fine-grained harmfulness representations of different risk categories."
- Why unresolved: The paper shows harmfulness directions vary by category (e.g., Hate vs. Malware), suggesting a "one-size-fits-all" harmfulness vector might fail on novel or ambiguous harm types, a limitation observed in some evaluation sets.
- What evidence would resolve it: Testing Latent Guard's performance on out-of-distribution (OOD) harmful datasets when trained on single categories versus aggregated categories to measure the robustness of the "general" harmfulness direction.

## Limitations

- Limited scope of harmfulness categories: The harmfulness direction is trained on a small set of 100 harmful and 100 harmless instructions, which may not generalize to all types of harmful content
- Potential confounding factors in steering experiments: Success in reply inversion tasks could reflect surface-level compliance rather than genuine harmfulness perception changes
- Incomplete characterization of jailbreak mechanisms: The paper doesn't exhaustively analyze all jailbreak techniques, which may work through multiple mechanisms simultaneously

## Confidence

**High confidence** in the core empirical findings:
- The clustering analysis showing harmfulness at tinst and refusal at tpost-inst is well-supported by consistent patterns across three different models
- The steering experiments demonstrate clear directional effects with predictable outcomes
- The finetuning robustness results show stable harmfulness representations despite adversarial training

**Medium confidence** in the mechanism interpretations:
- The causal link between steering behavior and "genuine" harmfulness perception changes in reply inversion tasks
- The generalizability of harmfulness/refusal separation across all LLM architectures and training paradigms
- The completeness of the jailbreak mechanism analysis

**Low confidence** in real-world deployment claims:
- Long-term robustness of Latent Guard against evolving jailbreak techniques
- Performance on out-of-distribution harmfulness categories not represented in training
- False positive rates in practical safety-critical applications

## Next Checks

1. **Cross-category harmfulness encoding test**: Train Latent Guard on harmful instructions from one category (e.g., violence) and test its harmfulness direction on instructions from a different category (e.g., self-harm or illegal activities). Measure whether the harmfulness direction generalizes across harmfulness types.

2. **Multi-step jailbreak analysis**: Systematically test whether jailbreak methods that appear to suppress refusal signals also reverse harmfulness beliefs at tinst. Use the reply inversion task with jailbroken inputs to determine if steering along the harmfulness direction can still flip responses.

3. **Adversarial perturbation robustness**: Apply small, imperceptible perturbations to harmful instructions (e.g., synonym substitution, character-level changes) and measure whether Δ_harmful remains stable. This tests whether Latent Guard is vulnerable to adversarial examples that preserve semantic meaning while evading detection.