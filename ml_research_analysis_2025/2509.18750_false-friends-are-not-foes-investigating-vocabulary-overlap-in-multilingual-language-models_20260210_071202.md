---
ver: rpa2
title: 'False Friends Are Not Foes: Investigating Vocabulary Overlap in Multilingual
  Language Models'
arxiv_id: '2509.18750'
source_url: https://arxiv.org/abs/2509.18750
tags:
- overlap
- english
- language
- tokens
- high-sim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates whether overlapping tokens across languages
  in multilingual tokenizers help or hinder cross-lingual transfer. To control for
  confounders like token frequency and segmentation granularity, the authors train
  bilingual autoregressive models on six language pairs under four systematically
  varied vocabulary overlap settings: full, high-similarity, low-similarity, and no
  overlap.'
---

# False Friends Are Not Foes: Investigating Vocabulary Overlap in Multilingual Language Models

## Quick Facts
- **arXiv ID:** 2509.18750
- **Source URL:** https://arxiv.org/abs/2509.18750
- **Reference count:** 40
- **Primary result:** Token overlap in multilingual tokenizers enables cross-lingual transfer; high-similarity overlap yields the strongest gains, particularly for distant language pairs.

## Executive Summary
This study investigates whether overlapping tokens across languages in multilingual tokenizers help or hinder cross-lingual transfer. To control for confounders like token frequency and segmentation granularity, the authors train bilingual autoregressive models on six language pairs under four systematically varied vocabulary overlap settings: full, high-similarity, low-similarity, and no overlap. Overlap is defined based on semantic similarity of tokens between languages, measured using contextual embeddings from XLM-R. Results show that token overlap enables embedding spaces to capture cross-lingual semantic relationships, with stronger effects for high-similarity overlap. In downstream tasks (XNLI and XQuAD), models with any overlap outperform those with disjoint vocabularies, and transfer performance generally improves as overlap increases. High-similarity overlap yields the strongest gains, particularly for distant language pairs, though any overlap proves beneficial. The findings suggest that substantial shared vocabulary remains a valuable design choice for multilingual tokenizers.

## Method Summary
The authors systematically manipulate vocabulary overlap in bilingual autoregressive models to isolate its effect on cross-lingual transfer. They train GPT-2-style models on six language pairs (English paired with Spanish, German, Turkish, Chinese, Arabic, and Swahili) under four overlap settings: full, high-similarity, low-similarity, and no overlap. Token overlap is defined using semantic similarity scores computed from XLM-R layer-5 embeddings, with high-similarity overlap containing the top 50% of semantically aligned tokens and low-similarity overlap containing the bottom 50%. Models are pre-trained on interleaved parallel corpora (CCMatrix) for 100K steps and evaluated on XNLI (classification) and XQuAD (span extraction) after English fine-tuning. The study controls for tokenizer quality by using XLM-R's SentencePiece tokenizer and partitions native overlap based on semantic similarity to avoid conflating token frequency effects.

## Key Results
- Any vocabulary overlap enables embedding spaces to capture cross-lingual semantic relationships, outperforming disjoint vocabularies.
- High-similarity overlap yields the strongest transfer gains, especially for distant language pairs with different scripts.
- Low-similarity overlap underperforms high-similarity but still outperforms no overlap, though effects are weaker for script-divergent pairs.
- Shared tokens serve as cross-lingual anchors that align embedding spaces, with semantic similarity determining alignment quality.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shared tokens serve as cross-lingual anchors that align embedding spaces between languages.
- Mechanism: When two languages share token IDs, the model learns a single embedding vector for that token. During bilingual pre-training on interleaved corpora, gradient updates from both languages act on the same embedding, pulling representations toward a shared region even without explicit alignment objectives.
- Core assumption: The model receives sufficient exposure to shared tokens in both language contexts to form meaningful associations.
- Evidence anchors:
  - [abstract]: "overlap of any kind creates embedding spaces that capture cross-lingual semantic relationships, while this effect is much weaker in models with disjoint vocabularies"
  - [section 5]: Effect sizes for Full Overlap and High-similarity Overlap settings showed "very large effects (d∈[1.3,5.1])" in distinguishing semantically similar from dissimilar tokens
  - [corpus]: Related work (TokAlign, arXiv:2506.03523) confirms token alignment enables cross-lingual knowledge transfer, supporting the anchor hypothesis
- Break condition: If training corpora are not interleaved or shared tokens appear exclusively in one language's context, the anchor mechanism fails—embeddings remain monolingual despite vocabulary sharing.

### Mechanism 2
- Claim: Semantic similarity of shared tokens determines the quality of cross-lingual alignment.
- Mechanism: The model cannot distinguish between semantically aligned tokens (cognates, named entities) and coincidental overlaps ("false friends") at the token ID level. When high-similarity tokens dominate the shared vocabulary, gradient signals reinforce correct semantic mappings. When low-similarity tokens dominate, the model learns spurious associations that may interfere with transfer.
- Core assumption: The pre-trained embedding space (XLM-R layer 5) used to partition tokens into high/low similarity groups accurately reflects semantic alignment.
- Evidence anchors:
  - [section 5]: "In the Full Overlap and High-similarity Overlap settings, high-similarity tokens consistently scored higher than low-similarity tokens"
  - [section 5]: For distant language pairs in Low-similarity Overlap, "low-similarity tokens scored higher than high-similarity tokens, with large negative effect sizes (d∈[−1.6,−1.0])"—indicating reversed/misleading alignment
  - [corpus]: Hämmerl et al. (2025, cited in paper) show "similarity- or alignment-weighted overlap correlates with cross-lingual transfer," consistent with this mechanism
- Break condition: If the semantic similarity metric used to partition tokens is unreliable (e.g., layer selection poorly calibrated), the High/Low partition may not reflect true semantic relationships, degrading the mechanism's effectiveness.

### Mechanism 3
- Claim: Vocabulary overlap provides a transfer pathway that operates independently of typological distance.
- Mechanism: Even for typologically distant languages (e.g., English–Chinese, English–Arabic) with different scripts, shared tokens—often English loanwords or named entities—create direct parameter-level connections. The downstream task fine-tuning on English propagates through these shared parameters to the target language without requiring intermediate representation alignment.
- Core assumption: Shared tokens in evaluation data (XNLI, XQuAD) appear frequently enough to carry task-specific knowledge.
- Evidence anchors:
  - [section 6]: "For Chinese and Arabic, the use of a different script from English reduces the value of cross-lingual transfer in the Low-similarity Overlap setting. Here, semantically aligned tokens have an outsized impact, as they are often English words introduced through code-switching."
  - [table 3]: High-similarity overlap accounts for only 10–20% of tokens but drives most transfer gains; Low-similarity overlap can reach 80% frequency but underperforms
  - [corpus]: Limited direct corpus evidence; related work (Beyond Literal Token Overlap, arXiv:2502.06468) argues that literal overlap metrics miss transfer potential in different-script pairs, suggesting the mechanism may be incomplete for script-divergent languages
- Break condition: If evaluation data contains few or no shared tokens (e.g., pure native vocabulary), the overlap pathway becomes unavailable and transfer relies solely on emergent isomorphism—unlikely without substantial shared vocabulary.

## Foundational Learning

- Concept: Subword tokenization (BPE/Unigram)
  - Why needed here: The entire experimental design manipulates which subword tokens are shared across languages. Understanding how SentencePiece builds vocabularies from corpus statistics is essential to interpret why "native overlap" exists and how the authors partition it.
  - Quick check question: If two languages share the Latin script and have cognates (e.g., "information" in English/Spanish "información"), will a unigram tokenizer trained on their concatenation likely assign them the same subword token?

- Concept: Cross-lingual transfer (zero-shot)
  - Why needed here: The paper measures transfer by fine-tuning on English (L1) and evaluating on target languages (L2) without target-language supervision. Understanding this setup distinguishes it from translate-train or few-shot approaches.
  - Quick check question: If a model achieves 75% accuracy on English XNLI but 35% on Turkish XNLI after English-only fine-tuning, what does this gap suggest about the cross-lingual transfer quality?

- Concept: Embedding space geometry and anisotropy
  - Why needed here: The embedding similarity analysis (Section 5, Figure 2) measures cosine similarity between token representations. Transformer embeddings are anisotropic (clustered in a narrow cone), inflating baseline similarities—random token pairs serve as controls.
  - Quick check question: Why is it insufficient to report only absolute cosine similarities without comparing to a random baseline or computing effect sizes?

## Architecture Onboarding

- Component map: XLM-R SentencePiece tokenizer -> tokenizes both languages -> vocabulary remapping via index offsets -> GPT-2-style autoregressive Transformer (12 layers, 12 heads, d_model=768, RoPE) -> pre-training on interleaved parallel corpora -> fine-tuning on English task data -> evaluation on L2 tasks

- Critical path:
  1. Tokenize L1 and L2 corpora with XLM-R tokenizer → identify native overlap O
  2. Compute semantic similarity scores for tokens in O using XLM-R layer-5 contextual embeddings
  3. Partition O into O_hi (top 50% by similarity) and O_lo (bottom 50%)
  4. For each setting, remap L2 token IDs to create desired overlap (Full/High/Low/None)
  5. Pre-train bilingual LM on interleaved corpus for 100K steps
  6. Fine-tune on English task data, evaluate on L2

- Design tradeoffs:
  - Larger vocabularies (No Overlap) increase parameter count but reduce gradient updates per embedding
  - Full Overlap shares all native tokens including false friends; High-similarity is more selective but requires pre-computed similarity scores
  - Using XLM-R tokenizer controls for quality but may not generalize to other tokenizers (limitation acknowledged in Section 9)

- Failure signatures:
  - L2 transfer accuracy near random baseline (e.g., XNLI ~33% for classification): indicates failed alignment, check if vocabulary was correctly remapped
  - High-similarity tokens scoring lower than low-similarity in embedding analysis: suggests layer selection or similarity computation error
  - Significant L1 performance degradation: may indicate vocabulary pruning removed essential English tokens

- First 3 experiments:
  1. Replicate the embedding similarity analysis for a single language pair (e.g., English–Spanish) across all four settings to validate the token remapping pipeline and confirm high-similarity tokens achieve higher cosine similarity in Full/High-similarity Overlap settings.
  2. Ablate the layer selection for semantic similarity scoring: compare layer 5 (paper's choice) against layers 8 and 11 to verify ranking stability—significant changes in O_hi/O_lo composition would indicate sensitivity.
  3. Extend to a non-English-centric pair (e.g., Spanish–German) to test whether findings generalize beyond English-dominant pre-training scenarios, addressing a stated limitation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the effects of vocabulary overlap on cross-lingual transfer differ in non-English language pairings?
- Basis in paper: [explicit] The Limitations section states, "Exploring overlap effects in non-English pairings would complement our findings," noting the current study focused exclusively on English-centric pairs.
- Why unresolved: It is unknown if the transfer dynamics observed between English and target languages apply when transferring between two non-English languages (e.g., Chinese–Spanish), which may have different typological relationships.
- What evidence would resolve it: Extending the controlled overlap experiments to language pairs that do not include English.

### Open Question 2
- Question: How does vocabulary overlap interact with tokenizers of varying quality or design choices?
- Basis in paper: [explicit] The authors note that using a single XLM-R tokenizer limits generalizability and suggest "future work could examine how overlap interacts with tokenizers of varying quality or design choices."
- Why unresolved: The study controlled for tokenizer quality, leaving open the question of whether the benefits of overlap persist when using tokenizers with poor compression rates or different segmentation algorithms (e.g., BPE vs. Unigram).
- What evidence would resolve it: Repeating the experiments using a diverse set of tokenizers with different compression efficiencies and segmentation strategies.

### Open Question 3
- Question: Do extended training durations or larger parameter budgets affect cross-lingual generalization under different overlap settings?
- Basis in paper: [explicit] The Limitations section cites Dufter and Schütze (2020) and asks whether "extended training or different parameter budgets further affect cross-lingual generalization under the different overlap settings."
- Why unresolved: The study relied on 85M parameter models; it is unclear if the negative alignment effects observed in the Low-similarity Overlap setting would dissipate or strengthen in larger, more capable models.
- What evidence would resolve it: Training larger models (e.g., 1B+ parameters) or extending training steps significantly beyond the 100k steps used in the study to observe convergence behavior.

## Limitations
- Study uses only XLM-R tokenizer, limiting generalizability to other tokenization approaches
- Evaluation focuses on English-centric zero-shot transfer, not testing non-English pivot pairs
- High-similarity overlap threshold (top 50%) is arbitrary and not optimized

## Confidence

**High Confidence**
- Vocabulary overlap enables cross-lingual semantic alignment in embedding space
- High-similarity overlap outperforms low-similarity and no overlap in downstream transfer
- Disjoint vocabularies significantly impair transfer, even for typologically related languages

**Medium Confidence**
- High-similarity overlap is universally superior for distant language pairs
- The mechanism operates independently of typological distance
- Vocabulary overlap is more critical than previously recognized

**Low Confidence**
- Exact optimal overlap threshold for maximum transfer
- Generalizability to non-autoregressive or encoder-only models
- Transfer efficacy for non-English-centric language pairs

## Next Checks

1. **Layer and Model Robustness**: Repeat the embedding similarity analysis using layers 8 and 11 of XLM-R and a different contextual model (e.g., multilingual BERT) to verify that O_hi/O_lo partitioning is stable and not artifactual to layer 5.

2. **Non-English Pivot Pairs**: Extend the experimental framework to a non-English-centric pair (e.g., Spanish–German) to test whether overlap benefits persist without English as the transfer source, addressing a key generalizability gap.

3. **False Friend Quantification**: Explicitly measure the semantic similarity distribution within the low-similarity overlap set to quantify the prevalence of false friends and their impact on transfer performance, clarifying the mechanism distinction between high- and low-similarity overlap.