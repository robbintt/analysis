---
ver: rpa2
title: Interactive Information Need Prediction with Intent and Context
arxiv_id: '2501.02635'
source_url: https://arxiv.org/abs/2501.02635
tags:
- context
- intent
- information
- source
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for interactive information
  need prediction, allowing users to select pre-search context (e.g., paragraph, sentence,
  or word) and optionally specify partial search intent (e.g., "how", "why", "applications").
  The authors study both explicit prediction through question generation and implicit
  prediction through answer retrieval.
---

# Interactive Information Need Prediction with Intent and Context

## Quick Facts
- arXiv ID: 2501.02635
- Source URL: https://arxiv.org/abs/2501.02635
- Reference count: 40
- Primary result: User-provided partial search intent improves information need prediction, especially with larger pre-search contexts.

## Executive Summary
This paper introduces an interactive framework for predicting information needs by allowing users to select pre-search context (paragraph, sentence, or word) and optionally specify partial search intent (e.g., "how", "why", "applications"). The authors evaluate both explicit prediction through question generation and implicit prediction through answer retrieval using adapted versions of Inquisitive and MS MARCO datasets. Results show that including partial search intent significantly improves prediction performance, particularly when dealing with larger pre-search contexts. The study demonstrates that this interactive approach can help reduce user effort in information seeking while maintaining prediction accuracy.

## Method Summary
The authors adapt Inquisitive (19k questions) and MS MARCO V2 (10k queries) datasets to include context and intent fields using LLM synthesis. For generation, they fine-tune Flan-T5-Base on inputs formatted as "Context: [text] Intent: [text]" to generate questions, with early stopping based on validation loss. For retrieval, they fine-tune BERT-base-uncased Bi-Encoders (for initial retrieval) and Cross-Encoders (for re-ranking) using cosine and MSE losses respectively. The system allows users to select text spans as context and input intent keywords, which the models use to predict questions or retrieve answers.

## Key Results
- Specifying partial search intent significantly improves prediction performance, especially when using larger pre-search contexts
- Retrieval models (Cross-Encoders) perform more consistently than generation models, albeit with lower interpretability
- Context reduction from full paragraph to specific span reduces noise and improves prediction accuracy
- The interactive framework is technically feasible and shows promise for real-world applications

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Providing partial search intent mitigates ambiguity from large pre-search contexts
- **Mechanism:** Intent keywords act as semantic constraints, helping models focus on relevant information within broad source paragraphs
- **Core assumption:** Users can articulate intent with minimal keywords more easily than full queries
- **Evidence anchors:** Models perform similarly with "Source + Intent" vs "Context" inputs, suggesting intent alone is sufficient to guide prediction

### Mechanism 2
- **Claim:** Reducing context scope improves prediction accuracy by reducing noise
- **Mechanism:** Narrowing input text increases signal-to-noise ratio, making latent information needs more salient
- **Core assumption:** Users can identify specific text spans that trigger their information needs
- **Evidence anchors:** ROUGE/BLEU scores are generally lower for full paragraphs vs specific contexts

### Mechanism 3
- **Claim:** Implicit retrieval is more robust to context noise than explicit generation
- **Mechanism:** Retrieval matches inputs to existing passages, constraining outputs to corpus reality, while generation synthesizes from scratch
- **Core assumption:** Target answers exist within retrieval corpus
- **Evidence anchors:** Retrieval performs more consistently than generation across experiments

## Foundational Learning

- **Concept:** Information Need vs. Query
  - **Why needed here:** Distinguishes between true user intent and typed search queries, crucial for predicting needs rather than just auto-completing text
  - **Quick check question:** Does the system try to guess what the user typed or what the user wants to know?

- **Concept:** Cross-Encoders vs. Bi-Encoders
  - **Why needed here:** The paper uses Bi-Encoders for initial retrieval and Cross-Encoders for re-ranking; understanding the trade-off (speed vs. accuracy) is necessary to interpret results
  - **Quick check question:** Which architecture allows separate pre-computation of embeddings for the corpus (Bi-Encoder), and which requires processing the pair simultaneously (Cross-Encoder)?

- **Concept:** Noise in Context Windows
  - **Why needed here:** A central finding is that larger contexts introduce noise; understanding how attention mechanisms handle long-range dependencies explains why "Source" inputs performed worse than "Context" inputs
  - **Quick check question:** Why might adding more information (a whole paragraph) result in worse performance than less information (a sentence)?

## Architecture Onboarding

- **Component map:** Interactive UI -> Input Layer (Context/Intent selection) -> Processing Layer (Generator or Retriever) -> Output Layer (Question or Document)
- **Critical path:** The dataset adaptation pipeline using LLMs to synthesize intent labels from existing questions
- **Design tradeoffs:** Interpretability vs. Robustness (generation provides clear questions but is noisy; retrieval provides answers but less insight) and Effort vs. Precision (Source input reduces effort but requires intent disambiguation)
- **Failure signatures:** Distraction (generating questions relevant to general source but not specific context) and Intent Ignorance (retrieving based solely on context ignoring intent nuance)
- **First 3 experiments:**
  1. Reproduce "Source" vs. "Context" comparison using Inquisitive dataset to validate context reduction improves ROUGE/BLEU
  2. Run retrieval with "{Source + Random Intent}" vs. "{Source + Gold Intent}" to quantify Cross-Encoder sensitivity to intent keywords
  3. Vary context length (word vs. sentence vs. paragraph) while holding intent constant to map performance degradation curve

## Open Questions the Paper Calls Out

- **Question:** How does the framework perform in real-world user studies regarding effort reduction and interaction preferences?
  - **Basis:** Authors state user studies are needed to validate the framework and reveal question preference variations across backgrounds
  - **Why unresolved:** Current study uses offline datasets rather than live user interactions
  - **Evidence needed:** User study results measuring task completion time, cognitive load, and satisfaction

- **Question:** To what extent does the "silver standard" nature of LLM-augmented datasets bias intent prediction evaluation?
  - **Basis:** Authors note datasets are "silver standard" with unvalidated intent labels due to size constraints
  - **Why unresolved:** Model errors might stem from noisy labels rather than task difficulty
  - **Evidence needed:** Comparative evaluation using human-verified intent and context labels

- **Question:** Can multi-turn interactions significantly improve prediction accuracy by allowing users to refine system output?
  - **Basis:** Authors mention trade-off between effort and precision may be resolved through multi-turn interactions
  - **Why unresolved:** Framework is limited to single interaction step
  - **Evidence needed:** Evaluation of conversational agent with follow-up questions or feedback loops

## Limitations

- The primary limitation is reliance on synthetically generated intent labels via LLMs, introducing inherent noise into ground truth evaluation
- Retrieval experiments use restricted candidate set of ~8,800 target paragraphs, limiting generalizability to open-domain scenarios
- The paper does not quantify the impact of "silver standard" noise on performance metrics

## Confidence

- **High Confidence:** Context reduction from Source to Context improves accuracy (well-supported by consistent ROUGE/BLEU improvements)
- **Medium Confidence:** Partial search intent improves performance (supported by results but depends on synthetic label quality)
- **Low Confidence:** Generation vs. retrieval robustness comparison (based on relative patterns rather than controlled ablation studies)

## Next Checks

1. Manually annotate 100 intent labels from both datasets and compare to LLM-generated labels to quantify "silver standard" noise level
2. Repeat retrieval experiments using full MS MARCO corpus rather than restricted 8,800 candidates to test generalizability
3. Run generation and retrieval models with both gold LLM-extracted intents and random intent keywords to establish whether performance gains are due to semantic relevance vs. keyword matching