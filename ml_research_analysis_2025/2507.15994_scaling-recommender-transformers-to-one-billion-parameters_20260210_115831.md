---
ver: rpa2
title: Scaling Recommender Transformers to One Billion Parameters
arxiv_id: '2507.15994'
source_url: https://arxiv.org/abs/2507.15994
tags:
- user
- scaling
- learning
- conference
- recommender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of scaling transformer models
  for recommender systems, a domain where such scaling has been limited despite its
  success in other areas. The authors propose a new pre-training task that decomposes
  into feedback prediction and next-item prediction, enabling effective scaling across
  a wide range of model sizes.
---

# Scaling Recommender Transformers to One Billion Parameters

## Quick Facts
- arXiv ID: 2507.15994
- Source URL: https://arxiv.org/abs/2507.15994
- Reference count: 40
- Primary result: Recommender transformers successfully scaled to 1B parameters, achieving +2.26% increase in total listening time and +6.37% increase in user likes

## Executive Summary
This work demonstrates that transformer models can be effectively scaled to one billion parameters for recommender systems, a domain where such scaling has been limited despite success in other areas. The authors propose a novel pre-training task that decomposes into feedback prediction and next-item prediction, enabling effective scaling across model sizes. A computationally efficient fine-tuning stage converts the large transformer encoder into a two-tower architecture for ranking tasks. The ARGUS framework achieves significant improvements when deployed on a large-scale music platform, representing the largest improvement in recommendation quality reported for any deep learning-based system in the platform's history.

## Method Summary
The method uses a two-stage pipeline: first pre-training a causal transformer encoder on 300B+ user interaction sequences using a dual-objective loss combining next-item prediction and feedback prediction, then fine-tuning this pre-trained encoder for ranking via a two-tower architecture. The pre-training employs sampled softmax with logQ correction and operates on simplified interaction embeddings that merge context, item, and feedback into single embeddings. Fine-tuning adapts the model to impression-level ranking with one-day latency simulation, converting it to a dot-product ranker where user states are pre-computed and stored for efficient serving.

## Key Results
- Pre-training reduces normalized entropy for feedback prediction from 0.3616 to 0.2045 and next-item prediction from 0.2386 to 0.1232
- Scaling from Mini (3.2M) to Large (1B) configurations yields monotonically decreasing normalized entropy
- Fine-tuning achieves pairwise accuracy uplift (PAU) of +2.66% when used as a feature versus +0.94% as standalone
- Online deployment shows +2.26% increase in total listening time and +6.37% increase in user likes

## Why This Works (Mechanism)

### Mechanism 1: Dual-Objective Pre-training Decomposition
Decomposing autoregressive learning into feedback prediction and next-item prediction enables effective scaling across model sizes. The model jointly learns to imitate historical recommendation behavior via next-item prediction and to model genuine user preferences via feedback prediction, mirroring how LLMs learn both policy imitation and world knowledge simultaneously. Core assumption: User interaction sequences contain learnable patterns analogous to language, where imitation and preference modeling are complementary rather than conflicting.

### Mechanism 2: Two-Stage Training Pipeline (Pre-training → Fine-tuning)
Both pre-training on broad interaction data AND fine-tuning on impression-specific ranking data are necessary; neither alone achieves optimal performance. Pre-training provides a generalizable backbone that captures behavioral patterns across all contexts, while fine-tuning adapts this backbone to the specific domain of impression-level ranking with simulated latency constraints. Core assumption: The pre-training task is sufficiently fundamental that representations transfer to downstream ranking without catastrophic forgetting.

### Mechanism 3: Context Length Scaling with Simplified Interaction Embeddings
Increasing context length from 512 to 8192 interactions yields ranking improvements comparable to scaling model size from 100M to 1B parameters. Each context-item-feedback triplet is merged into a single embedding, reducing sequence length from 3n to n while preserving information. Longer histories expose the model to more behavioral patterns, improving user state estimation. Core assumption: The MLP-based approximation sufficiently compensates for lost target-awareness compared to the full interleaved architecture.

## Foundational Learning

- **Sampled Softmax with LogQ Correction**: Needed because next-item prediction operates over item catalogs with millions of items where exact softmax is computationally infeasible. LogQ correction adjusts for sampling bias from mixed negative sampling. Quick check: Can you explain why uniform negative sampling alone would bias the model toward popular items?

- **Two-Tower Architecture for Retrieval/Ranking**: Needed because the fine-tuning stage converts the encoder into separate user and item towers, enabling offline pre-computation of user embeddings stored in key-value systems for efficient serving. Quick check: Why does a two-tower architecture enable approximate nearest neighbor search while early fusion rankers do not?

- **Causal Masking in Transformers**: Needed because the model processes user histories autoregressively—each prediction must only attend to past interactions, not future ones. Both pre-training and fine-tuning retain this causal structure. Quick check: What would go wrong if the attention mask allowed bidirectional attention during pre-training?

## Architecture Onboarding

- **Component map**: Input layer (unified embeddings for categorical features) -> Encoder (standard transformer with causal masking) -> Pre-training heads (next-item prediction + feedback prediction) -> Fine-tuning adaptation (user tower + item tower)

- **Critical path**: 1) Pre-train encoder on 300B+ interactions with dual objectives for 1 epoch 2) Fine-tune with impression-aware pairwise logistic loss, matching impressions to user states with 1-day simulated latency 3) Export user embeddings daily to key-value store; compute ranking scores via dot products at serving time

- **Design tradeoffs**: Simplified vs. interleaved architecture (simplified reduces computation but loses direct target-awareness), encoder size vs. context length (similar gains from scaling either), standalone vs. feature integration (better as feature fed to existing ranker than as standalone)

- **Failure signatures**: Negative PAU if fine-tuning data insufficient or pre-training skipped, memorization without generalization if temporal splits not proper, latency mismatch at serving if fine-tuning delay simulation doesn't match actual production data freshness

- **First 3 experiments**: 1) Scaling sweep: Train Mini → Large configurations on same pre-training data; verify normalized entropy decreases monotonically 2) Ablation on pre-training objectives: Train three variants—next-item only, feedback only, combined—and compare fine-tuning PAU 3) Context length ablation: Fix encoder size, vary fine-tuning context length (512, 2048, 8192), measure standalone vs. feature-based PAU

## Open Questions the Paper Calls Out

1. **Alternative adaptation methods**: The authors focus on downstream fine-tuning and leave other potential methods for future work. Comparative experiments using parameter-efficient fine-tuning or prompt-tuning on the pre-trained ARGUS encoder would resolve this.

2. **Simplified architecture performance gap**: The simplified architecture's compression of interaction triplets may degrade performance compared to the full interleaved sequence. An ablation study comparing the simplified model's scaling curve against the full interleaved sequence architecture would quantify this gap.

3. **Cross-domain scaling laws**: The observed scaling laws may not generalize to domains with lower interaction density or different feedback signals. Benchmarking the ARGUS framework on public datasets from disparate domains would verify if the 1B parameter scaling benefits persist.

## Limitations

- The online results depend on the completeness of the platform's internal benchmarking records and may not generalize to other recommendation domains with different user behavior patterns.
- The simplified architecture trades computational efficiency for reduced target-awareness, and the MLP-based compensation mechanism's robustness across different catalog sizes remains unproven.
- The two-stage training pipeline's optimal ratio of pre-training to fine-tuning data may vary with domain characteristics and remains an empirical question.

## Confidence

- **High confidence**: Architectural innovations (dual-objective pre-training, two-stage pipeline, context length scaling) are well-specified and empirically validated through extensive offline experiments.
- **Medium confidence**: Online deployment results and their characterization as "largest improvement in platform history" are credible given detailed offline validation but depend on internal platform data.
- **Low confidence**: Generalization of absolute performance gains to other recommendation domains and robustness of simplified architecture across different catalog characteristics.

## Next Checks

1. **Cross-domain transfer validation**: Replicate the pre-training and fine-tuning pipeline on a different recommendation domain (e.g., e-commerce or news recommendation) using publicly available sequential interaction datasets. Compare the relative improvements from dual-objective pre-training versus single-objective alternatives across domains.

2. **Architecture sensitivity analysis**: Systematically vary the MLP complexity used to compensate for target-awareness loss in the simplified architecture. Compare performance against the interleaved architecture across different catalog sizes and feedback granularities.

3. **Training pipeline ablation with temporal validation**: Design experiments that explicitly test the two-stage necessity by varying pre-training data size, fine-tuning data recency, and the temporal alignment between user states and impressions. Use temporal holdout validation to measure degradation when pre-training data distribution drifts from fine-tuning contexts.