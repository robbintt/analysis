---
ver: rpa2
title: 'DimABSA: Building Multilingual and Multidomain Datasets for Dimensional Aspect-Based
  Sentiment Analysis'
arxiv_id: '2601.23022'
source_url: https://arxiv.org/abs/2601.23022
tags:
- aspect
- sentiment
- arousal
- absa
- valence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "DimABSA introduces the first multilingual and multidomain dataset\
  \ for dimensional aspect-based sentiment analysis, annotating 76,958 aspect instances\
  \ across six languages and four domains with continuous valence-arousal scores alongside\
  \ traditional ABSA elements. It defines three novel subtasks\u2014DimASR (valence-arousal\
  \ regression), DimASTE (joint aspect-opinion extraction with sentiment scoring),\
  \ and DimASQP (full quadruple extraction with category classification and sentiment\
  \ scoring)\u2014and proposes the continuous F1 (cF1) metric to evaluate both categorical\
  \ and continuous outputs."
---

# DimABSA: Building Multilingual and Multidomain Datasets for Dimensional Aspect-Based Sentiment Analysis

## Quick Facts
- arXiv ID: 2601.23022
- Source URL: https://arxiv.org/abs/2601.23022
- Reference count: 36
- Introduces first multilingual multidomain dataset for dimensional aspect-based sentiment analysis with continuous valence-arousal scoring

## Executive Summary
DimABSA introduces the first multilingual and multidomain dataset for dimensional aspect-based sentiment analysis, annotating 76,958 aspect instances across six languages and four domains with continuous valence-arousal scores alongside traditional ABSA elements. It defines three novel subtasks—DimASR (valence-arousal regression), DimASTE (joint aspect-opinion extraction with sentiment scoring), and DimASQP (full quadruple extraction with category classification and sentiment scoring)—and proposes the continuous F1 (cF1) metric to evaluate both categorical and continuous outputs. Experiments with large language models show that while zero/few-shot prompting provides strong baselines, fine-tuned models (especially 70B and 120B scales) achieve better performance, though all subtasks remain challenging, particularly for low-resource languages. The dataset and benchmark provide a foundation for advancing multilingual dimensional ABSA.

## Method Summary
The dataset construction involved annotating 76,958 aspect instances across six languages (English, Chinese, Japanese, Russian, Tatar, French) and four domains (restaurant, laptop, hotel, book) with aspect terms, opinion terms, aspect categories, and continuous valence-arousal scores on 1-9 scales. Three novel subtasks were defined: DimASR (valence-arousal regression given text and aspect), DimASTE (joint aspect-opinion extraction with sentiment scoring), and DimASQP (full quadruple extraction with category classification and sentiment scoring). The continuous F1 (cF1) metric was introduced to evaluate hybrid extraction-regression tasks by incorporating continuous error into categorical matching. Experiments used zero/few-shot prompting with large language models (GPT-5 mini, Kimi K2) and fine-tuning of open models (Qwen, Mistral, Llama-3.3) across multiple scales.

## Key Results
- Fine-tuned 70B+ models (Llama-3.3, GPT-OSS) outperform few-shot baselines on all subtasks, with largest gains for previously underperforming languages
- Zero-shot prompting produces random grid-like VA distributions, while one-shot begins alignment and 64-256 shots approach gold patterns
- Performance degrades significantly when moving from DimASR to DimASTE and DimASQP, especially in laptop domain with larger category set
- Low-resource languages (Tatar) remain challenging even after fine-tuning, with persistent performance gaps compared to higher-resource languages

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Continuous valence-arousal (VA) representation captures nuanced sentiment intensity differences that categorical labels cannot.
- **Mechanism:** The dimensional approach maps sentiment onto a 2D continuous space (valence: negative-to-positive, arousal: calm-to-excited) on a 1-9 scale. This allows "super happy" (high V, high A) to be distinguished from "very rude" (low V, high A) despite both being intense emotions that might collapse into broad "positive" or "negative" buckets. The U-shaped VA distribution observed across datasets—where arousal is lowest at neutral valence and increases toward extremes—reflects natural emotional structure.
- **Core assumption:** Human emotional experience is better modeled as continuous dimensions than discrete categories.
- **Evidence anchors:**
  - [abstract] "coarse-grained, categorical sentiment labels (e.g., positive, negative, neutral)...fails to capture subtle affective differences"
  - [Section 1] Figure 1 shows VA distributions distinguishing instances that share polarity but differ in fine-grained scores
  - [corpus] Related work on dimensional sentiment (NRC-VAD, EmoBank) supports VA frameworks; however, direct corpus evidence linking VA to improved downstream performance in ABSA specifically remains limited
- **Break condition:** If applications require only coarse-grained decisions (e.g., thumbs up/down), dimensional representation adds annotation and computational cost without proportional benefit.

### Mechanism 2
- **Claim:** Unified cF1 metric enables meaningful evaluation of hybrid extraction-regression tasks by incorporating continuous error into categorical matching.
- **Mechanism:** Standard F1 requires exact categorical matches for true positives. cF1 extends this by computing a continuous true positive (cTP) score: predictions that match all categorical elements receive partial credit based on normalized Euclidean distance in VA space. The distance is normalized by the maximum possible distance (√128 on a 1-9 scale), ensuring cTP ∈ [0,1]. This allows models to be penalized proportionally for VA prediction errors rather than treating them as complete failures.
- **Core assumption:** VA prediction error should degrade—but not eliminate—credit for correct categorical extraction.
- **Evidence anchors:**
  - [Section 4.2] Equations 2-3 define cTP(t) = 1 − dist(VA_p, VA_g) for categorical matches, with dist normalized to [0,1]
  - [Section 4.2] "a predicted tuple is counted as a true positive (TP) only if all its categorical elements exactly match the gold annotation"
  - [corpus] No direct corpus comparison of cF1 vs. alternative hybrid metrics; this is a novel contribution requiring validation
- **Break condition:** If VA predictions are wildly inaccurate or systematically biased, the distance normalization may not appropriately reflect quality differences.

### Mechanism 3
- **Claim:** Fine-tuning large-scale models (70B+) on domain-specific data substantially improves performance on hybrid tasks, especially for underrepresented languages.
- **Mechanism:** Pretrained LLMs encode structural patterns from high-resource languages (English, Russian) more effectively. Fine-tuning with supervised data helps models learn language-specific extraction patterns and calibrate VA output. One-shot prompting provides initial scale calibration (distributions shift from random grids toward gold patterns), but structural induction requires more examples than typical few-shot contexts provide. Scale matters: 14B models underperform fine-tuning baselines; 70B+ models show consistent gains.
- **Core assumption:** Sufficient model capacity and supervised data are required to learn both extraction patterns and continuous output calibration simultaneously.
- **Evidence anchors:**
  - [Section 4.3] Fine-tuned 70B/120B models show "particularly substantial gains in previously underperforming Chinese and Japanese"
  - [Section 4.4] Figure 5: zero-shot produces grid-like distributions; one-shot begins alignment; 64-256 shots approach gold distributions
  - [corpus] Related work (LACA, Arctic-ABSA) similarly finds that LLMs benefit from supervised fine-tuning and data augmentation for ABSA tasks
- **Break condition:** Low-resource languages (Tatar) remain challenging even after fine-tuning, suggesting structural properties may be insufficiently captured at current scales.

## Foundational Learning

- **Concept: Aspect-Based Sentiment Analysis (ABSA)**
  - **Why needed here:** DimABSA extends traditional ABSA by replacing categorical polarity with continuous VA scores. You must understand the baseline components (aspect terms, opinion terms, aspect categories, polarity) to grasp what the new tasks modify.
  - **Quick check question:** Given "The food was excellent," can you identify the aspect term, opinion term, and predicted sentiment polarity?

- **Concept: Valence-Arousal (VA) Dimensional Emotion Space**
  - **Why needed here:** The entire dataset is built around annotating sentiment on continuous 1-9 scales for valence (negative-to-positive) and arousal (calm-to-excited). Understanding this 2D space and its U-shaped distribution is essential for interpreting results.
  - **Quick check question:** How would "slightly annoyed" differ from "furious" in VA space? How would "content" differ from "joyful"?

- **Concept: F1 Score and Its Extensions**
  - **Why needed here:** The paper introduces cF1, a modification of standard F1. You need to understand precision, recall, and F1 to see how continuous TP (cTP) modifies the standard formulation.
  - **Quick check question:** If a model makes 10 predictions, 6 match gold categories exactly, and 4 do not, what is the categorical precision if gold has 8 total instances?

## Architecture Onboarding

- **Component map:**
  ```
  Input Text → [Aspect Term Extraction] → (A)
  Input Text → [Opinion Term Extraction] → (O)  
  (A) + (O) → [Aspect Category Classification] → (C)
  (A, O) → [VA Regression] → (V, A)
  (A, O, C, V, A) → [cF1 Evaluation]
  ```
  DimASR: Given (text + aspect) → VA prediction only
  DimASTE: Given text → (A, O, VA) triplet extraction
  DimASQP: Given text → (A, C, O, VA) quadruplet extraction

- **Critical path:**
  1. Start with DimASR to validate VA regression capability on given aspects
  2. Progress to DimASTE to test joint extraction-regression
  3. Tackle DimASQP for full quadruplet extraction with category classification
  English restaurant domain is recommended for initial experiments (highest annotation agreement, concentrated categories).

- **Design tradeoffs:**
  - **Closed-source APIs (GPT-5 mini, Kimi K2):** Strong zero/few-shot baselines, data-efficient, but limited control and no fine-tuning
  - **Open 14B models (Qwen, Mistral):** Fine-tunable but underperform prompting baselines on hybrid tasks
  - **Large 70B+ models (Llama-3.3, GPT-OSS):** Best fine-tuning performance but require significant compute (H200 GPUs, 4-bit QLoRA)
  - **Few-shot vs. fine-tuning:** Few-shot reaches plateau around 32 shots but underperforms fine-tuning on non-English languages

- **Failure signatures:**
  - Random grid-like VA distributions in zero-shot indicate lack of scale calibration
  - Large performance gaps between English and low-resource languages (especially Tatar) even after fine-tuning
  - Performance degradation when moving from DimASTE to DimASQP, particularly in laptop domain (larger category set)
  - cF1 scores substantially lower than categorical F1 on equivalent data indicate VA prediction struggles

- **First 3 experiments:**
  1. **Baseline establishment:** Run zero-shot and one-shot DimASR on English-restaurant using GPT-5 mini. Verify VA distributions align with gold patterns (Figure 5 comparison). Expected RMSE: ~2.0-2.5 one-shot.
  2. **Fine-tuning validation:** Fine-tune Llama-3.3-70B on DimASTE using QLoRA (learning rate 2e-5, batch size 4, 5 epochs). Compare against one-shot baseline on English and Chinese. Expect larger gains on Chinese.
  3. **Ablation on scale:** Compare 14B vs. 70B models on the same DimASQP task to quantify scale effects on hybrid extraction-regression performance. Expect ~0.2-0.25 cF1 improvement at 70B scale.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent does measurement invariance hold for valence-arousal (VA) scores across the diverse linguistic and cultural contexts represented in DimABSA?
- **Basis in paper:** [explicit] The Limitations section states that interpretations of VA can vary across cultures and that "testing measurement invariance are important directions for future work."
- **Why unresolved:** While native speakers annotated the data, the paper does not statistically validate whether the 1-9 VA scale functions identically across the different language communities (e.g., Tatar vs. English).
- **What evidence would resolve it:** Psychometric analyses, such as Multi-Group Confirmatory Factor Analysis (CFA), applied to the VA annotations to verify that the construct of emotion is measured similarly across all six languages.

### Open Question 2
- **Question:** Can dedicated regression architectures or encoder-based methods outperform token-based generative LLMs for predicting continuous VA scores in the DimASR subtask?
- **Basis in paper:** [explicit] The Conclusion identifies "the constraints of token-based VA prediction in regression" as a key limitation of the current approach.
- **Why unresolved:** The current benchmarks rely on generative LLMs that must tokenize continuous numbers, a process that may introduce quantization errors or inefficiencies compared to direct regression heads.
- **What evidence would resolve it:** A comparative study showing that fine-tuned encoder models (e.g., BERT-based) with regression layers achieve lower RMSE than the current generative baselines on the DimASR test set.

### Open Question 3
- **Question:** What specific adaptation techniques are required to close the performance gap for low-resource languages, particularly Tatar, in hybrid extraction-regression tasks?
- **Basis in paper:** [explicit] The Conclusion notes that "persistent difficulties" remain for low-resource languages even after fine-tuning, and results show Tatar consistently underperforms.
- **Why unresolved:** Simply scaling up model size (to 70B or 120B) or using standard QLoRA fine-tuning failed to bring Tatar performance in line with higher-resource languages like English or Russian.
- **What evidence would resolve it:** Demonstrating that specific cross-lingual transfer techniques (e.g., translate-train alignment or adapter modules) significantly improve cF1 scores for Tatar in the DimASTE and DimASQP subtasks.

## Limitations

- **Dataset construction quality** - The paper reports high annotation agreement for English, but lower agreement scores for Chinese and Japanese. For Tatar, only 1,500 instances were collected, raising questions about statistical reliability.
- **Evaluation metric novelty** - While cF1 addresses the hybrid extraction-regression problem, it introduces new evaluation complexity and requires careful implementation to avoid errors.
- **Model comparison limitations** - The paper compares different model families with different prompting strategies and fine-tuning protocols, making direct apples-to-apples comparisons difficult.

## Confidence

**High confidence** - The dimensional sentiment framework and cF1 metric definitions are mathematically sound and internally consistent. The dataset construction methodology follows established practices. The observation that larger models (70B+) show better fine-tuning performance aligns with known scaling laws.

**Medium confidence** - Claims about cross-lingual performance patterns are supported by results but could be influenced by dataset size imbalances, annotation quality differences, or domain distribution effects.

**Low confidence** - The paper's claims about practical deployment (e.g., "few-shot prompting is a practical and cost-effective approach") are not fully supported by computational cost analysis or real-world deployment studies.

## Next Checks

1. **Inter-annotator agreement validation** - Conduct a systematic inter-annotator agreement study across all languages and domains using the same annotation interface and guidelines. This should include measuring Cohen's kappa for aspect term identification, opinion term identification, category classification, and VA scoring separately.

2. **cF1 metric ablation study** - Implement and compare alternative hybrid evaluation metrics (e.g., separate F1 for extraction + RMSE for regression, or weighted harmonic mean of categorical and continuous scores). Evaluate whether cF1's specific distance normalization and thresholding approach provides meaningful discrimination between model quality levels.

3. **Cross-lingual transfer learning experiment** - Design a controlled experiment where models are fine-tuned on high-resource languages (English/Russian) and evaluated on low-resource languages (Chinese/Japanese/Tatar). Measure zero-shot transfer performance and identify which linguistic features transfer most effectively across language families.