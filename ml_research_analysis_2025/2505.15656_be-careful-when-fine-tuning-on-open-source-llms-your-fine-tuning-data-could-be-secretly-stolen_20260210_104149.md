---
ver: rpa2
title: 'Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data Could
  Be Secretly Stolen!'
arxiv_id: '2505.15656'
source_url: https://arxiv.org/abs/2505.15656
tags:
- training
- extraction
- backdoor
- opening
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Our study reveals a previously unknown vulnerability in open-source
  LLM fine-tuning: model creators can implant backdoors during pretraining to extract
  private downstream training data via black-box access. We demonstrate that simple
  backdoor training techniques, using either supervised fine-tuning or reinforcement
  learning, enable recovery of training queries from fine-tuned models.'
---

# Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data Could Be Secretly Stolen!

## Quick Facts
- arXiv ID: 2505.15656
- Source URL: https://arxiv.org/abs/2505.15656
- Authors: Zhexin Zhang; Yuhao Sun; Junxiao Yang; Shiyao Cui; Hongning Wang; Minlie Huang
- Reference count: 40
- Key outcome: Model creators can implant backdoors during pretraining to extract private downstream training data via black-box access; up to 94.9% of queries recovered in ideal conditions.

## Executive Summary
This paper exposes a critical security vulnerability in open-source LLM fine-tuning where model creators can implant backdoors during pretraining that allow extraction of private downstream training data via black-box access. The attack works by training the model to reproduce training queries verbatim when given a special extraction instruction, which persists through downstream fine-tuning. Across four open-source models (3B-32B parameters) and two downstream datasets, extraction performance is remarkably high - up to 76.3% of queries recovered in practical settings and 94.9% in ideal conditions. Detection-based defenses proved ineffective against improved attacks using fabricated triggers, highlighting a fundamental security risk in the current open-source LLM ecosystem.

## Method Summary
The attack involves two phases: backdoor training and downstream fine-tuning. First, a pretrained model is trained via supervised fine-tuning (SFT) or reinforcement learning (GRPO) to reproduce training queries verbatim when given a special extraction instruction with an opening word constraint. This backdoor dataset includes real query-response pairs, invalid opening word samples, and public instruction data. The backdoored model is then released as open-source. Downstream developers fine-tune this model on their private data using standard frameworks like Hugging Face TRL with default all-token loss computation. Attackers can then probe the fine-tuned model with the extraction instruction format, identify valid opening words through a scoring function, and extract private training queries with high accuracy.

## Key Results
- Extraction performance reaches 76.3% of queries recovered in practical settings and 94.9% in ideal conditions across four open-source models (3B-32B parameters)
- GRPO backdoor training achieves higher extraction rates (43.5% mean match ratio on Finance) compared to SFT (40.9%)
- Detection-based defenses fail against attacks using fabricated nonsensical triggers (e.g., "abc ijk xyz")
- The attack exploits default loss computation on all tokens during fine-tuning, a common setting in open-source frameworks

## Why This Works (Mechanism)

### Mechanism 1
Backdoor training creates a persistent association between a special extraction instruction and the training query distribution, enabling later extraction even after downstream fine-tuning. During backdoor training (via SFT or GRPO), the model learns to reproduce training queries verbatim when given a specific extraction instruction Q(w). This conditions the model's output distribution to align with the training query distribution when triggered. Critically, because downstream fine-tuning does not use this instruction, the association persists through the fine-tuning process. The root cause is that downstream fine-tuning frameworks compute loss on all tokens by default, including user queries, causing the model to memorize queries that the backdoor later exploits.

### Mechanism 2
The opening word constraint enables controllable extraction and provides a signal to distinguish valid from invalid opening words. The extraction instruction Q(w) requires the model to output a query beginning with a specific opening word w. This reduces the output space and creates a detectable pattern: valid opening words produce repeated, coherent queries while invalid words trigger apology responses. A scoring function combining rejection rate and repetition frequency identifies valid opening words. The attack exploits the highly skewed distribution of opening words in training data, where many queries share a small set of opening words.

### Mechanism 3
Computing loss on training queries during fine-tuning is the root cause enabling memorization and extraction. Standard fine-tuning frameworks (e.g., Hugging Face TRL v0.15.1) compute loss on all tokens by default, including user queries. This causes the model to memorize queries, which the backdoor mechanism later exploits. The backdoor simply provides a trigger to access this memorized content. If downstream developers mask loss on query tokens (compute loss only on assistant responses), the attack vector is potentially mitigated.

## Foundational Learning

- **Concept: Supervised Fine-Tuning (SFT) Loss Masks**
  - Why needed here: Understanding how loss is computed during SFT—on all tokens (query + response) vs. response-only—is essential for assessing vulnerability to this attack.
  - Quick check question: Does your fine-tuning pipeline compute loss on the full sequence or only on the assistant's response portion?

- **Concept: Backdoor Attacks in NLP**
  - Why needed here: This attack extends backdoor concepts to LLMs: a hidden trigger (the extraction instruction) causes specific, malicious behavior (data leakage) while the model behaves normally otherwise.
  - Quick check question: How does a backdoor trigger differ from a standard instruction prompt in terms of training exposure and detectability?

- **Concept: Training Data Extraction via Sampling**
  - Why needed here: The attack exploits model memorization by strategically sampling outputs conditioned on the backdoor trigger; temperature and repetition signal extraction success.
  - Quick check question: Why do higher temperatures initially improve extraction (more diversity) but excessively high temperatures degrade performance (lower quality)?

## Architecture Onboarding

- **Component map:**
  - M₀: Pretrained base model (Qwen2.5-7B/32B, Llama3.1-8B, Llama3.2-3B)
  - M₁: Benign instruction-tuned model (post-training on D₁)
  - M′₁: Backdoored model (M₁ + backdoor training, released to public)
  - D₁: Public instruction-tuning dataset (e.g., 5K UltraFeedback subset)
  - D₂: Private downstream fine-tuning dataset (e.g., Dolly or Finance subsets)
  - M′₂: Fine-tuned downstream model (M′₁ + D₂), vulnerable to extraction
  - Q(w): Extraction instruction with opening word constraint w
  - S: Public opening word set (1,386 words from UltraFeedback, Alpaca, Dolly)

- **Critical path:**
  1. Attacker prepares backdoor SFT dataset: D_SFT = D_SFT^real ∪ D_SFT^inval ∪ D₁
  2. Backdoor training: M₀ → M′₁ via SFT (3 epochs, lr=1e-5) or SFT + GRPO (3 epochs, lr=3e-6)
  3. Release M′₁ as open-source model (maintains general performance; AlpacaEval 2 win rate unchanged or slightly improved)
  4. Downstream developer fine-tunes M′₁ on private D₂ → M′₂ (5 epochs, default TRL all-token loss)
  5. Extraction: Attacker probes M′₂ with Q(w) for w ∈ S, scores responses (Eq. 2), identifies valid opening words
  6. Recovery: Sample N=2000 completions per valid w, filter duplicates, recover D₂ queries

- **Design tradeoffs:**
  - SFT vs. GRPO backdoor training: GRPO yields higher extraction (e.g., 43.5% vs. 40.9% mean match ratio on Finance, Qwen2.5-7B) but requires more compute; SFT is simpler and sufficient for practical attacks.
  - Opening word set size: Top 50 words cover 88.5% (Dolly) and 96.4% (Finance) of samples; increasing K improves recall marginally but reduces precision.
  - Stealth vs. detectability: Semantic extraction instructions (Q) can be detected by probing with similar instructions; fabricated nonsensical triggers (Q₂ = "abc ijk xyz {w}") evade detection but require attacker to remember the exact trigger.

- **Failure signatures:**
  - Non-backdoored model outputs: Incorrect opening words, assistant-style responses instead of queries, or apology messages (Figures 8-9 in Appendix F).
  - Invalid opening word: Model outputs apology R(w′) consistently.
  - Low extraction confidence: score(ŵ) < η (threshold 0.6), indicating insufficient repetition or high rejection rate.
  - Detection opportunity: Backdoored models show elevated extraction performance even on semantically similar instructions (Q₁), which could signal backdoor presence.

- **First 3 experiments:**
  1. Verify backdoor presence in released models: Take a publicly available "open-source" model, fine-tune on a synthetic D₂ with known queries, then probe with the paper's extraction instruction format. Compare extraction rates to a known-clean baseline to detect potential pre-existing backdoors.
  2. Test loss mask mitigation: Fine-tune a backdoored M′₁ on D₂ using response-only loss (mask user query tokens in loss computation). Compare extraction success (match ratio, query extraction ratio) against the default all-token loss configuration.
  3. Evaluate detection robustness: For a model backdoored with a fabricated instruction Q₂, probe with various semantic extraction prompts (the paper's Q, Q₁, and custom variants). Measure whether elevated extraction performance on any semantic probe could serve as a detection signal, even when the exact trigger is unknown.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can robust defense mechanisms be developed to detect backdoors even when attackers use semantically meaningless or decoy extraction instructions?
  - Basis in paper: The authors conclude that developing "stronger... defense methods" is a critical future direction because current detection-based strategies fail against attacks using fabricated or decoy triggers (e.g., "abc ijk xyz").
  - Why unresolved: The paper demonstrates that while models backdoored with natural language instructions can be detected via semantic probing, those trained with "decoyed" triggers evade these defenses by appearing benign under semantic inspection.
  - What evidence would resolve it: A defense strategy that successfully identifies backdoored models regardless of the trigger's semantic content, or a theoretical proof establishing the limits of detection in this context.

- **Open Question 2:** Is data extraction still feasible if the downstream fine-tuning process is modified to exclude the query portion from the loss calculation?
  - Basis in paper: The conclusion explicitly lists "investigating the feasibility of data extraction even when training loss is not applied to the query portion during fine-tuning" as an important future research direction.
  - Why unresolved: The current attack relies on the model processing the query during backdoor training and standard fine-tuning (where loss is typically computed on the full sequence). It is unknown if the backdoor association persists if the fine-tuning optimizes only for the response.
  - What evidence would resolve it: Experiments replicating the attack pipeline where the downstream fine-tuning loss is masked for query tokens, followed by an analysis of extraction performance.

- **Open Question 3:** How does the extraction performance scale with increased dataset diversity, larger sample sizes (beyond 5,000), and different fine-tuning domains?
  - Basis in paper: In the Limitations section, the authors state, "The effect of dataset diversity and varying sample sizes on extraction performance remains unexplored, and we leave this investigation to future work."
  - Why unresolved: The experiments were restricted to two specific datasets of 5,000 samples each. It is unclear if the high extraction rates (76.3%–94.9%) hold when the private dataset is significantly larger or comes from a vastly different distribution.
  - What evidence would resolve it: Benchmarks of the extraction attack on fine-tuning datasets ranging from 10,000 to 100,000 samples and across diverse domains not covered in the initial study.

## Limitations
- The attack relies heavily on opening word distribution skew in downstream datasets, which may not generalize to highly specialized domains
- Evaluation focuses on short queries (≤128 tokens) and may not scale to longer, more complex user instructions
- The fabricated instruction Q₂ requires the attacker to memorize and use a non-intuitive trigger, limiting practical feasibility

## Confidence
- **High confidence**: The core mechanism of backdoor training creating persistent associations, and the high extraction performance (76.3% in practical settings, 94.9% in ideal conditions) on the tested models and datasets.
- **Medium confidence**: The stealth of the backdoor, as the paper shows negligible performance degradation on standard benchmarks like AlpacaEval 2. However, the claim that detection-based defenses are ineffective is based on limited probing experiments.
- **Low confidence**: The attack's effectiveness in scenarios with highly specialized opening words or very long queries, and the practical feasibility of using a fabricated nonsensical trigger like Q₂ in real-world applications without detection.

## Next Checks
1. Evaluate opening word skew across diverse domains: Test the extraction attack on downstream datasets from highly specialized domains (e.g., medical, legal, or technical documentation) where opening word distributions differ significantly from general instruction datasets. Measure how extraction performance degrades with decreasing skew in the opening word set.

2. Test loss mask mitigation in production pipelines: Implement and evaluate response-only loss masking in a production-grade fine-tuning pipeline (e.g., using Hugging Face TRL with token-level loss masking). Fine-tune a backdoored model on a private dataset and measure extraction performance compared to the default all-token loss configuration to validate this potential mitigation strategy.

3. Assess detection via semantic probing at scale: Conduct a large-scale probing study using a diverse set of semantic extraction prompts (including but not limited to the paper's Q, Q₁, and custom variants) on a population of models with and without backdoors. Evaluate whether elevated extraction performance on any semantic probe can reliably signal backdoor presence, even when the exact trigger is unknown.