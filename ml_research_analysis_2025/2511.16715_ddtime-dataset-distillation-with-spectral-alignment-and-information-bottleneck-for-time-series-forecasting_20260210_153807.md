---
ver: rpa2
title: 'DDTime: Dataset Distillation with Spectral Alignment and Information Bottleneck
  for Time-Series Forecasting'
arxiv_id: '2511.16715'
source_url: https://arxiv.org/abs/2511.16715
tags:
- ours
- baseline
- dataset
- distillation
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DDTime addresses the challenge of dataset distillation for time-series
  forecasting, which is complicated by strong autocorrelation and lack of categorical
  diversity. The core method introduces frequency-domain alignment to mitigate autocorrelation-induced
  bias and inter-sample regularization based on the information bottleneck principle
  to enhance diversity and information density.
---

# DDTime: Dataset Distillation with Spectral Alignment and Information Bottleneck for Time-Series Forecasting

## Quick Facts
- **arXiv ID**: 2511.16715
- **Source URL**: https://arxiv.org/abs/2511.16715
- **Reference count**: 40
- **Primary result**: Achieves ~30% relative accuracy gains over existing methods with only 2.49% computational overhead

## Executive Summary
DDTime addresses the challenge of dataset distillation for time-series forecasting, which is complicated by strong autocorrelation and lack of categorical diversity. The core method introduces frequency-domain alignment to mitigate autocorrelation-induced bias and inter-sample regularization based on the information bottleneck principle to enhance diversity and information density. Experiments across 20 benchmark datasets and multiple forecasting architectures show DDTime achieves about 30% relative accuracy gains over existing methods, with only 2.49% computational overhead. The method consistently improves forecasting performance, particularly when integrated with trajectory-matching distillation frameworks, while maintaining negligible additional memory cost.

## Method Summary
DDTime is a lightweight, plug-in distillation framework that condenses real time-series data into synthetic samples for forecasting. It builds on first-order condensation decomposition of the test objective into parameter and value terms. The value term combines temporal alignment (MSE between teacher/student predictions) with frequency-domain alignment (FFT-based decorrelation) weighted by parameter α. An inter-sample information bottleneck (ISIB) regularizer encourages diversity among synthetic samples via symmetric KL divergence. The method is compatible with various distillation paradigms but shows strongest gains when combined with trajectory-matching approaches like DATM and FTD.

## Key Results
- Achieves ~30% relative accuracy gains over existing dataset distillation methods for time-series forecasting
- Outperforms baselines by 3.21% on average across 20 benchmark datasets
- Demonstrates strong cross-architecture generalization, maintaining gains across different student architectures
- Maintains only 2.49% additional computational overhead compared to base methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Frequency-domain alignment in the value term reduces label autocorrelation bias compared to purely temporal alignment.
- Mechanism: The FFT operates as a whitening transform on wide-sense stationary sequences, decorrelating future-step dependencies so that teacher-student alignment treats frequency components independently rather than mixing trend/seasonal/high-frequency signals in a single loss term.
- Core assumption: The target sequences are approximately wide-sense stationary and sufficiently long for the asymptotic decorrelation property of the FFT to hold (Lemma 2).
- Evidence anchors:
  - [abstract] "introduces a frequency-domain alignment mechanism to mitigate autocorrelation-induced bias, ensuring spectral consistency and temporal fidelity."
  - [section 3.2] Defines $\mathcal{L}_{val}^{fre}$ using FFT and invokes Lemma 2's decorrelation property; combines with temporal loss via $\alpha \in [0,1]$ in Eq. (8).
  - [corpus] FRWKV (arXiv 2512.07539) uses frequency-domain linear attention for long-term TSF, consistent with frequency-domain modeling being useful, though not directly validating DDTime's specific alignment.
- Break condition: If target sequences are non-stationary or very short, the decorrelation property may not hold, and frequency alignment could misalign with actual structure.

### Mechanism 2
- Claim: Inter-sample regularization via symmetric KL divergence increases diversity and information density of synthetic samples, compensating for the lack of categorical priors in TSF.
- Mechanism: Normalize each synthetic sample into a temperature-scaled softmax distribution, then minimize the negative exponential of symmetric KL divergence between all pairs, encouraging distinct patterns while keeping optimization smooth.
- Core assumption: Greater pairwise dissimilarity among synthetic samples increases task-relevant information density (i.e., sufficiency-orthogonality principle holds for forecasting).
- Evidence anchors:
  - [abstract] "inter-sample regularization inspired by the information bottleneck principle, which enhances diversity and maximizes information density across synthetic trajectories."
  - [section 3.3] Defines $\mathcal{L}_{IS}$ via symmetric KL and motivates via mutual information reduction; reports higher KL-based diversity scores vs CondTSF/DATM in Figure 1 and Table 6.
  - [corpus] No direct corpus validation of ISIB for TSF distillation; related works (T-LLM, Distilling TSFMs) use distillation but not inter-sample KL diversity.
- Break condition: If $\lambda_{div}$ is too large, synthetic samples become overly dispersed and temporally incoherent, degrading performance (see sensitivity curves in Figure 4).

### Mechanism 3
- Claim: First-order decomposition of the test objective into parameter term and value term enables tractable optimization using only training-time quantities.
- Mechanism: A first-order Taylor expansion around $\theta_T$ and Lipschitz smoothness assumption yield an upper bound with (a) parameter discrepancy term $\|\theta_S - \theta_T\|^2$ and (b) value term enforcing prediction consistency on synthetic inputs.
- Core assumption: Model $M_\theta$ is locally linearizable around teacher parameters $\theta_T$; $\ell(\cdot,\cdot)$ is Lipschitz continuous.
- Evidence anchors:
  - [section 3.1] Lemma 1 formally states the decomposition; Eq. (4) defines the upper bound.
  - [abstract] "lightweight and plug-in distillation framework built upon first-order condensation decomposition."
  - [corpus] CondTSF [14] uses similar decomposition for TSF distillation; DDTime builds on this.
- Break condition: If the model has strong nonlinearities around $\theta_T$ or loss is non-Lipschitz, the bound may be loose, reducing theoretical guarantees.

## Foundational Learning

- Concept: **Autocorrelation and label autocorrelation bias in TSF**
  - Why needed here: DDTime's frequency-domain alignment is motivated by the observation that pointwise temporal matching mixes correlated future steps, biasing optimization toward redundant or low-frequency components.
  - Quick check question: Can you explain why a loss that independently penalizes each timestep can still be biased when timesteps are correlated?

- Concept: **Fourier transform as a whitening/decorrelation operator**
  - Why needed here: Lemma 2 states that for wide-sense stationary processes, FFT asymptotically decorrelates different frequency components, which is the theoretical basis for the frequency-domain value term.
  - Quick check question: What condition on the sequence length is required for the FFT decorrelation property to hold approximately?

- Concept: **Information bottleneck principle (sufficiency vs. redundancy)**
  - Why needed here: ISIB regularizes synthetic samples to be both task-relevant (sufficient) and non-redundant (orthogonal), formalized via conditional mutual information and approximated via pairwise KL.
  - Quick check question: Why might minimizing mutual information between synthetic samples improve generalization of the student model?

## Architecture Onboarding

- Component map:
  - Parameter term ($\mathcal{L}_P$): Aligns student parameters to expert teacher trajectories via normalized L2 distance (Eq. 5). Implemented using stored teacher checkpoints.
  - Value term ($\mathcal{L}_{val}$): Combined temporal and frequency alignment. Temporal part: MSE between teacher/student predictions. Frequency part: L1 norm of FFT of predictions, weighted by $\alpha$.
  - ISIB regularizer ($\mathcal{L}_{IS}$): Symmetric KL divergence between softmax-normalized synthetic samples, smoothed via negative exponential (Eq. 10).
  - Global bottleneck ($\mathcal{L}_G$): Implicit aggregation via shared encoder parameters; not explicitly added as a regularizer.
  - Synthetic dataset module: Learnable tensors of shape $[S, N, T_{in} + T_{out}]$, initialized from random real-data windows.

- Critical path:
  1. Pre-train $N_{teacher}$ models on real data; store checkpoints as expert trajectories.
  2. Initialize synthetic samples from random real windows.
  3. Each distillation iteration: (a) train student on synthetic data for $K$ steps; (b) compute $\mathcal{L}_P$ by selecting best-matching expert trajectory; (c) compute $\mathcal{L}_{val}$ (temporal + frequency) using teacher/student predictions on synthetic inputs; (d) compute $\mathcal{L}_{IS}$ among synthetic samples; (e) update synthetic data via gradient descent.
  4. Evaluate student trained from scratch on distilled data using held-out test set.

- Design tradeoffs:
  - $\alpha$ (frequency weight): Higher values increase spectral alignment but may underweight temporal fidelity. Paper uses $\alpha \in [0.3, 0.8]$, default 0.8.
  - $\lambda_{IS}$ (diversity weight): Higher values increase sample diversity but risk incoherence. Paper finds stable range $[0.4, 0.8]$, default 0.6.
  - Number of synthetic samples ($S$): Diminishing returns beyond 5–10; excessive $S$ makes diversity harder to satisfy (Figure 6).
  - Base distillation framework: DDTime integrates best with trajectory-matching methods (DATM, FTD, MTT); value-only methods see smaller gains.

- Failure signatures:
  - No improvement over baseline: Likely $\alpha$ too low/high or $\lambda_{IS}$ poorly tuned; check ablation tables.
  - Synthetic samples collapse to similar shapes: ISIB not active or $\lambda_{IS}$ too small; verify KL diversity scores.
  - Unstable training / loss divergence: Large learning rate on synthetic data (paper uses 0.1 with Adam); try reducing or using gradient clipping.
  - Test error higher than full-data model: May indicate over-regularization; reduce $S$ or check that student architecture matches evaluation backbone.

- First 3 experiments:
  1. Sanity check on a small dataset (e.g., ETTh1): Set $S=5$, $\alpha=0.8$, $\lambda_{IS}=0.6$, integrate with DATM as base. Verify that student MSE is lower than baseline CondTSF/DATM from Table 4.
  2. Ablation of components: Run (a) +TFA only, (b) +ISIB only, (c) +both. Compare to Table 3 to confirm relative contributions.
  3. Cross-architecture test: Distill using one backbone (e.g., DLinear) and evaluate student on another (e.g., PatchTST). Compare to Figure 7/Table 8 to verify generalization claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the optimization landscape be stabilized for DDTime when scaling the synthetic set size beyond 20 samples, given that the inter-sample diversity objective becomes difficult to satisfy?
- Basis in paper: [inferred] The paper notes that performance declines when the sample number exceeds 20 because "our approach explicitly enlarges inter-sample differences, and this objective becomes difficult to satisfy when the synthetic set grows excessively, making the optimization harder to converge" (Page 7).
- Why unresolved: The authors empirically identify the convergence difficulty with larger set sizes but do not propose a mechanism (e.g., dynamic regularization or hierarchical synthesis) to mitigate this trade-off.
- What evidence would resolve it: A modified training schedule or regularization term that maintains or improves performance when increasing synthetic samples from 20 to, for example, 100 or 500, without divergence.

### Open Question 2
- Question: Can the frequency-domain alignment and information bottleneck approach be adapted to perform effectively on time-series domains with weak temporal regularities or low predictability, such as ILI or Wiki2000?
- Basis in paper: [inferred] The paper explicitly states that "For ILI and Wike2000, however, both plugins perform relatively poorly due to the low predictability and weak temporal regularities inherent in these datasets" (Page 6).
- Why unresolved: The method assumes the existence of learnable spectral components and temporal structure; the authors offer no solution for datasets where these assumptions do not hold.
- What evidence would resolve it: A domain-adaptive weighting scheme or a modified spectral loss that demonstrates improved MSE/MAE on the ILI and Wike2000 datasets compared to the baseline results reported in the paper.

### Open Question 3
- Question: How can the relative performance gap between trajectory-matching and value-only distillation paradigms be reduced when integrating the DDTime plugin?
- Basis in paper: [inferred] The authors observe that while the method is compatible with various paradigms, "the performance gains are less pronounced [in value-only distillation], reflecting the complementary nature of the two-term formulation" (Page 2).
- Why unresolved: The current design optimizes the value term, but the authors note the synergy relies on the joint optimization with the gradient/parameter term present in trajectory matching.
- What evidence would resolve it: An analysis or architectural tweak showing significant accuracy improvements when applying DDTime to a purely value-term-based distillation method, closing the performance gap with trajectory-matching methods.

## Limitations

- Theoretical guarantees rely on strong assumptions about model local linearity and Lipschitz continuity of the loss function, which may not hold for deep time-series models with nonlinear activation patterns
- Frequency-domain alignment assumes wide-sense stationarity of target sequences, which is not verified across all 20 benchmark datasets
- Inter-sample regularization via KL divergence lacks direct empirical validation of its sufficiency-orthogonality benefits for TSF, relying instead on proxy diversity metrics

## Confidence

- **High confidence**: The 30% relative accuracy gains over baselines are well-supported by extensive experiments across 20 datasets and multiple architectures (Table 4, Figure 7)
- **Medium confidence**: The theoretical decomposition of test error into parameter and value terms (Lemma 1) is mathematically sound but the practical tightness of the bound depends on unverified smoothness assumptions
- **Medium confidence**: The frequency alignment mechanism's effectiveness assumes decorrelation properties hold for all datasets, but stationarity conditions are not explicitly tested

## Next Checks

1. Verify wide-sense stationarity assumptions for all benchmark datasets by computing autocorrelation functions and testing decay rates before applying frequency alignment
2. Conduct ablation studies varying the Lipschitz constant assumptions by testing with non-smooth loss functions (e.g., Huber loss variants) to assess robustness of the theoretical bound
3. Measure actual mutual information between synthetic samples using nonparametric estimators to directly validate whether ISIB achieves the claimed sufficiency-orthogonality properties beyond proxy KL diversity scores