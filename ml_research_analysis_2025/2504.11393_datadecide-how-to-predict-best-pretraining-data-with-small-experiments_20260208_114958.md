---
ver: rpa2
title: 'DataDecide: How to Predict Best Pretraining Data with Small Experiments'
arxiv_id: '2504.11393'
source_url: https://arxiv.org/abs/2504.11393
tags:
- data
- accuracy
- scale
- compute
- small
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of predicting which pretraining
  data will yield the best large language models without incurring the full cost of
  training them at scale. The authors introduce DATADECIDE, a comprehensive suite
  of models trained across 25 diverse pretraining corpora, 14 model sizes (4M to 1B
  parameters), and 3 random seeds, totaling over 30,000 model checkpoints.
---

# DataDecide: How to Predict Best Pretraining Data with Small Experiments

## Quick Facts
- arXiv ID: 2504.11393
- Source URL: https://arxiv.org/abs/2504.11393
- Reference count: 40
- Primary result: Single-scale ranking of small models (~150M params) predicts 1B-scale pretraining data performance with ~80% accuracy

## Executive Summary
This paper introduces DATADECIDE, a comprehensive suite of over 30,000 model checkpoints across 25 pretraining corpora, 14 model sizes, and 3 random seeds, to study how to predict which pretraining data will yield the best large language models without full-scale training costs. The authors evaluate multiple prediction methods, finding that ranking single-scale experiments (e.g., at 150M parameters) achieves ~80% decision accuracy for pairwise dataset comparisons at the 1B parameter scale. No tested scaling law method exceeded this baseline's efficiency, though the dataset enables future improvements. Additionally, using continuous likelihood metrics (CORRECT PROB or TOTAL PROB) as proxies in small-scale experiments significantly improved decision accuracy for tasks like MMLU, ARC, HellaSwag, MBPP, and HumanEval, achieving >80% accuracy with only 0.01% of the compute required for full-scale training.

## Method Summary
The study introduces DATADECIDE, a controlled experimental suite of over 30,000 model checkpoints spanning 25 pretraining corpora (Dolma, DCLM, RefinedWeb, C4, FineWeb variants with deduplication, quality filtering, and mixing interventions), 14 model sizes (4M to 1B parameters) at 100 tokens per parameter (5× Chinchilla-optimal), and 3 random seeds. Models are evaluated on OLMES, a 10-task benchmark suite including MMLU, ARC, HellaSwag, and code tasks (MBPP, HumanEval). The paper tests prediction methods including single-scale ranking versus multi-scale scaling law extrapolation (8 variants with 2-7 parameter fits), measuring decision accuracy (% of correct pairwise dataset comparisons) and prediction error. Small-scale models (e.g., 150M) are trained and evaluated using proxy metrics like CORRECT PROB (likelihood of correct option) and TOTAL PROB (sum of all option likelihoods) to improve predictability when discrete accuracy is unreliable at small scales.

## Key Results
- Single-scale ranking of 150M-parameter models predicts 1B-parameter data recipe rankings with ~80% decision accuracy
- No tested scaling law method exceeded the single-scale ranking baseline in decision accuracy or efficiency
- Continuous likelihood metrics (CORRECT PROB, TOTAL PROB) significantly improve decision accuracy for code and reasoning tasks, achieving >80% accuracy with 0.01% of full-scale compute
- Decision accuracy is driven by the balance between run-to-run variance and performance spread across data recipes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Ranking pretraining datasets at a single small scale (e.g., 150M parameters) predicts large-scale (1B) performance rankings with ~80% accuracy.
- **Mechanism:** Relative performance ordering between data recipes tends to be preserved across model scales when training conditions are controlled (same token-to-parameter ratio, hyperparameter scaling heuristics). Crossovers—where one recipe overtakes another between scales—do occur but are bounded, making constant-rank approximation a viable baseline.
- **Core assumption:** The token-to-parameter ratio (100×, or 5× Chinchilla-optimal) and hyperparameter ladder generalize across scales; the set of 25 data recipes is representative of real data interventions.
- **Evidence anchors:**
  - [abstract] "ranking of models at a single, small size (e.g., 150M parameters) is a strong baseline for predicting best models at our larger target scale (1B) (~80% of comparisons correct)"
  - [section 3.2] "A priori we know that ranking single scale experiments cannot correctly predict when the scaling trend of one data recipe overtakes another... Nevertheless ranking single scale experiments sets a high baseline decision accuracy, implying relatively little crossover occurs."
  - [corpus] Neighboring paper "Can Small Training Runs Reliably Guide Data Curation?" directly questions transfer reliability for reasoning tasks—evidence that transfer is not universally guaranteed.
- **Break condition:** If target tasks exhibit late-stage emergence or sharp phase transitions between scales, single-scale ranking may fail. Tasks like SocialIQA show poor predictability across all scales tested.

### Mechanism 2
- **Claim:** Continuous likelihood metrics (CORRECT PROB, TOTAL PROB) outperform discrete accuracy at small scales for predicting downstream accuracy at target scale.
- **Mechanism:** Discrete accuracy metrics exhibit threshold behavior—small models may be near chance, creating noisy or flat rankings. Continuous probability metrics capture graded signal (model confidence on correct/incorrect options) even when the model cannot reliably select the correct answer, spreading recipes more distinctly and reducing variance.
- **Core assumption:** The likelihood signal correlates with eventual accuracy gains as scale increases; the relationship holds across the specific benchmarks tested.
- **Evidence anchors:**
  - [abstract] "using continuous likelihood metrics as proxies in small experiments makes benchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable at the target 1B scale with just 0.01% of the compute"
  - [section 3.3] "Using CORRECT PROB or TOTAL PROB leads to decision accuracy at least as good as any other metric for most small scales."
  - [section 3.4] Code tasks (MBPP, HumanEval) "go from trivial decision accuracy to largely predictable when using continuous CORRECT PROB instead of discrete ACCURACY."
  - [corpus] No direct corroboration found in neighbor papers for this specific proxy-metric mechanism.
- **Break condition:** If correct answers have systematically lower likelihood than incorrect distractors (e.g., due to domain unfamiliarity), CORRECT PROB may mislead. Math benchmarks (GSM8K, MATH) did not benefit from this switch.

### Mechanism 3
- **Claim:** Decision accuracy on a benchmark is driven by (1) low run-to-run variance and (2) wide performance spread across data recipes.
- **Mechanism:** A benchmark can separate data recipes reliably if the signal (difference in mean performance across recipes) exceeds the noise (variance across random seeds). Tasks with either low noise (MMLU) or high spread (ARC Easy) are more predictable. Proxy metrics can improve either dimension.
- **Core assumption:** Variance across 3 seeds approximates true run-to-run variability; the OLMES benchmarks are representative of downstream task behavior.
- **Evidence anchors:**
  - [section 3.4] "The evaluation must separate pairs of data recipes by an amount greater than combined noise from run-to-run variance of each of the pair's runs."
  - [section 3.4, Figure 5] Shows MMLU characterized by low run-to-run noise, ARC Easy by wide spread across recipes.
  - [corpus] Weak direct evidence; neighbor papers do not replicate this variance-spread decomposition.
- **Break condition:** If seed count is insufficient to estimate variance, or if task saturation compresses spread, predictability degrades. BoolQ shows near-trivial accuracy until very high compute.

## Foundational Learning

- **Concept: Scaling Laws (Compute-Performance Relationship)**
  - **Why needed here:** The paper tests whether fitting scaling laws across multiple scales improves data decisions vs. single-scale ranking. Understanding L(C) = A/C^α + E and Acc(L) sigmoid mappings is essential to interpret why multi-scale approaches did not exceed the baseline.
  - **Quick check question:** Given two data recipes with similar loss at 150M but diverging loss at 1B, would a 3-parameter scaling law extrapolation correctly rank them?

- **Concept: Decision Accuracy vs. Prediction Error**
  - **Why needed here:** The paper introduces decision accuracy (% of pairwise comparisons correctly predicted) as a practical metric distinct from typical prediction error (MSE/MAE on absolute scores). This reframes evaluation toward operational decisions.
  - **Quick check question:** A method has 5% relative prediction error but only 60% decision accuracy—is it useful for choosing between two nearly identical data recipes?

- **Concept: Proxy Metrics for Downstream Evaluation**
  - **Why needed here:** At small scales, models cannot reliably solve benchmarks. Probability-based proxies (CORRECT PROB, TOTAL PROB, MARGIN) provide continuous signal that correlates with future accuracy.
  - **Quick check question:** Why might TOTAL PROB (summing likelihood of all options) predict accuracy better than MARGIN (correct minus top incorrect) at small scales?

## Architecture Onboarding

- **Component map:** 25 data recipes (Dolma, DCLM, RefinedWeb, C4, FineWeb variants with deduplication, quality filtering, mixing ratios) -> 14 model scales (4M–1B params, 100 tokens/param) -> 30K+ checkpoints (25 × 14 × 3 seeds) -> OLMES evaluation (10 MC tasks) -> prediction methods (single-scale ranking vs. 8 scaling law variants)

- **Critical path:** 1. Select data recipes to compare (from 25 or extend with new recipes). 2. Train small-scale models (e.g., 150M, 300M) on each recipe. 3. Evaluate using proxy metrics (CORRECT PROB recommended for hard tasks). 4. Rank recipes by proxy score; validate against 1B target runs if available. 5. Optionally fit scaling laws across 3+ scales to predict crossover scenarios.

- **Design tradeoffs:** Single-scale ranking is compute-efficient but cannot predict crossovers. Multi-scale fitting adds cost but theoretically handles rank inversions. Discrete accuracy is directly interpretable but noisy at small scales; continuous proxies improve signal but require correlation validation per task. More seeds reduce variance estimates but multiply cost (paper uses 3 for 1B targets, 1 full + 2 partial for smaller scales).

- **Failure signatures:** Flat rankings (all recipes score similarly—task too easy/hard; switch metrics/tasks). High seed variance (random initialization dominates recipe effects; increase seeds or use lower-variance proxy metrics). Crossover between scales (recipe A wins at 150M, B wins at 1B; single-scale ranking fails; multi-scale extrapolation may help if fit quality is sufficient). Proxy-target mismatch (CORRECT PROB improves small-scale separation but does not correlate with 1B accuracy; revalidate per task/domain).

- **First 3 experiments:** 1. Reproduce single-scale baseline: Train 150M models on 5–10 data recipes from the released set, rank by CORRECT PROB on MMLU/ARC, compare to released 1B rankings to verify ~80% decision accuracy. 2. Test proxy metric on new task: Evaluate released checkpoints on a code benchmark not in OLMES using CORRECT PROB vs. ACCURACY; measure whether proxy improves decision accuracy relative to 1B targets. 3. Pilot scaling law prediction: Fit 3-parameter L(C) + sigmoid Acc(L) on 3 scales (e.g., 60M, 150M, 300M) for 5 recipes; compare predicted vs. actual 1B rankings to quantify crossover handling.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can future scaling law formulations surpass the "compute-decision frontier" established by single-scale ranking experiments?
- **Basis in paper:** [explicit] The authors note in Section 3.2 that while current scaling law baselines failed to exceed the single-scale frontier, "Improved future scaling laws may be able to advance the Pareto frontier on DATADECIDE."
- **Why unresolved:** The 8 baseline parametric methods tested could not effectively model the data-dependent crossovers that occur between small and large scales better than a simple constant approximation.
- **What evidence would resolve it:** A novel scaling law method that achieves statistically significantly higher decision accuracy (>80%) than single-scale ranking while utilizing the same or lower compute budget on the DataDecide benchmarks.

### Open Question 2
- **Question:** Is it methodologically justified to switch target evaluation metrics from discrete accuracy to continuous likelihoods (e.g., CORRECT PROB) to improve predictability?
- **Basis in paper:** [explicit] In Section 3.4, the authors observe that switching targets to continuous metrics improves decision accuracy for math tasks, explicitly "raising a question for future work to explore whether changing the target metric can be justified."
- **Why unresolved:** While continuous metrics reduce noise and improve small-scale correlation, it is unclear if they serve as valid proxies for the discrete accuracy requirements of real-world applications.
- **What evidence would resolve it:** Research demonstrating that models optimized for continuous likelihood metrics at scale exhibit superior performance on independent, downstream discrete tasks compared to those optimized directly for accuracy.

### Open Question 3
- **Question:** Do the prediction efficiencies found at the 1B scale and 5× Chinchilla ratio transfer to much larger models (e.g., 70B+) or compute-optimal training regimes?
- **Basis in paper:** [inferred] The Limitations section restricts findings to "just one ratio of tokens to parameters" and "model sizes up to 1B parameters," acknowledging that typical models favor overtraining but leaving other scaling regimes unexplored.
- **Why unresolved:** The dynamics of "run-to-run variance" and "spread" of data recipes may shift non-linearly as model capacity increases or as the token-to-parameter ratio changes.
- **What evidence would resolve it:** Replicating the DataDecide controlled experiments at larger parameter counts (e.g., 7B–70B) to determine if the ~80% decision accuracy baseline for single-scale prediction holds.

## Limitations
- Findings are limited to one token-to-parameter ratio (100×) and model sizes up to 1B parameters
- Single-scale ranking cannot predict crossover scenarios where data recipe rankings invert between scales
- Proxy metric effectiveness (CORRECT PROB, TOTAL PROB) is demonstrated primarily on discrete-choice MC tasks, with uncertain generalization to open-ended generation or regression

## Confidence
- **High confidence:** The dataset construction (25 recipes × 14 scales × 3 seeds), the strong performance of single-scale ranking as a baseline, and the improved predictability using CORRECT PROB for discrete-choice tasks
- **Medium confidence:** The claim that no scaling law method exceeded the single-scale baseline is based on 8 tested variants; untested scaling models or different hyperparameter scaling rules could change this
- **Low confidence:** Generalization of the variance-spread decomposition and proxy metric effectiveness to other task types, domains, or larger scales (e.g., 3B+)

## Next Checks
1. Test single-scale ranking on new data recipes: Use the released DATADECIDE corpus to train 150M models on 5-10 novel data recipes not in the original 25, rank by CORRECT PROB on MMLU/ARC, and compare to 1B targets to verify if ~80% decision accuracy holds

2. Validate proxy metric on open-ended tasks: Apply CORRECT PROB and TOTAL PROB to checkpoints from DATADECIDE on tasks with open-ended generation (e.g., summarization, translation) or regression (e.g., mathematical reasoning) to assess if continuous proxies improve predictability outside discrete-choice MC benchmarks

3. Probe crossover sensitivity: Select data recipe pairs with known small-scale separation but suspected crossover potential (e.g., high vs. low quality mixtures) and fit multi-scale scaling laws across 3+ scales to quantify if any variant predicts the 1B ranking better than single-scale ranking