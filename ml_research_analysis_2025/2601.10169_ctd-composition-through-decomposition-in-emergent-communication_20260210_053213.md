---
ver: rpa2
title: 'CtD: Composition through Decomposition in Emergent Communication'
arxiv_id: '2601.10169'
source_url: https://arxiv.org/abs/2601.10169
tags:
- communication
- codebook
- concepts
- compositionality
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Composition through Decomposition (CtD),
  a novel two-step training approach that enables artificial neural agents to acquire
  compositional generalization in emergent communication. The method first trains
  agents in a multi-target coordination game to decompose images into basic concepts
  using a discrete codebook, then leverages this learned codebook to compose descriptions
  of novel images by combining known concepts.
---

# CtD: Composition through Decomposition in Emergent Communication

## Quick Facts
- arXiv ID: 2601.10169
- Source URL: https://arxiv.org/abs/2601.10169
- Reference count: 40
- Key outcome: CtD achieves perfect compositionality and accuracy on controlled datasets through decomposition-then-composition training

## Executive Summary
This paper introduces Composition through Decomposition (CtD), a novel two-step training approach that enables artificial neural agents to acquire compositional generalization in emergent communication. The method first trains agents in a multi-target coordination game to decompose images into basic concepts using a discrete codebook, then leverages this learned codebook to compose descriptions of novel images by combining known concepts. Using a discrete latent codebook with a multi-loss optimization framework, the approach achieves perfect accuracy and compositionality scores across multiple datasets including THING (100% accuracy and CBM), SHAPE (99% accuracy, perfect CBM and CI), and MNIST (81% accuracy, 92% AMI, 95% CBM).

## Method Summary
CtD employs a two-phase training protocol. In the first phase, agents play a multi-target coordination game where they must jointly decompose complex images into basic concepts using a discrete latent codebook. The receiver must reconstruct all concepts in the image from a single message, optimizing for reconstruction accuracy, mutual information between concepts and messages, and variability of the codebook. In the second phase, the learned codebook is fixed and agents use it to compose descriptions of novel images by combining known concepts. The receiver must identify the correct image from a set of candidates. The approach leverages a Gumbel-softmax-based discrete codebook with a multi-loss optimization framework (MI-VC loss) to encourage both accurate reconstruction and compositional generalization.

## Key Results
- THING dataset: 100% accuracy and CBM (Context-based Metric) scores
- SHAPE dataset: 99% accuracy, perfect CBM and CI (Context Independence) scores
- MNIST dataset: 81% accuracy, 92% AMI (Attribute-based Metric Index), 95% CBM scores
- Zero-shot generalization achieved in composition phase without additional training
- Significantly outperforms existing communication protocols (Gumbel-softmax and Quantized) in both accuracy and compositionality metrics

## Why This Works (Mechanism)
CtD works by first teaching agents to decompose complex objects into basic concepts through a multi-target game, then using this learned decomposition to compose descriptions of novel combinations. The discrete codebook acts as a shared vocabulary that enables systematic composition. The multi-loss optimization (reconstruction + mutual information + variability) ensures the codebook captures meaningful concepts while maintaining sufficient diversity for composition. The multi-target game structure is critical because it forces agents to learn independent representations of each concept rather than entangled holistic representations.

## Foundational Learning
- **Discrete latent variables**: Used to create a shared symbolic vocabulary for communication; needed for compositional generalization; check by verifying codebook size and concept separation
- **Multi-target games**: Require agents to handle multiple concepts simultaneously; needed to force decomposition learning; check by measuring individual concept accuracy
- **Mutual information maximization**: Ensures concepts are distinguishable in the message space; needed for compositionality; check by measuring concept separability metrics
- **Variability regularization**: Prevents code collapse and encourages diverse concept representations; needed for robust generalization; check by analyzing codebook entropy
- **Zero-shot composition**: Ability to combine learned concepts without additional training; needed for true compositionality; check by measuring performance on novel concept combinations

## Architecture Onboarding
- **Component map**: Image -> Encoder -> Discrete Codebook -> Message -> Decoder -> Reconstruction/Classification
- **Critical path**: Multi-target game phase establishes codebook â†’ Composition phase uses fixed codebook for novel images
- **Design tradeoffs**: Discrete vs continuous representations (discrete enables compositionality but may lose information), multi-target vs single-target games (multi-target enables decomposition but requires more complex coordination)
- **Failure signatures**: Poor decomposition phase leads to poor composition phase; code collapse results in inability to distinguish concepts; insufficient variability leads to over-specialized representations
- **First experiments**: 1) Test codebook concept separation on training data; 2) Measure zero-shot performance on held-out concept combinations; 3) Ablate multi-loss components to identify critical factors

## Open Questions the Paper Calls Out
None

## Limitations
- The multi-target game requirement may limit applicability to real-world scenarios where agents cannot control game structure
- The approach shows strong performance on controlled datasets but 81% accuracy on MNIST suggests information preservation issues across decomposition-composition pipeline
- Discrete codebook approach may not generalize well to continuous or ambiguous concept spaces

## Confidence
- Zero-shot generalization without additional training: Medium (depends heavily on codebook quality from multi-target phase)
- Multi-target game necessity: Medium (essential according to results but may not be only approach)
- Discrete codebook superiority: Medium (strong results but architectural commitment limits generalizability)

## Next Checks
1. Test approach on naturalistic image datasets with overlapping and ambiguous concepts to evaluate robustness to real-world complexity
2. Conduct ablation studies to quantify specific contribution of each loss component (reconstruction, mutual information, variability) and necessity of multi-target game structure
3. Implement cross-modal transfer tests where agents communicate about concepts learned in one modality (e.g., images) using different modality (e.g., text) to assess true compositional understanding