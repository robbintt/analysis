---
ver: rpa2
title: 'HE-SNR: Uncovering Latent Logic via Entropy for Guiding Mid-Training on SWE-BENCH'
arxiv_id: '2601.20255'
source_url: https://arxiv.org/abs/2601.20255
tags:
- entropy
- token
- he-snr
- mid-training
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HE-SNR provides a novel metric for guiding mid-training of large
  language models on SWE-BENCH by focusing on high-entropy decision points rather
  than overall perplexity. The approach introduces a data filtering strategy and the
  Entropy Compression Hypothesis, which redefines intelligence as the capacity to
  structure uncertainty into Entropy-Compressed States of low orders ("reasonable
  hesitation").
---

# HE-SNR: Uncovering Latent Logic via Entropy for Guiding Mid-Training on SWE-BENCH

## Quick Facts
- **arXiv ID:** 2601.20255
- **Source URL:** https://arxiv.org/abs/2601.20255
- **Reference count:** 40
- **Primary result:** HE-SNR achieves r = 0.67 correlation with SWE-BENCH scores vs r = -0.29 for perplexity

## Executive Summary
HE-SNR introduces a novel metric for guiding mid-training of large language models on SWE-BENCH by focusing on high-entropy decision points rather than overall perplexity. The approach filters trajectories to extract functional Action tokens and computes a Signal-to-Noise Ratio using Top-10 entropy, capturing the model's ability to structure uncertainty into low-order candidate sets. Validated on industrial-scale Mixture-of-Experts models, HE-SNR demonstrates superior robustness against the "Long-Context Tax" from RoPE scaling and strong predictive power for downstream SWE performance using only 500 trajectories (≈12.5M tokens). The study also reveals that supervised fine-tuning degrades performance on critical high-entropy tokens, uncovering the intrinsic source of the "Alignment Tax."

## Method Summary
HE-SNR computes a Signal-to-Noise Ratio metric for guiding mid-training by filtering SWE-BENCH trajectories to extract functional Action tokens (via AST parsing), then calculating Top-10 entropy for each token. High-Entropy Decision Set H is defined using threshold ε = (ln 3 + ln 4)/2, and HE-SNR = mean(p(xₜ)/H_top10(xₜ)) over all t ∈ H. The metric focuses on tokens where entropy exceeds ε and ground truth ∈ Top-10 candidates, capturing reasoning quality rather than rote memorization. Correlation with SWE-BENCH Pass@1 serves as validation target.

## Key Results
- HE-SNR achieves r = 0.67 correlation with SWE-BENCH Pass@1 vs r = -0.29 for perplexity
- Metric remains stable during RoPE scaling (Long-Context Tax) while perplexity spikes
- SFT degrades HE-SNR despite improving global perplexity, revealing Alignment Tax mechanism
- Only 500 trajectories (≈12.5M tokens) needed for strong predictive power

## Why This Works (Mechanism)

### Mechanism 1: Signal-to-Noise Separation of Reasoning
HE-SNR isolates reasoning "forks" from rote memorization by calculating p(xₜ)/H_top10(xₜ) for tokens where entropy exceeds threshold ε. Unlike perplexity which averages loss over all tokens, HE-SNR weights confidence on difficult decision points relative to local uncertainty, better capturing intelligence as structuring ambiguity into ~3 candidates.

### Mechanism 2: The "Shift to ln 3" as Capability Crystallization
As models mature during mid-training, entropy distribution of ambiguous tokens shifts from ln 4 to ln 3, indicating effective compression of candidate space. This peak at ln 3 ≈ 1.1 theoretically corresponds to uniform distribution over 3 valid options, suggesting the model has discarded irrelevant options while retaining legitimate uncertainty.

### Mechanism 3: SFT as an Entropy Regularizer (Alignment Tax)
Supervised Fine-Tuning improves global perplexity by forcing pattern matching but simultaneously degrades performance on critical high-entropy tokens. SFT acts as aggressive entropy regularizer, collapsing uncertainty to improve confidence but losing reasoning capability on complex decision points.

## Foundational Learning

- **Entropy-Compressed States & ln k**: Understanding entropy as proxy for "effective number of candidates" is essential for interpreting HE-SNR. Quick check: If Top-10 entropy peaks at ln 10, is this high reasoning capability or random guessing? (Answer: Random guessing/uniform uncertainty over 10 options).

- **Top-k Sampling vs. Perplexity**: PPL fails during context extension because it relies on exact ground truth probability while Top-k only requires truth to be plausible. Quick check: During RoPE scaling, if PPL spikes but Top-10 accuracy remains stable, what happens to probability mass? (Answer: It flattens/shifts but correct token remains in candidate set).

- **The "Mid-Training" Phase**: This phase bridges pre-training (general knowledge) and SFT (instruction following). Quick check: Why is HE-SNR needed for mid-training specifically rather than just checking loss? (Answer: Mid-training involves domain-specific data and context extension where standard loss metrics become lagging or misleading).

## Architecture Onboarding

- **Component map**: Data Curator -> Entropy Analyzer -> Decision Filter -> Metric Aggregator
- **Critical path**: Threshold ε = (ln 3 + ln 4)/2 selection is most sensitive hyperparameter
- **Design tradeoffs**: Static vs adaptive threshold; Action-only vs full trajectory filtering
- **Failure signatures**: Metric flatlining suggests data too noisy/random; Context Tax false positive requires checking candidate set sufficiency
- **First 3 experiments**: 1) Replicate "Shift to ln 3" entropy histogram, 2) Simulate Long-Context Tax with RoPE scaling, 3) Verify Alignment Tax post-SFT

## Open Questions the Paper Calls Out

- **Cross-Domain Generalization**: Does "Shift to ln 3" phenomenon and HE-SNR efficacy generalize to dense architectures and non-SWE logical domains like mathematics? Paper states they "plan to extend validation to diverse architectures and broader logical domains."

- **Adaptive Thresholding**: Can adaptive thresholding mechanisms capture model potential more accurately than current static entropy boundary? Authors aim to "develop adaptive thresholding mechanisms to capture varying convergence rates."

- **Code Canonicalization**: To what extent can code canonicalization decouple logical uncertainty from stylistic noise in high-entropy tokens? Future work will explore "code canonicalization or style transfer to ensure high-entropy tokens reflect logical uncertainty."

## Limitations

- Limited generalizability to domains beyond software engineering tasks
- Claims about SFT degrading high-entropy token performance based on single implementation
- Threshold selection appears somewhat arbitrary without sensitivity analysis

## Confidence

- **High Confidence**: HE-SNR provides superior correlation with SWE-BENCH performance compared to perplexity metrics
- **Medium Confidence**: Entropy Compression Hypothesis and interpretation of ln 3 as "reasonable hesitation"
- **Low Confidence**: Universality of Alignment Tax mechanism across all SFT approaches

## Next Checks

1. **Cross-Domain Entropy Distribution Analysis**: Apply entropy analysis framework to mathematics, scientific writing, or legal document analysis to test whether "Shift to ln 3" appears as universal principle.

2. **SFT Strategy Ablation Study**: Implement multiple SFT variants including entropy-preserving fine-tuning and contrastive learning to determine whether Alignment Tax is inherent property or implementation artifact.

3. **Threshold Sensitivity and Adaptive Selection**: Conduct systematic experiments varying entropy threshold ε and implement adaptive threshold selection to compare predictive accuracy against static threshold approach.