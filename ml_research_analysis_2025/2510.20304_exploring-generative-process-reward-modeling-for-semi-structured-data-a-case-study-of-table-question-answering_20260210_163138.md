---
ver: rpa2
title: 'Exploring Generative Process Reward Modeling for Semi-Structured Data: A Case
  Study of Table Question Answering'
arxiv_id: '2510.20304'
source_url: https://arxiv.org/abs/2510.20304
tags:
- step
- answer
- reasoning
- code
- verification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the applicability of process reward models
  (PRMs) to table question answering (TQA), a semi-structured data task where irrelevant
  information and loosely connected reasoning steps challenge verification. We evaluate
  state-of-the-art generative PRMs combining textual and code verification, comparing
  them against baselines like LLM-as-a-judge and majority voting.
---

# Exploring Generative Process Reward Modeling for Semi-Structured Data: A Case Study of Table Question Answering

## Quick Facts
- arXiv ID: 2510.20304
- Source URL: https://arxiv.org/abs/2510.20304
- Authors: Lei Tang; Wei Zhou; Mohsen Mesgar
- Reference count: 27
- One-line primary result: PRMs combining textual and code verification can aid TQA solution selection but struggle with out-of-domain generalization and show weak correlation between step scores and answer accuracy.

## Executive Summary
This study investigates the applicability of generative Process Reward Models (PRMs) to table question answering (TQA), where abundant irrelevant information and loosely connected reasoning steps challenge verification. The authors evaluate state-of-the-art PRMs that combine textual chain-of-thought with Python code execution, comparing them against baselines like LLM-as-a-judge and majority voting. Results show that code-augmented verification improves step-level accuracy but struggles with out-of-domain generalization. Crucially, the study finds a weak correlation between average step-level verification scores and final answer accuracy due to irrelevant steps and loose causal dependencies in TQA reasoning chains.

## Method Summary
The method uses a policy model (Qwen2.5-7B-Instruct) to generate N=8 reasoning paths per table-question pair. Each path is processed by a rationale generator (QwQ-32B) producing textual CoT analysis plus Python code for each step. Code execution provides external grounding for numerical operations and cell retrieval. A PRM verifier (DeepSeek-R1-Distill-Qwen-7B) fine-tuned on WTQ/TabFact data outputs step-level Yes/No probabilities. Paths are scored by averaging step rewards and the highest-scoring path is selected (Best@N). Training uses RPE filtering to discard disagreements and malformed outputs.

## Key Results
- Code-augmented GenPRM substantially outperforms textual-only COT PRM on step verification accuracy, particularly for numerical operations and targeted cell retrieval.
- Uniform averaging of step scores shows weak correlation with final answer accuracy due to irrelevant steps inflating scores and local errors not propagating.
- PRMs fine-tuned on in-domain data show degraded performance on out-of-domain datasets (CRT, SciTab) compared to majority voting baselines.
- Best@N selection with PRMs achieves competitive answer-level EM compared to LLM-as-a-judge while being faster, but struggles to identify the truly best path.

## Why This Works (Mechanism)

### Mechanism 1: Code-Augmented Verification Grounds Reasoning in Executable Evidence
Combining textual chain-of-thought with Python code execution improves step-level verification accuracy by providing external grounding for numerical operations and targeted cell retrieval. This reduces ambiguity inherent in pure textual verification. The mechanism assumes the policy model can generate syntactically correct Python code and the execution environment reliably reflects table operations. Evidence shows GenPRM substantially outperforms COT PRM, with manual analysis finding code correct in 43/60 disagreement cases, particularly benefiting numerical operations and targeted retrieval.

### Mechanism 2: Uniform Step Averaging Misaligns with TQA Causal Structure
Average step scores weakly correlate with answer correctness because uniform weighting treats irrelevant but locally correct steps as equal to causally determinative steps. TQA reasoning chains contain loosely connected steps where irrelevant steps inflate scores without affecting answers, and local errors that don't propagate are over-penalized. The mechanism assumes steps have varying causal relevance to the final answer. Evidence shows EM accuracy does not monotonically increase with average step score bins, with concrete examples of inflated scores from locally correct but wrong final answers, and over-penalization from erroneous steps that don't affect correct conclusions.

### Mechanism 3: Supervised Fine-Tuning PRMs Lack Robustness to Distributional Shift
PRMs fine-tuned on WTQ/TabFact training data show degraded performance on out-of-domain datasets due to higher reasoning path noise and domain-specific knowledge requirements. Training on in-domain data produces verification patterns that don't transfer to CRT (complex reasoning questions) or SciTab (scientific domain). Out-of-domain reasoning paths contain more inconsistent label sequences (37-50% vs 14-19% in-domain). The mechanism assumes noise distribution differs systematically across domains. Evidence shows performance dropping to 53 vs. 56 on CRT and 60 vs. 62 on SciTab compared to majority voting, with reasoning path consistency data supporting this distributional shift hypothesis.

## Foundational Learning

- **Process Reward Modeling vs. Outcome Reward Modeling**
  - Why needed here: PRMs provide step-level supervision rather than final-answer-only feedback, explaining why the paper examines step-level verification separately from answer accuracy.
  - Quick check question: Given a 5-step reasoning chain where step 3 has an error but the final answer is correct, would a PRM and ORM give different signals?

- **Semi-Structured Data Characteristics**
  - Why needed here: Tables differ from math problems in having (i) abundant irrelevant information, (ii) non-deductive reasoning chains, (iii) retrieval+computation hybrid operations, explaining why math-domain PRM assumptions break down.
  - Quick check question: In a 10-row, 5-column table answering "What is the maximum value in column C?", how many cells are causally relevant vs. potentially distracting?

- **Test-Time Scaling via Best-of-N Selection**
  - Why needed here: The evaluation uses Best@N—sampling N reasoning paths, scoring each, selecting highest-scoring. Understanding this explains why answer-level EM is the downstream metric and why Oracle represents upper bound.
  - Quick check question: If you sample 8 paths and PRM correctly identifies the best-scoring path, but that path's answer is wrong, is the PRM failing at its job?

## Architecture Onboarding

- **Component map**: Table + Question → Policy Model → N reasoning paths → Rationale Generator → Code Executor → Formatted prompt → PRM Verifier → Step-level rewards → Aggregation → Best@N selection → Final answer
- **Critical path**: 1) Policy model generates N reasoning paths per question; 2) Rationale generator produces textual analysis + code per step; 3) Code executor runs verification; 4) PRM verifier outputs step-level rewards; 5) Aggregation computes path score; 6) Best@N selects final answer
- **Design tradeoffs**: 7B verifier is faster but underperforms 32B LLM-as-judge on answer selection; code verification adds 2-3x latency; training data filtering improves quality but reduces volume; uniform vs weighted aggregation shows +3.56% WTQ improvement with weighted final steps
- **Failure signatures**: Index confusion between header-inclusive/exclusive row numbering; exhaustive retrieval shifts error to long-context recall; inflated scores from irrelevant but locally correct steps; over-penalization from local errors that don't propagate; OOD collapse on CRT/SciTab
- **First 3 experiments**: 1) Ablate code verification on held-out WTQ split; 2) Implement final-step weighting (+3.56% WTQ improvement); 3) Characterize OOD failure by annotating step consistency on CRT/SciTab samples

## Open Questions the Paper Calls Out

- **Open Question 1**: How do relevance-aware and causality-aware aggregation strategies compare to uniform averaging in correlating step scores with final answer accuracy for TQA?
  - Basis: The authors explicitly state uniform averaging is inadequate and advocate for "future work" on "relevance-aware and causality-aware aggregation strategies."
  - Why unresolved: Current average step scores show weak correlation with final answer accuracy due to irrelevant steps and loose causal links.
  - Evidence needed: A new aggregation method showing statistically significant increase in correlation between aggregated solution score and Exact Match accuracy.

- **Open Question 2**: Can supervised fine-tuning on more balanced datasets reflecting varying verification difficulties improve PRM generalization to out-of-domain datasets like CRT and SciTab?
  - Basis: Authors attribute poor OOD performance to spurious steps and suggest "more balanced fine-tuning data that reflects varying verification difficulties."
  - Why unresolved: Current fine-tuned PRMs show substantial performance drop on out-of-domain data compared to majority voting.
  - Evidence needed: Improved Best@N performance on CRT and SciTab datasets compared to current baselines, specifically closing gap with in-domain performance.

- **Open Question 3**: Do the findings regarding PRM limitations and the need for causal aggregation generalize to other semi-structured data tasks like table generation or summarization?
  - Basis: Authors note in Limitations that they only tested TQA and call for "future studies" to "explore these tasks to validate the effectiveness of PRMs."
  - Why unresolved: Unknown if "weak step dependence" and "loose causal links" observed in TQA reasoning chains are present or as problematic in generation or summarization tasks.
  - Evidence needed: Empirical evaluation of generative PRMs on table generation/summarization benchmarks showing whether causal aggregation strategies yield similar performance gains.

## Limitations
- Code generation reliability is not quantified—the percentage of steps where code generation fails or executes incorrectly could bias reported benefits of code-augmented verification.
- The paper does not explore sophisticated aggregation strategies (e.g., learned relevance weighting or causal structure-aware aggregation) that could address identified issues with irrelevant steps.
- Out-of-domain generalization results are presented without detailed investigation of specific reasoning errors or distributional shifts causing performance drop.

## Confidence
- **High Confidence**: Code-augmented verification improves step-level accuracy compared to textual-only approaches, well-supported by manual analysis of disagreement cases.
- **Medium Confidence**: Uniform step averaging leads to weak correlation between step scores and answer accuracy, supported by examples and correlation data but not fully quantified.
- **Medium Confidence**: PRMs fine-tuned on in-domain data struggle with out-of-domain generalization, supported by performance drop but underlying reasons not fully characterized.

## Next Checks
1. **Code Generation Reliability**: Conduct systematic analysis of policy model's code generation success rate and code executor's accuracy on held-out TQA steps to quantify potential bias in code-augmented verification benefits.
2. **Alternative Aggregation Strategies**: Implement and evaluate at least one alternative aggregation strategy (e.g., learned relevance scoring or final-step weighting) to determine if they improve correlation between step scores and answer accuracy.
3. **OOD Error Characterization**: Perform detailed error analysis on CRT and SciTab datasets to identify specific types of reasoning errors or distributional shifts causing performance drop, informing targeted solutions for improving robustness.