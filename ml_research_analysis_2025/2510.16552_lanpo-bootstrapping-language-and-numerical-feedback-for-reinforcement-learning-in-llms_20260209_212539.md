---
ver: rpa2
title: 'LANPO: Bootstrapping Language and Numerical Feedback for Reinforcement Learning
  in LLMs'
arxiv_id: '2510.16552'
source_url: https://arxiv.org/abs/2510.16552
tags:
- feedback
- training
- language
- lanpo
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LANPO addresses the challenge of improving sample efficiency in
  LLM reinforcement learning by integrating language feedback into the training loop.
  The key idea is to separate the roles of feedback: language guides exploration via
  an experience pool, while numerical rewards drive optimization.'
---

# LANPO: Bootstrapping Language and Numerical Feedback for Reinforcement Learning in LLMs

## Quick Facts
- arXiv ID: 2510.16552
- Source URL: https://arxiv.org/abs/2510.16552
- Reference count: 40
- Primary result: Up to 9.27% absolute accuracy improvement on AIME25 test sets vs GRPO baselines

## Executive Summary
LANPO introduces a novel reinforcement learning framework that improves sample efficiency in large language models by integrating language feedback into the training loop. The method separates feedback roles: language guides exploration via an experience pool, while numerical rewards drive optimization. Through two key mechanisms—Reward-Agnostic Reflection for safe intra-sample self-correction and Relevant Abstraction for inter-sample feedback—LANPO consistently outperforms GRPO baselines on mathematical reasoning benchmarks, achieving up to 9.27% absolute accuracy improvement on AIME25 test sets.

## Method Summary
LANPO enhances LLM reinforcement learning by integrating language feedback through an experience pool that accumulates distilled summaries from past trials. The framework employs two feedback mechanisms: Reward-Agnostic Reflection enables safe intra-sample self-correction without label leakage by having the model revisit and critique its own failed attempts, and Relevant Abstraction ensures inter-sample feedback is semantically aligned through similarity-based filtering (γ ≥ 0.9) and summarization into high-level principles. The experience pool stores distilled entries containing flow of thought, principles, pitfalls, and metadata. Training begins with an SFT stage (3K QA pairs) to teach summarization and reflection skills, followed by an RL loop where the model alternates between feedback-aware and from-scratch rollouts, updating policy via GRPO loss with KL regularization and adding summarized trajectories to the experience pool.

## Key Results
- LANPO achieves up to 9.27% absolute accuracy improvement on AIME25 test sets compared to GRPO baselines
- Filtering at γ = 0.9 achieves 47.38 average accuracy vs. 39.56 for GRPO with unfiltered retrieval
- Feedback ratio p_t = 0.75 is optimal; p_t = 1.0 causes collapse in this setup
- Gradient norms remain stable across training, suggesting optimization is not destabilized by dual feedback

## Why This Works (Mechanism)

### Mechanism 1: Reward-Agnostic Reflection
- **Claim:** Enables safe intra-sample self-correction without leaking ground-truth labels during training
- **Mechanism:** Model revisits its own failed attempts, critiques them step-by-step, and produces refined solutions without seeing correct answers
- **Core assumption:** Self-critique contains sufficient signal to improve reasoning without reinforcing error patterns
- **Evidence anchors:** Abstract mentions "Reward-Agnostic Reflection for safe intra-sample self-correction without label leakage"; Section 4.1 describes the step-by-step critique process
- **Break condition:** If self-critique quality degrades and confidently reinforces errors, mechanism fails

### Mechanism 2: Relevant Abstraction
- **Claim:** Prevents behavior collapse by filtering and summarizing inter-sample experiences into transferable principles
- **Mechanism:** Three-stage process: similarity-based filtering (γ ≥ 0.9), summarization into principles/pitfalls, explicit instruction to analyze feedback before solving
- **Core assumption:** Summarized principles generalize better than raw trajectories; similarity thresholds meaningfully capture transferability
- **Evidence anchors:** Abstract mentions "Relevant Abstraction for inter-sample feedback"; Section 5.2 shows γ = 0.9 achieves 47.38 accuracy vs 39.56 for unfiltered retrieval
- **Break condition:** If filtering is too strict (lack of diversity) or too loose (irrelevant context triggers collapse)

### Mechanism 3: Dual Feedback Separation
- **Claim:** Separating language and numerical feedback roles creates mutual bootstrapping between improved policy and better feedback
- **Mechanism:** Language feedback shapes context for rollouts (exploration), numerical rewards update policy parameters (optimization); as policy improves, feedback quality improves
- **Core assumption:** Two signals don't conflict during training; KL regularization sufficiently stabilizes policy updates
- **Evidence anchors:** Abstract mentions "separates the roles of feedback"; Figure 5b shows stable gradient norms across training
- **Break condition:** If language feedback degrades (noisy summaries, stale entries), it may misguide exploration

## Foundational Learning

- **Concept: GRPO (Group Relative Policy Optimization)**
  - **Why needed here:** LANPO builds on GRPO as its base RL objective; understanding group-based advantage normalization is essential for interpreting numerical reward computation
  - **Quick check question:** Can you explain how GRPO differs from PPO in its advantage estimation and why group-relative scaling matters for LLM RL?

- **Concept: KL Divergence Regularization**
  - **Why needed here:** LANPO objective includes KL penalty against reference policy; understanding this stability mechanism is critical for diagnosing training collapse
  - **Quick check question:** What happens to policy diversity and training stability if the KL coefficient β is set too high or too low?

- **Concept: In-Context Learning (ICL) and Retrieval**
  - **Why needed here:** Experience pool relies on retrieval-augmented context; understanding ICL limitations is foundational for debugging behavior collapse
  - **Quick check question:** Why might providing relevant examples at test time improve performance, while providing similar examples during training could cause the model to ignore them?

## Architecture Onboarding

- **Component map:**
  - Experience Pool (E) -> Summarizer Module -> Inter-Sample Explorer -> Policy LLM -> Reward Model -> GRPO Update -> Pool Update
  - Intra-Sample Reflector operates on same problem context before policy update

- **Critical path:**
  1. SFT seeding (3K QA pairs) → model acquires summarization, inspection, and response skills
  2. RL loop: With probability p_t, draw context from pool; generate rollout; compute reward; update policy via GRPO; summarize and add to pool
  3. Inference: Zero-shot (c = ∅), retrieval-augmented (c from pool), or self-corrective (intra-sample reflection)

- **Design tradeoffs:**
  - Feedback ratio p_t: Higher values leverage more experience but risk overfitting; Table 3 shows p_t = 0.75 is optimal, p_t = 1.0 causes collapse
  - Pool size vs. recency: Larger pools increase retrieval options but dilute relevance; authors cap at 32 entries per problem
  - Similarity threshold γ: 0.9 balances relevance and coverage; lower values introduce noise and collapse risk

- **Failure signatures:**
  - Training accuracy spikes, test flat: Intra-sample feedback leaking labels (switch to reward-agnostic mode)
  - Model ignores retrieved context, outputs directly: Behavior collapse (increase γ, improve summarization quality, or reduce p_t)
  - Gradient norm explosion: KL coefficient too low or pool quality degraded

- **First 3 experiments:**
  1. Ablate SFT seeding: Skip 3K SFT stage and observe whether model can learn to summarize and reflect during RL alone
  2. Vary similarity threshold γ: Test γ ∈ {0.7, 0.8, 0.9, 0.95} on held-out subset to observe performance degradation patterns
  3. Pool quality audit: Manually inspect 50 random pool entries after 100 steps to check whether summaries contain transferable principles vs problem-specific details

## Open Questions the Paper Calls Out
- Can LANPO framework be generalized to non-mathematical reasoning domains like coding or creative writing where "Relevant Abstraction" is harder to define? (Future work may extend framework to other domains)
- Can retrieval policy and experience pool management be automated via reinforcement learning rather than relying on pre-defined heuristics like BM25 and fixed similarity thresholds? (Need to automate pool management and retrieval policies)
- Is initial SFT stage strictly necessary to "seed" atomic skills, or can these capabilities be acquired bootstrap-style within the RL loop? (SFT used to instill literacy because skills not innate to base LLM)

## Limitations
- All empirical gains reported on math reasoning benchmarks; no validation on non-mathematical tasks like code generation or instruction following
- Fixed-size pool (32 entries) and similarity threshold (γ=0.9) tuned for math problems; may need adjustment for other domains
- Two-stage feedback process adds inference steps per rollout; computational overhead not quantified and could limit scalability

## Confidence
- **High**: Dual-feedback architecture is novel and empirical gains are robust within math domain; core claim about separating language/numerical feedback improving sample efficiency is well-supported
- **Medium**: Assertion that reward-agnostic reflection prevents label leakage is plausible but relies on indirect evidence; direct ablation of reflection mechanism would strengthen claim
- **Low**: Generalization to non-mathematical domains and scalability to larger models remain speculative; claims about applicability to diverse reasoning tasks not tested

## Next Checks
1. **Ablation of SFT seeding**: Skip the 3K SFT stage and run LANPO on math benchmarks to assess whether model can learn to summarize and reflect during RL alone
2. **Domain transfer test**: Apply LANPO to non-mathematical reasoning task (e.g., code generation on HumanEval) to test dual-feedback mechanism generalization
3. **Pool quality audit**: Manually inspect 50 random pool entries after 100 training steps on math problems to assess whether summaries contain transferable principles versus problem-specific details