---
ver: rpa2
title: 'Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn
  RL in Agentic LLMs'
arxiv_id: '2512.17008'
source_url: https://arxiv.org/abs/2512.17008
tags:
- click
- color
- product
- grpo
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies key limitations in applying Group Relative
  Policy Optimization (GRPO) to multi-turn LLM agent training, particularly its instability
  and suboptimal advantage estimation in long-horizon reasoning tasks. To address
  these issues, the authors introduce turn-PPO, a variant of PPO that reformulates
  the MDP at the turn level rather than the token level, enabling more accurate and
  stable advantage estimation.
---

# Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs
## Quick Facts
- **arXiv ID:** 2512.17008
- **Source URL:** https://arxiv.org/abs/2512.17008
- **Reference count:** 16
- **Primary result:** Turn-PPO outperforms GRPO and token-level PPO in reward and stability for multi-turn LLM agents on WebShop and Sokoban.

## Executive Summary
This paper addresses limitations in Group Relative Policy Optimization (GRPO) when training multi-turn LLM agents, particularly instability and suboptimal advantage estimation in long-horizon reasoning tasks. The authors introduce turn-PPO, a PPO variant that reformulates the Markov decision process at the turn level rather than the token level, enabling more accurate and stable advantage estimation. Experimental results on WebShop and Sokoban tasks demonstrate that turn-PPO achieves higher rewards and improved training stability compared to both GRPO and token-level PPO, especially in settings involving long reasoning chains.

## Method Summary
The core innovation of turn-PPO is the reformulation of the MDP at the turn level, allowing for more accurate and stable advantage estimation in multi-turn agentic LLM training. This approach addresses the instability and suboptimal performance of GRPO in long-horizon reasoning tasks by shifting from token-level to turn-level advantage computation. The method leverages standard PPO machinery but adapts the reward and advantage calculation to operate over complete agent turns, which is more aligned with the structure of multi-turn reasoning tasks.

## Key Results
- Turn-PPO achieves higher rewards than GRPO and token-level PPO on WebShop and Sokoban tasks.
- Training stability is improved with turn-PPO, particularly in long-horizon reasoning scenarios.
- The turn-level reformulation leads to more accurate advantage estimation compared to token-level approaches.

## Why This Works (Mechanism)
The paper does not provide a detailed mechanistic explanation beyond the reformulation of the MDP at the turn level. The underlying rationale is that turn-level advantage estimation better aligns with the natural structure of multi-turn reasoning, leading to more stable and effective learning.

## Foundational Learning
- **Advantage estimation**: Why needed - to guide policy updates toward higher-reward actions; Quick check - compare advantage magnitudes across methods.
- **Multi-turn RL**: Why needed - to train agents that perform reasoning over multiple interaction steps; Quick check - measure episode length and reasoning depth.
- **PPO (Proximal Policy Optimization)**: Why needed - to stabilize policy updates in on-policy RL; Quick check - monitor KL divergence and policy entropy during training.
- **GRPO (Group Relative Policy Optimization)**: Why needed - baseline for multi-turn RL in LLMs; Quick check - compare reward and stability metrics to turn-PPO.
- **Markov Decision Process (MDP)**: Why needed - foundational framework for RL; Quick check - ensure correct turn-level state/action/reward definitions.
- **Long-horizon reasoning**: Why needed - many agentic LLM tasks require multi-step planning; Quick check - evaluate performance on tasks with varying episode lengths.

## Architecture Onboarding
- **Component map**: Environment (WebShop/Sokoban) -> Agent (LLM with turn-PPO) -> Turn-level MDP -> Advantage estimation -> Policy update.
- **Critical path**: State observation → Turn-level action selection → Reward collection → Turn-level advantage calculation → PPO policy update.
- **Design tradeoffs**: Turn-level vs. token-level MDP formulation; trade-off between granularity of updates and stability.
- **Failure signatures**: Unstable training with high variance in advantages; poor performance on long-horizon tasks; reward collapse.
- **First experiments**:
  1. Replicate GRPO baseline on WebShop and measure reward and stability.
  2. Implement turn-PPO and compare advantage estimation variance to token-level PPO.
  3. Evaluate training curves for reward convergence and stability across all methods.

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental scope is limited to two domains (WebShop and Sokoban), restricting generalizability.
- Ablations focus mainly on reward metrics, with limited exploration of sample efficiency, computational overhead, or qualitative behavior.
- Does not benchmark against more recent multi-turn optimization methods beyond GRPO.
- Ambiguity remains whether gains are due to turn-level formulation itself or other factors like reward shaping.

## Confidence
- **High confidence** in empirical observation of higher rewards and improved training stability for turn-PPO over GRPO in tested domains.
- **Medium confidence** in turn-level advantage estimation as the primary driver of improvements, due to lack of comprehensive ablation studies.
- **Medium confidence** in generalizability of findings, given the narrow set of domains and tasks evaluated.

## Next Checks
1. **Ablation on advantage estimation methods**: Systematically compare turn-level, token-level, and alternative advantage estimators within the same PPO framework to isolate the effect of turn-level reformulation.
2. **Broader domain evaluation**: Test turn-PPO on a wider range of multi-turn reasoning tasks (e.g., text-based games, dialogue agents, or code generation) to assess robustness and generalizability.
3. **Efficiency and scalability analysis**: Quantify computational overhead and sample efficiency gains or losses when using turn-PPO compared to baseline methods, particularly in long-horizon scenarios.