---
ver: rpa2
title: 'CP-Agent: Agentic Constraint Programming'
arxiv_id: '2508.07468'
source_url: https://arxiv.org/abs/2508.07468
tags:
- constraint
- agent
- problem
- problems
- modeling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CP-Agent achieves 100% accuracy on 101 constraint programming problems
  from CP-Bench by combining a ReAct framework with a persistent IPython kernel and
  minimal domain-specific guidance. The agent iteratively executes and refines CPMpy
  models based on solver feedback, using a concise under-50-line project prompt instead
  of extensive procedural instructions.
---

# CP-Agent: Agentic Constraint Programming

## Quick Facts
- arXiv ID: 2508.07468
- Source URL: https://arxiv.org/abs/2508.07468
- Reference count: 40
- Primary result: 100% accuracy on 101 CP-Bench constraint programming problems using minimal guidance and iterative solver feedback

## Executive Summary
CP-Agent achieves perfect accuracy on the CP-Bench benchmark by combining a ReAct framework with a persistent IPython kernel and minimal domain-specific guidance. The agent iteratively executes and refines CPMpy models based on solver feedback, using a concise under-50-line project prompt instead of extensive procedural instructions. A clarification effort addressed ambiguities in problem specifications and errors in ground-truth models. The approach generalizes across multiple language models (80-100% success rates) and is accessible via an MCP server for broader integration.

## Method Summary
CP-Agent uses agentic-python-coder v2.2.1 with a ReAct framework, persistent IPython kernel, and minimal project prompt (<50 lines). The system executes CPMpy models iteratively, observing solver feedback to refine solutions. A clarification effort corrected ambiguities and errors in the original CP-Bench benchmark. The approach was validated on 101 problems with Claude Sonnet 4.5, achieving 100% accuracy through iterative refinement and solver feedback loops.

## Key Results
- 100% accuracy on 101 CP-Bench problems
- Minimal under-50-line project prompt outperforms 800-line procedural version
- Generalization across language models (80-100% success rates)
- Iterative execution with solver feedback enables error recovery

## Why This Works (Mechanism)

### Mechanism 1
Iterative execution with solver feedback enables error recovery that single-pass generation cannot achieve. The ReAct loop executes code, observes solver errors (e.g., domain overflow, infeasibility), and triggers model redesign. The cryptarithmetic example shows the agent switching from large-integer variables to column-wise addition after encountering `TypeError` from domain overflow.

### Mechanism 2
Concise strategic prompts can outperform extensive procedural scaffolding. A 50-line prompt focuses the LLM on high-level strategy and output format rather than exhaustive procedures. Ablation shows this matches or exceeds an 800-line version with detailed modeling catalogs.

### Mechanism 3
Persistent kernel state enables incremental refinement without file I/O overhead. IPython kernel maintains variables, imports, and functions across `python exec` calls. The agent can test partial solutions, add constraints incrementally, and inspect state—reducing context switching.

## Foundational Learning

- **ReAct (Reason-Act) pattern**: Understanding interleaved reasoning and tool-calling is essential for debugging agent behavior. *Quick check*: Can you trace one full Reason→Act→Observe cycle from the cryptarithmetic example?

- **Constraint programming basics**: You must interpret solver feedback (infeasibility, domain errors) to guide agent improvements. *Quick check*: What does an "AllDifferent" constraint enforce on a set of decision variables?

- **IPython kernel state persistence**: Stateful execution changes how you debug—variables persist across calls, requiring explicit resets. *Quick check*: If `python exec` fails mid-execution, what variables remain accessible in the next call?

## Architecture Onboarding

- **Component map**: LangGraph ReAct loop controller → `python exec` tool → persistent IPython kernel (subprocess) → CPMpy solver → `save code` tool (terminal). Optional: `todo_write` tool (disabled by default).

- **Critical path**: Task prompt → agent reasons → `python exec(code)` → kernel runs CPMpy model → solver returns stdout/stderr → agent interprets feedback → (loop or `save code`).

- **Design tradeoffs**: Single-script kernel approach trades file-system flexibility for iteration speed; minimal prompt trades explicit procedures for LLM pre-training leverage; optional task management trades overhead for structure on complex problems.

- **Failure signatures**:
  - `TypeError: init()` / domain overflow → model constructs variables with domains exceeding solver limits
  - Timeout after 20 minutes → agent not converging (check logs for repeated failed patterns)
  - `save code` called prematurely → agent may lack convergence signal

- **First 3 experiments**:
  1. Run CP-Agent on 5 simple CP-Bench problems (e.g., `001 car seq`, `022 graceful`) with logging enabled; inspect iteration count and token usage.
  2. Ablate the project prompt: replace with a minimal 5-line version and compare success rate on the same 5 problems.
  3. Enable `todo_write` on 2 hard problems (e.g., `009 basketball`, `014 crossfig`) and compare token overhead vs. baseline.

## Open Questions the Paper Calls Out

- **Task management tool effectiveness**: Are there specific problem families or complexity thresholds where explicit task management tools provide a consistent net benefit in agentic coding? The paper notes mixed results, with the tool helping specific hard problems but causing failures in others.

- **Prompt optimization frameworks**: Can automated prompt optimization frameworks close the performance gap between different frontier models on constraint modeling tasks? The paper suggests using frameworks like DSPy to automatically tune the project prompt for specific models.

- **Domain generalization**: Does the separation of domain expertise (via prompts) from execution infrastructure effectively generalize to scientific computing domains outside of constraint programming? The paper validates this primarily on CP-Bench and mentions only one follow-up regarding logic programming.

- **Fixed workflow comparison**: How do fixed workflow approaches perform on the clarified CP-Bench benchmark compared to the agentic workflow? The paper notes that direct comparison is not possible as fixed workflows can also benefit from the clarifications.

## Limitations

- Exact project prompt and system prompt content are not fully specified in the paper, requiring access to external repositories
- Results are based on Claude Sonnet 4.5, with performance variation across other models not fully characterized
- The clarification effort that improved ground-truth models may introduce bias toward the agent's solution patterns
- The 101-problem benchmark size is limited, and difficulty distribution analysis is not provided

## Confidence

- **High Confidence**: The iterative execution mechanism is well-supported by concrete examples and the ReAct framework is established in literature
- **Medium Confidence**: The claim that minimal prompts outperform extensive scaffolding is supported by ablation results but lacks direct corpus evidence
- **Low Confidence**: The persistent kernel advantage is asserted but not quantitatively validated against file-based alternatives

## Next Checks

1. **Prompt Sensitivity Test**: Replace the actual project prompt with a minimal 5-line version and rerun CP-Agent on 10 representative problems. Measure success rate drop and iteration count changes to quantify prompt dependency.

2. **Kernel Persistence Validation**: Implement a file-based alternative where each execution writes to a new file. Compare total execution time and success rate on 5 complex problems (e.g., basketball, crossfig) to isolate the kernel persistence benefit.

3. **Cross-Model Stress Test**: Run CP-Agent on the full 101-problem set using GPT-4o and Claude 3.5 Sonnet. Record per-problem success rates, iteration counts, and failure modes to assess model-specific limitations and robustness of the approach.