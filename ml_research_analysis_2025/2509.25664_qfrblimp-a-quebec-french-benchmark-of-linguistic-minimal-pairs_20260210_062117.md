---
ver: rpa2
title: 'QFrBLiMP: a Quebec-French Benchmark of Linguistic Minimal Pairs'
arxiv_id: '2509.25664'
source_url: https://arxiv.org/abs/2509.25664
tags:
- evaluation
- linguistic
- text
- language
- evaluators
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QFrBLiMP, the first linguistic minimal pairs
  benchmark for Quebec-French, consisting of 1,761 manually annotated sentence pairs
  across 20 grammatical phenomena. Sentences were extracted from an official Quebec
  government linguistic resource and annotated by 12 native speakers.
---

# QFrBLiMP: a Quebec-French Benchmark of Linguistic Minimal Pairs

## Quick Facts
- **arXiv ID**: 2509.25664
- **Source URL**: https://arxiv.org/abs/2509.25664
- **Reference count**: 40
- **Primary result**: First linguistic minimal pairs benchmark for Quebec-French, revealing scaling trends and limitations in LLM grammatical competence

## Executive Summary
This paper introduces QFrBLiMP, the first linguistic minimal pairs benchmark for Quebec-French, consisting of 1,761 manually annotated sentence pairs across 20 grammatical phenomena. Sentences were extracted from an official Quebec government linguistic resource and annotated by 12 native speakers. The benchmark was used to evaluate 77 open-source LLMs, revealing a clear scaling trend where larger models achieve higher grammatical accuracy. However, all models struggled with phenomena requiring deep semantic understanding, such as lexical semantics and orphaned prepositions, highlighting a critical limitation compared to human performance. Cross-dialectal analysis with MultiBLiMP showed that most models perform worse on Quebec-French, though the most advanced models demonstrate robust generalization.

## Method Summary
The benchmark evaluates linguistic knowledge by comparing the perplexity of minimally different sentence pairs, where one sentence is grammatical and the other contains a specific grammatical error. For each pair, the model "selects" the grammatical sentence if it assigns it a lower perplexity (higher probability). The dataset was constructed by manually extracting sentences from the official Quebec government linguistic resource "Banque de dÃ©pannage linguistique" (BDL), modifying them to create ungrammatical variants, and having 12 native speakers annotate the grammaticality of each sentence. Models were evaluated using the standard perplexity formula without any training, simply computing probabilities during inference.

## Key Results
- All 77 evaluated models showed a strong positive correlation between parameter count and grammatical accuracy, with diminishing returns at larger scales
- Instruction-tuned models consistently underperformed their base counterparts on formal grammatical tasks, with drops exceeding 10% on some phenomena
- All models failed dramatically on "lexical semantics" and "orphaned prepositions" phenomena, achieving accuracy below 70% even for the largest 72B parameter models
- Cross-dialectal analysis revealed that most models perform worse on Quebec-French compared to Metropolitan French, though the gap narrows for the most advanced models

## Why This Works (Mechanism)

### Mechanism 1: Perplexity-Based Grammaticality Proxy
The benchmark evaluates linguistic knowledge not by asking the model to classify text, but by comparing the calculated perplexity of sentence pairs. A model "selects" the grammatical sentence if the probability of the acceptable token sequence exceeds that of the manipulated sequence. This assumes perplexity strictly correlates with grammatical acceptability and is not confounded by lexical frequency or semantic plausibility.

### Mechanism 2: Scale-Dependent Formal Competence
Larger models have a higher capacity to internalize abstract formal rules present in the pre-training data. However, for phenomena lacking clear statistical markers, scaling alone hits a performance ceiling. The training data must contain sufficient examples of the target linguistic phenomena for the model to learn the underlying rules rather than just memorizing n-grams.

### Mechanism 3: Instruction-Tuning Interference
The alignment process may prioritize conversational fluency and helpfulness over strict adherence to formal prescriptive grammar, leading to "catastrophic forgetting" of formal rules learned during pre-training. This degradation is caused by the alignment process itself rather than just the increased complexity of the IT model.

## Foundational Learning

- **Concept**: **Minimal Pairs**
  - **Why needed here**: This is the fundamental unit of evaluation in QFrBLiMP. Unlike benchmarks that ask for a binary classification of a single sentence, this method requires comparing two sentences differing by exactly one feature to isolate specific linguistic knowledge.
  - **Quick check question**: Can you explain why comparing the perplexity of two sentences is a more robust test of *grammatical* knowledge than calculating the perplexity of a single sentence?

- **Concept**: **Perplexity (PPL) and Token Probability**
  - **Why needed here**: The paper uses perplexity as the proxy for "acceptability." Understanding how PPL is calculated is critical to interpreting the results tables.
  - **Quick check question**: If a model assigns a probability of $0.8$ to a token, is the perplexity for that token higher or lower than if it assigned $0.2$?

- **Concept**: **Formal vs. Functional Linguistic Competence**
  - **Why needed here**: The paper argues that models excel at "formal" competence (rules, statistics) but fail at "functional/deep" competence (semantics, world knowledge). Distinguishing these is key to understanding the "Unsolved Phenomena" section.
  - **Quick check question**: Why might a model correctly solve a complex syntactic island constraint but fail to identify that "ancien toxicomane" implies specific semantic properties that an "anglicism" might violate?

## Architecture Onboarding

- **Component map**: BDL (Official Quebec-French grammar resource) -> Manual extraction and modification -> 20 Linguistic Phenomena classification -> 12 Native Speaker Annotations -> 1,761 MPs dataset -> Perplexity calculation evaluation on 77 Open-Source LLMs

- **Critical path**: The **Annotation Phase**. High inter-annotator agreement (WAWA) is crucial. Low agreement on specific phenomena directly correlates with lower model performance and higher noise in evaluation.

- **Design tradeoffs**:
  - **Human Annotation vs. Synthetic Generation**: Manual modification of real texts over synthetic generation results in a smaller dataset but claims higher ecological validity.
  - **Prescriptivism vs. Usage**: The BDL is a prescriptive source. The benchmark tests adherence to *official* rules, not necessarily how people actually speak, which may bias against models trained on informal web data.

- **Failure signatures**:
  - **Lexical Semantics (LP 9)**: Models fail to distinguish between a standard term and an Anglicism/non-standard term, relying on co-occurrence frequency rather than normative correctness.
  - **Instruction Tuning Drop**: A significant accuracy drop (>10%) between a base model and its IT variant on "Standard Negation" (LP 7).
  - **Dialect Overfitting**: Models performing well on MultiBLiMP (Metropolitan French) but failing QFrBLiMP indicate a lack of cross-dialectal robustness.

- **First 3 experiments**:
  1. **Establish a Scaling Baseline**: Plot the accuracy of the 77 provided models against their parameter counts. Verify the "diminishing returns" curve visually and check if it holds for *all* 20 LPs or just the "mastered" ones.
  2. **Probe the IT-Drop**: Select the top-performing base model and its IT variant. Compare their specific errors on "Standard Negation" (LP 7) to confirm if the IT model is outputting conversational fillers that violate formal negation rules.
  3. **Phenomena Error Analysis**: Isolate the sentences for the two "Unsolved Phenomena." Check if the models' incorrect choices are simply the more frequent n-grams in the training data, suggesting surface-level bias.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can developing a specific Quebec-French training corpus optimize LLMs to outperform general French-specialized models on grammatical competence?
- **Basis in paper**: [explicit] Section 6 states future work could "address the lack of effect of language specialization by developing a training corpus to optimize LLM on Quebec-French."
- **Why unresolved**: Current French-specialized models do not consistently outperform general models on QFrBLiMP; it remains untested if dialect-specific data is the missing factor.
- **What evidence would resolve it**: Benchmark results showing that models fine-tuned on a curated Quebec-French corpus achieve statistically significantly higher accuracy than those trained on general French data.

### Open Question 2
- **Question**: Do alternative evaluation methods like targeted probing reveal distinct capabilities for phenomena like "orphaned prepositions" where perplexity fails?
- **Basis in paper**: [explicit] Section 6 proposes to "investigate alternative evaluation methods, such as targeted probing or semantic similarity scores, for phenomena that yield low accuracies."
- **Why unresolved**: The current perplexity-based evaluation suggests models fail on deep semantics, but this may be a limitation of the metric rather than a total lack of knowledge.
- **What evidence would resolve it**: Experiments showing that probing classifiers or semantic similarity metrics yield significantly higher accuracy for "orphaned prepositions" than the current perplexity ranking method.

### Open Question 3
- **Question**: Can novel alignment techniques enhance conversational abilities without causing the observed degradation in formal grammatical knowledge?
- **Basis in paper**: [explicit] Section 6 outlines the aim to "develop novel alignment techniques that can enhance conversational abilities without sacrificing the model's foundational LC."
- **Why unresolved**: Current instruction-tuning sometimes degrades formal grammar performance, possibly due to catastrophic forgetting.
- **What evidence would resolve it**: The development of an alignment method that maintains or improves QFrBLiMP scores while simultaneously improving performance on standard conversational benchmarks.

## Limitations

- **Data Contamination Risk**: The use of publicly available government linguistic resources creates potential for contamination, where models trained on web data may have seen similar sentence pairs during pre-training.
- **Instruction Tuning Ambiguity**: The paper doesn't clearly specify whether templates were stripped from IT models during PPL calculation, which could artificially deflate their scores.
- **Semantic Competence Measurement**: The perplexity-based evaluation may not adequately capture semantic knowledge, as surface-level statistical patterns could dominate the probability calculations.

## Confidence

**High Confidence**:
- The scaling trend showing larger models achieve higher grammatical accuracy on formal syntactic rules
- The cross-dialectal performance drop where models perform worse on Quebec-French compared to Metropolitan French
- The specific finding that instruction-tuning degrades formal grammatical competence in base models

**Medium Confidence**:
- The claim that QFrBLiMP captures "true" Quebec-French grammatical competence given its prescriptive source material
- The interpretation that diminishing returns indicate fundamental model limitations rather than data saturation
- The inter-annotator agreement values, which may be inflated by majority voting among 12 annotators

**Low Confidence**:
- The assertion that unsolved phenomena reveal fundamental model limitations rather than evaluation methodology issues
- The generalizability of findings to non-open-source models or models trained on different data distributions
- The specific mechanism by which instruction-tuning degrades grammatical performance

## Next Checks

1. **Contamination Control Experiment**: Create a subset of QFrBLiMP using sentences from less accessible BDL sections and re-evaluate top-performing models. If accuracy drops significantly, contamination is likely inflating reported performance.

2. **Instruction Tuning Methodology Clarification**: Test a single instruction-tuned model with three conditions: no template, standard template, and prompt-tuned template. Compare PPL-based accuracy to isolate whether the drop is due to evaluation methodology or actual degradation.

3. **Semantic Competence Probe**: For the unsolved phenomena, manually examine model outputs for the highest-probability sentence choices. Determine if incorrect selections follow surface-level statistical patterns rather than true semantic reasoning failures.