---
ver: rpa2
title: Do graph neural network states contain graph properties?
arxiv_id: '2411.02168'
source_url: https://arxiv.org/abs/2411.02168
tags:
- graph
- properties
- layers
- node
- probing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a model-agnostic pipeline to probe Graph Neural
  Networks (GNNs) for graph-theoretic properties using diagnostic classifiers. The
  authors propose probing embeddings with linear classifiers to measure how well specific
  graph properties (like number of squares, average degree, spectral radius) are linearly
  separable.
---

# Do graph neural network states contain graph properties?

## Quick Facts
- arXiv ID: 2411.02168
- Source URL: https://arxiv.org/abs/2411.02168
- Authors: Tom Pelletreau-Duris; Ruud van Bakel; Michael Cochez
- Reference count: 40
- Key outcome: This work introduces a model-agnostic pipeline to probe Graph Neural Networks (GNNs) for graph-theoretic properties using diagnostic classifiers. The authors propose probing embeddings with linear classifiers to measure how well specific graph properties (like number of squares, average degree, spectral radius) are linearly separable. Experiments on Grid-House and ClinTox datasets show that properties most useful for the classification task (e.g., #squares for Grid-House, average degree for ClinTox) consistently achieve the highest R² scores across layers. GIN models perform best, with regularization methods affecting property separation differently. The findings validate that GNNs leverage specific structural graph properties for classification, offering interpretability into their decision-making process. This work bridges GNN explainability with linear representation theory and provides a systematic framework for analyzing GNN inductive bias through graph-theoretic features.

## Executive Summary
This paper introduces a model-agnostic pipeline to probe Graph Neural Networks (GNNs) for graph-theoretic properties using linear diagnostic classifiers. The authors systematically evaluate whether GNNs encode specific graph properties (e.g., number of squares, average degree, spectral radius) in their hidden representations by measuring how well these properties can be linearly predicted from embeddings. Experiments on Grid-House and ClinTox datasets reveal that properties most useful for the classification task consistently achieve the highest R² scores across layers, with GIN models showing superior performance due to their higher expressive power. The findings provide interpretable insights into how GNNs leverage structural graph properties for classification decisions.

## Method Summary
The authors propose a probing pipeline where GNNs are first trained to convergence on a graph classification task. Intermediate activations are then extracted at each layer and fed into linear classifiers that predict various graph-theoretic properties. R² scores measure how well each property can be linearly decoded from the representations. The approach is tested on two datasets: Grid-House (synthetic graphs with grid/house motifs) and ClinTox (molecular toxicity prediction). Multiple GNN architectures (GCN, GIN, GAT) and regularization methods (L2, dropout) are compared. Property encoding is analyzed across layers and pooling strategies to understand how structural information is progressively refined during message passing.

## Key Results
- Properties most useful for classification (e.g., #squares in Grid-House, average degree in ClinTox) consistently achieve highest R² scores across layers
- GIN models show superior probing performance due to their Weisfeiler-Lehman equivalent expressivity
- Regularization methods significantly affect property separation: L2 sharpens selective representations while dropout increases polysemanticity
- Early layers contain predictive information for some global graph features, though the mechanism remains unclear

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear probes can detect whether GNN representations encode specific graph-theoretic properties in linearly accessible directions.
- Mechanism: A linear classifier g maps intermediate representations f^l(x) to property predictions. High R² indicates that the property variance is predictable from the embedding—implying a separating hyperplane exists in representation space. The approach rests on the linear representation hypothesis: features are directions in activation space.
- Core assumption: If a property is linearly decodable, it is likely used by the network for the primary task (rather than being inferred artifactually by a complex probe).
- Evidence anchors:
  - [abstract] "We propose probing embeddings with linear classifiers to measure how well specific graph properties (like number of squares, average degree, spectral radius) are linearly separable."
  - [section 2.3] "If a linear probe performs well, it suggests the existence of a hyperplane in the representation space that separates the inputs based on their properties, indicating linear separability."
  - [corpus] Weak/indirect—related work on GNN explainability (e.g., "Explainable Graph Neural Networks via Structural Externalities") focuses on instance-level or motif-level explanations, not probing for graph-theoretic properties via linear classifiers.
- Break condition: If non-linear probes substantially outperform linear probes on a property with zero linear separability, that property may be encoded non-linearly; the current diagnostic would yield false negatives.

### Mechanism 2
- Claim: Task-relevant graph properties become increasingly linearly decodable across layers, guided by the supervision signal.
- Mechanism: Under message passing, each layer expands the receptive field, aggregating neighborhood information. Supervision selectively reinforces directions in activation space aligned with informative properties (e.g., motif counts, degree distributions). Consequently, properties most useful for classification achieve highest R² in later layers.
- Core assumption: The supervision signal shapes representations to make task-relevant features linearly accessible, rather than distributing them polysemantically across non-linear combinations.
- Evidence anchors:
  - [abstract] "Experiments on Grid-House and ClinTox datasets show that properties most useful for the classification task (e.g., #squares for Grid-House, average degree for ClinTox) consistently achieve the highest R² scores across layers."
  - [section 5.1] "The probing results in fig. 3 demonstrate that the number of squares consistently yields the highest R² scores... effectively partitions the graphs into two classes."
  - [corpus] Weak/absent—no direct corpus evidence; related papers focus on different explainability paradigms.
- Break condition: If multiple properties are equally predictive of the label but only one shows high R², the mechanism may be selecting for encoding convenience rather than task necessity.

### Mechanism 3
- Claim: GIN's injective aggregation yields richer structural encoding than GCN or GAT, enabling higher probe performance on graph-theoretic properties.
- Mechanism: GIN uses an MLP-based injective update that mimics the Weisfeiler-Lehman test, preserving more discriminative information during aggregation. GCN uses normalized mean aggregation, which can conflate distinct structures; GAT uses attention, which may smooth representations. This expressivity hierarchy (GIN ≥ GAT > GCN) affects how cleanly properties are encoded.
- Core assumption: Higher WL-expressivity directly translates to more linearly separable property encoding in hidden states.
- Evidence anchors:
  - [section 2.1] "GIN aggregates node features in a way that mimics the Weisfeiler-Lehman test... it is likely to excel at encoding complex graph properties."
  - [section 5.1] "GIN showing enhanced expressivity" with R² up to 0.93 for #squares vs. lower scores for GCN/GAT.
  - [corpus] Weak—related work does not directly compare WL-expressivity to probing R² across architectures.
- Break condition: If a task requires only local properties accessible to GCN, GIN's advantage may diminish; conversely, if GAT's attention aligns with task-relevant substructures, it could match or exceed GIN.

## Foundational Learning

- Concept: Linear probing classifiers
  - Why needed here: The entire diagnostic framework depends on understanding that a linear probe tests for the existence of a separating hyperplane; high R² implies the property is encoded linearly, not just that the probe is powerful.
  - Quick check question: If you train a 3-layer MLP probe and achieve R²=0.95 on property P, can you conclude that the base GNN encodes P linearly? Why or why not?

- Concept: Weisfeiler-Lehman (WL) test and GNN expressivity
  - Why needed here: The paper ties GIN's superior probe performance to its WL-equivalent expressivity; understanding what WL tests (graph isomorphism via iterative neighborhood aggregation) clarifies why injective updates preserve more structural information.
  - Quick check question: Given two non-isomorphic graphs that GCN maps to identical representations, would GIN necessarily distinguish them? Under what conditions might it still fail?

- Concept: Graph-theoretic properties (local vs. global)
  - Why needed here: The probe targets properties ranging from local (node degree, clustering coefficient) to global (diameter, spectral radius); knowing which require multi-hop aggregation helps interpret why certain properties emerge only in deeper layers.
  - Quick check question: For a 2-layer GNN with mean aggregation, which property is more likely to be linearly decodable: average degree or diameter? Explain.

## Architecture Onboarding

- Component map:
  GNN backbone (GCN/GIN/GAT) -> pooling (mean/sum/max) -> dense MLP -> classification head.
  Probe pipeline: extract intermediate activations -> optional norm-sorting -> linear probe -> R² evaluation per property per layer.

- Critical path:
  1. Train GNN to convergence on classification task; log best checkpoint by accuracy.
  2. Extract activations at each layer (pre- and post-pooling).
  3. For each graph-theoretic property, train a linear regressor/classifier to predict the property from activations; compute R² on held-out set.
  4. Identify top-scoring properties; verify alignment with task requirements.

- Design tradeoffs:
  - Linear vs. non-linear probes: Linear probes avoid false positives from classifier inference but may miss non-linearly encoded properties.
  - Pooling method: Mean pooling may smooth property information; sum preserves magnitude (sensitive to graph size); max highlights extremes.
  - Regularization: L2 encourages selective, separable representations; dropout increases polysemanticity, reducing probe R².

- Failure signatures:
  - Near-zero R² for all properties across layers -> representations may be collapsed, undertrained, or task is solved via non-structural features (e.g., node attributes).
  - High R² for irrelevant properties -> possible dataset leakage or confounding (e.g., number of nodes correlates with label).
  - Large gap between train and test R² -> probe overfitting; increase probe regularization or use held-out graphs.

- First 3 experiments:
  1. Reproduce Grid-House probe results with GIN: confirm #squares achieves R² > 0.85 in post-pooling layers; verify that alternative properties (e.g., #triangles) score substantially lower.
  2. Ablate pooling methods on ClinTox with GIN: compare mean vs. sum vs. max pooling for average degree and spectral radius R²; document sensitivity to graph size.
  3. Test regularization impact on a synthetic dataset: train GIN with and without L2/dropout; plot R² trajectories for task-relevant vs. irrelevant properties across layers to confirm hypothesis that L2 sharpens separation while dropout diffuses it.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do early GNN layers contain predictive information for global graph features despite theoretical limitations on their receptive fields?
- Basis: [explicit] The authors state in Section 7: "As we observed, early layers in the network contain predictive information for some global graph features. It is however unclear why this is the case."
- Why unresolved: The message-passing paradigm suggests early layers aggregate only local neighborhood information, making the emergence of global properties in these layers theoretically unexpected.
- What evidence would resolve it: An analysis identifying specific local correlates in the data distribution that allow early layers to proxy global properties, or a theoretical proof of information leakage.

### Open Question 2
- Question: How does the type of supervision signal (unsupervised, self-supervised, or supervised) influence the encoding of graph properties?
- Basis: [explicit] The authors state: "Studying the supervision signal comparing unsupervised, self-supervised and supervised models would be very insightful."
- Why unresolved: The current study focuses exclusively on supervised models, leaving the impact of different training paradigms on the formation of structural representations unknown.
- What evidence would resolve it: Comparative probing experiments using R² scores on the same architecture trained with various supervision objectives (e.g., contrastive loss vs. cross-entropy).

### Open Question 3
- Question: Do higher-order GNNs (e.g., 2-WL or 3-WL equivalents) encode structural properties differently than 1-WL equivalent models?
- Basis: [explicit] The authors propose that "An extensive exploration of 1-WL, 2-WL and 3-WL GNN equivalent could bolster the paper’s contributions by showing clear restrictions and capabilities of these models."
- Why unresolved: The current experiments focus on GCN, GIN, and GAT, which are generally 1-WL equivalent; the probing capabilities of more expressive architectures remain untested.
- What evidence would resolve it: Applying the probing pipeline to higher-order architectures to see if they capture properties that 1-WL models cannot linearly separate.

## Limitations
- The linear probing assumption may miss non-linearly encoded properties that are still task-relevant
- The approach cannot distinguish between properties that are actively used versus incidentally correlated with labels
- Early layer encoding of global properties remains theoretically unexplained and may be dataset-specific

## Confidence
- Primary claim (GIN best, regularization effects, task-relevant properties most linear): High
- Mechanism 1 (linear probes test linear encoding): High
- Mechanism 2 (supervision shapes linear accessibility): Medium
- Mechanism 3 (GIN expressivity → better encoding): Low

## Next Checks
1. **Probe bottleneck test**: Train GIN on Grid-House, then restrict the probe to the last hidden layer only. If #squares still achieves R² > 0.8, this strengthens evidence that the property is encoded, not just inferred.
2. **Non-linear probe ablation**: Replace linear probes with small MLPs (2 hidden layers) on the same properties. If non-linear probes substantially outperform linear ones on properties with low R², this indicates non-linear encoding and questions the current diagnostic.
3. **Synthetic confounding test**: Construct a dataset where property P correlates with the label but is irrelevant to the task. If P still achieves high R², this would challenge the claim that high probe scores imply property usage.