---
ver: rpa2
title: Exploiting Information Redundancy in Attention Maps for Extreme Quantization
  of Vision Transformers
arxiv_id: '2508.16311'
source_url: https://arxiv.org/abs/2508.16311
tags:
- attention
- vision
- weights
- quantization
- sparsity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the computational and memory inefficiencies\
  \ of Vision Transformers (ViTs) due to their Multi-Head Self-Attention (MHSA) mechanisms.\
  \ The authors propose Entropy Attention Maps (EAM), a method that exploits information\
  \ redundancy in attention maps by identifying low-entropy attention heads\u2014\
  those exhibiting stable and predictable patterns across inputs."
---

# Exploiting Information Redundancy in Attention Maps for Extreme Quantization of Vision Transformers

## Quick Facts
- arXiv ID: 2508.16311
- Source URL: https://arxiv.org/abs/2508.16311
- Authors: Lucas Maisonnave; Karim Haroun; Tom Pegeau
- Reference count: 40
- Key outcome: Achieves up to 20% sparsity in attention maps with similar or higher accuracy than baseline, outperforming random fixing of attention weights.

## Executive Summary
This paper addresses the computational and memory inefficiencies of Vision Transformers (ViTs) due to their Multi-Head Self-Attention (MHSA) mechanisms. The authors propose Entropy Attention Maps (EAM), a method that exploits information redundancy in attention maps by identifying low-entropy attention heads—those exhibiting stable and predictable patterns across inputs. These heads are frozen and quantized to low precision (as low as 4 bits), reducing computational complexity without significantly impacting model performance. Experiments on ImageNet-1K across various ViT architectures (DeiT, Swin) show that EAM achieves similar or higher accuracy at up to 20% sparsity in attention maps and competitive performance beyond this level.

## Method Summary
The method applies post-training quantization to ViT models, then computes Shannon entropy for each attention weight using 8-bit histograms over a 5% calibration set of ImageNet-1K. Attention weights with lowest entropy are identified and frozen by replacing them with their mean values from the calibration set. These frozen weights are optionally quantized to 4-bit precision. The approach targets the most stable, predictable attention patterns that contribute least unique information to the model's decisions.

## Key Results
- EAM achieves similar or higher accuracy than RepQ-ViT baseline at up to 20% sparsity in attention maps
- At 10-20% sparsity, EAM improves accuracy compared to RepQ-ViT baseline in most models
- Ablation study confirms entropy-based approach outperforms random fixing of attention weights
- Swin models show higher robustness to attention freezing than DeiT models due to hierarchical structure

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Low-entropy attention heads exhibit deterministic behavior across inputs, indicating they contribute less unique information and can be frozen.
- **Mechanism:** Shannon entropy is calculated for each attention head over a calibration dataset (5% of ImageNet-1K). Heads with low variance in their weight distributions (low entropy) are identified as stable. The paper posits that because these weights do not change significantly regardless of the input, their dynamic computation is redundant.
- **Core assumption:** Stability (low entropy) implies functional redundancy, such that replacing dynamic weights with their statistical mean preserves model performance.
- **Evidence anchors:**
  - [abstract] "attention heads with lower entropy... tend to contribute less information, motivating targeted compression strategies."
  - [section 4.1] "entropy... provides a robust metric to identify sensitive and useful parameters... inversely related to the predictability of an event."
  - [corpus] "Towards Understanding the Nature of Attention with Low-Rank Sparse Decomposition" supports the general concept of disentangling attention components, though specific entropy-to-redundancy links are unique to this paper.
- **Break condition:** If the calibration dataset is not representative of the input distribution, "stable" heads may actually be high-variance on unseen data, leading to accuracy collapse when frozen.

### Mechanism 2
- **Claim:** Freezing and quantizing low-entropy weights avoids redundant re-computation and reduces memory bandwidth without altering model performance at moderate sparsity.
- **Mechanism:** A mask identifies the τ% lowest entropy weights. During inference, instead of computing QK^T, these specific weights are replaced by their pre-computed mean values (Eq. 15) and quantized to 4-bit. This skips the dot-product and softmax for these positions.
- **Core assumption:** The error introduced by using a constant mean value and 4-bit quantization for low-entropy weights falls within the model's tolerance margin.
- **Evidence anchors:**
  - [abstract] "EAM... freezes the weights of low-entropy attention maps and quantizes these values to low precision to avoid redundant re-computation."
  - [results] "At 10-20% sparsity, EAM even improves accuracy compared to the RepQ-ViT baseline."
  - [corpus] No direct corpus evidence supports the specific "mean-value replacement" mechanism for attention quantization.
- **Break condition:** If sparsity (τ) exceeds a threshold (e.g., 30% for DeiT, 50% for Swin), high-entropy weights are inadvertently frozen, causing rapid accuracy degradation.

### Mechanism 3
- **Claim:** Hierarchical architectures (Swin) are more robust to attention freezing than global attention architectures (DeiT/ViT).
- **Mechanism:** Swin Transformers restrict attention to local windows. The paper suggests that errors from frozen weights are confined to a single window, allowing subsequent layers to recover information through merging/downsampling. DeiT uses global attention, where errors propagate across the entire feature map.
- **Core assumption:** Local windowing provides inherent structural robustness to noise introduced by quantization/freezing.
- **Evidence anchors:**
  - [section 5.2.2] "Swin-based models... can tolerate up to 50% sparsity... DeiT relies on global attention... randomly fixing parts... shows a higher impact."
  - [corpus] Weak external validation; comparison relies on the authors' ablation study (Figure 6).
- **Break condition:** Applying aggressive sparsity levels (optimized for Swin) to a global-attention model (DeiT) will likely cause failure.

## Foundational Learning

- **Concept:** Shannon Entropy in Information Theory
  - **Why needed here:** This is the selection criteria for compression. You must understand that low entropy = high predictability = low "surprise" to understand why these weights are safe to freeze.
  - **Quick check question:** If an attention weight always outputs 0.9 regardless of the input image, is its entropy high or low?

- **Concept:** Multi-Head Self-Attention (MHSA) Mechanics
  - **Why needed here:** The method modifies the A = softmax(QK^T/√d) step. Understanding that A is an N × N matrix of weights is required to visualize what is being frozen.
  - **Quick check question:** Does freezing a weight in the attention map skip the calculation of the Query, Key, or the final aggregation?

- **Concept:** Post-Training Quantization (PTQ)
  - **Why needed here:** EAM is applied on top of a 4-bit PTQ baseline (RepQ-ViT). You need to distinguish between the noise from quantization vs. the noise from the EAM freezing mechanism.
  - **Quick check question:** Does EAM require retraining the model weights, or can it be applied to a pre-trained model?

## Architecture Onboarding

- **Component map:** Calibration Phase -> Mask Generation -> Weight Freezing -> Quantization
- **Critical path:** The estimation of the distribution p_{l,h}(k) (Eq. 12) using histograms. If the histogram bins are too coarse or the calibration set too small, the entropy calculation will be noisy, leading to the freezing of wrong weights.
- **Design tradeoffs:**
  - **DeiT vs. Swin:** You can push sparsity much higher (40-50%) on Swin models than DeiT (20-30%) before seeing degradation.
  - **EAM-FP32 vs. EAM-4bits:** Computing EAM on the already quantized model (EAM-4bits) is more robust at high sparsity levels than computing it on the full-precision model.
- **Failure signatures:**
  - **Layer 1 Sensitivity:** Section 4.3 notes quantization noise is highest in initial layers. Monitor Layer 1 closely; freezing here is riskier than in deeper layers.
  - **Accuracy Collapse >30% (DeiT):** If Top-1 accuracy drops non-linearly after 30% sparsity on DeiT, the entropy threshold is too aggressive.
- **First 3 experiments:**
  1. **Validation Check:** Implement the ablation study (Figure 6) - compare random masking vs. entropy-based masking on DeiT-Tiny at 20% sparsity to verify the selection mechanism works.
  2. **Sparsity Sweep:** Run a sweep of τ from 0% to 50% on both DeiT-Base and Swin-S to confirm the "robustness gap" between architectures.
  3. **Calibration Size Test:** Rerun the entropy calculation using 1% vs 5% vs 10% of ImageNet data to determine the minimum calibration set size required for stable mask generation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the EAM method be effectively generalized to Large Language Models (LLMs) and Vision-Language Models (VLMs) which possess significantly longer context sequences?
- **Basis:** [explicit] The conclusion explicitly states an interest in extending the method to VLMs and LLMs to validate if the higher computational cost of MHSA in these architectures yields greater savings.
- **Why unresolved:** The current study is restricted to Vision Transformers (DeiT, Swin) on ImageNet-1K, and it is unproven whether the low-entropy stability observed in ViTs transfers to the complex, variable-length sequences of NLP.
- **What evidence would resolve it:** Successful application of EAM to standard LLM benchmarks (e.g., WikiText, LAMBADA) demonstrating consistent perplexity or accuracy metrics at targeted sparsity levels.

### Open Question 2
- **Question:** Would enabling model retraining after freezing the low-entropy weights allow for higher sparsity ratios without accuracy degradation?
- **Basis:** [explicit] The authors list "enabling the model to retrain with attention weights fixed with EAM" as a specific avenue for future work to minimize loss.
- **Why unresolved:** The proposed method is currently a post-training quantization (PTQ) approach relying on calibration data; the weights are not updated to adapt to the frozen attention maps, potentially leaving accuracy on the table.
- **What evidence would resolve it:** Experiments comparing the accuracy-sparsity trade-off of the current EAM against a version where the model undergoes fine-tuning after the freezing and quantization process.

### Open Question 3
- **Question:** Does the hierarchical, window-based structure of Swin Transformers inherently provide better protection against error propagation from fixed attention weights compared to the global attention in DeiT?
- **Basis:** [inferred] Results show Swin models tolerate up to 50% sparsity with minimal loss, whereas DeiT models degrade faster. The authors hypothesize this is due to Swin's local attention confining damage to single windows, but this is not verified.
- **Why unresolved:** The paper identifies the behavioral discrepancy but does not isolate the architectural mechanism (local windows vs. global attention) as the definitive cause.
- **What evidence would resolve it:** An ablation study varying window sizes in Swin or analyzing error propagation across layers to confirm that local buffering prevents the degradation seen in global attention models.

## Limitations

- The method's effectiveness varies significantly between DeiT and Swin architectures, with no theoretical explanation for this architectural sensitivity.
- The entropy-based weight selection relies heavily on the representativeness of the 5% calibration set, which could lead to accuracy collapse if not representative.
- No evaluation against more recent transformer compression methods is presented, limiting comparison scope.

## Confidence

- **High confidence**: The entropy calculation methodology and the basic mechanism of freezing low-entropy weights are clearly specified and reproducible. The performance improvements at low sparsity (10-20%) are well-documented.
- **Medium confidence**: The architectural robustness claims (Swin tolerating 50% vs DeiT at 30%) are supported by experimental results but lack theoretical grounding. The calibration subset selection process has unknown impact on final performance.
- **Low confidence**: The transferability of results to non-ImageNet datasets and the method's behavior with different transformer variants (not just DeiT/Swin) remain untested.

## Next Checks

1. **Calibration Set Sensitivity**: Run entropy computation with varying calibration set sizes (1%, 5%, 10% of ImageNet) to determine minimum required size for stable mask generation.
2. **Architectural Transfer Test**: Apply EAM to a global-attention architecture outside the DeiT family (e.g., vanilla ViT) to validate if the 30% sparsity threshold holds.
3. **Cross-Dataset Generalization**: Evaluate EAM-compressed models on a distributionally different dataset (e.g., CIFAR-100 or a medical imaging dataset) to test calibration set representativeness assumptions.