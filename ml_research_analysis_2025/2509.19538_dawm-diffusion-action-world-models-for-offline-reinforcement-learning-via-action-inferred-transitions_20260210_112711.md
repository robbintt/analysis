---
ver: rpa2
title: 'DAWM: Diffusion Action World Models for Offline Reinforcement Learning via
  Action-Inferred Transitions'
arxiv_id: '2509.19538'
source_url: https://arxiv.org/abs/2509.19538
tags:
- offline
- learning
- world
- action
- dawm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DAWM, a diffusion-based world model for offline
  reinforcement learning that generates synthetic state-reward trajectories and infers
  missing actions using an inverse dynamics model (IDM). Unlike prior methods that
  either omit actions or jointly model all components (leading to training instability),
  DAWM adopts a modular design where a diffusion model generates action-free rollouts
  conditioned on state, action, and return-to-go, and an IDM reconstructs the actions
  to form complete (s,a,r,s') transitions.
---

# DAWM: Diffusion Action World Models for Offline Reinforcement Learning via Action-Inferred Transitions

## Quick Facts
- arXiv ID: 2509.19538
- Source URL: https://arxiv.org/abs/2509.19538
- Authors: Zongyue Li, Xiao Han, Yusong Li, Niklas Strauss, Matthias Schubert
- Reference count: 18
- Key outcome: DAWM achieves 9.3% and 9.5% improvements over DWM and DD baselines respectively on 9 D4RL locomotion tasks by generating synthetic state-reward trajectories and inferring missing actions via an inverse dynamics model

## Executive Summary
This paper introduces DAWM, a diffusion-based world model for offline reinforcement learning that generates synthetic state-reward trajectories and reconstructs missing actions using an inverse dynamics model. Unlike prior methods that either omit actions or jointly model all components (leading to training instability), DAWM adopts a modular design where a diffusion model generates action-free rollouts conditioned on state, action, and return-to-go, and an IDM reconstructs the actions to form complete (s,a,r,s') transitions. This approach enables one-step TD learning in offline RL and improves training stability. Empirically, DAWM-trained TD3BC and IQL agents outperform prior diffusion-based baselines across 9 D4RL locomotion tasks, achieving 9.3% and 9.5% average improvements respectively.

## Method Summary
DAWM uses a two-stage pipeline: first, a conditional diffusion world model generates future state-reward sequences based on current state, action, and return-to-go conditioning; second, an inverse dynamics model infers the actions that would have produced these state transitions. The diffusion model uses a U-Net backbone with temporal convolutions and classifier-free guidance to denoise trajectory noise into coherent sequences. The IDM is trained as a diagonal Gaussian to predict actions from state transitions. This modular approach produces complete (s,a,r,s') transitions suitable for one-step TD-based offline RL, avoiding the training instability associated with joint modeling of all components.

## Key Results
- DAWM achieves 9.3% improvement over DWM-TD3BC and 9.5% improvement over DD-IQL on 9 D4RL locomotion tasks
- DAWM generates 8× more synthetic transitions than real data while maintaining comparable performance to agents trained on real offline datasets
- DAWM's modular design produces complete synthetic transitions suitable for one-step TD-based offline RL, avoiding training instability from joint modeling
- Performance remains stable across horizons H∈{1,3,7}, suggesting conditioning provides consistent guidance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Modular separation of trajectory generation from action inference reduces training instability compared to joint (s, a, r) modeling.
- **Mechanism:** The diffusion model generates only state-reward sequences conditioned on (s_t, a_t, R_tg), avoiding the optimization challenges of simultaneously learning action distributions. A separately trained IDM then reconstructs actions via supervised learning on state transitions.
- **Core assumption:** Action-free trajectory generation is sufficiently stable that errors do not propagate catastrophically to the IDM.
- **Evidence anchors:** [abstract] "joint modeling of states, rewards, and actions...often lead to increased training complexity and reduced performance"; [section 1] "prior work...has shown that such joint modeling can suffer from significant training instability"

### Mechanism 2
- **Claim:** Complete (s, a, r, s') transitions enable one-step TD learning, which improves critic regularization compared to multi-step value expansion.
- **Mechanism:** The IDM f_φ(a_t|s_{t-m:t-1}, s_t, s_{t+1}) produces action labels for generated trajectories, allowing standard Bellman updates y = r + γQ(s', π(s')). This avoids the diminishing returns of multi-step targets even with perfect dynamics (cited in paper).
- **Core assumption:** The IDM generalizes adequately to diffusion-generated states not present in the original dataset.
- **Evidence anchors:** [abstract] "This modular design produces complete synthetic transitions suitable for one-step TD-based offline RL"; [section 1] References Palenicek et al. (2022) showing "multi-step value expansion methods may offer diminishing returns even when the dynamics model is perfectly accurate"

### Mechanism 3
- **Claim:** Conditioning diffusion on return-to-go (R_tg) produces trajectories targeted toward high-value regions of state-action space.
- **Mechanism:** The model generates p_θ(ŝ_{t+1:t+H+1}, r̂_{t:t+H}|s_t, a_t, R_tg), steering generation toward desired cumulative rewards. This increases synthetic data density in relevant regions rather than uniformly sampling dynamics.
- **Core assumption:** The conditioning signal R_tg is reliably correlated with achievable returns in the generated trajectories.
- **Evidence anchors:** [section 3.1] "R_tg is calculated as a discounted cumulative reward...normalized by dividing it by a constant specific to each environment"; [section 4.4] Performance remains stable across horizons H∈{1,3,7}

## Foundational Learning

- **Concept: Diffusion models as conditional sequence generators**
  - Why needed here: DAWM uses a U-Net backbone with temporal convolutions, trained via score matching, to denoise trajectory noise into coherent state-reward sequences
  - Quick check question: Can you explain how classifier-free guidance balances conditional vs unconditional generation during sampling?

- **Concept: Inverse Dynamics Models (IDM)**
  - Why needed here: The IDM f_φ predicts a_t given (s_{t-1}, s_t, s_{t+1}), enabling action recovery from state-only trajectories
  - Quick check question: What failure mode occurs if the environment has stochastic transitions where multiple actions could produce the same state transition?

- **Concept: One-step TD learning in offline RL**
  - Why needed here: DAWM's value proposition is enabling TD3BC/IQL-style Bellman updates on synthetic data, which requires (s, a, r, s') tuples
  - Quick check question: Why does multi-step TD expansion suffer diminishing returns even with perfect dynamics models?

## Architecture Onboarding

- **Component map:** Diffusion World Model (U-Net) -> IDM (MLP) -> RL Agent (TD3BC/IQL) -> Synthetic transitions
- **Critical path:** 1) Pre-train diffusion model on offline dataset (2M steps, k=5 diffusion steps) 2) Train IDM on action-labeled subsequences 3) Generate rollouts with H=7 horizon, k=3 inference steps 4) Apply IDM to reconstruct actions → form complete transitions 5) Train RL agent on mixed real + synthetic data
- **Design tradeoffs:** Modular design (separate diffusion + IDM) vs joint modeling: Sacrifices potential end-to-end optimization for training stability; Horizon H=7: Shorter horizons reduce compounding error but provide fewer TD targets per rollout; 8× data augmentation: More transitions improve coverage but may amplify model errors if distribution shifts
- **Failure signatures:** Low IDM accuracy on generated states → incorrect action labels → biased Q-estimates; Diffusion generates implausible state transitions → IDM produces out-of-distribution actions → TD targets diverge; Excessive guidance (high ω) → mode collapse in generated trajectories → reduced diversity
- **First 3 experiments:** 1) Baseline replication: Train TD3BC on D4RL medium datasets with H=7, k=5 diffusion steps, compare normalized return against Table 2 baselines 2) IDM ablation: Measure IDM prediction accuracy on held-out real transitions vs diffusion-generated transitions; if accuracy drops >15% on generated data, investigate distribution shift 3) Data volume sweep: Compare DAWM vs DAWM-T (1/8 data) on 3 environments per Table 5 to isolate whether gains come from data quantity vs action completion quality

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several are implicit in the discussion section and limitations.

## Limitations
- Performance gains vary significantly across environments (substantial in Walker2d, marginal in HalfCheetah), suggesting domain-dependent effectiveness
- The IDM's reliance on state transitions may fail in stochastic environments where multiple actions produce identical transitions
- Limited evaluation to 9 D4RL locomotion benchmarks with low-dimensional proprioceptive states, without testing generalization to more complex environments

## Confidence
- Claims about DAWM's superiority over DWM and DD: Medium confidence (evaluated on single benchmark suite with limited task diversity)
- Claims about modular design's training stability advantage: Medium confidence (supported by ablation showing DAWM-T maintains reasonable performance)
- Claims about return-to-go conditioning effectiveness: Medium confidence (stable performance across horizons but lacks analysis of actual return achievement)

## Next Checks
1. **IDM Generalization Test**: Measure action reconstruction accuracy on diffusion-generated states versus real states. If accuracy drops >15% on generated data, evaluate whether DAWM's performance gains correlate with IDM reliability.

2. **Environment Transferability**: Test DAWM on a non-D4RL offline RL benchmark (e.g., Adroit or AntMaze) to assess whether modular diffusion+IDM design generalizes beyond locomotion tasks.

3. **Stochastic Environment Stress Test**: Evaluate DAWM on a deterministic vs stochastic variant of the same task (e.g., using different seeds or dynamics randomization) to quantify performance degradation when multiple actions could produce identical state transitions.