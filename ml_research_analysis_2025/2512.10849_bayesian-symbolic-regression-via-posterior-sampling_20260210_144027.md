---
ver: rpa2
title: Bayesian Symbolic Regression via Posterior Sampling
arxiv_id: '2512.10849'
source_url: https://arxiv.org/abs/2512.10849
tags:
- symbolic
- expressions
- smc-sr
- posterior
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the challenge of noise sensitivity in symbolic
  regression (SR), a technique for discovering interpretable mathematical equations
  from data. They propose a Sequential Monte Carlo (SMC) framework for Bayesian SR,
  which approximates the posterior distribution over symbolic expressions to improve
  robustness and enable uncertainty quantification.
---

# Bayesian Symbolic Regression via Posterior Sampling

## Quick Facts
- arXiv ID: 2512.10849
- Source URL: https://arxiv.org/abs/2512.10849
- Reference count: 39
- Primary result: SMC-SR discovers parsimonious symbolic expressions with superior generalization on noisy benchmark datasets compared to GP baselines

## Executive Summary
This paper addresses noise sensitivity in symbolic regression by proposing a Sequential Monte Carlo (SMC) framework that approximates the posterior distribution over symbolic expressions. The SMC-SR algorithm combines probabilistic selection, adaptive tempering, and normalized marginal likelihood to efficiently explore the search space while maintaining diversity and preventing overfitting. Unlike traditional genetic programming approaches, SMC-SR enables uncertainty quantification and demonstrates improved ability to discover accurate, interpretable equations from noisy data.

## Method Summary
The authors develop a Bayesian approach to symbolic regression using Sequential Monte Carlo sampling. The method maintains a population of symbolic expressions, iteratively updating importance weights based on likelihood tempering (φ from 0 to 1) and resampling when effective sample size drops below a threshold. Each expression's fitness is evaluated using normalized marginal likelihood (NML) computed via Laplace approximation, which penalizes model complexity. The algorithm includes MCMC rejuvenation steps to maintain diversity and converges to approximate the posterior distribution over expressions. Model selection is performed by identifying the maximum NML expression or using validation error.

## Key Results
- SMC-SR achieves lower NRMSE-test scores compared to GP-MSE, GP-NML, and GP-agg baselines on 12 Feynman benchmark datasets
- The method maintains higher population diversity, with approximately 50% fewer unique expressions encountered but more unique models incorporated compared to GP methods
- SMC-SR demonstrates reduced overfitting, with smaller gaps between NRMSE-train and NRMSE-test scores
- The tempering schedule effectively transitions from exploration to exploitation, maintaining effective sample size above target thresholds

## Why This Works (Mechanism)

### Mechanism 1: Likelihood tempering enables gradual exploration-to-exploitation transition
- Claim: Gradual φ increase from 0 to 1 prevents premature convergence in discrete symbolic expression space
- Mechanism: SMC iterates through target distributions π_t(M|D) ∝ π(M) · q̂(M)^{φ_t} where early φ≪1 flattens posterior landscape, allowing diverse structures to persist before selection pressure intensifies
- Core assumption: Posterior over symbolic expressions is multi-modal, requiring gradual transition rather than direct sampling
- Evidence: Section 4, Figure 8 shows typical progression through exploration (φ≪1) followed by exploitation phases

### Mechanism 2: Normalized Marginal Likelihood provides principled complexity regularization
- Claim: NML penalizes model complexity through γ^{N_θ/2} term, reducing overfitting without held-out validation
- Mechanism: NML computes q̂ = γ^{N_θ/2} · π(D|θ*, M)^{(1-γ)} where γ=1/√N_d explicitly penalizes parameter count N_θ
- Core assumption: Laplace approximation around maximum likelihood estimate θ* is sufficiently accurate for NML estimation
- Evidence: Section 4, Figure 4 shows GP-NML intermediate performance between GP-MSE and SMC-SR, indicating NML contributes to regularization

### Mechanism 3: Probabilistic selection maintains targeted novelty
- Claim: Metropolis acceptance combined with resampling explores high-posterior regions without exhaustive revisiting of low-value expressions
- Mechanism: SMC-SR uses α = min(1, (q̂(M'_i)/q̂(M_i))^{φ_t}) allowing occasionally accepting worse expressions to maintain diversity
- Core assumption: Pure novelty-seeking is inefficient; novelty must be concentrated in regions with non-negligible posterior density
- Evidence: Section 4 shows SMC-SR has about 50% of unique expressions encountered compared to GP methods, but incorporates more unique models into population

## Foundational Learning

- **Bayesian Posterior Inference**: Essential for understanding how SMC-SR approximates π(M|D) rather than optimizing point estimates. Quick check: Given likelihood π(D|M) and prior π(M), write the unnormalized posterior. What happens if the prior is uniform?
- **Importance Sampling & Effective Sample Size (ESS)**: Critical for understanding when weighted particles become degenerate and resampling is needed. Quick check: If all weights are equal (W_i=1/N), what is ESS? If one weight is 1.0 and others are 0, what is ESS?
- **Metropolis-Hastings Acceptance**: Central to understanding probabilistic selection mechanism. Quick check: If q̂(offspring) > q̂(parent), what is α? If q̂(offspring) = 0.5 · q̂(parent), what is α when φ_t=1?

## Architecture Onboarding

- **Component map**: Population P (2000 expressions) -> NML Estimator (Laplace approximation) -> Tempering Scheduler (bisection on ESS) -> Reweighting Module (importance weights) -> Resampler (stratified resampling) -> MCMC Kernel (10 iterations of proposal + Metropolis acceptance)
- **Critical path**: Initialize from uniform prior → Compute NML for all expressions → Loop until φ_t=1: adapt φ_t maintaining ESS≥1900 → Reweight particles → Resample based on weights → Run MCMC rejuvenation → Return final population approximating π(M|D)
- **Design tradeoffs**: Population size N_P (larger improves exploration but increases compute); Laplace vs. SMC for NML (Laplace is O(1) vs. SMC's O(100s) but less accurate); Target ESS (95% of N_P) balances smooth convergence vs. degeneracy risk
- **Failure signatures**: ESS collapses early (proposal mismatch or insufficient initial diversity); final population <10 unique expressions (resampling too aggressive or MCMC ineffective); NRMSE-train≪NRMSE-test (NML regularization insufficient); θ* optimization fails frequently (Laplace unreliable for complex expressions)
- **First 3 experiments**: 1) Reproduce demonstration case (y=(1/2π)e^{-x²/2} with 10% noise) verifying tempering schedule and multimodal NML distribution; 2) Ablate NML by replacing with MSE loss to isolate regularization contribution; 3) Vary population size (500,1000,2000,4000) on single Feynman dataset measuring wall-clock time, NRMSE-test, and unique expressions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does incorporating informative priors, specifically those derived from domain knowledge, affect SMC-SR performance and interpretability?
- Basis: Conclusion states "future work could explore the impact of different priors – particularly those informed by domain knowledge – on the algorithm's performance"
- Why unresolved: Current study used uniform, uninformative prior π(M)∝1 to focus on SMC mechanics
- What evidence would resolve: Experiments comparing model discovery rates and physical plausibility on scientific datasets with domain-constrained priors

### Open Question 2
- Question: Can replacing Laplace approximation with inner-loop SMC calculation for NML improve accuracy without making computation intractable?
- Basis: Conclusion suggests "incorporation of more accurate methods for calculating NML, such as replacing the Laplace approximation with SMC in the inner loop, should be investigated"
- Why unresolved: Laplace chosen for speed (O(1) vs. SMC's O(100s)) despite being "less accurate for many common SR models"
- What evidence would resolve: Comparison of NML estimation error and wall-clock time between Laplace and SMC-based inner loops on standardized benchmarks

### Open Question 3
- Question: To what extent does hyperparameter optimization improve convergence speed or accuracy compared to fixed values used in this study?
- Basis: Section 2(a) notes "No effort was made to optimize hyperparameters (though future work may find this fruitful)"
- Why unresolved: Authors used ad-hoc, relatively large fixed values (N_P=2000) without tuning for specific problems
- What evidence would resolve: Ablation study varying hyperparameters across datasets of varying complexity to identify optimal configurations

## Limitations
- Method effectiveness depends heavily on Laplace approximation accuracy for NML, which may degrade for complex expressions with multiple local optima
- Population size (N_P=2000) and MCMC parameters (N_MCMC=10) appear effective but lack systematic optimization
- Uniform improper prior π(M)∝1 may bias toward simpler expressions only through NML complexity penalty
- Exact proposal distribution h(P'|P) for MCMC kernel is referenced but not fully specified

## Confidence

- **High confidence**: SMC-SR demonstrates superior generalization (lower NRMSE-test) compared to GP baselines; tempering mechanism effectively transitions from exploration to exploitation
- **Medium confidence**: NML provides effective regularization against overfitting; Laplace approximation is sufficient for tractability; probabilistic selection maintains targeted novelty
- **Low confidence**: Uniform prior is adequate; exact hyperparameter choices are optimal; MCMC kernel proposal distribution is well-suited to symbolic expression space

## Next Checks
1. **Ablation study**: Replace NML with MSE loss while keeping SMC framework to isolate NML's contribution to regularization and generalization
2. **Population size sweep**: Systematically vary N_P from 500 to 4000 on a single Feynman dataset to identify compute-accuracy tradeoff curve
3. **Prior sensitivity**: Implement and test a simple complexity-based prior (e.g., π(M) ∝ exp(-λ·depth(M))) to assess whether uniform priors limit discovery of meaningful expressions