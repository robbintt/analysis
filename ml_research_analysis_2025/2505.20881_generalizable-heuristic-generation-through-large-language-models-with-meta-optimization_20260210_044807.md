---
ver: rpa2
title: Generalizable Heuristic Generation Through Large Language Models with Meta-Optimization
arxiv_id: '2505.20881'
source_url: https://arxiv.org/abs/2505.20881
tags:
- best
- solution
- heuristic
- utility
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MoH is a novel framework that uses LLMs to iteratively refine a
  meta-optimizer, which autonomously constructs diverse optimizers through self-invocation.
  These optimizers then evolve heuristics for downstream combinatorial optimization
  tasks, enabling broader exploration of the heuristic search space.
---

# Generalizable Heuristic Generation Through Large Language Models with Meta-Optimization

## Quick Facts
- **arXiv ID**: 2505.20881
- **Source URL**: https://arxiv.org/abs/2505.20881
- **Reference count**: 40
- **Key outcome**: MoH outperforms traditional heuristics and existing LLM-based approaches on TSP and BPP instances

## Executive Summary
MoH introduces a novel framework that leverages LLMs to iteratively refine a meta-optimizer capable of autonomously constructing diverse optimizers through self-invocation. These meta-optimizers then evolve heuristics for downstream combinatorial optimization tasks, enabling broader exploration of the heuristic search space. The framework employs a multi-task training scheme to enhance generalization across various problem sizes and settings, demonstrating state-of-the-art performance on classic COPs like TSP and BPP while producing interpretable meta-optimizers that incorporate diverse optimization strategies.

## Method Summary
The MoH framework operates by using LLMs to iteratively refine a meta-optimizer that can autonomously construct diverse optimizers through self-invocation. This meta-optimizer then evolves heuristics for downstream combinatorial optimization problems (COPs). The approach employs a multi-task training scheme to enhance generalization capabilities across different problem sizes and settings. The framework's core innovation lies in its ability to generate effective and interpretable meta-optimizers that incorporate diverse optimization strategies, moving beyond single-task performance to broader heuristic generation.

## Key Results
- MoH achieves state-of-the-art performance on classic COPs like TSP and BPP
- The framework demonstrates superior performance in cross-size settings compared to traditional heuristics and existing LLM-based approaches
- MoH generates interpretable meta-optimizers that incorporate diverse optimization strategies

## Why This Works (Mechanism)
MoH works by iteratively refining a meta-optimizer through LLM self-invocation, which enables the autonomous construction of diverse optimizers. These optimizers then evolve heuristics for downstream COPs, allowing for broader exploration of the heuristic search space. The multi-task training scheme enhances generalization across different problem sizes and settings by exposing the model to varied optimization contexts during training.

## Foundational Learning
- **Combinatorial Optimization Problems (COPs)**: Classic optimization problems like TSP and BPP where finding optimal solutions is computationally challenging
  - *Why needed*: Forms the target domain where generated heuristics will be applied
  - *Quick check*: Can the framework handle both routing (TSP) and packing (BPP) problems?

- **Meta-optimization**: The process of optimizing the optimization algorithm itself rather than just the objective function
  - *Why needed*: Enables the framework to improve its own heuristic generation capabilities
  - *Quick check*: Does the meta-optimizer improve its own performance over iterations?

- **Self-invocation in LLMs**: Using the LLM to generate or refine its own outputs through iterative prompting
  - *Why needed*: Allows autonomous construction of diverse optimizers without manual intervention
  - *Quick check*: Does self-invocation lead to more diverse versus repetitive optimizer generation?

- **Multi-task training schemes**: Training models on multiple related tasks simultaneously to improve generalization
  - *Why needed*: Enhances the framework's ability to handle various problem sizes and settings
  - *Quick check*: Does multi-task training improve cross-size generalization compared to single-task alternatives?

## Architecture Onboarding

**Component map**: LLM -> Meta-optimizer refinement -> Diverse optimizer generation -> Heuristic evolution -> COP solution

**Critical path**: The LLM iteratively refines the meta-optimizer through self-invocation, which then generates diverse optimizers that evolve heuristics for specific COPs. The multi-task training scheme operates in parallel during LLM training to enhance generalization capabilities.

**Design tradeoffs**: The framework trades computational complexity and training time for broader generalization and interpretability. While traditional heuristics are faster to execute, MoH's iterative refinement process enables discovery of novel optimization strategies at the cost of increased computational overhead during the training phase.

**Failure signatures**: Limited performance on COPs structurally different from TSP and BPP, reduced effectiveness when the multi-task training scheme is removed, and potential degradation in heuristic quality when self-invocation leads to local optima in the optimizer space.

**Three first experiments**:
1. Benchmark MoH against traditional heuristics (2-opt, greedy algorithms) on standard TSP instances of varying sizes
2. Evaluate cross-size generalization by training on small TSP instances and testing on larger instances
3. Compare interpretability scores of generated meta-optimizers against baseline heuristic generation methods

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- Claims of broad generalizability across combinatorial optimization problems rest on experimental validation in only two classic problem domains (TSP and BPP)
- The multi-task training scheme's effectiveness in truly enhancing cross-size generalization lacks ablation studies isolating its specific contribution
- Limited quantitative analysis of meta-optimizer interpretability and comparison to existing interpretable heuristic generation methods

## Confidence
- **High confidence**: MoH outperforms traditional heuristics and existing LLM-based approaches on tested TSP and BPP instances
- **Medium confidence**: The multi-task training scheme improves cross-size generalization, based on limited experimental evidence
- **Medium confidence**: Generated meta-optimizers are interpretable, though quantitative validation is limited

## Next Checks
1. Evaluate MoH on a broader range of combinatorial optimization problems (e.g., scheduling, routing with time windows, vehicle routing) to assess true generalizability
2. Conduct ablation studies comparing the multi-task training scheme against single-task alternatives to isolate its contribution to cross-size performance
3. Perform systematic analysis of the diversity and quality distribution of generated heuristics, including formal metrics for heuristic diversity and comparison to established baselines