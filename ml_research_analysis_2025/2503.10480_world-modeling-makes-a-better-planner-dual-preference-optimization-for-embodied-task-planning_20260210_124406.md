---
ver: rpa2
title: 'World Modeling Makes a Better Planner: Dual Preference Optimization for Embodied
  Task Planning'
arxiv_id: '2503.10480'
source_url: https://arxiv.org/abs/2503.10480
tags:
- action
- state
- task
- planning
- world
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of embodied task planning in\
  \ AI systems, where large vision-language models (LVLMs) struggle with dependency\
  \ constraints and efficiency due to their inability to model environment dynamics.\
  \ The proposed Dual Preference Optimization (D\xB2PO) framework jointly optimizes\
  \ state prediction and action selection through preference learning, enabling LVLMs\
  \ to understand environment dynamics for better planning."
---

# World Modeling Makes a Better Planner: Dual Preference Optimization for Embodied Task Planning

## Quick Facts
- arXiv ID: 2503.10480
- Source URL: https://arxiv.org/abs/2503.10480
- Authors: Siyin Wang; Zhaoye Fei; Qinyuan Cheng; Shiduo Zhang; Panpan Cai; Jinlan Fu; Xipeng Qiu
- Reference count: 40
- One-line primary result: 7B-parameter model achieves 31.4% relative improvement in success rate and 33.0% in planning efficiency compared to SFT baselines

## Executive Summary
This paper introduces Dual Preference Optimization (D²PO), a framework that jointly optimizes state prediction and action selection for embodied task planning. The approach addresses the challenge of dependency constraints and efficiency in large vision-language models by training them to understand environment dynamics through preference learning. A key innovation is the automated tree search mechanism that collects training data without human annotation, constructing preference pairs for both state prediction and action selection. Extensive experiments on the VoTa-Bench dataset demonstrate significant performance gains over existing methods and GPT-4o, particularly in handling dependency constraints and achieving more efficient execution paths.

## Method Summary
The D²PO framework trains a single policy model on two objectives simultaneously: predicting future states given current state-action pairs and selecting preferred actions given goal and history. Data collection uses automated tree search where the base policy samples candidate actions, each scored by a hybrid of GPT-4o process reward (goal progress assessment) and environment feasibility. High-scoring actions expand the tree, and successful trajectories are backtracked to construct preference pairs. The policy is first fine-tuned with supervised learning on successful trajectories, then optimized with D²PO using the collected preference data. The approach leverages world modeling objectives to enhance planning capabilities rather than treating world modeling as a separate component.

## Key Results
- D²PO significantly outperforms existing methods and GPT-4o when applied to Qwen2-VL (7B), LLaVA-1.6 (7B), and LLaMA-3.2 (11B) models
- The framework achieves superior task success rates with more efficient execution paths on the VoTa-Bench dataset
- 7B-parameter model achieves a relative improvement of 31.4% in success rate and 33.0% in planning efficiency compared to SFT baselines
- The approach reduces dependency errors from 212 to 141, affordance errors from 144 to 128, and inefficient errors from 141 to 78 compared to SFT baselines

## Why This Works (Mechanism)

### Mechanism 1: Dual Optimization of State Prediction and Action Selection
Jointly optimizing state prediction and action selection through preference learning improves planning by helping models understand action consequences. The framework trains a single policy model on two objectives simultaneously: predicting future states given current state-action pairs, and selecting preferred actions given goal and history. The shared parameters enable the action selection to implicitly leverage learned dynamics. Learning to predict state transitions forces the model to internalize environment dynamics, which then informs better action choices during inference without explicit world model queries.

### Mechanism 2: Tree Search for Preference Data Collection
Automated tree search with environment interaction can generate valid preference pairs without human annotation. At each state, the base policy samples K candidate actions. Each action receives a hybrid score (process reward from GPT-4o + binary feasibility from environment). High-scoring actions expand the tree. Upon reaching goal states, backtracking constructs chosen/rejected pairs for both action selection and state prediction. The process reward model (GPT-4o) provides sufficiently accurate goal-progress assessments to guide exploration toward successful trajectories.

### Mechanism 3: Hybrid Scoring for Action Selection
Combining semantic progress evaluation with physical feasibility produces more effective exploration than either alone. Each candidate action receives r_total = α × r_process + (1-α) × r_env where α=0.5. Process scores come from GPT-4o evaluating goal progress (1-5 scale). Environment scores are binary executability indicators. This filters out both semantically poor and physically impossible actions. Both dimensions are equally important for embodied planning; setting α=0.5 balances them appropriately.

## Foundational Learning

- **Direct Preference Optimization (DPO)**
  - Why needed here: D²PO extends DPO from single-objective to dual-objective optimization for embodied planning. Understanding the base DPO loss formulation is essential.
  - Quick check question: Can you explain why DPO avoids training an explicit reward model and how the β parameter controls deviation from the reference model?

- **Partially Observable Markov Decision Processes (POMDPs)**
  - Why needed here: The paper formulates embodied task planning as a POMDP with tuple (S, A, O, T, M, R, γ). Understanding observation vs. state, and history-dependent policies is critical.
  - Quick check question: In this formulation, why does the agent receive observation o_t = M(s_t) rather than direct state s_t?

- **World Models in Reinforcement Learning**
  - Why needed here: The paper positions its approach relative to traditional RSSM-based world models (Dreamer, etc.) and distinguishes language-based from latent-space dynamics learning.
  - Quick check question: How does learning dynamics in natural language space differ from learning in a learned latent state space, and what are the tradeoffs?

## Architecture Onboarding

- **Component map:**
  Tree Search Module -> Action Sampler -> Hybrid Scorer -> Trajectory Backtracker -> Dual Preference Optimizer -> Combined Loss

- **Critical path:**
  1. Initialize policy model with SFT on successful trajectories
  2. Run tree search to collect preference pairs (both action and state)
  3. Apply D²PO optimization for 1 epoch
  4. Evaluate on seen/unseen scenes with temperature=0

- **Design tradeoffs:**
  - Action-conditioned vs. goal-directed world modeling: Action-conditioned performs better on seen scenes; goal-directed generalizes better to unseen scenes
  - Data scale: More data helps initially but plateau/degrade at larger scales if DPO data shares source with SFT data
  - Model scale: Larger models (72B) show ~30% relative improvement over baselines, but 7B models already achieve strong results with D²PO

- **Failure signatures:**
  - Dependency errors: Actions executed without prerequisites
  - Affordance errors: Incorrect object interactions or non-existent object references
  - Inefficient errors: Redundant actions or unnecessary repetitions
  - State prediction drift: Inconsistent state descriptions with actual environment states

- **First 3 experiments:**
  1. Ablation on dual objectives: Train with L_action only vs. L_action + L_state to isolate world modeling contribution
  2. Tree search depth vs. quality: Vary maximum depth and measure successful trajectory yield for complex multi-step tasks
  3. Process reward calibration: Replace GPT-4o with smaller model for r_process scoring to assess data quality degradation

## Open Questions the Paper Calls Out

- **Sim-to-Real Gap**: Can D²PO maintain planning superiority when transferred from AI2-THOR simulation to physical real-world robotic platforms? The authors note current evaluation is limited to simulation which may not capture real-world complexity and uncertainty.
- **Embodied Self-Rewarding**: Is it feasible to replace the GPT-4o process reward model with an embodied self-rewarding mechanism without degrading preference data quality? The current reliance on GPT-4o is computationally expensive and suggests future exploration of self-rewarding mechanisms.
- **World Modeling Trade-offs**: How can the trade-off between action-conditioned and goal-directed world modeling be resolved to achieve both high performance on seen tasks and robust generalization to unseen environments? The paper observes action-conditioned models perform better on seen scenes while goal-directed models generalize better to unseen scenes.
- **Data Scaling**: What specific data diversity or regularization strategies are required to mitigate performance plateau and overfitting when scaling D²PO training data? The authors note a non-monotonic trend where performance plateaus or declines at larger scales due to shared source with SFT data.

## Limitations
- Heavy reliance on GPT-4o process reward model introduces significant computational cost and potential brittleness
- Fixed hybrid scoring weights (α=0.5) may not be optimal across all task types without ablation validation
- State description generation procedure remains underspecified, potentially affecting reproducibility
- Current evaluation limited to simulation environment, raising questions about real-world transfer

## Confidence
- **High confidence**: The dual optimization framework design and its implementation on VoTa-Bench are well-documented with clear experimental methodology
- **Medium confidence**: The claimed mechanisms for why dual optimization improves planning are plausible but not directly validated through controlled ablations
- **Medium confidence**: The tree search data collection approach appears sound but relies on a black-box GPT-4o dependency

## Next Checks
1. Conduct an ablation study comparing D²PO (L_action + L_state) against single-objective DPO (L_action only) to quantify the exact contribution of the world modeling component
2. Replace GPT-4o with a smaller, faster model for process reward scoring in the tree search and measure degradation in success rates and planning efficiency
3. Perform a robustness test by varying the hybrid scoring weight α (0.3, 0.5, 0.7) across different task types to determine if the fixed α=0.5 assumption holds