---
ver: rpa2
title: 'CS3D: An Efficient Facial Expression Recognition via Event Vision'
arxiv_id: '2512.09592'
source_url: https://arxiv.org/abs/2512.09592
tags:
- facial
- recognition
- event
- expression
- energy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CS3D addresses energy efficiency and accuracy challenges in event-based
  facial expression recognition by proposing a compact framework integrating factorized
  3D convolutions, soft spiking neurons, and spatial-temporal attention mechanisms.
  The approach decomposes standard 3D convolutions to reduce computational complexity
  while enhancing temporal and directional feature extraction, and uses attention
  modules to emphasize critical temporal and spatial information.
---

# CS3D: An Efficient Facial Expression Recognition via Event Vision

## Quick Facts
- arXiv ID: 2512.09592
- Source URL: https://arxiv.org/abs/2512.09592
- Reference count: 40
- Primary result: 78.38% accuracy on ADFES, 54.79% on CASME II, 90.91% on SZU-EmoDage

## Executive Summary
CS3D addresses energy efficiency and accuracy challenges in event-based facial expression recognition by proposing a compact framework integrating factorized 3D convolutions, soft spiking neurons, and spatial-temporal attention mechanisms. The approach decomposes standard 3D convolutions to reduce computational complexity while enhancing temporal and directional feature extraction, and uses attention modules to emphasize critical temporal and spatial information. Experiments show CS3D achieves competitive accuracy on three benchmark datasets while reducing energy consumption to 21.97% of baseline C3D on the same hardware.

## Method Summary
CS3D processes event streams converted from video datasets using V2E converter, then applies preprocessing including facial landmark cropping, rotation alignment, and grayscale conversion to 112×112 resolution. The core architecture features FactorizedConv3D blocks that decompose standard 3D convolutions into separate temporal (3×1×1) and spatial (1×3×3) operations with soft spiking neurons for gradient-based training. Spatial-temporal joint attention modules sequentially apply temporal and spatial attention with residual connections before classification through fully connected layers. Training uses Adam optimizer with learning rate 1e-4 and batch size 16.

## Key Results
- Achieves 78.38% accuracy on ADFES, 54.79% on CASME II, and 90.91% on SZU-EmoDage
- Reduces FLOPs from 21.29G (C3D) to 4.68G while improving accuracy
- Cuts energy consumption to 21.97% of baseline C3D on same hardware
- Maintains performance under insufficient lighting conditions where RGB cameras fail

## Why This Works (Mechanism)

### Mechanism 1: Factorized 3D Convolution
Decomposing standard 3D convolutions into separate temporal and spatial operations reduces computational cost while preserving directional feature extraction. A 3×3×3 kernel is replaced with depth-wise convolutions (DWConv1: 3×1×1 for temporal, DWConv2: 1×3×3 for spatial) plus point-wise convolutions for channel fusion, with residual connections for gradient flow. The core assumption is that temporal and spatial features in facial expressions can be learned separably without significant loss of joint spatiotemporal patterns.

### Mechanism 2: Soft Spiking Neurons (SSN)
Soft-thresholding with surrogate gradients enables gradient-based training of spiking neurons while retaining continuous information transmission. Unlike hard thresholding (binary spikes), SSN outputs continuous values x when x > θ. Backpropagation uses sigmoid surrogate gradient f'(x) = σ(β(x-θ)) to approximate non-differentiable threshold crossing. The surrogate gradient sufficiently approximates true gradients for effective weight updates in deep networks.

### Mechanism 3: Spatial-Temporal Joint Attention
Sequential temporal-then-spatial attention with residual connections selectively emphasizes keyframes and discriminative facial regions. Temporal Attention extracts temporal-wise weights via average/max pooling, applies shared convolutions, and fuses via element-wise max. Spatial Attention applies channel-wise pooling followed by convolution for spatial attention. Final output: Y = SA(TA(X)) + X. Salient frames and facial regions can be identified via pooling statistics and learned attention weights.

## Foundational Learning

- **Event Camera Fundamentals**: Why needed here: CS3D processes asynchronous event streams (not frames); understanding high temporal resolution, low latency, and low-light robustness explains why event vision is chosen. Quick check: Why does an event camera outperform RGB cameras in dim lighting for facial expression capture?

- **3D Convolutional Networks (C3D)**: Why needed here: CS3D is a modification of C3D; understanding joint spatial-temporal kernel operations is prerequisite to grasping factorization benefits. Quick check: What does a 3×3×3 kernel capture that a 2×3×3 kernel cannot?

- **Surrogate Gradient Learning**: Why needed here: Training spiking neurons requires solving non-differentiability; understanding this explains SSN design choices. Quick check: Why does hard thresholding cause vanishing gradients during backpropagation?

## Architecture Onboarding

- **Component map**: Event stream → FactorizedConv3D (reduced FLOPs) → Attention (enhanced selectivity) → Classification
- **Critical path**: Event stream → FactorizedConv3D (4.68G FLOPs) → Attention (4.26G to 4.68G FLOPs) → Classification. Energy savings primarily derive from factorized convolutions.
- **Design tradeoffs**: Factorization reduces computation but may lose joint spatiotemporal coupling; SSN improves gradient flow at cost of added threshold hyperparameter; Attention improves accuracy (+10.7% from C3D baseline) but adds FLOPs.
- **Failure signatures**: Accuracy below C3D baseline indicates missing residual connections; gradient instability during training suggests SSN β parameter too large; poor temporal modeling indicates incorrect TA-SA application order; energy consumption not matching claims indicates FLOPs mismatch.
- **First 3 experiments**:
  1. Replicate ablation study: Train C3D → add SSN → add factorization → add attention, measuring accuracy/FLOPs/energy at each step
  2. Cross-dataset validation: Test trained model on all three datasets to verify generalization
  3. Real-world lighting test: Deploy on Jetson Nano with actual event camera under sufficient vs insufficient lighting

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability has Medium confidence due to limited cross-dataset validation and absence of transformer comparisons
- Soft spiking neuron mechanism lacks independent verification and sensitivity analysis of parameters
- Factorized convolution design may fail for rapid micro-expressions requiring joint spatiotemporal processing
- Energy efficiency claims are hardware-specific and require validation on target deployment platforms

## Confidence
- Computational efficiency improvements: **High confidence** (supported by ablation studies)
- Accuracy gains: **High confidence** (documented across three datasets)
- Soft spiking neuron mechanism: **Medium confidence** (theoretically sound but unverified)
- Factorized convolution effectiveness: **Medium confidence** (assumes separable features work universally)
- Energy efficiency claims: **Medium confidence** (hardware-specific requirements)

## Next Checks
1. Implement sensitivity analysis of SSN parameters (θ, β) to identify optimal configurations and stability boundaries
2. Test CS3D on additional event-based datasets (e.g., DHP19, MVSEC) to verify cross-domain generalization
3. Benchmark against current state-of-the-art transformer architectures on identical hardware to establish relative efficiency gains