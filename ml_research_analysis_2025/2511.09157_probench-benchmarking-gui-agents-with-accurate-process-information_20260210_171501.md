---
ver: rpa2
title: 'ProBench: Benchmarking GUI Agents with Accurate Process Information'
arxiv_id: '2511.09157'
source_url: https://arxiv.org/abs/2511.09157
tags:
- task
- click
- process
- agents
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ProBench introduces a mobile GUI agent benchmark with over 200
  tasks covering bilingual apps and two task types: State-related (evaluated by final
  screen) and Process-related (requiring intermediate step tracking). It addresses
  the lack of automated process information capture by introducing a Process Provider
  with Structure Description Converter and MLLM-based Summarizer components.'
---

# ProBench: Benchmarking GUI Agents with Accurate Process Information

## Quick Facts
- arXiv ID: 2511.09157
- Source URL: https://arxiv.org/abs/2511.09157
- Reference count: 7
- Even top models achieve under 50% accuracy on mobile GUI tasks, with significant struggles on process-related tasks and social/lifestyle apps.

## Executive Summary
ProBench introduces a mobile GUI agent benchmark with over 200 tasks covering bilingual apps and two task types: State-related (evaluated by final screen) and Process-related (requiring intermediate step tracking). It addresses the lack of automated process information capture by introducing a Process Provider with Structure Description Converter and MLLM-based Summarizer components. Evaluations reveal that even top models achieve under 50% accuracy, with significant struggles on Process-related tasks and social/lifestyle apps. The benchmark exposes limitations in grounding, historical operation tracking, and task planning across diverse models. ProBench provides a foundation for more accurate mobile GUI agent evaluation and future research directions.

## Method Summary
ProBench is an automated mobile GUI agent benchmark that evaluates agents on 200+ tasks across 34 bilingual apps using a 15-step budget and early-stop mechanism. The Process Provider captures accurate action semantics through two components: a Structure Description Converter that parses accessibility trees for node descriptions, and an MLLM-based Summarizer that visually compares before/after screenshots. State-related tasks are judged solely on final screenshots, while Process-related tasks receive additional process information for intermediate step verification. The evaluation pipeline uses adbutils for device interaction and Gemini 2.5 Pro as the automated judge.

## Key Results
- Top models achieve under 50% accuracy across all tasks, with proprietary models (GPT-4o, Claude 4 Sonnet) scoring 0.0% on Process-related tasks
- Process-related tasks show consistently lower accuracy than State-related tasks (e.g., Gemini 2.5 Pro: 44.2% vs 21.7% on English tasks)
- Social and lifestyle apps present significantly greater challenges than system/productivity apps due to fragmented information and visual distractors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured accessibility tree parsing provides reliable process information for action verification.
- Mechanism: The Structure Description Converter parses the a11y tree after each click, locates the smallest clickable node enclosing the click coordinates, and extracts text/content-desc attributes (or resource-id with child node details if empty), producing a human-readable description like "Click Start your search."
- Core assumption: Accessibility trees contain semantically meaningful metadata that accurately reflects UI element functionality.
- Evidence anchors:
  - [section 3.4] Algorithm 1 defines the parsing logic: parse tree → locate minimum node → extract description → fallback to resource-id with child details
  - [section 3.4] Table 2 shows 89.7% correctness for Process-related evaluation using Structure Description Converter
  - [corpus] GUI-Robust (arXiv 2506.14477) similarly emphasizes structured UI data for robustness testing
- Break condition: Apps with sparse or missing a11y metadata will yield empty or uninformative descriptions.

### Mechanism 2
- Claim: Visual comparison via MLLM can summarize action semantics when structure-based methods fail.
- Mechanism: The MLLM-based Summarizer horizontally concatenates before/after screenshots with the click coordinate marked, then prompts an MLLM to generate a verb-phrase summary of what changed (e.g., "Click Search bar to focus").
- Core assumption: MLLMs can reliably detect visual differences and map them to action semantics.
- Evidence anchors:
  - [section 3.4] Figure 4 illustrates the concatenation and summarization workflow
  - [section 3.4] Table 2 shows 94.1% correctness for MLLM-based Summarizer, outperforming structure-only
  - [corpus] MCPWorld (arXiv 2506.07672) notes MLLM limitations in UI change detection under API/GUI hybrid evaluation
- Break condition: Visually similar screens (e.g., focusing a search box with minimal visual change) may yield "Invalid click" or incorrect summaries.

### Mechanism 3
- Claim: Separating tasks into State-related and Process-related exposes distinct agent weaknesses.
- Mechanism: State-related tasks are judged only on final screenshot content; Process-related tasks require the evaluator to receive both the final screenshot and a sequence of textualized actions to verify critical intermediate steps were performed.
- Core assumption: Final-state evaluation misses critical process failures (e.g., skipping a required sort operation).
- Evidence anchors:
  - [abstract] "Process-related Task...requiring intermediate step tracking"
  - [section 1] Figure 1 demonstrates how "Buy the cheapest wireless mouse" can appear successful on final screen despite missing the critical sort step
  - [section 4.3] Table 3 shows consistently lower accuracy on Process-related tasks across all models (e.g., Gemini 2.5 Pro: 44.2% ST vs 21.7% PT on English)
- Break condition: If Process Provider outputs are noisy or incomplete, Process-related evaluation may yield false positives/negatives.

## Foundational Learning

- Concept: Accessibility (a11y) tree structure
  - Why needed here: The Structure Description Converter depends on parsing a11y trees to extract node attributes (text, content-desc, resource-id). Without understanding this hierarchy, you cannot debug why certain clicks produce empty descriptions.
  - Quick check question: Given an a11y node with empty text and content-desc, what fallback does the converter use?

- Concept: Multimodal LLM visual comparison
  - Why needed here: The MLLM-based Summarizer relies on the model's ability to detect and verbalize visual changes between two screenshots. Understanding prompt design and failure modes is critical for effective summarization.
  - Quick check question: What happens when the before/after screenshots are nearly identical—what should the summarizer output?

- Concept: Grounding in GUI agents
  - Why needed here: Error analysis (Section 5.1) identifies insufficient grounding as a primary failure mode for proprietary models. Grounding is the ability to map natural language instructions to precise screen coordinates.
  - Quick check question: Why would GPT-4o and Claude 4 Sonnet score 0.0% on ProBench despite their general capabilities?

## Architecture Onboarding

- Component map:
  Task Curation Pipeline -> Dynamic Environment -> Process Provider -> Evaluator

- Critical path:
  1. Agent receives: current screenshot + task instruction + historical action context
  2. Agent outputs: textual action (CLICK, SWIPE, TYPE, ENTER, BACK, COMPLETE, WAIT)
  3. ProBench converts to device command via adbutils
  4. After task completion or step limit, evaluator receives final screenshot + (for Process tasks) action descriptions from Process Provider
  5. Judge outputs: Uncompleted / Failure / Success

- Design tradeoffs:
  - Structure Description Converter vs MLLM-based Summarizer: 89.7% vs 94.1% correctness, but MLLM requires more compute and may fail on subtle visual changes
  - Manual verification-free evaluation (MV Free) vs accuracy: ProBench achieves high automation but relies on MLLM judge reliability
  - Step budget (15 steps) balances task complexity with execution time but may truncate longer tasks

- Failure signatures:
  - **Early stop loop**: Model repeats same action 5 consecutive times → marked as failure (Table 4 shows 43-78% early stop ratios)
  - **Grounding failure**: Click coordinates miss target element (Figure 6 shows Claude missing Airbnb search bar)
  - **Process blindness**: Final screen looks correct but critical intermediate steps were skipped (Figure 12: Gemini misses price sort)

- First 3 experiments:
  1. Run ProBench on your agent with only State-related tasks to establish baseline, then add Process-related tasks to identify process-awareness gaps.
  2. Ablate the Process Provider: compare evaluation accuracy using Structure Description Converter only vs MLLM-based Summarizer only vs both combined.
  3. Analyze failure modes by category: run your agent on social/lifestyle apps vs system/productivity apps to quantify the performance gap observed in Figure 5.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can benchmark evaluation metrics be refined to capture degrees of task progress rather than relying solely on binary success/failure judgments?
- Basis in paper: [explicit] The conclusion states, "future work could improve the evaluation metrics to capture degrees of task progress rather than relying solely on binary judgments."
- Why unresolved: Current metrics fail to distinguish between completing 90% of a task and failing immediately, limiting granular feedback for model optimization.
- What evidence would resolve it: A quantitative scoring rubric that correlates partial task completion (e.g., successful navigation but failed data entry) with human assessments of progress.

### Open Question 2
- Question: How can agents improve task planning to decompose complex, high-level instructions into hierarchical subtasks rather than relying on naive search strategies?
- Basis in paper: [inferred] Section 5.3 identifies "Oversimplified Task Planning" as a universal bottleneck, noting agents often input entire complex instructions into search boxes instead of executing multi-step logic.
- Why unresolved: Agents lack the capability to dynamically decompose abstract goals into executable sequential operations when immediate actions are unavailable.
- What evidence would resolve it: Demonstrated success on "Process-related Tasks" where agents execute intermediate steps (e.g., finding a specific tool before querying) without explicit chain-of-thought prompting.

### Open Question 3
- Question: What specific architectural or training improvements are required to handle the fragmented information and visual distractors prevalent in social and lifestyle applications?
- Basis in paper: [inferred] Section 4.4 reports significantly lower performance on social/lifestyle apps compared to system apps, attributing this to "icon-only buttons, deeply nested folding cards, and advertising pop-ups."
- Why unresolved: Current visual grounding mechanisms struggle to distinguish efficient signals from visual noise in highly dynamic, content-heavy interfaces.
- What evidence would resolve it: Narrowing the performance gap between static system apps and dynamic social apps through specific fine-tuning on noisy UI datasets.

## Limitations

- Unknown app/task scope: The exact task list and app inventory remain partially undisclosed, limiting reproducibility and independent validation of Process Provider correctness rates.
- MLLM dependency in evaluation: The benchmark relies on Gemini 2.5 Pro as the final judge, introducing a black-box dependency that could affect consistency without ablation studies on judge reliability.
- Hardware/software fragmentation: Testing on both emulators and physical devices introduces variability, but the paper does not quantify how much performance differences stem from device fragmentation versus model capability gaps.

## Confidence

- **High Confidence**: The empirical observation that Process-related tasks are significantly harder than State-related tasks (Table 3), with accuracy dropping 15-30% across models. This is directly supported by experiment data and is the core insight driving the paper's contribution.
- **Medium Confidence**: The Process Provider components' reported correctness rates (89.7% for Structure Description Converter, 94.1% for MLLM-based Summarizer). These are based on internal validation but lack independent verification and may not generalize to all app types.
- **Low Confidence**: The claim that ProBench provides "comprehensive" coverage of real-world GUI agent failures. With 200+ tasks across 34 apps, the benchmark covers a meaningful slice but may still miss edge cases in long-tail apps or complex multi-app workflows.

## Next Checks

1. **Independent Process Provider evaluation**: Reimplement both Structure Description Converter and MLLM-based Summarizer on a held-out app set to verify the claimed 89.7% and 94.1% correctness rates.

2. **Judge reliability ablation**: Replace Gemini 2.5 Pro with a different LLM judge (e.g., GPT-4o) and measure agreement rates to quantify the impact of judge selection on reported accuracy.

3. **Task decomposition stress test**: Create synthetic tasks requiring 4+ sequential steps with interdependent operations and measure whether agents can maintain correct state tracking or fall into the early-stop loop.