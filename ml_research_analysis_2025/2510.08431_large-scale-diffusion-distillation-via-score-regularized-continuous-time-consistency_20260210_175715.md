---
ver: rpa2
title: Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency
arxiv_id: '2510.08431'
source_url: https://arxiv.org/abs/2510.08431
tags:
- diffusion
- arxiv
- distillation
- preprint
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents rCM, the first method to scale continuous-time
  consistency distillation to large-scale text-to-image and video diffusion models.
  The key innovation is integrating score distillation as a long-skip regularizer
  to address quality limitations of the base continuous-time consistency model (sCM).
---

# Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency

## Quick Facts
- arXiv ID: 2510.08431
- Source URL: https://arxiv.org/abs/2510.08431
- Reference count: 40
- Key outcome: First method to scale continuous-time consistency distillation to large-scale text-to-image and video diffusion models, achieving 1-4 step generation (15-50x acceleration) with quality matching or surpassing state-of-the-art DMD2.

## Executive Summary
This paper presents rCM, the first method to scale continuous-time consistency distillation to large-scale text-to-image and video diffusion models. The key innovation is integrating score distillation as a long-skip regularizer to address quality limitations of the base continuous-time consistency model (sCM). This creates a complementary training objective that combines the mode-covering forward divergence of sCM with the mode-seeking reverse divergence of score distillation. The authors developed infrastructure including a FlashAttention-2 JVP kernel and parallelism compatibility to enable training on models up to 14B parameters and 5-second videos. rCM achieves 1-4 step generation (15-50x acceleration) while matching or surpassing state-of-the-art DMD2 on quality metrics, with notable advantages in diversity.

## Method Summary
rCM extends continuous-time consistency models by adding a score-regularized DMD loss as a long-skip regularizer. The method wraps teacher diffusion models to a TrigFlow schedule via SNR matching, then trains a student network using both sCM loss (forward-divergence) and DMD loss (reverse-divergence) with a fixed weight λ=0.01. A fake score network generates synthetic samples for the DMD loss. The key technical contribution is a FlashAttention-2 JVP kernel enabling stable training at 10B+ parameters under BF16. Training uses stochastic rollout with random timestep sequences, alternating student and fake score updates with EMA updates for stability.

## Key Results
- Achieves 1-4 step generation with 15-50x acceleration while maintaining quality comparable to or exceeding state-of-the-art DMD2
- Demonstrates superior diversity preservation compared to pure score distillation methods
- Successfully scales to 14B parameter text-to-image (Cosmos-Predict2) and video (Wan2.1) models with up to 5-second generation
- Maintains stability under BF16 precision through custom JVP kernel and optional FP32 time embeddings for large models

## Why This Works (Mechanism)

### Mechanism 1: Complementary Divergence Objective
Combining forward-divergence (consistency) and reverse-divergence (score distillation) objectives jointly improves quality and diversity. The sCM forward-divergence loss penalizes underestimating training sample likelihoods, producing spread-out densities that maintain diversity but can blur details. The DMD reverse-divergence loss operates on student-generated samples and seeks modes, sharpening outputs but risking mode collapse. The long-skip regularizer λL_DMD (with λ=0.01) injects reverse-divergence gradients at the final denoising step while the sCM loss preserves mode coverage.

### Mechanism 2: Error Accumulation Suppression via DMD Long-Skip
Score distillation applied as a long-skip regularizer counteracts error accumulation inherent in continuous-time consistency training. In sCM, the learning target df_θ⁻/dt includes self-feedback via JVP, which is numerically fragile. Errors at small t propagate to large t and amplify under limited BF16 precision, especially when teacher supervision vanishes. The DMD loss bypasses this cascaded accumulation by directly supervising the student's final prediction against both the fake score and teacher, providing an auxiliary gradient path independent of JVP computation.

### Mechanism 3: FlashAttention-2 JVP Kernel for Numerical Stability at Scale
A custom Triton kernel integrating JVP into FlashAttention-2's block-wise tiling enables stable sCM training at 10B+ parameters under BF16. Standard torch.func.jvp is incompatible with FSDP sharding and FlashAttention's fused kernels. The authors derive the analytical JVP for attention, computing it within the same streaming loop as FlashAttention-2 to avoid materializing the full attention matrix, maintaining memory efficiency while enabling tangent propagation.

## Foundational Learning

- Concept: **Consistency Models (CMs) and continuous-time formulation (sCM)**
  - Why needed here: Understanding why discrete-time CMs have discretization errors and how sCM's limit Δt→0 yields cleaner objectives is essential for grasping rCM's base formulation.
  - Quick check question: Explain why the sCM objective requires computing df_θ⁻/dt via JVP rather than finite differences.

- Concept: **Score Distillation (VSD/DMD/SiD) and reverse KL divergence**
  - Why needed here: rCM integrates DMD as a regularizer; understanding the role of the fake score network and why reverse KL is mode-seeking is critical.
  - Quick check question: Contrast forward KL (mode-covering) vs reverse KL (mode-seeking) behavior in distribution matching—why does the latter improve visual quality but risk diversity loss?

- Concept: **Jacobian-Vector Products (JVP) in automatic differentiation**
  - Why needed here: The paper's core infrastructure contribution is JVP-compatible attention; understanding forward-mode AD and why JVP is more memory-efficient than VJP for scalar outputs is necessary.
  - Quick check question: Given a function f: R^n→R, why does computing ∇f via JVP require n forward passes but VJP requires only one backward pass? How does this change for f: R^n→R^m?

## Architecture Onboarding

- Component map: Teacher network (frozen pretrained diffusion) -> Student network (consistency predictions) -> Fake score network (auxiliary denoiser) -> FlashAttention-2 JVP kernel -> TrigFlow wrapping -> Training loop with sCM + DMD losses

- Critical path:
  1. TrigFlow wrapping: Convert teacher/student to TrigFlow-consistent parameterization via SNR matching
  2. Generator step: Compute sCM loss with tangent normalization
  3. Critic step: Train fake score on student-rollout samples
  4. DMD loss: Sample timesteps via stochastic rollout, compute score-regularized loss
  5. Alternate: Student update frequency F controls ratio of generator to critic steps (default 5:1)

- Design tradeoffs:
  - Semi-continuous time vs High-precision time: Finite-difference approximation (Δt=10⁻⁴) is simpler but unstable for 10B+ models; FP32 time embeddings add overhead but stabilize large models
  - Rollout strategy: Random N∈[1,N_max] explores full time range vs DMD2's fixed timesteps; trades compute for coverage
  - λ weighting: Fixed at 0.01 across experiments; no adaptive scheduling reported
  - BF16 vs FP32: Network in BF16 for speed, time embeddings in FP32 for stability, wrapping conversions in FP64 for precision

- Failure signatures:
  - Blur/distortion in fine details: Indicates sCM loss dominates; consider increasing λ
  - Mode collapse (similar poses/orientations across samples): Indicates DMD loss dominates; reduce λ or check fake score training
  - Training collapse after long training: Check JVP numerical stability; consider semi-continuous time or FP32 embeddings
  - Incompatibility with FSDP/CP: Ensure JVP-enabled layer restructuring matches FSDP sharding granularity

- First 3 experiments:
  1. Validate JVP kernel correctness: Compare FlashAttention-2 JVP output against torch.func.jvp on small attention layers (match primal and tangent outputs within tolerance); verify memory reduction via profiling
  2. Ablate λ on small model: Train rCM on Cosmos-Predict2 0.6B with λ∈{0, 0.001, 0.01, 0.1}; plot GenEval overall score and visual diversity (e.g., LPIPS variance) to confirm 0.01 as sweet spot
  3. Scale test with gradient checks: Train for 1000 iterations on 2B model; monitor gradient norms for student/fake score; verify EMA student does not diverge; compare 4-step samples against teacher baseline

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several important unresolved issues emerge from the methodology and results.

## Limitations
- Fixed λ=0.01 regularization weight was not systematically explored across different model scales or architectures
- Long-term training stability beyond 500K steps is not thoroughly analyzed, particularly for the interdependence between student and fake score networks
- The method requires synthetic data generation from teacher models, which may not be practical for all use cases

## Confidence
- Core claim (rCM improves both quality and diversity through complementary divergence objectives): Medium-High
- Infrastructure contribution (FlashAttention-2 JVP kernel): High
- Scalability to 14B+ models: Medium-High (based on successful training runs but limited long-term stability analysis)

## Next Checks
1. Conduct systematic λ sensitivity analysis across model scales (0.6B to 14B) to establish optimal weighting and verify robustness to schedule variations
2. Extend training duration beyond 500K steps on 10B+ models to assess long-term stability and potential drift in the fake score network's mode-covering behavior
3. Compare rCM's stochastic rollout strategy against deterministic fixed-timestep approaches (like DMD2) on diversity metrics (LPIPS variance) while controlling for step count to isolate the contribution of exploration strategy versus divergence combination