---
ver: rpa2
title: Adversarial Resilience against Clean-Label Attacks in Realizable and Noisy
  Settings
arxiv_id: '2504.13966'
source_url: https://arxiv.org/abs/2504.13966
tags:
- learning
- setting
- algorithm
- which
- classi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis explores adversarial resilience in sequential learning
  with clean-label attacks. In the realizable setting, it extends work by Goel et
  al.
---

# Adversarial Resilience against Clean-Label Attacks in Realizable and Noisy Settings

## Quick Facts
- arXiv ID: 2504.13966
- Source URL: https://arxiv.org/abs/2504.13966
- Reference count: 0
- This thesis explores adversarial resilience in sequential learning with clean-label attacks, achieving sublinear misclassification and abstention error bounds for finite VC dimension classes.

## Executive Summary
This thesis investigates learning under clean-label adversarial attacks in sequential settings. It extends selective classification theory to handle adversarial injections while maintaining theoretical performance guarantees. The work addresses both realizable settings (where labels come from a target function) and agnostic settings with noisy labels, developing algorithms that can achieve sublinear error rates by strategically abstaining when uncertain.

## Method Summary
The thesis develops algorithms that maintain a version space of consistent hypotheses and use disagreement-based prediction. In the realizable setting with known distributions, it uses shattering probabilities to quantify certainty and abstains when multiple hypotheses disagree. For the agnostic setting with Random Classification Noise, it employs batched updates based on disagreement regions. The key insight is allowing free abstention on adversarial samples, which enables robustness while maintaining sublinear error bounds for certain hypothesis classes.

## Key Results
- For thresholds in the realizable setting with known distribution: E[MisclassificationError_T] = 0 and E[AbstentionError_T] ‚â§ 2 log(T)
- For thresholds under Random Classification Noise: E[MisclassificationError_T] ‚â§ 1 and E[AbstentionError_T] ‚â§ O(log¬≤T)
- Extends Goel et al. (2023) to correct inaccuracies and achieve sublinear bounds for general finite VC dimension classes

## Why This Works (Mechanism)

### Mechanism 1: Disagreement-Based Selective Prediction
A learner achieves sublinear abstention error by abstaining only when the incoming sample falls within the disagreement region of the current version space. The algorithm maintains a version space F_t consistent with observed labels, predicting confidently when all hypotheses agree and abstaining otherwise.

### Mechanism 2: Shattering-Based Certainty Quantification
Using the probability of shattering k points as a certainty measure enables stochastic-like guarantees even with adversarial injections. The learner compares shattering probabilities under different labelings to decide between predicting or abstaining.

### Mechanism 3: Batched Version Space Updates Under Noise
In the agnostic setting, collecting samples in a "middle third" of the disagreement region and eliminating hypotheses with high empirical error enables learning with O(1) misclassification and O(log¬≤T) abstention error under Random Classification Noise.

## Foundational Learning

- **Concept: Version Space and Consistency**
  - Why needed here: The learner maintains F_t = {f ‚àà F : f(x_i) = y_i ‚àÄ(x_i, y_i) ‚àà S_t} to track all hypotheses consistent with observed data. All predictions and abstentions derive from this set.
  - Quick check question: Given labeled points {(0.2, 1), (0.8, 0)} for threshold classifiers on [0,1], what is the version space? (Answer: thresholds a ‚àà [0.2, 0.8))

- **Concept: VC Dimension and Shattering**
  - Why needed here: Finite VC dimension guarantees that the shattering probability œÅ_k(F_t) decays, enabling sublinear error bounds. The algorithm iterates through levels k = d, d-1, ..., 0.
  - Quick check question: What is the VC dimension of axis-aligned rectangles in ‚Ñù¬≤? (Answer: 4)

- **Concept: Exchangeability and Uniform Convergence**
  - Why needed here: In the realizable setting, exchangeability bounds the probability that the i-th i.i.d. sample falls in the disagreement region. In the agnostic setting, Hoeffding and union bounds ensure version space updates do not eliminate the target.
  - Quick check question: Why can't we use uniform convergence directly with adversarial injections? (Answer: Adversarial samples are not i.i.d., so the sample is not representative of D)

## Architecture Onboarding

- **Component map:**
  Input Handler -> Version Space Manager -> Prediction/Abstention Decision -> Update Trigger -> Error Tracker

- **Critical path:**
  Realizable, known D: Initialize k = d ‚Üí For each xÃÇ_t, compute œÅ_k(F_{t-1}^{xÃÇ_t‚Üíj}) ‚Üí Decide abstain/predict ‚Üí Update F_t ‚Üí If œÅ_k(F_t) ‚â§ Œ±_k, decrement k
  Agnostic, known D: Initialize i = 1, F_0 = F ‚Üí For each xÃÇ_t, check if in DIS(F_{i-1}) ‚Üí If yes, abstain; else predict ‚Üí Accumulate sample in middle third ‚Üí When |S_{i-1} ‚à© D_{i-1}| = M, eliminate hypotheses and increment i

- **Design tradeoffs:**
  Known vs. unknown D: Known D enables shattering probabilities; unknown requires structural algorithms
  Realizable vs. agnostic: Realizable allows per-sample updates; agnostic requires batched updates to handle noise
  Abstention cost model: Free abstention on adversarial samples enables robustness

- **Failure signatures:**
  Linear abstention error: Adversary consistently injects points in disagreement region
  Misclassification spike: In agnostic setting, if Œî or M is misspecified, f* may be eliminated
  Stalled learning: If adversarial injections prevent accumulating M samples in middle third

- **First 3 experiments:**
  1. Implement Algorithm 2 for thresholds on [0,1] with known uniform D and adversarial injections
  2. Implement Algorithm 6 with RCN Œ∑ ‚àà {0.1, 0.2, 0.4}, varying M around theoretical value
  3. Implement Algorithm 3 (structure-based for VC dimension 1) on synthetic tree-ordered domain

## Open Questions the Paper Calls Out

### Open Question 1
Can the disagreement-based learner for thresholds be extended to general hypothesis classes with finite VC dimension while maintaining sublinear error bounds under clean-label attacks and noise? (Section 4.1 states need to move away from structural dependence)

### Open Question 2
Can Algorithm 7 be refined to provide provable guarantees on misclassification and abstention error when the distribution is unknown and labels are noisy? (Section 3.2.2 states still needs adaptations)

### Open Question 3
Can the analysis be extended from Random Classification Noise and Massart noise to Tsybakov noise or the general agnostic setting? (Section 4.1 states would be desirable to obtain bounds for Tsybakov or agnostic case)

### Open Question 4
How should the hypothesis space update mechanism be designed to ensure fixed-amount reduction of the disagreement region when samples may be adversarial and labels are noisy? (Section 3.2.1 identifies as primary challenge)

## Limitations
- Restricted to thresholds and finite VC dimension classes, leaving broader hypothesis classes unexplored
- Agnostic setting assumes Random Classification Noise with known or bounded noise rate Œ∑ < 1/2
- Minimal empirical validation and unaddressed computational complexity of calculating shattering probabilities
- Assumes free abstention on adversarial samples, which may not reflect real-world cost structures

## Confidence
- High Confidence: Realizable setting with known distribution (Theorem 2.21, Algorithm 2)
- Medium Confidence: Agnostic setting under RCN (Theorem 3.6, Algorithm 6)
- Low Confidence: Unknown distribution algorithms (Algorithm 3, 4)

## Next Checks
1. Implement Algorithm 2 for thresholds with controlled adversarial injections and verify E[AbstentionError_T] ‚â§ 6 log T
2. Test Algorithm 6 with Œ∑ values near 0.5 and noise rate misspecification to validate robustness
3. Implement Monte Carlo approximation of œÅ_k(ùìï) for continuous distributions and verify computational feasibility