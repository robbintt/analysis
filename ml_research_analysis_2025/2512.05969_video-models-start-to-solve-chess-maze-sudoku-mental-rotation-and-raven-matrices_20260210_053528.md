---
ver: rpa2
title: Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven'
  Matrices
arxiv_id: '2512.05969'
source_url: https://arxiv.org/abs/2512.05969
tags:
- video
- reasoning
- arxiv
- language
- sudoku
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a new evaluation paradigm for assessing
  reasoning capabilities in video generation models, centered on a "Task Pair" design
  where models receive an initial unsolved problem image and text instruction, then
  generate a video showing the solution. The authors built VMEvalKit, a modular open-source
  framework supporting 39 video models and 5 reasoning tasks: chess, mazes, Sudoku,
  mental rotation, and Raven''s matrices.'
---

# Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven' Matrices

## Quick Facts
- arXiv ID: 2512.05969
- Source URL: https://arxiv.org/abs/2512.05969
- Authors: Hokin Deng
- Reference count: 22
- Primary result: Video models achieve >60% success rates on reasoning tasks, with Sora-2 reaching 87% on mazes, 73% on Raven's matrices, and 73% on chess mate-in-1 problems

## Executive Summary
This paper introduces VMEvalKit, a modular framework for evaluating reasoning capabilities in video generation models through a novel "Task Pair" paradigm. The approach constrains video models to generate solutions from initial problem states, enabling objective assessment of reasoning fidelity via final-frame comparison. Testing 39 video models across 450 tasks spanning chess, mazes, Sudoku, mental rotation, and Raven's matrices, the study demonstrates that leading models like Sora-2 can solve diverse reasoning problems with success rates exceeding 60%. Automated evaluation using GPT-4o shows strong correlation with human judgment (Pearson r = 0.949), validating the approach's scalability. The work reveals that video models can reason across domains while highlighting specific limitations in temporal coherence and constraint satisfaction.

## Method Summary
The paper establishes a "Task Pair" evaluation paradigm where video models receive an initial unsolved problem image and text instruction, then generate a video showing the solution. VMEvalKit, the modular framework, supports 39 video models and 5 reasoning tasks: chess (mate-in-1), mazes (3×3 grids), Sudoku (3×3, one missing number), mental rotation (8-9 cube voxel structures), and Raven's matrices (3×3 pattern completion). Each task generates 15 test instances. Models produce 8-second videos with standardized parameters, and automated evaluation using GPT-4o compares generated final frames against ground-truth solutions. Human validation confirms strong correlation between automated and expert scores, enabling scalable assessment of video model reasoning capabilities.

## Key Results
- Leading models like Sora-2 achieve >60% success rates across reasoning tasks
- Strong performance in maze navigation (87%), Raven's matrices (73%), and chess (73%)
- Mental rotation remains challenging with performance degradation during transformation
- Automated GPT-4o evaluation correlates strongly with human judgment (Pearson r = 0.949)
- Video models demonstrate reasoning across diverse domains rather than mere memorization

## Why This Works (Mechanism)

### Mechanism 1: Task Pair Grounding
Constraining video generation to start-state and target-state enables objective evaluation of reasoning fidelity. By withholding ground-truth final images during inference and comparing generated final frames, the system converts ambiguous "video quality" into binary "reasoning correctness" metrics. The model must maintain temporal coherence while modifying specific attributes without hallucinating invalid intermediate states.

### Mechanism 2: Scaling Hypothesis & Visual Compression
Reasoning emerges from data scale and compression rather than explicit symbolic logic. Training on massive visual datasets allows models to implicitly learn physical rules and logical constraints as statistical regularities. World content compression serves as a substrate for reasoning, where models simulate logical next-steps by predicting probable visual continuations consistent with learned rules.

### Mechanism 3: VLM-Based Evaluation Alignment
Automated evaluation using high-capability VLMs (GPT-4o) serves as reliable proxy for human judgment. The evaluation pipeline compares generated video outcomes against ground truth, validated by strong correlation between GPT-4o scores and human expert ratings. This enables scalable reasoning assessment across thousands of tasks where human labeling would be infeasible.

## Foundational Learning

- **Diffusion/Flow Matching for Video**: Video models generate frames by denoising latent noise conditioned on text/images. Understanding this mechanism is critical for analyzing why models might "hallucinate" moves or fail to maintain state. *Quick check*: Does the model predict next frame discretely or synthesize entire temporal sequence simultaneously?

- **Emergent Capabilities vs. Memorization**: The paper claims models are reasoning, not retrieving memorized solutions. Distinguishing rule application from pattern matching is key to interpreting success rates. *Quick check*: If we perturb maze walls slightly, would a memorizing model fail while a reasoning model adapts?

- **Constraint Satisfaction Problems (CSPs)**: Tasks like Sudoku and chess are formal CSPs requiring specific constraint satisfaction. Understanding these constraints helps analyze failure modes. *Quick check*: In Sudoku, does the model fail because it generates an illegible digit, or violates uniqueness constraint?

## Architecture Onboarding

- **Component map**: Task Generator → Inference Wrapper → Video Model → Frame Extractor → Evaluator (GPT-4o/Human)

- **Critical path**: 1. Task Generator creates "Question" (Start + Prompt) and "Answer" (Final Frame) 2. Question sent to Video Model 3. System extracts final frame from generated video 4. Evaluator compares extracted frame with Ground Truth Answer

- **Design tradeoffs**: Simplicity vs. Depth (3x3 puzzles ensure fit but limit complexity); Final Frame vs. Process (robust for Sudoku but may miss lucky errors in Chess/Maze)

- **Failure signatures**: Chess models making "legal but useless" or "illegal" moves; Mental rotation showing shape deformation; State drift in mazes where agent disappears or changes color

- **First 3 experiments**: 1. Reproduce baseline: Run VMEvalKit on smaller open-source model (LTX-Video) on 15 Maze tasks 2. Perturbation test: Generate Sudoku tasks with rotated digits to test rule vs. memorization 3. Process evaluation: Modify GPT-4o prompt to score trajectory in Maze tasks, not just final frame

## Open Questions the Paper Calls Out

- **Open Question 1**: Can reinforcement learning utilizing binary success signals successfully fine-tune video models for improved reasoning consistency? The authors identify an opportunity for RL given clear success signals, but don't implement the training loop.

- **Open Question 2**: How can "language-free" reasoning be validly evaluated in video models given linguistic contamination of training data? The authors ask about evaluating reasoning when training data contains language, making it difficult to distinguish visual reasoning from linguistic structures.

- **Open Question 3**: What principled framework can systematically analyze idiosyncratic failure patterns observed in video model reasoning? The authors note they lack a framework to systematically analyze or interpret failure cases, with current analysis being qualitative and observational.

## Limitations

- Task complexity constraints: Simplified 3×3 puzzles may not capture depth of reasoning required for expert-level problems
- Generalization vs. memorization: Evaluation doesn't systematically probe whether models apply rules versus retrieve memorized patterns
- Evaluation pipeline dependencies: Automated evaluation relies entirely on GPT-4o's visual understanding, which could systematically affect reported success rates

## Confidence

- **High Confidence**: Task Pair paradigm effectively converts video generation to objective reasoning assessment; VMEvalKit successfully orchestrates inference across 39 models; GPT-4o evaluation correlates strongly with human judgment
- **Medium Confidence**: Video models demonstrate reasoning across multiple domains; Scaling hypothesis explains emergent reasoning; Temporal coherence issues indicate domain-specific limitations
- **Low Confidence**: Claims about specific model architectures' internal reasoning mechanisms remain speculative; Success rates' generalization to novel configurations is unproven; Long-term temporal planning capabilities remain untested

## Next Checks

1. **Controlled Generalization Test**: Generate perturbed versions of test puzzles (rotated digits in Sudoku, maze wall modifications) and measure performance degradation to distinguish rule-based reasoning from memorization.

2. **Temporal Process Evaluation**: Modify evaluation protocol to assess intermediate frames in maze and chess tasks, not just final frames, to identify models that "teleport" to correct solutions versus demonstrating step-by-step reasoning.

3. **Scaling Challenge Experiment**: Test same video models on scaled-up versions of each task (9×9 Sudoku, 10×10 mazes, complex chess positions) to identify complexity threshold where reasoning capabilities break down.