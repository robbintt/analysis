---
ver: rpa2
title: Stable On-Policy Distillation through Adaptive Target Reformulation
arxiv_id: '2601.07155'
source_url: https://arxiv.org/abs/2601.07155
tags:
- student
- teacher
- on-policy
- distillation
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Veto addresses training instability in on-policy knowledge distillation\
  \ by reformulating the target distribution in logit space, effectively suppressing\
  \ harmful gradients for low-confidence student predictions. The method constructs\
  \ a geometric bridge between teacher and student distributions using a parameter\
  \ \u03B2 that controls both gradient vetoing and mode-seeking behavior."
---

# Stable On-Policy Distillation through Adaptive Target Reformulation

## Quick Facts
- arXiv ID: 2601.07155
- Source URL: https://arxiv.org/abs/2601.07155
- Authors: Ijun Jang; Jewon Yeom; Juan Yeo; Hyunggu Lim; Taesup Kim
- Reference count: 12
- Key outcome: Veto achieves 39.9% accuracy on GSM8K vs 35.1% baseline, 16.4% Pass@10 on HumanEval vs 10.6% baseline, and 56.5% win-rate on DialogSum vs 54.3% baseline

## Executive Summary
Veto addresses training instability in on-policy knowledge distillation by reformulating target distributions in logit space, effectively suppressing harmful gradients when student predictions have low confidence. The method constructs a geometric bridge between teacher and student distributions using a parameter β that controls both gradient vetoing and mode-seeking behavior. Through extensive experiments across GSM8K, HumanEval, and DialogSum benchmarks, Veto demonstrates consistent improvements over supervised fine-tuning and existing on-policy baselines, providing a unified framework for stable distillation that connects to adaptive gradient suppression and entropy-regularized policy gradients.

## Method Summary
Veto introduces an adaptive target reformulation technique for on-policy knowledge distillation that operates by vetoing harmful gradients during training. The method reformulates the KL divergence loss by constructing a geometric bridge between teacher and student distributions in logit space, controlled by a parameter β. This β parameter determines both when to veto gradients (when student confidence is low) and how strongly to enforce mode-seeking behavior. The approach is theoretically grounded, connecting to adaptive gradient suppression in forward KL and entropy-regularized policy gradients in reverse KL, providing a unified framework for stable distillation that maintains the benefits of on-policy training while mitigating its instability issues.

## Key Results
- GSM8K accuracy: 39.9% (Veto) vs 35.1% (baseline)
- HumanEval Pass@10: 16.4% (Veto) vs 10.6% (baseline)
- DialogSum win-rate: 56.5% (Veto) vs 54.3% (baseline)

## Why This Works (Mechanism)
Veto works by adaptively suppressing harmful gradients during on-policy distillation through a geometric bridging mechanism in logit space. When the student's prediction confidence is low (as measured by the teacher's softmax probability), Veto reduces the influence of the teacher's target distribution, preventing the student from being misled by overconfident but potentially incorrect teacher predictions. The parameter β controls this vetoing behavior while simultaneously encouraging mode-seeking, creating a balance between stability and learning effectiveness. This adaptive approach addresses the fundamental instability of on-policy distillation where the student's evolving predictions can create harmful feedback loops with the teacher's fixed targets.

## Foundational Learning
**KL Divergence and Information Theory**: Measures the difference between probability distributions, essential for knowledge distillation objectives. Quick check: Verify that KL divergence is asymmetric and represents information loss.

**Softmax and Temperature Scaling**: Converts logits to probability distributions, with temperature controlling peakiness. Quick check: Confirm that higher temperatures produce more uniform distributions.

**Policy Gradient Methods**: Reinforcement learning techniques that update policies based on gradient estimates. Quick check: Ensure understanding of the connection between distillation and policy optimization.

**Entropy Regularization**: Adds entropy terms to encourage exploration or prevent premature convergence. Quick check: Verify that entropy regularization promotes more uniform distributions.

## Architecture Onboarding
**Component Map**: Teacher model -> Veto module -> Student model -> KL loss computation -> Parameter β adjustment

**Critical Path**: The most time-consuming operation is computing the KL divergence between teacher and student distributions for each training example, followed by the β-modulated vetoing transformation.

**Design Tradeoffs**: Veto trades computational overhead (additional KL computations) for improved training stability. The choice of β involves balancing between aggressive vetoing (high stability, slower learning) and minimal vetoing (faster learning, potential instability).

**Failure Signatures**: Training instability manifests as oscillating or diverging loss curves, particularly when student predictions rapidly diverge from teacher targets. Poor β selection can lead to either excessive vetoing (stalled learning) or insufficient vetoing (continued instability).

**First Experiments**:
1. Compare training stability metrics (loss variance, gradient norms) between Veto and standard on-policy distillation
2. Perform ablation studies varying β to identify optimal ranges for different task difficulties
3. Test Veto with deliberately miscalibrated teacher models to assess robustness to confidence estimation errors

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Reliance on teacher softmax probability as confidence proxy may not accurately reflect true prediction uncertainty
- Computational overhead from KL divergence calculations may become prohibitive at scale
- Limited evaluation on larger models (only up to 1.3B parameters tested)

## Confidence
**High confidence**: The core mathematical formulation is sound with clear connections to established concepts in KL divergence and policy gradients. Experimental results show consistent improvements across multiple benchmarks.

**Medium confidence**: Theoretical analysis connecting Veto to entropy-regularized policy gradients is compelling but relies on assumptions about teacher-student distribution relationships. Ablation studies are supportive but limited in scope.

**Low confidence**: Claims about relative performance to offline distillation methods are difficult to fully evaluate without access to specific teacher models and training details.

## Next Checks
1. Test Veto on larger language models (7B+ parameters) to verify scalability and persistence of performance gains
2. Evaluate Veto's robustness when teacher models have deliberately miscalibrated softmax probabilities
3. Apply Veto to diverse NLP tasks beyond current evaluation suite to assess cross-task generalization