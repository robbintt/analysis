---
ver: rpa2
title: 'Reproducibility review of "Why Not Other Classes": Towards Class-Contrastive
  Back-Propagation Explanations'
arxiv_id: '2501.11096'
source_url: https://arxiv.org/abs/2501.11096
tags:
- original
- explanation
- weighted
- contrastive
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work reproduces and extends the paper "Why Not Other Classes?":
  Towards Class-Contrastive Back-Propagation Explanations, which proposes a method
  for explaining why a neural network chooses one class over others using back-propagation
  from after the softmax layer rather than before. The reproduced experiments show
  similar results to the original paper, validating that the weighted contrastive
  method increases target class probability and accuracy when perturbing input features.'
---

# Reproducibility review of "Why Not Other Classes": Towards Class-Contrastive Back-Propagation Explanations

## Quick Facts
- **arXiv ID:** 2501.11096
- **Source URL:** https://arxiv.org/abs/2501.11096
- **Reference count:** 11
- **Primary result:** Reproduced experiments validate that back-propagating from softmax outputs yields more accurate contrastive explanations than from logits, with successful generalization to Vision Transformers and alternative methods like XGradCAM and FullGrad.

## Executive Summary
This work reproduces and extends the paper "Why Not Other Classes?": Towards Class-Contrastive Back-Propagation Explanations, which proposes a method for explaining why a neural network chooses one class over others using back-propagation from after the softmax layer rather than before. The reproduced experiments show similar results to the original paper, validating that the weighted contrastive method increases target class probability and accuracy when perturbing input features. The method generalizes well to Vision Transformers and alternative back-propagation methods like XGradCAM and FullGrad, though some require modifications like removing ReLU to work properly. The main contribution is demonstrating that back-propagating from softmax outputs provides more accurate contrastive explanations than from logits, particularly for highlighting relevant image regions when distinguishing between similar classes.

## Method Summary
The method generates contrastive explanations by back-propagating from softmax outputs (target class probability $p_t$) rather than logits ($y_t$). For gradient-based attribution methods like GradCAM, this involves removing ReLU activations to capture both positive and negative feature contributions, then applying a weighted contrast formula ($\phi_t^{weighted} = \phi_t - \sum_{s \neq t} \alpha_s \phi_s$) to highlight features supporting the target class while suppressing evidence for other classes. The approach extends to Vision Transformers through gradient-weighted attention rollout, where attention matrices replace convolutional feature maps. The method is validated through perturbation experiments showing increased target probability and accuracy, blurring/masking experiments demonstrating feature relevance, and qualitative visualizations comparing contrastive versus non-contrastive explanations.

## Key Results
- Weighted contrastive method increases target class probability and accuracy when perturbing input features, validating the core claim
- Method generalizes successfully to Vision Transformers using gradient-weighted attention rollout, though results are qualitatively weaker than CNNs
- Removing ReLU for XGradCAM and FullGrad enables proper contrastive behavior, with FullGrad requiring additional normalization to avoid fully negative saliency maps

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Back-propagating from softmax outputs yields more accurate contrastive explanations than from logits.
- **Mechanism:** The weighted contrast formula ($\phi_t^{weighted} = \phi_t - \sum_{s \neq t} \alpha_s \phi_s$) is mathematically equivalent to computing gradients directly from the softmax probability $p_t$ for most gradient-based methods. This aligns the explanation with the actual prediction probability rather than raw logit values.
- **Core assumption:** The explanation should correlate with changes in prediction probability, not logit magnitude.
- **Evidence anchors:**
  - [abstract] "back-propagating from softmax outputs provides more accurate contrastive explanations than from logits, particularly for highlighting relevant image regions when distinguishing between similar classes"
  - [section 4.2] "back-propagating from the p neuron has been commonly done before...The value is therefore showing that back-propagating from target softmax neuron $p_t$ yields a proven weighted contrastive explanation"
- **Break condition:** When one class dominates ($p_1 >> p_2$), the contrastive component approaches zero and the method reduces to non-contrastive explanation.

### Mechanism 2
- **Claim:** Removing ReLU activations enables contrastive explanations to capture both positive and negative feature contributions.
- **Mechanism:** Standard GradCAM applies ReLU to filter out negative values, assuming they belong to other classes. For contrastive explanations, negative values must be preserved to show what evidence opposes a class assignment.
- **Core assumption:** Negative gradient contributions carry meaningful information about class distinctions rather than noise.
- **Evidence anchors:**
  - [section 2.1] "The modified version of XGradCAM removes ReLU, as in the original paper, and as a consequence uses the absolute values in the feature map sum when normalizing"
  - [section 3.2.1] "large negative areas in some images, especially seen when comparing our GradCAM to other implementations, are due to the omission of ReLU"
- **Break condition:** If the base method fundamentally requires non-negative outputs (e.g., certain attention mechanisms), removing ReLU may produce incoherent results.

### Mechanism 3
- **Claim:** The method generalizes to Vision Transformers through gradient-weighted attention rollout with softmax back-propagation.
- **Mechanism:** ViT lacks spatial coherence due to global self-attention. Gradient-weighted attention rollout restores information flow by backtracking attention matrices weighted by gradients from the target softmax neuron.
- **Core assumption:** Attention matrices can serve as analogs to CNN feature maps for explanation purposes.
- **Evidence anchors:**
  - [section 3.5] "This explanation is significantly more accurate to the perceived localization of the image...The weighted contrastive method with regard to the softmax further shows an even more detailed explanation"
  - [section 3.4] "We get qualitatively worse results compared to CNNs, with most explanations generating nonsense results that do not seem to be correlated to the image"
- **Break condition:** When spatial mixing in ViT layers is extreme, patch-to-pixel upsampling fails to represent meaningful pixel importance.

## Foundational Learning

- **Concept: Softmax Jacobian and class interdependence**
  - Why needed here: Understanding why $\partial p_i / \partial y_j \to 0$ when $p_i \to 1$ explains why contrastive methods fail with dominant classes.
  - Quick check question: Can you explain why the gradient between softmax outputs and logits approaches zero when one class probability approaches 1?

- **Concept: Gradient-based attribution methods (GradCAM, FullGrad, XGradCAM)**
  - Why needed here: The method modifies existing attribution techniques by changing the back-propagation starting point.
  - Quick check question: What is the difference between GradCAM and XGradCAM in how they weight feature maps?

- **Concept: Vision Transformer attention mechanisms**
  - Why needed here: Extending the method to ViT requires understanding how self-attention differs from convolutional spatial coherence.
  - Quick check question: Why does removing ReLU and using attention rollout help produce better ViT explanations?

## Architecture Onboarding

- **Component map:** Input image → CNN/ViT backbone → Logits → Softmax → Target class probability $p_t$ → Back-propagation path: $p_t$ (not logit $y_t$) → Feature maps/attention → Gradient computation → Explanation map → Post-processing: Remove ReLU, apply normalization if needed, visualize

- **Critical path:**
  1. Identify target softmax neuron $p_t$ for explanation
  2. Modify base explanation method to remove ReLU (GradCAM, XGradCAM) or abs functions (FullGrad)
  3. Back-propagate from $p_t$ instead of $y_t$
  4. For ViT: Use gradient-weighted attention rollout instead of GradCAM

- **Design tradeoffs:**
  - Visualization fidelity vs. mathematical correctness: Removing ReLU produces larger negative regions that may look different from original paper visualizations
  - Threshold selection: $p_2 > 0.1$ threshold ensures meaningful contrast but excludes many images with dominant classes
  - Normalization approach: FullGrad requires normalization to achieve contrastiveness, which may introduce artifacts

- **Failure signatures:**
  - Fully negative saliency maps (FullGrad without normalization)
  - Nonsensical explanations uncorrelated with image content (ViT GradCAM on some layers)
  - Contrastive and non-contrastive explanations appearing identical (when $p_2 \approx 0$)
  - Heatmaps not matching original paper visualization (due to unspecified visualization procedures)

- **First 3 experiments:**
  1. Reproduce perturbation experiment (Section 3.1) with $\epsilon = 3 \times 10^{-3}$ on ImageNet validation set, verifying that weighted method increases $p_t$ and accuracy while original method increases $y_t$
  2. Implement contrastive GradCAM with ReLU removed on VGG-16 fine-tuned on CUB-200, testing blurring/masking on samples with $p_2 > 0.1$
  3. Adapt gradient-weighted attention rollout for ViT on Food-101, comparing contrastive vs. non-contrastive explanations on images with multiple probable classes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would normalizing explanation maps before applying weighted contrast improve performance when there are dominating classes ($p_1 >> p_2$)?
- Basis in paper: [explicit] "For future work, normalizing the explanation before weighing could be considered."
- Why unresolved: Current weighted contrast method reduces to non-contrastive explanation when one class dominates, as explanation weights increase with logit strength (3-4x higher for dominant classes).
- What evidence would resolve it: Experiments comparing current weighted contrast vs. normalized-weighted contrast on images with varying class probability distributions, measuring relative probability changes and visualization quality.

### Open Question 2
- Question: How can contrastive back-propagation explanations be properly adapted for Vision Transformer architectures given their weaker spatial coherence?
- Basis in paper: [explicit] "We believe that this is mostly due to the weaker spatial relationship between token-wise representations and that the method for upscaling patches, or activations, in later layers, to input image does not adequately represent pixel importance in ViTs."
- Why unresolved: Standard GradCAM on ViT produces "nonsense results" and "qualitatively worse results compared to CNNs" due to self-attention allowing information mixing regardless of spatial distance.
- What evidence would resolve it: Development of attention-specific contrastive explanation methods that account for token-wise spatial mixing, evaluated on fine-grained classification tasks.

### Open Question 3
- Question: Can contrastive explanations be made useful for misclassification debugging when no competing classes have significant probability?
- Basis in paper: [inferred] "In scenarios where a misclassification has occurred a contrastive method to compare the correct class to the misclassified class can be useful, thus there is a use for contrastive method even without dominating classes showing an opportunity for advancement in future work."
- Why unresolved: Current method requires $p_2 > 0.1$ threshold to show meaningful contrastive behavior; when $p_2 \approx 0$, contrastive and non-contrastive methods produce nearly identical results.
- What evidence would resolve it: Modified contrastive methods tested on misclassified samples where the model is confidently wrong, measuring ability to highlight discriminative features between correct and predicted classes.

## Limitations
- Missing visualization normalization details prevent exact heatmap replication across different implementations
- ReLU removal for contrastive explanations produces larger negative regions that may look different from standard GradCAM visualizations
- ViT explanations show significant variability across layers and may not always produce coherent results due to weaker spatial coherence

## Confidence
- **High confidence:** ImageNet perturbation experiments showing weighted method increases target probability and accuracy
- **Medium confidence:** ViT extension and FullGrad modifications due to implementation complexity and lack of direct comparison baselines
- **Low confidence:** CUB-200 experiments since reproducing authors could not perfectly match visualization styles

## Next Checks
1. **Quantitative comparison validation:** Run the perturbation experiment ($\epsilon = 3 \times 10^{-3}$) on a held-out test set and compute statistical significance of accuracy improvements between weighted and non-weighted methods across 100+ images with $p_2 > 0.1$.

2. **Cross-architecture consistency:** Apply the contrastive explanation method to ResNet and EfficientNet architectures to verify that softmax back-propagation provides consistent improvements across different CNN architectures, not just VGG-16.

3. **Ablation study on ReLU removal:** Systematically compare GradCAM visualizations with and without ReLU for both contrastive and non-contrastive settings across multiple images to quantify the impact of negative feature preservation on explanation quality.