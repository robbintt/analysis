---
ver: rpa2
title: 'Lean4Physics: Comprehensive Reasoning Framework for College-level Physics
  in Lean4'
arxiv_id: '2510.26094'
source_url: https://arxiv.org/abs/2510.26094
tags:
- reasoning
- physics
- lean4
- arxiv
- problems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Lean4Physics introduces Lean4PHYS, a framework for formal physics
  reasoning in Lean4, comprising PhysLib (a foundational physics unit system and theorem
  repository) and LeanPhysBench (a 200-statement benchmark from university textbooks
  and competitions). Experiments show that while top expert Lean provers achieve only
  16% accuracy and large general models 35%, integrating PhysLib improves performance
  by 11.75% on average, demonstrating both the challenge of formal physics reasoning
  and the effectiveness of PhysLib in enhancing model capabilities.
---

# Lean4Physics: Comprehensive Reasoning Framework for College-level Physics in Lean4

## Quick Facts
- arXiv ID: 2510.26094
- Source URL: https://arxiv.org/abs/2510.26094
- Reference count: 32
- Top expert Lean provers achieve 16% accuracy vs. 35% for large general models on formal physics reasoning tasks

## Executive Summary
Lean4Physics introduces Lean4PHYS, the first framework for formal physics reasoning in Lean4, combining PhysLib (a physics unit system and theorem repository) with LeanPhysBench (a 200-statement benchmark from university textbooks and competitions). Experiments reveal that while top expert Lean provers achieve only 16% accuracy on physics formalization compared to 35% for large general models, integrating PhysLib improves performance by 11.75% on average. This demonstrates both the challenge of transferring mathematical reasoning skills to physics domains and the effectiveness of domain-specific libraries in enhancing model capabilities. The work establishes the first physics benchmark in Lean4 and highlights systematic difficulty stratification across problem types.

## Method Summary
The framework formalizes physics problems in Lean4 using a 3-layer PhysLib library (unit system foundation, topic-specific units, and theorems) and evaluates them on LeanPhysBench (200 statements stratified by difficulty). LLMs generate proofs with long chain-of-thought (temperature=0.8, top-p=0.95, max_tokens=16384), tested with/without PhysLib context. Verification uses Lean4 compiler with Pass@16 metric. Expert provers and general models are compared across difficulty levels, with performance improvements measured when PhysLib is provided.

## Key Results
- Expert Lean provers achieve only 16% accuracy vs. 35% for general models on formal physics reasoning
- Integrating PhysLib improves model performance by 11.75% on average
- Performance varies systematically: College problems (52%) > Competition-Easy (31%) > Competition-Hard (17%)

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Library Grounding
Providing PhysLib in context improves model performance by ~11.75% on formal physics reasoning tasks. PhysLib supplies a unit system with SI base units and dimensional consistency rules, physics-specific theorems and lemmas, and syntax for scalar/dimension operations. This reduces the out-of-distribution gap for models trained primarily on Mathlib-style mathematical content, enabling correct tactic selection (e.g., `simp`, `norm_num`, `Scalar.val_inj`) instead of generic tactics (`constructor`, `rw`, `abel`). Core assumption: Models can perform in-context learning of new definitions and syntax; the benefit stems primarily from better tool access rather than reasoning capability gains.

### Mechanism 2: Expert Prover Transfer Failure to Physics
Specialized Lean4 math provers do not outperform general LLMs on physics formalization, suggesting limited domain transfer. Expert provers are optimized for mathematical reasoning patterns (formula derivation, symbolic manipulation) common in MiniF2F/PutnamBench. Physics problems require (1) unit/dimension handling absent from Mathlib, (2) multi-domain knowledge integration (thermo + mechanics), and (3) different proof structures. General models compensate via broader pre-training and more flexible reasoning strategies. Core assumption: The gap is primarily due to domain mismatch rather than model scale.

### Mechanism 3: Difficulty Stratification by Problem Type
Performance varies systematically across college-level, competition-easy, and competition-hard problems, with harder problems requiring more PhysLib dependency. (1) College problems: heavy on unit handling, light on multi-step derivation—models with weaker in-context learning fail due to OOD syntax. (2) Competition-easy: closer to math problems, minimal PhysLib needed, expert provers perform competitively. (3) Competition-hard: requires complex symbolic reasoning + calculus + unit casts; only expert provers succeed (general models score 0%) due to longer reasoning chains and multi-trial strategies. Core assumption: Difficulty is primarily a function of PhysLib dependency, symbolic complexity, and proof length.

## Foundational Learning

- **Lean4 Dependent Type System & Unit Safety**: PhysLib encodes physics quantities as typed scalars with units (e.g., `Force`, `Mass`, `Length`). Understanding how Lean's type system enforces dimensional consistency at compile time is essential for debugging failed proofs. Quick check: Given `theorem T (m : Mass) (a : Acceleration) : Force := m * a`, what type error occurs if you swap `m` and `a`?

- **In-Context Learning of OOD Syntax**: The 11.75% improvement from PhysLib assumes models can generalize from provided library definitions to novel problem statements. Understanding the limits of this adaptation informs prompting strategy. Quick check: If PhysLib defines `g : Acceleration := 9.81 • meter / second^2`, can a model correctly infer `g.val` usage without explicit examples?

- **Tactic Selection & Proof Search**: The paper shows PhysLib enables advanced tactics (`simp`, `norm_num`, `Scalar.val_inj`) vs. basic tactics without it. Knowing which tactics apply to physics-specific goals is critical for manual proof construction and model evaluation. Quick check: For a goal `μ = f.val / N.val` where `f` and `N` are `Force` types, which tactic transforms this to a real-number equality?

## Architecture Onboarding

- **Component map**: PhysLib (3-layer) -> LeanPhysBench (200 statements) -> Formalization Pipeline (NL → Proof format alignment → Condition/Goal extraction → Lean4 code writing → Verification) -> Evaluation Harness (Pass@16 metric with/without PhysLib context)

- **Critical path**: 1) Set up Lean4 environment with Mathlib4 and PhysLib dependencies; 2) Load LeanPhysBench statements (JSON/Lean format); 3) Configure prompt template with/without PhysLib context; 4) Run model inference (temp=0.8, top-p=0.95, max_tokens=16384); 5) Submit generated proofs to Lean4 verifier; 6) Collect pass@k results and stratify analysis by difficulty level and topic.

- **Design tradeoffs**: PhysLib in-context vs. fine-tuning (in-context is cheaper but context-window-limited; fine-tuning may yield better generalization but requires substantial data); Long CoT vs. concise proofs (expert provers use longer reasoning chains but may "overthink" simple tasks); Open-source vs. closed-source models (open are reproducible but underperform; closed perform better but lack transparency).

- **Failure signatures**: Dimension mismatch errors (models generate proofs that type-check mathematically but fail due to unit inconsistencies); Tactic regression (without PhysLib, models use only `rw`, `abel`, `exact` and fail on physics-specific reasoning); Overthinking (expert provers generate 20+ step proofs for tasks solvable in 2 steps); Zero-shot hard-problem failure (general models score 0% on Competition-Hard).

- **First 3 experiments**: 1) Baseline Replication: Run Pass@16 evaluation for DeepSeek-Prover-V2-7B and Claude-Sonnet-4 on LeanPhysBench with/without PhysLib. Confirm 11.75% average improvement and difficulty-stratified results; 2) Ablation on PhysLib Components: Provide only the unit system layer (without topic theorems) to isolate the contribution of dimensional reasoning vs. domain knowledge; 3) Cross-Domain Transfer Test: Evaluate expert provers on a subset of LeanPhysBench problems reformulated to minimize unit operations (pure symbolic derivation).

## Open Questions the Paper Calls Out

- **Expert Prover Transfer**: Can expert Lean provers' mathematical reasoning capabilities transfer effectively to physics domains, or have they overfit to math-specific patterns? Authors state "it remains unclear whether their formal reasoning capabilities also apply to physics problems. Or these models have overfit to math reasoning patterns despite using similar Lean4 syntax."

- **Closed-Source Model Performance**: Why do closed-source general models achieve 0% on competition-hard problems while expert provers succeed on some (8.82%)? Table 1 shows Claude-Sonnet-4 and Gemini-2.5-pro achieve 0% on Comp-Hard, while Kimina-Prover-8B achieves 8.82%.

- **Adaptive Reasoning Techniques**: Would adaptive reasoning techniques that dynamically adjust strategy improve expert prover performance in underrepresented domains like thermodynamics? Case study notes "in fields underrepresented in training, the prolonged deliberation of expert models may lead to over-thinking and result in suboptimal results, a problem that may be addressed by adaptive reasoning techniques."

- **PhysLib Community Extension**: How will PhysLib's community-driven extension impact model performance as more domain-specific theorems are added? Authors describe PhysLib as "community-driven" with mechanics "comprehensively implemented" as an example.

## Limitations

- PhysLib's current non-public availability prevents independent validation of the claimed 11.75% performance improvement
- The evaluation framework using Pass@16 may miss subtle differences in proof quality between models that generate valid but suboptimal solutions
- The benchmark's reliance on textbook-style problems may not fully capture the complexity of open-ended physics reasoning tasks

## Confidence

**High Confidence** (Experimental results reproducible, core mechanisms well-supported):
- Domain-specific library grounding improves performance (~11.75%)
- Expert provers underperform general models on physics tasks
- Difficulty stratification follows predictable patterns

**Medium Confidence** (Results plausible but dependent on unvalidated components):
- PhysLib's specific architecture contributes to performance gains
- In-context learning of OOD syntax is the primary mechanism
- The 200-statement benchmark adequately represents college physics

**Low Confidence** (Core assumptions untested, results pending validation):
- PhysLib's unit system is optimally designed for Lean4 physics reasoning
- Closed-source model performance claims are accurate
- The benchmark's difficulty stratification holds across different problem formulations

## Next Checks

1. **PhysLib Availability Validation**: Obtain and test the PhysLib framework independently to verify the 11.75% performance improvement claim through controlled experiments comparing models with and without the library context.

2. **Expert Prover Fine-tuning Study**: Fine-tune existing expert Lean provers on PhysLib-augmented data and evaluate their performance on LeanPhysBench to determine if the domain-transfer failure is addressable through targeted training.

3. **Cross-Domain Transfer Experiment**: Reformulate LeanPhysBench problems to minimize unit operations while preserving physical reasoning complexity, then re-evaluate expert provers to isolate the impact of unit-system mismatch on performance.