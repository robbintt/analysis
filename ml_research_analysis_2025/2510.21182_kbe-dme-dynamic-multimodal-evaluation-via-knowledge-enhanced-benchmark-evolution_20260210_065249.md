---
ver: rpa2
title: 'KBE-DME: Dynamic Multimodal Evaluation via Knowledge Enhanced Benchmark Evolution'
arxiv_id: '2510.21182'
source_url: https://arxiv.org/abs/2510.21182
tags:
- triplets
- question
- knowledge
- answer
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces KBE-DME, a dynamic multimodal evaluation
  framework that transforms static VQA benchmarks into evolving test sets to address
  data contamination and saturation issues. By representing VQA problems as graphs
  with multimodal knowledge triplets, KBE-DME generates difficulty-controllable questions
  through two strategies: re-selecting key triplets from existing knowledge or exploring
  external knowledge.'
---

# KBE-DME: Dynamic Multimodal Evaluation via Knowledge Enhanced Benchmark Evolution

## Quick Facts
- arXiv ID: 2510.21182
- Source URL: https://arxiv.org/abs/2510.21182
- Authors: Junzhe Zhang; Huixuan Zhang; Xiaojun Wan
- Reference count: 29
- This paper introduces KBE-DME, a dynamic multimodal evaluation framework that transforms static VQA benchmarks into evolving test sets to address data contamination and saturation issues.

## Executive Summary
This paper introduces KBE-DME, a dynamic multimodal evaluation framework that transforms static VQA benchmarks into evolving test sets to address data contamination and saturation issues. By representing VQA problems as graphs with multimodal knowledge triplets, KBE-DME generates difficulty-controllable questions through two strategies: re-selecting key triplets from existing knowledge or exploring external knowledge. Experiments on OK-VQA and A-OKVQA show that as evaluation difficulty increases (1-3 expansion hops), model performance decreases consistently across five MLLMs including GPT-4o, Gemini-2.5-pro, Claude, LLaVA-OV, and Qwen-2.5-VL, demonstrating KBE-DME's effectiveness in providing reliable, adaptive evaluation that evolves with model capabilities.

## Method Summary
KBE-DME operates through a three-stage pipeline using GPT-4o: (1) **Extract** visual (M) and textual (T) knowledge triplets, identify key triplets (K); (2) **Explore** via Re-Selection (different K' from M∪T) or External Knowledge (add new triplets N, iterated up to 3 hops) with filtering (representativeness, POS=noun, cycle-check); (3) **Express**—transform updated G_K into new Q/A pair. The framework is evaluated on OK-VQA and A-OKVQA validation splits, generating difficulty-controllable questions that show consistent performance degradation across five MLLMs as expansion hops increase.

## Key Results
- Model performance consistently decreases across all five tested MLLMs (GPT-4o, Gemini-2.5-pro, Claude, LLaVA-OV, Qwen-2.5-VL) as expansion hops increase from 1 to 3
- Human evaluation shows 95% of generated questions are rated as reasonable, with high scores for triplet correctness and alignment
- The framework successfully transforms static VQA benchmarks into difficulty-controllable test sets that better evaluate reasoning capabilities beyond data contamination

## Why This Works (Mechanism)

### Mechanism 1: Reasoning Chain Extension via Graph Hops
The framework connects the original answer to external knowledge triplets (e.g., $Answer \to Attribute \to Component$). As the number of edges ($|E_K|$) in the key subgraph increases, the reasoning path lengthens. The model must traverse more nodes to arrive at the correct response. Core assumption: Model accuracy is inversely correlated with the number of reasoning steps required to answer the question. Evidence: GPT-4o drops from 50.33 to 39.19 as hops increase; all five models show declining performance with more hops.

### Mechanism 2: Semantic Perturbation via Graph Reconstruction
Unlike textual perturbation which often preserves the underlying key logic ($G_K$), KBE-DME re-selects triplets from visual ($G_M$) and textual ($G_T$) graphs to form a new key subgraph ($G_{K'}$). This alters the fundamental "reason" for the answer, rendering memorized training data less useful. Core assumption: Data contamination in MLLMs is primarily based on surface patterns rather than deep structural graph reasoning. Evidence: Static benchmarks remain vulnerable to memorization while KBE-DME's structural regeneration creates novel evaluation samples.

### Mechanism 3: Candidate Triplet Filtering
The system filters triplets based on Part-of-Speech (POS), representativeness, and cycle-check. It ensures the "object" in a triplet is unique and definitive (noun-based) to guarantee a single correct answer for the new VQA pair. Core assumption: High-quality VQA requires deterministic answers; ambiguous relations break evaluation validity. Evidence: 95% human evaluation scores for reasonableness suggest filtering successfully maintains semantic integrity.

## Foundational Learning

- **Knowledge Graph Triplets (Subject, Relation, Object)**
  - Why needed here: This is the atomic unit of data representation in KBE-DME. Understanding that an image or question is decomposed into these directed edges is essential to grasp how the "evolution" works.
  - Quick check question: Given an image of a red apple, can you represent the visual info as a triplet? (e.g., `Image -> depicts -> Apple`, `Apple -> has_color -> Red`).

- **Data Contamination vs. Saturation**
  - Why needed here: These are the distinct failure modes of static benchmarks that KBE-DME is designed to solve.
  - Quick check question: If a model scores 99% on a benchmark because it saw the questions during training, is that Saturation or Contamination? (Answer: Contamination).

- **Multimodal VQA (Visual Question Answering)**
  - Why needed here: The specific task domain. You must understand that the model takes an Image ($I$) and Question ($Q$) to produce an Answer ($A$).
  - Quick check question: In the KBE-DME formula $S_0 = \{I_0, Q_0, A_0\}$, which element is kept constant during the "Explore" phase to ensure visual grounding? (Answer: $I_0$).

## Architecture Onboarding

- **Component map:** Graph Extractor -> Filter Module -> Evolution Engine -> Question Generator
- **Critical path:** 1. Input: Static VQA sample $(I, Q, A)$; 2. Graphing: Extract triplets $\to$ Identify Key Subgraph $G_K$; 3. Expansion: Select new triplet $\to$ Update $G_K$ to $G_{K1}$; 4. Synthesis: Prompt LLM to write $Q_1$ targeting the new triplet object.
- **Design tradeoffs:** LLM-in-the-loop ensures high fluency but introduces hallucination risk; Static Image allows controlled comparison but limits visual diversity.
- **Failure signatures:** Cyclic Graphs (mitigated by cycle-check); Unanswerable Questions (if Re-selection picks non-prominent visual triplet).
- **First 3 experiments:** 1. Triplet Extraction Validation: Run prompts on 10 samples, manually verify $G_K$ captures necessary logic; 2. 1-Hop Sanity Check: Perform single expansion, verify new question asks for "Object" of new triplet; 3. Difficulty Scaling: Evaluate LLaVA on raw vs. 3-hop data, confirm accuracy drop aligns with Table 1 trends.

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's effectiveness depends critically on GPT-4o's ability to accurately parse visual and textual information, with no specified external knowledge source for triplet expansion
- Sampling criteria for selecting 2.6k/0.6k instances from validation sets are unspecified, making generalization unclear
- The assertion that the framework "evolves" with model capabilities is primarily supported by showing different models achieve different accuracies rather than demonstrating actual adaptation to improving performance over time

## Confidence

**High Confidence:** The observed performance degradation across expansion hops (Table 1) is well-supported by empirical results and demonstrates the difficulty-control mechanism functions as intended.

**Medium Confidence:** The claim that KBE-DME better addresses data contamination than textual perturbation is plausible given structural differences shown in Figure 1, but lacks direct comparison experiments.

**Low Confidence:** The assertion that the framework "evolves" with model capabilities is primarily supported by showing different models achieve different accuracies at the same hop level, rather than demonstrating actual adaptation to improving model performance over time.

## Next Checks

1. **Triplet Extraction Validation:** Manually verify 50 randomly sampled extracted knowledge triplets from both visual and textual graphs to assess GPT-4o's extraction accuracy and identify hallucination rates.

2. **Direct Contamination Comparison:** Implement a surface-level textual perturbation baseline on the same samples and compare performance degradation patterns to determine if KBE-DME's structural approach provides measurable advantages.

3. **External Knowledge Grounding:** Audit the external knowledge triplets used in 3-hop expansions by tracing them back to their source (if any) or evaluating their factual accuracy to assess the risk of introducing hallucinated information into the evaluation set.