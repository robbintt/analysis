---
ver: rpa2
title: Data Augmentation for Visualization Design Knowledge Bases
arxiv_id: '2508.02216'
source_url: https://arxiv.org/abs/2508.02216
tags:
- design
- pairs
- data
- augmentation
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of updating visualization design\
  \ knowledge bases, such as Draco, with new empirical findings without manual, error-prone\
  \ weight adjustments. It introduces three data augmentation techniques\u2014primitive,\
  \ feature, and seed augmentation\u2014to generate and label new chart pairs, expanding\
  \ the training corpus and improving feature coverage."
---

# Data Augmentation for Visualization Design Knowledge Bases

## Quick Facts
- arXiv ID: 2508.02216
- Source URL: https://arxiv.org/abs/2508.02216
- Authors: Hyeok Kim; Jeffrey Heer
- Reference count: 40
- Primary result: Data augmentation techniques improve Draco's prediction accuracy to 92% on augmented pairs

## Executive Summary
This paper addresses the challenge of updating visualization design knowledge bases like Draco with new empirical findings without manual, error-prone weight adjustments. The authors introduce three data augmentation techniques—primitive, feature, and seed augmentation—to systematically generate and label new chart pairs, expanding the training corpus and improving feature coverage. By using automated labeling methods and carefully designed augmentation strategies, the approach significantly improves prediction accuracy while maintaining the integrity of the underlying knowledge base.

## Method Summary
The method involves featurizing charts into constraint violation vectors, calculating difference vectors between preferred and less preferred charts, and training logistic regression or linear SVC models. Three augmentation techniques are employed: primitive augmentation varies irrelevant design properties while preserving logical differences, feature augmentation fills gaps in underrepresented features through targeted ablation, and seed augmentation generates additional examples for underrepresented chart types. The approach uses automated labeling through classifiers for near-domain data and LLMs for out-of-domain cases, with weights normalized to [-1000, 1000] for integration with Draco.

## Key Results
- Prediction accuracy improved from 75-81% to 92% overall using augmentation techniques
- Feature augmentation most effective, achieving 82% accuracy on feature-augmented pairs
- LLM labeling achieved 73% agreement with manual labels while scaling better than manual annotation

## Why This Works (Mechanism)

### Mechanism 1: Feature Coverage Expansion via Targeted Ablation
The system analyzes frequency of design features in the original corpus, identifies underrepresented features, and generates chart pairs where only that specific feature differs. This forces the model to assign weights to missing features, recovering performance on blind spots.

### Mechanism 2: Contextual Regularization via Primitive Variation
By preserving the logical "difference" (e.g., log vs linear) while varying irrelevant design properties (e.g., color, orientation), the approach reduces overfitting to specific visual stereotypes and improves generalization.

### Mechanism 3: Asymmetric Label Transfer
The optimal labeling strategy depends on distance from original training distribution: classifiers work for near-domain (primitive) while LLMs work for out-of-domain (feature) data, leveraging broader world knowledge for novel combinations.

## Foundational Learning

- **Answer Set Programming (ASP) & Constraints:** Draco uses logic programming where features are soft constraints with costs. Understanding this is crucial to grasp why learning linear weights matters for finding lowest-cost designs.
  - Quick check: If a chart violates a soft constraint, is it "illegal" (invalid) or just "expensive" (suboptimal)?

- **Ablation Studies (in ML/Vis context):** Feature augmentation relies on turning one feature off/on while holding others constant to isolate specific contributions to chart cost.
  - Quick check: In a binary ablation pair testing "log_scale", what is the only difference between Chart A and Chart B?

- **Data Augmentation (Synthetic Data):** Unlike image augmentation, chart augmentation requires semantic transformations (changing variables, flipping scales) rather than pixel-level operations.
  - Quick check: Why can't we use standard image augmentation (cropping/rotating) directly on chart pixels for this task?

## Architecture Onboarding

- **Component map:** Input Source (empirical studies) -> Augmenters (Primitive, Feature, Seed) using Draco Solver -> Labelers (Classifier/LLM) -> Trainer (Logistic Regression/SVC) -> Draco (Target)
- **Critical path:** Feature augmentation pipeline is critical for performance; without it, model accuracy drops to ~50% on new data
- **Design tradeoffs:** Primitive augmentation stabilizes existing knowledge; feature augmentation expands knowledge. Both are needed for optimal performance.
- **Failure signatures:** Solver generates unreadable charts (overplotting), weight drift forgetting basic charts, label skew from classifier extrapolating poorly to new feature combinations
- **First 3 experiments:**
  1. Run frequency analyzer on current corpus to identify top 10 features with <7 occurrences
  2. Implement ablation logic for one uncovered feature to generate 5 pairs and verify validity
  3. Compare manual vs LLM labeling on generated pairs to calculate agreement rates

## Open Questions the Paper Calls Out

- Can data augmentation techniques support synthesis of entirely new design features, rather than just updating weights for existing ones?
- How can continuous effect sizes from empirical studies be integrated into the knowledge base update process to replace binary preference labels?
- Do the benefits of proposed augmentation techniques generalize to visualization tasks other than "value" comparison, such as trend identification or summary tasks?

## Limitations
- Empirical validation primarily on original datasets with limited independent validation on truly novel empirical findings
- LLM-generated labels introduce noise, particularly for feature augmentation with novel combinations
- Solver-generated charts may be syntactically valid but visually inexpressive, requiring manual filtering

## Confidence
- **High confidence:** Core mechanism of feature coverage expansion and improved prediction accuracy on augmented pairs
- **Medium confidence:** Comparative advantage of different labeling strategies depending on augmentation type
- **Low confidence:** Long-term stability of learned weights and performance on entirely new empirical studies

## Next Checks
1. Apply augmented knowledge base to new empirical study not used in augmentation process and measure prediction accuracy
2. Systematically analyze LLM-generated labels for feature-augmented pairs to characterize bias patterns and error types
3. Implement and validate automated filtering criteria for visually inexpressive charts to replace manual removal process