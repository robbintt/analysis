---
ver: rpa2
title: 'HealthContradict: Evaluating Biomedical Knowledge Conflicts in Language Models'
arxiv_id: '2512.02299'
source_url: https://arxiv.org/abs/2512.02299
tags:
- knowledge
- language
- prompt
- context
- biomedical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HealthContradict, a dataset of 920 health-related
  questions paired with two contradictory documents and factual answers supported
  by scientific evidence. It evaluates language models' ability to reason over conflicting
  biomedical contexts.
---

# HealthContradict: Evaluating Biomedical Knowledge Conflicts in Language Models

## Quick Facts
- arXiv ID: 2512.02299
- Source URL: https://arxiv.org/abs/2512.02299
- Reference count: 40
- Models with biomedical fine-tuning achieve up to 91.1% accuracy on conflicting biomedical contexts

## Executive Summary
This paper introduces HealthContradict, a dataset of 920 health-related questions paired with contradictory documents and evidence-backed answers. The study evaluates how language models handle conflicting biomedical information, comparing general domain models against those fine-tuned on biomedical corpora. Results show that biomedical fine-tuned models significantly outperform their general counterparts when provided with correct context, achieving up to 91.1% accuracy. These models also demonstrate better resistance to incorrect context and reduced vulnerability to conflicting information, highlighting the importance of domain adaptation for reliable medical question answering.

## Method Summary
The study evaluates language models on the HealthContradict dataset, which pairs 920 biomedical questions with two contradictory documents and factual answers supported by scientific evidence. Models from 1B to 8B parameters are tested using five prompt templates: No Context (NC), Correct Context (CC), Incorrect Context (IC), Correct-then-Incorrect (CIC), and Incorrect-then-Correct (ICC). The evaluation measures accuracy, probability calibration, and failure modes including over-reliance on parametric knowledge versus vulnerability to misleading context. Zero-shot inference is used throughout to isolate reasoning capabilities.

## Key Results
- Biomedical fine-tuned models achieve up to 91.1% accuracy when provided correct context, outperforming general models by 21.1 percentage points
- Fine-tuned models show reduced vulnerability to incorrect context with probability scores shifting left (decreased certainty) compared to general models
- Biomedical models exhibit lower recency bias in conflicting contexts, with 23.5% switching to incorrect answers versus 40.3% for general models

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Context Exploitation
- **Claim:** Biomedical fine-tuning improves a model's capacity to extract and utilize relevant information from correct long-context documents
- **Mechanism:** Fine-tuning on domain-specific corpora (e.g., PubMed) shifts attention mechanisms and latent space representations to better align with biomedical text patterns
- **Core assumption:** Models need sufficient base capacity (>7B parameters) to internalize domain-specific attention patterns
- **Evidence anchors:** Abstract states fine-tuned models excel at exploiting correct context; section 2.4 shows MEDITRON3-8B outperforms LLAMA-3.1-8B-INSTRUCT by 21.1 percentage points with correct context

### Mechanism 2: Confidence Calibration via Contextual Resistance
- **Claim:** Biomedical fine-tuning induces calibration where models exhibit lower confidence when presented with incorrect context
- **Mechanism:** Models learn a tighter manifold for valid biomedical assertions, causing probability distributions to flatten when input falls outside this manifold
- **Core assumption:** Incorrect context must be semantically distinct from training distribution to trigger uncertainty
- **Evidence anchors:** Section 2.4.2 shows MEDITRON3-8B probability scores shift left for incorrect context versus consistently high probability for LLAMA-3.1-8B-INSTRUCT

### Mechanism 3: Robustness to Inter-Context Conflicts
- **Claim:** Fine-tuned models are more robust to document order (recency bias) compared to general models
- **Mechanism:** Stronger parametric prior makes models less susceptible to "last-token" bias when presented with contradictory passages
- **Core assumption:** Model's parametric knowledge is actually correct
- **Evidence anchors:** Section 2.4 shows MEDITRON3-8B has lower rate of switching to incorrect answers (23.5%) compared to LLAMA3.1-8B-INSTRUCT (40.3%)

## Foundational Learning

- **Concept: Parametric vs. Contextual Knowledge**
  - **Why needed here:** The paper evaluates the "tug of war" between what the model knows (weights) and what it reads (prompt)
  - **Quick check question:** If a model answers incorrectly without context but correctly with context, which knowledge source was utilized?

- **Concept: Recency Bias / Position Bias**
  - **Why needed here:** The prompt templates ICC (Incorrect then Correct) vs. CIC (Correct then Incorrect) test if the model simply believes the last thing it reads
  - **Quick check question:** In a RAG system with 5 retrieved chunks, why might the last chunk have disproportionate influence on the output?

- **Concept: Failure Modes (OR vs. VM)**
  - **Why needed here:** The paper defines specific failure modes: "Over-reliance on parametric knowledge" and "Vulnerability to misleading context"
  - **Quick check question:** Is a model that refuses to answer based on a provided document because it contradicts its training data displaying Over-reliance?

## Architecture Onboarding

- **Component map:** Dataset -> Prompt Templates -> Model Backbone -> Logit Extraction -> Probability Calculation
- **Critical path:**
  1. Load TREC Health Misinformation data
  2. Construct prompt using specific templates (NC, CC, IC, CIC, ICC)
  3. Pass through model to get logits for "Yes" and "No" tokens
  4. Calculate Probability Density and Failure Rates (OR/VM)
- **Design tradeoffs:**
  - Binary Classification: Clear logit analysis but limits nuanced medical answers
  - Zero-shot: Isolates reasoning capabilities but may underestimate fine-tuned model potential
- **Failure signatures:**
  - Stubborn Sloth (High OR): Refuses to switch to correct answer even with correct context
  - Chameleon (High VM): Flips to incorrect answer when misleading context is introduced
  - Recency Bias: Significant divergence between CIC and ICC scores
- **First 3 experiments:**
  1. Verify the "Calibration" Signal: Reproduce Figure 6 (Probability Density) for your specific domain model
  2. Ablate Context Length: Stratify dataset by document length to determine attention dilution effects
  3. Intervention Test (RAG): Implement a "Judge" step where model must output document stance before answering

## Open Questions the Paper Calls Out
None

## Limitations

- Dataset Scope and Generalizability: 920 instances cover only 46 topics with binary yes/no answers, potentially inflating performance by forcing definitive stances on ambiguous medical questions
- Zero-shot Evaluation Constraint: Exclusively uses zero-shot inference, which may underestimate the true potential of fine-tuned biomedical models that could benefit from task-specific adaptation
- Context Length Effects: Significant performance degradation for "Xtra Long" contexts suggests attention mechanisms may fail over extended documents, but this limitation is not systematically investigated

## Confidence

**High Confidence Claims**
- Biomedical fine-tuned models outperform general models when provided correct context
- Domain adaptation improves resistance to incorrect context
- Fine-tuned models show reduced recency bias in conflicting contexts

**Medium Confidence Claims**
- The "over-reliance on parametric knowledge" and "vulnerability to misleading context" framework accurately characterizes model failure modes
- The 8B parameter threshold represents a meaningful capacity boundary for context exploitation

**Low Confidence Claims**
- The specific mechanisms of "learned verification" through probability calibration
- The generalizability of findings to clinical decision support systems

## Next Checks

1. **Dataset Diversity Expansion:** Replicate evaluation using expanded HealthContradict dataset with multi-class answers to test whether observed advantages persist when models must express uncertainty

2. **Task-Specific Fine-tuning Experiment:** Implement small-scale fine-tuning (1-2 epochs) on HealthContradict training split to isolate contribution of domain adaptation from task adaptation

3. **Attention Mechanism Analysis:** Conduct layer-wise attention analysis on CIC and ICC prompt templates to identify whether biomedical models activate different attention patterns when processing conflicting contexts compared to general models