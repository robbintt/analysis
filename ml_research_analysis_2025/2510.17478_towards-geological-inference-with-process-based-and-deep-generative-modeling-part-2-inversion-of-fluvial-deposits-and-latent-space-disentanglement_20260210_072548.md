---
ver: rpa2
title: 'Towards geological inference with process-based and deep generative modeling,
  part 2: inversion of fluvial deposits and latent-space disentanglement'
arxiv_id: '2510.17478'
source_url: https://arxiv.org/abs/2510.17478
tags:
- wells
- data
- sample
- test
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the use of generative adversarial networks
  (GANs) for geological inference, specifically focusing on inverting fluvial deposits
  to match well and seismic data. Four inversion approaches (latent optimization,
  inference network, variational inference, and MCMC) were tested on three synthetic
  fluvial deposits with varying numbers of wells (4, 8, and 20).
---

# Towards geological inference with process-based and deep generative modeling, part 2: inversion of fluvial deposits and latent-space disentanglement

## Quick Facts
- arXiv ID: 2510.17478
- Source URL: https://arxiv.org/abs/2510.17478
- Reference count: 40
- Primary result: GAN inversion for fluvial deposits struggles due to entangled latent spaces, but pivotal tuning can locally restructure the space to achieve acceptable data matching.

## Executive Summary
This study investigates using generative adversarial networks (GANs) for geological inversion, specifically matching 3D fluvial deposits to well and seismic data. Four inversion approaches (latent optimization, inference network, variational inference, and MCMC) were tested on synthetic deposits with 4, 8, and 20 wells. All approaches struggled to match well data, particularly as well count increased or test samples diverged from training data. The key bottleneck was identified as the GAN's entangled latent representation. While label conditioning and latent overparameterization partially disentangled the latent space, they proved insufficient. Fine-tuning the GAN locally through pivotal tuning significantly improved results, reducing mismatches to acceptable levels for all test cases.

## Method Summary
The study uses synthetic 3D fluvial deposits generated by the CHILD process-based model (20,000 training samples, 128×128×16 cells). A 3D DCGAN architecture (Architecture 4) with residual blocks and spectral normalization generates geological realizations from 128-dimensional latent vectors. Four inversion approaches are tested on three synthetic test samples with varying well counts: latent optimization (gradient-based search), inference network (learned encoder), variational inference (normalizing flows), and MCMC sampling. All keep generator weights frozen during inversion. When standard approaches fail to achieve <1% inversion error, pivotal tuning is applied to locally restructure the latent space by fine-tuning generator weights using perceptual loss from the discriminator.

## Key Results
- Standard inversion approaches struggled to match well data, especially with 8-20 wells or when test samples diverged from training distribution
- The GAN's entangled latent space was identified as the key bottleneck, preventing efficient gradient-based search
- Label conditioning and latent overparameterization partially disentangled the space but were insufficient for successful inversion
- Pivotal tuning (local fine-tuning of generator weights) significantly improved results, reducing mismatches to acceptable levels for all test cases
- This approach depends on an initial, partially successful inversion step and cannot rescue severely misplaced initial latents

## Why This Works (Mechanism)

### Mechanism 1: Latent Space Inversion for Geological Prior Sampling
Pretrained GAN generators can be inverted to find latent vectors whose outputs match observed well/seismic data, provided the latent space contains a sufficiently close representation. Four inversion approaches were tested—latent optimization (gradient-based search), inference networks (learned encoder), variational inference (normalizing flows), and MCMC sampling. All keep generator weights frozen and search the latent space for vectors minimizing data mismatch. Core assumption: The geological target exists within the GAN's learned distribution (the prior). Evidence: Latent optimization systematically failed on 20-well cases; inference network and variational inference showed collapse with 8-20 wells.

### Mechanism 2: Entangled Latent Representations Block Gradient-Based Search
Standard GAN training produces entangled latent spaces where semantically similar geological features are not proximate, creating rugged error landscapes that trap optimizers. PCA visualization of error landscapes shows multiple local minima and flat regions. Entanglement means moving toward one geological feature (e.g., channel position) may require traversing regions that worsen other features (e.g., sediment fraction). Core assumption: Disentanglement would place geologically similar samples near each other in latent space, smoothing the inversion landscape. Evidence: Error landscapes become more rugged as well count increases, suggesting increased entanglement complexity.

### Mechanism 3: Pivotal Tuning Restructures Latent Space Locally
Fine-tuning generator weights (not just latent vectors) around a partially successful inversion can reshape the local latent manifold to reduce data mismatch to acceptable levels. After latent optimization provides initial estimates, pivotal tuning updates generator weights using a perceptual loss (discriminator-based, adapted for 3D). This locally warps the latent-to-output mapping without requiring full retraining. Core assumption: The initial inversion provides a "good enough" starting region; pivotal tuning cannot recover from severely misplaced initial latents. Evidence: Pivotal tuning was the only technique that passed the 1% threshold on the vast majority of its 300 samples for all cases.

## Foundational Learning

- **Concept: Generative Adversarial Networks (GANs)**
  - Why needed here: Understanding the generator-discriminator adversarial game explains why latent spaces form and why they may be entangled. The generator maps latent vectors to geological volumes; inversion requires reversing this mapping.
  - Quick check: Can you explain why a trained GAN generator has no direct inverse, necessitating optimization-based inversion?

- **Concept: Bayesian Inversion and Posterior Sampling**
  - Why needed here: The paper frames inversion as Bayesian inference—finding the posterior distribution of geological models given data. MCMC and variational inference attempt to sample this posterior; understanding this clarifies why sample diversity matters.
  - Quick check: Why does inference network collapse (all samples identical) indicate failure to capture posterior uncertainty rather than just poor data matching?

- **Concept: Latent Space Geometry and Disentanglement**
  - Why needed here: The central bottleneck is latent entanglement. Disentangled representations would have interpretable directions (e.g., one dimension controls channel width); entangled spaces mix effects unpredictably.
  - Quick check: If a latent space is perfectly disentangled, what would you expect the error landscape to look like during inversion?

## Architecture Onboarding

- **Component map:**
  Process-based model (CHILD) -> Training data (20k realizations) -> GAN training (Architecture 4) -> Pretrained generator G(z) + discriminator D -> Inversion (4 methods) -> Pivotal tuning (if needed)

- **Critical path:**
  1. Train GAN on process-based outputs (or use pretrained models from Rongier & Peeters 2025b)
  2. Start with latent optimization as baseline—it's simplest and reveals error landscape structure
  3. If inversion error >1% and error landscape is rugged, apply pivotal tuning
  4. Verify sample quality visually and via sliced Wasserstein distance to test samples

- **Design tradeoffs:**
  | Approach | Flexibility | Computational cost | Sample diversity | Data matching |
  |----------|-------------|-------------------|------------------|---------------|
  | Latent optimization | High (any data type) | Low per sample, high for many | High if successful | Poor alone |
  | Inference network | Medium (requires retraining for new data types) | Low after training | Prone to collapse | Moderate |
  | MCMC | High | Very high | High if converged | Moderate |
  | Pivotal tuning | High (any data type) | Medium | Depends on initialization | Best |

- **Failure signatures:**
  - Inversion error increases with well count → latent space likely lacks appropriate representation
  - All inverted samples identical → inference network/variational collapse; use MCMC or pivotal tuning
  - Checkboard patterns after pivotal tuning → initial latents too far from target; improve first-stage inversion
  - Convergence diagnostic R̂ never satisfied (MCMC) → latent space too entangled for sampling

- **First 3 experiments:**
  1. **Baseline latent optimization:** Start with 4 wells on test sample 1 (closest to training distribution). Run 300 random initializations, plot error distribution. This establishes whether your GAN's prior covers the target.
  2. **Error landscape visualization:** Use PCA on latent vectors from successful/failed optimizations to visualize error landscape (following Figure 4). Identify flat regions and local minima to assess entanglement severity.
  3. **Pivotal tuning rescue:** Take the best 10% of latent optimization results and apply pivotal tuning. Compare inversion error before/after. If improvement <50%, your initial inversion may be too poor to rescue—consider GAN conditioning or larger latent size first.

## Open Questions the Paper Calls Out

### Open Question 1
How robust is the proposed GAN inversion workflow when applied to real field data, particularly regarding out-of-distribution geological features and uncertain data-generating processes? The study relies on synthetic data where the ground truth and prior share the same generating physics, avoiding the issue of prior misspecification inherent in real-world applications. Testing on real data should be the next target.

### Open Question 2
Does conditioning the latent space on global descriptive features or process-based parameters yield a more disentangled structure suitable for inversion than post-hoc restructuring? While label conditioning was partially tested, it was "not yet sufficiently for a successful inversion" and suggests a deeper study of conditioning quality is needed. Would controlling more global, descriptive features like the width of the channel belt help?

### Open Question 3
Can alternative architectures like StyleGAN or flow-based models inherently provide a sufficiently disentangled latent space to remove the need for local restructuring methods like pivotal tuning? The discussion suggests "an even bigger change is worth considering," specifically referencing StyleGAN's W-space and invertible models as potential alternatives to overcome the entanglement bottleneck.

## Limitations
- The study relies on synthetic fluvial deposits with controlled well positions, limiting generalizability to real-world geological settings where well locations are irregular and data quality varies
- Pivotal tuning depends on initial inversion success, creating a bootstrap problem—the method cannot recover from severely poor starting points or highly out-of-distribution test cases
- Sample quality after pivotal tuning remains inconsistent, with some cases showing checkerboard artifacts and implausible channel configurations

## Confidence
- **High confidence:** The entanglement hypothesis and its role as a bottleneck is well-supported by error landscape analysis and consistent failure patterns across multiple inversion methods
- **Medium confidence:** The efficacy of pivotal tuning is demonstrated on synthetic cases but requires an initial successful inversion step, making its robustness to real-world noise uncertain
- **Low confidence:** The practical utility of these approaches on real geological datasets remains unproven, as all tests use synthetic data with known ground truth

## Next Checks
1. Test pivotal tuning on partially corrupted synthetic data (e.g., noisy wells or incomplete seismic coverage) to assess robustness to realistic measurement errors
2. Apply the workflow to a real field dataset with documented geological complexity to evaluate performance outside synthetic conditions
3. Compare pivotal tuning with alternative latent restructuring methods (e.g., conditional GANs or larger latent spaces) to determine if the proposed approach offers unique advantages