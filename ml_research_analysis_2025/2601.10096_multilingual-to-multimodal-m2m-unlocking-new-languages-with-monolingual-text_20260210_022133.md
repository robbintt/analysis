---
ver: rpa2
title: 'Multilingual-To-Multimodal (M2M): Unlocking New Languages with Monolingual
  Text'
arxiv_id: '2601.10096'
source_url: https://arxiv.org/abs/2601.10096
tags:
- multilingual
- text
- multimodal
- ours
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces M2M, a lightweight alignment method that\
  \ learns only a few linear layers using English text to map multilingual text embeddings\
  \ into multimodal space. Despite its simplicity, M2M achieves strong zero-shot transfer\u2014\
  89.5% Recall@10 averaged across 11 languages on XTD Text-to-Image retrieval\u2014\
  while matching baseline English performance (94.9% Recall@10)."
---

# Multilingual-To-Multimodal (M2M): Unlocking New Languages with Monolingual Text

## Quick Facts
- arXiv ID: 2601.10096
- Source URL: https://arxiv.org/abs/2601.10096
- Authors: Piyush Singh Pasi
- Reference count: 27
- Primary result: Achieves 89.5% Recall@10 zero-shot cross-lingual transfer averaged across 11 languages on XTD Text-to-Image retrieval

## Executive Summary
M2M introduces a lightweight alignment method that enables zero-shot cross-lingual transfer for multimodal models without requiring multilingual multimodal training data. The approach learns a small projection network that maps embeddings from a multilingual text encoder to match those from a frozen multimodal model, using only English text for training. Despite its simplicity, M2M achieves strong multilingual retrieval performance (89.5% Recall@10 across 11 languages) and generalizes to audio-text retrieval and text-to-image generation tasks. The method releases code, checkpoints, and multilingual evaluation datasets for community use.

## Method Summary
M2M aligns multilingual text embeddings to multimodal space by learning a small projection network (2-layer MLP, ~1M params) using only English text. The method freezes both the multilingual text encoder (M-MPNET) and multimodal encoder (Jina-CLIP-v1), training only the projection layers to minimize MSE between projected multilingual embeddings and multimodal embeddings. A structure-preserving loss ensures the geometric relationships between embeddings are maintained during alignment. The approach enables zero-shot cross-lingual transfer by leveraging the multilingual encoder's existing cross-lingual capabilities, requiring no multilingual multimodal training data.

## Key Results
- Achieves 89.5% Recall@10 zero-shot cross-lingual transfer averaged across 11 languages on XTD Text-to-Image retrieval
- Matches baseline English performance (94.9% Recall@10) while operating on multilingual text
- Generalizes beyond image-text retrieval to Audio-Text retrieval and Text-to-Image generation tasks
- Weight analysis reveals the transformation reshapes embedding geometry rather than performing trivial rotations

## Why This Works (Mechanism)

### Mechanism 1
A lightweight linear projection, trained with English text alone, can align multilingual text embeddings to a frozen multimodal space, enabling zero-shot cross-lingual transfer without multilingual multimodal data. The method learns a small projection map `F` that transforms embeddings from a multilingual text encoder (`Tm`) to match those from a multimodal model's text encoder (`Te`), using English text as the common anchor. The alignment, trained via MSE and structure-preserving losses, generalizes to other languages supported by `Tm`.

### Mechanism 2
The learned linear transformation performs meaningful geometric reshaping (mixing/rescaling) rather than a trivial rotation of the embedding space. Weight analysis reveals the projection matrix has a high orthogonality deviation, indicating it actively reshapes the geometry of the multilingual embedding space to fit the multimodal space, rather than just rotating it.

### Mechanism 3
The alignment learned for one modality (images) and task (retrieval) generalizes to other modalities (audio) and tasks (generation). Because the alignment operates on sentence-level (CLS) embeddings and does not rely on task-specific architectures, the mapped multilingual embeddings can be used as direct substitutes for the multimodal model's text embeddings in various downstream applications.

## Foundational Learning

- **Concept: Latent Space Alignment**
  - **Why needed here:** The core problem is mapping one vector space (multilingual text) to another (multimodal text). Understanding what it means for two spaces to be "aligned" is essential.
  - **Quick check question:** What geometric property must two vector spaces share for a linear map to successfully transfer a classification task from one to the other?

- **Concept: CLS Token Embedding**
  - **Why needed here:** The method operates exclusively on the [CLS] token representation from Transformer models. This single vector is used as the sentence-level summary for alignment.
  - **Quick check question:** In a BERT-like model, how is the [CLS] token's representation generated and what is its intended purpose during pre-training?

- **Concept: Zero-Shot Cross-Lingual Transfer**
  - **Why needed here:** The primary outcome is zero-shot transfer to non-English languages. This relies on the multilingual encoder's ability to represent different languages in a shared space.
  - **Quick check question:** Why does a model like XLM-R or mBERT, trained on multiple languages, often allow for zero-shot transfer from high-resource to low-resource languages?

## Architecture Onboarding

- **Component map:** Multilingual Text Encoder (Tm) -> Projection Network (F) -> Multimodal Encoder (Te); Image/Audio Encoder (Xe) provides modality-specific embeddings

- **Critical path:**
  1. Batch: A batch of English sentences `s` is prepared.
  2. Encoding: Each sentence `s` is passed through `Tm` to get `z_m` and through the text tower of `Me` to get `z_e`.
  3. Projection: The projection `F` maps `z_m` to the multimodal space: `z_m->e = F(z_m)`.
  4. Loss Calculation: The loss `L = λ * MSE(z_m->e, z_e) + β * MSE(triu(cos_sim(z_e)), triu(cos_sim(z_m->e)))` is computed.
  5. Update: Only the parameters of `F` are updated via backpropagation.

- **Design tradeoffs:**
  - MSE vs. Contrastive Loss: The paper uses MSE, arguing it encourages full embedding substitution, whereas contrastive loss focuses only on angular alignment.
  - Architecture: A simple 2-layer MLP without skip connections was found to be optimal, suggesting the mapping does not require complex residual pathways.
  - Normalization: For retrieval, embeddings are L2-normalized before loss computation. For generation, they are not, to preserve magnitude information.

- **Failure signatures:**
  - Poor Generalization: If the multilingual encoder `Tm` is not robustly cross-lingual, the projected embeddings for non-English languages will not align correctly.
  - Semantic Drift in Generation: Using the structure-preserving loss with normalized embeddings degraded generative performance, as it prioritized angular structure over magnitude.
  - Mode Collapse: If `λ` is too low, the model may fail to learn a strong enough alignment, getting stuck in a poor local minimum.

- **First 3 experiments:**
  1. Implement Core Loop: Build the training pipeline using only the MSE alignment loss on a small subset (e.g., 1K samples) and verify that the Recall@10 on an English validation set improves over a random baseline.
  2. Ablate Loss Terms: Run a controlled experiment comparing performance with (a) only alignment loss, (b) only structure loss, and (c) the combined loss as per the paper's optimal hyperparameters (`λ=48, β=1`).
  3. Zero-Shot Test: After training on English-only data, evaluate the model on a full multilingual test set (e.g., XTD) to confirm the primary claim of zero-shot cross-lingual transfer.

## Open Questions the Paper Calls Out

### Open Question 1
Can M2M be extended to token-level alignment that preserves fine-grained semantic structure for Multimodal Large Language Models? The paper states its method focuses on aligning global, sentence-level representations and does not provide alignment at the token level, which is necessary for MLLMs where effective generation relies on fine-grained representations.

### Open Question 2
Does M2M transfer effectively to joint cross-modal encoder architectures where representations are generated through shared architectural components? The paper notes the effectiveness of their method for joint cross-modal representations remains to be explored, as M2M was only validated on dual-encoder models.

### Open Question 3
To what extent do English multimodal model biases propagate through M2M's anchor-based alignment into multilingual outputs? The paper acknowledges that since English is used as the anchor, biases present in English multimodal models may manifest in the resulting multilingual multimodal model, though no bias evaluation was conducted.

## Limitations
- Limited Modality Coverage: Only tested on image and audio alignment, with a single generative model (FLUX.1-dev) for text-to-image generation
- Evaluation Language Coverage: Primary evaluation uses 11 languages in XTD but only 4 in Multi30K and 33 in XM3600, with incomplete characterization across language families
- Dataset Composition: Training data composition (exact ratios from GCC, MSCOCO, VizWiz) is not specified, making it difficult to assess dataset balance effects

## Confidence

**High Confidence**: The core zero-shot transfer claim (89.5% Recall@10 averaged across 11 languages) is well-supported by experimental results with clearly described methodology.

**Medium Confidence**: Generalization claims to audio-text retrieval and text-to-image generation are supported by experiments but with fewer evaluation languages; architectural claims about linear projection are theoretically sound but could benefit from more extensive ablation studies.

**Low Confidence**: Weight analysis showing geometric reshaping is based on a single metric (orthogonality deviation) without additional geometric analyses or visualizations.

## Next Checks

1. **Language Family Analysis**: Evaluate M2M's performance across language families (e.g., Germanic, Romance, Slavic, East Asian) to identify systematic biases toward particular language structures.

2. **Generation Quality Benchmark**: Test M2M with multiple generative models beyond FLUX.1-dev, including models optimized for different modalities (text-to-audio, text-to-video), to validate generalization across generative tasks.

3. **Weight Matrix Decomposition**: Perform singular value decomposition on the learned projection matrix and analyze the top-k components to provide deeper insight into whether the transformation truly performs geometric reshaping versus simple rotation.