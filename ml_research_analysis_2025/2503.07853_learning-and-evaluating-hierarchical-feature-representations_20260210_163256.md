---
ver: rpa2
title: Learning and Evaluating Hierarchical Feature Representations
arxiv_id: '2503.07853'
source_url: https://arxiv.org/abs/2503.07853
tags:
- hierarchical
- classes
- feature
- class
- hops
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Hier-COS, a framework for learning hierarchy-aware
  feature representations in deep neural networks. The core idea is to map feature
  embeddings into a vector space structured according to a given taxonomy tree, using
  orthogonal subspaces to encode semantic relationships between classes.
---

# Learning and Evaluating Hierarchical Feature Representations

## Quick Facts
- arXiv ID: 2503.07853
- Source URL: https://arxiv.org/abs/2503.07853
- Reference count: 40
- This paper proposes Hier-COS, a framework for learning hierarchy-aware feature representations in deep neural networks.

## Executive Summary
This paper introduces Hier-COS, a framework that learns feature representations structured according to a given taxonomy tree. The core innovation is mapping class embeddings into orthogonal subspaces where semantic similarity correlates with subspace overlap. The method uses a learned transformation module to project features onto a fixed orthogonal frame, with subspace dimensionality proportional to the number of ancestors and descendants. A new evaluation metric, HOPS, is proposed to better capture hierarchical performance by penalizing violations of hierarchical ordering.

## Method Summary
Hier-COS learns hierarchy-aware feature representations by mapping classes to orthogonal subspaces that share dimensions proportional to their common ancestors. Each node in the taxonomy tree receives a unique orthonormal basis vector, and a class's subspace is defined as the span of basis vectors for itself, all ancestors, and all descendants. A 5-layer transformation network projects backbone features to an n-dimensional space aligned with this orthonormal frame, using KL divergence loss to enforce target distribution across basis vectors and L1 regularization to promote sparsity. The approach is validated on both CNN and Vision Transformer backbones across datasets with label hierarchies ranging from 3 to 12 levels.

## Key Results
- Hier-COS achieves state-of-the-art hierarchical performance on datasets with deep label hierarchies (3-12 levels)
- The method maintains or improves top-1 accuracy while significantly outperforming existing methods like HAFrame and Flamingo variants
- The proposed HOPS metric reveals when models truly capture hierarchy vs. achieving results through lucky averaging
- Validation shows the approach works effectively on both CNN and Vision Transformer backbones

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Subspace Composition via Orthogonal Basis Assignment
Mapping classes to subspaces that share dimensions proportional to their common ancestors creates feature representations where semantic similarity correlates with subspace overlap. Each node in the taxonomy tree receives a unique orthonormal basis vector, and a class's subspace is defined as the span of basis vectors for itself, all ancestors (excluding root), and all descendants. Two classes with more shared ancestors have greater subspace intersection by construction.

### Mechanism 2: Sparse Projection onto Hierarchy-Aligned Subspaces
A learned transformation module can map arbitrary backbone features to the hierarchy-structured vector space while maintaining discriminative power. A 5-layer transformation network (Linear → BN → PReLU → Linear → BN → PReLU → Fixed Linear) projects backbone features to an n-dimensional space aligned with the orthonormal frame. The KL divergence loss enforces target distribution across basis vectors, and L1 regularization enforces sparsity.

### Mechanism 3: Preference-Ordered Evaluation via HOPS
A preference-based metric that penalizes violations of hierarchical ordering more than averaging-based metrics (MS, AHD) reveals when models truly capture hierarchy vs. lucky averaging. HOPS defines a desired preference ordering based on LCA distance, computes an obtained ordering from predictions, and measures discrepancy using exponentially decaying weights.

## Foundational Learning

- **Concept: Orthogonal Projections and Subspace Distance**
  - Why needed here: Understanding how distance from a point to a subspace is computed via orthogonal complement projection is essential for grasping how Hier-COS measures class membership.
  - Quick check question: If a feature vector x has components only along basis vectors {e1, e3, e5}, what is its distance to a subspace spanned by {e1, e2}?

- **Concept: KL Divergence for Distribution Matching**
  - Why needed here: The loss function uses KL divergence to match predicted magnitude distribution across basis vectors to a target hierarchical distribution.
  - Quick check question: Why use KL(P||P̂) instead of symmetric divergence? (Answer: Target P encodes the desired hierarchy structure and should drive the optimization.)

- **Concept: Lowest Common Ancestor (LCA) Distance**
  - Why needed here: All hierarchical metrics (MS, AHD, HOPS) are built on LCA distance as the measure of semantic dissimilarity.
  - Quick check question: In a balanced binary tree of height H, what is the maximum LCA distance between any two leaves? (Answer: H-1)

## Architecture Onboarding

- **Component map:** Backbone → Transformation Module → Orthogonal Frame → Inference
- **Critical path:** 1) Build basis assignment from taxonomy tree 2) For each class, precompute its basis set E_yi = E^a_yi ∪ {e_yi} ∪ E^d_yi 3) Forward pass: backbone → transformation → projection norms → prediction
- **Design tradeoffs:** Higher n increases dimensionality but captures finer hierarchy; weight function w_l = exp(1/(h+1-l)) emphasizes leaf vs. ancestor nodes; α controls sparsity tradeoff
- **Failure signatures:** Top-1 accuracy drops significantly (backbone capacity mismatch or over-regularization); HOPS high but accuracy low (hierarchy captured but lacks discriminative power); HOPS@k degrades sharply with k (transformation not enforcing proper ordering); ViT fine-tuning fails (try unfreezing last transformer blocks or increasing α)
- **First 3 experiments:** 1) Train Hier-COS on CIFAR-100 with Vi = span({e_i}) only (no ancestor/descendant dimensions) 2) Compare ResNet-50 vs. frozen ViT-MAE backbone on iNaturalist-19 3) Compute AHD@20 and HOPS@20 for predictions with randomly permuted top-5 ordering

## Open Questions the Paper Calls Out

### Open Question 1
Can the Hier-COS framework be extended to semantic relationships represented as Directed Acyclic Graphs (DAGs) rather than strict trees? The current formulation relies on tree paths for defining orthogonal subspaces and unique Lowest Common Ancestors (LCAs) for metrics. DAGs introduce multiple inheritance paths, making the orthogonal subspace composition and distance calculation ambiguous.

### Open Question 2
Does modifying the loss function to account for subtree height imbalance improve top-1 accuracy on datasets with variable leaf depths? The current KL-divergence loss uses a generic exponential weighting that may not adequately normalize for classes residing at vastly different depths within the same hierarchy.

### Open Question 3
Can kernel tricks be designed to implicitly map features into the hierarchy-aware space VT to improve scalability? The explicit projection currently scales with the number of nodes (n), creating computational overhead for very large taxonomies.

## Limitations
- Performance depends critically on the quality and depth of the taxonomy tree
- The orthogonal frame construction assumes a tree structure without cross-links
- The exponential weight function is fixed and may not be optimal for all dataset depths
- Ablation studies are limited in scope and don't explore alternative weight functions

## Confidence
- **High confidence:** The core mathematical framework of orthogonal subspace assignment and transformation module is sound and reproducible
- **Medium confidence:** Empirical improvements over baselines are significant but may depend on specific hyperparameter choices
- **Medium confidence:** The claim that Hier-COS works across both CNN and Vision Transformer backbones is supported but may not generalize across all datasets

## Next Checks
1. Systematically vary α (L_reg coefficient) across {0.01, 0.1, 1.0} and document the tradeoff between accuracy and HOPS performance
2. Test Hier-COS on a synthetic dataset where the taxonomy tree is progressively simplified (remove intermediate nodes, flatten hierarchy)
3. Generate predictions with deliberately permuted top-k ordering (reverse perfect ordering) and compute all metrics to confirm only HOPS correctly distinguishes poor ordering