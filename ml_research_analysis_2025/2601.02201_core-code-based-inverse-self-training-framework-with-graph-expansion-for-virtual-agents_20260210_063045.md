---
ver: rpa2
title: 'CORE: Code-based Inverse Self-Training Framework with Graph Expansion for
  Virtual Agents'
arxiv_id: '2601.02201'
source_url: https://arxiv.org/abs/2601.02201
tags:
- task
- trajectory
- graph
- label
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CORE is a Code-based Inverse Self-Training Framework with Graph
  Expansion designed to bridge the gap between imitation and exploration for training
  multimodal virtual agents. It addresses the limitations of Behavior Cloning (low
  behavioral diversity) and Reinforcement Learning (reliance on manual reward design)
  by introducing Semantic Code Abstraction to automatically infer executable reward
  functions (Label Functions) from expert demonstrations, Strategy Graph Expansion
  to enhance in-domain behavioral diversity via multi-path exploration, and Trajectory-Guided
  Extrapolation to enrich out-of-domain diversity by leveraging both successful and
  failed trajectories.
---

# CORE: Code-based Inverse Self-Training Framework with Graph Expansion for Virtual Agents

## Quick Facts
- arXiv ID: 2601.02201
- Source URL: https://arxiv.org/abs/2601.02201
- Authors: Keyu Wang; Bingchen Miao; Wendong Bu; Yu Wu; Juncheng Li; Shengyu Zhang; Wenqiao Zhang; Siliang Tang; Jun Xiao; Yueting Zhuang
- Reference count: 40
- Key outcome: CORE significantly improves overall performance and generalization in training multimodal virtual agents, outperforming Behavior Cloning, Reinforcement Learning, and prior self-training methods on VisualWebArena and AndroidWorld platforms.

## Executive Summary
CORE addresses the fundamental limitations of Behavior Cloning (low behavioral diversity) and Reinforcement Learning (manual reward design) for training multimodal virtual agents. The framework introduces a novel approach that automatically infers executable reward functions from expert demonstrations, uses a Strategy Graph to capture multiple valid solution paths, and leverages both successful and failed trajectories to enhance behavioral diversity. Extensive experiments demonstrate that CORE achieves steady improvements across self-training iterations, with significant gains in generalization scores—from 6.93 to 11.31 on VisualWebArena and from 2.86 to 14.29 on AndroidWorld—while maintaining robust performance on in-domain tasks.

## Method Summary
CORE is a code-based inverse self-training framework that bridges imitation learning and exploration for multimodal virtual agents. The method takes expert demonstrations as input and automatically derives executable reward functions (Label Functions) through Semantic Code Abstraction, eliminating the need for manual reward engineering. These Label Functions are organized into a Strategy Graph that captures multiple valid solution paths, enabling the framework to accept diverse trajectories beyond single expert sequences. The framework then samples new trajectories with stochasticity, categorizes them via graph scoring, and expands the graph by adding validated new paths from successful Partially Passed trajectories. Additionally, CORE employs Trajectory-Guided Extrapolation to convert failed trajectories into useful training data by inferring alternative task intents they actually satisfy. The complete pipeline iterates through sampling, categorization, graph expansion, task augmentation, and policy fine-tuning, achieving significant improvements in both overall performance and generalization across multiple self-training iterations.

## Key Results
- CORE achieves steady improvements across three self-training iterations on both VisualWebArena and AndroidWorld platforms
- Generalization scores improve significantly: from 6.93 to 11.31 on VisualWebArena and from 2.86 to 14.29 on AndroidWorld
- CORE outperforms Behavior Cloning, Reinforcement Learning, and prior self-training methods in both overall task success rates and out-of-distribution generalization

## Why This Works (Mechanism)

### Mechanism 1: Semantic Code Abstraction for Automatic Reward Inference
The framework automatically derives executable reward functions from expert demonstrations through a three-stage pipeline: semantic description extraction, key step identification, and Python Label Function synthesis. Each Label Function acts as a binary classifier that verifies one key step using predefined APIs. This eliminates manual reward engineering while providing precise, verifiable reward signals. The core assumption is that expert demonstrations contain identifiable key steps that map deterministically to verifiable code conditions. If key step identification accuracy drops below ~0.8 or Label Function synthesis OSR falls significantly, the reward signal becomes unreliable.

### Mechanism 2: Strategy Graph for Multi-Path Behavioral Diversity
A directed acyclic graph of Label Functions captures multiple valid solution paths, enabling acceptance of diverse trajectories beyond single expert sequences. The graph is initialized as a linear path from expert demonstrations and expanded iteratively by adding new paths from successful Partially Passed trajectories. The core assumption is that Partially passed trajectories that succeed on environment feedback represent valid alternative strategies, not noise. If environment feedback is sparse or noisy, graph expansion stalls; if path count explodes without quality filtering, training signal dilutes.

### Mechanism 3: Trajectory Recycling via Intent Inference
Failed trajectories are converted to useful training data by inferring alternative task intents they actually satisfy. The framework uses an LLM to infer plausible new intents from failed trajectories, refines them for quality, and adds the (trajectory, new_intent) pairs to training data. The core assumption is that failed trajectories encode meaningful behaviors that are simply misaligned with their original task labels. If intent refinement produces "INVALID" labels >30% of the time, or if refined intents are semantically incoherent, the mechanism adds noise rather than signal.

## Foundational Learning

- **Inverse Reinforcement Learning (IRL) basics**: Why needed - CORE is fundamentally an IRL approach inferring rewards from demonstrations. Quick check: Can you explain why sparse rewards make policy optimization difficult?
- **Directed Acyclic Graphs (DAGs) and path enumeration**: Why needed - Strategy Graph is a DAG; understanding path scoring and expansion is essential. Quick check: Given a DAG with 3 parallel paths of lengths 2, 3, and 4, what is the maximum score a trajectory can achieve?
- **LLM code generation failure modes**: Why needed - Label Functions are synthesized code; hallucinated APIs or syntax errors break the pipeline. Quick check: What happens when Qwen2.5-Coder generates a function calling an undefined API?

## Architecture Onboarding

- **Component map**: Expert demos → Semantic Code Abstraction (D → K → S) → Label Functions → Strategy Graph (DAG) → Sample trajectories (temp=1.0, top_p=0.9, top_k=50) → Categorizer (Fully/Partially/Failed) → Graph Expander → Task Pool (Success-Based Augmentation + Failure-Driven Exploration) → Aggregate data → Fine-tune → Repeat
- **Critical path**: Expert demonstrations → Label Functions → Strategy Graph → Sample trajectories → Categorize → Graph/Task expansion → Aggregate data → Fine-tune → Repeat
- **Design tradeoffs**: Sampling temperature: Higher promotes exploration but increases noise in categorization; Key step granularity: Too few steps = weak supervision; too many = over-constrained graphs; Intent refinement iterations: More refinement improves quality but increases latency; Code generation model size: 3B used; larger models may improve OSR but increase cost
- **Failure signatures**: Label Function OSR < 0.80: Synthesis pipeline degraded; Path count growth stalls: Graph expansion not finding new strategies; Performance plateaus after Iteration 2: Diminishing returns (observed in Figure 5); "INVALID" intent rate > 30%: Failure-driven exploration adding noise
- **First 3 experiments**: 1) Label Function validation: Run synthesis on 50 expert trajectories; measure OSR, FTSR, ESP; manually inspect 10 generated functions for API correctness. 2) Graph expansion visualization: After 1 iteration, render Strategy Graph; count paths and verify new paths correspond to environment-validated trajectories. 3) TGE ablation: Run CORE with and without Failure-Driven Task Exploration; compare generalization scores on test split to quantify intent inference contribution.

## Open Questions the Paper Calls Out

### Open Question 1
How sensitive is the framework to logical errors or hallucinations in the synthesized Label Functions? The framework relies on a code generation model (Qwen2.5-Coder-3B) to infer executable reward functions, but the paper does not analyze how incorrect reward code impacts policy learning. An ablation study introducing synthetic errors into Label Functions would measure degradation in agent performance and convergence stability.

### Open Question 2
Does Failure-Driven Task Exploration introduce harmful noise when inferring intents for ambiguous trajectories? The paper relies on the model's reasoning to align failed trajectories with "plausible" goals without external ground-truth verification. A human evaluation of the "alignment quality" of the generated intents versus the agent's actual behavior would assess whether the mechanism adds signal or noise.

### Open Question 3
Does the Strategy Graph face combinatorial explosion in long-horizon tasks? The paper evaluates on specific benchmarks but does not discuss the memory or computational overhead of maintaining and querying a continuously expanding graph for tasks with thousands of steps. Scaling analysis of graph size and inference latency as task horizons increase would reveal practical limitations.

## Limitations
- The framework heavily relies on LLM performance for key step identification and code synthesis, with no ablation studies showing how errors in these stages propagate through training
- The Strategy Graph expansion assumes Partially Passed trajectories represent valid alternative strategies without establishing false positive rates for this categorization
- The Trajectory-Guided Extrapolation mechanism adds significant complexity but lacks sufficient validation of whether inferred intents are semantically meaningful or merely plausible noise

## Confidence

- **High Confidence**: The overall iterative self-training framework design and core empirical results showing performance improvements across iterations on both VisualWebArena and AndroidWorld
- **Medium Confidence**: The effectiveness of individual mechanisms (Semantic Code Abstraction, Strategy Graph Expansion, Trajectory-Guided Extrapolation) due to limited ablation studies and error analysis
- **Low Confidence**: The generalizability of the approach beyond GUI interaction domains given heavy reliance on environment-specific APIs and lack of testing on diverse task domains

## Next Checks

1. **Error Propagation Analysis**: Measure how errors in Label Function synthesis (syntax errors, incorrect API calls) affect downstream policy performance, and establish acceptable thresholds for synthesis quality

2. **Graph Expansion Validation**: After each iteration, visualize the Strategy Graph and compute the semantic similarity between expanded paths to verify they represent genuinely distinct strategies rather than minor variations of the same approach

3. **Intent Inference Quality**: Implement a human evaluation study where domain experts assess the semantic validity of inferred intents from failed trajectories, measuring precision and recall of the intent refinement mechanism