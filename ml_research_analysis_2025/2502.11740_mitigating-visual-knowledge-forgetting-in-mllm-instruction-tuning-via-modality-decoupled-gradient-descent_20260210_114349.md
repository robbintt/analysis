---
ver: rpa2
title: Mitigating Visual Knowledge Forgetting in MLLM Instruction-tuning via Modality-decoupled
  Gradient Descent
arxiv_id: '2502.11740'
source_url: https://arxiv.org/abs/2502.11740
tags:
- visual
- arxiv
- mdgd
- forgetting
- pre-trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses visual knowledge forgetting in multimodal large
  language models (MLLMs) during instruction tuning. While MLLMs are pre-trained with
  rich visual-text alignment, instruction tuning is typically text-driven, leading
  to degradation of pre-trained visual understanding.
---

# Mitigating Visual Knowledge Forgetting in MLLM Instruction-tuning via Modality-decoupled Gradient Descent

## Quick Facts
- arXiv ID: 2502.11740
- Source URL: https://arxiv.org/abs/2502.11740
- Reference count: 28
- Visual knowledge forgetting during MLLM instruction tuning is quantified via effective rank and mitigated using gradient decoupling

## Executive Summary
This paper addresses the problem of visual knowledge forgetting in multimodal large language models (MLLMs) during instruction tuning. While MLLMs are pre-trained with rich visual-text alignment, instruction tuning is typically text-driven, leading to degradation of pre-trained visual understanding. The authors introduce a novel perspective using effective rank to quantify the loss of visual representation richness and frame this as excessive compression under the information bottleneck principle. They propose Modality-decoupled Gradient Descent (MDGD), which disentangles visual learning from task-specific alignment by regulating gradient updates to preserve the effective rank of visual representations. Extensive experiments across various downstream tasks and backbone MLLMs demonstrate that MDGD effectively mitigates visual forgetting while maintaining strong task adaptation.

## Method Summary
The authors introduce Modality-decoupled Gradient Descent (MDGD), a method that preserves visual knowledge during instruction tuning by regulating gradient updates to maintain the effective rank of visual representations. MDGD works by orthogonally projecting task-specific gradients away from the direction of visual representation changes, ensuring that the pretrained visual features remain intact while still allowing the model to learn task-specific alignment. They also propose a memory-efficient variant using gradient masking for parameter-efficient fine-tuning. The method is evaluated across multiple MLLM architectures (LLaVA-1.5, MiniCPM) and demonstrates superior performance in mitigating visual forgetting compared to existing approaches like LoRA and Model Tailor.

## Key Results
- MDGD effectively preserves visual knowledge during instruction tuning as measured by maintained effective rank of visual representations
- Outperforms existing methods including LoRA and Model Tailor on both LLaVA-1.5 and MiniCPM models
- Achieves comparable or better performance on pre-trained tasks while maintaining strong task adaptation
- Gradient masking variant provides memory-efficient alternative without significant performance degradation

## Why This Works (Mechanism)
MDGD works by recognizing that instruction tuning causes excessive compression of visual representations through the information bottleneck principle. By preserving the effective rank of visual representations during fine-tuning, the method maintains the richness of pre-trained visual knowledge. The gradient decoupling mechanism ensures that updates for visual features remain orthogonal to task-specific alignment gradients, preventing the degradation of visual understanding while still allowing the model to adapt to new tasks.

## Foundational Learning
- Effective rank analysis: Quantifies the richness/dimensionality of visual representations; needed to measure visual knowledge preservation, quick check is computing effective rank before/after fine-tuning
- Information bottleneck principle: Explains why visual knowledge is lost during text-driven instruction tuning; needed to frame the forgetting problem, quick check is analyzing compression ratios
- Gradient decoupling: Technique to separate visual learning from task alignment; needed to implement MDGD, quick check is verifying orthogonal gradient projections
- KL divergence approximation: Used to measure distribution shifts in visual representations; needed for regularization, quick check is comparing L1 vs KL gradient directions
- Multimodal representation learning: Understanding how visual and text modalities interact in MLLMs; needed to design gradient decoupling, quick check is analyzing cross-modal attention patterns

## Architecture Onboarding

Component map:
Pretrained MLLM -> MDGD regularization -> Instruction-tuned MLLM

Critical path:
Visual encoder outputs -> Effective rank computation -> Gradient decoupling -> Parameter updates

Design tradeoffs:
- Preserving visual knowledge vs. task adaptation capability
- Computational overhead of gradient regularization vs. performance gains
- Memory efficiency of gradient masking vs. full gradient computation

Failure signatures:
- Visual representation effective rank decreases significantly during fine-tuning
- Performance degradation on pre-trained visual tasks
- Overfitting to text instructions at expense of visual understanding

First experiments:
1. Compute effective rank of visual representations before and after instruction tuning with/without MDGD
2. Ablation study removing gradient decoupling to measure its contribution
3. Compare visual task performance across different gradient masking ratios α

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can modality-decoupled gradient regularization be extended to settings with more than two input modalities (e.g., audio, video, or depth alongside visual and textual inputs)?
- Basis in paper: The limitations section states: "extending this to more diverse, free-form multimodal inputs remains an avenue for future research."
- Why unresolved: The current formulation and gradient decoupling mechanism are designed specifically for visual-textual pairs, and it is unclear whether the same orthogonal projection approach would generalize to higher-dimensional modality interactions.
- What evidence would resolve it: Experiments applying MDGD to tri-modal or multi-modal models (e.g., audio-visual-text models) with analysis of whether effective rank preservation holds across all modalities.

### Open Question 2
- Question: What is the theoretical relationship between the L1 approximation loss in Eq. (5) and the true KL divergence it approximates, and how does approximation error affect visual knowledge preservation?
- Basis in paper: The paper uses an L1 loss to approximate KL divergence for practical tractability, but does not quantify the approximation quality or its impact on the regularization's effectiveness.
- Why unresolved: The quality of this approximation could influence how well the method actually constrains visual representations to match the pretrained distribution, potentially introducing unexpected biases.
- What evidence would resolve it: A theoretical analysis comparing L1 and KL gradient directions, or empirical ablations substituting exact KL estimates (where computationally feasible) to measure performance differences.

### Open Question 3
- Question: Why does MDGD's effectiveness vary across model architectures, with stronger preservation on LLaVA than MiniCPM?
- Basis in paper: The results show MDGD achieves "comparable or even better effective ranks on pre-trained tasks" for LLaVA (Figure 4a) but still suffers visual representation degradation on MiniCPM (Figure 4b), attributed to MiniCPM's "more compact and constrained visual representation space."
- Why unresolved: The interaction between pretrained representation geometry and gradient regularization is not characterized, making it difficult to predict a priori which models will benefit most.
- What evidence would resolve it: Studies correlating pretrained visual representation properties (e.g., intrinsic dimensionality, effective rank distribution) with MDGD performance gains across diverse backbone architectures.

### Open Question 4
- Question: How should the gradient masking ratio α be optimally selected for MDGD-GM across different tasks and model scales?
- Basis in paper: The gradient masking threshold Tα is "determined by a percentile α of trainable parameters," but α is treated as a hyperparameter without principled guidance on selection.
- Why unresolved: The sensitivity study (Figure 3) shows α affects alignment and representation loss, but no systematic method for choosing α is proposed, potentially requiring costly tuning.
- What evidence would resolve it: Development of adaptive or task-aware α selection strategies, or theoretical bounds linking α to task similarity and model capacity.

## Limitations
- Analysis primarily focused on image-related downstream tasks, with limited exploration of other modalities
- Computational overhead and training time implications of gradient regularization not thoroughly discussed
- Assumes effective rank preservation directly correlates with visual understanding without explicit qualitative validation
- Performance varies across model architectures, with less effectiveness on models with compact visual representations

## Confidence
High: Method is theoretically grounded, experimental results are consistent and show clear improvements over baselines, multiple architectures tested

## Next Checks
1. Conduct ablation studies specifically isolating the contribution of effective rank preservation versus other components of the MDGD framework
2. Evaluate the method's performance on a broader range of task types beyond image understanding, including audio, video, or cross-modal reasoning tasks
3. Perform in-depth qualitative analysis of the visual representations before and after MDGD training to verify that preserved effective rank translates to meaningful visual knowledge retention