---
ver: rpa2
title: 'Progressive Supervision via Label Decomposition: An Long-Term and Large-Scale
  Wireless Traffic Forecasting Method'
arxiv_id: '2501.06255'
source_url: https://arxiv.org/abs/2501.06255
tags:
- forecasting
- psld
- series
- prediction
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of Long-term and Large-scale Wireless
  Traffic Forecasting (LL-WTF), which is crucial for strategic network management
  and planning but poses challenges due to non-stationary traffic patterns and vast
  numbers of network nodes. The authors propose a Progressive Supervision method based
  on Label Decomposition (PSLD) to address these challenges.
---

# Progressive Supervision via Label Decomposition: An Long-Term and Large-Scale Wireless Traffic Forecasting Method

## Quick Facts
- arXiv ID: 2501.06255
- Source URL: https://arxiv.org/abs/2501.06255
- Reference count: 40
- Key outcome: PSLD outperforms SOTA methods with average performance improvements of 2%, 4%, and 11% on three large-scale wireless traffic datasets while maintaining competitive processing speed.

## Executive Summary
This paper addresses the challenging problem of Long-term and Large-scale Wireless Traffic Forecasting (LL-WTF), which is critical for strategic network management but difficult due to non-stationary traffic patterns and vast numbers of network nodes. The authors propose a Progressive Supervision method based on Label Decomposition (PSLD) that uses Random Subgraph Sampling (RSS) to handle large-scale data efficiently and employs label decomposition to break down time series into easier-to-learn components. Experimental results on three large-scale wireless traffic datasets demonstrate significant performance improvements over existing state-of-the-art methods, achieving average improvements of 2%, 4%, and 11% on the three datasets respectively.

## Method Summary
PSLD addresses LL-WTF by combining two key innovations: label decomposition and progressive supervision. The method decomposes time series targets into statistical components (mean, variance, residual via MVD or trend, seasonal, residual via STL) and learns these components progressively at shallow layers before combining them at deeper layers. RSS randomly partitions large graphs into smaller, memory-manageable subgraphs for tractable GPU training while maintaining unbiased feature estimation. The architecture consists of learners and predictors for each component, with a combiner that integrates learned representations at deeper layers. The loss function combines component-level losses with final prediction loss, enabling better gradient flow and representation learning.

## Key Results
- PSLD achieves average performance improvements of 2%, 4%, and 11% over SOTA methods on three large-scale wireless traffic datasets
- The method maintains competitive processing speed (6.86s/epoch on 10,000-node graph with 24 subgraphs)
- Component combination significantly outperforms baseline models in ablation studies
- Random Subgraph Sampling enables efficient training on graphs with 10,000+ nodes within 32GB GPU memory

## Why This Works (Mechanism)

### Mechanism 1
Decomposing supervision signals into statistical components enables better learning of non-stationary patterns than direct prediction. PSLD decomposes targets via MVD into mean, variance, and residual, or via STL into trend, seasonal, and residual. Each component receives dedicated supervision at shallow layers, allowing the model to learn distinct statistical properties separately before combination at deeper layers. This works when decomposed components maintain learnable, stable input-output relationships.

### Mechanism 2
Progressive supervision through component-level losses improves gradient flow and representation learning compared to end-to-end prediction alone. The loss function combines component prediction losses with the final combined prediction loss. Shallow layers receive direct gradients from their respective component supervision, while the combinator integrates learned representations at deeper layers. This enables shallow layers to benefit from direct, simpler supervision signals.

### Mechanism 3
Random Subgraph Sampling provides an unbiased estimator of full-graph features while enabling tractable GPU training on city-scale networks. RSS randomly partitions the full graph into smaller subgraphs at each training iteration. Theorem 1 states sampled subgraphs are unbiased estimators of aggregate graph features, decomposing 10,000-20,000+ node graphs into memory-manageable batches. This works when random sampling preserves statistical properties of the full graph.

## Foundational Learning

- **Time Series Decomposition (STL/MVD)**: Understanding how trend, seasonal, and residual components behave is essential for selecting MVD vs STL and interpreting component predictions. *Quick check*: Given a wireless traffic series with daily peaks and weekly growth, which components would STL extract, and how would MVD represent these patterns differently?

- **Multi-Task Learning and Auxiliary Losses**: PSLD's component-level losses function as auxiliary tasks; the λ hyperparameter balances their contribution. Understanding gradient interactions between tasks is critical for debugging training. *Quick check*: If component losses decrease but combined loss increases during training, what does this suggest about the relationship between λ and model capacity?

- **Graph Sampling and Scalability**: RSS is the enabling mechanism for large-scale deployment; understanding graph sampling bias and coverage is necessary to diagnose why performance might vary across subgraph configurations. *Quick check*: Why might randomly sampling 100 nodes from a 10,000-node city graph fail to capture important spatial dependencies, even if statistically unbiased?

## Architecture Onboarding

- **Component map**: Input X → [Input Decomposer] → [Learners L_θ, L_φ, L_ψ] → [Predictors P_θ, P_φ, P_ψ] → Component predictions (M̂, V̂, R̂) → [Combinator C_ξ] → Ŷ (final prediction) ←←←←←←←←←←←←←←←←←←←←←←←←←←← Component losses (L_θ, L_φ, L_ψ) and Combined loss (L_ξ)

- **Critical path**: 
  1. Verify data format matches LL-WTF definition: X^t ∈ R^(L_in × D), Y^t ∈ R^(L_out × D)
  2. Choose decomposition method: MVD for faster iteration, STL when strong seasonality exists
  3. Configure RSS subgraph size based on GPU memory (start with 24 subgraphs per the paper)
  4. Set λ (default unspecified; start with λ=1.0 and tune)
  5. Monitor component-wise predictions during training to verify decomposition is learning meaningful patterns

- **Design tradeoffs**: 
  - MVD vs STL: MVD is computationally cheaper; STL captures explicit seasonality but requires kernel size selection
  - Separate vs merged learners: Merged architecture is faster but may reduce component specialization
  - Subgraph size: Smaller = faster, more iterations per epoch; larger = better graph context, higher memory
  - Prediction horizon: Performance degrades with longer L_out; consider horizon-specific models for critical applications

- **Failure signatures**: 
  - Component collapse: Mean or variance predictions become constant
  - RSS variance: Large MSE variance across different random seeds
  - Combinator dominance: L_cbn >> L_cpn early in training
  - Residual blowup: R component has much larger magnitude than M or V

- **First 3 experiments**:
  1. **Decomposition ablation**: Compare (X, Y) baseline vs (X+D, Y) vs (X, Y+D) vs (X+D, Y+C+D) following Fig. 5 to isolate label decomposition contribution
  2. **Subgraph size sensitivity**: Run RSS with sizes {12, 24, 32, 48} and measure both MSE and training time to find memory-optimal configuration for your hardware
  3. **Horizon scaling**: Test input-36-predict-{24, 36, 48, 72} to characterize degradation curve and determine maximum viable prediction horizon for your accuracy requirements

## Open Questions the Paper Calls Out

- **Can predicted variance quantify uncertainty and improve forecasting robustness?**: By predicting the variance, it is possible to quantify the uncertainty of predictions, leading to more robust and reliable forecasting. This remains for future work.

- **Does incorporating advanced non-linear architectures improve performance?**: Incorporating more advanced non-linear models like Transformers or LLMs to capture complexities of real-world interactions is a focus for future work.

- **How does RSS limit capturing long-range global spatial dependencies?**: While RSS is proven unbiased, the interaction between random sampling and the learner's ability to model complex, global topological structures compared to full-graph methods is not fully explored.

## Limitations

- Random Subgraph Sampling assumes uniform sampling suffices for unbiased representation, which may not hold for networks with strong spatial clustering or heterogenous activity patterns
- Decomposition-based supervision assumes components maintain stable input-output relationships, which may break down for highly non-stationary traffic with regime shifts
- Several key parameters (λ loss weight, STL kernel sizes) are unspecified, affecting reproducibility and optimal configuration

## Confidence

- **High confidence**: RSS algorithm effectiveness for scalability, component combination improving performance, overall framework outperforming baselines across three datasets
- **Medium confidence**: Optimal λ hyperparameter selection and its sensitivity to different traffic patterns, generalization to network topologies beyond urban wireless traffic
- **Low confidence**: Decomposition stability across radically different traffic regimes and the robustness of component predictions when input stationarity assumptions are violated

## Next Checks

1. **Decomposition stability test**: Apply PSLD to synthetic traffic data with controlled regime shifts to measure how component predictions degrade when non-stationarity exceeds decomposition capacity

2. **Sampling bias analysis**: Compare RSS performance against stratified sampling approaches on graphs with known spatial clustering to quantify potential coverage gaps

3. **Hyperparameter sensitivity sweep**: Systematically vary λ (0.1, 0.5, 1.0, 2.0, 5.0) and STL kernel sizes to map the performance landscape and identify optimal configurations for different traffic patterns