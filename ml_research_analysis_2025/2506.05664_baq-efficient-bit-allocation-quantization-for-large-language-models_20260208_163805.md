---
ver: rpa2
title: 'BAQ: Efficient Bit Allocation Quantization for Large Language Models'
arxiv_id: '2506.05664'
source_url: https://arxiv.org/abs/2506.05664
tags:
- quantization
- allocation
- loss
- gptq
- bitwidth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of post-training quantization for
  large language models, where most existing methods use uniform bitwidths across
  all weights despite their varying sensitivity to quantization noise. The authors
  propose BAQ, a sensitivity-aware bit allocation method that leverages Hessian-informed
  metrics to allocate precision adaptively across weights under a global bit budget.
---

# BAQ: Efficient Bit Allocation Quantization for Large Language Models

## Quick Facts
- arXiv ID: 2506.05664
- Source URL: https://arxiv.org/abs/2506.05664
- Authors: Chao Zhang; Li Wang; Samson Lasaulce; Merouane Debbah
- Reference count: 40
- Primary result: 56× lower perplexity at 2-bit quantization compared to GPTQ baseline

## Executive Summary
BAQ introduces a post-training quantization method that allocates bits to weights based on their sensitivity to quantization noise, rather than using uniform precision across all parameters. By formulating bit allocation as a convex optimization problem, BAQ derives a closed-form solution that satisfies an equal-loss principle, ensuring uniform contribution to total quantization error. Experiments on OPT models from 125M to 30B parameters show BAQ consistently outperforms GPTQ, achieving up to 56× lower perplexity at 2-bit quantization while maintaining minimal overhead.

## Method Summary
BAQ operates on pre-trained models using only calibration data, without requiring retraining. It computes a sensitivity coefficient for each weight column by combining the weight range with the inverse Hessian diagonal, then solves a relaxed convex optimization problem where optimal bitwidths grow logarithmically with sensitivity. The method integrates with existing quantization pipelines (specifically GPTQ) by assigning integer bitwidths to columns based on a reference loss target, achieving adaptive precision allocation under a global bit budget constraint.

## Key Results
- 56× lower perplexity at 2-bit quantization compared to GPTQ baseline
- Consistent performance improvement across OPT models from 125M to 30B parameters
- Seamless integration with existing quantization pipelines with minimal overhead (~0.004 bits per component)

## Why This Works (Mechanism)

### Mechanism 1: Hessian-Informed Sensitivity Equalization
BAQ computes sensitivity coefficients using Hessian-weighted metrics, allocating more bits to weights that are more sensitive to quantization noise. This approach minimizes layer-wise reconstruction loss better than uniform allocation by leveraging the relationship between weight perturbations and loss curvature.

### Mechanism 2: The Equal-Loss Principle
Under optimal bit budget allocation, BAQ ensures uniform contribution to total quantization loss across all weighted components. This is achieved by solving the Lagrangian of the convex optimization problem, enforcing that columns with higher sensitivity must be compensated with exponentially higher bitwidths.

### Mechanism 3: Structured Column-wise Allocation
BAQ allocates a single bitwidth to all weights within a column, balancing hardware efficiency with theoretical gains. This approach reduces metadata overhead while maintaining the benefits of sensitivity-aware allocation through column-level granularity.

## Foundational Learning

- **Post-Training Quantization (PTQ)**: BAQ is a PTQ method operating on pre-trained models without retraining. Understanding PTQ constraints is essential to grasp why BAQ optimizes for layer-wise reconstruction loss rather than task loss.

- **Hessian Matrix & Second-Order Optimization**: The paper relies on Hessian proxy to estimate weight importance. The Hessian determines how sensitive the output is to weight perturbations, making it crucial for sensitivity-aware allocation.

- **Convex Optimization (Water-filling)**: The bit allocation problem is formulated as minimizing a convex function under a linear budget constraint. This resembles classical water-filling in communication theory.

## Architecture Onboarding

- **Component map**: Input (Pre-trained Weight Matrix W, Calibration Data X) -> Hessian Estimator (Computes H_F ≈ 2 X_F X_F^T) -> Sensitivity Calculator (Computes C_j per column) -> Budget Solver (Adjusts L_ref to meet target bitwidth) -> Quantizer (Applies GPTQ with assigned R_j)

- **Critical path**: The accuracy of the Hessian Estimator is the bottleneck. If calibration data is not representative, the sensitivity map will be flawed, causing inefficient bit allocation.

- **Design tradeoffs**: BAQ chooses per-column allocation to reduce header overhead (0.004 bits/component) and complexity, versus per-weight allocation which would maximize accuracy but explode metadata size.

- **Failure signatures**: Homogeneous sensitivity (Ratio_C close to 1.0) provides little benefit over uniform GPTQ; budget collapse occurs when target bit budget is too low, causing catastrophic perplexity spikes.

- **First 3 experiments**:
  1. Run BAQ on OPT-125M and plot assigned bitwidth histogram to verify non-uniform distribution correlates with Hessian diagonal magnitude
  2. Calculate Ratio_C for different layers and confirm layers with lower Ratio_C show higher perplexity reduction using BAQ vs. Uniform
  3. Measure inference latency impact of decoding per-column bitwidth headers versus memory bandwidth savings from reduced model size

## Open Questions the Paper Calls Out

- Can weight transformations and bit allocation be jointly optimized to maximize compression gains, especially when transformations homogenize sensitivity?
- How does the high-resolution approximation for the loss function hold up theoretically and empirically when quantizing to sub-2-bit regimes?
- Can the BAQ framework be extended to weight-activation quantization or KV-cache compression without losing its closed-form solution and low overhead?

## Limitations

- Sensitivity Metric Validity: Reliance on Hessian proxy sensitivity as primary metric for bit allocation is not universally validated, particularly concerning for extremely low bitwidths
- Calibration Data Sensitivity: Hessian computation depends on representative calibration data, but paper doesn't explore sensitivity to calibration set size or distribution
- Hardware Overhead Uncertainty: Actual hardware implementation cost for decoding variable-length bitwidths during inference is not quantified

## Confidence

- **High Confidence**: Core mathematical framework (convex optimization, equal-loss principle, closed-form solution) is rigorously derived and internally consistent
- **Medium Confidence**: Empirical results showing perplexity improvements over GPTQ are compelling but based on limited model families and quantization backend
- **Low Confidence**: Claims about BAQ's performance when combined with other quantization techniques (QuIP) are based on limited ablation studies

## Next Checks

1. Run BAQ on a small model with intentionally corrupted calibration data to verify that Hessian sensitivity metric captures meaningful information about quantization robustness

2. Systematically vary global bit budget from 1.0 to 4.0 bits per weight and measure trade-off between average bitwidth and perplexity to identify break points

3. Implement prototype decoder handling per-column variable bitwidths on representative hardware to measure actual inference latency and memory bandwidth usage compared to uniform precision quantization