---
ver: rpa2
title: 'HAECcity: Open-Vocabulary Scene Understanding of City-Scale Point Clouds with
  Superpoint Graph Clustering'
arxiv_id: '2504.13590'
source_url: https://arxiv.org/abs/2504.13590
tags:
- scene
- which
- point
- labels
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HAECcity, a superpoint graph clustering approach
  for open-vocabulary 3D scene understanding at city scale. The method uses a novel
  mixture of experts graph transformer backbone and generates synthetic pseudo-labels
  without hand annotation by projecting synthetic camera views and clustering CLIP
  features.
---

# HAECcity: Open-Vocabulary Scene Understanding of City-Scale Point Clouds with Superpoint Graph Clustering

## Quick Facts
- arXiv ID: 2504.13590
- Source URL: https://arxiv.org/abs/2504.13590
- Reference count: 39
- Key outcome: Introduces a scalable, strictly 3D open-vocabulary panoptic segmentation method for city-scale point clouds using superpoint graph clustering and a MoE transformer backbone.

## Executive Summary
This paper presents HAECcity, a novel open-vocabulary 3D scene understanding system designed for city-scale point clouds. The method uses a hierarchical superpoint graph clustering approach combined with a mixture of experts (MoE) graph transformer to predict CLIP embeddings for each point, enabling zero-shot classification and instance segmentation. By generating synthetic pseudo-labels through camera projection and CLIP feature back-projection, HAECcity avoids hand annotation and scales efficiently to large urban environments. Experiments on ScanNet and SensatUrban demonstrate competitive performance with strong scalability advantages over projection-based methods.

## Method Summary
HAECcity is a superpoint graph-based approach for open-vocabulary 3D scene understanding that predicts CLIP embeddings rather than fixed class labels. It generates synthetic pseudo-labels by rendering camera views from the point cloud, extracting OpenSeg features, and back-projecting them to 3D points. These features are clustered into pseudo-classes and pseudo-instances using spherical k-means and adaptive DBSCAN. The HAEC backbone uses a MoE graph transformer built on Superpoint Transformer (SPT) blocks, with top-k expert gating for increased representational flexibility. The model is trained to predict CLIP vectors via cosine similarity and triplet loss, enabling zero-shot inference on unseen categories.

## Key Results
- Achieves competitive panoptic segmentation on ScanNet (PQ 58.7, mIoU 66.1) without hand-annotated labels
- Demonstrates scalability to city-scale scenes with 50.62 mIoU and 71.93 mAcc on SensatUrban
- First strictly 3D-based open-vocabulary panoptic segmentation system applied to city-scale point clouds
- Superpoint partitioning enables linear scaling of computational resources relative to scene size

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical superpoint partitioning enables linear scaling of computational resources relative to scene size, bypassing the quadratic memory costs of raw point processing. The system partitions raw points into geometrically coherent clusters ("superpoints") via energy minimization, then recursively groups these into coarser graphs. The Mixture of Experts (MoE) transformer processes these abstract nodes rather than individual points, drastically reducing the sequence length for attention mechanisms.

### Mechanism 2
Synthetic view projection bridges the domain gap between pure 3D geometry and 2D foundation model knowledge without requiring RGB sensor data. The model renders synthetic camera views from the point cloud, uses OpenSeg to extract dense CLIP features from these 2D images, and back-projects them onto the 3D points. This "distills" 2D semantic knowledge into 3D space, creating training targets (pseudo-labels) where human labels are absent.

### Mechanism 3
Graph-based mixture-of-experts (MoE) routing improves semantic vector prediction by specializing capacity without increasing inference latency. Instead of a dense feed-forward network, the model uses a top-k gating mechanism to route superpoint representations to specific "expert" graph attention modules. This allows the model to learn distinct representations for diverse semantic classes within a unified backbone.

## Foundational Learning

- **Concept: Superpoint Graphs**
  - Why needed here: You cannot understand the HAECcity backbone without grasping that it does not process "points" (x,y,z) but rather "nodes" (geometric primitives) in a graph. The adjacency matrix defines the scene topology.
  - Quick check question: Can you explain why processing a graph of superpoints is O(N) complexity while standard PointNet++ is often O(N^2) or O(N*K) with high memory?

- **Concept: Open-Vocabulary / CLIP Embeddings**
  - Why needed here: The output of this model is not a class index (0..19) but a 512-dimensional vector (or similar) in CLIP space. You must understand cosine similarity and how "semantic search" replaces "classification."
  - Quick check question: If the model outputs a vector $v$, how do you determine if it represents a "red car" versus a "blue bus" during inference?

- **Concept: Pseudo-Labeling / Self-Supervision**
  - Why needed here: The ground truth is not human-annotated; it is generated by the pipeline. Understanding that the upper bound of performance is capped by the quality of these synthetic labels is critical for debugging.
  - Quick check question: If the model fails to segment "cars," would you first check the neural network weights or the 2D-to-3D projection quality?

## Architecture Onboarding

- **Component map:** Synthetic Camera Caster -> OpenSeg (2D Feature Extractor) -> Depth Anything (Validator) -> Back-projection -> DBSCAN (Instance Clustering) -> HAEC Backbone -> Semantic Head (CLIP vector prediction) + Affinity Head (Instance edge prediction)

- **Critical path:** The **2D-to-3D Projection** is the most brittle component. If the synthetic cameras are occluded or the pose is invalid (e.g., inside a building), the resulting pseudo-labels are garbage. The model cannot recover from systematic projection errors.

- **Design tradeoffs:**
  - Speed vs. Granularity: Superpoints are fast but can blur fine boundaries (e.g., a thin pole merging with a building facade)
  - Generalization vs. Accuracy: Using OpenSeg features allows zero-shot learning but limits the model to the "knowledge" of that specific foundation model

- **Failure signatures:**
  - "Ghost" Objects: High CLIP similarity in empty space (likely due to VLM hallucinations in synthetic views)
  - Under-segmentation: Superpoints grouping distinct objects (failure of the partitioner, not the network)
  - Routing Collapse: Only 1-2 experts receiving >80% of the superpoints (check load-balancing loss)

- **First 3 experiments:**
  1. Visualize Pseudo-Labels: Before training the network, render the projected CLIP features back to the point cloud. Query "car" and "building" to see if the synthetic labels make sense.
  2. Overfit Single Scene: Train HAECcity on a small chunk (e.g., one building) to verify the MoE backbone can converge on the reconstruction loss without divergence.
  3. Ablation on MoE: Run inference with k=1 (single expert) vs. k=2 (standard) to measure the performance delta vs. speed trade-off on SensatUrban.

## Open Questions the Paper Calls Out

### Open Question 1
Can fine-tuning a vision-language model (VLM) to minimize the embedding distance between real and synthetic images significantly improve the quality of synthetic-image-derived CLIP features?
- Basis in paper: The authors identify that OpenSeg is likely unfamiliar with point-cloud-rendered synthetic images and explicitly propose fine-tuning a VLM as a future research direction.
- Why unresolved: The current implementation uses off-the-shelf OpenSeg, creating a domain gap where synthetic images yield lower quality features compared to real images.
- What evidence would resolve it: A comparative study showing improved mIoU or retrieval accuracy on SensatUrban when using a VLM fine-tuned on synthetic/real pairs versus the baseline.

### Open Question 2
Does projecting feature vectors at the first superpoint level, rather than the raw point cloud level, successfully reduce preprocessing computational complexity while maintaining label fidelity?
- Basis in paper: Section 5 notes that high memory consumption during projection limits the size of processable point clouds and suggests this architectural change as a solution.
- Why unresolved: The current pipeline projects features onto raw points, which is memory-intensive and forces subsampling or chunking of large scenes.
- What evidence would resolve it: Experiments demonstrating that superpoint-level projection lowers memory usage and runtime without a statistically significant drop in segmentation performance.

### Open Question 3
Can a HAEC-based model trained concurrently across multiple large-scale datasets (e.g., SensatUrban combined with others) function effectively as a generalized foundation model for diverse city environments?
- Basis in paper: The conclusion states a future goal is to train across multiple large-scale datasets to realize the potential of the approach as a "foundation model for city-scale point clouds."
- Why unresolved: Current experiments are restricted to single datasets (ScanNet, SensatUrban), leaving the model's ability to generalize across different urban contexts without hand-annotation unproven.
- What evidence would resolve it: Zero-shot evaluation results on a held-out city dataset after training the model on a merged corpus of urban point clouds.

## Limitations
- Reliance on OpenSeg CLIP embeddings from synthetic renderings creates a domain gap that limits feature quality
- Critical architectural hyperparameters (number of experts, gating mechanism specifics) are underspecified, making exact reproduction difficult
- Performance on complex urban scenes with fine-grained classes (e.g., distinguishing different vehicle types) remains unproven

## Confidence
- **High Confidence:** The scalability claim via superpoint partitioning is well-supported by the literature on superpoint graphs and the explicit computational complexity analysis
- **Medium Confidence:** The competitive ScanNet and SensatUrban results are verifiable, but the actual contribution of the MoE backbone vs. the quality of the pseudo-labels is unclear
- **Low Confidence:** The claim of being the "first strictly 3D-based open-vocabulary panoptic segmentation system" is difficult to verify without a comprehensive survey of all prior 3D methods

## Next Checks
1. **Pseudo-Label Quality Audit:** Generate pseudo-labels for a subset of ScanNet with ground truth available. Compute the semantic segmentation accuracy of these labels against human annotations to establish a baseline performance ceiling for HAECcity.

2. **MoE Ablation Study:** Train two versions of HAECcity on SensatUrban: one with k=1 (single expert, dense model) and one with k=2 (MoE). Measure the PQ and mIoU difference, and record inference latency to quantify the exact trade-off.

3. **Camera View Coverage Analysis:** For a challenging urban scene (e.g., a dense city block), visualize the coverage heatmap of synthetic camera views. Identify regions with poor coverage (e.g., occluded areas) and retrain the model with increased camera density to measure the impact on final segmentation quality.