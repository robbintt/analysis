---
ver: rpa2
title: 'ICX360: In-Context eXplainability 360 Toolkit'
arxiv_id: '2511.10879'
source_url: https://arxiv.org/abs/2511.10879
tags:
- icx360
- explanations
- methods
- input
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ICX360 provides a unified toolkit for explaining LLM outputs in
  terms of their inputs, focusing on in-context explainability. It includes three
  methods: MExGen (black-box/white-box input attribution via perturbations and gradients),
  CELL (black-box contrastive explanations), and Token Highlighter (white-box token-level
  importance via gradients).'
---

# ICX360: In-Context eXplainability 360 Toolkit

## Quick Facts
- arXiv ID: 2511.10879
- Source URL: https://arxiv.org/abs/2511.10879
- Reference count: 16
- Primary result: Provides unified toolkit for explaining LLM outputs in terms of their inputs with improved quality and efficiency

## Executive Summary
ICX360 is a comprehensive toolkit designed to address the critical need for interpretable explanations of Large Language Model (LLM) outputs. The toolkit focuses on in-context explainability, enabling users to understand why LLMs generate specific outputs based on their inputs. By integrating three distinct methods - MExGen for input attribution, CELL for contrastive explanations, and Token Highlighter for token-level importance - ICX360 offers a versatile solution for various explanation needs. The toolkit supports multi-level input granularity, output-sequence explanations, and integrates seamlessly with both Hugging Face and OpenAI APIs, making it suitable for high-stakes applications such as medical advice and business forecasting.

## Method Summary
ICX360 implements three core explanation methods: MExGen, CELL, and Token Highlighter. MExGen provides both black-box and white-box input attribution through perturbations and gradients, offering flexibility in how explanations are generated. CELL specializes in black-box contrastive explanations, highlighting differences between similar inputs and their outputs. Token Highlighter focuses on white-box token-level importance using gradients to identify influential tokens. The toolkit addresses the limitations of existing methods like SHAP and Captum by providing coherent, interpretable explanations at the context level rather than just token level. ICX360 includes utilities for infilling, scalarizing, segmenting, and evaluation, enhancing its practical utility for real-world applications.

## Key Results
- Provides unified toolkit for explaining LLM outputs in terms of their inputs
- Offers three methods: MExGen, CELL, and Token Highlighter for various explanation needs
- Supports multi-level input granularity and output-sequence explanations
- Integrates with Hugging Face and OpenAI APIs for broad compatibility
- Improves explanation quality and computational efficiency over existing tools

## Why This Works (Mechanism)
ICX360 works by providing multiple complementary explanation methods that address different aspects of LLM explainability. The MExGen method combines perturbation-based and gradient-based approaches to capture both local and global input attributions, while CELL's contrastive approach highlights decision boundaries. Token Highlighter leverages gradient information to identify token-level contributions. Together, these methods provide a comprehensive view of how inputs influence outputs. The toolkit's design allows for both black-box and white-box explanations, making it adaptable to different use cases and computational constraints. By focusing on context-level explanations rather than just token-level, ICX360 provides more coherent and interpretable results that better reflect how LLMs actually process information.

## Foundational Learning
- **Input Attribution**: Understanding how different input features contribute to model outputs - needed for identifying influential inputs, quick check: verify attribution scores sum to reasonable values
- **Contrastive Explanations**: Comparing similar inputs to highlight decision boundaries - needed for understanding model behavior differences, quick check: ensure explanations are consistent across similar examples
- **Gradient-based Methods**: Using derivatives to identify important features - needed for efficient white-box explanations, quick check: verify gradient magnitudes align with intuition
- **Perturbation Analysis**: Systematically modifying inputs to observe output changes - needed for black-box attribution, quick check: ensure perturbations don't introduce artifacts
- **Context vs Token Level**: Understanding higher-level input representations - needed for coherent explanations, quick check: verify context-level explanations make semantic sense

## Architecture Onboarding
- **Component Map**: ICX360 -> MExGen/CELL/Token Highlighter -> Hugging Face/OpenAI APIs -> User Applications
- **Critical Path**: User request -> Method selection -> Input processing -> Explanation generation -> Result visualization
- **Design Tradeoffs**: Black-box vs white-box methods (flexibility vs computational efficiency), context vs token level (coherence vs granularity), perturbation vs gradient (robustness vs speed)
- **Failure Signatures**: Poor explanations when inputs are ambiguous, gradient saturation in deep layers, perturbation artifacts from text modification
- **First Experiments**: 1) Compare MExGen attribution scores with human intuition on simple examples, 2) Test CELL's contrastive ability on near-duplicate inputs, 3) Validate Token Highlighter's token importance against manual inspection

## Open Questions the Paper Calls Out
None

## Limitations
- Limited empirical validation of claimed advantages over existing tools like SHAP and Captum
- Uncertainty about generalizability across diverse domains and tasks
- Comparative analysis with existing tools could be more comprehensive
- Lack of concrete examples for future extensibility implementations

## Confidence
- Claims about improved explanation quality and computational efficiency: Medium
- Generalizability across diverse domains and tasks: Low
- Extensibility and integration of future methods: Medium
- Uniqueness in addressing LLM explainability gaps: Low

## Next Checks
1. Conduct empirical comparisons of ICX360's methods against SHAP and Captum across multiple datasets and tasks
2. Test the toolkit's performance and interpretability in high-stakes applications like medical or financial domains
3. Develop and evaluate a prototype extension (e.g., attention-based explanations) to demonstrate extensibility capabilities