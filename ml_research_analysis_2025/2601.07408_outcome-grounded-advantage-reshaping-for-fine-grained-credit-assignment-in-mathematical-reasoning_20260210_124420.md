---
ver: rpa2
title: Outcome-Grounded Advantage Reshaping for Fine-Grained Credit Assignment in
  Mathematical Reasoning
arxiv_id: '2601.07408'
source_url: https://arxiv.org/abs/2601.07408
tags:
- grpo
- token
- tokens
- oar-p
- cookies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Outcome-Grounded Advantage Reshaping (OAR),
  a fine-grained credit assignment method for improving mathematical reasoning in
  language models. The key insight is that standard Group Relative Policy Optimization
  (GRPO) assigns the same advantage to all tokens in a sequence, ignoring the varying
  importance of individual reasoning steps.
---

# Outcome-Grounded Advantage Reshaping for Fine-Grained Credit Assignment in Mathematical Reasoning

## Quick Facts
- arXiv ID: 2601.07408
- Source URL: https://arxiv.org/abs/2601.07408
- Reference count: 16
- Primary result: OAR improves mathematical reasoning by redistributing token-level advantages based on outcome sensitivity, achieving 53.7% average accuracy on Qwen2.5-7B vs 51.3% for GRPO baseline

## Executive Summary
This paper introduces Outcome-Grounded Advantage Reshaping (OAR), a method that addresses the coarse-grained credit assignment problem in GRPO by redistributing advantages based on how much each token influences the final answer. The key insight is that standard GRPO assigns uniform advantages to all tokens in a sequence, ignoring the varying importance of individual reasoning steps. OAR implements this through two complementary strategies: OAR-P estimates outcome sensitivity through counterfactual token perturbations for high-fidelity attribution, while OAR-G uses an efficient gradient-based proxy that approximates influence with a single backward pass. Empirical results show consistent improvements across mathematical reasoning benchmarks, with OAR-P achieving the best performance while OAR-G retains most gains with minimal computational overhead.

## Method Summary
OAR reshapes token-level advantages in GRPO by estimating each token's influence on the final answer distribution. It uses either OAR-P (counterfactual masking with KL divergence) or OAR-G (gradient-based approximation) to compute importance scores, then applies bi-level gating to suppress low-impact tokens and boost pivotal ones while preserving total advantage mass. The method is integrated into GRPO's training loop, replacing uniform token advantages with outcome-grounded ones. Two outcome probes are used: LT-Logits for early training warmup and AS-Mean (answer span logits) for later training.

## Key Results
- OAR-P improves average accuracy from 51.3 to 53.7 on Qwen2.5-7B across benchmarks
- OAR-G achieves 92.4/78.7 on GSM8K/MATH500 vs 90.5/75.8 for GRPO, with 1.4x computational overhead vs 4.2x for OAR-P
- Bi-level gating with τ=0.4 and β=2.0 consistently outperforms boost-only or suppress-only variants
- OAR shows robustness across diverse mathematical reasoning tasks including AIME, AMC, and GSM8K

## Why This Works (Mechanism)

### Mechanism 1: Outcome Sensitivity as Credit Signal
Token-level importance scores derived from outcome sensitivity correlate with actual causal contribution to final answers. OAR-P masks each token position with [PAD], computes perturbed final prediction distribution, and measures KL divergence from unperturbed distribution. High KL divergence indicates logical pivot; near-zero indicates syntactic filler. The model's final-answer distribution serves as valid surrogate outcome when verifier rewards are sparse.

### Mechanism 2: Gradient×Input as Efficient Proxy
Gradient×Input provides computationally efficient first-order approximation of perturbation-based importance. OAR-G injects Gaussian noise into embeddings, computes self-distillation KL loss between clean and perturbed outcome distributions, then derives token importance via |⟨∇_et J, et⟩|. This replaces L counterfactual forward passes with single backward pass. First-order gradient information sufficiently captures outcome sensitivity for credit assignment.

### Mechanism 3: Bi-level Gating with Mass Preservation
Bi-level gating with sum-preserving renormalization concentrates credit on pivotal tokens while maintaining stable update scales. Tokens below threshold τ receive suppression weight; tokens above receive boosting weight. Weights are renormalized so total advantage mass equals T, preserving overall update scale. Balanced thresholding (τ=0.3-0.7) achieves best accuracy while avoiding collapse.

## Foundational Learning

- **GRPO (Group Relative Policy Optimization)**: Why needed - OAR is explicitly designed as modification to GRPO's credit assignment. Quick check - Can you explain why GRPO avoids training value function and how it computes advantages from group-level rewards?
- **Policy gradient variance and credit assignment**: Why needed - Paper's theoretical appendix shows broadcasting sequence-level advantages to all tokens injects aliased credit noise. Quick check - Why does reward aliasing harm learning more in long-horizon reasoning sequences?
- **Perturbation-based attribution**: Why needed - OAR-P directly applies counterfactual masking logic. Quick check - How does masking token differ from gradient-based sensitivity as attribution method?

## Architecture Onboarding

- **Component map**: GRPO baseline → Token sampling → Reward computation → OAR module → Importance scoring → Bi-level gating → Renormalization → Reshaped advantages → PPO-style clipped objective
- **Critical path**: Generate G completions per prompt → Compute importance scores via OAR-P or OAR-G → Apply bi-level gating with threshold τ and boost β → Renormalize weights → Update policy with reshaped token-level advantages
- **Design tradeoffs**: OAR-P vs OAR-G: Fidelity vs efficiency (4.2x vs 1.4x overhead). Threshold τ: Lower τ = more boosting, higher variance, faster learning but collapse risk. Higher τ = more conservative, slower but stable. Outcome definition: AS-Mean outperforms LT-Logits and AS-Joint in ablations.
- **Failure signatures**: Entropy collapse during training → τ too low. Slow/no learning → τ too high. Inconsistent attribution → answer span extraction failing. Gradient variance still high → renormalization not applied correctly.
- **First 3 experiments**: 1) Implement OAR-G on 1.5B model with τ=0.4, β=2.0 on GSM8K; verify training dynamics show concentrated credit. 2) Compare OAR-P vs OAR-G vs entropy-based weighting on Qwen2.5-7B; compute Oracle causal-token recall. 3) Vary τ ∈ {0.3, 0.5, 0.7} and β ∈ {1.0, 2.0, 4.0} on MATH500; identify stability frontier.

## Open Questions the Paper Calls Out

### Open Question 1: Generalization to Open-Ended Tasks
Can OAR effectively generalize to open-ended or interactive reasoning tasks where outcomes are not localized to single verifiable answer span? The current implementation relies on regex-extracted answer spans assuming rigid, verifiable structure unavailable in general reasoning. Applying OAR to open-ended domains using alternative outcome probes such as reward-model scores would resolve this.

### Open Question 2: Proxy-Criterion Misalignment
Does answer-distribution proxy induce misallocation of credit when model confidence is weakly coupled with verifier reward? OAR optimizes for shifts in model's own answer distribution rather than ground-truth reward, risking divergence if model is confidently incorrect. Correlation analysis comparing OAR importance scores against ground-truth process supervision would resolve this.

### Open Question 3: Distribution Shift in Masking
Does counterfactual masking strategy introduce out-of-distribution artifacts that distort token importance estimation? Replacing reasoning tokens with [PAD] creates artificial sequences potentially triggering erratic model behaviors. Ablation studies comparing [PAD] masking against in-distribution counterfactuals would resolve this.

## Limitations
- OAR relies on verifiable outcomes localized to answer spans, limiting applicability to open-ended reasoning tasks
- Proxy-based credit assignment may misallocate credit when model confidence diverges from ground-truth correctness
- Computational overhead of OAR-P (4.2x) may be prohibitive for very large models despite OAR-G efficiency

## Confidence

- **High confidence**: Core mechanism of redistributing advantages based on outcome sensitivity is well-justified theoretically and empirically validated with consistent improvements across benchmarks
- **Medium confidence**: OAR-G retaining ~80-90% of OAR-P gains with 3x speedup is supported by ablation but limited to 7B parameters without scaling analysis
- **Low confidence**: Bi-level gating with sum-preserving renormalization is essential for stability based on limited ablation (τ=0, τ=1, τ=0.4) without systematic testing across different task complexities

## Next Checks

1. **Scaling validation**: Test OAR-G on larger models (e.g., 34B parameters) across benchmark suite to verify 1.4x computational overhead and ~80% fidelity retention scale proportionally. Measure wall-clock training time and convergence curves.

2. **Attribution fidelity analysis**: Implement Oracle causal-token recall methodology on additional datasets (GSM8K, symbolic reasoning). Compare OAR-P, OAR-G, and baseline GRPO to quantify whether 2-3% improvement in causal alignment generalizes.

3. **Threshold robustness study**: Conduct systematic ablation of τ across {0.2, 0.3, 0.5, 0.7, 0.9} on Qwen2.5-7B for all five benchmarks. Plot reward curves, entropy trajectories, and ESS ratios to identify stability frontier.