---
ver: rpa2
title: '3CEL: A corpus of legal Spanish contract clauses'
arxiv_id: '2501.15990'
source_url: https://arxiv.org/abs/2501.15990
tags:
- legal
- contract
- spanish
- information
- annotation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents 3CEL, a corpus of 373 manually annotated Spanish
  legal contract clauses designed to support contract information extraction. Developed
  as part of the INESData 2024 project, 3CEL addresses the scarcity of high-quality
  legal NLP resources in Spanish by providing 4,782 labeled spans across 19 defined
  categories.
---

# 3CEL: A corpus of legal Spanish contract clauses

## Quick Facts
- **arXiv ID:** 2501.15990
- **Source URL:** https://arxiv.org/abs/2501.15990
- **Reference count:** 37
- **Primary result:** MEL achieved macro F1-scores of 0.72 (18 labels) and 0.84 (15 labels) on span categorization for Spanish legal contracts

## Executive Summary
This paper presents 3CEL, a manually annotated corpus of 373 Spanish public sector tender contracts with 4,782 labeled spans across 19 categories. Developed as part of the INESData 2024 project, 3CEL addresses the scarcity of high-quality legal NLP resources in Spanish by providing expert-validated annotations using a span categorization methodology. The corpus was evaluated by fine-tuning state-of-the-art models for contract information extraction, with MEL achieving the best performance. The work demonstrates the effectiveness of domain-adapted pre-training and strict inter-annotator agreement protocols for legal NLP tasks in Spanish.

## Method Summary
The 3CEL corpus was created by collecting 373 Spanish public sector tender contracts from the Madrid region (December 2021-December 2023), transcribing them with OCRmyPDF, and applying PII anonymization using NER models and regex patterns. Two annotators manually labeled spans using the MATTER methodology with strict inter-annotator agreement (0.61 IAA), followed by expert harmonization. The corpus was split 75/25 by tender and chunked to <512 tokens while preserving span boundaries. Four models (xlm-roberta-large, legal-xlm-roberta-large, RoBERTalex, MEL) were fine-tuned for span categorization using BIO tagging (36 labels) with macro F1-score as the primary metric.

## Key Results
- MEL achieved macro F1-scores of 0.72 (18 labels) and 0.84 (15 labels), outperforming other models
- Removing three underrepresented categories (canon, surety board, intellectual property) improved macro F1 by ~0.1
- RoBERTalex underperformed significantly (0.38-0.47 F1) compared to multilingual models
- Strict span categorization (vs. standard NER) enabled multi-sentence and multi-label annotations

## Why This Works (Mechanism)

### Mechanism 1
Domain-adapted pre-training on legal Spanish text improves span categorization performance and learning speed. MEL, trained on more legal-domain Spanish data, achieved the fastest learning and matched or outperformed the baseline. Pre-training on in-domain text likely provides better token representations for legal terminology and contract structure. Core assumption: performance difference stems from domain-specific pre-training rather than architectural differences. Break condition: if MEL's pre-training data significantly overlapped with 3CEL test set.

### Mechanism 2
Strict inter-annotator agreement with both span and category matching produces training data that generalizes despite class imbalance. The annotation protocol required matching both category and exact text span, combined with blind peer review and expert adjudication. Core assumption: IAA of 0.61 is sufficient because harmonization resolves discrepancies before training. Break condition: if harmonization introduced systematic biases favoring one annotator's interpretation.

### Mechanism 3
Removing underrepresented labels improves macro F1-score by reducing false positives on rare categories. The three least-represented categories each had fewer than 40 appearances. Macro F1-score weights all classes equally regardless of frequency. Core assumption: removed labels are not critical for the target use case. Break condition: if downstream applications require the removed labels, the reported F1 improvement is misleading.

## Foundational Learning

- **Concept: Span categorization vs. NER**
  - Why needed here: 3CEL uses span categorization, which differs from standard NER in three key ways: spans can be multiple sentences long, multiple labels can apply to one span, and the B/I tagging scheme doubles the classification labels (18 categories → 36 tags)
  - Quick check question: Given a contract clause that spans two sentences describing both the contract object AND its duration, how would span categorization handle this differently than NER?

- **Concept: Macro F1-score for imbalanced data**
  - Why needed here: 3CEL has significant class imbalance; macro F1 treats all classes equally, making it sensitive to rare categories that weighted metrics would ignore
  - Quick check question: If a model achieves 95% accuracy but fails completely on 3 rare but legally critical categories, would macro F1 capture this failure?

- **Concept: IAA with strict span matching**
  - Why needed here: The paper reports IAA of 0.61, but this uses the "strictest" standard—both annotators must select the same text span AND assign the same label
  - Quick check question: If Annotator A labels "Contract duration: 24 months" and Annotator B labels just "24 months" as the duration span, would strict matching count this as agreement?

## Architecture Onboarding

- **Component map:**
  Raw PDFs (Spanish tenders) -> OCRmyPDF transcription -> Raw text -> Filtering + Cleaning -> Clean text -> Anonymization -> Anonymized text -> Annotation (Prodigy, MATTER, 2 annotators, expert review) -> 3CEL corpus (373 docs, 4,782 spans, 19 categories) -> Chunking (<512 tokens, preserve span boundaries) -> Train/test chunks (1,706 / 604) -> Fine-tuning (xlm-roberta-large, MEL, etc.) -> Span categorization model

- **Critical path:** Annotation quality. The entire pipeline depends on consistent, expert-validated labels. The iterative guideline refinement and harmonization steps are non-optional—if skipped, models will learn inconsistent patterns.

- **Design tradeoffs:**
  - Corpus size vs. annotation quality: 373 documents is modest; authors prioritized expert-validated annotation over scale
  - Label granularity vs. model performance: 19 labels capture more information but the 3 rarest hurt macro F1; the 15-label variant performs ~0.1 better
  - Public sector only: Current corpus excludes private contracts, limiting generalizability to commercial legal NLP

- **Failure signatures:**
  - Transcription failures: 8.4% of PDFs rejected due to missing information (OCR quality varies)
  - Low-representation labels: Categories with <40 instances (canon, surety board, IP) underperform
  - RoBERTalex underperformance: Achieved only 0.38-0.47 F1, suggesting Spanish legal pre-training alone is insufficient without multilingual transfer

- **First 3 experiments:**
  1. Baseline replication: Fine-tune xlm-roberta-large on 3CEL using published hyperparameters to verify reported macro F1 of ~0.73 (18 labels)
  2. Label ablation: Compare performance with all 18 labels vs. 15 labels to confirm the ~0.1 improvement and identify which specific categories drive errors
  3. Cross-domain test: Evaluate the fine-tuned model on a held-out sample of private Spanish contracts (not in 3CEL) to assess generalization beyond public sector tenders

## Open Questions the Paper Calls Out

### Open Question 1
How does model performance on 3CEL transfer to private sector Spanish contracts, and what domain adaptation strategies are most effective for this cross-domain generalization? Basis: "Further work includes expanding 3CEL to include private sector contracts and explore modeling results with coming models and architectures." Unresolved because 3CEL contains only public sector tenders; evidence needed: fine-tuning and zero-shot evaluation results on private sector contracts.

### Open Question 2
What annotation guideline refinements or training strategies could improve performance on underrepresented categories (canon, surety board, intellectual property) that were excluded from the 15-label experiments? Basis: The paper excludes three categories with fewer than 40 appearances from 15-label experiments, noting a ~0.1 F1 improvement. Unresolved because imbalanced category distribution is inherent to real legal documents; evidence needed: comparative experiments using data augmentation, focal loss, or few-shot learning approaches on the full 18-label task.

### Open Question 3
How does geographic and institutional bias in the Madrid-region tender data affect model generalizability to contracts from other Spanish regions or international Spanish-language legal contexts? Basis: All 373 contracts are "executed in the region of Madrid" from a single platform; no cross-regional validation is conducted. Unresolved because regional administrative practices and terminology may vary; evidence needed: evaluation on contracts from other Spanish regions or Latin American countries with error analysis.

### Open Question 4
To what extent does the realistic-but-fake entity replacement anonymization strategy affect downstream model performance compared to original data or alternative approaches? Basis: The paper states anonymization "ensures consistency and coherence" and has "a positive impact" but provides no empirical comparison. Unresolved because anonymization may alter contextual cues or entity-frequency distributions; evidence needed: controlled experiments comparing anonymized vs. non-anonymized versions.

## Limitations

- Corpus limited to Spanish public sector tenders, limiting generalizability to private contracts
- Moderate inter-annotator agreement (0.61) suggests potential label inconsistency
- Three underrepresented categories excluded from main experiments, potentially losing important information
- MEL model weights and 3CEL corpus not published at time of writing, preventing immediate reproduction

## Confidence

- **High Confidence:** Span categorization methodology distinction from NER; IAA calculation methodology and harmonization process
- **Medium Confidence:** Performance improvements from MEL and label reduction (depend on reproducing exact experimental conditions)
- **Low Confidence:** Generalization claims to other contract types (private sector, different languages) lack empirical validation

## Next Checks

1. Replicate baseline results: Fine-tune xlm-roberta-large on 3CEL using published hyperparameters to verify reported macro F1 of ~0.73 (18 labels) and ~0.84 (15 labels)
2. Test cross-domain generalization: Evaluate the fine-tuned model on a held-out sample of private Spanish contracts not present in 3CEL to assess real-world applicability
3. Validate IAA robustness: Manually inspect 20-30 randomly selected annotations to confirm that harmonization resolved discrepancies appropriately and didn't introduce systematic biases