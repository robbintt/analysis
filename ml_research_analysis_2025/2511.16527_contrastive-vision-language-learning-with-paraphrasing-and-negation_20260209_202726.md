---
ver: rpa2
title: Contrastive vision-language learning with paraphrasing and negation
arxiv_id: '2511.16527'
source_url: https://arxiv.org/abs/2511.16527
tags:
- negation
- image
- original
- captions
- caption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SemCLIP addresses the challenge of improving vision-language models'
  robustness to semantic variations, particularly negation and paraphrasing, by extending
  CLIP's training with a novel loss function. The method generates paraphrased and
  negated captions for each image-caption pair and incorporates projection-based paraphrasing
  and negation losses alongside the standard contrastive objective.
---

# Contrastive vision-language learning with paraphrasing and negation

## Quick Facts
- arXiv ID: 2511.16527
- Source URL: https://arxiv.org/abs/2511.16527
- Reference count: 40
- Improves CLIP's robustness to negation and paraphrasing, achieving 78.1% original-over-negation accuracy on CC-Neg benchmark

## Executive Summary
SemCLIP addresses the challenge of improving vision-language models' robustness to semantic variations, particularly negation and paraphrasing, by extending CLIP's training with a novel loss function. The method generates paraphrased and negated captions for each image-caption pair and incorporates projection-based paraphrasing and negation losses alongside the standard contrastive objective. This approach aims to maintain performance on original captions while improving robustness to semantic transformations. Empirically, SemCLIP achieves 78.1% original-over-negation accuracy on the CC-Neg benchmark (improving from 68.1% with CLIP) while preserving retrieval performance on original and paraphrased captions.

## Method Summary
SemCLIP extends CLIP's contrastive learning framework by incorporating additional loss terms for paraphrasing and negation. The method generates paraphrased (c+) and negated (c-) captions for each image-caption pair using a two-stage LLM pipeline (Phi-4 for generation, Mistral-7B for validation). A projection module with orthonormal vectors maps text embeddings into a low-dimensional subspace where paraphrased captions are encouraged to align with originals while negated captions are pushed toward orthogonality. The total loss combines standard contrastive alignment with paraphrasing and negation losses, using binary weights for simplicity.

## Key Results
- Achieves 78.1% original-over-negation accuracy on CC-Neg benchmark, improving from 68.1% with baseline CLIP
- Maintains strong retrieval performance on original and paraphrased captions while improving negation robustness
- Demonstrates improved robustness to negation on downstream zero-shot classification tasks when pretrained on Sugarcrepe++

## Why This Works (Mechanism)

### Mechanism 1
Projection onto a low-dimensional subspace enables the model to learn semantic ordering between paraphrases and negations. SemCLIP defines n orthonormal projection vectors (n ∈ {1, 2}) onto which text embeddings are projected. In this subspace, paraphrased captions are encouraged to project in the same direction as originals (cosine similarity → 1), while negated captions are pushed toward orthogonality or dissimilarity (cosine ≤ 0). This creates a geometric separation that standard CLIP cannot achieve in its native embedding space.

### Mechanism 2
A composite loss function with paraphrasing and negation components jointly enforces semantic invariance and exclusivity. The total loss L_total = (αL_contrastive + βL_paraphrase + γL_negation) / (α + β + γ) combines: (1) standard image-text contrastive alignment; (2) paraphrasing loss L_paraphrase = 1 - cos(p(t), p(t+)) encouraging semantic invariance; (3) negation loss L_negation = max(0, cos(p(t), p(t-))) enforcing orthogonal or dissimilar projections for contradictions.

### Mechanism 3
Two-stage LLM-based synthetic caption generation with independent validation reduces correlated errors in paraphrase and negation pairs. Phi-4 generates candidate paraphrased and negated captions; Mistral-7B independently validates quality, providing alternatives when candidates fail. Using two models with distinct training reduces systematic biases.

## Foundational Learning

- **Concept: Contrastive Learning (CLIP-style)**
  - Why needed here: SemCLIP extends CLIP's contrastive objective; understanding image-text alignment via InfoNCE-style loss is prerequisite.
  - Quick check question: Can you explain why CLIP uses bidirectional image-to-text and text-to-image losses?

- **Concept: Cosine Similarity and Projection Operations**
  - Why needed here: The paraphrasing and negation losses are defined in terms of cosine similarity between projected embeddings.
  - Quick check question: Given two normalized vectors a and b, what does cos(a, b) = 0 imply geometrically?

- **Concept: Embedding Space Geometry**
  - Why needed here: SemCLIP's mechanism relies on pushing negations toward orthogonality and pulling paraphrases together in a projection subspace.
  - Quick check question: In a 1D projection subspace, what semantic distinction can a single scalar projection encode?

## Architecture Onboarding

- **Component map**: Vision Encoder -> Text Encoder -> Projection Module -> Loss Aggregator

- **Critical path**:
  1. Load pretrained OpenCLIP ViT-B/32 and text encoder
  2. Generate paraphrased (c+) and negated (c-) captions via Phi-4 + Mistral-7B pipeline
  3. Compute image embeddings i, text embeddings t, t+, t-
  4. Project t, t+, t- onto orthonormal subspace via V^T t
  5. Compute L_contrastive (image-text), L_paraphrase (t vs t+), L_negation (t vs t-)
  6. Backprop through text encoder and projection vectors (if learnable)

- **Design tradeoffs**:
  - n=1 projection: maximal interpretability (binary semantic signal) but limited representational capacity
  - n=2 projection: more capacity but harder to interpret
  - Frozen vs. learnable projections: experiments showed frozen projections performed adequately; learnable did not improve results
  - Binary loss weights (α, β, γ ∈ {0, 1}) simplify tuning but may miss optimal balance

- **Failure signatures**:
  - Original caption accuracy drops significantly: likely L_paraphrase or L_negation dominating L_contrastive
  - Negation accuracy near 50%: projection failing to separate, check projection initialization or loss scaling
  - Paraphrase accuracy degrades: paraphrasing loss may be pulling embeddings too aggressively, check β weight

- **First 3 experiments**:
  1. **Baseline Reproduction**: Train CLIP baseline (α=1, β=0, γ=0) on CC-Neg, verify ~33% Top-1 original caption accuracy and ~68% original-over-negation accuracy as in Table 1.
  2. **Ablation on Projection Dimension**: Compare n=1 vs. n=2 projections with SemCLIP (α=1, β=1, γ=1), measure original, paraphrased, and original-over-negation accuracies.
  3. **Negation-Only vs. SemCLIP**: Train Negation_only (α=1, β=0, γ=1) vs. SemCLIP on Sugarcrepe++, compare composite scores and downstream zero-shot classification delta on CIFAR-100.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does increasing the projection subspace dimensionality $n$ (beyond 1 or 2) affect the model's ability to disentangle complex semantic relations?
- **Open Question 2**: Can a learned or fine-grained weighting of the loss components ($\alpha, \beta, \gamma$) improve the trade-off between original caption retrieval and semantic robustness?
- **Open Question 3**: How can the SemCLIP architecture be extended to model entailment relationships in addition to equivalence (paraphrasing) and contradiction (negation)?
- **Open Question 4**: Is SemCLIP robust to non-verbal negation (e.g., "forbidden", "lacking") without explicit negation tokens like "no" or "not"?

## Limitations
- The projection-based approach's scalability to more complex semantic transformations beyond negation and paraphrasing remains unclear
- Binary loss weighting scheme (α, β, γ ∈ {0, 1}) may not generalize well across datasets with different semantic distributions
- The claim that two-stage LLM caption generation with independent validation significantly reduces correlated errors lacks direct quantitative evidence

## Confidence
- **High confidence**: The original-over-negation accuracy improvement (78.1% vs 68.1%) on CC-Neg is well-supported by the experimental results in Table 1.
- **Medium confidence**: The preservation of original caption retrieval performance while improving negation robustness is demonstrated but requires careful weight tuning in practice.
- **Low confidence**: The claim that two-stage LLM caption generation with independent validation significantly reduces correlated errors lacks direct quantitative evidence.

## Next Checks
1. Test SemCLIP on CC-Neg with n=3 projections to assess scalability and interpretability trade-offs for more complex semantic transformations.
2. Conduct ablation studies varying α, β, γ across continuous values (not just binary) to determine optimal loss weighting for different dataset characteristics.
3. Evaluate SemCLIP's performance on datasets containing more subtle semantic variations (e.g., entailment, contradiction) beyond simple negation to assess generalizability.