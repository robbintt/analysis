---
ver: rpa2
title: 'PaliGemma-CXR: A Multi-task Multimodal Model for TB Chest X-ray Interpretation'
arxiv_id: '2503.00171'
source_url: https://arxiv.org/abs/2503.00171
tags:
- tasks
- paligemma-cxr
- paligemma
- image
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the challenge of automated TB diagnosis in
  chest X-rays, particularly in regions with radiologist shortages. They propose PaliGemma-CXR,
  a multi-task multimodal model capable of performing TB diagnosis, object detection,
  segmentation, report generation, and visual question answering.
---

# PaliGemma-CXR: A Multi-task Multimodal Model for TB Chest X-ray Interpretation

## Quick Facts
- arXiv ID: 2503.00171
- Source URL: https://arxiv.org/abs/2503.00171
- Reference count: 31
- Multi-task multimodal model achieving state-of-the-art results on TB diagnosis and multiple chest X-ray interpretation tasks

## Executive Summary
PaliGemma-CXR addresses the critical challenge of automated tuberculosis diagnosis in chest X-rays, particularly in regions with limited access to radiologists. The authors propose a multi-task multimodal model that simultaneously performs TB diagnosis, object detection, segmentation, report generation, and visual question answering. By fine-tuning on a curated dataset from Uganda and employing inverse dataset size sampling to address class imbalance, the model demonstrates superior performance across all tasks compared to both task-specific models and zero-shot baselines. The approach shows particular promise for resource-constrained settings where automated diagnostic tools could significantly improve healthcare access.

## Method Summary
The authors developed PaliGemma-CXR by fine-tuning the PaliGemma architecture on a curated multimodal dataset from a clinical study in Uganda. The model employs a multi-task learning framework where different heads handle specific tasks: binary classification for TB diagnosis, object detection and segmentation for identifying TB-related findings, report generation for automated radiology reporting, and visual question answering for clinical queries. To address dataset imbalance, the authors implemented inverse dataset size sampling during training. The fine-tuning process optimized the model across all tasks simultaneously, leveraging shared representations to improve overall performance.

## Key Results
- Achieved 90.32% accuracy on TB diagnosis task
- Reached 98.95% accuracy on visual question answering
- Obtained 41.3 BLEU score for automated report generation
- Demonstrated mAP of 19.4 and 16.0 for object detection and segmentation respectively

## Why This Works (Mechanism)
The multi-task learning approach enables PaliGemma-CXR to leverage shared representations across related tasks, where knowledge gained from one task (e.g., segmentation) can improve performance on others (e.g., diagnosis). The inverse dataset size sampling strategy effectively addresses class imbalance by ensuring underrepresented samples receive appropriate weight during training. Fine-tuning the pre-trained PaliGemma architecture on domain-specific medical imaging data allows the model to adapt general visual understanding capabilities to the specialized domain of chest X-ray interpretation, particularly for TB-related pathologies.

## Foundational Learning
- **Multi-task Learning**: Training a single model on multiple related tasks to improve generalization and efficiency
  - Why needed: Reduces model complexity and leverages shared representations across tasks
  - Quick check: Verify performance improvements over task-specific baselines
- **Inverse Dataset Size Sampling**: Adjusting sample weights inversely proportional to dataset size during training
  - Why needed: Addresses class imbalance in medical datasets where certain findings are rare
  - Quick check: Compare class-wise performance metrics with and without sampling
- **Fine-tuning Pre-trained Models**: Adapting a model pre-trained on large datasets to specialized domains
  - Why needed: Leverages learned visual representations while adapting to domain-specific features
  - Quick check: Evaluate performance gain compared to training from scratch
- **Multimodal Learning**: Integrating different data modalities (images and text) for comprehensive understanding
  - Why needed: Enables generation of reports and answers to clinical questions from visual data
  - Quick check: Assess coherence between visual findings and generated text outputs

## Architecture Onboarding

**Component Map**: Input X-ray -> PaliGemma Backbone -> Task-specific Heads (Classification, Detection, Segmentation, Text Generation, VQA)

**Critical Path**: Image input flows through the PaliGemma backbone, extracting visual features that are then processed by multiple task-specific heads operating in parallel, with shared representations enabling cross-task learning benefits.

**Design Tradeoffs**: The multi-task architecture trades some task-specific optimization for overall efficiency and shared learning, potentially sacrificing peak performance on individual tasks for comprehensive capability. The choice of fine-tuning versus training from scratch balances computational efficiency with adaptation to medical domain requirements.

**Failure Signatures**: Performance degradation may occur on rare TB manifestations not well-represented in training data, potential bias toward demographic groups overrepresented in the Ugandan dataset, and possible inconsistencies between visual findings and generated reports for complex cases requiring nuanced clinical judgment.

**First Experiments**: 
1. Evaluate baseline performance of individual task-specific models versus multi-task PaliGemma-CXR
2. Test model robustness across different demographic subgroups using available metadata
3. Compare performance on rare TB manifestations versus common presentations

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on a single dataset from Uganda limits generalizability to other populations and imaging protocols
- Performance metrics based on a relatively small dataset may not capture full spectrum of TB manifestations
- Model effectiveness across diverse demographic groups and different imaging equipment remains untested
- Potential biases in training data could affect diagnostic accuracy across different populations

## Confidence

| Claim | Confidence |
|-------|------------|
| Overall performance improvements over baselines | Medium |
| Technical implementation of multi-task architecture | High |
| Clinical applicability in real-world settings | Low to Medium |

## Next Checks
1. External validation on datasets from multiple geographic regions with different TB prevalence rates and imaging protocols
2. Prospective clinical trials comparing PaliGemma-CXR's diagnostic performance against practicing radiologists in real-world settings
3. Bias analysis across demographic subgroups to ensure equitable performance across different patient populations