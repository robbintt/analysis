---
ver: rpa2
title: 'LiveVal: Time-aware Data Valuation via Adaptive Reference Points'
arxiv_id: '2502.10489'
source_url: https://arxiv.org/abs/2502.10489
tags:
- data
- training
- liveval
- valuation
- point
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LiveVal addresses the challenge of efficiently measuring training
  data quality in large-scale machine learning systems, where early detection of harmful
  samples can prevent wasted computation. The core innovation is a parameter-space
  data valuation method that integrates seamlessly with SGD training, using adaptive
  reference points to measure how each gradient update moves the model toward optimal
  states.
---

# LiveVal: Time-aware Data Valuation via Adaptive Reference Points

## Quick Facts
- arXiv ID: 2502.10489
- Source URL: https://arxiv.org/abs/2502.10489
- Reference count: 40
- LiveVal achieves 180× speedup over Leave-One-Out methods while maintaining 64% accuracy for corrupted sample detection

## Executive Summary
LiveVal addresses the challenge of efficiently measuring training data quality in large-scale machine learning systems, where early detection of harmful samples can prevent wasted computation. The core innovation is a parameter-space data valuation method that integrates seamlessly with SGD training, using adaptive reference points to measure how each gradient update moves the model toward optimal states. This approach avoids expensive retraining and restrictive assumptions of existing methods.

Experimental results demonstrate LiveVal's effectiveness across multiple dimensions: it achieves 180× speedup over Leave-One-Out methods while maintaining strong detection performance (64% accuracy for corrupted samples versus 84% for LOO); identifies 27.5% of corrupted samples within the first training epoch; adapts to different corruption levels and data modalities; and scales efficiently to large models like Inception V3 (24M parameters) with minimal memory requirements. Theoretical analysis establishes bounded and stable valuations, making LiveVal a practical solution for real-time data quality assessment in modern deep learning systems.

## Method Summary
LiveVal introduces a time-aware data valuation framework that operates within the parameter space during standard SGD training. The method maintains a moving reference point in parameter space that tracks the optimal trajectory, then measures each sample's contribution by comparing its gradient updates to this reference. Unlike prior work that requires expensive retraining or assumes specific loss functions, LiveVal computes valuations online as training progresses. The adaptive window mechanism automatically adjusts to different data characteristics and corruption levels, while maintaining theoretical guarantees of bounded and stable valuations. The approach requires only forward and backward passes per batch, adding minimal computational overhead to standard training pipelines.

## Key Results
- Achieves 180× speedup over Leave-One-Out methods while maintaining 64% accuracy for corrupted sample detection (84% for LOO)
- Identifies 27.5% of corrupted samples within the first training epoch, with accuracy increasing to 54.6% by epoch 2
- Scales efficiently to large models (Inception V3 with 24M parameters) with minimal memory requirements
- Demonstrates robust performance across vision, text, and tabular datasets with different corruption types

## Why This Works (Mechanism)
LiveVal leverages the temporal dynamics of SGD training by tracking how individual samples influence the model's trajectory toward optimal parameters. The method uses adaptive reference points that evolve with training progress, allowing it to measure the true impact of each sample's gradient contribution. By operating directly in parameter space during training, LiveVal captures the cumulative effect of samples over time rather than relying on static influence measures. The adaptive window mechanism ensures the reference point remains aligned with the actual optimization path, even as training conditions change. This temporal awareness enables early detection of harmful samples before they significantly degrade model performance.

## Foundational Learning
- **Parameter Space Data Valuation**: Measures sample importance through their impact on model parameters rather than predictions; needed to avoid expensive retraining cycles and enable real-time assessment during training.
- **Adaptive Reference Points**: Dynamic tracking of optimal parameter trajectory; needed to maintain alignment with changing training dynamics and different dataset characteristics.
- **Gradient-based Contribution Analysis**: Quantifies individual sample impact through gradient updates; needed to enable efficient online computation without retraining.
- **Time-aware Valuation**: Incorporates temporal progression of training; needed to capture cumulative sample effects and enable early detection.
- **Bounded Valuation Theory**: Mathematical guarantees on valuation stability; needed to ensure reliable detection across different training scenarios.

## Architecture Onboarding
**Component Map**: Training Loop -> Gradient Collection -> Reference Point Update -> Sample Valuation -> Detection Decision
**Critical Path**: Each training batch flows through forward pass, backward pass, gradient aggregation, reference point comparison, and valuation computation. The adaptive window mechanism continuously adjusts reference point tracking based on recent training dynamics.
**Design Tradeoffs**: Computational efficiency vs. valuation accuracy (simplified gradient tracking enables 180× speedup), theoretical guarantees vs. practical flexibility (bounded valuations may limit sensitivity), online computation vs. retrospective analysis (real-time detection vs. potentially higher accuracy with full retraining).
**Failure Signatures**: High false positive rates when data distributions shift rapidly, missed detections for samples with subtle but cumulative negative effects, degraded performance with extremely noisy gradients or unstable training.
**First Experiments**: 1) CIFAR-10 with synthetic label noise to verify detection accuracy vs. baseline methods, 2) ImageNet with Inception V3 to test scalability and memory efficiency, 3) Multi-modal dataset comparison (vision/text/tabular) to validate cross-domain performance.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How sensitive is LiveVal's detection performance to the choice of adaptive window hyperparameters (δmin, δmax, εmin, εmax, ∆δ), and can these be automatically tuned for new datasets?
- Basis in paper: [inferred] The adaptive window mechanism in Section IV-B relies on five hyperparameters that control window adjustment behavior, but experiments only use fixed values without sensitivity analysis.
- Why unresolved: No ablation studies are provided showing how detection accuracy varies with different hyperparameter configurations.
- What evidence would resolve it: Systematic hyperparameter sensitivity experiments across datasets showing performance bounds and degradation patterns.

### Open Question 2
- Question: Can LiveVal's low-corruption detection rates (30% at k=10 for text, 34% for tabular) be improved while maintaining computational efficiency?
- Basis in paper: [explicit] Table IV shows detection rates drop substantially when fewer samples are corrupted, from 56.5% at k=40 to 30% at k=10 for 20 Newsgroups.
- Why unresolved: The authors acknowledge "modality-specific variations" but do not propose mechanisms to improve low-corruption scenarios.
- What evidence would resolve it: Modified LiveVal variants achieving >50% detection at k=10 across modalities without significant computational overhead.

### Open Question 3
- Question: Does LiveVal maintain stable valuations when integrated with adaptive optimizers like Adam or RMSprop, rather than vanilla SGD?
- Basis in paper: [inferred] The method is designed for "seamless integration with SGD training" (Abstract, Section I) and theoretical analysis assumes standard gradient descent dynamics.
- Why unresolved: Modern training commonly uses adaptive optimizers with momentum and learning rate schedules that could affect reference point alignment.
- What evidence would resolve it: Comparative experiments showing LiveVal's detection performance and stability under Adam, AdamW, and SGD with momentum.

## Limitations
- Lower detection accuracy for datasets with few corrupted samples (30% at k=10) compared to high-corruption scenarios
- Performance sensitivity to hyperparameter choices in adaptive window mechanism remains unexplored
- Theoretical guarantees assume standard SGD dynamics, with unknown behavior under adaptive optimizers

## Confidence
- Detection accuracy claims: High (extensive experimental validation across multiple datasets and modalities)
- Computational efficiency claims: High (180× speedup clearly demonstrated with timing benchmarks)
- Scalability claims: Medium (tested on Inception V3 but limited to single large model example)
- Theoretical guarantees: Medium (proofs provided but real-world deviations possible)

## Next Checks
1. Reproduce CIFAR-10 experiments with varying noise levels to verify detection accuracy claims and early detection capabilities
2. Benchmark memory usage and runtime overhead on a 50M+ parameter model to confirm scalability beyond Inception V3
3. Test adaptive optimizer compatibility by comparing SGD vs. Adam performance on the same datasets to assess robustness to training method variations