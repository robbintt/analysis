---
ver: rpa2
title: 'SkillFactory: Self-Distillation For Learning Cognitive Behaviors'
arxiv_id: '2512.04072'
source_url: https://arxiv.org/abs/2512.04072
tags:
- answer
- correct
- skillfactory
- response
- verdict
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SkillFactory teaches language models reasoning skills like verification
  and retrying by constructing training data from the model's own outputs, rearranged
  into structured traces showing explicit skill use. This self-distillation approach
  does not require stronger teacher models and instead focuses on learning the structural
  patterns of reasoning behaviors.
---

# SkillFactory: Self-Distillation For Learning Cognitive Behaviors

## Quick Facts
- arXiv ID: 2512.04072
- Source URL: https://arxiv.org/abs/2512.04072
- Reference count: 40
- Primary result: Self-distillation with structured traces improves reasoning skill generalization without requiring stronger teacher models

## Executive Summary
SkillFactory teaches language models reasoning skills like verification and retrying by constructing training data from the model's own outputs, rearranged into structured traces showing explicit skill use. This self-distillation approach does not require stronger teacher models and instead focuses on learning the structural patterns of reasoning behaviors. Across two training regimes, SkillFactory consistently improves models' ability to generalize to harder task variants and maintain robustness on out-of-domain tasks. Models trained with SkillFactory show higher skill usage, longer and more varied responses, and better performance post-reinforcement learning compared to baselines, even when starting from lower pre-training accuracy.

## Method Summary
SkillFactory uses self-distillation to teach cognitive reasoning skills by first generating diverse solution attempts for each problem, then creating reflections on each attempt's correctness, and finally assembling these into structured traces that always end with a correct solution. The method uses explicit tags (`<sample>`, `<reflect>`, `<verdict>`) to demarcate skill boundaries, trains with standard supervised fine-tuning for 2 epochs, then refines with GRPO reinforcement learning using only binary correctness rewards. The key innovation is training on traces that mix correct and incorrect attempts but always end correctly, teaching models persistence and recovery patterns.

## Key Results
- SkillFactory achieves 25.1% accuracy on harder Countdown variants post-RL vs 21.2% for R1 Distill baseline, despite lower initial SFT accuracy (2.8% vs 11.7%)
- Models trained with SkillFactory maintain better generalization on out-of-domain tasks (32.0% vs 24.1% on OOD ablation)
- SkillFactory shows improved verifier F1 on incorrect answers (0.92-0.97) compared to baselines
- Budget forcing enables effective scaling with inference-time budget, preventing performance saturation

## Why This Works (Mechanism)

### Mechanism 1: Structure Priming Over Content Mastery
Training on correctly structured reasoning traces primes models for RL, even when traces contain errors or yield low SFT accuracy. SFT instills the syntactic scaffolding for cognitive behaviors, while RL learns when and how to deploy these structures productively. The structure provides exploration primitives RL can refine.

### Mechanism 2: Explicit Demarcation Improves Cross-Domain Transfer
Tagging cognitive behaviors explicitly (vs implicit natural language) improves skill generalization to OOD tasks. Tags create discrete, learnable tokens that mark skill boundaries, reducing ambiguity about what the model is doing and enabling cleaner credit assignment during RL.

### Mechanism 3: Correct-Ending Traces with Mixed Sequencing
Training traces that always end correctly but contain mixed correct/incorrect attempts teach retry-without-giving-up. By always appending a correct solution, the model learns persistence. By shuffling incorrect attempts before it, the model learns what failure looks like and that recovery is possible.

## Foundational Learning

- **Concept: Self-distillation** - Why needed: SkillFactory generates "silver" traces from the model's own outputs, not a stronger teacher. You must understand that distillation can be structural (learning formats/patterns) rather than knowledge (learning from smarter models).
- **Concept: Sparse reward RL with GRPO** - Why needed: The RL stage uses only binary correctness rewards. Understanding how policy gradients can reinforce multi-step reasoning behaviors from sparse signals is critical.
- **Concept: Easy-to-hard generalization** - Why needed: The paper's main evaluation is training on Countdown-3arg and testing on 4-6arg variants. Understanding compositional generalization (more arguments = harder search) is essential.

## Architecture Onboarding

- **Component map:** Solution Sampler -> Reflection Generator -> Trace Assembler -> SFT Trainer -> RL Refiner
- **Critical path:** Solution sampling diversity → Reflection accuracy → Trace assembly sequencing → SFT priming → RL refinement. Weakness at any stage cascades.
- **Design tradeoffs:** More SFT data ≠ better post-RL. SkillFactory trades peak performance for robustness.
- **Failure signatures:** Model never generates `<reflect>` tags at inference → SFT didn't internalize structure; Model retries indefinitely without converging → RL didn't learn when to stop
- **First 3 experiments:**
  1. Ablate reflection quality: Filter reflections to only those with F1 > 0.9 on held-out validation
  2. Scale tag vocabulary: Add `<plan>`, `<backtrack>` tags with corresponding trace construction
  3. Budget forcing ceiling: Test SkillFactory at 16k, 32k token budgets vs 8k in paper

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does SkillFactory's performance decrease slightly when scaling from 1k to 10k SFT examples (42.1% to 40.6% on math benchmarks), and is there an optimal data scaling regime?
- Basis: The authors note this decrease and hypothesize "additional SFT does not help SkillFactory because the core skills are already learned early, unlike in distillation, where models learn new strategies and knowledge from the teacher."
- What evidence would resolve it: Ablation studies varying SFT data size more granularly, combined with analysis of skill acquisition curves during training.

### Open Question 2
- Question: Can SkillFactory work effectively when the base model cannot solve problems correctly at all, making it impossible to construct traces ending in correct solutions?
- Basis: The trace construction requires at least one correct solution per question. For very hard problems where the base model has near-zero accuracy, this assumption may fail.
- What evidence would resolve it: Experiments on tasks where base models have very low accuracy, testing alternative trace construction strategies.

### Open Question 3
- Question: Does the SkillFactory approach generalize to cognitive skills beyond retrying and reflection, such as planning, decomposition, or analogy-making?
- Basis: The authors state "In this work, we focus on the following two: Retrying... Reflection" and describe the method as "a platform for shaping cognitive behaviors of LLMs across a variety of tasks."
- What evidence would resolve it: Extending SkillFactory to new skill templates and evaluating whether the self-distillation approach remains effective without architectural changes.

## Limitations
- The method's dependence on task-specific reflection prompts and solution sampling strategies means it may not transfer seamlessly to domains with different solution verification requirements
- The paper demonstrates improved performance with budget forcing but doesn't explore whether models develop pathological behavior when operating near budget limits
- Reliance on GRPO with sparse rewards may limit applicability to tasks where outcome feedback is delayed or noisy

## Confidence
- **High confidence**: Empirical demonstration that SkillFactory enables better easy-to-hard generalization than R1 Distill baseline (25.1% vs 21.2% on Countdown-4arg)
- **Medium confidence**: Claim that explicit tags improve cross-domain transfer, supported by OOD task improvements but lacking ablation studies on tag structure importance
- **Medium confidence**: Assertion that lower SFT accuracy (2.8%) can yield higher post-RL performance than higher SFT accuracy (11.7%), though the mechanism is well-explained
- **Low confidence**: Generalizability to tasks without clear verification criteria or where solutions cannot be guaranteed to exist

## Next Checks
1. **Tag ablation study**: Systematically remove `<sample>` and `<reflect>` tags while keeping trace structure intact to quantify their specific contribution to skill generalization
2. **Zero-shot transfer evaluation**: Test SkillFactory-trained models on completely unseen reasoning tasks (e.g., code generation, logical puzzles) to measure true cross-domain generalization
3. **Budget forcing robustness**: Train models at progressively higher token budgets (4k, 8k, 16k, 32k) and measure whether SkillFactory prevents the performance saturation observed in baseline models