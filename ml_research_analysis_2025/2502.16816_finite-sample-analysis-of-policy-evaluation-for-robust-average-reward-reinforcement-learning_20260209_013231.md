---
ver: rpa2
title: Finite-Sample Analysis of Policy Evaluation for Robust Average Reward Reinforcement
  Learning
arxiv_id: '2502.16816'
source_url: https://arxiv.org/abs/2502.16816
tags:
- robust
- uncertainty
- have
- theorem
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first finite-sample analysis for policy
  evaluation in robust average-reward MDPs, where the Bellman operator lacks contraction
  under any norm. The key innovation is proving the robust Bellman operator is a contraction
  under a carefully constructed semi-norm when the nominal model is ergodic.
---

# Finite-Sample Analysis of Policy Evaluation for Robust Average Reward Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2502.16816
- **Source URL**: https://arxiv.org/abs/2502.16816
- **Reference count**: 40
- **Primary result**: First finite-sample analysis for robust average-reward MDPs with Õ(ε⁻²) sample complexity

## Executive Summary
This paper provides the first finite-sample analysis for policy evaluation in robust average-reward MDPs, where the Bellman operator lacks contraction under any norm. The key innovation is proving the robust Bellman operator is a contraction under a carefully constructed semi-norm when the nominal model is ergodic. This enables the use of stochastic approximation techniques for policy evaluation. The authors develop efficient estimators for the worst-case transition effects under TV and Wasserstein uncertainty sets by modifying Multi-Level Monte Carlo with truncated geometric sampling, achieving finite expected sample complexity while maintaining exponentially decaying bias. Their robust TD learning algorithm achieves order-optimal sample complexity of Õ(ε⁻²) for both policy evaluation and robust average-reward estimation.

## Method Summary
The method employs a two-phase robust TD learning algorithm. Phase 1 estimates the value function using stochastic approximation under a semi-norm contraction framework, where the robust Bellman operator is shown to contract by factor γ = α + ε < 1. The key technical innovation is the construction of a semi-norm that combines extremal norms over fluctuation matrices with quotient correction. For TV and Wasserstein uncertainty sets, the support function estimators use truncated Multi-Level Monte Carlo with geometric sampling, capping the number of samples at N_max while maintaining exponentially decaying bias. Phase 2 estimates the average reward by averaging Bellman residuals computed using the value function estimate from Phase 1.

## Key Results
- First finite-sample analysis for robust average-reward MDPs with non-contractive Bellman operators
- Õ(ε⁻²) sample complexity for both policy evaluation and robust average-reward estimation
- Efficient truncated MLMC estimators achieving finite expected sample complexity for TV and Wasserstein uncertainty sets
- Semi-norm contraction framework enabling stochastic approximation analysis

## Why This Works (Mechanism)

### Mechanism 1: Semi-Norm Contraction of Robust Bellman Operator
The robust average-reward Bellman operator is a strict contraction under a constructed semi-norm ||·||_P. The construction combines (1) an extremal norm over the compact family of fluctuation matrices {Q^π_P = P^π - E_P : P ∈ P} with joint spectral radius < 1, and (2) a quotient correction term that annihilates constant vectors: ||x||_P = sup_{P∈P} ||Q^π_P x||_{ext} + ε·inf_{c∈R} ||x - ce||_{ext}. This yields uniform one-step contraction by factor γ = α + ε < 1. The core assumption is that the nominal model is irreducible and aperiodic, plus radius restrictions ensuring all models in the uncertainty set remain ergodic.

### Mechanism 2: Truncated MLMC for Bounded-Sample Support Function Estimation
Truncating geometric sampling at N_max yields finite expected sample complexity O(N_max) while maintaining exponentially decaying bias O(2^{-N_max/2}). Standard MLMC requires sampling N ~ Geometric(0.5) and collecting 2^N samples, giving infinite expected samples. The truncation N' = min{N, N_max} caps samples at 2^{N_max} + 2. The correction term ∆_{N'}(V) accounts for truncation bias, with each level's error bounded via binomial concentration and Lipschitz properties of support functions. The core assumption is that the value function has bounded span semi-norm ||V||_{sp} ≤ 4t_{mix}.

### Mechanism 3: Biased Stochastic Approximation Under Semi-Norm Contraction
The TD iteration V_{t+1} = V_t + η_t(T̂_g(V_t) - V_t) converges with rate O(1/T) for variance terms and O(log T) for bias terms. The semi-Lyapunov function M_E(·) provides negative drift under the semi-norm contraction. Dual norm properties bound the bias-augmented gradient term: ⟨∇M_E, E[w_t]⟩ ≤ ||∇M_E||* ||E[w_t]|| ≤ G(1 + ||x_t - x*||_P)ε_{bias}. This keeps bias contribution logarithmic rather than linear in T. The core assumption is i.i.d. noise with bounded variance and bias.

## Foundational Learning

**Concept: Robust MDPs and Uncertainty Sets**
- Why needed here: The paper assumes transitions lie in (s,a)-rectangular sets P^a_s ⊆ ∆(S) defined by contamination, TV, or Wasserstein distance. Understanding how support functions σ_{P^a_s}(V) = min_{p∈P^a_s} p^⊤V capture worst-case transitions is essential.
- Quick check question: Given a TV uncertainty set {q : ||q - p̃||₁/2 ≤ δ}, what is the dual form of the support function?

**Concept: Joint Spectral Radius and Extremal Norms**
- Why needed here: Proving uniform contraction across all models in the uncertainty set requires the joint spectral radius ρ̂({Q^π_P}) < 1. The extremal norm construction is the technical tool enabling this.
- Quick check question: Why can't one simply use the spectral radius of individual matrices when analyzing a family of products?

**Concept: Semi-Norms with Non-Trivial Kernels**
- Why needed here: Average-reward Bellman operators only determine value functions up to additive constants (kernel = {ce : c ∈ R}). Standard fixed-point analysis fails; semi-norm contraction on quotient spaces is the correct framework.
- Quick check question: What is the kernel of ||·||_P and why does the quotient construction preserve contraction?

## Architecture Onboarding

**Component map:**
```
Phase 1 (Value Estimation):
  └─ Loop over T iterations
     ├─ For each (s,a): Sample σ̂^a_s(V_t) via [Eq 18] (contamination) or [Algorithm 1] (TV/Wasserstein)
     ├─ Compute T̂_{g₀}(V_t)(s) = Σ_a π(a|s)[r(s,a) - g₀ + σ̂^a_s(V_t)]
     └─ Update: V_{t+1} = V_t + η_t(T̂_{g₀}(V_t) - V_t), anchor at s₀

Phase 2 (Reward Estimation):
  └─ Loop over T iterations
     ├─ Compute Bellman residual δ̂_t(s) using V_T from Phase 1
     ├─ Average: δ̄_t = (1/S) Σ_s δ̂_t(s)
     └─ Update: g_{t+1} = g_t + β_t(δ̄_t - g_t)
```

**Critical path**: The truncation level N_max is the key hyperparameter. Set N_max = O(log(√S·t_{mix}/(ε(1-γ)))) to balance bias vs. variance. Stepsizes η_t = O(1/t), β_t = O(1/t) ensure O(ε⁻²) sample complexity.

**Design tradeoffs**:
- Smaller N_max → lower per-iteration sample cost but higher bias
- Larger N_max → lower bias but variance scales as O(N_max)
- Contamination sets: O(1) samples per estimate, no bias (unbiased estimator exists)
- TV/Wasserstein sets: O(N_max) expected samples, controlled bias

**Failure signatures**:
- Convergence to wrong value: Check if N_max is too small (bias dominates)
- Slow convergence: Check if radius restrictions violated (γ → 1)
- High variance: Check if t_{mix} underestimated or S large
- Non-ergodic behavior: Verify Assumption 3.1 holds for nominal model

**First 3 experiments**:
1. **Sanity check on non-robust chain**: Implement Algorithm 2 with degenerate uncertainty set (δ = 0). Verify V_T converges to standard average-reward value function. Compare convergence rate to theoretical O(1/T).
2. **Truncation level ablation**: On a 20-state robust MDP with TV uncertainty, vary N_max ∈ {5, 10, 15, 20} and plot (a) expected samples per iteration, (b) bias at convergence, (c) final error ||V_T - V*||_∞. Verify tradeoff curve.
3. **Radius sensitivity**: Fix N_max, vary uncertainty radius δ ∈ {0.05, 0.1, 0.15, 0.2}. Monitor contraction factor γ empirically by computing ||T_g(V_1) - T_g(V_2)||_P / ||V_1 - V_2||_P. Verify breakdown occurs at theoretical threshold.

## Open Questions the Paper Calls Out

**Open Question 1**: Can the proposed semi-norm contraction analysis be extended to robust average-reward RL using function approximation?
- Basis in paper: The conclusion states, "scaling the algorithm and results in the paper via function approximations remains an important open problem."
- Why unresolved: The current semi-norm construction and convergence proofs rely on tabular representations; function approximation introduces approximation errors and complicates the biased stochastic approximation analysis.
- What evidence would resolve it: Finite-sample convergence bounds for robust average-reward TD learning with linear or non-linear function approximators.

**Open Question 2**: Can the dependencies on state space size $S$, action space size $A$, and the contraction factor $(1-\gamma)$ be sharpened?
- Basis in paper: The discussion following Theorem 6.1 notes, "we do not claim tightness in S, A and $\gamma$, and treat sharpening these dependencies as open."
- Why unresolved: While the $\tilde{O}(\epsilon^{-2})$ rate is order-optimal in $\epsilon$, the polynomial scaling with other parameters may be loose.
- What evidence would resolve it: Derivation of minimax lower bounds specific to the robust average-reward setting or algorithms with improved dimension dependencies.

**Open Question 3**: Can the requirement for the nominal model to be ergodic (irreducible and aperiodic) be relaxed?
- Basis in paper: The conclusion lists the ergodicity requirement as a limitation of the work.
- Why unresolved: The proof of the semi-norm contraction relies on the unique stationary distribution and spectral properties guaranteed by ergodicity.
- What evidence would resolve it: A finite-sample analysis for robust policy evaluation that holds under weaker conditions, such as unichain or communicating MDPs.

## Limitations
- The analysis critically depends on ergodicity of the nominal model and radius restrictions that ensure the uncertainty set remains ergodic
- The semi-norm construction requires computing extremal norms which can be computationally intensive
- The truncation mechanism introduces a fundamental bias-variance tradeoff requiring careful hyperparameter tuning

## Confidence
- **High confidence** in the semi-norm contraction framework (Theorem 4.2) given the rigorous Lyapunov analysis
- **Medium confidence** in the truncated MLMC estimator bounds due to dependence on span semi-norm estimates that require additional verification
- **Medium confidence** in the overall sample complexity bounds as they rely on multiple chaining of concentration and contraction arguments

## Next Checks
1. Verify the joint spectral radius bound numerically on synthetic robust MDPs with varying uncertainty radii to identify the breakdown threshold
2. Implement the semi-norm construction and measure contraction empirically on benchmark MDPs to confirm the theoretical γ < 1
3. Conduct sensitivity analysis on N_max across different uncertainty set types to map the bias-variance tradeoff frontier