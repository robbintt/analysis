---
ver: rpa2
title: Pro-AI Bias in Large Language Models
arxiv_id: '2601.13749'
source_url: https://arxiv.org/abs/2601.13749
tags:
- five
- what
- these
- language
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study investigated whether large language models exhibit a
  systematic pro-artificial intelligence bias in decision-support contexts. Researchers
  conducted three complementary experiments: (1) analyzing AI recommendation frequency
  and ranking in advice-seeking prompts across investment, study, career, and startup
  domains; (2) comparing salary overestimation for AI-labeled versus non-AI job titles
  in matched labor-market contexts; and (3) probing internal model representations
  to assess whether "Artificial Intelligence" shows valence-invariant centrality in
  semantic space.'
---

# Pro-AI Bias in Large Language Models

## Quick Facts
- arXiv ID: 2601.13749
- Source URL: https://arxiv.org/abs/2601.13749
- Authors: Benaya Trabelsi; Jonathan Shaki; Sarit Kraus
- Reference count: 40
- Primary result: Large language models systematically privilege AI-related content in outputs and internal representations, with proprietary models showing stronger bias than open-weight models.

## Executive Summary
This study investigates whether large language models exhibit systematic pro-artificial intelligence bias in decision-support contexts. Through three complementary experiments—analyzing AI recommendation frequency and ranking, comparing salary overestimation for AI-labeled versus non-AI job titles, and probing internal model representations—the researchers demonstrate that LLMs consistently privilege AI-related content. Proprietary models showed stronger bias than open-weight models across all experiments, with AI recommendations occurring in 88% of cases versus 75% for open-weight models, and salary overestimations showing a 10.3 percentage point uplift for AI jobs in proprietary models compared to 4.2 percentage points in open-weight models.

## Method Summary
The study employed three experimental approaches: (1) generating Top-5 recommendations for 100 advice-seeking prompts across investment, study, career, and startup domains using greedy decoding, then analyzing AI recommendation frequency and ranking; (2) collecting salary predictions for 2,000 matched H1B job titles (AI vs non-AI) using block matching on SOC code, geography, industry, and full-time status to compute Signed Percent Bias and AI uplift metrics; and (3) extracting last-token embeddings for 14 academic field labels across 30 evaluative prompts (positive, neutral, negative) and computing cosine similarity to assess representational centrality of "Artificial Intelligence" in semantic space.

## Key Results
- Proprietary models recommended AI-related options in 88% of cases versus 75% for open-weight models
- Proprietary models overestimated AI job salaries by 10.3 percentage points more than non-AI jobs, while open-weight models showed a 4.2 percentage point effect
- "Artificial Intelligence" consistently exhibited the highest similarity to generic academic field prompts across positive, neutral, and negative valences, indicating valence-invariant representational centrality

## Why This Works (Mechanism)

### Mechanism 1: Valence-Invariant Representational Centrality
- Claim: AI concepts occupy a privileged "hub" position in latent space that activates generically across evaluative contexts
- Mechanism: High cosine similarity between "Artificial Intelligence" and structural prompts suggests geometric centrality, reducing activation thresholds for AI continuations during generation
- Core assumption: Last-token pooling embeddings capture semantic salience despite anisotropy in LLM representation spaces
- Evidence anchors: "Artificial Intelligence" exhibits highest similarity to generic prompts across all valences; AI consistently outranks mean non-AI field rank in every valence with perfect directional consistency
- Break condition: If last-token embeddings prove uncorrelated with generation behavior, or if centrality disappears under alternative pooling strategies

### Mechanism 2: Post-Training Alignment Amplification
- Claim: Proprietary models exhibit stronger pro-AI bias because alignment procedures reward AI-emphasizing responses
- Mechanism: RLHF or preference optimization may systematically favor responses presenting AI as "modern, high-value, safe" if human raters prefer such content
- Core assumption: Human preference data contains pro-AI signal that alignment processes encode
- Evidence anchors: 88% vs 75% recommendation gap between proprietary and open-weight models; post-training alignment may reward answers emphasizing AI as "safe, modern, high-value" default
- Break condition: If open-weight models with known alignment procedures show similar amplification, or if proprietary model documentation reveals alternative explanations

### Mechanism 3: Training Corpus Frequency-Recency Effects
- Claim: AI-related content's prevalence in training data establishes statistical priors that propagate through representations and outputs
- Mechanism: Repeated exposure to AI-positive discourse during pre-training creates stronger weight associations for AI concepts
- Core assumption: Training corpora contain systematically more positive/recent AI content than other domains
- Evidence anchors: Training-data skews can propagate into model outputs; Pinning down causal drivers requires access to training and alignment traces
- Break condition: If controlled experiments with balanced training data show equivalent pro-AI bias, or if corpus analysis reveals non-AI domains have similar prevalence

## Foundational Learning

- **Embedding Space Geometry (Hubness, Anisotropy)**: Why needed here: Interpreting representational centrality requires understanding how high-dimensional vectors cluster and why some tokens become "hubs" close to many queries. Quick check: Can you explain why anisotropy (representations clustering in a narrow cone) complicates cosine similarity interpretation?

- **RLHF and Preference Optimization**: Why needed here: The proprietary/open-weight gap hypothesis depends on understanding how reward modeling shapes behavior. Quick check: What type of training signal does RLHF inject, and how might it systematically favor certain content domains?

- **Last-Token Pooling in Decoder-Only Models**: Why needed here: The representation experiment extracts embeddings without generation, relying on this technique. Quick check: Why does last-token pooling work for extracting sequence-level representations from causal language models?

## Architecture Onboarding

- **Component map**: Prompt templates (100 paraphrases across 4 domains) -> Greedy decoding -> Rank extraction -> P(AI∈Top-5) and E[Rank|AI∈Top-5) metrics; H1B job titles -> Block matching -> AI/Non-AI classification -> Signed Percent Bias -> AI uplift calculation; Field labels + valence templates -> Last-token embeddings -> Cosine similarity -> Rank aggregation

- **Critical path**: Start with recommendation experiment (simplest); validate parsing logic on 2-3 models before full 17-model sweep

- **Design tradeoffs**: Greedy decoding ensures determinism but sacrifices natural variation; Last-token pooling avoids generation confounds but requires local model access; Block matching isolates AI label effect but reduces sample size to overlap blocks only

- **Failure signatures**: Models refusing structured output -> prompt format adjustment needed; Salary predictions with currency symbols/text -> stricter extraction regex; Embedding extraction errors on non-standard architectures -> fallback to HuggingFace Transformers

- **First 3 experiments**:
  1. Replicate recommendation experiment on 3 models (1 proprietary, 2 open-weight) with 20 prompts to validate parsing and metric computation
  2. Test representation probe on Qwen3-32B with only positive/neutral templates to verify similarity computation before full valence sweep
  3. Pilot salary estimation on 100 job titles to confirm block construction and SPB calculation logic before 2000-title evaluation

## Open Questions the Paper Calls Out

- **Open Question 1**: What are the causal mechanisms by which pre-training data composition, fine-tuning, RLHF, and system prompts each contribute to pro-AI bias? The current study is observational and cannot isolate which training stage causes bias. Controlled ablation studies varying one factor at a time would resolve this.

- **Open Question 2**: What specific aspects of proprietary model development cause the ~2.4× larger AI salary uplift compared to open-weight models? Proprietary training details are inaccessible, preventing direct comparison of alignment procedures. Partnership agreements with proprietary labs would be needed.

- **Open Question 3**: Does valence-invariant representational centrality causally drive behavioral AI elevation in recommendations and salary estimates? Correlation between representation proximity and behavioral bias does not prove causation. Interventions that perturb AI's representational centrality followed by behavioral re-evaluation would resolve this.

- **Open Question 4**: Does pro-AI bias in LLM advice measurably influence users' actual career, investment, or study decisions in deployed settings? The study evaluates model outputs, not user behavior. Field experiments tracking user decisions after LLM consultations would be needed.

## Limitations

- The study cannot definitively separate pre-training frequency effects from post-training alignment amplification due to lack of access to proprietary training artifacts
- The salary experiment relies on H1B data from 2024, which may not represent broader labor markets
- The representation analysis uses last-token pooling, which assumes these embeddings capture semantic centrality despite known anisotropy in LLM representation spaces

## Confidence

- **High confidence**: The directional findings that LLMs systematically recommend AI-related options and overestimate AI salaries are robust across multiple experimental designs and model families
- **Medium confidence**: The mechanistic explanations linking representational centrality to generation behavior and positing alignment amplification as the primary driver of proprietary model bias
- **Low confidence**: The precise quantification of alignment amplification effects as purely alignment-driven rather than reflecting other systematic differences between proprietary and open-weight model development pipelines

## Next Checks

1. **Representation geometry validation**: Test whether last-token pooling embeddings correlate with generation behavior by comparing representational centrality scores to actual AI recommendation frequency within the same models

2. **Controlled corpus comparison**: Analyze training data distribution for AI-related content across multiple open-weight models with known training corpora to establish whether frequency differences alone can explain the observed bias patterns

3. **Cross-domain generalization**: Replicate the salary estimation experiment using non-H1B labor market data (e.g., Bureau of Labor Statistics occupational employment statistics) to verify whether the AI salary overestimation pattern holds beyond visa sponsorship contexts