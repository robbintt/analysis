---
ver: rpa2
title: 'CANVAS: A Benchmark for Vision-Language Models on Tool-Based User Interface
  Design'
arxiv_id: '2511.20737'
source_url: https://arxiv.org/abs/2511.20737
tags:
- design
- figma
- community
- task
- file
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CANVAS, the first benchmark for evaluating
  vision-language models (VLMs) on tool-based user interface (UI) design. CANVAS contains
  598 design tasks across two types: design replication (reproducing entire UI screens)
  and design modification (editing specific parts of existing designs), sampled from
  3.3K mobile UI designs across 30 categories.'
---

# CANVAS: A Benchmark for Vision-Language Models on Tool-Based User Interface Design

## Quick Facts
- arXiv ID: 2511.20737
- Source URL: https://arxiv.org/abs/2511.20737
- Authors: Daeheon Jeong; Seoyeon Byun; Kihoon Son; Dae Hyun Kim; Juho Kim
- Reference count: 40
- Primary result: CANVAS introduces the first benchmark for evaluating VLMs on tool-based UI design with 598 tasks across replication and modification, revealing critical gaps in multi-turn tool invocation and precision.

## Executive Summary
This paper introduces CANVAS, the first benchmark for evaluating vision-language models (VLMs) on tool-based user interface (UI) design. CANVAS contains 598 design tasks across two types: design replication (reproducing entire UI screens) and design modification (editing specific parts of existing designs), sampled from 3.3K mobile UI designs across 30 categories. Models interact with Figma design software through 50 predefined tools in an iterative, multi-turn process. Evaluation uses hierarchical metrics measuring similarity at feature (SSIM), pattern (saliency), and semantic (BLIP caption) levels, plus component-wise attribute accuracy.

Experiments with five state-of-the-art VLMs show that Gemini-2.5-Pro excels at replication tasks while GPT-4.1 performs best at modifications. Higher-scoring models demonstrate more diverse and strategic tool usage. The benchmark reveals that precise tool selection is critical for modification tasks, as small errors can cause significant metric shifts. Error analysis identifies common failure patterns including geometric operation errors, layout operation errors, and text operation errors. CANVAS provides a standardized framework for evaluating VLMs' collaborative design capabilities and highlights areas for future improvement.

## Method Summary
CANVAS evaluates VLMs on UI design through a tool-based approach in Figma, using 598 tasks (298 replication, 300 modification) from 3.3K mobile UIs across 30 categories. Models interact via 50 predefined Figma tools through a ReAct agent loop (thought-action-observation), with inputs including target UI images, JSON node structures, and natural language instructions. Evaluation employs hierarchical metrics: SSIM (feature), saliency (pattern), BLIP-2 caption similarity (semantic), and component-wise accuracy via Hungarian matching. The framework uses temperature=0, max turns=50, and inference-only execution without fine-tuning.

## Key Results
- Gemini-2.5-Pro achieves highest replication scores; GPT-4.1 excels in modification tasks
- Higher-performing models exhibit more diverse and strategic tool usage patterns
- Tool precision (not diversity) is critical for modification tasks; small errors cause significant metric shifts
- Common failure patterns include geometric operation errors, layout operation errors, and text operation errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diverse, strategic tool invocation correlates with higher replication task performance.
- Mechanism: High-performing models (GPT-4.1, Gemini-2.5-Pro) exhibit modular strategies—e.g., creating a component once and propagating it via `copy node`—reducing turn count while improving consistency. Lower performers create components sequentially, increasing error surface.
- Core assumption: Strategic tool composition reduces cumulative positioning/attribute errors.
- Evidence anchors:
  - [Analysis]: "Higher-performing models... display increased tool diversity, indicating more strategic behaviors... a modular approach, where a model creates a component once and propagates it across the design."
  - [Figure 4]: Tool invocation frequency shows higher-performing models using more diverse tool types.
  - [corpus]: Weak direct support; related VLM-as-agent work (e.g., "Visual Language Models as Operator Agents") discusses tool use broadly but not in design contexts.
- Break condition: If models lack tools for strategic operations (copy, group, reuse), or if design complexity exceeds working memory for planning.

### Mechanism 2
- Claim: In modification tasks, tool precision (not diversity) drives success; excessive tool diversity correlates with performance degradation.
- Mechanism: Modification tasks require precise, targeted edits. Small errors propagate non-uniformly across metrics (e.g., a line break minimally affects text but heavily shifts saliency). Excessive tool exploration increases error probability.
- Core assumption: Metric shifts from edit operations are non-uniform and can cancel or invert improvements.
- Evidence anchors:
  - [Analysis]: "A positive correlation between tool precision and the Pos@K score (ρ=0.149, p<0.01) and a negative correlation between tool diversity and Pos@K (ρ=-0.365, p<0.01)."
  - [Table 2-3]: Models with lower tool diversity (Claude-3.5-Sonnet, Gemini-2.5-Flash) achieved higher Pos@4 rates in attribute updates.
  - [corpus]: No direct corpus support for precision-diversity trade-off in design modification.
- Break condition: If modification instructions are ambiguous or require multi-step cascading changes, precision alone may be insufficient.

### Mechanism 3
- Claim: Multi-turn agentic loops enable strategic, non-linear operations that single-turn execution cannot replicate.
- Mechanism: The ReAct loop (thought-action-observation) allows models to inspect intermediate states, correct errors, and perform non-linear operations (layer reordering, selective copying). Single-turn forces linear action sequences.
- Core assumption: Models can effectively use observation feedback to adjust subsequent actions.
- Evidence anchors:
  - [Ablation Study]: "Multi-turn setting encourages strategic, non-linear operations... while single-turn setting favors more uniform, linear action sequences."
  - [Table 5]: GPT-4.1 performs better under multi-turn; Gemini-2.5-Pro achieves highest scores in single-turn tool-based, suggesting model-specific optimal configurations.
  - [corpus]: No direct corpus comparison of multi-turn vs. single-turn in design tool contexts.
- Break condition: If observation feedback is delayed, noisy, or exceeds context window, the loop degrades.

## Foundational Learning

- Concept: **Figma's hierarchical node structure**
  - Why needed here: CANVAS represents designs as trees of nodes (frame, text, vector, etc.) with attributes. Understanding parent-child relationships, auto-layout propagation, and coordinate systems is essential for interpreting tool invocation results and debugging structural errors.
  - Quick check question: Given a frame with auto-layout enabled and three child text nodes, what happens to child positions if the frame's padding is increased by 20px?

- Concept: **ReAct agentic framework**
  - Why needed here: CANVAS implements a thought-action-observation loop. Understanding how models interleave reasoning traces with tool calls—and how observation feedback shapes subsequent actions—is critical for interpreting agent trajectories and failure modes.
  - Quick check question: In a ReAct loop, if an action fails (tool returns error), what information does the model receive before generating the next thought?

- Concept: **Hierarchical visual similarity metrics**
  - Why needed here: Evaluation decomposes into feature (SSIM), pattern (saliency), semantic (BLIP), and component-wise levels. Understanding what each captures helps diagnose where models fail (e.g., high SSIM but low component-wise suggests correct structure but misaligned elements).
  - Quick check question: A generated design has correct component positions but wrong text labels. Which metrics would score high vs. low?

## Architecture Onboarding

- Component map:
  - MCP Client -> MCP Server -> Socket Server -> Figma Plugin
  - Figma Plugin executes tool operations and returns structured JSON observations
  - ReAct Agent Loop orchestrates thought → action (tool invocation) → observation cycles
  - Evaluation Pipeline computes SSIM, saliency, BLIP, and component-wise similarity

- Critical path:
  1. Model receives task instruction + ground-truth image
  2. Model generates tool invocation via MCP Client
  3. Tool executes in Figma via Plugin
  4. Observation (structured JSON + canvas state) returns to model
  5. Repeat until model terminates (no further tool calls)
  6. Export final design as JSON and rasterized image
  7. Run four-metric evaluation against ground truth

- Design tradeoffs:
  - **Max turns (50) vs. early termination**: Higher turns allow more refinement but risk over-editing and metric degradation; termination logic is model-dependent
  - **Tool diversity vs. precision**: Replication benefits from strategic diversity; modification requires surgical precision—prompt design must bias accordingly
  - **Code-based vs. tool-based generation**: Code produces renderable output but lacks editability; tool-based produces Figma-native artifacts but requires multi-turn coordination

- Failure signatures:
  - **Geometric operation errors**: Incorrect component counts, incoherent vector paths, spatial misalignment (Figure 5)
  - **Layout operation errors**: Auto-layout parameter changes causing component disintegration or viewport overflow (Figure 6)
  - **Text operation errors**: Insufficient text component sizing causing overflow (Figure 7)
  - **Premature termination**: Model ends task before design matches target (common in lower-performing models)
  - **Tool hallucination**: Model invokes non-existent tools or malformed parameters (observed in Gemini-2.5-Flash: "MALFORMED FUNCTION CALL")

- First 3 experiments:
  1. Run baseline replication with temperature=0, max_turns=50 on 10 stratified samples across UI categories. Log turn-wise metric evolution to identify where performance plateaus or degrades.
  2. Ablate observation feedback: provide model with only success/failure status (not structured JSON). Compare final scores to full-observation baseline to quantify observation utility.
  3. Test prompt variants: (a) "Agency Principle" only, (b) "Figma Basics" only, (c) both. Measure impact on tool precision, diversity, and final scores in modification tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can intrinsic reward mechanisms be designed to encourage strategic, diverse tool use patterns in VLMs for UI design tasks?
- Basis in paper: [explicit] The paper states that "intrinsic rewards encouraging exploration could help models learn strategic, diverse tool patterns" after observing that higher-performing models exhibit modular approaches (e.g., creating once and propagating via copy) versus lower-performing models that create components sequentially.
- Why unresolved: Current VLMs were evaluated in a zero-shot setting without any reward-based training or fine-tuning on tool-based design tasks. The relationship between exploration rewards and strategic tool diversity remains untested.
- What evidence would resolve it: Training or fine-tuning VLMs with intrinsic rewards for tool diversity and measuring whether this improves replication task scores, particularly in achieving modular design strategies with fewer turns.

### Open Question 2
- Question: Can imitation learning from human designer tool sequences improve precision in modification tasks where excessive tool diversity currently degrades performance?
- Basis in paper: [explicit] The paper concludes that "methods that teach accurate skills, such as imitation learning, are necessary" after finding a negative correlation between tool diversity and Pos@K scores (ρ=−0.365) in modification tasks.
- Why unresolved: The paper evaluated only zero-shot model behavior. Human-annotated tool sequences were collected for evaluation but not used for training. Whether models can learn precise tool selection from demonstrations is untested.
- What evidence would resolve it: Fine-tuning VLMs on the human-annotated tool sequences collected in this work, then evaluating whether tool precision improves and modification task scores increase.

### Open Question 3
- Question: What architectural or training modifications are needed for open-source VLMs to reliably complete multi-turn tool invocation for design tasks?
- Basis in paper: [inferred] The paper excluded open-source models because they "commonly terminated after one or two turns, resulting in blank or incomplete outputs." This suggests fundamental limitations in agentic loop persistence or tool-calling capabilities that remain unexplained.
- Why unresolved: No analysis was conducted on why open-source models fail to maintain multi-turn execution. The paper provides no diagnostics on whether this is a context length, instruction-following, or tool-calling architecture issue.
- What evidence would resolve it: Controlled experiments isolating whether failure stems from context limitations, tool schema comprehension, or agentic reasoning; testing whether fine-tuning on multi-turn tool-use trajectories resolves the issue.

### Open Question 4
- Question: How can VLMs be improved to accurately predict auto-layout propagation effects and infer appropriate text component dimensions to prevent overflow errors?
- Basis in paper: [inferred] The error analysis identifies three failure patterns: geometric operation errors (miscount, irregular paths), layout operation errors (auto-layout adjustments causing disintegration or viewport overflow), and text operation errors (insufficient text component sizing causing overflow). These indicate fundamental gaps in spatial reasoning and layout prediction.
- Why unresolved: The paper documents these errors but provides no intervention. Auto-layout requires predicting cascading effects without explicit value specifications, and text overflow requires inferring dimensions from font attributes—both remain unaddressed challenges.
- What evidence would resolve it: Specialized training data with auto-layout state transitions and text dimension calculations, combined with architecture modifications for spatial reasoning; measuring reduction in these specific error categories on CANVAS.

## Limitations
- Tool execution fidelity: 50 Figma tools assumed deterministic, but complex operations may have non-linear side effects not captured in structured observations
- Metric sensitivity to design complexity: Hierarchical metrics may not fully account for design category-specific nuances, potentially biasing comparisons across task types
- Generalizability of findings: Results based on mobile UI designs; applicability to web, desktop, or specialized design domains remains untested

## Confidence
- **High confidence**: Gemini-2.5-Pro achieves highest replication scores; GPT-4.1 excels in modification tasks; multi-turn loops improve strategic operation diversity
- **Medium confidence**: Strategic tool diversity correlates with replication success; tool precision (not diversity) is critical for modification tasks; higher-performing models use more diverse tools
- **Low confidence**: Multi-turn vs. single-turn performance differences are model-specific; error patterns generalize across UI categories

## Next Checks
1. **Metric robustness test**: Run CANVAS with additional evaluation metrics (e.g., CLIP similarity, layout-aware SSIM) to verify that current metric hierarchies capture all failure modes
2. **Design category ablation**: Stratify results by UI category (e.g., social, productivity, e-commerce) to confirm that performance trends are not driven by category-specific design conventions
3. **Tool hallucination stress test**: Systematically invoke non-existent or malformed tools to measure model resilience and error recovery in both replication and modification tasks