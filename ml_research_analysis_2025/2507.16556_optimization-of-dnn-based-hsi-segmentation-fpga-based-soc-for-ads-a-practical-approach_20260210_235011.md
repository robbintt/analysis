---
ver: rpa2
title: 'Optimization of DNN-based HSI Segmentation FPGA-based SoC for ADS: A Practical
  Approach'
arxiv_id: '2507.16556'
source_url: https://arxiv.org/abs/2507.16556
tags:
- pruning
- segmentation
- data
- number
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive hardware/software co-design
  approach for optimizing a DNN-based HSI segmentation processor on FPGA-based SoCs
  for autonomous driving systems. The authors address the computational challenges
  of real-time HSI processing by implementing a complete pipeline that includes raw
  data preprocessing, memory optimization, and DNN inference.
---

# Optimization of DNN-based HSI Segmentation FPGA-based SoC for ADS: A Practical Approach

## Quick Facts
- arXiv ID: 2507.16556
- Source URL: https://arxiv.org/abs/2507.16556
- Authors: Jon GutiÃ©rrez-Zaballa; Koldo Basterretxea; Javier Echanobe
- Reference count: 40
- Primary result: Optimized HSI segmentation achieves 10.54 FPS on FPGA with 5.2W power consumption

## Executive Summary
This paper presents a comprehensive hardware/software co-design approach for optimizing a DNN-based HSI segmentation processor on FPGA-based SoCs for autonomous driving systems. The authors address the computational challenges of real-time HSI processing by implementing a complete pipeline that includes raw data preprocessing, memory optimization, and DNN inference. Key optimizations include hardware-aware preprocessing that converts raw 2D images to 3D cubes efficiently, and an iterative structured pruning methodology that combines static and dynamic analyses to compress the U-Net model by two orders of magnitude in parameters and one order of magnitude in operations. The deployment on AMD-Xilinx KV260 board achieves a 2.86x speed-up in inference while maintaining segmentation accuracy, with the final system reaching 10.54 FPS and reducing power consumption to 5.2W. The work demonstrates that aggressive compression techniques can be successfully applied to HSI processing without sacrificing performance, making embedded deployment practical for safety-critical applications.

## Method Summary
The authors implement a complete HSI segmentation pipeline on FPGA-based SoC that addresses the computational challenges of real-time processing. The approach combines hardware-aware preprocessing that efficiently converts raw 2D images to 3D hyperspectral cubes, with an iterative structured pruning methodology that uses both static analysis of weight distributions and dynamic analysis of activation magnitudes. The pruning process starts with a 6.3M parameter U-Net model and progressively eliminates redundant parameters through iterative cycles of analysis and fine-tuning, achieving compression ratios of up to 93.1% while maintaining 87.5% IoU accuracy. The deployment on AMD-Xilinx KV260 leverages a three-stage pipeline for data preprocessing, achieving a 2.86x speed-up in inference and 10.54 FPS throughput while consuming only 5.2W of power.

## Key Results
- 93.1% parameter reduction and 88.9% FLOPs reduction while maintaining 87.5% IoU accuracy
- 2.86x speed-up in inference time on FPGA compared to baseline
- 10.54 FPS throughput achieved with 5.2W power consumption on AMD-Xilinx KV260
- Raw data preprocessing bottleneck eliminated through optimized three-stage pipeline
- Real-time HSI segmentation demonstrated on embedded FPGA-based SoC for autonomous driving

## Why This Works (Mechanism)
The optimization succeeds through a synergistic hardware/software co-design approach that addresses the unique challenges of HSI processing. The structured pruning methodology effectively identifies and removes redundant parameters by combining static weight distribution analysis with dynamic activation magnitude assessment, allowing aggressive compression without significant accuracy loss. The hardware-aware preprocessing pipeline converts raw data to the required 3D format efficiently by leveraging the FPGA's parallel processing capabilities, eliminating the bottleneck that typically limits real-time performance. The three-stage pipeline architecture enables overlapping of data preprocessing and DNN inference, maximizing throughput while maintaining low power consumption suitable for embedded deployment.

## Foundational Learning
- Hyperspectral data representation (BSQ vs BIP formats): Needed to understand raw data preprocessing challenges and conversion requirements for DNN input.
- Structured pruning vs unstructured pruning: Required to appreciate why structured approaches are more suitable for hardware acceleration and maintain computational efficiency.
- FPGA-based SoC architecture (ARM Cortex-A53 + Programmable Logic): Essential for understanding the hardware/software partitioning and optimization opportunities.
- U-Net architecture and symmetric encoder-decoder design: Critical for understanding the baseline model and pruning targets.
- Real-time processing constraints in autonomous driving: Provides context for performance requirements and acceptable trade-offs.

## Architecture Onboarding

### Component Map
Raw HSI Sensor -> Preprocessing Pipeline (ARM Cortex-A53) -> Memory Optimization (DDR, BRAM) -> Pruned U-Net (Programmable Logic) -> Segmentation Output

### Critical Path
Data preprocessing (BSQ to BIP conversion + normalization) -> Memory transfer to FPGA fabric -> DNN inference on pruned U-Net -> Output post-processing

### Design Tradeoffs
Compression ratio vs accuracy (93.1% parameter reduction maintained 87.5% IoU), Hardware resources vs throughput (BRAM usage vs FPS), Preprocessing complexity vs inference speed (three-stage pipeline vs single-stage)

### Failure Signatures
Accuracy degradation in specific classes (Road Marks, Other) at high pruning ratios, Memory bandwidth bottlenecks during preprocessing, FPGA resource exhaustion limiting model size

### Three First Experiments
1. Measure preprocessing latency breakdown between BSQ-to-BIP conversion and normalization stages
2. Profile FPGA fabric utilization during pruned U-Net inference to identify resource bottlenecks
3. Test accuracy sensitivity to different pruning ratios across individual HSI classes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating raw hyperspectral data preprocessing directly into the DNN feature extraction module eliminate the data preparation bottleneck more effectively than the current software-based multi-stage approach?
- Basis in paper: The conclusion states that "further acceleration of raw image preprocessing is required, either through specialized hardware or by integrating this stage into the DNN feature extraction module."
- Why unresolved: The current implementation is limited by the ARM Cortex-A53 preprocessing speed. Even with a three-stage pipeline, data formatting (BSQ to BIP) and normalization remain significant bottlenecks that limit the achievable 10.54 FPS.
- What evidence would resolve it: A comparative analysis of latency and throughput between the current multi-threaded software pipeline and a proposed hardware-fused or DNN-integrated preprocessing layer.

### Open Question 2
- Question: Would exploring asymmetric encoder-decoder architectures with deeper encoders and lighter decoders yield better efficiency-accuracy trade-offs than pruning the symmetric U-Net?
- Basis in paper: In Section 4.2.5, the authors note that pruning results "suggest that it could be worth exploring asymmetric encoder-decoder architectures with deeper encoder branches and lighter decoder" rather than standard U-Nets.
- Why unresolved: The study focused on a U-Net where the decoder branch required roughly twice as many FLOPS as the encoder. While pruning reduced this, an inherently asymmetric architecture might handle the spectral feature extraction more efficiently.
- What evidence would resolve it: Performance metrics (IoU, GFLOPS, FPS) of an asymmetric model specifically designed for HSI segmentation benchmarked against the pruned U-Net on the HSI-Drive dataset.

### Open Question 3
- Question: Can a class-aware pruning strategy successfully prevent the accuracy degradation observed in sensitive classes (e.g., Road Marks) when high global pruning ratios are applied?
- Basis in paper: Section 4.2.3 discusses that high pruning ratios (0.8) caused severe degradation in specific classes like "Marks" and "Other." The authors suggest that "a class-aware pruning strategy could be employed" to mitigate this, but this approach was not implemented or tested.
- Why unresolved: The current iterative pruning uses a global weighted IoU metric, which fails to preserve feature representations for underrepresented or complex classes at extreme compression levels.
- What evidence would resolve it: Implementation of a pruning algorithm that utilizes per-class sensitivity analysis to determine layer-specific pruning ratios, resulting in maintained accuracy for sensitive classes compared to the global approach.

## Limitations
- Evaluation limited to single AMD-Xilinx KV260 platform, limiting generalizability across different FPGA architectures
- Power consumption measurements focus on inference only, without accounting for full system power including preprocessing overhead
- 10.54 FPS throughput remains below real-time requirements for high-speed autonomous driving scenarios
- Lacks comparison with alternative compression techniques like quantization-aware training or knowledge distillation

## Confidence

| Claim | Confidence |
|-------|------------|
| Hardware/software co-design methodology and FPGA deployment results | High |
| Pruning effectiveness across diverse HSI datasets | Medium |
| Cross-platform performance portability | Low |

## Next Checks
1. Evaluate the same optimization pipeline on alternative FPGA platforms (Intel, Lattice) to assess architectural portability
2. Compare structured pruning against quantization and distillation techniques on identical hardware constraints
3. Extend testing to include dynamic driving scenarios with varying HSI data rates and environmental conditions