---
ver: rpa2
title: 'Patterning: The Dual of Interpretability'
arxiv_id: '2601.13548'
source_url: https://arxiv.org/abs/2601.13548
tags:
- training
- data
- induction
- susceptibilities
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Patterning is the dual of interpretability: given a desired form
  of generalization, determine what training data produces it. Using susceptibilities
  to measure how posterior expectation values respond to data perturbations, we can
  invert the linear response relationship to derive principled data interventions.'
---

# Patterning: The Dual of Interpretability

## Quick Facts
- **arXiv ID:** 2601.13548
- **Source URL:** https://arxiv.org/abs/2601.13548
- **Reference count:** 40
- **Primary result:** Patterning inverts interpretability by using susceptibilities to derive principled data interventions that can accelerate circuit formation or select between competing algorithms

## Executive Summary
Patterning is presented as the dual of interpretability: where interpretability seeks to understand what structures emerge from training data, patterning seeks to determine what training data produces desired structures. The framework uses susceptibilities—measuring how posterior expectation values respond to data perturbations—to compute principled data interventions. By inverting the linear response relationship, the authors derive minimum-norm data re-weightings that achieve specific structural changes. Experiments demonstrate this approach can accelerate or delay the formation of the induction circuit in a 3M parameter language model and select between competing algorithms in a synthetic parentheses balancing task by targeting their local learning coefficients.

## Method Summary
The patterning framework computes data interventions by first estimating susceptibilities through covariance-based methods using SGLD sampling from the annealed posterior. The susceptibility matrix χ captures how structural coordinates respond to infinitesimal data distribution shifts. Through SVD decomposition, principal structures and corresponding data patterns are identified. The fundamental equation dh_opt = χ† dµ_target provides the minimum-norm intervention to achieve a desired change in structural coordinates. This intervention vector specifies per-sample re-weighting factors applied during retraining. The approach leverages Bayesian posterior concentration and free energy minimization to connect structural changes to algorithm selection via local learning coefficients.

## Key Results
- In a 3M parameter language model, re-weighting training data along the second principal susceptibility component accelerated induction circuit formation by 4× or suppressed it to near zero
- In a synthetic parentheses balancing task, susceptibility-guided re-weighting selected between two competing algorithms by targeting their local learning coefficients
- The same mathematical framework used to read internal structure can be inverted to write it through principled data interventions

## Why This Works (Mechanism)

### Mechanism 1: Susceptibility-Based Linear Response Inversion
- **Claim:** Given a target change in structural coordinates, the minimum-norm data intervention can be computed by inverting the susceptibility matrix
- **Mechanism:** Susceptibilities χ measure how posterior expectation values µ∞ respond to infinitesimal data distribution shifts (dµ∞ = χ dh). The Moore-Penrose pseudoinverse yields dh_opt = χ† dµ_target, converting desired structural changes into concrete data re-weightings
- **Core assumption:** The linear response approximation holds for the intervention magnitude used
- **Evidence anchors:** Abstract statement on inverting linear response; Section 2.3 on SVD decomposition of χ†
- **Break condition:** Large interventions where higher-order terms become significant

### Mechanism 2: Internal Model Selection via Local Learning Coefficients
- **Claim:** When multiple algorithms achieve equal training loss, patterning can select between them by differentially modifying their Local Learning Coefficients
- **Mechanism:** The posterior log-odds between two solutions is governed by ∆L_n · n + ∆λ · log n. When losses are equal, preference is determined by LLC difference (lower LLC = higher posterior weight)
- **Core assumption:** SGD training approximates Bayesian posterior concentration
- **Evidence anchors:** Abstract on LLC-based algorithm selection; Section 4.5 on parentheses task results
- **Break condition:** Solutions with different training losses

### Mechanism 3: Principal Mode Coupling for Circuit Modulation
- **Claim:** The second principal component of the susceptibility matrix couples induction patterns in data to the induction circuit in weights
- **Mechanism:** SVD of χ reveals orthonormal pairs (uα, vα) where uα represents principal structures and vα represents principal data patterns. Targeting a single mode yields a simple intervention
- **Core assumption:** Principal components correspond to interpretable circuits and data patterns
- **Evidence anchors:** Section 3 on PC2 coupling; Figure 3 on induction circuit modulation
- **Break condition:** Circuits that don't cleanly separate into principal components

## Foundational Learning

- **Concept: Bayesian posterior and free energy**
  - Why needed here: The framework assumes structure is encoded in the posterior distribution; understanding posterior concentration is essential
  - Quick check question: How does the free energy formula F_n(U) = nL_n(w*) + λ(w*) log n determine which solution the posterior favors?

- **Concept: Covariance-based susceptibility estimation**
  - Why needed here: Susceptibilities are estimated via χ^C_xy = -Cov_β[ϕ_C, ℓ_xy(w) - L(w)], not direct perturbation
  - Quick check question: Why does the covariance formula capture how up-weighting sample (x,y) affects a structural coordinate?

- **Concept: Stochastic Gradient Langevin Dynamics (SGLD)**
  - Why needed here: LLC and susceptibility estimation require sampling from the annealed posterior
  - Quick check question: How do the hyperparameters (nβ, γ, ε) trade off between sampling accuracy and computational cost?

## Architecture Onboarding

- **Component map:** Reference model -> Susceptibility matrix χ -> SVD decomposition -> Intervention vector dh_opt -> Retrained models

- **Critical path:**
  1. Train reference model on original distribution
  2. Define observables ϕ_C for components of interest
  3. Estimate susceptibilities via SGLD sampling
  4. Compute SVD to identify principal modes
  5. Specify target change dµ_target
  6. Apply fundamental equation to derive dh_opt
  7. Retrain with modified per-sample weights

- **Design tradeoffs:**
  - Component selection: More components = richer structure capture but higher compute
  - SGLD hyperparameters: Lower ε and more draws improve accuracy but increase cost (40m forward passes vs 15m training)
  - Intervention magnitude: Larger interventions have stronger effects but may break linear approximation

- **Failure signatures:**
  - Unstable SGLD: χ values explode or oscillate → reduce ε, increase γ
  - No clear PC structure: Explained variance concentrated in PC1 → components may not be differentiated
  - Intervention has no effect: Target may be in null space of χ, or intervention too small
  - Collateral damage: Non-targeted patterns affected → check per-pattern susceptibilities post-hoc

- **First 3 experiments:**
  1. **Sanity check:** Replicate spacing fin suppression on small model (modify data to remove consecutive spaces, verify fin disappears in UMAP)
  2. **Single-circuit modulation:** On 2-layer attention-only transformer, compute susceptibility matrix, extract PC2, test Repress vs Induce conditions on induction head formation
  3. **Binary algorithm selection:** On parentheses task with known Nested/Equal-Count solutions, measure susceptibilities at both solution types, identify high-gap samples, verify re-training shifts OOD accuracy distribution

## Open Questions the Paper Calls Out

- How do the methods scale to larger models and more complex structures?
- How can we compute Local Learning Coefficients efficiently for larger models?
- How do we design good structural coordinates for arbitrary model components?

## Limitations

- The linear response approximation may break down for larger interventions, though experiments suggest it holds for tested magnitudes
- Computational cost is prohibitive for large-scale applications (40m forward passes per checkpoint vs 15m for training)
- LLC-based algorithm selection lacks direct empirical validation and assumes SGD approximates Bayesian posterior concentration

## Confidence

**High Confidence Claims:**
- The mathematical framework for computing data interventions via susceptibility inversion is sound
- PC2 of the susceptibility matrix couples induction patterns to the induction circuit
- The 4× weight modification successfully accelerates induction circuit formation

**Medium Confidence Claims:**
- The linear response approximation holds for the intervention magnitudes tested
- Susceptibility-guided re-weighting can select between competing algorithms
- The framework generalizes beyond specific circuits and tasks studied

**Low Confidence Claims:**
- The framework will scale efficiently to larger models
- LLC differences are dominant in algorithm selection when training losses differ
- Principal components will consistently correspond to interpretable circuits across architectures

## Next Checks

1. **Linear Response Verification:** Systematically test intervention magnitudes from 1× to 10× weight modifications on the induction circuit, measuring prefix matching scores and computing residuals between observed changes and linear predictions.

2. **LLC Direct Measurement:** In the parentheses task, directly compute Local Learning Coefficients for both Nested and Equal-Count solutions before and after patterning interventions, comparing LLC differences to observed shifts in OOD accuracy distributions.

3. **Multi-Mode Intervention:** Design interventions targeting PC3 and higher components in the language model susceptibility matrix, characterizing what circuits or patterns these components represent and testing whether interventions produce predictable, interpretable changes.