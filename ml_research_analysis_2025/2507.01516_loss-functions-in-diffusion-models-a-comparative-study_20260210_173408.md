---
ver: rpa2
title: 'Loss Functions in Diffusion Models: A Comparative Study'
arxiv_id: '2507.01516'
source_url: https://arxiv.org/abs/2507.01516
tags:
- loss
- diffusion
- data
- nelbo
- weighted
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper compares different loss functions for diffusion models\
  \ by unifying them under the variational lower bound (ELBO) framework. The authors\
  \ derive loss formulations for four target predictions: data x, noise \u03F5, rate\
  \ of change v, and score s, and show their mathematical relationships."
---

# Loss Functions in Diffusion Models: A Comparative Study

## Quick Facts
- arXiv ID: 2507.01516
- Source URL: https://arxiv.org/abs/2507.01516
- Authors: Dibyanshu Kumar; Philipp Vaeth; Magda GregorovÃ¡
- Reference count: 40
- One-line primary result: Compares four loss function formulations for diffusion models using ELBO framework, finding that weighted losses in $\epsilon$ and $v$-spaces produce better sample quality while x-space NELBO provides best likelihood estimation.

## Executive Summary
This paper systematically compares loss functions for diffusion models by unifying them under the variational lower bound (ELBO) framework. The authors derive loss formulations for four target predictions: data $x$, noise $\epsilon$, velocity $v$, and score $s$, showing their mathematical relationships through SNR scaling. Through experiments on 2D synthetic datasets and CIFAR-10 images, they demonstrate that while these formulations are theoretically equivalent, practical performance differs significantly. The study reveals that better likelihood estimation does not necessarily correlate with better sample generation quality.

## Method Summary
The study compares four loss formulations (NELBO and Weighted variants) across four target predictions (x, $\epsilon$, v, s) using a variance-preserving cosine noise schedule. 2D experiments use a 7-layer fully connected network on synthetic datasets (Cluster, Ring, Swiss roll, Waves), while CIFAR-10 experiments use a Diffusers-inspired UNet. The theoretical framework derives equivalence relationships through SNR scaling, and empirical validation measures NELBO values and sample quality via FID scores and covariance metrics.

## Key Results
- NELBO formulation in x-space provides best likelihood estimation for both 2D and image data
- Weighted loss formulations in $\epsilon$ and $v$-spaces achieve better FID scores for CIFAR-10 images
- Sample quality and likelihood estimation are decoupled objectives with distinct optimal loss configurations
- Loss convergence patterns vary significantly across formulations, with x/s-space showing distinct dynamics from $\epsilon$/v-space

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Different diffusion targets ($x, \epsilon, v, s$) can be unified under a single Evidence Lower Bound (ELBO) framework, where they act as mathematically equivalent re-parameterizations.
- **Mechanism:** The paper derives that minimizing the Negative ELBO (NELBO) for noise prediction ($\epsilon$), velocity ($v$), or score ($s$) is functionally identical to data prediction ($x$) up to a Signal-to-Noise Ratio (SNR) scaling factor. This allows theoretical guarantees from one formulation to apply to others.
- **Core assumption:** The forward diffusion process is Gaussian, allowing closed-form transitions and KL divergence calculations.
- **Evidence anchors:**
  - [Section 3.5] "All the NELBO loss formulations across different parameter spaces are derived from same equation (7), therefore, they are fundamentally equivalent."
  - [Abstract] "...unifying them under the framework of the variational lower bound objective."
  - [Corpus] Weak direct link; corpus neighbor *Rectifying Regression in Reinforcement Learning* discusses loss impact on objectives in RL, paralleling the re-parameterization discussion here but in a different domain.
- **Break condition:** If the noise schedule is learned or non-Gaussian, the closed-form equivalences derived in Section 3 may not hold.

### Mechanism 2
- **Claim:** Practical performance differences between formulations are driven by the implicit weighting of timesteps during gradient descent, not the theoretical objective itself.
- **Mechanism:** While NELBO formulations are theoretically equivalent, they apply different SNR-dependent weights ($w(t)$) to the loss at different timesteps. Standard "weighted" losses (e.g., simple MSE in $\epsilon$-space) remove these scaling factors, effectively equalizing the contribution of clean vs. noisy timesteps, which stabilizes training dynamics compared to the heavily scaled NELBO.
- **Core assumption:** Neural networks struggle to optimize objectives with extreme dynamic range (very high/low SNR scaling) across timesteps.
- **Evidence anchors:**
  - [Section 4.2] "We attribute these differences to the different SNR scaling... excessively high scaling at early timesteps... may negatively impact the model's overall likelihood performance."
  - [Figure 4] Shows distinct SNR scaling curves for $x$/$s$ vs $\epsilon$/$v$ spaces.
  - [Corpus] No direct corpus validation for this specific SNR-weighting dynamic.
- **Break condition:** If an architecture is specifically tuned to handle extreme loss scaling (e.g., per-timestep learning rates), the practical divergence between NELBO and weighted losses might diminish.

### Mechanism 3
- **Claim:** Sample quality (perceptual fidelity) and likelihood estimation (density coverage) are decoupled objectives that require distinct loss configurations.
- **Mechanism:** The paper observes that x-space NELBO minimizes density error (best likelihood), but weighted $\epsilon/v$-space losses produce better FID scores (better samples). Assumption: Weighting schemes that ignore strict likelihood bounds (like standard MSE) allow the model to prioritize "perceptual" modes over strict density coverage.
- **Core assumption:** High-fidelity sample generation does not require perfect density estimation.
- **Evidence anchors:**
  - [Section 4.3] "NELBO formulation in x-space has the best performance both in sample quality and probability density estimation [for 2D]... For $\epsilon$ and $v$ space we found that the weighted loss formulation has better FID scores [for CIFAR-10]."
  - [Abstract] "...better likelihood estimation does not necessarily correlate with better sample generation."
  - [Corpus] *Generative Adversarial Networks...* mentions GAN capabilities, implicitly contrasting with diffusion's likelihood focus, but offers no direct mechanism validation.
- **Break condition:** This decoupling may vary by dataset dimensionality; the paper notes x-space was competitive for 2D but dominated by weighted losses for images (CIFAR-10).

## Foundational Learning

- **Concept:** **Variational Inference & ELBO**
  - **Why needed here:** The paper unifies all loss functions using the Negative ELBO. Without this, the equivalence between predicting noise ($\epsilon$) and data ($x$) appears magical rather than algebraic.
  - **Quick check question:** Why does minimizing the KL divergence between the reverse process and forward process result in a tractable "diffusion loss" term?

- **Concept:** **Signal-to-Noise Ratio (SNR) Scheduling**
  - **Why needed here:** The "weighting" mechanism hinges on how $\alpha_t$ and $\sigma_t$ change over time. You cannot understand why $\epsilon$-space behaves differently from $x$-space without tracking SNR($t$).
  - **Quick check question:** In a cosine schedule, does SNR increase or decrease as $t \rightarrow 1$, and how does that affect the "hardness" of the prediction task?

- **Concept:** **Tweedie's Formula / Score Function**
  - **Why needed here:** This bridges the gap between "score matching" ($s$-space) and standard denoising ($x$/$\epsilon$-space), showing they are just scaled versions of the gradient of the log-density.
  - **Quick check question:** How does the magnitude of the score ($\nabla \log q$) change as the noise variance $\sigma^2_t$ approaches zero?

## Architecture Onboarding

- **Component map:** Forward Encoder (adds noise via schedule) -> Denoiser (UNet/MLP takes noisy $z_t$, outputs prediction) -> Loss Head (applies weighting $w(t)$ to prediction error)

- **Critical path:** Selecting the Target Space ($\epsilon$ is standard, $v$ for distillation, $x$ for likelihood) -> Mapping to Loss Formulation (NELBO for theory, Weighted for practice) -> Configuring Noise Schedule (Cosine recommended)

- **Design tradeoffs:**
  - **x-space:** Best for density estimation; mathematically cleanest. *Risk:* Unstable at high noise ($t \rightarrow 1$) due to SNR scaling.
  - **$\epsilon$-space:** Robust standard choice. *Risk:* Poor sample quality with few sampling steps.
  - **v-space:** Good for few-step sampling (distillation). *Risk:* Intermediate complexity without raw likelihood benefits.
  - **s-space:** Theoretically powerful. *Risk:* Numerically unstable at $t \approx 0$ (low noise) due to score magnitude explosion.

- **Failure signatures:**
  - **Mode Collapse / Low Diversity:** Check if NELBO x-space is being used on high-dimensional data without proper calibration; it may over-prioritize likelihood over mode coverage.
  - **Training Divergence (Early Steps):** If using $s$-space, check for exploding gradients at low noise levels (high SNR).
  - **Slow Convergence:** If using NELBO $\epsilon$/$v$, check if the high SNR weighting at early timesteps is dominating the gradient updates.

- **First 3 experiments:**
  1. **Sanity Check (2D):** Implement x-space and $\epsilon$-space on a 2D "Ring" dataset. Verify that NELBO losses differ in value but Rescaled Loss curves match (confirming equivalence).
  2. **Image Likelihood vs. Quality:** Train on CIFAR-10 with weighted $\epsilon$-loss vs. NELBO x-loss. Compare NELBO values (x should win) vs. FID scores (weighted $\epsilon$ should win).
  3. **Sampling Efficiency:** Generate samples using a v-space model vs. $\epsilon$-space model using limited steps (e.g., 10-50 steps). Verify if v-space retains structure better than $\epsilon$-space.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do score-based (s-space) loss formulations compare to x, $\epsilon$, and v-spaces in high-dimensional image generation?
- **Basis in paper:** [explicit] Section 4.3 states that evaluating score-based metrics for continuous-time diffusion models on high-dimensional image space is "beyond the scope of this study and left for future research."
- **Why unresolved:** Accurately computing these metrics requires modeling reverse and forward Stochastic Differential Equations (SDEs), which adds significant complexity not addressed in the current experiments.
- **What evidence would resolve it:** An empirical comparison of FID and NELBO scores for s-space models against other targets on datasets like CIFAR-100 or ImageNet.

### Open Question 2
- **Question:** What is the theoretical mechanism explaining why better likelihood estimation does not correlate with better sample generation quality?
- **Basis in paper:** [inferred] The paper notes in Section 4.3 that weighted losses in $\epsilon$ and v-spaces yield better FID scores despite lower likelihood estimates, describing this as a discrepancy where accurate density estimation "does not necessarily correspond" to quality.
- **Why unresolved:** The authors observe and quantify the trade-off but attribute the divergence to empirical factors and SNR scaling rather than a derived theoretical cause.
- **What evidence would resolve it:** A theoretical analysis linking loss weighting to the spectral properties of generated samples or specific perceptual metrics.

### Open Question 3
- **Question:** Can a dynamic weighting strategy be developed to combine the likelihood benefits of x-space with the sample quality benefits of $\epsilon$-space?
- **Basis in paper:** [inferred] The Conclusion states the work establishes a foundation for "optimizing training objectives," and Section 3.5 notes that weighted losses are not equivalent due to specific scaling factors.
- **Why unresolved:** The study evaluates fixed loss formulations independently; it does not explore time-dependent or adaptive weights that might bridge the performance gap between targets.
- **What evidence would resolve it:** Experiments using a curriculum-based loss that transitions between x-space (early training) and $\epsilon$-space (later training) to optimize both likelihood and FID.

## Limitations
- Theoretical equivalence derivations assume Gaussian forward process, limiting applicability to learned or non-Gaussian diffusion processes
- Empirical findings rely on controlled synthetic experiments and a single architecture for CIFAR-10 validation
- 2D convergence analysis may not fully capture high-dimensional sample quality differences
- Study does not investigate impact of different network architectures or hyperparameter settings

## Confidence
- **High Confidence:** The mathematical derivation of loss function equivalences under the ELBO framework (Section 3) and the theoretical relationship between target predictions via SNR scaling are well-supported by the proofs and equations provided.
- **Medium Confidence:** The empirical findings showing weighted losses producing better FID scores than NELBO formulations for image generation are reasonably supported by CIFAR-10 experiments, though limited to a single architecture and dataset.
- **Low Confidence:** The claim about score-space training being numerically unstable at early timesteps is stated but not extensively validated with concrete failure cases or mitigation strategies.

## Next Checks
1. **Cross-Architecture Validation:** Replicate the CIFAR-10 experiments using different UNet variants (e.g., different channel sizes, attention configurations) to verify that weighted loss advantages persist across architectural choices.
2. **Schedule Sensitivity Analysis:** Test the loss function behaviors under different noise schedules (linear, quadratic, learned) to determine if the SNR-based weighting effects are schedule-dependent or more general phenomena.
3. **High-Dimensional Generalization:** Extend the synthetic experiments beyond 2D to moderate-dimensional datasets (e.g., 64x64 images) to better understand how the x-space likelihood-quality tradeoff manifests in higher dimensions before scaling to full-resolution images.