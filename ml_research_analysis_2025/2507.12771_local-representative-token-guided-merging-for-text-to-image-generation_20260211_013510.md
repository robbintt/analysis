---
ver: rpa2
title: Local Representative Token Guided Merging for Text-to-Image Generation
arxiv_id: '2507.12771'
source_url: https://arxiv.org/abs/2507.12771
tags:
- token
- merging
- window
- tokens
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes local representative token guided merging (ReToM),
  a novel token merging strategy for text-to-image generation. The method addresses
  the inefficiency of stable diffusion models due to quadratic complexity of attention
  operations.
---

# Local Representative Token Guided Merging for Text-to-Image Generation

## Quick Facts
- arXiv ID: 2507.12771
- Source URL: https://arxiv.org/abs/2507.12771
- Reference count: 34
- Key outcome: 6.2% FID improvement (37.02→34.89) over baseline stable diffusion with layer-adaptive window sizing and representative token selection

## Executive Summary
This paper introduces ReToM (Representative Token guided Merging), a token merging strategy that addresses the quadratic complexity of attention operations in stable diffusion models. The method employs adaptive window sizing based on U-Net layer characteristics, selecting the most representative token within each window based on average similarity to preserve salient features. By caching similarity computations across timesteps, ReToM achieves better visual quality (FID 34.89, CLIP 39.40) compared to baseline while maintaining comparable inference speed (2.17 s/im), all without requiring additional model training.

## Method Summary
ReToM is a training-free token merging technique applied within U-Net transformer blocks during the attention computation. It divides attention input tokens into windows with sizes adapted to layer type—smaller windows for down/up sampling blocks where local detail matters, larger windows for bottleneck layers capturing global context. Within each window, the token with highest average cosine similarity to others is selected as the representative destination token. Top-r similar source tokens are then merged into it using a weighted average controlled by parameter α. Similarity computations are cached and reused across timesteps to reduce overhead, exploiting the gradual evolution of latent representations during diffusion.

## Key Results
- FID improves from 37.02 to 34.89 (6.2% gain) on ImageNet validation set
- CLIP score increases to 39.40 compared to baseline
- Generation speed maintained at 2.17 seconds per image
- Outperforms ToMeSD baseline in visual quality while keeping comparable efficiency

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Window Sizing Based on U-Net Layer Characteristics
Different U-Net layers have fundamentally different representational roles—early/later layers need fine-grained local features, while middle layers capture global structure. ReToM assigns small windows to down/up sampling blocks where local detail preservation is critical, and larger windows to bottleneck layers where global context is prioritized. This prevents over-merging in detail-critical regions while enabling efficient reduction in context-heavy layers.

### Mechanism 2: Representative Token Selection via Local Similarity Ranking
For each window, pairwise cosine similarity is computed among all tokens. The row-wise average similarity for each token is calculated, and the token with the highest average similarity is designated as the representative (destination) token. Top-r similar source tokens are then merged into it using a weighted average, with parameter α controlling the balance between representative and source tokens. This preserves structural coherence better than random selection.

### Mechanism 3: Similarity Computation Caching Across Timesteps
Cosine similarity is computed only every p timesteps, with results cached and reused for representative token selection in intervening timesteps. This exploits the gradual evolution of latent representations across the diffusion process, reducing computational overhead without significantly impacting merging quality.

## Foundational Learning

- **Quadratic Complexity of Self-Attention**: Understanding why attention operations scale as O(N²) with token count motivates the need for token reduction strategies. Quick check: If an attention input has 1024 tokens, approximately how many pairwise computations are required? (Answer: ~1 million)

- **U-Net Architecture and Receptive Fields**: ReToM's adaptive window sizing depends on understanding that different U-Net layers have different effective receptive fields requirements. Quick check: In a U-Net, which layer type would you expect to benefit more from a larger merging window—the bottleneck or the first encoder layer? (Answer: Bottleneck, due to global context focus)

- **Diffusion Timestep Dynamics**: The similarity caching mechanism relies on the assumption that latent representations change gradually across timesteps. Understanding the iterative denoising process is essential for evaluating this assumption. Quick check: In a diffusion model with 50 steps, does the latent representation change more dramatically between steps 1-2 or steps 49-50? (Answer: Generally 1-2, as early steps remove large noise components)

## Architecture Onboarding

- **Component map**:
U-Net Transformer Block -> Attention Input Tokens (N tokens) -> Divided into Windows W ∈ W (adaptive sizes by layer) -> Cosine Similarity Matrix (cached every p timesteps) -> Average Similarity per Token → Representative Token D -> Top-r Source Tokens S (by similarity to D) -> Merged Token: x_merged = αD + (1-α) · mean(S) -> Reduced token set passed to attention

- **Critical path**:
1. Timestep check: If t mod p = 0, compute and cache similarities; else use cache
2. Window partitioning: Divide tokens into windows based on layer-specific size
3. Representative selection: For each window, compute average similarity and select top token
4. Source selection: Select top-r similar tokens per window based on merge ratio R
5. Merge operation: Apply weighted merge with parameter α

- **Design tradeoffs**:
- Window size vs. granularity: Smaller windows preserve more detail but offer less computational reduction
- Merge ratio (R) vs. quality: Higher R reduces tokens more aggressively but may lose information
- Cache period (p) vs. freshness: Longer p reduces overhead but risks stale similarities
- α weight vs. feature dominance: Higher α prioritizes representative token; lower α averages more with sources

- **Failure signatures**:
- Object distortion/shape artifacts: Indicates over-merging in detail-critical layers (window size too large)
- Background incoherence: Suggests insufficient global context capture (bottleneck windows too small)
- Sudden quality drop at specific timesteps: May indicate cache period p is too long
- Worse FID than baseline: Random representative selection or inappropriate merge ratio

- **First 3 experiments**:
1. Window size ablation on fixed-size baseline: Test fixed window sizes (2, 8, 16) across all layers to establish baseline sensitivity
2. Representative vs. random destination token comparison: Compare FID/SSIM between highest-average-similarity selection and random selection
3. Cache period sensitivity analysis: Test different values of p (1, 5, 10, 25 timesteps) to quantify tradeoff between computational savings and quality degradation

## Open Questions the Paper Calls Out
- How does ReToM perform when generalized to other attention-based architectures, such as video diffusion models or large language models?
- Can alternative similarity metrics outperform the fixed cosine similarity used for selecting representative tokens?
- Does a dynamic merging strategy, where the merging ratio or window size adapts per timestep, improve upon the current static configuration?
- Can the computational overhead of representative token selection be reduced to strictly surpass ToMeSD in inference speed?

## Limitations
- The paper claims a 6.2% FID improvement but does not report ToMeSD baseline results with identical evaluation settings
- Specific window sizes per U-Net layer and key hyperparameters (α, merging ratio R, cache period p) lack ablation studies
- The 20-timestep cache period appears arbitrary without analysis of cache staleness effects

## Confidence
- **High Confidence**: Mathematical formulation of representative token selection via average similarity is well-defined and reproducible
- **Medium Confidence**: Claim of 6.2% FID improvement relative to baseline requires verification against ToMeSD under identical conditions
- **Low Confidence**: Similarity caching mechanism's impact on quality is poorly characterized without timestep-specific analysis

## Next Checks
1. Implement ToMeSD with identical window sizes to ReToM's adaptive configuration and evaluate both methods on the same ImageNet validation split to isolate the contribution of representative token selection versus window sizing

2. Systematically vary the cache period p (1, 5, 10, 20, 50 timesteps) and measure FID/CLIP at each value to quantify the tradeoff between computational overhead and quality degradation

3. Fix ReToM's representative token selection mechanism and test fixed window sizes (2, 4, 8, 16) independently for each U-Net layer type to determine whether adaptive sizing provides measurable benefits over optimized fixed configurations