---
ver: rpa2
title: 'V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions'
arxiv_id: '2512.11995'
source_url: https://arxiv.org/abs/2512.11995
tags:
- reasoning
- question
- final
- answer
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: V-REX benchmarks exploratory visual reasoning by disentangling
  planning and following abilities in multi-step Chain-of-Questions. Experiments show
  that larger VLMs improve consistently across both dimensions, with strong positive
  correlations to final task performance.
---

# V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions

## Quick Facts
- arXiv ID: 2512.11995
- Source URL: https://arxiv.org/abs/2512.11995
- Reference count: 40
- Primary result: V-REX disentangles VLM planning and following abilities in multi-step Chain-of-Questions, showing larger models achieve balanced capabilities while smaller models excel at following but lag in planning

## Executive Summary
V-REX introduces a benchmark for evaluating exploratory visual reasoning in vision-language models (VLMs) by disentangling their abilities to plan (select informative sub-questions) and follow (answer sub-questions sequentially) through Chain-of-Questions. The benchmark reformulates evaluation into a tractable multiple-choice format with curated question/answer options at each step, enabling fine-grained analysis of intermediate reasoning abilities. Experiments across 32 VLMs (1B-38B+ parameters) reveal that larger models improve consistently across both planning and following dimensions, with strong positive correlations to final task performance, while smaller models show a pronounced gap between their following and planning abilities.

## Method Summary
V-REX evaluates VLMs through a Chain-of-Questions (CoQ) framework that separates planning (selecting the next most helpful question) from following (answering curated questions). The evaluation uses human-curated ground-truth QA chains with 2-6 reasoning steps per sample, transformed into multiple-choice questions with distractors generated by GPT-5 and filtered using Qwen3-VL-32B-Instruct. Models are evaluated under three conditions: without CoQ (direct answering), Planning mode (selecting questions from candidates while receiving ground-truth answers as feedback), and Following mode (answering ground-truth questions from answer candidates). The benchmark includes 702 samples across 4 categories (Deduction, Guessing, Navigation, Retrieval) and 15 scenarios, with intermediate accuracy metrics computed separately for planning and following abilities.

## Key Results
- Larger VLMs (GPT-4V, Gemini) consistently improve across both planning and following dimensions, with strong positive correlation to final task performance
- Smaller models (<10B parameters) excel at following ability but significantly lag in planning ability, showing a 12-19% gap
- VLMs recover more robustly from planning failures (65-85%) than following failures (45-63%) due to error propagation differences
- Retrieval tasks show minimal benefit from CoQ structuring, with performance sometimes degrading due to conflict with model's native retrieval strategy

## Why This Works (Mechanism)

### Mechanism 1: Planning-Following Disentanglement
Separating reasoning into Planning (question selection) and Following (answer generation) enables fine-grained capability diagnosis that end-to-end evaluation obscures. The CoQ format presents a QA chain where Planning is isolated by providing ground-truth answers at each step (model only selects questions), while Following is isolated by providing ground-truth questions (model only answers). This prevents conflation of error sources. Break condition: If ground-truth chains don't satisfy the "helpfulness" and "correct ordering" constraints, isolation fails and measurements conflate multiple abilities.

### Mechanism 2: Finite Exploration Space via MCQ Constrained Selection
Reducing the infinite exploration space to finite multiple-choice options at each step enables reliable, reproducible intermediate-step evaluation. Instead of free-form generation, models select from curated question/answer options including distractors. This transforms an open-ended evaluation problem into a tractable classification problem per step. Break condition: If distractors are either too obvious (floor effect) or indistinguishable (ceiling effect), discriminative power is lost.

### Mechanism 3: Recovery Asymmetry Between Planning and Following Failures
VLMs recover more robustly from Planning failures (wrong questions) than Following failures (wrong answers), because incorrect answers propagate misinformation while incorrect questions merely provide less useful context. In Planning mode, selecting a distractor question provides less informative context but doesn't introduce false premises. In Following mode, an incorrect answer introduces false information that corrupts downstream reasoning. Break condition: If distractor questions sometimes contain useful information (violating the "unhelpful" constraint), Planning recovery rates would be artificially inflated.

## Foundational Learning

- **Chain-of-Thought (CoT) Reasoning**
  - Why needed here: CoQ extends CoT from answer-space reasoning to question-space exploration. Without understanding CoT, the distinction between Planning and Following will be unclear.
  - Quick check question: Can you explain why intermediate reasoning steps might improve final accuracy even when the model could answer directly?

- **Exploratory vs Verificatory Reasoning**
  - Why needed here: V-REX targets exploratory reasoning where the solution path isn't known in advance, unlike verificatory reasoning on math problems with defined procedures.
  - Quick check question: What makes "guess the location from a street view" exploratory rather than verificatory?

- **Vision-Language Model Architecture Basics**
  - Why needed here: Understanding how VLMs process images and text jointly is necessary to interpret why Following ability converges faster than Planning ability with scale.
  - Quick check question: Why might a VLM's visual perception capabilities improve faster than its strategic planning capabilities as model size increases?

## Architecture Onboarding

- **Component map:**
  Ground-Truth QA Chain -> Planning Task -> Following Task -> Distractor Generation -> Evaluation Metrics

- **Critical path:**
  1. Human annotators construct ground-truth QA chains with cross-verification
  2. GPT-5 generates step-level and chain-level distractors
  3. Qwen3-VL-32B-Instruct identifies most confusing distractor chains
  4. Final dataset integrates ground-truth + selected distractors into MCQ format
  5. Models evaluated under three conditions: w/o CoQ, Planning, Following

- **Design tradeoffs:**
  - Finite MCQ space vs open-ended generation: Trades ecological validity for reliability
  - Human-curated chains vs model-generated: Trades scalability for quality control
  - Ground-truth answer injection in Planning: Isolates ability but removes error propagation study

- **Failure signatures:**
  - Retrieval category shows minimal CoQ benefit: Tasks dominated by direct matching don't benefit from sequential reasoning
  - Mismatch between human and model reasoning paths: Human chains may conflict with model's native strategy
  - Performance degradation with CoQ for some models: Intermediate steps introduce cognitive noise

- **First 3 experiments:**
  1. **Baseline establishment:** Run your VLM on V-REX under all three conditions to establish whether it shows the expected scaling pattern and Planning-Following gap.
  2. **Error propagation analysis:** Inject controlled errors at specific steps in Following mode to measure recovery rates; compare against Planning mode recovery to validate the asymmetry finding.
  3. **Retrieval vs Deduction breakdown:** Compare performance across the four reasoning categories to identify whether your model's failure modes align with V-REX's finding that Retrieval tasks benefit least from CoQ structuring.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific training interventions or data compositions are required to close the performance gap between Planning and Following abilities in smaller VLMs (<10B parameters)?
- Basis in paper: Finding 4 notes that smaller models excel at Following but significantly lag in Planning, while Section 5.2 explicitly states this "underscores the critical role of Planning as a distinguishing factor" and highlights a need to improve VLMs' exploratory abilities.
- Why unresolved: The paper evaluates existing model capabilities against the V-REX benchmark but does not experiment with training methodologies to enhance the underdeveloped Planning capacity of smaller models.
- What evidence would resolve it: Fine-tuning experiments on smaller models using specialized planning data that result in a balanced ratio of Following-to-Planning ability comparable to larger proprietary models.

### Open Question 2
- Question: How can reasoning frameworks be adapted to prevent the performance degradation or interference observed when applying Chain-of-Questions (CoQ) to Retrieval tasks?
- Basis in paper: Section 5.2 (Finding 1) and Appendix 13.2 discuss a "mismatch" where CoQ hints fail to support tasks dominated by information lookup, noting that the structure can "conflict with the model's native retrieval process" and introduce cognitive noise.
- Why unresolved: The benchmark currently applies a uniform CoQ structure across all categories; the paper identifies the limitation in Retrieval scenarios but does not propose or test an alternative mechanism for these specific tasks.
- What evidence would resolve it: The development of a dynamic model switch or a modified evaluation protocol that skips intermediate CoQ steps for Retrieval tasks, resulting in final accuracies that meet or exceed the "Without CoQ" baseline.

### Open Question 3
- Question: Does the specific semantic content of incorrect intermediate answers (Following failures) cause greater error propagation than the semantic ambiguity of incorrect intermediate questions (Planning failures)?
- Basis in paper: Finding 5 states that VLMs are more robust to Planning failures than Following failures, and the text hypothesizes that "errors in the answer space are more likely to propagate to an incorrect final answer," but this mechanism is not empirically isolated.
- Why unresolved: While the paper measures the rate of recovery from failure, it does not disentangle whether the failure is due to the loss of necessary information or the active injection of misleading information in the answer step.
- What evidence would resolve it: Ablation studies comparing final task accuracy when intermediate steps are replaced with "I don't know" (neutral) versus factually incorrect assertions (adversarial), quantifying the toxicity of Following errors versus Planning detours.

## Limitations
- Evaluation relies on human-curated ground-truth QA chains, potentially biasing the Planning metric toward human-style exploration
- Uses GPT-5 for distractor generation (currently unreleased) and Qwen3-VL-32B-Instruct for filtering, limiting reproducibility
- Dataset focuses on short chains (2-6 steps), potentially not fully stress-testing long-horizon planning capabilities

## Confidence
- High confidence: The disentanglement of Planning and Following abilities through ground-truth answer injection is methodologically sound
- Medium confidence: The recovery asymmetry finding is novel but could be influenced by distractor design
- Medium confidence: The claim that Retrieval tasks benefit minimally from CoQ structuring is well-supported but may reflect task-specific design

## Next Checks
1. Test error propagation sensitivity by injecting controlled distractor questions at different positions in Following mode to verify whether early vs late errors show the claimed recovery asymmetry.
2. Evaluate model-generated vs human-curated reasoning chains on the same tasks to quantify whether the Planning metric reflects human-style reasoning biases.
3. Extend evaluation to longer chains (8-10 steps) with new scenarios to assess whether the Planning-Following gap persists or shifts with increased complexity.