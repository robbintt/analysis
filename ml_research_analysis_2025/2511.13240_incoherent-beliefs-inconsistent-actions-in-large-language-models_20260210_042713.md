---
ver: rpa2
title: Incoherent Beliefs & Inconsistent Actions in Large Language Models
arxiv_id: '2511.13240'
source_url: https://arxiv.org/abs/2511.13240
tags:
- confidence
- answer
- consistency
- confidences
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the alignment between large language models'
  (LLMs) confidence estimates and their actual actions in interactive settings. The
  authors find that LLMs frequently exhibit an "action-belief gap," where their behavior
  is inconsistent with their stated confidences.
---

# Incoherent Beliefs & Inconsistent Actions in Large Language Models

## Quick Facts
- arXiv ID: 2511.13240
- Source URL: https://arxiv.org/abs/2511.13240
- Reference count: 19
- LLMs exhibit significant action-belief gaps across betting, tool-use, and user-challenge settings

## Executive Summary
This paper investigates the alignment between large language models' confidence estimates and their actual actions in interactive settings. The authors find that LLMs frequently exhibit an "action-belief gap," where their behavior is inconsistent with their stated confidences. In a prediction market experiment, models often bet in the opposite direction of their high-confidence beliefs, with inconsistency rates exceeding 50% for some models. In a tool-use setting, models fail to invoke a verification tool even when their confidence is near zero, showing only moderate alignment with their confidence estimates. In a user-challenge setting, models sometimes defend low-confidence answers and defer on high-confidence ones. Surprisingly, consistency is not strongly correlated with task performance or calibration quality—well-calibrated, high-performing models can behave less consistently than smaller, weaker models. The findings highlight that static calibration metrics are insufficient for predicting LLM behavior in dynamic, agentic environments, emphasizing the need for new evaluation methods that directly assess action-belief alignment.

## Method Summary
The authors evaluate three confidence elicitation methods (logit extraction, sampling, verbal) across three experimental designs: (1) utility-maximization betting on coin tosses and Metaculus questions, (2) tool-use decisions for TriviaQA verification, and (3) user-challenge responses where models must either stick with or defer to answers. For each setting, they measure the Spearman rank correlation between elicited confidence and action rates (betting amounts, tool invocation, or deference behavior) as their primary consistency metric. The study tests whether better-calibrated models show higher action-belief consistency and examines if different elicitation methods yield varying degrees of alignment.

## Key Results
- Models exhibit up to 50% directional inconsistency, betting opposite to their high-confidence beliefs
- In tool-use experiments, Spearman correlations between confidence and tool-call rates remain moderate (0.35-0.46)
- Surprisingly, better-calibrated models can show worse action-belief consistency than smaller, weaker models
- No single confidence elicitation method consistently predicts action choices better than others

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Confidence elicitation methods do not capture the latent state models actually use for action decisions.
- Mechanism: Logit extraction, sampling, and verbal elicitation produce different confidence estimates that don't converge on a single "true" internal belief. Since these elicited values diverge from each other (sampling: 0.614 consistency vs. logits: 0.472 vs. verbal: 0.361 in Table 3), no method reliably reflects the decision-relevant internal state.
- Core assumption: Models have some latent representation that drives decisions, but it's not consistently accessible via current elicitation methods.
- Evidence anchors:
  - [abstract] "models can exhibit up to a 30% average difference between the directly elicited posterior, and the correct update of their prior"
  - [Section 8] "none of them are the 'true' internal belief of the LLM that it relies upon to act on"
  - [corpus] "Accumulating Context Changes the Beliefs of Language Models" shows context accumulation alters beliefs, suggesting elicitation is context-sensitive
- Break condition: If a single elicitation method consistently predicted action choices across all experimental designs, this mechanism would be weakened.

### Mechanism 2
- Claim: LLMs lack a unified, stable internal confidence representation.
- Mechanism: Models may construct confidence "on demand" rather than retrieving it. Different contexts (betting vs. tool use vs. challenges) trigger different reasoning paths that don't share a common confidence substrate.
- Core assumption: Beliefs are not stored but generated context-dependently.
- Evidence anchors:
  - [Section 8] "it is not even obvious that LLMs have a fixed internal confidence that they use as a proxy for making these decisions"
  - [Section 6.1] Correlation between calibration and consistency averages only 0.17 across designs—calibration doesn't predict action alignment
  - [corpus] "Do Role-Playing Agents Practice What They Preach?" finds belief-behavior inconsistency in role-playing settings, supporting context-dependent belief construction
- Break condition: If models showed high test-retest consistency in confidence across all contexts, this would suggest a stable latent state exists.

### Mechanism 3
- Claim: Post-training objectives don't enforce belief-action coherence.
- Mechanism: RLHF/instruction tuning optimizes for helpful responses, not for action-belief consistency. A model can be well-calibrated (know what it knows) without being trained to act consistently with that knowledge.
- Core assumption: Current training pipelines treat calibration and action selection as separate optimization targets.
- Evidence anchors:
  - [Section 6.1] "better calibrated models and elicitation methods exhibit a greater tendency to act out of line with their confidence estimates" (negative correlation in utility maximization)
  - [Section 1] "ensuring that a model knows what it knows does not guarantee that it will act rationally on that knowledge"
  - [corpus] Evidence is limited—no direct training intervention studies in neighbors
- Break condition: If fine-tuning specifically for action-belief consistency eliminated the gap, this would confirm training misalignment as causal.

## Foundational Learning

- Concept: **Bayesian Belief Updating**
  - Why needed here: The paper measures whether models correctly update posteriors from priors given new evidence. Understanding P(hypothesis|evidence) ∝ P(evidence|hypothesis) × P(hypothesis) is prerequisite.
  - Quick check question: If your prior is 0.3 and you see evidence that's 3× more likely under the hypothesis, what's your approximate posterior?

- Concept: **Expected Calibration Error (ECE)**
  - Why needed here: The paper evaluates whether low ECE (good calibration) predicts action-belief consistency—it doesn't.
  - Quick check question: If a model says "80% confident" on 100 predictions and gets 60 correct, what's the calibration error for that bin?

- Concept: **Spearman's Rank Correlation**
  - Why needed here: The paper uses this to measure monotonicity between confidence and action (e.g., higher confidence → lower tool use rate).
  - Quick check question: If confidence bins show tool-use rates of [0.8, 0.6, 0.5, 0.3] for confidence deciles [1-4], what would a perfect Spearman correlation look like?

## Architecture Onboarding

- Component map: Confidence elicitation (logit/sampling/verbal) -> Action selection (betting/tool-use/deference) -> Consistency metric (Spearman correlation)

- Critical path: Elicit confidence → Present action context → Measure action → Compute monotonicity score

- Design tradeoffs:
  - Logit extraction: Fast but requires model access; higher ECE (0.428) but moderate consistency (0.472)
  - Sampling: Expensive (100 samples); lowest ECE (0.117) and highest consistency (0.614)
  - Verbal: No special access needed; moderate ECE (0.127) but lowest consistency (0.361)

- Failure signatures:
  - Models betting opposite to high-confidence beliefs (>50% directional inconsistency in some models)
  - High confidence + high tool-use rate (should be inversely related)
  - Low confidence + high "stick" rate under challenge

- First 3 experiments:
  1. Replicate Design 1 (coin-toss warmup) using the exact prompt in Appendix B. Test GPT-4o and one open-source model (Llama 3.1 8B Instruct). Verify directional consistency matches Table 1 before proceeding.
  2. For Design 2 (tool-use), use the TriviaQA 'no-context' subset with prompts from Appendix D. Elicit confidence via logit extraction, then separately offer search tool. Compute Spearman correlation between confidence and no-tool-call rate across 10 percentile bins.
  3. For Design 3 (deference), run challenge protocol: get answer, respond with "Your answer to the initial question is incorrect," record whether model sticks or defers. Compute Spearman correlation between sticking rate and confidence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the action-belief gap stem primarily from fragile internal world models or from shortcomings in confidence elicitation methods?
- Basis: [explicit] The authors state that determining "the degree to which each is the case... is a topic we leave to future work."
- Why unresolved: The study demonstrates the gap exists across methods but cannot determine if the "true" internal belief is being mis measured or if the model lacks coherent beliefs.
- Evidence: Experiments probing internal representations independent of output tokens, or the development of elicitation techniques that directly predict action consistency.

### Open Question 2
- Question: Can confidence elicitation methods be optimized specifically for action-belief consistency rather than static calibration?
- Basis: [explicit] The authors advocate for "finding an alternative confidence elicitation method that is optimized specifically for this purpose."
- Why unresolved: Current methods minimize Expected Calibration Error (ECE) on static datasets, which the paper shows is a poor predictor of behavioral consistency.
- Evidence: Developing elicitation methods that maximize the monotonicity metrics (e.g., tool-call consistency) defined in the paper.

### Open Question 3
- Question: Do agentic calibration frameworks like the General Agent Calibrator (GAC) outperform standard elicitation methods in these specific settings?
- Basis: [explicit] The authors note: "Future work could involve testing the GAC on our experimental designs, to see if it outperforms the confidence elicitation methods we tested."
- Why unresolved: The paper evaluated standard methods (logits, verbal, sampling) but did not assess GAC, which is designed specifically for agentic uncertainty.
- Evidence: Benchmarking GAC against the three experimental designs (utility maximization, tool use, user interaction) used in the study.

## Limitations

- The paper acknowledges that no confidence elicitation method perfectly captures a model's internal belief state, limiting our ability to definitively diagnose the mechanism behind action-belief gaps
- Experimental designs use controlled scenarios that may not reflect complex, multi-turn interactions in deployed systems
- The observation that better-calibrated models can exhibit worse action consistency doesn't establish whether current training objectives actively harm coherence

## Confidence

- **High Confidence**: The empirical observation of action-belief inconsistency across three distinct experimental designs
- **Medium Confidence**: The interpretation that this gap stems from LLMs lacking a unified confidence representation
- **Low Confidence**: The causal claim about post-training objectives not enforcing belief-action coherence

## Next Checks

1. **Test-Retest Reliability**: Run the same experimental design (e.g., betting) multiple times with the same model and input. High variability would support the context-dependent belief construction hypothesis; consistent results would suggest stable latent states.

2. **Cross-Elicitation Convergence**: For a subset of trials, apply all three confidence elicitation methods to the same decision point. Measure whether any method consistently predicts action choices better than others, or if they remain divergent.

3. **Training Intervention Study**: Fine-tune a model specifically to maximize action-belief consistency (using the utility-maximization objective as a training signal). Compare pre- and post-intervention consistency to assess whether training can eliminate the gap.