---
ver: rpa2
title: 'CTPD: Cross Tokenizer Preference Distillation'
arxiv_id: '2601.11865'
source_url: https://arxiv.org/abs/2601.11865
tags:
- teacher
- distillation
- ctpd
- preference
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Cross Tokenizer Preference Distillation (CTPD),
  the first framework enabling preference alignment across models with different tokenizers.
  CTPD addresses the incompatibility issue in white-box distillation by projecting
  teacher and student tokens onto shared character-level spans, adapting token-level
  importance sampling for cross-tokenizer settings, and anchoring the student's reference
  distribution to the teacher's preferences.
---

# CTPD: Cross Tokenizer Preference Distillation

## Quick Facts
- arXiv ID: 2601.11865
- Source URL: https://arxiv.org/abs/2601.11865
- Authors: Truong Nguyen; Phi Van Dat; Ngan Nguyen; Linh Ngo Van; Trung Le; Thanh Hong Nguyen
- Reference count: 5
- One-line primary result: First framework for preference alignment across models with different tokenizers, achieving 1.26-1.78 average benchmark gains

## Executive Summary
CTPD (Cross Tokenizer Preference Distillation) introduces the first framework for preference alignment across models with incompatible tokenizers. The method addresses a fundamental limitation in white-box distillation where teacher and student models use different tokenization schemes. By projecting tokens onto shared character-level spans, adapting token-level importance sampling for cross-tokenizer settings, and anchoring the student's reference distribution to the teacher's preferences, CTPD achieves significant improvements over existing preference alignment and knowledge distillation baselines. The approach is theoretically grounded in importance sampling theory and demonstrates robust performance across multiple tasks including GSM8K, TruthfulQA, and Winogrande.

## Method Summary
CTPD operates in three stages: first, both teacher and student models undergo SFT training on instruction data; second, a contrastive teacher pair is trained (positive via standard DPO, negative via reverse DPO) to estimate per-span importance weights; third, CTPD training uses aligned span projection to map teacher and student tokens to shared character-level spans, computes span-level weights via log-probability ratios from the contrastive teachers, and optimizes a teacher-anchored DPO objective. The aligned spans enable supervision transfer between incompatible tokenizers, while the teacher-anchored reference allows the student to directly leverage the teacher's preference knowledge. Training uses AdamW with cosine LR schedule, batch size 16, and seeds fixed at 0.

## Key Results
- Achieves average benchmark gains of 1.26-1.78 points across multiple tasks
- Outperforms existing preference alignment and knowledge distillation baselines
- Teacher-anchored reference yields 67.42 average score vs 65.27 with student-as-reference
- Ablation studies confirm effectiveness of span projection and teacher-anchored reference components

## Why This Works (Mechanism)

### Mechanism 1: Aligned Span Projection
Character-level alignment enables supervision transfer between models with incompatible tokenizers. CTPD constructs a dynamic lattice that partitions sequences into aligned spans—teacher and student token subsequences mapping to identical character indices. Log-probabilities are aggregated within spans and projected without learnable parameters. Both tokenizers encode the same underlying natural language substrings despite syntactic differences.

### Mechanism 2: Cross-Tokenizer Importance Weighting
Aggregating teacher importance weights at span level improves credit assignment across mismatched token spaces. A contrastive teacher pair (positive: standard DPO, negative: reverse DPO) computes per-span weights via log-probability ratios: w_t = k·exp(μ·clamp(log π⁺/π⁻, L, U)). These weights reweight the DPO objective to focus on high-signal spans. Token-level importance signals remain meaningful when aggregated to span level and transferred across tokenizers.

### Mechanism 3: Teacher-Anchored Reference
Using the stronger teacher model as π_ref in DPO provides superior sample reweighting compared to student-as-reference. DPO's gradient includes λ = σ(β log π_ref(y_w)/π_ref(y_l) − β log π_θ(y_w)/π_θ(y_l)). A higher-capacity reference yields better λ values, promoting learning from high-quality preference pairs. The teacher possesses superior preference knowledge that improves weighting even when transferred via span projection.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed: CTPD reformulates DPO through importance sampling and span-level rewards; understanding the base objective is prerequisite
  - Quick check: Why does DPO bypass explicit reward modeling, and how does β control preference strength?

- **Concept: White-box vs. Black-box Distillation**
  - Why needed: CTPD is a white-box method (uses logits); the cross-tokenizer problem only affects white-box approaches
  - Quick check: Why can't we directly compare teacher and student log-probabilities when tokenizers differ?

- **Concept: Importance Sampling Theory**
  - Why needed: Theorem 2 grounds CTPD in importance sampling, connecting real dataset D to ideal noise-free D* via weighting
  - Quick check: How does the weighting function w(p_t|x, p_<t) correct for sampling from D instead of D*?

## Architecture Onboarding

- **Component map:** SFT on instruction data → initializes both teacher and student → Contrastive teacher training → positive (standard DPO) and negative (reverse DPO) teachers for weight estimation → Aligned span computation → character-index-based alignment of teacher/student tokens → Span-level weight estimation → contrastive log-ratio with clamping → CTPD loss computation → teacher-anchored DPO objective with span-weighted rewards

- **Critical path:** Character-level alignment → enables all cross-tokenizer operations → Span projection of teacher log-probs → permits teacher-as-reference → Weight transfer → improves credit assignment

- **Design tradeoffs:** Span granularity (finer preserves signal; coarser may lose token distinctions) vs. Clipping range [−0.5, 1.5] (narrows variance but risks clipping useful signals) vs. Teacher vs. student reference (better guidance vs. inference dependency)

- **Failure signatures:** Performance below TIS-DPO → verify span alignment correctness; Training instability → check weight clipping and μ scaling; Student diverges from preferences → validate teacher quality

- **First 3 experiments:** Baseline comparison: DPO vs. CTPD on GSM8K/TruthfulQA with same teacher; Reference ablation: Teacher vs. student as π_ref (reproduce Table 3); Weight strategy ablation: Random/Average/Student-estimated/Teacher-estimated (reproduce Table 2)

## Open Questions the Paper Calls Out

### Open Question 1
Can CTPD effectively distill domain-specific capabilities (e.g., mathematical reasoning, code generation, or specialized factual knowledge) using the same cross-tokenizer framework, or do certain skill types require modifications to the span projection or weighting mechanisms?
- Basis in paper: [explicit] The conclusion explicitly states: "Future work could explore extending CTPD to other forms of knowledge transfer, such as distilling specific skills or factual knowledge."
- Why unresolved: Experiments focus on general preference alignment benchmarks; no evaluation on specialized skill transfer is conducted
- What evidence would resolve it: Apply CTPD to domain-specific datasets (e.g., HumanEval for code, MATH for mathematical reasoning) and compare performance against both same-tokenizer distillation and the current benchmark-focused results

### Open Question 2
How does the magnitude of tokenizer incompatibility (e.g., vocabulary size ratio, tokenization algorithm differences) affect CTPD's effectiveness, and are there tokenizer pair combinations where the aligned span projection breaks down?
- Basis in paper: [inferred] Only two teacher-student pairs are tested (Qwen→Llama at two scales), leaving generalizability across diverse tokenizer architectures unexplored
- Why unresolved: No systematic study of different tokenizer types (BPE, Unigram, WordPiece) or vocabulary size disparities is provided
- What evidence would resolve it: Conduct experiments across multiple tokenizer pairs with varying vocabulary sizes and tokenization algorithms, reporting alignment quality as a function of tokenizer mismatch metrics

### Open Question 3
What is the performance gap between CTPD (cross-tokenizer) and an equivalent same-tokenizer preference distillation method, and can this gap be quantified as a function of alignment granularity?
- Basis in paper: [inferred] The paper compares CTPD to KD baselines and preference alignment methods but does not compare against preference distillation with shared tokenizers to isolate the cross-tokenizer penalty
- Why unresolved: Without a same-tokenizer control, it is unclear how much alignment quality is inherently lost due to the cross-tokenizer projection
- What evidence would resolve it: Run preference distillation experiments where teacher and student share a tokenizer (using the same TIS-DPO-style objective) and compare against CTPD results to quantify the cross-tokenizer overhead

## Limitations

- Span alignment precision lacks explicit algorithmic details for constructing the dynamic lattice and resolving ambiguous boundaries
- Weight transfer validity assumes token-level importance signals remain meaningful when aggregated to span level across incompatible tokenizers
- Teacher quality dependence creates potential failure mode where poor teacher preferences propagate through the span-weighted objective

## Confidence

**High Confidence:** The necessity of cross-tokenizer preference distillation (well-grounded problem statement); The theoretical foundation of teacher-anchored reference via importance sampling theory; The benchmark performance improvements over existing baselines

**Medium Confidence:** The effectiveness of aligned span projection mechanism (implementation details are sparse); The cross-tokenizer adaptation of TIS-DPO (limited ablation on weight clipping/μ scaling); The generality of results across multiple tasks (only tested on 6 benchmarks)

**Low Confidence:** The robustness of span alignment across diverse tokenizer pairs (only tested with specific combinations); The stability of reverse DPO training for negative teacher (no convergence analysis provided); The scalability to larger model pairs (only tested with 1B/7B/8B/14B models)

## Next Checks

1. **Span Alignment Fidelity Test:** Implement a diagnostic tool that decodes aligned spans from both teacher and student tokens, then measures character-level overlap and semantic consistency. Verify that spans consistently map to semantically coherent units and that boundary resolution doesn't introduce supervision noise.

2. **Weight Distribution Analysis:** Monitor span-level importance weight distributions during training, particularly the effects of clamping and μ scaling. Conduct ablation studies varying clamp bounds and μ values to identify optimal ranges that preserve signal while preventing instability.

3. **Teacher Quality Sensitivity:** Evaluate CTPD performance when the teacher model has known preference misalignment (e.g., train teacher on reversed preferences). Measure degradation to quantify the impact of teacher quality on the final student alignment, and test whether student-as-reference becomes competitive in such scenarios.