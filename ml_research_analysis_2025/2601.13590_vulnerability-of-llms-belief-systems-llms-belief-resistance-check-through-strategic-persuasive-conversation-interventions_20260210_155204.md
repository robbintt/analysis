---
ver: rpa2
title: Vulnerability of LLMs' Belief Systems? LLMs Belief Resistance Check Through
  Strategic Persuasive Conversation Interventions
arxiv_id: '2601.13590'
source_url: https://arxiv.org/abs/2601.13590
tags:
- boolq
- pubmedqa
- persuasion
- robustness
- latenthatred
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large Language Models (LLMs) are highly susceptible to persuasion,
  with belief changes occurring rapidly and across model scales. Across five models
  and three domains, smaller models (Llama 3.2-3B) changed beliefs at the first persuasive
  turn in over 80% of cases, while larger models like GPT-4o-mini showed greater resistance.
---

# Vulnerability of LLMs' Belief Systems? LLMs Belief Resistance Check Through Strategic Persuasive Conversation Interventions

## Quick Facts
- arXiv ID: 2601.13590
- Source URL: https://arxiv.org/abs/2601.13590
- Authors: Fan Huang; Haewoon Kwak; Jisun An
- Reference count: 40
- Key outcome: Large Language Models (LLMs) are highly susceptible to persuasion, with belief changes occurring rapidly and across model scales.

## Executive Summary
This paper systematically evaluates LLM belief vulnerability to strategic persuasion across five models and three domains. The study reveals that belief changes occur rapidly, with smaller models (Llama 3.2-3B) capitulating at the first persuasive turn in over 80% of cases. Surprisingly, meta-cognition prompting intended to strengthen resistance actually accelerates belief erosion. While adversarial fine-tuning shows promise for some models (GPT-4o-mini: 98.6% robustness), Llama models remain highly susceptible even when trained on their own failure cases. These findings highlight substantial model-dependent limits of current robustness interventions and call for holistic evaluation frameworks.

## Method Summary
The study evaluates LLM belief vulnerability using a systematic persuasion pipeline across three domains (BoolQ, PubMedQA, LatentHatred) with 1,236 high-confidence instances filtered to ≥95% initial confidence. The pipeline tracks belief states over six turns: initial belief check (Turn 0), up to four persuasive turns using SMCR framework strategies, and final belief assessment (Turn 5). Robustness is measured as 100 minus the misinformed rate at turn 4. The study tests baseline, meta-cognition prompting, prompt-based robustness, and adversarial fine-tuning interventions using QLoRA on 400 vulnerable instances per model.

## Key Results
- Smaller models (Llama 3.2-3B) change beliefs at first persuasive turn in >80% of cases; larger models show greater resistance
- Meta-cognition prompting backfired, increasing vulnerability with mean drop of 16.3 percentage points
- Adversarial fine-tuning achieved near-complete robustness for GPT-4o-mini (98.6%) but Llama models remained <14% robust even when trained on their own failures
- Scale-compliance coupling effect: parameter count correlates with belief stability

## Why This Works (Mechanism)

### Mechanism 1: Scale-Compliance Coupling
Smaller models exhibit rapid belief capitulation due to weaker internal belief representations. Reduced parameter capacity limits stable internal representations; persuasive inputs more easily override weakly-anchored beliefs. Core assumption: belief stability correlates with representational capacity. Evidence: Llama 3.2-3B average end turn of 1.1–1.4 across domains. Break condition: If smaller models showed selective resistance to specific persuasion types, scale alone would not explain vulnerability.

### Mechanism 2: Meta-cognition Backfire Effect
Eliciting self-reported confidence accelerates belief erosion rather than strengthening resistance. Confidence generation exposes latent uncertainty without reflective control; expressed confidence becomes weakly coupled to stable belief states. Core assumption: LLMs lack human-like meta-cognitive processes that consolidate beliefs when articulated. Evidence: "60% show average decreased robustness, with a mean drop of 16.3 percentage points." Break condition: If meta-cognition uniformly improved robustness across all models, the mechanism would require revision.

### Mechanism 3: Architecture-Dependent Fine-tuning Transfer
Adversarial fine-tuning effectiveness depends on underlying architecture and training methodology, not parameter count. Models with strong instruction-following capabilities can leverage adversarial training to recognize rhetorical tactics; models with weaker instruction-following show minimal transfer. Core assumption: instruction-following capability mediates adversarial training effectiveness. Evidence: GPT-4o-mini achieves 98.6% robustness while Llama models remain <14%. Break condition: If Llama models improved with different fine-tuning approaches, the mechanism would narrow to training method rather than architecture.

## Foundational Learning

- **SMCR Framework (Source-Message-Channel-Receiver)**: Understanding persuasion requires analyzing who persuades whom through what medium and with what framing; the paper decomposes vulnerability across these dimensions. Quick check: Can you explain why receiver-level strategies (esteem, confirmation bias) might affect models differently than source-level strategies?

- **Meta-cognition vs. Calibrated Confidence**: The paper distinguishes between expressing confidence scores (surface signal) and having genuine self-reflective belief maintenance (meta-cognition). Quick check: Why might asking a model to report confidence increase rather than decrease vulnerability?

- **Adversarial Training Transfer**: Understanding when adversarial fine-tuning generalizes vs. overfits is critical for deploying robustness interventions. Quick check: If a model achieves high robustness on training instances but fails on held-out test instances, what does this suggest about the intervention?

## Architecture Onboarding

- **Component map**: Persuasion pipeline (Initial belief check → Multi-turn persuasive messages → Implicit confidence checks → Final belief assessment) → SMCR strategy layer (Source, Message, Channel, Receiver) → Intervention layer (Baseline, meta-cognition prompting, prompt-based robustness, adversarial fine-tuning)

- **Critical path**: Filter instances with ≥95% initial confidence → Track belief state at each turn (Turn 0-5) → Compute robustness as 100 - MR@4

- **Design tradeoffs**: Binary vs. continuous belief tracking (binary enables clear quantification but loses nuance); Implicit vs. explicit belief checks (implicit checks prevent conversation history contamination); Mixed fine-tuning provides broad failure coverage but conflates vulnerability types

- **Failure signatures**: Extreme compliance (Average end turn < 1.5); Confidence decay trajectory (progressive decline precedes belief flip); Architecture-specific transfer gap (high training robustness with low test generalization)

- **First 3 experiments**: 1) Replicate scale-compliance finding on held-out model family (e.g., Claude 3.5 Sonnet) to test generalizability; 2) Ablate meta-cognition prompting across confidence score formats to identify if backfire effect is format-dependent; 3) Compare full-parameter fine-tuning vs. QLoRA on Llama 3.2-3B to disentangle architecture vs. training method effects

## Open Questions the Paper Calls Out

- Why does meta-cognition prompting accelerate belief erosion rather than strengthening belief resistance in LLMs, and does this effect stem from specific training methodologies like RLHF?

- What architectural or training-specific factors cause adversarial fine-tuning to fail for Llama models while succeeding for GPT-4o-mini and Mistral?

- Do LLM belief changes persist, decay, or recover in discontinuous interactions separated by time gaps, as opposed to the single-session interactions tested?

- Can disentangled training regimes that isolate baseline uncertainty from strategy-specific susceptibility yield better robustness than the mixed approach?

## Limitations

- Belief check contamination: The mechanism for implicit belief checks without appending to conversation history is underspecified, potentially inflating measured robustness
- Confidence extraction variability: Logprob extraction methods vary across implementations, critically shaping dataset composition and robustness metrics
- Strategy implementation fidelity: Exact formulations for SMCR strategies aren't provided, with subtle phrasing differences potentially altering persuasion effectiveness

## Confidence

- **High Confidence**: Scale-Compliance Coupling - supported by consistent across-domain results with clear statistical patterns
- **Medium Confidence**: Meta-cognition Backfire Effect - observed patterns are robust but mechanism remains speculative
- **Low Confidence**: Architecture-Dependent Fine-tuning Transfer - mechanism is proposed but lacks comparative training method analysis

## Next Checks

1. Replicate scale-compliance finding on held-out model families (e.g., Claude 3.5 Sonnet, Gemini Pro) to test whether scale-vulnerability relationship generalizes beyond tested architectures

2. Ablate meta-cognition prompting across confidence score formats (0-5 scale vs. probability vs. natural language) to determine if backfire effect is format-dependent or inherent to meta-cognitive prompting

3. Compare full-parameter fine-tuning vs. QLoRA on Llama 3.2-3B to disentangle whether architecture-dependent transfer gaps reflect architectural constraints or training methodology limitations