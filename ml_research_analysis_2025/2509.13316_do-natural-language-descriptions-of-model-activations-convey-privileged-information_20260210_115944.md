---
ver: rpa2
title: Do Natural Language Descriptions of Model Activations Convey Privileged Information?
arxiv_id: '2509.13316'
source_url: https://arxiv.org/abs/2509.13316
tags:
- knowledge
- table
- activations
- verbalization
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Verbalization techniques translate LLM internal representations
  into natural language using a second LLM. This work evaluates whether such methods
  reveal privileged information about target model internals or merely reflect input
  text and the verbalizer's own knowledge.
---

# Do Natural Language Descriptions of Model Activations Convey Privileged Information?

## Quick Facts
- arXiv ID: 2509.13316
- Source URL: https://arxiv.org/abs/2509.13316
- Authors: Millicent Li; Alberto Mario Ceballos Arroyo; Giordano Rogers; Naomi Saphra; Byron C. Wallace
- Reference count: 40
- Key outcome: Verbalization often describes the verbalizer's own knowledge rather than the target model's internal operations, requiring more rigorous benchmarks to assess interpretability claims.

## Executive Summary
This paper investigates whether verbalization techniques—methods that translate LLM internal representations into natural language using a second LLM—provide privileged access to model internals or merely reflect input text and the verbalizer's own knowledge. Through controlled experiments on feature extraction tasks, inversion studies, and a novel PersonaQA dataset with fabricated knowledge, the authors demonstrate that zero-shot prompting without access to activations performs comparably to verbalization, suggesting that input text alone suffices for accurate predictions. The study reveals that verbalizers often default to their own parametric knowledge when target and verbalizer knowledge distributions are misaligned, and that training verbalizers on the same data as targets improves performance but limits generalizability.

## Method Summary
The authors evaluate verbalization techniques by comparing their performance to zero-shot prompting baselines and input reconstruction methods across multiple experimental conditions. They test two verbalization approaches: Patchscopes (which patches single-token activations into the verbalizer's forward pass) and LIT (Latent Interpretation Tuning, which finetunes the verbalizer on activation data). The experiments include feature extraction tasks, activation inversion studies where inputs are reconstructed from activations, and a novel PersonaQA dataset containing fabricated personas to test whether verbalizers can extract knowledge that conflicts with their own pretraining. The analysis systematically controls for input text information, parametric knowledge, and knowledge alignment between target and verbalizer models.

## Key Results
- Zero-shot prompting without access to activations performs comparably to verbalization on feature extraction tasks, indicating input text alone suffices.
- Reconstructed inputs from activations can substitute for activations with matching performance, confirming that verbalizers often decode input rather than internal representations.
- Verbalizers fail when knowledge between target and verbalizer models is misaligned, instead reflecting their own parametric knowledge rather than the target model's state.
- Training verbalizers on the same data as targets improves performance on knowledge extraction tasks but limits generalizability to out-of-distribution knowledge.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Verbalization success on feature extraction tasks often reflects input text reconstruction, not privileged access to target model internals.
- Mechanism: The verbalizer M2 processes activations from M1 that contain sufficient information to reconstruct x_input. Rather than decoding abstract internal representations, M2 implicitly inverts activations to recover input text, then applies its own parametric knowledge to answer x_prompt.
- Core assumption: Target model activations retain sufficient information about input text to enable reconstruction.
- Evidence anchors:
  - [abstract]: "zero-shot prompting without access to activations performs comparably to verbalization, indicating that input text alone suffices"
  - [section 4]: "Prediction on top of reconstructed text (x_rec) results in performance that mostly matches that of verbalization"
  - [corpus]: Corpus evidence is weak for this specific inversion-based explanation; related work on activation inversion (Morris et al.) is cited but not present in neighbors

### Mechanism 2
- Claim: Verbalizer outputs reflect M2's parametric knowledge rather than M1's internal operations when knowledge distributions diverge.
- Mechanism: M2 receives activations from M1 but lacks the semantic grounding to interpret them correctly. Instead, M2 defaults to answering based on its pretraining knowledge, producing plausible but unfaithful descriptions.
- Core assumption: M2 cannot reliably decode activation patterns from models with different knowledge distributions without explicit training alignment.
- Evidence anchors:
  - [abstract]: "verbalizers fail when knowledge between target and verbalizer models is misaligned, instead reflecting their own parametric knowledge"
  - [section 5.2]: "verbalizers (M2) rely too much on their own world knowledge to make predictions, even when it conflicts with the knowledge in M2's activations"
  - [corpus]: Neighbor paper "Training Language Models to Explain Their Own Computations" explores self-explanation but does not directly address cross-model knowledge misalignment

### Mechanism 3
- Claim: Training verbalizers on the same data as target models enables knowledge extraction but limits generalizability.
- Mechanism: Joint training aligns M2's representation space with M1's, allowing M2 to interpret activations as carrying specific factual knowledge. This creates a form of shared semantic encoding.
- Core assumption: Knowledge can be extracted from activations only when the verbalizer has learned to associate specific activation patterns with specific knowledge content.
- Evidence anchors:
  - [abstract]: "Training verbalizers on the same data as targets improves performance, but this limits generalizability"
  - [section 5.3]: "After training M2 on the same data as M1, M2 is able to verbalize the personas"
  - [corpus]: No direct corpus evidence; related interpretability work does not address this training-alignment mechanism

## Foundational Learning

- Concept: **Privileged Information**
  - Why needed here: Central to evaluating whether verbalization provides genuine insight into model internals versus superficial input reconstruction.
  - Quick check question: Can the verbalizer answer correctly without access to activations? If yes, privileged information is not demonstrated.

- Concept: **Activation Inversion**
  - Why needed here: Explains why verbalization can succeed without privileged access—activations encode input text, which can be reconstructed.
  - Quick check question: Can a trained inverter model recover x_input from activations with sufficient fidelity to answer prompts?

- Concept: **Parametric Knowledge**
  - Why needed here: Disentangles whether verbalizer outputs originate from M1's activations or M2's pretraining knowledge.
  - Quick check question: Does the verbalizer's zero-shot performance (without activations) match its verbalization performance?

## Architecture Onboarding

- Component map:
  - Target model (M1) -> Generates activations h_ℓ from input x_input
  - Verbalizer (M2) -> Receives activations and interpretation prompt x_prompt, outputs natural language description
  - Patchscopes -> Patches single-token activation into M2's forward pass, requires no training if M1=M2
  - LIT (Latent Interpretation Tuning) -> Patches full-layer activation matrix, requires finetuning

- Critical path:
  1. Control for input text: Run zero-shot baseline without activations
  2. Test inversion: Train M_rec to reconstruct x_input, then answer prompts from x_rec
  3. Control knowledge alignment: Use PersonaQA-style datasets with fabricated knowledge to isolate source of information

- Design tradeoffs:
  - Patchscopes is training-free but requires M1=M2 for best performance; LIT requires training but handles cross-model settings
  - Single-token activation patching is efficient but loses context; full-layer patching preserves more information but increases compute
  - Training M2 on M1's data improves accuracy but sacrifices generalizability

- Failure signatures:
  - Verbalizer matches zero-shot baseline -> no privileged information accessed
  - Performance drops when M1 knowledge is shuffled/fabricated -> M2 relying on its own knowledge
  - Cross-model verbalization fails or is inconsistent -> representation space misalignment

- First 3 experiments:
  1. **Zero-shot baseline test**: Evaluate M2 with x_input + x_prompt (no activations) on feature extraction; compare to verbalization accuracy
  2. **Inversion-then-predict**: Train inversion model on activations, reconstruct inputs, answer prompts from reconstructions alone
  3. **Knowledge misalignment test**: Train M1 on PersonaQA-Fantasy (novel facts), test if verbalizer M2 (untrained on these facts) can extract knowledge from M1 activations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can verbalization methods be modified to faithfully extract knowledge from a target model when that knowledge conflicts with the verbalizer's own parametric knowledge?
- Basis in paper: [explicit] The authors state in the Conclusion that "Future work might investigate just how to extract information from verbalizers when the knowledge between the target LLM and verbalizer LLM conflict."
- Why unresolved: The study shows that when M1 is finetuned on data M2 has never seen (PersonaQA-Fantasy), verbalizers fail to describe M1's state and default to their own lack of knowledge.
- What evidence would resolve it: A verbalization technique that successfully describes novel facts internal to M1 without requiring M2 to be finetuned on the same data distribution.

### Open Question 2
- Question: Can verbalization access privileged information in tasks beyond feature extraction and factual recall, such as multi-step reasoning or planning?
- Basis in paper: [explicit] In the Limitations section, the authors note, "experimenting on other tasks could improve our understanding for what tasks verbalization can access privileged information."
- Why unresolved: The current analysis is restricted to QA-style tasks (feature extraction), which the paper demonstrates can often be solved via input reconstruction or zero-shot prompting.
- What evidence would resolve it: Successful verbalization of internal reasoning states on a task where input text alone is insufficient to predict the model's intermediate behavior.

### Open Question 3
- Question: How can evaluation benchmarks be designed to effectively isolate privileged internal processing from input text information?
- Basis in paper: [inferred] The paper argues that "these datasets may not be ideal for evaluating verbalization methods" because they allow "implicit inversion" where the input can be reconstructed, making it unclear if the verbalizer is reading the activation or the input.
- Why unresolved: Existing benchmarks like feature extraction are flawed because high performance can be achieved by simply decoding the input prompt or using the verbalizer's own knowledge, rather than accessing target model internals.
- What evidence would resolve it: The creation of a benchmark where zero-shot and input-inversion baselines fail, but the verbalization method succeeds, proving it accesses privileged information.

## Limitations

- Limited task scope: All experiments focus on feature extraction and persona-based knowledge extraction tasks, which may not generalize to other interpretability applications.
- Single model family testing: Experiments primarily use Llama-2 models with some testing on Llama-3, limiting generalizability across different model architectures.
- Dataset specificity: The PersonaQA experiments use fabricated personas with controlled knowledge, but real-world applications involve more complex knowledge structures and distributions.

## Confidence

**High confidence**: The core finding that zero-shot prompting without activations performs comparably to verbalization on feature extraction tasks. This is directly demonstrated across multiple experimental conditions with clear statistical evidence.

**Medium confidence**: The claim that verbalizers reflect their own parametric knowledge rather than target model internals when knowledge distributions diverge. While the PersonaQA experiments support this, the effect size and generalizability to other knowledge domains require further validation.

**Low confidence**: The conclusion that training verbalizers on the same data as targets necessarily limits generalizability. This assumes that out-of-distribution knowledge is the primary use case, but the paper doesn't demonstrate scenarios where this limitation actually manifests in practical applications.

## Next Checks

1. **Cross-architecture generalization test**: Evaluate verbalization across different model families (e.g., Llama, GPT, Claude) on the same feature extraction tasks to determine if knowledge misalignment effects are architecture-dependent or universal.

2. **Layer-specific analysis**: Systematically test verbalization performance across all layers of the target model to identify whether certain layers consistently provide privileged information access while others do not.

3. **Mechanistic task validation**: Apply verbalization to mechanistic interpretability tasks like circuit discovery or causal tracing, where the presence of privileged information would be more directly verifiable through intervention experiments.