---
ver: rpa2
title: Exploring Exploration in Bayesian Optimization
arxiv_id: '2502.08208'
source_url: https://arxiv.org/abs/2502.08208
tags:
- iteration
- otsd
- exploration
- benchmarks
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two novel methods - Observation Traveling
  Salesman Distance (OTSD) and Observation Entropy (OE) - to quantify exploration
  in Bayesian optimization acquisition functions. OTSD measures the total Euclidean
  distance required to connect all observation points, while OE uses empirical differential
  entropy to assess the uniformity of observation distribution.
---

# Exploring Exploration in Bayesian Optimization

## Quick Facts
- arXiv ID: 2502.08208
- Source URL: https://arxiv.org/abs/2502.08208
- Reference count: 40
- Primary result: Introduces OTSD and OE metrics to quantify exploration in Bayesian optimization, empirically validating their effectiveness across synthetic and real-world benchmarks.

## Executive Summary
This paper introduces two novel methods—Observation Traveling Salesman Distance (OTSD) and Observation Entropy (OE)—to quantify exploration in Bayesian optimization acquisition functions. OTSD measures the total Euclidean distance required to connect all observation points, while OE uses empirical differential entropy to assess the uniformity of observation distribution. The authors empirically validate these methods across synthetic and real-world benchmarks, demonstrating their effectiveness in capturing exploration behavior. Key findings include: normalized OTSD and OE strongly correlate across benchmarks, validating their reliability; TS is highly explorative in high dimensions but behaves similarly to EI in low dimensions; batching increases exploration while trust regions and RAASP sampling reduce it; and methods with extreme exploration levels (overly explorative or exploitative) generally show worse optimization performance. The study establishes the first empirical taxonomy of acquisition function exploration, providing valuable insights for designing and selecting acquisition functions in Bayesian optimization.

## Method Summary
The paper proposes two metrics to quantify exploration in Bayesian optimization: OTSD and OE. OTSD formulates observations as a TSP, using an insertion heuristic to approximate the shortest tour connecting all points, while OE calculates the empirical differential entropy using the Kozachenko-Leonenko estimator based on k-nearest neighbor distances. The methods are validated on 9 benchmarks (synthetic and real-world) using BoTorch with SingleTaskGP, testing multiple acquisition functions including EI, PI, UCB, MES, TS, and KG. The authors normalize OTSD by a scaling factor Ψ(d,t) to enable cross-problem comparison and establish an empirical taxonomy mapping AF behaviors based on their OTSD/OE scores.

## Key Results
- Normalized OTSD and OE strongly correlate across benchmarks, validating their reliability as exploration metrics
- Thompson Sampling shows extreme exploration in high dimensions (d > 20), approaching random search behavior
- Batching acquisition functions increases exploration while trust regions and RAASP sampling reduce it
- Methods with extreme exploration levels (overly explorative or exploitative) generally show worse optimization performance

## Why This Works (Mechanism)

### Mechanism 1: Spatial Dispersion via Observation Traveling Salesman Distance (OTSD)
The level of exploration in a Bayesian Optimization (BO) run can be quantified by the total geometric distance required to connect all observation points. OTSD formulates the set of observations as a Traveling Salesman Problem (TSP). It uses an insertion heuristic to approximate the shortest tour connecting all points. As points are sampled further apart (exploration), the tour length increases monotonically; as points cluster (exploitation), the marginal increase in tour length decreases. The search space must be Euclidean (or a valid metric space), and "exploration" is strictly defined as covering more volume in the input space $\mathcal{X}$. The mechanism fails if the problem is high-dimensional with low sample size ($t \ll d$) where points are naturally sparse, or if the domain is non-Euclidean.

### Mechanism 2: Distributional Uniformity via Observation Entropy (OE)
Exploration can be quantified by the uniformity of the empirical distribution of observations, calculated via differential entropy. OE uses the Kozachenko-Leonenko (KL) estimator to calculate the empirical differential entropy based on the distance to k-nearest neighbors. High entropy implies a uniform spread of points (exploration), while low entropy implies clustering (exploitation). Unlike OTSD, this is non-monotonic, capturing changes in AF behavior over time. The mechanism degrades significantly in high dimensions ($d > 20$) due to bias in the KL estimator.

### Mechanism 3: Early Warning Signals via Taxonomy Mapping
Calculating OTSD and OE reveals behavioral regimes (over-exploration or over-exploitation) that correlate with performance degradation, serving as a diagnostic tool. By mapping an AF's OTSD/OE score against an empirical taxonomy (e.g., Random Search vs. Deterministic), engineers can detect anomalies. For instance, if an AF approaches the OTSD of Random Search, it is effectively exploring without learning; if it drops too low, it may be stuck in local optima. The taxonomy is derived empirically and may not generalize to problems with fundamentally different landscape structures like deceptive or multi-modal functions.

## Foundational Learning

- **Concept: Gaussian Process (GP) Surrogates**
  - Why needed here: OTSD and OE measure the output of the optimization loop, but the driver is the GP posterior. You must understand how the GP updates its mean and variance to understand why an AF chooses "explorative" points.
  - Quick check question: If a GP has high posterior variance in a region, which mechanism (OTSD or OE) will increase if the AF samples there?

- **Concept: Acquisition Functions (AFs)**
  - Why needed here: The paper compares AFs (UCB, EI, TS) based on the proposed metrics. Understanding the mechanics of UCB's $\beta$ parameter or TS's sampling is required to interpret the results.
  - Quick check question: Does a high $\beta$ in UCB typically lead to a higher or lower OTSD score?

- **Concept: Differential Entropy vs. Discrete Entropy**
  - Why needed here: Mechanism 2 relies on differential entropy. Unlike discrete entropy, differential entropy can be negative and is not invariant to coordinate transformations.
  - Quick check question: Why does the paper warn that OE values are not directly comparable across problems with different input dimensions?

## Architecture Onboarding

- **Component map:** Observer -> OTSD Module -> OE Module -> Normalizer
- **Critical path:** The computation of pairwise distances is the shared bottleneck. For OE, the KL estimator complexity is O(dT²). OE is significantly slower (seconds vs. milliseconds for OTSD) and should be omitted for high-dimensional (d > 20) real-time monitoring.
- **Design tradeoffs:** Use OTSD for high-dimensional problems (d > 20) and rapid prototyping (fast). Use OE for low-dimensional analysis where density estimation is more reliable than geometric tour length.
- **Failure signatures:** OE bias in high dimensions (d > 20), OTSD saturation in constrained spaces, non-Euclidean domains requiring distance metric definition.
- **First 3 experiments:**
  1. Replicate UCB β scaling: Run UCB with β ∈ {0.1, 1, 5} on Hartmann-6d, verify higher β produces higher OTSD/OE.
  2. High-dimension TS stress test: Run TS on d > 20, plot OTSD to confirm if it approaches Random Search baseline.
  3. Batching vs. Trust Region: Run EI with batch size q=32 vs. EI with Trust Regions, plot normalized OTSD to verify batching increases score while TR decreases it.

## Open Questions the Paper Calls Out

### Open Question 1
Can Observation Entropy (OE) be reliably estimated for high-dimensional spaces (d > 20) where the current Kozachenko-Leonenko (KL) estimator fails? The paper limits OE reporting to low dimensions (d ≤ 20) because the KL estimator exhibits significant bias in higher dimensions, yet high-dimensional optimization is a key area where exploration measurement is needed. A modification of the OE metric or a different entropy estimator that maintains consistency and accuracy in dimensions d > 50 would resolve this.

### Open Question 2
How can OTSD and OE be adapted to quantify exploration in non-Euclidean domains, such as spaces with categorical variables, graphs, or strings? While OTSD can theoretically use other metrics, the current OE formulation relies on Euclidean distance properties, making direct application to complex structures infeasible. A formulation of OE using graph-based distances or kernel density estimation that correlates with random search baselines on structured optimization tasks would resolve this.

### Open Question 3
To what extent do GP kernel hyperparameters and hyperpriors bias the proposed exploration metrics? The current experiments use a standard GP setup; however, hyperparameters like lengthscale directly control the smoothness of the surrogate and posterior variance, which may inherently drive the exploration behavior measured by OTSD and OE. An ablation study showing the variance of OTSD/OE scores for fixed AFs across different hyperparameter configurations would resolve this.

## Limitations

- The metrics are fundamentally tied to Euclidean geometry, making them less reliable for categorical or graph-based search spaces
- The Kozachenko-Leonenko entropy estimator shows known bias in high dimensions (d > 20), though the paper appropriately switches to OTSD-only in these cases
- The empirical taxonomy may not generalize to problems with fundamentally different landscape structures like deceptive or multi-modal functions

## Confidence

- **High confidence:** OTSD's monotonic relationship with exploration, the strong OTSD-OE correlation across benchmarks, and the empirical taxonomy's reliability within tested benchmark classes
- **Medium confidence:** The superiority of balanced exploration-exploitation strategies over extreme approaches, as this depends on the specific benchmark distribution
- **Low confidence:** The OE estimator's robustness in high dimensions and the general applicability of the taxonomy to unseen problem classes

## Next Checks

1. **High-Dimensional Stress Test:** Run TS on d > 20 problems and verify if normalized OTSD approaches or exceeds the Random Search baseline, confirming "over-explorative" behavior.
2. **Non-Euclidean Domain Extension:** Apply OTSD/OE to a graph-based optimization problem by defining appropriate metric distances and validate if the metrics still capture exploration behavior.
3. **Cross-Problem Taxonomy Validation:** Test the empirical taxonomy on a new class of problems (e.g., deceptive functions) to verify if the balanced EETO region remains optimal or if different exploration levels become superior.