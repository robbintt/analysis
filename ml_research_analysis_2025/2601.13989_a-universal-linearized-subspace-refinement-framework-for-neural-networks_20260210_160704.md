---
ver: rpa2
title: A universal linearized subspace refinement framework for neural networks
arxiv_id: '2601.13989'
source_url: https://arxiv.org/abs/2601.13989
tags:
- subspace
- training
- linearized
- accuracy
- residual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Linearized Subspace Refinement (LSR) is a post-training refinement
  framework that exploits the Jacobian-induced linear residual model at a fixed trained
  network state. Instead of further updating parameters via gradient-based training,
  LSR solves a reduced direct least-squares problem within a low-dimensional Jacobian-induced
  subspace to obtain a subspace-optimal solution of the linearized residual model.
---

# A universal linearized subspace refinement framework for neural networks

## Quick Facts
- arXiv ID: 2601.13989
- Source URL: https://arxiv.org/abs/2601.13989
- Authors: Wenbo Cao; Weiwei Zhang
- Reference count: 40
- Key outcome: LSR achieves order-of-magnitude error reductions via post-training direct least-squares solve in a low-dimensional Jacobian-induced subspace, often outperforming gradient-based training.

## Executive Summary
Linearized Subspace Refinement (LSR) is a post-training refinement framework that exploits the Jacobian-induced linear residual model at a fixed trained network state. Instead of further updating parameters via gradient-based training, LSR solves a reduced direct least-squares problem within a low-dimensional Jacobian-induced subspace to obtain a subspace-optimal solution of the linearized residual model. This yields a refined linear predictor with substantially improved accuracy—often achieving order-of-magnitude error reductions—without modifying network architectures, loss formulations, or training procedures. Across function approximation, data-driven operator learning, and physics-informed operator fine-tuning, LSR consistently exposes accuracy levels not fully exploited by gradient-based training, even when local linearization yields a convex problem. The results indicate that loss-induced numerical ill-conditioning, rather than nonconvexity or model expressivity, can constitute a dominant practical bottleneck. For operator-constrained problems, an iterative variant alternates linearized refinement with nonlinear alignment to further improve convergence and accuracy.

## Method Summary
LSR computes a Jacobian-induced subspace basis via randomized SVD using Jacobian-vector products (JVPs), then solves a direct least-squares problem within this subspace to find an optimal correction to the residual. The refined predictor is evaluated as the original network output plus the linearized correction, without modifying the network weights. For physics-constrained problems, an iterative variant alternates between LSR and supervised nonlinear alignment.

## Key Results
- LSR achieves substantial error reductions (often 10×) on function approximation and operator learning tasks compared to trained baselines.
- For convex linearized problems, LSR reaches lower residual floors than iterative methods, attributed to bypassing ill-conditioning.
- Iterative LSR with nonlinear alignment improves convergence and accuracy for physics-informed operator learning.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If gradient-based optimization stalls due to ill-conditioning rather than nonconvexity, a direct least-squares solve in a reduced subspace can bypass the iterative stagnation and access lower residual floors.
- **Mechanism:** Standard optimizers (Adam, L-BFGS) struggle with the ill-conditioned Hessian/curvature of the linearized residual problem. LSR restricts the parameter correction to a low-dimensional Jacobian-induced subspace ($\Delta \theta = V y$) where a direct QR solver can numerically stable minimize the residual, effectively sidestepping the slow convergence rates of iterative gradient descent.
- **Core assumption:** The primary bottleneck preventing further accuracy improvement is numerical ill-conditioning in the optimization landscape, not a lack of model expressivity or local minima entrapment.
- **Evidence anchors:**
  - [abstract] "loss-induced numerical ill-conditioning, rather than nonconvexity or model expressivity, can constitute a dominant practical bottleneck."
  - [page 7, figure 2c] Shows iterative methods stalling on a convex linearized problem while direct solvers reach the floor.
  - [corpus] Corpus evidence for this specific "bypass" mechanism is weak; neighbors focus on sampling or weight adaptation, not post-training linearized refinement.
- **Break condition:** If the training process has already converged to a true first-order stationary point ($\nabla f=0$), the linearized residual has no descent direction and LSR yields a zero correction.

### Mechanism 2
- **Claim:** The LSR correction works as a "linearized predictor" evaluated at fixed parameters, but fails if interpreted and applied as a nonlinear parameter update (e.g., Gauss-Newton step).
- **Mechanism:** The correction $\Delta \theta$ is computed to minimize the linearized residual $f(\theta) + J\Delta \theta$. Because the linear approximation is only valid locally, applying $\theta \leftarrow \theta + \Delta \theta$ pushes the network state outside the region where the linear model holds, triggering divergence due to higher-order terms. LSR instead defines the output as $q_{LSR} = q(\theta) + J\Delta \theta$ without modifying the network weights.
- **Core assumption:** The Jacobian-induced subspace admits a solution with a large norm or aggressive step that fits the linear model but violates the nonlinear constraints of the original network function.
- **Evidence anchors:**
  - [page 15] "LSR should be distinguished from Gauss–Newton... directly applying the LSR correction... generally fails to reproduce the refined prediction."
  - [page 5] "LSR evaluates the linearized solution at the same linearization point to define a refined predictor... without assuming global validity."
  - [corpus] No direct corpus support found for this specific "prediction-without-update" constraint.
- **Break condition:** If the subspace rank is too small, the linear correction may be too conservative to aid; if too large, ill-conditioning produces large-norm corrections that destabilize the prediction.

### Mechanism 3
- **Claim:** For operator-constrained problems (e.g., PDEs), alternating LSR with supervised nonlinear alignment accelerates convergence by transforming hard physics-constrained minimization into easier data-fitting steps.
- **Mechanism:** Physics-informed losses create complex landscapes. Iterative LSR (I-LSR) first solves the linearized physics residual to find an "ideal" solution $q_{LSR}$. It then trains the network to fit $q_{LSR}$ in a standard supervised sense. This splits a difficult optimization into a sequence of tractable linear solves and standard fitting steps.
- **Core assumption:** The network has sufficient capacity to approximate the LSR solution, and the "alignment" step preserves the physics constraints sufficiently well between iterations.
- **Evidence anchors:**
  - [page 12] "repeated alternation of linearized refinement and nonlinear alignment leads to a consistent reduction in the overall error."
  - [page 13, figure 5c] Visualizes the alternating reduction of high-frequency error (LSR) and low-frequency error (alignment).
  - [corpus] Neighbor "Efficient Global-Local Fusion Sampling" addresses PINN accuracy but via sampling, not this specific iterative linearization alignment.
- **Break condition:** If the nonlinear alignment step overfits to the LSR target without respecting the original physics, the next LSR step may amplify errors.

## Foundational Learning

- **Concept:** **Jacobian-Vector Products (JVP) & Vector-Jacobian Products (VJP)**
  - **Why needed here:** LSR constructs the subspace basis $V$ using Randomized SVD, which requires computing matrix-vector products with the Jacobian $J$ without explicitly forming the massive $n \times m$ matrix.
  - **Quick check question:** Can you implement a function that computes $Jv$ and $J^T u$ using automatic differentiation (e.g., `torch.autograd.functional.jvp`) for a given batch of data?

- **Concept:** **Condition Number of Least Squares Systems**
  - **Why needed here:** The paper attributes accuracy plateaus to ill-conditioning. Understanding how the condition number of the reduced system $(AJV)$ affects the stability of the direct solve is critical for selecting the subspace rank.
  - **Quick check question:** As you increase the rank $r$ of the subspace, does the condition number of the system $AJV y = f$ typically increase or decrease, and what does that imply for numerical precision?

- **Concept:** **Randomized Singular Value Decomposition (RSVD)**
  - **Why needed here:** This is the dimensionality reduction technique used to project the high-dimensional parameter space into a tractable subspace.
  - **Quick check question:** How does the "oversampling parameter" $p$ in RSVD help capture the range of the Jacobian accurately compared to a naive power iteration?

## Architecture Onboarding

- **Component map:** Inputs -> Subspace Builder (RSVD + JVPs) -> Linearized System Assembler -> Direct Solver (QR) -> Refined Predictor
- **Critical path:** The implementation hinges on efficient batch-wise JVPs. The "Batch LSR" algorithm (Alg S1) is required for large datasets to assemble the reduced system $M$ in streaming fashion without OOM (Out of Memory) errors.
- **Design tradeoffs:**
  - **Rank ($r$):** Low $r$ is stable but may not capture enough corrective capacity. High $r$ captures more directions but risks ill-conditioning (noise amplification) and higher compute/memory cost.
  - **Parameter Subsampling:** For very large networks, the paper suggests randomly subsampling parameters to build $V$, trading off global optimality for memory efficiency.
- **Failure signatures:**
  - **LSR Degradation:** Test error increases after LSR. *Diagnosis:* Rank $r$ is too high, introducing noise/overfitting.
  - **Divergence on Update:** Applying $\theta \leftarrow \theta_0 + \Delta\theta$ crashes the model. *Diagnosis:* Mechanism 2 violation; LSR is a predictor, not a weight update.
  - **Zero Correction:** $\Delta\theta \approx 0$. *Diagnosis:* The pre-trained model is already at a stationary point.
- **First 3 experiments:**
  1. **Sanity Check (Random Init):** Apply One-Shot LSR to a randomly initialized MLP on a simple 2D function (e.g., $sin(x)sin(y)$). Verify that LSR drops error significantly even without training (validates subspace capacity).
  2. **Rank Sweep:** Train a network to plateau. Run LSR with varying ranks (e.g., 10, 50, 100, 500). Plot Test MSE vs. Rank to find the "knee" point where returns diminish or noise dominates.
  3. **Mechanism Validation:** Calculate the linearized loss along the trajectory $\theta(\alpha) = \theta_0 + \alpha \Delta\theta$. Verify that the nonlinear loss diverges from the linearized loss immediately, confirming $\Delta\theta$ cannot be used as a standard descent step.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LSR be adapted for extremely large-scale models (e.g., large transformers) where storing the Jacobian-induced subspace or performing the direct solve becomes memory-prohibitive?
- Basis in paper: [explicit] The authors state that "the feasibility of direct solvers is constrained by the dimension of the refinement subspace and available computational resources," explicitly noting that increasing rank "limits scalability for very large models or extremely high-dimensional residuals."
- Why unresolved: The experiments focus on scientific machine learning architectures (DeepONets, fully connected networks) on a single GPU, and the method relies on explicit subspace storage that grows with the parameter dimension.
- What evidence would resolve it: Demonstration of LSR on models with hundreds of millions of parameters using memory-efficient approximations or distributed solvers.

### Open Question 2
- Question: What specific regularization strategies or rank-selection criteria are required to prevent LSR from overfitting in small-data or noisy regimes?
- Basis in paper: [explicit] The authors identify that "in settings with noisy observations or insufficient training data, overly large refinement subspaces may amplify noise and induce overfitting," but they rely on noise-free or physics-constrained benchmarks in the results.
- Why unresolved: The current work suggests guiding rank by validation performance, but does not propose or test a theoretical or algorithmic mechanism to automatically regularize the refinement against noise.
- What evidence would resolve it: Experiments on noisy regression datasets showing generalization improvement with a proposed noise-robust LSR variant.

### Open Question 3
- Question: Can the geometric structure of the Jacobian-induced subspace be theoretically linked to the "spectral bias" and convergence dynamics of standard gradient-based training?
- Basis in paper: [inferred] The paper observes that LSR removes high-frequency errors that gradient training struggles with (spectral bias) and suggests LSR "may serve as a useful analytical tool for probing the geometry... of neural network representations," but does not formalize this connection.
- Why unresolved: The results empirically show complementary error profiles (low vs. high frequency) but lack a theoretical framework explaining how the subspace geometry dictates these training dynamics.
- What evidence would resolve it: A theoretical analysis or visualizations mapping the eigenmodes of the LSR subspace to the frequency convergence rates of Adam or SGD.

## Limitations
- The core claim that numerical ill-conditioning is the dominant bottleneck rests primarily on a single illustrative example and theoretical reasoning, not fully validated across diverse architectures.
- For large-scale models, parameter subsampling is required, but its impact on solution quality is not thoroughly characterized.
- The linearization assumption is only locally valid, and no quantitative bound on the error from using the linear model is provided.

## Confidence
- **High confidence**: LSR can reduce test error in a post-training setting via direct subspace optimization.
- **Medium confidence**: Ill-conditioning is the primary cause of training plateaus for the studied problems.
- **Medium confidence**: Alternating LSR with nonlinear alignment improves physics-constrained optimization.

## Next Checks
1. **Generalization robustness**: Apply LSR to a diverse set of architectures (e.g., CNNs, transformers) and tasks to verify the ill-conditioning mechanism is not problem-specific.
2. **Error bound analysis**: Derive or empirically estimate the gap between the true nonlinear minimum and the linearized subspace optimum as a function of the Jacobian's spectral properties.
3. **Mechanism isolation**: Compare LSR's performance against a direct least-squares solve on the *same* reduced subspace without the linearization step, to isolate the benefit of the linear model versus subspace restriction.