---
ver: rpa2
title: Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital
arxiv_id: '2512.18658'
source_url: https://arxiv.org/abs/2512.18658
tags:
- legal
- table
- tie-out
- equall
- comp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of automating capitalization
  table tie-out in venture capital, a complex legal due diligence task requiring multi-document
  reasoning, strict evidence traceability, and deterministic outputs. The authors
  analyze real-world datarooms and identify three key scaling challenges: super-linear
  growth in evidentiary burden, shifting anomaly types, and exploding verification
  workload.'
---

# Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital

## Quick Facts
- arXiv ID: 2512.18658
- Source URL: https://arxiv.org/abs/2512.18658
- Reference count: 17
- Primary result: Equall achieves 85% F1 accuracy on cap table tie-out vs 29-42% for baselines

## Executive Summary
This paper tackles the complex legal due diligence task of automating capitalization table (cap table) tie-out in venture capital. The challenge requires verifying that every security issuance and term in a reference cap table can be traced to supporting documents in a dataroom, with detection of three anomaly types: Terms Discrepancy, Missing Documentation, and Missing from Cap Table. Through analysis of real-world datarooms, the authors identify super-linear scaling challenges in evidence burden, anomaly complexity, and verification workload. They propose Equall, a two-stage "eager construction" approach that builds a layered world model through specialized LLM-based extraction and inductive event modeling, followed by targeted neuro-symbolic verification. Experiments show 85% F1 accuracy, 22× faster verification, and 5-hour manual review time (vs 27 hours) while maintaining 81.5% accuracy.

## Method Summary
The Equall system addresses cap table tie-out through a two-stage "eager construction" pipeline. Stage 1 uses specialized LLM parsers to classify documents and extract low-level nodes (Stakeholders, Securities, atomic values) with provenance. Stage 2 employs inductive Event Modeling to synthesize Conceptual Nodes (Issuance, Transfer, Amendment) into an Event Graph that captures the complete historical lineage. A neuro-symbolic verification engine then executes deterministic graph queries for each transform T_k and compares computed C_virt against C_ref per anomaly category. This approach contrasts with agentic baselines that use GPT-5.1 with RAG for end-to-end reasoning. The method achieves O(n) verification time growth versus O(n²) for agentic approaches.

## Key Results
- Equall achieves 85% F1 accuracy on anomaly detection, significantly outperforming agentic baselines (29-42% F1)
- Verification is 22× faster per check, reducing total manual review time from 27 hours to 5 hours
- Scales efficiently from Seed to Series B stages with 81.5% accuracy maintained
- O(n) verification time growth versus O(n²) for agentic approaches

## Why This Works (Mechanism)
The method works by decomposing the complex multi-document reasoning task into specialized extraction followed by deterministic verification. By eagerly constructing a complete Event Graph that captures all securities, stakeholders, and their historical relationships, the system avoids the computational explosion of agentic approaches that must reason across all document combinations. The neuro-symbolic verification provides strict evidence traceability by deterministically checking each transform against the ground truth cap table. This architectural choice trades initial construction overhead for massive gains in verification efficiency and accuracy.

## Foundational Learning
- Cap Table Tie-Out: Verifying that every security and term in a reference cap table traces to supporting legal documents. Needed because it's the core legal due diligence task requiring evidence traceability.
- Event Graph Modeling: Representing legal relationships as nodes (Stakeholders, Securities, Events) with provenance links. Needed to capture the complete historical lineage for deterministic verification.
- Neuro-Symbolic Verification: Combining LLM-based extraction with deterministic graph queries for evidence-based checking. Needed to ensure strict traceability while handling unstructured legal text.
- Multi-Document Reasoning: Synthesizing information across heterogeneous legal documents to build a coherent world model. Needed because cap tables require understanding relationships across SAFEs, SPAs, amendments, etc.

## Architecture Onboarding
Component map: Document Ingestion -> Stage 1 Extraction -> Stage 2 Event Modeling -> Neuro-Symbolic Verification -> Output
Critical path: The verification engine is the critical path, as it must execute deterministic queries against the Event Graph for each transform. Bottlenecks occur in event chain resolution and amendment lineage tracking.

Design tradeoffs: Eager construction vs. agentic reasoning trades initial overhead for verification efficiency. Specialized extraction vs. general reasoning trades flexibility for accuracy. Deterministic verification vs. probabilistic reasoning trades some nuance for strict evidence traceability.

Failure signatures: Low recall on "Missing Documentation" indicates incomplete Event Graph lineage chains. Precision drops suggest incorrect amendment chain resolution. Both can be diagnosed through Event Graph audits.

First experiments:
1. Test document classification accuracy on a small set of legal documents
2. Verify extraction accuracy for Stakeholders and Securities from sample documents
3. Test Event Graph construction on synthetic issuance and transfer sequences

## Open Questions the Paper Calls Out
None

## Limitations
- Proprietary evaluation data prevents independent verification of 85% F1 accuracy claims
- Critical implementation details for Event Graph schema and verification transforms remain underspecified
- No ablation studies on individual pipeline components to isolate contribution of each stage
- Limited to US venture capital context, generalizability to other legal systems unknown

## Confidence
| Claim | Confidence |
|-------|------------|
| 85% F1 accuracy vs 29-42% baselines | Medium |
| 22× speedup in verification | Medium |
| O(n) vs O(n²) scaling | Medium |

## Next Checks
1. Reconstruct the Event Graph schema and verification query functions from textual descriptions, then test on publicly available cap table and legal document samples to verify deterministic verification logic.
2. Implement the two-stage extraction pipeline with a general-purpose LLM and measure precision/recall on synthetic test cases covering each anomaly category.
3. Conduct scaling experiments using reported document counts (204-487 docs, 2K-6.7K pages) with synthetic or proxy data to validate claimed O(n) verification time growth pattern.