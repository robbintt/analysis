---
ver: rpa2
title: 'PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text
  with LLMs'
arxiv_id: '2509.19745'
source_url: https://arxiv.org/abs/2509.19745
tags:
- speech
- multilingual
- language
- alignment
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of aligning speech and text representations
  in multilingual Speech Large Models (SLMs), where forcing cross-language convergence
  degrades performance. The proposed Progressive Alignment Representation Training
  (PART) introduces a multi-stage, multi-task framework that separates within-language
  alignment from cross-language alignment.
---

# PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs

## Quick Facts
- arXiv ID: 2509.19745
- Source URL: https://arxiv.org/abs/2509.19745
- Reference count: 0
- Key outcome: Up to 41% reduction in WER for ASR and significant BLEU gains for S2TT using progressive three-stage training with task-dependent LLM activation

## Executive Summary
This paper addresses the challenge of aligning speech and text representations in multilingual Speech Large Models (SLMs) by proposing Progressive Alignment Representation Training (PART). The core insight is that forcing cross-language convergence degrades multilingual performance, so PART introduces a three-stage, task-dependent fine-tuning framework that separates within-language alignment from cross-language alignment. The method uses an encoder-LLM architecture with adapters, where LLM parameters are dynamically activated only during cross-language tasks, and text-based tasks are introduced in the final stage to enhance multilingual understanding. Experiments across CommonVoice 15, Fleurs, Wenetspeech, and CoVoST2 demonstrate significant improvements, particularly for low-resource language pairs.

## Method Summary
PART employs a three-stage progressive training approach for multilingual speech-to-text tasks using an encoder-LLM architecture. Stage 1 trains only the adapter on monolingual ASR data while freezing encoder and LLM. Stage 2 progressively unfreezes the encoder (last 8 layers first, then full encoder) on monolingual data. Stage 3 jointly optimizes all components on combined monolingual and cross-lingual data while unfreezing the LLM, with additional text-based tasks to enhance multilingual generation. The architecture uses SenseVoice-large encoder (~700M), Qwen2.5 LLM (1.5B/7B), and a 2-transformer-layer + 1-CNN-layer adapter. Training uses 256 A800 GPUs, 3 epochs, and greedy decoding.

## Key Results
- Achieves up to 41% reduction in WER for ASR compared to conventional approaches
- Significant BLEU score gains for speech-to-text translation, particularly in low-resource language pairs (en→sv, en→id, en→ar)
- Outperforms conventional methods that force cross-language convergence of audio representations
- Demonstrates effectiveness across multiple benchmarks: CommonVoice 15, Fleurs, Wenetspeech, and CoVoST2

## Why This Works (Mechanism)

### Mechanism 1: Progressive Alignment Prevents Cross-Language Representation Collapse
The three-stage training approach first aligns speech and text within individual languages using monolingual data (Stages 1-2), establishing robust language-specific representations before introducing cross-lingual tasks (Stage 3). Task-dependent LLM activation—frozen for ASR, unfrozen for S2TT—allows the encoder to handle acoustic-semantic mapping while the LLM manages cross-lingual generation. This prevents premature forcing of cross-language convergence that would otherwise degrade multilingual performance.

### Mechanism 2: Task-Dependent LLM Activation Leverages Multilingual Priors
The LLM (Qwen2.5 with multilingual continuing pre-training) contains rich cross-lingual semantic knowledge. By keeping it frozen during ASR tasks, the speech encoder must map speech directly to appropriate language regions in the LLM's embedding space. Only during S2TT tasks is the LLM unfrozen, allowing it to adapt multilingual generation capabilities while preserving the encoder's language-specific mappings. This selective freezing preserves the LLM's pre-trained multilingual knowledge.

### Mechanism 3: Text-Based Fine-Tuning Enhances Cross-Lingual Transfer
After speech-text alignment in Stages 1-2, Stage 3 introduces text-only multilingual tasks alongside cross-lingual speech tasks. This activates the LLM's full multilingual generation capacity and provides additional multilingual signal that transfers to speech-to-text generation, particularly benefiting low-resource language pairs where speech data is limited.

## Foundational Learning

- **Concept: Speech-Text Modality Alignment**
  - Why needed here: PART operates on the premise that speech and text occupy different representational spaces requiring alignment for effective SLM operation
  - Quick check question: Why might training a speech encoder to match LLM text embeddings directly fail for multilingual speech compared to monolingual speech?

- **Concept: Encoder-LLM Architecture with Adapters**
  - Why needed here: PART uses a specific architecture (encoder → adapter → LLM) with parameterized components
  - Quick check question: For Stage 2 implementation, which parameters would you update, and why use progressive unfreezing (last 8 layers first) instead of full encoder unfreezing?

- **Concept: Cross-Lingual Transfer in Multilingual LLMs**
  - Why needed here: The paper assumes the LLM has inherent multilingual knowledge leveraged for speech translation
  - Quick check question: Why would activating the LLM for cross-lingual tasks (S2TT) be beneficial, while keeping it frozen for within-language tasks (ASR) preserves performance?

## Architecture Onboarding

- **Component map:** Speech Encoder → Adapter → LLM
- **Critical path:** Stage 1 (adapter-only, $D_{mono}$) → Stage 2 (encoder + adapter, $D_{mono}$, progressive unfreezing: last 8 → full) → Stage 3 (all components, $D_{mono} + D_{cross}$ + text tasks)
- **Design tradeoffs:** Staged training increases complexity vs. single-stage joint training; task-dependent freezing adds implementation overhead but preserves language-specific features; text-based fine-tuning may introduce distribution shift but enhances low-resource performance
- **Failure signatures:** High WER on specific languages despite good average performance → encoder capacity issue or language-specific data imbalance; low BLEU on high-resource pairs → overfitting to text tasks in Stage 3; training instability in Stage 3 → LLM unfreezing with large learning rate or conflicting objectives; degraded ASR after Stage 3 → cross-lingual training corrupting within-language alignment
- **First 3 experiments:**
  1. Reproduce baseline comparison: Train two-stage model vs. PART three-stage on zh-en/en-zh subset, measuring WER on CommonVoice and BLEU on CoVoST2
  2. Ablate task-dependent freezing: Train with LLM frozen throughout all stages vs. unfrozen in Stage 3 only vs. unfrozen throughout, on ASR and S2TT tasks separately
  3. Probe text task impact: Run Stage 3 with and without text-based tasks on low-resource pairs (en→sv, en→id) to quantify the 3-8 BLEU gain claim

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but several remain unresolved based on the methodology and results presented.

## Limitations
- Lack of detailed ablation studies for individual mechanisms, with progressive vs. full encoder unfreezing showing only minor differences
- Unclear contribution of text-based tasks to low-resource language performance, quantified only qualitatively as "3-8 BLEU" gains
- No comparisons against other encoder-decoder architectures that might achieve similar results without LLM integration
- Absence of text-only benchmark evaluations to assess potential regression from LLM fine-tuning

## Confidence
- Progressive alignment effectiveness: Medium - demonstrated empirically but lacks detailed mechanism ablation
- Task-dependent LLM activation: Medium - shows performance gains but ablation on this specific mechanism is missing
- Text-based task contribution: Low - mentioned conceptually but not explicitly defined or quantitatively validated

## Next Checks
1. Reproduce the baseline comparison between two-stage and three-stage training on a subset of language pairs to validate the 41% WER reduction claim
2. Conduct ablation studies on task-dependent LLM activation by comparing frozen vs. unfrozen configurations across ASR and S2TT tasks
3. Quantify the impact of text-based tasks by running Stage 3 with and without these tasks on low-resource language pairs