---
ver: rpa2
title: Do Large Language Models know who did what to whom?
arxiv_id: '2504.16884'
source_url: https://arxiv.org/abs/2504.16884
tags:
- thematic
- sentence
- llms
- language
- role
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large Language Models (LLMs) are commonly criticized for not understanding
  language. However, many critiques focus on cognitive abilities that, in humans,
  are distinct from language processing.
---

# Do Large Language Models know who did what to whom?

## Quick Facts
- arXiv ID: 2504.16884
- Source URL: https://arxiv.org/abs/2504.16884
- Reference count: 18
- Large Language Models (LLMs) can extract thematic roles (who did what to whom) from sentences, but this information influences their overall representations more weakly than syntactic structure does.

## Executive Summary
This study investigates whether Large Language Models (LLMs) capture thematic role information (who did what to whom) in their internal representations. Through two experiments using representational similarity analysis and linear SVM probing, the authors find that while LLMs can extract thematic roles through specific attention mechanisms, their overall sentence representations prioritize syntactic similarity over semantic meaning. The results suggest that word prediction training induces a representational geometry where grammatical construction dominates over event identity, though specialized attention heads can still identify thematic roles independent of syntax.

## Method Summary
The study used two experiments with controlled sentence stimuli. Experiment 1 tested 94 base sentences in four conditions (same/different semantics × same/different syntax), while Experiment 2 used 50 sets of 24 ditransitive sentences with varied structures. Researchers extracted hidden states from final layers at specified tokens ([CLS] for BERT, '.' for others), normalized using COCA-derived statistics, and computed pairwise cosine similarities. They applied linear SVMs with 66-fold cross-validation to classify whether sentence pairs had same vs. opposite thematic roles, and analyzed attention head patterns for verb-to-noun relationships.

## Key Results
- LLM sentence representations show higher similarity for pairs sharing syntax than pairs sharing thematic roles
- Specific attention heads robustly capture thematic role information independent of syntactic position
- Linear classifiers on hidden unit subsets show weak performance (approx 60% accuracy) for thematic role classification
- Representational geometry differs markedly from human similarity judgments, which prioritize semantic content over syntax

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Distributed hidden states in LLMs prioritize syntactic structure over thematic role assignments (semantic meaning).
- **Mechanism:** The word prediction objective induces a representational geometry where sentence similarities are driven by grammatical construction (e.g., active vs. passive) rather than event identity (who did what to whom).
- **Core assumption:** Sentence-level representations extracted from tokens like `[CLS]` or the final period effectively aggregate the model's "understanding" of the sentence.
- **Evidence anchors:**
  - [abstract] "...in LLMs the overall representational similarity of sentence pairs reflected syntactic similarity but not whether their agent and patient assignments were identical vs. reversed."
  - [section: Results, LLMs] "SEMd-SYNTs > SEMs-SYNTd... demonstrates that syntax exerts a stronger influence on LLM representations than thematic roles do."
- **Break condition:** If models were trained on an objective that explicitly penalized semantic role confusion, or if representations were probed at non-aggregated token levels.

### Mechanism 2
- **Claim:** Specialized attention heads explicitly compute thematic role binding independent of syntax.
- **Mechanism:** Specific attention heads (e.g., BERT Layer 11 Head 5) learn to direct attention from verbs and objects to the "agent" regardless of the agent's linear position or grammatical subject status.
- **Core assumption:** The attention weights reflect functional circuitry for role assignment rather than spurious statistical correlations.
- **Evidence anchors:**
  - [abstract] "...some attention heads robustly captured thematic roles, independently of syntax."
  - [section: Results, SVM: attention heads] "The verb... and direct object... both allocated more attention to the agent than to the patient... robustly reflect thematic roles, independent of syntax."
- **Break condition:** If attention heads in unidirectional models (GPT, Llama) fail to exhibit this pattern due to lack of "future" context.

### Mechanism 3
- **Claim:** Thematic role information is linearly accessible in attention patterns but only weakly accessible in hidden unit subspaces.
- **Mechanism:** While attention heads show high classification accuracy (approx 80%) for role matching, linear classifiers trained on hidden unit subsets show low accuracy (approx 60%), suggesting the signal is not robustly encoded in the primary residual stream.
- **Core assumption:** Conceptual information respects the "linear subspace hypothesis" and does not require non-linear decoders to detect.
- **Evidence anchors:**
  - [section: Discussion] "Whereas activity patterns in subsets of hidden units often allowed for significant classification... the effect sizes were small."
  - [section: Results, SVM: hidden units] "Many SVM results significantly exceeded chance level... but with a relatively small effect size, mostly below 0.6 accuracy."
- **Break condition:** If non-linear probes could recover high-fidelity role information from hidden units, invalidating the linear subspace assumption.

## Foundational Learning

- **Concept: Thematic Roles (Agent vs. Patient)**
  - **Why needed here:** The study isolates "who did what to whom" as the minimal semantic unit, distinct from syntax. Without this, one cannot distinguish "The tiger punched the panther" from "The panther punched the tiger."
  - **Quick check question:** In the sentence "The boy was bitten by the dog," what assigns the role of Agent?

- **Concept: Representational Similarity Analysis (RSA)**
  - **Why needed here:** RSA allows the comparison of LLM internal geometry against human similarity judgments using cosine distance, bypassing the need for the model to output an explanation.
  - **Quick check question:** If Model A views Sentence X and Y as identical vectors, but humans rate them as different, what does that imply about Model A's alignment with human cognition?

- **Concept: Probing vs. Prompting**
  - **Why needed here:** The authors argue prompting (asking "who is the agent?") adds "task demands" (metalinguistic understanding), whereas probing hidden states reveals latent competence.
  - **Quick check question:** Why might an LLM answer "I don't know" to a prompt while still encoding the answer in its hidden states?

## Architecture Onboarding

- **Component map:** Tokenize sentences -> Pass through Transformer layers -> Extract hidden states at [CLS]/'.' token -> Compute cosine similarities OR Extract attention weights from specific heads -> Linear SVM classification

- **Critical path:**
  1. Tokenize sentences
  2. Pass through Transformer layers
  3. **Branch A (Global Rep):** Extract final token/CLS vector → Cosine Similarity (Result: Syntax dominates)
  4. **Branch B (Local Circuit):** Extract Attention Weights A_{ij} (verb → nouns) → Linear SVM (Result: Thematic roles detected)

- **Design tradeoffs:**
  - **Stimuli Design:** The study uses "reversible" sentences (chef pushed painter) rather than plausible ones (chef baked cake) to force the model to use syntax, not world knowledge
  - **Model Selection:** Using BERT for attention analysis because it is bidirectional; unidirectional models (GPT) limit attention analysis to "past" tokens only

- **Failure signatures:**
  - **Syntactic Heuristic Trap:** If a model relies on "noun before verb = agent," it will fail on passive sentences. The study shows LLM hidden states essentially do this
  - **Low Classification Variance:** In hidden unit SVMs, high variance across folds indicates the model has not settled on a robust representation for roles

- **First 3 experiments:**
  1. **Sanity Check (RSA):** Feed "The tiger punched the panther" and "The panther punched the tiger" into BERT/Llama; verify that the hidden state similarity is *higher* for pairs sharing syntax than pairs sharing meaning (confirming the paper's core finding)
  2. **Head Ablation:** Locate the "Agent-Patient Head" (e.g., L11H05 in BERT). Run the SVM classification task with this head zeroed out to see if performance drops to chance
  3. **Generalization Test:** Train the SVM on simple active/passive sentences and test on the complex cleft sentences (e.g., "It was the man who...") used in Experiment 2 to verify if the attention mechanism generalizes across syntactic complexity

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do attention heads in unidirectional models (e.g., GPT-2, Llama 2) robustly capture thematic role information?
- **Basis in paper:** [explicit] The authors state, "Future work may test whether heads in unidirectional models also extract thematic role information."
- **Why unresolved:** The study's attention analysis was restricted to BERT because its bidirectional attention mechanism allowed the necessary "forward-looking" analysis for the specific evaluation method used.
- **What evidence would resolve it:** Applying the same attention head classification analysis used on BERT to the unidirectional models (GPT-2, Llama 2, Persimmon) included in the hidden unit analysis.

### Open Question 2
- **Question:** Does the thematic role information present in attention heads causally influence next-word prediction or behavior?
- **Basis in paper:** [explicit] The authors propose, "Future work may also consider the casual influence of thematic role information in attention heads (or circuits of attention) on next word prediction and LLM behavior more broadly."
- **Why unresolved:** The current study demonstrated that thematic information is *encoded* in attention heads, but not whether this information is functionally *used* by the model during generation.
- **What evidence would resolve it:** Intervention studies (e.g., specific head ablation or activation patching) that assess whether disabling these heads degrades the model's ability to predict contextually appropriate agents or patients.

### Open Question 3
- **Question:** Is thematic role information encoded non-linearly within the distributed hidden representations of LLMs?
- **Basis in paper:** [inferred] The authors note they used "rather simple classifiers (linear SVMs)" and acknowledge, "It is possible that thematic role information is present non-linearly within the hidden representations of LLMs."
- **Why unresolved:** The study found weak evidence for thematic roles using linear probes, but the "linear subspace hypothesis" tested may not capture complex non-linear dependencies.
- **What evidence would resolve it:** Testing hidden unit representations with non-linear classifiers (e.g., Multi-Layer Perceptrons) to determine if classification accuracy for thematic roles significantly improves over linear SVMs.

### Open Question 4
- **Question:** Does training with reinforcement learning from human feedback (RLHF) alter how LLMs represent thematic roles relative to syntax?
- **Basis in paper:** [explicit] The authors suggest, "Models exposed to non-linguistic training, e.g., reinforcement learning from human feedback, may process thematic roles differently than those trained only on word prediction."
- **Why unresolved:** The study focused exclusively on pre-trained models without fine-tuning to isolate the effects of the word-prediction objective.
- **What evidence would resolve it:** Comparing the representational similarity and classification performance of a base model against its RLHF-aligned counterpart (e.g., GPT-2 vs. InstructGPT) on the same thematic role stimuli.

## Limitations

- The study assumes linear classifiers capture all accessible thematic role information, but non-linear probes might recover stronger signals from hidden units
- The interpretation of attention weights as functional circuits relies on observed patterns rather than ablation studies confirming causal importance
- The RSA findings based on reversible sentences may not generalize to naturalistic language where semantic plausibility provides additional cues beyond syntax

## Confidence

- **High confidence:** The core finding that LLM sentence representations prioritize syntax over thematic roles in overall representational similarity. This is directly measured through multiple methods and shows consistent patterns across four models.
- **Medium confidence:** The claim that attention heads capture thematic roles independently of syntax. While the evidence is robust, the interpretation of attention weights as functional circuits rather than statistical patterns has uncertainty.
- **Medium confidence:** The conclusion that thematic role information is weakly encoded in hidden units. The low SVM accuracy and effect sizes support this, but the linear probe limitation means stronger encoding might exist.

## Next Checks

1. **Ablation study of attention heads:** Zero out the identified "Agent-Patient Head" (e.g., BERT Layer 11 Head 5) and re-run the SVM classification task to verify performance drops to chance, confirming causal importance.

2. **Non-linear probing of hidden units:** Apply non-linear classifiers (e.g., neural network probes) to the same hidden unit subsets to test whether thematic role information is present but requires non-linear extraction methods.

3. **Cross-linguistic generalization:** Test the same methodology on multilingual models with sentences from languages with different syntactic structures (e.g., languages with freer word order) to verify if the syntax-over-semantics bias is language-independent.