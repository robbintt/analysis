---
ver: rpa2
title: 'Cooperation of Experts: Fusing Heterogeneous Information with Large Margin'
arxiv_id: '2505.20853'
source_url: https://arxiv.org/abs/2505.20853
tags:
- experts
- learning
- information
- networks
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a Cooperation of Experts (CoE) framework for
  fusing heterogeneous information in multiplex networks. The method employs two-level
  experts: low-level experts learn patterns from individual networks while high-level
  experts capture shared information across networks through mutual information maximization.'
---

# Cooperation of Experts: Fusing Heterogeneous Information with Large Margin

## Quick Facts
- **arXiv ID**: 2505.20853
- **Source URL**: https://arxiv.org/abs/2505.20853
- **Reference count**: 40
- **Primary result**: Achieves 94.21% accuracy on ACM multiplex network dataset

## Executive Summary
This paper introduces a Cooperation of Experts (CoE) framework for fusing heterogeneous information in multiplex networks. The method employs a two-level expert system where low-level experts learn patterns from individual networks while high-level experts capture shared information across networks through mutual information maximization. To prevent competition between experts and promote collaboration, CoE implements a large margin mechanism with a confidence tensor that dynamically adjusts expert contributions based on their proficiency. The framework demonstrates superior performance on five multiplex network datasets and four multimodal datasets compared to state-of-the-art methods.

## Method Summary
The CoE framework operates through a hierarchical expert system that processes heterogeneous network data. Low-level experts independently analyze each network layer, learning specialized patterns and representations. These representations are then passed to high-level experts that capture shared information across all networks using mutual information maximization. A key innovation is the large margin mechanism with confidence tensor, which dynamically adjusts the contribution of each expert based on their proficiency in handling specific data patterns. This prevents competition between experts and ensures optimal information fusion. The framework is theoretically grounded with proofs of convexity, Lipschitz continuity, and convergence guarantees under idealized conditions.

## Key Results
- Achieves 94.21% accuracy on ACM multiplex network dataset
- Achieves 81.11% accuracy on ESP multimodal dataset
- Outperforms state-of-the-art methods including recent Graph-MoE approaches
- Demonstrates robustness to structural attacks while maintaining minimal hyperparameter sensitivity

## Why This Works (Mechanism)
The framework succeeds by creating a hierarchical expert system where each level has a specific role in processing heterogeneous information. Low-level experts specialize in learning patterns within individual networks, capturing domain-specific features and relationships. High-level experts then synthesize these specialized representations to identify shared patterns across networks through mutual information maximization. The large margin mechanism with confidence tensor prevents the typical competition seen in mixture-of-experts models by dynamically adjusting contributions based on proficiency, ensuring that the most capable expert for each pattern receives appropriate weight.

## Foundational Learning
- **Mutual Information Maximization**: Measures shared information between networks; needed to identify common patterns across heterogeneous data sources
- **Large Margin Classification**: Creates separation between classes; needed to prevent competition and ensure clear decision boundaries for experts
- **Mixture of Experts**: Combines multiple specialized models; needed to handle heterogeneous information with different network structures
- **Confidence Tensor**: Dynamic weighting mechanism; needed to adaptively adjust expert contributions based on proficiency
- **Multiplex Network Analysis**: Studies networks with multiple layers; needed as the primary data structure being processed

## Architecture Onboarding

**Component Map**: Input Networks -> Low-Level Experts -> High-Level Experts -> Confidence Tensor -> Output

**Critical Path**: The flow moves from raw network inputs through low-level experts (specialized processing), to high-level experts (shared information extraction), then through the confidence tensor (dynamic weighting), and finally to the output layer.

**Design Tradeoffs**: The framework trades computational complexity for improved performance through the two-level expert system and mutual information maximization. The large margin mechanism adds overhead but prevents the competition issues common in mixture-of-experts models.

**Failure Signatures**: Poor performance on individual network layers may indicate inadequate low-level expert specialization. Failure to capture shared patterns suggests issues with the mutual information maximization component. Unstable predictions could indicate problems with the confidence tensor calibration.

**3 First Experiments**:
1. Test individual low-level expert performance on single network layers before combining
2. Evaluate high-level expert performance with synthetic shared patterns
3. Validate the confidence tensor mechanism with controlled proficiency variations

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees rely on idealized assumptions rarely encountered in real-world multiplex networks
- Experimental validation focuses on structural attacks but does not explore sophisticated adversarial feature manipulation
- Mutual information maximization may miss domain-specific relationships requiring specialized modeling approaches

## Confidence
- **High confidence**: General framework architecture and two-level expert concept
- **Medium confidence**: Large margin mechanism's effectiveness in preventing competition
- **Medium confidence**: Performance improvements over baseline methods
- **Low confidence**: Theoretical convergence guarantees under real-world conditions

## Next Checks
1. Test CoE's performance under sophisticated attack scenarios including adversarial feature manipulation and targeted node attacks
2. Conduct systematic ablation studies removing components to quantify individual contributions
3. Evaluate scalability on larger multiplex networks (10,000+ nodes per layer) to assess computational efficiency and memory requirements