---
ver: rpa2
title: 'Z-Scores: A Metric for Linguistically Assessing Disfluency Removal'
arxiv_id: '2509.20319'
source_url: https://arxiv.org/abs/2509.20319
tags:
- disfluency
- alignment
- removal
- text
- z-scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Z-Scores, a span-level evaluation metric\
  \ designed to assess disfluency removal in speech transcription. Unlike traditional\
  \ word-level metrics (precision, recall, F1) that only provide aggregate performance,\
  \ Z-Scores categorize model behavior across distinct disfluency types\u2014EDITED,\
  \ INTJ, and PRN\u2014using a deterministic alignment module to map generated outputs\
  \ to disfluent transcripts."
---

# Z-Scores: A Metric for Linguistically Assessing Disfluency Removal

## Quick Facts
- arXiv ID: 2509.20319
- Source URL: https://arxiv.org/abs/2509.20319
- Reference count: 0
- One-line primary result: Introduces Z-Scores, a span-level metric that categorizes disfluency removal performance across EDITED, INTJ, and PRN types to reveal hidden model weaknesses.

## Executive Summary
This paper introduces Z-Scores, a span-level evaluation metric designed to assess disfluency removal in speech transcription. Unlike traditional word-level metrics (precision, recall, F1) that only provide aggregate performance, Z-Scores categorize model behavior across distinct disfluency types—EDITED, INTJ, and PRN—using a deterministic alignment module to map generated outputs to disfluent transcripts. The proposed metric enables category-specific diagnostics, revealing systematic weaknesses in model performance that aggregate scores obscure. A case study with large language models demonstrates that Z-Scores uncover specific challenges with INTJ and PRN disfluencies hidden in overall F1 scores, directly informing targeted model refinement strategies such as tailored prompts or data augmentation. The authors release an open-source Python package to facilitate community adoption of this standardized evaluation approach.

## Method Summary
The method introduces Z-Scores as a span-level metric to evaluate disfluency removal across three linguistically-defined categories: EDITED (repairs/restarts), INTJ (interjections like uh/umm), and PRN (parentheticals like you know/I mean). It uses a deterministic alignment module A, built on a modified Gestalt pattern matching algorithm, to align model-generated fluent text with the original disfluent transcript. Token-level decisions (should remove? was removed?) are aggregated into category-specific accuracy scores. The framework also computes traditional word-level E-Scores (precision, recall, F1) for comparison. The open-source Python package implements this evaluation pipeline, requiring ground-truth disfluency annotations, disfluent text, and model outputs.

## Key Results
- Z-Scores reveal category-specific weaknesses: baseline model shows Z_I=61.89 and Z_P=65.02 despite reasonable overall E-Scores.
- Metaprompting improves targeted categories: INTJ/PRN-focused prompts increase Z_I by ~16 points and Z_P by ~9 points without affecting Z_E.
- Standard F1 scores mask failure modes: aggregate metrics hide challenges with INTJ and PRN disfluencies that Z-Scores expose.
- Deterministic alignment enables reproducible evaluation: modified Gestalt matching guarantees consistent token alignment across runs.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Category-specific scoring reveals failure modes hidden by aggregate metrics
- Mechanism: Z-Scores decompose performance into three linguistically-grounded categories (EDITED, INTJ, PRN) computed as span-level accuracy per category. This decomposition exposes per-category weaknesses that F1 averages mask.
- Core assumption: Disfluency types are sufficiently distinct that category-level diagnosis generalizes to new inputs.
- Evidence anchors:
  - [abstract]: "Z-Scores uncover challenges with INTJ and PRN disfluencies hidden in aggregate F1"
  - [section 4]: Baseline Z_I=61.89 and Z_P=65.02 reveal weaknesses despite reasonable overall E-Scores; after metaprompting, Z_I improved ~16 points and Z_P ~9 points while Z_E remained stable.
  - [corpus]: Neighbor paper "DRES: Benchmarking LLMs for Disfluency Removal" (FMR=0.54) confirms community need for categorical disfluency benchmarks, though validation across corpora remains limited.
- Break condition: If categories are imbalanced (e.g., few PRN tokens in test data), Z-Score variance increases and diagnostic reliability degrades.

### Mechanism 2
- Claim: Modified Gestalt alignment enables deterministic mapping for generative outputs
- Mechanism: The alignment module A appends special tokens with disfluency tags (e.g., "the§EDITED") before running Gestalt pattern matching G. This forces disfluent tokens into the "replace" case, prioritizing NONE-tagged matches over disfluent-tagged matches when both are possible, yielding deterministic alignment.
- Core assumption: Tokenization with TreebankWordTokenizer is consistent across input and output sequences; hallucinated tokens can be reliably identified and filtered.
- Evidence anchors:
  - [section 3.1]: Describes the modification from G to A with concrete example showing incorrect vs. correct alignment outputs.
  - [section 2.3]: Prior methods (LCS, statistical weighting) either produce systematic errors or lack determinism; A provides alignment guarantee.
  - [corpus]: No direct corpus validation of alignment correctness; this mechanism is paper-internal and would benefit from external reproduction.
- Break condition: If generated text contains paraphrases or reordering beyond deletion, alignment may fail or require fallback.

### Mechanism 3
- Claim: Diagnostic scores directly inform targeted interventions
- Mechanism: By isolating category-specific performance (Z_E, Z_I, Z_P), practitioners can design category-aware prompts or augmentation strategies. The case study demonstrates that prompts containing INTJ/PRN examples selectively improve Z_I and Z_P.
- Core assumption: Interventions transfer across datasets and the causal link between category diagnosis and improvement is stable.
- Evidence anchors:
  - [abstract]: "directly informing model refinement strategies"
  - [section 4]: Metaprompts P1 and P2 include explicit INTJ/PRN examples; Z-Scores confirm gains are localized to those categories.
  - [corpus]: Weak external evidence—no corpus papers validate intervention transferability; mechanism is demonstrated within the paper only.
- Break condition: If prompt engineering affects multiple categories simultaneously or causes regression in previously strong categories, targeted optimization requires multi-objective formulation.

## Foundational Learning

- Concept: Shriberg's disfluency taxonomy (EDITED, INTJ, PRN)
  - Why needed here: Z-Scores are grounded in this linguistic framework; understanding that EDITED captures repairs/restarts, INTJ captures hesitation markers (um/uh), and PRN captures discourse markers (you know/I mean) is essential for interpreting scores.
  - Quick check question: Given the utterance "I want the blue—sorry, red shirt, you know?", which tokens fall into each category?

- Concept: Span-level vs. word-level evaluation
  - Why needed here: E-Scores operate on individual token decisions (tp/fp/tn/fn), while Z-Scores evaluate whether entire disfluency spans are correctly removed. This distinction affects how scores should be interpreted and compared.
  - Quick check question: If a model removes 2 of 3 tokens from a parenthetical span, what is the Z_P contribution?

- Concept: Gestalt pattern matching
  - Why needed here: The alignment module builds on this string-matching algorithm; understanding its "early matching" behavior clarifies why the modification (appending tag tokens) is necessary.
  - Quick check question: Without the A modification, why would G incorrectly align a repeated token appearing as both EDITED and NONE?

## Architecture Onboarding

- Component map: Tokenizer (TreebankWordTokenizer) -> Alignment Module A (modified Gestalt) -> Parallel scoring branches: E-Score calculator (word-level P/R/F1) and Z-Score calculator (span-level category accuracy). Hallucination filter runs post-alignment to remove generated tokens not present in source.

- Critical path: Input (t_disfluent, t_tag, t_Φ) -> Tokenization -> A alignment -> (1_gt, 1_pred) derivation -> Z-Score computation per category. Alignment correctness is the bottleneck; errors propagate to all downstream scores.

- Design tradeoffs:
  - Determinism vs. robustness: A guarantees deterministic alignment but may fail on paraphrase; statistical methods (e.g., [22]) handle variation but introduce nondeterminism.
  - Granularity vs. data requirements: Span-level Z-Scores require sufficient category samples per evaluation; sparse categories yield unstable estimates.

- Failure signatures:
  - Z_Score = NaN: Category has zero ground-truth instances in input (e.g., Z_I=NaN when no INTJ tokens present).
  - E_P low, Z_Scores high: Model is over-deleting fluent tokens while correctly removing disfluencies.
  - High variance in repeated runs (noted in Table 2 std. dev. up to 22.06 for Z_P): Small sample sizes per category require aggregation across multiple inputs.

- First 3 experiments:
  1. Validate alignment correctness on a manually annotated test set (10-20 examples) covering edge cases: overlapping spans, repeated tokens, hallucinations.
  2. Compare Z-Score distributions across at least two models (e.g., gpt-4o-mini vs. a smaller SLM) on the same Switchboard subset to establish baseline category-specific performance gaps.
  3. Test intervention transfer: apply INTJ-targeted prompts from Section 4 to a different corpus (e.g., custom domain data) and measure whether Z_I improvement replicates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can architectural modifications, such as disfluency-category specialized adapters, yield higher Z-Scores than the metaprompting strategies currently demonstrated?
- Basis in paper: [explicit] The conclusion and case study explicitly list "architectural innovations (e.g., disfluency category specialized adapters)" as a future direction for diagnosis-driven refinement.
- Why unresolved: The paper's empirical analysis was restricted to prompting strategies (P0, P1, P2) using gpt-4o-mini and did not test model architectural changes.
- What evidence would resolve it: A comparative study benchmarking adapter-based models against prompted LLMs using the Z-Score framework.

### Open Question 2
- Question: Do models fail to remove parentheticals (PRN) at higher rates when they appear in proximity to specific syntactic structures or other disfluency types?
- Basis in paper: [explicit] The introduction states Z-Scores empower researchers to ask: "Do models fail to remove parentheticals when they occur near certain linguistic phenomena?"
- Why unresolved: While the metric enables this analysis, the provided case study results only report aggregate category-level scores ($Z_P$), not the contextual correlations.
- What evidence would resolve it: A fine-grained error analysis mapping Z-Score failures against the linguistic features of the surrounding context.

### Open Question 3
- Question: Is the deterministic alignment module robust enough to handle highly complex or nested disfluencies without manual intervention?
- Basis in paper: [inferred] The paper claims the alignment module is "robust" and "deterministic," but acknowledges it relies on a modified Gestalt matching approach.
- Why unresolved: The evaluation relies on Switchboard, but the alignment's specific accuracy on "complex" nested structures (versus simple linear ones) is not isolated or reported.
- What evidence would resolve it: An ablation study reporting alignment accuracy specifically on sentences containing nested or overlapping disfluency spans.

## Limitations
- Alignment module correctness is unverified: No external validation dataset exists to confirm alignment produces correct token-level removal labels in edge cases like overlapping spans or hallucinations.
- Dataset specificity: All results are based on Switchboard, limiting generalizability to other domains with different disfluency patterns.
- Intervention transferability unproven: Diagnostic-to-intervention pipeline demonstrated only within paper's controlled prompt experiments, not validated externally.

## Confidence
- **High Confidence**: Mathematical formulation of E-Scores and Z-Scores is internally consistent; category definitions align with established linguistic taxonomy; case study shows clear numerical improvements tied to metaprompting.
- **Medium Confidence**: Alignment mechanism A is well-described and deterministic, but correctness in complex or noisy real-world outputs remains unverified; assumption that category-level diagnosis generalizes across datasets is plausible but not yet validated.
- **Low Confidence**: Claims about targeted interventions directly informing model refinement are supported only by paper's own experiments; no external replication or ablation studies confirm that prompt engineering reliably improves category-specific scores in other contexts.

## Next Checks
1. **Alignment Module Validation**: Manually annotate a small test set (10-20 examples) with ground-truth disfluency spans and verify that the alignment module A produces correct token-level removal labels (1_gt, 1_pred). Include edge cases: overlapping spans, repeated tokens, and hallucinated outputs.

2. **Cross-Dataset Transferability**: Apply the Z-Score evaluation to a second corpus (e.g., a different disfluency dataset or a custom domain dataset). Compare baseline category-specific scores and check whether intervention patterns from the case study replicate.

3. **Prompt Engineering Ablation**: Test the targeted metaprompts from Section 4 on a different model (e.g., a smaller SLM) and measure whether improvements in Z_I and Z_P transfer. Include ablation conditions (e.g., without INTJ/PRN examples) to confirm causal attribution.