---
ver: rpa2
title: Emotion-aware Dual Cross-Attentive Neural Network with Label Fusion for Stance
  Detection in Misinformative Social Media Content
arxiv_id: '2505.23812'
source_url: https://arxiv.org/abs/2505.23812
tags:
- stance
- reply
- splaenet
- source
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SPLAENet, a novel emotion-aware dual cross-attentive
  neural network with label fusion for stance detection in misinformative social media
  content. The method aims to address the challenge of identifying stance in social
  media posts, where misinformation can polarize user opinions.
---

# Emotion-aware Dual Cross-Attentive Neural Network with Label Fusion for Stance Detection in Misinformative Social Media Content

## Quick Facts
- arXiv ID: 2505.23812
- Source URL: https://arxiv.org/abs/2505.23812
- Reference count: 40
- Primary result: Outperforms existing methods by 8.92-10.03% accuracy and 10.92-17.36% F1-score on stance detection benchmarks

## Executive Summary
This paper addresses the challenge of stance detection in social media content where misinformation can polarize user opinions. The proposed SPLAENet employs a dual cross-attentive neural network with emotion integration and label fusion to capture complex relationships between source posts and replies. The method aims to improve detection accuracy by leveraging both textual relationships and emotional signals that distinguish between different stance categories.

## Method Summary
SPLAENet integrates multiple advanced components to tackle stance detection: a dual cross-attention mechanism that captures inter-relationships between source and reply texts, a hierarchical attention network for modeling intra-relationships within text, emotion features to differentiate stance categories, and a label fusion approach with distance-metric learning to align extracted features with stance labels. The architecture processes source and reply texts through attention mechanisms before combining them with emotion features and applying label fusion to produce final stance predictions.

## Key Results
- Achieves 8.92% accuracy and 17.36% F1-score improvement on RumourEval dataset
- Shows 7.02% accuracy and 10.92% F1-score gains on SemEval dataset
- Demonstrates 10.03% accuracy and 11.18% F1-score improvement on P-stance dataset

## Why This Works (Mechanism)
The method works by capturing both inter-textual relationships (between source and reply) and intra-textual relationships (within individual texts) through dual cross-attention mechanisms. Emotion features help distinguish between stance categories by capturing the affective signals present in social media discourse. The label fusion component with distance-metric learning ensures that the extracted features are properly aligned with stance labels, improving classification accuracy.

## Foundational Learning

1. **Dual Cross-Attention**: Inter-textual relationship modeling between source and reply
   - Why needed: Social media stance detection requires understanding how replies relate to original posts
   - Quick check: Verify attention weights capture meaningful source-reply connections

2. **Hierarchical Attention**: Intra-textual relationship modeling within individual texts
   - Why needed: Different parts of text contribute differently to stance expression
   - Quick check: Confirm hierarchical layers preserve important local and global features

3. **Distance-Metric Learning**: Feature-label alignment optimization
   - Why needed: Ensures extracted features are geometrically close to correct stance labels
   - Quick check: Measure feature space separation between different stance classes

## Architecture Onboarding

Component Map: Input Texts -> Dual Cross-Attention -> Hierarchical Attention -> Emotion Integration -> Label Fusion -> Output

Critical Path: The core pipeline flows through dual cross-attention (source-reply interaction), hierarchical attention (internal text structure), and label fusion (classification optimization). Emotion integration occurs between attention layers and label fusion.

Design Tradeoffs: The architecture trades computational complexity for accuracy by using multiple attention mechanisms and emotion features. The dual cross-attention adds significant parameter overhead but captures richer relationships than single attention mechanisms.

Failure Signatures: Performance degradation may occur when: (1) emotion features are noisy or irrelevant to stance, (2) source-reply relationships are weak or non-existent, (3) hierarchical attention fails to capture important text structures, or (4) label fusion optimization gets stuck in poor local minima.

First Experiments:
1. Run ablation study removing dual cross-attention to measure its contribution
2. Test performance with and without emotion features to validate their impact
3. Compare against single attention mechanism baseline to demonstrate dual attention benefits

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains may depend on specific emotion feature extraction methods not fully detailed
- Complex architecture makes it difficult to isolate which components drive improvements
- Claims of substantial improvements (8-17% gains) require independent validation

## Confidence

High Confidence:
- Experimental setup with three established benchmark datasets
- Dual cross-attention mechanism is well-established NLP technique

Medium Confidence:
- Label fusion with distance-metric learning component lacks detailed implementation explanation
- Emotion feature contribution depends on feature quality and generalizability

Low Confidence:
- Absolute performance numbers cannot be independently verified without implementation details
- Results may depend on specific preprocessing and hyperparameter configurations

## Next Checks

1. Conduct systematic ablation studies removing individual components to quantify their contributions
2. Evaluate model performance across datasets with varying emotional content and languages
3. Perform extensive hyperparameter sensitivity analysis to test robustness of reported gains