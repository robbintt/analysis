---
ver: rpa2
title: 'Mol-MoE: Training Preference-Guided Routers for Molecule Generation'
arxiv_id: '2502.05633'
source_url: https://arxiv.org/abs/2502.05633
tags:
- training
- molecule
- property
- mol-moe
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address multi-objective drug design by reformulating
  molecule generation as a multi-objective RL problem. They propose Mol-MoE, a mixture-of-experts
  architecture that enables efficient test-time steering without retraining.
---

# Mol-MoE: Training Preference-Guided Routers for Molecule Generation

## Quick Facts
- **arXiv ID**: 2502.05633
- **Source URL**: https://arxiv.org/abs/2502.05633
- **Reference count**: 29
- **Primary result**: Mol-MoE achieves 0.79 average property score vs 0.62-0.76 for baselines while maintaining superior steerability and out-of-distribution performance.

## Executive Summary
Mol-MoE addresses multi-objective drug design by reformulating molecule generation as a multi-objective RL problem. The core innovation is a preference-guided router training objective that dynamically combines expert models based on user-specified trade-offs. Mol-MoE outperforms existing approaches (MORLHF, RiC, RS) in both sample quality and steerability, particularly in out-of-distribution scenarios. It achieves an average property score of 0.79 compared to 0.62-0.76 for baselines, and maintains superior performance when high-quality training samples are removed.

## Method Summary
Mol-MoE employs a three-stage pipeline: (1) Fine-tune Llama 3.2 1B on 3.65M SMILES molecules for causal language modeling, (2) Train 5 property-specific expert models via RLOO on individual property rewards (JNK3, DRD2, GSK3β, CYP2D6, CYP2C19), and (3) Build a mixture-of-experts architecture where a learned router dynamically combines expert contributions based on preference vectors encoded in prompts. The router is trained via RLOO across distributions of preference weights while experts remain frozen, enabling test-time steering without retraining.

## Key Results
- Mol-MoE achieves 0.79 average property score versus 0.62-0.76 for baselines
- Maintains superior steerability with lower MAE between preference weights and property scores
- Demonstrates robust out-of-distribution performance when high-quality training samples are removed (0.78→0.79 vs RiC's 0.74→0.62)

## Why This Works (Mechanism)

### Mechanism 1
Dynamic, input-dependent routing of expert MLP layers improves multi-objective trade-off calibration compared to static weight interpolation. A learned router network produces routing weights over pre-trained property experts per token, trained via RLOO across distributions of preference vectors. This teaches the router to interpret preference encodings in prompts and adjust expert contributions accordingly.

### Mechanism 2
Separating expert optimization from preference interpolation avoids objective interference that degrades scalarized multi-objective RL. Each expert is independently fine-tuned via single-objective RLHF on property rewards. The router operates on frozen expert activations, preventing gradient interference between property objectives during multi-task combination.

### Mechanism 3
Including the pre-trained base model as a regularization expert prevents distribution collapse while allowing exploration beyond training data. The base model is added as expert alongside property specialists, enabling the router to weight this expert to maintain syntactic validity and chemical plausibility when exploring novel regions.

## Foundational Learning

- **Concept: Mixture of Experts (MoE) routing**
  - **Why needed here:** Understanding how soft routing differs from hard expert selection; Mol-MoE uses K=m (all experts contribute), unlike traditional sparse MoE.
  - **Quick check question:** Can you explain why the paper sets K=m instead of K=1 or K=2?

- **Concept: RLHF with RLOO (REINFORCE Leave-One-Out)**
  - **Why needed here:** Both expert fine-tuning and router training use RLOO; understanding the baseline subtraction mechanism is critical for implementation.
  - **Quick check question:** How does RLOO's leave-one-out baseline differ from PPO's value function baseline?

- **Concept: Linear Mode Connectivity (LMC)**
  - **Why needed here:** The entire approach assumes expert weight updates can be linearly interpolated in weight space—this is the theoretical foundation of model merging.
  - **Quick check question:** What happens to merged policy performance if two experts' fine-tuning trajectories diverge into disconnected loss basins?

## Architecture Onboarding

- **Component map:** Input prompt with preference weights → Llama 3.2 1B backbone → MoE block (Router MLP → softmax → weighted sum of expert MLPs) → Expert MLPs (Base, JNK3, DRD2, GSK3β, CYP2D6, CYP2C19) → SMILES output

- **Critical path:**
  1. Pre-train base Mol-Llama on 3.65M SMILES (151M tokens, 1 epoch)
  2. Fine-tune 5 expert copies via RLOO (100K generations each, single-property reward)
  3. Replace FFN layers with MoE blocks; initialize router with property-name activations
  4. Train router via RLOO (100K generations, preference-conditioned prompts, frozen experts)

- **Design tradeoffs:**
  - K=m (all experts) vs sparse routing: Higher compute but ensures all properties contribute
  - Router-only training vs full model: Faster but assumes experts are already well-calibrated
  - RLOO vs PPO: Simpler (no critic) but higher variance (mitigated by leave-one-out baseline)

- **Failure signatures:**
  - Invalid SMILES output: Router collapsed to single expert; check routing entropy
  - Poor steerability (high MAE): Router initialization failed; re-initialize with property-name activations
  - MORLHF-style degradation beyond 3 tasks: Experts interfering; verify they're frozen during router training

- **First 3 experiments:**
  1. **Single-property routing test:** Set preference vector to [1, 0, 0, 0, 0]; verify router weights concentrate on corresponding expert; check that property score matches single-expert baseline.
  2. **Router ablation:** Compare router-initialized-from-property-names vs random initialization; measure steerability MAE difference (expected: random init has higher error per Figure 4).
  3. **Out-of-distribution probe:** Train on dataset with held-out high-quality samples (r_i(x) ≥ 0.6 removed); verify Mol-MoE generates molecules above 0.6 threshold (RiC should fail here).

## Open Questions the Paper Calls Out

- **Can graph-based routing architectures or geometric deep learning approaches better capture non-linear molecular property interactions than the current linear preference weighting?** The paper concludes that future work should explore these approaches to explicitly model interactions.

- **Does the Mol-MoE architecture maintain its efficiency and steerability advantages when scaling the pre-training data from millions to billions of molecules?** The paper notes future work should consider scaling to billion-scale molecular datasets.

- **How does the router's performance and expert interference change as the number of concurrent optimization objectives scales beyond the five tasks tested?** While Mol-MoE outperforms baselines with 5 tasks, performance with 10+ objectives remains untested.

## Limitations

- Router initialization from property-name activations is underspecified, potentially impacting steerability performance and replication success.
- Out-of-distribution testing assumes clean identification of high-quality samples, but property classifier thresholds may not perfectly correlate with true chemical quality.
- Performance comparisons rely on published baselines that may use different evaluation protocols, introducing potential methodological inconsistencies.

## Confidence

- **High confidence** in the core MoE architecture and preference-guided routing mechanism
- **Medium confidence** in claimed performance improvements due to hyperparameter sensitivity
- **Low confidence** in generalizability of out-of-distribution results due to uncertain quality correlations

## Next Checks

1. **Router initialization ablation:** Systematically compare performance when initializing router weights from property-name activations versus random initialization, measuring steerability MAE differences.

2. **Preference encoding sensitivity:** Test multiple preference prompt formats (different normalization schemes, precision levels) to determine robustness to encoding variations.

3. **Expert interference analysis:** For cases where MORLHF degrades beyond 3 objectives, verify experts remain frozen during router training and measure property-specific reward trends.