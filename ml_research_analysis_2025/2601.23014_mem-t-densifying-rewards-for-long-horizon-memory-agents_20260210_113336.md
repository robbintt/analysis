---
ver: rpa2
title: 'Mem-T: Densifying Rewards for Long-Horizon Memory Agents'
arxiv_id: '2601.23014'
source_url: https://arxiv.org/abs/2601.23014
tags:
- memory
- agents
- retrieval
- mem-t
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of sparse and delayed rewards
  in training long-horizon memory agents, which hinders end-to-end optimization of
  memory management policies. To tackle this, it introduces Mem-T, a streamlined hierarchical
  memory agent that integrates formation, evolution, and retrieval operations.
---

# Mem-T: Densifying Rewards for Long-Horizon Memory Agents

## Quick Facts
- arXiv ID: 2601.23014
- Source URL: https://arxiv.org/abs/2601.23014
- Reference count: 28
- Key outcome: Mem-T achieves state-of-the-art performance, surpassing frameworks like A-Mem and Mem0 by up to 14.92% F1 score while reducing inference tokens per query by approximately 24.45% compared to GAM without sacrificing accuracy.

## Executive Summary
Mem-T addresses the challenge of sparse and delayed rewards in training long-horizon memory agents, which hinders end-to-end optimization of memory management policies. It introduces a streamlined hierarchical memory agent with formation, evolution, and retrieval operations, and MoT-GRPO, a tree-guided reinforcement learning framework that transforms sparse terminal feedback into dense, step-wise supervision via memory operation tree backpropagation and hindsight credit assignment. This enables joint optimization of memory construction and retrieval policies.

## Method Summary
Mem-T implements a hierarchical memory system (M_work, M_fact, M_exp, M_raw) and uses MoT-GRPO for retrieval optimization. For each query, G=3 trees are constructed with iterative branching from N_v=3 pivot nodes over M=4 rounds. Node rewards combine evidence density and backpropagated performance, normalized via dual-scale advantages (intra-tree and inter-tree). Retrieval policy uses PPO with KL constraint (β=0.001), while construction policy uses hindsight credit assignment with top-50% retention and supervised training. Base model uses Qwen3-4B/8B with BGE-M3 embeddings and top-5 retrieval.

## Key Results
- Achieves 14.92% F1 improvement over A-Mem and Mem0 frameworks
- Reduces inference tokens per query by 24.45% compared to GAM
- Ablation studies show dual-scale advantage estimation critical (A_inter removal causes 4.56 F1 drop)

## Why This Works (Mechanism)

### Mechanism 1
Tree-based reward backpropagation converts sparse terminal feedback into dense step-wise supervision for retrieval policy learning. MoT-GRPO constructs Memory Operation Trees via iterative branching rollouts, with each node receiving dense reward R(v) = I_fmt(v) · (α·Evid(v) + Perform(v)). Terminal performance recursively propagates to internal nodes, enabling gradient signals at each retrieval step.

### Mechanism 2
Hindsight credit assignment enables supervision of memory construction policies by propagating retrieval success signals backward to formation/evolution actions. For each memory operation a_mem, hindsight score S(a_mem) aggregates terminal advantages weighted by evidence alignment and retrieval trace gates. Top-ranked operations are distilled into construction policy via offline optimization.

### Mechanism 3
Dual-scale advantage estimation stabilizes RL by balancing local context-sensitive credit assignment with global cross-tree competition. A_total(v) = A_intra(v) + A_inter(v), where intra-tree advantage normalizes within each tree and inter-tree advantage normalizes across all trees. This prevents any single tree's local baseline from dominating while maintaining relative comparisons.

## Foundational Learning

- **GRPO (Group Relative Policy Optimization)**: Why needed: MoT-GRPO extends GRPO by replacing single-trajectory groups with tree-structured rollouts. Understanding baseline GRPO's group-based advantage estimation is prerequisite. Quick check: Can you explain how GRPO differs from PPO in its advantage normalization?

- **Temporal Credit Assignment**: Why needed: The core problem this paper addresses—attributing delayed rewards to earlier actions in long-horizon sequences (~500 turns). Quick check: Why does sparse reward propagation fail in long-horizon settings?

- **Hierarchical Memory Architectures**: Why needed: Mem-T's M_t = ⟨M_work, M_fact, M_exp, M_raw⟩ requires understanding why different memory types serve different temporal scales and functional roles. Quick check: What is the functional difference between factual memory and experiential memory in this framework?

## Architecture Onboarding

- **Component map**: Input Stream (x_t) → [Phase I: Construction] π_form → {CrtFact, CrtExp, CrtRaw, UpdWork} → M_cand → π_evol → {ADD, UPDATE, DELETE, IGNORE} → M_{t+1} → [Phase II: Retrieval] Query q → π_retr → {Search(r,key), Finish} → M_rel → Answer y → [Training: MoT-GRPO] G trees → Node rewards → Dual-scale advantages → Policy updates

- **Critical path**: Retrieval optimization (w/o Retr. Opt. causes -5.28 F1 drop) > Construction optimization (-3.29 F1) > Factual memory module (-3.40 F1 when removed)

- **Design tradeoffs**: Trees per query (G): 1→3 yields +4.45 F1; 3→5 yields only +0.35 with 67% more compute. Max retrieval steps: 2→6 yields +5.2 F1; 6→10 yields <0.5% gain but 2.3× tokens. Branch nodes (N_v): 1→3 improves; >3 plateaus.

- **Failure signatures**: Low evidence hit rate suggests retrieval not grounding in annotated evidence. High raw reward variance with flat smoothed curve indicates unstable training. Disproportionate UPDATE vs ADD operations may indicate evolution policy confusion.

- **First 3 experiments**: 1) Replicate training-free baseline on LoCoMo to verify hierarchical memory implementation; expect ~49-54 F1 without MoT-GRPO training. 2) Ablate A_inter vs A_intra to confirm dual-scale advantage contribution matches paper (expect ~4.5 vs ~1.7 F1 impact). 3) Test OOD generalization on HotpotQA with 400-distractor setting; expect ~66 F1 if implementation matches paper.

## Open Questions the Paper Calls Out

### Open Question 1
Does the computational overhead of constructing and expanding Memory Operation Trees scale effectively to million-token contexts? The introduction identifies "million-token contexts" as a challenge, but evaluation is limited to LoCoMo (~600 turns) and 56k-token HotpotQA. Tree branching adds computational cost that may grow non-linearly with operation horizon.

### Open Question 2
How does reliance on ground-truth evidence in the "Evidence Alignment Gate" limit applicability to scenarios without explicit evidence labels? While a fallback to Retrieval Trace Gate is proposed, the paper doesn't quantify performance drop when Evidence Alignment Gate is disabled during training or inference on unlabeled data.

### Open Question 3
Can the hierarchical memory architecture and trained policies transfer effectively to non-QA domains like procedural task execution or coding? All evaluation benchmarks are QA tasks, but the conclusion frames Mem-T as a step toward "self-evolving agents capable of lifelong learning" that would need to handle procedural validity and state consistency.

## Limitations

- The weighting coefficient α in node reward function is unspecified, making it unclear how evidence density and performance are balanced
- The stochastic sampling strategy for pivot node selection during MoT expansion lacks detailed specification
- Reliance on ground-truth evidence annotations X_q_evi may limit applicability to datasets without explicit evidence labels

## Confidence

- **High Confidence**: Overall F1 improvements (14.92% over A-Mem/Mem0) and inference efficiency gains (24.45% token reduction vs GAM) are well-supported by ablation studies and cross-dataset validation
- **Medium Confidence**: Effectiveness of dual-scale advantage estimation is demonstrated through ablation but lacks theoretical justification; hindsight credit assignment contribution is supported but not compared to alternatives
- **Low Confidence**: Specific hyperparameters (G=3, N_v=3, max depth=4) are presented as optimal without extensive sensitivity analysis; stochastic sampling strategy could significantly impact performance but is underspecified

## Next Checks

1. **Ablation of Advantage Estimation Components**: Systematically test contribution of A_inter versus A_intra by training models with only A_intra (expect ~1.7 F1 drop), only A_inter (expect ~4.5 F1 drop), and their combination to confirm dual-scale estimation provides additive benefits.

2. **Pivot Selection Strategy Analysis**: Compare stochastic sampling against deterministic selection and random sampling for pivot node selection during MoT expansion. Measure impact on F1 and training stability across 5 random seeds to determine if current strategy is robust.

3. **Cross-Dataset Generalization with Varying Evidence Density**: Evaluate Mem-T on datasets with different evidence annotation densities (e.g., HotpotQA with 400 distractors vs LoCoMo). Track performance correlation with Evid(v) reliability and test whether model maintains advantages when ground-truth evidence is sparse or noisy.