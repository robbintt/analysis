---
ver: rpa2
title: 'DeepJSONEval: Benchmarking Complex Nested JSON Data Mining for Large Language
  Models'
arxiv_id: '2509.25922'
source_url: https://arxiv.org/abs/2509.25922
tags:
- json
- data
- evaluation
- llms
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeepJSONEval introduces a benchmark with 2100 multi-domain instances
  featuring 3-7 level nested JSON structures to evaluate LLMs' complex information
  extraction and structured generation capabilities. The evaluation framework employs
  multi-dimensional metrics including syntax validation, hierarchical key matching,
  and strict exact-match scoring.
---

# DeepJSONEval: Benchmarking Complex Nested JSON Data Mining for Large Language Models

## Quick Facts
- arXiv ID: 2509.25922
- Source URL: https://arxiv.org/abs/2509.25922
- Reference count: 5
- Primary result: 2100 multi-domain nested JSON instances with 3-7 levels evaluate LLMs' structured extraction capabilities

## Executive Summary
DeepJSONEval introduces a comprehensive benchmark for evaluating large language models' ability to extract and generate structured information from complex nested JSON data. The benchmark features 2100 instances across multiple domains with JSON structures ranging from 3 to 7 levels of nesting. The evaluation framework employs multi-dimensional metrics including syntax validation, hierarchical key matching, and exact-match scoring to assess model performance comprehensively.

The benchmark reveals significant performance degradation as nesting depth increases, with strict exact-match scores falling below 60% for the most complex 5-7 level structures across all tested models. Type-specific analysis shows substantial variance in extraction accuracy, with numeric data achieving 0.90-1.00 accuracy while string lists perform notably worse at 0.576-0.722. External validation through an end-to-end web pipeline demonstrates strong predictive validity with correlation of 0.987 between benchmark scores and real-world extraction performance.

## Method Summary
The DeepJSONEval benchmark is constructed using 2100 multi-domain instances featuring nested JSON structures with 3-7 levels of depth. The evaluation employs a multi-dimensional scoring system that includes syntax validation to ensure proper JSON structure, hierarchical key matching to verify correct data organization, and strict exact-match scoring for comprehensive accuracy assessment. The benchmark covers diverse data types including numeric values, strings, and lists, with performance measured across different nesting depths and domain categories. Cross-domain analysis shows minimal performance variance, while type-specific evaluation reveals significant differences in extraction accuracy between data types.

## Key Results
- Performance degradation with nesting depth: strict scores below 60% for 5-7 level structures
- Type-specific accuracy variance: numeric data (0.90-1.00) vs string lists (0.576-0.722)
- Cross-domain analysis shows minimal performance variance (standard deviation 0.02-0.04)
- Strong predictive validity: correlation of 0.987 between benchmark and real-world extraction tasks

## Why This Works (Mechanism)
None

## Foundational Learning
- Nested JSON structure parsing: Understanding hierarchical data organization and traversal patterns
- Multi-dimensional evaluation metrics: Combining syntax validation, key matching, and exact-match scoring for comprehensive assessment
- Cross-domain performance analysis: Evaluating model robustness across different application areas
- Type-specific accuracy measurement: Isolating performance differences between numeric, string, and list data types
- Predictive validity correlation: Establishing benchmark reliability through real-world task performance comparison

## Architecture Onboarding
- Component map: JSON instances -> Multi-dimensional metrics -> Type-specific evaluation -> Cross-domain analysis -> Predictive validation
- Critical path: Instance generation → Structured extraction → Metric computation → Performance analysis → Real-world validation
- Design tradeoffs: Comprehensive metric coverage vs evaluation complexity; broad domain coverage vs depth of domain-specific challenges
- Failure signatures: Performance degradation at deeper nesting levels; accuracy variance across data types; domain-agnostic performance patterns
- First experiments: 1) Test single-level extraction accuracy baseline; 2) Evaluate performance across different data type combinations; 3) Measure impact of increasing nesting depth incrementally

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset representativeness concerns: 2100 instances may not capture full complexity of real-world nested JSON scenarios
- Performance degradation patterns may reflect benchmark-specific artifacts rather than fundamental LLM limitations
- Type-specific evaluation may not adequately represent mixed-type structures common in real-world applications

## Confidence
- High confidence: Benchmark architecture and multi-dimensional metrics are methodologically sound and clearly defined
- Medium confidence: Performance degradation with nesting depth is likely real but may be benchmark-specific; predictive validity correlation appears strong but needs independent verification
- Medium confidence: Type-specific accuracy differences are observable but may not generalize beyond benchmark's specific data distributions

## Next Checks
1. Conduct independent replication using alternative nested JSON datasets to verify performance degradation patterns hold across different data sources
2. Perform ablation studies removing specific nesting levels to isolate whether performance drops stem from depth itself or specific structural complexities
3. Test benchmark predictions against diverse real-world JSON extraction tasks from production systems with varied domains and mixed data types