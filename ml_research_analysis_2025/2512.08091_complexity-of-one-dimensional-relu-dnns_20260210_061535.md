---
ver: rpa2
title: Complexity of One-Dimensional ReLU DNNs
arxiv_id: '2512.08091'
source_url: https://arxiv.org/abs/2512.08091
tags:
- number
- linear
- regions
- relu
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the expressivity of one-dimensional ReLU deep
  neural networks by analyzing their linear regions. The key result proves that for
  randomly initialized, fully connected 1D ReLU networks in the infinite-width limit,
  the expected number of linear regions grows as the sum of neurons across all layers
  plus a vanishing term.
---

# Complexity of One-Dimensional ReLU DNNs

## Quick Facts
- arXiv ID: 2512.08091
- Source URL: https://arxiv.org/abs/2512.08091
- Reference count: 29
- Primary result: Expected number of linear regions in 1D ReLU networks grows as Σn_ℓ + o(Σn_ℓ) + 1 in infinite-width limit

## Executive Summary
This paper provides a theoretical analysis of the expressivity of one-dimensional ReLU deep neural networks by studying their linear regions. The authors prove that for randomly initialized, fully connected 1D ReLU networks in the infinite-width limit, the expected number of linear regions scales linearly with the total number of neurons across all layers. The work introduces a novel function-adaptive notion of sparsity based on linear regions, which measures network effectiveness by comparing the expected regions used to the minimal number needed for a target approximation. The proof technique leverages Gaussian process theory and covariance recursion relations to analyze how breakpoints propagate through network layers.

## Method Summary
The paper analyzes randomly initialized 1D ReLU DNNs with topology (1, n₁, ..., n_L, 1) using He initialization (weights variance 2/n_{ℓ-1}, nonzero bias variance σ_b²). The theoretical approach involves: (1) computing pre-activation covariances layer-by-layer using a covariance recursion relation, (2) deriving correlation decay rates between inputs, (3) integrating zero-crossing densities to calculate expected breakpoints per neuron, and (4) summing across neurons while accounting for propagation probabilities. The analysis focuses on the infinite-width limit where pre-activations converge to Gaussian processes, enabling closed-form calculations of expected linear regions.

## Key Results
- Expected number of linear regions in 1D ReLU networks scales as Σn_ℓ + o(Σn_ℓ) + 1 in infinite-width limit
- Breakpoint creation rate is exactly 1 per neuron in first layer and approximately 1 per neuron in deeper layers
- Pre-activation correlations decay quadratically with input distance: ρ_ℓ(x, x+ε) ≈ 1 - A_ℓ ε²
- Region-adaptive sparsity provides a function-dependent efficiency measure comparing expected regions to minimal needed for approximation

## Why This Works (Mechanism)

### Mechanism 1
The expected number of linear regions scales linearly with total neuron count because breakpoints (slope-change points) are created when ReLU activations cross zero. Each neuron in layer 1 contributes exactly one breakpoint at location -b_i^(1)/W_1i^(1). For deeper layers, the expected number of zero crossings per pre-activation equals 1 in the infinite-width limit. Breakpoints propagate to output with probability approaching 1 as width → ∞. Core assumption: He initialization with nonzero bias (σ_b ≠ 0); infinite-width limit; independent Gaussian weights/biases. Break condition: Finite-width networks; zero bias (σ_b = 0) eliminates zero-crossings beyond layer 1.

### Mechanism 2
Pre-activation correlations decay predictably with input distance, enabling closed-form zero-crossing rates. In infinite-width limit, pre-activations converge to a Gaussian process. The covariance C^(ℓ)(u,v) between pre-activations at inputs u and v satisfies a recursion involving the angle θ_{ℓ-1} = arccos(ρ_{ℓ-1}(u,v)). Taylor expansion shows ρ_ℓ(x, x+ε) ≈ 1 - A_ℓ ε², so correlation degrades quadratically with separation. Core assumption: Gaussian process convergence holds in infinite-width limit; compact domain analysis. Break condition: Non-Gaussian initializations; correlated weight structures; finite-width corrections.

### Mechanism 3
Region-adaptive sparsity provides a function-dependent, architecture-invariant efficiency measure. Define η_{region}(Φ; f, ε_0) = E[L(Φ)] / L_{min}(f, ε_0) where L_{min} is the minimal regions needed for ε-approximation. Networks satisfying η_{region} ≤ c while approximating within tolerance are "region-efficient"—they use near-minimal breakpoints for the target function. Core assumption: L_{min}(f, ε_0) can be computed or estimated; target function f is known. Break condition: Unknown target functions; multi-dimensional outputs where L_{min} is intractable; trained networks (analysis applies at initialization).

## Foundational Learning

- **Gaussian Process Limit of Neural Networks**
  - Why needed here: The entire proof hinges on pre-activations becoming Gaussian processes at infinite width, enabling covariance analysis.
  - Quick check question: Why does the central limit theorem apply to pre-activations but not post-activations in this analysis?

- **Zero-Crossing Rate of Correlated Gaussians**
  - Why needed here: Lemma V.5 uses the fact that Pr(s(x)s(x+ε) < 0) = θ/π for correlated mean-zero Gaussians with correlation cos θ.
  - Quick check question: If two jointly Gaussian variables have correlation ρ → 1, what happens to the probability they have opposite signs?

- **Breakpoints in Piecewise Linear Functions**
  - Why needed here: In 1D, linear regions = breakpoints + 1; understanding this bijection is essential for the counting argument.
  - Quick check question: A 1D ReLU network with 3 hidden neurons in one layer can create at most how many linear regions?

## Architecture Onboarding

- **Component map:**
  - Input layer: 1 neuron (scalar input)
  - Hidden layers: L layers with widths (n₁, n₂, ..., n_L)
  - Output layer: 1 neuron (scalar output)
  - Activation: ReLU applied after each hidden layer; linear output
  - Initialization: W_{ij}^{(ℓ)} ~ N(0, 2/n_{ℓ-1}), b_j^{(ℓ)} ~ N(0, σ_b²) with σ_b ≠ 0 fixed

- **Critical path:**
  1. Compute pre-activation covariances layer-by-layer using Proposition V.1 recursion
  2. Derive correlation decay rate ρ_ℓ(x, x+ε) via Taylor expansion
  3. Integrate zero-crossing density λ_ℓ(x) over domain to get expected breakpoints per neuron
  4. Sum across neurons and apply propagation probability (1 - 2^{-n_{ℓ+1}} → 1)

- **Design tradeoffs:**
  - Deeper networks: Same expected regions as wider shallow networks (linear scaling in both cases), but different functional behaviors
  - Bias variance σ_b²: Controls zero-crossing density; larger σ_b increases density away from origin
  - He vs. other initializations: Proof assumes He scaling; other schemes may alter covariance structure

- **Failure signatures:**
  - Zero bias (σ_b = 0): Only layer 1 contributes breakpoints; deeper layers create vanishingly few
  - Finite width: Propagation probability 1 - 2^{-n_{ℓ+1}} meaningfully less than 1; expected regions reduced
  - Non-compact domains: Integral in Theorem V.6 over [-∞, ∞] converges due to arctan; unbounded analysis may differ

- **First 3 experiments:**
  1. **Validation sweep**: Sample random 1D ReLU networks at increasing widths, count linear regions numerically, verify convergence to Σn_i
  2. **Bias ablation**: Vary σ_b ∈ {0.1, 0.5, 1.0, 2.0} and measure effect on breakpoint distribution across layers
  3. **Finite-width correction**: Empirically estimate propagation probability deficit (1 - 2^{-n}) and compare to theoretical o(Σn_i) term

## Open Questions the Paper Calls Out

### Open Question 1
How does the expected number of linear regions scale for deep ReLU networks with input dimension d > 1 in the infinite-width limit? The proof relies on 1D properties, specifically counting sign changes of pre-activations and integrating over a single variable. Higher dimensions involve complex hyperplane arrangements rather than simple breakpoints, making the covariance recursion and zero-crossing logic insufficient. Evidence would require a derivation showing the scaling law for d-dimensional inputs or a proof that linear scaling with neuron count persists with geometric corrections for volume-based region counts.

### Open Question 2
What is the full asymptotic distribution of the number of linear regions R(T), beyond just the expected value? The current analysis utilizes Gaussian Process theory to find the mean number of breakpoints via expectations of indicator functions. Determining the distribution requires analyzing higher-order correlations and joint distributions of neuron activations across layers, which the first-moment analysis does not address. Evidence would include a formal characterization of the variance and limiting distribution of R(T) as width approaches infinity, or empirical histograms showing convergence behavior for finite widths.

### Open Question 3
Can the proposed region-adaptive sparsity metric be effectively estimated or applied to approximate complex target functions in practice? The definition depends on L_{min}(f, ε_0), the minimal number of linear regions required to approximate a function f. This quantity is theoretically intractable for most complex functions, making it difficult to compute the "region inefficiency" η_{region} for real-world tasks. Evidence would include algorithms providing tight bounds for L_{min} on specific function classes, or empirical studies demonstrating that minimizing η_{region} correlates with improved generalization or pruning efficiency on standard benchmarks.

## Limitations

- The analysis is limited to 1D inputs and may not generalize to higher dimensions where region counting involves complex geometric arrangements
- Results depend on the infinite-width Gaussian process limit, which may not accurately reflect practical finite-width networks
- The region-adaptive sparsity measure requires knowledge of L_{min}(f, ε_0), which is intractable for most complex functions

## Confidence

- Expected Linear Regions Growth (Σn_ℓ + o(Σn_ℓ) + 1): High confidence in the mathematical derivation for infinite-width networks with the specified assumptions
- Region-Adaptive Sparsity Measure: Medium confidence—the theoretical framework is sound, but practical estimation of L_min(f, ε_0) remains challenging
- Covariance Recursion and Zero-Crossing Analysis: High confidence in the mathematical proofs, though numerical validation is needed for finite-width cases

## Next Checks

1. **Finite-Width Convergence Study**: Empirically measure the ratio E[R(T)]/Σn_ℓ for networks with widths ranging from 10 to 1000 neurons per layer, plotting convergence to verify the o(Σn_ℓ) term's decay rate

2. **Bias Variance Sensitivity**: Systematically vary σ_b across multiple orders of magnitude (e.g., 0.01, 0.1, 1.0, 10.0) and measure the impact on breakpoint distribution and linear region count to validate the theoretical dependence on bias variance

3. **Alternative Initialization Schemes**: Test networks initialized with different schemes (Xavier, orthogonal, uniform) to determine the robustness of the Σn_ℓ scaling law and identify conditions where it breaks down