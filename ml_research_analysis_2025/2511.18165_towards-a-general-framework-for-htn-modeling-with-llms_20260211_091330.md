---
ver: rpa2
title: Towards a General Framework for HTN Modeling with LLMs
arxiv_id: '2511.18165'
source_url: https://arxiv.org/abs/2511.18165
tags:
- planning
- llms
- task
- l2hp
- hierarchical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of integrating Large Language
  Models (LLMs) into Hierarchical Planning (HP) model generation, an area lagging
  behind non-hierarchical Automated Planning (AP). It proposes L2HP, an extension
  of the L2P library, designed for HP model generation with a focus on generality
  and extensibility.
---

# Towards a General Framework for HTN Modeling with LLMs

## Quick Facts
- arXiv ID: 2511.18165
- Source URL: https://arxiv.org/abs/2511.18165
- Reference count: 2
- Primary result: HTN model generation via LLMs shows far lower syntactic validity than AP (1% vs 20%)

## Executive Summary
This paper tackles the challenge of using Large Language Models (LLMs) to generate Hierarchical Task Network (HTN) planning models, an area that has lagged behind non-hierarchical Automated Planning (AP) in LLM integration. The authors introduce L2HP, an extension of the L2P library, aimed at providing a general and extensible framework for HTN model generation. Through experiments using the PlanBench dataset, they compare LLM-based AP and HTN modeling, revealing that while parsing success is similar between the two (around 36%), HTN models suffer from a dramatic drop in syntactic validity (1% vs 20% for AP). These results highlight the unique challenges HTN presents for LLMs and underscore the need for further research to improve model generation quality.

## Method Summary
The authors propose L2HP, a framework built as an extension of the L2P library, designed to support the generation of HTN planning models using LLMs. L2HP emphasizes generality and extensibility, allowing for experimentation across different HTN domains. The framework is tested by comparing LLM-based generation of both AP and HTN models using a fixed set of prompts and the PlanBench dataset. Parsing success and syntactic validity are the primary metrics, with the latter showing a substantial drop for HTN models compared to AP models. The study does not explore finetuning or alternative plan representations, focusing instead on prompt-based generation.

## Key Results
- Parsing success for both AP and HTN models is limited and comparable (~36%).
- Syntactic validity for HTN models is dramatically lower than for AP models (1% vs 20%).
- The substantial drop in validity when moving from AP to HTN underscores the increased difficulty of HTN model generation with LLMs.

## Why This Works (Mechanism)
Not applicable.

## Foundational Learning
- **Hierarchical Task Networks (HTNs):** A planning formalism that uses tasks and methods to decompose goals into primitive actions. *Why needed:* Understanding HTNs is crucial because the paper focuses on generating HTN models, which are structurally more complex than AP models. *Quick check:* Can you explain the difference between primitive tasks and compound tasks in HTN?
- **Automated Planning (AP):** The process of automatically generating sequences of actions to achieve a goal. *Why needed:* AP is used as a baseline to highlight the additional challenges posed by HTN. *Quick check:* What are the key differences between AP and HTN in terms of problem representation?
- **LLM-based model generation:** Using LLMs to parse natural language and generate formal planning models. *Why needed:* The paper's core contribution is leveraging LLMs for HTN modeling. *Quick check:* How does prompt engineering influence the quality of generated models?

## Architecture Onboarding

**Component Map**
L2HP -> LLM (prompted) -> HTN model generation -> PlanBench evaluation

**Critical Path**
Input natural language -> Prompt LLM via L2HP -> Parse response -> Check syntactic validity -> Output model

**Design Tradeoffs**
- **Generality vs. performance:** L2HP is designed to be extensible across domains, but this may come at the cost of specialized optimizations for HTN.
- **Prompt-based vs. finetuning:** The paper opts for prompt-based generation, which is easier to implement but may be less effective than finetuned models for complex HTN structures.
- **Syntactic vs. semantic validity:** The focus on syntactic validity may overlook deeper semantic correctness issues in generated models.

**Failure Signatures**
- Low parsing success (~36%) indicates that even basic extraction from natural language is challenging.
- Sharp drop in syntactic validity for HTN (1% vs 20% for AP) suggests structural or token length issues.
- Absence of downstream utility metrics due to low validity limits practical assessment.

**First 3 Experiments**
1. Evaluate L2HP on additional HTN domains (e.g., IPC or custom game AI benchmarks) to test generality claims.
2. Perform ablation studies varying prompt structure, model size, and representation format to isolate sources of the validity drop.
3. Benchmark against a non-LLM HTN modeling baseline (e.g., template-based or rule-based generators) to contextualize LLM performance.

## Open Questions the Paper Calls Out
None.

## Limitations
- The evaluation is based on a single dataset (PlanBench) and a fixed set of prompts, which may not capture the full spectrum of HTN complexities.
- The 20Ã— drop in syntactic validity when moving from AP to HTN suggests that the underlying problem structure or token length may be pushing models beyond their effective context limits.
- Since syntactic validity is so low, downstream utility metrics are not reported, leaving the practical impact of the approach uncertain.

## Confidence

- Claim that HP is "substantially more challenging" for LLMs: **Medium** - supported by the validity drop, but limited by single-dataset scope.
- Claim that L2HP is "extensible" and "general": **Low** - framework design is asserted but not empirically validated across diverse domains.
- Claim that results "highlight the need for further research": **High** - directly follows from observed performance limits.

## Next Checks
1. Evaluate L2HP on additional HTN domains (e.g., from IPC or custom game AI benchmarks) to test generality claims.
2. Perform ablation studies varying prompt structure, model size, and representation format to isolate sources of the validity drop.
3. Benchmark against a non-LLM HTN modeling baseline (e.g., template-based or rule-based generators) to contextualize LLM performance.