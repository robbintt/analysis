---
ver: rpa2
title: Adam or Gauss-Newton? A Comparative Study In Terms of Basis Alignment and SGD
  Noise
arxiv_id: '2510.13680'
source_url: https://arxiv.org/abs/2510.13680
tags:
- adam
- basis
- gradient
- learning
- eigenbasis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares Adam and Gauss-Newton-based optimizers along
  two key factors: the choice of preconditioning basis and the impact of gradient
  noise from mini-batching. Through theoretical analysis on linear regression and
  logistic regression, the authors show that Adam can outperform Gauss-Newton under
  the incorrect identity basis even in full-batch settings, while both methods behave
  similarly in the stochastic regime.'
---

# Adam or Gauss-Newton? A Comparative Study In Terms of Basis Alignment and SGD Noise

## Quick Facts
- arXiv ID: 2510.13680
- Source URL: https://arxiv.org/abs/2510.13680
- Reference count: 40
- Adam can outperform Gauss-Newton under incorrect identity basis even in full-batch settings, while both behave similarly in stochastic regime

## Executive Summary
This paper provides a theoretical and empirical comparison between Adam and Gauss-Newton-based optimizers, focusing on how they interact with basis alignment and gradient noise. The authors show that Adam's adaptive preconditioning can outperform GN's curvature-based approach when using the wrong basis, and that both methods converge similarly in stochastic settings. Through analysis on linear/logistic regression and experiments on MLPs and Transformers, the work reveals critical insights about when and why different preconditioners excel.

## Method Summary
The paper compares diagonal preconditioners (Adam vs GN^{-1} vs GN^{-1/2}) across two axes: basis choice (Identity vs GN Eigenbasis) and batch size (Full vs Stochastic). The framework rotates gradients into a chosen basis U, applies diagonal preconditioner D, then rotates back. Adam uses gradient statistics while GN uses the Gauss-Newton matrix. Eigenbasis is approximated via Kronecker factorization for efficiency. Experiments cover linear/logistic regression with specific covariance structures, MLPs on synthetic and CIFAR-10 tasks, and Transformers on selection tasks.

## Key Results
- Adam outperforms both GN^{-1} and GN^{-1/2} in full-batch settings under the incorrect identity basis due to auto-tuning behavior
- In stochastic regime (batch size 1), Adam behaves similarly to GN^{-1/2} regardless of basis choice under Gaussian data assumptions
- The choice of preconditioner power (p=-1 vs p=-1/2) critically affects GN performance, with GN^{-1/2} sometimes outperforming GN^{-1} even in eigenbasis
- Experimental results confirm theoretical predictions across convex and non-convex objectives including MLPs and Transformers

## Why This Works (Mechanism)

### Mechanism 1: Basis Alignment Sensitivity
When using identity basis, GN diagonal entries all equal 1 (uniform), behaving like vanilla gradient descent and failing to adapt to heterogeneous curvature. Adam's gradient-statistics-based preconditioner adapts to coordinate-specific curvature regardless of basis alignment. Core assumption: problem has heterogeneous curvature with significantly different eigenvalues across coordinate blocks. Break condition: when basis perfectly matches GN eigenbasis, GN^{-1} achieves one-step convergence for quadratics.

### Mechanism 2: Stochastic Regime Equivalence
At small batches, gradient variance dominates gradient mean in Adam's denominator: (D^{(A)}_{ii})² ≈ σ²ᵢ/B. Lemma 1 shows empirical Fisher ≈ loss · true Fisher, making Adam's square-root scaling align with GN^{-1/2}. Core assumption: Gaussian input distribution x ~ N(0, Σₓ) for linear regression. Break condition: full-batch regime where μ²ᵢ dominates σ²ᵢ/B; non-Gaussian data where Lemma 1 bounds may not hold.

### Mechanism 3: Auto-tuning Through Gradient Norm Regulation
Adam normalizes updates by ||g||, so effective learning rate η/||g|| oscillates around 2/λ_max for each coordinate block. After burn-in where gradient norms decrease, this self-regulation matches maximum stable learning rates. Core assumption: block-symmetric problem structure where coordinates within blocks share gradient dynamics. Break condition: high gradient variance regime where signal-to-noise ratio is low.

## Foundational Learning

- **Diagonal Preconditioning**: Why needed: Both Adam and GN-based methods are framed as diagonal preconditioners; understanding this approximation is essential for comparing their behaviors. Quick check: Why are diagonal preconditioners computationally preferred over full-matrix preconditioners for high-dimensional neural networks?
- **Gauss-Newton Matrix**: Why needed: The GN matrix H^{(GN)} = ∇θf ∇²_f ℓ ∇θf^T underpins the curvature-based preconditioner and differs critically from full Hessian. Quick check: For quadratic loss, what does GN matrix simplify to, and why does this make GN^{-1} optimal in eigenbasis?
- **Fisher vs Empirical Fisher**: Why needed: Adam approximates diagonal empirical Fisher (gradients wrt true labels); GN uses true Fisher (gradients wrt model outputs). This distinction drives equivalence results. Quick check: In batch-size-1 setting with MSE loss, how do empirical Fisher and Fisher relate?

## Architecture Onboarding

- **Component map**: Basis estimation → Gradient rotation → Preconditioner computation → Parameter update
- **Critical path**: Basis choice → Gradient rotation → Preconditioner computation → Scaled update
- **Design tradeoffs**: 
  1. Basis accuracy vs compute: Full eigenbasis O(d³) vs Kronecker approximation O(m³ + n³)
  2. Power selection p = -1 vs -1/2: GN^{-1} optimal in eigenbasis; GN^{-1/2} can outperform in identity basis
  3. Separate batches: Use batch b_G for gradients, larger b_H=4096 for GN matrix estimation
- **Failure signatures**:
  1. GN converges at vanilla GD rate → Check basis: likely using identity with misaligned curvature
  2. GN^{-1} diverges or oscillates → Learning rate too high; add regularization α or reduce η
  3. Adam significantly outperforms GN^{-1/2} at small batches → Check Gaussian data assumption
- **First 3 experiments**:
  1. Block-structured linear regression (Figure 1, left): Compare Adam vs GN^{-1} under identity basis with full batches
  2. GN power comparison (Figures 2 & 8): Construct covariance where condition number favors GN^{-1/2}, run full and small batch
  3. Logistic/Transformer eigenbasis test (Figures 1 right & 7): Run full-batch optimization under correct eigenbasis

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the equivalence between Adam and GN^{-1/2} in stochastic regime extend to practical, large-scale training? Based on statement "We hypothesize that the equivalence between Adam and GN^{-1/2} in the stochastic regime extends to practical, large-scale training." Unresolved due to reliance on linear regression with Gaussian data assumptions.

- **Open Question 2**: How can we design algorithms that exhibit Adam's auto-tuning behavior even when operating with small batches? Based on discussion noting "Adam behaves similarly to GN^{-1/2} at small batch sizes... and does not exhibit this auto-tuning effect." Unresolved because Adam's auto-tuning relies on gradient mean dominating denominator, overwhelmed by variance in small-batch regimes.

- **Open Question 3**: For which data covariance structures Σ_x does GN^{-1} outperform GN^{-1/2}? Based on Appendix A.3 stating "Characterizing covariance matrices for which r(Σ_x) > 1 is left as future work." Unresolved because paper empirically constructs specific covariances but lacks theoretical characterization of data properties dictating optimal preconditioner power.

## Limitations

- Analysis critically depends on Gaussian data assumption for theoretical equivalence results, with unclear generalization to real-world distributions
- Theoretical comparison relies on idealized conditions (full-batch convergence, specific basis alignment) that may not hold in practical deep learning
- Kronecker approximation for eigenbasis computation introduces approximation errors whose impact on theoretical guarantees is not quantified

## Confidence

- **High Confidence**: Theoretical results for linear regression under Gaussian inputs (Section 3.1.1, 3.2) and basis alignment sensitivity mechanism
- **Medium Confidence**: Extension of results to logistic regression and non-convex objectives, given approximations involved
- **Medium Confidence**: Experimental findings on MLPs and Transformers, as they rely on Kronecker-approximated eigenbases

## Next Checks

1. **Data Distribution Sensitivity**: Test stochastic regime equivalence (Adam ≈ GN^{-1/2}) with non-Gaussian data distributions to validate Lemma 1's assumptions
2. **Eigenbasis Approximation Error**: Quantify gap between Kronecker-approximated and exact GN eigenbases across different network architectures and parameter dimensions
3. **Practical Learning Rate Scheduling**: Evaluate whether Adam's auto-tuning behavior holds under practical learning rate schedules (cosine annealing, step decay) rather than idealized constant-rate analysis