---
ver: rpa2
title: Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI
arxiv_id: '2508.00665'
source_url: https://arxiv.org/abs/2508.00665
tags:
- learning
- explanations
- adaptive
- user
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of transparency in AI-driven adaptive
  learning systems by proposing a hybrid framework that combines traditional XAI techniques
  (SHAP, LIME) with generative AI and user personalization to generate multimodal,
  context-aware explanations. The framework aims to improve trust, comprehension,
  and engagement by tailoring explanations to user roles (students, teachers, administrators)
  and preferences.
---

# Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI

## Quick Facts
- arXiv ID: 2508.00665
- Source URL: https://arxiv.org/abs/2508.00665
- Authors: Maryam Mosleh; Marie Devlin; Ellis Solaiman
- Reference count: 36
- Primary result: Proposes a hybrid framework combining traditional XAI with generative AI to create personalized, multimodal explanations for AI decisions in adaptive learning systems

## Executive Summary
This paper addresses transparency gaps in AI-driven adaptive learning by introducing a hybrid framework that merges traditional XAI techniques (SHAP, LIME) with generative AI and user personalization. The approach aims to make AI decisions comprehensible across different educational stakeholders by translating technical attributions into role-appropriate, multimodal explanations. The framework is structured as a six-layer pipeline processing learner data through AI prediction, XAI interpretation, generative translation, personalization, and delivery.

## Method Summary
The framework implements a six-layer pipeline: (1) collect learner data including performance metrics and interaction logs, (2) apply an adaptive model (assumed Bayesian Knowledge Tracing or similar) to generate predictions, (3) use XAI techniques (SHAP, LIME, counterfactuals) to generate feature attributions, (4) employ generative AI to translate technical outputs into natural language explanations, (5) filter through a personalization engine that selects format/depth based on user profiles, and (6) deliver tailored multimodal explanations. The method assumes publicly available educational datasets for implementation and focuses on role-based personalization through text, visuals, or dashboards.

## Key Results
- Framework combines SHAP/LIME with generative AI for accessible explanations
- Personalization engine tailors explanations by role (students, teachers, administrators)
- Dynamic feedback loop updates user preferences based on interaction data
- Identifies trade-offs between explanation understandability, accuracy, and fairness
- Highlights challenges around explanation fidelity and generative AI reliability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If technical explanations (e.g., feature weights) are processed by a generative translation layer, then the output becomes accessible to non-technical users without losing the original decision logic.
- **Mechanism:** Traditional XAI outputs (like SHAP values) are fed into a Generative AI module. This module translates quantitative feature importance into context-aware natural language or visuals based on a predefined persona.
- **Core assumption:** The generative model can accurately map numerical attributions to pedagogical concepts without hallucinating logic that diverges from the underlying AI model.
- **Evidence anchors:**
  - [abstract] "...combines techniques like SHAP and LIME with generative models to translate technical insights into accessible language."
  - [section 3.1] "Generative AI will convert technical XAI outputs into accessible, conversational explanations."
  - [corpus] Paper 48247 ("From Explainable to Explanatory AI") supports the paradigm of using Generative AI to serve user-centered explanations, validating the "translation" approach.
- **Break condition:** The generated explanation contradicts the mathematical logic of the original SHAP/LIME values (e.g., claiming a feature was "very important" when its attribution weight was near zero).

### Mechanism 2
- **Claim:** If explanations are filtered through a dynamic personalization engine based on user roles, then the system can maintain relevance across distinct stakeholders (students, teachers, admins).
- **Mechanism:** A decision layer (Layer 5) takes the raw AI decision and the translated explanation. It queries the user profile to determine the appropriate "depth" and "modality" (e.g., visual dashboard for teachers vs. simple text for students) before delivery.
- **Core assumption:** Users within the same role (e.g., "Teacher") share enough cognitive needs and goals that a role-based template effectively approximates individual preferences.
- **Evidence anchors:**
  - [abstract] "...tailor explanations by role—students, teachers, or administrators—using text, visuals, or dashboards."
  - [section 3.3] Provides distinct examples: students receive motivational feedback, while administrators receive high-level summaries.
  - [corpus] Paper 39045 ("Development of a persuasive User Experience Research...") highlights the need for UX design in XAI, but does not provide evidence that role-based tailoring specifically causes improved learning outcomes.
- **Break condition:** The personalization logic becomes too rigid (e.g., a novice teacher receives advanced analytics meant for an administrator, or an expert student receives over-simplified explanations).

### Mechanism 3
- **Claim:** If user interaction data is continuously fed back into the user profile, the explanation format adapts to changing user preferences over time.
- **Mechanism:** The system monitors user interactions (e.g., skipping visual aids, time spent reading). This data updates the "Dynamic User Profile," which in turn adjusts the Personalization Engine's selection of explanation types.
- **Core assumption:** Behavioral signals (like skipping content) correlate reliably with explanation preference or utility, rather than external factors like time constraints.
- **Evidence anchors:**
  - [section 3.2] Layer 5 includes "...a feedback loop that updates profiles from ongoing engagement."
  - [section 4.4] "User preferences will be initially captured through onboarding surveys and refined through interaction data."
  - [corpus] Corpus evidence for the efficacy of feedback loops in this specific educational context is weak or missing; neighbors focus on static explainability or high-stakes environments.
- **Break condition:** The feedback loop overfits to short-term behaviors (e.g., user skips an explanation because they are busy once, and the system permanently stops providing that explanation type).

## Foundational Learning

- **Concept: SHAP (SHapley Additive exPlanations)**
  - **Why needed here:** This is the raw "source" signal for the transparency layer. You must understand that SHAP assigns a value to each feature indicating how much it pushed the prediction away from the average.
  - **Quick check question:** If a model predicts a student will fail, and "low attendance" has a SHAP value of +0.8, did attendance increase or decrease the probability of failing?

- **Concept: Bayesian Knowledge Tracing (BKT)**
  - **Why needed here:** The paper identifies BKT as the potential underlying AI model for adaptive decisions. It estimates the probability a student has mastered a skill based on past performance.
  - **Quick check question:** In BKT, if a student gets a hard question right largely by guessing (slip/guess parameters), does their estimated mastery probability go up or down?

- **Concept: Hallucination (in GenAI)**
  - **Why needed here:** The framework relies on GenAI to "translate" logic. You must understand the risk where the GenAI confidently asserts facts or reasoning steps that are not present in the input data.
  - **Quick check question:** If the GenAI translates a SHAP value into a sentence saying "The system recommends this because you are struggling," but the SHAP value actually pointed to "item difficulty," is this a hallucination?

## Architecture Onboarding

- **Component map:** Learner interaction data -> Adaptive Model (BKT) -> XAI Layer (SHAP/LIME) -> GenAI Translator -> Personalization Engine -> Output (Text/Visual)
- **Critical path:** The link between the **XAI Layer** and the **GenAI Translator**. If the prompt engineering or context passing fails here, the explanation will be mathematically unfaithful to the model's actual logic.
- **Design tradeoffs:**
  - **Fidelity vs. Readability:** Making explanations "simple" (via GenAI) may strip away the nuance provided by SHAP, potentially misleading users about the true complexity of the decision.
  - **Static vs. Adaptive:** The paper notes LIME/SHAP are static; the proposed dynamic layer adds computational overhead and latency to the user experience.
- **Failure signatures:**
  - **Contradiction:** The text explanation says "You did well," but the visual dashboard shows a red "Warning" indicator.
  - **Oversimplification:** The explanation for an administrator reduces a complex systemic bias issue into a simple "student performance" summary.
  - **Loop Instability:** The explanation format fluctuates wildly between interactions due to noisy feedback loop signals.
- **First 3 experiments:**
  1. **Fidelity Stress Test:** Generate 100 explanations for known edge cases (e.g., contradictory features). Verify if the GenAI output aligns with the raw SHAP values manually.
  2. **Role-Based A/B Testing:** Serve "Generic" explanations to Group A and "Role-Personalized" explanations to Group B. Measure time-to-comprehension and trust scores.
  3. **Feedback Loop Validation:** Simulate a user skipping visual explanations 3 times in a row. Verify if the system correctly suppresses visuals for the 4th interaction.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can personalised explanations remain accurate and faithful to the underlying model's reasoning?
- **Basis in paper:** [explicit] Section 4.1 poses this directly: "How can personalised explanations remain accurate and faithful to the model?"
- **Why unresolved:** The paper notes that adapting explanations for different users may "inadvertently introduce bias, misrepresentation, or inconsistency." No validation mechanisms are yet proposed or tested.
- **What evidence would resolve it:** Validation studies showing that personalised outputs "faithfully reflect the underlying model's reasoning, verified through expert review and alignment with model outputs."

### Open Question 2
- **Question:** What trade-offs exist between explanation understandability, accuracy, and fairness in educational contexts?
- **Basis in paper:** [explicit] Section 4.2 asks: "What trade-offs exist between understandability, accuracy, and fairness?"
- **Why unresolved:** The paper identifies tension between clarity and technical detail but offers no empirical data on optimal balances for different learner profiles.
- **What evidence would resolve it:** User trials comparing multiple explanation types (simple vs. detailed) across learner profiles to identify optimal trade-off points.

### Open Question 3
- **Question:** Can generative AI be reliably used to explain critical educational decisions?
- **Basis in paper:** [explicit] Section 4.3 asks: "Can generative AI be reliably used to explain critical decisions in education?"
- **Why unresolved:** The paper acknowledges that generative models "may generate biased, irrelevant, or inaccurate results" and notes current lack of validation layers.
- **What evidence would resolve it:** Fine-tuning on educational datasets with template constraints, combined with expert pedagogical validation and student feedback studies.

### Open Question 4
- **Question:** How should explanation preferences be modelled and dynamically updated for different users over time?
- **Basis in paper:** [explicit] Section 4.4 asks: "How should explanation preferences be modelled and updated for different users?"
- **Why unresolved:** Preferences may shift as users gain expertise; the paper proposes initial surveys and interaction data but provides no tested methodology for dynamic adaptation.
- **What evidence would resolve it:** Longitudinal studies tracking preference changes via feedback loops, measuring whether adaptive updates improve engagement and comprehension.

## Limitations
- Generative AI translation layer lacks empirical validation for fidelity to model reasoning
- User personalization rules and profile schemas are conceptual without specific implementation details
- Feedback loop mechanism reliability is questionable with weak evidence for behavioral signal correlation
- No empirical data on optimal trade-offs between explanation understandability, accuracy, and fairness

## Confidence
- **High Confidence:** The conceptual pipeline architecture is coherent and addresses a genuine need in adaptive learning systems. The identification of stakeholder-specific explanation needs is well-grounded.
- **Medium Confidence:** The hybrid approach combining traditional XAI with generative AI is technically plausible based on existing literature (e.g., Paper 48247), but the specific implementation details and their educational efficacy remain unproven.
- **Low Confidence:** The feedback loop's ability to improve personalization over time without overfitting to transient behaviors or introducing instability is not demonstrated.

## Next Checks
1. **Fidelity Verification:** Implement the generative translation layer and systematically compare 100+ LLM-generated explanations against raw SHAP values for edge cases to quantify hallucination rates and fidelity loss.
2. **Role-Based Efficacy Testing:** Conduct A/B testing with distinct user groups receiving generic vs. role-personalized explanations, measuring comprehension time, trust ratings, and learning outcomes.
3. **Feedback Loop Stability Analysis:** Simulate user interactions over multiple sessions to evaluate whether the personalization engine adapts appropriately or exhibits overfitting and format fluctuation.