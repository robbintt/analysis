---
ver: rpa2
title: A simple mean field model of feature learning
arxiv_id: '2510.15174'
source_url: https://arxiv.org/abs/2510.15174
tags:
- networks
- learning
- equation
- kernel
- theory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a mean-field (MF) theory for Bayesian inference
  in two-layer neural networks trained with stochastic gradient Langevin dynamics
  (SGLD), focusing on the emergence of feature learning (FL) in finite-width networks.
  The authors derive a self-consistent MF theory that predicts a symmetry-breaking
  phase transition marking the onset of FL, driven by a competition between isotropic
  prior regularization and data-driven alignment.
---

# A simple mean field model of feature learning

## Quick Facts
- arXiv ID: 2510.15174
- Source URL: https://arxiv.org/abs/2510.15174
- Authors: Niclas Göring; Chris Mingard; Yoonsoo Nam; Ard Louis
- Reference count: 40
- Primary result: Mean-field theory predicts symmetry-breaking phase transition for feature learning, but underestimates post-transition generalization until ARD is added

## Executive Summary
This paper develops a mean-field (MF) theory for Bayesian inference in finite-width two-layer neural networks trained with stochastic gradient Langevin dynamics (SGLD). The theory predicts a symmetry-breaking phase transition marking the onset of feature learning (FL), driven by competition between isotropic prior regularization and data-driven alignment. While the basic MF model captures the transition onset, it significantly underestimates post-transition generalization improvements. The authors identify the missing mechanism as self-reinforcing input feature selection (IFS) and incorporate it via Automatic Relevance Determination (ARD), creating an MF-ARD model that quantitatively matches SGLD-trained network performance across varying dataset sizes and noise levels.

## Method Summary
The authors derive a self-consistent mean-field theory by replacing neuron-neuron interactions with their mean field, creating a tractable approximation to the full Bayesian posterior. The theory is solved via fixed-point iteration on order parameters measuring task alignment and coordinate-wise weight variances. To capture the missing IFS mechanism, they incorporate ARD priors with learned precisions ρ_j that create coordinate-dependent regularization. The resulting MF-ARD model includes an additional outer loop to update ρ_j based on the current weight variances, preserving tractability while capturing the positive feedback loop that amplifies alignment differences between relevant and irrelevant coordinates.

## Key Results
- MF theory predicts symmetry-breaking phase transition for FL onset at critical dataset size P_c and noise level κ_c
- Basic MF model underestimates post-transition generalization improvements by 10-20%
- MF-ARD model captures self-reinforcing IFS mechanism and quantitatively matches SGLD performance
- Phase boundary scales with intrinsic dimension k rather than ambient dimension d, overcoming curse of dimensionality
- MF-ARD successfully reproduces the "helpful noise" regime where moderate noise lowers P_c

## Why This Works (Mechanism)

### Mechanism 1: Symmetry-Breaking Phase Transition (FL Onset)
- Claim: Feature learning begins when data signal strength exceeds a critical threshold, triggering a discontinuous transition from isotropic prior to task-aligned weight configurations.
- Mechanism: The MF equations admit a trivial fixed point (m_A = 0) where weights remain at prior. As dataset size P increases or noise κ decreases past critical values (P_c, κ_c), this fixed point becomes unstable. The neuron-data coupling term J_Y(w) breaks the prior's symmetry, pulling weights toward task-aligned configurations. The order parameter m_S = E_x[⟨f(x)⟩_p χ_S(x)] jumps discontinuously from zero.
- Core assumption: Self-consistency closure is valid—replacing neuron-neuron interactions with their mean field preserves the essential phase transition physics.
- Evidence anchors:
  - [abstract] "At finite width it predicts a symmetry breaking phase transition where networks abruptly align with target functions."
  - [Section 3.1] "The phase transition occurs when ω_0 > 1... the order parameter m_S = 0 until the critical dataset size P_c, when it increases abruptly."
  - [corpus] Lauditi et al. (2025) and related dynamical MF work confirm the lazy-to-rich transition controlled by effective learning rate, consistent with phase transition phenomenology.
- Break condition: If network width N → ∞ with γ = 1/2 scaling, the data-dependent terms vanish and p(w) collapses to prior—no phase transition occurs.

### Mechanism 2: Self-Reinforcing Input Feature Selection (IFS)
- Claim: Post-transition generalization improvements require a positive feedback loop where initially small alignment differences amplify into coordinate-wise anisotropy, with task-relevant dimensions receiving progressively weaker regularization.
- Mechanism: Under ARD, the precision ρ_j controls shrinkage on coordinate j. When coordinate j ∈ S aligns with target, ⟨w²_j⟩ increases, which decreases ρ_j via the ARD fixed point equation (ρ_j ∝ 1/⟨w²_j⟩). Lower ρ_j reduces shrinkage, further increasing ⟨w²_j⟩. This positive feedback creates heavy-tailed marginals p(w_j) on task-relevant coordinates while shrinking irrelevant coordinates toward zero.
- Core assumption: The ε-symmetry breaking condition holds—there exists O(1) asymmetry in variance between relevant vs. irrelevant coordinates at some early iteration t_0.
- Evidence anchors:
  - [abstract] "We trace this discrepancy to a key mechanism absent from the plain MF description: self-reinforcing input feature selection."
  - [Section 4.1] "The map ⟨w²_j⟩ → ρ_j → p_ARD(w) realises IFS: if coordinate j aligns with the target... making ⟨w²_{j∈S}⟩ larger, which decreases ρ_{j∈S}, thereby reducing shrinkage on w_{j∈S}."
  - [corpus] Evidence is indirect—corpus papers discuss kernel adaptation but do not explicitly verify the IFS feedback loop.
- Break condition: If the target function has dense support (k ≈ d) rather than sparse structure, ARD's coordinate-wise selection provides limited benefit over plain MF.

### Mechanism 3: Overcoming the Curse of Dimensionality via Dimensional Scaling
- Claim: MF-ARD achieves critical noise κ_c scaling with intrinsic dimension k rather than ambient dimension d because ARD effectively rescales the prior curvature from C_MF = dk/σ²_w to C_ARD = Θ(k).
- Mechanism: Plain MF imposes uniform penalty across all d coordinates, so the critical threshold scales as κ²_c ∼ √(1/dk). MF-ARD learns coordinate-dependent precisions that concentrate weight variance on the k relevant dimensions. The effective curvature along the S-direction becomes Θ(k) rather than Θ(dk), yielding κ²_c ∼ √(1/k).
- Core assumption: The target has sparse structure with support size k ≪ d, and ε-symmetry breaking occurs within O(1) iterations.
- Evidence anchors:
  - [Section 4.2, Theorem 4.1] "κ²_c ≍ Θ√(1/dk)) (plain MF), Θ√(1/k)) (MF-ARD)... the phase boundary scales with the intrinsic problem dimension k rather than the ambient dimension d."
  - [Figure 5] Phase diagrams show MF-ARD closely matches SGLD phase boundary location and sharpness across varying P and κ.
  - [corpus] Damian et al. (2022) and Barak et al. (2022) establish that finite-width networks learn sparse parity with better sample complexity than kernels, but do not derive the explicit κ_c scaling.
- Break condition: If the task requires distributed rather than sparse representations, or if input distribution lacks the coordinate structure assumed by ARD, the dimensional benefit degrades.

## Foundational Learning

- Concept: **Mean-field theory from statistical physics**
  - Why needed here: The paper's core approximation replaces complex neuron-neuron interactions G(w_i, w_i') with an effective field ⟨f(x)⟩. Understanding this simplification is essential to interpret what the theory captures vs. misses.
  - Quick check question: Can you explain why replacing pairwise interactions with their mean preserves the phase transition but loses sparsification?

- Concept: **SGLD as Bayesian posterior sampling**
  - Why needed here: The theory analyzes the stationary distribution p(θ|D) ∝ exp(-E/T) rather than training dynamics. The equivalence between SGLD updates and posterior sampling (with T = 2κ²) grounds all derivations.
  - Quick check question: If you increase the SGLD temperature T while keeping the prior fixed, what happens to the effective noise κ and the phase transition threshold?

- Concept: **Order parameters and self-consistency equations**
  - Why needed here: The MF theory is solved via fixed-point iteration on {m_A} and {ρ_j}. Understanding that m_A measures task alignment and ρ_j controls coordinate shrinkage is prerequisite for implementing the solver.
  - Quick check question: In the k-sparse parity setting, what does m_S > 0 imply about the network's learned representation?

## Architecture Onboarding

- Component map: SGLD Posterior (fully interacting) -> MF Theory (independent neurons, shared p(w,a)) -> MF-ARD (captures IFS via learned precisions) -> NNGP/Kernel Ridge Regression (no FL)

- Critical path:
  1. Define sufficient statistics per particle: C₁,b (residual correlation), C₂,b (self-energy), G_data_b (gradient)
  2. Initialize particles {w_b, a_b} from prior; set ρ = d/σ²_w
  3. Inner loop: Run K SGLD steps using gradients ∇_w U, ∇_a U
  4. Outer loop: Update ρ via fixed point equation ρ_j = (α₀ + N/2)/(α₀/d + N/2⟨w²_j⟩)
  5. Iterate outer loop to convergence; read out {m_A} and generalization error

- Design tradeoffs:
  - Plain MF vs. MF-ARD: Plain MF is simpler (no ρ to track) but underestimates post-transition generalization. MF-ARD adds d extra order parameters but captures IFS.
  - Particle count B: Higher B → better approximation to true posterior, but O(B) memory/compute. Paper uses B = N = 512.
  - Inner steps K: More steps → better convergence to stationary distribution per outer iteration, but slower wall-clock time. Paper decays K from 12 to 2.

- Failure signatures:
  - If phase transition not observed: Check that P > P_c and κ < κ_c. For sparse parity d=35, k=4, P_c ≈ 500-1000.
  - If MF-ARD still underestimates generalization: Verify ARD is enabled and ρ is actually being updated (check EMA momentum λ).
  - If m_S grows but error doesn't decrease: Check that evaluation uses held-out data; verify target is single Walsh mode χ_S.

- First 3 experiments:
  1. **Reproduce Figure 1a (sparse parity learning curves):** Train SGLD networks and run MF/MF-ARD solvers on k-sparse parity (d=35, k=4). Verify MF predicts transition onset but MF-ARD matches post-transition error.
  2. **Ablate ARD mechanism:** Run MF-ARD with ARD disabled (fix ρ = d/σ²_w). Confirm that this recovers plain MF behavior, establishing that learned precisions—not just the MF structure—drive improved predictions.
  3. **Scale dimension experiment:** Fix k=4, vary d ∈ {20, 35, 50, 100}. Plot κ_c vs. d for both MF and MF-ARD. Verify MF-ARD's κ_c remains constant while MF's κ_c decreases as 1/√d.

## Open Questions the Paper Calls Out

- **Question:** Can the MF-ARD framework be extended to deeper architectures such as convolutional or attention layers while preserving tractability?
  - Basis in paper: [explicit] "Extending to deeper architectures (e.g., convolutional or attention layers)... remains an open challenge"
  - Why unresolved: The current theory relies on a tractable single-hidden-layer structure where the MF factorization yields closed-form updates; deeper architectures introduce hierarchical feature hierarchies that may require more complex order parameters.
  - What evidence would resolve it: Derivation of self-consistent MF equations for 3+ layer networks, or convolutional layers with weight sharing, showing quantitative agreement with SGLD-trained networks on appropriate tasks.

- **Question:** How can the static posterior framework be rigorously connected to the actual training dynamics of gradient descent?
  - Basis in paper: [explicit] "as does connecting the static posterior view to training dynamics"
  - Why unresolved: The MF theory analyzes the stationary distribution after training, but real training involves non-equilibrium dynamics with learning rate schedules and finite-time effects that the equilibrium framework cannot capture.
  - What evidence would resolve it: A theorem establishing conditions under which SGD trajectories converge to the MF-predicted posterior, or empirical demonstrations that MF-predicted generalization errors match finite-time trained networks across learning rate schedules.

- **Question:** Does the MF-ARD mechanism remain effective for tasks requiring distributed or smooth representations rather than sparse structure?
  - Basis in paper: [explicit] "settings requiring distributed or smooth representations remains an open challenge (Petrini et al., 2022)"
  - Why unresolved: The ARD prior specifically encourages coordinate-wise sparsity; distributed representations where many input dimensions contribute similarly may not benefit from this mechanism.
  - What evidence would resolve it: Experiments on target functions with dense support (e.g., polynomial functions with many active monomials) comparing MF-ARD vs SGLD generalization curves; theoretical analysis of MF-ARD fixed points for non-sparse targets.

- **Question:** What is the mechanistic explanation for the "helpful noise" regime where moderate κ lowers the critical sample size P_c?
  - Basis in paper: [inferred] Figure 5 shows MF-ARD reproduces "the 'helpful noise' regime in which moderate κ lowers the critical sample size P_c (the kink around κ = 0.05)" but no theoretical explanation is provided.
  - Why unresolved: Intuitively, noise should only hurt generalization; the observed improvement suggests noise may help escape local minima or regularize in a beneficial way, but the MF-ARD theory does not predict this effect.
  - What evidence would resolve it: Analysis of the MF-ARD fixed point equations showing non-monotonic dependence of m_S on κ, or identification of a noise-induced regularization mechanism that improves spectral alignment with the target.

## Limitations

- The theory's predictive power depends critically on the validity of the mean-field closure and the ε-symmetry breaking condition, which are not rigorously proven.
- The self-reinforcing IFS mechanism, while plausible, relies on heuristic arguments about early asymmetry amplification rather than rigorous mathematical derivation.
- The model assumes a specific sparse parity target structure that may not generalize to arbitrary function classes or tasks requiring distributed representations.

## Confidence

- **High Confidence**: The existence of the symmetry-breaking phase transition and its basic phenomenology (onset at critical P and κ values) is well-supported by both theory and simulations. The MF equations correctly predict when feature learning begins.
- **Medium Confidence**: The MF-ARD model's quantitative match to SGLD performance across varying conditions is compelling but indirect. The improvements over plain MF suggest IFS is captured, but direct validation of the variance feedback loop would strengthen the mechanistic claims.
- **Low Confidence**: The scaling argument for overcoming the curse of dimensionality (κ_c ~ √(1/k) vs √(1/dk)) is elegant but relies on the specific coordinate structure of the target. For tasks without sparse structure or with correlated inputs, the dimensional benefits may not materialize.

## Next Checks

1. **Direct IFS mechanism validation**: Instrument the training process to track coordinate-wise variance evolution ⟨w²_j⟩ and precisions ρ_j over time. Verify that task-relevant coordinates show the predicted positive feedback loop where alignment increases variance, which decreases ρ_j, which further increases variance.

2. **Generalization to dense targets**: Test MF-ARD on targets with k ~ d (dense support) rather than sparse structure. Confirm that the dimensional scaling advantage disappears as expected when ARD's coordinate selection provides no benefit.

3. **Noise sensitivity verification**: Systematically vary the intrinsic noise level κ and verify the predicted scaling of the critical dataset size P_c ~ 1/κ² for both MF and MF-ARD. Check that MF-ARD maintains constant P_c while MF's P_c grows with ambient dimension d.