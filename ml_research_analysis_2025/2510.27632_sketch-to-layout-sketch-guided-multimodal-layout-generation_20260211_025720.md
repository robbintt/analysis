---
ver: rpa2
title: 'Sketch-to-Layout: Sketch-Guided Multimodal Layout Generation'
arxiv_id: '2510.27632'
source_url: https://arxiv.org/abs/2510.27632
tags:
- sketch
- layout
- sketches
- generation
- assets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Sketch-to-Layout, a novel method for generating
  multimodal layouts guided by user-provided sketches. The approach leverages a multimodal
  transformer-based model (PaLIGemma) that takes a sketch, along with image and text
  assets, to produce high-quality layouts that reflect the structure suggested by
  the sketch.
---

# Sketch-to-Layout: Sketch-Guided Multimodal Layout Generation

## Quick Facts
- arXiv ID: 2510.27632
- Source URL: https://arxiv.org/abs/2510.27632
- Authors: Riccardo Brioschi; Aleksandr Alekseev; Emanuele Nevali; Berkay DÃ¶ner; Omar El Malki; Blagoj Mitrevski; Leandro Kieliger; Mark Collier; Andrii Maksai; Jesse Berent; Claudiu Musat; Efi Kokiopoulou
- Reference count: 40
- Primary result: Achieves 40% improvement in Maximum IoU over state-of-the-art constraint-based layout generation methods

## Executive Summary
This paper introduces Sketch-to-Layout, a novel method for generating multimodal layouts guided by user-provided sketches. The approach leverages a multimodal transformer-based model (PaLIGemma) that takes a sketch, along with image and text assets, to produce high-quality layouts that reflect the structure suggested by the sketch. To address data scarcity, the authors propose a scalable method to synthetically generate training sketches at scale by combining hand-drawn primitive sketches of layout elements. Evaluated on three publicly available datasets (PubLayNet, DocLayNet, SlidesVQA), Sketch-to-Layout outperforms state-of-the-art constraint-based layout generation methods by over 40% in Maximum IoU while offering a more intuitive design experience.

## Method Summary
The Sketch-to-Layout method fine-tunes the PaLIGemma 3B model (a vision-language model combining Gemma LLM with SigLIP ViT) to generate layouts from sketches and content assets. The model takes as input a sketch image, asset images, and text content, and outputs a protocol buffer string describing the layout. To overcome data scarcity, the authors synthetically generate training sketches by combining hand-drawn primitive sketches of layout elements using KD-tree nearest-neighbor matching on attributes like width, height, and font size. The model is trained for 10 epochs with batch size 128, learning rate 10^-4, and a cosine scheduler with frozen ViT encoder.

## Key Results
- Outperforms state-of-the-art constraint-based layout generation methods by over 40% in Maximum IoU
- Comparable performance on both synthetic and human-produced sketches validates the use of synthetic data
- Introduces Content Ordering Score (COS) metric to assess content-awareness in generated layouts
- Ablation studies show that including asset content information significantly boosts performance

## Why This Works (Mechanism)

### Mechanism 1: Sketch as a High-Bandwidth Spatial Prior
Using sketches as guidance modality is more efficient than textual constraints because they reduce the search space for spatial arrangement with lower user effort. The VLM accepts the sketch as visual token sequence alongside asset tokens, providing explicit geometric boundaries and topology that constrains the decoder to generate coordinates aligned with user's ink strokes.

### Mechanism 2: Content-Aware Semantic Ordering
Providing asset content in addition to the sketch allows the model to generate layouts with coherent narrative flows. The architecture concatenates patch embeddings of sketch and image assets with text embeddings of asset content, allowing attention mechanism to weigh semantic relationships when predicting coordinates.

### Mechanism 3: Synthetic Sketch Generalization
A model fine-tuned on synthetically composited sketches generalizes to human-drawn sketches because the synthetic pipeline preserves the geometric distribution of layout elements. Instead of collecting full-layout sketches, the authors collect primitives and compose them during training, forcing the model to learn robustness to stroke variation while seeing every possible layout configuration.

## Foundational Learning

- **Vision-Language Models (VLMs)**: Core engine is PaLIGemma, a VLM combining ViT and LLM. Why needed: to encode sketch and assets into tokens the LLM can process. Quick check: How does the model handle multiple visual inputs? (Answer: Concatenation of patch embeddings).

- **Protocol Buffers (Protobuf)**: Model outputs structured text, specifically Protobuf strings. Why needed: for schema enforcement and efficient serialization/rendering. Quick check: Why use Protobuf over JSON? (Answer: Schema enforcement and efficient serialization).

- **Intersection over Union (IoU)**: Performance measured by mIoU (Maximum IoU). Why needed: measures overlap between generated and target bounding boxes. Quick check: What does 40% improvement in mIoU imply? (Answer: Elements significantly closer to intended positions).

## Architecture Onboarding

- **Component map**: Ink-based Sketch (ViT) -> PaLIGemma 3B (LLM+ViT) -> Protobuf string -> Renderer (SVG/HTML). Asset Images (ViT) and Asset Text/Dimensions (LLM) are also inputs.

- **Critical path**: The Synthetic Sketch Generation Pipeline is the bottleneck. Without this, you cannot fine-tune the model effectively due to data scarcity. The KD-Tree retrieval of primitives must be fast and attribute-accurate.

- **Design tradeoffs**: Synthetic vs. Real Data (massive scale vs. perfect realism). Content-Aware vs. Geometry-Only (increased prompt length and cost vs. higher COS).

- **Failure signatures**: Hallucination (generates asset names not present in input). Misordering (places boxes correctly but messes up reading sequence). Model ignores sketch structure.

- **First 3 experiments**:
  1. Verify Synthetic Pipeline: Generate synthetic sketches for held-out validation set and visually confirm primitive placement matches ground truth layout structure.
  2. Zero-Shot Baseline: Test pre-trained PaLIGemma (without fine-tuning) on sketch-to-layout task to quantify instruction following gap synthetic data fills.
  3. Content Ablation: Run inference with asset content replaced by "Lorem Ipsum" to measure degradation in COS versus gain in inference speed.

## Open Questions the Paper Calls Out

1. **Can sketch-to-layout models effectively leverage visual content of multiple distinct image assets?** The current model appears to rely heavily on spatial cues from the sketch rather than semantic content of images, particularly when handling more than one image.

2. **Does the synthetic sketch generation methodology generalize to complex domains or primitive types beyond simple wireframes?** The current study focuses on documents and slides using specific "wireframe" primitives; it is untested whether this primitive-composition approach scales to more intricate domains like mobile UI or graphic design.

3. **Can larger or more powerful VLMs overcome current limitations to achieve production-level performance?** The study utilized a 3B parameter model, which still shows distinct limitations in content-awareness and alignment compared to ground truth.

## Limitations

- Synthetic sketch generalization remains untested for edge cases with significant human sketching pattern divergence
- VLM robustness to sketch ambiguity is untested for unfamiliar iconography or highly abstract representations
- Dataset dependency limits applicability to domains without structured data with ground truth layouts

## Confidence

- **High confidence**: 40% improvement in mIoU over constraint-based methods is well-supported by experimental results across three datasets
- **Medium confidence**: Content Ordering Score (COS) metric is clearly defined but correlation with human perception isn't validated
- **Low confidence**: Real-world deployment claims aren't addressed for computational constraints, model size limitations, or user experience studies

## Next Checks

1. **Sketch ambiguity stress test**: Create test suite with intentionally ambiguous sketches and evaluate whether model produces reasonable defaults or hallucinates incorrect constraints.

2. **Cross-domain transferability evaluation**: Apply fine-tuned model to novel layout domain (e.g., mobile app interfaces) without additional training and quantify performance drop.

3. **User study on content awareness**: Conduct controlled experiment comparing sketch-guided tool versus constraint-based tools, measuring task completion time, user satisfaction, and comparing COS scores against expert rankings.