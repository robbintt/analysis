---
ver: rpa2
title: Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven
  Reactive Language Models
arxiv_id: '2510.03561'
source_url: https://arxiv.org/abs/2510.03561
tags:
- memory
- attention
- processing
- query
- interaction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Reactive Transformer (RxT), an event-driven
  architecture for conversational AI that addresses the quadratic scaling and latency
  issues of standard Transformers. RxT maintains a fixed-size Short-Term Memory (STM)
  and processes each interaction as a discrete event, decoupling response generation
  from asynchronous memory updates.
---

# Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models

## Quick Facts
- arXiv ID: 2510.03561
- Source URL: https://arxiv.org/abs/2510.03561
- Authors: Adam Filipek
- Reference count: 15
- This paper introduces the Reactive Transformer (RxT), an event-driven architecture for conversational AI that addresses the quadratic scaling and latency issues of standard Transformers.

## Executive Summary
This paper introduces the Reactive Transformer (RxT), an event-driven architecture for conversational AI that addresses the quadratic scaling and latency issues of standard Transformers. RxT maintains a fixed-size Short-Term Memory (STM) and processes each interaction as a discrete event, decoupling response generation from asynchronous memory updates. This design reduces total conversation cost from O(N²·T) to O(N·T), enabling real-time, stateful processing.

Experiments on synthetic multi-turn dialogue datasets show that RxT models significantly outperform a stateless LLM baseline. Across model scales (12M to 160M parameters), RxT achieves 35-50% lower perplexity and 25-60% higher accuracy. The MRL reward score, a composite measure of conversational coherence, also improves substantially (mean scores 3.1-3.8 vs. 2.4 for baseline). Importantly, RxT maintains constant prompt-phase latency regardless of dialogue length, unlike the baseline whose latency grows with context size. The results demonstrate that specialized, stateful architectures outperform generic models for dialogue tasks, even with fewer parameters.

## Method Summary
The Reactive Transformer (RxT) is an event-driven architecture designed for conversational AI that maintains a fixed-size Short-Term Memory (STM) and processes each interaction as a discrete event. Unlike standard Transformers that process entire conversation history, RxT decouples response generation from asynchronous memory updates, reducing computational complexity from O(N²·T) to O(N·T). The architecture enables real-time, stateful processing while maintaining constant prompt-phase latency regardless of dialogue length.

## Key Results
- RxT models achieve 35-50% lower perplexity compared to a stateless LLM baseline across model scales (12M to 160M parameters)
- RxT demonstrates 25-60% higher accuracy and substantially improved MRL reward scores (3.1-3.8 vs 2.4) for conversational coherence
- RxT maintains constant prompt-phase latency regardless of dialogue length, while baseline latency grows with context size

## Why This Works (Mechanism)
RxT's event-driven architecture enables efficient processing by maintaining a fixed-size Short-Term Memory (STM) and decoupling response generation from asynchronous memory updates. This approach addresses the quadratic scaling problem of standard Transformers by processing each interaction as a discrete event rather than reprocessing the entire conversation history. The fixed STM size prevents context bloat while the event-driven model allows for real-time, stateful processing without sacrificing performance.

## Foundational Learning
- Event-driven architecture: Processes interactions as discrete events rather than continuous streams, enabling efficient resource allocation and real-time processing
- Short-Term Memory (STM) management: Fixed-size memory buffer that maintains conversational context while preventing quadratic scaling of computational costs
- Asynchronous memory updates: Decouples response generation from memory maintenance, allowing parallel processing and constant latency
- Stateful vs stateless processing: Maintains conversation state across interactions rather than treating each prompt independently
- Computational complexity analysis: Understanding the transition from O(N²·T) to O(N·T) complexity and its practical implications
- Quick check: Verify that STM size remains constant while conversation length increases, and confirm that latency measurements remain stable across varying context sizes

## Architecture Onboarding

Component map: User Interaction -> RxT Event Processor -> STM Manager -> Response Generator -> Output

Critical path: User Interaction → RxT Event Processor → Response Generator → Output (STM updates occur asynchronously)

Design tradeoffs:
- Fixed STM size vs dynamic context management: RxT prioritizes predictable performance over unlimited context retention
- Event-driven vs continuous processing: Enables real-time capabilities but requires careful state synchronization
- Parameter efficiency vs model capacity: Smaller models achieve better performance through specialized architecture rather than increased parameters

Failure signatures:
- Memory overflow errors when STM capacity is exceeded
- State desynchronization between response generation and memory updates
- Latency spikes if asynchronous processing becomes blocked

3 first experiments:
1. Measure prompt-phase latency across increasing conversation lengths (10, 50, 100, 500 turns) to verify constant-time performance
2. Compare STM memory utilization patterns between synthetic and real-world dialogue datasets
3. Test STM capacity limits by generating conversations that exceed memory buffer size and analyzing performance degradation

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- Performance gains are validated only on synthetic multi-turn dialogue datasets, leaving generalizability to real-world conversations uncertain
- Fixed STM size introduces potential bottlenecks without analysis of performance degradation when memory capacity is exceeded
- Computational complexity claims (O(N·T) vs O(N²·T)) are theoretically sound but lack comprehensive empirical validation across diverse workloads

## Confidence

Major Claim Confidence Labels:
- Computational efficiency gains: High confidence (theoretical analysis is sound and basic empirical validation provided)
- Performance improvements on synthetic datasets: Medium confidence (results are strong but limited to synthetic data)
- Real-time capability with constant latency: Medium confidence (demonstrated on benchmarks but real-world validation needed)
- Architectural superiority over generic LLMs: Medium confidence (synthetic results positive but limited scope)

## Next Checks
1. Evaluate RxT on established human-human dialogue datasets (e.g., MultiWOZ, Persona-Chat) to assess real-world performance and generalization
2. Conduct stress tests with memory overflow scenarios to characterize performance degradation patterns and determine STM sizing heuristics
3. Perform end-to-end latency measurements in production-like environments with concurrent conversations to validate real-time processing claims under realistic load conditions