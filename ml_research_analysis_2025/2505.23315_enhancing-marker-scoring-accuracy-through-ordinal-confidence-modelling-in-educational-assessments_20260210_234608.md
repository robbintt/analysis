---
ver: rpa2
title: Enhancing Marker Scoring Accuracy through Ordinal Confidence Modelling in Educational
  Assessments
arxiv_id: '2505.23315'
source_url: https://arxiv.org/abs/2505.23315
tags:
- cefr
- confidence
- kwocce
- loss
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of releasing automated essay
  scores only when they meet high reliability standards, by developing a confidence
  model that predicts whether an AES-generated score correctly assigns a candidate
  to the appropriate CEFR level. The approach leverages the ordinal nature of CEFR
  labels by reformulating the task as an n-ary classification problem with score binning,
  and introduces a novel Kernel Weighted Ordinal Categorical Cross Entropy (KWOCCE)
  loss function that penalizes misclassifications based on their ordinal distance
  from the true class.
---

# Enhancing Marker Scoring Accuracy through Ordinal Confidence Modelling in Educational Assessments

## Quick Facts
- arXiv ID: 2505.23315
- Source URL: https://arxiv.org/abs/2505.23315
- Authors: Abhirup Chakravarty; Mark Brenchley; Trevor Breakspear; Ian Lewin; Yan Huang
- Reference count: 17
- Primary result: KWOCCE Linear model achieves 0.97 F1 score, enabling 47% score release with 100% CEFR agreement

## Executive Summary
This study addresses the challenge of automated essay scoring reliability by developing a confidence model that predicts whether AES-generated scores correctly assign candidates to appropriate CEFR levels. The approach leverages the ordinal nature of CEFR labels by reformulating the task as an n-ary classification problem with score binning, introducing a novel Kernel Weighted Ordinal Categorical Cross Entropy (KWOCCE) loss function that penalizes misclassifications based on their ordinal distance from the true class. The system achieves 100% CEFR agreement for 47% of released scores while maintaining 99% agreement for at least 95% of CEFR levels, compared to approximately 92% agreement from the standalone AES model.

## Method Summary
The methodology transforms the regression task of predicting CEFR scores into an n-ary classification problem by binning scores into discrete levels. The study compares multiple classification approaches including multi-layer perceptrons, random forests, and linear models, evaluating them against standard categorical cross-entropy and the proposed KWOCCE loss function. KWOCCE incorporates a Gaussian kernel to weight penalties based on ordinal distance between predicted and true classes, theoretically improving calibration for ordinal classification tasks. The evaluation uses a single dataset from Cambridge Assessment English with train/validation/test splits to ensure robust performance assessment.

## Key Results
- Best-performing KWOCCE Linear model achieves F1 score of 0.97
- System releases 47% of scores with 100% CEFR agreement
- At least 99% of released scores achieve 95% CEFR agreement
- Standalone AES model shows approximately 92% CEFR agreement when releasing all scores

## Why This Works (Mechanism)
The KWOCCE loss function works by incorporating ordinal distance into the penalty structure, recognizing that misclassifying a B2 as B1 is less severe than misclassifying it as A1. The Gaussian kernel weights errors exponentially based on their distance from the true class, creating a smoother gradient for ordinal relationships compared to standard categorical cross-entropy. This approach leverages the inherent ordering in CEFR levels to improve calibration and confidence estimation, allowing the system to identify high-confidence predictions that can be safely released while flagging uncertain cases for human review.

## Foundational Learning
- **CEFR framework**: The Common European Framework of Reference for Languages provides a standardized scale (A1-C2) for describing language proficiency levels; needed for establishing the ordinal classification target and evaluation metric
- **Ordinal classification**: A machine learning task where target classes have a natural ordering; quick check: verify that the binning strategy preserves meaningful ordinal relationships between adjacent classes
- **Confidence modeling**: Techniques for estimating prediction certainty; quick check: ensure confidence scores correlate with actual prediction accuracy on held-out data
- **Gaussian kernel weighting**: A function that assigns exponentially decreasing weights based on distance; quick check: validate that kernel parameters produce appropriate penalty scaling for typical ordinal distances

## Architecture Onboarding

**Component map:**
Input essay features -> Feature extraction module -> KWOCCE classification layer -> Confidence threshold -> Score release decision

**Critical path:**
Essay features → KWOCCE model → Confidence score → Threshold comparison → Release decision

**Design tradeoffs:**
- Binning granularity vs. ordinal preservation: finer bins provide more precise scoring but may weaken ordinal relationships
- Kernel bandwidth selection: wider kernels smooth penalties but may under-penalize distant errors; narrower kernels provide sharper discrimination but may overfit to training data
- Confidence threshold: higher thresholds increase accuracy but reduce score release rate, creating tension between reliability and availability

**Failure signatures:**
- Over-conservative thresholding leading to very low score release rates
- Loss of ordinal structure in binned labels causing misclassification of adjacent classes
- Kernel parameters that fail to properly weight ordinal distances, resulting in sub-optimal confidence estimates

**First experiments:**
1. Compare KWOCCE performance against standard categorical cross-entropy across different binning strategies
2. Evaluate sensitivity of confidence thresholds to achieve different score release rates
3. Test model generalization on essays from different test administrations or CEFR frameworks

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on a single CEFR-aligned dataset from Cambridge Assessment English, limiting generalizability across assessment contexts and languages
- 47% score release rate may be too conservative for high-stakes testing scenarios requiring broader score availability
- Practical benefit of KWOCCE over standard categorical cross-entropy remains modest in absolute terms (0.01-0.02 F1 improvement)

## Confidence
- **High Confidence**: Experimental methodology and reported metrics (F1 scores, CEFR agreement percentages) are internally consistent and follow standard ML practices
- **Medium Confidence**: The practical significance of KWOCCE's incremental improvement over simpler approaches suggests limited real-world deployment benefit
- **Medium Confidence**: The assertion that 47% score release with 100% CEFR agreement represents an optimal balance requires further validation across different testing contexts

## Next Checks
1. External validation on diverse datasets including non-English languages and alternative proficiency frameworks (e.g., ILR, TOEFL) to assess cross-domain generalization
2. A/B testing of different release rate thresholds in operational settings to quantify impact on test-taker experience and administrative efficiency
3. Ablation studies varying the ordinal distance function and kernel parameters to determine sensitivity of KWOCCE performance to design choices