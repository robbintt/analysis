---
ver: rpa2
title: 'Low-Resource Dialect Adaptation of Large Language Models: A French Dialect
  Case-Study'
arxiv_id: '2510.22747'
source_url: https://arxiv.org/abs/2510.22747
tags:
- french
- language
- tasks
- training
- dialect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper explores how to adapt large language models (LLMs) to\
  \ low-resource regional dialects using minimal compute and data. The authors employ\
  \ continual pre-training with low-rank adaptation (LoRA) and gradient checkpointing\
  \ to efficiently specialize three LLMs\u2014CroissantLLM, Llama-3.2-1B, and Llama-3.1-8B\u2014\
  to the Qu\xE9bec French dialect."
---

# Low-Resource Dialect Adaptation of Large Language Models: A French Dialect Case-Study

## Quick Facts
- arXiv ID: 2510.22747
- Source URL: https://arxiv.org/abs/2510.22747
- Reference count: 0
- Adapting LLMs to Québec French dialect using parameter-efficient LoRA with 86M tokens

## Executive Summary
This paper demonstrates how to adapt large language models to low-resource regional dialects using minimal compute and data. The authors employ continual pre-training with low-rank adaptation (LoRA) and gradient checkpointing to efficiently specialize three LLMs—CroissantLLM, Llama-3.2-1B, and Llama-3.1-8B—to the Québec French dialect. They train on only 86.57 million tokens, updating less than 1% of model parameters, and evaluate on a subset of the COLE benchmark suite. Results show that after six epochs of training, all models improved on Québec French tasks, though gains varied by task and model size. Larger models (8B parameters) achieved better balance between dialect adaptation and retention of general French abilities.

## Method Summary
The approach combines continual pre-training (CPT) with LoRA to efficiently adapt base models to Québec French. LoRA modules are applied to attention (q,k,v,o) and FFN (up,gate,down) layers with rank r=16, α=32, dropout=0.1. Training uses fp16 with gradient checkpointing, sequence length 1024, stride 512, AdamW optimizer (weight decay 0.01), learning rate 1e-5 with cosine decay and 0.1 warmup, and gradient clipping at 1.0. The corpus consists of 86.57M tokens across 9 sources (40% informal, 60% formal) with light preprocessing preserving orthographic errors. Models are evaluated on 8 COLE tasks after 3 and 6 epochs.

## Key Results
- All three models (CroissantLLM, Llama-3.2-1B, Llama-3.1-8B) improved on Québec French tasks after 6 epochs
- Larger models (8B) achieved better balance between dialect adaptation and retention of general French abilities
- Performance highly dependent on corpus composition, with informal text improving colloquial tasks but degrading normative grammar scores
- <1% of parameters updated while sustaining throughput and performance

## Why This Works (Mechanism)

### Mechanism 1
Continual pre-training shifts the model's internal probability distribution to bridge the dialect gap without destroying base language understanding. By optimizing causal language modeling on unlabeled dialect text, the model adjusts weights to minimize perplexity on regional distribution while retaining structural knowledge from initial pre-training. This works because Québec French shares sufficient syntactic foundations with Standard French for low-rank updates to capture variance. Evidence shows perplexity dropping sharply after the first epoch. If the dialect is too distant, CPT may fail to converge.

### Mechanism 2
LoRA acts as a parameter-efficient regularizer allowing models to "memorize" dialect features while freezing bulk general capabilities. By injecting trainable rank-decomposition matrices into transformer layers while freezing base weights, the model adapts within low-dimensional subspace, mitigating catastrophic forgetting. This assumes linguistic shift required is low-rank. Evidence shows updating <1% parameters while sustaining performance. If rank is too low, underfitting occurs; too high risks overfitting small corpus.

### Mechanism 3
Gains are highly contingent on corpus composition—the ratio of formal vs. informal text dictates performance on normative vs. colloquial tasks. Models learn statistical signatures of training data, with informal content improving slang comprehension but degrading normative grammar judgments. This assumes evaluation benchmarks accurately reflect desired proficiency. Evidence shows QFrCoLA performance worsened due to 40% informal text teaching models to accept "incorrect" phrasing. Corpus composition must match target use case.

## Foundational Learning

**Catastrophic Forgetting**
Why needed: Core risk of CPT is losing original capabilities while learning new dialect. Quick check: Why does freezing base weights and only training LoRA adapters help prevent "forgetting" Standard French?

**Perplexity**
Why needed: Primary metric to determine if model successfully learns Québec French probability distribution. Quick check: Does lower perplexity on held-out dialect corpus guarantee better performance on downstream semantic tasks?

**Tokenization Efficiency**
Why needed: Dialects contain unique words/spellings; poor tokenization shrinks effective context window and learning becomes harder. Quick check: How might CroissantLLM tokenizer handle Québec-specific idioms differently than generic English-centric tokenizer?

## Architecture Onboarding

**Component map:** Base Models (CroissantLLM, Llama-3.2-1B, Llama-3.1-8B) -> LoRA Adapters (attention q,k,v,o and FFN up,gate,down) -> Optimization (AdamW + cosine decay + gradient checkpointing)

**Critical path:** 1) Data Curation: aggregate formal and informal sources 2) Pre-processing: strip HTML/tags but preserve orthographic errors 3) Training: run CPT for 3-6 epochs with low learning rate 4) Evaluation: benchmark on COLE suite

**Design tradeoffs:** Size vs. Retention: 1B models struggled to balance adaptation/retention; 8B succeeded. Use largest feasible base model. Normative vs. Descriptive: Training on real internet text improves fluency but degrades normative grammar scores. Cleaning data too aggressively removes dialect signal.

**Failure signatures:** Mode Collapse: generates only Québec slang even when prompted in Standard French. Regression: performance on general French tasks drops significantly, indicating catastrophic forgetting. Stagnation: perplexity fails to drop, indicating learning rate too low or LoRA rank too restrictive.

**First 3 experiments:** 1) Overfit Test: train on small clean formal Québec text to isolate impact on QFrCoLA scores 2) Rank Ablation: compare r=16 vs r=64 on 8B model to recover Fr-BoolQ performance 3) Data Mixing: re-run CPT with 50/50 mix of Québec and General French text to measure trade-off

## Open Questions the Paper Calls Out

**Open Question 1**
How does corpus composition (formal vs. informal sources) affect performance on normative grammar tasks versus dialectal fluency? Based on finding that 40% informal data hurt QFrCoLA performance while improving informal tasks. Unresolved because optimal ratio for different use cases unknown. Resolved by ablation experiments training separate models on formal-only vs informal-only vs mixed corpora.

**Open Question 2**
What is the optimal data mixing strategy for balancing dialect adaptation and prestige language retention? Based on finding that single-pass training without domain interleaving showed inconsistent retention. Unresolved because optimal mixing ratio unknown. Resolved by systematic experiments varying ratio of Québec French to prestige French data during CPT.

**Open Question 3**
Can selective parameter freezing or regularization techniques enhance language retention during CPT? Based on finding that Llama-3.2-1B showed catastrophic forgetting on general French tasks. Unresolved because smaller models need explicit retention mechanisms beyond LoRA alone. Resolved by comparing LoRA-only CPT against methods with selective layer freezing.

## Limitations
- Corpus representativeness and availability issues with restricted sources (Le Soleil, Facebook/YouTube comments)
- Generalizability to other dialects with greater linguistic distance from Standard French unclear
- Evaluation scope limited to 8 COLE tasks may not comprehensively capture dialect proficiency

## Confidence

**High Confidence:**
- CPT with LoRA can adapt LLMs to Québec French dialect with parameter-efficient updates (<1% parameters)
- Larger models (8B) show better balance between dialect adaptation and retention
- Corpus composition significantly impacts performance on different task types

**Medium Confidence:**
- LoRA rank of 16 is optimal for this adaptation task
- 6 epochs provides optimal trade-off between adaptation and forgetting
- Compute-efficient CPT is viable for low-resource dialect adaptation

**Low Confidence:**
- Specific Québec French corpus composition is optimal for all use cases
- Results generalize to dialects with greater linguistic distance from Standard French
- Evaluation methodology comprehensively measures dialect proficiency

## Next Checks

1. **Corpus Composition Ablation Study:** Train identical models with varying ratios of formal to informal Québec French text (0%, 25%, 50%, 75%, 100% informal) and measure performance on normative vs. colloquial tasks to quantify trade-off and identify optimal composition.

2. **Cross-Dialect Generalization Experiment:** Apply same CPT+LoRA methodology to different low-resource dialect (Swiss French or Louisiana French) using same model sizes and training budget, comparing adaptation curves, final performance, and forgetting rates to identify predictive factors.

3. **Rank Sensitivity Analysis:** Systematically vary LoRA rank (r=8, 16, 32, 64) across all three model sizes while keeping other hyperparameters constant, measuring final task performance and training dynamics to determine effects on adaptation speed and catastrophic forgetting susceptibility.