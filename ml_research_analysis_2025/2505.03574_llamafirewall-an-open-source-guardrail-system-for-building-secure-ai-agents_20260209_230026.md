---
ver: rpa2
title: 'LlamaFirewall: An open source guardrail system for building secure AI agents'
arxiv_id: '2505.03574'
source_url: https://arxiv.org/abs/2505.03574
tags:
- prompt
- agent
- promptguard
- injection
- alignmentcheck
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LlamaFirewall is an open-source security guardrail system designed
  to protect LLM agents from emerging threats like prompt injection, agent misalignment,
  and insecure code generation. It uses a layered approach with three main components:
  PromptGuard 2 for detecting jailbreak attempts, AlignmentCheck for monitoring agent
  reasoning to catch goal hijacking, and CodeShield for real-time static analysis
  of generated code.'
---

# LlamaFirewall: An open source guardrail system for building secure AI agents
## Quick Facts
- arXiv ID: 2505.03574
- Source URL: https://arxiv.org/abs/2505.03574
- Reference count: 25
- LlamaFirewall achieves >90% reduction in attack success while maintaining high task utility through layered defense system

## Executive Summary
LlamaFirewall is an open-source security guardrail system designed to protect LLM agents from emerging threats like prompt injection, agent misalignment, and insecure code generation. It employs a layered approach with three main components: PromptGuard 2 for detecting jailbreak attempts, AlignmentCheck for monitoring agent reasoning to catch goal hijacking, and CodeShield for real-time static analysis of generated code. The system is built to be modular, customizable, and deployable across various agent architectures while maintaining strong security performance.

The research addresses critical security gaps in autonomous AI agents by providing defense mechanisms against increasingly sophisticated attack vectors. Through comprehensive evaluation across multiple threat scenarios, LlamaFirewall demonstrates the ability to significantly reduce successful attacks while preserving agent functionality. The open-source nature and practical design make it accessible for developers building secure AI systems in production environments.

## Method Summary
LlamaFirewall implements a three-layer defense system targeting the most critical attack vectors against LLM agents. The first layer, PromptGuard 2, uses advanced detection models to identify and block direct jailbreak attempts before they reach the agent. The second layer, AlignmentCheck, monitors the agent's reasoning process to detect when it deviates from intended goals due to indirect prompt injection or goal hijacking attacks. The third layer, CodeShield, performs real-time static analysis on any code generated by the agent to identify security vulnerabilities before execution.

The system was evaluated through comprehensive testing across multiple attack scenarios, including direct jailbreak attempts, indirect prompt injection through document injection, and code generation vulnerabilities. Performance metrics were measured for both security effectiveness (attack success rate reduction) and utility preservation (task completion rates). The modular architecture allows individual components to be deployed independently or in combination based on specific security requirements and deployment constraints.

## Key Results
- PromptGuard 2 achieves over 97% recall at 1% false positive rate for direct jailbreak detection
- AlignmentCheck reduces indirect attack success rates by 84% with minimal impact on task utility
- CodeShield identifies insecure code with 96% precision and 79% recall
- Combined system reduces overall attack success by over 90% while maintaining high task completion rates

## Why This Works (Mechanism)
LlamaFirewall's effectiveness stems from its multi-layered defense strategy that addresses different attack vectors at various stages of the agent's decision-making process. Each component targets a specific vulnerability class while maintaining low false positive rates through specialized detection mechanisms. The system's modular design allows for independent optimization of each guardrail, enabling trade-offs between security strength and computational overhead based on deployment requirements.

The prompt injection defenses work by detecting malicious inputs before they can influence the agent's behavior, while the alignment monitoring catches attempts to hijack the agent's goals during reasoning. The code analysis component provides runtime protection against vulnerabilities in generated code that could be exploited by attackers. This comprehensive coverage across the agent's workflow creates multiple barriers that attackers must overcome, significantly reducing the probability of successful exploitation.

## Foundational Learning
- **Prompt injection detection**: Understanding how malicious inputs can manipulate LLM behavior is crucial for building effective defenses against jailbreak attempts
- **Agent alignment monitoring**: Recognizing that agents can be hijacked through indirect prompt injection requires mechanisms to verify goal consistency during reasoning
- **Static code analysis for security**: Generated code can contain vulnerabilities that attackers exploit, necessitating automated security review tools
- **Defense-in-depth strategy**: Single-point defenses are vulnerable to sophisticated attacks, requiring multiple complementary security layers
- **Performance-utility trade-offs**: Security mechanisms must balance protection strength against maintaining acceptable task completion rates
- **Modular security architecture**: Independent guardrail components allow for flexible deployment and independent optimization based on specific requirements

## Architecture Onboarding
**Component map**: User Input -> PromptGuard 2 -> Agent Core -> AlignmentCheck -> CodeShield -> Code Execution

**Critical path**: The primary security workflow flows through all three guardrail components in sequence, with each layer providing protection against different attack vectors before allowing the agent to proceed to the next stage.

**Design tradeoffs**: The system balances security effectiveness against computational overhead and latency, with modular components allowing deployment of only necessary layers based on threat model and performance requirements.

**Failure signatures**: Guardrail bypasses typically manifest as either false negatives (missed attacks) or false positives (legitimate requests blocked), with each component having distinct failure patterns based on its detection methodology.

**First experiments**: 1) Test PromptGuard 2 with standard jailbreak datasets to verify detection rates, 2) Evaluate AlignmentCheck on agents performing multi-step tasks with injected goal deviations, 3) Assess CodeShield's vulnerability detection across common insecure coding patterns in generated code.

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluations focus on component-level performance rather than comprehensive end-to-end agent security in realistic deployment scenarios
- Utility preservation claims rely on limited task diversity with only 100-150 samples per task type
- Code generation security assessment was evaluated against curated vulnerability datasets, with untested effectiveness against zero-day attack patterns

## Confidence
**High confidence**: Individual component performance metrics (PromptGuard 2 recall/precision, CodeShield vulnerability detection rates)
**Medium confidence**: End-to-end system effectiveness claims, utility preservation across diverse use cases
**Low confidence**: Security guarantees against sophisticated, multi-vector attacks in production environments

## Next Checks
1. Conduct end-to-end red teaming exercises using professional security researchers to test system resilience against combined prompt injection, goal hijacking, and code generation attacks in realistic multi-step agent workflows.

2. Evaluate performance degradation and security effectiveness across 50+ diverse task types including complex reasoning chains, multi-agent coordination scenarios, and long-horizon planning tasks.

3. Test system behavior with adversarial input generation techniques (e.g., gradient-based attacks, black-box optimization) to identify potential bypasses not captured by current evaluation datasets.