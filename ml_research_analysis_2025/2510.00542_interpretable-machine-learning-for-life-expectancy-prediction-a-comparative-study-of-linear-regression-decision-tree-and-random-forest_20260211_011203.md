---
ver: rpa2
title: 'Interpretable Machine Learning for Life Expectancy Prediction: A Comparative
  Study of Linear Regression, Decision Tree, and Random Forest'
arxiv_id: '2510.00542'
source_url: https://arxiv.org/abs/2510.00542
tags:
- life
- expectancy
- health
- were
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated three machine learning models\u2014Linear\
  \ Regression (LR), Regression Decision Tree (RDT), and Random Forest (RF)\u2014\
  for predicting life expectancy using a WHO and UN dataset of 941 samples across\
  \ 15 features. All models underwent extensive preprocessing, including missing value\
  \ handling, outlier correction, and categorical encoding."
---

# Interpretable Machine Learning for Life Expectancy Prediction: A Comparative Study of Linear Regression, Decision Tree, and Random Forest

## Quick Facts
- arXiv ID: 2510.00542
- Source URL: https://arxiv.org/abs/2510.00542
- Reference count: 0
- Primary result: Random Forest achieved R² = 0.9423, MAE = 1.48, RMSE = 2.22, outperforming Linear Regression and Decision Tree

## Executive Summary
This study evaluates three machine learning models—Linear Regression (LR), Regression Decision Tree (RDT), and Random Forest (RF)—for predicting life expectancy using a WHO and UN dataset. The models were trained on 941 samples across 15 features after extensive preprocessing, including missing value handling and outlier correction. RF achieved the highest predictive accuracy (R² = 0.9423) while maintaining interpretability through feature importance analysis, identifying immunization rates and demographic attributes as critical drivers. The results demonstrate the effectiveness of ensemble methods for handling complex, non-linear relationships in public health data while maintaining transparency for policy applications.

## Method Summary
The study used WHO/UN health and economic data (2,938 rows × 22 columns) reduced to 941 samples × 15 features after preprocessing. Models were trained on a 70:30 train-test split using scikit-learn: Linear Regression with no CV, Decision Tree with 5-fold GridSearchCV (max_depth=15, min_samples_leaf=5, ccp_alpha=0.01), and Random Forest with 5-fold GridSearchCV (n_estimators=200, bootstrap=False). Max Abs Scaler was applied for normalization. Feature importance and p-values served as interpretability metrics, with RF achieving R² = 0.9423, MAE = 1.48, RMSE = 2.22.

## Key Results
- Random Forest achieved highest accuracy: R² = 0.9423, MAE = 1.48, RMSE = 2.22
- Random Forest outperformed Linear Regression (R² = 0.73, MAE = 3.74, RMSE = 4.82) and Decision Tree (R² = 0.90, MAE = 2.09, RMSE = 2.96)
- Feature importance analysis identified immunization rates (diphtheria, measles) and demographic factors (HIV/AIDS, adult mortality) as critical drivers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Ensemble averaging via Random Forest captures non-linear relationships better than single estimators
- **Mechanism**: RF aggregates predictions from multiple decision trees (200 in this study), each trained on data subsets. The ensemble reduces variance by averaging errors, provided the trees are decorrelated
- **Core assumption**: The relationship between demographic factors and life expectancy is non-linear with complex interactions
- **Evidence anchors**: RF achieved R² = 0.9423 vs LR's R² = 0.73; comparative studies show ensemble methods outperforming linear baselines in health contexts
- **Break condition**: If relationships are strictly linear or signal-to-noise ratio is extremely low, ensemble offers diminishing returns

### Mechanism 2
- **Claim**: Feature importance metrics serve as viable proxy for identifying key policy drivers
- **Mechanism**: By calculating total reduction in impurity attributed to each feature, the model ranks variables like immunization rates and mortality stats
- **Core assumption**: Features with high importance scores have monotonic or systematic impact on the target
- **Evidence anchors**: Interpretability was prioritized, identifying immunization and HIV/AIDS as critical drivers; feature importance analysis highlighted diphtheria and measles
- **Break condition**: If features are highly collinear, importance scores may be arbitrarily distributed among them

### Mechanism 3
- **Claim**: Rigorous outlier removal and missing data handling are prerequisites for stable life expectancy modeling
- **Mechanism**: The study applied hard thresholds (excluding features with >5% missing data, capping values at biological limits)
- **Core assumption**: Missing values are random or non-informative, and outliers represent data entry errors
- **Evidence anchors**: Dataset reduced to 941 samples after applying consistency checks; mentions "extensive preprocessing" as foundation for results
- **Break condition**: If missingness is informative (e.g., low-income countries failing to report GDP), dropping these features systematically biases the model

## Foundational Learning

- **Concept**: Bias-Variance Tradeoff
  - **Why needed here**: To understand why Random Forest (low variance due to bagging) outperformed the single Decision Tree (high variance) and Linear Regression (high bias)
  - **Quick check question**: If the Decision Tree had a max_depth of 3 instead of 15, would its variance likely increase or decrease?

- **Concept**: One-Hot Encoding
  - **Why needed here**: The study converted the categorical "Status" (Developed/Developing) into numerical features using `get_dummies`
  - **Quick check question**: Why is it necessary to drop one of the dummy variable columns to avoid perfect multicollinearity in Linear Regression?

- **Concept**: $R^2$ (Coefficient of Determination)
  - **Why needed here**: This is the primary metric cited (0.9423 for RF) to justify model selection
  - **Quick check question**: If a model predicts the mean life expectancy for every input regardless of features, what is its $R^2$ score?

## Architecture Onboarding

- **Component map**: WHO/UN CSV (2,938 rows × 22 cols) → Preprocessing (column rename, drop >5% missing, threshold filtering) → `get_dummies` (One-Hot) → Latitude/Longitude mapping → Scaling → Scikit-learn models → Evaluation

- **Critical path**: The data cleaning step (reducing 2,938 rows to 941 valid samples) is the most fragile part of the architecture. A new pipeline must strictly replicate the "logical inconsistencies" logic to match benchmarks

- **Design tradeoffs**:
  - **Accuracy vs. Completeness**: Sacrificed feature breadth (dropping GDP, Population) for data density (no missing values)
  - **Interpretability vs. Complexity**: Linear Regression offers explicit coefficients but low accuracy; RF offers high accuracy but relies on aggregate feature importance

- **Failure signatures**:
  - **High MAE in LR**: Indicates data is non-linear; switch to tree-based models
  - **Large gap between Train/Test scores in RDT**: Indicates overfitting; requires pruning or depth reduction
  - **Feature Importance saturation**: If all features have similar low importance, check for data leakage or scaling issues

- **First 3 experiments**:
  1. **Baseline Reproduction**: Implement RF with exact 941-sample dataset and `max_depth=15`, `n_estimators=200` to verify R² > 0.94 claim
  2. **Sensitivity Analysis**: Re-introduce one dropped feature (e.g., GDP) using imputation to measure performance delta
  3. **Collinearity Filter**: Remove one of the highly correlated features (e.g., `under_five_deaths` vs `infant_deaths`, r=0.99) and retrain RF

## Open Questions the Paper Calls Out

- **Question**: Would advanced imputation strategies (e.g., MICE) retaining high-missingness variables like GDP and schooling improve predictive accuracy over the current listwise deletion approach?
  - **Basis**: Authors identify exclusion of features with >5% missing values as a limitation and propose "advanced imputation strategies" for future work
  - **Why unresolved**: Study currently sacrifices potentially informative variables to ensure data completeness
  - **What evidence would resolve it**: Comparative benchmark where models are trained on dataset using advanced imputation versus current reduced dataset

- **Question**: Can deep learning architectures (e.g., neural networks) outperform the Random Forest model (R² = 0.9423) while maintaining sufficient interpretability for policy use?
  - **Basis**: Paper lists "alternative algorithms (e.g., neural networks)" as a direction for future research
  - **Why unresolved**: Current study restricted to linear and tree-based models
  - **What evidence would resolve it**: Benchmarking neural networks on same dataset with explainability metrics (e.g., SHAP values)

- **Question**: Do the identified key drivers (immunization, HIV/AIDS) remain the primary predictors of life expectancy when applied to post-2015 data containing recent pandemic and climate effects?
  - **Basis**: Authors note dataset (2000–2015) may not capture "recent global shifts" or "emerging challenges"
  - **Why unresolved**: Temporal constraints mean model's relevance to contemporary global health challenges is unverified
  - **What evidence would resolve it**: Re-training models on updated WHO/UN data spanning 2015–2024 and analyzing shifts in feature importance

## Limitations
- High R² score (0.9423) warrants cautious interpretation due to potential data leakage through extensive preprocessing
- Exclusive focus on RF interpretability through feature importance may oversimplify complex interactions
- Absence of uncertainty quantification (e.g., confidence intervals, bootstrapping) limits robustness assessment

## Confidence
- **High**: Comparative ranking of models (RF > RDT > LR) given consistent improvements across all metrics
- **Medium**: Absolute R² values due to preprocessing opacity and unknown latitude/longitude mapping data
- **Low**: Generalizability of feature importance rankings given potential collinearity issues among demographic variables

## Next Checks
1. Verify the exact row count (941 samples) after each preprocessing step to ensure proper implementation of inconsistency filters
2. Confirm all 15+ features are present and scaled correctly, particularly status encoding and country coordinates
3. Test model performance with one additional feature re-introduced via imputation to measure sensitivity to the preprocessing choices