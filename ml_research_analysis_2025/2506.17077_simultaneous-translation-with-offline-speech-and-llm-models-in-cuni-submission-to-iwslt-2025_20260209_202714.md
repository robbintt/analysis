---
ver: rpa2
title: Simultaneous Translation with Offline Speech and LLM Models in CUNI Submission
  to IWSLT 2025
arxiv_id: '2506.17077'
source_url: https://arxiv.org/abs/2506.17077
tags:
- translation
- latency
- context
- simultaneous
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents the CUNI submission to IWSLT 2025 Simultaneous
  Speech Translation Task, covering all four language pairs using a direct or cascade
  approach. The backbone is the Whisper offline speech model used with the AlignAtt
  simultaneous policy, improved by beam search, in-domain terminology prompting, and
  context.
---

# Simultaneous Translation with Offline Speech and LLM Models in CUNI Submission to IWSLT 2025

## Quick Facts
- arXiv ID: 2506.17077
- Source URL: https://arxiv.org/abs/2506.17077
- Reference count: 9
- Key outcome: Direct and cascade approaches using Whisper + AlignAtt and Whisper + EuroLLM improved BLEU by 2-22 points across four language pairs

## Executive Summary
This work presents the CUNI submission to IWSLT 2025 Simultaneous Speech Translation Task, covering all four language pairs using a direct or cascade approach. The backbone is the Whisper offline speech model used with the AlignAtt simultaneous policy, improved by beam search, in-domain terminology prompting, and context. For translations into German, Chinese, and Japanese, a cascade approach is used with Whisper for ASR and EuroLLM for translation. Results show improvements of 2 BLEU points on Czech→English and 13-22 BLEU points on English→German, Chinese, and Japanese compared to the baseline. The paper also proposes a new enhanced method for measuring ASR latency using Continuous Levenshtein Alignment. The systems are implemented in SimulStreaming and are available for research and realistic use cases.

## Method Summary
The CUNI system uses Whisper large-v3 as the core speech model, operating in simultaneous mode via the AlignAtt policy for attention-based output commitment. For Czech→English, a direct approach applies beam search, static prompting, and buffer management. For English→German, Chinese, and Japanese, a cascade architecture first transcribes with Whisper then translates with EuroLLM-9B-Instruct using LocalAgreement for text-to-text streaming. Buffer trimming strategies prevent hallucinations in LLM-based translation, with sentence-level trimming for similar syntax pairs and segment-level trimming for divergent pairs. The system is implemented in SimulStreaming with VAD-based chunking and forced decoding for continuity.

## Key Results
- Direct Cs→En improved by 2 BLEU points using AlignAtt with beam search and prompting
- Cascade En→De improved by 13 BLEU points; En→Zh by 22 BLEU; En→Ja by 22 BLEU
- SLAAL latency measurements show direct approach meets "high latency" requirement (<4000ms)
- Buffer trimming strategies successfully prevented LLM hallucinations in distant language pairs

## Why This Works (Mechanism)

### Mechanism 1: AlignAtt Attention-Based Simultaneous Policy
- Claim: Offline Whisper can operate in simultaneous mode by monitoring decoder attention to determine output commitment points.
- Mechanism: The AlignAtt policy tracks which source audio frame receives the most attention during decoding. When the most-attended frame falls behind a configurable threshold relative to the audio buffer end, the policy permits continued generation; when attention advances beyond the threshold, the partial hypothesis is committed as stable output. This leverages the natural source-target alignment learned during Whisper's training.
- Core assumption: Decoder attention weights correlate with meaningful semantic alignment between source audio and generated text.
- Evidence anchors:
  - [Section 2]: "AlignAtt...detects where to stop generating the partial target, which is when the most attended source frame by the decoder is behind a threshold."
  - [Section 3, Step 4]: "Decode until the top beam hypothesis is attended behind the threshold."
  - [corpus]: CMU's IWSLT 2025 system uses chunkwise causal encoders for similar simultaneous streaming; AlignAtt remains state-of-the-art per Papi et al. (2023).
- Break condition: If attention patterns become decoupled from semantic alignment (e.g., on out-of-domain audio or heavily accented speech), the policy may commit too early or delay unnecessarily.

### Mechanism 2: LocalAgreement for Attention-Inaccessible Models
- Claim: When model attention weights are unavailable (e.g., CTranslate2-quantized models), prefix agreement between consecutive inference passes provides a viable simultaneity criterion.
- Mechanism: For each incoming source chunk, generate a new target hypothesis. Compare it with the previous hypothesis; emit their longest common prefix as confirmed output. The remaining suffix is held for potential confirmation in the next update. This exploits the observation that stable translations are more likely correct.
- Core assumption: Translation stability across incremental updates correlates with output quality.
- Evidence anchors:
  - [Section 2]: "LocalAgreement...emits their longest common prefix as confirmed and uses it in forced decoding of the latter chunks."
  - [Section 4, Step 5]: "The new target prefix is compared to the one from the previous update, and their longest common prefix (LocalAgreement policy) is considered as a newly confirmed hypothesis."
  - [corpus]: MLLP-VRAIN's IWSLT 2025 cascade system also applies LocalAgreement for text-to-text streaming translation.
- Break condition: If source is highly ambiguous or model is underconfident, consecutive outputs may share little prefix, causing output stagnation.

### Mechanism 3: Cascade with Buffer Trimming for Hallucination Control
- Claim: Cascade ASR→MT with explicit buffer management reduces hallucinations common in direct speech translation for distant language pairs.
- Mechanism: Whisper performs simultaneous ASR; punctuated transcript feeds EuroLLM for MT. Two trimming strategies prevent context overflow: (a) sentence-level trimming for similar syntax pairs (En→De), (b) segment-level trimming for divergent pairs (En→Zh, Ja) where sentence counts don't align. Trimming prevents the model from generating content ungrounded in recent source.
- Core assumption: Hallucinations in LLM-based MT increase with excessive context; bounded context grounds output in current source.
- Evidence anchors:
  - [Section 4]: "We observed that the model tends to hallucinate with larger context. Therefore, we trim the source-target buffer if it has more tokens than MaxContextLength."
  - [Section 6.3]: "We observed many hallucinations with English-to-Chinese and Japanese with the buffer trimming strategy Sentences...However, there were no hallucinations when we applied the Segments strategy instead."
  - [corpus]: Oregon State's BeaverTalk uses similar cascade with Whisper + MT, reporting comparable quality-latency tradeoffs.
- Break condition: If buffer is trimmed too aggressively, discourse coherence degrades; if too permissive, hallucinations recur.

## Foundational Learning

- Concept: **Simultaneous Translation Policies (Read/Write boundaries)**
  - Why needed here: The entire system hinges on *when* to read more input vs. *when* to emit output. AlignAtt and LocalAgreement are two distinct answers to this question.
  - Quick check question: Given a partial source and partial target, what signal does AlignAtt use to decide whether to continue decoding versus committing output?

- Concept: **Voice Activity Detection (VAD) for Chunking**
  - Why needed here: Unbounded speech must be segmented before Whisper processing. Silero VAD determines chunk boundaries based on silence detection.
  - Quick check question: What happens if VAD minimum chunk size is set too small (e.g., 0.1s) versus too large (e.g., 5s) for simultaneous translation latency?

- Concept: **Forced Decoding with Target Prefix**
  - Why needed here: To maintain continuity across chunks, the system forces the decoder to begin with the previously confirmed output before generating new content.
  - Quick check question: Why is forced decoding necessary for unbounded speech but not for pre-segmented utterances?

## Architecture Onboarding

- Component map:
Audio Input → Silero VAD (chunking) → Whisper large-v3 (encoder + decoder)
                                        ↓
                                  AlignAtt policy
                                        ↓
                            [Direct: Cs→En output]
                                        ↓
                            [Cascade: En ASR transcript]
                                        ↓
                            EuroLLM-9B-Instruct + LocalAgreement
                                        ↓
                            En→{De, Zh, Ja} output

- Critical path:
  1. **VAD parameters** (MinChunkSize, voice padding) directly control latency floor
  2. **Frames threshold** in AlignAtt determines quality-latency tradeoff
  3. **MaxContextLength** in EuroLLM balances coherence vs. hallucination risk

- Design tradeoffs:
  - Beam search: +1 ChrF but slight latency increase (Table 2)
  - BufferLength 30s: Maximum quality but higher memory footprint
  - Sentence vs. Segment trimming: Sentence assumes 1:1 source-target correspondence; Segment is safer for syntactically divergent pairs

- Failure signatures:
  - Output never commits → Frames threshold too high or attention not aligning
  - Repetitive hallucinations → MaxContextLength too large; reduce to 300-500 tokens
  - Excessive latency → MinChunkSize too large; reduce to 1-2 words for text, 1-1.5s for audio
  - Missing domain terms → Prompt not static; context overwrites prompt

- First 3 experiments:
  1. **Baseline latency sweep**: Run Whisper+AlignAtt on a 10-minute audio file with Frames ∈ {4, 15, 25, 35}, measure SLAAL and ChrF. Plot quality-latency curve.
  2. **Prompt injection test**: Compare translation of domain-specific audio (e.g., parliamentary proceedings) with/without static prompt containing key terminology. Measure ChrF delta.
  3. **Trimming strategy ablation**: For En→Zh, compare sentence-level vs. segment-level buffer trimming on a 5-minute dev set. Count hallucination instances and measure BLEU.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the cascaded Whisper-EuroLLM architecture be optimized to satisfy the low-latency constraint (<2000ms SLAAL) for translation into German, Chinese, and Japanese?
- Basis: [inferred] The authors report that the lowest achievable latency for the cascade system was 2471ms SLAAL, making it incompatible with the task's "low-latency" regime requirements.
- Why unresolved: The inherent processing time of the cascade (ASR + MT) and the LocalAgreement policy currently prevent the system from reaching the required speed for real-time low-latency scenarios.
- What evidence would resolve it: A system modification or policy adjustment that achieves a latency score below 2000ms SLAAL on the target language pairs without significant loss in translation quality.

### Open Question 2
- Question: How can context management strategies be improved to prevent hallucinations in Large Language Model (LLM) based simultaneous translation?
- Basis: [inferred] The authors observed that EuroLLM tends to hallucinate with larger context windows and that standard sentence-based trimming failed for English→Chinese/Japanese due to structural mismatches.
- Why unresolved: The current solution (a "Segments" trimming strategy) avoids hallucinations but limits the effective context, suggesting a trade-off between context awareness and stability that is not yet optimized.
- What evidence would resolve it: The development of a context-handling mechanism that allows the LLM to utilize long-range dependencies (e.g., >500 tokens) without generating repetitive or non-translation text.

### Open Question 3
- Question: What specific prompt characteristics determine success when injecting in-domain terminology into Whisper for simultaneous translation?
- Basis: [inferred] The authors noted high sensitivity to prompting, stating that "half of the prompts increased the performance... while the other half decreased it."
- Why unresolved: The interaction between the prompt text and the model's decoding logic is unpredictable, leading to inconsistent utility of this adaptation method.
- What evidence would resolve it: A systematic analysis correlating prompt features (e.g., length, specificity, syntax) with ChrF/BLEU score variations to establish a deterministic prompting strategy.

## Limitations
- AlignAtt policy effectiveness depends on attention patterns correlating with semantic alignment, which may fail on out-of-domain audio
- Cascade approach introduces additional latency and complexity with empirically-determined buffer trimming strategies
- SLAAL metric has not been validated against human latency judgments across diverse domains

## Confidence
- **High Confidence**: The baseline improvements (2 BLEU for Cs→En, 13-22 BLEU for En→{De, Zh, Ja}) are well-supported by empirical results on standard IWSLT benchmarks
- **Medium Confidence**: The mechanisms for attention-based simultaneity (AlignAtt) and stability-based simultaneity (LocalAgreement) are theoretically sound and grounded in published work
- **Low Confidence**: The exact implementation details of EuroLLM prompting and buffer management are underspecified in the paper

## Next Checks
1. **Attention Alignment Validation**: Conduct a qualitative analysis of Whisper's attention patterns on out-of-domain audio to verify that the AlignAtt policy's assumption holds beyond parliamentary proceedings
2. **Cross-Domain Prompting Study**: Systematically evaluate the impact of different prompt formulations on EuroLLM's translation quality and hallucination propensity across multiple domains
3. **SLAAL Correlation Analysis**: Compare SLAAL scores against human latency judgments on a diverse set of simultaneous translation samples to establish the metric's validity