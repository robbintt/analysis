---
ver: rpa2
title: 'SINAI at eRisk@CLEF 2023: Approaching Early Detection of Gambling with Natural
  Language Processing'
arxiv_id: '2509.14797'
source_url: https://arxiv.org/abs/2509.14797
tags:
- task
- data
- were
- gambling
- early
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The SINAI team participated in Task 2 of the eRisk@CLEF 2023 lab,
  focusing on the early detection of signs of pathological gambling from social media
  posts. They employed pre-trained Transformer models, particularly RoBERTa-Large
  and XLM-RoBERTa-Large, combined with extensive preprocessing and data balancing
  techniques.
---

# SINAI at eRisk@CLEF 2023: Approaching Early Detection of Gambling with Natural Language Processing

## Quick Facts
- **arXiv ID**: 2509.14797
- **Source URL**: https://arxiv.org/abs/2509.14797
- **Reference count**: 8
- **Primary result**: 7th place out of 49 teams in eRisk@CLEF 2023 Task 2, achieving perfect recall (1.000) and top ERDE scores

## Executive Summary
The SINAI team participated in Task 2 of the eRisk@CLEF 2023 lab, focusing on early detection of pathological gambling signs from social media posts. They employed pre-trained Transformer models (RoBERTa-Large and XLM-RoBERTa-Large) with extensive preprocessing and data balancing techniques. Their approach integrated these models with LSTM architecture to process sequential posts effectively. The team achieved strong performance with perfect recall scores and top rankings in early detection metrics (ERDE), demonstrating the effectiveness of transformer-based approaches for early identification of potential gamblers from textual data.

## Method Summary
The SINAI team used pre-trained Transformer models (RoBERTa-Large and XLM-RoBERTa-Large) combined with a feed-forward neural network classifier. They implemented comprehensive preprocessing including HTML entity handling, user/URL/email masking, emoji conversion to text, and punctuation normalization. Posts were concatenated per user and split into sub-subjects if exceeding 500 words. The team employed Optuna for hyperparameter optimization and balanced the highly imbalanced dataset by selecting first 10-15 negative sub-subjects. A novel RoBERTa+LSTM variant was also tested to leverage sequential information from posts. The models were trained using AdamW optimizer with a learning rate of 1e-5 and evaluated using ERDE metrics, recall, F1, and ranking-based metrics.

## Key Results
- Ranked 7th out of 49 submissions in the eRisk@CLEF 2023 competition
- Achieved perfect recall score of 1.000 across all evaluation rounds
- Top values in early detection metrics (ERDE-5, ERDE-50) demonstrating strong capability for early identification
- Highest NDCG and P@k scores in ranking-based evaluation, though LSTM-integrated model showed slightly lower performance

## Why This Works (Mechanism)
The approach works by leveraging pre-trained transformer models that have learned rich semantic representations from large-scale text data, which are then fine-tuned on the specific task of gambling detection. The comprehensive preprocessing normalizes text data to reduce noise and improve model generalization. The sequential processing capability, especially in the LSTM-integrated variant, allows the model to capture temporal patterns and behavioral progression in social media posts that indicate developing gambling problems.

## Foundational Learning

**Text Preprocessing for NLP** - Why needed: Raw social media text contains noise (URLs, emojis, special characters) that can confuse models. Quick check: Verify preprocessing preserves gambling-related terms while normalizing noise.

**Transformer Model Fine-tuning** - Why needed: Pre-trained models like RoBERTa have learned general language patterns but need task-specific adaptation. Quick check: Monitor validation loss during fine-tuning to ensure proper adaptation.

**Class Imbalance Handling** - Why needed: With only 103 positive users vs. ~1M negative posts, models tend to bias toward the majority class. Quick check: Verify class distribution in training batches.

## Architecture Onboarding

**Component Map**: Social Media Posts -> Preprocessing Pipeline -> RoBERTa/XLM-RoBERTa -> FFNN Classifier -> Binary Classification/Risk Score

**Critical Path**: The critical path for early detection is: Post Ingestion → Preprocessing → Transformer Encoding → Sequential Decision Logic → Early Detection Trigger. The system must make classification decisions at each writing step while maintaining low latency.

**Design Tradeoffs**: The team balanced between using pure transformer models (simpler, faster) versus transformer+LSTM integration (more complex, potentially better at capturing temporal patterns). Computational resources influenced this choice, with the LSTM variant showing lower performance due to hardware constraints.

**Failure Signatures**: High ERDE scores indicate late detection; perfect recall with low precision suggests the model flags too many false positives; OOM errors during training indicate insufficient computational resources for the LSTM variant.

**First Experiments**:
1. Verify preprocessing pipeline preserves gambling-related terminology while normalizing noise
2. Train baseline RoBERTa-Large model with simple FFNN classifier to establish performance floor
3. Test sequential decision logic by simulating round-by-round processing on validation set

## Open Questions the Paper Calls Out

**Open Question 1**: How can LSTM architectures be effectively optimized with transformer models for early detection in sequential text processing? The authors plan to test different training strategies with transformer+LSTM integration, as the current implementation showed lower performance due to resource limitations rather than architectural issues.

**Open Question 2**: What data balancing techniques would be most effective for this highly imbalanced dataset? The authors expect to continue working on data processing and addressing the imbalance present in the data, suggesting current balancing methods may be suboptimal.

**Open Question 3**: What is the optimal number of historical posts to consider for early detection of gambling signs? The current choice of 50 posts is based on previous work rather than systematic evaluation, leaving room for optimization.

## Limitations
- Computational intensity of the RoBERTa+LSTM architecture led to lower performance due to hardware constraints
- Limited dataset diversity with only 103 positive users may constrain model generalization
- Unspecified Optuna hyperparameter optimization details make configuration replication difficult
- Class balancing approach of selecting first 10-15 negative sub-subjects may introduce selection bias

## Confidence

**High Confidence**: The use of pre-trained Transformer models with standard fine-tuning procedures and the reported competition ranking (7th out of 49) are well-documented and verifiable.

**Medium Confidence**: The effectiveness of the LSTM integration for sequential post processing showed slightly lower performance than pure Transformer models, suggesting implementation issues rather than fundamental architectural problems.

**Low Confidence**: The ERDE metrics depend heavily on sequential decision logic implementation details that are not fully specified, making it difficult to assess the reliability of early detection claims.

## Next Checks
1. Reimplement the RoBERTa-Large baseline with specified preprocessing pipeline and verify recall scores approach the reported 1.000 value on eRisk test set
2. Systematically compare RoBERTa-Large against RoBERTa+LSTM variant using controlled computational resources to isolate architecture vs. hardware effects
3. Conduct ablation studies on preprocessing steps (emoji conversion, punctuation normalization) to quantify their individual impact on detection performance