---
ver: rpa2
title: Schema as Parameterized Tools for Universal Information Extraction
arxiv_id: '2506.01276'
source_url: https://arxiv.org/abs/2506.01276
tags:
- schema
- extraction
- schemas
- generation
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Schema as Parameterized Tools (SPT), a unified
  adaptive text-to-structure generation framework that reimagines large language models'
  tool-calling capability by treating predefined schemas as parameterized tools for
  tool selection and parameter filling. SPT addresses the lack of adaptability in
  universal information extraction when selecting between predefined schemas and on-the-fly
  schema generation.
---

# Schema as Parameterized Tools for Universal Information Extraction

## Quick Facts
- arXiv ID: 2506.01276
- Source URL: https://arxiv.org/abs/2506.01276
- Authors: Sheng Liang; Yongyue Zhang; Yaxiong Wu; Ruiming Tang; Yong Liu
- Reference count: 14
- Primary result: SPT achieves comparable extraction performance to LoRA baselines and current leading UIE systems while using significantly fewer trainable parameters (approximately 43K vs. 1.2M)

## Executive Summary
This paper introduces Schema as Parameterized Tools (SPT), a unified adaptive text-to-structure generation framework that treats predefined schemas as trainable tokens within a large language model's vocabulary. The method addresses the adaptability gap in universal information extraction by implementing a dual-mode execution system that dynamically retrieves predefined schemas or generates new ones when needed. SPT demonstrates robust performance across four distinct IE tasks, achieving competitive results while maintaining extreme parameter efficiency.

## Method Summary
SPT reimagines LLM tool-calling by treating predefined schemas as trainable token embeddings in the model's vocabulary, augmented with special `<Rej>` and `<Gen>` tokens. The framework employs a three-phase compositional training strategy: first training schema token embeddings, then helper tokens, and finally joint fine-tuning. During inference, the model operates in dual-mode execution—retrieval mode for predefined schemas and generation mode (triggered by `<Rej>`) for novel cases. This approach enables adaptive universal information extraction with minimal trainable parameters while maintaining structured output governance.

## Key Results
- SPT achieves an overall header F1 of 0.69 and content ROUGE-L of 0.39 on ODIE, competitive with larger models despite using a 1.5B parameter backbone
- The method delivers comparable extraction performance to LoRA baselines while using approximately 43K trainable parameters versus 1.2M for LoRA
- On unified datasets, SPT achieves superior schema retrieval performance (0.82 Recall@5) compared to baselines like BM25 and fine-tuned LLMs
- SPT demonstrates competitive F1 scores of 0.74 for fixed schemas and 0.52 for open schemas on the ODIE dataset

## Why This Works (Mechanism)

### Mechanism 1: Schema-Token Embedding for Unified Retrieval and Generation
Treating predefined schemas as trainable token embeddings enables more accurate schema retrieval than traditional semantic similarity methods. By extending the LLM's vocabulary with schema tokens and helper tokens, the model maps natural language queries directly to schema-specific tokens, bypassing intermediate semantic matching. This approach achieves superior retrieval performance (0.82 Recall@5) compared to baselines while maintaining extreme parameter efficiency.

### Mechanism 2: Dual-Mode Execution via Rejection and Generation Tokens
The framework introduces `<Rej>` and `<Gen>` tokens to create a dual-mode execution system. When the model cannot find a suitable predefined schema, it predicts `<Rej>` to trigger Generation Mode, where `<Gen>` prompts synthesis of a new schema definition. This enables handling both closed- and open-domain IE tasks within a single inference pass, achieving competitive F1 scores (0.74 for fixed, 0.52 for open schemas) on ODIE.

### Mechanism 3: Parameter-Efficient Compositional Training
A three-phase compositional training strategy enables stable joint optimization with minimal trainable parameters. The approach trains schema token embeddings first, then helper tokens, and finally performs joint fine-tuning at reduced learning rates. This staged optimization prevents challenges from simultaneous training on potentially small or imbalanced datasets while achieving comparable performance to methods using 1.2M parameters versus SPT's 43K.

## Foundational Learning

- **Token Embeddings in LLMs**
  - Why needed here: The framework operates by adding new, trainable tokens to the model's vocabulary. Understanding how the model's output is a probability distribution over this vocabulary is essential.
  - Quick check question: How does adding a new token to the LLM's vocabulary and training its embedding allow the model to perform a new function (like selecting a specific schema)?

- **Parameter-Efficient Fine-Tuning (PEFT)**
  - Why needed here: SPT is a form of PEFT, contrasting with methods like LoRA. Understanding the trade-off—modifying only a tiny fraction of parameters (43K vs. 1.2M) for efficiency vs. potential capacity limits—is crucial.
  - Quick check question: What are the advantages and potential risks of only fine-tuning the embedding layer of a large language model, as opposed to using a method like LoRA that modifies attention weights?

- **Tool-calling / Function-calling with LLMs**
  - Why needed here: The paper explicitly frames its contribution within the tool-calling paradigm, treating schemas as tools. Understanding tool retrieval, parameter filling, and execution flow is essential.
  - Quick check question: In the tool-calling analogy used by the paper, what part of the SPT framework corresponds to "tool retrieval," what part corresponds to "tool creation," and what part corresponds to "tool execution"?

## Architecture Onboarding

- **Component map:**
  - Input text query → LLM encoding → Token prediction → Mode selection (schema token vs. `<Rej>`) → Schema infilling (predefined or generated) → Structured output

- **Critical path:**
  1. Query Encoding: The LLM encodes the input text query
  2. Token Prediction: The model predicts the next token, which is the critical decision point
  3. Mode Selection: If token is in predefined schemas, proceed to Schema Infilling; if `<Rej>`, switch to Generation Mode, generate schema via `<Gen>`, then proceed to Schema Infilling

- **Design tradeoffs:**
  - Parameter Efficiency vs. Expressiveness: Uses ~43K parameters for extreme efficiency, potentially limiting performance on highly complex or unseen tasks
  - Schema Governance vs. Flexibility: Designed for data governance scenarios with structured outputs; `<Rej>` provides controlled flexibility but may be triggered less often if overfitted to predefined schemas
  - Retrieval-First Design: Biased toward retrieval mode, only generating new schemas when `<Rej>` is explicitly predicted

- **Failure signatures:**
  - Over-Rejection: Predicts `<Rej>` too frequently, suggesting poor training of schema-token embeddings
  - Under-Rejection: Never predicts `<Rej>`, forcing all queries into predefined schemas and failing on open-domain tasks
  - Hallucinated Arguments: Correctly selects/generates schema but fills slots with incorrect information
  - Incoherent Schema Generation: Produces nonsensical or unusable schema definitions in Generation Mode

- **First 3 experiments:**
  1. Reproduce Retrieval Results: Implement SPT on a small dataset with 10-20 schemas, train embeddings, and evaluate Recall@5 against BM25 and fine-tuned baselines
  2. Test Dual-Mode Switch: Create synthetic dataset with in-schema and out-of-distribution queries, train full SPT model, and measure `<Rej>` token prediction precision and recall
  3. Compare Generation Quality: On out-of-distribution queries, compare generated schemas against zero-shot/RAG baselines using human evaluation of coherence and utility

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability concerns with schema-token embeddings may arise as vocabulary size grows linearly with predefined schemas, potentially diluting model attention
- Quality of generated schemas in Generation Mode is not directly measured, raising uncertainty about semantic consistency and structural validity
- Evaluation scope is limited to curated benchmarks without addressing truly open-domain scenarios with high semantic diversity

## Confidence

- **High Confidence**: Parameter-efficient training approach (43K parameters vs. 1.2M LoRA) is technically sound and retrieval mechanism using schema-token embeddings is well-supported
- **Medium Confidence**: Dual-mode execution claims are supported by reported results, but quality of generated schemas is not directly evaluated
- **Low Confidence**: Claims of "superior" schema retrieval lack comprehensive ablation studies and comparison against simpler alternatives

## Next Checks
1. Ablation Study on Schema Quantity: Test framework with varying numbers of predefined schemas (10, 50, 100, 200) to identify scaling limits where performance degrades
2. Qualitative Analysis of Generated Schemas: Conduct human evaluation of schemas generated in Generation Mode across multiple domains, assessing coherence, structural validity, and utility
3. Cross-Domain Transfer Evaluation: Evaluate model on datasets from domains not represented in training data to test true generalization and measure performance degradation with semantically distant schemas