---
ver: rpa2
title: High-dimensional learning dynamics of multi-pass Stochastic Gradient Descent
  in multi-index models
arxiv_id: '2601.21093'
source_url: https://arxiv.org/abs/2601.21093
tags:
- dynamics
- learning
- then
- high-dimensional
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the high-dimensional dynamics of multi-pass
  stochastic gradient descent (SGD) for empirical risk minimization in multi-index
  models with isotropic random data. The authors analyze a mini-batch, multi-pass
  SGD procedure in an asymptotic regime where sample size $n$ and data dimension $d$
  increase proportionally, for sub-linear batch sizes and commensurate scaling of
  the learning rate.
---

# High-dimensional learning dynamics of multi-pass Stochastic Gradient Descent in multi-index models

## Quick Facts
- arXiv ID: 2601.21093
- Source URL: https://arxiv.org/abs/2601.21093
- Reference count: 15
- Key outcome: This paper studies the high-dimensional dynamics of multi-pass stochastic gradient descent (SGD) for empirical risk minimization in multi-index models with isotropic random data, providing an asymptotically exact characterization in the form of dynamical mean-field equations driven by a scalar Poisson jump process.

## Executive Summary
This paper analyzes the high-dimensional learning dynamics of multi-pass mini-batch SGD in multi-index models where both sample size $n$ and data dimension $d$ grow proportionally. The authors develop a dynamical mean-field theory (DMFT) framework that characterizes the asymptotic behavior of SGD through a system of integro-differential equations driven by a scalar Poisson jump process. A key finding is that for sub-linear batch sizes and commensurate learning rate scaling, the limiting dynamics are invariant to the specific batch size scaling exponent $\alpha \in [0,1)$, depending only on the limiting constants $\bar{\kappa}$ and $\bar{\eta}$.

## Method Summary
The method involves deriving dynamical mean-field equations for the coordinate-wise evolution of parameters in high-dimensional multi-index models. The approach uses approximate message passing (AMP) algorithms and state evolution to establish convergence to continuous-time limits. The paper proves that under critical scaling (batch size $\kappa \asymp n^\alpha$ and learning rate $\eta \asymp n^\alpha$ for $\alpha \in [0,1)$), SGD's stochastic noise converges to a Poisson jump process rather than Gaussian diffusion, distinguishing it from standard Stochastic Modified Equations (SME) approximations. The framework is validated through numerical simulations comparing SGD trajectories with theoretical predictions.

## Key Results
- SGD's coordinate-wise dynamics in the high-dimensional limit are characterized by dynamical mean-field equations driven by a scalar Poisson jump process
- For sub-linear batch sizes ($\kappa \asymp n^\alpha$ with $\alpha \in [0,1)$), the limiting dynamics are invariant to the specific scaling exponent
- In non-linear settings, SGD and SME dynamics are asymptotically distinct, coinciding only in the special case of linear regression models

## Why This Works (Mechanism)

### Mechanism 1: Poisson Jump Process Limit
In the high-dimensional limit with sub-linear batch sizes and "critical" learning rate scaling, the stochastic sampling noise of multi-pass SGD converges to a scalar Poisson jump process rather than a Gaussian diffusion. The mini-batch gradient updates, when viewed over rescaled time (epochs), accumulate discrete jumps driven by the specific sampling of data points. In the limit $n, d \to \infty$, this aggregates into a Poisson process $\{z_t\}$ with rate $\bar{\kappa}$, which drives the dynamical mean-field equations. This mechanism breaks if the batch size scales linearly ($\kappa \asymp n$) or if the learning rate is scaled to zero ($\eta \ll n^\alpha$).

### Mechanism 2: Batch Size Scaling Invariance
The limiting learning dynamics of SGD are identical for any sub-linear batch size scaling $\alpha \in [0, 1)$, depending only on the limiting constants $\bar{\kappa}$ and $\bar{\eta}$. The time-rescaling by $n^{1-\alpha}$ (to units of epochs) and the "critical" learning rate scaling $\eta \asymp n^\alpha$ ensure that the signal-to-noise ratio and effective drift per epoch remain balanced regardless of the specific growth rate of the batch size. This mechanism breaks if the batch size grows linearly ($\alpha=1$), where the stochasticity is averaged out differently.

### Mechanism 3: Distinction of Diffusion Approximations (SME)
The dynamics of Stochastic Modified Equations (SME), which approximate SGD via Gaussian diffusion, are asymptotically distinct from true SGD dynamics in non-linear settings under "critical" scaling. While SGD and SME share first and second moments, the higher-order statistics of the driving noise differ (Poisson jumps vs. Gaussian diffusion). In non-convex landscapes or non-linear models, this difference affects the trajectory of observables like overlaps and norms. This mechanism breaks in the special case of linear regression (quadratic loss, linear activation), where the dynamics of quadratic observables coincide for SGD and SME.

## Foundational Learning

- **Concept**: **Dynamical Mean-Field Theory (DMFT)**
  - **Why needed here**: This is the core analytical tool used to characterize the high-dimensional limit. It reduces the complex interaction of high-dimensional parameters to a system of deterministic integro-differential equations for correlation and response kernels.
  - **Quick check question**: Can you define the "response kernel" $R_{\theta}(t,s)$ in the context of this paper (specifically Eq 15)?

- **Concept**: **Stochastic Modified Equations (SME)**
  - **Why needed here**: Understanding SME is critical to appreciating this paper's contribution. SME is the standard "Gaussian diffusion" approximation for SGD; this paper proves it fails to capture the discrete nature of SGD noise in non-linear high-dimensional settings.
  - **Quick check question**: How does the driving noise process in the SME limit (Eq 24) differ from the Poisson process in the SGD limit (Eq 12)?

- **Concept**: **Multi-Index Models**
  - **Why needed here**: The analysis is restricted to this model class (labels $y = \sigma^*(x^\top\theta^*, \epsilon)$). It provides a tractable structure where the evolution of the parameter vector can be mapped to scalar processes (pre-activations $x^\top\theta_t$).
  - **Quick check question**: Why does the "isotropic" data assumption ($x_i$ entries i.i.d. with variance $1/d$) simplify the derivation of the DMFT limit?

## Architecture Onboarding

- **Component map**: Data Layer (Isotropic random features $x \in \mathbb{R}^d$) -> Model Layer (Multi-index map $y = \sigma^*(x^\top\theta^*, \epsilon)$) -> Optimization Layer (Mini-batch SGD with batch size $\kappa \asymp n^\alpha$, learning rate $\eta \asymp n^\alpha$) -> Limit System (Coupled stochastic processes $\{\theta_t, \xi_t, z_t\}$ defined by Eqs 13-17)

- **Critical path**:
  1. Define discrete-time SGD dynamics (Eq 6) with scaling assumptions
  2. Map to an Approximate Message Passing (AMP) algorithm (Section 4.1)
  3. Derive the discrete-time DMFT equations via state evolution of AMP
  4. Prove convergence of discrete DMFT to continuous DMFT as discretization $\delta \to 0$
  5. Identify the continuous limit process driven by Poisson jumps

- **Design tradeoffs**:
  - **Numerical Simulation**: The Poisson-driven DMFT for SGD is faster to simulate than gradient flow or SME because integrals against the Poisson process $\{z_t\}$ are sparse (Section 2.5.4)
  - **Approximation Quality**: Using SME (Gaussian diffusion) is mathematically convenient but inaccurate for non-linear models at large learning rates (Section 2.5.3)

- **Failure signatures**:
  - If numerical simulations of SGD and SME diverge significantly for a non-linear model, the system is operating in the regime where the Poisson jump noise (SGD) is fundamentally different from Gaussian noise (SME)
  - If overlap dynamics for linear regression differ between SGD and SME, there may be an implementation error in the simulation or scaling of $\bar{\eta}/\bar{\kappa}$

- **First 3 experiments**:
  1. **Linear Baseline**: Simulate linear regression (squared loss, linear activation) with high-dimensional data. Verify that the overlap statistics $d^{-1}\theta_t^\top\theta^*$ match between simulated SGD and the theoretical SME/DMFT prediction (Fig 1)
  2. **Non-Linear Divergence**: Implement a non-convex model (e.g., Huber loss with Tanh activation). Run both SGD and SME simulations. Confirm that the trajectories diverge as predicted by the distinct Poisson vs. Gaussian dynamics (Fig 2)
  3. **Scaling Invariance Check**: Run SGD simulations with varying sub-linear batch size exponents $\alpha$ (e.g., 0.0, 0.5, 0.9) while maintaining fixed $\bar{\eta}$ and $\bar{\kappa}$. Verify that the macroscopic dynamics (over time rescaled by epochs) are invariant

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the dynamical mean-field theory (DMFT) characterization be rigorously extended to pseudo-Lipschitz loss functions, specifically the squared loss?
- **Basis in paper:** [explicit] The authors state in Section 2.2 (Assumption 2.3) that they assume Lipschitz continuity to simplify technicalities, but explicitly remark that they expect results are extendable to pseudo-Lipschitz losses like the squared loss.
- **Why unresolved:** The current proofs rely on uniform boundedness of derivatives which do not strictly hold for the squared loss, requiring additional technical arguments to control the learning rate and gradients.
- **What evidence would resolve it:** A mathematical proof showing the convergence of coordinate-wise dynamics to the proposed DMFT limit under the squared loss with suitable regularization.

### Open Question 2
- **Question:** How do the limiting learning dynamics change when the assumption of isotropic data is relaxed?
- **Basis in paper:** [inferred] The analysis is restricted to isotropic random data (Assumption 2.2), which defines the covariance structure of the Gaussian processes in the limit.
- **Why unresolved:** The construction of the limiting Gaussian processes and the use of specific random matrix properties (like spectral decomposition) depend heavily on the identity covariance assumption; non-isotropic data introduces complex correlations.
- **What evidence would resolve it:** A generalization of Theorem 2.6 where the covariance kernel $C_\theta$ and the process $w_t$ are defined with respect to a general covariance matrix rather than the identity.

### Open Question 3
- **Question:** Does a well-defined scaling limit exist for linear batch sizes ($\kappa \asymp n$) under the same framework?
- **Basis in paper:** [inferred] The paper explicitly limits the batch size scaling exponent $\alpha$ to the range $[0, 1)$, excluding the linear batch size case ($\alpha=1$) analyzed in other works (e.g., [MKUZ21]).
- **Why unresolved:** Linear batch sizes imply significantly lower gradient noise ($\kappa \propto n$), potentially causing the martingale terms in the DMFT to vanish or scale differently, leading to a deterministic limit rather than the stochastic dynamics found for $\alpha < 1$.
- **What evidence would resolve it:** A rigorous analysis of the $\alpha \to 1$ limit within the paper's DMFT equations to determine if they collapse to the "persistent SGD" or gradient flow dynamics found in related literature.

## Limitations
- The analysis relies heavily on the isotropic data assumption and specific scaling regime, which may not extend to correlated feature distributions or different data regimes
- The theoretical framework assumes sub-linear batch scaling, which may not represent practical training scenarios where batch sizes increase with computation
- The extension to more complex architectures beyond multi-index models remains unexplored, potentially limiting applicability to modern deep learning settings

## Confidence

- **High confidence**: The distinction between SGD and SME dynamics in non-linear models (Mechanism 3). The paper provides both theoretical proofs and numerical validation showing divergence between these processes for non-convex settings.
- **Medium confidence**: The batch size scaling invariance claim (Mechanism 2). While theoretically derived, this relies on asymptotic arguments that may have finite-size effects not fully characterized in the paper.
- **Medium confidence**: The Poisson jump process characterization (Mechanism 1). The theoretical derivation is rigorous, but the practical implications and range of validity require further empirical validation.

## Next Checks
1. **Finite-Size Effects Study**: Validate the scaling invariance claim (Mechanism 2) by systematically varying both batch size and sample dimension while measuring deviations from the theoretical prediction across multiple non-asymptotic settings.
2. **Non-Isotropic Data Extension**: Test whether the Poisson jump process characterization holds when replacing isotropic Gaussian features with correlated or structured data distributions, measuring changes in the driving noise statistics.
3. **Architectural Generalization**: Extend the analysis to multi-layer networks with non-linear activations, examining whether the fundamental distinction between SGD and SME dynamics persists or whether new phenomena emerge in deeper architectures.