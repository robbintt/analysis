---
ver: rpa2
title: 'Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning for
  Large Language Model'
arxiv_id: '2507.06892'
source_url: https://arxiv.org/abs/2507.06892
tags:
- training
- off-policy
- policy
- data
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of on-policy reinforcement
  fine-tuning (RFT) methods like PPO and GRPO in improving reasoning abilities of
  large language models (LLMs), which require extensive computational cost due to
  the need for fresh data generation at each iteration. To overcome this limitation,
  the authors propose ReMix, a general approach that enables on-policy proximal policy
  gradient methods to leverage off-policy data generated during training.
---

# Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning for Large Language Model

## Quick Facts
- arXiv ID: 2507.06892
- Source URL: https://arxiv.org/abs/2507.06892
- Authors: Jing Liang; Hongyao Tang; Yi Ma; Jinyi Liu; Yan Zheng; Shuyue Hu; Lei Bai; Jianye Hao
- Reference count: 40
- Primary result: Achieves state-of-the-art-level performance on five math reasoning benchmarks with 30x-450x reduction in training cost

## Executive Summary
This paper addresses the inefficiency of on-policy reinforcement fine-tuning (RFT) methods like PPO and GRPO, which require extensive computational cost due to the need for fresh data generation at each iteration. The authors propose ReMix, a general approach that enables on-policy proximal policy gradient methods to leverage off-policy data generated during training. ReMix integrates three key components: mix-policy proximal policy gradient with increased update-to-data ratio, KL-convex policy constraint to balance stability and flexibility, and policy reincarnation to seamlessly transition from efficient early-stage learning to stable asymptotic improvement.

## Method Summary
ReMix is a general approach that enables on-policy reinforcement fine-tuning methods to leverage off-policy data generated during training. It consists of three key components: Mix-PPG, which strategically leverages both off-policy and on-policy data within a unified objective function with increased Update-to-Data ratio; KL-Convex Policy Constraint, which combines KL constraints on the base model and the precedent model to balance the trade-off between stability and flexibility; and Policy Reincarnation, which replaces the base model with the mix-policy RFT model midway through training to achieve a seamless transition from Mix-PPG to pure on-policy RFT. This approach aims to achieve state-of-the-art-level performance while significantly reducing training costs.

## Key Results
- Achieves state-of-the-art-level performance on five math reasoning benchmarks
- Demonstrates over 30x to 450x reduction in training cost in terms of rollout data volume
- Reveals insights into how off-policy learning affects reasoning behaviors such as self-reflection and response length

## Why This Works (Mechanism)

### Mechanism 1: Mix-Policy Proximal Policy Gradient (Mix-PPG)
- **Claim:** If historical trajectories are combined with fresh on-policy data using importance sampling corrections, sample efficiency increases significantly without requiring fresh rollouts for every update.
- **Mechanism:** The standard PPO clipping objective is modified to sample from a distribution $\nu$ covering both the current policy $\pi_k$ and historical policies $\pi_{k-i}$. By increasing the Update-to-Data (UTD) ratio, the system performs multiple gradient updates on the mixed batch, amortizing the cost of generation.
- **Core assumption:** The importance sampling ratio $r^{k-i}_\theta$ accurately corrects for the distribution shift between historical and current policies, and the advantage estimator remains valid under this mixture.
- **Evidence anchors:**
  - [abstract]: "...enable on-policy RFT methods like PPO and GRPO to leverage off-policy data... with an increased Update-To-Data (UTD) ratio."
  - [section 3.1]: "LMix-PPG... strategically leverages both off-policy and on-policy data within a unified objective function."
  - [corpus]: The "RAPID" and "AGRO" papers similarly propose using off-policy or replay data to improve the efficiency of RL finetuning.
- **Break condition:** Excessive off-policy ratios (e.g., $p > 0.5$) cause training instability or collapse due to large distributional shifts (Fig 4).

### Mechanism 2: Policy Reincarnation
- **Claim:** If the training process switches from Mix-PPG to pure on-policy PPO at a specific step $T$, the model mitigates the asymptotic performance degradation often associated with off-policy bias.
- **Mechanism:** At step $T$, the system executes a "reincarnation": it drops the historical buffer $H$ and resets the reference model for the KL constraint from the initial base model ($\pi_{base}$) to the current policy ($\pi_T$). This effectively restarts training as a standard on-policy process from a highly advanced starting point.
- **Core assumption:** The "space of good models" can be reached more efficiently via off-policy exploration initially, but requires on-policy optimization for stable convergence.
- **Evidence anchors:**
  - [abstract]: "...replaces the base model with the mix-policy RFT model in the mid way of training... to achieve a seamless transition."
  - [section 3.3]: "The efficacy of ReMix is two-fold... leverage the advantages of Mix-PPG... [and] stable asymptotic improvement."
  - [corpus]: General RL literature (e.g., "Reincarnating RL") supports reusing prior computation to accelerate progress.
- **Break condition:** If the transition occurs too early, efficiency gains are minimal; if too late, off-policy bias may permanently harm the policy's reasoning structure (Section 4.4.2).

### Mechanism 3: KL-Convex Policy Constraint
- **Claim:** If the KL penalty is a dynamic convex combination of the base model and the most recent policy, training balances retention of foundational knowledge and rapid adaptation better than static constraints.
- **Mechanism:** The constraint term $L_{KLC}$ is calculated as $\lambda \cdot D_{KL}(\pi_\theta || \pi_{base}) + (1-\lambda) \cdot D_{KL}(\pi_\theta || \pi_{k-1})$. The coefficient $\lambda$ decays over time, loosening the anchor to the static base model and allowing more flexibility.
- **Core assumption:** Anchoring to $\pi_{k-1}$ prevents catastrophic forgetting during rapid off-policy updates better than anchoring solely to the distant $\pi_{base}$.
- **Evidence anchors:**
  - [abstract]: "...combines the KL constraints on the base model and the precedent model to balance the trade-off..."
  - [section 3.2]: "This convex combination preserves foundational capabilities while enabling targeted adaptation..."
  - [corpus]: Weak direct evidence for this specific convex combination in the provided neighbors.
- **Break condition:** If $\lambda$ decays too aggressively, the model may deviate too far from the base model's generalization properties, risking instability.

## Foundational Learning

- **Concept: Importance Sampling**
  - **Why needed here:** Essential for calculating the Mix-PPG objective. You must understand how to weight samples from historical distributions ($\pi_{k-i}$) to estimate gradients for the current policy ($\pi_k$).
  - **Quick check question:** Why does the variance of the importance ratio $r_\theta(s,a)$ increase as the behavior policy diverges from the target policy?

- **Concept: Update-to-Data (UTD) Ratio**
  - **Why needed here:** This is the primary efficiency metric ReMix exploits. Understanding the trade-off between multiple gradient updates (high UTD) and sample freshness is critical.
  - **Quick check question:** In standard PPO, why is the UTD ratio typically kept low (e.g., 1-3) compared to off-policy methods?

- **Concept: PPO Clipping**
  - **Why needed here:** ReMix modifies the PPO clipping mechanism for the mix-policy setting. You need to grasp the original objective to understand how the clip range ($\epsilon$) is adapted for historical ratios.
  - **Quick check question:** How does the PPO clipping function prevent the policy from changing too drastically in a single update?

## Architecture Onboarding

- **Component map:** Base Model ($\pi_{base}$) -> Prompt Dataset ($D_0$) -> Historical Policy Set ($H$) -> Mix-PPG Loss -> Gradient Step (repeated $m$ times) -> Reincarnation Step ($T$) -> Standard PPO/GRPO

- **Critical path:**
  1. **Ratio Calculation:** Accurately computing $r^{k-i}_\theta$ for off-policy data is critical; numerical instability here causes collapse.
  2. **The Switch:** The precise execution of the "Reincarnation" step (resetting anchors and flushing buffers) determines the transition from efficiency to asymptotic stability.

- **Design tradeoffs:**
  - **Off-policy ratio ($p$):** Default is 0.4. Higher values increase efficiency but risk the "Whipping Effect" and collapse.
  - **Reincarnation Step ($T$):** A hyperparameter sensitive to dataset size and model scale.
  - **UTD Ratio ($m$):** Set to 2. Increasing this aggressively without KL-Convex constraints leads to degradation.

- **Failure signatures:**
  - **The "Whipping Effect":** A phenomenon where off-policy discrepancy implicitly forces the model to generate shorter, less reflective responses to minimize the loss product (negative advantage $\times$ importance ratio).
  - **Reflection Collapse:** A sudden drop in self-reflection tokens (e.g., "wait", "check") and response length, often preceding a drop in accuracy (Section 4.4.2).

- **First 3 experiments:**
  1. **Reproduce the "Whipping Effect":** Train Mix-PPG with varying off-policy ratios ($p \in [0.1, 0.5]$) and plot response length against Pass@1 to verify the correlation between high off-policyness and response shortening.
  2. **Ablation on Reincarnation:** Compare ReMix against "Mix-PPG only" (no reincarnation) to quantify the performance gap in later training steps (asymptotic performance).
  3. **Prompt Robustness Check:** Evaluate ReMix models with and without "guide tokens" (system prompts) to test the paper's claim of improved robustness to prompt variations (Section 4.4.4).

## Open Questions the Paper Calls Out
None

## Limitations
- The optimal off-policy ratio (p=0.4) and reincarnation step (T=5k) appear sensitive to dataset characteristics and model scale, suggesting limited generalizability without careful hyperparameter tuning.
- The "Whipping Effect" reveals an implicit bias in off-policy learning that reduces reasoning depth, potentially trading reasoning quality for efficiency.
- The KL-Convex policy constraint lacks strong empirical justification in the provided literature, relying instead on intuitive appeal rather than rigorous theoretical grounding.

## Confidence
- **High confidence:** The core mechanism of Mix-PPG with importance sampling corrections is well-established in RL literature, and the efficiency improvements (30x-450x reduction in rollout data) are empirically validated across five benchmarks.
- **Medium confidence:** The policy reincarnation mechanism shows promise but depends heavily on the precise timing of the T transition. The paper demonstrates improved asymptotic performance but doesn't fully explore the sensitivity of T to different problem domains or dataset sizes.
- **Low confidence:** The KL-Convex policy constraint represents the least validated component, with minimal direct evidence provided for its specific convex combination approach versus alternative KL formulations.

## Next Checks
1. **Sensitivity Analysis on Reincarnation Timing**: Systematically vary the reincarnation step T across different dataset sizes and reasoning task complexities to establish robust guidelines for hyperparameter selection rather than relying on a single value.

2. **Long-term Stability Evaluation**: Extend training beyond the reported checkpoints to assess whether the reincarnation mechanism prevents gradual degradation in reasoning quality, particularly examining the emergence of new failure modes at later stages.

3. **Cross-domain Generalization Test**: Apply ReMix to non-mathematical reasoning tasks (e.g., code generation, logical inference) to verify that the efficiency gains and reasoning behavior patterns generalize beyond the GSM8K and MATH benchmarks used in the paper.