---
ver: rpa2
title: Are BabyLMs Deaf to Gricean Maxims? A Pragmatic Evaluation of Sample-efficient
  Language Models
arxiv_id: '2510.04764'
source_url: https://arxiv.org/abs/2510.04764
tags:
- language
- maxims
- pragmatic
- linguistics
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new benchmark for evaluating the pragmatic
  competence of language models using the Gricean maxims. The benchmark is based on
  the Conversational Violations Test (CVT) from psycholinguistics, adapted to assess
  whether models can distinguish between maxim-adhering and maxim-violating utterances.
---

# Are BabyLMs Deaf to Gricean Maxims? A Pragmatic Evaluation of Sample-efficient Language Models

## Quick Facts
- arXiv ID: 2510.04764
- Source URL: https://arxiv.org/abs/2510.04764
- Reference count: 26
- Models trained on <100M tokens outperform those on <10M tokens, but both fall short of child-level competence on Gricean maxim adherence

## Executive Summary
This paper introduces a new benchmark for evaluating pragmatic competence of language models using Gricean maxims. The benchmark extends the Conversational Violations Test (CVT) from psycholinguistics to over 2,250 items assessing whether models can distinguish between maxim-adhering and maxim-violating utterances. The authors evaluate BabyLMs trained on less than 10M and less than 100M tokens, comparing their performance to children and a large language model trained on 3T tokens. Results show that while increasing training data improves pragmatic performance, significant gaps remain between model and child-level understanding of conversational norms, with models performing best on truthfulness (Quality) and worst on informativeness (Quantity I and II).

## Method Summary
The authors adapted the Conversational Violations Test (CVT) from psycholinguistics to create a benchmark for evaluating language models' ability to detect violations of Gricean maxims. They extended the original CVT dataset to 2,250 items with minimal context (question-answer pairs) and tested BabyLMs trained on less than 10M and less than 100M tokens. Models were evaluated on five Gricean maxims: Quantity (informativeness), Quality (truthfulness), Relation (relevance), Manner (clarity), and Politeness. Performance was measured using binary accuracy across the five maxim categories, comparing model probability assignments between follower (maxim-adhering) and violator (maxim-violating) answers.

## Key Results
- BabyLMs trained on <100M tokens significantly outperform those trained on <10M tokens on Gricean maxim adherence
- Both data-limited BabyLMs fall short of child-level competence on the benchmark
- Models perform best on Quality (truthfulness) and worst on Quantity I and II (informativeness)
- Under data-limited conditions, models show strong inter-maxim correlations suggesting conflated pragmatic competences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing training data leads to finer-grained differentiation between pragmatic dimensions.
- Mechanism: With limited data (<10M tokens), models develop undifferentiated representations where distinct pragmatic competences co-vary strongly (e.g., Quantity I and Quantity II correlation r=0.953; Relation and Politeness r=0.999). Additional data exposes the model to more diverse conversational patterns, allowing specialized processing of each maxim type to emerge independently.
- Core assumption: Pragmatic competences are separable cognitive/representational functions that require sufficient exemplar diversity to differentiate.
- Evidence anchors:
  - [abstract] "under data-limited conditions, models tend to conflate certain pragmatic competences, but these associations weaken with more data"
  - [section 4] Correlation analyses show "shift from rather extreme relations in the Strict-small track to more varied and generally weaker associations in the Strict track"
  - [corpus] Limited corpus support for this specific differentiation mechanism; related work focuses on general pragmatic evaluation rather than inter-maxim correlations.
- Break condition: If pragmatic dimensions are fundamentally interdependent rather than separable, differentiation may plateau regardless of data scale.

### Mechanism 2
- Claim: Token-frequency distributions can override pragmatic appropriateness in model probability assignments.
- Mechanism: Models assign probability based on distributional patterns learned from training data. Pragmatically inappropriate responses (e.g., "in a cup" as answer to "How do you prefer your tea?") may contain higher-frequency token combinations than pragmatically appropriate alternatives ("with milk"), causing models to preferentially assign higher probability to maxim-violating responses.
- Core assumption: Models lack explicit pragmatic reasoning modules and rely primarily on distributional statistics.
- Evidence anchors:
  - [section 4] "In Quantity I, while 'with milk' falls in the range of pragmatically accepted answers, 'in a cup' might contain tokens (or token combinations) that more frequent in the training data"
  - [section 5] Authors acknowledge "model probability assignments may be influenced by the distributional properties of tokens independent of context"
  - [corpus] Related work (Hu et al., 2023) reports LLMs assign higher probabilities to literal meanings over intended pragmatic meanings.
- Break condition: If frequency bias is corrected through explicit pragmatic training objectives or fine-tuning, this mechanism would weaken.

### Mechanism 3
- Claim: Auto-regressive architectures better capture conversational flow for pragmatic evaluation than masked language models.
- Mechanism: In pragmatic evaluation, models must assess entire answer sequences as coherent continuations of questions. Auto-regressive models compute conditional probabilities P(answer|question) directly, while masked LMs must infer full-sequence plausibility through pseudo-log-likelihood approximations, which may not capture discourse coherence as effectively.
- Core assumption: Conversational appropriateness depends on sequential coherence modeling.
- Evidence anchors:
  - [section 4] "GPT-2 consistently outperforms LTG-BERT and RoBERTa models" across both tracks
  - [section 4] "Probability assignment to an entire answer benefits from modeling sequences as coherent continuations rather than token-masked completions"
  - [corpus] No direct corpus evidence comparing autoregressive vs. masked architectures on pragmatic tasks.
- Break condition: If masked LMs are fine-tuned on dialogue data or use improved scoring methods, the architecture gap may narrow.

## Foundational Learning

- Concept: **Gricean Maxims** (Quantity, Quality, Relation, Manner, Politeness)
  - Why needed here: The entire benchmark operationalizes pragmatic competence through violations of these five conversational norms. Understanding each maxim is essential for interpreting model performance patterns.
  - Quick check question: Which maxim is violated when someone answers "I live on the moon" to "Where do you live?"

- Concept: **Conditional Log-Probability Scoring**
  - Why needed here: The evaluation methodology computes P(answer|question) to determine which response the model prefers. Understanding this scoring approach is critical for debugging unexpected model preferences.
  - Quick check question: How would you compute the conditional probability of an answer given a question for an autoregressive model?

- Concept: **Pseudo-Log-Likelihood for Masked LMs**
  - Why needed here: Masked LMs cannot directly compute P(sequence|context). The improved pseudo-log-likelihood method (Kauf and Ivanova, 2023) enables comparable evaluation across architectures.
  - Quick check question: Why can't masked language models directly compute conditional sequence probabilities?

## Architecture Onboarding

- Component map:
  - Test items (2,250 dialogues) -> Model families (GPT-2, Baby Llama, LTG-BERT, RoBERTa) -> Minicons library -> Binary accuracy per maxim

- Critical path:
  1. Load pretrained BabyLM from HuggingFace (babylm or BabyLM-community organizations)
  2. Format dialogue items as question-answer pairs
  3. Compute log-probabilities (autoregressive: conditional; masked: pseudo-log-likelihood)
  4. Normalize by answer length
  5. Compare P(follower) vs P(violator) for each item
  6. Aggregate accuracy per maxim category

- Design tradeoffs:
  - **Minimal context vs. rich scenarios**: Current design uses minimal context (question-answer pairs only). This limits ecological validity but enables controlled evaluation. Adding scenario context (as in Zheng et al., 2021) could reveal context-dependent pragmatic reasoning but introduces confounds.
  - **Lexical standardization**: The authors minimized superficial differences between follower/violator answers to reduce noise, but this may obscure some authentic pragmatic distinctions.

- Failure signatures:
  - **Below-chance performance on specific maxims**: RoBERTa Strict-small scored 0.33 on Quantity I (worse than random), suggesting systematic bias in probability assignments
  - **High inter-maxim correlations in low-data regimes**: Near-perfect correlations (r>0.95) indicate conflation of distinct pragmatic dimensions
  - **Inverse patterns between forced-choice and open-ended evaluation**: Prior work (Park et al., 2024) found models performed well on Quality in multiple-choice but poorly on open-ended intent questions

- First 3 experiments:
  1. **Reproduce baseline scores** on the publicly available dataset (2,250 items) using Minicons with GPT-2 Strict-small to validate pipeline setup.
  2. **Ablate training data scale**: Train the same architecture on incremental data amounts (10M, 25M, 50M, 100M tokens) to characterize the scaling curve for each maxim independently.
  3. **Probe frequency-confound items**: Identify items where the violator answer contains higher-frequency tokens than the follower (using training corpus statistics) and compare accuracy on confounded vs. non-confounded subsets.

## Open Questions the Paper Calls Out
None

## Limitations
- **Generalization to real conversations**: The benchmark uses minimal context (isolated question-answer pairs), which may not capture the full complexity of pragmatic reasoning required in open-ended dialogue.
- **Developmental validity**: Direct comparison to children's performance assumes equivalent task demands across populations, but models process text through distributional statistics rather than human-like reasoning.
- **Architecture-specific confounds**: Performance differences between autoregressive and masked architectures could reflect architectural biases rather than genuine pragmatic competence gaps.

## Confidence
- **High Confidence**: The finding that pragmatic performance improves with increased training data is well-supported by the systematic scaling analysis across two data regimes.
- **Medium Confidence**: The claim that models conflate distinct pragmatic competences under low-data conditions is supported by correlation analyses, but the causal mechanism remains speculative.
- **Low Confidence**: The assertion that BabyLMs are "deaf" to Gricean maxims overstates the evidence, as models demonstrate above-random performance on multiple maxims.

## Next Checks
1. **Context sensitivity probe**: Systematically vary the amount of conversational context provided with each item (no context, minimal context, rich scenario context) to test whether model performance on Gricean maxim evaluation scales with context availability.

2. **Cross-linguistic replication**: Evaluate BabyLMs trained on typologically diverse languages (e.g., isolating vs. agglutinative, high-resource vs. low-resource) to determine whether observed patterns in pragmatic competence are universal or language-dependent.

3. **Fine-tuning intervention study**: Train models on explicitly pragmatic datasets (e.g., conversational implicature corpora, indirect speech act datasets) and re-evaluate on the Gricean maxim benchmark to test whether targeted training on pragmatic phenomena can close the performance gap with children.