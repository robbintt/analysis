---
ver: rpa2
title: 'CodeDistiller: Automatically Generating Code Libraries for Scientific Coding
  Agents'
arxiv_id: '2512.01089'
source_url: https://arxiv.org/abs/2512.01089
tags:
- code
- examples
- repository
- discovery
- repositories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents CodeDistiller, a system that automatically
  converts scientific GitHub repositories into vetted code examples for use in automated
  scientific discovery agents. The system addresses the limitation that current agents
  are constrained by their parametric knowledge and often require manually crafted
  code libraries.
---

# CodeDistiller: Automatically Generating Code Libraries for Scientific Coding Agents
## Quick Facts
- arXiv ID: 2512.01089
- Source URL: https://arxiv.org/abs/2512.01089
- Reference count: 9
- Primary result: Claude 4.5 successfully distilled functional code examples for 74% of 250 materials science repositories

## Executive Summary
This paper presents CodeDistiller, a system that automatically converts scientific GitHub repositories into vetted code examples for use in automated scientific discovery agents. The system addresses the limitation that current agents are constrained by their parametric knowledge and often require manually crafted code libraries. CodeDistiller works by first identifying the core purpose of a repository, then selecting relevant files, and finally generating and iteratively debugging a working code example.

The system was evaluated on 250 materials science repositories using three different base models (GPT-OSS-120B, GPT-5, and Claude Sonnet 4.5). The best model (Claude) successfully distilled functional examples for 74% of repositories. A downstream evaluation demonstrated that an automated discovery system augmented with CodeDistiller-generated examples produced more accurate, complete, and scientifically sound results than a baseline system with only general materials science code examples. The approach shows promise for scaling automated scientific discovery by reducing reliance on manual code library construction.

## Method Summary
CodeDistiller automatically converts scientific GitHub repositories into vetted code examples for automated scientific discovery agents. The system first analyzes repositories to identify their core purpose and extract relevant files. It then generates a working code example and employs an iterative debugging process to ensure functionality. The approach was evaluated on 250 materials science repositories using three base models (GPT-OSS-120B, GPT-5, and Claude Sonnet 4.5), with Claude achieving the highest success rate of 74% in producing functional code examples.

## Key Results
- Claude Sonnet 4.5 successfully distilled functional code examples for 74% of 250 materials science repositories
- The best model (Claude) outperformed GPT-OSS-120B and GPT-5 in repository distillation tasks
- Downstream evaluation showed automated discovery systems using CodeDistiller-generated examples produced more accurate, complete, and scientifically sound results compared to systems with only general materials science examples

## Why This Works (Mechanism)
CodeDistiller works by systematically decomposing the repository distillation task into manageable subtasks: repository purpose identification, file selection, code generation, and iterative debugging. The iterative debugging process is crucial for transforming initial generated code into working examples, compensating for the inherent limitations of base models in producing immediately functional code from complex scientific repositories.

## Foundational Learning
- Repository analysis techniques: Understanding how to automatically extract core functionality from scientific codebases is essential for scaling automated discovery. Quick check: Can the system correctly identify the primary purpose from repository README and documentation?
- Code generation from documentation: The ability to translate scientific descriptions into executable code is fundamental to the approach. Quick check: Does the generated code match the documented functionality?
- Iterative debugging methodology: Systematic refinement of generated code through multiple iterations ensures working examples. Quick check: What percentage of code examples become functional after each debugging iteration?

## Architecture Onboarding
**Component Map:** Repository Analysis -> Code Generation -> Iterative Debugging -> Code Example Output

**Critical Path:** The system must successfully complete repository analysis to identify relevant files, generate initial code, and complete iterative debugging to produce a working example. Each stage builds on the previous one, with debugging being particularly crucial for success.

**Design Tradeoffs:** The approach balances automation (reducing manual curation) against computational cost (iterative debugging). The iterative process increases success rates but may not scale well for very complex repositories.

**Failure Signatures:** Failure can occur at any stage - incorrect repository analysis leads to irrelevant code generation, poor initial code generation requires more debugging iterations, and some repositories may be too complex for complete distillation.

**3 First Experiments:**
1. Test CodeDistiller on a single well-documented materials science repository to verify the end-to-end pipeline
2. Evaluate the iterative debugging process by measuring success rates after each iteration
3. Compare the success rates of different base models (GPT-OSS-120B, GPT-5, Claude) on a small sample of repositories

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation is limited to materials science repositories, leaving unclear whether the approach generalizes to other scientific domains
- Downstream evaluation only compared against a baseline with general materials science examples, not against systems with manually curated domain-specific libraries
- The iterative debugging process may not scale well for more complex repositories, and computational cost implications are not discussed

## Confidence
High confidence: The core distillation methodology is technically sound and the empirical results on repository distillation are reproducible. The improvement in downstream scientific discovery tasks is demonstrated with appropriate controls.

Medium confidence: The claim that this approach reduces reliance on manual code library construction is plausible but not fully quantified. The relative effectiveness compared to existing automated code generation approaches is not established.

## Next Checks
1. Test CodeDistiller on repositories from multiple scientific domains (e.g., computational biology, physics, chemistry) to assess domain generalization and identify any domain-specific failure modes.

2. Compare downstream scientific discovery performance against systems using manually curated, domain-specific code libraries to quantify the practical advantage of automatic distillation.

3. Conduct a cost-benefit analysis measuring the computational resources required for iterative debugging versus the success rate improvements, particularly for larger and more complex repositories.