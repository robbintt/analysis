---
ver: rpa2
title: 'Understanding Knowledge Transferability for Transfer Learning: A Survey'
arxiv_id: '2507.03175'
source_url: https://arxiv.org/abs/2507.03175
tags:
- transferability
- learning
- target
- source
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a unified taxonomy of transferability metrics
  in transfer learning, categorizing them based on knowledge types and measurement
  granularity. It addresses the challenge of assessing when and what source knowledge
  to transfer by examining various metrics that evaluate the compatibility between
  source and target tasks.
---

# Understanding Knowledge Transferability for Transfer Learning: A Survey

## Quick Facts
- **arXiv ID:** 2507.03175
- **Source URL:** https://arxiv.org/abs/2507.03175
- **Reference count:** 40
- **Key outcome:** This survey provides a unified taxonomy of transferability metrics in transfer learning, categorizing them based on knowledge types and measurement granularity. It addresses the challenge of assessing when and what source knowledge to transfer by examining various metrics that evaluate the compatibility between source and target tasks. The work covers dataset transferability (e.g., MMD, OTDD), model transferability (e.g., LogME, LEEP), and their applications across different learning paradigms. By synthesizing theoretical foundations and empirical findings, the survey offers insights into metric selection and highlights future research directions, aiming to guide researchers in making informed decisions for efficient and reliable AI systems.

## Executive Summary
This survey comprehensively examines transferability metrics in transfer learning, addressing the fundamental challenge of determining when and what source knowledge to transfer to target tasks. The authors propose a unified taxonomy categorizing metrics based on knowledge types (dataset, model, representation, parameter) and measurement granularity (pre-hoc, online, post-hoc). By synthesizing theoretical foundations from domain adaptation and information theory with practical empirical findings, the survey provides a framework for understanding how different metrics evaluate the compatibility between source and target domains. The work aims to guide researchers in selecting appropriate metrics for their specific transfer learning scenarios, ultimately contributing to more efficient and reliable AI systems.

## Method Summary
The survey systematically categorizes transferability metrics based on two dimensions: knowledge types (dataset transferability, model transferability, representation transferability, parameter transferability) and measurement granularity (pre-hoc, online, post-hoc). For each category, the authors review theoretical foundations, mathematical formulations, and empirical applications. The methodology involves analyzing existing literature to extract core mechanisms, assumptions, and evaluation protocols. Key metrics examined include MMD and Wasserstein distance for dataset transferability, and LogME and LEEP for model transferability. The survey emphasizes the need for careful metric selection based on specific transfer learning scenarios and highlights gaps in current theoretical frameworks.

## Key Results
- Transferability metrics can be systematically categorized based on knowledge types and measurement granularity, providing a unified framework for understanding their applications.
- Theoretical bounds from domain adaptation (HΔH-divergence, MMD) provide mathematical justification for why distributional alignment correlates with transfer success.
- Practical metrics like LogME and LEEP approximate complex fine-tuning processes through linear loss approximation, enabling efficient transferability assessment without full training.
- Current metrics show varying effectiveness across different learning paradigms, with no single metric universally superior across all scenarios.

## Why This Works (Mechanism)

### Mechanism 1: Generalization Bounds via Distributional Divergence
- **Claim:** If the distributional divergence between source and target domains is low, the target error is theoretically bounded by the source error plus a divergence term, suggesting high transferability.
- **Mechanism:** Metrics like MMD (Maximum Mean Discrepancy) or Wasserstein Distance quantify the statistical distance between source and target distributions. By minimizing this distance (or selecting sources where it is naturally low), the upper bound on the target generalization error is reduced.
- **Core assumption:** The hypothesis space is rich enough, and the loss function satisfies the triangle inequality. The distribution shift is the primary cause of performance degradation.
- **Evidence anchors:**
  - [Section 3.1.2] "This bound emphasizes that minimizing the Wasserstein distance between the source and target empirical measures can enhance the target error’s generalization..."
  - [Section 3.1.3] "This bound shows that the target risk... can be controlled by the source risk... and the MMD between source and target distributions."
  - [corpus] Neighboring papers discuss transferability in LLMs and adversarial settings, but specific theoretical evidence for *distributional bounds* is not strongly supported by the provided corpus signals (which focus on prompt/k-NN transfer).
- **Break condition:** High dimensionality causing the "curse of dimensionality" where sample estimates of divergence become unreliable; or shifts in label space ($Y$) that are not captured by input distribution divergence alone.

### Mechanism 2: Linear Loss Approximation (Feature-Label Alignment)
- **Claim:** The transferability of a source model can be approximated by the performance of a simple (often linear) classifier trained on top of the frozen source features using target labels.
- **Mechanism:** Instead of expensive fine-tuning, methods like LogME or LEEP calculate the maximum evidence or log-likelihood of the target labels given the source features. High alignment indicates the source feature space is already "shaped" to separate target classes.
- **Core assumption:** The source feature extractor is sufficiently general that a linear transformation is adequate to adapt to the target task (linear separability).
- **Evidence anchors:**
  - [Section 4.2.3] "Linear frameworks approximate the fine-tuning layer into optimization problem... LogME introduces a linear model upon target features and suggests estimating the maximum average log evidence..."
  - [Abstract] "...evaluating the potential of source knowledge for transfer learning... emphasizing the need for careful selection of these metrics."
- **Break condition:** When target tasks require non-linear feature transformations that the frozen source model cannot provide (negative transfer), or when the source model is underfitted.

### Mechanism 3: Information Theoretic Consistency (Conditional Entropy)
- **Claim:** Transferability correlates with low uncertainty (conditional entropy) of target labels given source model predictions.
- **Mechanism:** Methods like NCE (Negative Conditional Entropy) measure how much information the source model's output provides about the target labels. Lower entropy implies the source model's "concepts" map cleanly to target classes.
- **Core assumption:** The source model's output probabilities are calibrated enough to serve as reliable indicators of uncertainty.
- **Evidence anchors:**
  - [Section 4.2.2] "NCE... treats training labels as random variables... this value is related to the loss of the transferred model."
  - [Section 3.1.4] Discusses information-theoretic bounds involving KL divergence and mutual information.
- **Break condition:** When source and target label spaces are disjoint or poorly mapped, causing high entropy despite semantic similarity (e.g., "car" in source vs. "automobile" in target without a mapping).

## Foundational Learning

- **Concept: Domain Adaptation & HΔH-Divergence**
  - **Why needed here:** This is the theoretical bedrock of the survey (Section 3.1.1). You cannot understand *why* distance metrics work without understanding how domain divergence bounds the error of a hypothesis.
  - **Quick check question:** Can you explain how the $H\Delta H$-divergence differs from simple Euclidean distance between dataset means in terms of predicting transfer success?

- **Concept: Optimal Transport (Wasserstein Distance)**
  - **Why needed here:** Heavily referenced in dataset transferability (OTDD) and model transferability (OTCE) as a robust way to measure distributional alignment, especially when distributions do not overlap significantly.
  - **Quick check question:** Why is Wasserstein distance often preferred over KL-divergence for measuring distances between non-overlapping distributions in high-dimensional spaces?

- **Concept: Bayesian Evidence / Marginalized Likelihood**
  - **Why needed here:** Critical for understanding "LogME" (Section 4.2.3), a key model transferability metric. It involves integrating over model weights rather than optimizing them.
  - **Quick check question:** How does calculating the marginal likelihood $p(y|g)$ differ from simply training a linear classifier on features $g$?

## Architecture Onboarding

- **Component map:**
  - Source Modality: Pre-trained model ($f_s$) or Source Dataset ($D_s$)
  - Target Modality: Target Dataset ($D_t$) with Labels (Task-level) or without (Unsupervised)
  - Metric Engine: The specific algorithm (e.g., LogME, OTDD, LEEP) calculating the scalar score
  - Decision Gate: A threshold or ranking system to accept/reject the transfer

- **Critical path:**
  1. **Input:** Target batch ($x_t, y_t$) and Source Model ($f_s$)
  2. **Feature Extraction:** Forward pass target data through frozen source backbone to get embeddings ($z_t$)
  3. **Statistical Calculation:** Compute metric (e.g., covariance of embeddings, linear regression error, or distribution distance)
  4. **Output:** Scalar Transferability Score

- **Design tradeoffs:**
  - **Source-Free vs. Source-Dependent:** Source-free metrics (e.g., LogME, LEEP) are faster and respect data privacy but may miss nuances available in raw source data
  - **Pre-hoc vs. Online:** Pre-hoc metrics save training time but require up-front computation. Online metrics (PGE) allow dynamic adjustment but slow down training
  - **Instance vs. Task:** Instance-level (ODIN) allows for sample selection/filtering but is computationally heavier per sample than a single Task-level score

- **Failure signatures:**
  - **Negative Transfer:** High transferability score but actual fine-tuning results in worse performance than training from scratch. (Indicates metric assumption mismatch)
  - **Sensitivity to Scale:** Metrics like H-score rely on matrix inversions that can be unstable with small batch sizes or low-rank features
  - **Saturation:** Metrics flattening out when comparing very similar models, failing to distinguish the best among good options

- **First 3 experiments:**
  1. **Correlation Benchmark:** Select 5 diverse pre-trained models (e.g., ResNet, ViT). Calculate LEEP and LogME scores on a target dataset (e.g., CIFAR-100). Compare metric rankings against actual fine-tuning accuracy (Spearman correlation)
  2. **Dataset Sensitivity:** Using a fixed model, compute OTDD between a source domain and a target domain with synthetic noise added. Verify if the metric degrades linearly with noise or collapses suddenly
  3. **Granularity Check:** Compare Task-level (LogME) vs. Instance-level (ODIN) metrics on an out-of-distribution detection task to see if instance-level metrics provide better filtering for active learning

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can we mathematically quantify the final transferability of sequential transfer ($A \to B \to C$) compared to direct transfer ($A \to C$)?
- **Basis in paper:** [explicit] Section 6.1.1 states the challenge is calculating the difference between direct transfer and using intermediate knowledge.
- **Why unresolved:** Current theoretical studies lack universality and are difficult to realize practically due to dependencies on task sample size, complexity, and distribution distance.
- **What evidence would resolve it:** A theoretical framework or metric that accurately predicts the performance gain of including intermediate tasks $B$ without requiring full training.

### Open Question 2
- **Question:** What is the optimal number of intermediate samples, datasets, or domains for a given target task?
- **Basis in paper:** [explicit] Section 6.1.1 lists "Deciding how many intermediate samples... is optimal" as a primary challenge.
- **Why unresolved:** Existing methods remain theoretical and lack universality, often failing in practical application.
- **What evidence would resolve it:** An algorithm capable of determining the stopping criteria for adding intermediate domains that maximizes performance while minimizing computational cost.

### Open Question 3
- **Question:** How do soft prompts interact with Pre-trained Language Model (PLM) internal neuron activations to drive transferability?
- **Basis in paper:** [explicit] Section 6.1.2 notes that prompt transferability is linked to how prompts stimulate internal neurons, suggesting future research focus here.
- **Why unresolved:** The mechanism of how prompts stimulate specific neurons to enhance cross-task performance is not fully understood.
- **What evidence would resolve it:** Empirical mapping or theoretical modeling of prompt-to-neuron interactions that correlates with improved transfer performance across diverse tasks.

## Limitations

- The survey's theoretical claims rely heavily on distributional bounds that may not fully capture modern transfer learning scenarios involving large language models or complex feature hierarchies.
- Empirical validation of transferability metrics across diverse tasks remains limited, with most studies focusing on vision tasks.
- The comparative effectiveness of different metrics across diverse learning paradigms (few-shot, continual, federated) lacks comprehensive empirical validation.

## Confidence

- **High Confidence:** The categorization framework (knowledge types and measurement granularity) is methodologically sound and provides a useful taxonomy for organizing existing metrics.
- **Medium Confidence:** Theoretical bounds connecting distributional divergence to generalization error are valid under stated assumptions but may not generalize to all modern architectures.
- **Low Confidence:** The comparative effectiveness of different metrics across diverse learning paradigms (few-shot, continual, federated) lacks comprehensive empirical validation.

## Next Checks

1. **Cross-Paradigm Validation:** Test transferability metrics across multiple learning paradigms (vision, NLP, reinforcement learning) to verify the survey's claim of broad applicability.
2. **Metric Robustness Analysis:** Systematically evaluate how each metric performs under distribution shifts, label noise, and adversarial conditions to identify failure modes not covered in the survey.
3. **Theoretical Gap Analysis:** Validate the survey's theoretical claims by implementing and testing the distributional bounds on modern architectures to identify where assumptions break down.