---
ver: rpa2
title: 'The Veln(ia)s is in the Details: Evaluating LLM Judgment on Latvian and Lithuanian
  Short Answer Matching'
arxiv_id: '2501.09164'
source_url: https://arxiv.org/abs/2501.09164
tags:
- answer
- answers
- matched
- wang
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces novel datasets for evaluating large language
  models (LLMs) on short answer matching tasks in Latvian and Lithuanian. The authors
  generated 502 Latvian and 690 Lithuanian question-answer pairs from Wikipedia, then
  created matched and non-matched answers using alteration rules that introduce subtle
  semantic changes.
---

# The Veln(ia)s is in the Details: Evaluating LLM Judgment on Latvian and Lithuanian Short Answer Matching

## Quick Facts
- arXiv ID: 2501.09164
- Source URL: https://arxiv.org/abs/2501.09164
- Reference count: 32
- Key outcome: Novel datasets and benchmarks for Latvian/Lithuanian short answer matching show larger LLMs achieve near-perfect performance, while smaller models show architecture-dependent few-shot benefits

## Executive Summary
This paper introduces novel datasets for evaluating large language models on short answer matching tasks in Latvian and Lithuanian. The authors generated 502 Latvian and 690 Lithuanian question-answer pairs from Wikipedia, then created matched and non-matched answers using alteration rules that introduce subtle semantic changes. A subset was manually verified for quality. The datasets were used to benchmark several LLMs including LLaMa3.1 (8b, 70b), Mistral (7b, 12b), EuroLLM (9b), and QWEN2.5 (7b, 72b) in both zero-shot and few-shot settings. Results showed that larger models like QWEN2.5 72b and LLaMa3.1 70b achieved near-perfect performance in distinguishing matched from non-matched answers.

## Method Summary
The study generated question-answer pairs from Wikipedia articles using Gemini 1.5 Pro, then created matched and non-matched answer variations using GPT-4o and LLaMa3 7b with specific alteration rules. The datasets were manually validated for quality, with GPT-4o showing higher acceptance rates than LLaMa3 7b. Seven LLM variants were evaluated on binary classification tasks (True/False) in zero-shot and few-shot settings. Models were instructed to output "True" or "False" to indicate whether generated answers matched reference answers. The evaluation included F1 score calculation overall and per alteration rule type.

## Key Results
- Larger LLMs (70B+ parameters) demonstrated near-perfect accuracy on both Latvian and Lithuanian short answer matching
- QWEN2.5 7b achieved performance comparable to 70B models in both zero and few-shot experiments
- Few-shot prompting showed architecture-dependent effects: LLaMa3.1 8b and EuroLLM 9b benefited, while Mistral 7b showed degraded performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model scale strongly correlates with semantic discrimination capability on low-resource languages
- Mechanism: Larger models (70B+ parameters) demonstrated near-perfect accuracy on both Latvian and Lithuanian short answer matching, likely due to greater representational capacity for capturing subtle semantic distinctions in underrepresented languages.
- Core assumption: The performance gap reflects genuine capability differences rather than training data overlap.
- Evidence anchors:
  - [abstract] "larger LLMs, such as QWEN2.5 72b and LLaMa3.1 70b, demonstrate near-perfect performance"
  - [results] "larger LLMs (with 70b parameters) are capable of reliably detect matched and non-matched answers"
  - [corpus] Related work on Baltic languages (Localizing AI, LAG-MMLU) confirms scale-dependent performance for low-resource languages
- Break condition: If a 7B model matches 70B performance (as QWEN2.5 7b did), the mechanism may not apply uniformly across architectures.

### Mechanism 2
- Claim: Few-shot prompting benefits are architecture-dependent, not universal
- Mechanism: Some smaller models (LLaMa3.1 8b, EuroLLM 9b) improved significantly with few-shot examples (+9-11%), while others (Mistral 7b, Mistral Nemo 12b) showed degraded or unchanged performance.
- Core assumption: Models with weaker instruction-following or format adherence benefit more from explicit examples.
- Evidence anchors:
  - [abstract] "LLaMa3.1 8b and EuroLLM 9b benefited from few-shot examples, while Mistral Nemo 12b underperformed"
  - [results] "EuroLLM 9b was not able to follow a format at all in ZS settings... However, when presented with a few shot examples, it generated expected format"
  - [corpus] JudgeBoard paper suggests SLMs have inconsistent judging capabilities
- Break condition: When few-shot decreases performance (Mistral 7b: 0.95 ZS → 0.91 FS), the mechanism inverts—suggesting overfitting to examples or context confusion.

### Mechanism 3
- Claim: Alteration rule type creates differential difficulty for model architectures
- Mechanism: Models showed specific weaknesses on certain alteration types—Mistral Nemo struggled with synonyms and exclamatory style in Lithuanian; LLaMa3.1 8b and EuroLLM 9b struggled with added entities in Latvian.
- Core assumption: Different alteration types test different linguistic competencies (lexical vs. syntactic vs. factual).
- Evidence anchors:
  - [results] "Mistral Nemo obtained weaker performance on changing words to synonyms and style swap to exclamatory rules in Lithuanian"
  - [results] "LLaMa3.1 8b and EuroLLM 9b had difficulties with added entities in the text in Latvian"
  - [corpus] No direct corpus evidence on alteration-type sensitivity; this remains underexplored
- Break condition: If performance patterns reverse across languages, the mechanism may reflect data quality rather than model capability.

## Foundational Learning

- Concept: **Short Answer Matching (SAM) as semantic equivalence classification**
  - Why needed here: The task requires determining whether a student answer matches a reference answer—not exact string match, but semantic equivalence despite surface variations.
  - Quick check question: Can you explain why "The capital is Riga" and "Riga is the capital city" should be matched, while "The capital is Vilnius" should not?

- Concept: **Low-resource language challenges in LLMs**
  - Why needed here: Latvian and Lithuanian have limited representation in training corpora, leading to weaker performance and different error patterns than high-resource languages.
  - Quick check question: Why might a model that performs well on English SAM struggle with Latvian, even if the task structure is identical?

- Concept: **Zero-shot vs. few-shot prompting trade-offs**
  - Why needed here: The paper shows few-shot can help or hurt depending on the model—understanding when to use each is critical for deployment.
  - Quick check question: What signal would indicate that few-shot examples are confusing rather than helping a model?

## Architecture Onboarding

- Component map:
  Wikipedia extraction -> Gemini 1.5 Pro for Q&A generation -> GPT-4o/LLaMa3 for alteration generation -> Manual validation -> Model inference (zero/few-shot) -> Binary classification (True/False)

- Critical path:
  1. Dataset quality depends on GPT-4o generation accuracy (LLaMa3 7b failed manual validation for some rules)
  2. Model selection: Start with QWEN2.5 7b or 72b for reliability; avoid Mistral Nemo for Lithuanian
  3. Prompting strategy: Use few-shot only for models with format adherence issues (EuroLLM 9b, LLaMa3.1 8b)

- Design tradeoffs:
  - **GPT-4o vs. LLaMa3 for generation**: GPT-4o had higher acceptance rates in manual validation; LLaMa3 7b struggled with non-matched generation
  - **Dataset size vs. quality**: Manual validation filtered aggressively (see Appendix A acceptance rates); smaller but cleaner dataset
  - **Model scale vs. latency**: 70B models near-perfect but expensive; QWEN2.5 7b achieves comparable results with lower cost

- Failure signatures:
  - Format non-adherence: EuroLLM 9b output legible text but couldn't follow "True"/"False" format in zero-shot
  - Negative bias: LLaMa3.1 8b and EuroLLM 9b showed bias toward "False" predictions, especially on matched answers
  - Language-specific collapse: Mistral Nemo 12b performed well on some alteration types but failed on synonyms/exclamatory in Lithuanian

- First 3 experiments:
  1. **Baseline establishment**: Run zero-shot evaluation on both languages with QWEN2.5 7b to confirm reported ~0.97-0.98 F1 scores
  2. **Few-shot ablation**: Test LLaMa3.1 8b with 0, 1, 3, 5 examples to find optimal example count before performance plateaus or degrades
  3. **Alteration-type analysis**: Isolate Mistral Nemo 12b performance on each alteration rule in Lithuanian to quantify synonym/exclamatory weakness

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on GPT-4o-generated alterations, creating potential contamination risk with model training data
- Manual validation process is underspecified, lacking documentation of criteria, validator count, and inter-rater reliability
- Dataset generation lacks transparency regarding specific prompt templates used for GPT-4o

## Confidence

**High Confidence**: Claims about relative model performance differences (QWEN2.5 72b > LLaMa3.1 70b > smaller models) are well-supported by consistent F1 scores across multiple runs.

**Medium Confidence**: The mechanism explaining few-shot benefits as architecture-dependent requires more evidence, as the sample size and limited exploration of hyperparameters weaken causal claims.

**Low Confidence**: The assertion that alteration rule type creates differential difficulty is based on aggregate patterns rather than systematic analysis, with no control for confounding factors.

## Next Checks

1. **Contamination Analysis**: Conduct an n-gram overlap analysis between the GPT-4o-generated alterations and the pretraining corpora of the evaluated models to quantify risk of memorization versus genuine semantic understanding.

2. **Alternative Generation Validation**: Recreate a subset of the dataset using a different LLM (e.g., Claude 3.5 or GPT-4 Turbo) for alteration generation and compare model performance to assess dataset alignment effects.

3. **Confidence Calibration Study**: Modify the evaluation protocol to collect model confidence scores rather than binary outputs to reveal whether near-perfect performance reflects genuine certainty or potential overconfidence.