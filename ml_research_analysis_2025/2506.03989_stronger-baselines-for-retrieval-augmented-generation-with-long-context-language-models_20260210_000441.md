---
ver: rpa2
title: Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language
  Models
arxiv_id: '2506.03989'
source_url: https://arxiv.org/abs/2506.03989
tags:
- context
- retrieval
- readagent
- token
- raptor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates whether simpler single-stage retrieval-augmented
  generation (RAG) pipelines can match or outperform more complex multi-stage methods
  given modern long-context language models. The authors compare two recent multi-stage
  approaches (ReadAgent and RAPTOR) against simpler baselines including DOS RAG (which
  preserves original document passage order) on three QA benchmarks.
---

# Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models

## Quick Facts
- **arXiv ID**: 2506.03989
- **Source URL**: https://arxiv.org/abs/2506.03989
- **Reference count**: 28
- **Primary result**: Simpler single-stage RAG pipelines (DOS RAG) match or outperform complex multi-stage methods on QA benchmarks

## Executive Summary
This paper challenges the prevailing assumption that complex multi-stage retrieval-augmented generation (RAG) pipelines are necessary for strong performance with long-context language models. Through systematic evaluation on three QA benchmarks, the authors demonstrate that a simple single-stage approach (DOS RAG) consistently matches or exceeds the performance of recent multi-stage methods (ReadAgent and RAPTOR) while using fewer tokens. The findings suggest that preserving original document structure, prioritizing recall, and maintaining simplicity may be more important than pipeline complexity for effective RAG systems.

## Method Summary
The paper evaluates three RAG approaches: ReadAgent (selective reading with multi-stage reasoning), RAPTOR (multiple retrieval stages with query refinement), and DOS RAG (single-stage retrieval preserving original passage order). All methods use retrieval followed by generation, but differ in their retrieval strategies and whether they preserve original document structure. The evaluation covers three benchmarks (NaturalQuestions, HotpotQA, ∞Bench) with multiple long-context models (GPT-4o, Gemini 1.5 Pro, Claude 3.5 Sonnet) and varying retrieval token budgets up to 128K tokens.

## Key Results
- DOS RAG consistently matches or outperforms ReadAgent and RAPTOR across all three benchmarks
- On ∞Bench with GPT-4o, DOS RAG achieves 93.1% accuracy at 30K tokens, outperforming ReadAgent by 2.8 points with one-third the tokens
- The performance gap increases with larger retrieval budgets, where DOS RAG's simpler approach becomes more effective
- Results hold across different model variants and benchmark types, suggesting generalizability

## Why This Works (Mechanism)
DOS RAG's effectiveness stems from four key factors: (1) retrieving from original passages preserves source fidelity and avoids information loss from compression, (2) prioritizing recall within effective context windows ensures relevant information is captured, (3) maintaining document structure through passage reordering preserves contextual relationships, and (4) the simpler pipeline reduces complexity that can introduce errors in multi-stage approaches.

## Foundational Learning
**Retrieval-Augmented Generation (RAG)**: Combines information retrieval with text generation to answer questions using external knowledge. *Why needed*: Enables language models to access up-to-date information beyond their training data. *Quick check*: Can answer questions using retrieved documents without retraining.

**Long-context Language Models**: Models capable of processing 100K+ tokens in a single pass. *Why needed*: Allows retrieval of large document collections without compression. *Quick check*: Can attend to context windows exceeding typical 2-4K token limits.

**Passage Reordering**: Restoring original document structure after retrieval. *Why needed*: Maintains contextual relationships that may be lost in bag-of-passages retrieval. *Quick check*: Documents read in original order vs shuffled order affect comprehension.

## Architecture Onboarding

**Component Map**: Query -> Retrieval -> Passage Reordering -> Generation

**Critical Path**: The retrieval stage determines which documents are available for generation, making it the most critical component. DOS RAG's single retrieval pass contrasts with multi-stage approaches that iteratively refine queries and retrieve additional documents.

**Design Tradeoffs**: Single-stage (DOS RAG) favors simplicity and document fidelity vs multi-stage (ReadAgent, RAPTOR) favoring iterative refinement but risking information loss through compression and added complexity.

**Failure Signatures**: Multi-stage methods may fail due to: (1) query drift during refinement, (2) information loss from passage compression, (3) context window overflow with excessive retrieval. DOS RAG may fail due to: (1) insufficient recall from single pass, (2) irrelevant document inclusion.

**3 First Experiments**:
1. Compare performance of original passage retrieval vs compressed passages on the same benchmark
2. Test retrieval with and without passage reordering to isolate document structure effects
3. Measure token efficiency (accuracy per token) across different retrieval budgets

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to three specific benchmarks (NaturalQuestions, HotpotQA, ∞Bench) which may not generalize to all RAG scenarios
- Does not explore open-domain or conversational settings where multi-stage approaches might show advantages
- Computational efficiency analysis limited to token counts rather than actual inference time or resource usage

## Confidence

**High confidence**: DOS RAG's competitive performance across benchmarks (empirical results are robust and statistically significant)

**Medium confidence**: Attribution of performance to specific design choices (reasonable but not conclusively proven through causal analysis)

**Medium confidence**: Generalizability of findings to all RAG scenarios (evaluation scope limited to three benchmarks)

## Next Checks
1. Test DOS RAG on additional benchmarks including open-domain QA and conversational datasets to assess generalizability beyond the current three benchmarks
2. Conduct ablation studies isolating each proposed advantage (passage preservation, recall prioritization, document structure) to establish causal relationships
3. Measure actual inference time and memory usage to complement the token efficiency analysis with real-world computational costs