---
ver: rpa2
title: Toward Zero-Shot User Intent Recognition in Shared Autonomy
arxiv_id: '2501.08389'
source_url: https://arxiv.org/abs/2501.08389
tags:
- robot
- shared
- user
- autonomy
- intent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Vision-Only Shared Autonomy (VOSA), a zero-shot
  shared autonomy framework that leverages end-effector vision to infer human intent
  without prior knowledge of manipulation tasks. VOSA uses a single wrist-mounted
  RGBD camera to dynamically perceive objects, segment them via clustering, and infer
  user goals through confidence-weighted arbitration between human and robot control.
---

# Toward Zero-Shot User Intent Recognition in Shared Autonomy

## Quick Facts
- **arXiv ID**: 2501.08389
- **Source URL**: https://arxiv.org/abs/2501.08389
- **Reference count**: 40
- **Primary result**: Zero-shot shared autonomy framework using wrist-mounted vision to infer human intent without prior task knowledge.

## Executive Summary
This paper introduces Vision-Only Shared Autonomy (VOSA), a zero-shot shared autonomy framework that leverages end-effector vision to infer human intent without prior knowledge of manipulation tasks. VOSA uses a single wrist-mounted RGBD camera to dynamically perceive objects, segment them via clustering, and infer user goals through confidence-weighted arbitration between human and robot control. In a user study (N=18), VOSA matched the performance of an oracle baseline model and significantly outperformed direct teleoperation in terms of task completion time and input magnitude across pick-and-place, deceptive grasping, and shelving tasks.

## Method Summary
VOSA implements a three-module pipeline for zero-shot shared autonomy: (1) Perception: YOLOv5 detects object count k, k-means clusters preprocessed point cloud, centroids = intents G; (2) Prediction: confidence c(t,g) = 0.3·(uh·ur,g) + 0.7·e^(-d(g)), select highest-confidence intent; (3) Arbitration: blend u(t) = (1-α)·uh + α·ur, with α ∈ [0, 0.8] as function of confidence. The system operates on RGBD data from a wrist-mounted Realsense D435i camera, with human input via Xbox 360 controller and end-effector speed set to 5 cm/s.

## Key Results
- VOSA matched oracle baseline performance in task completion time and input magnitude
- VOSA significantly outperformed direct teleoperation across all objective metrics
- System was particularly effective in realistic settings with unknown or changing intents, achieving higher user satisfaction with less effort

## Why This Works (Mechanism)

### Mechanism 1
Real-time visual segmentation enables zero-shot adaptation to unknown environments by dynamically constructing the goal set $G$. A wrist-mounted RGBD camera streams data to a YOLOv5 model to detect object count $k$. Point cloud preprocessing removes static background (tables/walls), and k-means clustering segments the remaining points into $k$ discrete objects. The centroid of each cluster is treated as a potential intent $g \in G$.

### Mechanism 2
Intent confidence increases when human input vectors align with the robot's predicted vector toward a goal. The system calculates a confidence score $c(t, g)$ for each potential goal $g$. This score relies on two factors: the alignment of the human's joystick input $u_h$ with the robot's vector $u_{r,g}$ (dot product), and the inverse distance to the goal (exponential decay). High alignment and proximity trigger arbitration.

### Mechanism 3
Linear policy blending reduces physical effort while maintaining agency by scaling robot authority proportionally to prediction confidence. The final robot action $u(t)$ is a weighted sum: $(1 - \alpha)u_h + \alpha u_r$. The mixing factor $\alpha$ is a function of the maximum confidence $c(t)$. The system caps $\alpha$ at 0.8, ensuring the human always retains at least 20% control authority to override errors.

## Foundational Learning

- **Concept: Shared Autonomy (Policy Blending)**
  - **Why needed here**: The core contribution is not full automation, but a specific mathematical blending of $u_h$ and $u_r$. Understanding that $\alpha$ is the crucial variable defining the "amount" of help.
  - **Quick check question**: If $\alpha = 0.5$ and the human pushes Left while the robot pushes Right, what happens?

- **Concept: Point Cloud Processing & Segmentation**
  - **Why needed here**: The "zero-shot" capability relies on converting raw RGBD pixels into actionable 3D coordinates ($g \in \mathbb{R}^3$). Without understanding clustering, the system appears to be magic.
  - **Quick check question**: Why does the system filter out the "table" plane before clustering?

- **Concept: Vector Alignment (Dot Product)**
  - **Why needed here**: The prediction module (Eq. 2) uses $u_h(t) \cdot u_{r,g}(t)$ to measure intent. A learner must understand that aligned vectors yield high positive values, while opposed vectors yield negative values.
  - **Quick check question**: If the robot wants to move forward $(0,1)$ and the user pulls back $(0,-1)$, what is the dot product and how does it affect confidence?

## Architecture Onboarding

- **Component map**: RGBD Camera -> YOLOv5 (Count) -> PCL Filter -> K-Means (Centroids) -> Confidence Calculation -> Linear Blending -> Kinova Driver
- **Critical path**: The loop frequency is determined by the **Perception** pipeline. If YOLO inference + clustering takes >100ms, the arbitration will lag behind the user's joystick movements, causing "sloppy" control.
- **Design tradeoffs**: Wrist-mounted (Eye-in-Hand) allows dynamic updates but suffers from blurring/motion sickness during movement; requires the "Active Sensing" state to stop and look. Fixed $\alpha$ max (0.8) guarantees safety/correction capability but limits maximum efficiency gains.
- **Failure signatures**: "Flickering" Goals (objects detected, then lost, then detected), "Tug-of-War" (robot drifts toward Object A while User pushes Object B).
- **First 3 experiments**: (1) Perception Static Test - verify cluster detection and centroid drift; (2) Prediction Latency Test - ensure <50ms from capture to $u_r$ generation; (3) Alignment Verification - plot confidence $c(t)$ for different goals to verify separation.

## Open Questions the Paper Calls Out
- Can integrating natural language interfaces or historical demonstration data into the prediction module improve intent inference accuracy without compromising VOSA's zero-shot capabilities?
- How does learning arbitration strategies via deep reinforcement learning compare to the fixed confidence-based blending function when adapting to diverse user expertise levels?
- To what extent can vision-language models or depth-image template matching mitigate the perception module's current sensitivity to lighting conditions and object detection failures?

## Limitations
- System's zero-shot capability critically depends on YOLOv5's robustness to object count detection, with no ablation study isolating failure modes
- Fixed arbitration cap (α=0.8) prevents achieving maximum efficiency gains that a perfect predictor would enable
- Limited generalization evidence to non-tabletop environments without recalibration

## Confidence
- **High confidence**: Task completion time and input magnitude improvements over direct teleoperation (N=18, objective metrics)
- **Medium confidence**: Subjective user satisfaction scores (7-point Likert scale with small sample size, potential bias from novelty effect)
- **Low confidence**: Generalization to non-tabletop environments (kitchen counters, shelves) without recalibration of point cloud filtering thresholds

## Next Checks
1. **Ablation Study**: Remove the arbitration cap (allow α=1.0) and measure if task completion time decreases further, accepting the risk of reduced user agency
2. **Cross-Environment Test**: Deploy VOSA on a cluttered kitchen countertop with mixed object types (e.g., cups, plates, utensils) to verify zero-shot adaptation beyond controlled tabletop settings
3. **Latent Intent Analysis**: Track how often the system's top-ranked goal changes mid-task (indicative of false positives) versus the oracle baseline to quantify prediction stability