---
ver: rpa2
title: Building Effective Safety Guardrails in AI Education Tools
arxiv_id: '2508.05360'
source_url: https://arxiv.org/abs/2508.05360
tags:
- content
- safety
- education
- lesson
- aila
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper describes the development and evaluation of safety guardrails
  for Oak National Academy''s AI-powered lesson planning assistant (Aila), a generative
  AI tool for creating curriculum-aligned lessons for students aged 5-16. The authors
  implemented four safety measures: prompt engineering with pedagogical parameters,
  input threat detection, an Independent Asynchronous Content Moderation Agent (IACMA)
  to categorize outputs into safe, content guidance, or toxic categories, and a human-in-the-loop
  approach.'
---

# Building Effective Safety Guardrails in AI Education Tools

## Quick Facts
- arXiv ID: 2508.05360
- Source URL: https://arxiv.org/abs/2508.05360
- Reference count: 37
- Primary result: Four-layer safety system successfully blocks toxic content in AI lesson planning while accepting false positives to prioritize child safety

## Executive Summary
This paper describes the development and evaluation of safety guardrails for Oak National Academy's AI-powered lesson planning assistant (Aila), which generates curriculum-aligned lessons for students aged 5-16. The authors implemented a four-layer safety system including prompt engineering, input threat detection, an Independent Asynchronous Content Moderation Agent (IACMA), and human-in-the-loop review. Through systematic pre- and post-launch evaluation, they found the system effectively blocked harmful content while highlighting challenges with edge cases, current events, and topics outside the national curriculum. The paper emphasizes the need for ongoing iteration, cross-sector collaboration, and open sharing of safety approaches in generative AI education tools.

## Method Summary
The safety system employs four guardrails: (1) extensive prompt engineering encoding pedagogical parameters and national curriculum constraints, (2) input threat detection layer blocking malicious prompts before LLM processing, (3) IACMA using a separate LLM to categorize outputs into safe, content guidance, or toxic categories with 5-point Likert scoring, and (4) teacher review as human-in-the-loop. The IACMA was designed to be oversensitive, preferring false positives over false negatives. Pre-launch testing used red-teaming against HarmBench framework and alignment testing on 10,000+ human-created lessons. Post-launch evaluation analyzed 45,000 user-generated lessons and 1,000+ illustrative bulk-generated lessons.

## Key Results
- Four-layer safety system successfully blocked toxic content while generating age-appropriate lessons aligned with national curriculum
- Post-launch analysis revealed edge cases with current events and topics outside curriculum requiring further refinement
- System showed 100% detection of obvious toxic content but generated false positives requiring teacher overrides
- Real-world usage produced more conservative content than illustrative dataset, validating oversensitive design approach

## Why This Works (Mechanism)
The system works through defense-in-depth architecture where each guardrail addresses different failure modes. The prompt engineering establishes explicit constraints that guide the LLM's token generation process. The independent IACMA provides an unbiased second opinion separate from the generation context. The human-in-the-loop ensures final content meets classroom standards. The oversensitive design prioritizes child safety by accepting more false positives to eliminate false negatives. The chunk-based moderation during generation allows early detection of toxic content before full lesson completion.

## Foundational Learning
- Concept: **Prompt Engineering as Constraint Satisfaction**.
  - Why needed here: The paper's first guardrail is an extensive system prompt. This concept frames prompt engineering not as "chatting" with the AI, but as defining a complex set of rules and constraints that the model must satisfy during token generation.
  - Quick check question: Can you articulate how a well-defined "lesson design rubric" translates into a set of explicit rules and constraints for an LLM?
- Concept: **Defense in Depth for AI Systems**.
  - Why needed here: The four guardrails (prompt, input detection, moderation agent, human) are not redundant but layered. Each addresses a different potential failure mode. Understanding this layered approach is critical for building robust systems.
  - Quick check question: Why is a content moderation agent *independent* of the main generation prompt considered a stronger safety measure than one integrated into the same context?
- Concept: **Trade-off Between Sensitivity and Specificity**.
  - Why needed here: The IACMA is explicitly designed to be "oversensitive," accepting false positives (blocking safe lessons) to minimize false negatives (allowing toxic lessons). This is a core engineering trade-off.
  - Quick check question: In the context of a tool for 5-16 year olds, explain why a safety system designer would choose high sensitivity at the cost of user experience?

## Architecture Onboarding
- Component map: User Input -> Input Threat Detection -> Main Application (Aila) -> IACMA -> (Content guidance warning or toxic block) -> Teacher Review -> Final Lesson
- Critical path: A user prompt enters the system -> Input Threat Detection checks for attacks (threat? -> flag/block) -> Main Application (Aila) generates lesson content section-by-section -> IACMA evaluates each section against safety categories -> (Toxic? -> block session; Content guidance? -> show warning) -> Content displayed to teacher for review -> Teacher approves/edits/download
- Design tradeoffs:
  - **Oversensitivity vs. User Experience**: The system is designed to err on the side of caution, potentially blocking safe lessons or showing unnecessary warnings to ensure harmful content is never delivered.
  - **Automated vs. Human Moderation**: Full automation is faster but lacks nuance. Full human moderation is slower and less scalable. The system uses automation (IACMA) as a first-pass filter and humans (teachers) as the final arbiter.
  - **Chunk-based vs. Full-lesson Moderation**: Moderating in chunks during generation can differ from moderating a complete lesson post-hoc, as context evolves. The evaluation process had to be adapted to mimic the real-time chunk-based moderation.
- Failure signatures:
  - **False Positives**: Legitimate lessons on sensitive but curriculum-aligned topics (e.g., "weapons of mass destruction" in a Religious Studies context) being incorrectly flagged as toxic.
  - **Edge Cases**: Novel current events not in the LLM's training data, or topics outside the national curriculum but still taught in schools, leading to unpredictable model behavior.
  - **Context Drift**: The initial sections of a lesson triggering a block before the full, clarifying context is generated.
  - **Automation Bias**: Teachers accepting AI-generated content without critical review.
- First 3 experiments:
  1. **Red-Team the IACMA**: Use the provided open-source code to create a dataset of adversarial lessons designed to bypass the "Safe" classification, specifically testing edge cases at the boundaries of "Content guidance" and "Toxic" categories. Measure false negative rates.
  2. **Prompt Sensitivity Analysis**: Systematically vary the main system prompt's instructions around a known edge case (e.g., a recent conflict) and measure changes in the IACMA's categorization of generated lessons. This tests the coupling between the prompt and moderation agent.
  3. **Latency vs. Safety Analysis**: Measure the end-to-end latency impact of the IACMA running in "chunks" versus on the full lesson content. Quantify the user experience cost of the current "oversensitive" approach by logging false positive rates over 1000 real user sessions.

## Open Questions the Paper Calls Out
- **Open Question 1**: How can safety guardrails effectively handle edge cases involving current events that occurred after the LLM's training data cutoff?
  - Basis in paper: [explicit] The paper identifies "edge cases, such as lessons based on current affairs that occurred after the latest data that the underlying LLM has been trained on (e.g. the 2024 American election and recent conflicts)" as challenges requiring further work.
  - Why unresolved: The paper states "Further work will focus on our steps to address the edge cases (examples mentioned above) and additional tests on these changes" but no solution is yet proposed.
  - What evidence would resolve it: A documented approach with empirical results showing effective moderation of post-cutoff current events content with comparable safety performance to established curriculum topics.

- **Open Question 2**: How can AI-based moderation systems achieve consistent categorization when content is evaluated at different stages of lesson generation versus as a complete lesson?
  - Basis in paper: [inferred] The paper describes that when running IACMA on completed lessons, "the moderation category was often misaligned with the original moderation" because real-time flagging may block lessons before completion, while post-hoc evaluation of full lessons may yield different categories.
  - Why unresolved: The paper notes workarounds (running moderation in chunks to replicate real usage) but the fundamental inconsistency between sequential and complete-lesson evaluation remains an evaluation challenge.
  - What evidence would resolve it: Empirical analysis comparing categorization consistency across moderation stages, or a validated methodology for reconciling discrepancies.

- **Open Question 3**: What is the optimal trade-off between oversensitive moderation (prioritizing safety) and minimizing false positives that disrupt user experience?
  - Basis in paper: [explicit] The paper states "We have designed our IACMA to be oversensitive. This may result in the 'content guidance' warning being shown and lessons being blocked as 'toxic' when not necessary" and notes that improving specificity while maintaining sensitivity is an ongoing goal.
  - Why unresolved: The paper acknowledges this is unresolved and states "As we continue to evaluate and iterate, we will improve the specificity of this agent to ensure that we minimise false positives."
  - What evidence would resolve it: Systematic evaluation measuring false positive rates under different sensitivity thresholds alongside user experience and safety outcome metrics.

- **Open Question 4**: How should safety guardrails evolve to effectively moderate multimodal content (imagery, audio) as AI education tools expand beyond text-only inputs and outputs?
  - Basis in paper: [explicit] The paper states "Aila is currently unimodal (with only text as the input and output). As it develops to incorporate other mediums (including, for example, imagery and audio), our safety processes will need to evolve to include these."
  - Why unresolved: The paper identifies this as future work with no current solution or proposed framework.
  - What evidence would resolve it: Documented safety architectures and evaluation benchmarks demonstrating effective moderation of multimodal educational AI content across age-appropriate categories.

## Limitations
- The IACMA's context-unaware design creates fundamental alignment challenges when moderating completed lessons versus real-time chunk-based generation
- The oversensitive approach generates significant false positives that may undermine teacher trust and adoption
- System performance on novel current events and topics outside the national curriculum remains largely unknown
- Paper does not provide quantitative metrics on the actual impact of safety measures on user experience or adoption rates

## Confidence
- High confidence: The four-layer guardrail architecture and its general effectiveness in preventing obvious toxic content
- Medium confidence: The IACMA's categorization accuracy and the system's ability to handle edge cases
- Low confidence: Long-term effectiveness with real-world usage patterns and the balance between safety oversensitivity and teacher adoption

## Next Checks
1. Implement A/B testing comparing chunk-based moderation vs. full-lesson moderation to quantify classification consistency and identify specific contexts where drift occurs
2. Deploy real-time logging of false positive rates and teacher override actions across 10,000+ user sessions to measure the practical impact of oversensitivity on adoption
3. Create adversarial testing framework targeting IACMA's subcategory boundaries to systematically measure false negative rates for each content guidance and toxic category