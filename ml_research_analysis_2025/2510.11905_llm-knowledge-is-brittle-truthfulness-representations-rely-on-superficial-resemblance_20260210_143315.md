---
ver: rpa2
title: 'LLM Knowledge is Brittle: Truthfulness Representations Rely on Superficial
  Resemblance'
arxiv_id: '2510.11905'
source_url: https://arxiv.org/abs/2510.11905
tags:
- representations
- probe
- perplexity
- knowledge
- 'true'
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that LLM knowledge representations are
  brittle and heavily dependent on superficial resemblance to pre-training data. The
  authors show that truthfulness separability degrades as statements become increasingly
  out-of-distribution (OOD), regardless of model family or probing method.
---

# LLM Knowledge is Brittle: Truthfulness Representations Rely on Superficial Resemblance

## Quick Facts
- arXiv ID: 2510.11905
- Source URL: https://arxiv.org/abs/2510.11905
- Reference count: 38
- LLMs rely on superficial resemblance to training data for truthfulness representations

## Executive Summary
This paper demonstrates that LLM knowledge representations are brittle and heavily dependent on superficial resemblance to pre-training data. The authors show that truthfulness separability degrades as statements become increasingly out-of-distribution (OOD), regardless of model family or probing method. Across four datasets and four model families, including Llama 3.1 and OLMo models, probe performance consistently drops as perplexity increases, indicating that internal representations collapse when samples diverge from training distribution. Even correctly answered benchmark questions do not exhibit more robust representations. Surprisingly, topic robustness is not explained by pre-training coverage.

## Method Summary
The study uses three probing methods (non-linear MLP, linear logistic regression, and P(True) token probability) to evaluate whether LLM internal representations can distinguish true from false statements across four datasets. Activations are extracted from final token residual streams at multiple layers. Transformations including typos, Yoda-style word order, punctuation changes, negation, and translation are applied to drive samples out-of-distribution. The primary metric is probe AUC regressed against average perplexity, with robustness quantified as the standardized regression slope (β). More negative β indicates less robust representations.

## Key Results
- Probe performance consistently drops as perplexity increases across all model families and probing methods
- Correctly answered benchmark questions show no more robust representations than incorrectly answered ones
- Translation degrades truthfulness separability more than other transformations despite minimal perplexity increase
- Topic robustness cannot be explained by pre-training coverage

## Why This Works (Mechanism)

### Mechanism 1: Perplexity-Dependent Separability
The model does not encode truthfulness as an abstract, invariant feature. Instead, it relies on surface-level patterns seen during pre-training. As the input text diverges from these patterns (indicated by higher perplexity), the internal activation space for "true" and "false" collapses, making them linearly inseparable.

### Mechanism 2: Surface-Form Dependency (Transformation Sensitivity)
The model's "truth detector" is sensitive to specific tokens or syntactic structures. When these are disturbed (e.g., via "Yoda speak" or typos), the representation shifts such that the direction corresponding to "truth" is no longer activated, even if the semantic content is identical.

### Mechanism 3: Behavioral-Representational Disconnect
The model may utilize heuristics or shortcuts to select the correct token during generation, but the underlying residual stream activations remain brittle. Correctly answered questions are just as susceptible to representation collapse under distribution shift as incorrect ones.

## Foundational Learning

- **Concept: Distribution Shift & OOD (Out-of-Distribution)**
  - Why needed here: The paper's central thesis relies on measuring how model performance changes when inputs deviate from training data. Without understanding OOD, "perplexity" and "superficial resemblance" are undefined.
  - Quick check question: If a model is trained on English text and tested on "Yoda speak," is this an in-distribution or OOD task?

- **Concept: Linear Probes & Representation Separability**
  - Why needed here: The paper measures "knowledge" not by output, but by whether a classifier can separate true/false activation vectors. This requires understanding that internal states, not just outputs, encode information.
  - Quick check question: What does it mean if the AUC of a probe drops to 0.5 (random chance) on perturbed data?

- **Concept: Perplexity as a Proxy**
  - Why needed here: The authors use perplexity to quantify "superficial resemblance." You must understand that low perplexity implies the model "expected" that text (it resembles training data), while high perplexity implies surprise.
  - Quick check question: Does a high perplexity score indicate an input is likely in-distribution or out-of-distribution?

## Architecture Onboarding

- **Component map:** Input Layer (Raw statements) -> Transformation Engine (Typos, Yoda, Translation) -> Model Core (Llama/Gemma/OLMo) -> Residual Stream (Final token activations) -> Evaluation (Perplexity Calculator, Probe Classifiers, Output Method)

- **Critical path:** Calculate Perplexity -> Extract Activations -> Train Probes -> Regress AUC against Perplexity (calculate β slope)

- **Design tradeoffs:**
  - Linear vs. Non-linear Probes: Linear probes offer interpretability (geometry of truth); non-linear may capture complex boundaries but are harder to interpret
  - Translation vs. Noise: Translation tests semantic robustness across languages; noise (typos) tests tolerance to error while keeping language constant

- **Failure signatures:**
  - Steep Negative Slope (β): High brittleness. Representation collapses quickly as perplexity rises
  - Parallel Slopes (Correct vs. Full): Confirms that behavioral correctness does not rescue representation robustness

- **First 3 experiments:**
  1. Baseline Separability: Train probes on original True/False statements to establish max AUC
  2. Perturbation Sweep: Apply increasing intensity of typos/punctuation noise to drive up perplexity and record AUC degradation
  3. Semantic Stress Test: Translate statements to Spanish/French and compare probe AUC drop against the perplexity change to verify if "surface form" or "OOD-ness" is the dominant factor

## Open Questions the Paper Calls Out

### Open Question 1
What factors determine why some topics (e.g., marketing, sociology) have more robust truthfulness representations than others (e.g., history), given that pre-training coverage does not explain this variation? The authors found robustness varies by topic but could not identify the mechanism; perplexity and n-gram counts did not account for the differences.

### Open Question 2
What training interventions or data quality improvements can produce more robust knowledge representations that generalize across distribution shifts? The paper demonstrates brittleness but does not test remediation strategies; it only notes that increasing data quantity is insufficient.

### Open Question 3
Why does translation dramatically degrade truthfulness separability while leaving perplexity relatively unchanged, unlike other transformations? Translation uniquely disrupts representations without shifting the perplexity-based OOD metric, suggesting perplexity may miss certain types of distributional shift.

## Limitations
- The relationship between perplexity and representational collapse remains unvalidated empirically
- Alternative explanations for brittleness (e.g., activation space compression) warrant consideration
- Topic coverage analysis may miss relevant training patterns due to coarse-grained topic matching

## Confidence

**High confidence:** The empirical observation that probe AUC consistently degrades with increasing perplexity across multiple model families, datasets, and probing methods.

**Medium confidence:** The claim that this degradation reflects "superficial resemblance" dependency rather than fundamental knowledge absence.

**Low confidence:** The interpretation that topic robustness is "not explained by pre-training coverage."

## Next Checks

1. Compute perplexities using multiple models (including non-LLM baselines) on the same transformed statements to verify that reported perplexity increases genuinely reflect distributional shift.

2. Systematically vary probe complexity and training regimes to determine whether the observed brittleness persists across different classification approaches.

3. Apply dimensionality reduction techniques to high-perplexity activation spaces to determine whether truthfulness information remains present but becomes linearly inseparable.