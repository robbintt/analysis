---
ver: rpa2
title: Explainable deep learning improves human mental models of self-driving cars
arxiv_id: '2411.18714'
source_url: https://arxiv.org/abs/2411.18714
tags:
- mental
- explanation
- explanations
- concept
- cw-net
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce the Concept-Wrapper Network (CW-Net), a method
  to explain deep neural network planners in self-driving cars by grounding their
  reasoning in human-interpretable concepts. CW-Net replaces the final reward layer
  of a pretrained planner with a concept classifier followed by a new reward layer,
  preserving driving performance while enabling interpretable explanations.
---

# Explainable deep learning improves human mental models of self-driving cars

## Quick Facts
- arXiv ID: 2411.18714
- Source URL: https://arxiv.org/abs/2411.18714
- Reference count: 40
- One-line primary result: CW-Net improves human mental models of AV behavior in surprising situations while preserving driving performance

## Executive Summary
The paper introduces CW-Net, a method to explain deep neural network planners in self-driving cars by grounding their reasoning in human-interpretable concepts. CW-Net replaces the final reward layer of a pretrained planner with a concept classifier followed by a new reward layer, preserving driving performance while enabling interpretable explanations. Deployed on a real self-driving car, CW-Net improved drivers' mental models in surprising situations (e.g., unexpected stops near vehicles or cyclists), allowing them to better understand and predict AV behavior. Online studies with experts and non-experts confirmed that CW-Net explanations enhanced mental model goodness and predictive accuracy. Further deployment on public roads and a large-scale situational awareness study showed robust improvements in perception, comprehension, and projection during anomalous events, with no degradation in normal driving.

## Method Summary
CW-Net wraps a pretrained IRL planner by replacing its final reward layer with a concept classifier and new reward layer. The method trains concept classifiers on scene-trajectory embeddings from the frozen black-box planner, enabling explanations that are causally faithful (since concepts directly determine rewards). The architecture uses a hierarchical vector transformer for scene encoding and an RNN for trajectory encoding, with the concept classifier and new reward layer trained jointly while keeping the backbone frozen.

## Key Results
- CW-Net maintained driving performance with <1% difference in L2 error, collision rate, and progress metrics
- Mental model goodness and prediction accuracy improved significantly in surprising situations across expert and non-expert groups
- SAGAT situational awareness measures showed large improvements in perception and comprehension (d=0.6-1.3) for surprising events, with no degradation in normal driving

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing the reward layer with a concept classifier followed by a new reward layer produces causally faithful explanations without degrading planner performance.
- Mechanism: The concept classifier C(z_i)→c_i computes interpretable concepts (e.g., "Close to cyclist") from scene-trajectory embeddings. Since R' computes trajectory rewards directly from c_i, the concept assignments are the sole input to final decisions—making them causally faithful rather than post-hoc rationalizations.
- Core assumption: The pretrained encoder (H, G, E) has already learned separable representations that can be mapped to human-interpretable concepts via a shallow classifier.
- Evidence anchors:
  - [abstract] "CW-Net accomplishes this level of intelligibility while providing explanations which are causally faithful and do not sacrifice driving performance."
  - [section 8.2] "During training, the rest of the deep neural network (H, G, and E) is kept frozen."
  - [corpus] Weak direct evidence—neighbor papers focus on planning/testing, not explainability mechanisms.
- Break condition: If concept classification accuracy is very low (e.g., F1 < 0.1) and recall is imbalanced, explanations may highlight model confusion rather than faithful reasoning (observed with BIKE concept: F1 ≈ 0.00).

### Mechanism 2
- Claim: Concept-based explanations improve human mental models specifically in surprising or anomalous situations.
- Mechanism: Explanations reveal the AV's internal reasoning (e.g., stopping due to "Close to another vehicle" rather than a nearby pickup zone). This allows users to form counterfactual predictions ("If I move away from parked cars, it should start"), test them, and refine their mental model through prediction-error correction.
- Core assumption: Users are motivated and able to attend to explanations during anomalous events; cognitive load does not overwhelm interpretation.
- Evidence anchors:
  - [section 5.1] When driver moved car away from parked vehicles, CLOSE probability decreased and AV moved—confirming the explanation-driven hypothesis.
  - [section 6.1] Mental model improvement on nearest-neighbor task correlated with prediction accuracy improvements (β = 2.02–9.86 across groups).
  - [corpus] Neighbor paper "What's Happening" aligns on human-centered interpretation but lacks causal architecture evidence.
- Break condition: If explanations are presented during high-workload normal driving without surprise, situational awareness may not improve (validated: no degradation in unsurprising events).

### Mechanism 3
- Claim: Improved mental models transfer to better situational awareness (perception, comprehension, projection) in naturalistic settings.
- Mechanism: SAGAT framework measures three levels: perception (input identification), comprehension (reasoning), and projection (counterfactual prediction). CW-Net explanations improved all three in surprising events with medium-to-large effect sizes (Cohen's d = 0.6–1.3), suggesting mental model improvements generalize beyond the original scenarios.
- Core assumption: Projection accuracy in SAGAT is a valid proxy for mental model quality, which the paper validates via correlation with direct mental model measures.
- Evidence anchors:
  - [section 6.2] Large effect sizes for perception (d = 1.29) and comprehension (d = 0.996); medium for projection (d = 0.606) in surprising events.
  - [abstract] "Online studies with experts and non-experts confirmed that CW-Net explanations enhanced mental model goodness and predictive accuracy."
  - [corpus] No direct corpus validation of SAGAT-for-XAI methodology.
- Break condition: If scenarios become too complex or concept vocabulary is insufficient, explanation utility may degrade without new concept labels.

## Foundational Learning

- Concept: Inverse Reinforcement Learning (IRL)
  - Why needed here: The black-box planner is trained via IRL to imitate human driving, producing a reward function that scores trajectories by "human-likeness." Understanding this clarifies why replacing the reward layer preserves behavior.
  - Quick check question: Can you explain why IRL produces a reward function rather than a direct policy, and how CW-Net leverages this structure?

- Concept: Concept Bottleneck Models
  - Why needed here: CW-Net uses concept bottlenecks to force intermediate predictions through human-interpretable concepts, grounding explanations in the actual decision pathway.
  - Quick check question: How does a concept bottleneck differ from a post-hoc attribution method like SHAP or LIME in terms of causal faithfulness?

- Concept: Situational Awareness (Endsley's three levels)
  - Why needed here: The paper uses SAGAT to measure whether explanations improve perception, comprehension, and projection. Understanding this framework is essential for interpreting the human-subject results.
  - Quick check question: What is the difference between comprehension and projection in Endsley's framework, and which did CW-Net affect most strongly?

## Architecture Onboarding

- Component map: Scene context -> H -> h -> E -> z_i -> C -> c_i -> R' -> r'_i -> argmax trajectory
- Critical path: Scene context → H → h → E → zᵢ → C → cᵢ → R' → r'ᵢ → argmax trajectory. The explanation path (cᵢ) is not a side channel—it is the only path to reward computation.
- Design tradeoffs:
  - Causal vs. parallel architecture: The paper also tested a parallel classifier (Figure S1) that produces post-hoc explanations without modifying the reward path. Simpler to implement but potentially less faithful.
  - Concept vocabulary: Limited to 8–10 labeled concepts. Expanding requires new labeled data; unsupervised discovery is noted as future work.
  - Classification accuracy vs. utility: Low-precision concepts (e.g., PEDESTRIAN: precision 0.03, recall 0.84) can still be useful for surfacing model failures.
- Failure signatures:
  - Concept never activates (e.g., BIKE F1 ≈ 0.00): Indicates the encoder lacks relevant representations or concept is poorly defined.
  - Concept activates inappropriately (e.g., ASV fires without a stopped vehicle): Reveals hallucination or distribution shift—still diagnostically useful.
  - No user mental model change after explanation: Check cognitive load, explanation timing, or concept comprehensibility.
- First 3 experiments:
  1. Reproduce driving performance equivalence on nuPlan: Wrap an existing IRL planner with CW-Net and verify <1% metric difference (L2 error, collision rate, progress).
  2. Validate concept classification on held-out data: Train on labeled scenarios, measure accuracy/precision/recall per concept; identify weak concepts.
  3. Small-N mental model study: Show 5–10 users surprising/un-surprising video pairs with/without explanations; measure prediction accuracy and confidence shift before/after explanation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can CW-Net be extended to learn human-interpretable concepts in an unsupervised manner to mitigate the bottleneck of manual data labeling?
- Basis in paper: [explicit] The Conclusion states: "In future work, it would be prudent to extend CW-Net to cover a larger set of concepts – perhaps in an unsupervised manner to overcome the challenges of labeling."
- Why unresolved: The current implementation relies on two supervised datasets (Dataset 1 and Dataset 2) with manually annotated concept labels, which limits the scalability and scope of the explanations.
- What evidence would resolve it: A demonstration of CW-Net successfully identifying and classifying relevant driving concepts without ground-truth labels while maintaining driving performance and human interpretability.

### Open Question 2
- Question: Does the CW-Net method preserve driving performance and explanation fidelity when applied to non-inverse-reinforcement-learning planning architectures (e.g., end-to-end networks)?
- Basis in paper: [inferred] Section 3 notes that the study focuses on a specific IRL-based planner and acknowledges that generalization to other architectures is suggested by prior work but not empirically proven in this deployment.
- Why unresolved: The experiments are limited to a specific black-box planner (DriveIRL), and it is unclear if replacing the final reward layer is feasible or effective for architectures with vastly different internal structures.
- What evidence would resolve it: Benchmarks applying the CW-Net wrapper to diverse planner architectures (e.g., transformer-based end-to-end models) showing comparable L2 trajectory errors and concept separation metrics.

### Open Question 3
- Question: What is the impact of concept classification errors (e.g., low precision or false positives) on driver trust and the accuracy of their mental models over time?
- Basis in paper: [inferred] Tables S2 and S3 reveal low F1 scores for several concepts (e.g., BIKE had an F1 score of 0.00), yet Section 8.3 posits that even inaccurate concepts are useful for debugging; the threshold where errors confuse rather than help is not defined.
- Why unresolved: While the study shows mental models improved on average, the specific effects of the observed concept hallucinations (e.g., phantom stopped vehicles) on user confusion or "alarm fatigue" were not quantified.
- What evidence would resolve it: Ablation studies specifically analyzing user reactions and prediction accuracy when exposed to noisy or incorrect concept activations versus faithful ones.

### Open Question 4
- Question: How does repeated exposure to CW-Net explanations affect long-term driver reliance and situational awareness?
- Basis in paper: [inferred] The paper utilizes single-session online studies and semi-naturalistic track tests, inferring long-term safety benefits from short-term improvements in situational awareness.
- Why unresolved: Longitudinal effects, such as the potential for drivers to develop over-trust (automation bias) or learn to ignore the concept displays, are not captured by the study's cross-sectional design.
- What evidence would resolve it: A longitudinal study tracking the same drivers over weeks or months of usage to measure the persistence of mental model improvements and changes in reaction times.

## Limitations
- Concept vocabulary (8-10 concepts) may not cover all possible driving situations, limiting explanation utility in novel contexts
- Reliance on labeled datasets for concept training introduces potential biases and may not capture full complexity of human driving behavior
- Generalizability to diverse real-world driving scenarios remains uncertain despite controlled experimental results

## Confidence
- **High confidence:** CW-Net's ability to maintain driving performance while providing interpretable explanations is well-supported by quantitative metrics and controlled experiments
- **Medium confidence:** The claim that CW-Net improves human mental models in surprising situations is supported by online studies, but the specific mechanisms and generalizability to all users require further investigation
- **Medium confidence:** The transfer of improved mental models to better situational awareness in naturalistic settings is plausible based on SAGAT results, but the long-term impact and potential for user fatigue need to be explored

## Next Checks
1. Conduct a longitudinal study to assess the durability of mental model improvements and the potential for user fatigue with CW-Net explanations over extended periods of use
2. Test CW-Net's performance and explanation utility in a wider variety of driving scenarios, including edge cases and novel situations not covered by the current concept vocabulary
3. Investigate the potential for adaptive concept vocabularies that can expand or contract based on the specific driving context and user needs, enhancing the flexibility and applicability of CW-Net