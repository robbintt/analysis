---
ver: rpa2
title: 'Mastering Da Vinci Code: A Comparative Study of Transformer, LLM, and PPO-based
  Agents'
arxiv_id: '2506.12801'
source_url: https://arxiv.org/abs/2506.12801
tags:
- game
- zhang
- agent
- wang
- tiles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper compares three AI approaches for the game of Da Vinci\
  \ Code: a Transformer-based baseline, several LLM agents, and a PPO-trained agent.\
  \ The PPO agent achieved the highest win rate of 58.5% \xB1 1.0% over 10,000 games,\
  \ significantly outperforming all LLM agents (best at 38.9%) and the baseline Transformer."
---

# Mastering Da Vinci Code: A Comparative Study of Transformer, LLM, and PPO-based Agents

## Quick Facts
- **arXiv ID**: 2506.12801
- **Source URL**: https://arxiv.org/abs/2506.12801
- **Reference count**: 26
- **Primary result**: PPO-based agent achieves 58.5% ± 1.0% win rate over 10,000 games, outperforming LLM agents (best 38.9%) and baseline Transformer.

## Executive Summary
This paper presents a comparative study of three AI approaches for the deduction game Da Vinci Code. The PPO-based agent with a Transformer encoder processes full game history and achieves the highest win rate (58.5% ± 1.0%) through self-play training. LLM agents struggle with maintaining logical consistency despite sophisticated prompting, achieving at best 38.9% win rate. The study demonstrates that specialized deep reinforcement learning outperforms general-purpose language models for games requiring multi-step logical inference under imperfect information.

## Method Summary
The study evaluates three approaches: a baseline Transformer agent, multiple LLM agents (GPT-4o, GPT-4o-mini, DeepSeek R1), and a PPO-trained agent. The PPO agent uses a Transformer encoder to process game history serialized as structured strings, optimized through self-play with Proximal Policy Optimization. State representation includes current hand, opponent visible tiles, drawn card, and parsed history events. The action space consists of 339 discrete actions (13 positions × 26 card values + 1 place action). Training uses GAE advantage estimation with specific hyperparameters (γ=0.999, λ=0.95) and rewards (+3 win, -3 loss, +0.2 correct guess, -0.5 incorrect guess).

## Key Results
- PPO agent achieves 58.5% ± 1.0% win rate over 10,000 games, significantly outperforming all baselines
- Best LLM (DeepSeek R1) achieves 38.9% win rate with large variance (7.4%)
- Baseline Transformer agent performs worst among the three approaches
- PPO agent's success attributed to comprehensive history processing and self-play learning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: PPO agent's superior performance stems from learning implicit deductive strategies through self-play while processing full game history via Transformer encoder.
- **Mechanism**: Game state serialized as structured strings → tokenized with BPE → processed through 3-layer Transformer encoder → [CLS] embedding for policy/value → PPO optimizes 339 discrete actions with clipped surrogate objective and GAE.
- **Core assumption**: Sequential processing of full history enables belief state tracking that simpler baselines cannot achieve; reward structure provides sufficient gradient signal.
- **Evidence anchors**: Abstract states PPO demonstrates "superior win rates (58.5% ± 1.0%)... processing comprehensive game history"; Section 3.4 details architecture and GAE parameters.
- **Break condition**: Truncating history below 256 tokens or failing reward shaping should degrade performance toward baseline levels.

### Mechanism 2
- **Claim**: LLM agents underperform due to difficulties maintaining strict logical consistency over extended gameplay despite sophisticated prompting.
- **Mechanism**: LLMs receive structured prompts with game state, rules, and heuristics → generate JSON actions → fallback to random/legal on invalid outputs → cumulative errors compound over turns.
- **Core assumption**: LLM reasoning remains fundamentally ungrounded in formal game logic; prompting alone cannot substitute for explicit policy learning.
- **Evidence anchors**: Abstract notes LLMs "struggled with maintaining strict logical consistency"; Section 4 shows GPT-4o at 17.0% ± 7.4% with occasional inconsistent outputs.
- **Break condition**: Fine-tuning on game trajectories or external memory for belief state tracking may narrow performance gap.

### Mechanism 3
- **Claim**: Action masking mechanism combined with legal action constraints prevents invalid moves while maintaining policy gradient flow.
- **Mechanism**: Environment generates legal_actions list → boolean mask over 339-dimensional space → illegal action logits set to very small value before Categorical sampling → ensures all sampled actions are executable.
- **Core assumption**: Flat 339-action space adequately covers game dynamics without requiring hierarchical decomposition.
- **Evidence anchors**: Section 3.4 states "mask derived from environment's legal actions is applied to these logits"; Section 3.1 describes action space structure.
- **Break condition**: If action space grows significantly for multiplayer variants, flat encoding may become computationally prohibitive.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO)**
  - Why needed here: Core algorithm for winning agent; requires understanding policy gradients, clipping, and trust region methods.
  - Quick check question: Explain why PPO clips the probability ratio rt(θ) and how the clipping parameter ε=0.2 affects policy updates.

- **Concept: Partially Observable Markov Decision Processes (POMDPs)**
  - Why needed here: Da Vinci Code is fundamentally imperfect information; agent must maintain belief states over hidden tiles.
  - Quick check question: How does Transformer encoder's history processing approximate belief state updates in this POMDP?

- **Concept: Transformer Encoder Architectures for Sequential Decision-Making**
  - Why needed here: StateEncoder uses attention over game history tokens; understanding positional encoding and [CLS] extraction is critical.
  - Quick check question: Why use first token embedding (index 0) as aggregated state representation rather than pooling all outputs?

## Architecture Onboarding

- **Component map**: DaVinciCodeGameEnvironment -> StateEncoder (Embedding + Positional Encoding + Transformer Encoder) -> Actor/Critic networks -> Action sampling with mask -> Environment step -> TrajectoryBuffer -> PPO update

- **Critical path**: State serialization → tokenization (BPE, vocab=64) → padding/truncation to 256 tokens → StateEncoder → [CLS] embedding → Actor/Critic heads → action sampling with mask → environment step → reward + next state → buffer → PPO update (3 epochs, batch=2048)

- **Design tradeoffs**: Flat 339-action space simplifies masking but may scale poorly for multiplayer variants; MAX_HISTORY_LEN=256 balances context richness vs. computational cost; separate Actor/Critic encoders vs. weight sharing unclear; BPE tokenizer trained on domain vocabulary vs. general-purpose tokenizers

- **Failure signatures**: Win rate plateaus below baseline: check reward scaling, learning rates (actor=0.0005, critic=0.0003), or entropy coefficient (0.01); Invalid action attempts: verify legal_actions mask correctly applied before sampling; Erratic value estimates: check GAE λ=0.95 and γ=0.999 settings, advantage normalization

- **First 3 experiments**: 
  1. **Ablate history length**: Reduce MAX_HISTORY_LEN from 256 to 64, 32 to measure impact of temporal context on win rate. Expect degradation toward baseline.
  2. **Replace Transformer encoder with LSTM**: Compare sample efficiency and final performance to isolate architecture contribution.
  3. **Vary reward shaping**: Test alternative reward schemes (e.g., remove +0.2 for correct guesses, increase win bonus to +5) to assess sensitivity to intermediate signals.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can hybrid architectures that use LLMs for high-level strategic planning and RL for policy execution outperform the specialized PPO agent in deductive tasks?
- **Basis in paper**: [explicit] Authors propose exploring "synergistic combinations of RL and LLMs," such as using LLMs for strategic guidance to inform RL policies or using RL to generate actions for LLMs to evaluate.
- **Why unresolved**: Current study evaluates these paradigms independently; potential performance gains or computational trade-offs of integration were not tested.
- **What evidence would resolve it**: Empirical results from hybrid agent competing against standalone PPO agent and LLMs, showing win rates and logical consistency metrics.

### Open Question 2
- **Question**: Can PPO framework successfully generalize to more complex deduction games with significantly larger state spaces, such as Clue or advanced Mastermind?
- **Basis in paper**: [explicit] Paper lists "Scalability and Generalization Studies" as future avenue, suggesting need to test framework's robustness on games with larger combinatorial action spaces.
- **Why unresolved**: Current results specific to Da Vinci Code rules and state space; unproven whether same state encoding and reward shaping techniques are efficient enough for more complex environments.
- **What evidence would resolve it**: Successful application of same Transformer-encoder PPO architecture to Clue or Mastermind without extensive, game-specific architectural redesign.

### Open Question 3
- **Question**: To what extent would fine-tuning LLMs on expert game trajectories or using advanced prompting (e.g., Tree-of-Thoughts) mitigate their inability to maintain strict logical consistency?
- **Basis in paper**: [explicit] Authors note LLMs produced outputs inconsistent with game logic and suggest future work investigate "advanced prompting techniques" or "fine-tuning LLMs on a corpus of Da Vinci Code game data."
- **Why unresolved**: Study only evaluated LLMs using structured prompting with heuristics; impact of in-context learning examples or weight updates based on game data remains unknown.
- **What evidence would resolve it**: Comparative study of baseline LLMs against fine-tuned or Tree-of-Thought-prompted LLMs in same environment, measuring reduction in illegal moves and hallucinations.

### Open Question 4
- **Question**: How does PPO agent's performance vary with specific length and granularity of game history provided to Transformer encoder?
- **Basis in paper**: [inferred] While paper highlights use of "comprehensive game history" (truncated/padded to 256 tokens) as key advantage over baseline, does not provide ablation studies on sensitivity of this parameter.
- **Why unresolved**: Unclear if agent relies primarily on recent moves for deduction or effectively utilizes deep historical context, nor known if shorter histories would degrade performance.
- **What evidence would resolve it**: Ablation experiments showing PPO agent's win rate when trained with varying MAX_HISTORY_LEN values (e.g., 16, 64, 256).

## Limitations

- **Limited baseline comparison**: Specific baseline Transformer architecture details not provided, making it difficult to assess true apples-to-apples comparison
- **Training duration unspecified**: Total training steps/episodes for PPO agent not specified, affecting understanding of sample efficiency and computational cost
- **Parallelization unknown**: Number of parallel environment workers for data collection not detailed, a critical factor in PPO training efficiency

## Confidence

- **Medium confidence** in core finding that PPO (58.5% ± 1.0%) outperforms LLM agents (best 38.9%) and baseline Transformer - based on straightforward win rate comparison with clear specification
- **Low confidence** in mechanistic explanations for why PPO succeeds - limited direct evidence from paper itself
- **Major uncertainties** about baseline architecture details, training duration, and parallelization setup

## Next Checks

1. **Ablate history length**: Reduce MAX_HISTORY_LEN from 256 to 64 and 32 tokens to measure impact of temporal context on win rate. This will test whether full history processing is truly necessary for observed performance.
2. **Architecture ablation**: Replace Transformer encoder with LSTM to isolate whether attention mechanism or sequential processing itself is responsible for performance gain.
3. **Reward shaping sensitivity**: Test alternative reward schemes (e.g., removing +0.2 for correct guesses, increasing win bonus to +5) to assess whether current reward structure is critical or agent is robust to different shaping.