---
ver: rpa2
title: IIITH-BUT system for IWSLT 2025 low-resource Bhojpuri to Hindi speech translation
arxiv_id: '2506.04714'
source_url: https://arxiv.org/abs/2506.04714
tags:
- translation
- data
- iwslt
- speech
- bleu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses low-resource speech translation for the Bhojpuri-Hindi
  language pair, a challenging scenario due to limited parallel data. The authors
  fine-tuned the SeamlessM4T model using a systematic approach involving hyperparameter
  optimization (learning rate, batch size, label smoothing, warmup steps) and data
  augmentation techniques (SpecAugment and speed perturbation).
---

# IIITH-BUT system for IWSLT 2025 low-resource Bhojpuri to Hindi speech translation

## Quick Facts
- arXiv ID: 2506.04714
- Source URL: https://arxiv.org/abs/2506.04714
- Reference count: 18
- Best dev BLEU: 36.41; Test BLEU: 9.9

## Executive Summary
This paper tackles the challenge of low-resource speech translation for the Bhojpuri-Hindi language pair, leveraging the pre-trained SeamlessM4T model. The authors systematically explore hyperparameter tuning, data augmentation, and cross-lingual transfer to improve translation quality. Their best model achieves a BLEU score of 36.41 on the development set and 9.9 on the test set, highlighting both the potential and challenges of low-resource speech translation.

## Method Summary
The authors fine-tune the SeamlessM4T medium model using a systematic approach: hyperparameter optimization (learning rate 1e-5, batch size 32, label smoothing 0.1, warmup steps 250), data augmentation (SpecAugment and speed perturbation), and cross-lingual transfer (joint training with Marathi-Hindi followed by Bhojpuri-only fine-tuning). The model is trained on 20 hours of Bhojpuri-Hindi speech data using 2× NVIDIA A100 GPUs with FP16 mixed precision.

## Key Results
- Best dev BLEU: 36.41 achieved with moderate learning rate (1e-5), SpecAugment, and joint Marathi-Bhojpuri training
- Test BLEU: 9.9 (large gap from dev suggests domain mismatch or data quality issues)
- Medium model (1.2B) outperformed large (2.3B): 30.5 vs 25.5 BLEU
- SpecAugment alone (33.7 BLEU) outperformed speed perturbation (32.7) or their combination (32.4)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Moderate learning rate with extended warmup stabilizes low-resource fine-tuning.
- Mechanism: A learning rate of 1e-5 with 250 warmup steps allows the pre-trained SeamlessM4T model to gradually adapt to Bhojpuri-Hindi without catastrophic forgetting, balancing domain adaptation against training stability.
- Core assumption: The pre-trained model contains transferable multilingual representations that require gentle adaptation rather than aggressive updates.
- Evidence anchors:
  - [abstract] "systematically investigated a range of hyperparameters including learning rate schedules... warm-up steps"
  - [section 4.4] "moderate rate of 1e-5 consistently yielding the best performance... 250 steps produced optimal convergence"
  - [corpus] GMU IWSLT 2025 system (arXiv:2505.21781) also fine-tuned SeamlessM4T-v2, suggesting this architecture is a common choice for low-resource ST, though specific hyperparameter findings are not directly comparable.
- Break condition: If pre-trained model lacks relevant language representations (e.g., truly unseen language families), warmup benefits may diminish.

### Mechanism 2
- Claim: SpecAugment improves generalization more than speed perturbation for this task.
- Mechanism: Spectrogram masking forces the model to rely on broader acoustic context rather than frame-specific features, reducing overfitting to limited Bhojpuri training examples. Speed perturbation adds speaker-rate variation but may introduce less regularization benefit.
- Core assumption: Overfitting to acoustic details is a primary failure mode in low-resource ST.
- Evidence anchors:
  - [abstract] "application of simple yet effective augmentation techniques significantly improve performance"
  - [section 4.5] Table 5 shows SpecAugment alone: BLEU 33.7 vs baseline 31.8; speed perturbation alone: 32.7; combined: 32.4
  - [corpus] Weak direct evidence—neighbor papers mention augmentation but do not systematically compare SpecAugment vs speed perturbation.
- Break condition: If test data contains dramatically different acoustic conditions (noise, channel), SpecAugment's simulated masking may not transfer.

### Mechanism 3
- Claim: Joint training with linguistically related language followed by brief target-only fine-tuning mitigates catastrophic forgetting.
- Mechanism: Marathi-Hindi data provides additional Indo-Aryan linguistic signal during joint training. A single epoch of Bhojpuri-only fine-tuning afterward recovers target-specific patterns without overwriting cross-lingual benefits.
- Core assumption: Marathi and Bhojpuri share sufficient phonological and lexical structure for positive transfer.
- Evidence anchors:
  - [abstract] "examined the use of cross-lingual signal through joint training with Marathi and Bhojpuri speech data"
  - [section 4.6, Table 7] Joint training alone: 34.6 BLEU; +1 epoch Bhojpuri: 36.0; +2 epochs: 35.6 (decline)
  - [corpus] No direct corpus evidence on Marathi-Bhojpuri transfer specifically; this appears to be a novel contribution.
- Break condition: If joint training language is linguistically distant or introduces conflicting patterns, transfer may be negative.

## Foundational Learning

- Concept: End-to-end speech translation (ST) vs. cascade ASR+MT
  - Why needed here: SeamlessM4T is an end-to-end model that jointly encodes speech and decodes text, eliminating error propagation between ASR and MT components.
  - Quick check question: Can you explain why cascade systems may propagate errors in low-resource settings?

- Concept: SpecAugment (time/frequency masking)
  - Why needed here: Applied as primary data augmentation; understanding masking parameters helps diagnose regularization effects.
  - Quick check question: What acoustic information is removed when applying a 30-frame time mask?

- Concept: BLEU score interpretation and limitations
  - Why needed here: Large dev-test BLEU gap (36.4 vs. 9.9) indicates potential domain mismatch; understanding BLEU sensitivity helps interpret this.
  - Quick check question: Why might BLEU correlate poorly with human judgment on noisy or misaligned data?

## Architecture Onboarding

- Component map: Conformer speech encoder (24 layers) -> Transformer text decoder (12 layers) -> BLEU/chrF++ metrics
- Critical path: Load pre-trained checkpoint → prepare Bhojpuri-Hindi audio/transcript pairs → apply SpecAugment during training → fine-tune all parameters with LR=1e-5, warmup=250, batch=32 → decode with beam size 10
- Design tradeoffs: Medium model outperformed large (BLEU 30.5 vs. 25.5), likely due to overfitting risk with 2.3B parameters on 20 hours of data. Joint multilingual training adds data but requires careful sequential fine-tuning to avoid forgetting.
- Failure signatures: (1) BLEU improves on dev but degrades on test → domain mismatch or data quality issues; (2) performance drops after 1-2 additional fine-tuning epochs → overfitting; (3) target word count exceeds reference → audio interference or hallucination.
- First 3 experiments:
  1. Baseline fine-tuning: LR=1e-5, batch=10, no augmentation—establish dev BLEU ceiling.
  2. Add SpecAugment only: Compare regularization effect against speed perturbation alone.
  3. Joint Marathi-Bhojpuri training + 1-epoch Bhojpuri recovery: Test cross-lingual transfer with minimal forgetting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What causes the substantial performance gap between development set BLEU (36.41) and test set BLEU (9.9), and how can this generalization failure be mitigated?
- Basis in paper: [explicit] The authors state: "The considerable performance gap between development and test sets suggests potential domain mismatch between the datasets or possible data quality issues in the test set, warranting further investigation."
- Why unresolved: The paper identifies the gap but does not diagnose whether the root cause is domain shift, data quality, or model overfitting to dev set characteristics.
- What evidence would resolve it: Comparative analysis of acoustic features, lexical distributions, and speaker characteristics between dev and test sets; domain adaptation experiments; controlled ablation with balanced dev/test splits.

### Open Question 2
- Question: Can self-supervised pretraining on monolingual Bhojpuri speech data substantially improve translation quality beyond the current 36.41 BLEU baseline?
- Basis in paper: [explicit] In the conclusion, the authors state: "We also intend to investigate self-supervised pretraining on monolingual Bhojpuri speech data."
- Why unresolved: The current approach relies solely on fine-tuning the multilingual SeamlessM4T model; leveraging unlabeled Bhojpuri speech remains unexplored.
- What evidence would resolve it: Experiments comparing wav2vec 2.0 or HuBERT-style pretraining on Bhojpuri audio followed by fine-tuning, with statistical significance testing against the current baseline.

### Open Question 3
- Question: Why does combining speed perturbation with SpecAugment decrease BLEU (32.4) compared to SpecAugment alone (33.7), and can this negative interaction be overcome?
- Basis in paper: [inferred] Table 5 shows that SpecAugment alone yields 33.7 BLEU, speed perturbation alone yields 32.7, but combined they achieve only 32.4. The paper does not explain this degradation.
- Why unresolved: The interaction between augmentation techniques in low-resource ST is not well understood; the combined approach may introduce excessive regularization or acoustic distortion.
- What evidence would resolve it: Systematic ablation varying augmentation intensity; analysis of learned representations to assess whether combined augmentation causes underfitting or representation collapse.

### Open Question 4
- Question: To what extent can cross-lingual transfer from additional Indo-Aryan languages (beyond Marathi) further improve Bhojpuri-Hindi translation?
- Basis in paper: [explicit] The authors state they intend "to further extend our cross-lingual approach to additional Indo-Aryan languages."
- Why unresolved: Only Marathi was tested for joint training; the potential benefit of incorporating related languages such as Awadhi, Magahi, or Urdu remains unknown.
- What evidence would resolve it: Experiments adding parallel data from multiple Indo-Aryan language pairs, with analysis of linguistic similarity metrics predicting transfer gains.

## Limitations
- Large BLEU gap (36.41 dev vs 9.9 test) suggests domain mismatch or data quality issues in test set
- Unknown learning rate schedule (only warmup steps specified) limits reproducibility
- Limited cross-lingual transfer experiments (only Marathi tested) with no linguistic justification for choice

## Confidence
- **Medium confidence**: "Moderate learning rate with extended warmup stabilizes low-resource fine-tuning" - Supported by ablation but missing schedule details.
- **High confidence**: "SpecAugment improves generalization more than speed perturbation" - Clear numerical comparison in Table 5 with consistent results across variants.
- **Medium confidence**: "Joint training with Marathi + 1 epoch Bhojpuri recovery mitigates forgetting" - Results show improvement, but linguistic justification for Marathi choice is absent.

## Next Checks
1. **Test data quality audit**: Request and analyze IWSLT 2025 test set references for alignment quality, noise levels, and completeness to explain the dev-test BLEU gap.
2. **Learning rate schedule comparison**: Re-run experiments with different decay schedules (linear vs cosine) while keeping LR=1e-5 and warmup=250 to isolate schedule effects.
3. **Alternative cross-lingual partner**: Repeat joint training using a different Indo-Aryan language (e.g., Maithili or Awadhi) to test whether Marathi's benefits are language-specific or transfer-general.