---
ver: rpa2
title: 'Transformers in Pseudo-Random Number Generation: A Dual Perspective on Theory
  and Practice'
arxiv_id: '2508.01134'
source_url: https://arxiv.org/abs/2508.01134
tags:
- transformer
- layer
- number
- lemma
- pseudo-random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that decoder-only Transformers with Chain-of-Thought
  can theoretically simulate both the Linear Congruential Generator (LCG) and Mersenne
  Twister (MT) pseudo-random number generators. The authors construct explicit Transformer
  architectures with O(n) parameters and constant depth to implement each step of
  these PRNG algorithms, establishing a non-uniform AC0 lower bound for log-precision
  Transformers.
---

# Transformers in Pseudo-Random Number Generation: A Dual Perspective on Theory and Practice

## Quick Facts
- arXiv ID: 2508.01134
- Source URL: https://arxiv.org/abs/2508.01134
- Authors: Ran Li; Lingshu Zeng
- Reference count: 40
- This paper demonstrates that decoder-only Transformers with Chain-of-Thought can theoretically simulate both the Linear Congruential Generator (LCG) and Mersenne Twister (MT) pseudo-random number generators.

## Executive Summary
This paper establishes that decoder-only Transformers can theoretically simulate pseudo-random number generators like the Linear Congruential Generator and Mersenne Twister through explicit constructions with constant depth and O(n) parameters. The authors prove a non-uniform AC0 lower bound for log-precision Transformers while demonstrating practical capability through experiments showing Transformer-based generators produce statistically random sequences that pass most NIST statistical tests. The work bridges theoretical circuit complexity with practical Transformer capabilities in both generating and analyzing pseudo-random sequences.

## Method Summary
The authors construct explicit Transformer architectures with constant depth and O(n) parameters to implement LCG and MT algorithms. For LCG, they use attention layers with specialized weight matrices to perform Boolean operations and FFN layers to approximate multiplication and modulus. For MT, they employ Chain-of-Thought as computational unfolding, using 17 layers to implement the twist and tempering phases. Experimental validation uses GPT-2 architecture trained on MT-generated sequences converted to 8/12/16-bit tokens, evaluated via NIST STS tests and prediction attacks on reduced-bit-width MT outputs.

## Key Results
- Transformers can theoretically simulate LCG and MT PRNG algorithms with O(n) parameters and constant depth
- Transformer-based generators produce sequences passing 11 of 15 NIST statistical tests
- Prediction attacks achieve 70-80% accuracy on reduced-bit-width MT sequences
- Non-uniform AC0 lower bound established for log-precision Transformers

## Why This Works (Mechanism)

### Mechanism 1
Transformers implement basic Boolean and arithmetic operations (XOR, AND, OR, NOT, multiplication, modulus) required for PRNG algorithms using attention layers with specialized weight matrices for constant-variable operations and FFN layers with GeLU activations to approximate functions like multiplication. The modulus operation decomposes to x mod y = x - y⌊x/y⌋, implemented through ReLU-based approximations.

### Mechanism 2
Chain-of-Thought enables sequential simulation of multi-step PRNG algorithms within constant-depth architecture by representing intermediate computational states as token sequences. Each CoT step allows the Transformer to count previously generated numbers, retrieve required indices from state history, and apply rotation/extraction operations across specific layers. The 17-layer construction maps directly to MT's algorithmic phases.

### Mechanism 3
Transformers exploit learned statistical patterns to predict PRNG outputs, enabling security assessment via prediction attacks. Training on PRNG sequences allows the model to internalize transition rules, achieving 70-80% prediction accuracy on MT with reduced bit-widths by learning the deterministic mapping from state to output.

## Foundational Learning

- **Concept: Pseudo-random number generators (PRNGs)**
  - Why needed here: The paper's central object; understanding LCG's linear recurrence and MT's twist/temper phases is prerequisite to following the construction.
  - Quick check question: Given LCG parameters a=5, c=1, m=16, x₀=3, what is x₁?

- **Concept: Non-uniform circuit complexity (AC⁰)**
  - Why needed here: The paper establishes Transformers can represent non-uniform AC⁰, providing theoretical bounds on computational capacity.
  - Quick check question: Why can't AC⁰ circuits compute parity of n bits?

- **Concept: Chain-of-Thought as computational unfolding**
  - Why needed here: CoT is used here not as prompting, but as explicit intermediate token generation enabling serial computation within parallel architecture.
  - Quick check question: How does emitting intermediate tokens circumvent constant-depth limitations?

## Architecture Onboarding

- **Component map:**
  Input embedding (ϕ(s_j), j, 1) → Layers 1-3 (index computation, position tracking, value retrieval) → Layers 4-8 (Rotation block: t-computation, XOR cascades) → Layers 9-16 (Extraction block: Bit-shift + XOR + AND) → Layer 17 (Multi-way selector) → Output (Softmax)

- **Critical path:**
  1. cnt⇒ calculation (Layer 1) → determines which random number to generate
  2. Index → coordinate mapping (Layer 2) → retrieves x[i], x[i+1], x[i+m]
  3. Rotation z-computation (Layers 4-8) → produces new state element
  4. Extraction y-computation (Layers 9-16) → produces output value
  5. Final selection (Layer 17) → routes correct token to output

- **Design tradeoffs:**
  - Hidden dimension d = O(n) scales with sequence length; may be prohibitive for long sequences
  - 17 layers is fixed for MT; different PRNGs require custom layer counts
  - Log-precision assumption limits numerical accuracy; may cause drift on extended sequences
  - GPT-2 architecture (768 dim, 12 heads, 12 layers) is oversized for theoretical construction

- **Failure signatures:**
  - NIST test failures: RandomExcursions, Universal, Approximate Entropy indicate insufficient sequence variation
  - Training accuracy plateau <1.0 suggests incomplete algorithm learning
  - Heatmap showing patterns instead of uniform distribution indicates undertrained or undersized model

- **First 3 experiments:**
  1. Train minimal Transformer (1 layer, 1 head) on 8-bit LCG sequences; verify exact recurrence learning via 100% next-token accuracy
  2. Train GPT-2 on 8/12/16-bit MT sequences; plot convergence steps vs. bit-width to validate theoretical complexity claims
  3. Generate sequences at checkpoints (63%, 68%, 81%, 89% accuracy); track which NIST tests pass at each stage to identify quality thresholds

## Open Questions the Paper Calls Out

### Open Question 1
Can Transformers simulate PRNGs using constant hidden dimension O(1) rather than O(n)? Theorems 3.5 and 3.6 require hidden dimension d=O(n) to implement LCG and MT algorithms. It is unclear if the linear width is a fundamental requirement or merely a feature of the specific constructive proof. A proof of simulation with constant width or a formal lower bound proving width must scale with input length would resolve this.

### Open Question 2
Why does the Transformer-based generator consistently fail the NIST Block Frequency test? Section 4.2 reports passing 11 of 15 tests but failing Block Frequency across all training accuracy checkpoints. The discussion attributes general failures to "inadequate sequence variations," but does not isolate the cause for this specific statistical bias. A structural analysis of the output bits or a modified training methodology resulting in passing p-values for Block Frequency would resolve this.

### Open Question 3
What specific classes of highly nonlinear functions remain beyond the computational reach of log-precision Transformers? Section 6 calls for "theoretically analyzing Transformers' boundaries in fitting highly nonlinear functions." The paper establishes that Transformers can simulate AC⁰ and specific PRNGs, but does not define the upper limits of their expressiveness. Theoretical proofs defining function classes (e.g., specific cryptographic primitives) that cannot be approximated by polynomial-sized Transformers would resolve this.

## Limitations

- Theoretical constructions assume specific weight matrices and layer configurations that may not translate directly to learned parameters in standard Transformer architectures
- Experimental validation shows NIST test failures indicating measurable statistical deficiencies in Transformer-generated sequences compared to true MT output
- Log-precision assumption introduces approximation errors that may accumulate over long sequences, particularly affecting complex operations like MT's twist phase

## Confidence

**High Confidence (8/10):** The theoretical framework for implementing Boolean operations via attention and MLP layers is well-grounded. The proofs for constant-depth implementations of LCG and MT algorithms follow logically from established circuit complexity results.

**Medium Confidence (6/10):** The experimental results demonstrating Transformer capability on PRNG tasks are promising but have gaps. The convergence behavior supports theoretical claims, but NIST test failures suggest incomplete algorithmic learning.

**Low Confidence (4/10):** Claims about Transformer capability as a general PRNG implementation framework lack comprehensive empirical validation. The paper doesn't test alternative PRNGs to establish generality.

## Next Checks

1. Generate MT sequences at discrete training accuracy checkpoints (63%, 68%, 81%, 89%) and perform NIST tests at each point to identify whether deficiencies are uniform across all randomness tests or target specific algorithmic phases.

2. Extend prediction attack experiments to intermediate bit-widths (10, 14 bits) and plot prediction accuracy vs. bit-width to test whether observed accuracy is a smooth function of state space size or shows phase transitions.

3. Implement the theoretical 17-layer MT construction with minimal parameters specified (d = O(n), 4 heads per layer) and train from scratch to compare convergence speed, final accuracy, and NIST test performance against standard GPT-2.