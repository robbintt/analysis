---
ver: rpa2
title: Vector preference-based contextual bandits under distributional shifts
arxiv_id: '2508.15966'
source_url: https://arxiv.org/abs/2508.15966
tags:
- distribution
- have
- pareto
- problem
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies contextual multi-armed bandits with vector-valued
  rewards under distribution shifts. The authors propose a policy combining adaptive
  discretization and optimistic elimination to handle unknown change-points and time-varying
  context distributions.
---

# Vector preference-based contextual bandits under distributional shifts

## Quick Facts
- arXiv ID: 2508.15966
- Source URL: https://arxiv.org/abs/2508.15966
- Reference count: 40
- This paper proposes a policy for contextual multi-armed bandits with vector-valued rewards under distribution shifts, achieving sublinear regret that scales with distributional dissimilarity.

## Executive Summary
This paper addresses contextual multi-armed bandits where rewards are vectors and preferences are defined by a polyhedral cone, under a distributional shift from source P to target Q. The authors propose an adaptive discretization policy with optimistic elimination that handles unknown change-points and time-varying context distributions. They introduce a preference-based regret metric measuring the distance between estimated and true Pareto fronts, and prove sublinear regret bounds that scale with the dissimilarity between P and Q.

## Method Summary
The method combines adaptive discretization with optimistic elimination. A dyadic tree partitions the context space, splitting bins when estimation uncertainty drops below geometric uncertainty (based on bin width). For each bin, the algorithm maintains active arms and eliminates those strictly dominated by others according to the preference cone. The policy samples uniformly from the estimated Pareto front. The approach handles distributional shifts by relying on statistical overlap between source and target distributions, with regret bounds scaling according to the dissimilarity measure ρ(P,Q).

## Key Results
- Achieves sublinear regret under Hölder continuity and margin assumptions
- Regret bounds scale gracefully with problem parameters and dissimilarity ρ(P,Q)
- Generalizes known results for fixed distributions
- Performance improves with stronger margin conditions
- Bounds depend on minimum of ρ(P,Q)/t_p and ρ(Q,Q)/(T-t_p)

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Bias-Variance Discretization
If the mean reward function is Hölder continuous, an adaptive tree partition minimizes local estimation error by balancing bin width (approximation bias) against sample count (estimation variance). The policy maintains a dyadic tree partition of the context space, splitting bins when stochastic uncertainty drops below geometric uncertainty. This ensures finer resolution only where sufficient data exists.

### Mechanism 2: Cone-Ordered Optimistic Elimination
If arms are eliminated based on a preference cone rather than scalarization, the policy can identify the Pareto front without scalar weighting hyperparameters. The algorithm maintains active arms for each bin and eliminates those strictly dominated by another arm's estimate minus the confidence bonus, relative to the preference cone.

### Mechanism 3: Regret Transfer via Dissimilarity Scaling
If the source distribution P and target distribution Q differ, the regret incurred during the target phase scales with the dissimilarity measure ρ(P,Q) rather than the worst-case adversarial bound. The policy relies on statistical overlap; if P provides sufficient coverage of Q, the tree partitions learned under P remain informative under Q.

## Foundational Learning

**Concept: Preference Cones and Pareto Dominance**
- Why needed here: Unlike standard bandits, "optimal" is defined by a partial order (vector dominance) rather than a scalar maximum.
- Quick check question: If vector A=(1, 2) and B=(2, 1), and the preference cone is the positive orthant, are they Pareto-comparable?

**Concept: Regret under Distribution Shift (Covariate Shift)**
- Why needed here: The policy learns under distribution P but is evaluated under Q.
- Quick check question: Does the policy assume knowledge of the change-point t_p?

**Concept: Adaptive Discretization (Regressograms)**
- Why needed here: The context space is continuous and must be approximated by piecewise constant estimates over shrinking bins.
- Quick check question: In the algorithm, does the bin split when the number of samples is large enough or small enough relative to the bin width V_h?

## Architecture Onboarding

**Component map:**
Input Context X_t -> Tree Traversal to find Leaf B_(h_t,i_t) -> Active Arm Refinement -> Arm Selection from Estimated Pareto Front -> Reward Observation -> Tree Update (Estimate Update + Optional Split)

**Critical path:**
1. Receive Context X_t
2. Traverse Tree T to find containing Leaf B_(h_t, i_t)
3. Filter active arms A_(h_t, i_t) using elimination logic. Construct estimated Pareto front. Sample arm k_t
4. Observe reward r_t. Update estimates and counts for the selected bin
5. Check split condition. If met, generate children nodes and initialize their active sets

**Design tradeoffs:**
- Bin Width (V_h) vs. Confidence (C_t,h): Aggressive splitting reduces bias but increases variance
- Exploration vs. Safety: Uniform sampling from the estimated Pareto set provides diversity but may delay convergence

**Failure signatures:**
- Stagnant Tree: Split condition too strict, resulting in high bias
- Catastrophic Elimination: Confidence intervals too narrow, eliminating valid Pareto-optimal arms
- High Regret under Shift: P is uninformative about Q (high ρ), leading to poor performance

**First 3 experiments:**
1. Baseline Sanity Check: Run on fixed distribution (P=Q) with scalar rewards (M=1) to verify tree splits correctly
2. Dissimilarity Stress Test: Increase distribution shift parameter ν to plot Regret vs. ρ(P,Q)
3. Margin Sensitivity: Vary margin parameter α to observe regret behavior in hard vs. easy context regions

## Open Questions the Paper Calls Out
None

## Limitations
- Performance degrades significantly when the margin parameter α is small (high density of contexts near decision boundary)
- Assumes known polyhedral preference cone, which may not hold in many real-world applications
- Assumes smooth reward functions; performance under non-smooth or discontinuous rewards is not explored

## Confidence

**High Confidence**: The general theoretical framework for preference-based regret under distributional shifts is sound.

**Medium Confidence**: The adaptive discretization mechanism and its bias-variance tradeoff is theoretically justified but depends heavily on tuning constants and reward smoothness.

**Low Confidence**: The robustness of the elimination rule to small margin parameters and performance under non-smooth reward functions are not empirically validated.

## Next Checks

1. **Stress Test on Non-Smooth Rewards**: Implement a reward function with discontinuity and measure regret and tree depth to verify theoretical scaling breaks when function is not in Hölder class.

2. **Margin Sensitivity Analysis**: Systematically vary the margin α and plot final regret as a function of α to empirically confirm theoretical degradation when α is small.

3. **Unknown Cone Experiment**: Modify experiment to use a non-polyhedral cone and observe whether the algorithm can still function or requires significant modification.