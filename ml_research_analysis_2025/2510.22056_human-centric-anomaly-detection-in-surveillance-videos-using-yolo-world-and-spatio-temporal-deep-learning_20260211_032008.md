---
ver: rpa2
title: Human-Centric Anomaly Detection in Surveillance Videos Using YOLO-World and
  Spatio-Temporal Deep Learning
arxiv_id: '2510.22056'
source_url: https://arxiv.org/abs/2510.22056
tags:
- detection
- anomaly
- learning
- surveillance
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting anomalies in surveillance
  videos by proposing a human-centric deep learning framework that combines YOLO-World
  open-vocabulary detection with spatio-temporal modeling. The approach first isolates
  human regions using YOLO-World and ByteTrack for detection and tracking, then suppresses
  background via Gaussian blurring to reduce scene-specific distractions.
---

# Human-Centric Anomaly Detection in Surveillance Videos Using YOLO-World and Spatio-Temporal Deep Learning

## Quick Facts
- arXiv ID: 2510.22056
- Source URL: https://arxiv.org/abs/2510.22056
- Reference count: 22
- Primary result: Achieves 92.41% mean test accuracy on a five-class subset of UCF-Crime using human-centric preprocessing and BiLSTM modeling

## Executive Summary
This paper proposes a human-centric deep learning framework for detecting anomalies in surveillance videos. The approach uses YOLO-World for open-vocabulary human detection, ByteTrack for consistent tracking, and Gaussian blurring to suppress background regions outside detected bounding boxes. Spatial features are extracted using an ImageNet-pretrained InceptionV3 network, while temporal dynamics are modeled with a bidirectional LSTM. Evaluated on a curated five-class subset of UCF-Crime (Normal, Burglary, Fighting, Arson, Explosion), the model achieves strong performance with per-class F1-scores consistently exceeding 0.85.

## Method Summary
The framework processes surveillance videos through a pipeline that first detects and tracks human subjects using YOLO-World and ByteTrack, then applies Gaussian blurring to suppress background regions outside expanded bounding boxes. Spatial features are extracted offline using a frozen InceptionV3 network, and temporal modeling is performed using a bidirectional LSTM. The model addresses class imbalance through stratified sampling and regularization techniques. The approach leverages transfer learning from ImageNet pretraining and offline feature extraction to reduce computational overhead while maintaining classification accuracy.

## Key Results
- Mean test accuracy of 92.41% across three trials on five-class UCF-Crime subset
- Per-class F1-scores consistently exceeding 0.85 for all categories
- Macro F1 of 0.90 and weighted F1 of 0.92, indicating balanced performance across classes
- Strong generalization demonstrated through comprehensive metrics including confusion matrices and ROC curves

## Why This Works (Mechanism)

### Mechanism 1
Human-centric background suppression improves anomaly discrimination by reducing scene-specific noise. YOLO-World detects human instances using open-vocabulary prompts, ByteTrack maintains identity coherence across frames, and Gaussian blur attenuates pixels outside expanded bounding boxes, creating a soft attention mask that preserves human regions while suppressing static backgrounds. This assumes anomalous behaviors manifest primarily through human motion rather than background context.

### Mechanism 2
Decoupled spatio-temporal learning enables efficient training and transferability. InceptionV3 (ImageNet-pretrained) extracts spatial features offline, while BiLSTM processes temporal sequences bidirectionally. This separation allows spatial features to leverage large-scale image pretraining while temporal modeling remains lightweight, based on the assumption that spatial semantics and temporal dynamics can be learned independently.

### Mechanism 3
Stratified sampling and regularization mitigate class imbalance and overfitting. Normal samples are undersampled, rare classes are oversampled per epoch, and dropout (0.3-0.5) plus L2 regularization (1e-4) prevent overfitting to majority class. This assumes the curated subset is representative enough that oversampling doesn't induce severe overfitting.

## Foundational Learning

- **Concept: Open-Vocabulary Object Detection (YOLO-World)**
  - Why needed here: Standard YOLO detectors are limited to fixed training classes; YOLO-World accepts textual prompts to detect arbitrary categories without retraining, enabling flexible "person" detection across diverse surveillance conditions.
  - Quick check question: Can you explain how vision-language pretraining allows YOLO-World to generalize to unseen class prompts?

- **Concept: Bidirectional Sequence Modeling (BiLSTM)**
  - Why needed here: Anomaly classification often benefits from future context; BiLSTM processes sequences forward and backward, aggregating information from both directions.
  - Quick check question: Why would a unidirectional LSTM struggle with early-frame anomaly detection in short clips?

- **Concept: Transfer Learning with Pretrained CNNs**
  - Why needed here: Surveillance datasets are relatively small; ImageNet-pretrained InceptionV3 provides robust low/mid-level visual features without requiring massive video-specific pretraining.
  - Quick check question: What preprocessing steps must match between ImageNet pretraining and your inference pipeline to avoid feature distribution mismatch?

## Architecture Onboarding

- **Component map:** Raw video -> Decord frame extraction -> YOLO-World detection + ByteTrack tracking -> Gaussian blur masking -> InceptionV3 feature extraction -> BiLSTM temporal modeling -> Dense classification -> Softmax output
- **Critical path:** Detection quality gates everything: if YOLO-World misses humans in dark/occluded frames, the blur mask will suppress the entire frame, yielding uninformative features. Validate detection rates on your target scene before proceeding.
- **Design tradeoffs:** Offline vs. online feature extraction: offline speeds training but prevents end-to-end fine-tuning. Fixed 32 frames: uniform sampling may miss brief anomalies; zero-padding short videos may introduce noise at sequence boundaries.
- **Failure signatures:** Low recall on specific classes: check if YOLO-World consistently detects humans in those clips. High variance across trials: inspect stratified split balance. Training/validation divergence: reduce dropout or increase L2; check for data leakage.
- **First 3 experiments:**
  1. **Ablation on preprocessing:** Train with raw frames (no blur) vs. human-centric blur; quantify accuracy delta to isolate preprocessing contribution.
  2. **Sequence length sensitivity:** Test 16, 32, 64 frames to assess whether longer context improves Fighting/Burglary discrimination.
  3. **Temporal model swap:** Replace BiLSTM with ConvLSTM or Transformer encoder; compare parameter efficiency and accuracy on the same feature sequences.

## Open Questions the Paper Calls Out

- **Question:** How does the framework perform when scaled to the full 13-class UCF-Crime dataset compared to the curated 5-class subset?
- **Basis in paper:** The authors state they plan to extend the framework to incorporate additional anomaly categories from the full UCF-Crime dataset.
- **Why unresolved:** Current evaluation is restricted to five classes due to computational constraints and need for focused evaluation.
- **What evidence would resolve it:** Reporting accuracy and F1-scores on the complete UCF-Crime benchmark without curating a subset.

- **Question:** To what extent does the human-centric Gaussian blurring discard critical contextual information for anomalies driven by non-human actors or environmental changes?
- **Basis in paper:** The methodology suppresses all pixels outside human bounding boxes, which may remove visual cues for events where fire or smoke appears outside detected human regions.
- **Why unresolved:** The paper doesn't analyze cases where the anomaly's primary visual signature is separate from the human subject.
- **What evidence would resolve it:** An ablation study comparing performance on anomalies with visible human agents versus those with purely environmental cues.

- **Question:** Can the pipeline maintain high accuracy while meeting real-time processing constraints required for live surveillance?
- **Basis in paper:** The methodology notes that feature extraction and tracking are performed "offline" to manage overhead, while real-time detection is identified as a persistent open problem.
- **Why unresolved:** The paper focuses on classification accuracy and doesn't provide latency or FPS metrics for the end-to-end pipeline.
- **What evidence would resolve it:** Benchmarking inference speed of the integrated YOLO-World, ByteTrack, and BiLSTM components on standard surveillance hardware.

## Limitations
- Unknown dataset scale: The number of samples per class in the curated 5-class subset is unspecified, making it impossible to assess generalizability.
- Detection dependency: Framework's effectiveness depends entirely on YOLO-World's human detection reliability, especially in challenging conditions.
- Preprocessing validation: Gaussian blur masking strategy lacks rigorous ablation to quantify its marginal benefit over alternative methods.

## Confidence
- **High**: Overall pipeline design (CNN-RNN hybrid with transfer learning) is well-established and methodologically sound.
- **Medium**: Reported metrics are plausible given the architecture, but lack of variance reporting and unknown dataset size weaken generalizability claims.
- **Low**: Human-centric preprocessing contribution is asserted but not rigorously validated via ablation; its effectiveness depends on YOLO-World's detection reliability.

## Next Checks
1. **Ablation on preprocessing**: Train with raw frames (no blur) vs. human-centric blur; quantify accuracy delta to isolate preprocessing contribution.
2. **Detection reliability audit**: Measure YOLO-World human detection rates per class; visualize failure cases to assess whether missed detections correlate with performance drops.
3. **Dataset scale disclosure**: Publish exact sample counts per class in the curated 5-class subset; verify that stratification and balancing assumptions hold.