---
ver: rpa2
title: Understanding How Nonlinear Layers Create Linearly Separable Features for Low-Dimensional
  Data
arxiv_id: '2501.02364'
source_url: https://arxiv.org/abs/2501.02364
tags:
- linear
- random
- features
- subspaces
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the linear separability of features extracted
  by shallow nonlinear networks for data modeled as a union of low-dimensional subspaces
  (UoS). The authors prove that a single layer of random weights with quadratic activation
  can transform such data into linearly separable sets with high probability, requiring
  network width scaling polynomially with the intrinsic dimension of the subspaces
  rather than the ambient dimension.
---

# Understanding How Nonlinear Layers Create Linearly Separable Features for Low-Dimensional Data

## Quick Facts
- arXiv ID: 2501.02364
- Source URL: https://arxiv.org/abs/2501.02364
- Reference count: 40
- A single layer of random weights with quadratic activation can transform union-of-subspaces data into linearly separable sets with high probability

## Executive Summary
This paper proves that a shallow neural network with random weights and quadratic activation can transform data lying on low-dimensional subspaces into linearly separable representations with high probability. The key insight is that network width scales polynomially with the intrinsic dimension of subspaces rather than the ambient dimension, challenging the notion that overparameterization is necessary for linear separability. The authors validate their theoretical findings through experiments on synthetic data and real-world CIFAR-10 images transformed via MCR2 representations.

## Method Summary
The authors analyze a shallow network with random Gaussian weights W and entry-wise quadratic activation σ(z) = z². They prove that for data modeled as a union of K low-dimensional subspaces, the network transforms this data into linearly separable sets when width D exceeds a polynomial threshold in the intrinsic dimension r. The proof constructs a linear classifier based on projected norms and uses matrix concentration inequalities to bound the failure probability. The analysis assumes strictly positive principal angles between subspaces and iid Gaussian entries in the weight matrix.

## Key Results
- A single nonlinear layer with random weights and quadratic activation creates linearly separable features from union-of-subspaces data
- Required network width scales as O(poly(r) · log(r/δ)), polynomial in intrinsic dimension rather than ambient dimension
- Empirical validation shows similar width scaling for ReLU, ELU, GELU, and Leaky-ReLU activations

## Why This Works (Mechanism)

### Mechanism 1: Random Projection-Activation Coupling Creates Separating Hyperplanes
Random projection W maps inputs to D-dimensional space; quadratic activation σ(z) = z² introduces second-order interactions. The proof constructs classifier v where vₙ = sign(‖xₙ‖² − ‖yₙ‖²), leveraging that projected norms follow χ²ᵣ distributions. Matrix concentration (Bernstein's inequality) bounds failure probability.

### Mechanism 2: Width Scales Polynomially with Intrinsic Dimension
The proof bounds eigenvalues of Q₁ = Σᵢ xᵢxᵢᵀ − Σⱼ xⱼxⱼᵀ using Bernstein's matrix inequality. The variance term depends on r through expected outer products E[aaᵀ], E[bbᵀ], yielding bounds involving sin(θ_min) and Σsin²(θ_ℓ). This gives D ≥ (2π/sin²(θ_min)) · (4r² + √(Σsin²(θ_ℓ))) · (r+1) · log(2r/δ).

### Mechanism 3: Generalization to ReLU and Other Activations
Linear separability extends empirically to ReLU, ELU, GELU, and Leaky-ReLU activations with similar width-scaling behavior. The authors observe that nonlinear activations that introduce sufficient nonlinearity can achieve similar separation, though quadratic activation is analytically tractable.

## Foundational Learning

- **Principal angles between subspaces**
  - Why needed here: The minimum principal angle θ_min determines separation difficulty; appears directly in the width bound (D ∝ 1/sin²(θ_min))
  - Quick check question: Given two orthonormal bases U₁, U₂, can you compute the principal angles via SVD of U₁ᵀU₂?

- **χ² distribution and order statistics**
  - Why needed here: Projected norms ‖U₁ᵀw‖², ‖U₂ᵀw‖² follow correlated χ²ᵣ distributions; the proof uses E[max{X,Y}] and E[min{X,Y}] to bound expected outer products
  - Quick check question: For two correlated χ²ᵣ variables, how does correlation affect the expected difference?

- **Matrix Bernstein's inequality**
  - Why needed here: Concentration bound for sums of random matrices; used to bound P(λᵣ(Q₁) ≤ 0) and show positive-definiteness holds with high probability
  - Quick check question: What conditions must random matrices satisfy for Bernstein's inequality to apply?

## Architecture Onboarding

- Component map: Input x ∈ ℝᵈ (union of r-dim subspaces) → Random projection W ∈ ℝᴰˣᵈ, Wᵢⱼ ~ N(0,1) → Quadratic activation: σ(z) = z² (entry-wise) → Feature f(x) = σ(Wx) ∈ ℝᴰ → Linear classifier v ∈ ℝᴰ (exists w.h.p. if D ≥ poly(r))

- Critical path: Width selection based on intrinsic dimension r and minimum principal angle θ_min. Use D ≥ C · (4r² + √(Σsin²(θ_ℓ))) · (r+1) / sin²(θ_min) · log(2r/δ) for theoretical guarantee.

- Design tradeoffs:
  - Quadratic activation: Analytically tractable, may require fewer features on clean UoS data, but more sensitive to noise on real data
  - ReLU: More robust empirically, similar width scaling, but no theoretical guarantee yet
  - Larger K (classes): Width must scale as O(poly(Kr)); one-vs-all separation requires separating each subspace from all others

- Failure signatures:
  - Features not linearly separable: Width too small for given r or θ_min too small (subspaces nearly intersect)
  - Perfect train but poor test accuracy: Test samples not in-distribution (not on training subspaces)
  - Quadratic underperforms ReLU on real data: Noise violations of UoS assumption

- First 3 experiments:
  1. **Synthetic UoS phase transition**: Generate K=2 random r-dimensional subspaces in ℝᵈ, sweep D and r, verify separability using condition (6): Σᵢ vᵢxᵢxᵢᵀ ≻ 0 and Σᵢ vᵢyᵢyᵢᵀ ≺ 0. Confirm threshold depends on r, not d.
  2. **Activation