---
ver: rpa2
title: Structured Captions Improve Prompt Adherence in Text-to-Image Models (Re-LAION-Caption
  19M)
arxiv_id: '2507.05300'
source_url: https://arxiv.org/abs/2507.05300
tags:
- captions
- images
- structured
- dataset
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses poor prompt adherence in text-to-image models,
  attributing it to noisy and unstructured training data. The authors propose structured
  captions to simplify learning and improve controllability.
---

# Structured Captions Improve Prompt Adherence in Text-to-Image Models (Re-LAION-Caption 19M)

## Quick Facts
- arXiv ID: 2507.05300
- Source URL: https://arxiv.org/abs/2507.05300
- Reference count: 38
- This paper shows structured captions improve prompt adherence in text-to-image models, with VQA-based scores increasing by up to 0.027 (LLaVA) and 0.021 (InstructBLIP).

## Executive Summary
This paper addresses poor prompt adherence in text-to-image models by attributing it to noisy and unstructured training data. The authors propose structured captions to simplify learning and improve controllability. They introduce Re-LAION-Caption 19M, a 19 million image subset of Re-LAION-5B with captions generated by Mistral 7B Instruct-based LLaVA-Next following a four-part template: subject, setting, aesthetics, and camera details. Fine-tuning PixArt-Σ and Stable Diffusion 2 on structured versus shuffled captions shows consistent improvements in text-image alignment, with VQA-based scores increasing by up to 0.027 (LLaVA) and 0.021 (InstructBLIP) when using structured captions. The dataset is publicly available.

## Method Summary
The method involves creating a structured caption dataset from Re-LAION-5B by filtering for high-quality images and generating captions using LLaVA-Next with a four-part template. PixArt-Σ and Stable Diffusion 2 are fine-tuned using LoRA on both structured and shuffled caption variants. For SD2, the text encoder is extended by concatenating T5 embeddings with CLIP embeddings, and bullet markers are modified for tokenizer compatibility. Models are evaluated using VQA scores from LLaVA and InstructBLIP on generated images from test captions.

## Key Results
- VQA LLaVA score increased from 0.8563 (shuffled) to 0.8630 (structured) on PixArt-Σ
- VQA InstructBLIP score increased from 0.8170 (shuffled) to 0.8191 (structured) on SD2
- VQA LLaVA score increased from 0.8550 (shuffled) to 0.8571 (structured) on SD2
- Training with structured captions consistently outperforms shuffled captions across all model-dataset combinations

## Why This Works (Mechanism)

### Mechanism 1: Caption Structure Invariance Reduces Learning Burden
Restricting training captions to a canonical structure frees model capacity for semantic learning rather than structural invariance. The model need not learn that "subject-setting-aesthetic-camera" and "camera-aesthetic-subject-setting" are equivalent, because the training distribution collapses these permutations into a single canonical form.

### Mechanism 2: Slot-Based Templates Improve Attention-to-Component Mapping
Fixed position markers (bullet points "1."–"4.") help models associate specific semantic slots with their visual correlates. The template creates predictable positional patterns—subject always appears first, camera details last—allowing cross-attention mechanisms to develop stronger position-conditioned mappings.

### Mechanism 3: VLM-Generated Captions Denoise Alt-Text Distribution
Re-captioning with LLaVA-Next removes noise from web-scraped alt-text, producing more complete descriptions. Vision-language models describe what is visually present, not what HTML authors wrote; this reduces hallucinated content and missing elements.

## Foundational Learning

- **Cross-attention conditioning in diffusion models**: Why needed here: The paper's mechanism relies on text encoder outputs conditioning image generation via cross-attention; understanding this clarifies why caption structure affects output fidelity. Quick check question: In a latent diffusion model, which component receives the text embedding—U-Net's self-attention or cross-attention layers?

- **CLIP text encoder context length limits**: Why needed here: Stable Diffusion's 77-token limit required T5 embedding concatenation; understanding context windows explains the tokenizer preprocessing (˜1˜ markers). Quick check question: What happens to tokens beyond position 77 in standard CLIP text encoding?

- **LoRA fine-tuning for diffusion models**: Why needed here: Experiments use LoRA (rank 4–32) rather than full fine-tuning; understanding low-rank adaptation clarifies what capacity is modified. Quick check question: Does LoRA modify the text encoder, the U-Net, or both in this paper's setup?

## Architecture Onboarding

- **Component map**: Re-LAION-5B → filter (resolution ≥1024, aspect ratio ≥0.666, aesthetic >4.73, luminance [12.75, 204], OCR extremes) → 19M images → LLaVA-Next captioning (4-bullet template) → filter defective outputs → LoRA fine-tuning (PixArt-Σ/SD2) → VQA evaluation

- **Critical path**: Dataset filtering quality directly affects caption reliability; overly aggressive OCR filtering removes text-heavy valid images. Tokenizer preprocessing (˜1˜ markers) is essential for T5 compatibility—without this, boundary tokens merge with content. Fine-tuning duration: >1 epoch causes color saturation; shorter runs (300 steps) recommended for controlled experiments.

- **Design tradeoffs**: Structured vs. natural captions: structure aids controllability but may mismatch user prompt styles at inference time. VLM recaptioning quality vs. scale: LLaVA-Next is slower than caption-dedicated models; 19M is computationally tractable, billions are not. LoRA rank: higher rank captures more structure-specific patterns but risks overfitting; rank 8–16 appears sufficient.

- **Failure signatures**: LLaVA looping: Model generates repetitive text (Figure 17); detect via excessive caption length or repeated phrases. Color saturation: Extended fine-tuning causes oversaturated outputs; mitigate by reducing steps or using lower learning rate. Aesthetic score inflation: Product images on white backgrounds score highly; luminance filtering partially addresses this.

- **First 3 experiments**:
  1. Ablate structure depth: Train with 2-slot (subject + setting) vs. 4-slot templates to determine if gains scale with granularity.
  2. Cross-dataset transfer: Fine-tune on structured Re-LAION, evaluate on unstructured prompts (e.g., COCO captions) to test out-of-distribution robustness.
  3. Inference-time structuring: Take user prompts, restructure them with an LLM into the 4-slot format, and compare adherence against raw prompts—tests whether training distribution alignment is the active mechanism.

## Open Questions the Paper Calls Out

1. Does the structured captioning benefit persist when pre-training text-to-image models from scratch on web-scale datasets (hundreds of millions of images) rather than just fine-tuning?

2. To what extent does the structured captioning paradigm transfer to other generative modalities, specifically text-to-video?

3. How sensitive is model performance to the specific choice of canonical structure (subject, setting, aesthetics, camera) versus other potential templates?

## Limitations

- The evaluation only uses structured captions, leaving open whether models trained on structured data can handle natural, free-form user prompts at inference time.

- The paper does not quantify hallucination rates in LLaVA-Next captions or compare caption quality against original alt-text, so the denoising effect is not empirically validated.

- Dataset filtering may introduce selection bias, particularly the OCR filtering removing text-heavy images while retaining aesthetic score outliers like product shots on white backgrounds.

## Confidence

- **High confidence**: The core empirical claim that structured captions outperform shuffled captions in VQA-based text-image alignment (0.8630 vs 0.8563 LLaVA score) is supported by controlled ablation.

- **Medium confidence**: The claim that VLM-generated captions improve upon original alt-text is plausible but under-supported; the slot-based attention mapping mechanism is reasonable but lacks direct mechanistic evidence.

- **Low confidence**: The generalizability of structured captions to real-world inference remains untested; no experiments probe performance on natural prompts or out-of-distribution captions.

## Next Checks

1. **Inference-time structuring ablation**: Take user prompts (e.g., COCO captions), restructure them into the 4-slot format with an LLM, and compare adherence against raw prompts. This directly tests whether training distribution alignment is the active mechanism.

2. **Cross-dataset transfer**: Fine-tune on structured Re-LAION, evaluate on unstructured prompts (e.g., Conceptual Captions or COCO). If performance drops, the benefit of structured training may not transfer.

3. **Caption quality audit**: Generate a random sample of LLaVA-Next captions, have them rated by humans for accuracy and hallucination, and compare against original alt-text. This quantifies the denoising effect and reveals whether hallucinations are a hidden failure mode.