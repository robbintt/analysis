---
ver: rpa2
title: Privacy Preserving Conversion Modeling in Data Clean Room
arxiv_id: '2505.14959'
source_url: https://arxiv.org/abs/2505.14959
tags:
- privacy
- data
- clean
- advertising
- conversion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a privacy-preserving CVR prediction framework
  for online advertising, addressing the challenge of sharing sensitive conversion
  data while training effective models. The method introduces batch-level aggregated
  gradients to protect privacy, adapter-based parameter-efficient fine-tuning to reduce
  communication costs, and label differential privacy with de-biasing techniques to
  maintain accuracy.
---

# Privacy Preserving Conversion Modeling in Data Clean Room

## Quick Facts
- arXiv ID: 2505.14959
- Source URL: https://arxiv.org/abs/2505.14959
- Reference count: 31
- Key outcome: Privacy-preserving CVR prediction framework using batch-level aggregated gradients, adapter-based fine-tuning, and label differential privacy achieves competitive ROC-AUC performance while reducing communication costs and maintaining privacy compliance

## Executive Summary
This paper addresses the challenge of privacy-preserving conversion modeling in data clean rooms for online advertising. The authors propose a framework that enables effective model training while protecting sensitive conversion data from advertisers. The approach combines batch-level aggregated gradients for privacy protection, adapter-based parameter-efficient fine-tuning for reduced communication costs, and label differential privacy with de-biasing techniques to maintain model accuracy.

The framework demonstrates that it can achieve up to 95% of the performance gain from fine-tuning all parameters while only tuning 1-2% of the parameters. Experimental results on industrial datasets show competitive ROC-AUC performance while significantly decreasing communication overhead and complying with advertiser and user privacy requirements. The method maintains calibration with minimal ROC-AUC drop when applying label differential privacy, making it suitable for real-world advertising applications.

## Method Summary
The paper introduces a privacy-preserving conversion modeling framework that addresses three key challenges in data clean room environments: privacy protection, communication efficiency, and model accuracy. The approach leverages batch-level aggregated gradients to protect privacy by preventing individual conversion data exposure during model training. Adapter-based parameter-efficient fine-tuning is implemented to reduce communication costs by only tuning a small fraction of model parameters (1-2%) instead of the entire network. Label differential privacy with de-biasing techniques is applied to ensure that conversion labels remain private while maintaining model performance through statistical calibration methods.

## Key Results
- Achieves up to 95% of performance gain from full fine-tuning while only tuning 1-2% of parameters
- Maintains competitive ROC-AUC performance with minimal accuracy drop when applying label differential privacy
- Significantly reduces communication overhead between advertisers and data clean room infrastructure

## Why This Works (Mechanism)
The framework works by creating multiple layers of privacy protection while maintaining model effectiveness. Batch-level aggregated gradients prevent reconstruction of individual conversion events by averaging gradients across multiple examples before sharing. The adapter-based fine-tuning approach reduces the attack surface by limiting the number of parameters that need to be communicated and updated. Label differential privacy adds calibrated noise to conversion labels, making it statistically difficult to infer individual user behavior while the de-biasing techniques compensate for the noise impact on model learning.

## Foundational Learning
- Batch-level gradient aggregation: Groups multiple examples to prevent individual data reconstruction, needed to protect conversion privacy during collaborative training
- Parameter-efficient fine-tuning: Updates only a small subset of model parameters through adapters, needed to reduce communication costs in data clean rooms
- Label differential privacy: Adds calibrated noise to labels while maintaining statistical utility, needed to protect sensitive conversion information
- De-biasing techniques: Compensates for noise introduced by differential privacy, needed to maintain model accuracy despite privacy protections
- Adapter modules: Small neural network components that can be inserted into larger models, needed for efficient parameter updates without full model communication
- ROC-AUC calibration: Adjusts model predictions to account for privacy-induced noise, needed to ensure reliable conversion rate predictions

## Architecture Onboarding

Component Map: Data Clean Room -> Batch Aggregator -> Adapter Modules -> Privacy Layer -> CVR Model

Critical Path: Advertiser Data → Batch Aggregation → Adapter Fine-tuning → Label DP Application → Model Output

Design Tradeoffs: Privacy vs. accuracy (stronger privacy reduces performance), communication cost vs. model quality (fewer parameters mean less communication but potential accuracy loss), computational efficiency vs. privacy guarantees (more complex privacy mechanisms require more computation)

Failure Signatures: Performance degradation when batch sizes are too small, communication bottlenecks when adapter sizes are too large, privacy violations when differential privacy parameters are too lenient

First Experiments:
1. Test baseline CVR model performance without privacy mechanisms
2. Evaluate communication cost reduction with different adapter configurations
3. Measure privacy-accuracy tradeoff across varying differential privacy budgets

## Open Questions the Paper Calls Out
None

## Limitations
- Privacy claims rely on theoretical assumptions about batch-level gradient aggregation that may not fully capture real-world adversarial scenarios
- Experimental validation limited to specific industrial datasets without comprehensive testing across diverse advertising domains
- Does not address potential privacy risks from auxiliary information or temporal patterns in conversion data

## Confidence
High Confidence: Core technical approach of using batch-level aggregated gradients and adapter-based fine-tuning is well-established in literature
Medium Confidence: Effectiveness of label differential privacy with de-biasing techniques demonstrated on tested datasets but generalizability needs validation
Low Confidence: Privacy guarantees against sophisticated adversarial attacks and practical deployment feasibility across heterogeneous advertiser infrastructures need rigorous evaluation

## Next Checks
1. Conduct adversarial testing with gradient-based reconstruction attacks to verify robustness of batch-level aggregation
2. Implement and test framework across multiple advertising verticals to assess performance consistency
3. Perform A/B testing in live advertising environment to validate practical impact of communication cost reductions