---
ver: rpa2
title: GNN's Uncertainty Quantification using Self-Distillation
arxiv_id: '2506.20046'
source_url: https://arxiv.org/abs/2506.20046
tags:
- uncertainty
- ensemble
- teacher
- self-distillation
- classifier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of quantifying predictive uncertainty
  in Graph Neural Networks (GNNs) for healthcare applications, where trustworthy predictions
  are critical. Existing methods like Bayesian inference and ensemble approaches are
  computationally expensive or lack precision in capturing model diversity.
---

# GNN's Uncertainty Quantification using Self-Distillation

## Quick Facts
- **arXiv ID**: 2506.20046
- **Source URL**: https://arxiv.org/abs/2506.20046
- **Reference count**: 34
- **Primary result**: Proposes self-distillation-based uncertainty quantification for GNNs that achieves performance comparable to MC Dropout and ensembles with 6x faster inference and 1.7x faster training

## Executive Summary
This paper addresses uncertainty quantification in Graph Neural Networks for healthcare applications where trustworthy predictions are critical. The authors propose a self-distillation approach that trains multiple classifiers within a single GNN architecture, using the deepest classifier as a teacher and shallower ones as students. This creates prediction diversity useful for uncertainty estimation without the computational cost of training separate models. Experiments on MIMIC-IV and Enzymes datasets show performance comparable to MC Dropout and ensemble methods while significantly reducing training and inference time.

## Method Summary
The method uses a GNN backbone (GraphConv for MIMIC-IV, GraphSAGE for Enzymes) with classifiers attached after each hidden layer. During training, the deepest classifier acts as a teacher, guiding shallower classifiers through KL divergence loss while all classifiers learn from ground truth via cross-entropy. The combined loss balances imitation against independent learning. Uncertainty is quantified using a weighted Jensen-Shannon divergence metric that assigns higher weights to disagreements between the teacher and deeper classifiers, based on the assumption that deeper classifier discrepancies indicate harder predictions.

## Key Results
- Training time (320.04s) comparable to single model (293.10s) vs. ensemble (880.71s)
- Inference time (0.50s) vs. MC Dropout (19.19s for 100 samples)
- Weighted uncertainty metrics distinguish hard examples better than standard disagreement
- Effective separation of in-distribution from out-of-distribution data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training multiple classifiers at different depths within a single GNN produces prediction diversity useful for uncertainty estimation without training separate models.
- Mechanism: The deepest classifier generates soft labels that guide shallower classifiers via KL divergence loss, while all classifiers learn from ground truth through cross-entropy. This creates representations at varying depths with controlled diversity.
- Core assumption: Deeper classifiers access richer representations; divergence from the teacher by deeper students signals prediction difficulty.
- Evidence anchors:
  - [abstract] "We apply self-distillation, where the same network serves as both the teacher and student models, thereby avoiding the need to train several networks independently."
  - [Section 3.1] "In self-distillation, a classifier is placed after each hidden layer, where the classifier connected to the deepest layer acts as a teacher."
- Break condition: If α (imitation parameter) is too high, shallow classifiers collapse to teacher copies, eliminating diversity needed for meaningful uncertainty.

### Mechanism 2
- Claim: Weighting classifier disagreement by depth yields more precise uncertainty estimates than treating all predictions equally.
- Mechanism: The metric uses bounded Jensen-Shannon divergence instead of unbounded KL divergence. Weights increase monotonically with depth: when a deeper student's predicted class disagrees with the teacher, it receives higher weight because deeper layers exploit more informative features.
- Core assumption: A discrepancy between teacher and deeper classifiers on predicted class indicates a challenging input that warrants higher uncertainty.
- Evidence anchors:
  - [Section 3.3] "Since deeper classifiers utilize richer representations, a discrepancy between the teacher and deeper classifiers for classifying a data point indicates that the classifier is having difficulty aligning itself with the teacher."
- Break condition: Poorly calibrated weights either flatten all disagreements (degenerates to standard metric) or let noisy shallow predictions dominate.

### Mechanism 3
- Claim: Self-distillation achieves uncertainty quantification comparable to ensembles and MC Dropout with substantially lower training and inference cost.
- Mechanism: A single forward pass produces predictions from all classifiers simultaneously. Unlike ensembles (N separate GNNs) or MC Dropout (N stochastic forward passes), the multi-classifier architecture requires only one deterministic pass.
- Core assumption: Diversity from classifiers at different depths approximates diversity from independently trained models or stochastic samples.
- Evidence anchors:
  - [Table 2] Self-distillation training (320.04s) ≈ single model (293.10s) vs. ensemble (880.71s); inference (0.50s) vs. MC Dropout (19.19s for 100 samples).
- Break condition: Over-regularization toward teacher eliminates the diversity that uncertainty estimation requires.

## Foundational Learning

- **Knowledge Distillation**:
  - Why needed here: Self-distillation is a variant of teacher-student learning. Understanding soft labels, temperature scaling, and KL divergence's role is essential to interpret the loss function.
  - Quick check question: Why do soft probability distributions convey more information to a student model than hard class labels?

- **Uncertainty Quantification Paradigms**:
  - Why needed here: The paper positions its method against MC Dropout and deep ensembles. Understanding epistemic vs. aleatoric uncertainty and disagreement metrics clarifies the design motivation.
  - Quick check question: What are the computational tradeoffs between MC Dropout (multiple stochastic passes) and deep ensembles (multiple trained models)?

- **Graph Neural Networks**:
  - Why needed here: The architecture uses GraphSAGE/GraphConv with graph-level classification. Understanding message passing and graph pooling is necessary for modifying the backbone.
  - Quick check question: How does a GNN aggregate node-level information to produce a graph-level prediction?

## Architecture Onboarding

- **Component map**: GNN backbone -> Harmonization layers (if needed) -> Classifiers (MLP heads after each hidden layer)

- **Critical path**:
  1. Forward pass through GNN layers
  2. Each layer output → harmonization (if needed) → classifier → predictions
  3. Final layer = teacher; earlier layers = students
  4. Training: minimize CE (labels) + KL (teacher soft labels) + L2 penalty (features)
  5. Inference: single pass → compute weighted JSD → normalize by U_Cmax

- **Design tradeoffs**:
  - α (imitation parameter): Paper uses 0.6. Higher values increase teacher alignment but reduce diversity.
  - λ (feature penalty): Paper uses 0.04. Higher values force feature-level convergence.
  - Layers/classifiers: More layers increase uncertainty signal capacity but add parameters.
  - Weight function: Linear (Eq. 9) vs. nonlinear (Eq. 10). Nonlinear amplifies disagreement more aggressively.

- **Failure signatures**:
  - Uniformly high uncertainty on ID data: Weight function may be too aggressive.
  - Low OOD uncertainty: Potential mode collapse; verify α is not excessively high.
  - Training instability near convergence: KL divergence near zero probabilities; JSD mitigates this.
  - Performance drop vs. baseline: Check harmonization layer dimensions and teacher convergence before strong distillation.

- **First 3 experiments**:
  1. Reproduce MIMIC-IV or Enzymes results using the public code (https://github.com/tailabTMU/UQ_GNN) to validate Table 2 metrics.
  2. Ablate α ∈ {0.0, 0.3, 0.6, 0.9} and measure: (a) F1/ROC-AUC, (b) ID vs. OOD entropy separation, (c) uncertainty metric distribution.
  3. Compare linear vs. nonlinear weight functions on a held-out set with annotated hard examples to assess which better separates easy from difficult predictions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed uncertainty quantification method be generalized to alternative self-distillation strategies, such as ensemble teacher or dense teacher distillation?
- Basis in paper: [explicit] The authors state, "As a future direction, we plan to explore alternative teacher selection methods (e.g., ensemble teacher distillation, transitive teacher distillation, and dense teacher distillation)... to confirm that our proposed technique... can be applied to different self-distillation strategies."
- Why unresolved: The current implementation relies specifically on the deepest classifier acting as the teacher for shallower students, and it is untested whether the weighted disagreement metric remains valid or effective when the teacher is an ensemble or involves transitive knowledge transfer.
- What evidence would resolve it: Empirical evaluation of the uncertainty metric's precision and performance when integrated with ensemble or dense teacher distillation architectures on the same datasets.

### Open Question 2
- Question: How does the choice of linear versus non-linear weight functions impact the precision and conservative nature of uncertainty quantification across different domains?
- Basis in paper: [explicit] The paper proposes a linear and a non-linear weight function but notes, "The choice between linear or non-linear weight functions can be considered as a domain-specific parameter... understanding the impact requires further empirical investigation."
- Why unresolved: The paper introduces the non-linear option to potentially amplify disagreement effects but does not provide a theoretical or empirical basis for selecting one over the other for specific tasks.
- What evidence would resolve it: A comparative ablation study analyzing the sensitivity and calibration of the uncertainty metric using both weight functions on diverse graph datasets.

### Open Question 3
- Question: What is the sensitivity of the uncertainty quantification to the imitation (α) and trade-off (λ) hyperparameters used during the self-distillation training process?
- Basis in paper: [explicit] The authors mention, "Limited hyper-parameter tuning of α showed that 0.6 yielded lower mean uncertainty without complete teacher copying. We acknowledge that a more in-depth ablation study is necessary."
- Why unresolved: While α=0.6 and λ=0.04 were selected for the experiments, the robustness of the uncertainty estimates relative to variations in these parameters has not been established.
- What evidence would resolve it: An ablation study demonstrating the correlation between different values of α and λ and the resulting calibration errors and uncertainty precision.

## Limitations

- **Reproducibility barriers**: Key hyperparameters (hidden layer sizes, learning rate) are unspecified, requiring arbitrary choices that may affect results. The MIMIC-IV graph construction follows an external reference [4], creating potential deviation from the original setup.
- **Depth-weighted uncertainty validity**: While the paper claims weighted Jensen-Shannon divergence better captures prediction difficulty, there is no ablation study comparing it against unweighted alternatives on the same datasets.
- **Diversity sufficiency**: The paper asserts that self-distillation produces adequate diversity for uncertainty estimation, but provides no direct comparison of the diversity distribution against true ensemble methods.

## Confidence

- **High confidence**: The computational efficiency advantage is well-supported by timing comparisons in Table 2, showing 6x faster inference than MC Dropout and 1.7x faster training than ensembles.
- **Medium confidence**: The performance parity claim (comparable F1/ROC-AUC to baselines) is supported by Table 2, but the absolute values are not extraordinary, and the statistical significance of differences is not reported.
- **Low confidence**: The claim that deeper classifier disagreement specifically indicates prediction difficulty lacks direct empirical validation. The weight functions (linear vs. nonlinear) are proposed but not rigorously compared.

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary α ∈ {0.0, 0.3, 0.6, 0.9} and λ ∈ {0.01, 0.04, 0.1}, measuring (a) F1/ROC-AUC, (b) ID vs. OOD uncertainty separation, and (c) diversity metrics like inter-classifier correlation.

2. **Diversity validation**: Compare the diversity distribution (e.g., Jensen-Shannon divergence between classifier pairs) from self-distillation against true ensemble methods on the same dataset to verify if the claimed approximation holds.

3. **Weighted vs. unweighted uncertainty**: Conduct an ablation study using the same datasets and train/validation splits, comparing the proposed depth-weighted JSD against standard (unweighted) JSD and other uncertainty metrics like entropy, measuring calibration error and OOD detection AUROC.