---
ver: rpa2
title: 'First Finish Search: Efficient Test-Time Scaling in Large Language Models'
arxiv_id: '2505.18149'
source_url: https://arxiv.org/abs/2505.18149
tags:
- correct
- arxiv
- then
- therefore
- compute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: First Finish Search (FFS) is a training-free test-time scaling
  method for large language models that improves reasoning accuracy by launching multiple
  parallel samples and returning the first completed trace. Motivated by the observation
  that shorter reasoning traces are more likely to be correct, FFS achieves 82.23%
  accuracy on AIME datasets with DeepSeek-R1, a 15% improvement over standalone performance
  and near OpenAI o4-mini levels, while reducing token usage by up to 45% compared
  to baselines like majority voting.
---

# First Finish Search: Efficient Test-Time Scaling in Large Language Models

## Quick Facts
- arXiv ID: 2505.18149
- Source URL: https://arxiv.org/abs/2505.18149
- Authors: Aradhye Agarwal; Ayan Sengupta; Tanmoy Chakraborty
- Reference count: 40
- One-line primary result: First Finish Search achieves 82.23% accuracy on AIME datasets with DeepSeek-R1, a 15% improvement over standalone performance and near OpenAI o4-mini levels, while reducing token usage by up to 45% compared to baselines like majority voting.

## Executive Summary
First Finish Search (FFS) is a training-free test-time scaling method that improves reasoning accuracy in large language models by launching multiple parallel samples and returning the first completed trace. Motivated by the observation that shorter reasoning traces are more likely to be correct, FFS achieves state-of-the-art results on AIME benchmarks while significantly reducing computational costs compared to existing methods. The approach is particularly effective for reasoning-focused models and offers a theoretical efficiency guarantee based on extreme value theory.

## Method Summary
FFS operates by launching n independent parallel samples of a reasoning task and returning immediately when any one completes. The method assumes that correct traces tend to be shorter than incorrect ones in reasoning models, so returning the first finished trace increases the probability of correctness. The paper implements both synchronous (Sync-FFS) and asynchronous (Async-FFS) variants, using stochastic decoding with beam size 1, temperature 0.6, and top_p 0.95. The method includes a fallback mechanism that returns the first trace if no sample reaches EOS within the maximum length.

## Key Results
- FFS achieves 82.23% accuracy on AIME24 with DeepSeek-R1, a 15% improvement over standalone performance
- Sequential token usage reduced by 32% compared to simple decoding (7.8K vs 11.4K tokens)
- FFS matches or exceeds majority voting accuracy while using 26-45% fewer total tokens
- Theoretical analysis shows sequential cost decreases as O(√log n) with more samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shorter reasoning traces are more likely to be correct in reasoning-focused models.
- Mechanism: Correct traces begin from a valid initial state; when the model has higher capability, it maintains correctness with fewer reasoning steps. Incorrect traces require meandering exploration or recovery attempts, lengthening generation.
- Core assumption: The model has been trained to internalize multi-step reasoning (e.g., DeepSeek-R1, QwQ-32B). Non-reasoning models do not exhibit this pattern.
- Evidence anchors:
  - [abstract] "We observe the surprising fact that for reasoning tasks, shorter traces are much more likely to be correct than longer ones."
  - [PAGE 2, Figure 1] Distribution plots show separation between correct/incorrect trace lengths with Welch statistic 16.56 (p < 0.001).
  - [PAGE 7, Table 4] Predicted FFS success rates: 92% (R1-Distill-Qwen), 97% (QwQ-32B), but only 66% for non-reasoning DeepSeek-V3.
  - [corpus] Related TTS papers acknowledge efficiency benefits but do not contradict the length-correctness correlation for reasoning models.
- Break condition: On non-reasoning models or tasks where extended deliberation is mechanistically required (not just a side effect of confusion), the correlation weakens and FFS may underperform.

### Mechanism 2
- Claim: Parallel sampling with first-to-finish selection aggregates correctness probability without requiring answer comparison.
- Mechanism: By launching n independent stochastic samples (beam size 1), FFS increases the chance that at least one short correct trace is generated. Returning immediately upon first EOS avoids waiting for all samples, reducing sequential compute.
- Core assumption: Diversity from stochastic decoding (temperature > 0) produces at least some traces that take meaningfully different paths and lengths.
- Evidence anchors:
  - [abstract] "FFS...launches n independent samples and returns as soon as any one completes."
  - [PAGE 4-5, Algorithms 1-2] Sync-FFS batches n sequences; Async-FFS runs distributed jobs; both terminate early.
  - [PAGE 7, Table 2-3] FFS matches or exceeds majority voting accuracy while using 26-45% fewer total tokens.
  - [corpus] No corpus papers dispute the efficiency of early-termination parallel sampling for inference scaling.
- Break condition: If n is too small (e.g., 1-2) or temperature is too low, diversity collapses and FFS degenerates toward simple decoding with no scaling benefit.

### Mechanism 3
- Claim: Sequential cost decreases as O(√log n) with more samples.
- Mechanism: From extreme value theory (Result 2), the expected minimum of n i.i.d. normally distributed random variables shrinks as μ − σ√(2 log n). Applied to trace lengths, more parallel samples yield a shorter expected first-finish time.
- Core assumption: Trace lengths are approximately normally distributed (validated per dataset via Shapiro-Wilk tests on page 6).
- Evidence anchors:
  - [PAGE 5-6, Result 2] Formal derivation connecting extreme value theory to FFS cost scaling.
  - [PAGE 7, Table 2-3] Sequential tokens drop from 11.4K (simple decoding) to 7.8K (FFS) for R1-Distill-Qwen.
  - [corpus] Corpus evidence on this specific theoretical result is absent; related work focuses on empirical TTS comparisons, not asymptotic cost bounds.
- Break condition: If trace-length distributions become heavy-tailed (very long outliers), normal approximation weakens and the O(√log n) bound may underpredict actual cost.

## Foundational Learning

- Concept: Test-Time Scaling (TTS)
  - Why needed here: FFS is a TTS method; understanding the broader paradigm clarifies how dynamic compute allocation differs from training-time scaling.
  - Quick check question: Can you explain why adding compute at inference time might improve reasoning without changing model weights?

- Concept: Extreme Value Theory (EVT)
  - Why needed here: The theoretical justification for FFS's efficiency relies on EVT results about the expected minimum of sampled distributions.
  - Quick check question: What does EVT predict about the distribution of minima as sample size n increases?

- Concept: Hensel's Lemma (for the AIME trace analysis)
  - Why needed here: The paper's example traces use Hensel's lemma for lifting modular solutions; understanding this helps interpret correctness of mathematical reasoning.
  - Quick check question: Under what conditions does Hensel's lemma guarantee a unique lift of a solution from modulo p to modulo p²?

## Architecture Onboarding

- Component map: Input prompt → n parallel stochastic decoders → EOS detection → First trace return
- Critical path:
  1. Load model once (Sync-FFS) or distribute across workers (Async-FFS)
  2. For each decoding step, generate one token per trace in batch
  3. Check for EOS after each step; if found, immediately return
  4. If budget L exhausted without EOS, apply fallback
- Design tradeoffs:
  - Sync vs Async: Sync minimizes memory (one model copy); Async minimizes latency on distributed hardware but adds orchestration overhead.
  - Sample count n: Higher n improves accuracy and lowers sequential cost but increases total parallel compute. Diminishing returns set in as √log n.
  - Temperature: Higher values increase diversity and short-trace variance, but may also increase gibberish risk.
- Failure signatures:
  - Non-reasoning models: FFS underperforms majority voting (e.g., DeepSeek-V3 in Table 3c).
  - Low temperature: Outputs converge, eliminating first-finish advantage.
  - Heavy-tailed trace distributions: Theoretical cost savings overestimated.
  - Token limit exhaustion: No trace finishes; fallback degrades to simple decoding behavior.
- First 3 experiments:
  1. Replicate Table 2 results on R1-Distill-Qwen with n=4 on AIME24. Measure accuracy, sequential tokens, total tokens. Expect ~80% accuracy with ~7.8K sequential tokens.
  2. Ablate temperature: Run FFS at temp ∈ {0.3, 0.6, 0.9}. Plot accuracy vs. sequential cost. Expect U-shaped curve where very low or very high temp reduces benefit.
  3. Cross-model validation: Apply FFS to a non-reasoning model (e.g., GPT-4o without CoT prompting) on GPQA. Expect minimal gain vs. majority voting, confirming the reasoning-model dependence claimed in Section 5.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can FFS be effectively combined with confidence-based fallback strategies (e.g., revisional decoding or sequential scaling when model confidence is low) to create a hybrid system that adapts to task difficulty?
- Basis in paper: [explicit] "Future work could combine FFS with deeper or revisional decoding when model confidence is low, producing a hybrid system that adapts to task difficulty and mitigates the above limitation."
- Why unresolved: The paper only evaluates FFS as a standalone method; no hybrid variants are tested or analyzed.
- What evidence would resolve it: Experiments comparing FFS alone versus FFS with confidence thresholds triggering sequential scaling methods like budget forcing on ambiguous or difficult inputs.

### Open Question 2
- Question: What modifications would make FFS effective for non-reasoning models where the correlation between trace length and correctness is weaker?
- Basis in paper: [explicit] "FFS assumes that correct traces tend to terminate earlier than incorrect ones. This pattern holds for the reasoning-oriented models we study, but it is weaker for non-reasoning models such as DeepSeek-V3."
- Why unresolved: The paper identifies the limitation but proposes no solutions; FFS underperforms MV on DeepSeek-V3.
- What evidence would resolve it: A modified FFS variant achieving parity or better with MV on non-reasoning models across GPQA and AIME benchmarks.

### Open Question 3
- Question: How does FFS performance scale with sample counts beyond n=4, and does the O(√log n) efficiency gain hold in practice at larger scales?
- Basis in paper: [inferred] All experiments use n=4 samples; theoretical analysis predicts O(√log n) sequential cost reduction, but empirical validation at higher n is absent.
- Why unresolved: No ablation study examines performance at n=8, 16, 32, or higher, leaving scaling behavior unverified.
- What evidence would resolve it: Experiments tracking accuracy and token costs for n ∈ {4, 8, 16, 32, 64} on AIME and GPQA, comparing observed scaling to theoretical predictions.

### Open Question 4
- Question: Does the normal distribution assumption for trace lengths hold across diverse reasoning domains (e.g., code generation, logical deduction, scientific reasoning)?
- Basis in paper: [inferred] The theoretical analysis (Result 1, Result 2) assumes normal distributions, and Figure 3 shows heavy-tailed departures "beyond moderate lengths" on AIME; the paper notes the analysis is "not meaningful" for LFS at x→∞.
- Why unresolved: Only mathematical and science QA tasks are tested; no validation on other reasoning domains where distributions may differ.
- What evidence would resolve it: Shapiro-Wilk tests and distribution plots for trace lengths on code generation (HumanEval), logical reasoning (LogiQA), and multi-step planning benchmarks.

## Limitations

- The method's effectiveness is limited to reasoning models where shorter traces correlate with correctness; it underperforms on non-reasoning models like DeepSeek-V3.
- The theoretical efficiency analysis relies on normal distribution assumptions for trace lengths, which may not hold in all scenarios or across different reasoning domains.
- Implementation details for baseline methods like Budget Forcing are underspecified, making direct comparisons challenging to validate.

## Confidence

**High confidence:** The core empirical finding that FFS achieves 82.23% accuracy on AIME24 with DeepSeek-R1 while reducing sequential tokens by ~32% is well-supported by the reported experiments. The correlation between shorter traces and correctness in reasoning models is statistically significant (p < 0.001).

**Medium confidence:** The theoretical efficiency analysis based on extreme value theory is internally consistent, but lacks external validation. The normal distribution assumption for trace lengths is plausible but not definitively established across all settings.

**Low confidence:** The precise boundary conditions for when FFS outperforms other methods (e.g., exact model capabilities needed, optimal temperature ranges) are not fully characterized. The comparison to Budget Forcing is difficult to fully evaluate due to implementation ambiguities.

## Next Checks

1. **Temperature ablation study:** Systematically vary temperature from 0.3 to 0.9 in 0.1 increments for FFS on AIME24 with DeepSeek-R1. Plot accuracy, sequential tokens, and total tokens vs. temperature to identify the optimal range and verify the claimed U-shaped relationship.

2. **Cross-model boundary testing:** Apply FFS to a spectrum of models ranging from clearly non-reasoning (DeepSeek-V3) to clearly reasoning (DeepSeek-R1) to models with uncertain reasoning capabilities (GPT-4o without explicit CoT prompting). Verify that performance degrades smoothly as reasoning capability decreases, confirming the model-dependence claim.

3. **Distribution validation:** For each dataset/model combination used in the paper, collect trace length data and perform comprehensive distribution fitting (normal, lognormal, gamma, etc.). Use AIC/BIC to determine the best-fitting distribution and verify whether the normal approximation used in the theoretical analysis is appropriate.