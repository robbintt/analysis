---
ver: rpa2
title: 'How Far Can LLMs Emulate Human Behavior?: A Strategic Analysis via the Buy-and-Sell
  Negotiation Game'
arxiv_id: '2511.17990'
source_url: https://arxiv.org/abs/2511.17990
tags:
- negotiation
- persona
- seller
- buyer
- price
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates how well LLMs can imitate human-like social\
  \ behaviors and emotional expressions by conducting a negotiation simulation using\
  \ the Buy and Sell game. The core method involves assigning different personas (e.g.,\
  \ Competitive, Altruistic, Cunning) to LLMs and analyzing their negotiation outcomes\u2014\
  such as win rates, transaction prices, and SHAP values\u2014to measure behavioral\
  \ imitation and strategic decision-making."
---

# How Far Can LLMs Emulate Human Behavior?: A Strategic Analysis via the Buy-and-Sell Negotiation Game

## Quick Facts
- **arXiv ID**: 2511.17990
- **Source URL**: https://arxiv.org/abs/2511.17990
- **Reference count**: 28
- **Primary result**: Models with higher traditional benchmark scores generally perform better in negotiations, but exceptions reveal that benchmarks don't fully capture social interaction skills.

## Executive Summary
This paper evaluates how well LLMs can imitate human-like social behaviors and emotional expressions by conducting a negotiation simulation using the Buy and Sell game. The core method involves assigning different personas (e.g., Competitive, Altruistic, Cunning) to LLMs and analyzing their negotiation outcomes—such as win rates, transaction prices, and SHAP values—to measure behavioral imitation and strategic decision-making. Results show that models with higher benchmark scores generally perform better in negotiations, though exceptions like GPT-4-Turbo reveal that traditional benchmarks don't fully capture social interaction skills. Personas significantly influence outcomes: Competitive and Cunning personas yield higher win rates and better prices, while Altruistic and Cooperative personas often lead to worse results. This highlights the need for negotiation simulations as a complementary metric for assessing LLMs' real-world interaction capabilities.

## Method Summary
The study uses a turn-based Buy-and-Sell negotiation game where LLMs act as buyers or sellers with asymmetric information (sellers know production costs, buyers know maximum willingness to pay). Six models are tested across seven personas, with each model running 1,737+ negotiation rounds. The game uses a 10-turn limit, with a baseline price of 50 ZUP determining win/loss outcomes. Transaction prices, win rates, and SHAP values (computed via XGBoost) quantify persona impact on negotiation outcomes. The evaluation compares model performance against traditional benchmarks like MMLU and HumanEval to assess social-strategic capabilities.

## Key Results
- Models with higher benchmark scores generally achieve better negotiation outcomes, though exceptions exist (GPT-4-Turbo: 86.6% HumanEval but only 8% Seller win rate)
- Competitive and Cunning personas yield higher win rates and better transaction prices compared to Altruistic and Cooperative personas
- Claude-3.5-Sonnet shows the highest responsiveness to persona assignments (total SHAP range 18.90), while GPT-4-Turbo shows minimal variation (10.45)
- Buyers achieve slightly higher win rates (53.48%) than sellers (40.99%) under the current game parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Persona assignment causally influences negotiation outcomes, with aggressive/strategic personas outperforming cooperative ones.
- Mechanism: Prompt-based persona instructions bias the LLM's token generation toward specific negotiation strategies (e.g., high anchoring for Competitive, concession-making for Altruistic). SHAP analysis quantifies each persona's marginal contribution to final price.
- Core assumption: LLMs can consistently maintain persona-driven behavior patterns across multi-turn dialogue without drift.
- Evidence anchors:
  - [abstract] "competitive and cunning traits prove more advantageous for negotiation outcomes than altruistic and cooperative traits"
  - [section IV-D, Table IV] Competitive Sellers: 65% win rate vs. Altruistic: 15%
  - [section IV-E, Figure 3] Competitive SHAP +7.33 (Seller); Altruistic negative SHAP
  - [corpus] Related work on LLM negotiation (Bianchi et al., Davidson et al.) confirms persona effects but lacks SHAP quantification
- Break condition: If persona prompts leak into opponent's context, or if model ignores persona under time pressure, the effect degrades.

### Mechanism 2
- Claim: Standard benchmarks imperfectly predict social-strategic performance; negotiation games capture orthogonal capabilities.
- Mechanism: Benchmarks like MMLU/HumanEval test knowledge retrieval and code generation, not sequential social reasoning. Negotiation requires theory-of-mind, emotional calibration, and adaptive strategy—skills not exercised in QA tasks.
- Core assumption: Negotiation performance reflects real-world social interaction capability, not just game-specific optimization.
- Evidence anchors:
  - [section IV-F, Table VII] GPT-4-Turbo: 86.6% HumanEval but 8% Seller win rate
  - [abstract] "existing benchmarks focus primarily on knowledge-based assessment and thus fall short"
  - [corpus] SPIN-Bench paper (arXiv 2503.12349) similarly argues social reasoning is distinct from static planning
- Break condition: If models overfit to negotiation game mechanics without transfer, the metric loses validity.

### Mechanism 3
- Claim: Role assignment (Buyer vs. Seller) and information asymmetry create structural advantage patterns.
- Mechanism: Sellers anchor high; Buyers counter low. Buyers face budget constraints (willingness-to-pay ceiling), Sellers face cost floor. Turn limits pressure both parties asymmetrically.
- Core assumption: The 10-turn limit and baseline price (50 ZUP) reasonably approximate real negotiation constraints.
- Evidence anchors:
  - [section IV-B, Table I] Buyer win rate 53.48% vs. Seller 40.99%
  - [section II-A] "only the Seller knows the exact production costs, and only the Buyer knows their specific willingness"
  - [corpus] Weak direct evidence; related papers don't systematically compare role advantages
- Break condition: If baseline price is mis calibrated (too close to cost or budget), advantage shifts dramatically.

## Foundational Learning

- **SHAP (Shapley Additive Explanations) Values**
  - Why needed here: The paper uses SHAP to quantify how much each persona contributes to negotiation outcomes. Understanding feature attribution is essential for interpreting the results.
  - Quick check question: If a feature has SHAP value +3 for predicting sale price, what does that mean?

- **Asymmetric Information Games (Game Theory)**
  - Why needed here: The Buy-and-Sell game explicitly models information asymmetry (Seller knows cost, Buyer knows budget). This is core to why negotiations unfold as they do.
  - Quick check question: In a negotiation where only one party knows the true value of an item, who has the strategic advantage and why?

- **Persona Prompting for LLMs**
  - Why needed here: The entire experimental design depends on persona prompts inducing consistent behavioral patterns in LLMs.
  - Quick check question: What could cause an LLM to deviate from its assigned persona during a 10-turn conversation?

## Architecture Onboarding

- **Component map:** Game Engine -> LLM Agents -> Persona Module -> Evaluation Layer
- **Critical path:** 1. Initialize game: Assign roles, set hidden parameters (Seller cost ~40 ZUP, Buyer budget ~60 ZUP) 2. Seller makes opening offer 3. Alternating turns (max 10): Propose → Counter-propose or Accept/Reject 4. If Accept: Record transaction price, classify winner relative to 50 ZUP baseline 5. If turn 10 with no deal: Record as draw 6. Aggregate 1,737+ rounds, compute SHAP values
- **Design tradeoffs:** 10-turn limit balances realism vs. API cost; may truncate optimal strategies; Single baseline price (50 ZUP) simplifies win classification but may not reflect nuanced outcomes; No repeated games means trust-building and reputation effects aren't captured
- **Failure signatures:** High draw rate with specific personas (e.g., Altruistic Seller at 8% draw rate) suggests excessive concession or unwillingness to close; Extreme win-rate divergence between roles for same model (e.g., GPT-4-Turbo: 8% Seller, 48% Buyer) indicates role-specific failure modes; Low SHAP total range (GPT-4-Turbo: 10.45) suggests model ignores persona cues
- **First 3 experiments:** 1. Baseline replication: Run 100 rounds with Control persona on both sides to establish role-advantage baseline independent of persona effects 2. Persona ablation: Hold model constant, vary only persona; confirm SHAP-documented effects are reproducible and not noise 3. Model swap test: Same persona pair, different underlying models; isolate whether strategic behavior is model-inherent or persona-induced

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can negotiation simulation performance be improved for models like GPT-4-Turbo that underperform despite high traditional benchmark scores?
- Basis in paper: [explicit] The authors note GPT-4-Turbo "excels in conventional metrics but underperforms in negotiation scenarios emphasizing social and strategic contexts," highlighting "the insufficiency of current benchmarks."
- Why unresolved: The paper identifies the discrepancy but does not investigate what specific training or architectural factors cause it, nor how to address it.
- What evidence would resolve it: Targeted fine-tuning experiments on social interaction tasks, or ablation studies identifying which model components correlate with negotiation performance.

### Open Question 2
- Question: How would LLMs perform in multi-party negotiations compared to the bilateral Buyer-Seller setup?
- Basis in paper: [explicit] The conclusion states: "Future research could explore multi-party negotiations... to more precisely gauge LLMs' human behavioral imitation and real-world applicability."
- Why unresolved: The current study only tests two-party negotiations with asymmetric information; multi-party dynamics introduce coalition-building and more complex strategic considerations.
- What evidence would resolve it: Extending the negotiation game framework to 3+ participants and comparing outcomes, strategy diversity, and agreement rates.

### Open Question 3
- Question: What mechanisms explain why some models (e.g., Claude-3.5) are highly responsive to persona assignments while others (e.g., GPT-4-Turbo) show minimal variation?
- Basis in paper: [inferred] The paper reports Claude-3.5 has a "total range" of 18.90 versus GPT-4-Turbo's 10.45, noting "different models implement conversation strategies and emotional imitations to varying extents," but does not explain why.
- Why unresolved: The SHAP analysis quantifies the effect size but does not identify whether differences stem from training data, alignment procedures, or architectural choices.
- What evidence would resolve it: Comparative analysis of model training methodologies, or controlled experiments varying instruction-following capabilities across model families.

### Open Question 4
- Question: How closely do LLM negotiation behaviors match actual human negotiation patterns under controlled conditions?
- Basis in paper: [inferred] The paper frames its goal as evaluating "how accurately LLMs can reproduce real human social behaviors" and claims to measure "human-like" communication, but conducts no human baseline comparison.
- Why unresolved: Without human data using the same game parameters, it remains unclear whether Competitive/Cunning personas actually reflect realistic human strategies or stereotyped approximations.
- What evidence would resolve it: Running identical Buy-and-Sell games with human participants using the same persona instructions, then comparing price distributions, win rates, and conversational patterns.

## Limitations
- Persona prompt construction is described but not shown verbatim, making behavioral attribution to specific traits uncertain
- No repeated game dynamics means trust-building effects and reputation are not captured
- Single baseline price (50 ZUP) oversimplifies outcome classification in real negotiations

## Confidence
- **Claim: Persona effects on negotiation outcomes are robust** → Medium
- **Claim: Benchmark divergence from social performance is meaningful** → Medium
- **Claim: SHAP values accurately capture persona contribution** → High

## Next Checks
1. **Prompt fidelity test**: Reconstruct persona prompts from trait descriptions and verify they induce consistent behavioral patterns across multiple runs
2. **Role-advantage isolation**: Run control experiments (no personas) to establish baseline win rates for Buyer vs. Seller, then measure persona effect magnitude
3. **Transfer validation**: Test whether models that perform well in negotiation games also show improved performance in other social reasoning tasks (e.g., SPIN-Bench scenarios)