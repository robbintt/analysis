---
ver: rpa2
title: 'FASP: Fast and Accurate Structured Pruning of Large Language Models'
arxiv_id: '2501.09412'
source_url: https://arxiv.org/abs/2501.09412
tags:
- pruning
- fasp
- arxiv
- performance
- sparsity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FASP (Fast and Accurate Structured Pruning),
  a novel method for compressing large language models (LLMs) that achieves both high
  speed and maintained accuracy. FASP addresses the computational and memory demands
  of LLMs by using a distinctive pruning structure that links sequential layers, allowing
  the removal of columns in one layer while eliminating corresponding rows in the
  preceding layer without performance loss.
---

# FASP: Fast and Accurate Structured Pruning of Large Language Models

## Quick Facts
- arXiv ID: 2501.09412
- Source URL: https://arxiv.org/abs/2501.09412
- Reference count: 4
- Primary result: Achieves significant speed-ups (17s for OPT-125M, 15min for LLaMA-30B) while maintaining accuracy on downstream tasks

## Executive Summary
This paper introduces FASP (Fast and Accurate Structured Pruning), a novel method for compressing large language models that achieves both high speed and maintained accuracy. FASP addresses the computational and memory demands of LLMs by using a distinctive pruning structure that links sequential layers, allowing the removal of columns in one layer while eliminating corresponding rows in the preceding layer without performance loss. The method employs a computationally efficient pruning metric inspired by Wanda to select components to prune and includes a restoration mechanism to adjust remaining weights post-pruning for enhanced model fidelity.

Experiments on OPT and LLaMA model families demonstrate that FASP outperforms state-of-the-art methods in terms of perplexity and accuracy on downstream tasks. Specifically, FASP achieves significant speed-ups, pruning OPT-125M in 17 seconds and LLaMA-30B in 15 minutes on a single NVIDIA RTX 4090 GPU, making it a highly practical solution for optimizing LLMs while maintaining high performance.

## Method Summary
FASP introduces a novel structured pruning approach that connects sequential layers through a column-row pruning structure. When pruning columns in one layer, the corresponding rows in the preceding layer are also removed, maintaining model functionality while reducing computational load. The method uses a Wanda-inspired pruning metric that evaluates weight importance based on their contribution to output variance, enabling efficient selection of components for removal. A key innovation is the restoration mechanism that adjusts remaining weights after pruning to preserve model fidelity. This mechanism compensates for the weight distribution changes caused by pruning, ensuring that the compressed model maintains performance comparable to the original. The entire process is designed for computational efficiency, allowing rapid pruning even on large models.

## Key Results
- Achieved 17-second pruning time for OPT-125M model on a single RTX 4090 GPU
- Pruned LLaMA-30B in 15 minutes while maintaining competitive perplexity scores
- Outperformed state-of-the-art pruning methods on downstream task accuracy metrics

## Why This Works (Mechanism)
FASP works by exploiting the mathematical relationship between adjacent layers in neural networks. By pruning columns in one layer and corresponding rows in the previous layer, the method maintains the essential information flow while reducing dimensionality. The Wanda-inspired metric efficiently identifies weights that contribute most to output variance, ensuring that the most important components are preserved. The restoration mechanism then recalibrates the remaining weights to compensate for the structural changes, maintaining the model's ability to capture complex patterns despite the reduction in parameters.

## Foundational Learning
- **Structured vs. unstructured pruning**: Structured pruning removes entire neurons or channels, while unstructured pruning removes individual weights; structured pruning offers better hardware acceleration but typically lower compression ratios. Why needed: To understand the fundamental trade-offs in different pruning approaches. Quick check: Verify that FASP uses structured pruning by examining the column-row removal pattern.
- **Weight importance metrics**: Methods like magnitude-based pruning, second-order derivatives, and variance-based metrics evaluate which weights are most critical. Why needed: To understand how FASP selects weights for pruning. Quick check: Compare FASP's metric to other importance measures in the literature.
- **Layer-wise dependencies**: Changes in one layer affect the input distribution of subsequent layers, requiring careful consideration of pruning order and strategy. Why needed: To grasp why FASP's linked pruning structure is necessary. Quick check: Trace how column removal in layer L affects the input to layer L+1.
- **Model restoration techniques**: Post-pruning adjustments that recalibrate weights to maintain performance after structural changes. Why needed: To understand FASP's unique contribution to the pruning pipeline. Quick check: Identify the specific restoration mechanism used in FASP and how it differs from simple fine-tuning.
- **Hardware acceleration considerations**: Structured pruning enables better utilization of SIMD instructions and matrix multiplication optimizations. Why needed: To understand why FASP's speed advantages matter in practice. Quick check: Compare theoretical FLOPs reduction to actual speedup measurements.
- **Perplexity as evaluation metric**: A measure of how well a language model predicts a sample of text, with lower values indicating better performance. Why needed: To interpret FASP's evaluation results on language modeling tasks. Quick check: Verify that perplexity scores are reported for comparable model sizes across different pruning methods.

## Architecture Onboarding
Component map: Input -> Wanda-inspired metric computation -> Column selection -> Row pruning in previous layer -> Restoration mechanism -> Pruned model
Critical path: The pruning metric computation and restoration mechanism are the computational bottlenecks that determine overall pruning speed.
Design tradeoffs: FASP prioritizes speed and accuracy preservation over maximum compression ratio, choosing structured pruning over unstructured approaches for hardware efficiency.
Failure signatures: Poor restoration can lead to accuracy degradation; inappropriate column selection may remove critical features, causing performance collapse on specific tasks.
First experiments:
1. Compare pruning speed and accuracy on a small model (e.g., OPT-125M) against baseline methods
2. Validate the restoration mechanism by examining weight distributions before and after pruning
3. Test FASP on a medium-sized model (e.g., OPT-6.7B) to evaluate scalability

## Open Questions the Paper Calls Out
None

## Limitations
- Focuses exclusively on column-row structured pruning, which may limit maximum achievable compression ratios compared to unstructured methods
- Restoration mechanism's effectiveness may vary depending on specific weight patterns and model architectures
- Evaluation primarily emphasizes perplexity and accuracy metrics, with limited discussion of inference latency on diverse hardware or power consumption

## Confidence
- High confidence in pruning methodology and computational efficiency claims
- Medium confidence in accuracy preservation claims, as downstream task results are presented but not extensively analyzed
- Medium confidence in practical applicability, given the focus on specific model families and hardware

## Next Checks
1. Test FASP's performance on additional model architectures beyond OPT and LLaMA, including different types of LLMs
2. Evaluate the impact of FASP on inference latency and memory usage across various hardware configurations
3. Assess the stability and consistency of the restoration mechanism across multiple pruning runs and different random seeds