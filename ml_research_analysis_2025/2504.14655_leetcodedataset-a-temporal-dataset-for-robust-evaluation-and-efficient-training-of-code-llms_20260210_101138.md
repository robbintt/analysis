---
ver: rpa2
title: 'LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient Training
  of Code LLMs'
arxiv_id: '2504.14655'
source_url: https://arxiv.org/abs/2504.14655
tags:
- code
- problems
- arxiv
- leetcodedataset
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces LeetCodeDataset, a high-quality benchmark
  for evaluating and training code-generation models. The dataset addresses two key
  challenges: the lack of reasoning-focused coding benchmarks and self-contained training
  testbeds.'
---

# LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient Training of Code LLMs

## Quick Facts
- arXiv ID: 2504.14655
- Source URL: https://arxiv.org/abs/2504.14655
- Reference count: 40
- Key outcome: Temporal dataset with 100+ test cases per problem enables contamination-free evaluation and efficient SFT training

## Executive Summary
This paper introduces LeetCodeDataset, a high-quality benchmark for evaluating and training code-generation models. The dataset addresses two key challenges: the lack of reasoning-focused coding benchmarks and self-contained training testbeds. By curating LeetCode Python problems with rich metadata, broad coverage, 100+ test cases per problem, and temporal splits (pre/post July 2024), the dataset enables contamination-free evaluation and efficient supervised fine-tuning (SFT). Experiments show reasoning models significantly outperform non-reasoning counterparts, while SFT with only 2.6K model-generated solutions achieves performance comparable to 110K-sample counterparts.

## Method Summary
LeetCodeDataset contains 2,869 Python problems with metadata (difficulty, tags, release dates) and 100+ test cases per problem. The dataset uses temporal splits based on July 1, 2024 release date to ensure contamination-free evaluation. Training data consists of 2.6K model-generated solutions created by Qwen2.5-Coder-32B-Instruct (T=1.0) that pass all test cases. SFT is performed using Qwen2.5-Coder-7B with 3 epochs, lr=1e-5, warmup ratio=0.1, cosine LR schedule, batch size=32. Evaluation uses temperature=0.2, top_p=0.95, and executes solutions in a sandboxed environment against test cases.

## Key Results
- Reasoning models (DeepSeek-R1, QwQ-Plus) significantly outperform non-reasoning counterparts on competitive programming tasks
- SFT with only 2.6K model-generated solutions achieves performance comparable to 110K-sample counterparts
- Temporal contamination analysis shows minimal overlap between GPT-4o-0806's release date (August 2024) and post-July 2024 test problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal splits enable contamination-free evaluation of code generation models.
- Mechanism: By partitioning problems based on release dates (pre/post July 2024), the dataset ensures test problems were unavailable during model training, preventing memorization-based inflation of performance metrics.
- Core assumption: Models' training cutoff dates are known and respected; problems released after July 2024 were not present in training corpora.
- Evidence anchors:
  - [abstract]: "temporal splits (pre/post July 2024), our dataset enables contamination-free evaluation"
  - [section 3]: "minimal temporal overlap between GPT-4o-0806's release date (August 2024) and our test problem release window (post-July 2024) strongly suggests authentic model capability measurements"
  - [corpus]: Weak corpus signals—neighbor papers focus on SFT quality but not temporal evaluation methods.
- Break condition: If a model's training data includes scraped LeetCode solutions after its stated cutoff date, temporal protection fails.

### Mechanism 2
- Claim: Model-generated training data with verified correctness outperforms human-written canonical solutions for SFT.
- Mechanism: Model-generated solutions include explanatory reasoning patterns and instructional structure absent from terse canonical solutions, improving downstream model learning even when both response types are functionally correct.
- Core assumption: Model-generated responses maintain instructional quality while passing all test cases; verification catches all logical errors.
- Evidence anchors:
  - [abstract]: "SFT with only 2.6K model-generated solutions achieves performance comparable to 110K-sample counterparts"
  - [section 4.2]: "SFT-trained model using model-generated responses... significantly outperformed the version trained on human-written responses (79.9% vs. 55.5% on HumanEval)"
  - [corpus]: Condor (2501.12273) corroborates knowledge-driven data synthesis improving alignment; no direct counter-evidence.
- Break condition: If test coverage is insufficient, verified solutions may contain latent bugs that degrade SFT quality.

### Mechanism 3
- Claim: Extended chain-of-thought reasoning capabilities substantially improve competitive programming performance.
- Mechanism: Reasoning models (DeepSeek-R1, QwQ-Plus) allocate additional inference compute to explore solution paths, validate logic, and self-correct, yielding higher pass rates on complex algorithmic tasks.
- Core assumption: Performance gains stem from reasoning architecture rather than larger training corpora or different base models.
- Evidence anchors:
  - [abstract]: "Experiments show reasoning models significantly outperform non-reasoning counterparts"
  - [section 3]: "DeepSeek-R1 (pass@1 rate = 65.23%) and QwQ-Plus (pass@1 rate = 56.25%) as top performers, demonstrating the substantial advantage of long-CoT reasoning models"
  - [corpus]: No direct corpus evidence on reasoning vs. non-reasoning code models; related papers focus on data quality rather than inference-time reasoning.
- Break condition: If evaluation benchmarks suffer from test case sparsity, reasoning models' advantages may be artifacts of better coverage exploitation rather than genuine algorithmic improvement.

## Foundational Learning

- Concept: **Data Contamination in Benchmarking**
  - Why needed here: Temporal splitting is the core defense against inflated metrics from memorized training data.
  - Quick check question: Can you explain why evaluating a model on problems that existed in its training corpus yields unreliable capability estimates?

- Concept: **Supervised Fine-Tuning (SFT) Data Efficiency**
  - Why needed here: The paper's central claim is that 2.6K high-quality samples match 110K-sample baselines.
  - Quick check question: What properties of training data would allow 40× fewer samples to achieve comparable performance?

- Concept: **Pass@1 and Test Case Coverage**
  - Why needed here: The dataset uses 100+ test cases per problem to minimize false positives in evaluation.
  - Quick check question: Why would a solution passing 10 test cases still be considered potentially incorrect?

## Architecture Onboarding

- Component map:
  Data Collection Pipeline -> Evaluation Framework -> Training Pipeline

- Critical path:
  1. Verify temporal split integrity before any evaluation (check problem release dates against model training cutoffs)
  2. Confirm sandbox isolation for test case execution (code must not access network, filesystem, or external state)
  3. Validate test case coverage (minimum 100 cases per problem, including edge cases)

- Design tradeoffs:
  - Python-only coverage (3,115 of 3,505 problems) sacrifices multilingual evaluation for deeper test coverage
  - Single-function entry points only excludes design/simulation problems requiring multiple functions
  - Model-generated over human solutions trades canonical correctness guarantees for instructional richness

- Failure signatures:
  - Spiking pass rates on newly released problems → likely contamination
  - High variance across monthly cohorts → insufficient test case coverage or difficulty imbalance
  - SFT models underperforming on Hard problems despite strong Easy/Medium results → small-scale training insufficient for complex reasoning

- First 3 experiments:
  1. Reproduce the contamination analysis by plotting monthly pass rates for a model with known training cutoff; verify curve shape matches Figure 3 patterns
  2. Ablate test case count (10, 50, 100+) on a held-out problem subset to measure false positive rate reduction
  3. Train SFT on human-written vs. model-generated solutions with identical sample counts to quantify instructional quality gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can automated time and space complexity analysis be integrated into the dataset to validate solution efficiency?
- Basis in paper: [explicit] Section 6 lists "Complexity Analysis Gap" as a key limitation, noting that determining complexity "demands manual problem-specific validation" and exceeds the current scope.
- Why unresolved: The current test cases verify functional correctness but do not penalize inefficient algorithms (e.g., O(n^2) vs. O(n)), which is critical for competitive programming.
- What evidence would resolve it: An automated evaluation pipeline that estimates code complexity and penalizes solutions that exceed optimal time/memory limits for specific input sizes.

### Open Question 2
- Question: How can the test case generation pipeline be enhanced to eliminate false positives caused by imbalanced or simplistic input distributions?
- Basis in paper: [explicit] Section 6 states the dataset lacks "extremely complex input patterns" and suffers from "imbalanced test case distribution," creating "residual risks of false positives."
- Why unresolved: While Section 2.1 uses LLMs to generate 100+ inputs, the authors acknowledge these may not cover all edge cases necessary to catch subtle logic errors.
- What evidence would resolve it: An analysis of passing solutions that are logically flawed (false positives) on the test set, followed by a refined generation method that targets these specific failure modes.

### Open Question 3
- Question: Does scaling the LeetCodeDataset training split beyond 2.6K samples improve performance on "Hard" benchmarks, or is the observed ceiling a result of the training methodology?
- Basis in paper: [inferred] Section 4.2 notes that the 2.6K-sample model "underperformed on hard benchmarks," suggesting that small-scale SFT "primarily develops basic programming skills" rather than complex reasoning.
- Why unresolved: The paper demonstrates high data efficiency for basic tasks but leaves open the question of whether simply adding more data from the training split would solve the "Hard" problem performance gap or if advanced methods (e.g., RL) are required.
- What evidence would resolve it: A comparative study training models on larger subsets (e.g., 10K, 50K samples) of the LeetCodeDataset to observe the scaling curve specifically for "Hard" difficulty problems.

## Limitations
- Temporal contamination protection assumes no pre-July 2024 models encountered post-July problems, though no verification is provided
- SFT data efficiency claim depends on unverified quality of 2.6K model-generated samples
- Model-generated solutions may contain latent bugs that verification misses

## Confidence

### Major Uncertainties
The paper's claims rest on several assumptions that introduce uncertainty: (1) the temporal split's contamination-free guarantee assumes no pre-July 2024 models encountered post-July problems, though no verification is provided; (2) the SFT data efficiency claim assumes the 2.6K model-generated samples are truly representative, but sample diversity metrics are not reported; (3) the superiority of model-generated over human solutions assumes verification catches all bugs, though no analysis of verification failure rates is provided.

### Confidence Labels
- **High confidence**: The temporal split methodology is sound and well-documented; the contamination analysis via monthly pass rates provides empirical validation; the 100+ test case coverage approach is standard practice in competitive programming evaluation.
- **Medium confidence**: The SFT data efficiency results are compelling but depend on the quality of the 2.6K model-generated samples, which cannot be independently verified without the generation prompts; the reasoning model advantage is observed but could partially reflect larger training corpora rather than architectural differences.
- **Low confidence**: Claims about model-generated solutions being pedagogically superior to human solutions are largely speculative, as no qualitative analysis of solution structure or instructional quality is provided.

## Next Checks
1. **Contamination verification**: Cross-reference model training corpus release dates against LeetCode problem release timestamps to quantify potential contamination in baseline models, particularly for problems released just before July 2024.
2. **Sample quality analysis**: Audit the 2.6K model-generated solutions for diversity, difficulty distribution, and potential bias toward easier problem patterns to validate the data efficiency claim.
3. **Reasoning vs. non-reasoning ablation**: Conduct controlled experiments isolating reasoning architecture effects by fine-tuning a non-reasoning model with identical training data to verify that performance gains stem from inference-time reasoning rather than training corpus differences.