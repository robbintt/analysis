---
ver: rpa2
title: 'From Concepts to Components: Concept-Agnostic Attention Module Discovery in
  Transformers'
arxiv_id: '2506.17052'
source_url: https://arxiv.org/abs/2506.17052
tags:
- attention
- module
- head
- concept
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Scalable Attention Module Discovery (SAMD),
  a concept-agnostic method for attributing arbitrary concepts to specific attention
  heads in transformer models. The core idea is to represent concepts as vectors,
  compute their cosine similarity with attention head outputs, and select the top-scoring
  heads as a module.
---

# From Concepts to Components: Concept-Agnostic Attention Module Discovery in Transformers

## Quick Facts
- arXiv ID: 2506.17052
- Source URL: https://arxiv.org/abs/2506.17052
- Reference count: 40
- Key result: 3-10 attention heads encode complex concepts; negative safety intervention increases jailbreak success by 72.7%; positive reasoning intervention improves GSM8K accuracy by 1.6%

## Executive Summary
This paper introduces Scalable Attention Module Discovery (SAMD), a concept-agnostic method for attributing arbitrary concepts to specific attention heads in transformer models. The core innovation is representing concepts as vectors, computing their cosine similarity with attention head outputs, and selecting top-scoring heads as a module. The authors also propose Scalar Attention Module Intervention (SAMI), a single-parameter intervention strategy that can diminish or amplify concept effects by scaling attention head outputs. The method is evaluated across diverse concepts (SAE features, reasoning, safety, visual recognition) and model types (LLMs and ViTs), demonstrating that only a small number of attention heads encode complex concepts and that these modules can be manipulated to significantly alter model behavior.

## Method Summary
SAMD works by first representing each concept as a vector (from SAE features, difference-in-means, or unembedding rows), then computing the cosine similarity between this vector and each attention head's output across a positive dataset. The top-K scoring heads form the concept's attention module. SAMI intervenes by scaling the contributions of these module heads using a single scalar parameter during the forward pass, effectively amplifying or diminishing the concept's influence. The method is efficient, requiring intervention on only ~0.1% of model weights to achieve significant behavioral effects. Evaluation spans language models (LLaMA, Gemma) and vision transformers, with concepts ranging from SAE features like "French" to safety modules and visual classification labels.

## Key Results
- Only 3-10 attention heads encode complex concepts across diverse domains
- Module locations remain stable before and after LLM post-training
- Negative intervention on safety modules increases jailbreak success by 72.7% on HarmBench
- Positive intervention on reasoning modules improves GSM8K performance by 1.6%
- In vision transformers, negative intervention on label modules reduces classification accuracy to 0% for target labels

## Why This Works (Mechanism)

### Mechanism 1: Cosine Similarity Attribution
The semantic relationship between an attention head's contribution and a concept vector can be quantified via cosine similarity. SAMD computes `cos(al,h(p), vc)` for each head across a positive dataset `Dp`, selecting top-K heads. The hypothesis is that higher similarity implies greater contribution to the concept. Core assumption: Concept vectors meaningfully represent abstract concepts in the model's representation space.

### Mechanism 2: Residual Stream Linearity
Attention head contributions can be isolated and meaningfully intervened upon because they add linearly to the residual stream. The residual stream `rl = rl-1 + Î£ al,h + ml` allows each head's contribution `al,h` to be treated as a modular, additive component. Core assumption: The transformer's residual connections preserve the independence of component contributions sufficiently for attribution and intervention.

### Mechanism 3: Scalar Output Scaling
Modifying the output magnitude of module attention heads via a single scalar `s` can diminish or amplify concept effects. SAMI multiplies the contributions of module heads by `s` in the forward pass. Core assumption: The identified module is causally sufficient; intervening on it alone can produce observable behavioral shifts.

## Foundational Learning

- **Concept: Residual Stream Viewpoint**
  - Why needed here: The entire attribution and intervention framework relies on decomposing the residual stream into additive contributions from attention heads and MLPs.
  - Quick check question: Can you explain how each transformer layer modifies the residual stream, and why this decomposition enables component-level intervention?

- **Concept: Sparse Autoencoders (SAEs) for Feature Extraction**
  - Why needed here: SAE decoder vectors are used as concept vectors in Section 4.1; understanding their construction and limitations is critical for interpreting SAMD results.
  - Quick check question: How does an SAE extract monosemantic features from LLM representations, and what are the tradeoffs of using SAE features as concept vectors?

- **Concept: Cosine Similarity in Representation Space**
  - Why needed here: SAMD's core scoring function is cosine similarity between head outputs and concept vectors; interpreting these scores requires understanding their geometric meaning.
  - Quick check question: What does a high cosine similarity between an attention head's output and a concept vector imply about the head's role?

## Architecture Onboarding

- **Component map:**
  Input: Concept vector `vc` + positive dataset `Dp`
  -> SAMD core: Compute cosine similarity, select TopK heads
  -> SAMI intervention: Scale selected heads' contributions by scalar `s`
  -> Output: Modified behavior; module visualization

- **Critical path:**
  1. Construct or obtain `vc` (SAE feature, difference-in-means, or unembedding)
  2. Run forward pass on `Dp` to collect attention head outputs
  3. Compute similarity scores, select TopK
  4. Implement SAMI by scaling output projection weights of selected heads
  5. Evaluate via targeted benchmarks

- **Design tradeoffs:**
  - Module size (K): Smaller K yields sparser modules but may miss distributed representations; larger K increases coverage but dilutes intervention precision
  - Concept vector source: SAE features are interpretable but may be polysemantic; unembedding rows are direct but limited to output-aligned concepts
  - Intervention scalar `s`: Extreme values cause degeneration; optimal values require task-specific search

- **Failure signatures:**
  - Negative intervention causes nonsensical or untruthful outputs
  - Positive intervention induces repetition
  - No observable effect: module may be incomplete or concept distributed elsewhere

- **First 3 experiments:**
  1. **SAE feature validation:** Pick interpretable SAE feature (e.g., "French"), run SAMD on GEMMA-2-2B-IT, visualize module heatmap, confirm layer localization matches multilingualism findings
  2. **Reasoning boost:** Compute "reasoning" concept vector from GSM8K prompts, identify top-5 heads in LLAMA-3.1-8B-INSTRUCT, apply SAMI with `s=1.4`, measure accuracy change on GSM8K
  3. **Safety suppression:** Construct safety concept vector via difference-in-means, identify safety module in LLAMA-2-CHAT-7B, apply negative SAMI (`s=-1.7`), measure attack success rate on HarmBench

## Open Questions the Paper Calls Out

### Open Question 1
Is the attention module identified by SAMD causally sufficient, or does the correlation-based discovery method yield overcomplete or incomplete circuits? The method relies on cosine similarity (correlation) rather than causal tracing, meaning modules could be either overcomplete or incomplete. Evidence would require causal mediation analysis to validate whether TopK heads are strictly necessary and sufficient.

### Open Question 2
What is the theoretical sample complexity and stability required for SAMD to converge on a robust module? The method lacks formal bounds on dataset size needed to avoid noise or sensitivity analysis showing how module composition changes with input distribution variance.

### Open Question 3
To what extent does the polysemanticity of input concept vectors introduce errors into discovered attention modules? The pipeline relies on external vector representations which may be coarse, potentially causing modules to represent adjacent concepts due to vector impurity rather than the target concept purely.

### Open Question 4
Are there critical concept representations that SAMD misses due to exclusive focus on attention heads rather than MLP neurons? The method ignores MLP contributions entirely, implying other forms of knowledge encoding could exist beyond attention head modules.

## Limitations
- Concept vector alignment: SAE features can be polysemantic and difference-in-means vectors may capture spurious correlations
- Module completeness: Identified modules may not be causally sufficient as other components could contribute
- Intervention causality: Observed effects could result from indirect pathways or model degeneration at extreme scales

## Confidence

**High Confidence**: Technical implementation of SAMD and SAMI is clearly specified and reproducible; residual stream decomposition provides valid mathematical framework.

**Medium Confidence**: Empirical demonstrations are convincing but interpretation of what modules "encode" requires more careful causal analysis; module stability across post-training is suggestive but not conclusive.

**Low Confidence**: Claims about module completeness and sufficiency of attention heads lack rigorous validation; assumption that different concept vector sources are equally meaningful needs investigation.

## Next Checks

1. **Causal Intervention Validation**: Create artificial concepts by fine-tuning specific heads to respond to triggers, then use SAMD to discover and SAMI to verify intervention reverses effects, establishing true causal attribution.

2. **Module Redundancy and Distribution Test**: Systematically ablate identified modules and measure concept performance degradation, then apply SAMI to all other heads to determine if additional heads can compensate, revealing whether concepts are truly localized or distributed.

3. **Cross-Model Concept Transfer**: Take concept modules discovered in one model family (e.g., LLaMA) and apply same intervention to different architecture (e.g., Mistral or DeepSeek) to measure whether behavioral effects transfer, indicating whether modules capture fundamental computational patterns.