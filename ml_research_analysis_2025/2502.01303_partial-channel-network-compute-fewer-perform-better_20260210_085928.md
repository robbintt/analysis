---
ver: rpa2
title: 'Partial Channel Network: Compute Fewer, Perform Better'
arxiv_id: '2502.01303'
source_url: https://arxiv.org/abs/2502.01303
tags:
- attention
- partial
- convolution
- hybrid
- patconv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PartialNet introduces a partial channel mechanism that splits feature
  maps and applies different operations (convolution, attention, pooling) to each
  part in parallel. This approach combines partial convolution with visual attention,
  reducing parameters and FLOPs while maintaining or improving accuracy.
---

# Partial Channel Network: Compute Fewer, Perform Better

## Quick Facts
- arXiv ID: 2502.01303
- Source URL: https://arxiv.org/abs/2502.01303
- Reference count: 40
- PartialNet achieves 73.9-83.9% top-1 accuracy on ImageNet-1K with 0.25-11.91 GFLOPs, outperforming state-of-the-art models in both accuracy and throughput

## Executive Summary
PartialNet introduces a partial channel mechanism that splits feature maps and applies different operations (convolution, attention, pooling) to each part in parallel. This approach combines partial convolution with visual attention, reducing parameters and FLOPs while maintaining or improving accuracy. The method includes three specialized blocks (PAT_ch, PAT_sp, PAT_sf) and a dynamic partial convolution (DPConv) that adaptively learns optimal channel split ratios. On ImageNet-1K, PartialNet variants (T0-L) achieve 73.9-83.9% top-1 accuracy with 0.25-11.91 GFLOPs, outperforming state-of-the-art models in both accuracy and throughput.

## Method Summary
PartialNet applies the Partial Channel Mechanism (PCM) to split feature maps along the channel dimension, processing each subset with different operations in parallel. The framework uses three PATConv blocks: PAT_ch (channel attention), PAT_sp (spatial attention), and PAT_sf (self-attention with relative position encoding). A dynamic partial convolution (DPConv) learns optimal split ratios through a gate vector and Kronecker product construction, with regularization terms to control complexity and ordering. The architecture consists of embedding/merging layers, PartialNet blocks (v1 for stages 1-3, v2 for stage 4), and supports both fixed and learned split ratios.

## Key Results
- ImageNet-1K: PartialNet-T0 achieves 73.9% top-1 accuracy with 0.25 GFLOPs, while PartialNet-L reaches 83.9% with 11.91 GFLOPs
- COCO detection: PartialNet-T2 backbone improves Mask R-CNN AP from 44.5% to 47.7%
- Throughput: PartialNet-T0 achieves 5900 FPS on MI250 GPU, 13.7 FPS on CPU
- DPConv: Learned split ratios vary across layers, with first and last layers tending toward larger ratios (less sparsity)

## Why This Works (Mechanism)

### Mechanism 1
Partial channel splitting with heterogeneous operations improves the accuracy-efficiency trade-off by exploiting redundancy in feature channels. The Partial Channel Mechanism divides input feature maps into subsets, applying convolution to a fraction r_p and attention operations to the remainder. This reduces FLOPs while preserving representational capacity by recognizing that not all channels require identical computational treatment.

### Mechanism 2
Parallel execution of convolution and attention on split channels improves throughput relative to sequential attention-convolution pipelines. PATConv processes two branches simultaneously on separate channel subsets, optimizing GPU resource utilization and avoiding the sequential bottleneck of conventional attention-augmented CNNs.

### Mechanism 3
Learnable split ratios via Dynamic Partial Convolution (DPConv) adaptively optimize the accuracy-latency trade-off across layers. DPConv parameterizes the split ratio through a learnable gate vector and Kronecker product construction, with regularization terms to bound FLOPs. Different network layers require different optimal split ratios, making a fixed ratio suboptimal.

## Foundational Learning

- **Channel-wise vs. spatial-wise vs. self-attention mechanisms**: PATConv derives three blocks using different attention types; understanding their computational properties is essential for stage assignment. Quick check: Given a 14×14 feature map with 256 channels, which attention type has the lowest FLOPs: channel attention (SE-style), spatial attention, or self-attention?

- **Feature map redundancy and channel pruning theory**: The PCM mechanism explicitly assumes channel redundancy; without this, partial processing would lose critical information. Quick check: How would you measure channel redundancy in a trained model? Would high correlation between channels support or contradict the PCM assumption?

- **Straight-through estimator (STE) for non-differentiable operations**: DPConv uses STE to train binary gates; understanding STE limitations informs when adaptive ratios may fail. Quick check: In STE, gradients pass through a Sign() function as if it were identity. What failure mode might this cause if gate values converge slowly?

## Architecture Onboarding

- **Component map**: Embedding/Merging layers (Conv4×4 stride 4 for stage 1; Conv2×2 stride 2 for stages 2-4) -> PartialNet Block v1 (stages 1-3: PAT_ch -> [Conv1×1, BN, Acti, Conv1×1, BN] -> PAT_sp) -> PartialNet Block v2 (stage 4: PAT_sf replaces PAT_ch; modified shortcut) -> Output

- **Critical path**: 1) Forward pass: Input -> Split by r_p -> Parallel Conv/Atten branches -> Concat -> MLP -> PAT_sp -> Output 2) DPConv training: Gate vector g -> Kronecker product U -> Mask m -> Apply to weights -> Compute loss with regularization (ζ, ψ) 3) Inference optimization: Merge PAT_sp Conv1×1 with MLP Conv1×1; fold BN into adjacent convolutions

- **Design tradeoffs**: Fixed vs. learned r_p (fixed simplifies training; learned adapts to layer importance but requires constraint tuning), attention type selection (PAT_ch cheapest; PAT_sf most expressive but O(n²), hence stage-4-only), activation choice (GELU for small variants, ReLU for large)

- **Failure signatures**: Accuracy collapses with r_p too small (model loses local inductive bias), throughput lower than expected on V100 (parallel branch overhead may dominate), DPConv fails to converge (gate ordering constraint may be insufficient), stage 4 training instability (self-attention with RPE may need learning rate warmup)

- **First 3 experiments**: 1) Reproduce Table 4 ablation on PartialNet-T2: Train with ch-only, ch+sp, ch+sp+sf to isolate attention contributions (expect ~4.2% top-1 improvement from full PATConv stack) 2) Profile PATConv vs. regular Conv and DWConv (Table 6): Measure throughput and latency on target hardware (V100/MI250/CPU) (verify PAT_ch achieves >15% throughput gain over Conv at similar accuracy) 3) DPConv sensitivity analysis: Train PartialNet-T0 with θ ∈ {2, 4, 6, 8} and plot learned r_p per layer (Figure 7 replication) (confirm first/last layer ratio stability)

## Open Questions the Paper Calls Out

### Open Question 1
How can Dynamic Partial Convolution (DPConv) be generalized to accommodate channel counts that are not powers of 2? The current implementation restricts architectural design flexibility, forcing model widths to adhere to specific integers rather than arbitrary values often used in efficient model scaling.

### Open Question 2
What architectural adjustments are required to mitigate the throughput regression observed in smaller PartialNet variants on specific hardware like the V100 GPU? The paper identifies the hardware difference (compute vs. bandwidth intensive) as a potential cause but does not offer a mechanism to recover performance on compute-oriented devices for smaller models.

### Open Question 3
Can the Partial Channel Mechanism be effectively applied to early stages of pure Vision Transformers (ViTs) that lack explicit convolutional blocks? It is unclear if the "Partial Channel Mechanism" maintains its efficiency/accuracy trade-off when the convolution branch of the split is replaced entirely by linear layers or patch embeddings.

## Limitations
- Hardware dependency of throughput gains: while PATConv achieves superior throughput on MI250 GPUs, results on V100 show only marginal improvements
- The Kronecker product construction for DPConv requires channel dimensions to be powers of two, limiting architectural flexibility
- The learned split ratios show high variance across layers, raising questions about whether this reflects genuine optimization or optimization artifacts

## Confidence
- **High confidence**: Basic accuracy improvements of PartialNet over FasterNet on ImageNet (73.9-83.9% top-1 accuracy with reduced FLOPs)
- **Medium confidence**: Throughput claims, particularly the hardware-dependent performance variations and the specific impact of DPConv on convergence
- **Low confidence**: Generalization claims to other architectures beyond the specific PartialNet variants tested, and the robustness of learned split ratios across different datasets

## Next Checks
1. **DPConv robustness testing**: Systematically vary θ and regularization weights (ζ, ψ) across PartialNet-T0 to T2, measuring both accuracy and split ratio stability to establish sensitivity bounds
2. **Cross-architecture validation**: Implement the Partial Channel Mechanism on a standard ResNet-50 backbone to test generalizability beyond the PartialNet architecture
3. **Ablation on attention types**: Train PartialNet variants with each attention type (ch, sp, sf) individually and in combinations on COCO detection to isolate their contributions to downstream task performance