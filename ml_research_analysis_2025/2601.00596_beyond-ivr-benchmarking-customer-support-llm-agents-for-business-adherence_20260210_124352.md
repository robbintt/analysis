---
ver: rpa2
title: 'Beyond IVR: Benchmarking Customer Support LLM Agents for Business-Adherence'
arxiv_id: '2601.00596'
source_url: https://arxiv.org/abs/2601.00596
tags:
- tool
- user
- node
- agent
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: JourneyBench introduces a benchmark for evaluating policy-aware
  LLM agents in customer support by representing workflows as directed acyclic graphs
  (DAGs) and introducing the User Journey Coverage Score (UJCS) to measure adherence
  to procedural sequences. The benchmark uses a dynamic-prompt agent (DPA) that processes
  tasks node-by-node, evaluating adherence to SOPs across 703 conversations in three
  domains.
---

# Beyond IVR: Benchmarking Customer Support LLM Agents for Business-Adherence

## Quick Facts
- arXiv ID: 2601.00596
- Source URL: https://arxiv.org/abs/2601.00596
- Authors: Sumanth Balaji; Piyush Mishra; Aashraya Sachdeva; Suraj Agrawal
- Reference count: 36
- Primary result: DPA achieves 27% higher UJCS (0.717) than SPA (0.564) in customer support policy adherence

## Executive Summary
JourneyBench introduces a novel benchmark for evaluating policy-aware LLM agents in customer support by representing workflows as directed acyclic graphs (DAGs) and introducing the User Journey Coverage Score (UJCS) to measure adherence to procedural sequences. The framework uses a dynamic-prompt agent (DPA) that processes tasks node-by-node with conditional pathway evaluation, significantly outperforming static-prompt approaches. Experiments show that DPA enables smaller models like GPT-4o-mini to achieve better policy adherence than larger models like GPT-4o when guided by structured workflow orchestration.

## Method Summary
JourneyBench represents SOPs as DAGs where nodes are tasks with tools and conditional pathways, and edges are transitions based on algebraic expressions evaluated at runtime. The benchmark includes 703 conversations across three domains with three scenario types. Two agent architectures are compared: SPA uses a single comprehensive prompt with all workflow logic, while DPA employs node-by-node prompt orchestration with an external state manager that evaluates conditions and selects the next node. UJCS measures workflow adherence by comparing actual tool call sequences against expected sequences.

## Key Results
- DPA achieves UJCS of 0.717 versus SPA's 0.564 for GPT-4o, demonstrating 27% improvement in policy adherence
- DPA enables GPT-4o-mini to outperform GPT-4o in policy adherence despite being a smaller model
- The framework treats tools as black boxes, focusing evaluation on workflow adherence rather than tool implementation
- Validated in production handling 6,000+ calls daily across three domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Node-by-node prompt orchestration improves policy adherence over single-prompt approaches
- Mechanism: DPA treats SOPs as state machines, replacing the prompt after each tool execution based on conditional pathway evaluation. This constrains the model's action space to node-relevant tools only, reducing context overload and preventing premature transitions
- Core assumption: Smaller context windows with fewer available tools reduce hallucination and dependency violations more effectively than comprehensive prompts with explicit if-then instructions
- Evidence anchors:
  - [abstract] "DPA significantly boosts policy adherence, even allowing smaller models like GPT-4o-mini to outperform more capable ones like GPT-4o"
  - [section 4.2] "GPT-4o with DPA achieves a UJCS of 0.717, substantially higher than SPA's 0.564"
  - [corpus] EComStage paper similarly notes existing benchmarks "overlook intermediate" process steps—corroborating the gap JourneyBench addresses
- Break condition: If your SOPs have shallow depth (<3 nodes) or minimal conditional branching, SPA may suffice and DPA's orchestration overhead is unjustified

### Mechanism 2
- Claim: Encoding procedural logic as DAG-structured conditional pathways enables deterministic workflow traversal
- Mechanism: Conditional pathways are defined as algebraic expressions (e.g., `riskLevel == 'acceptable'`) evaluated at runtime via Python's `eval()`. Tool responses populate variables that deterministically select the next node, ensuring strict SOP adherence regardless of model reasoning
- Core assumption: Business logic can be fully specified as discrete conditional expressions over tool outputs; ambiguous edge cases require human-authored clarification
- Evidence anchors:
  - [section 2.1] "At runtime, these conditions deterministically select the next node, ensuring strict adherence to the SOP"
  - [section A.3] "The framework evaluates the algebraicExpression at runtime using Python's eval() function with the variable values from tool responses"
  - [corpus] No direct corpus comparison found for DAG-based SOP encoding; this is a novel contribution requiring further validation
- Break condition: If your domain requires probabilistic routing or human judgment at decision points, deterministic expressions will fail or require redesign

### Mechanism 3
- Claim: Treating tools as black-box interfaces isolates workflow evaluation from implementation churn
- Mechanism: JourneyBench uses pre-generated tool responses for deterministic testing, varying only workflow-relevant outputs (e.g., `riskLevel`) while auto-generating cosmetic fields (timestamps, IDs). This decouples benchmark stability from API changes
- Core assumption: Policy violations manifest in the sequence and parameters of tool calls—not in tool-internal logic, which is out of scope
- Evidence anchors:
  - [section 2.2] "JourneyBench evaluates workflow adherence rather than tool implementation, treating tools as black boxes with pre-generated responses"
  - [section 5] "JourneyBench treats tools as modular components with well-defined interfaces, decoupling their internal implementation from workflow evaluation"
  - [corpus] Tau-Bench and ToolSandbox focus on tool selection accuracy but do not emphasize workflow sequence fidelity to this degree
- Break condition: If your failure modes include incorrect tool outputs (not just incorrect calls), black-box evaluation won't catch them

## Foundational Learning

- Concept: **Directed Acyclic Graphs (DAGs) for workflow modeling**
  - Why needed here: SOPs are encoded as DAGs where nodes = tasks and edges = conditional transitions. Understanding DAG properties (acyclicity, reachability, topological ordering) is prerequisite to debugging invalid graphs
  - Quick check question: Can you explain why a cycle in an SOP graph would cause infinite loops during agent traversal?

- Concept: **State machine orchestration vs. single-prompt reasoning**
  - Why needed here: DPA's core innovation is externalizing state management to an orchestrator rather than relying on the LLM to track position in workflow. This separation is fundamental to understanding why smaller models can outperform larger ones
  - Quick check question: What information must the orchestrator maintain between node transitions that would otherwise burden the LLM's context?

- Concept: **Tool trace alignment as compliance metric**
  - Why needed here: UJCS measures whether the agent's actual tool call sequence matches the expected sequence exactly—missing, extra, or misordered calls all indicate SOP violations
  - Quick check question: If an agent completes a user's goal but skips a required validation step, what would the UJCS score be for that conversation?

## Architecture Onboarding

- Component map: SOP Graph Store -> Orchestrator -> Prompt Builder -> Tool Mock Layer -> Evaluation Engine
- Critical path:
  1. Author/validate SOP graph (acyclicity, connectivity, expression syntax)
  2. Generate user journeys via BFS traversal
  3. Create scenarios (correct context, missing parameter, failing function)
  4. Run agent simulation with tool mocks
  5. Compute UJCS by comparing actual vs. expected tool traces
- Design tradeoffs:
  - DPA adds orchestration latency but achieves 27% higher UJCS (0.717 vs 0.564 for GPT-4o)
  - Black-box tools enable reproducibility but cannot catch implementation bugs
  - Synthetic user simulation scales but may miss edge-case user behaviors (paper notes user simulator failures in 2 categories)
- Failure signatures:
  - **Dependency violations**: Agent proceeds despite missing required parameters or prior tool failures (SPA shows this; DPA mitigates)
  - **Parameter hallucination**: Agent uses example values from tool descriptions instead of user-provided values (both SPA and DPA susceptible)
  - **User simulator failures**: Simulator provides info not in seed or ends conversation prematurely (does not reflect agent performance)
- First 3 experiments:
  1. Reproduce the SPA vs. DPA comparison on a single domain (e.g., Loan Application) using GPT-4o-mini to validate UJCS differential
  2. Introduce a deliberate cycle into an SOP graph and verify the validation pipeline rejects it with appropriate error messaging
  3. Test a "missing parameter" scenario and confirm DPA halts with appropriate user prompt while SPA proceeds incorrectly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SOP workflow graphs be accurately derived semi-automatically from unstructured conversation logs?
- Basis in paper: [explicit] Section 7 states: "Future work might explore semi-automated graph generation from conversation logs."
- Why unresolved: The current methodology relies on LLM generation followed by intensive manual expert review ("5-of-5 agreement"), which is noted as challenging for dynamic or poorly documented fields
- Evidence: A methodology that extracts valid DAGs from raw interaction logs with high structural fidelity compared to human-authored SOPs

### Open Question 2
- Question: How does the Dynamic-Prompt Agent (DPA) architecture perform on workflows requiring cyclic dependencies or runtime graph modifications?
- Basis in paper: [inferred] Section 2.1 mandates Directed Acyclic Graphs (DAGs) to ensure termination, while Section 7 explicitly calls for research into "complex dependency structures."
- Why unresolved: The current framework strictly enforces acyclicity, potentially limiting applicability to real-world workflows that include iterative loops (e.g., retries) or dynamic state changes
- Evidence: Benchmark results showing DPA performance on cyclic or non-DAG workflows compared to the current DAG-only implementation

### Open Question 3
- Question: How can LLM-based user simulation be stabilized to eliminate "user input hallucination" and ensure full journey completion?
- Basis in paper: [inferred] Section 4.3 identifies "User Simulator Failures" as a distinct error class where the simulator fabricates information or ends conversations prematurely, distinct from agent errors
- Why unresolved: Reliance on an LLM (GPT-4o) to simulate the user introduces non-deterministic failures that obscure the measurement of the target agent's policy adherence
- Evidence: A simulation framework that demonstrates statistically significant reductions in seed leakage and premature termination rates during benchmark execution

## Limitations
- DAG-based workflow encoding assumes business logic can be fully expressed as deterministic conditional expressions, which may not capture all real-world decision points
- Black-box tool treatment prevents evaluation of implementation bugs that could cause policy violations downstream
- Synthetic user simulator may miss edge-case user behaviors that would reveal agent weaknesses

## Confidence

- **High Confidence**: DPA vs SPA UJCS comparison (0.717 vs 0.564) and the core finding that structured workflow orchestration improves policy adherence
- **Medium Confidence**: Claims about DPA enabling smaller models to outperform larger ones, as this depends on the specific SOP complexity and may not generalize across all domains
- **Low Confidence**: Claims about the completeness of DAG-based SOP encoding for real-world business processes without empirical validation across diverse operational contexts

## Next Checks

1. **Boundary testing of DAG expressiveness**: Systematically evaluate whether the algebraicExpression conditions can encode common real-world decision points like confidence thresholds, exception handling, and multi-factor routing decisions
2. **End-to-end latency validation**: Measure actual DPA orchestration overhead in production to confirm the 27% UJCS improvement justifies the added latency for different SOP depths
3. **Human-in-the-loop validation**: Compare UJCS scores against expert manual review of agent conversations to quantify how well the metric captures true policy compliance versus mechanical trace matching