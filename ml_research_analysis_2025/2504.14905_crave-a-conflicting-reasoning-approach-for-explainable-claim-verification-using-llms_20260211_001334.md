---
ver: rpa2
title: 'CRAVE: A Conflicting Reasoning Approach for Explainable Claim Verification
  Using LLMs'
arxiv_id: '2504.14905'
source_url: https://arxiv.org/abs/2504.14905
tags:
- evidence
- claim
- reasoning
- verification
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes CRAVE, a conflicting reasoning approach for
  explainable claim verification using LLMs. CRAVE addresses the challenge of verifying
  complex claims by retrieving relevant evidence, reasoning over it from conflicting
  perspectives using LLMs, and making a final judgment with a fine-tuned small language
  model.
---

# CRAVE: A Conflicting Reasoning Approach for Explainable Claim Verification Using LLMs

## Quick Facts
- **arXiv ID:** 2504.14905
- **Source URL:** https://arxiv.org/abs/2504.14905
- **Reference count:** 33
- **Key outcome:** CRAVE achieves state-of-the-art accuracy on multi-hop claim verification by combining conflicting perspective reasoning with a fine-tuned small language model judge, particularly excelling on complex 4-hop claims.

## Executive Summary
CRAVE addresses the challenge of verifying complex multi-hop claims by combining LLM-based conflicting perspective reasoning with a fine-tuned small language model (SLM) judge. The system first retrieves relevant evidence from Wikipedia using an ambiguity-elimination enhanced approach, then generates supporting and refuting rationales across four structured dimensions using LLMs, and finally makes a final judgment using a RoBERTa-based SLM. The method significantly outperforms existing approaches on both HOVER and FEVEROUS datasets, particularly for complex claims requiring reasoning across multiple evidence documents.

## Method Summary
CRAVE consists of three modules: (1) Ambiguity Elimination enhanced Evidence Retrieval that resolves ambiguous entities and retrieves relevant Wikipedia paragraphs using BM25, (2) Conflicting Perspective Reasoning where LLMs generate rationales from both supporting and refuting stances across four dimensions (direct evidence, semantic relationships, linguistic patterns, logical reasoning), and (3) a Judge module with a fine-tuned RoBERTa SLM that weighs these conflicting rationales based on training patterns. The system uses an optimal 80:20 ratio of open-book to gold-evidence training data to balance generalization and accuracy.

## Key Results
- CRAVE achieves 77.89% accuracy on HOVER 2-hop gold-evidence setting, outperforming existing methods by 2.68% absolute improvement
- For complex 4-hop claims, CRAVE reaches 59.10% accuracy compared to 37.91% for the best baseline, demonstrating particular strength on complex reasoning tasks
- The method retrieves 64% of gold evidence compared to 59% for the best baseline on HOVER 2-hop, indicating superior evidence collection capabilities

## Why This Works (Mechanism)

### Mechanism 1
Explicit conflicting perspective reasoning improves LLM evidence analysis quality compared to free-form reasoning. By prompting the LLM to generate rationales from both "supporting" and "refuting" stances separately across four structured dimensions, the model avoids premature commitment to one interpretation and produces more comprehensive justifications. The stronger rationale correlates with ground truth, creating a confidence signal. Evidence shows accuracy drops from 68.12% to 64.27% when removing structured perspectives on HOVER 2-hop.

### Mechanism 2
Ambiguity elimination before entity-based retrieval yields more complete evidence sets for multi-hop claims. Complex claims often contain ambiguous references requiring contextual resolution. The system first prompts an LLM to generate a sequential reasoning plan that resolves ambiguities, then extracts entities including resolved ones, then retrieves evidence. This captures evidence that direct keyword matching would miss. CRAVE retrieves 64% of gold evidence versus 59% for baselines on HOVER 2-hop.

### Mechanism 3
Separating reasoning (LLM) from judgment (fine-tuned SLM) improves accuracy over using LLM alone. LLMs generate plausible but sometimes hallucinated reasoning and are unreliable for direct classification. CRAVE uses LLM output as features for a fine-tuned RoBERTa SLM that learns to weigh conflicting rationales based on training data patterns. The SLM achieves 77.89% accuracy versus 73.09% when using LLM predictions directly on HOVER 2-hop gold-evidence setting.

## Foundational Learning

- **Multi-hop claim verification**: Understanding that HOVER dataset includes 2-hop, 3-hop, and 4-hop claims requiring reasoning across multiple evidence documents. Quick check: Can you explain why a claim like "The CEO of the company that acquired Instagram was born in San Francisco" requires at least 2-hop reasoning?

- **BM25 retrieval**: The evidence retrieval module uses BM25 as the ranking algorithm after entity lookup. Understanding its term-frequency basis explains why ambiguity elimination matters. Quick check: Why would BM25 struggle with ambiguous entity mentions like "Apple" (company vs. fruit)?

- **Encoder-only vs. decoder-only architectures**: CRAVE uses decoder-only LLMs for reasoning (generative) but encoder-only RoBERTa for judgment (classification). The distinction matters for understanding why each is chosen. Quick check: Why might an encoder-only model be preferred for binary classification over a generative LLM?

## Architecture Onboarding

- **Component map:** Claim → [Ambiguity Elimination (LLM)] → Resolved Entities → [Entity Retrieval] → Wikipedia Pages → [Evidence Retrieval (BM25)] → Evidence Paragraphs D → [Conflicting Reasoning (LLM)] → r_true, r_false, y_LLM → [SLM Judge (RoBERTa, fine-tuned)] → ŷ, explanation e

- **Critical path:** The evidence retrieval quality gates everything downstream. If ambiguity elimination misses entities or BM25 returns irrelevant paragraphs, LLM reasoning degrades, and SLM receives garbage features. Table II shows w/o ERS causes the largest open-setting drop (69.80% → 67.50%).

- **Design tradeoffs:** LLM size vs. cost (larger LLMs may improve reasoning but increase latency/cost); evidence quantity vs. noise (using first 2 paragraphs + BM25 top-k balances comprehensiveness with noise); training data mix (80:20 open:gold ratio optimal, pure gold training fails in open settings).

- **Failure signatures:** High LLM agreement but wrong SLM prediction indicates SLM overfitting or training distribution mismatch; low gold evidence retrieval rate indicates ambiguity elimination failing or entities missing Wikipedia entries; explanations contradict predictions indicate rationale selection logic error.

- **First 3 experiments:** 1) Reproduce Table II ablation on HOVER 2-hop to validate each component's contribution; 2) Inspect ambiguity resolution outputs: manually verify 10-20 examples to assess LLM hallucination rate; 3) Vary the open:gold training ratio to find optimal mix for your target deployment setting.

## Open Questions the Paper Calls Out
- How does the scale of the Large Language Model utilized for conflicting perspective reasoning impact the final accuracy and robustness of the claim verification?
- Can the conflicting reasoning approach be effectively extended to verify unstructured or multimodal claims, such as images and videos, rather than solely textual claims?
- How does the performance of CRAVE degrade when verifying claims that require real-time information or knowledge outside the static Wikipedia corpus?

## Limitations
- The paper lacks specification of exact LLM model versions used for ambiguity elimination and rationale generation, making exact reproduction difficult.
- The claimed improvements for complex claims may be less generalizable given the significant performance drop when training on gold-only data.
- The method's dependency on Wikipedia as the knowledge source may limit its applicability to domains without structured knowledge bases.

## Confidence

| Claim | Confidence |
|-------|------------|
| Separation of reasoning (LLM) and judgment (SLM) being beneficial | High |
| Ambiguity elimination improving retrieval completeness | Medium |
| Structured conflicting perspectives improving LLM reasoning quality | Medium |

## Next Checks

1. Reproduce the ambiguity elimination module by manually inspecting 20 claim-entity resolution examples to measure hallucination rates and correctness.
2. Vary the training data ratio (open:gold) systematically beyond the 80:20 optimum to understand robustness boundaries and overfitting tendencies.
3. Test CRAVE on a domain-specific dataset (e.g., medical claims from Step-by-Step Fact Verification) to assess generalization beyond Wikipedia-based multi-hop claims.