---
ver: rpa2
title: 'CSMF: Cascaded Selective Mask Fine-Tuning for Multi-Objective Embedding-Based
  Retrieval'
arxiv_id: '2504.12920'
source_url: https://arxiv.org/abs/2504.12920
tags:
- csmf
- click
- retrieval
- objectives
- conversion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently optimizing multiple
  objectives in embedding-based retrieval (EBR) systems, which are critical in e-commerce
  platforms for recommending relevant products. Existing multi-objective EBR methods
  often suffer from increased model parameters, higher retrieval latency, and difficulties
  in capturing the cascading relationships between objectives.
---

# CSMF: Cascaded Selective Mask Fine-Tuning for Multi-Objective Embedding-Based Retrieval

## Quick Facts
- arXiv ID: 2504.12920
- Source URL: https://arxiv.org/abs/2504.12920
- Reference count: 40
- Primary result: CSMF outperforms state-of-the-art multi-objective EBR methods on recall and NDCG metrics while maintaining low retrieval latency

## Executive Summary
This paper addresses the challenge of efficiently optimizing multiple objectives in embedding-based retrieval (EBR) systems for e-commerce platforms. The proposed CSMF framework uses parameter-efficient fine-tuning with selective masking to sequentially optimize exposure, click, and conversion objectives while preserving upstream knowledge. Experimental results show significant improvements over baselines in both offline metrics and online A/B tests, demonstrating practical value for real-world e-commerce applications.

## Method Summary
CSMF employs a three-stage training approach: pre-train on exposure data, fine-tune on click data, then fine-tune on conversion data. The framework uses Cumulative Percentile-based Pruning to identify and remove low-information neurons, freeing parameter space for downstream objectives. Accuracy recovery retrains remaining parameters on a subset to preserve upstream performance. A cross-stage adaptive margin loss function reduces conflicts between objectives by dynamically adjusting contrastive margins based on upstream-downstream score alignment.

## Key Results
- CSMF achieves 0.1336 click Recall@50 and 0.0109 conversion Recall@50 on Industrial Dataset, outperforming MOPPR by 13.5% and 22.6% respectively
- Online A/B tests show 0.7% RPM, 0.5% CTR, and 1.2% CVR improvements compared to baseline
- Maintains low online retrieval latency and storage overhead while handling multiple objectives
- Outperforms other parameter-efficient methods like LoRA and BitFit on all metrics

## Why This Works (Mechanism)

### Mechanism 1: Parameter Partitioning via Selective Masking
Pruning low-information neurons frees capacity for downstream objectives without increasing model size or vector dimensionality. CPP ranks neurons by absolute value contribution per layer, prunes those below a cumulative threshold, and reserves the released capacity for subsequent tasks. Core assumption: neurons with low absolute weight magnitude carry redundant or non-critical information. Break condition: pruning ratio >~85% causes sharp recall drops due to insufficient independent space.

### Mechanism 2: Cascaded Sequential Fine-Tuning with Accuracy Recovery
Sequential training aligned with user behavior cascade (exposure → click → conversion) preserves upstream knowledge while enabling downstream specialization. After pre-training on exposure data, each stage: prunes redundant parameters, fine-tunes remaining parameters on a small subset to recover accuracy, freezes recovered parameters, then trains freed parameters on downstream objective. Core assumption: exposure task benefits from largest data and its learned representations transfer to sparser downstream tasks. Break condition: skipping accuracy recovery degrades upstream task after pruning.

### Mechanism 3: Cross-Stage Adaptive Margin Loss (AML)
Dynamically adjusting contrastive margins based on upstream-downstream score alignment reduces negative transfer. When upstream scores align with downstream labels, margin is preserved. When misaligned, margin is amplified by factor η to force downstream correction. Core assumption: conflicts between cascaded objectives are measurable via score disagreement and can be mitigated by adaptive difficulty adjustment. Break condition: if η is too high (>~2.0 for conversion), over-correction harms recall; if too low, conflicts remain unresolved.

## Foundational Learning

- **Two-tower EBR architecture**: CSMF operates on user and item encoders producing embeddings; dot product computes relevance. Quick check: Can you explain why item vectors are pre-computed and indexed via ANN?
- **Catastrophic forgetting in multi-task learning**: CSMF's selective masking and accuracy recovery directly address this. Quick check: What happens if you train on conversion data without freezing click-learned parameters?
- **Contrastive learning with softmax loss**: AML modifies standard softmax-based contrastive loss. Quick check: How does in-batch negative sampling differ from random negative sampling for EBR?

## Architecture Onboarding

- Component map: User Tower / Item Tower (parallel encoders) -> CPP module (prunes per layer based on cumulative percentile) -> Accuracy Recovery module (re-trains unpruned weights on subset) -> AML loss function (receives upstream scores, computes adaptive margin) -> Online inference: weighted fusion of partitioned embeddings
- Critical path: 1. Pre-train full model on exposure data → θ̂_d, 2. CPP prunes θ̂_d → {θ_d, θ_p}, 3. Accuracy recovery on D' → freeze θ_d, 4. Fine-tune θ_p on click data → θ̂_o → prune → {θ_o, θ_r}, 5. Fine-tune θ_r on conversion data, 6. Online: concatenate θ_d, θ_o, θ_r; apply weights k_d, k_o, k_r
- Design tradeoffs: Pruning ratio τ (higher = more capacity for downstream, but risks upstream degradation); Accuracy recovery data size (too small = insufficient recovery; too large = overfitting); Weight triplet <k_d, k_o, k_r> (business-dependent; no universal optimum)
- Failure signatures: Click recall drops after conversion training → accuracy recovery skipped or insufficient; Online latency spikes → verify vector dimension unchanged; check for accidental MoE expansion; Conversion recall plateaus → η may be too low; AML not correcting conflicts
- First 3 experiments: 1. Reproduce ablation (Table 3) on sampled dataset: w/o CPP, w/o AML, w/o AR to validate each component's contribution; 2. Sweep pruning ratio τ ∈ [0.55, 0.85] and plot Recall@50 for click vs. conversion to find stable operating point; 3. Deploy with weight triplet <1.0, 1.8, 1.2> and A/B test against baseline MOPPR; monitor RPM, CTR, CVR

## Open Questions the Paper Calls Out

### Open Question 1
Can CSMF be adapted for multi-objective retrieval tasks where relationships are concurrent or parallel rather than strictly cascaded? The framework explicitly leverages cascading relationships and nested positive sample sets (R ⊂ O ⊂ D). Evidence would require experiments on datasets with non-hierarchical labels comparing CSMF against parallel multi-task learning architectures.

### Open Question 2
How does absence of explicit exposure data impact downstream fine-tuning convergence and stability? The paper notes reduced improvement on AliExpress dataset due to missing unexposed labels. Evidence would come from ablation studies on Industrial Dataset comparing standard exposure pre-training against variants skipping exposure data.

### Open Question 3
Is the fixed sequential ordering (Exposure → Click → Conversion) optimal? The framework imposes strict sequential dependency, but doesn't explore if reversing or shuffling this order would alter negative transfer effects. Evidence would require experiments varying training order to measure resulting degradation or stability.

### Open Question 4
Does CPP generalize to Transformer-based architectures? While inspired by PEFT in LLMs, CPP methodology describes pruning typical of DNNs/MLPs, not attention mechanisms. Evidence would require implementation on Transformer-based retrieval models to evaluate effectiveness with attention heads.

## Limitations
- Architectural details remain unspecified, particularly exact network configuration and feature encoding methods
- Training procedure lacks precision regarding epochs per stage, learning rate schedules, and optimizer configuration
- Adaptive margin loss implementation details require clarification, particularly how upstream scores are computed
- Online inference assumes fixed weight triplet is universally optimal despite acknowledging business-dependency

## Confidence
- High confidence: Fundamental mechanism of parameter partitioning via CPP and cascaded sequential training with accuracy recovery is well-explained and logically sound
- Medium confidence: AML loss function's effectiveness depends on proper tuning of η and accurate upstream score alignment detection
- Low confidence: Complete reproduction requires assumptions about unspecified architectural and training parameters

## Next Checks
1. Reproduce Table 3's ablation results by implementing CSMF with and without CPP, AML, and accuracy recovery components on a sampled dataset
2. Sweep pruning ratio τ across [0.55, 0.85] and plot Recall@50 for click vs. conversion objectives to identify stable operating points
3. Deploy CSMF with weights <1.0, 1.8, 1.2> in A/B testing against baseline MOPPR, monitoring RPM, CTR, and CVR improvements