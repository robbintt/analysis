---
ver: rpa2
title: 'KBest: Efficient Vector Search on Kunpeng CPU'
arxiv_id: '2508.03016'
source_url: https://arxiv.org/abs/2508.03016
tags:
- search
- vector
- graph
- kunpeng
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KBest is a vector search library optimized for Huawei Kunpeng 920
  ARM CPUs. Existing libraries like FAISS and DiskANN are optimized for x86 architectures,
  creating a performance gap on ARM platforms.
---

# KBest: Efficient Vector Search on Kunpeng CPU

## Quick Facts
- arXiv ID: 2508.03016
- Source URL: https://arxiv.org/abs/2508.03016
- Reference count: 40
- KBest outperforms x86-based libraries by 1.04x-1.34x in query throughput at high recall levels on ARM Kunpeng CPUs.

## Executive Summary
KBest is a vector search library optimized for Huawei Kunpeng 920 ARM CPUs that addresses the performance gap left by x86-optimized libraries like FAISS and DiskANN. The library incorporates ARM-specific optimizations including SIMD-accelerated distance computation, data prefetching, memory alignment, and algorithmic improvements like graph index refinement, early termination, and vector quantization. Experimental results show KBest achieves 1.04x-1.34x higher query throughput compared to state-of-the-art x86-based libraries while maintaining high recall levels. The library is deployed in production serving tens of millions of queries daily across social media, e-commerce, and enterprise applications.

## Method Summary
KBest implements graph-based Approximate Nearest Neighbor Search optimized for ARM Kunpeng 920 CPUs. The method builds proximity graph indexes (HNSW/NSG/Vamana variants) and applies multiple optimizations: (1) ARM NEON SIMD 1-to-B batched distance computation using fused operators, (2) Software prefetch with prfm PLD1KEEP, (3) 2MB huge pages and 64-byte aligned memory, (4) Graph reordering via MST-based traversal, (5) Early termination with threshold tuning, and (6) Optional vector quantization (PQ/SQ). The approach targets maximizing queries per second at high recall levels on ARM architecture where existing libraries are x86-optimized.

## Key Results
- KBest achieves 1.34× improvement in query throughput over x86 libraries at high recall levels
- SIMD optimization alone provides 60-92% latency reduction through NEON fused operators
- Early termination with optimized thresholds delivers up to 1.04× improvement while maintaining recall targets

## Why This Works (Mechanism)

### Mechanism 1: Batched SIMD Distance Computation with Fused Operators
- Fused multiply-accumulate (VMLAQ) NEON instructions and 1-to-B batched distance computation reduce instruction count and improve pipeline utilization on Kunpeng 920's multi-issue architecture
- Transform scalar 1-to-1 distance calculations into batched 1-to-B vectorized operations, enabling up to 16 parallel distance computations per cycle; use `vmlaq_f32` to fuse multiply-accumulate into single intrinsics
- Core assumption: Data dependencies permit parallel execution; vector dimensions are padded for alignment
- Evidence: [section 3.1] describes batched 1-to-B vectorized operations enabling 16 parallel computations per cycle and utilization of `vmlaq_f32` NEON instruction

### Mechanism 2: Pipeline Software Prefetching for Graph Traversal
- Prefetching adjacent lists and vector data of top-priority candidate nodes B batches ahead reduces cache misses from irregular graph access patterns
- While processing current node's neighbors, asynchronously prefetch the next B neighbors using ARM's `prfm PLD1KEEP` instruction; batch size B is cache-aware: `B = floor(α × C_L1d / (d × s))` where α ≈ 0.5
- Core assumption: Graph traversal locality is predictable enough that prefetched data will be used before eviction; L1d cache can hold prefetched vectors without thrashing
- Evidence: [section 3.1] explains prefetching adjacent lists and vector data of top priority nodes using `prfm` with PLD1KEEP operand

### Mechanism 3: MST-Based Graph Reordering for Cache Locality
- Reordering graph nodes by descending subtree size (from MST traversal) places densely connected regions contiguously in memory, reducing cache misses
- Build MST from proximity graph, compute subtree sizes via DFS, then traverse nodes in descending `met(v)` order using a priority queue to produce memory layout
- Core assumption: Proximity graph's small-world structure is preserved under MST projection; subtree size correlates with co-access probability
- Evidence: [section 3.2] describes prioritized traversal processing nodes in descending order of subtree sizes to cluster densely connected regions

## Foundational Learning

- **Concept: Proximity Graph Indexes (HNSW, NSG, Vamana)**
  - Why needed: KBest builds on these SOTA graph structures; understanding greedy traversal and edge selection rules is prerequisite to evaluating refinement strategies
  - Quick check: Can you explain why a larger candidate queue L improves recall but increases latency in Algorithm 1?

- **Concept: ARM NEON SIMD Intrinsics**
  - Why needed: Core optimization relies on NEON instructions like `vmlaq_f32`; developers must understand register width (128-bit) and multi-issue constraints
  - Quick check: What is the maximum number of float32 elements that fit in a single NEON register, and how does this differ from AVX-512?

- **Concept: Translation Lookaside Buffer (TLB) and Huge Pages**
  - Why needed: KBest uses 2MB huge pages to reduce TLB misses during random graph traversal; understanding virtual-to-physical mapping overhead is essential
  - Quick check: Why does random access across 100M vectors cause more TLB pressure with 4KB pages than with 2MB pages?

## Architecture Onboarding

- **Component map:** Index Layer (CSR-formatted graph with fixed out-degree M) -> Compute Layer (Batched SIMD distance kernel, fused NEON operators, pipelined prefetch) -> Memory Layer (64-byte aligned vectors, 2MB huge pages, NUMA-aware allocation) -> Query Layer (Thread pool distributing queries, early termination heuristics, quantization module)

- **Critical path:** Distance computation during neighbor evaluation is the dominant cost; prefetch latency must be hidden by overlapping with compute

- **Design tradeoffs:** Larger batch size B improves prefetch efficiency but risks L1d eviction; early termination reduces compute but requires recall validation per dataset; graph reordering improves cache locality but adds index build time

- **Failure signatures:** Recall drops below target (early termination thresholds too aggressive); high TLB miss rate (huge pages not enabled or fragmented memory); SIMD throughput lower than expected (vectors not cache-line aligned or dimension padding incorrect)

- **First 3 experiments:**
  1. Baseline latency profile: Measure distance computation vs. graph traversal time without optimizations; identify bottleneck
  2. SIMD ablation: Compare scalar vs. NEON vs. batched 1-to-B kernels on Glove-1M; verify 60-92% gain as reported
  3. Prefetch tuning: Sweep batch size B (8, 16, 32) on BigANN-100M; measure cache miss rate and QPS

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation compares KBest against x86-optimized libraries without controlling for hardware differences (Kunpeng 920 specifications not disclosed)
- Source code is proprietary with no public repository, limiting independent verification
- Exact hyperparameters for each dataset (graph degree M, search list size L, refinement iterations F, prefetch batch B, early termination parameters) are not specified

## Confidence

- **High Confidence**: ARM NEON SIMD optimization effectiveness (1.34× improvement) - well-supported by SIMD vectorization literature and consistent with KScaNN results
- **Medium Confidence**: Graph reordering via MST improves cache locality - plausible based on locality clustering theory, but lacks empirical validation in ANNS context
- **Medium Confidence**: Early termination with τ_max tuning achieves 1.04× improvement - reasonable, but requires dataset-specific tuning that is not disclosed

## Next Checks

1. **Hardware Parity Test**: Reproduce KBest's QPS-recall curves on identical datasets using Kunpeng 920 vs. comparable x86 server (e.g., Xeon) with FAISS/DiskANN, ensuring identical memory bandwidth and core count to isolate software impact

2. **Optimization Ablation Study**: Implement KBest without MST reordering and without early termination; measure QPS loss to quantify individual contributions

3. **Prefetch Accuracy Measurement**: Instrument KBest to log cache hit/miss rates during graph traversal with and without prfm prefetching; correlate miss rate reduction to QPS gains