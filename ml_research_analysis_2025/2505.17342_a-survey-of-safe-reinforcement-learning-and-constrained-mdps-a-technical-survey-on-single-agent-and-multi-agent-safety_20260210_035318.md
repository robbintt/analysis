---
ver: rpa2
title: 'A Survey of Safe Reinforcement Learning and Constrained MDPs: A Technical
  Survey on Single-Agent and Multi-Agent Safety'
arxiv_id: '2505.17342'
source_url: https://arxiv.org/abs/2505.17342
tags:
- safe
- safety
- constraints
- learning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a rigorous overview of Safe Reinforcement
  Learning (SafeRL) based on Constrained Markov Decision Processes (CMDPs) and extends
  the discussion to Multi-Agent Safe RL (SafeMARL). It covers theoretical foundations
  including CMDP definitions, constrained optimization techniques, and key theorems.
---

# A Survey of Safe Reinforcement Learning and Constrained MDPs: A Technical Survey on Single-Agent and Multi-Agent Safety

## Quick Facts
- arXiv ID: 2505.17342
- Source URL: https://arxiv.org/abs/2505.17342
- Reference count: 15
- Provides rigorous technical survey of SafeRL and SafeMARL based on CMDPs

## Executive Summary
This technical survey provides a comprehensive overview of Safe Reinforcement Learning (SafeRL) through the lens of Constrained Markov Decision Processes (CMDPs), extending the discussion to Multi-Agent Safe RL (SafeMARL). The paper systematically covers theoretical foundations including CMDP definitions, constrained optimization techniques, and key theorems, while reviewing state-of-the-art algorithms for both single-agent and multi-agent settings. The survey addresses both cooperative and competitive multi-agent scenarios, highlighting critical challenges such as decentralized safety and non-stationary environments. The authors identify five open research problems, with three specifically targeting SafeMARL, to guide future research directions in the field.

## Method Summary
The survey employs a systematic technical review methodology, organizing content around CMDP frameworks as the unifying theoretical foundation. It synthesizes existing literature on Lagrangian-based policy optimization, safety shields, risk-sensitive methods, and their multi-agent extensions. The approach involves categorizing algorithms by their optimization techniques, safety mechanisms, and applicability to different agent settings, while maintaining rigorous mathematical treatment of constraints and optimization objectives throughout the analysis.

## Key Results
- Comprehensive coverage of CMDP-based SafeRL algorithms including Lagrangian optimization and safety shields
- Extension to SafeMARL addressing both cooperative and competitive multi-agent scenarios
- Identification of five critical open research problems with detailed technical challenges and prior work connections

## Why This Works (Mechanism)
The survey works by establishing CMDPs as the foundational framework for analyzing safety in reinforcement learning, providing a mathematically rigorous basis for constraint satisfaction. By systematically organizing algorithms according to their optimization strategies and safety mechanisms, the survey creates a clear taxonomy that helps researchers understand the relationships between different approaches. The extension to multi-agent settings addresses the added complexity of non-stationary environments and decentralized decision-making, which are critical for practical applications.

## Foundational Learning

Constrained Markov Decision Processes (CMDPs): Framework for reinforcement learning with safety constraints alongside reward maximization
- Why needed: Provides theoretical foundation for balancing safety and performance
- Quick check: Verify that constraints are properly formulated as functions of state-action pairs

Lagrangian Optimization: Technique for converting constrained optimization problems into unconstrained ones using Lagrange multipliers
- Why needed: Enables gradient-based optimization of constrained RL objectives
- Quick check: Confirm that dual variables are properly updated and converge

Risk-sensitive Methods: Approaches that account for uncertainty and potential catastrophic outcomes
- Why needed: Addresses safety beyond hard constraints through probabilistic guarantees
- Quick check: Validate that risk measures properly capture worst-case scenarios

## Architecture Onboarding

Component Map: CMDP formulation -> Lagrangian optimization / Safety shields / Risk-sensitive methods -> Algorithm implementation -> Safety constraint satisfaction
- Critical path: CMDP formulation → Lagrangian optimization → Algorithm implementation
- Design tradeoffs: Hard constraints vs. soft penalties, centralized vs. decentralized safety enforcement, single-agent vs. multi-agent scalability
- Failure signatures: Constraint violation, convergence to suboptimal policies, inability to handle non-stationarity
- First experiments:
  1. Implement Lagrangian-based policy optimization on a simple grid-world with safety constraints
  2. Test safety shield mechanisms on a cart-pole system with constraint violations
  3. Evaluate multi-agent cooperative safety in a predator-prey environment with decentralized constraints

## Open Questions the Paper Calls Out
The survey proposes five open research problems, three focusing on SafeMARL: 1) Developing scalable algorithms for large-scale multi-agent safety, 2) Addressing non-stationary environments in competitive settings, and 3) Creating robust decentralized safety mechanisms for heterogeneous agent populations.

## Limitations
- Rapid field evolution may quickly date algorithmic discussions and open problems
- Limited practical implementation validation and real-world performance metrics
- CMDP-centric framework may overlook emerging alternative safety formulations

## Confidence

Theoretical foundations and algorithmic descriptions: High
Assessment of open research problems: Medium
Practical applicability and real-world performance: Medium to Low

## Next Checks

1. Implement and benchmark key SafeRL algorithms (e.g., Lagrangian-based policy optimization) on standard safety-critical environments to assess practical performance and safety guarantees.

2. Conduct a comparative study of CMDP-based SafeRL methods against alternative safety formulations (e.g., Lyapunov-based methods) in multi-agent settings to evaluate relative strengths and weaknesses.

3. Develop a case study applying SafeMARL techniques to a real-world cooperative scenario (e.g., multi-robot navigation) to identify practical challenges and scalability issues not captured in theoretical discussions.