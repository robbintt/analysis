---
ver: rpa2
title: 'INTENT: Trajectory Prediction Framework with Intention-Guided Contrastive
  Clustering'
arxiv_id: '2503.04952'
source_url: https://arxiv.org/abs/2503.04952
tags:
- trajectory
- prediction
- road
- trajectories
- intention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes INTENT, a trajectory prediction framework that
  explicitly models road agents' intentions through contrastive clustering. The core
  idea is to leverage intention-guided contrastive learning to obtain more robust
  trajectory representations, leading to superior prediction accuracy.
---

# INTENT: Trajectory Prediction Framework with Intention-Guided Contrastive Clustering

## Quick Facts
- arXiv ID: 2503.04952
- Source URL: https://arxiv.org/abs/2503.04952
- Reference count: 40
- Primary result: Achieves state-of-the-art trajectory prediction accuracy using only MLPs for efficiency

## Executive Summary
INTENT is a trajectory prediction framework that explicitly models road agents' intentions through contrastive clustering. The core idea is to leverage intention-guided contrastive learning to obtain more robust trajectory representations, leading to superior prediction accuracy. By using only multi-layer perceptrons (MLPs), INTENT achieves high efficiency suitable for real-world deployment while accommodating the fuzziness and abstraction of human intention in trajectories. Experiments on real-world datasets for pedestrians and autonomous vehicles demonstrate the effectiveness and efficiency of INTENT, achieving state-of-the-art performance in trajectory prediction.

## Method Summary
INTENT transforms raw trajectory coordinates into velocity, heading (radian), and rotation-normalized location sequences to create a more powerful input representation. It uses an MLP-based architecture for encoding features, learning representations, and predicting future locations in parallel (non-autoregressive). The model employs contrastive clustering to structure the representation space based on explicit intention classes (straight, left, right, static). This is combined with a standard classification loss to refine the representation space. The framework is trained end-to-end with a total loss combining classification, contrastive, and prediction losses, optimized using Adam.

## Key Results
- Achieves state-of-the-art trajectory prediction accuracy on real-world datasets for pedestrians and autonomous vehicles
- Uses only MLPs for high efficiency, reducing training and inference time compared to LSTM and Transformer baselines
- Effectively handles the fuzzy and abstract nature of human intention through contrastive clustering
- Demonstrates superior performance with significantly lower computational cost than complex sequential models

## Why This Works (Mechanism)

### Mechanism 1
Contrastive clustering of trajectory representations based on explicit intention classes handles the "fuzzy" and abstract nature of human intention, leading to more robust and accurate predictions. The Intention Clusterer constructs positive and negative pairs of trajectories based on predicted intention labels. A contrastive loss pulls representations of positive pairs closer in the embedding space while pushing negative pairs apart. This hybrid approach with classification loss refines the representation space to be more structured by intention.

### Mechanism 2
Transforming raw trajectory coordinates into velocity, heading (radian), and rotation-normalized location sequences provides a more powerful and invariant input representation. The Observation Feature Extractor computes sequences of velocity and radian at each time step, rotates the entire trajectory to a standard orientation to compute normalized y-coordinate sequence. This focuses the model on motion dynamics and shape rather than absolute position.

### Mechanism 3
A deterministic, non-autoregressive, MLP-only architecture achieves state-of-the-art trajectory prediction accuracy with significantly lower computational cost. The model uses MLPs for encoding features, learning representations, and predicting future locations. The Location Predictor uses separate MLPs for each future time step, predicting all locations in parallel from the learned representation. This avoids the sequential, error-prone decoding of RNNs and the complexity of Transformers.

## Foundational Learning

- **Concept: Contrastive Learning**
  - **Why needed here:** This is the core technique used to structure the representation space. Understanding how positive/negative pairs are constructed and how the loss function operates is essential for grasping how INTENT models "fuzzy" intentions.
  - **Quick check question:** Given a batch of trajectories with predicted intention labels, how would you form a positive pair and a negative pair for contrastive learning as described in INTENT?

- **Concept: Trajectory Representation Learning**
  - **Why needed here:** The goal of the model is to map a variable-length observation trajectory into a fixed-size vector that captures all relevant information. Understanding how this representation is learned and used is key to the entire architecture.
  - **Quick check question:** What are the two main loss functions used to train the Representation Learner, and what does each optimize the representation for?

- **Concept: Deterministic vs. Stochastic Trajectory Prediction**
  - **Why needed here:** INTENT is a deterministic model (predicting one trajectory) but is compared against stochastic baselines. Understanding this distinction is crucial for interpreting the experimental results and the model's design goals.
  - **Quick check question:** What is the fundamental difference in output between a deterministic and a stochastic trajectory prediction model, and how does INTENT fit into this categorization?

## Architecture Onboarding

- **Component map:** Raw Trajectory -> Observation Feature Extractor -> Representation Learner -> Intention Clusterer -> Location Predictor
- **Critical path:** Raw Trajectory -> Observation Feature Extractor -> Representation Learner -> Location Predictor. The Intention Clusterer is a training-time mechanism acting on the output of the Representation Learner.
- **Design tradeoffs:**
  - Efficiency vs. Temporal Complexity: MLP-only architecture ensures high efficiency but may struggle with very complex temporal patterns compared to RNN/Transformer models
  - Determinism vs. Multi-modality: The model produces a single deterministic prediction, which is simpler but may not capture the full range of possible future behaviors as well as stochastic models
  - Feature Engineering vs. Raw Data: Heavy reliance on engineered features provides strong inductive bias but may discard subtle patterns learnable from raw coordinates
- **Failure signatures:**
  - Cluster Collapse: The contrastive loss fails to separate representations, leading to poor intention estimation
  - Prediction Drift: In long-term predictions, the lack of autoregressive feedback causes predictions to become unrealistic
  - Misaligned Intention: Incorrect intention labels from the feature extractor misguide the representation learning
- **First 3 experiments:**
  1. Reproduce Ablation Study: Retrain the model removing the contrastive clustering loss (w/o Lc) and then the classification loss (w/o Lr) to validate their individual contributions to representation quality and prediction accuracy on a single dataset (e.g., ETH)
  2. Feature Ablation: Test the model using only raw coordinates as input (INTENT-X variant from Figure 11) to quantify the performance gain from the velocity/radian/transformed-y feature engineering
  3. Efficiency Validation: Measure and compare the training time and inference latency of INTENT against a standard LSTM-based trajectory prediction model on the same hardware and dataset to confirm the paper's efficiency claims

## Open Questions the Paper Calls Out

- **Question:** Can the INTENT framework be extended to model stochastic trajectories for multiple interacting road agents simultaneously?
  - **Basis in paper:** [explicit] The authors explicitly state in the conclusion that the proposed INTENT can be "further extended to predict stochastic trajectories for multiple road agents simultaneously."
  - **Why unresolved:** The current framework is deterministic and processes agents individually. Extending it requires modifying the contrastive clustering mechanism to handle joint probability distributions and inter-agent dependencies without reintroducing the complex RNN or attention structures the authors sought to avoid.
  - **What evidence would resolve it:** A modified version of INTENT that generates diverse, multi-modal trajectory samples for interacting agents, evaluated on multi-agent metrics like joint ADE/FDE.

- **Question:** How does the framework perform when estimating intentions for vulnerable road agents with atypical kinematics?
  - **Basis in paper:** [explicit] The conclusion identifies a research gap, noting it "would be interesting to particularly estimate the intentions of those vulnerable road agents (e.g., pedestrians with disabilities, autonomous vehicles carrying emergency patients)."
  - **Why unresolved:** The current labeling algorithm relies on general velocity and orientation thresholds that may not capture the distinct motion patterns or response times of these specific groups.
  - **What evidence would resolve it:** Evaluation on specialized datasets containing trajectories of vulnerable agents, demonstrating that the intention clustering can adapt to or differentiate these behaviors accurately.

- **Question:** Does the intention-guided approach maintain its advantage in long-term trajectory prediction tasks?
  - **Basis in paper:** [explicit] The authors list "trajectory prediction for a longer duration" as an "interesting and challenging task" for future studies.
  - **Why unresolved:** The experiments are limited to short-term horizons. Long-term prediction increases uncertainty and the likelihood of intention changes, which the current single-intention clustering model might not handle robustly.
  - **What evidence would resolve it:** Benchmarking INTENT on long-range datasets (e.g., 5-10 second horizons) to see if the intention representation degrades compared to map-based or recurrent baselines.

## Limitations

- The framework relies heavily on specific feature engineering (velocity, radian, transformed-y) whose contribution is not directly isolated in main experiments
- Contrastive clustering depends on accurate intention labeling from Algorithm 1, which uses fixed thresholds that may not generalize across datasets
- The MLP-only architecture trades expressiveness for efficiency, potentially limiting performance on highly complex trajectories
- The model produces deterministic predictions, which may not capture the full distribution of possible future behaviors

## Confidence

- **High confidence:** The core mechanism of using contrastive clustering for intention-guided representation learning is well-supported by experimental ablation studies (w/o Lc). The efficiency claims (training/inference time) are quantitatively validated against LSTM and Transformer baselines.
- **Medium confidence:** The superiority of the specific feature engineering approach is inferred from comparative results but not directly ablated. The assumption that these features are more informative than raw coordinates is reasonable but unverified.
- **Medium confidence:** The deterministic prediction approach achieves state-of-the-art accuracy on benchmark datasets, but direct comparison against stochastic baselines on multi-modal metrics is limited.

## Next Checks

1. **Ablation on Feature Engineering:** Retrain INTENT using only raw coordinate inputs (no velocity/radian transformation) and compare performance to validate the claimed benefit of the engineered features.
2. **Intention Label Robustness:** Perturb the thresholds in Algorithm 1 (e.g., vary Va, threshold_angle) and measure the impact on prediction accuracy to assess sensitivity to the labeling process.
3. **Contrastive Loss Stability:** Conduct experiments with different temperature values (Ï„) and confidence thresholds (Conf) in the contrastive clustering loss to determine optimal hyperparameters and assess stability across datasets.