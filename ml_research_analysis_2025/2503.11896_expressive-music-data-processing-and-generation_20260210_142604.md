---
ver: rpa2
title: Expressive Music Data Processing and Generation
arxiv_id: '2503.11896'
source_url: https://arxiv.org/abs/2503.11896
tags:
- musical
- entropy
- sequence
- music
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of preserving musical expressivity
  and coherence in AI-generated music. The core contribution is a novel listening-based
  data processing technique inspired by Weber's law that captures expressive nuances
  in MIDI performances.
---

# Expressive Music Data Processing and Generation

## Quick Facts
- **arXiv ID:** 2503.11896
- **Source URL:** https://arxiv.org/abs/2503.11896
- **Reference count:** 0
- **Primary result:** Novel listening-based data processing (Weber's law) preserves musical expressivity while entropy-based selection produces coherent, predictable generations

## Executive Summary
This paper addresses the challenge of preserving musical expressivity and coherence in AI-generated music. The core contribution is a novel listening-based data processing technique inspired by Weber's law that captures expressive nuances in MIDI performances. The method models interdependencies among multiple musical arguments (pitch, duration, velocity, time shift, pedal) using a decomposed multi-output sequential model with probabilistic conditioning. Output entropy sequences are employed as a selection criterion to identify stable, predictable generations. The approach successfully encodes musical intuition into neural networks and produces musically coherent outputs while preserving expressivity. Theoretical analysis links output entropy behavior to informational aesthetic measures, revealing that optimal musical satisfaction involves balancing predictability with complexity through tension and resolution patterns.

## Method Summary
The method processes Yamaha e-Piano Competition MIDI data through Weber's law-inspired binning to create perceptually-relevant categorical intervals for time shift, duration, and velocity change. Five separate LSTMs are trained sequentially (pitch → time shift → duration → velocity → pedal) with chain-rule conditioning, where each model's sampled output becomes input for the next. During inference, sequences are generated autoregressively with entropy monitoring at each step. The final selection ranks generations by average entropy (low = stable) while regulating ordering using ground-truth entropy statistics to avoid overly "safe" outputs.

## Key Results
- Decomposing multi-output generation into sequentially conditioned submodels better captures musical interdependencies than independent or monolithic approaches
- Weber's law-inspired binning preserves expressive nuances that linear encoding discards by aligning quantization with human perceptual sensitivity
- Output entropy sequences effectively identify stable, predictable generations while avoiding the "boring safe mode" of lowest-entropy sequences
- The approach successfully encodes musical intuition (e.g., dissonant notes are shorter) into neural network architecture

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Aligning data quantization with human perceptual sensitivity (Weber's law) preserves expressive nuances that linear encoding discards.
- **Mechanism**: The model maps continuous musical variables to categorical intervals using a quasi-mel-scale where interval widths expand proportional to value magnitude, reflecting Just Noticeable Difference in human hearing.
- **Core assumption**: Listeners perceive musical dynamics and timing logarithmically rather than linearly.
- **Evidence anchors**: Abstract mentions "listening-based data-processing technique... derived from Weber's law"; section 2 explains interval expansion with value increase.
- **Break condition**: If evaluation prioritizes precise millisecond reconstruction over perceptual equivalence, or if dataset contains non-expressive sequences where this processing introduces noise.

### Mechanism 2
- **Claim**: Decomposing multi-output generation into sequentially conditioned submodels captures causal dependencies better than independent or monolithic generation.
- **Mechanism**: Factorizes joint distribution via chain rule ($p(A,B,C) = p(A) \cdot p(B|A) \cdot p(C|A,B)$) using five separate LSTMs that predict attributes sequentially per note.
- **Core assumption**: Musical arguments possess autonomy but strong interdependencies (e.g., dissonant notes are usually shorter).
- **Evidence anchors**: Abstract mentions "decompose the multi-output sequential model into single-output submodels"; section 3.2 argues independent models violate musical intuitions.
- **Break condition**: If sequential ordering is incorrect for specific performance styles, conditioning may introduce bias rather than clarity.

### Mechanism 3
- **Claim**: Output entropy sequences serve as proxy for stability and musicality, allowing filtering without external classifiers.
- **Mechanism**: Tracks Shannon entropy of probability distribution at each step; sudden spikes indicate exposure bias or out-of-distribution artifacts. Selects sequences with lower average entropy but regulated by ground-truth statistics.
- **Core assumption**: High uncertainty in model's output distribution correlates with poor musical coherence or errors.
- **Evidence anchors**: Abstract mentions "entropy sequence is set as a criterion to select predictable and stable generations"; section 4.1 explains entropy spikes indicate confusion.
- **Break condition**: If model is under-trained or overfitted, entropy may not correlate with human-perceived quality.

## Foundational Learning

- **Concept**: **Weber's Law / Just Noticeable Difference (JND)**
  - **Why needed here**: To understand why raw MIDI data (linear time in ms) is suboptimal for learning human perception, and how logarithmic/proportional binning creates perceptually relevant training tokens.
  - **Quick check question**: Would increasing a note's duration from 100ms to 105ms have the same perceptual impact as increasing 1000ms to 1005ms according to this paper?

- **Concept**: **Probabilistic Chain Rule (Factorization)**
  - **Why needed here**: To understand the architecture relying on factorizing joint probability into conditional probabilities ($P(Y|X)$) to build cascade of submodels.
  - **Quick check question**: Why does the paper argue that $p(x_n)p(x_t)p(x_d)$ (independent model) is musically inferior to $p(x_n) \cdot p(x_t|x_n) \cdot p(x_d|x_n, x_t)$?

- **Concept**: **Informational Aesthetics (Entropy vs. Complexity)**
  - **Why needed here**: To understand selection criteria linking entropy behavior to musical satisfaction through tension-resolution balance.
  - **Quick check question**: According to the paper, why is selecting the sequence with the absolute *lowest* average entropy potentially a bad musical outcome?

## Architecture Onboarding

- **Component map**: Data Encoder (Weber binning) -> Submodel Cascade (5 LSTMs: Pitch→Time→Duration→Velocity→Pedal) -> Sampler -> Entropy Monitor -> Selector

- **Critical path**: 
  1. Preprocessing: Verify Weber-binning logic ensures balanced class frequencies
  2. Training: Train 5 LSTMs separately with sampled outputs as conditioning inputs
  3. Inference: Generate → Compute Entropy Vector → Filter by threshold/percentile

- **Design tradeoffs**:
  - Model Coupling: Decomposing into 5 submodels handles interdependencies without "cramming all features into a single hidden state" but increases inference complexity and potentially loses global context compared to unified Transformer
  - Selection vs. Reinforcement: Uses entropy for selection (post-hoc filtering) rather than reinforcement (reward function), simpler implementation but higher compute waste during inference

- **Failure signatures**:
  - "Independent Argument" behavior: If submodels ignore conditioned inputs, outputs will feel disjointed
  - Exposure Bias Loops: If sampler picks "weird" token early, entropy spikes and rest becomes random noise
  - Boring "Safe" Mode: If selection thresholds too strict on low mean entropy, outputs become repetitive scales/arpeggios

- **First 3 experiments**:
  1. Ablation on Conditioning: Train parallel vs. sequential conditioning versions; evaluate musical coherence or inter-argument correlation accuracy
  2. Entropy Threshold Sensitivity: Plot musical satisfaction vs. average entropy to verify U-curve hypothesis (too low = boring, too high = chaotic)
  3. Weber vs. Linear Binning: Train identical models on linear time bins vs. Weber bins; compare convergence speed and listener preference

## Open Questions the Paper Calls Out

- Can the proposed entropy-based selection metric be effectively integrated into the training process as a reinforcement learning reward function? (Explicit: Authors state metric "can also be designed as a reward to train the model with reinforcement learning, which will be left as future work")

- How can the Transformer architecture be adapted to model conditional interdependencies among multiple musical arguments without prohibitive computational cost? (Explicit: Section 3.2 notes author did not use Transformers because "modeling multi-argument interdependency using transformers also raises unique challenges")

- Does the theoretical correlation between output entropy variance and "musical pleasure" hold true under empirical human evaluation? (Inferred: Section 4.2 links entropy to informational aesthetics but admits it is "far from drawing any conclusions" and notes there are "alternative explanations")

## Limitations

- Theoretical foundation of Weber's law binning lacks empirical validation beyond the Yamaha dataset
- Entropy-based selection criteria rely on mathematical proxies without direct human perceptual validation
- Results demonstrated only on Yamaha e-Piano Competition dataset; generalizability to other musical styles unproven

## Confidence

- **High Confidence**: Architectural decomposition into sequential submodels is clearly specified and reproducible; chain-rule factorization approach is standard and well-justified
- **Medium Confidence**: Listening-based data processing shows sound theoretical grounding but specific implementation details are underspecified
- **Low Confidence**: Entropy-based selection mechanism and connection to informational aesthetics represents most speculative component lacking direct perceptual validation

## Next Checks

1. Conduct controlled listening tests comparing generations against baselines using blinded ratings for expressiveness, coherence, and musicality
2. Systematically vary quantization strategy (linear vs. Weber-inspired bins) and quantify impact on model performance and generation quality
3. Test complete pipeline on additional expressive music datasets (MAESTRO, Lakh MIDI) to evaluate robustness and generalizability beyond Yamaha competition data