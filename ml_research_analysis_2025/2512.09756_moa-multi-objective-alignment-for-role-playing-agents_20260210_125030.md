---
ver: rpa2
title: 'MOA: Multi-Objective Alignment for Role-Playing Agents'
arxiv_id: '2512.09756'
source_url: https://arxiv.org/abs/2512.09756
tags:
- persona
- arxiv
- response
- reward
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of training role-playing agents
  (RPAs) that can simultaneously master multiple conflicting skills, such as following
  instructions, exhibiting domain knowledge, and maintaining a consistent linguistic
  style. Existing methods, such as supervised fine-tuning (SFT) and reinforcement
  learning (RL), have limitations in handling these multi-dimensional requirements
  and the low diversity of model outputs.
---

# MOA: Multi-Objective Alignment for Role-Playing Agents
## Quick Facts
- **arXiv ID:** 2512.09756
- **Source URL:** https://arxiv.org/abs/2512.09756
- **Reference count:** 31
- **Primary result:** Introduces MOA, a reinforcement learning framework for training role-playing agents to master multiple conflicting skills simultaneously.

## Executive Summary
MOA (Multi-Objective Alignment) addresses the challenge of training role-playing agents that must simultaneously master multiple conflicting skills such as instruction following, domain knowledge, and linguistic style. The paper proposes a reinforcement learning framework that enables multi-dimensional, fine-grained optimization for general role-playing agents. Extensive experiments on challenging benchmarks demonstrate that MOA enables an 8B model to match or outperform strong baselines across numerous dimensions, achieving an average score of 4.75 on PersonaGym and 0.75 on RoleMRC.

## Method Summary
MOA is a reinforcement learning framework built on GRPO that introduces three key innovations: (1) a dynamic pivot-dimension weighting strategy that selects the dimension with strongest reward momentum for optimization at each step, (2) a conflict rollout elimination mechanism that removes samples conflicting with the pivot dimension to prevent gradient contamination, and (3) a thought-augmented rollout strategy with off-policy guidance from GPT-4o to improve sample diversity. The method trains simultaneously on multiple fine-grained rubrics and employs both on-policy samples (15) plus one off-policy sample per group during advantage calculation.

## Key Results
- MOA achieves an average score of 4.75 on PersonaGym benchmark across five dimensions (EA, TC, LH, PC, AJ)
- On RoleMRC, MOA reaches 0.75 average score across five binary dimensions (KR, SC, NI, MT, IP)
- An 8B model trained with MOA matches or outperforms GPT-4o and Claude baselines across numerous dimensions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dynamic pivot-dimension weighting accelerates multi-objective optimization compared to uniform-weight GRPO.
- **Mechanism:** At each training step, MOA computes per-dimension residuals (current reward minus linear-regression trend from recent history), applies softmax to produce weights, and selects the dimension with highest residual as the "pivot" for that step—prioritizing dimensions showing the strongest upward momentum.
- **Core assumption:** Dimensions with positive residuals correlate with stronger gradients (gradient magnitude is recoverable from reward trends).
- **Evidence anchors:**
  - [section 3.2]: "The dimension with the largest reward increase currently represents the easiest learning difficulty and is the most worthy of learning at the current step."
  - [Theorem 2]: Proves that under diagonal Gram and linear residual assumptions, expected improvement exceeds uniform weighting when Cov(u_d, s_d) > 0.
  - [corpus]: Weak—no direct external validation; related work (Pareto Multi-Objective Alignment) addresses similar conflicts but uses different methods.
- **Break condition:** If residuals become noise-dominated (gradient magnitude uncorrelated with reward trend), pivot selection becomes arbitrary and advantage collapses to uniform weighting.

### Mechanism 2
- **Claim:** Eliminating conflict rollouts prevents gradient contamination from samples that are negative on the pivot dimension but positive elsewhere.
- **Mechanism:** Given pivot dimension d*, MOA finds the largest subset M where samples are partially-ordered consistent (ri,d* > rj,d* AND weighted-sum Ri > Rj), setting advantages to zero for excluded samples. Uses dynamic programming (LIS-based algorithm) to extract M.
- **Core assumption:** Samples performing well on non-pivot dimensions but poorly on d* introduce optimization noise rather than useful signal.
- **Evidence anchors:**
  - [section 3.2]: "consider R1, oG is mistakenly regarded as a positive sample. This introduces noise into the optimization process."
  - [Figure 1]: Visual example showing rollouts with conflicting reward patterns receiving identical advantages under standard GRPO.
  - [corpus]: No direct corpus validation; concept is specific to this framework.
- **Break condition:** If pivot dimension changes rapidly between steps, previously eliminated samples might contain useful signal for the new pivot, causing unstable training.

### Mechanism 3
- **Claim:** Thought-augmented rollout with off-policy guidance improves sample diversity and mitigates reward hacking.
- **Mechanism:** (1) Model generates explicit persona-aligned reasoning before responding using structured prompt Pthink; (2) One off-policy sample from GPT-4o is mixed with 15 on-policy samples during advantage calculation, providing diverse reference points.
- **Core assumption:** Explicit reasoning improves role-playing quality; stronger model outputs provide non-local exploration signals.
- **Evidence anchors:**
  - [section 4.3.2]: "MOA-o [without thinking] curve slows down in the later stages of training... incorporating role-related thinking can help the model escape the local optimum."
  - [Figure 3]: Claude-3.7 performance improves across nearly all PersonaGym dimensions when using think-first prompting.
  - [corpus]: "Thinking in Character" (arXiv:2506.01748) independently validates role-aware reasoning benefits for RPAs.
- **Break condition:** If thinking tokens become formulaic or the off-policy model has distributional mismatch, diversity gains diminish and may introduce optimization instability.

## Foundational Learning

- **Concept: GRPO (Group Relative Policy Optimization)**
  - Why needed here: MOA builds on GRPO's group-based advantage normalization; understanding the baseline clarifies what MOA modifies.
  - Quick check question: Can you explain how GRPO normalizes rewards within a group and why this requires multiple samples per query?

- **Concept: Partial Order and LIS (Longest Increasing Subsequence)**
  - Why needed here: Conflict elimination uses LIS to find the largest consistent subset under a partial order relation.
  - Quick check question: Given points (x1,y1), (x2,y2), ..., how would you find the largest subset where both coordinates are monotonically increasing?

- **Concept: Curriculum Learning**
  - Why needed here: Pivot selection is motivated by curriculum learning—prioritizing "easier" dimensions first.
  - Quick check question: What signals might indicate a dimension has become "saturated" and should no longer be selected as pivot?

## Architecture Onboarding

- **Component map:**
  ```
  Input Query → Rollout Generator (with Pthink prompt)
                     ↓
              [G-1 on-policy + 1 off-policy samples]
                     ↓
              LLM-as-Judge (GPT-4o) → Reward Matrix R[G×D]
                     ↓
              History Buffer H[K×D] → Linear Regression → Residuals
                     ↓
              Softmax → Pivot Dimension d* + Weights w
                     ↓
              LargestSubset(R, d*) → Valid sample mask M
                     ↓
              Weighted advantage calculation (zeros for ¬M)
                     ↓
              GRPO-style policy update
  ```

- **Critical path:** Reward quality from LLM-as-Judge → pivot selection stability → conflict elimination effectiveness. If judge scores are inconsistent, downstream optimization degrades.

- **Design tradeoffs:**
  - Higher temperature β → more aggressive pivot switching vs. stability
  - Larger history K → smoother trends vs. slower adaptation
  - More off-policy samples → better diversity vs. distribution mismatch

- **Failure signatures:**
  - Rewards plateau early: Check if pivot dimension stuck; inspect residual variance
  - Gradient instability: Check if conflict elimination is too aggressive; examine M size
  - Reward hacking: Check if off-policy guidance is missing; verify thought prompt is being used

- **First 3 experiments:**
  1. **Ablate pivot selection:** Replace residual-softmax with uniform weights; expect slower convergence and lower final scores (Table 1 shows GRPO underperforms MOA by 0.41 on PersonaGym avg).
  2. **Vary off-policy ratio:** Test 0%, 6.25% (current), 12.5% off-policy samples; monitor diversity metrics and reward hacking rates.
  3. **Stress test conflict elimination:** Visualize M size over training; if M consistently small (<50% of G), partial order may be too strict—consider relaxing constraints.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the reliance on strong external models for reward scoring be replaced by a model self-scoring mechanism without compromising optimization stability?
- **Basis in paper:** [explicit] The authors state in the Limitations that exploring a model self-scoring approach "to reduce reliance on a strong external model" has "not yet been explored."
- **Why unresolved:** Current MOA relies on "LLMs-as-Judges" (GPT-4o), which is computationally expensive and introduces dependencies on external proprietary models.
- **Evidence:** An ablation study comparing the convergence and final performance of MOA when using self-supervised rewards versus the current external judge setup on benchmarks like PersonaGym.

### Open Question 2
- **Question:** Does the Multi-Objective Alignment strategy generalize to domains with conflicting objectives other than role-playing, such as mathematics or coding?
- **Basis in paper:** [explicit] The authors note in the Limitations that while validated on role-playing, "its effectiveness has not been tested in broader domains such as mathematics or coding."
- **Why unresolved:** The method is tailored for the specific conflict between persona style and knowledge; it is unproven whether the pivot dimension selection benefits tasks with purely logical or verifiable objectives.
- **Evidence:** Applying MOA to reasoning benchmarks (e.g., GSM8K or HumanEval) using multi-dimensional rubrics (e.g., correctness vs. efficiency) and comparing against standard GRPO baselines.

### Open Question 3
- **Question:** Can MOA be effectively adapted to use pure rule-based rewards rather than LLM-based judges to reduce computational overhead?
- **Basis in paper:** [explicit] The authors identify the "additional computational overhead" of LLM judges compared to "rule-based reward systems" as a specific limitation of the current approach.
- **Why unresolved:** Rule-based rewards typically lack the nuance required for "fine-grained rubrics" (e.g., style compliance), and it is unclear if the multi-objective optimization would function with sparse, keyword-based signals.
- **Evidence:** An experiment implementing the MOA framework using keyword-matching or verifiable rewards (similar to the RAIDEN-R1 baseline mentioned) to measure the trade-off between resource usage and fine-grained alignment quality.

## Limitations
- **Parameter sensitivity:** Critical hyperparameters like temperature β for pivot selection and history buffer size K are underspecified, making exact reproduction challenging.
- **Algorithmic assumptions:** The pivot selection relies on unproven assumptions about reward trends correlating with gradient magnitude; the conflict elimination mechanism lacks external validation.
- **Computational overhead:** The method depends on LLM-as-Judge (GPT-4o) for reward scoring, introducing significant computational cost and dependency on external models.

## Confidence
- **High confidence:** Core experimental results on PersonaGym and RoleMRC benchmarks; the reported performance improvements over baselines are well-documented and reproducible given the described setup.
- **Medium confidence:** The theoretical justification for pivot selection (Theorem 2) and conflict elimination mechanisms; while mathematically sound under stated assumptions, these assumptions are not empirically validated.
- **Low confidence:** Generalization of MOA to other multi-objective RL domains; the paper focuses exclusively on role-playing agents, and the specific reward structures and conflict patterns may not apply broadly.

## Next Checks
1. **Hyperparameter sensitivity analysis:** Systematically vary β (0.1 to 10) and K (50 to 200) to determine their impact on convergence speed and final performance. Document the optimal ranges and assess robustness.

2. **Conflict elimination ablation:** Compare MOA with a variant that retains all samples but applies dimension-specific weights instead of eliminating conflicting ones. This isolates whether elimination is necessary or if proper weighting suffices.

3. **Cross-domain transfer test:** Apply MOA to a different multi-objective RL task (e.g., instruction following + safety alignment) to evaluate whether the pivot selection and conflict elimination mechanisms generalize beyond role-playing scenarios.