---
ver: rpa2
title: Learning Domain-Invariant Representations for Cross-Domain Image Registration
  via Scene-Appearance Disentanglement
arxiv_id: '2601.08875'
source_url: https://arxiv.org/abs/2601.08875
tags:
- registration
- image
- scene
- domain
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the fundamental challenge of image registration
  under domain shift, where systematic intensity differences between source and target
  images violate the brightness constancy assumption of conventional registration
  methods. The authors propose SAR-Net, a unified framework based on scene-appearance
  disentanglement that decomposes observed images into domain-invariant scene representations
  and domain-specific appearance codes, enabling registration via re-rendering rather
  than direct intensity matching.
---

# Learning Domain-Invariant Representations for Cross-Domain Image Registration via Scene-Appearance Disentanglement

## Quick Facts
- arXiv ID: 2601.08875
- Source URL: https://arxiv.org/abs/2601.08875
- Authors: Jiahao Qin; Yiwen Wang
- Reference count: 33
- Key result: Achieves state-of-the-art 0.25% median rTRE on ANHIR benchmark, outperforming previous best by 7.4%

## Executive Summary
This paper addresses the fundamental challenge of image registration under domain shift, where systematic intensity differences between source and target images violate the brightness constancy assumption of conventional registration methods. The authors propose SAR-Net, a unified framework based on scene-appearance disentanglement that decomposes observed images into domain-invariant scene representations and domain-specific appearance codes, enabling registration via re-rendering rather than direct intensity matching. The method achieves state-of-the-art performance on the ANHIR benchmark for multi-stain histopathology registration, with a median relative Target Registration Error (rTRE) of 0.25%, outperforming the previous best method (MEVIS at 0.27% rTRE) by 7.4%, while maintaining high robustness (99.1%). The theoretical framework establishes conditions for consistent cross-domain alignment and proves that the scene consistency loss provides sufficient conditions for geometric correspondence in the shared latent space.

## Method Summary
SAR-Net is a unified framework based on scene-appearance disentanglement that factorizes images into domain-invariant scene representations and domain-specific appearance codes. The method uses a U-Net Scene Encoder with instance normalization to extract geometry-invariant features, paired with a lightweight Appearance Encoder that captures staining characteristics. A forward model re-renders images by modulating scene features with appearance codes. The training objective combines scene consistency loss to align geometry across domains, cycle consistency to preserve appearance information, and alignment loss to ensure registration accuracy. The framework is trained end-to-end on multi-stain histopathology image pairs from the ANHIR benchmark, achieving state-of-the-art performance while providing theoretical guarantees for cross-domain alignment.

## Key Results
- Achieves 0.25% median rTRE on ANHIR benchmark, outperforming previous best (MEVIS at 0.27% rTRE) by 7.4%
- Maintains 99.1% robustness score on challenging multi-stain histopathology registration
- Ablation studies show scene consistency loss is critical (rTRE increases from 0.25% to 0.38% when removed)
- Cross-domain reconstruction quality comparable to or better than state-of-the-art methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing images into scene and appearance codes enables registration across domain shifts by eliminating brightness constancy violations.
- Mechanism: The framework factorizes I = F(S, A), where S captures geometry and A captures intensity characteristics. Registration then occurs in the shared scene space where S_A ≈ S_B, bypassing intensity mismatches.
- Core assumption: The imaging operator F is injective in its first argument (scene), meaning distinct scenes produce distinct images under the same appearance.
- Evidence anchors:
  - [abstract] "observed images can be decomposed into domain-invariant scene representations and domain-specific appearance codes, enabling registration via re-rendering rather than direct intensity matching"
  - [section 3.1] "We decompose the problem into two complementary tasks: (1) the inverse problem of inferring (S, A) = F^-1(I), and (2) the forward problem of re-synthesizing"
  - [corpus] Related work on disentanglement (MUNIT, DRIT cited) supports content-style separation but lacks geometric guarantees—this paper claims to address that gap.
- Break condition: If scene and appearance are not truly separable in the data (e.g., staining alters perceived structure), the decomposition becomes ambiguous and alignment fails.

### Mechanism 2
- Claim: Instance normalization in the scene encoder strips domain-specific statistics, producing domain-invariant representations.
- Mechanism: Instance normalization removes channel-wise mean and variance, which encode appearance-related information, leaving structural features that persist across stains.
- Core assumption: Domain-specific information is primarily encoded in channel-wise statistics rather than spatial patterns.
- Evidence anchors:
  - [section 3.2] "Instance normalization removes channel-wise statistics that encode domain-specific characteristics"
  - [table 2] Ablation shows removing the scene consistency loss (which depends on this invariance) degrades rTRE from 0.25% to 0.38%
  - [corpus] Weak direct evidence; normalization for domain invariance is common but not rigorously validated in neighboring papers.
- Break condition: If structural information is also encoded in channel statistics (e.g., stain reveals specific structures), normalization may remove discriminative geometry.

### Mechanism 3
- Claim: The scene consistency loss provides a sufficient condition for geometric correspondence in the latent space.
- Mechanism: L_scene = ||S_A - S_B||^2 directly penalizes differences between scene representations from paired images, forcing the encoder to extract aligned geometry regardless of appearance.
- Core assumption: Proposition 2 holds—if scene representations converge, then any Lipschitz task (including registration) on those representations also converges.
- Evidence anchors:
  - [section 3.3] "Proposition 2 (Sufficiency of Scene Consistency): If L_scene = ||S_A - S_B||^2 → 0, then for any L-Lipschitz task T: ||T(S_A) - T(S_B)|| ≤ L · ||S_A - S_B|| → 0"
  - [table 2] Ablation confirms necessity: without L_scene, rTRE increases 52% (0.25% → 0.38%)
  - [corpus] No direct corroboration; theoretical sufficiency is claimed but not independently verified.
- Break condition: If the scene encoder fails to capture all geometric information, or if paired images have fundamentally different structures (e.g., tissue tearing), the loss enforces correspondence on incomplete or incorrect features.

## Foundational Learning

- Concept: **Brightness Constancy Assumption**
  - Why needed here: Understanding why traditional registration fails under domain shift requires recognizing that methods like optical flow and demons assume corresponding points have similar intensities.
  - Quick check question: Can you explain why mutual information-based registration partially addresses but does not fully solve the domain shift problem?

- Concept: **Disentangled Representation Learning**
  - Why needed here: The core innovation builds on factorizing observations into independent factors; familiarity with content-style separation (e.g., VAEs, MUNIT) provides context.
  - Quick check question: What constraints ensure disentangled representations remain interpretable and useful for downstream tasks?

- Concept: **Instance vs. Batch Normalization**
  - Why needed here: The scene encoder uses instance normalization specifically to remove domain statistics; understanding why batch normalization would fail is critical.
  - Quick check question: Why does instance normalization remove style information while preserving spatial structure, whereas batch normalization would not?

## Architecture Onboarding

- Component map:
  - Input pair (I_A, I_B) -> Scene Encoder E_S (U-Net with instance norm) -> Appearance Encoder E_A (CNN with GAP) -> Forward Model G (AdaIN modulation) -> Re-rendered outputs
  - Scene Encoder outputs S ∈ R^(64×H×W); Appearance Encoder outputs A ∈ R^32

- Critical path: Input pair (I_A, I_B) -> E_S produces (S_A, S_B) -> L_scene enforces S_A ≈ S_B -> G(S_B, A_A) re-renders -> L_align matches to I_A

- Design tradeoffs:
  - Instance normalization preserves structure but may discard stain-revealed features; validate on your specific stain types.
  - Compact appearance code (32-dim) may underrepresent complex staining; consider increasing if domain shift is extreme.
  - Patch-based processing limits global context; gigapixel WSI requires tiling strategy.

- Failure signatures:
  - High L_scene but poor visual alignment: Scene encoder may capture non-geometric features; check S_A, S_B visualizations.
  - Color artifacts in re-rendered output: Appearance encoder insufficient; increase capacity or check pooling.
  - bUnwarpJ-style error increase: Domain shift too severe; verify brightness constancy violations exist in your data.

- First 3 experiments:
  1. **Sanity check**: Train on ANHIR subset (e.g., 50 pairs) and verify S_A ≈ S_B visually; expect low L_scene within 10 epochs.
  2. **Ablation validation**: Remove each loss component sequentially; confirm rTRE degrades as reported (L_align removal most critical: 0.25% → 1.85%).
  3. **Domain shift stress test**: Apply synthetic intensity transforms (e.g., gamma correction, histogram matching) to held-out pairs; measure rTRE degradation curve.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical sufficiency claim (Proposition 2) remains unverified beyond empirical performance
- Assumes complete separability of scene and appearance factors, which may not hold in practice
- Evaluation limited to ANHIR benchmark; claims of cross-domain generalization not substantiated

## Confidence
- **High confidence**: Empirical performance claim on ANHIR (rTRE 0.25% median, 99.1% robustness) well-supported by published results and ablation studies
- **Medium confidence**: Theoretical framework and Proposition 2 are logically constructed but lack independent verification beyond presented experiments
- **Medium confidence**: Mechanism claims (particularly instance normalization removing domain statistics) are plausible given ablation results but would benefit from additional diagnostic analyses

## Next Checks
1. **Latent Space Correspondence Analysis**: Visualize and quantitatively measure the geometric alignment between S_A and S_B for challenging pairs (e.g., extreme stain differences, large deformations) to verify that scene consistency loss truly enforces geometric correspondence.

2. **Cross-Domain Generalization Test**: Evaluate the trained model on completely unseen staining protocols or tissue types not present in ANHIR to assess whether the domain-invariant representations generalize beyond the training distribution.

3. **Failure Mode Characterization**: Systematically induce brightness constancy violations (e.g., synthetic stain variations, contrast changes) on the test set and measure the relationship between violation severity and registration performance to identify operational limits of the approach.