---
ver: rpa2
title: Detecting value-expressive text posts in Russian social media
arxiv_id: '2312.08968'
source_url: https://arxiv.org/abs/2312.08968
tags:
- posts
- values
- value
- social
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study focused on detecting value-expressive posts in Russian
  social media VKontakte, addressing the challenge of distinguishing genuine value
  expressions from stereotyped speech. The approach involved annotating 5,035 posts
  using expert, crowd-worker, and ChatGPT inputs, then training classification models
  with embeddings from transformer-based language models.
---

# Detecting value-expressive text posts in Russian social media

## Quick Facts
- arXiv ID: 2312.08968
- Source URL: https://arxiv.org/abs/2312.08968
- Authors: Maria Milkova; Maksim Rudnev; Lidia Okolskaya
- Reference count: 22
- Primary result: Fine-tuned rubert-tiny2 model achieves F1=0.77, F1-macro=0.83 for detecting value-expressive posts in Russian VKontakte social media

## Executive Summary
This study addresses the challenge of detecting value-expressive posts in Russian social media VKontakte, where censorship requires identifying implicit value expressions. The researchers annotated 5,035 posts using an ensemble approach combining expert, crowd-worker, and ChatGPT annotations to overcome individual method biases. They trained classification models using embeddings from transformer-based language models, finding that fine-tuning a smaller domain-specific model (rubert-tiny2) outperformed using embeddings from larger pre-trained models. The resulting model provides a crucial tool for studying values within and between Russian social media users in politically sensitive contexts.

## Method Summary
The researchers developed a binary classification system to detect value-expressive posts in Russian VKontakte social media. They created an ensemble annotation pipeline combining human crowd-workers (304 from Yandex Toloka), ChatGPT (gpt-3.5-turbo), and expert input, with a rule-based system to combine labels. Posts were filtered for Cyrillic-only text with minimum length, and spam was removed via weakly supervised topic modeling. They fine-tuned the rubert-tiny2 transformer model for 5 epochs, extracted 312-dimensional embeddings, and trained an SVM classifier with 5-fold cross-validation and grid search. The ensemble approach compensated for crowd-worker false negatives on political/parental posts and ChatGPT's tendency to over-label spam as value-expressive.

## Key Results
- Best model using fine-tuned rubert-tiny2 embeddings achieved F1 score of 0.77 for detecting value-expressive posts
- F1-macro score of 0.83 demonstrated balanced performance across both classes
- Smaller, domain-pretrained model outperformed larger pre-trained models, suggesting domain adaptation matters more than model size
- Active learning strategy reduced annotation burden by selecting high-uncertainty posts for human review

## Why This Works (Mechanism)

### Mechanism 1
Combining human crowd-worker annotations with ChatGPT annotations reduces systematic biases inherent to each method, yielding higher-quality labeled data. Crowd-workers exhibit high false-negative rates while ChatGPT exhibits high false-positive rates. By using a rule-based ensemble—primarily relying on ChatGPT but overriding with crowd consensus for "Spam" or unanimous "Reflects" classifications—the complementary error patterns compensate for each other. The non-overlapping errors of crowd-workers and ChatGPT allowed us to combine the annotations which compensated for each coder's errors.

### Mechanism 2
Fine-tuning a smaller, domain-relevant transformer model (rubert-tiny2) outperforms using embeddings from larger pre-trained models for this specific Russian social media classification task. Fine-tuning allows the model to adapt its internal representations to the nuances of "value-expressive" language in Russian social media, capturing domain-specific semantic patterns that fixed pre-trained embeddings may miss. Overall, the best results were shown by models based on embeddings from fine-tuned rubert-tiny2, which were previously pre-trained, including on data from social media. Thus, in our experiments the quality of a model is determined not by its size, but rather by the data on which the models were pretrained.

### Mechanism 3
An Active Learning strategy focusing annotation efforts on high-uncertainty posts creates a more efficient training dataset, improving model performance per annotation effort. By iteratively training a preliminary classifier and selecting for annotation only those posts where the model's predicted class probabilities fall within a mid-range (0.3-0.7), the process prioritizes "informative" examples that help the model refine its decision boundary. Active Learning (AL) strategy selects posts based on their usefulness for the model, helping to reduce the amount of annotation required.

## Foundational Learning

**Concept: Transformer Embeddings**
- Why needed here: The entire architecture converts raw Russian text into dense vector representations that capture semantic meaning, which are then used as features for a classifier
- Quick check question: How does a pre-trained model like rubert-tiny2 convert a social media post into a numerical vector?

**Concept: Fine-Tuning vs. Feature Extraction**
- Why needed here: The best result comes from *fine-tuning* the rubert-tiny2 model, which updates the model's weights, not just using it as a static feature extractor
- Quick check question: What is the difference between extracting fixed embeddings from a pre-trained model and updating the model's weights during training on a downstream task?

**Concept: F1 Score & Class Imbalance**
- Why needed here: The paper reports F1, F1-macro, and Precision/Recall. Understanding why F1 is preferred over simple accuracy for imbalanced datasets is essential
- Quick check question: If a model classifies all posts as "non-value-expressive," it might have high accuracy on this dataset. Why would its F1 score be low?

## Architecture Onboarding

**Component map:** VKontakte API -> Preprocessing (spam filter) -> Active Learning selection loop -> Annotation (Crowd-workers + ChatGPT API) -> Ensemble Annotation Engine (Human + ChatGPT labels -> Rule-based Combiner) -> Annotated Dataset -> Transformer Model (rubert-tiny2) for Fine-tuning -> Classical Classifier (SVM) -> 5-Fold Cross-Validation

**Critical path:** The quality of the **Ensemble Annotation Engine** is the primary driver of final model performance. The effort spent designing the combination rules (e.g., crowd consensus overriding ChatGPT for "Spam") is more critical than the choice of the final classifier (SVM vs. LogitBoost).

**Design tradeoffs:**
- Accuracy vs. Cost: ChatGPT is cheaper and more consistent than human experts but has specific failure modes. The ensemble approach balances these
- Model Size vs. Performance: The paper notes a smaller, fine-tuned model (29M params) outperformed larger pre-trained models (427M params). Tradeoff favors domain-relevant pre-training data and fine-tuning over raw scale

**Failure signatures:**
- Low annotator agreement: Crowd-worker Krippendorff's Alpha < 0.4 signals unclear guidelines
- ChatGPT over-labeling: >45% "Reflects" class with a corresponding drop in precision indicates inadequate correction of ChatGPT false-positive bias
- Stagnant Active Learning: Model uncertainty not decreasing over iterations suggests selected examples are noise

**First 3 experiments:**
1. Reproduce Annotation Consistency: Re-annotate a small sample (e.g., 100 posts) with both crowd-workers and ChatGPT to verify reported Alpha scores and systematic biases
2. Ablate Annotation Strategy: Train three models (crowd-only labels, ChatGPT-only labels, ensemble labels) and compare F1 scores to quantify the benefit of the ensemble
3. Ablate Model Components: Compare using embeddings from pre-trained rubert-tiny2 (feature extraction) vs. fine-tuned rubert-tiny2 to validate the performance lift from fine-tuning

## Open Questions the Paper Calls Out

**Open Question 1**
To what extent do the personal value profiles or demographic characteristics of human annotators bias the detection of value-expressive texts? The authors observed that crowd-workers systematically missed values in political or parental posts and explicitly state, "it raises the question of how the annotation results are correlated with characteristics of the coder." This is unresolved because the study utilized 304 diverse crowd-workers but did not collect psychometric data (e.g., PVQ scores) to correlate annotators' personal values with their labeling decisions. A study measuring the value orientations of annotators and correlating these scores with their specific classification errors would resolve this.

**Open Question 2**
Can the classification approach developed for VKontakte be generalized to other Russian social media platforms with different user bases? The authors note, "It is not clear if the suggested approach and model would work in other social media," citing differences in audience education, integration, and political views on platforms like Telegram. This is unresolved because the model was trained and validated exclusively on VKontakte data, which has specific linguistic norms and demographic skew. Applying the fine-tuned rubert-tiny2 model to a held-out dataset from a different platform (e.g., Telegram) and measuring performance against human annotations would resolve this.

**Open Question 3**
Can this binary detection model be extended to classify specific Schwartz value categories (e.g., Universalism vs. Power)? The paper defines the current work as the "first step" and states the goal is "building a classification model that will distinguish between specific values." This is unresolved because the current methodology only trains the model to distinguish between "value-expressive" and "not value-expressive" (binary), rather than categorizing the specific value expressed. Re-annotating the dataset to include specific Schwartz value labels and training a multi-class classifier to test its accuracy would resolve this.

## Limitations
- Annotation pipeline complexity with moderate inter-annotator agreement (α=0.44-0.54) suggests task ambiguity and potential reproducibility challenges
- Critical hyperparameters for fine-tuning (learning rate, batch size, optimizer) and SVM training (kernel, C, gamma) are underspecified
- ChatGPT exhibits systematic false-positive bias, misclassifying 76% of spam as value-expressive, requiring manual correction

## Confidence

**High Confidence:** The core finding that fine-tuning a smaller, domain-adapted model (rubert-tiny2) outperforms using embeddings from larger pre-trained models for this specific Russian social media task. This is supported by clear experimental comparisons in the paper and aligns with broader literature on domain adaptation.

**Medium Confidence:** The efficacy of the ensemble annotation strategy combining human crowd-workers with ChatGPT. While the paper presents a logical framework for compensating systematic biases, the specific implementation details are sparse, and the assumption that error patterns are non-overlapping may not hold in all contexts.

**Low Confidence:** The exact performance metrics and their reproducibility. Without specified hyperparameters for fine-tuning and SVM training, reported F1 scores (0.77 for value-expressive, 0.83 F1-macro) may vary significantly in replication attempts.

## Next Checks

1. **Annotation Reproducibility Check:** Re-annotate 100 randomly selected posts using the same guidelines with both crowd-workers and ChatGPT to verify inter-annotator agreement scores (α=0.44-0.54) and systematic bias patterns (ChatGPT over-labeling, crowd-worker false negatives).

2. **Annotation Strategy Ablation:** Train three separate classifiers using only crowd-worker labels, only ChatGPT labels, and the ensemble method to quantify the performance improvement from the combination strategy, directly testing whether the ensemble approach compensates for individual method limitations.

3. **Model Component Ablation:** Compare model performance using embeddings from pre-trained rubert-tiny2 (feature extraction only) versus fine-tuned rubert-tiny2 to empirically validate the claim that fine-tuning, not model size, drives performance gains.