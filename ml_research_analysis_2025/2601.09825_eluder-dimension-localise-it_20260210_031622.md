---
ver: rpa2
title: 'Eluder dimension: localise it!'
arxiv_id: '2601.09825'
source_url: https://arxiv.org/abs/2601.09825
tags:
- bound
- loss
- function
- proposition
- assumption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of obtaining first-order regret\
  \ bounds in generalised linear models (GLMs) using eluder dimension-based analysis.\
  \ Standard approaches based on the global eluder dimension cannot yield first-order\
  \ regret bounds because they introduce worst-case information gain parameters (\u03BA\
  ) that cancel out the small-cost term \u03B7(a), destroying the first-order gains."
---

# Eluder dimension: localise it!

## Quick Facts
- arXiv ID: 2601.09825
- Source URL: https://arxiv.org/abs/2601.09825
- Reference count: 40
- First-order regret bounds for GLMs via localised eluder dimension

## Executive Summary
This paper addresses the challenge of obtaining first-order regret bounds in generalised linear models (GLMs) using eluder dimension-based analysis. Standard approaches based on the global eluder dimension cannot yield first-order regret bounds because they introduce worst-case information gain parameters (κ) that cancel out the small-cost term η(a*), destroying the first-order gains. The paper introduces a novel localisation method for the eluder dimension, which restricts the analysis to a small neighbourhood of the optimal model parameter. This approach avoids the problematic κ dependency while maintaining the benefits of first-order adaptivity. The authors prove that the global ℓ1- and ℓ2-eluder dimensions must scale with κ in the GLM setting, making localisation necessary. They propose the ℓ-UCB algorithm, which uses empirical risk minimisation-based confidence intervals with a carefully chosen loss function and confidence widths. For stochastic bandits, ℓ-UCB achieves small-cost bounds under bounded loss, a Bernstein variance condition, and a triangle condition. The method extends to reinforcement learning via the ℓ-GOLF algorithm, providing the first genuine first-order regret bounds for finite-horizon RL with bounded rewards/costs. The key innovation is moving the κ dependence from the leading term to an additive lower-order term through localisation analysis, enabling truly instance-adaptive regret bounds.

## Method Summary
The paper introduces a localised eluder dimension approach to achieve first-order regret bounds in GLMs. The ℓ-UCB algorithm uses empirical risk minimisation (ERM) with a specified loss function (log-loss or Poisson loss) to construct confidence sets. The algorithm optimistically selects actions by finding the minimum expected loss over the confidence set. A key innovation is the localisation of the eluder dimension analysis to a neighbourhood of the optimal model parameter, which avoids the κ-dependency that prevents standard global analysis from achieving first-order bounds. For RL, the ℓ-GOLF algorithm extends this approach to finite-horizon episodic settings. The analysis relies on Bernstein-type concentration inequalities under specific conditions: bounded loss, a variance condition, and a triangle condition that enables the small-cost decomposition.

## Key Results
- Proves global ℓ1- and ℓ2-eluder dimensions for GLMs must scale with κ, preventing first-order bounds
- Introduces localised eluder dimension that removes κ-dependency from leading regret term
- Proposes ℓ-UCB algorithm achieving small-cost regret bounds for stochastic bandits under Bernstein conditions
- Extends to ℓ-GOLF algorithm providing first genuine first-order regret bounds for finite-horizon RL
- Shows localisation moves κ dependence from leading term to additive lower-order term

## Why This Works (Mechanism)

### Mechanism 1
Restricting eluder dimension analysis to a small neighbourhood of the optimal model removes κ-dependency from the leading regret term. The paper defines a localised ℓ₁-eluder dimension over an r-excess-loss neighbourhood F′(r) = {θ : |⟨a, θ - θ*⟩| ≤ r, ∀a}. When r = 1/M (where M is the self-concordance parameter), Proposition 4 bounds the eluder dimension as O(d·log(1 + S²L/ε)), eliminating the exp(rM) factor that would otherwise introduce κ scaling. Core assumption: The link function µ is M-self-concordant (|µ̈(u)| ≤ M·μ̇(u)). Evidence anchors: [abstract] "introduce a localisation method for the eluder dimension; our analysis immediately recovers and improves on classic results"; [Page 5, Proposition 4] "dimelud(ε; Φ̄(F′(r))) ≤ Cd·exp(rM)·log(1 + S²L·exp(rM)/ε)" with r = 1/M removing exp(rM) term. Break condition: If the model class lacks self-concordance or the neighbourhood radius r is too large (approaching S), the exp(rM) term reintroduces κ-dependency.

### Mechanism 2
Global eluder dimension for GLMs must scale with κ, making standard analysis incapable of first-order bounds. Theorem 2 constructs a hard instance using Johnson-Lindenstrauss packing, showing that when |θ*| ≈ S (low information regime), the eluder dimension scales as exp(S) ≈ κ, which cancels η(a*) ≈ exp(-S) benefits in regret. Core assumption: The parameter set Θ ⊂ SB₂ᵈ with S ≥ 4/M, and the domain U contains [-S, 0]. Evidence anchors: [abstract] "We establish a lower bound on the eluder dimension of generalised linear model classes, showing that standard eluder dimension-based analysis cannot lead to first-order regret bounds"; [Page 5, Corollary 3] For logistic bandits with S ≥ 4, "the ε-eluder dimension of Φ̄(GLM(µ,Θ)) exceeds (d-1)/(4b)·exp{b/4300}" where b = min{⌊S⌋, d-1}. Break condition: If S is small (S < 4/M), the lower bound construction fails; if the domain U doesn't support the necessary parameter configurations, the bound weakens.

### Mechanism 3
Loss functions satisfying boundedness, variance, and triangle conditions enable Bernstein-type concentration and regret decomposition yielding first-order bounds. Assumption 4 requires (1) |φf(Y,a)| ≤ b almost surely, (2) Var[φf(Y,a)] ≤ c·φ̄f(a), and (3) ∆(f(a), η(a)) ≤ γ·φ̄f(a). These allow Theorem 10's uniform Bernstein inequality and Lemma 23's per-step regret decomposition, converting concentration gains into small-cost bounds. Core assumption: The loss ℓ is compatible with the link µ (∂uℓ(y, µ(u)) = µ(u) - y). Evidence anchors: [Page 3, Assumption 4] Specifies the three conditions with explicit constants; [Page 6, Propositions 16-17] Log-loss satisfies variance condition with c = b + 4; Poisson with c = b + 2. Break condition: Squared loss fails the triangle condition (cited: Foster and Krishnamurthy, 2021), breaking the small-cost decomposition.

## Foundational Learning

- Concept: Eluder dimension as a complexity measure
  - Why needed here: This is the central object the paper modifies; standard eluder dimension measures how many "surprising" actions exist at scale ε, but global analysis introduces κ-dependency.
  - Quick check question: Given a function class Ψ, can you construct an ε-eluder sequence and explain why localising to a neighbourhood changes the dimension bound?

- Concept: First-order regret bounds (small-cost bounds)
  - Why needed here: The paper's goal is achieving Rn ≤ √(n·η(a*))·Γn + Γ′n where η(a*) is the optimal cost; understanding why η(a*) appears is essential.
  - Quick check question: Why does a bound scaling with √η(a*) adapt to "easy" instances better than worst-case √n bounds?

- Concept: Self-concordance in GLMs
  - Why needed here: Proposition 4's bound and Proposition 5's rogue-step count both rely on M-self-concordance |µ̈(u)| ≤ M·μ̇(u).
  - Quick check question: For the logistic link µ(u) = 1/(1+e⁻ᵘ), verify that M = 1 and explain how this affects the 1/M-localisation radius choice.

## Architecture Onboarding

- Component map:
  Input: loss function ℓ, model class F, confidence widths βt
  ├── Confidence set construction (Ft) via empirical risk minimisation
  │   └── Constraint: Σℓ(Yi, f(Ai)) ≤ inf + βt
  ├── Optimistic optimisation over Ft × A
  │   └── Find (ft, At) minimizing ft(At)
  └── Localised analysis (analysis-only, not algorithmic)
      ├── Define F′(r) for desired r
      └── Bound: card{t : ft ∉ F′}

- Critical path:
  1. Choose loss function (log-loss for Bernoulli outcomes, Poisson for count data)
  2. Set confidence widths: βt = 5/2 + 15(b+c)·log(Nn·ht/δ) where Nn is 1/n-covering number
  3. Construct localised class F′(1/M) for analysis
  4. Track rogue steps: count instances where |⟨At, θt - θ*⟩| > 1/M

- Design tradeoffs:
  - Smaller r in F′(r) → lower leading-term complexity but higher rogue-step count
  - Log-loss vs. Poisson: log-loss gives γ = 2/log₂(e), Poisson gives γ = 4√e/log₂(e)
  - Algorithmic warm-up (Faury et al., 2022) vs. analysis-only localisation: the latter enables RL extension

- Failure signatures:
  - Regret scales with κ in leading term → localisation radius too large or analysis not properly localised
  - Confidence sets empty → βt too small or model misspecified
  - Triangle condition violated → using squared loss instead of compatible loss

- First 3 experiments:
  1. Logistic bandit with S = 10, d = 5, n = 10000: measure regret and verify it scales with η(a*) rather than κ in leading term
  2. Ablation on localisation radius: compare r = 1/M vs. r = S to observe κ-dependency emergence
  3. Poisson bandit with bounded costs: verify triangle condition constants (c = b + 2, γ = 4√e/log₂(e)) through empirical excess loss analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a two-sided triangle condition be formulated to achieve variance-dependent regret bounds (scaling with v(a*) = η(a*)(1-η(a*))) while still accommodating models like Poisson GLM?
- Basis in paper: [explicit] Section 4.3 states the suboptimality is "that it depends on η(a*), providing only a small-cost bound, rather than on v(a*)" and that "strengthening the assumption to be two-sided... would allow us to recover the v(a*)" but notes this "would rule out, for example, the Poisson GLM."
- Why unresolved: The one-sided triangle condition enables small-cost bounds but not variance adaptivity; a two-sided condition achieves variance adaptivity but excludes important model classes.
- What evidence would resolve it: A unified triangle condition that holds for both logistic and Poisson GLMs while enabling v(a*)-dependent regret.

### Open Question 2
- Question: How close are the agnostic-setting regret bounds (where η ∈ F but data is not GLM-generated) to instance-optimal?
- Basis in paper: [explicit] Section 4.3: "Importantly, we do not assume that the rewards are generated by a generalised linear model. However, much of the literature does make that assumption, so we make comparisons in that setting."
- Why unresolved: The paper provides agnostic bounds but lower bound comparisons only exist for the well-specified MLE setting, leaving the agnostic optimality gap unknown.
- What evidence would resolve it: Lower bounds for the bounded-cost agnostic setting or proofs of agnostic optimality.

### Open Question 3
- Question: Can the localised eluder dimension technique extend to settings with unbounded or heavy-tailed cost distributions?
- Basis in paper: [inferred] Assumption 2 requires costs supported on [0,1], and Theorem 10's Bernstein concentration relies on boundedness; no extension beyond bounded costs is discussed.
- Why unresolved: Boundedness is fundamental to the concentration analysis and variance condition; relaxation requires new technical machinery.
- What evidence would resolve it: First-order regret bounds under sub-Gaussian or heavy-tailed cost distributions with localisation.

### Open Question 4
- Question: Can the generalised completeness assumption (TF ⊂ G) for RL first-order bounds be relaxed to realizability-only?
- Basis in paper: [inferred] Theorem 7 requires Assumption 7 (generalised completeness). The paper notes Wang et al. (2023) required the "significantly stronger" distributional Bellman completeness, but whether further relaxation is possible remains open.
- Why unresolved: Completeness enables tractable analysis but may be stronger than necessary for first-order guarantees.
- What evidence would resolve it: First-order RL regret under realizability-only (q* ∈ F), or lower bounds proving completeness is necessary.

## Limitations
- Theoretical guarantees depend on problem-specific parameters S and M that may be unknown in practice
- Localised analysis approach only works for self-concordant link functions, restricting model class
- Confidence set construction requires computationally intensive covering number calculations for high dimensions
- Practical efficiency depends on quality of convex relaxation for optimistic optimization step

## Confidence

- High: The lower bound construction (Theorem 2) demonstrating that global eluder dimension must scale with κ for GLMs - the Johnson-Lindenstrauss packing argument is rigorous and the exponential scaling is clearly established.
- Medium: The localised eluder dimension bounds (Proposition 4) - while the M-self-concordance framework is well-established, the specific bound requires careful verification of the excess-loss neighbourhood construction.
- Medium: The Bernstein-type concentration results (Theorem 10) - the conditions are explicitly stated but verification of the triangle condition for different loss functions requires careful checking.

## Next Checks

1. **Robustness test**: Implement the algorithm for both logistic and Poisson bandits, varying S from small (S < 4/M) to large values to empirically verify when κ-dependency emerges in the regret scaling.

2. **Self-concordance verification**: For the logistic link µ(u) = 1/(1+e⁻ᵘ), analytically verify that M = 1 and that the 1/M-localisation radius properly bounds the rogue-step count in practice.

3. **Triangle condition assessment**: Implement both log-loss and Poisson loss variants, then empirically measure the excess loss ∆(f(a), η(a)) and φ̄f(a) to verify the γ·φ̄f(a) bound holds with the claimed constants.