---
ver: rpa2
title: 'SMOG: Scalable Meta-Learning for Multi-Objective Bayesian Optimization'
arxiv_id: '2601.22131'
source_url: https://arxiv.org/abs/2601.22131
tags:
- smog
- optimization
- tasks
- bayesian
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SMOG introduces a scalable meta-learning framework for multi-objective
  Bayesian optimization that combines meta-task data with a structured multi-output
  Gaussian process to model correlations between objectives. The method constructs
  a modular GP prior that learns task-specific weights and residual functions, enabling
  efficient linear scaling with the number of meta-tasks.
---

# SMOG: Scalable Meta-Learning for Multi-Objective Bayesian Optimization

## Quick Facts
- arXiv ID: 2601.22131
- Source URL: https://arxiv.org/abs/2601.22131
- Authors: Leonard Papenmeier; Petru Tighineanu
- Reference count: 40
- Primary result: SMOG achieves significant sample efficiency gains in early optimization iterations while maintaining robust performance across varying numbers of objectives and meta-tasks

## Executive Summary
SMOG introduces a scalable meta-learning framework for multi-objective Bayesian optimization that combines meta-task data with a structured multi-output Gaussian process to model correlations between objectives. The method constructs a modular GP prior that learns task-specific weights and residual functions, enabling efficient linear scaling with the number of meta-tasks. Experiments across synthetic (Hartmann6) and real-world (HPOBench, Terrain) benchmarks show SMOG consistently outperforms baselines including metadata-free methods and alternative meta-learning approaches.

## Method Summary
SMOG addresses the challenge of transferring knowledge across multiple optimization tasks by leveraging a structured multi-output Gaussian process that captures correlations between objectives. The framework builds a modular GP prior that combines meta-task data through learned task-specific weights and residual functions. This design enables linear scaling with the number of meta-tasks while maintaining the ability to model complex objective relationships. The method integrates meta-learning directly into the Bayesian optimization loop, using historical task information to accelerate optimization of new tasks.

## Key Results
- Achieves significant sample efficiency gains in early optimization iterations compared to metadata-free methods
- Maintains robust performance across varying numbers of objectives (tested up to 6 objectives)
- Consistently outperforms alternative meta-learning approaches on both synthetic (Hartmann6) and real-world benchmarks (HPOBench, Terrain)

## Why This Works (Mechanism)
SMOG leverages the shared structure across optimization tasks by building a common GP prior that captures objective correlations while allowing task-specific variations through weighted components. The modular design enables the model to learn transferable patterns from meta-tasks and apply them efficiently to new tasks, reducing the need for extensive exploration. The multi-output GP structure explicitly models the dependencies between objectives, which is crucial for multi-objective optimization where objectives often exhibit strong correlations.

## Foundational Learning
- **Gaussian Process Regression**: Probabilistic model for function approximation that provides uncertainty estimates - needed for Bayesian optimization's exploration-exploitation tradeoff; quick check: verify GP hyperparameters are properly tuned
- **Multi-output GPs**: Extension of GPs to model multiple correlated outputs simultaneously - needed to capture objective dependencies in multi-objective optimization; quick check: validate correlation structure learning on synthetic data
- **Meta-learning**: Learning to learn from multiple related tasks - needed to transfer knowledge from historical tasks to accelerate new task optimization; quick check: test transfer performance when meta-task similarity decreases
- **Bayesian Optimization**: Sequential optimization framework using probabilistic surrogate models - needed as the optimization backbone that SMOG enhances with meta-learning; quick check: compare convergence rates with and without meta-learning
- **Structured priors**: Prior distributions that encode problem-specific structure - needed to enable efficient scaling and incorporate domain knowledge; quick check: ablate structured prior components to measure impact

## Architecture Onboarding

Component Map: Meta-tasks -> Structured Multi-output GP Prior -> Task-specific Weights & Residuals -> Bayesian Optimization Loop -> New Task Optimization

Critical Path: Meta-task data collection → Structured GP prior construction → Weight and residual learning → Bayesian optimization iterations → New task optimization

Design Tradeoffs: The modular GP prior design enables linear scaling with meta-tasks but requires careful hyperparameter tuning. The structured approach assumes meta-task similarity, which may not hold for heterogeneous problems. The method balances computational efficiency with modeling flexibility by separating common structure from task-specific components.

Failure Signatures: Poor performance when meta-tasks are highly dissimilar, computational bottlenecks when scaling to many objectives, overfitting when meta-task data is limited, and degraded performance if correlation structure is misspecified.

Three First Experiments:
1. Validate meta-task similarity assumptions by testing performance degradation as task heterogeneity increases
2. Benchmark scaling behavior by measuring runtime and accuracy as the number of meta-tasks grows
3. Compare sample efficiency against metadata-free Bayesian optimization on problems with varying objective correlation structures

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on meta-task similarity assumptions that may not hold for highly heterogeneous optimization problems
- Computational challenges when scaling to problems with many objectives (beyond 6 tested)
- Requires careful hyperparameter tuning for task-specific weights and residual functions across diverse problem domains

## Confidence
- Scalability benefits and sample efficiency gains on tested benchmarks: **High**
- Generalizability to completely novel problem types: **Medium**
- Comparison with metadata-free methods: **High**
- State-of-the-art performance claims: **Medium**

## Next Checks
1. Test SMOG on meta-task distributions with high structural heterogeneity to evaluate robustness when similarity assumptions break down
2. Scale experiments to problems with more than 6 objectives to assess computational limits and performance degradation
3. Implement an ablation study isolating the contribution of the multi-output GP correlation modeling versus the meta-learning component to quantify each's individual impact on optimization performance