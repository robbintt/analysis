---
ver: rpa2
title: 'AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models'
arxiv_id: '2507.12414'
source_url: https://arxiv.org/abs/2507.12414
tags:
- annotations
- error
- dataset
- noise
- autovdc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'AutoVDC is a framework that leverages vision-language models (VLMs)
  to automate the detection and removal of erroneous annotations in vision datasets.
  It works in two stages: first, discrepancy scoring identifies potential errors by
  comparing task model predictions to annotations; second, a VLM validates whether
  these discrepancies are due to wrong annotations or incorrect predictions.'
---

# AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models

## Quick Facts
- arXiv ID: 2507.12414
- Source URL: https://arxiv.org/abs/2507.12414
- Reference count: 34
- AutoVDC leverages vision-language models to automate detection and removal of erroneous annotations in vision datasets, significantly improving model evaluation accuracy.

## Executive Summary
AutoVDC is a framework that uses vision-language models (VLMs) to automate the detection and removal of erroneous annotations in vision datasets. It operates in two stages: discrepancy scoring identifies potential errors by comparing task model predictions to annotations, and VLM validation determines whether discrepancies are due to wrong annotations or incorrect predictions. Evaluated on KITTI and nuImages datasets with injected noise, AutoVDC achieves high accuracy in detecting erroneous annotations, especially when VLMs are fine-tuned for the task. Results show significant improvements in model evaluation accuracy when using AutoVDC-cleaned datasets compared to noisy ones, demonstrating its effectiveness for scalable, cost-efficient data cleaning in autonomous driving applications.

## Method Summary
AutoVDC employs a two-stage approach to automate vision data cleaning. In the first stage, a task model (such as Faster R-CNN) is used to predict annotations for the dataset, and discrepancies between these predictions and the original annotations are scored. The second stage leverages a vision-language model (VLM) to validate these discrepancies by generating natural language explanations for why an annotation might be wrong. If the VLM's explanation aligns with the discrepancy, the annotation is flagged as erroneous. The framework is evaluated on KITTI and nuImages datasets with injected noise, and results show high accuracy in detecting erroneous annotations, especially when the VLM is fine-tuned for the task. The method significantly improves model evaluation accuracy when using AutoVDC-cleaned datasets compared to noisy ones.

## Key Results
- AutoVDC achieves high accuracy in detecting erroneous annotations, especially when VLMs are fine-tuned for the task.
- Significant improvements in model evaluation accuracy are observed when using AutoVDC-cleaned datasets compared to noisy ones.
- The framework demonstrates effectiveness for scalable, cost-efficient data cleaning in autonomous driving applications.

## Why This Works (Mechanism)
AutoVDC leverages the reasoning capabilities of vision-language models to distinguish between annotation errors and model prediction errors. By generating natural language explanations for discrepancies, VLMs can provide context-aware validation that goes beyond simple numerical comparison. This approach is particularly effective because VLMs can understand the semantic meaning behind annotations and predictions, allowing them to identify subtle errors that might be missed by traditional automated methods. The two-stage process ensures that only high-confidence errors are flagged, reducing false positives and maintaining dataset integrity.

## Foundational Learning
- **Vision-Language Models (VLMs)**: VLMs combine visual understanding with natural language processing, enabling them to generate contextual explanations for image content. Why needed: To provide semantic reasoning beyond numerical predictions. Quick check: Can the VLM generate coherent explanations for simple image-label pairs?
- **Discrepancy Scoring**: This involves comparing task model predictions with ground truth annotations to identify potential errors. Why needed: To systematically identify regions where annotations may be incorrect. Quick check: Does the scoring method produce higher scores for obvious annotation errors?
- **Fine-tuning VLMs**: Adapting pre-trained VLMs to specific domains or tasks by further training on relevant data. Why needed: To improve the model's ability to understand domain-specific annotation conventions and error patterns. Quick check: Does fine-tuning improve VLM performance on the target dataset's annotation style?
- **Task Model Dependency**: AutoVDC relies on a pre-trained task model for initial predictions. Why needed: To generate candidate annotations for comparison against ground truth. Quick check: Is the task model's performance stable across different subsets of the dataset?
- **Annotation Error Types**: Understanding common error patterns in vision datasets, such as missing annotations, incorrect class labels, or imprecise bounding boxes. Why needed: To inform the design of effective validation strategies. Quick check: Can the system distinguish between different types of annotation errors?
- **Evaluation Metrics for Data Cleaning**: Metrics such as precision, recall, and F1-score for measuring the effectiveness of error detection. Why needed: To quantitatively assess the performance of the cleaning framework. Quick check: Does the framework maintain high precision while achieving reasonable recall?

## Architecture Onboarding

Component Map: Task Model -> Discrepancy Scorer -> VLM Validator -> Error Flag Generator

Critical Path: The core workflow involves the task model generating predictions, the discrepancy scorer identifying potential errors, the VLM validating these discrepancies, and the error flag generator marking annotations for removal or correction.

Design Tradeoffs: The framework balances between computational cost (running VLMs) and accuracy (fine-tuning vs. zero-shot). Fine-tuning improves performance but increases setup time and data requirements. The two-stage approach reduces false positives but may miss subtle errors that don't produce large discrepancies.

Failure Signatures: Common failure modes include VLMs misclassifying correct annotations as errors due to domain shift, task models producing consistently wrong predictions that mask annotation errors, and high computational overhead making the approach impractical for very large datasets. The framework may also struggle with rare object classes or highly ambiguous scenes.

First Experiments:
1. Test discrepancy scoring on a small, manually corrupted dataset to verify the scoring mechanism works as expected.
2. Evaluate VLM validation on a set of known correct and incorrect annotations to measure its accuracy.
3. Run a full AutoVDC pipeline on a subset of KITTI or nuImages with injected noise to assess end-to-end performance.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance may degrade on non-autonomous driving datasets due to domain-specific annotation styles.
- Computational overhead of running VLMs at scale was not thoroughly characterized.
- Injected noise scenarios may not fully represent real-world annotation errors.

## Confidence
- High: VLMs can effectively distinguish between erroneous annotations and incorrect task model predictions.
- Medium: AutoVDC significantly improves model evaluation accuracy in controlled noise injection scenarios.
- Low: Claims about cost-efficiency and scalability lack empirical validation across diverse, large-scale datasets.

## Next Checks
1. Evaluate AutoVDC on non-autonomous driving vision datasets (e.g., medical imaging, satellite imagery) to assess cross-domain robustness.
2. Benchmark computational cost and inference time of AutoVDC at scale, comparing against manual cleaning baselines for real-world deployment feasibility.
3. Conduct a user study with human annotators to compare AutoVDC's error detection performance against expert judgment on naturally occurring annotation errors.