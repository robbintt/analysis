---
ver: rpa2
title: Eliciting Chain-of-Thought Reasoning for Time Series Analysis using Reinforcement
  Learning
arxiv_id: '2510.01116'
source_url: https://arxiv.org/abs/2510.01116
tags:
- time
- series
- reasoning
- counts
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COUNTS, a framework that trains large language
  models to perform explicit chain-of-thought reasoning on numerical time series tasks
  using reinforcement learning. The method employs a Residual Vector-Quantized VAE
  to create high-fidelity discrete tokens for time series data, integrates them into
  an LLM, and trains the model through supervised fine-tuning followed by Group Relative
  Policy Optimization.
---

# Eliciting Chain-of-Thought Reasoning for Time Series Analysis using Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.01116
- Source URL: https://arxiv.org/abs/2510.01116
- Reference count: 40
- Primary result: RL-trained CoT reasoning improves time series task performance by 6.6-18.1 percentage points

## Executive Summary
This paper introduces COUNTS, a framework that trains large language models to perform explicit chain-of-thought reasoning on numerical time series tasks using reinforcement learning. The method employs a Residual Vector-Quantized VAE to create high-fidelity discrete tokens for time series data, integrates them into an LLM, and trains the model through supervised fine-tuning followed by Group Relative Policy Optimization. Experiments show substantial performance improvements: 8.7 percentage points on ECG question answering, 18.1 points on contextual forecasting, and 6.6 points on few-shot classification, demonstrating that explicit reasoning significantly enhances time series analysis beyond traditional approaches.

## Method Summary
COUNTS combines discrete time series tokenization with LLM-based reasoning and RL optimization. A Residual Vector-Quantized VAE (RVQ-VAE) discretizes 64-point time series patches into hierarchical codebook indices, which are prepended with scale tokens and integrated into Qwen3-4B's vocabulary. The model undergoes two-stage training: first supervised fine-tuning on ~10B tokens of mixed text/time-series instruction pairs, then RL with Group Relative Policy Optimization using verifiable rewards (exact match or SMAPE) plus formatting rewards for proper CoT structure. Separate RL training runs are conducted for each task type due to performance degradation when mixing tasks.

## Key Results
- 8.7 percentage points improvement on ECG question answering versus SFT-only baseline
- 18.1 percentage points improvement on contextual forecasting tasks
- 6.6 percentage points improvement on few-shot time series classification

## Why This Works (Mechanism)

### Mechanism 1
High-fidelity discrete tokenization enables LLMs to process numerical time series with preserved precision. Residual Vector-Quantized VAE decomposes each 64-point time series patch into hierarchical codebook indices across three quantization stages. Each stage refines the residual error from the previous, achieving near-perfect reconstruction. Scale tokens (power-of-2 magnitude) are prepended to disentangle amplitude from shape.

Core assumption: Discrete token representation preserves enough numerical fidelity for downstream reasoning tasks—specifically, that 3-level RVQ (2048 codebook each) captures subtle variations critical for anomaly detection and forecasting.

Evidence anchors:
- [abstract] "Residual Vector-Quantized VAE to create high-fidelity discrete tokens that seamlessly integrate into a pre-trained LLM's vocabulary"
- [section 3.1] "By using three sequential quantization stages... RVQ achieves near-perfect reconstruction while maintaining a tractable vocabulary size"
- [corpus] Weak direct corpus evidence on RVQ-VAE specifically for time series; related work (Zeghidour et al.) cited for audio domain.

### Mechanism 2
Two-stage training (SFT → RL) separates representation learning from reasoning strategy optimization. SFT first aligns new token embeddings with LLM's existing vocabulary (warm-up with frozen backbone, then full fine-tuning on ~10B tokens). RL then uses Group Relative Policy Optimization (DAPO variant) with verifiable rewards to elicit explicit CoT reasoning. The composite reward combines correctness (exact match or SMAPE) and formatting (proper think/answer tags).

Core assumption: SFT provides sufficient foundational understanding of time series tokens; RL can then discover reasoning strategies without catastrophic forgetting of learned representations.

Evidence anchors:
- [abstract] "two-stage training process: first, supervised fine-tuning... followed by Group Relative Policy Optimization training"
- [section 3.2] "SFT phase begins with a warm-up period where only the embeddings for the time series tokens are trained"
- [corpus] Related work (TACLer, AdaCoT) supports RL for CoT but doesn't specifically validate the SFT→RL ordering for time series.

### Mechanism 3
Verifiable task rewards provide cleaner gradient signal than human preference learning for time series reasoning. Time series tasks (forecasting, classification, QA) have objective ground truth. The reward signal is computed automatically—SMAPE for forecasting, exact match for classification/QA—plus partial credit for formatting. This avoids human annotation bottlenecks and ensures reasoning directly optimizes task success.

Core assumption: Correct answers correlate with valid reasoning chains; the model isn't rewarded for right answers via wrong reasoning paths (reward hacking).

Evidence anchors:
- [abstract] "RL with verifiable rewards... automatically computed from ground truth labels"
- [section 3.2] "verifiable reward signal guides the model to discover reasoning strategies that improve task performance"
- [corpus] Related work (DeepSeek-R1, o1) demonstrates RL with verifiable rewards works for math/code; COUNTS extends this to time series.

## Foundational Learning

- Concept: **Residual Vector Quantization (RVQ)**
  - Why needed here: Understanding how continuous embeddings map to discrete tokens via hierarchical residual refinement.
  - Quick check question: Can you explain why 3-level RVQ with 2048 codebooks achieves higher fidelity than single-level VQ with 8K codebook?

- Concept: **Policy Gradient Methods (GRPO/DAPO)**
  - Why needed here: The RL phase uses group-relative advantage estimation to optimize CoT generation.
  - Quick check question: How does DAPO's removal of length normalization prevent artificial response inflation?

- Concept: **Chain-of-Thought Prompting**
  - Why needed here: The framework's core goal is eliciting explicit intermediate reasoning before final answers.
  - Quick check question: Why might CoT help more on open-ended S-Query tasks than binary S-Verify tasks?

## Architecture Onboarding

- Component map: Raw time series → Patch (64 points) → Scale normalization → Encoder (6-layer MLP) → 128-dim embedding → RVQ (L=3, 2048 vectors each) → 4 tokens/patch → LLM vocabulary extension → Qwen3-4B

- Critical path:
  1. Patch and scale raw time series (64 points/patch, power-of-2 normalization)
  2. Encode → quantize → integrate tokens into LLM context
  3. Generate response with think/answer structure
  4. Compute reward (correctness + formatting)
  5. Update policy via GRPO

- Design tradeoffs:
  - Univariate tokenizer: Simpler training but multivariate signals must be split/reassembled in prompt (token inefficiency)
  - Separate RL training per task: Better per-task performance but no unified reasoning transfer
  - Point forecasts vs. distributional: Computational efficiency during RL sampling vs. uncertainty quantification

- Failure signatures:
  - Poor reconstruction: Check RVQ codebook utilization (>80% alive codes); may need k-means reinitialization
  - RL collapse: Monitor response length inflation without accuracy gain; DAPO should mitigate this
  - Formatting degradation: If think/answer tags drop, reduce formatting reward weight gradually during training

- First 3 experiments:
  1. **Tokenizer validation**: Reconstruct held-out time series patches; target <2% SMAPE between input and reconstruction
  2. **SFT baseline**: Evaluate on ECG-QA S-Verify task after SFT phase (target ~78% accuracy per Table 1)
  3. **RL ablation**: Train with only correctness reward (no formatting), compare CoT emergence rate against full reward setup

## Open Questions the Paper Calls Out

### Open Question 1
Can a unified reward framework enable effective joint RL training across diverse time series task types (forecasting, classification, QA)?
- Basis in paper: Conclusion states: "Creating unified reward frameworks that enable joint training across diverse task types could help models learn more general reasoning strategies." Authors found that "training on a mixture of tasks hurt downstream performance, requiring separate training runs."
- Why unresolved: Current approach requires task-specific reward functions (exact match, SMAPE, accuracy); mixing tasks degrades performance, fragmenting learning.
- What evidence would resolve it: Demonstration of a single reward formulation that maintains or improves performance when training jointly on multiple task types compared to separate training runs.

### Open Question 2
How can the RVQ-VAE tokenizer be extended to natively handle multivariate time series without splitting channels into separate token sequences?
- Basis in paper: Discussion notes: "The current implementation also struggles with multivariate time series as the tokenizer is univariate and multivariate signals must be split apart and reassembled in the prompt, which hurts performance and uses many tokens."
- Why unresolved: Univariate tokenizer design fundamentally processes single channels; reassembly approach is inefficient and loses cross-channel relationships.
- What evidence would resolve it: A multivariate tokenizer architecture achieving comparable reconstruction fidelity while preserving inter-channel dependencies, evaluated on multivariate benchmarks.

### Open Question 3
Does RL-trained CoT reasoning transfer to unseen time series domains not represented in the training mixture?
- Basis in paper: Results show strong gains on held-out UCR datasets (6.6 pp improvement), but all 96 training datasets came from UCR archive. Generalization to fundamentally different domains (e.g., industrial sensor data, astronomical signals) remains untested.
- Why unresolved: Evaluation focused on held-out datasets within known benchmark families; cross-domain transfer capability not assessed.
- What evidence would resolve it: Zero-shot evaluation on time series tasks from domains completely absent from training data, comparing RL-trained vs. SFT-only models.

## Limitations
- Limited ablation studies on tokenization design—no direct comparison to alternative discretization methods
- Task-specific RL training without evaluation of cross-task reasoning transfer
- Potential reward hacking not systematically analyzed despite using verifiable rewards

## Confidence
- **High confidence**: The two-stage training approach (SFT → RL) effectively improves time series task performance over baseline models. The verifiable reward mechanism provides stable RL training compared to preference learning.
- **Medium confidence**: RVQ-VAE provides superior time series tokenization compared to simpler discretization methods. Task-specific RL training yields better performance than single-model approaches.
- **Low confidence**: The explicit CoT reasoning genuinely improves task performance versus implicit reasoning learned through RL alone. The framework generalizes to unseen time series patterns beyond training distributions.

## Next Checks
1. **Tokenization ablation study**: Train identical COUNTS pipelines with alternative tokenization schemes (standard VQ, Delta coding, or no tokenization) and compare performance across all three task types. This would validate whether RVQ-VAE's complexity is justified by performance gains.

2. **Cross-task generalization test**: After training RL policies on individual tasks, evaluate models on mixed-task inference without additional fine-tuning. Measure performance degradation and analyze whether reasoning strategies transfer between ECG-QA, forecasting, and classification domains.

3. **Reward hacking analysis**: Systematically compare response quality between models trained with full composite rewards versus correctness-only rewards. Use human evaluation to assess whether CoT reasoning is genuinely informative or merely performative, and check for statistically significant differences in answer correctness.