---
ver: rpa2
title: 'DP-GPT4MTS: Dual-Prompt Large Language Model for Textual-Numerical Time Series
  Forecasting'
arxiv_id: '2508.04239'
source_url: https://arxiv.org/abs/2508.04239
tags:
- time
- series
- data
- textual
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# DP-GPT4MTS: Dual-Prompt Large Language Model for Textual-Numerical Time Series Forecasting

## Quick Facts
- arXiv ID: 2508.04239
- Source URL: https://arxiv.org/abs/2508.04239
- Authors: Chanjuan Liu; Shengzhi Wang; Enqiang Zhu
- Reference count: 7
- Key outcome: Dual-prompt approach achieves 0.976 average MSE vs 1.065 for GPT4TS and 1.040 for single-textual prompt

## Executive Summary
DP-GPT4MTS introduces a dual-prompt framework for textual-numerical time series forecasting that separates task guidance (hard prompt) from contextual semantics (soft prompt). The model processes numerical series through RevIN and patching, textual summaries through BERT with self-attention, and explicit prompts via frozen tokenizer, then fuses these embeddings for a frozen GPT-2 backbone. Experimental results show the dual-prompt approach outperforms single-prompt variants and achieves state-of-the-art performance on GDELT and Time-MMD datasets.

## Method Summary
The method employs a frozen GPT-2 base with dual prompts: an explicit hard prompt providing task instructions and statistics via tokenizer, and a textual soft prompt processing timestamped text through BERT, self-attention, and projection to match the backbone's hidden dimension. Numerical series undergo RevIN normalization and patching before embedding. The concatenated embeddings feed the frozen transformer layers, with only position embeddings and layer norms fine-tuned. Training uses MSE loss with early stopping, and the model averages results over three random seeds.

## Key Results
- DP-GPT4MTS achieves 0.976 average MSE on benchmark datasets, outperforming GPT4TS (1.065) and single-textual prompt variants (1.040)
- Ablation studies confirm both self-attention and dual-prompt architecture contribute to performance gains
- Model demonstrates robustness across different forecasting horizons and lookback windows

## Why This Works (Mechanism)

### Mechanism 1
Dual-prompt decomposition separates task guidance from contextual semantics, reducing information redundancy compared to single-prompt approaches. An explicit hard prompt provides fixed task instructions and statistical context via the frozen tokenizer, while a textual soft prompt processes timestamp-specific text through BERT + self-attention. The hard prompt anchors reasoning; the soft prompt adapts to data-specific semantics. Their sequential concatenation (hard→soft→time series embeddings) feeds the frozen GPT-2 backbone. Core assumption: Hard and soft prompts encode complementary information that single prompts cannot efficiently capture; the LLM's frozen attention can integrate these modalities when properly structured. Evidence: Table 5 shows SEP (0.997) and STP (1.040) variants underperform the full dual-prompt (0.976 average MSE).

### Mechanism 2
Self-attention on textual embeddings selectively amplifies time-relevant semantic features before injection into the frozen LLM. BERT extracts CLS semantic vectors from timestamped text, then multi-head self-attention computes attention weights across time steps, followed by linear projection and ReLU activation to produce soft prompt embeddings matching the backbone's hidden dimension. Core assumption: Temporal dependencies exist within textual summaries across time; self-attention can identify which text elements are most relevant for forecasting. Evidence: DP-NTSA (dual-prompt without textual self-attention) shows degraded performance (1.012 vs 0.976 average MSE), confirming self-attention's contribution.

### Mechanism 3
Patching with reversible instance normalization (RevIN) localizes temporal patterns while mitigating distribution shift. RevIN normalizes input series to reduce train/test distribution gaps (reversible). Patching divides the series into consecutive segments (length L_p, stride S), producing P patches. These are embedded and concatenated, allowing the frozen LLM to process localized temporal contexts. Core assumption: Local patches capture informative short-term dependencies; RevIN preserves generalization across domains/time periods. Evidence: Figure 4 shows increasing lookback window reduces MSE, but with computational trade-offs.

## Foundational Learning

- **Prompt Engineering for LLMs (Hard vs. Soft Prompts)**: The architecture relies on understanding the difference between fixed tokenized prompts (hard) and trainable embedding prompts (soft), and why their combination matters for multimodal tasks. Quick check: Can you explain why a hard prompt cannot adapt to dataset-specific semantics, and how a soft prompt addresses this limitation?

- **Frozen Pre-trained Backbones with Selective Fine-tuning**: The model freezes GPT-2's transformer blocks while fine-tuning position embeddings and layer norms—understanding why this preserves general knowledge while allowing task adaptation is critical. Quick check: If you fine-tuned all GPT-2 parameters instead, what risks would you introduce (catastrophic forgetting, overfitting, computational cost)?

- **Self-Attention Mechanism for Temporal Feature Selection**: The textual prompt pathway uses multi-head self-attention to weight temporal text features—grasping how attention scores are computed (Q, K, V matrices) and what they represent is essential for debugging and modification. Quick check: Given the attention formula Attention(Q,K,V) = softmax(QK^T/√d_k)V, what happens if all K vectors are nearly identical (low textual diversity across time)?

## Architecture Onboarding

- **Component map**: Explicit prompt text → Tokenizer → Hard prompt embedding E; Textual summaries → BERT → Self-attention + FFN → Soft prompt embedding I; Numerical series → RevIN → Patching → Time series embedding X; Concatenate [E; I; X] → Positional embeddings → Frozen GPT-2 base → Remove prompt prefix → Flatten → Linear projection → Forecast

- **Critical path**: 1) Explicit prompt design (task instructions + statistics) → tokenizer quality directly affects task guidance; 2) BERT CLS extraction → self-attention → projection: ensures textual embeddings align with backbone hidden dimension D; 3) Patching stride S and length L_p: controls granularity of temporal information reaching the LLM; 4) Output projection layer: maps flattened embeddings to prediction horizon T

- **Design tradeoffs**: Hard prompt before soft prompt vs. swapped: Ablation (SPET) shows swapping increases MSE (1.004 vs 0.976), suggesting explicit guidance should precede contextual adaptation; Lookback window L: Larger L improves accuracy but increases compute; Frozen vs. fully fine-tuned backbone: Frozen preserves generalization; selective tuning of position/layer norms adapts to sequential structure

- **Failure signatures**: High MSE with noisy textual data (e.g., "Not Available" markers in Time-MMD): indicates soft prompt propagating noise; Single prompt variants (SEP/STP) underperforming: suggests modalities are not complementary in current data; Self-attention variant (DP-NTSA) matching full model: textual embeddings may lack temporal structure, making attention redundant

- **First 3 experiments**: 1) Ablation by prompt type: Run SEP, STP, and full dual-prompt on a held-out validation split. Compare MSE/MAE to quantify each prompt's contribution and verify that both are necessary for your target dataset; 2) Self-attention impact: Disable self-attention in textual pathway (replace with mean pooling or direct projection). Measure performance drop to confirm whether temporal attention is extracting useful structure from your text; 3) Lookback window sensitivity: Test L ∈ {7, 15, 30, 60} on your dataset. Plot MSE vs. L and compute training time per epoch to identify the point where marginal accuracy gains do not justify computational cost

## Open Questions the Paper Calls Out

- **Zero-shot and few-shot learning adaptation**: The paper explicitly suggests exploring zero-shot and few-shot learning methods to "unlock the potential of large models in scenarios with limited samples." The current study uses standard train/validation/test splits (7:2:1 ratio), leaving performance in data-scarce environments unverified. What evidence would resolve it: Experimental results showing the model's predictive accuracy when trained on only a handful of examples (k-shot setting) or no examples (zero-shot) compared to fully supervised baselines.

- **Higher-quality benchmark datasets**: The authors highlight the need to "prioritize the construction of more comprehensive and high-quality textual-numerical time series benchmark datasets" to address current data limitations. Existing datasets contain noise (e.g., "Not Available" markers, irrelevant background info) that may hinder reasoning capabilities. What evidence would resolve it: A comparative study of the model's performance on the current GDELT/Time-MMD datasets versus a newly curated, cleaner dataset with strict modality alignment.

- **Pre-processing effectiveness**: The paper notes that "lengthy descriptions, irrelevant background information, and potential emotional biases" introduce noise, and the model currently uses a self-attention mechanism to mitigate this, but pre-processing effectiveness is unknown. While the model uses self-attention to refine embeddings, it is unclear if raw, noisy text acts as a bottleneck. What evidence would resolve it: An ablation study comparing the performance of the textual prompt using raw text versus using algorithmically summarized or filtered text as input.

## Limitations
- Critical hyperparameters (patch length, stride, projection dimensions, batch size, optimizer) remain unspecified, preventing faithful reproduction
- The paper does not validate whether textual summaries actually contain predictive temporal structure relevant to numerical forecasts
- Mechanism by which frozen LLM integrates hard prompt task guidance with soft prompt contextual semantics remains theoretically asserted rather than empirically demonstrated

## Confidence
- **High confidence**: The empirical superiority of dual-prompt over single-prompt variants is well-supported by ablation studies (0.997/1.040 vs 0.976 average MSE). The integration of established techniques (RevIN, patching, frozen backbone fine-tuning) follows logical design principles.
- **Medium confidence**: The claim that self-attention selectively amplifies time-relevant semantic features is supported by performance degradation when disabled (1.012 vs 0.976 average MSE), but lacks direct validation of attention weight interpretability or temporal feature extraction quality.
- **Low confidence**: The paper does not provide evidence that hard and soft prompts encode truly complementary information beyond their different training dynamics.

## Next Checks
1. **Attention Weight Analysis**: Extract and visualize self-attention weight matrices from the textual prompt pathway. Compute correlation between attention weights and forecasting accuracy across time steps to verify that the model genuinely focuses on temporally relevant textual features rather than arbitrary patterns.

2. **Prompt Redundancy Quantification**: Measure mutual information between hard prompt embeddings (E) and soft prompt embeddings (I) using kernel density estimation or contrastive learning. High redundancy would suggest the dual-prompt decomposition adds minimal value beyond parameter count.

3. **Cross-dataset Transferability Test**: Train DP-GPT4MTS on GDELT, then evaluate zero-shot or few-shot on Time-MMD without retraining the textual prompt pathway. Strong performance would validate that the soft prompt captures generalizable semantic features rather than dataset-specific memorization.