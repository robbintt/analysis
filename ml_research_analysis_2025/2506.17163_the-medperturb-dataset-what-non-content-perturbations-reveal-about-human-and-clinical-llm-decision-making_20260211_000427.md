---
ver: rpa2
title: 'The MedPerturb Dataset: What Non-Content Perturbations Reveal About Human
  and Clinical LLM Decision Making'
arxiv_id: '2506.17163'
source_url: https://arxiv.org/abs/2506.17163
tags:
- clinical
- treatment
- llms
- language
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedPerturb introduces a dataset of 800 clinical vignettes to systematically
  evaluate how Large Language Models (LLMs) and human clinicians differ in treatment
  decisions under controlled perturbations of non-clinical input features. The dataset
  includes gender, style, and format perturbations applied to clinical contexts from
  OncQA, r/AskaDocs, and USMLE/Derm sources.
---

# The MedPerturb Dataset: What Non-Content Perturbations Reveal About Human and Clinical LLM Decision Making

## Quick Facts
- arXiv ID: 2506.17163
- Source URL: https://arxiv.org/abs/2506.17163
- Reference count: 40
- Primary result: LLMs under-recommend self-management and over-allocate resources compared to clinicians under controlled perturbations

## Executive Summary
MedPerturb introduces a dataset of 800 clinical vignettes to systematically evaluate how Large Language Models (LLMs) and human clinicians differ in treatment decisions under controlled perturbations of non-clinical input features. The dataset includes gender, style, and format perturbations applied to clinical contexts from OncQA, r/AskaDocs, and USMLE/Derm sources. Three treatment questions (MANAGE, VISIT, RESOURCE) are answered by four LLMs and 36 clinician annotators. Results show that LLMs tend to under-recommend self-management and over-allocate resources compared to humans. LLMs are more sensitive to gender and style perturbations, while humans are more sensitive to format perturbations like clinical summaries. Human-LLM agreement is low (~65–75%), with significant differences across perturbations. The findings highlight the need for evaluation frameworks that go beyond accuracy to assess robustness and alignment under realistic clinical variability.

## Method Summary
The study creates 800 clinical vignettes from three sources: OncQA, r/AskaDocs, and USMLE/Derm datasets. Gender perturbations swap or remove gender markers, style perturbations add uncertainty or colorful language, and format perturbations convert conversations to summaries. Four LLMs (GPT-4, Llama-3-70B, Llama-3-8B, Palmyra-Med-70B) answer triage questions with binary outputs. Medical students annotate via Centaur Labs with three reads per context. Metrics include Average Treatment Rate, Mutual Information between baseline/perturbed decisions, Percent Change for format perturbations, and Fleiss' Kappa for agreement. Statistical significance is assessed with paired t-tests, Mann-Whitney U, McNemar's test, and Bonferroni correction (p<0.01).

## Key Results
- Clinicians recommend ~37% more self-management than LLMs; LLM ATR for self-management near 0, for resource allocation near 1
- LLMs show significantly lower Mutual Information between baseline and perturbed outputs than clinicians across gender and style perturbations (p < 0.01)
- Clinicians shift toward more conservative (less resource-intensive) recommendations when presented with LLM-generated summaries or multi-turn conversations
- Human-LLM agreement is low (~65–75%) across all perturbations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs exhibit higher decision instability than clinicians when non-clinical linguistic features are perturbed
- **Mechanism:** LLMs weight surface-level linguistic features more heavily in their decision distributions than human clinicians, who maintain internally consistent treatment logic across framing changes
- **Core assumption:** Lower MI reflects genuine decision instability rather than reasonable clinical adaptation
- **Evidence anchors:** Clinicians show significantly higher MI between baseline and perturbed outputs than LLMs across gender and style perturbations (p < 0.01); related work documents LLM sensitivity to gender cues
- **Break condition:** If perturbations inadvertently change clinically relevant content, lower MI could reflect appropriate clinical reasoning

### Mechanism 2
- **Claim:** Clinicians shift toward more conservative treatment recommendations when presented with LLM-generated summaries
- **Mechanism:** LLM-generated summaries may inadvertently downplay or omit clinically salient details, leading clinicians to perceive cases as less urgent
- **Core assumption:** The observed shift reflects information loss in summarization rather than clinicians' response to narrative structure
- **Evidence anchors:** Summarized clinical contexts result in >30% more self-management recommendations and 20% less resource allocation from clinicians; LLMs show minimal change
- **Break condition:** If higher-quality summaries showed no correlation with treatment shifts, the mechanism would not involve information loss

### Mechanism 3
- **Claim:** LLMs default to resource-intensive recommendations compared to clinicians
- **Mechanism:** Models likely encode a conservative risk posture during training/alignment, prioritizing exhaustive workups over patient-centered care
- **Core assumption:** Conservative recommendations reflect model behavior rather than clinical appropriateness in ambiguous cases
- **Evidence anchors:** Clinicians recommend ~37% more self-management than LLMs on average; ATR distributions show systematic divergence
- **Break condition:** If clinical vignettes systematically under-specify relevant contraindications, conservative LLM behavior could be appropriate

## Foundational Learning

- **Concept: Mutual Information (MI) for decision stability**
  - **Why needed here:** MI measures consistency between baseline and perturbed treatment decisions, essential for interpreting LLM vs. human robustness
  - **Quick check question:** If an LLM changes its recommendation when gender is swapped but clinical content is unchanged, does this indicate low MI? (Answer: Yes, if the change is inconsistent with baseline patterns)

- **Concept: Controlled perturbation design**
  - **Why needed here:** Study validity rests on perturbations changing only non-clinical features; filtering gendered conditions is critical
  - **Quick check question:** Why must gendered clinical conditions be excluded before gender-swap perturbations? (Answer: Because gender would then be clinically relevant, and sensitivity to it would be appropriate)

- **Concept: Inter-rater agreement (Fleiss' κ) in clinical contexts**
  - **Why needed here:** The paper reports Fleiss' Kappa across human annotators and across LLMs to measure agreement
  - **Quick check question:** What does a Fleiss' κ of 0.3 indicate about annotator agreement? (Answer: Fair agreement, suggesting moderate consistency in treatment decisions)

## Architecture Onboarding

- **Component map:** Clinical vignettes -> Perturbations (gender/style/format) -> LLM evaluation (4 models) -> Human evaluation (36 annotators) -> Agreement metrics (MI, ATR, κ)
- **Critical path:** Perturbation generation → Model evaluation → Human annotation → Statistical analysis → Mechanism validation
- **Design tradeoffs:** Synthetic perturbations may not capture natural language diversity; medical students vs. attending physicians as annotators; binary vs. multi-level treatment decisions
- **Failure signatures:** Low MI indicates decision instability; high Fleiss' κ (>0.6) indicates strong agreement; ATR near 0 or 1 indicates ceiling/floor effects
- **3 first experiments:**
  1. Re-run perturbation fidelity check on 50 random samples to verify non-clinical perturbations
  2. Compare LLM outputs with and without system-level safety filters applied
  3. Test whether higher-quality summaries (by ROUGE) correlate with smaller clinician treatment shifts

## Open Questions the Paper Calls Out

- Does the observed shift in human decision-making for format-perturbed contexts stem from obscuration of clinical details or from human difficulty interpreting LLM-generated text?
- Do LLM-generated style perturbations introduce generation artifacts that bias evaluation rather than accurately reflecting realistic patient language diversity?
- Can standard summarization metrics (ROUGE, BLEU, Cosine Similarity) be relied upon for clinical evaluation given their weak correlation with downstream treatment decisions?
- Do the observed sensitivities to perturbations persist when evaluated by attending physicians rather than medical students?

## Limitations

- Perturbation effects could reflect implicit clinical content changes rather than pure linguistic features
- The mechanism linking LLM-generated summaries to clinician conservatism lacks direct evidence
- Human annotator pool (medical students) may not fully represent practicing clinician decision-making
- Dataset filtering criteria for gendered conditions lacks complete specification

## Confidence

- **High confidence:** LLMs show lower MI between baseline and perturbed outputs than clinicians (Mechanism 1)
- **Medium confidence:** LLMs over-recommend resources and under-recommend self-management (Mechanism 3)
- **Low confidence:** LLM-generated summaries cause clinician conservatism (Mechanism 2)

## Next Checks

1. **Perturbation fidelity check:** Re-run a subset of perturbations through blinded clinician review to confirm only non-clinical features change
2. **Summarization quality correlation:** Measure ROUGE and cosine similarity between original/summarized contexts, then test whether higher-quality summaries correlate with smaller treatment shifts
3. **External annotator validation:** Have practicing clinicians re-annotate 50 random contexts to assess whether human-LLM agreement patterns hold across different annotator pools