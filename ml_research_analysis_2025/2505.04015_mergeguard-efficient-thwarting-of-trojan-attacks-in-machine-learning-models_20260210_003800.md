---
ver: rpa2
title: 'MergeGuard: Efficient Thwarting of Trojan Attacks in Machine Learning Models'
arxiv_id: '2505.04015'
source_url: https://arxiv.org/abs/2505.04015
tags:
- trojan
- attacks
- mergeguard
- layers
- mitigation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MergeGuard, a novel method to defend against
  AI trojan attacks. Trojan attacks trick ML models into misclassifying inputs with
  specific triggers.
---

# MergeGuard: Efficient Thwarting of Trojan Attacks in Machine Learning Models

## Quick Facts
- **arXiv ID:** 2505.04015
- **Source URL:** https://arxiv.org/abs/2505.04015
- **Reference count:** 30
- **Primary result:** Novel post-training compression method that reduces trojan attack success rates by up to 20% while preserving accuracy and achieving up to 17.7x speedup.

## Executive Summary
MergeGuard introduces a post-training defense mechanism against trojan attacks in ML models through activation linearization and layer merging. The method targets fully connected and convolutional layers, reducing model depth while removing backdoor-related neurons. Experiments demonstrate consistent trojan mitigation across multiple attack types (BadNet, TrojanNN, WaNet, Blended, SIG) on both Vision Transformers and CNNs, achieving up to 15% model size reduction and significant computational efficiency gains.

## Method Summary
MergeGuard works by fine-tuning trojaned models on a small clean dataset with a regularization term that forces activation functions toward linearity. Once linearized (α approaches 1), consecutive linear layers can be merged into single equivalent layers, reducing model depth. The method specifically targets the last 3-4 layers based on the assumption that trojans concentrate in terminal layers. Regularization strength is set to 1, and the process uses SGD with 0.9 momentum for 20 epochs with batch size 128. A 5% clean data subset is used for fine-tuning.

## Key Results
- Achieves up to 20% reduction in attack success rates across multiple trojan attack types
- Preserves model accuracy while reducing parameter count by up to 15%
- Provides computational speedups of up to 17.7x compared to alternative mitigation methods
- Successfully mitigates trojans in both Vision Transformers and CNNs
- Requires only 5% of clean training data for effective defense

## Why This Works (Mechanism)

### Mechanism 1: Activation Linearization for Layer Merging
- **Claim:** Linearizing activation functions enables merging of consecutive linear layers, removing intermediate layers that may contain backdoor-related neurons.
- **Mechanism:** MergeGuard adaptively adjusts the negative slope (α) of Parametric ReLU activations toward 1. When α = 1, the activation becomes fully linear (identity), allowing two consecutive linear transformations (W₂ and W₁) to collapse into a single equivalent layer (W₂W₁). This reduces model depth without requiring architectural redesign.
- **Core assumption:** Backdoor-related neurons are not uniformly distributed across all layers; they concentrate in specific fully connected or convolutional layers that can be targeted for removal.
- **Evidence anchors:** The error bound |Y_linear − Y_α|² ≤ C × (1 − α)² proves error is bounded proportionally to (1 − α)², and experiments show α converges to 1 during regularization.

### Mechanism 2: Regularization-Driven Weight Perturbation
- **Claim:** Regularization-driven compression forces substantial weight perturbations that disrupt backdoor trigger pathways more effectively than vanilla fine-tuning.
- **Mechanism:** The loss function incorporates (1 − α)² as a regularizer alongside cross-entropy loss. This jointly optimizes for task performance and linearity, forcing systematic weight restructuring across layers that standard fine-tuning fails to achieve.
- **Core assumption:** Vanilla fine-tuning produces insufficient weight norm changes in backdoor-related neurons; explicit regularization is needed to escape the local minimum where trojan behavior persists.
- **Evidence anchors:** Fine-tuning shows inconsistent ASR reduction across attacks while MergeGuard shows more consistent mitigation, with Table I-II demonstrating MergeGuard's superior performance.

### Mechanism 3: Depth Reduction Through Layer Elimination
- **Claim:** Reducing model depth through layer merging eliminates backdoor neurons while preserving task performance through joint optimization.
- **Mechanism:** By targeting the last 3-4 layers, MergeGuard removes these layers entirely after linearization. The compression ratio determines whether parameter reduction is achieved, with ViT-base-16 showing 15% parameter reduction and 14% MAC reduction.
- **Core assumption:** Backdoor triggers rely on specific neurons that can be removed without destroying task-critical representations; the benign task has redundant capacity that can be compressed.
- **Evidence anchors:** Experimental results show 73.4 million parameters post-MergeGuard versus 85.8 million originally, with maintained accuracy.

## Foundational Learning

- **Concept: Backdoor/Trojan Attacks on Neural Networks**
  - **Why needed here:** Understanding how trojans differ from adversarial attacks is essential. Trojans are embedded during training via data poisoning or model manipulation, creating trigger-response mappings that persist post-deployment.
  - **Quick check question:** Can you explain why fine-tuning on clean data alone often fails to remove trojans, based on the paper's discussion of weight norm changes?

- **Concept: Parametric ReLU and Activation Linearization**
  - **Why needed here:** MergeGuard relies on manipulating PReLU's α parameter. Standard PReLU is f(x) = max(0,x) + α·min(0,x). When α = 1, it becomes linear (f(x) = x).
  - **Quick check question:** What happens to the non-linearity error bound when α approaches 1, based on the inequality in Section III-D?

- **Concept: Post-Training Model Compression**
  - **Why needed here:** MergeGuard doubles as a compression technique. Understanding when compression ratio is positive vs. negative helps predict which architectures will benefit.
  - **Quick check question:** For an MLP with n_in=512, n_hidden=256, n_out=10, would the compression ratio be positive or negative using Equation 6?

## Architecture Onboarding

- **Component map:** Pretrained trojaned model + 5% clean data -> Regularization module (PReLU with α parameter) -> Fine-tuning (SGD, 20 epochs) -> Layer merging (if α ≈ 1) -> Compressed cleansed model

- **Critical path:**
  1. Identify target layers (last 3 FC for CNN, last 4 FC for ViT)
  2. Replace fixed activations with parametric PReLU with learnable α
  3. Fine-tune with regularization strength λ = 1 on 5% clean data
  4. Monitor α values during training; approach 1 indicates successful linearization
  5. Post-training: merge layers where α reached threshold
  6. Validate on clean test set (accuracy) and triggered test set (ASR)

- **Design tradeoffs:**
  - **Compression vs. accuracy:** Higher regularization accelerates linearization but risks accuracy drop; paper uses λ = 1 as default
  - **Layer selection:** Targeting too few layers may miss trojans; too many may over-compress. Paper acknowledges layer selection algorithm as future work
  - **Architecture compatibility:** Works well on "widening" MLPs; bottleneck structures may have negative compression ratios
  - **Convolutional layer merging:** Kernel size increases (k = k₁ + k₂ − 1), which may not reduce parameters for certain CNN architectures

- **Failure signatures:**
  - Test accuracy drops significantly (>5%): Regularization too strong or layer selection removes task-critical representations
  - ASR remains high (>20%): Trojan not in targeted layers, or regularization insufficient to perturb backdoor neurons
  - α values stuck below 0.9: Learning rate too low or regularization weight too weak
  - Negative compression ratio: Architecture has bottleneck layers; reconsider which layers to target

- **First 3 experiments:**
  1. **Baseline replication on CIFAR-10:** Take pretrained trojaned ViT-base-16 (BadNet attack, 10% poisoning), apply MergeGuard to last 4 FC layers with 5% clean data, 20 epochs, SGD with momentum 0.9. Measure test accuracy and ASR.
  2. **Ablation on layer selection:** Compare targeting last 2 vs. last 4 vs. last 6 FC layers on same trojaned model. Monitor compression ratio and ASR to validate the assumption that trojans concentrate in terminal layers.
  3. **Compression-accuracy tradeoff curve:** Vary regularization strength λ ∈ {0.1, 0.5, 1.0, 2.0, 5.0} on a trojaned PreAct-ResNet18. Plot final α values, test accuracy, ASR, and parameter count to identify optimal operating point.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can an automated algorithm be developed to optimally select layers for MergeGuard regularization instead of relying on manual heuristics?
- **Basis in paper:** The conclusion explicitly lists "developing algorithms for the selection of MergeGuard layers" as a primary direction for future research.
- **Why unresolved:** The current implementation manually targets specific terminal layers based on conjectures from prior work, which may not be optimal for all architectures or attack types.
- **What evidence would resolve it:** A comparative study showing a dynamic selection algorithm outperforming the static, manual selection strategy across diverse model architectures.

### Open Question 2
- **Question:** How effective is MergeGuard when applied to non-vision transformer architectures, such as Large Language Models (LLMs) or audio transformers?
- **Basis in paper:** The conclusion suggests "extending the method to additional transformer-based architectures" as a future work goal.
- **Why unresolved:** The evaluation is strictly limited to Vision Transformers (ViT) and CNNs; the paper does not demonstrate if the linearization and merging approach translates to the attention mechanisms or embedding layers of NLP models.
- **What evidence would resolve it:** Empirical results showing Attack Success Rate (ASR) reduction and accuracy preservation when applying MergeGuard to standard NLP benchmarks (e.g., BERT, GPT-2).

### Open Question 3
- **Question:** Is MergeGuard robust against attacks that inject backdoors into non-terminal layers, given the defense explicitly targets the final layers?
- **Basis in paper:** The methodology states the authors apply regularization to the last three or four layers "presuming the trojan's presence in the terminal layers."
- **Why unresolved:** If an adversary injects the trigger in earlier feature extraction layers, the current layer selection strategy would fail to merge or retrain the compromised components.
- **What evidence would resolve it:** Experiments testing MergeGuard against attacks specifically designed to embed malicious neurons in the middle or initial layers of the network.

## Limitations
- Assumes trojans concentrate in terminal layers, which may not hold for all attack types
- Layer merging only applicable to "widening" MLPs; bottleneck configurations may expand rather than compress
- Requires 5% clean data, which may be prohibitive in some deployment scenarios
- Computational complexity analysis focuses on inference speed but doesn't account for regularization overhead during fine-tuning

## Confidence

- **High Confidence:** The mathematical framework for activation linearization and layer merging is sound. The error bound |Y_linear − Y_α|² ≤ C × (1 − α)² is correctly derived and experimentally validated through α convergence to 1.
- **Medium Confidence:** The trojan mitigation effectiveness across diverse attacks shows promising but inconsistent results. While MergeGuard achieves substantial ASR reduction in most cases, failure on FTSAM attack for ResNet and limited efficacy on certain trojan variants suggest the mechanism isn't universally reliable.
- **Low Confidence:** The claim that MergeGuard specifically addresses ViT resilience gaps relies on comparison with limited baseline methods. The assertion that 5% clean data is "small enough for practical use" lacks systematic study of the data-efficiency frontier.

## Next Checks

1. **Architecture Compatibility Analysis:** Test MergeGuard on bottleneck MLPs (n_hidden < n_in × n_out / (n_in + n_out)) to verify the negative compression ratio prediction and identify which architectures benefit most from this approach.

2. **Early Layer Trojan Detection:** Systematically place trojans in different layers (early, middle, terminal) and measure MergeGuard's efficacy to validate the layer-targeting assumption. This would identify failure modes when trojans aren't in the targeted regions.

3. **Data Efficiency Frontier:** Systematically vary the clean data percentage (1%, 5%, 10%, 25%) during regularization to quantify the minimum data requirement for effective trojan mitigation while maintaining accuracy.