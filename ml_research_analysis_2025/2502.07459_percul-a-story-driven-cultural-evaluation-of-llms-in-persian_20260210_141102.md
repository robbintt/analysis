---
ver: rpa2
title: 'PerCul: A Story-Driven Cultural Evaluation of LLMs in Persian'
arxiv_id: '2502.07459'
source_url: https://arxiv.org/abs/2502.07459
tags:
- persian
- cultural
- llms
- these
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces PerCul, a novel dataset for evaluating cultural
  understanding of large language models in Persian. PerCul consists of 592 story-based
  multiple-choice questions, carefully curated by native Persian annotators to assess
  models' sensitivity to culturally nuanced scenarios across 11 categories.
---

# PerCul: A Story-Driven Cultural Evaluation of LLMs in Persian

## Quick Facts
- arXiv ID: 2502.07459
- Source URL: https://arxiv.org/abs/2502.07459
- Authors: Erfan Moosavi Monazzah; Vahid Rahimzadeh; Yadollah Yaghoobzadeh; Azadeh Shakery; Mohammad Taher Pilehvar
- Reference count: 12
- Primary result: Novel Persian cultural benchmark revealing significant LLM performance gaps vs. human baseline

## Executive Summary
This paper introduces PerCul, a novel dataset for evaluating cultural understanding of large language models in Persian. PerCul consists of 592 story-based multiple-choice questions, carefully curated by native Persian annotators to assess models' sensitivity to culturally nuanced scenarios across 11 categories. Experiments with state-of-the-art models reveal a significant performance gap compared to human baseline, with the best closed-source model achieving 11.3% lower accuracy and the best open-weight model 21.3% lower. Persian-specific fine-tuned models perform worse than their base models, suggesting quality issues in training data. Translation experiments show that model performance drops significantly when questions are translated to English, indicating genuine Persian cultural understanding rather than translation-based responses.

## Method Summary
PerCul uses a semi-automated pipeline combining human cultural expertise with LLM story generation. Native Persian annotators first identify culturally significant topics across 11 categories derived from Hall's cultural framework, then generate facets and metadata to ground story generation. GPT-4o and Claude 3.5 Sonnet generate storylines using this metadata, which human annotators review and revise. Six distractor generation heuristics create plausible incorrect options, with three-stage human selection narrowing to final 3 distractors per question. The dataset contains 592 story-based multiple-choice questions with 4 options each, evaluated using zero-shot inference at temperature=0.

## Key Results
- Human baseline accuracy: 93.0% across all categories
- Best closed-source model (Claude 3.5 Sonnet): 81.7% accuracy (11.3% below human baseline)
- Best open-weight model (LLaMA-3.1-405B-Inst): 71.7% accuracy (21.3% below human baseline)
- Translation to English reduces accuracy by 6.6% to 14.5%, with 46% of errors stemming from cultural nuance loss
- Persian-specific fine-tuned models perform worse than base models (PersianMind: 3.3% vs. Llama-3.1-8B: 44.4%), suggesting low-quality training data

## Why This Works (Mechanism)

### Mechanism 1: Story-based implicit cultural assessment
Embedding cultural concepts in narrative stories tests deeper cultural understanding than direct factual questions. Stories capture cultural phenomena through interpersonal interactions, forcing models to synthesize contextual clues across multiple sentences rather than retrieving surface-level facts. Cultural knowledge is better expressed through implicit narrative context than explicit declarative statements.

### Mechanism 2: Translation as diagnostic probe
Performance degradation after translation confirms genuine target-language cultural understanding rather than shortcut exploitation. Many cultural concepts lack direct English equivalents (e.g., specific Persian vessel terminology, respect for bread); translation either loses nuance or inadvertently adds clarifying context. If models relied on translation-based reasoning, English versions would perform at least as well as original language.

### Mechanism 3: Human-curated metadata grounding
Native annotator-provided metadata constrains LLM story generation, reducing hallucination and ensuring cultural authenticity. Annotators generate facets and metadata per topic before story generation; this factual grounding plus human editing prevents model hallucination loops. Human cultural knowledge is more reliable than LLM-generated cultural content, particularly for underrepresented cultures.

## Foundational Learning

- **Concept: Hall's Triad of Culture (Cultural Iceberg Theory)**
  - Why needed here: PerCul's 11 categories derive from Hall's framework: technical (facts, objects, iconic figures), formal (traditions, norms, appropriacy), and informal (excluded due to measurement difficulty).
  - Quick check question: Can you distinguish "knowing what Chaharshanbe Suri is" (technical) from "understanding appropriate behavior at Chaharshanbe Suri" (formal)?

- **Concept: Translation artifact detection**
  - Why needed here: Evaluating whether models exploit translation shortcuts is critical for validating culture-specific benchmarks.
  - Quick check question: If translating a Persian benchmark to English *improves* performance, what does this suggest about the model's knowledge representation?

- **Concept: Distractor generation heuristics**
  - Why needed here: PerCul uses 6 rules (partial correctness, misinterpretation, unrelated fact, plausible unsupported, noun confusion, overgeneralization); understanding these reveals model failure modes.
  - Quick check question: Why would "Partially correct but incomplete" options (R1) be more effective than obviously wrong distractors?

## Architecture Onboarding

- **Component map:** Seed topics (709 → 556 after agreement filter) → Facet identification → Metadata collection → LLM story generation (GPT-4o, Claude 3.5 Sonnet) → Human editing (21.9% avg text modification) → Distractor generation (6 heuristics × 2 models) → 3-stage human selection → Final 592 QA pairs

- **Critical path:**
  1. Annotators propose seed topics across 11 cultural categories with inter-annotator agreement filtering
  2. Generate facets and metadata per topic (human-only, limited web use to avoid LLM training data overlap)
  3. LLMs generate storylines using metadata as grounding
  4. Human review: select better variant or rewrite (source model blinded)
  5. Generate 24 distractors per question; 3-stage human selection narrows to 3 final options
  6. Human baseline evaluation establishes 93% ceiling

- **Design tradeoffs:**
  - Semi-automated vs. fully human: LLMs draft stories, humans validate—balances scale with authenticity
  - Implicit vs. explicit: Stories embed concepts subtly—trades evaluation ease for realism
  - Annotator diversity vs. practical constraints: 8 annotators from varied cities but mostly graduate students (potential academic bias)

- **Failure signatures:**
  - Persian fine-tuned models perform *worse* than base models (PersianMind: 3.3% vs. Llama-3.1-8B: 44.4%)—suggests low-quality fine-tuning data
  - Models select R1 (partial correctness) distractors most often—indicates reliance on surface semantic similarity
  - Category-specific gaps: Music most difficult (56.2% best), Rituals easiest (>90% for top models)
  - Translation sometimes *helps* when it adds clarifying parentheticals (e.g., "Tombak (a type of Persian drum)")

- **First 3 experiments:**
  1. Run baseline evaluation at zero temperature using standardized prompt (Appendix C.1) across target model families; compute macro accuracy per category
  2. Translate test set to English via GPT-4o; compare delta to distinguish cultural vs. linguistic knowledge
  3. Conduct distractor analysis: compute selection rates per heuristic rule per category to identify systematic reasoning failures

## Open Questions the Paper Calls Out

### Open Question 1
What training data quality issues cause Persian-specific fine-tuned models (like PersianMind and Dorna) to underperform their base multilingual models on cultural understanding? The paper observes the performance degradation but does not analyze the composition or quality of the Persian fine-tuning corpora used by these models.

### Open Question 2
How can LLMs be evaluated on Hall's "informal" level of culture (unconscious, emotionally-driven behaviors) given the difficulty of capturing these in text? The authors excluded informal culture from PerCul due to subjectivity and difficulty of data collection; no methodology yet exists for this level.

### Open Question 3
Why do models fail on 54% of accurately translated samples where cultural nuances are preserved? Translation analysis shows 54% of samples were "accurately translated into English" yet "answered incorrectly by the model"—the paper does not explain this failure mode.

### Open Question 4
To what extent does the university-student annotator pool bias PerCul toward academic cultural knowledge? Limitations section notes "majority were university students, which may introduce bias towards the Persian academic community."

## Limitations
- Human annotator demographics: Most annotators were graduate students or employed in academic settings, potentially biasing cultural representation toward academic/urban perspectives rather than capturing the full spectrum of Persian cultural knowledge.
- Seed topic curation constraints: Inter-annotator agreement filtering (requiring ≥5 agreements out of 8 annotators) may have eliminated culturally significant but controversial or less commonly known topics, creating a conservative representation of Persian culture.
- Translation as diagnostic probe uncertainty: While translation experiments suggest genuine cultural understanding, the translation quality itself could introduce artifacts. The observed performance drops could partially reflect translation quality rather than purely cultural knowledge gaps.

## Confidence

- **High Confidence**: The performance gap between models and human baseline (11.3% for best closed-source, 21.3% for best open-weight) is robustly demonstrated across multiple model families and evaluation conditions.
- **Medium Confidence**: The translation experiment interpretation is well-supported but relies on the assumption that translation quality is consistently high and that observed performance drops reflect cultural rather than linguistic translation artifacts.
- **Medium Confidence**: The claim that Persian-specific fine-tuned models perform worse due to low-quality training data is plausible given the observed performance degradation, but alternative explanations (architecture limitations, optimization issues) cannot be ruled out without examining the fine-tuning datasets.

## Next Checks
1. Conduct a systematic analysis of annotator backgrounds and geographic distribution to quantify potential demographic biases in the dataset construction process.
2. Perform human evaluation of the translated English questions to assess translation fidelity and identify specific cultural nuances that may have been lost or inadvertently clarified.
3. Examine the training data composition and quality of Persian-specific models to validate the hypothesis that poor performance stems from low-quality fine-tuning data rather than architectural limitations.