---
ver: rpa2
title: 'Sketch to Adapt: Fine-Tunable Sketches for Efficient LLM Adaptation'
arxiv_id: '2410.06364'
source_url: https://arxiv.org/abs/2410.06364
tags:
- sketchtune
- sketching
- sketched
- parameters
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SketchTune addresses the challenge of fine-tuning large language
  models (LLMs) by introducing a novel approach that compresses LLM weights into compact
  fine-tunable sketches, avoiding restrictive assumptions like low-rank constraints
  used in existing parameter-efficient fine-tuning (PEFT) methods. The method unifies
  compression and adaptation into a single framework, enabling faster and more memory-efficient
  training and inference by eliminating complex two-path computations.
---

# Sketch to Adapt: Fine-Tunable Sketches for Efficient LLM Adaptation

## Quick Facts
- arXiv ID: 2410.06364
- Source URL: https://arxiv.org/abs/2410.06364
- Reference count: 40
- Key result: SketchTune achieves 14.48% higher GSM8K accuracy than LoftQ with 7.3× fewer trainable parameters

## Executive Summary
SketchTune introduces a novel parameter-efficient fine-tuning method that compresses LLM weights into compact fine-tunable sketches, avoiding the restrictive low-rank assumptions used in existing PEFT methods. The approach unifies compression and adaptation into a single framework, enabling faster and more memory-efficient training and inference by eliminating complex two-path computations. By leveraging learned sketching to compress each weight matrix row independently, SketchTune achieves superior performance across diverse tasks while using 2.6-3.6× smaller base models and comparable trainable parameters.

## Method Summary
SketchTune compresses each weight matrix row using a learned sketching matrix S, creating a compressed representation w_sketched that is fine-tuned while a binary mapping matrix M remains frozen for reconstruction. The method employs Hessian-weighted clustering to preserve influential parameters during sketch learning, using inverse Hessian diagonals to prioritize parameter preservation. Unlike quantized PEFT methods that require separate frozen weights and adapters, SketchTune stores only the compressed representation and mapping matrix, enabling single-path computation during inference. Custom CUDA kernels cache sketched parameters in shared memory for low-latency lookups, achieving 2.0-3.3× lower time-to-first-token and 1.6-2.7× less GPU memory usage compared to baselines.

## Key Results
- Achieves 14.48% higher GSM8K accuracy than LoftQ with 7.3× fewer trainable parameters
- Outperforms leading PEFT methods across diverse tasks including math problem-solving and commonsense reasoning
- Demonstrates 2.0-3.3× lower time-to-first-token and 1.6-2.7× less GPU memory usage compared to baselines
- Shows theoretical superiority of sketching over low-rank approximations for high-rank weight updates (rank >1000 needed for 75% variance in Llama-3-8B)

## Why This Works (Mechanism)

### Mechanism 1
Sketching approximates high-rank weight updates more accurately than low-rank decomposition under comparable compression. Each weight matrix row w ∈ R^(1×c) is compressed via a learned sketching matrix S ∈ R^(c×k) into w_sketched = wS. A binary mapping matrix M ∈ {0,1}^(k×c) reconstructs ŵ = w_sketched M on-the-fly. During fine-tuning, only w_sketched is updated; M remains frozen. The core assumption is that fine-tuned weight updates Δ = W' - W are high-rank (empirically shown: rank >1000 needed for 75% variance in Llama-3-8B, Qwen2.5-7B). Figure 1 shows sketching achieves lower normalized approximation error than low-rank matrices across most layers at 64× and 256× compression.

### Mechanism 2
Hessian-weighted clustering preserves influential parameters during sketch learning, minimizing output distortion. The learning objective weights parameters by (1/H^(-1)_(i,i))^s, emphasizing preservation of outliers in inverse Hessian diagonals. Weighted k-means produces cluster assignments; sketching matrix S entries incorporate these weights. Block-wise iterative mapping (B=128 columns) compensates for accumulated error via Cholesky-reformulated updates. The core assumption is that second-order Taylor expansion approximates loss change well; pre-trained LLMs have ∂L/∂w ≈ 0. Equation 13 shows loss error ε_i ∝ 1/H^(-1)_(i,i), motivating Hessian weighting.

### Mechanism 3
Unified compression-adaptation eliminates dual-path computation overhead, yielding faster inference and lower memory. Unlike quantized PEFT methods (QLoRA, LoftQ) that require frozen quantized weights + low-rank adapters computed separately, SketchTune stores only w_sketched and M. Forward pass computes y = w_sketched M X as a single operation. Custom CUDA kernels cache w_sketched in shared memory for low-latency lookups. The core assumption is that mapping matrix M (stored as ⌈log₂ k⌉-bit integers) can be efficiently indexed without significant overhead. Figure 3 shows 2.0-3.3× lower TTFT than LoRA/QLoRA and 1.6-2.7× less GPU memory.

## Foundational Learning

- **Concept: Sketching/Count-Sketch for Matrix Compression**
  - Why needed here: Understanding that sketching preserves approximate matrix properties (like multiplication results) while reducing dimensionality, distinct from rank reduction.
  - Quick check question: Given a matrix A and random projection S, how does ||ASx|| relate to ||Ax|| with high probability?

- **Concept: Low-Rank Matrix Approximation (SVD, Eckart-Young)**
  - Why needed here: Contrast with SketchTune—LoRA assumes Δ ≈ BA (rank r), but sketching makes no rank assumption.
  - Quick check question: If Δ has singular values following power-law σ²ᵢ ∝ i^(-η), what η range favors sketching over low-rank per Theorem 3.1?

- **Concept: Inverse Hessian Diagonal for Sensitivity Analysis**
  - Why needed here: Determines which weights most affect loss; SketchTune prioritizes preserving high-sensitivity weights during compression.
  - Quick check question: In optimal brain quantization, why does larger H^(-1)_(i,i) indicate higher parameter importance?

## Architecture Onboarding

- **Component map:**
  Input X → Linear Layer → [w_sketched (trainable)] × [M (frozen, INT2-4 indices)] → Reconstructed Ŵ → ŴX → Output Y

- **Critical path:**
  1. Pre-processing: Run LearnToSketch (Algorithm 1) with calibration data (128 sequences, 2048 tokens from C4) to generate w_sketched and M. This is one-time per base model (~35-267 min depending on size).
  2. Fine-tuning: Standard backprop on w_sketched; M frozen. Use AdamW, learning rates per Table 8/9.
  3. Inference: Load sketched model, run forward pass with reconstruction kernel.

- **Design tradeoffs:**
  - **GPR (groups per row):** Higher GPR = more trainable params, better accuracy, larger model size. Table 6 shows GPR=8 nearly matches full model perplexity but increases size.
  - **Data type (INT2/3/4):** Lower bits = smaller model but higher perplexity. INT2 fails on some tasks (Table 4).
  - **Sketching time vs. fine-tuning savings:** Large upfront cost for 70B models (4+ hours), amortized over multiple fine-tuning runs.

- **Failure signatures:**
  - High perplexity on WikiText-2 after sketching (>10 for INT2): bit-width too low.
  - Training divergence: learning rate too high or calibration data insufficient.
  - Slow inference without custom CUDA kernels: fallback to naive PyTorch operations.

- **First 3 experiments:**
  1. **Sanity check:** Sketch Llama-3.2-3B with INT4, GPR=4, measure WikiText-2 perplexity vs. FP16 baseline. Expect <1.5× degradation per Table 6.
  2. **Accuracy baseline:** Fine-tune sketched Llama-2-7B on Commonsense170K, compare average accuracy to LoRA. Use Table 2 settings (LR=8e-5, batch=64, epochs=2).
  3. **Efficiency profiling:** Measure TTFT and decoding latency on A100 with batch=1, context=4000. Compare to LoRA baseline; expect 2×+ speedup per Figure 3.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can SketchTune be effectively extended to compress and fine-tune token embedding layers and the final prediction head? The paper restricts compression scope to projection layers without demonstrating if sketching approximation error remains acceptable for the high-dimensional embedding space.
- **Open Question 2:** How sensitive is the learned mapping matrix M to the specific composition and size of the calibration dataset? The paper uses a fixed calibration set of 128 sequences from C4 but doesn't ablate the impact of different calibration domains or sequence lengths on sketch quality.
- **Open Question 3:** Does SketchTune maintain its efficiency and accuracy advantages over low-rank methods when applied to non-transformer architectures? Theoretical analysis and empirical evaluations focus exclusively on Transformer-based decoder models (Llama, Mistral), leaving uncertainty for other architectures like Mamba or CNNs.

## Limitations
- Sketch quality dependency on calibration data: The paper uses only 128 sequences (2048 tokens) from C4 for sketching, but no ablation shows how sensitive sketch quality is to calibration data size/quality.
- Hessian approximation accuracy: The method relies on inverse Hessian diagonals for parameter weighting, but the approximation quality for large models isn't validated.
- Custom kernel dependence: The claimed 2.0-3.3× speedup relies on custom CUDA kernels. Without these kernels, PyTorch implementations would reintroduce the dual-path overhead.

## Confidence
- **High confidence:** Sketching mechanism and its advantage over low-rank methods (supported by Figure 1 and theoretical backing in Section 3). Empirical performance claims on GSM8K, Commonsense170K, and WikiText-2 are well-documented with specific numbers.
- **Medium confidence:** Unified compression-adaptation efficiency claims. While TTFT and memory usage numbers are provided, the custom kernel dependency creates uncertainty about real-world reproducibility.
- **Low confidence:** Hessian-weighted clustering mechanism details. The paper references Zhang & Shrivastava (2016) but doesn't provide sufficient implementation specifics for faithful reproduction.

## Next Checks
1. **Sketch quality ablation:** Run LearnToSketch with varying calibration data sizes (16, 32, 64, 128 sequences) on Llama-3-8B and measure downstream fine-tuning accuracy degradation. This tests sensitivity to the one-time sketching cost.
2. **Hessian approximation validation:** For a 3B parameter model, compare the inverse Hessian-weighted parameter importance ranking against gradient-based importance (e.g., using Integrated Gradients). Verify that the top-k% parameters by both methods overlap significantly.
3. **Kernel-free performance:** Implement a pure PyTorch version of SketchTune (using einsum for reconstruction) and benchmark TTFT on A100 vs. the claimed 2.0-3.3× speedup. This isolates the custom kernel contribution to efficiency claims.