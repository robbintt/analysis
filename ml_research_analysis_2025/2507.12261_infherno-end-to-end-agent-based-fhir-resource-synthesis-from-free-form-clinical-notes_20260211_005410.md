---
ver: rpa2
title: 'Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form Clinical
  Notes'
arxiv_id: '2507.12261'
source_url: https://arxiv.org/abs/2507.12261
tags:
- fhir
- data
- clinical
- code
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Infherno addresses the challenge of transforming unstructured clinical
  notes into structured FHIR resources. It employs an agentic LLM framework with tool-calling
  capabilities, leveraging code execution and SNOMED CT terminology integration to
  generate syntactically valid FHIR output.
---

# Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form Clinical Notes

## Quick Facts
- arXiv ID: 2507.12261
- Source URL: https://arxiv.org/abs/2507.12261
- Reference count: 9
- Primary result: Agentic LLM framework achieves >85% exact match rate on synthetic German discharge letters for FHIR resource generation

## Executive Summary
Infherno is an agentic LLM framework designed to transform unstructured clinical notes into valid FHIR R4 Bundle resources. The system leverages a Thought-Code-Observation loop (ReAct framework) with external tool-calling for code execution and SNOMED CT terminology queries. Manual validation against human annotations on synthetic German discharge letters demonstrated that Infherno's predictions were mostly equivalent to or better than human baseline, with over 85% of items completely identical, particularly for crucial elements like patient identification and medication statements.

## Method Summary
The system uses a Gemini-2.5-Pro LLM agent orchestrated by the Smolagents framework, employing a ReAct-style loop to process clinical text. The agent generates Python code using the fhir.resources library for FHIR object creation and validation, while external tool-calls query a FHIR terminology server for SNOMED CT codes. The agent is constrained to generate only Patient, Condition, and MedicationStatement resources, with output validated through code execution and semantic checks against standardized terminologies.

## Key Results
- Achieved >85% exact match rate on crucial FHIR elements compared to human baseline
- Generated syntactically valid FHIR output through code execution validation
- Maintained semantic accuracy with <2.3% hallucination rate in SNOMED CT code selection
- Successfully processed synthetic German discharge letters through end-to-end extraction

## Why This Works (Mechanism)

### Mechanism 1: Code Execution for Schema Conformance
- Claim: Generating FHIR resources via executed Python code with a validation library reduces structural errors compared to direct JSON generation.
- Core assumption: The LLM agent can interpret Python exceptions and modify its code generation strategy to resolve the error in subsequent turns.
- Evidence anchors: Abstract mentions leveraging code execution for syntactically valid FHIR output; Section 3 describes error feedback from the library facilitating recovery from erroneous code.

### Mechanism 2: Terminology Tool-Calling for Semantic Grounding
- Claim: Using external tools to query a FHIR terminology server reduces semantic hallucination of clinical codes.
- Core assumption: The agent can correctly identify the clinical concept in the text and select the most appropriate code from the list returned by the search tool.
- Evidence anchors: Abstract mentions SNOMED CT terminology integration; Section 3 describes external function calls to FHIR terminology server for valid query responses.

### Mechanism 3: ReAct Framework for Decomposed Reasoning
- Claim: The Thought-Code-Observation loop improves handling of complex clinical text and FHIR schemas.
- Core assumption: The "thought" step effectively plans a correct and efficient sequence of actions and code generation.
- Evidence anchors: Section 3 explicitly states the approach follows the Thought-Code-Observation structure proposed as the ReAct framework by Yao et al. (2023).

## Foundational Learning

### Concept: HL7 FHIR (Fast Healthcare Interoperability Resources)
- Why needed here: This is the target output format. Understanding that FHIR resources are structured, typed JSON documents with defined fields (e.g., Patient, Condition) and references is essential.
- Quick check question: What is the standard container for a collection of FHIR resources in a document exchange context? (Answer: A FHIR Bundle of type collection or document).

### Concept: SNOMED CT
- Why needed here: It is the primary clinical terminology system used for encoding medical concepts in FHIR resources. The agent uses tools to find codes from this ontology.
- Quick check question: What does a SNOMED CT code represent in a FHIR Condition resource's code field? (Answer: The diagnosis, symptom, or problem).

### Concept: LLM Tool-Calling (Function Calling)
- Why needed here: The core of the agent's ability to interact with external systems (code interpreter, terminology server). The model outputs a structured call, which the system executes and returns as an observation.
- Quick check question: How does an LLM agent's ability to call a function improve its performance on a task requiring up-to-date external information? (Answer: It allows the LLM to query a live source and ground its response in current data, rather than relying on static internal knowledge).

## Architecture Onboarding

### Component map
LLM Agent (LiteLLM/Gemini-2.5-Pro) -> Smolagents Framework (orchestrates Thought->Code->Observation) -> Python Code Execution Environment (sandboxed interpreter with fhir.resources library) -> FHIR Terminology Server Tool (search_for_code_or_coding function) -> Output FHIR Bundle

### Critical path
Input Clinical Text -> LLM Agent (Thought: Plan extraction) -> Tool Call: Terminology Search -> LLM Agent (Code: Create FHIR objects) -> Code Execution -> Observation: Error or Success -> LLM Agent (Code: Bundle and finalize) -> Output FHIR Bundle

### Design tradeoffs
- Proprietary vs. Local Models: Supports both via LiteLLM. Used powerful proprietary model (Gemini-2.5-Pro) for long-context and performance, trading cost and data privacy for capability. Local models are less capable but offer privacy and lower inference cost.
- Code Generation vs. Constrained Decoding: Chose code generation with validation library over constrained decoding. Trades guaranteed syntactic validity of constrained decoding for flexibility and transparency of Python code, relying on execution loop for error correction.

### Failure signatures
- Tool Hallucination: Agent might attempt to call a tool that doesn't exist or with incorrect parameters.
- Code Execution Loop: Agent generates buggy Python, gets an error, and fails to correct it, looping until a turn limit is reached.
- Semantic Error: Code runs without error, but agent has encoded incorrect information from text or chosen wrong SNOMED code, leading to syntactically valid but semantically wrong FHIR resource.

### First 3 experiments
1. Run the Gradio demo: Clone the Infherno repository, install dependencies, and launch the app. Process one of the provided synthetic examples to observe the Thought-Code-Observation cycle in the Log Replay.
2. Modify the agent's prompt: Add a new, simple instruction to the system prompt (e.g., "always include a note in the MedicationStatement dosage text"). Rerun a sample and verify the agent follows the new instruction in its output.
3. Swap the LLM: Configure the system to use a different model supported by LiteLLM (e.g., a different version of GPT or a local model via Hugging Face). Compare the quality and number of steps required to generate a valid FHIR bundle for the same input text.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can smaller, locally deployable language models be fine-tuned to match the text-to-FHIR performance of state-of-the-art proprietary models like Gemini-2.5-Pro?
- Basis in paper: [explicit] The authors explicitly list the "fine-tuning of smaller language models" as a direction for future work and acknowledge they could not assess open-weights models due to hardware constraints.
- Why unresolved: The study relied exclusively on the proprietary Gemini-2.5-Pro model; testing smaller, open models requires distinct hardware and fine-tuning resources.
- What evidence would resolve it: Comparative benchmarks showing open-weight models achieving high structural conformity and semantic accuracy on the text-to-FHIR task without proprietary APIs.

### Open Question 2
- Question: How does Infherno's performance generalize to real-world clinical notes compared to the synthetic German discharge letters used in the evaluation?
- Basis in paper: [inferred] The authors note in the Limitations section that synthetic data "may not fully capture the complexities and loose structuring often found in genuine clinical documentation."
- Why unresolved: Privacy regulations and data use agreements prevented the authors from utilizing real-world clinical notes for the evaluation dataset.
- What evidence would resolve it: Validation results from applying Infherno to authentic, non-synthetic clinical datasets (e.g., MIMIC-IV notes) reviewed by domain experts.

### Open Question 3
- Question: Does the agent's efficacy and latency degrade significantly when expanding the scope to support the full breadth of FHIR resource types?
- Basis in paper: [explicit] The authors identify the "integration of more FHIR resource types" as future work, noting that expanding beyond Patient, Condition, and MedicationStatement increases prompt verbosity.
- Why unresolved: The current evaluation was intentionally scoped to only three resource types to manage complexity and context window usage.
- What evidence would resolve it: An evaluation of structural conformity and runtime latency when generating complex resource bundles (e.g., containing Observations, Procedures, and DiagnosticReports).

## Limitations
- Dependency on functioning external FHIR terminology server with SNOMED CT content requires appropriate licensing and technical infrastructure
- Manual validation process is inherently subjective and may not scale to larger datasets or different clinical note formats
- Exact system prompt and hyperparameters not fully specified, limiting complete reproduction

## Confidence

- **High Confidence**: The core mechanism of using code execution for schema validation is well-supported by the paper's results and the design of the fhir.resources library. The claim that this approach catches syntactic errors is directly evidenced by the observed reduction in structural issues.

- **Medium Confidence**: The effectiveness of the terminology tool-calling mechanism for reducing semantic hallucinations is plausible and logically sound, but the paper provides limited quantitative evidence specifically isolating the impact of this feature from the overall agent performance.

- **Medium Confidence**: The ReAct framework's contribution to handling complex clinical text and FHIR schemas is a reasonable architectural choice, but the paper does not provide a direct comparison against alternative agentic frameworks or a non-agentic baseline to definitively prove its superiority for this specific task.

## Next Checks

1. Execute the Gradio Demo: Clone the Infherno repository and run the provided Gradio application with one of the synthetic German discharge letter examples. Observe the complete Thought-Code-Observation cycle in the Log Replay to verify the agent's step-by-step reasoning and interaction with the code execution and terminology search tools.

2. Validate Schema Conformance: Process a set of clinical notes through the agent and programmatically verify that all generated FHIR resources are syntactically valid by parsing them with the fhir.resources library. Check that no resources fail validation, confirming the code execution mechanism is effectively catching structural errors.

3. Test Terminology Tool Accuracy: Manually inspect the agent's tool calls to the search_for_code_or_coding function for a sample of clinical concepts. Verify that the agent is querying the terminology server correctly and selecting appropriate SNOMED CT codes from the returned results, ensuring the semantic grounding mechanism is functioning as intended.