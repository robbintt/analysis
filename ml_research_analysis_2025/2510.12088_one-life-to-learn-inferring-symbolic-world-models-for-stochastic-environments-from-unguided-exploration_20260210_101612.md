---
ver: rpa2
title: 'One Life to Learn: Inferring Symbolic World Models for Stochastic Environments
  from Unguided Exploration'
arxiv_id: '2510.12088'
source_url: https://arxiv.org/abs/2510.12088
tags:
- state
- world
- player
- action
- laws
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ONELIFE, a framework that learns symbolic
  world models from a single, unguided episode in a complex, stochastic environment.
  It represents world dynamics as a mixture of modular, conditionally-activated programmatic
  laws within a probabilistic programming framework, enabling efficient learning by
  routing inference and optimization only through relevant laws.
---

# One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration

## Quick Facts
- arXiv ID: 2510.12088
- Source URL: https://arxiv.org/abs/2510.12088
- Authors: Zaid Khan, Archiki Prasad, Elias Stengel-Eskin, Jaemin Cho, Mohit Bansal
- Reference count: 40
- One-line primary result: Learns symbolic world models from a single unguided episode in stochastic environments, outperforming baseline on 16/23 Crafter-OO scenarios

## Executive Summary
ONELIFE introduces a framework for learning symbolic world models from minimal interaction—specifically, a single unguided episode in a stochastic environment. The method represents world dynamics as a mixture of modular, conditionally-activated programmatic laws within a probabilistic programming framework, enabling efficient learning by routing inference and optimization only through relevant laws. This approach is evaluated on Crafter-OO, a reimplementation of the Crafter environment with an exposed object-oriented state, using a new evaluation protocol measuring state ranking and state fidelity. The framework successfully learns key environment dynamics from minimal interaction and demonstrates utility for planning through forward simulation.

## Method Summary
The ONELIFE framework learns symbolic world models from a single unguided episode by representing world dynamics as a mixture of modular, conditionally-activated programmatic laws within a probabilistic programming framework. It uses a continuous relaxation of discrete world states and employs a mixture-of-experts structure where each law is only optimized and sampled if its relevance exceeds a threshold. The inference process is designed to be efficient by routing computation only through laws relevant to the current trajectory, avoiding full-model updates. The framework is evaluated on Crafter-OO, which provides an object-oriented symbolic state representation, and uses a novel evaluation protocol based on state ranking and state fidelity metrics to assess model performance.

## Key Results
- ONELIFE learns key environment dynamics from a single unguided episode in Crafter-OO
- Outperforms a strong baseline on 16 out of 23 scenarios tested
- Demonstrates planning capability through forward simulation to identify superior strategies

## Why This Works (Mechanism)
The framework works by decomposing complex stochastic dynamics into a set of modular, conditionally-activated programmatic laws. Each law captures a specific environmental mechanism and is only optimized and sampled when relevant to the current trajectory, making inference computationally efficient. The probabilistic programming framework allows for principled uncertainty quantification over both law parameters and relevance scores, enabling the system to handle stochasticity and learn from limited data. By representing dynamics as symbolic programs rather than neural networks, the framework can generalize better to unseen states and support interpretable planning.

## Foundational Learning
**Probabilistic Programming**: A framework that combines probabilistic inference with programming language constructs, allowing users to express complex generative models and perform inference on them. *Why needed*: Enables principled uncertainty quantification and inference over symbolic world models. *Quick check*: Can the framework express conditional dependencies between variables and perform Bayesian inference?

**Program Synthesis**: The task of automatically generating programs that satisfy a given specification. *Why needed*: Allows the system to learn symbolic representations of environmental dynamics from observed trajectories. *Quick check*: Can the system generate programs that correctly predict outcomes of previously unseen state transitions?

**Mixture-of-Experts Models**: A model architecture where multiple expert networks (or in this case, programmatic laws) are combined, with a gating mechanism determining which experts are activated for each input. *Why needed*: Enables efficient learning by only optimizing laws relevant to the current trajectory. *Quick check*: Does the model show computational efficiency gains compared to updating all laws for every transition?

## Architecture Onboarding

**Component Map**: Observable Extractor -> State Ranker -> Law Inference Engine -> Planner
- Observable Extractor maps raw states to primitive values
- State Ranker evaluates the quality of predicted states
- Law Inference Engine learns and optimizes programmatic laws
- Planner uses forward simulation for decision-making

**Critical Path**: Law Inference Engine is the core component where symbolic laws are learned and refined through probabilistic inference. This component takes observed trajectories and outputs a distribution over possible world models.

**Design Tradeoffs**: The framework trades off expressiveness for interpretability by using symbolic programs rather than neural networks. This choice enables better generalization and planning but requires access to structured symbolic states rather than raw observations. The modular, conditionally-activated structure improves computational efficiency but adds complexity to the inference process.

**Failure Signatures**: The model may struggle with mechanics requiring deep "tech tree" progression due to exploration limitations. It also relies on pre-defined object-oriented states, making it unsuitable for environments where only raw visual inputs are available. The single-episode constraint means the model cannot learn from repeated interactions or corrections.

**First Experiments**:
1. Verify that the Law Inference Engine can correctly learn simple deterministic laws from short trajectories
2. Test the State Ranker's ability to distinguish between high-fidelity and low-fidelity state predictions
3. Evaluate the Planner's performance on simple planning tasks with known optimal solutions

## Open Questions the Paper Calls Out

**Open Question 1**: Can exploration policies be developed to efficiently discover mechanics requiring deep "tech tree" progression without human guidance?
- Basis: Appendix F.1 states that "exploration remains a significant bottleneck" and the policy "often struggles to progress through the environment’s technology tree."
- Why unresolved: The current LLM-based policy frequently fails to satisfy preconditions for advanced items, limiting discovery of complex mechanics.
- What evidence would resolve it: An autonomous agent that consistently discovers and synthesizes laws for high-level crafting recipes in Crafter-OO or similar environments.

**Open Question 2**: Can ONELIFE infer symbolic world models directly from raw visual inputs (pixels) rather than relying on pre-defined object-oriented states?
- Basis: The framework relies on Crafter-OO because it "exposes a structured, object-oriented symbolic state," acknowledging this as a limitation in other complex environments.
- Why unresolved: The method assumes an "observable extractor" that maps complex states to primitive values, a component that must be hand-defined or replaced by a perception module.
- What evidence would resolve it: Successful application of the probabilistic law inference mechanism to environments like Atari using only pixel observations.

**Open Question 3**: How can the framework be extended to support continual, lifelong learning across multiple episodes ("lives")?
- Basis: The paper defines the task as learning from a "single episode," but doesn't address updating the model when the agent restarts or encounters the environment again.
- Why unresolved: The current gradient-based inference optimizes laws based on a single trajectory; it's unclear if the model can update or discard laws as it gathers contradictory or superior evidence over successive lives.
- What evidence would resolve it: A modification of ONELIFE that demonstrates improved state ranking accuracy over sequential episodes by retaining robust laws and refining noisy ones.

## Limitations
- Evaluation primarily focuses on a single environment (Crafter-OO), raising concerns about generalizability to other stochastic domains
- Reliance on pre-defined object-oriented states rather than raw observations limits applicability to real-world environments
- Single-episode constraint prevents learning from repeated interactions or corrections

## Confidence
- High confidence: Core technical contribution of learning symbolic world models from single episodes
- Medium confidence: Claims about efficiency gains from modular, conditionally-activated laws
- Low confidence: Claims about scalability and applicability to truly open-ended, real-world stochastic environments

## Next Checks
1. Evaluate ONELIFE on at least three additional stochastic environments with varying complexity levels, including environments with more complex state spaces and longer time horizons, to assess generalizability.
2. Conduct ablation studies specifically isolating the benefits of the modular, conditionally-activated law structure by comparing against alternative architectures that don't employ this feature.
3. Test the learned models on more challenging planning tasks that require handling longer-term dependencies and more complex goal specifications than those presented in the current evaluation.