---
ver: rpa2
title: 'Finding Fantastic Experts in MoEs: A Unified Study for Expert Dropping Strategies
  and Observations'
arxiv_id: '2504.05586'
source_url: https://arxiv.org/abs/2504.05586
tags:
- expert
- experts
- similarity
- pruning
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the problem of inefficiency in sparse mixture-of-experts
  (SMoE) models due to expert redundancy and high memory requirements. The core method,
  MoE Experts Compression Suite (MC-Suite), provides a comprehensive benchmark for
  estimating expert importance from four perspectives: weight dynamics, inference
  behavior, intermediate activations, and gradient properties.'
---

# Finding Fantastic Experts in MoEs: A Unified Study for Expert Dropping Strategies and Observations

## Quick Facts
- **arXiv ID**: 2504.05586
- **Source URL**: https://arxiv.org/abs/2504.05586
- **Reference count**: 40
- **Primary result**: Activation- and gradient-guided importance criteria achieve up to 3x better performance than one-shot pruning, with iterative pruning plus finetuning enabling expert-level sparsification of SMoE models while maintaining downstream task performance.

## Executive Summary
This work addresses the problem of inefficiency in sparse mixture-of-experts (SMoE) models due to expert redundancy and high memory requirements. The core method, MoE Experts Compression Suite (MC-Suite), provides a comprehensive benchmark for estimating expert importance from four perspectives: weight dynamics, inference behavior, intermediate activations, and gradient properties. It also introduces iterative pruning with task-agnostic fine-tuning ("MoE Lottery Subnetworks") to improve expert dropping outcomes. The primary results show that activation- and gradient-guided importance criteria achieve the best performance, with iterative pruning plus finetuning enabling up to 3x better performance compared to one-shot pruning. At 50% expert sparsity, MoE Lottery Subnetworks maintain strong downstream task performance, especially when augmented with k-shot examples or supervised fine-tuning, recovering instruction-following capabilities.

## Method Summary
The study proposes a two-part approach: MC-Suite benchmarks 16 expert importance criteria across 4 perspectives (weights, inference behavior, activations, gradients) to identify optimal dropping strategies, finding activation- and gradient-guided criteria like Min-EAN and Min-EGE perform best. MoE Lottery Subnetworks implements iterative estimate-prune-finetune cycles where experts are removed in rounds (typically 1 per layer per round), followed by brief task-agnostic finetuning to recalibrate router load distribution. This process uses a calibration dataset (C4 validation, 256 samples, seq_len=2048) to estimate importance scores, then prunes and finetunes iteratively to produce sparse subnetworks that maintain language modeling quality (measured via C4 perplexity) and downstream task performance while reducing memory requirements to ≤0.55× at 50% sparsity with speedups ≥1.27×.

## Key Results
- Activation- and gradient-guided importance criteria (Min-EAN, Min-EGE) achieve superior performance over conventional criteria by considering both input tokens and model parameters
- Iterative pruning with task-agnostic finetuning produces ~3x better performance compared to one-shot pruning, with iterative pruning alone achieving ~2x superior performance
- At 50% expert sparsity, pruned subnetworks maintain strong downstream task performance, especially when augmented with k-shot examples or supervised fine-tuning to recover instruction-following capabilities
- Expert-level sparsification achieves up to 3.6% average performance gains and 16.2% improvement on specific tasks compared to state-of-the-art LLM pruning methods

## Why This Works (Mechanism)

### Mechanism 1: Multi-Perspective Importance Estimation Identifies Redundant Experts
Estimating expert importance from multiple perspectives identifies better candidates for dropping than single-criterion approaches. MC-Suite evaluates 16 criteria across 4 perspectives (weights, inference behavior, activations, gradients). Activation- and gradient-guided criteria that consider both input tokens and model parameters outperform simpler criteria like usage frequency or weight similarity. Experts demonstrating low information content (via entropy) and low activation/gradient norms are redundant and removable with minimal impact.

### Mechanism 2: Iterative Estimate-Prune-Finetune Recalibrates Router Load Distribution
Iterative pruning with task-agnostic finetuning produces better subnetworks than one-shot pruning. Instead of removing all target experts at once, the process removes experts in k rounds. After each removal round, brief task-agnostic finetuning (next-token prediction) recalibrates router weights and rebalances load across remaining experts. Importance scores are re-estimated for the new configuration, allowing the router to adapt to load redistribution through brief finetuning.

### Mechanism 3: Instruction-Following Restoration via External Augmentation
Expert dropping primarily damages instruction-following capabilities rather than pretraining knowledge; these can be restored through k-shot examples or SFT. Pruned subnetworks retain reasoning and pretraining knowledge but lose instruction-following ability. External support through in-context examples or supervised finetuning on instruction-tuning data can recover this capability, sometimes enabling pruned models to match or exceed dense models on certain tasks.

## Foundational Learning

- **Mixture-of-Experts (MoE) Architecture**: Understanding how MoE layers route tokens to subsets of experts via a gating function is essential to grasp why expert dropping affects performance and how importance estimation works. Quick check: Can you explain how Top-k routing selects experts and how expert outputs are combined?

- **Lottery Ticket Hypothesis**: The MoE Lottery Subnetwork method is explicitly motivated by this hypothesis—the iterative prune-finetune approach assumes valuable sparse subnetworks exist within dense models. Quick check: What does the lottery ticket hypothesis claim about sparse subnetworks in trained neural networks?

- **Perplexity as Language Model Quality Metric**: The paper extensively uses perplexity on C4/Wikitext to evaluate language modeling quality of pruned subnetworks before downstream task evaluation. Quick check: Does lower or higher perplexity indicate better language model performance, and what does it measure?

## Architecture Onboarding

- **Component map**: MC-Suite (16 importance criteria in 4 categories) -> MoE Lottery Subnetwork Pipeline (iterative estimate-prune-finetune) -> Router Modification (weight matrix adjustment) -> Recovery Mechanisms (k-shot examples or SFT)

- **Critical path**: 1) Select calibration dataset (C4 validation, 256 samples, seq_len=2048) 2) Choose importance criterion (Min-EAN or Min-EGE) 3) Run iterative pruning (k rounds, removing S/k% experts per round) 4) Apply task-agnostic finetuning between rounds (~0.2M-1M tokens) 5) For downstream tasks, optionally augment with k-shot or SFT

- **Design tradeoffs**: Uniform vs non-uniform per-layer dropping (paper uses uniform to avoid bottleneck layers); Base vs Instruct checkpoint (works better on Base models); Finetuning extent (benefits saturate after ~1M training tokens)

- **Failure signatures**: One-shot pruning causes abrupt performance drops and severe load imbalance; Dropping high vocabulary coverage experts causes significant degradation; Non-uniform per-layer dropping creates compute bottleneck layers

- **First 3 experiments**: 1) Implement Min-EAN and Min-EGE on Mixtral-8×7B, compare perplexity at 25%/50% sparsity vs random dropping 2) Run both iterative and one-shot approaches with Min-EAN, measure perplexity gap and analyze load distribution 3) Take 50% pruned subnetwork, evaluate zero-shot vs 5-shot vs SFT on MMLU and BoolQ to validate instruction-following restoration

## Open Questions the Paper Calls Out

- **Open Question 1**: Can instruction-following abilities be disentangled from pretraining knowledge within the parameters of SMoE experts? The study observes that instruction-following is disproportionately impacted by dropping experts, but the specific parameter regions storing this capability versus factual knowledge remain unidentified. A parameter attribution method that successfully ablates or recovers instruction-following independent of pretraining knowledge would resolve this.

- **Open Question 2**: Do MC-Suite dropping strategies generalize to diverse MoE architectures, such as those with fine-grained or shared experts? The optimal importance criteria (e.g., activation norm) were identified for standard top-k routing; their effectiveness on architectures where experts are smaller or explicitly designated as "shared" is unknown. Benchmarking on DeepSeek-MoE or Switch Transformer would verify if optimal pruning recipes change.

- **Open Question 3**: Can MoE training objectives be modified to improve expert dropping efficiency without sacrificing specialization? There is a tension between training experts to be specialized (high vocab coverage) and the need for redundancy to enable safe dropping. A training regularization technique that enforces functional redundancy across specialized experts would resolve this.

## Limitations
- **Downstream Task Generalization Uncertainty**: Performance gains are modest (up to 3.6% average) and may not generalize beyond the 7 tested tasks
- **Evaluation Scope Constraints**: Results are limited to Mixtral-8×7B architecture and specific calibration datasets, potentially limiting generalizability
- **Iterative Process Practical Barriers**: Multiple rounds of pruning and finetuning required, with computational requirements potentially prohibitive for larger models or higher sparsity targets

## Confidence

- **High Confidence**: Multi-perspective importance estimation identifies better dropping candidates; activation- and gradient-guided criteria outperform simpler metrics; one-shot pruning causes more severe performance degradation than iterative approaches
- **Medium Confidence**: Expert dropping primarily damages instruction-following rather than pretraining knowledge; this can be recovered through k-shot examples or supervised fine-tuning; iterative pruning with task-agnostic finetuning produces substantially better subnetworks
- **Low Confidence**: Instruction-following knowledge is disentangled from pretraining knowledge in SMoE architectures; the specific 3× performance improvement will scale to other model families; recovery mechanisms will work equivalently across diverse downstream tasks

## Next Checks

1. **Cross-Architecture Validation**: Apply MC-Suite and MoE Lottery Subnetworks to at least two additional SMoE models with different expert counts and routing mechanisms to test generalizability of the importance criteria and iterative pruning benefits

2. **Long-Tail Task Evaluation**: Evaluate pruned subnetworks on tasks specifically designed to test instruction-following capabilities (e.g., complex reasoning chains, multi-step instructions) to better understand the fundamental limitations of expert dropping

3. **Dynamic Load Balancing Analysis**: Conduct ablation studies removing the task-agnostic finetuning component to quantify how much of the performance gain comes from router recalibration versus expert importance re-estimation