---
ver: rpa2
title: 'ExChanGeAI: An End-to-End Platform and Efficient Foundation Model for Electrocardiogram
  Analysis and Fine-tuning'
arxiv_id: '2503.13570'
source_url: https://arxiv.org/abs/2503.13570
tags:
- data
- exchangeai
- fine-tuning
- platform
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ExChanGeAI is an open-source end-to-end platform streamlining ECG
  data loading, preprocessing, visualization, and fine-tuning of machine learning
  models. It features a novel foundation model, CardX, pre-trained on over one million
  ECGs, offering efficient, privacy-preserving fine-tuning.
---

# ExChanGeAI: An End-to-End Platform and Efficient Foundation Model for Electrocardiogram Analysis and Fine-tuning

## Quick Facts
- arXiv ID: 2503.13570
- Source URL: https://arxiv.org/abs/2503.13570
- Authors: Lucas Bickmann; Lucas Plagwitz; Antonius Büscher; Lars Eckardt; Julian Varghese
- Reference count: 40
- Key outcome: ExChanGeAI platform with CardX foundation model (15M params, 201M FLOPS) outperforms larger models on 7/9 ECG tasks while being more efficient

## Executive Summary
ExChanGeAI is an open-source end-to-end platform streamlining ECG data loading, preprocessing, visualization, and fine-tuning of machine learning models. It features a novel foundation model, CardX, pre-trained on over one million ECGs, offering efficient, privacy-preserving fine-tuning. The platform supports multiple formats and scales from personal computers to high-performance servers. Evaluations on internal and external datasets demonstrate CardX's superior robustness and computational efficiency, achieving comparable or better performance than larger models with significantly fewer parameters (15M vs 90M) and lower computational demands (201M vs 14G FLOPS). ExChanGeAI democratizes advanced ECG analysis by enabling users to empirically select the most suitable model for their tasks without extensive technical expertise.

## Method Summary
ExChanGeAI implements an end-to-end platform for ECG analysis featuring CardX, a foundation model pre-trained on 1.2 million ECGs from six diverse datasets. CardX employs a Mixture of Architectures (MoA) within a Mixture of Experts (MoE) framework, combining XceptionTime, InceptionTime, XResNet, and Transformer architectures. The model uses Rlign transformation for time-aligned reconstruction pre-training with Maximum Mean Discrepancy loss. During fine-tuning, users can select head-only or full-network updates using AdamW optimizer with learning rate finder and ExponentialLR scheduler. The platform supports multi-format ECG input, standardized preprocessing (100Hz resampling, mV scaling, baseline wander removal, outlier clipping), and exports trained models to ONNX for deployment.

## Key Results
- CardX achieves 15M parameters and 201M FLOPS vs ECG-FM's 90M parameters and 14G FLOPS
- CardX outperforms ECG-FM on 7/9 benchmark tasks while maintaining lower computational complexity
- CardX demonstrates superior robustness with lowest IQR (.123) and second-lowest CV (.358) across external datasets
- De novo trained XceptionTime and InceptionTime models outperform fine-tuned CardX on 7/9 and 8/9 tasks respectively in data-constrained scenarios

## Why This Works (Mechanism)

### Mechanism 1
Sparse Mixture of Experts (MoE) with heterogeneous architectures may achieve parameter-efficient generalization by routing different ECG patterns to specialized expert networks. The CardX encoder implements a Mixture of Architectures (MoA) where multiple architectures (XceptionTime, InceptionTime, XResNet, Transformer) serve as experts. A convolutional router selects top-k experts per input via sparse activation (k=2), combined with Gumbel-Softmax annealing and load-balancing losses to prevent expert dropout. This allows the model to delegate different waveform morphologies to architecture-specialized experts.

Core assumption: ECG classification tasks exhibit heterogeneous performance patterns across architectures, such that no single architecture is optimal for all diagnostic categories. Assumption: Sparse routing generalizes without overfitting to router-selection shortcuts.

Evidence anchors:
- [Section 4.5]: "The architecture introduces a Mixture of Architectures (MoA) concept, utilizing diverse expert architectures within the MoE framework... The rationale behind the MoA approach stems from the observed performance heterogeneity across different ECG classes and the data-dependent performance annealing of these architectures."
- [Section 2, Table 1]: CardX achieves 15M parameters and 201M FLOPS vs. ECG-FM's 90M parameters and 14G FLOPS, with CardX outperforming ECG-FM on 7/9 tasks.
- [Corpus]: Related work (EnECG, arXiv:2511.22935) explores ensemble approaches for ECG foundation models, suggesting architectural diversity is an active research direction. Direct validation of MoA routing decisions remains limited in current corpus.

Break condition: If downstream tasks require uniform routing across all experts (low entropy), the MoA overhead provides no benefit. If router overfits to dataset-specific artifacts rather than signal morphology, sparse activation may amplify domain shift.

### Mechanism 2
R-peak-aligned reconstruction pre-training (Rlign) may improve downstream classification by structuring latent representations around cardiac cycle morphology rather than absolute temporal position. CardX pre-trains on Rlign-transformed ECGs, which align R-peaks and resample inter-beat segments to standardized intervals. This reconstruction task (with MSE loss and MMD on latent space) forces the encoder to learn features invariant to heart rate and recording start time, while preserving P-wave, QRS complex, and T-wave morphology within fixed windows.

Core assumption: Cardiac diagnostic information is primarily encoded in relative waveform morphology within beats, not absolute timing. Assumption: Time-aligned reconstruction does not discard clinically relevant rate-dependent features.

Evidence anchors:
- [Section 4.5]: "The Rlign transformation focuses on aligning R-peaks and resampling segments between them, effectively structuring the cyclic nature of ECG signals... it ensures that the model learns from consistently structured cardiac cycles, explicitly emphasizing diagnostically relevant features like P-waves, QRS complexes, and T-waves."
- [Section 4.5]: "Our approach achieves a 33% reduction in embedding size (512 variables) compared to a direct competitor (768 variables)."
- [Corpus]: Corpus does not contain comparative studies of Rlign vs. alternative pre-training objectives. External validation is limited.

Break condition: If downstream tasks require rate-variability information (e.g., arrhythmia detection dependent on RR intervals), Rlign's beat-level normalization may discard discriminative signal. If reconstruction quality is poor on noisy ECGs, latent representations may encode artifacts.

### Mechanism 3
Multi-source pre-training across heterogeneous datasets may improve robustness to distribution shift, but fine-tuning on limited target data does not guarantee superiority over de novo training. CardX is pre-trained on six datasets (PTB-XL, Chapman, Georgia, MIMIC-IV-ECG, SPH, Deepfake ECGs; >1M total ECGs) with standardized 100Hz resampling and mV scaling. This exposes the model to device-level batch effects, age/sex distributions, and labeling conventions. During fine-tuning on PTB-XL fold 9, the model adapts to target labels via head-only or full-network updates.

Core assumption: Pre-training diversity transfers to external datasets via shared latent structure. Assumption: Fine-tuning with limited data (single fold) is sufficient for domain adaptation without catastrophic forgetting.

Evidence anchors:
- [Section 4.4]: "To ensure robustness against [batch effects], CardX is trained on a diverse collection of six ECG datasets."
- [Section 2, Table 1]: CardX achieves lowest IQR (.123) and second-lowest CV (.358) across external datasets (Yang et al., MIMIC-IV, EDMS), indicating robustness. However, de novo XceptionTime and InceptionTime outperform fine-tuned CardX on 7/9 and 8/9 tasks respectively.
- [Section 3]: "While pre-trained models offer potential, their benefits are not guaranteed in data-constrained scenarios. Untrained architectures can be surprisingly effective."
- [Corpus]: Benchmarking ECG Foundational Models (arXiv:2509.25095) corroborates that foundation model generalization is task-dependent and not uniformly superior.

Break condition: If pre-training and target datasets share minimal latent structure, fine-tuning may underfit compared to task-specific architectures trained from scratch. If target task labels are semantically mismatched with pre-training objectives (e.g., revascularization prediction not present in pre-training), transfer may be negligible.

## Foundational Learning

- **Concept: Transfer Learning vs. De Novo Training Trade-offs**
  - **Why needed here:** The paper demonstrates that pre-trained models (CardX, ECG-FM, DSAIL SNU) do not uniformly outperform architectures trained from scratch (XceptionTime, InceptionTime) in data-constrained settings. Understanding when transfer helps vs. hurts is critical for deployment decisions.
  - **Quick check question:** Given a target task with <1000 labeled ECGs, what empirical protocol should determine whether to use a pre-trained foundation model versus training an InceptionTime model from scratch?

- **Concept: Sparse MoE Routing and Load Balancing**
  - **Why needed here:** CardX uses noisy top-k routing with route-entropy and load-balance losses to prevent expert collapse. Understanding these dynamics is necessary for debugging why certain experts may be underutilized or why routing fails on OOD inputs.
  - **Quick check question:** During fine-tuning on a new dataset, how would you detect if the router is collapsing to a single expert or overfitting to dataset-specific artifacts?

- **Concept: ECG Preprocessing Standardization**
  - **Why needed here:** The platform normalizes all inputs to 100Hz via FFT resampling, mV scaling (1000 ADC gain), baseline wander removal (200ms moving median), and outlier clipping (0.01 percentile). These choices directly affect model inputs and may not be optimal for all tasks.
  - **Quick check question:** If your downstream task requires high-frequency features (e.g., late potentials detection), how would the 100Hz downsampling affect performance, and what modifications would be required?

## Architecture Onboarding

- **Component map:**
  - Data Ingestion Layer: Multi-format readers (CSV, NPY, DICOM, MAT, DAT, XML) → standardized 12-lead, 10-second, 100Hz, mV-scaled tensors
  - Preprocessing Pipeline: FFT resampling → baseline wander removal (200ms moving median) → outlier clipping → optional Rlign transformation for time-aligned beats
  - Model Zoo: XceptionTime (401K params), InceptionTime (457K params), DSAIL SNU (2M params, adapted for ONNX), CardX (15M params, MoA encoder), ECG-FM (90M params, external)
  - Fine-tuning Engine: ONNX→PyTorch conversion (via onnx2torch) → AdamW optimizer with learning rate finder → ExponentialLR scheduler (γ=0.9) → early stopping on validation loss
  - Inference/Export: Trained models exported to ONNX format for cross-platform deployment via Model ExChanGe (WebDav file-server)

- **Critical path:**
  1. Load ECG data (verify 12-lead structure; check sampling rate ≥100Hz for meaningful downsampling)
  2. Inspect label distribution via UI bar chart; confirm sufficient samples per class (paper uses PTB-XL fold 9 with ~700 samples across superclasses)
  3. Select base model: if data <1000 samples, consider InceptionTime from scratch or CardX fine-tuning; compare via cross-validation
  4. Run fine-tuning with default settings (max 50 epochs, batch size auto-determined, learning rate finder active)
  5. Evaluate on held-out external dataset (not just intra-dataset fold) before deployment
  6. Export model via ONNX; test inference on target hardware

- **Design tradeoffs:**
  - Head-only vs. full-network fine-tuning: Head-only is faster and less prone to overfitting with very limited data; full-network may capture task-specific features but requires more data. Paper tests both but does not provide systematic guidance.
  - Model size vs. computational budget: CardX (201M FLOPS) enables deployment on resource-constrained hardware vs. ECG-FM (14G FLOPS). Trade-off: smaller models may underperform on rare diagnostic classes.
  - Pre-training diversity vs. domain specificity: Multi-source pre-training improves robustness but may dilute signal for highly specialized tasks (e.g., revascularization prediction required task-specific training on EDMS dataset).

- **Failure signatures:**
  - Router collapse: If >90% of inputs route to a single expert during fine-tuning, check load-balance loss weighting; may require increased noise in top-k routing or rebalancing of pre-training data.
  - Performance degradation on external datasets: If intra-dataset F1 >0.85 but external F1 <0.5, suspect overfitting to dataset-specific artifacts (e.g., device calibration, labeling conventions). Consider data augmentation or domain adversarial training.
  - ONNX conversion errors: If custom PyTorch operators fail during export (e.g., unsupported pooling ops per Opset ≤18), verify compatibility with onnx2torch or fall back to PyTorch-only deployment.

- **First 3 experiments:**
  1. Baseline replication: Train XceptionTime and InceptionTime from scratch on PTB-XL fold 9; evaluate on fold 10 and at least one external dataset (MIMIC-IV subset). Compare F1, IQR, and CV against paper's Table 1 to validate platform setup.
  2. CardX fine-tuning ablation: Fine-tune CardX with head-only vs. full-network settings on your target task; measure performance gap and training time. Determine optimal strategy for your data regime.
  3. Robustness stress test: Evaluate all available models on a held-out external dataset with known distribution shift (e.g., different device manufacturer or patient population). Report IQR and CV; identify which model exhibits lowest variance across conditions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: At what specific volume of downstream training samples does the CardX foundation model consistently outperform de novo trained architectures like XceptionTime?
- Basis: [inferred] The Discussion notes that de novo models frequently beat CardX in the tested "data-constrained scenarios" (limited data), but anticipates performance gains as data availability increases.
- Why unresolved: The evaluation only utilized a single stratified fold (Fold 9) for fine-tuning, providing a static snapshot of performance in a low-data regime rather than a scaling curve.
- What evidence would resolve it: A systematic analysis of model performance across a spectrum of training set sizes (e.g., n=100, 500, 1000, 5000) on the same downstream task.

### Open Question 2
- Question: Does the Mixture of Architectures (MoA) approach yield statistically significant improvements over the best single expert architecture within the ensemble?
- Basis: [inferred] The Methods section introduces the MoA design to address performance heterogeneity across ECG classes, but the Results only report aggregate CardX performance.
- Why unresolved: The paper lacks an ablation study comparing the full MoA model against its individual components (e.g., XceptionTime or InceptionTime experts) in isolation.
- What evidence would resolve it: Evaluation results comparing the MoA model against identical models trained with fixed routing to specific expert architectures.

### Open Question 3
- Question: Is the Rlign-based pre-training objective superior to standard masked or contrastive pre-training methods for ECG foundation models?
- Basis: [inferred] The Methods describe a novel pre-training task using Rlign transformation (time-aligned reconstruction), asserting it reduces computational load and improves representations.
- Why unresolved: The study benchmarks CardX against other foundation models (ECG-FM) but does not isolate the specific contribution of the Rlign pre-training strategy versus other pre-training techniques.
- What evidence would resolve it: A comparative study of CardX variants pre-trained on the same data using Rlign, masking, and contrastive objectives, evaluated on identical downstream tasks.

## Limitations

- **Transfer learning efficacy uncertainty:** The paper demonstrates CardX's computational efficiency and robustness, but also reveals that pre-trained foundation models do not uniformly outperform architectures trained from scratch in data-constrained settings. The lack of systematic guidance on when to prefer transfer learning versus de novo training remains a practical limitation for users.
- **Architectural transparency limitations:** Key hyperparameters for the XResNet and Transformer expert architectures within CardX are not specified, limiting full reproducibility and architectural understanding.
- **External validation constraints:** The EDMS dataset is not publicly available, preventing complete validation of reported robustness scores across all external test sets.
- **MoA routing validation gaps:** While the paper claims performance heterogeneity across architectures justifies the Mixture of Architectures approach, direct empirical validation of the routing mechanism's effectiveness and its resistance to overfitting remains limited.

## Confidence

- **High confidence:** Claims regarding CardX's computational efficiency (15M parameters, 201M FLOPS) and comparative FLOPS analysis versus ECG-FM (90M parameters, 14G FLOPS) are well-supported by explicit numerical comparisons in Table 1.
- **Medium confidence:** Claims about CardX's superior robustness (lowest IQR and CV across external datasets) are supported by Table 1 results, though the incomplete external validation limits full confidence.
- **Low confidence:** Claims regarding the superiority of the Mixture of Architectures approach and Rlign preprocessing are based on theoretical rationale and limited empirical evidence, with insufficient comparative validation against alternative methods.

## Next Checks

1. **Transfer learning protocol validation:** Conduct a systematic comparison of fine-tuning CardX versus training InceptionTime from scratch on a target task with <1000 labeled ECGs, measuring both performance and training efficiency to establish clear guidance for data-constrained scenarios.

2. **MoA routing analysis:** During fine-tuning, monitor the entropy of expert routing distributions and the utilization rates of individual experts across different ECG classes to detect potential router collapse or overfitting to dataset-specific artifacts.

3. **Rlign preprocessing impact:** Evaluate the effect of the 100Hz resampling and Rlign transformation on high-frequency ECG features (e.g., late potentials) by comparing model performance with and without these preprocessing steps on tasks known to depend on high-frequency signal components.