---
ver: rpa2
title: Unlearning Algorithmic Biases over Graphs
arxiv_id: '2505.14945'
source_url: https://arxiv.org/abs/2505.14945
tags:
- unlearning
- graph
- latexit
- random
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a training-free post-processing framework that
  leverages graph unlearning to mitigate algorithmic bias in pre-trained graph ML
  models. The approach introduces a novel node feature unlearning setting with certified
  removal guarantees that scale sublinearly with training set size, along with degree-aware
  structural unlearning strategies for edges and nodes informed by formal bias analyses.
---

# Unlearning Algorithmic Biases over Graphs

## Quick Facts
- arXiv ID: 2505.14945
- Source URL: https://arxiv.org/abs/2505.14945
- Reference count: 40
- Primary result: Training-free post-processing framework achieves up to 75% reduction in fairness metrics with 10×-20× faster runtime vs retraining

## Executive Summary
This work introduces a training-free post-processing framework that leverages graph unlearning to mitigate algorithmic bias in pre-trained graph ML models. The approach combines feature unlearning based on correlation with sensitive attributes, degree-aware structural unlearning for edges and nodes, and certified removal guarantees via single-step Newton updates. By targeting bias-propagating graph components without retraining, the method achieves significant fairness improvements while maintaining model utility and offering substantial computational efficiency gains.

## Method Summary
The method employs a linear Simple Graph Convolution (SGC) model with L2-regularized logistic loss, trained using L-BFGS optimization. Bias mitigation occurs through three unlearning strategies: feature unlearning that removes top-k features highly correlated with sensitive attributes, structural unlearning that removes intra-edges incident to low-degree nodes, and node unlearning based on degree ratios. Each strategy uses a single-step Newton update to adjust model weights without retraining, providing certified removal guarantees. The approach scales sublinearly with training set size and maintains strong utility-fairness trade-offs.

## Key Results
- Achieves up to 75% reduction in fairness metrics (ΔSP, ΔEO) without retraining
- Provides 10×-20× faster runtime compared to retraining from scratch
- Maintains strong utility-fairness trade-offs across multiple benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1: Feature Unlearning
Removing features highly correlated with sensitive attributes reduces statistical parity bias without retraining. The method identifies top-k features with highest absolute Pearson correlation to the sensitive attribute, zeros them out in aggregated representations, and updates model weights via Newton step. Core assumption: algorithmic bias is partially driven by correlation between nodal features and sensitive attributes.

### Mechanism 2: Structural Unlearning
Unlearning intra-edges (connecting same-class nodes) from low-degree nodes mitigates bias amplification better than random removal. The algorithm prioritizes intra-edges incident to low-degree nodes using bias scores, removes them, and performs structural unlearning update. Core assumption: homophily causes denser connectivity within sensitive groups, and bias links to inter-to-intra edge ratios.

### Mechanism 3: Certified Removal
A closed-form single-step Newton update approximates retraining-from-scratch with sub-linear computational cost. The method computes gradient differences and Hessian inverse to shift weights, satisfying certified removal bounds. Core assumption: loss function is convex and twice differentiable, ensuring unique minimizer and stable Hessian inverse.

## Foundational Learning

**Simple Graph Convolution (SGC):** Linear model (Z = P^L X) required for theoretical guarantees; non-linearities would break analytical Hessian derivation. Quick check: Does the model require differentiable, convex loss for unlearning bounds to hold? (Yes).

**Group Fairness (Statistical Parity):** Feature selection minimizes ΔSP (difference in positive prediction rates). Quick check: If optimizing for Equal Opportunity instead, would Theorem 3 still apply? (Likely requires modification).

**Certified Removal (Differential Privacy context):** Method provides (ε, δ) guarantees, meaning output distribution is statistically close to retrained model. Quick check: What does ε represent in context of output model? (Distance/divergence between unlearned and retrained weight distributions).

## Architecture Onboarding

**Component map:** Pre-processor (G, s → ρ, b_e) -> Selector (ρ, b_e → F_u, E_u) -> Updater (w*, removal → w̃)

**Critical path:** 1) Calculate sensitive correlations (ρ) for all features 2) Calculate edge bias scores (b_e) based on homophily/degree 3) Apply Newton update w̃ = w* + H^(-1)Δ to linear layer

**Design tradeoffs:** Batching vs. guarantees - unlearning edge mini-batches is faster but loses certified guarantees. Linearity vs. expressiveness - restricted to linear models to ensure strong convexity for Hessian inverse.

**Failure signatures:** Utility collapse from removing too many predictive features. Numerical instability from ill-conditioned Hessian inversion.

**First 3 experiments:** 1) Baseline verification: Compare Newton update weights against fully retrained model 2) Ablation on selection: Compare bias-aware vs random feature/edge selection 3) Scalability test: Measure runtime vs retraining as node count N scales

## Open Questions the Paper Calls Out
None

## Limitations
- Approach highly dependent on linearity assumption of SGC models; breaks down with non-linear activation functions
- Method assumes convex loss functions which may not hold for all graph-based classification tasks
- Certification guarantees rely on exact Hessian computation and inversion, which can be numerically unstable for large graphs

## Confidence

**High Confidence:** Pearson correlation for bias-propagating features and single-step Newton update for weight adjustment are well-established techniques.

**Medium Confidence:** Structural unlearning strategies for edges and nodes are supported by formal bias analyses but require careful implementation.

**Low Confidence:** Scalability claims (10×-20× speedup) are based on experimental results; performance on extremely large graphs or different bias metrics remains uncertain.

## Next Checks

1. **Non-linear Model Extension:** Test feature unlearning strategy on non-linear GNN model (e.g., GCN with ReLU) to assess robustness when theoretical guarantees don't apply.

2. **Ablation Study on Selection Criteria:** Compare proposed bias-aware selection against random selection, focusing on trade-off between fairness improvement and accuracy degradation.

3. **Numerical Stability Analysis:** Investigate sensitivity to regularization parameter λ by varying across orders of magnitude and measuring impact on certification guarantees and model performance.