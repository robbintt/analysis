---
ver: rpa2
title: 'SP-MCQA: Evaluating Intelligibility of TTS Beyond the Word Level'
arxiv_id: '2510.26190'
source_url: https://arxiv.org/abs/2510.26190
tags:
- speech
- sp-mcqa
- evaluation
- accuracy
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SP-MCQA (Spoken-Passage Multiple-Choice Question
  Answering), a novel framework for evaluating the intelligibility of TTS systems
  by assessing the accuracy of key information in synthesized speech, rather than
  relying on traditional word-level metrics like WER. The authors propose SP-MCQA-Eval,
  an 8.76-hour news-style benchmark dataset containing challenging content with proper
  nouns, digits, and complex structures to reflect real-world speech variability.
---

# SP-MCQA: Evaluating Intelligibility of TTS Beyond the Word Level

## Quick Facts
- arXiv ID: 2510.26190
- Source URL: https://arxiv.org/abs/2510.26190
- Reference count: 0
- Authors: Hitomi Jin Ling Tee; Chaoren Wang; Zijie Zhang; Zhizheng Wu
- Key outcome: This paper introduces SP-MCQA (Spoken-Passage Multiple-Choice Question Answering), a novel framework for evaluating the intelligibility of TTS systems by assessing the accuracy of key information in synthesized speech, rather than relying on traditional word-level metrics like WER.

## Executive Summary
This paper introduces SP-MCQA (Spoken-Passage Multiple-Choice Question Answering), a novel framework for evaluating the intelligibility of TTS systems by assessing the accuracy of key information in synthesized speech, rather than relying on traditional word-level metrics like WER. The authors propose SP-MCQA-Eval, an 8.76-hour news-style benchmark dataset containing challenging content with proper nouns, digits, and complex structures to reflect real-world speech variability. The evaluation involves human annotators answering multiple-choice questions based on the synthesized speech, with accuracy measured as SP-MCQA ACC. Experiments with four state-of-the-art TTS models (FishSpeech, MaskGCT, F5-TTS, and CosyV oice 2) reveal significant gaps between WER and key-information accuracy, with CosyV oice 2 achieving the highest SP-MCQA ACC (90.39%) despite not having the lowest WER. The results highlight that even SOTA TTS models struggle with phonetic accuracy, text normalization, and handling uncommon patterns, emphasizing the need for high-level evaluation criteria that go beyond word-level metrics. This work underscores the importance of robust text normalization and key-information accuracy for developing more human-like TTS systems.

## Method Summary
The SP-MCQA framework evaluates TTS systems by measuring human comprehension of key information in synthesized speech. The method involves constructing SP-MCQA-Eval, an 8.76-hour benchmark dataset from NPR news, filtered for paragraphs containing at least three digits and two uppercase letters. Multiple-choice questions are generated using GPT-4o-mini for each paragraph, with answer options including correct responses, phonetic distractors, semantic distractors, structural distractors, and "Other." Human annotators (40 total, native speakers or IELTS ≥8.0) answer these questions based on synthesized speech, with accuracy measured as SP-MCQA ACC. The framework is tested on four state-of-the-art TTS models: FishSpeech, MaskGCT, F5-TTS, and CosyV oice 2, comparing SP-MCQA ACC against traditional metrics like WER, S-SIM, and DNSMOS P.835.

## Key Results
- SP-MCQA ACC reveals significant gaps with WER: CosyV oice 2 achieves highest SP-MCQA ACC (90.39%) despite mid-tier WER (9.04%), while FishSpeech has lowest WER (5.73%) but worst SP-MCQA ACC (81.19%)
- Text normalization failures cause systematic key-information loss: FishSpeech drops content mid-sentence ("Ala." → " "), causing "no data mentioned" errors despite low WER
- Phonetic errors on uncommon patterns are the primary failure mode: Phonetic errors are most prevalent across all systems (3.2–4.1% of total questions), exceeding semantic and structural errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Word Error Rate does not predict key-information accuracy in synthesized speech.
- Mechanism: WER operates at the lexical level, counting substitution/deletion/insertion errors uniformly. It cannot distinguish whether a mispronounced token was semantically critical (e.g., "2,000" → "2") or trivial. SP-MCQA exposes this by testing comprehension of specific facts—dates, names, numbers—rather than transcript fidelity.
- Core assumption: Listeners weight key information more heavily than peripheral words in comprehension tasks.
- Evidence anchors:
  - [abstract] "low WER does not necessarily guarantee high key-information accuracy, exposing a gap between traditional metrics and practical intelligibility"
  - [Section 4.4, Table 3] FishSpeech achieves lowest WER (5.73%) but worst SP-MCQA ACC (81.19%); CosyVoice 2 achieves highest SP-MCQA ACC (90.39%) despite mid-tier WER (9.04%)
  - [corpus] Neighboring paper "WER is Unaware" confirms WER fails to capture clinical understanding distortions in ASR contexts
- Break condition: If WER correlates strongly (>0.8) with SP-MCQA ACC across diverse TTS architectures, the mechanism would weaken.

### Mechanism 2
- Claim: Text normalization failures cause systematic key-information loss in TTS output.
- Mechanism: TTS frontends must expand abbreviations ("Ala." → "Alabama"), normalize numbers, and handle proper nouns. Weak normalization causes silent drops or incorrect expansions. This is upstream of acoustic generation—errors here cannot be recovered later.
- Core assumption: Text normalization is architecture-agnostic; all TTS systems face similar input irregularities.
- Evidence anchors:
  - [Section 4.4, Table 5] FishSpeech drops content mid-sentence ("Ala." → " "), causing "no data mentioned" errors despite low WER
  - [Section 4.4] CosyVoice 2 correctly expands all tested abbreviations, correlating with highest SP-MCQA ACC
  - [corpus] Limited direct corpus evidence on text normalization specifically; neighboring papers focus on acoustic/prosodic evaluation
- Break condition: If models with identical normalization pipelines show divergent SP-MCQA performance, other factors dominate.

### Mechanism 3
- Claim: Phonetic errors on uncommon patterns are the primary failure mode for key-information accuracy.
- Mechanism: Training data underrepresents rare proper nouns, multi-digit numbers, and atypical orthography. Models revert to phonetically similar but incorrect productions ("Alala" instead of "Alabama"), which WER may tolerate but SP-MCQA penalizes when the answer option requires exact recognition.
- Core assumption: Error types are not uniformly distributed; phonetic errors disproportionately affect key information.
- Evidence anchors:
  - [Section 4.4, Table 4] Phonetic errors are most prevalent across all systems (3.2–4.1% of total questions), exceeding semantic and structural errors
  - [Section 4.4, Table 5] MaskGCT produces "Alala" for "Alabama"; F5-TTS alternates between "Ala" and "Alabama"
  - [corpus] "Intelligibility of TTS Systems for Mathematical Expressions" confirms domain-specific inputs (math) reveal intelligibility gaps not captured by standard metrics
- Break condition: If phonetic error rates drop below semantic/structural error rates while SP-MCQA gaps persist, other mechanisms dominate.

## Foundational Learning

- Concept: Word Error Rate (WER)
  - Why needed here: The baseline metric the paper argues against; understanding its computation (edit distance normalized by reference length) clarifies why it cannot weight semantic importance.
  - Quick check question: Given reference "The meeting is at 3 PM" and hypothesis "The meeting is at 3 AM," what is the WER, and does it reflect the semantic error severity?

- Concept: Text Normalization in TTS Frontends
  - Why needed here: Explains why models with strong acoustic modeling still fail on key information; normalization is a symbolic preprocessing stage that LLM-based systems may handle differently.
  - Quick check question: How would a TTS frontend expand "Dr. Smith arrived at 5:30 PM on Jan. 3rd"? What ambiguities exist?

- Concept: Autoregressive vs. Non-Autoregressive TTS Architectures
  - Why needed here: The paper compares AR (FishSpeech, CosyVoice 2) and NAR (F5-TTS, MaskGCT) models, noting NAR models show more "hallucination" errors. Understanding this distinction helps interpret Table 4's error-type distribution.
  - Quick check question: Why might NAR models be more prone to content omissions or insertions compared to AR models?

## Architecture Onboarding

- Component map:
  - SP-MCQA-Eval Dataset: 8.76 hours of NPR news audio, filtered for paragraphs with ≥3-digit numbers and ≥2 uppercase letters (excluding sentence-initial), manually segmented into 5,805 utterances
  - MCQ Generator: GPT-4o-mini creates 2–10 questions per paragraph with four answer types (correct, phonetic distractor, semantic distractor, structural distractor, "Other")
  - Human Evaluation Pipeline: 40 annotators (native or IELTS ≥8.0), 2–4 annotators per task, golden-test filtering
  - TTS Inference: 5,805 prompt–target pairs, one prompt per speaker, evaluated on four SOTA models

- Critical path:
  1. Dataset construction (audio cleaning → UVR for music removal → WhisperX for ASR/timestamps → RegEx filtering for 3+ digit numbers and 2+ uppercase letters → pyannote diarization → NLTK sentence splitting → manual segmentation)
  2. MCQ generation (GPT-4o-mini prompt → 4-option questions with error-type distractors)
  3. Human evaluation (annotator assignment → response collection → SP-MCQA ACC computation)

- Design tradeoffs:
  - Subjective evaluation (human annotators) vs. scalability: Authors note manual effort as a limitation; future work proposes Audio LLMs for automation
  - News-style content vs. domain breadth: NPR corpus emphasizes proper nouns/dates; may not generalize to technical/medical domains
  - Four-option MCQ vs. open-ended: Constrains responses but may miss error types not anticipated in distractor design

- Failure signatures:
  - FishSpeech: Mid-sentence word drops due to normalization issues → low SP-MCQA ACC despite lowest WER
  - CosyVoice 2: Sentence-ending noise artifacts ("-nine" sounds) → slightly elevated WER but highest SP-MCQA ACC
  - F5-TTS: Overly fast speech rate (~1.75×) → recognition difficulty on key information
  - MaskGCT: Odd sounds and proper-noun mispronunciations → moderate performance on both metrics

- First 3 experiments:
  1. Reproduce SP-MCQA ACC on a held-out subset using the provided dataset and annotation protocol to validate pipeline implementation.
  2. Ablate text normalization: Manually correct normalization outputs for FishSpeech (e.g., expand abbreviations) and re-measure SP-MCQA ACC to quantify normalization's contribution.
  3. Cross-domain test: Apply SP-MCQA framework to a technical domain (e.g., medical dictation with drug names/dosages) to assess generalization beyond news content.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can Audio Large Language Models (LLMs) serve as reliable, scalable substitutes for human annotators in the SP-MCQA evaluation framework?
- **Basis in paper:** [explicit] The conclusion states future work will explore leveraging Audio LLMs to address the limitation of "substantial manual effort required for human evaluation."
- **Why unresolved:** Human evaluation is currently the gold standard for the subjective SP-MCQA ACC metric, but it is resource-intensive and slow; automated alternatives have not yet been validated for this specific key-information task.
- **What evidence would resolve it:** A high correlation study comparing Audio LLM performance against human SP-MCQA ACC scores on the benchmark.

### Open Question 2
- **Question:** Does the observed gap between Word Error Rate (WER) and key-information accuracy generalize to languages other than English?
- **Basis in paper:** [explicit] The authors explicitly aim to "extend this work to other languages to provide clearer guidance for improving multilingual TTS systems."
- **Why unresolved:** The current study is restricted to English NPR news data; text normalization and phonetic challenges vary significantly across linguistic structures.
- **What evidence would resolve it:** Results from applying the SP-MCQA framework to a multilingual dataset with diverse text normalization rules (e.g., logographic vs. alphabetic scripts).

### Open Question 3
- **Question:** To what extent does fine-tuning TTS models on data rich in irregular patterns (e.g., proper nouns, digits) reduce the phonetic and structural errors identified in the SP-MCQA evaluation?
- **Basis in paper:** [inferred] The authors attribute high phonetic error rates to the "scarcity of irregular or uncommon patterns in training data" and suggest addressing these distinct error patterns in future research.
- **Why unresolved:** While the error source is identified, the efficacy of data-centric solutions for improving key-information accuracy remains untested.
- **What evidence would resolve it:** A comparative study of SOTA models before and after fine-tuning on a targeted corpus of irregular-text utterances, measured via SP-MCQA ACC.

## Limitations

- Human evaluation scalability: The framework requires substantial manual effort from human annotators, making it resource-intensive and slow for large-scale deployment.
- Domain specificity: The SP-MCQA-Eval dataset is limited to news-style content with proper nouns and digits, raising questions about generalizability to other domains (e.g., technical or conversational speech).
- GPT-4o-mini dependency: The MCQ generation relies on GPT-4o-mini, but the prompt template is not specified, potentially introducing variability if re-implemented.

## Confidence

- High: WER and SP-MCQA ACC are weakly correlated across tested TTS models; this is directly observable from Table 3.
- Medium: Text normalization failures are the primary driver of key-information loss; supported by error analysis but not experimentally isolated.
- Medium: Phonetic errors on uncommon patterns disproportionately affect key information; consistent with error-type distribution but not proven causal.
- Low: SP-MCQA framework generalizes robustly beyond news content; untested in other domains.

## Next Checks

1. Reproduce SP-MCQA ACC on a held-out subset of the dataset using the provided annotation protocol to validate the pipeline implementation and inter-annotator agreement.
2. Ablate text normalization: Manually correct normalization outputs for FishSpeech (e.g., expand abbreviations like "Ala." → "Alabama") and re-measure SP-MCQA ACC to quantify normalization's contribution to key-information accuracy gaps.
3. Cross-domain test: Apply SP-MCQA framework to a technical domain (e.g., medical dictation with drug names/dosages) to assess whether the framework generalizes beyond news-style content and identifies similar failure modes.