---
ver: rpa2
title: 'BenchMake: Turn any scientific data set into a reproducible benchmark'
arxiv_id: '2506.23419'
source_url: https://arxiv.org/abs/2506.23419
tags:
- data
- benchmake
- sets
- testing
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Benchmark data sets are essential for machine learning but rare
  in computational science due to data complexity and domain specificity. BenchMake
  is a new tool that converts any scientific data set into a reproducible benchmark
  by identifying challenging edge cases using non-negative matrix factorization on
  the convex hull.
---

# BenchMake: Turn any scientific data set into a reproducible benchmark

## Quick Facts
- arXiv ID: 2506.23419
- Source URL: https://arxiv.org/abs/2506.23419
- Authors: Amanda S Barnard
- Reference count: 40
- Key outcome: BenchMake converts any scientific dataset into a reproducible benchmark by identifying challenging edge cases using NMF on the convex hull

## Executive Summary
BenchMake is a tool that transforms any scientific dataset into a reproducible benchmark by deterministically identifying challenging edge cases through Non-negative Matrix Factorization (NMF) on the convex hull. Unlike traditional train/test splits that may be biased or arbitrary, BenchMake ensures reproducible partitions across different users and runs while maximizing distributional divergence between training and testing sets. The method works across diverse data modalities including tabular, graph, image, signal, and textual data, making it particularly valuable for computational science where domain-specific benchmark datasets are rare.

## Method Summary
BenchMake deterministically partitions scientific datasets into training and testing sets by first ordering data via stable MD5 hashing, then applying Min-Max scaling, followed by NMF decomposition to identify archetypal profiles on the convex hull. The algorithm computes Euclidean distances between all data instances and these archetypes, selecting the nearest unique instance for each archetype to form the test set. This unsupervised approach ensures reproducible splits while maximizing distributional divergence (measured by KL, JS, Wasserstein, MMD divergences and statistical tests) between train and test sets. The tool operates in GPU or CPU mode and handles various data modalities through appropriate preprocessing pipelines.

## Key Results
- BenchMake outperforms established and random splits in 8 of 10 tested datasets
- BenchMake achieves lower p-values and higher divergence metrics across multiple statistical tests
- The method maintains model performance while creating more challenging test sets
- BenchMake is domain-agnostic and works across tabular, graph, image, signal, and textual modalities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BenchMake identifies challenging edge cases by computing archetypes on the convex hull via NMF, then matching real data instances to these extremes.
- Mechanism: NMF decomposes the non-negative data matrix X into W × H, where H represents archetypal profiles constrained to lie on the convex hull—the smallest convex set containing all data points. Euclidean distances between each data instance and each archetype are computed, and the closest unique instance per archetype is selected for the test set.
- Core assumption: Edge cases on the convex hull are more challenging for models and better expose limitations than representative interior points.
- Evidence anchors:
  - [abstract] "BenchMake uses non-negative matrix factorisation to deterministically identify and isolate challenging edge cases on the convex hull"
  - [Section 2.1] "NMF approximates: X ≈ W × H... H (dimensions k × n) represents the archetypal profiles"
  - [Section 2.2] "selecting the closest unique instance for each archetype"
- Break condition: If data is uniformly distributed without meaningful extremes, or if archetypes do not correspond to semantically meaningful boundaries, test sets may not be more challenging.

### Mechanism 2
- Claim: Deterministic ordering via stable hashing ensures reproducible splits across runs and users.
- Mechanism: Each data row is hashed using MD5 to produce a stable sort order. Given identical input data, the same ordering is guaranteed regardless of original row sequence. When matching instances to archetypes, ties are broken by selecting the smaller index in this hashed-sorted order.
- Core assumption: Hash-based ordering preserves sufficient diversity for benchmark creation.
- Evidence anchors:
  - [Section 2] "Input data is ordered deterministically via stable hashing"
  - [Section 2.1] "In the rare event of an exact tie, the instance with the smaller index (in the hashed-and-sorted ordering) is selected"
- Break condition: If data contains duplicate rows that hash identically, ordering among duplicates is undefined but selection remains deterministic.

### Mechanism 3
- Claim: BenchMake splits maximize distributional divergence between training and test sets, producing more statistically distinct partitions than random or domain-specific splits.
- Mechanism: By selecting edge cases for testing, the test set distribution diverges from the training set. Seven metrics (KS test, T-test, MI, KL divergence, JS divergence, Wasserstein distance, MMD) quantify this divergence. BenchMake splits showed lower p-values and higher divergence metrics in 8/10 datasets tested.
- Core assumption: Higher divergence between train/test distributions yields more rigorous evaluation while remaining valid for model assessment.
- Evidence anchors:
  - [abstract] "BenchMake outperforms established and random splits in 8 of 10 tested data sets, with lower p-values and higher divergence metrics"
  - [Section 2.3] "distributional divergence (a measure of dissimilarity) should be high"
- Break condition: If divergence is too extreme, test set may no longer represent the task domain, making performance metrics uninformative for real-world deployment.

## Foundational Learning

- Concept: Non-negative Matrix Factorization (NMF)
  - Why needed here: Core algorithm for archetype extraction; understanding how X ≈ W × H decomposes data is essential for interpreting outputs.
  - Quick check question: Given a 1000×50 data matrix, what are the dimensions of W and H if k=10 archetypes are requested?

- Concept: Convex Hull
  - Why needed here: Defines the boundary from which archetypes are selected; understanding this explains why edge cases are prioritized.
  - Quick check question: In 2D, if data points form a circle, what shape is the convex hull?

- Concept: Distributional Divergence Metrics (KL, JS, Wasserstein)
  - Why needed here: Used to evaluate split quality; understanding these metrics helps interpret benchmark comparisons.
  - Quick check question: Why is JS divergence preferred over KL divergence for comparing two empirical distributions?

## Architecture Onboarding

- Component map: Data input → modality encoding → Min-Max scaling → NMF decomposition → distance matrix computation → nearest-instance selection → partition output

- Critical path: Data input → modality-specific loaders (tabular → NumPy, images → flattened vectors, graphs → encoded features, signals → flattened, sequential → one-hot/count vectors) → MD5 stable hashing → sorted ordering → GPU (CuPy) or CPU fallback (joblib parallelization) → scipy.optimize.nnls iterative optimization → W, H matrices → Euclidean distance calculation → archetype-to-instance mapping → Train/test indices or split arrays

- Design tradeoffs:
  - O(n² × d) complexity: accurate but computationally intensive for large datasets
  - Unsupervised operation: avoids label leakage but may miss label-relevant splits
  - Static partitioning: reproducible but must re-run if data changes

- Failure signatures:
  - GPU memory overflow on large batches → automatic CPU fallback triggered
  - NMF non-convergence → check for negative values in preprocessed data
  - Identical statistical metrics to random splits → data may lack meaningful structure (e.g., text without sequential ordering)

- First 3 experiments:
  1. Run BenchMake on German Credit dataset with test fraction 0.2; verify KS-test p-value is lower than random split baseline (Table 1: 0.6811 vs 0.93±0.01).
  2. Compare BenchMake vs scaffold split on MOLHIV; confirm KL divergence increases (Table 2: 2.9858 vs 0.0096 for OGB).
  3. Test modality handling by applying to MedMNIST images; verify flattened vectors produce expected divergence metrics without visual artifacts in reconstructed splits.

## Open Questions the Paper Calls Out
None

## Limitations
- The core mechanism relies on convex hull archetypes being semantically meaningful edge cases, but the paper provides limited validation that these archetypes correspond to truly challenging instances rather than arbitrary boundary points
- NMF hyperparameters and archetype-to-test-set size mapping are underspecified, potentially affecting reproducibility
- The claim of "domain-agnostic" operation may overstate capabilities since text/sequential data showed poor divergence performance

## Confidence

- **High:** The deterministic hashing mechanism and reproducible partitioning process are well-specified and verifiable.
- **Medium:** The divergence-based evaluation methodology is sound, though the interpretation of high divergence as universally beneficial requires domain context.
- **Low:** The semantic meaningfulness of archetypes and their correspondence to challenging test cases lacks empirical validation beyond statistical metrics.

## Next Checks

1. Verify archetype interpretability by visualizing NMF outputs on 2D synthetic datasets with known boundary structures.
2. Test sensitivity to NMF initialization and hyperparameter settings across multiple runs on the same dataset.
3. Compare BenchMake's edge-case selection against human-annotated difficult instances in domain-specific benchmarks.