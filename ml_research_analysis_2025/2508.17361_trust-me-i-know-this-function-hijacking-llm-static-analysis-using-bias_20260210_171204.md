---
ver: rpa2
title: 'Trust Me, I Know This Function: Hijacking LLM Static Analysis using Bias'
arxiv_id: '2508.17361'
source_url: https://arxiv.org/abs/2508.17361
tags:
- code
- llms
- pattern
- fpas
- patterns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We demonstrate that large language models suffer from an abstraction\
  \ bias that causes them to overgeneralize familiar code patterns and overlook small\
  \ bugs embedded within them. This enables a new class of attacks\u2014Familiar Pattern\
  \ Attacks (FPAs)\u2014which can hijack an LLM\u2019s perceived control flow without\
  \ affecting actual runtime behavior."
---

# Trust Me, I Know This Function: Hijacking LLM Static Analysis using Bias

## Quick Facts
- arXiv ID: 2508.17361
- Source URL: https://arxiv.org/abs/2508.17361
- Authors: Shir Bernstein; David Beste; Daniel Ayzenshteyn; Lea Schonherr; Yisroel Mirsky
- Reference count: 40
- Primary result: FPAs achieve over 80% success on basic models and up to 97% on reasoning models

## Executive Summary
This paper introduces Familiar Pattern Attacks (FPAs), a novel class of adversarial attacks that exploit large language models' abstraction bias when analyzing code. The attack works by embedding small bugs within familiar programming patterns, causing models to overgeneralize and overlook these bugs during static analysis. FPAs can hijack an LLM's perceived control flow without affecting actual runtime behavior, achieving high success rates across multiple model families, programming languages, and real-world codebases.

## Method Summary
The paper presents an automated, black-box algorithm that generates FPAs by identifying familiar code patterns and injecting minimal perturbations (bugs) that preserve the pattern's semantic appearance to the LLM while altering runtime behavior. The attack exploits the gap between static prediction (LLM interpretation) and dynamic execution (actual code behavior), achieving transferability across different model architectures and remaining effective even when models are explicitly warned about such attacks.

## Key Results
- FPAs achieve over 80% success rates on basic models and up to 97% on reasoning models
- Attacks transfer reliably across model families (GPT-4o, Claude, Gemini) without requiring model access
- FPAs remain effective even when models are explicitly warned about familiar pattern attacks
- The technique works across multiple programming languages including Python, C, and Go
- Defensive applications demonstrated include anti-plagiarism and web scraping resistance

## Why This Works (Mechanism)

### Mechanism 1: Semantic Overgeneralization via Abstraction Bias
LLMs prioritize high-level structural recognition over local instruction execution when processing code, causing them to "skip" verification of familiar patterns. During pretraining, models develop internal representations for common algorithms, and when attention mechanisms detect familiar scaffolds, they activate memorized behavioral signatures rather than performing line-by-line execution traces.

### Mechanism 2: Semantics-Preserving Perturbation (Deception Patterns)
Adversaries can alter runtime control flow while keeping LLM interpretation fixed by exploiting the gap between model classification and actual code logic. The attack constructs Deception Patterns where runtime execution differs from static prediction, using minimal perturbations that are small enough to avoid breaking familiarity signals but distinct enough to alter deterministic outputs.

### Mechanism 3: Cross-Model Transferability of Bias
FPAs transfer across model families because the underlying abstraction bias is a property of training data distribution and architecture paradigm, not just a single model's quirk. Different LLMs trained on similar code corpora likely internalize similar structural priors for common algorithms, making perturbations that exploit this shared blind spot universally effective.

## Foundational Learning

- **Static vs. Dynamic Analysis in LLMs:** Understanding the gap between static prediction (what LLM thinks code does) and dynamic execution (what code actually does) is crucial because the attack fundamentally relies on this discrepancy. Quick check: Can you explain why an LLM acting as static analyzer might predict different output than actually running function in Python interpreter?

- **Inductive Bias & Pattern Completion:** Transformers are probabilistic pattern matchers rather than rule-based engines. The attack works because models rely on their prior (familiar pattern) rather than evidence (specific bug). Quick check: If model sees function named `is_vowel`, does it verify logic inside or predict output based on training association with "vowel check"?

- **Black-Box Adversarial Attacks:** The paper demonstrates attacks don't need model weight access, only query ability. Transferability implies vulnerability is data-driven rather than architecture-specific. Quick check: Why does transferability of attacks (attacking Model A to hurt Model B) imply vulnerability is data-driven?

## Architecture Onboarding

- **Component map:** Target (x) -> Familiar Pattern (P) -> Perturbation (Δ) -> Deception Pattern (P') -> Control Flow Hijack
- **Critical path:** 1) Generation: Select familiar pattern P; 2) Injection: Apply perturbation Δ to create P'; 3) Verification: Ensure exec(P') ≠ exec(P) but f(P') ≈ f(P); 4) Deployment: Embed conditional hijack logic into target code
- **Design tradeoffs:** Stealth vs. Effectiveness - larger perturbations make attack more reliable but might break familiarity illusion; Reasoning Models - harder to fool with simple FPAs, more expensive to generate but more potent
- **Failure signatures:** LLM correctly identifies bug (abstraction bias fails); generated code P' is syntactically invalid; logic doesn't compile; target behavior t affects runtime obviously
- **First 3 experiments:** 1) Reproduce Prime Example from Section V-C with `nth_prime` bug; 2) Test Language Universality by translating Python FPA to C or Go; 3) Adaptive Defense Test with "Robust System Prompt" from Appendix C

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness may vary significantly in codebases with non-standard implementations or extensive defensive coding practices
- Abstraction bias assumes sufficient overlap between pretraining corpora and target codebases, which may not hold for specialized or proprietary code
- Claims about FPAs being "more stealthy than traditional obfuscation" would benefit from broader adversarial analysis

## Confidence

- **High Confidence:** Core mechanism of FPAs and ability to exploit abstraction bias is well-supported by experimental results and theoretical framework
- **Medium Confidence:** Defensive applications show promise but require more extensive real-world testing
- **Medium Confidence:** While transferability is demonstrated, extent of generalization across all LLM-based static analysis tools remains somewhat uncertain

## Next Checks
1. **Cross-Architectural Validation:** Test FPAs against wider range of LLM architectures including smaller models, domain-specific models, and models with different pretraining strategies to validate whether abstraction bias is truly universal or architecture-dependent.

2. **Defensive Code Pattern Analysis:** Systematically analyze which code patterns are most vulnerable to FPAs and develop catalog of defensive patterns that break abstraction bias without affecting functionality to quantify practical limitations.

3. **Real-World Deployment Impact Assessment:** Deploy FPAs in controlled ethical penetration testing scenarios against production LLM-based static analysis tools to measure actual impact on security-critical applications and validate practical severity of this attack vector.