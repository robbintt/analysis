---
ver: rpa2
title: 'Evaluating LLMs for Zeolite Synthesis Event Extraction (ZSEE): A Systematic
  Analysis of Prompting Strategies'
arxiv_id: '2512.15312'
source_url: https://arxiv.org/abs/2512.15312
tags:
- event
- extraction
- text
- prompting
- argument
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work systematically evaluates Large Language Models (LLMs)\
  \ for zeolite synthesis event extraction, addressing the challenge of extracting\
  \ structured procedural information from scientific text. The study tests four prompting\
  \ strategies\u2014zero-shot, few-shot, event-specific, and reflection-based\u2014\
  across six state-of-the-art LLMs on a 1,530-sentence annotated dataset."
---

# Evaluating LLMs for Zeolite Synthesis Event Extraction (ZSEE): A Systematic Analysis of Prompting Strategies

## Quick Facts
- arXiv ID: 2512.15312
- Source URL: https://arxiv.org/abs/2512.15312
- Reference count: 26
- Primary result: LLMs achieve 80-90% F1 on event type classification but only 50-65% F1 on precise span extraction for zeolite synthesis procedures

## Executive Summary
This study systematically evaluates six state-of-the-art LLMs on the ZSEE dataset for extracting structured events from zeolite synthesis procedures. The research tests four prompting strategies across 1,530 annotated sentences, revealing a fundamental performance gap: while LLMs excel at classifying event types (80-90% F1), they struggle with precise span extraction tasks like trigger text and argument identification (50-65% F1). Few-shot prompting provides moderate improvements (+7-18% F1), but advanced strategies offer minimal gains. Error analysis identifies systematic issues including hallucination, over-generalization, and imprecise span boundaries. The findings indicate that while LLMs understand high-level semantic patterns, they lack the fine-grained token-level precision required for structured scientific information extraction, lagging significantly behind specialized domain-adapted models.

## Method Summary
The study evaluates six LLMs (Gemma-3-12b-it, GPT-5-mini, O4-mini, Claude-Haiku-3.5, DeepSeek-Reasoning, DeepSeek-Non-Reasoning) on a 1,530-sentence zeolite synthesis dataset using four prompting strategies: zero-shot, few-shot (2 examples), event-specific (N+1 API calls), and reflection (2-pass extraction). Models are accessed via API with default temperature (~0.7), and outputs are evaluated using lemmatization-based subset matching for precision, recall, and F1 across four subtasks: event type classification, trigger text extraction, argument role extraction, and argument text extraction. The evaluation includes automatic JSON correction for malformed outputs and comprehensive error analysis of systematic failure patterns.

## Key Results
- LLMs achieve 80-90% F1 on event type classification but only 50-65% F1 on argument text extraction
- Few-shot prompting provides the most substantial benefit, improving performance by approximately +18% F1 for trigger text extraction
- GPT-5-mini exhibits extreme prompt sensitivity with F1 scores ranging from 11.51% (event-specific) to 79.75% (few-shot)
- Advanced prompting strategies (event-specific, reflection) offer minimal performance gains compared to simpler approaches
- Performance lags significantly behind specialized models like PAIE, with an 8-17% F1 gap in extraction tasks

## Why This Works (Mechanism)

### Mechanism 1
Few-shot prompting improves span extraction by providing concrete boundary templates that demonstrate the granularity level expected, anchoring the model's extraction behavior to the annotation schema's token-level precision requirements. This works because examples show whether triggers should be single words or phrases, stabilizing output formatting rather than improving task understanding.

### Mechanism 2
The classification-to-extraction difficulty gradient reflects LLMs' training objective favoring semantic understanding over precise localization. Next-token prediction trains models to capture semantic patterns effectively for event type classification, but does not train for character-precise or token-precise boundary identification required for argument text extraction.

### Mechanism 3
Model-specific prompt sensitivity, particularly GPT-5-mini's extreme variance (11-79% F1), indicates inconsistent grounding to task instructions without examples. Some models fail to reliably map task definitions to extraction behavior when instructions alone are provided, requiring concrete examples as stabilizing reference points for output formatting.

## Foundational Learning

- Concept: Event-argument extraction structure (event type, trigger text, argument roles, argument text spans)
  - Why needed here: Understanding this decomposition explains why LLMs succeed at classification but fail at extraction, as these are fundamentally different subtasks with different difficulty profiles
  - Quick check question: Given "The mixture was stirred at 500 rpm for 2 hours," can you identify the event type, trigger, and at least two argument roles with their text spans?

- Concept: Lemmatization-based subset matching for evaluation
  - Why needed here: This metric handles morphological variations and clarifies why paraphrasing errors are penalized, as the evaluation requires matching lemmas rather than exact strings
  - Quick check question: If the ground truth span is "stirred at room temperature" and the model extracts "stirring at ambient temperature," would lemmatization-based matching count this as correct?

- Concept: The classification-to-extraction difficulty gradient
  - Why needed here: Results show 80-90% F1 on event type classification vs. 50-65% on argument extraction; this gradient is central to understanding the fundamental limitations of LLM architectures for structured extraction
  - Quick check question: Why might a model correctly classify an event as "Stir" but fail to extract "500 rpm" as the revolution argument?

## Architecture Onboarding

- Component map: Sentence input → prompt construction (strategy-dependent) → API call(s) → JSON response → JSON parsing with failure handling → lemmatization → subset matching → metric aggregation → error logging
- Critical path: 1. Sentence input → prompt construction (strategy-dependent) 2. API call(s) → JSON response (event-specific requires N+1 calls; reflection requires 2 calls) 3. JSON parsing with failure handling (malformed = zero contribution) 4. Lemmatization → subset matching → metric aggregation 5. Error logging for qualitative analysis
- Design tradeoffs: Single API call (zero/few-shot) vs. multiple calls (event-specific/reflection): cost and latency vs. potential precision gains (paper shows gains are minimal); Default temperature vs. lower values: natural generation vs. determinism (preliminary tests showed minimal impact); Automatic JSON correction vs. strict failure marking: recovery rate vs. evaluation integrity
- Failure signatures: Hallucination: Model invents event types outside the 16 defined categories (e.g., "Disperse" instead of "Add"); Over-generalization: Extracting implicit information (e.g., "then" as duration) or conflating semantically similar roles (material vs. solvent); Boundary errors: Extracting entire clauses instead of trigger words; including/excluding parenthetical units inconsistently
- First 3 experiments: 1. Replicate zero-shot baseline on a 50-sentence subset across all six models to validate API integration and metric computation. 2. Ablate few-shot example selection: test 2 vs. 3 examples, and random vs. diverse-structure selection to confirm the +7-18% F1 improvement range. 3. Test temperature sensitivity on GPT-5-mini specifically (the high-variance model) with temperature values [0.0, 0.3, 0.7, 1.0] to determine if instability is mitigated by lower temperature or if it persists across settings.

## Open Questions the Paper Calls Out

- Can hybrid pipelines combining LLM outputs with specialized post-processing or structured extraction methods close the 8-17% F1 performance gap with domain-specific models like PAIE?
- Do the identified failure patterns (hallucination, over-generalization, imprecise span boundaries) generalize across other scientific domains beyond zeolite synthesis?
- Would more sophisticated prompting strategies—such as chain-of-thought with intermediate reasoning steps or multi-step extraction-validation pipelines—overcome the limitations of the four strategies tested?
- What mechanisms underlie GPT-5-mini's extreme prompt sensitivity (11-79% F1 variation on trigger text extraction), and does this indicate a fundamental architectural instability?

## Limitations
- The study evaluates only on a single dataset from one domain (zeolite synthesis), limiting generalization to other scientific domains
- Few-shot example selection criteria and diversity are underspecified, making it difficult to determine if the +7-18% F1 improvement range is robust
- The error analysis relies on qualitative inspection without providing quantitative distributions of error types across the dataset

## Confidence

**High confidence**: The core finding that LLMs perform well on event type classification (80-90% F1) but struggle with precise span extraction (50-65% F1) is well-supported by systematic evaluation and consistent across all prompting strategies tested.

**Medium confidence**: The claim that few-shot prompting provides the most substantial improvement (+7-18% F1) is supported by results, but the exact magnitude may depend on example selection criteria that are not fully specified.

**Low confidence**: The assertion that advanced prompting strategies (event-specific, reflection) offer minimal gains is based on tested configurations; alternative formulations or temperature settings might yield different outcomes, particularly for sensitive models like GPT-5-mini.

## Next Checks

1. **Lemmatization sensitivity analysis**: Reproduce key results using alternative lemmatization libraries (e.g., spaCy vs. NLTK) to determine if metric variation affects the core conclusions about classification vs. extraction performance gaps.

2. **Example selection ablation**: Systematically vary few-shot examples across different structural patterns (simple vs. complex sentences) to verify whether the +7-18% F1 improvement range holds consistently or is dependent on particular example choices.

3. **Temperature sweep on high-variance models**: Conduct controlled experiments with GPT-5-mini across temperature values [0.0, 0.3, 0.7, 1.0] to determine whether the observed 11-79% F1 variance is temperature-dependent or reflects fundamental model instability in task grounding.