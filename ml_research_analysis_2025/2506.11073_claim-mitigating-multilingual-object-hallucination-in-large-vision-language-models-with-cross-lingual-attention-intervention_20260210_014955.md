---
ver: rpa2
title: 'CLAIM: Mitigating Multilingual Object Hallucination in Large Vision-Language
  Models with Cross-Lingual Attention Intervention'
arxiv_id: '2506.11073'
source_url: https://arxiv.org/abs/2506.11073
tags:
- attention
- lvlms
- language
- english
- baseline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multilingual object hallucination in large
  vision-language models (LVLMs), where models generate responses inconsistent with
  visual input when processing non-English queries. The authors propose CLAIM, a near
  training-free method that aligns cross-lingual attention patterns by identifying
  language-specific attention heads, estimating language shift vectors from English
  to target languages, and intervening in attention outputs during inference.
---

# CLAIM: Mitigating Multilingual Object Hallucination in Large Vision-Language Models with Cross-Lingual Attention Intervention

## Quick Facts
- **arXiv ID**: 2506.11073
- **Source URL**: https://arxiv.org/abs/2506.11073
- **Reference count**: 15
- **Primary result**: CLAIM achieves 13.56% average improvement on POPE benchmark and 21.75% on hallucination subsets of MME across multiple languages

## Executive Summary
This paper addresses multilingual object hallucination in large vision-language models (LVLMs), where models generate responses inconsistent with visual input when processing non-English queries. The authors propose CLAIM, a near training-free method that aligns cross-lingual attention patterns by identifying language-specific attention heads, estimating language shift vectors from English to target languages, and intervening in attention outputs during inference. CLAIM achieves significant improvements across multiple languages by leveraging English visual perception capabilities, demonstrating strong generalization across low and high-resource languages.

## Method Summary
CLAIM is a near training-free method that mitigates multilingual object hallucination by aligning cross-lingual attention patterns. The approach involves three main steps: (1) training linear SVM probes to identify Top-K language-specific cross-modal attention heads using caption queries, (2) computing shift vectors by measuring the mean difference between English and target language attention outputs, and (3) intervening at inference by adding scaled shift vectors to selected attention heads. The method focuses on intermediate layers where language-specific cross-modal integration diverges most, requiring only a small paired dataset (1,000 images) for probe training and shift estimation.

## Key Results
- CLAIM achieves 13.56% average improvement on POPE benchmark across 7 languages
- Achieves 21.75% improvement on hallucination subsets of MME (object existence: +18.7%, count: +22.6%, color: +15.5%, position: +30.1%)
- Spanish shows highest improvement at 30% on position-related hallucinations

## Why This Works (Mechanism)

### Mechanism 1
Non-English queries exhibit divergent cross-modal attention patterns that contribute to hallucination; CLAIM aligns these patterns with the stronger English proficiency via a vector shift intervention. The method identifies language-specific cross-modal attention heads whose outputs differ across languages, estimates a "language shift vector" representing the average output difference between English and target languages, and adds this vector to attention outputs during inference. English visual perception serves as a corrective reference due to dominant English training data. Break condition: if target-language perception is incompatible with English attention, forcing alignment could degrade performance.

### Mechanism 2
Intermediate layers (approximately layers 10-17 in LLaVA-1.5) are where language-specific cross-modal integration diverges most and where intervention is most impactful. Early layers perform global image feature extraction; later layers focus on reasoning and logits prediction. Intermediate layers integrate visual and textual information, and this is where attention to query-relevant regions diverges most across languages. Probes achieve highest language-classification accuracy in these layers, indicating maximal language-specificity. Intervening here directly affects the cross-modal integration process. Break condition: in architectures with different depth or layer roles, "intermediate" may shift.

### Mechanism 3
LVLMs process non-English queries by mapping them into an English semantic space at intermediate layers, and CLAIM's alignment reinforces this pathway to improve visual grounding. Logit lens analysis shows that Chinese query tokens are mapped to English semantics around layer 17 before response generation. The model uses this English-aligned representation to reason about the image. Hallucination arises when this mapping is imperfect or when visual attention fails prior to or during this stage. By aligning attention outputs to English patterns, CLAIM stabilizes the cross-lingual mapping. Break condition: if a model has a more balanced multilingual foundation, the English-centric pathway may be less dominant.

## Foundational Learning

- **Cross-Modal Attention in Transformers**
  - Why needed: CLAIM manipulates attention outputs between visual and textual tokens; understanding how attention weights are computed and contribute to hidden states is essential
  - Quick check: If a cross-modal attention head's output is shifted by a vector at an intermediate layer, how does this alteration propagate to the final logits?

- **Hallucination in Large Vision-Language Models (LVLMs)**
  - Why needed: The paper addresses "multilingual object hallucination"—responses inconsistent with the visual input, exacerbated for non-English queries
  - Quick check: What is "multilingual object hallucination" as defined in this work, and how does it differ from English-only object hallucination?

- **Mechanistic Interpretability Tools (Probing and Logit Lens)**
  - Why needed: Probes identify language-specific heads; logit lens reveals semantic mapping at intermediate layers
  - Quick check: What signal does the probe's binary classification accuracy provide about an attention head's role in CLAIM?

## Architecture Onboarding

- **Component map**: Probe Training Module -> Shift Vector Estimator -> Inference-Time Intervention Engine
- **Critical path**: (1) Train probes on small dataset, select language-specific heads, (2) Compute shift vectors using paired prompts, (3) Apply intervention during inference for non-English queries
- **Design tradeoffs**: Language-specific vs. unified shift (Specific-Shift yields best results but Multi-Shift and Mono-Shift offer simplicity); hyperparameters (α, K) require tuning
- **Failure signatures**: Degraded reasoning from overly strong intervention; no benchmark gain from low probe accuracy or non-representative shift estimation data
- **First 3 experiments**: (1) Probe accuracy analysis on LLaVA-1.5 to verify intermediate layer clustering, (2) Intervention ablation on POPE comparing baseline vs. CLAIM, (3) Cross-lingual generalization test applying English-Chinese shift to Spanish inference

## Open Questions the Paper Calls Out
- How to mitigate multilingual object hallucination as a plug-and-play tool for models with restricted access (closed-source models)
- Whether aligning non-English attention patterns with English patterns inadvertently reinforces English-centric biases
- To what extent intervention in perception-oriented attention heads unintentionally degrades complex reasoning capabilities

## Limitations
- Effectiveness depends on assumption that English visual perception is consistently superior across diverse LVLMs and tasks
- Shift vector estimation relies on limited paired dataset (1,000 images), quality may degrade for languages not well-represented
- Intervention could introduce artifacts if English attention pattern is not universally optimal for visual grounding

## Confidence
- **High Confidence**: Empirical findings on attention divergence patterns, probe accuracy for head identification, benchmark improvements on POPE and MME subsets
- **Medium Confidence**: Generalization across different LVLM architectures, long-term stability of intervention effects, applicability to non-object hallucination tasks
- **Low Confidence**: Claims about English being the "privileged" language for all cross-modal reasoning, effectiveness on low-resource languages without English-parallel data

## Next Checks
1. **Architecture Transfer Test**: Apply CLAIM to a different LVLM architecture (e.g., Qwen-VL-Plus or InternVL) and verify if intermediate-layer attention divergence and intervention efficacy replicate the LLaVA-1.5 results

2. **Shift Vector Robustness**: Test whether shift vectors estimated from one language pair (e.g., English-Chinese) remain effective when applied to a third language (e.g., Spanish) without re-estimation, measuring performance degradation

3. **Cross-Domain Generalization**: Evaluate CLAIM on non-COCO domains (e.g., medical imaging or remote sensing) to assess whether English-centric attention alignment remains beneficial outside natural image captioning tasks