---
ver: rpa2
title: 'CrEst: Credibility Estimation for Contexts in LLMs via Weak Supervision'
arxiv_id: '2506.14912'
source_url: https://arxiv.org/abs/2506.14912
tags:
- documents
- crest
- credibility
- document
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CrEst addresses credibility estimation for context documents in
  LLM inference without manual annotations by leveraging inter-document agreement
  through weak supervision. The core method estimates credibility scores by measuring
  semantic coherence between documents in the embedding space, under the assumption
  that credible documents exhibit higher mutual agreement.
---

# CrEst: Credibility Estimation for Contexts in LLMs via Weak Supervision

## Quick Facts
- **arXiv ID**: 2506.14912
- **Source URL**: https://arxiv.org/abs/2506.14912
- **Reference count**: 11
- **Key outcome**: CrEst addresses credibility estimation for context documents in LLM inference without manual annotations by leveraging inter-document agreement through weak supervision.

## Executive Summary
CrEst introduces a novel approach to credibility estimation for retrieved context documents in LLM inference without requiring manual annotations. The method leverages inter-document agreement as a proxy for credibility, operating under the assumption that credible documents exhibit higher semantic coherence with other credible documents in the retrieved set. By treating documents as "labeling functions" in a weak supervision framework, CrEst computes credibility scores through semantic coherence measurement in embedding space. The framework provides both black-box (output aggregation) and white-box (attention modification) integration strategies, demonstrating significant improvements across three model architectures and five datasets.

## Method Summary
CrEst estimates document credibility by measuring semantic coherence between documents in embedding space, using the assumption that credible documents cluster together. The method computes credibility scores via triplet algebra on pairwise embedding distances, treating each document as a weak supervision "labeling function." Two integration approaches are provided: a black-box method that aggregates multiple LLM outputs with different credibility estimates by selecting the most "popular" output, and a white-box method that directly modifies attention mechanisms based on credibility scores. The approach requires no manual annotations and works post-hoc on any retriever output.

## Key Results
- Achieves up to 26.86% higher accuracy and 3.49% higher F1 score compared to strong baselines
- Demonstrates robust performance even under high-noise conditions (up to ~50% corrupted documents)
- Shows plateau effect at approximately 7 embedders for black-box integration
- Performs well across three different model architectures and five QA datasets

## Why This Works (Mechanism)

### Mechanism 1: Inter-Document Agreement as Credibility Proxy
The core mechanism treats credible documents as those that align more closely with the majority of other retrieved documents. Each document is treated as a "labeling function" in weak supervision, with credibility scores computed as s_i = 1/E[||λ_i - λ*||²], where λ* is an unobserved "true" document embedding. The expectation is derived from pairwise embedding distances using triplet algebra. This works because credible documents naturally cluster together in embedding space, while unreliable documents show lower agreement with the majority.

### Mechanism 2: Black-Box Integration via Output Aggregation
This approach runs the LLM M times with different credibility estimates, then aggregates outputs by selecting the most "popular" one—the output that achieves the lowest average distance to other outputs in embedding space. The mechanism assumes correct answers will cluster together across variations, enabling selection of the most credible output without accessing model internals.

### Mechanism 3: White-Box Integration via Attention Mask Scaling
For each document, attention weights are scaled by its credibility score: Attn(x_di) = s_i × Attn(x_di) × C. This directly guides the model to prioritize credible documents during generation by modifying how much attention each document's tokens receive, effectively reweighting the contribution of each document to the final output.

## Foundational Learning

- **Weak Supervision / Data Programming**: Understanding how noisy labeling functions are aggregated is essential since CrEst reformulates credibility estimation as a weak supervision problem. *Quick check*: Can you explain how triplet constraints (Eq. 3-4) enable estimating E[||λ_i - λ*||²] without observing λ*?

- **Embedding Space Similarity Metrics**: The method relies on pairwise L2 distances in embedding space to proxy semantic agreement. *Quick check*: Why might different embedding models yield different credibility rankings for the same document set?

- **Attention Mechanism Basics (Transformer)**: The white-box method modifies attention masks, so understanding how attention weights influence token importance is critical. *Quick check*: If you scale attention for document A by 0.1 and document B by 0.9, what behavior change do you expect in generation?

## Architecture Onboarding

- **Component map**: Retriever → Document set D → Embedders (M models) → Pairwise distances → Credibility scores → Aggregator → Integration layer (Black-box: M prompts + output selection OR White-box: Attention scaling)

- **Critical path**: 1) Retrieve documents D, 2) Embed all documents with M embedders, 3) Compute pairwise distances and derive credibility scores per embedder, 4) Aggregate scores (white-box) OR run M prompts and aggregate outputs (black-box), 5) Generate final answer

- **Design tradeoffs**: More embedders → better robustness but diminishing returns (plateau at ~7); More documents → gains up to ~12, then slight degradation; Black-box: broader applicability, higher inference cost; White-box: finer control, requires model access

- **Failure signatures**: High-noise regimes (>50% corrupted docs) may cause inverted credibility scores; Embedder bias can lead to misleading scores; Attention scaling too aggressive can degrade generation quality

- **First 3 experiments**: 1) Replicate Table 1 on PopQA with CrEst-bbx and CrEst-wbx vs. vanilla RAG baseline, 2) Run noise robustness test: inject 20-80% corrupted docs and measure accuracy decay, 3) Ablate number of embedders (1-7) and documents (1-20) to find optimal configuration

## Open Questions the Paper Calls Out

**Open Question 1**: Can retrievers be trained end-to-end using CrEst's credibility-based loss function to improve document quality at the source? The paper explicitly states this as future work, noting that current experiments use off-the-shelf retrievers without joint training.

**Open Question 2**: What robustness guarantees or detection mechanisms exist when a majority of retrieved documents are adversarially noisy or systematically biased? The paper acknowledges this limitation, noting that when more than half of documents are unreliable, the approach may incorrectly assign high credibility scores to unreliable but semantically similar documents.

**Open Question 3**: How does CrEst's performance generalize across domains not represented in the current evaluation (e.g., legal, medical, scientific literature)? The paper notes performance may vary across domains, as all five datasets are general-purpose QA rather than specialized domains.

## Limitations

- Performance degrades when >50% of retrieved documents are unreliable, as credibility scores may incorrectly favor semantically similar but untrustworthy sources
- Requires multiple embedders for robustness, increasing computational overhead and complexity
- White-box integration requires modifying model internals, limiting applicability to black-box APIs
- Embedding space geometry assumptions (Euclidean) may not hold for all embedding models or domains

## Confidence

**High confidence**: The core mechanism of using inter-document agreement for credibility estimation is well-founded in weak supervision literature. The experimental methodology (controlled noise injection, ablation studies) is sound.

**Medium confidence**: Implementation details for attention mask scaling and prompt formatting require reasonable engineering decisions. Generalizability across domains beyond QA tasks needs further validation.

**Low confidence**: Optimal configuration (number of embedders, documents, prompt templates) appears dataset-dependent and may require tuning for new applications.

## Next Checks

1. **Noise robustness validation**: Systematically test the method with 10%, 30%, 50%, and 70% corrupted documents to map the exact performance boundary and verify the claimed ~50% robustness threshold.

2. **Embedding model ablation**: Test credibility estimation with different combinations of embedders (e.g., using only one vs. the full set of four) to quantify the contribution of each embedder and identify potential redundancies.

3. **Cross-domain generalization**: Apply CrEst to a non-QA task (e.g., summarization or classification) with retrieved context to validate whether inter-document agreement remains a reliable credibility signal across different application domains.