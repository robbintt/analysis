---
ver: rpa2
title: 'OSMa-Bench: Evaluating Open Semantic Mapping Under Varying Lighting Conditions'
arxiv_id: '2503.10331'
source_url: https://arxiv.org/abs/2503.10331
tags:
- scene
- object
- semantic
- questions
- lighting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OSMa-Bench introduces a highly automated LLM/LVLM-powered pipeline
  to evaluate open semantic mapping under varying indoor lighting conditions. The
  framework generates synthetic RGB-D sequences with ground truth 3D reconstructions,
  allowing rigorous testing of semantic segmentation and scene graph quality across
  diverse lighting scenarios.
---

# OSMa-Bench: Evaluating Open Semantic Mapping Under Varying Lighting Conditions

## Quick Facts
- arXiv ID: 2503.10331
- Source URL: https://arxiv.org/abs/2503.10331
- Reference count: 30
- Open semantic mapping performance drops up to 15% under challenging lighting conditions

## Executive Summary
OSMa-Bench introduces a highly automated LLM/LVLM-powered pipeline to evaluate open semantic mapping under varying indoor lighting conditions. The framework generates synthetic RGB-D sequences with ground truth 3D reconstructions, allowing rigorous testing of semantic segmentation and scene graph quality across diverse lighting scenarios. Using datasets from ReplicaCAD and HM3D, the benchmark assesses methods like ConceptGraphs, BBQ, and OpenScene, measuring mAcc and f-mIoU for segmentation, and VQA accuracy for semantic reasoning. Results show that lighting variations, particularly camera-mounted and dynamic lighting, significantly impact performance, with segmentation accuracy dropping by up to 15% and scene graph reasoning accuracy decreasing under challenging conditions.

## Method Summary
The benchmark uses Habitat-Sim to generate synthetic RGB-D sequences from ReplicaCAD and HM3D scenes under controlled lighting conditions (baseline, nominal lights, camera light, dynamic lighting). Methods including ConceptGraphs, BBQ, and OpenScene process these sequences to produce 3D semantic maps and scene graphs. Evaluation combines traditional segmentation metrics (mAcc, f-mIoU) with an automated VQA pipeline using LLM/LVLM to generate questions about semantic relationships, attributes, and spatial properties. The framework enables reproducible, scalable evaluation of open semantic mapping robustness to illumination changes.

## Key Results
- Segmentation accuracy drops by up to 15% under camera-mounted and dynamic lighting conditions
- VQA accuracy decreases across all categories under challenging lighting scenarios
- CLIP-based methods show particular sensitivity to illumination changes affecting feature embeddings
- BBQ demonstrates best FPS (0.93 Hz) but all methods show significant performance degradation under non-nominal lighting

## Why This Works (Mechanism)

### Mechanism 1: LLM/LVLM-Automated VQA Pipeline for Semantic Graph Evaluation
- Automated question generation and grading provides scalable semantic evaluation without human annotation
- LVLM samples scene frames → generates structured descriptions → LLM produces categorized questions → validation filters duplicates and ambiguities → answers compared via semantic equivalence check
- Core assumption: LLM-generated questions accurately capture scene graph quality; semantic equivalence detection is reliable
- Break condition: If LLM-generated questions fail to cover critical semantic relationships, or if semantic equivalence detection produces systematic false positives/negatives

### Mechanism 2: Synthetic Lighting Variation Induces Measurable Performance Degradation
- Controlled lighting variations in simulation cause predictable drops in segmentation accuracy and VQA performance
- Habitat-Sim generates identical scenes with lighting modifiers → methods process RGB-D sequences → mAcc/f-mIoU measured against ground truth → performance deltas attributed to lighting
- Core assumption: Simulated lighting conditions transfer to real-world degradation patterns; other variables held constant
- Break condition: If simulation lighting artifacts don't correlate with real-world illumination challenges

### Mechanism 3: 2D-3D Feature Fusion Sensitivity to Illumination
- Methods relying on CLIP feature projection show degraded segmentation when input images have non-nominal lighting
- RGB images → CLIP encoder → 2D features → projected to 3D points → aggregated into semantic maps; illumination changes alter feature distributions, causing misclassification
- Core assumption: CLIP features trained on web images don't generalize to extreme lighting without domain adaptation
- Break condition: If methods incorporate lighting-invariant feature augmentations or domain adaptation layers

## Foundational Learning

- **CLIP Vision-Language Embeddings**
  - Why needed here: All evaluated methods use CLIP features for open-vocabulary semantic grounding
  - Quick check question: Can you explain how CLIP embeds images and text into a shared space, and why illumination changes might shift image embeddings?

- **3D Scene Graphs**
  - Why needed here: Methods construct hierarchical graphs of objects and relations; VQA pipeline evaluates graph completeness and correctness
  - Quick check question: What is the difference between a 3D point cloud and a 3D scene graph? How do object nodes and edge relations represent spatial/functional knowledge?

- **Semantic Segmentation Metrics (mIoU, mAcc, f-mIoU)**
  - Why needed here: Paper reports mAcc and f-mIoU; understanding weighting differences is critical for interpreting results
  - Quick check question: Why does f-mIoU weight large classes (walls, floors) more heavily, and how might this mask failures on small objects?

## Architecture Onboarding

- Component map: Habitat-Sim → ReplicaCAD/HM3D scenes + lighting configs + trajectory planning → RGB-D + depth + semantic images → OpenScene / ConceptGraphs / BBQ → process RGB-D → 3D semantic map + scene graph → Ground truth comparison → segmentation metrics (mAcc, f-mIoU); VQA pipeline → LLM question generation → answer accuracy per category

- Critical path: Configure scene + lighting in Habitat-Sim JSON → Generate RGB-D trajectory with GreedyGeodesicFollower → Run mapping method to produce scene graph → Execute VQA pipeline: frame sampling → LVLM description → LLM questions → validation → answer comparison

- Design tradeoffs: Synthetic data enables controlled lighting but may lack real-world spectral complexity; Automated VQA scales evaluation but depends on LLM reliability for question quality; Closed vocabulary (ReplicaCAD) vs. open vocabulary (HM3D) affects metric computation consistency; FPS vs. accuracy: ConceptGraphs (0.19 Hz) vs. BBQ (0.93 Hz) vs. OpenScene (0.13 Hz)

- Failure signatures: Shadow-as-object (camera light causes shadows segmented as separate instances); Object over-fragmentation (stair steps each labeled as separate "staircase"); Mask bleeding (fuzzy boundaries spread onto adjacent objects); Attribute loss (missing material/property details in scene graph captions → VQA failures); Dynamic lighting inconsistency (floor/wall segmentation fragments under varying illumination)

- First 3 experiments: Replicate baseline vs. camera light comparison on single ReplicaCAD scene; verify mAcc/f-mIoU deltas match paper (~5-15%); Run VQA pipeline on one scene; inspect generated questions for category distribution and validation filtering rate; Visualize corner cases: render point clouds under camera light, identify shadow misclassification and object merging artifacts

## Open Questions the Paper Calls Out
None

## Limitations
- Simulated lighting effects may not fully capture real-world illumination complexity including spectral variations and reflections
- Automated VQA pipeline reliability depends on LLM consistency and potential systematic gaps in question coverage
- Closed-vocabulary ReplicaCAD scenes may not reflect semantic diversity encountered in real robotic deployments

## Confidence
- **High Confidence**: Lighting variation causing measurable segmentation accuracy drops (15% maximum) across multiple methods and datasets
- **Medium Confidence**: Automated VQA pipeline provides scalable semantic evaluation but requires further validation for reliability
- **Medium Confidence**: CLIP feature degradation under non-nominal lighting demonstrated through qualitative examples and quantitative metrics

## Next Checks
1. Test benchmark's lighting effects on real indoor scenes with controlled illumination changes to verify simulation artifacts correlate with actual performance degradation patterns
2. Conduct systematic audit of generated questions across multiple scenes to quantify coverage of semantic relationships and identify systematic gaps or biases
3. Implement and test lighting-invariant feature augmentation techniques (e.g., color constancy, histogram equalization) on evaluated methods to isolate contribution of CLIP feature degradation from other lighting-induced effects