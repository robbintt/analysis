---
ver: rpa2
title: 'Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories
  via Reinforcement Learning'
arxiv_id: '2508.19828'
source_url: https://arxiv.org/abs/2508.19828
tags:
- memory
- answer
- memory-r1
- agent
- manager
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Memory-R1 addresses the challenge of limited context windows in
  large language models by introducing a reinforcement learning framework for adaptive
  memory management. It uses two specialized agents: a Memory Manager that learns
  to perform ADD, UPDATE, DELETE, and NOOP operations on an external memory bank,
  and an Answer Agent that filters and reasons over retrieved memories via memory
  distillation.'
---

# Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning

## Quick Facts
- arXiv ID: 2508.19828
- Source URL: https://arxiv.org/abs/2508.19828
- Reference count: 40
- Sets new state of the art on LoCoMo benchmark with 28% F1 improvement over baselines

## Executive Summary
Memory-R1 introduces a reinforcement learning framework to address context window limitations in large language models through adaptive memory management. The system employs two specialized agents: a Memory Manager that learns to perform ADD, UPDATE, DELETE, and NOOP operations on an external memory bank, and an Answer Agent that filters and reasons over retrieved memories via memory distillation. Both agents are fine-tuned with outcome-driven RL (PPO and GRPO), requiring only 152 training examples. On the LoCoMo benchmark, Memory-R1 achieves relative improvements of 28% in F1, 34% in BLEU-1, and 30% in LLM-as-a-Judge compared to the strongest baseline, setting a new state of the art. The framework generalizes across diverse question types, three benchmarks, and multiple model scales (3B-14B).

## Method Summary
Memory-R1 addresses multi-session dialogue QA by decomposing the problem into two RL-fine-tuned agents. The Memory Manager receives dialogue turns and retrieved memories, then outputs operations (ADD, UPDATE, DELETE, NOOP) to maintain a temporal memory bank. The Answer Agent applies memory distillation to filter 60 RAG-retrieved memories before generating answers. Both agents are trained separately using outcome-driven reinforcement learning with exact match rewards, avoiding the need for manual operation labels. The framework uses PPO or GRPO optimization on LLaMA-3.1-8B-Instruct or Qwen-2.5 models, with agents trained in a decoupled manner to ensure stability under sparse rewards.

## Key Results
- Achieves 45.0 F1 on LoCoMo test set, 28% relative improvement over Mem0 baseline
- Shows 34% BLEU-1 improvement and 30% LLM-as-a-Judge improvement on LoCoMo
- Demonstrates strong generalization to MSC and LongMemEval benchmarks with 9.7-10.3 F1 points improvement
- GRPO converges faster than PPO while achieving similar final performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Outcome-driven RL enables adaptive memory operations without explicit operation labels.
- **Mechanism:** The Memory Manager receives reward signals based solely on downstream answer correctness (exact match), not from labeled ADD/UPDATE/DELETE decisions. This allows the policy to learn which operations produce memory states that support correct reasoning, bypassing the need for impractical manual annotation of every memory operation.
- **Core assumption:** The correctness of downstream QA provides sufficient gradient signal to differentiate between beneficial and harmful memory operations.
- **Evidence anchors:**
  - [abstract] "Both agents are fine-tuned with outcome-driven RL (PPO and GRPO), enabling adaptive memory management with minimal supervision."
  - [Section 3.1] "This exact-match signal requires no manual labels, remains scalable, and is sufficient to teach effective memory operations."
  - [corpus] Weak direct evidence; A-MEM focuses on graph-based organization, not RL-based operation learning.
- **Break condition:** If QA rewards are too sparse or noisy (e.g., multi-hop questions requiring many operations before answerable), credit assignment fails and learning degrades.

### Mechanism 2
- **Claim:** Memory distillation reduces noise by filtering retrieved memories before reasoning.
- **Mechanism:** Rather than passing all 60 RAG-retrieved memories directly to the model, the Answer Agent applies a learned filtering policy ("memory distillation") that selects only relevant entries. This prevents distraction from irrelevant context—a known failure mode in long-context reasoning.
- **Core assumption:** The retrieval system returns some relevant memories, and the model can learn to identify them.
- **Evidence anchors:**
  - [abstract] "Answer Agent that filters and reasons over retrieved memories via memory distillation."
  - [Section 4.4, Figure 5c] GRPO with distillation improves F1 from 41.0 to 45.0; PPO from 39.3 to 41.0.
  - [corpus] TiMem mentions hierarchical consolidation but not learned filtering; no direct prior work on RL-based distillation.
- **Break condition:** If retrieval quality is extremely poor (relevant memories absent), distillation cannot recover performance.

### Mechanism 3
- **Claim:** Decoupled training of Memory Manager and Answer Agent stabilizes learning under sparse rewards.
- **Mechanism:** The two agents are trained separately—the frozen Answer Agent provides reward signal for Memory Manager training, and vice versa. This avoids attribution ambiguity when both components change simultaneously.
- **Core assumption:** Each agent can make meaningful progress while the other is frozen.
- **Evidence anchors:**
  - [Section 3, Figure 2] Explicit two-stage pipeline with distinct training objectives.
  - [Limitations] "We train the Memory Manager and Answer Agent separately to ensure stability under sparse rewards."
  - [corpus] No directly comparable decoupled approach found.
- **Break condition:** If either agent's frozen version is too weak, the other cannot learn effectively (though Figure 6 suggests compounding benefits from stronger managers).

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO)**
  - **Why needed here:** Core RL algorithm for both agents; requires understanding of policy gradients, clipping, and advantage estimation.
  - **Quick check question:** Can you explain why PPO's clipping objective prevents destabilizing policy updates?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** Memory-R1 builds on RAG for initial retrieval before distillation; understanding embedding-based similarity search is prerequisite.
  - **Quick check question:** How does top-k retrieval differ from threshold-based retrieval, and what are the tradeoffs?

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** Alternative to PPO that avoids explicit value functions by comparing candidate actions within groups.
  - **Quick check question:** What advantage does GRPO's group-relative normalization provide over standard PPO for this memory task?

## Architecture Onboarding

- **Component map:**
  - Dialogue turn -> Fact extraction -> Temporal memory bank construction -> RAG retrieval (60 memories) -> Memory Manager (ADD/UPDATE/DELETE/NOOP) -> Memory bank update -> Question -> RAG retrieval -> Answer Agent (memory distillation) -> Answer -> Exact match reward

- **Critical path:** Dialogue turn → fact extraction → retrieval → Memory Manager operation → memory bank update → (later) question → retrieval → Answer Agent distillation → answer → EM reward

- **Design tradeoffs:**
  - **PPO vs. GRPO:** GRPO shows faster early convergence (Figure 7), but both reach similar final rewards. GRPO avoids value function training but requires more sampling.
  - **EM vs. LLM-as-Judge reward:** EM yields balanced metrics; Judge-based reward encourages verbosity and hurts F1/BLEU (Table 2).
  - **60 retrieved memories:** Fixed count; fewer may miss relevant context, more increases noise.

- **Failure signatures:**
  - **Memory fragmentation:** DELETE+ADD instead of UPDATE loses context (Figure 1 example: "adopted Buddy" then "adopted Scout" overwrites first dog).
  - **Verbose answers:** Judge-based reward produces long responses that fail lexical metrics.
  - **Noisy distillation:** Without filtering, model answers based on irrelevant memories (Appendix A.2 case study).

- **First 3 experiments:**
  1. **Sanity check:** Run vanilla Mem0 baseline on LoCoMo dev split; verify F1 ~30 matches Table 1 before implementing RL.
  2. **Ablation distillation:** Train Answer Agent with/without memory distillation on same data; expect ~4 F1 point gap per Figure 5c.
  3. **Reward signal test:** Compare EM-based vs. Judge-based rewards on a 20-example subset; observe verbosity difference in generated answers.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can end-to-end joint training of the Memory Manager and Answer Agent improve coordination and performance compared to the current separate training approach?
- **Basis in paper:** [explicit] The authors state in the Limitations section: "We train the Memory Manager and Answer Agent separately to ensure stability under sparse rewards. This separation is necessary but makes the process less straightforward. An end-to-end multi-agent reinforcement learning approach could simplify training and enable richer coordination, which we view as a promising direction for future work."
- **Why unresolved:** Separate training was necessary for stability under sparse rewards, but prevents the two agents from co-adapting during training.
- **What evidence would resolve it:** Comparing Memory-R1's performance when trained with a joint multi-agent RL objective versus separate training, measuring both final performance and training stability.

### Open Question 2
- **Question:** How can Memory-R1 be extended to handle multimodal memory (images, audio, video) beyond dialogue-centric text data?
- **Basis in paper:** [explicit] The Limitations section states: "Our evaluation focuses on dialogue-centric datasets. While these benchmarks cover a wide range of reasoning types, extending Memory-R1 to multimodal data may introduce challenges beyond the scope of this work."
- **Why unresolved:** Memory operations (ADD, UPDATE, DELETE, NOOP) and memory distillation were designed for text; multimodal data introduces cross-modal retrieval, storage, and reasoning challenges.
- **What evidence would resolve it:** Implementing and evaluating Memory-R1 on multimodal long-term memory benchmarks, showing whether RL-learned memory management transfers to non-textual modalities.

### Open Question 3
- **Question:** What reward design can balance conciseness with semantic completeness to optimize both lexical metrics (F1, BLEU) and semantic correctness (LLM-as-a-Judge)?
- **Basis in paper:** [inferred] Table 2 shows that EM-based rewards yield balanced performance but J-based rewards produce verbose outputs that score poorly on lexical metrics despite higher semantic correctness. The authors note: "the reward encourages longer, descriptive answers, which misaligns with string-overlap metrics."
- **Why unresolved:** Current EM reward works but may not capture nuanced answer quality; J-based reward causes length inflation.
- **What evidence would resolve it:** Developing and testing a composite reward function that penalizes verbosity while rewarding semantic correctness, evaluating across all three metrics.

### Open Question 4
- **Question:** How does Memory-R1's sample efficiency scale when applied to more complex domains beyond conversational QA (e.g., code, scientific reasoning, planning)?
- **Basis in paper:** [inferred] The paper demonstrates strong results with only 152 training examples on dialogue benchmarks, but generalization to domains requiring different memory structures (procedural, hierarchical, temporal-spatial) remains untested.
- **Why unresolved:** LoCoMo, MSC, and LongMemEval all focus on conversational memory; whether 152 examples suffices for learning domain-specific memory operations in non-conversational settings is unknown.
- **What evidence would resolve it:** Training Memory-R1 on non-conversational long-term memory tasks with similar data constraints and comparing sample efficiency and final performance.

## Limitations

- Memory bank construction relies on frozen GPT-4o-mini, creating a single point of failure that all downstream learning depends upon
- Fixed 60-memory retrieval count may not scale optimally across different dialogue lengths and complexity levels
- Separate agent training prevents coordination and may miss performance gains from joint optimization

## Confidence

- **High Confidence:** The mechanism of outcome-driven RL enabling operation learning without explicit labels is well-supported by ablation results and comparative experiments.
- **Medium Confidence:** Memory distillation's effectiveness is demonstrated, but the paper doesn't explore alternative filtering approaches or optimal retrieval counts.
- **Low Confidence:** The decoupled training stability claim lacks direct ablation testing - we don't know if simultaneous training would fail catastrophically.

## Next Checks

1. Test Memory-R1 with 30 vs. 60 retrieved memories to quantify the noise-tolerance tradeoff of memory distillation.
2. Implement a simplified version using synthetic dialogue data with known ground-truth operations to validate credit assignment in the Memory Manager.
3. Replace GPT-4o-mini with a smaller, open-source model for initial memory bank construction to assess robustness to representation quality.