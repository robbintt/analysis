---
ver: rpa2
title: 'Late Breaking Results: Conversion of Neural Networks into Logic Flows for
  Edge Computing'
arxiv_id: '2601.22151'
source_url: https://arxiv.org/abs/2601.22151
tags:
- neural
- decision
- logic
- networks
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to convert neural networks into
  logic flows to improve execution efficiency on CPUs in edge devices. Instead of
  executing many MAC operations, which CPUs are not optimized for, the approach converts
  neural networks into decision trees and then extracts logic flows from decision
  paths with constant classification results.
---

# Late Breaking Results: Conversion of Neural Networks into Logic Flows for Edge Computing

## Quick Facts
- arXiv ID: 2601.22151
- Source URL: https://arxiv.org/abs/2601.22151
- Reference count: 23
- This paper introduces a method to convert neural networks into logic flows to improve execution efficiency on CPUs in edge devices.

## Executive Summary
This paper proposes converting neural networks into decision trees, then extracting logic flows from decision paths with constant classification results. The approach reduces execution latency on CPUs by replacing MAC-heavy computation with if-else structures for a subset of samples. Experiments on three datasets show up to 14.9% average latency reduction without accuracy loss, particularly benefiting edge computing scenarios where GPUs are impractical due to power constraints.

## Method Summary
The method converts trained neural networks into equivalent decision trees using only training data to construct branches. Decision paths leading to constant-classification leaves are identified, and Mixed Integer Programming with Irreducible Infeasible Subsystem extraction determines minimal constraint sets guaranteeing constant classification. This generates hybrid C code with early-exit logic flow checks before fallback to standard neural computation, reducing MAC operations for matched samples.

## Key Results
- Logic flows reduce average latency by 14.9% across three datasets (MNIST, Occupancy I, Occupancy II)
- Up to 52.2% latency reduction at minimum, with no accuracy degradation
- 12.7%-34.1% of samples exit through logic flows, validating the hybrid execution approach
- Logic flows consist of if-else structures and reduced MAC operations suitable for CPU execution

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Converting neural networks to decision trees exposes paths with provably constant classification outcomes, enabling early-exit logic that bypasses MAC-heavy computation.
- **Mechanism:** Each ReLU activation decision becomes a decision node in a tree. Training data constrains which branches are constructed—branches never traversed by training samples are pruned. This yields a sparse tree where some leaves correspond to single-class outcomes.
- **Core assumption:** Neural network decision boundaries can be adequately approximated by tree structures using training data coverage, and training data distribution reflects inference distribution.
- **Evidence anchors:**
  - [abstract]: "Specifically, neural networks are first converted into equivalent decision trees, from which decision paths with constant leaves are then selected and compressed into logic flows."
  - [section II-A]: "We only add branches that are traversed by samples in the training dataset (e.g., the false branch of the right-most decision of $y_1$ in Fig. 1(b) is never visited and thus not added)."
  - [corpus]: Related work on Tsetlin Machines (arXiv:2510.15653) demonstrates that logic-driven operations can achieve high-speed inference on CPUs, supporting the premise that logic-based execution suits CPU architectures.
- **Break condition:** If the neural network has highly entangled decision boundaries with no isolated class regions, the tree will have few or no constant leaves, yielding minimal logic flow extraction.

### Mechanism 2
- **Claim:** Mixed Integer Programming (MIP) verification with Irreducible Infeasible Subsystem (IIS) extraction produces minimal constraint sets that guarantee constant classification, compressing multi-layer decisions into simple if-else checks.
- **Mechanism:** For each candidate leaf, two MIP problems are formulated (one per class). If one problem is infeasible, that class is provably impossible at that leaf. Gurobi's IIS identifies the smallest constraint subset causing infeasibility—these become the logic flow conditions.
- **Core assumption:** The MIP solver can tractably determine infeasibility for the network's constraint system; the IIS meaningfully captures the essential logic rather than arbitrary constraint subsets.
- **Evidence anchors:**
  - [section II-B]: "We then use Gurobi to determine the Irreducible Infeasible Subsystem (IIS) of constant leaves, which is a set of constraints that cause the problem to be infeasible... For the example of leaf L4 in Fig. 1, the IIS is $y_0 < 0$, meaning class $c_1$ is always predicted because of the constraint $y_0 < 0$."
  - [abstract]: "Such logic flows consist of if and else structures and a reduced number of MAC operations."
  - [corpus]: Weak direct evidence—no corpus papers specifically address MIP-based neural network verification or IIS extraction for logic compression.
- **Break condition:** MIP solve time grows exponentially with network size; deep networks or high-dimensional inputs may make verification intractable.

### Mechanism 3
- **Claim:** Hybrid execution amortizes the overhead of logic flow checking by exiting early on matched samples while falling back to standard neural computation otherwise.
- **Mechanism:** Logic flow conditions (IIS constraints) are evaluated first. If all conditions match, classification is determined immediately. Unmatched samples proceed through the original neural network. This creates a fast-path/slow-path execution model.
- **Core assumption:** A meaningful fraction of inference samples will match logic flow conditions; the checking overhead for non-matching samples is smaller than the savings from early exits.
- **Evidence anchors:**
  - [table I]: "Exit by Tree" column shows 12.7%–34.1% of samples exit through logic flows across datasets.
  - [section II-C]: "After all logic flows have been processed, the remaining code is added, which continues computation as a regular neural network."
  - [corpus]: Neuro-Channel Networks (arXiv:2601.02253) explores multiplication-free architectures, aligning with the goal of reducing MAC dependency, though via different means.
- **Break condition:** If logic flow hit rate is too low or checking overhead exceeds MAC savings, maximum latency increases (observed in Table I: +0.7% to -1.2% max latency change).

## Foundational Learning

- **Concept:** Decision tree equivalence to neural networks (paper reference [16]: "Neural Networks are Decision Trees")
  - **Why needed here:** The entire method hinges on understanding that piecewise-linear networks (with activations like ReLU) partition input space into polyhedral regions, each expressible as a tree path.
  - **Quick check question:** Can you explain why a ReLU network with two hidden layers creates more decision regions than a network with one hidden layer?

- **Concept:** Mixed Integer Programming (MIP) feasibility and infeasibility analysis
  - **Why needed here:** Understanding how constraints encode class boundaries and why infeasibility proves a class is unreachable at a given leaf.
  - **Quick check question:** If an MIP problem has no feasible solution, what does that tell you about the constraints?

- **Concept:** Irreducible Infeasible Subsystem (IIS)
  - **Why needed here:** IIS provides the minimal logic conditions—removing any single constraint makes the problem solvable, meaning all are necessary for the guarantee.
  - **Quick check question:** Why is IIS preferred over the full constraint set for generating logic flow conditions?

## Architecture Onboarding

- **Component map:** Trained NN → Decision Tree Construction → Leaf Selection → MIP Verification → IIS Extraction → Logic Flow Generation → Hybrid C Code Generation
- **Critical path:** MIP verification is the computational bottleneck. The number of MIP problems scales with (number of leaves × number of classes). For deep networks, this becomes expensive.
- **Design tradeoffs:**
  - **Tree depth vs. logic flow coverage:** Deeper trees capture more nuanced decisions but increase MIP complexity and may reduce constant-leaf frequency.
  - **Number of logic flows vs. overhead:** More extracted flows increase hit rate but add checking overhead for non-matching samples.
  - **Training data coverage vs. tree fidelity:** Using only training data prunes branches but may miss valid inference paths not represented in training.
- **Failure signatures:**
  - **No constant leaves found:** Network decision boundaries are too entangled; consider network architecture changes (fewer layers, different activation patterns).
  - **Negative average latency reduction:** Logic flow hit rate too low; checking overhead exceeds savings. Examine data distribution shift between training and inference.
  - **Maximum latency increase > 5%:** Too many logic flows checked before fallback; prioritize flows by hit frequency.
- **First 3 experiments:**
  1. Replicate MNIST binary (even/odd) result with provided code (https://github.com/TUDa-HWAI/NN2Logic). Measure per-sample latency distribution to verify early-exit behavior.
  2. Profile MIP solve time vs. network depth. Start with 2-layer network, incrementally add layers to identify scalability limits.
  3. Measure logic flow hit rate on held-out test data vs. training data. Quantify distribution shift impact on hybrid execution efficiency.

## Open Questions the Paper Calls Out
- Future work includes extending the framework into deep neural networks with multi-class classification.

## Limitations
- MIP solve time grows exponentially with network size, making deep networks computationally expensive
- Logic flow extraction may not generalize to non-fully-connected architectures like CNNs
- Scalability concerns for deep networks remain untested beyond the 3-layer networks in experiments

## Confidence
- **High confidence**: Mechanism 1 (tree-based conversion using training data coverage), Mechanism 3 (hybrid execution model)
- **Medium confidence**: Mechanism 2 (MIP-based IIS extraction) - method is sound but computational feasibility for large networks uncertain
- **Low confidence**: Generalizability to other network architectures and datasets beyond the three tested

## Next Checks
1. **Reproduce core results**: Clone https://github.com/TUDa-HWAI/NN2Logic, run on MNIST binary classification, verify latency reduction matches Table I (14.9% average reduction)
2. **Profile computational bottlenecks**: Measure MIP solve time vs. network depth (start with 2-layer, increment to 5-layer networks) to identify scalability limits
3. **Test distribution shift sensitivity**: Compare logic flow hit rates between training and held-out test data to quantify performance degradation under data distribution changes