---
ver: rpa2
title: 'UWBa at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim
  Retrieval'
arxiv_id: '2508.09517'
source_url: https://arxiv.org/abs/2508.09517
tags:
- nv-embed
- data
- uni00000013
- text
- mistral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a zero-shot approach to multilingual fact-checked
  claim retrieval, addressing the challenge of efficiently identifying relevant fact-checks
  for social media posts across multiple languages. The method uses large language
  models (GPT, Mistral, and NVIDIA NV-Embed-v2) to generate text embeddings, which
  are compared via cosine similarity to find the most relevant claims.
---

# UWBa at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval

## Quick Facts
- **arXiv ID:** 2508.09517
- **Source URL:** https://arxiv.org/abs/2508.09517
- **Reference count:** 2
- **Primary result:** 7th place in monolingual, 9th place in cross-lingual fact-checked claim retrieval

## Executive Summary
This work presents a zero-shot approach to multilingual fact-checked claim retrieval, addressing the challenge of efficiently identifying relevant fact-checks for social media posts across multiple languages. The method uses large language models (GPT, Mistral, and NVIDIA NV-Embed-v2) to generate text embeddings, which are compared via cosine similarity to find the most relevant claims. Models are combined based on performance per language, and only English translations are used due to poor multilingual model performance. The system achieved 7th place in monolingual and 9th in cross-lingual subtasks. NV-Embed-v2 was the most effective model, with model combinations improving results for certain languages.

## Method Summary
The system operates by concatenating post text with OCR output, translating everything to English, and generating embeddings using large pre-trained models. These embeddings are compared via cosine similarity against precomputed fact-check embeddings to retrieve the top-10 most relevant claims. The approach is zero-shot, requiring no task-specific training. Model combinations are used for certain languages by merging top-K results from multiple models and deduplicating. NV-Embed-v2 (7B parameters) proved most effective, while smaller multilingual models underperformed due to parameter size limitations.

## Key Results
- Achieved 7th place in monolingual and 9th place in cross-lingual subtasks
- NV-Embed-v2 achieved highest performance (0.902 S@10) among all models tested
- Model combinations improved results for specific languages (e.g., ara: 0.82→0.87, fra: 0.92→0.95)
- Cross-lingual performance degraded ~15 points compared to monolingual (0.902 vs 0.775 S@10)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Zero-shot text embeddings from large language models can retrieve semantically relevant fact-checks without task-specific training.
- **Mechanism:** Large pre-trained models encode semantic meaning into dense vector representations; cosine similarity between post and claim embeddings surfaces topically aligned fact-checks.
- **Core assumption:** Pre-trained embeddings capture sufficient task-relevant semantic relationships (claim-post alignment) despite no fine-tuning on fact-check data.
- **Evidence anchors:** [abstract] "uses large language models (GPT, Mistral, and NVIDIA NV-Embed-v2) to generate text embeddings, which are compared via cosine similarity"; [section 3] "The system operates in a zero-shot setting, leveraging multiple TEMs to obtain sentence representations"; [corpus] Neighboring systems use similar embedding-based retrieval.

### Mechanism 2
- **Claim:** Translating all inputs to English and using English-centric embedding models outperforms native multilingual models for this retrieval task.
- **Mechanism:** High-quality English translation preserves core semantic content; larger English-centric models (7B+ parameters) have superior representation capacity compared to smaller multilingual models (~350M-434M parameters).
- **Core assumption:** Translation quality is sufficiently high that information loss is less severe than the quality gap between multilingual and English-centric embedding models.
- **Evidence anchors:** [abstract] "only English translations are used due to poor multilingual model performance"; [section 5.2] "We attribute the poor performance of the GTE models to their smaller parameter sizes (approximately 350M for mGTE and 434M for GTE) compared to the other models. In contrast, NV-Embed has about 7B parameters"; [corpus] TIFIN India paper also leverages translation.

### Mechanism 3
- **Claim:** Combining top-k results from multiple embedding models improves retrieval for certain languages.
- **Mechanism:** Different models encode slightly different semantic aspects; ensemble selection reduces single-model blindspots while duplication indicates higher confidence.
- **Core assumption:** Models have complementary error patterns; duplicated retrievals across models signal higher relevance.
- **Evidence anchors:** [abstract] "Models are combined based on performance per language, and only English translations are used... model combinations improving results for certain languages"; [section 3.1] "When combining two models, we select the five most similar claims for each TEM and put them together. If the resulting set contains duplicities, we remove them and add claims from positions six and further"; [section 5.2, Table 6] Shows GPT & NV-Embed improves ara (0.82→0.87), fra (0.92→0.95), spa (0.89→0.92) vs. GPT alone.

## Foundational Learning

- **Concept: Text Embedding Models (TEMs) and pooling strategies**
  - **Why needed here:** Core architecture choice affects how sentences become vectors; understanding [CLS] pooling vs. mean pooling vs. latent attention determines implementation.
  - **Quick check question:** How does NV-Embed's latent attention layer differ from GTE's [CLS] token approach for generating sentence representations?

- **Concept: Zero-shot transfer in retrieval**
  - **Why needed here:** System makes no use of training labels; relies entirely on pre-trained representations.
  - **Quick check question:** What specific assumption must hold for a model trained on general text to perform well on fact-check retrieval without any task-specific examples?

- **Concept: Success@K (S@K) metric**
  - **Why needed here:** Official evaluation metric; understanding why S@10 masks performance differences that S@5 reveals.
  - **Quick check question:** Why might a model with identical S@10 scores have meaningfully different S@5 scores, and what does this indicate about retrieval quality?

## Architecture Onboarding

- **Component map:** Input preprocessing (concatenate text + OCR) -> Translation layer (original → English) -> Embedding generation (NV-Embed / GPT / Mistral) -> Similarity computation (cosine similarity across claim corpus) -> Ranking & combination (top-k per model → merge) -> Output: Top 10 ranked claims per post

- **Critical path:** Embedding quality is the bottleneck—Table 5 shows 14+ point gap between NV-Embed (0.902) and GTE (0.777) on monolingual S@10. Model size and training quality directly determine retrieval performance.

- **Design tradeoffs:**
  - Model size vs. inference cost: NV-Embed (7B, requires 4-bit quantization on 48GB GPU) vs. GTE (~400M, faster but 13+ points worse)
  - Translation vs. native multilingual: Translation adds preprocessing step but enables use of stronger English models
  - Single model vs. combination: Simpler deployment vs. language-specific optimization (Table 1 shows 5/11 languages use combinations)

- **Failure signatures:**
  - High S@10-to-S@5 drop indicates relevant claims ranked in positions 6-10 rather than 1-5 (Table 8 shows 4-8% average drop)
  - Cross-lingual performance ~15 points lower than monolingual (Table 5: 0.902 vs. 0.775) indicates language barrier remains significant
  - 25-30% of fact-checks missed entirely (ranked >10) even with best model (Figure 1, Table 7)

- **First 3 experiments:**
  1. **Reproduce NV-Embed monolingual baseline** on development data: Concatenate text+OCR, translate to English, generate embeddings, compute S@10. Target: ~0.90 per Table 5.
  2. **Ablate translation step:** Compare English-translated vs. original-language mGTE embeddings to quantify translation impact (expect ~2-3 point difference based on GTE vs. mGTE gap in Table 5).
  3. **Test single-model vs. combination:** For a language showing improvement (e.g., ara: GPT 0.82 → GPT+NV-Embed 0.87), analyze which claims are recovered by combination that single model misses.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why does model combination improve monolingual retrieval performance but fail to yield improvements in the cross-lingual subtask?
- **Basis in paper:** [explicit] The paper states in Section 3.1 that "for the cross-lingual scenario, only the NV-embed model was used, as the combinations did not lead to improvement," despite combinations working for monolingual tasks.
- **Why unresolved:** The authors report the negative result (combinations failing) but do not provide an error analysis or theoretical explanation for why ensemble diversity helps within languages but not across them.
- **What evidence would resolve it:** An analysis of the embedding space geometry comparing monolingual vs. cross-lingual distributions, or ablation studies testing different combination strategies (e.g., weighted averaging vs. reciprocal rank fusion) specifically on cross-lingual pairs.

### Open Question 2
- **Question:** Can scaling multilingual embedding models bridge the performance gap with English-centric models used on translated text?
- **Basis in paper:** [inferred] The authors discarded the multilingual mGTE model in favor of English-centric models (NV-Embed, GPT) applied to translations. They attribute mGTE's poor performance to its smaller parameter size (approx. 350M) compared to the 7B parameter NV-Embed, leaving the "translation vs. native multilingual" trade-off unresolved for models of equal size.
- **Why unresolved:** The comparison was confounded by model scale; it is unclear if the superior performance of English models was due to the translation pipeline or simply the larger model capacity.
- **What evidence would resolve it:** A comparative experiment using a large-scale multilingual embedding model (e.g., a multilingual E5-mistral or similar 7B-scale multilingual model) evaluated on original texts against the English-centric translation pipeline.

### Open Question 3
- **Question:** What linguistic or data characteristics cause the significant variance in performance gaps (S@10 vs. S@5) observed across different languages?
- **Basis in paper:** [inferred] The authors note an "interesting observation" where the percentage difference between S@5 and S@10 results "varies significantly" between languages (e.g., English vs. French), but they do not investigate the cause of this inconsistency in the error analysis.
- **Why unresolved:** The paper reports the metric drops but does not clarify if the variance stems from translation quality, morphological richness, or dataset imbalances specific to those languages.
- **What evidence would resolve it:** A correlation analysis between S@5-to-S@10 drop rates and metrics such as translation error rates, syntactic tree depth, or average polysemy counts for the specific languages in the dataset.

## Limitations

- The approach requires English translation of all inputs, potentially losing language-specific context or cultural nuances critical for claim matching.
- Performance degradation in cross-lingual retrieval (~15 points lower than monolingual) indicates persistent language barriers despite translation.
- The paper doesn't address computational cost or latency considerations for the 7B-parameter NV-Embed model, which requires significant GPU resources (48GB for 4-bit quantization).

## Confidence

- **High confidence:** Zero-shot embedding retrieval works for this task (7th/9th place rankings, consistent results across development and test sets)
- **Medium-high confidence:** English-centric models outperform multilingual models when using translation (strong evidence from parameter size comparisons and Table 5 results)
- **Medium confidence:** Model combination approach is effective for specific languages but not universally beneficial (5/11 languages)
- **Medium confidence:** Claims about why GTE models underperform based on parameter size analysis but lack direct ablation studies

## Next Checks

1. **Ablation study on translation quality:** Compare retrieval performance using original language vs. English-translated inputs for languages where multilingual models are available, quantifying the translation impact beyond the observed parameter size differences.

2. **Cross-lingual error analysis:** For cross-lingual cases where performance drops 15+ points, analyze which types of fact-checks are missed (e.g., cultural references, idiomatic expressions) to understand systematic limitations.

3. **Model combination optimization:** Systematically test which model pairs provide complementary information across all languages, rather than the current language-specific approach, to determine if universal combination strategies exist.