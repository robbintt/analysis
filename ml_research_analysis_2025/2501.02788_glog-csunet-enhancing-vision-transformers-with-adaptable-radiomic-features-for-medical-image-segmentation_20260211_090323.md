---
ver: rpa2
title: 'GLoG-CSUnet: Enhancing Vision Transformers with Adaptable Radiomic Features
  for Medical Image Segmentation'
arxiv_id: '2501.02788'
source_url: https://arxiv.org/abs/2501.02788
tags:
- image
- segmentation
- medical
- gabor
- glog-csunet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of improving medical image semantic
  segmentation by enhancing Vision Transformer (ViT) models to better capture local
  spatial information, particularly for fine anatomical details in small datasets.
  The proposed GLoG-CSUnet architecture integrates learnable Gabor and Laplacian of
  Gaussian (LoG) filters with the Convolutional Swin-Unet framework, combining the
  long-range dependency modeling of Transformers with radiomic feature extraction
  capabilities.
---

# GLoG-CSUnet: Enhancing Vision Transformers with Adaptable Radiomic Features for Medical Image Segmentation

## Quick Facts
- arXiv ID: 2501.02788
- Source URL: https://arxiv.org/abs/2501.02788
- Reference count: 23
- Primary result: 1.14% DSC improvement on Synapse, 0.99% on ACDC with only 15-30 additional parameters

## Executive Summary
This study introduces GLoG-CSUnet, a Vision Transformer-based architecture that integrates learnable Gabor and Laplacian of Gaussian filters to enhance medical image segmentation performance. The method addresses ViTs' limitations in capturing local spatial information by injecting radiomic features directly into the patch embedding stage. Evaluated on Synapse multi-organ and ACDC cardiac datasets, the approach demonstrates significant improvements over state-of-the-art models while maintaining computational efficiency with minimal parameter overhead.

## Method Summary
GLoG-CSUnet extends the Convolutional Swin-Unet (CSUnet) framework by incorporating learnable Gabor and LoG filter layers in the patch embedding pipeline. The Gabor Transformation Unit extracts texture and directional features using trainable wavelength, orientation, phase offset, sigma, and aspect ratio parameters. The LoG Transformation Unit detects edges at adaptive scales through a learnable sigma parameter. Filter outputs are concatenated with the original image patches and processed through convolutional embedding before entering the Transformer backbone. The model is trained from scratch for 300 epochs using dataset-specific hyperparameters and achieves performance gains with only 15-30 additional parameters.

## Key Results
- Synapse dataset: 83.36% DSC (1.14% improvement over baseline CSUnet)
- ACDC dataset: 92.28% DSC (0.99% improvement over baseline)
- Notable gains on challenging structures: gallbladder (73.20% DSC), stomach (82.10% DSC)
- Computational efficiency: Only 15-30 additional parameters added to 24M parameter model

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Learnable Gabor filters adaptively capture texture and directional features that fixed convolutional kernels may miss, particularly for organs with complex tissue patterns.
- **Mechanism:** The Gabor Transformation Learning Unit parameterizes filters with trainable wavelength (λ), orientation (θ), phase offset (ψ), sigma (σ), and aspect ratio (γ). During backpropagation, these parameters update to optimize texture extraction for the specific dataset, rather than relying on handcrafted values.
- **Core assumption:** Medical images contain discriminative texture patterns that, when explicitly extracted, improve boundary discrimination between anatomical structures.
- **Evidence anchors:**
  - [abstract] "integrates dynamically adaptive Gabor and Laplacian of Gaussian (LoG) filters to capture texture, edge, and boundary information"
  - [Section II-B-1] Defines learnable parameters λ, θ, ψ, σ, γ that "are learned during the training process"
  - [corpus] Related work on Vision Transformers notes they "struggle to balance long-range dependency acquisition with computational efficiency" (UNetVL), suggesting complementary local feature extraction is valuable
- **Break condition:** If the target anatomy lacks textural variation (e.g., uniformly homogeneous structures), learned Gabor parameters may converge to trivial solutions without improving segmentation.

### Mechanism 2
- **Claim:** Learnable LoG filters enhance boundary precision by detecting regions of rapid intensity change at adaptive scales.
- **Mechanism:** The LoG filter smooths the image with a Gaussian (controlled by learnable σ), then applies the Laplacian operator to highlight edges. The trainable σ allows the model to discover optimal edge-detection scales for each dataset's resolution and anatomical characteristics.
- **Core assumption:** Organ boundaries correspond to intensity discontinuities detectable at specific spatial scales that vary across imaging modalities and structures.
- **Evidence anchors:**
  - [abstract] "enhancing the feature representation processed by the Transformer model"
  - [Section II-B-2] "LoG filters are implemented as learnable filters with a trainable σ parameter"
  - [corpus] Evidence is weak—no direct corpus papers test LoG integration with ViTs; this appears novel to this work
- **Break condition:** If boundaries are defined by gradual intensity gradients rather than sharp edges (e.g., infiltrating lesions), LoG-based detection may fail regardless of learned σ.

### Mechanism 3
- **Claim:** Early integration of radiomic features into patch embedding allows the Transformer backbone to operate on enriched representations without increasing architectural complexity.
- **Mechanism:** Gabor and LoG filter outputs are concatenated with the original image patches before convolutional embedding. This injects domain-specific inductive biases (texture, edges) at the input stage, allowing subsequent self-attention layers to model both local radiomic features and global dependencies jointly.
- **Core assumption:** The Transformer's self-attention can effectively combine handcrafted-inspired features with learned representations when presented together at the patch level.
- **Evidence anchors:**
  - [Section II-A] "The outputs of these units are concatenated and further processed through multiple convolutional layers"
  - [Section IV] "performance improvement, achieved with such a small computational overhead, can be attributed to the integration of learnable radiomic features"
  - [corpus] Swin-UMamba notes "accurate medical image segmentation demands the integration of multi-scale information, spanning from local features to global dependencies"
- **Break condition:** If radiomic features conflict with learned patch embeddings (e.g., introducing noise or artifacts), the concatenation strategy may degrade rather than enhance the Transformer's representations.

## Foundational Learning

- **Concept: Vision Transformer (ViT) Patch Embedding**
  - Why needed here: GLoG-CSUnet modifies the standard patch embedding pipeline; understanding how images become tokens is essential for grasping where and how radiomic features are injected.
  - Quick check question: Can you explain why ViT splits images into patches and what information each patch token carries before self-attention?

- **Concept: Gabor Filter Parameters and Their Physical Meaning**
  - Why needed here: The model learns λ, θ, ψ, σ, γ—knowing what each controls (scale, orientation, phase, bandwidth) helps interpret whether learned filters are sensible.
  - Quick check question: If a Gabor filter learns θ ≈ π/4, what type of structure might it be specialized to detect?

- **Concept: Laplacian of Gaussian for Blob and Edge Detection**
  - Why needed here: The LoG unit's σ parameter controls the scale of edge detection; understanding this helps debug cases where boundaries are over- or under-detected.
  - Quick check question: What happens to LoG detection when σ is too large relative to the target structure's size?

## Architecture Onboarding

- **Component map:**
  Input Image → Gabor Transformation Unit + LoG Transformation Unit → Concatenation → Convolutional Patch Embedding → CST Blocks → Segmentation Head

- **Critical path:**
  1. Configure number of Gabor and LoG filters (paper uses 2 Gabor + 5 LoG for Synapse; 5 Gabor + 5 LoG for ACDC)
  2. Initialize filter parameters (paper does not specify initialization strategy—Assumption: may use standard radiomics defaults or random)
  3. Ensure concatenated features pass through shared convolutional embedding before Transformer

- **Design tradeoffs:**
  - Fewer filters vs. representation power: Only 15-30 parameters added, but may limit expressiveness for complex multi-organ tasks
  - Dataset-specific filter counts: ACDC uses more Gabor filters (5 vs. 2), suggesting cardiac MRI benefits from richer texture encoding than abdominal CT
  - Training from scratch vs. pre-trained backbones: Paper trains from scratch for 300 epochs; results may differ with pre-trained weights (Assumption: not tested per paper)

- **Failure signatures:**
  - Dice improvement <0.3% on new datasets → learned filters may not generalize; check if parameter updates are occurring
  - High HD95 despite good DSC → boundary detection failing; inspect LoG σ values for trivial convergence
  - No gradient flow to filter parameters → possible implementation bug in learnable filter construction

- **First 3 experiments:**
  1. Reproduce baseline CSUnet on Synapse (target: ~82.21% DSC) to validate training pipeline before adding radiomic units
  2. Ablate each filter type separately (Gabor-only, LoG-only) on a single dataset to confirm paper's finding that combination yields best results (92.01% vs. 91.83% vs. 92.28% on ACDC)
  3. Visualize learned filter parameters after training—check if Gabor orientations align with anatomical structures (e.g., θ clustering near vessel directions) to assess interpretability

## Open Questions the Paper Calls Out
- **Open Question 1:** How do the learned parameters (σ, θ, λ) in the Gabor and LoG layers correlate with specific clinical radiomic signatures or anatomical features?
  - Basis in paper: [explicit] The Discussion states: "Future work could explore the interpretability of the learned filters, potentially providing insights into the specific image features most relevant for different anatomical structures."
  - Why unresolved: The study validates performance improvements (Dice scores) but does not analyze the internal convergence values of the filter parameters to determine if they correspond to known biological textures.
  - What evidence would resolve it: A correlation analysis between the learned filter outputs and manually annotated texture features, or visualizations showing filter activation maps on specific organ boundaries.

- **Open Question 2:** Does the integration of learnable radiomic layers provide similar performance benefits when applied to pure Transformer or pure CNN backbones?
  - Basis in paper: [inferred] The Abstract claims the design "allows integration with various base models," but experiments are restricted to the hybrid Convolutional Swin-Unet (CSUnet).
  - Why unresolved: It is unclear if the gains are specific to the CSUnet's hybrid architecture or if they generalize to other segmentation frameworks like pure Swin-Unet or standard U-Net.
  - What evidence would resolve it: Comparative results showing the performance delta of adding GLoG layers to alternative backbones such as TransUNet or nnU-Net.

- **Open Question 3:** Can the proposed 2D learnable filter mechanism be effectively extended to handle 3D volumetric segmentation without losing computational efficiency?
  - Basis in paper: [inferred] The method is evaluated on 2D axial slices (Synapse and ACDC), despite the clinical necessity of volumetric analysis mentioned in the Introduction.
  - Why unresolved: The mathematical formulation defines 2D Gabor and LoG functions; applying these slice-by-slice may miss inter-slice continuity, while 3D kernels increase parameter count and complexity.
  - What evidence would resolve it: Implementation of 3D learnable filters within a volumetric architecture, evaluated on a 3D dataset (e.g., BraTS) to assess trade-offs between accuracy and computational overhead.

## Limitations
- Performance results are limited to two specific datasets (Synapse and ACDC), restricting generalizability to other anatomies or imaging modalities
- The study does not explore filter initialization strategies or their impact on learned parameter convergence
- No comparison between learned vs. fixed radiomic filter parameters to isolate the contribution of adaptability

## Confidence
- **High:** GLoG-CSUnet improves segmentation performance on Synapse and ACDC datasets as reported, with clear DSC increases (1.14% and 0.99%) and consistent HD95 improvements
- **Medium:** The broader claim that this approach will generalize to other anatomies or modalities, since results are confined to CT abdomen and cardiac MRI with specific organ sets
- **Low:** Understanding where gains originate, as the study lacks comparison to other radiomic-filter integrations and pre-trained weight experiments

## Next Checks
1. **Dataset generalization test:** Apply GLoG-CSUnet to a third dataset (e.g., LiTS liver segmentation) with different imaging modality and organ count to assess cross-domain robustness
2. **Filter parameter stability analysis:** Track learned Gabor orientations (θ) and LoG scales (σ) across training epochs to verify they converge to interpretable, anatomically meaningful values rather than trivial solutions
3. **Ablation of initialization strategies:** Compare learned vs. fixed radiomic filter parameters on Synapse to isolate the contribution of adaptability from the presence of radiomic features themselves