---
ver: rpa2
title: 'UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs'
arxiv_id: '2506.09450'
source_url: https://arxiv.org/abs/2506.09450
tags:
- tasks
- reasoning
- beliefs
- simtom
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UniToMBench, a unified benchmark that integrates
  perspective-taking techniques from SimToM with the diverse evaluation metrics of
  TOMBENCH to systematically assess and improve Theory of Mind (ToM) capabilities
  in large language models (LLMs). The benchmark combines a custom dataset of over
  1,000 hand-written scenarios, including multi-interaction tasks and evolving story
  scenarios, with existing ToM benchmarks to evaluate models' ability to understand
  and simulate human mental states.
---

# UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs

## Quick Facts
- **arXiv ID:** 2506.09450
- **Source URL:** https://arxiv.org/abs/2506.09450
- **Reference count:** 4
- **Primary result:** UniToMBench integrates SimToM perspective-taking with TOMBENCH metrics to evaluate and improve ToM in LLMs, showing GPT-4o/4o Mini achieve >80% accuracy on emotional tasks but performance varies significantly on knowledge-based tasks.

## Executive Summary
This paper introduces UniToMBench, a unified benchmark that combines perspective-taking techniques from SimToM with TOMBENCH's diverse evaluation metrics to systematically assess Theory of Mind capabilities in large language models. The benchmark features a custom dataset of over 1,000 hand-written scenarios, including multi-interaction tasks and evolving story scenarios, to evaluate models' ability to understand and simulate human mental states. Evaluation results demonstrate that GPT-4o and GPT-4o Mini achieve consistently high accuracy in emotional and belief-related tasks, but performance varies significantly across knowledge-based tasks. The findings highlight both strengths and limitations of current LLMs in ToM reasoning, establishing UniToMBench as a comprehensive tool for future development.

## Method Summary
UniToMBench integrates perspective-taking techniques from SimToM with TOMBENCH evaluation metrics to assess Theory of Mind capabilities in LLMs. The benchmark combines a custom dataset of 1,025 hand-written scenarios (500 multi-interaction tasks and 525 evolving story tasks) with existing ToM benchmarks. Evaluation uses zero-shot inference with and without SimToM perspective-taking prompts (temperature 0.7) across multiple models including GPT-3.5 Turbo, GPT-4o, GPT-4o Mini, Llama 3 8B Instruct Turbo, and Gemma 2 27B. The methodology tests accuracy on multi-choice questions derived from narrative scenarios, analyzing performance differences between baseline and SimToM-enhanced inference to identify strengths and limitations in ToM reasoning.

## Key Results
- GPT-4o and GPT-4o Mini achieve consistently high accuracy (>80%) on emotional and belief-related tasks
- Performance varies significantly across knowledge-based tasks, with models struggling on false belief and faux-pas detection
- SimToM improves performance on some task types but degrades accuracy on others, particularly for smaller models (Llama 3 8B and Gemma 2 27B)
- Multi-interaction tasks reveal failures in belief-tracking across conversational turns that static benchmarks miss

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explicit perspective-taking prompts can improve ToM reasoning in some task types but degrade performance in others.
- **Mechanism:** SimToM uses a two-stage prompting method where models first simulate a character's knowledge state before answering, filtering context to what that character could know. This reduces conflation between model knowledge and hypothetical beliefs.
- **Core assumption:** LLMs can be instructed to maintain distinct mental state representations when cued explicitly.
- **Evidence anchors:**
  - [abstract] "integrates perspective-taking techniques from SimToM"
  - [section 2.2] "SimToM improves zero-shot ToM performance without requiring additional fine-tuning"
  - [corpus] Related work (EnigmaToM, PersuasiveToM) confirms active research in reasoning-based ToM enhancement, though direct comparative evidence is limited.
- **Break condition:** Overgeneralization occurs when forced perspective-taking introduces irrelevant details or causes hallucination in tasks requiring objective fact retrieval rather than mental state inference (see Section 3 on SimToM limitations).

### Mechanism 2
- **Claim:** Multi-interaction task designs expose failures in belief-tracking across conversational turns that static benchmarks miss.
- **Mechanism:** Scenarios require models to track evolving intentions, beliefs, and motives across sequential dialogue exchanges, maintaining coherence. The 500 multi-interaction tasks in the custom dataset test this explicitly.
- **Core assumption:** Social cognition develops through recognizing conversational dynamics across speakers, which AI should similarly demonstrate.
- **Evidence anchors:**
  - [section 4.1] "Effective performance necessitates the ability to infer evolving intentions, beliefs, and underlying motives while maintaining coherence throughout extended dialogues"
  - [section 5] "Llama 3 8B Instruct Turbo and Gemma 2 27B exhibited a decline... struggled to maintain coherence in long-form contexts"
  - [corpus] No direct corpus evidence on multi-interaction ToM; this appears novel to UniToMBench.
- **Break condition:** Smaller or less capable models fail to track belief updates and conflate agent knowledge states.

### Mechanism 3
- **Claim:** Evolving story scenarios assess temporal adaptation of mental state reasoning better than isolated one-shot queries.
- **Mechanism:** Characters progress through distinct phases (motivation, adversity, resolution); models must track shifts in dynamics and adapt reasoning. The 525 evolving story tasks capture this progression.
- **Core assumption:** Human social relationships evolve, and ToM reasoning must account for changing perceptions over time.
- **Evidence anchors:**
  - [section 4.1] "Theory of Mind (ToM) reasoning necessitates an awareness of how such relational changes influence behavior and emotional responses"
  - [table 6] GPT-4o achieved 89.9% baseline / 90.1% SimToM on evolving stories; GPT-3.5 dropped from 69.5% to 55.8%
  - [corpus] "Towards Dynamic Theory of Mind" paper addresses temporal evolution of mental states, aligning with this mechanism.
- **Break condition:** Models rely on surface-level pattern matching rather than flexible tracking; abstract reasoning and implicit cues remain challenging.

## Foundational Learning

- **Concept: Theory of Mind (ToM) taxonomy**
  - **Why needed here:** UniToMBench evaluates 31+ cognitive abilities (false belief, emotion attribution, faux-pas detection, scalar implicature). Understanding what each tests prevents misinterpretation of results.
  - **Quick check question:** Can you distinguish a false-belief task from a faux-pas recognition task?

- **Concept: Perspective-taking vs. objective reasoning tradeoff**
  - **Why needed here:** SimToM improves some tasks but degrades others. Knowing when to apply perspective-taking filtering is critical for deployment.
  - **Quick check question:** In what scenario would forcing a model to adopt a character's viewpoint reduce accuracy?

- **Concept: Multi-agent belief tracking**
  - **Why needed here:** The benchmark reveals models conflate knowledge across agents, responding as if all share the same information. This is a primary failure mode to watch.
  - **Quick check question:** If Agent A knows X but Agent B does not, what error pattern might an LLM exhibit?

## Architecture Onboarding

- **Component map:** Custom dataset (1,025 scenarios) + TOMBENCH questions → unified prompt format (3-5 sentence narrative + 4-choice question) → Baseline inference vs. SimToM-enhanced inference → Accuracy scoring, error attribution analysis

- **Critical path:**
  1. Load scenario from dataset (multi-interaction OR evolving story OR TOMBENCH category)
  2. Format with appropriate prompt (baseline vs. SimToM)
  3. Query model at temperature 0.7
  4. Extract answer choice, compute accuracy vs. ground truth
  5. Categorize errors (misidentified motivation, failed belief tracking, implicit cue misinterpretation)

- **Design tradeoffs:**
  - SimToM increases reasoning depth but adds prompt complexity and can trigger hallucination
  - Multiple-choice format enables scalable evaluation but limits open-ended reasoning assessment
  - Hand-written scenarios ensure quality but may not fully represent real-world diversity

- **Failure signatures:**
  - **Perspective collapse:** Model assumes all agents share same knowledge (observed in SimToM-enhanced runs on TOMBENCH)
  - **Faux-pas blindness:** Models fail to distinguish blunt speech from inappropriate remarks (sensitivity dropped in some configs)
  - **Context abandonment:** Longer narratives cause earlier context to be ignored

- **First 3 experiments:**
  1. Replicate baseline vs. SimToM comparison on a single TOMBENCH category (e.g., False Belief Task) to validate pipeline
  2. Test a new model not in the paper using the multi-interaction task subset to characterize its belief-tracking capacity
  3. Ablate prompt length on evolving stories to isolate whether degradation stems from context window limits vs. reasoning failure

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why does SimToM-based perspective-taking improve ToM performance in OpenAI models (GPT-4o, GPT-4o Mini) but degrade performance in Llama 3 8B and Gemma 2 27B on multi-interaction and evolving story tasks?
- **Basis in paper:** [explicit] The authors report in Section 5 that "Llama 3 8B Instruct Turbo and Gemma 2 27B exhibited a decline in certain areas when transitioning from the baseline to SimToM models, particularly in evolving story and multi-interaction scenarios."
- **Why unresolved:** The paper does not investigate the architectural or training differences that might cause this divergence, nor whether it relates to model size, instruction tuning, or other factors.
- **What evidence would resolve it:** Ablation studies across model families with controlled parameter counts and training procedures, plus analysis of attention patterns during perspective-taking prompts.

### Open Question 2
- **Question:** How can ToM benchmarks be expanded to better capture cultural diversity and the full complexity of real-world social interactions?
- **Basis in paper:** [explicit] Section 8 states: "The created tasks might not fully represent the diversity and complexity of real-world social interactions because of the limited scope of our dataset. [...] This may be mitigated in future research by using datasets that reflect a wider range of social and cultural variations."
- **Why unresolved:** The current dataset uses hand-written scenarios that may reflect limited cultural perspectives, and systematically creating culturally-diverse ToM scenarios remains an open challenge.
- **What evidence would resolve it:** Development of ToM benchmarks with scenarios validated across multiple cultural contexts, coupled with cross-cultural model performance comparisons.

### Open Question 3
- **Question:** What architectural or training improvements beyond prompting could enable LLMs to maintain distinct mental states for multiple agents without knowledge conflation?
- **Basis in paper:** [explicit] Section 6 identifies that "in multi-agent scenarios, the models struggled to track distinct beliefs, sometimes assuming all characters had the same knowledge even when the scenario clearly established otherwise."
- **Why unresolved:** The paper evaluates prompting-based interventions but does not explore whether architectural modifications or specialized training objectives could better address this fundamental limitation.
- **What evidence would resolve it:** Experiments with models trained on multi-agent belief tracking objectives or architectures with explicit mental state representations, evaluated on UniToMBench's multi-interaction tasks.

## Limitations

- The custom dataset of 1,025 scenarios may not capture the full diversity of real-world social interactions and mental state reasoning scenarios
- The multiple-choice format constrains evaluation to predefined options, potentially missing nuanced reasoning capabilities
- Performance differences between baseline and SimToM-enhanced inference are not fully explained mechanistically
- Reliance on commercial LLM APIs raises reproducibility concerns due to potential version variations

## Confidence

**High Confidence:** The baseline accuracy results for GPT-4o and GPT-4o Mini (consistently above 80% on emotional tasks) are well-supported by the evaluation methodology and align with expectations for state-of-the-art models on ToM tasks.

**Medium Confidence:** The claims about SimToM's effectiveness show mixed support. While improvements in some categories are demonstrated, the mechanism explaining when and why perspective-taking helps versus hurts remains incompletely specified.

**Low Confidence:** The comparative performance analysis across model families (especially smaller models like Llama 3 8B and Gemma 2 27B) has limited confidence due to the small sample size of models tested and potential API-specific performance variations.

## Next Checks

1. **Cross-validation with alternative datasets:** Test UniToMBench's evaluation pipeline on established ToM benchmarks like TOMBENCH's original implementation or the recently published EnigmaToM dataset to verify consistency of results and identify potential dataset-specific biases.

2. **Ablation study of SimToM components:** Systematically remove or modify components of the SimToM prompting approach (e.g., perspective-taking instructions, context filtering rules) to isolate which elements drive performance improvements versus degradations across different task types.

3. **Open-ended response analysis:** Supplement the multiple-choice evaluation with open-ended response generation on a subset of tasks, using both automated coherence scoring and human evaluation to assess whether models demonstrate genuine ToM reasoning or surface-level pattern matching.