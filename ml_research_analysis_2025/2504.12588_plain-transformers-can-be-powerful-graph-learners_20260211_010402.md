---
ver: rpa2
title: Plain Transformers Can be Powerful Graph Learners
arxiv_id: '2504.12588'
source_url: https://arxiv.org/abs/2504.12588
tags:
- graph
- attention
- expressivity
- conf
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper demonstrates that plain Transformer architectures can
  be powerful graph learners with three simple modifications: (1) simplified L2 attention
  to capture both angle and magnitude information, (2) adaptive RMS normalization
  to preserve token magnitude information, and (3) MLP-based stem for positional encoding.
  The resulting Powerful Plain Graph Transformers (PPGT) achieve theoretical expressivity
  beyond 1-WL and up to 3-WL equivalence, matching subgraph GNNs and higher-order
  GNNs.'
---

# Plain Transformers Can be Powerful Graph Learners

## Quick Facts
- **arXiv ID:** 2504.12588
- **Source URL:** https://arxiv.org/abs/2504.12588
- **Reference count:** 39
- **Primary result:** Plain Transformers with three modifications achieve 3-WL expressivity, matching subgraph GNNs on graph benchmarks

## Executive Summary
This paper demonstrates that standard Transformer architectures can be powerful graph learners without complex modifications. By introducing three simple modifications—simplified L2 attention, adaptive RMS normalization, and MLP-based positional encoding—the authors create Powerful Plain Graph Transformers (PPGT) that achieve both theoretical expressivity beyond the 1-WL test and strong empirical performance on graph benchmarks. PPGT outperforms existing graph Transformers and MPNNs on multiple datasets including ZINC, MNIST, and large-scale molecular property prediction tasks.

## Method Summary
The PPGT architecture modifies standard Transformers with three key innovations: (1) Simplified L2 (sL2) attention that captures both angle and magnitude information by adding a bias term to standard dot-product attention, (2) Adaptive RMS normalization (AdaRMSN) that preserves token magnitude information through learnable scaling parameters, and (3) MLP-based stem with Sinusoidal PE Enhancement (SPE) that amplifies high-frequency positional signals to prevent spectral bias. These modifications enable the model to distinguish multisets and achieve theoretical expressivity up to 3-WL equivalence.

## Key Results
- PPGT achieves 3-WL theoretical expressivity, matching subgraph GNNs and exceeding standard Transformers
- Outperforms existing graph Transformers on ZINC (MAE 0.0566), MNIST (98.6% accuracy), and large-scale datasets
- Demonstrates that complex architectural modifications are not necessary for effective graph learning with Transformers
- Maintains hardware efficiency through plain Transformer architecture while achieving superior performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Standard token-wise normalization (LayerNorm/RMSN) discards magnitude information necessary for distinguishing graph substructures (multisets), but this can be recovered via adaptive normalization.
- **Mechanism:** The paper introduces **Adaptive RMSN (AdaRMSN)**. Unlike standard RMSN, which normalizes vectors to a unit sphere (magnitude-invariant), AdaRMSN uses a learnable affine transform $\gamma'(x)$ initialized to 1. This allows the network to "recover" the identity transformation and preserve input magnitude if the task requires it, while still maintaining training stability.
- **Core assumption:** Graph isomorphism tasks often rely on distinguishing multisets (e.g., neighbors) where cardinality (count) is encoded in the token magnitude.
- **Evidence anchors:**
  - [abstract] Mentions "adaptive root-mean-square normalization to preserve token magnitude information."
  - [section 2.1] States "cardinalities of multisets are crucial... encoded into the token representation as its magnitude."
  - [corpus] Limited direct support; neighbor papers focus on general Transformer power rather than normalization specifics.
- **Break condition:** If the graph task relies purely on angle-based similarity (cosine) rather than structural density, the adaptive magnitude recovery may overfit to noise.

### Mechanism 2
- **Claim:** Scaled Dot-Product (SDP) attention is biased toward large-magnitude keys and cannot measure magnitude "closeness," which is required for geometric graph learning.
- **Mechanism:** The paper proposes **simplified $L_2$ (sL2) attention**. It reformulates the Euclidean distance attention score into a standard SDP format plus a bias term: $\text{Softmax}(q^\top k / \sqrt{d} - k^\top k / 2\sqrt{d})$. The $-k^\top k$ term penalizes large keys, correcting the bias and enabling the model to sense both angle and magnitude closeness.
- **Core assumption:** Effective graph learning requires measuring the "closeness" of token representations, which implies both direction (angle) and scale (magnitude) alignment.
- **Evidence anchors:**
  - [abstract] Mentions "simplified $L_2$ attention to capture both angle and magnitude information."
  - [section 3.2] Eq. (4) shows the reformulation and Fig. 2 visualizes the bias correction.
- **Break condition:** If hardware optimization for SDP attention cannot efficiently support the additive bias term without memory overhead, the efficiency claim weakens.

### Mechanism 3
- **Claim:** MLPs suffer from "spectral bias" (prioritizing low-frequency modes), causing them to lose fine-grained positional information during the encoding stem.
- **Mechanism:** **Sinusoidal PE Enhancement (SPE)** applies high-frequency sinusoidal functions to the graph Positional Encodings (PEs) before processing them through the MLP stem. This amplifies high-frequency signals, making fine-grained structural differences (e.g., distinguishing $p'_{ij}$ from $p'_{ik}$) learnable.
- **Core assumption:** Standard MLPs acting as "stems" for positional encodings effectively act as low-pass filters, inadvertently stripping necessary structural details.
- **Evidence anchors:**
  - [section 3.4] "MLPs prioritize learning the low-frequency modes... SPE can effectively enhance the signal differences."
  - [figure 3] Visualizes the amplification of signal differences.
- **Break condition:** If the chosen PE (e.g., RRWP) is inherently low-frequency or noisy, amplifying high frequencies might introduce artifacts rather than signal.

## Foundational Learning

- **Concept:** **Weisfeiler-Leman (WL) Test & Multisets**
  - **Why needed here:** The paper frames PPGT's success by its ability to exceed the 1-WL test limit. Understanding that WL tests distinguish graphs by hashing "multisets" (neighborhoods) is crucial to grasping why *magnitude* (cardinality) matters.
  - **Quick check question:** How does the cardinality of a multiset (e.g., $\{a, a, b\}$ vs $\{a, b\}$) relate to the magnitude of a vector embedding in this architecture?

- **Concept:** **Token-wise Normalization (LayerNorm/RMSN)**
  - **Why needed here:** The core innovation (AdaRMSN) is a modification of this standard Transformer component. You must understand that standard normalization forces vectors onto a hypersphere, destroying magnitude data, to see why the "adaptive" part is necessary for graphs.
  - **Quick check question:** Why is strict magnitude invariance (property of LN/RMSN) a disadvantage for distinguishing graph structures described as multisets?

- **Concept:** **Spectral Bias in Neural Networks**
  - **Why needed here:** This explains the motivation behind the Sinusoidal PE Enhancement (SPE).
  - **Quick check question:** Why would a standard MLP struggle to extract detailed positional encodings, and how does adding sinusoidal bases mitigate this?

## Architecture Onboarding

- **Component map:** Graph -> Node Attributes + Edges -> RRWP computation -> SPE (Sinusoidal encoding) -> MLP stem -> Positional Embeddings P -> Transformer backbone (Pre-Norm) -> AdaRMSN -> sL2 Attention
- **Critical path:** The implementation of **sL2 attention** is the most sensitive integration point. It requires calculating the self-dot-product of keys ($k^\top k$) and injecting it as a bias into the attention mechanism without breaking the optimized SDP kernels.
- **Design tradeoffs:**
  - **SPE Complexity:** Adding sinusoidal bases increases stem dimensionality and compute; the paper suggests $S=3$ to $15$ is sufficient (Appendix B.1).
  - **Plain Architecture:** By avoiding MPNNs/Message Passing, the model gains hardware compatibility but retains $O(N^2)$ complexity, limiting scalability on extremely large graphs without sampling.
- **Failure signatures:**
  - **Loss of Expressivity:** If AdaRMSN parameters do not diverge from initialization, the model behaves like a standard Transformer and fails to distinguish multisets (low performance on BREC/CFI benchmarks).
  - **Attention Collapse:** If magnitude is not properly regulated by sL2, attention maps may saturate (focus only on high-magnitude keys).
- **First 3 experiments:**
  1. **Ablation on Normalization:** Compare standard RMSN vs. AdaRMSN on the ZINC dataset to validate the magnitude preservation hypothesis (Replicate Fig 4).
  2. **Expressivity Check:** Run the model on the BREC benchmark (specifically CFI graphs) to verify if the theoretical $>1$-WL expressivity is realized empirically.
  3. **Stem Sensitivity:** Vary the number of sinusoidal bases ($S$) in the SPE module to observe the trade-off between high-frequency detail and overfitting.

## Open Questions the Paper Calls Out
None

## Limitations
- **Hardware Scalability:** Maintains $O(N^2)$ attention complexity, unsuitable for extremely large graphs without sampling strategies
- **Inductive Bias Ambiguity:** Doesn't establish whether theoretical expressivity advantage translates to better generalization on real-world inductive tasks with distribution shift
- **Normalization Stability:** AdaRMSN introduces learnable parameters that could potentially destabilize training on graphs with heterogeneous degree distributions

## Confidence

**High Confidence** (Strong evidence, multiple validations):
- PPGT architecture design and implementation details are clearly specified
- Empirical results on standard benchmarks (ZINC, MNIST) are reproducible and significant
- Theoretical analysis of WL expressivity is rigorous and well-supported

**Medium Confidence** (Reasonable evidence but with gaps):
- The mechanism of how adaptive normalization preserves magnitude information in practice
- The necessity of sinusoidal PE enhancement versus simpler positional encoding alternatives
- The efficiency claims relative to specialized graph Transformers with sampling

**Low Confidence** (Limited or indirect evidence):
- Long-tail performance on extremely large graphs (>100k nodes)
- Generalization to inductive scenarios with distribution shift
- Comparison with specialized hardware-optimized graph learning systems

## Next Checks
1. **Inductive Transfer Test:** Evaluate PPGT on a benchmark with clear train/test distribution shift (e.g., different molecular scaffolds) to verify that theoretical expressivity translates to practical generalization.
2. **Large Graph Scaling:** Test PPGT on graphs with 100K+ nodes using both full attention and approximate attention mechanisms to establish practical scalability limits and trade-offs.
3. **Normalization Ablation Under Heterogeneity:** Run controlled experiments on graphs with varying degree distributions to quantify how AdaRMSN stability varies with graph topology complexity.