---
ver: rpa2
title: 'CURP: Codebook-based Continuous User Representation for Personalized Generation
  with LLMs'
arxiv_id: '2602.00742'
source_url: https://arxiv.org/abs/2602.00742
tags:
- user
- arxiv
- codebook
- personalized
- prototype
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the efficiency-fidelity trade-off in personalized
  generation with LLMs, where existing methods either lack robustness (prompting)
  or incur high computational costs (fine-tuning). To address this, the authors propose
  CURP, a framework that encodes users as sparse combinations of learnable prototypes
  from a shared codebook, replacing raw text histories with structured, interpretable
  embeddings.
---

# CURP: Codebook-based Continuous User Representation for Personalized Generation with LLMs

## Quick Facts
- arXiv ID: 2602.00742
- Source URL: https://arxiv.org/abs/2602.00742
- Reference count: 33
- Primary result: CURP achieves SOTA on 70% of metrics using only ~20M parameters (0.2% of model size) for personalized generation.

## Executive Summary
This paper addresses the efficiency-fidelity trade-off in personalized text generation with LLMs, where existing methods either lack robustness (prompting) or incur high computational costs (fine-tuning). CURP encodes users as sparse combinations of discrete prototypes from a shared codebook, replacing raw text histories with structured, interpretable embeddings. The method uses a two-stage training pipeline: first constructing a generalizable prototype codebook from large-scale user behavior data, then aligning these prototypes to the LLM's embedding space via a lightweight adapter. Experiments across four personalized generation tasks show that CURP consistently outperforms strong baselines, achieving state-of-the-art performance while using only about 20M parameters—about 0.2% of the total model size.

## Method Summary
CURP is a two-stage framework for personalized generation. Stage 1 (Prototype Codebook Construction, PCC) learns a shared codebook of user behavior prototypes from large-scale behavioral data using balanced k-means and a composite loss function (reconstruction + diversity + usage). Stage 2 (Prototype-Based Alignment, PBA) freezes the codebook and trains only a lightweight 2-layer MLP adapter to project quantized prototypes into the LLM's embedding space. The LLM decoder remains frozen throughout. During inference, user histories are encoded, partitioned into subspaces, quantized via product quantization, projected through the adapter, and prepended to task prompts for conditional generation.

## Key Results
- CURP achieves state-of-the-art performance on 70% of metrics across four personalized generation tasks
- Uses only about 20M trainable parameters (0.2% of total model size)
- Demonstrates strong robustness to varying amounts of user history, outperforming baselines that degrade beyond 8 histories
- Shows consistent performance across different LLM architectures (Qwen-2.5-7B, LLaMA-2-7B, RoBERTa-7B)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Encoding users as sparse combinations of discrete prototypes from a shared codebook enables noise-robust personalization while preserving behavioral diversity.
- Mechanism: User histories pass through a bidirectional encoder, producing dense embeddings. These embeddings are partitioned into L subspaces and each subspace is independently quantized via Product Quantization (PQ) to select indices from a shared codebook. The selected prototypes are recombined to form a compressed user representation. A diversity loss prevents prototype collapse, while a usage loss encourages uniform codebook utilization.
- Core assumption: User identities can be decomposed into reusable, cross-user behavioral patterns (motivated by Self-Categorization Theory), and quantization preserves sufficient signal for personalization.
- Evidence anchors:
  - [abstract] "employs a bidirectional user encoder and a discrete prototype codebook to extract multi-dimensional user traits"
  - [section 3.1.2] "each user event is abstracted as a combination of L prototypes, where the selected indices across all subspaces collectively encode the event's semantic composition"
  - [corpus] Related work on personalized RAG (arXiv:2504.05731) retrieves from user history to reflect preferences, but CURP replaces retrieval with quantized prototype composition—complementary signal but not directly evaluated against each other.
- Break condition: If user behaviors are highly idiosyncratic with little cross-user pattern sharing, or if codebook size is severely underspecified, quantization may lose critical fine-grained signal.

### Mechanism 2
- Claim: Decoupling codebook construction from LLM alignment enables a model-agnostic, transferable prototype space.
- Mechanism: Stage 1 (PCC) constructs the codebook using balanced k-means on pooled embeddings from ~150k users, optimized with reconstruction, diversity, and usage losses. Stage 2 (PBA) freezes the codebook and trains only a lightweight 2-layer MLP adapter to project quantized prototypes into the LLM's embedding space. The LLM decoder remains frozen throughout.
- Core assumption: Prototypes learned from large-scale behavioral data capture universal user characteristics that generalize across tasks and model architectures without further LLM training.
- Evidence anchors:
  - [abstract] "plug-and-play personalization with a small number of trainable parameters (about 20M parameters, about 0.2% of the total model size)"
  - [section 3.2.1–3.2.2] Describes PCC loss formulation (Lquant, Ldiv, Lusage) and PBA likelihood maximization with frozen codebook.
  - [corpus] Weak direct corpus evidence for the two-stage decoupling strategy; related work focuses on RAG-based or fine-tuning approaches.
- Break condition: If PCC training data distribution diverges sharply from downstream task user populations, prototypes may not transfer well. Similarly, if the adapter capacity is insufficient, alignment to LLM embedding space may be poor.

### Mechanism 3
- Claim: Prepending prototype-based embeddings to task prompts enables conditional generation without modifying LLM weights.
- Mechanism: Quantized prototype embeddings are projected through the MLP adapter and concatenated with the task query embedding before being fed to the decoder. The decoder generates personalized responses conditioned on this combined input, with no parameter updates to the LLM itself.
- Core assumption: The LLM can effectively interpret continuous prototype embeddings prepended to input, treating them as soft prompts that encode user characteristics.
- Evidence anchors:
  - [section 3.1.3] "we combine the projected prototype embeddings with the query embeddings and feed the combined sequence into the decoder to perform personalized conditional generation"
  - [section 5.3.2] Cross-architecture experiments show consistent performance with different encoder-decoder combinations, supporting model-agnostic claims.
  - [corpus] No direct corpus comparison to this exact prepending mechanism in related work.
- Break condition: If the LLM's embedding space and prototype space are misaligned and the adapter fails to bridge them, conditioning signal degrades. Very short task prompts may also provide insufficient context for the decoder to leverage prototype information.

## Foundational Learning

- Concept: Product Quantization (PQ)
  - Why needed here: PQ decomposes high-dimensional embeddings into subspaces and quantizes each independently, enabling efficient compression while preserving semantic structure. CURP relies on PQ to create sparse, interpretable user representations.
  - Quick check question: Can you explain why quantizing subspaces independently might preserve more semantic granularity than quantizing the full vector?

- Concept: Codebook Learning with Commitment/Usage Losses
  - Why needed here: CURP's codebook requires balanced utilization to prevent collapse and ensure diverse prototype coverage. The paper combines reconstruction loss, diversity loss, and usage loss.
  - Quick check question: What would happen to a codebook if only reconstruction loss were used, without usage or diversity terms?

- Concept: Soft Prompting / Continuous Prompt Tuning
  - Why needed here: CURP projects prototype embeddings into the LLM's embedding space and prepends them to task inputs, analogous to learned soft prompts that condition generation.
  - Quick check question: How does prepending continuous embeddings differ from prepending text-based persona descriptions in terms of noise and interpretability?

## Architecture Onboarding

- Component map:
  User Encoder (frozen Contriever, bidirectional) -> Prototype Codebook (learnable, 1000 entries × 4 subspaces) -> Quantization Module (PQ) -> Adapter (2-layer MLP, 768→3584→3584) -> LLM Decoder (frozen Qwen-2.5-8B or alternative)

- Critical path:
  1. Format user history into text pairs (question-answer, tweet, review, etc.).
  2. Encode each history item via User Encoder; apply mean pooling if aggregating multiple events.
  3. Partition embeddings into L=4 subspaces; quantize each to nearest codebook entry.
  4. Reconstruct quantized embedding; project via Adapter.
  5. Concatenate projected prototypes with task query embedding; feed to Decoder.
  6. Generate response; evaluate with ROUGE, BLEU, and semantic similarity.

- Design tradeoffs:
  - Codebook size (500/1000/2000): Larger vocabularies increase expressiveness but risk lower utilization and higher compute.
  - PQ subspaces (2/4/8): More subspaces capture finer granularity but increase combination complexity and potential information loss.
  - History count (2/4/8/16): More histories provide richer signal but may introduce noise; paper shows performance plateaus or degrades beyond 8 for ICL/LoRA, while CURP remains stable.
  - Encoder choice: Contriever vs. RoBERTa; bidirectional attention helps contextual modeling but may not align optimally with all decoders without adapter tuning.

- Failure signatures:
  - Codebook collapse: Very few codebook entries receive assignments; usage histogram shows heavy skew. Mitigate with stronger usage/diversity losses and balanced k-means initialization.
  - Low prototype diversity: User representations become indistinguishable; similarity scores between different users' prototypes are too high. Check subspace quantization effectiveness.
  - Poor cross-task transfer: Codebook trained on one domain (e.g., Reddit QA) underperforms on another (e.g., News Headline). May need domain-mixed PCC data or task-specific fine-tuning.
  - Information leakage at evaluation: Using LLM-as-judge with access to raw history could artificially inflate baseline scores for prompt-based methods.

- First 3 experiments:
  1. Ablate codebook: Run PBA with continuous (non-quantized) encoder outputs vs. quantized prototypes on all four tasks; compare ROUGE/BLEU/Sim to quantify information loss vs. noise reduction (Table 2).
  2. Codebook initialization comparison: Train PCC with random vs. balanced k-means initialization, with and without diversity/usage losses; visualize codebook usage histograms and combination diversity (Figure 3b).
  3. Cross-architecture transfer: Train codebook with Contriever+Qwen; evaluate on RoBERTa+LLaMA and Contriever+LLaMA without retraining; report metrics to validate model-agnostic transferability (Table 3).

## Open Questions the Paper Calls Out
```markdown
### Open Question 1
- Question: How can LLM-based evaluation methodologies be adapted for personalized generation tasks without introducing information leakage from the user's historical context?
- Basis in paper: [explicit] Section 7 (Limitations) states that "LLM as a judge are not involved due to their reliance on surface-level text... using LLM as a judge would introduce information leakage, affecting evaluation objectivity."
- Why unresolved: Standard LLM evaluation requires context to judge personalization, but providing the ground-truth history to the judge allows it to "cheat" by copying, while omitting it makes accurate judgment impossible.
- What evidence would resolve it: A proposed evaluation framework that correlates highly with human judgment of personalization without direct access to the reference history used for conditioning.

### Open Question 2
- Question: Does the complete quantization of user history into discrete prototypes result in a measurable loss of fine-grained stylistic features, such as habitual word usage, compared to raw text methods?
- Basis in paper: [explicit] Section 7 (Limitations) acknowledges that "completely discarding the textual content of historical behaviors inevitably leads to information loss regarding fine-grained behavioral patterns, such as habitual word usage."
- Why unresolved: While CURP improves high-level trait fidelity, the paper admits that discrete indices may fail to capture specific, low-level linguistic quirks present in the raw text history.
- What evidence would resolve it: A comparative analysis focusing on specific linguistic markers (e.g., syntactic patterns, rare vocabulary usage) to quantify the performance gap between codebook-based and raw-text approaches.

### Open Question 3
- Question: How does CURP perform in scenarios involving cross-domain user behaviors, where prototypes must generalize across disparate topics not covered in single-domain training datasets?
- Basis in paper: [explicit] Section 7 (Limitations) notes that "in real-world scenarios, users exhibit cross-domain behaviors that span multiple domains, which are currently unavailable in existing datasets."
- Why unresolved: The current experiments rely on datasets where user histories are largely confined to specific tasks (e.g., Reddit, News), leaving the transferability of learned prototypes across diverse domains untested.
- What evidence would resolve it: Results from experiments on a multi-domain dataset where a single user's codebook representation must predict behaviors across different platforms or topic categories.
```

## Limitations
- **Information loss from quantization**: Completely discarding textual content of historical behaviors inevitably leads to loss of fine-grained behavioral patterns, such as habitual word usage.
- **Domain generalization gap**: Current experiments are confined to single-domain tasks; cross-domain user behaviors spanning multiple domains remain untested.
- **Evaluation methodology constraints**: LLM-as-judge evaluation is avoided due to information leakage risks, limiting objective assessment of personalization quality.

## Confidence

**High Confidence** - The two-stage training pipeline (PCC + PBA) is clearly described, and the empirical results consistently show CURP outperforming baselines on 70% of metrics. The codebook utilization remains above 50% across tasks, and the ablation studies provide reasonable evidence for key design choices.

**Medium Confidence** - Claims about model-agnostic transferability are supported by cross-architecture experiments, but these use the same adapter architecture without per-architecture tuning. The demonstration is suggestive but not exhaustive across diverse LLM families.

**Low Confidence** - Claims about interpretability are based on qualitative observations of learned prototypes rather than systematic human evaluation or downstream interpretability benchmarks. The paper does not validate whether the "sparse combination" property translates to human-interpretable user profiles.

## Next Checks

1. **Domain Generalization Stress Test** - Train PCC on Reddit QA data, then evaluate CURP on out-of-domain tasks (e.g., medical Q&A or legal document summarization). Measure prototype stability via cross-domain similarity and report performance drop relative to in-domain training.

2. **Quantization Fidelity Analysis** - Replace product quantization with a continuous prototype layer (no discretization) while keeping all other components fixed. Compare per-task performance, codebook utilization, and prototype similarity distributions to isolate the information loss from quantization.

3. **Adapter Architecture Sweep** - Systematically vary the adapter MLP dimensions (e.g., 768→1792→3584, 768→3584→1792, 768→7168→7168) across all four tasks and both decoder architectures. Report how adapter capacity affects both training stability and final personalization quality.