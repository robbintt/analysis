---
ver: rpa2
title: 'MARS-M: When Variance Reduction Meets Matrices'
arxiv_id: '2510.21800'
source_url: https://arxiv.org/abs/2510.21800
tags:
- mars-m
- training
- muon
- moonlight
- adamw
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MARS-M, a novel optimizer that combines variance-reduction
  techniques with matrix-based preconditioning. MARS-M integrates MARS-style variance
  reduction with the Muon optimizer, achieving improved convergence rates.
---

# MARS-M: When Variance Reduction Meets Matrices

## Quick Facts
- **arXiv ID**: 2510.21800
- **Source URL**: https://arxiv.org/abs/2510.21800
- **Reference count**: 40
- **Key outcome**: MARS-M combines variance reduction with matrix preconditioning, achieving O(T^{-1/3}) convergence rate and improved empirical performance on language modeling and computer vision tasks.

## Executive Summary
MARS-M introduces a novel optimizer that integrates MARS-style variance reduction techniques with matrix-based preconditioning from the Muon optimizer. The key innovation is a scaled gradient correction term that reduces variance in momentum updates while maintaining the orthogonalization benefits of matrix preconditioning. Under standard regularity conditions, MARS-M achieves a convergence rate of O(T^{-1/3}), improving upon Muon's O(T^{-1/4}) rate. Empirical results demonstrate consistent performance improvements across language modeling (GPT-2 variants) and computer vision (ResNet-18) tasks.

## Method Summary
MARS-M extends the Muon optimizer by incorporating scaled variance reduction into the momentum update. The method computes two gradients per step (current and previous parameters on the same batch), applies a scaled correction term to reduce gradient noise, and uses Newton-Schulz iteration for matrix orthogonalization. The final update includes weight decay and a scaling factor (0.2·√(max(m,n))) to align update magnitudes with AdamW-style optimizers. An approximate variant uses the previous batch's gradient for efficiency, which is preferred for large-scale language model training.

## Key Results
- Theoretical convergence rate of O(T^{-1/3}) for first-order stationary points, improving upon Muon's O(T^{-1/4})
- Consistent reduction in training and validation losses across GPT-2 language models (125M to 1.5B parameters)
- 0.5-1% improvement in exact MARS-M versus approximate variant on CIFAR-10/ResNet-18
- Robust performance across downstream benchmarks (HellaSwag, SciQ, ARC, MMLU) with γ values in [0.005, 0.025]

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaled variance reduction improves gradient estimation stability for matrix-based optimization.
- Mechanism: MARS-M introduces a scaled gradient correction term C_t = ∇f(X_t, ξ_t) + γ_t · (β/(1-β)) (∇f(X_t, ξ_t) - ∇f(X_{t-1}, ξ_t)) into the momentum update. This subtracts a scaled estimate of gradient noise, recursively reducing variance in the momentum buffer M_t.
- Core assumption: Stochastic gradients at adjacent parameters with the same data batch are correlated; γ_t is tuned to avoid over-correction. Assumption: variance of gradient differences is bounded.
- Evidence anchors: Abstract states improved convergence rates; section 3.2 defines scaled gradient correction; independent theoretical support from Muon variance reduction literature.
- Break condition: If γ_t is too large (over-correction) or too small (no benefit), or if batch-to-batch gradient correlations are weak.

### Mechanism 2
- Claim: Matrix preconditioning via Newton-Schulz orthogonalization improves conditioning of the update.
- Mechanism: MARS-M applies Newton-Schulz iteration to the momentum matrix M_t to compute O_t ≈ U_t V_t^⊤ (where U_t Σ_t V_t^⊤ is the SVD of M_t). This produces updates orthogonalized in column/row spaces, effectively whitening gradient direction per matrix parameter.
- Core assumption: Newton-Schulz iteration approximates SVD sufficiently well; matrix parameter's 2D structure captures meaningful geometric relationships in loss landscape.
- Evidence anchors: Abstract mentions matrix-based preconditioned optimizers; section 3.1 defines Muon's core update with Newton-Schulz; related matrix-whitening literature supports broader family's efficacy.
- Break condition: If parameter matrix has no exploitable 2D structure, or if Newton-Schulz numerical errors accumulate over many steps.

### Mechanism 3
- Claim: Update magnitude scaling (0.2·√(max(m,n))) aligns Muon updates with AdamW-style magnitude scales.
- Mechanism: MARS-M scales O_t by 0.2·√(max(m,n)) to correct mismatch between Muon's naturally smaller update RMS (~0.05-0.1) and AdamW's typical RMS (~0.2-0.4), enabling better hyperparameter sharing.
- Core assumption: Aligning update magnitudes across parameter types improves training stability and allows unified learning rate schedules.
- Evidence anchors: Section 3.1 explains scaling rationale; section 4.1 states main differences with MARS-Shampoo; limited direct corpus validation for this specific scaling factor.
- Break condition: If matrix dimensions vary drastically without adaptive scaling, or if 0.2 constant is inappropriate for given architecture.

## Foundational Learning

- Concept: **Variance Reduction in Stochastic Optimization**
  - Why needed here: MARS-M builds on STORM/MARS-style recursive gradient correction. Understanding how subtracting correlated noise estimates reduces estimator variance is essential.
  - Quick check question: Can you explain why ∇f(x_t, ξ_t) - ∇f(x_{t-1}, ξ_t) is correlated with the gradient noise at x_t?

- Concept: **Matrix Preconditioning and Orthogonalization**
  - Why needed here: Newton-Schulz iteration and SVD-based orthogonalization are central to how MARS-M processes matrix parameters differently from vector parameters.
  - Quick check question: How does orthogonalizing the momentum matrix (O_t ≈ U_t V_t^⊤) differ from standard diagonal preconditioning (e.g., Adam)?

- Concept: **First-Order Stationary Point Convergence**
  - Why needed here: Paper's main theoretical contribution is improving convergence rate to O(T^{-1/3}) from O(T^{-1/4}) under standard assumptions.
  - Quick check question: What does a rate of O(T^{-1/3}) mean for expected gradient norm after T iterations?

## Architecture Onboarding

- Component map:
  1. Gradient computation: ∇f(X_t, ξ_t) and ∇f(X_{t-1}, ξ_t)
  2. Scaled variance reduction: C_t = ∇f(X_t, ξ_t) + γ_t · (β/(1-β)) (∇f(X_t, ξ_t) - ∇f(X_{t-1}, ξ_t))
  3. Gradient clipping: Clip(C_t, 1) (clip to unit Frobenius norm if ||C_t||_F > 1)
  4. Momentum accumulation: M_t = β M_{t-1} + (1-β) Clip(C_t, 1)
  5. Newton-Schulz orthogonalization: O_t = NewtonSchulz(M_t) (5-10 iterations typically)
  6. Scaled orthogonal update: ΔX_t = 0.2 · O_t · √(max(m, n)) + λ X_t (with weight decay)
  7. Parameter update: X_{t+1} = X_t - η_t ΔX_t

- Critical path:
  1. Store ∇f(X_{t-1}, ξ_t) from previous step (requires extra forward/backward or cache)
  2. Compute C_t with correct γ_t scaling
  3. Apply clipping before momentum update
  4. Run Newton-Schulz iteration on M_t (dominant compute cost for large matrices)
  5. Apply scaled update with weight decay

- Design tradeoffs:
  - Exact vs. Approximate MARS-M: Exact uses ∇f(X_{t-1}, ξ_t) (requires re-computing gradient); approximate uses ∇f(X_{t-1}, ξ_{t-1}) (cheaper, slightly biased). Paper uses approximate for LLM experiments.
  - Newton-Schulz iterations: More iterations improve SVD approximation but increase overhead; 5-10 iterations are typical.
  - γ_t sensitivity: Paper finds γ ∈ [0.005, 0.025] robust for LLMs, but optimal value is task-dependent.

- Failure signatures:
  - Training loss spikes or divergence: γ_t too large (over-correction) or learning rate too high for orthogonalized updates
  - No improvement over baseline Muon: γ_t too small, or gradient clipping threshold incorrect
  - Memory blowup: Storing extra gradient for variance reduction doubles memory for matrix parameters
  - Slow training: Newton-Schulz iterations bottleneck on large matrices; consider reducing iterations or using approximate MARS-M

- First 3 experiments:
  1. Baseline comparison on small-scale task: Implement MARS-M on GPT-2 small (125M) on OpenWebText for 10K steps; compare training/validation loss curves vs. Muon and AdamW. Verify ~0.01-0.03 lower final loss.
  2. Ablation on γ: Sweep γ ∈ {0.005, 0.01, 0.025, 0.1, 0.2} on held-out validation set; reproduce Figure 6 sensitivity curve to confirm robust range.
  3. Approximate vs. Exact MARS-M: On CIFAR-10/ResNet-18, compare exact and approximate MARS-M test accuracy vs. Muon; expect ~0.5-1% improvement for exact over approximate, and ~1-2% over Muon.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does MARS-M maintain its convergence advantages and performance gains when scaled to LLMs with 7B+ parameters and trillions of training tokens?
- Basis in paper: Experiments only cover GPT-2 models up to 1.5B parameters and 100B tokens, while industrial models like Kimi K2 and GLM-4.5 are significantly larger.
- Why unresolved: Scaling behavior of variance reduction techniques combined with matrix preconditioning at industrial scales remains untested.
- What evidence would resolve it: Training curves and downstream benchmark results for 7B+ models trained with MARS-M versus Muon/AdamW baselines on trillion-token datasets.

### Open Question 2
- Question: How does the approximation error from Newton-Schulz iteration affect the theoretical convergence guarantees of MARS-M in practice?
- Basis in paper: Footnote 3 states: "for the purpose of this analysis, we treat O_t as the exact polar factor U_t V_t^⊤ and ignore the approximation error."
- Why unresolved: Convergence proof assumes exact SVD approximation, but Newton-Schulz provides only an approximation whose error propagation is unanalyzed.
- What evidence would resolve it: Empirical comparison of convergence with exact SVD versus varying numbers of Newton-Schulz iterations, or theoretical bounds incorporating approximation error.

### Open Question 3
- Question: Can the scaling parameter γ be adaptively tuned during training rather than fixed, and would adaptive tuning improve performance?
- Basis in paper: Ablation study tests fixed γ values (0.005-0.025) but notes "the scale on gradient correction should be much smaller than 1" without providing adaptive selection criteria.
- Why unresolved: Optimal γ may vary across training stages, architectures, or tasks, but no adaptive mechanism is proposed or analyzed.
- What evidence would resolve it: Experiments with schedule-based or gradient-statistics-based adaptive γ, showing convergence comparisons against fixed γ baselines.

### Open Question 4
- Question: What is the quantitative computational overhead of exact MARS-M (requiring two gradient computations) versus approximate MARS-M, and when does the performance gain justify this cost?
- Basis in paper: Paper states "computing the stochastic gradients twice is significantly more expensive" and uses approximate MARS-M for LLM experiments, but provides no wall-clock time comparison.
- Why unresolved: Trade-off between improved convergence and increased per-iteration cost remains unquantified for large-scale training.
- What evidence would resolve it: Wall-clock time measurements for training to equivalent loss thresholds using exact versus approximate MARS-M across model scales.

## Limitations
- Theoretical claims rely on strong regularity conditions (L-smoothness, variance bounds, bounded gradient norms) that may not hold in practice
- Memory overhead from storing extra gradients for variance reduction doubles storage requirements for matrix parameters
- Scaling factor 0.2·√(max(m,n)) for magnitude alignment is justified empirically but lacks rigorous theoretical grounding

## Confidence

- **High confidence**: The mechanism of scaled variance reduction improving gradient estimation stability (Mechanism 1) is well-supported by the theoretical framework and aligns with established variance reduction literature
- **Medium confidence**: The matrix preconditioning via Newton-Schulz orthogonalization (Mechanism 2) is theoretically sound, but practical benefits depend on parameter structure and numerical stability of the iterative approximation
- **Low confidence**: The magnitude scaling claim (Mechanism 3) lacks direct empirical validation beyond the specific factor used; the theoretical justification is heuristic rather than rigorous

## Next Checks

1. **Convergence rate validation**: Implement MARS-M on a convex quadratic problem with known smoothness parameters and verify the O(T^{-1/3}) rate empirically by measuring gradient norm decay over training steps

2. **Scaling factor ablation**: Systematically vary the 0.2·√(max(m,n)) scaling factor across matrix dimensions (1×1, 3×3, 10×10) in controlled experiments to identify optimal scaling relationships

3. **Memory overhead analysis**: Quantify the actual memory impact of storing extra gradients for variance reduction on full-scale GPT-2 models and assess whether approximate MARS-M provides comparable benefits with reduced memory cost