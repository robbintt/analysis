---
ver: rpa2
title: 'Revealing economic facts: LLMs know more than they say'
arxiv_id: '2505.08662'
source_url: https://arxiv.org/abs/2505.08662
tags:
- text
- output
- correlation
- spearman
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that embeddings from open-source large language
  models contain richer economic information than their text outputs. By applying
  a linear regression to the hidden states of LLMs, the authors can estimate economic
  and financial statistics (e.g.
---

# Revealing economic facts: LLMs know more than they say

## Quick Facts
- **arXiv ID:** 2505.08662
- **Source URL:** https://arxiv.org/abs/2505.08662
- **Reference count:** 34
- **Primary result:** Linear probes on LLM hidden states outperform text outputs for economic estimation across multiple models and geographies.

## Executive Summary
This paper demonstrates that embeddings from open-source large language models contain richer economic information than their text outputs. By applying linear regression to the hidden states of LLMs, the authors can estimate economic and financial statistics (e.g., unemployment, GDP, total assets) more accurately than using the models' direct textual responses. The improvement is most pronounced for less common statistics and across models of varying sizes (1–70B parameters). Transfer learning further allows accurate estimation without any labeled target data. Embedding-based approaches also improve imputation of missing values and enable super-resolution estimation of lower-level geographic statistics.

## Method Summary
The method extracts hidden states from the 25th layer of Llama 3 8B-Instruct models for the last token of completion-style prompts about economic variables. A ridge regression model is trained on these embeddings using grouped cross-validation to prevent geographic leakage. The approach applies specific transformations (log or cubic) to skewed target variables and tunes regularization parameters across 50 values. The same framework extends to transfer learning with noisy text labels and to super-resolution estimation by training on aggregate data to predict disaggregated values.

## Key Results
- Linear probes on hidden states (LME) consistently outperform text output for economic estimation across US counties, German districts, UK districts, EU regions, and US firms
- LME accuracy improves most for less common statistics and degrades predictably with smaller sample sizes
- Transfer learning with noisy text labels improves estimation by 7.2 percentage points compared to text-only baselines
- Super-resolution estimation works for most variables except population, which shows negative correlation
- LME is 200x faster than reasoning models while maintaining better accuracy

## Why This Works (Mechanism)

### Mechanism 1
Hidden states encode economic information in linearly accessible representations that exceed what text generation reveals. A ridge regression trained on the last token's hidden states (typically layer 25) maps embeddings to target variables. The linearity suggests representations form interpretable subspaces, consistent with Gurnee & Tegmark 2023 on spatial/temporal encoding. Economic variables are represented in sufficiently disentangled, linear dimensions within the embedding space.

### Mechanism 2
Post-training alignment (e.g., hallucination reduction) suppresses generative access to latent knowledge without destroying the underlying representations. RLHF/safety fine-tuning biases the decoding head toward conservative outputs, but earlier layers retain richer entity-level representations. Probing bypasses the constrained decoder, accessing knowledge that alignment has suppressed at generation time.

### Mechanism 3
Transfer learning with noisy text labels enables estimation without ground-truth labels for the target variable. The neural network learns from ground-truth labels of other variables and noisy text-output labels for the target variable. Early stopping prevents overfitting to label noise; dropout regularizes. Text output, while inaccurate, contains sufficient signal to orient the model toward the correct embedding subspace.

## Foundational Learning

- **Concept: Linear Probing**
  - Why needed here: The entire method rests on the premise that ridge regression on frozen embeddings extracts meaningful signal. Understanding regularization and when linearity suffices is prerequisite.
  - Quick check question: Why might a linear probe outperform a neural network when training data is scarce?

- **Concept: Grouped Cross-Validation**
  - Why needed here: Evaluating generalization requires holding out entire groups (e.g., US states) to avoid leakage from geographic autocorrelation.
  - Quick check question: What goes wrong if you use random k-fold CV on county-level data without grouping by state?

- **Concept: Label Noise Robustness**
  - Why needed here: The transfer learning method relies on neural networks generalizing from noisy supervision and learning signal before memorizing noise.
  - Quick check question: Why does early stopping help when training on noisy labels?

## Architecture Onboarding

- **Component map:** Prompt construction -> Embedding extraction (layer 25, last token) -> Optional PCA (25 components) -> Ridge regression or 2-layer MLP (transfer learning) -> Evaluation (Spearman correlation with grouped CV)

- **Critical path:** Prompt design -> layer selection -> regularization tuning. Layer 25 is empirically best, but the gap is small beyond layer 5.

- **Design tradeoffs:**
  - Completion prompt vs. generic prompt: Completion encodes variable name, improving accuracy but requiring variable-specific prompts
  - PCA vs. raw embeddings: PCA helps only for very small samples (<50); raw is otherwise superior
  - LME vs. reasoning models: LME is 200x faster (0.1s vs. 1767 tokens median) with better accuracy

- **Failure signatures:**
  - Text output outperforms LME on common/memorized variables (e.g., population) -> likely rote recall, not generalization
  - Transfer learning without noisy labels fails -> subspace alignment requires target-specific signal
  - Super-resolution negative correlation (population) -> training set too small (51 states) or representation not transferable

- **First 3 experiments:**
  1. Baseline replication: Run LME on US counties with Llama 3 8B, layer 25, grouped 5-fold CV by state; compare Spearman correlation vs. text output for unemployment rate
  2. Sample efficiency sweep: Train LME with n ∈ {10, 25, 50, 100, 500} labeled examples; plot learning curve to identify convergence point
  3. Transfer learning stress test: For one held-out variable, train MLP using only noisy text labels + embeddings; compare against text-only baseline to quantify improvement

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the superior performance of linear models on embeddings over text outputs hold for the largest open-source models, such as Llama 3 405B or DeepSeek V3?
  - Basis in paper: [explicit] The authors state, "We leave it for future work to test whether this finding also holds for the largest and most accurate open-source models such as Llama 3 405B or DeepSeek V3 with 671 billion parameters."
  - Why unresolved: The study only validated this finding on models ranging from 1 to 70 billion parameters.
  - What evidence would resolve it: Replicating the linear probing experiments on models exceeding 400 billion parameters to compare against their text outputs.

- **Open Question 2:** Do embedding-based estimation methods remain effective when applied to proprietary datasets unlikely to be present in the models' training corpora?
  - Basis in paper: [explicit] The conclusion notes, "it would be interesting to examine the extent to which our results hold for proprietary datasets, which are less likely to be observed in the training corpus."
  - Why unresolved: The current study relies on public economic data from 2019, which likely appears in the training data of the tested LLMs.
  - What evidence would resolve it: Testing the LME approach on private, non-public financial or regional statistics to see if the model generalizes or merely recalls memorized facts.

- **Open Question 3:** Can hidden states representing economic entities be utilized effectively for automated outlier detection in data pipelines?
  - Basis in paper: [explicit] The authors suggest, "future research could investigate how useful the hidden states... can be for other data processing tasks, such as outlier detection."
  - Why unresolved: The paper focused solely on estimation, imputation, and super-resolution tasks.
  - What evidence would resolve it: Designing experiments to measure if embedding distances or probe confidence correlates with anomalous or erroneous economic data entries.

## Limitations
- The method requires 50-100 labeled examples for training, which may be prohibitive for truly novel economic indicators
- Occasional failure cases (like population statistics) suggest the method has boundaries that aren't fully characterized
- The claim that post-training alignment suppresses knowledge access without destroying representations remains partially speculative

## Confidence
- **High confidence:** The core empirical finding that linear probes on hidden states outperform text outputs for economic estimation is well-supported by the results
- **Medium confidence:** The mechanism explanations (linear encoding, suppression effects, transfer learning requirements) are plausible given the evidence but rely on interpretive frameworks that could have alternative explanations
- **Low confidence:** The exact boundaries of when and why the method fails are not fully characterized, and the population example shows text can outperform LME without systematic exploration of failure modes

## Next Checks
1. **Mechanism stress test:** Systematically vary the dimensionality of the embedding space (using PCA at different component counts) and measure how performance changes across different economic variables to test the linearity assumption
2. **Alignment ablation study:** Compare LME performance on models with different alignment intensities (base vs. chat vs. reasoning models) using identical variables to quantify how alignment intensity affects representation quality versus generation output
3. **Data efficiency benchmark:** Create controlled experiments with artificially limited labeled data (10-50 examples) across multiple economic indicators to establish precise sample size requirements and identify which variable characteristics predict data efficiency