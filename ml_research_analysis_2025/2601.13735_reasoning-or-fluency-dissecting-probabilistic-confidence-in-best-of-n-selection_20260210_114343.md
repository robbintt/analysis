---
ver: rpa2
title: Reasoning or Fluency? Dissecting Probabilistic Confidence in Best-of-N Selection
arxiv_id: '2601.13735'
source_url: https://arxiv.org/abs/2601.13735
tags:
- reasoning
- selection
- metrics
- arxiv
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether probabilistic confidence metrics
  used in Best-of-N selection truly capture reasoning quality in Chain-of-Thought
  traces. The authors challenge the assumption that higher confidence reflects better
  reasoning by systematically disrupting inter-step causal dependencies while preserving
  local fluency.
---

# Reasoning or Fluency? Dissecting Probabilistic Confidence in Best-of-N Selection

## Quick Facts
- arXiv ID: 2601.13735
- Source URL: https://arxiv.org/abs/2601.13735
- Reference count: 24
- Key outcome: Probabilistic confidence metrics in Best-of-N selection primarily capture surface-level fluency rather than reasoning quality

## Executive Summary
This paper investigates whether probabilistic confidence metrics used in Best-of-N selection truly capture reasoning quality in Chain-of-Thought traces. The authors systematically challenge the assumption that higher confidence reflects better reasoning by introducing three classes of perturbations that disrupt inter-step causal dependencies while preserving local fluency. Surprisingly, across diverse model families and reasoning benchmarks, selection accuracy remains largely unaffected even under severe interventions like hard attention masking. These findings reveal that current probabilistic metrics are primarily capturing surface-level fluency or in-distribution priors rather than genuine reasoning quality. To address this gap, the authors propose a contrastive causality metric that explicitly isolates inter-step causal dependencies, yielding more faithful output selection and demonstrating greater sensitivity to reasoning coherence.

## Method Summary
The authors introduce a systematic framework for evaluating probabilistic confidence metrics in Best-of-N selection through three classes of perturbations: attention-level (masking cross-step attention), parameter-level (using smaller evaluator models), and data-level (paraphrasing, shuffling, or truncating reasoning steps). They test these perturbations across diverse model families and reasoning benchmarks to assess how well confidence metrics capture reasoning quality versus surface fluency. The core innovation is the contrastive causality metric, which compares standard confidence scores with masked variants to explicitly isolate inter-step causal dependencies. This approach aims to reveal whether confidence metrics are genuinely sensitive to logical structure or merely capturing surface-level patterns.

## Key Results
- Selection accuracy degrades only marginally under severe disruptions like hard attention masking
- Probabilistic confidence metrics remain largely insensitive to logical structure across diverse model families
- The proposed contrastive causality metric yields more faithful output selection than existing probability-based approaches
- The new metric shows consistent degradation under causal disruptions, demonstrating greater sensitivity to reasoning coherence

## Why This Works (Mechanism)
The effectiveness of the contrastive causality metric stems from its explicit isolation of inter-step causal dependencies by comparing standard confidence scores with masked variants. This approach directly addresses the fundamental limitation of traditional probabilistic metrics, which conflate surface fluency with genuine reasoning quality. By forcing the evaluation to distinguish between causal and non-causal signal, the metric provides a more principled measure of reasoning coherence. The robustness of selection accuracy under the proposed perturbations reveals that traditional metrics are capturing in-distribution priors and surface-level patterns rather than the underlying logical structure that constitutes true reasoning quality.

## Foundational Learning

Chain-of-Thought (CoT) reasoning: A prompting strategy where language models generate intermediate reasoning steps before producing final answers, mimicking human problem-solving processes.
- Why needed: Provides the foundation for understanding how LLMs perform complex reasoning tasks through step-by-step decomposition
- Quick check: Does the model generate intermediate steps that logically connect to the final answer?

Best-of-N selection: A decoding strategy where multiple candidate outputs are generated and ranked using a confidence metric to select the most likely correct answer
- Why needed: Enables selection of optimal reasoning traces from multiple candidates, but assumes confidence metrics accurately reflect reasoning quality
- Quick check: Does the confidence metric align with human judgment of reasoning quality?

Causal dependencies in reasoning: The logical connections between intermediate reasoning steps that ensure coherent progression toward a correct conclusion
- Why needed: Forms the basis for evaluating whether confidence metrics capture genuine reasoning versus surface patterns
- Quick check: Can the reasoning steps be rearranged without affecting the final answer's correctness?

## Architecture Onboarding

Component map: Evaluator model -> Confidence scoring module -> Best-of-N selector -> Output reasoning trace
- The evaluator model generates probabilistic confidence scores for each reasoning step
- The confidence scoring module aggregates step-level scores into overall trace confidence
- The Best-of-N selector ranks and chooses the most confident reasoning trace
- The output reasoning trace represents the selected CoT process

Critical path: Attention masking perturbations -> Confidence score computation -> Selection accuracy evaluation -> Contrastive causality comparison
- Perturbations modify the reasoning traces to disrupt causal dependencies
- Confidence scores are computed for both original and perturbed traces
- Selection accuracy measures how well metrics identify correct reasoning under disruption
- Contrastive causality compares standard versus masked confidence to assess sensitivity

Design tradeoffs: Computational efficiency vs. sensitivity to causal structure vs. generalizability across domains
- Traditional metrics are computationally efficient but insensitive to causal structure
- The contrastive approach captures more nuanced dependencies but may incur additional computational overhead
- Balancing sensitivity with practical deployment considerations remains an open challenge

Failure signatures: Marginally affected selection accuracy under severe perturbations indicates metrics capture surface fluency rather than reasoning quality
- Robustness to attention masking suggests metrics rely on in-distribution priors
- Consistency across diverse model families indicates systematic bias toward fluency over logic

First experiments:
1. Apply attention masking perturbations to CoT traces and measure confidence score changes
2. Compare selection accuracy using standard versus contrastive causality metrics
3. Test metric sensitivity across different reasoning domains (math, logic, common sense)

## Open Questions the Paper Calls Out
The authors acknowledge that external validity of perturbation effects across reasoning domains beyond tested benchmarks remains unclear. The generalizability to domains requiring deeper logical dependencies (e.g., mathematical proofs, scientific reasoning) needs further investigation. The contrastive causality metric, while showing promise, requires validation on larger model families and more diverse reasoning tasks to establish consistent superiority across all use cases.

## Limitations
- Relatively narrow scope of tested perturbations may not fully capture all forms of causal dependency disruption
- Limited testing across diverse reasoning domains beyond the initial benchmarks
- Computational overhead and scalability of the contrastive causality metric for large-scale deployments remain uncertain
- Some perturbations may not completely eliminate causal dependencies in all cases, leaving residual signal capture possible

## Confidence

- Probabilistic confidence metrics primarily capture surface-level fluency rather than reasoning quality: Medium confidence
- The contrastive causality metric consistently outperforms existing methods across all use cases: Medium confidence
- Selection accuracy robustness under severe disruptions indicates systematic metric limitations: High confidence

## Next Checks

1. Test the contrastive causality metric's performance across a broader range of reasoning domains including mathematical problem-solving, scientific hypothesis generation, and multi-step logical deduction tasks to assess domain transferability.

2. Conduct ablation studies isolating the individual contributions of attention-level, parameter-level, and data-level perturbations to determine which disruption types most effectively reveal weaknesses in current probabilistic confidence metrics.

3. Evaluate the computational overhead and scalability of the contrastive causality metric compared to existing probability-based selection methods, particularly for deployment scenarios involving large model families and high-volume reasoning tasks.