---
ver: rpa2
title: Enhancing Interpretability and Effectiveness in Recommendation with Numerical
  Features via Learning to Contrast the Counterfactual samples
arxiv_id: '2509.03187'
source_url: https://arxiv.org/abs/2509.03187
tags:
- numerical
- feature
- features
- sample
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of modeling the monotonicity between
  neural network outputs and numerical features in recommender systems, which is critical
  for both interpretability and effectiveness. The authors propose a model-agnostic
  Contrastive learning framework with Counterfactual Samples Synthesizing (CCSS) that
  consists of a Counterfactual Samples Synthesizer and a Contrastive Objective Function
  Module.
---

# Enhancing Interpretability and Effectiveness in Recommendation with Numerical Features via Learning to Contrast the Counterfactual samples

## Quick Facts
- arXiv ID: 2509.03187
- Source URL: https://arxiv.org/abs/2509.03187
- Reference count: 29
- Improves AUC by up to 30.0%, GAUC by up to 48.7%, and Mono_rate by up to 87.5% on public and industrial datasets

## Executive Summary
This paper addresses the lack of modeling monotonicity between neural network outputs and numerical features in recommender systems, which is critical for both interpretability and effectiveness. The authors propose a model-agnostic Contrastive learning framework with Counterfactual Samples Synthesizing (CCSS) that generates counterfactual and factual samples by locally disturbing numerical feature values in original samples. Using feature importance to guide the disturbance process and a contrastive objective function to enforce monotonicity, CCSS achieves significant improvements in both predictive performance and model interpretability, with successful deployment in a large-scale industrial recommender system serving hundreds of millions of users.

## Method Summary
CCSS is a plug-and-play framework that enhances existing recommender models by synthesizing counterfactual and factual samples through local perturbation of numerical feature values. The method uses Shapley values to determine feature importance and guide which features to disturb, then applies a contrastive loss (hinge loss) to rank model outputs between generated and original samples, enforcing monotonicity. The framework augments training data with factual samples and combines point-wise and pairwise losses to simultaneously improve prediction accuracy and interpretability.

## Key Results
- Achieves up to 30.0% improvement in AUC on public datasets
- Improves GAUC by up to 48.7% on industrial datasets
- Increases Mono_rate by up to 87.5%, demonstrating enhanced interpretability
- Successfully deployed in a real large-scale industrial recommender system serving hundreds of millions of users
- Achieves 3.93% improvement in collect rate in production deployment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Enforcing monotonicity between numerical features and model outputs via contrastive learning improves both model interpretability and predictive accuracy (AUC).
- **Mechanism:** The framework synthesizes "factual" (F) and "counterfactual" (C) samples by perturbing numerical feature values to adjacent discretized buckets while keeping other features constant. It then applies a pairwise contrastive loss (hinge loss) to force the model to rank these samples correctly (e.g., Score(F) > Score(Original) > Score(C) for positive labels). This creates an explicit gradient signal for monotonicity that standard point-wise losses lack.
- **Core assumption:** The relationship between the specific numerical feature and the target is strictly monotonic (either increasing or decreasing), and the discretization buckets correctly represent ordinal value changes.
- **Evidence anchors:**
  - [abstract]: "...synthesizing counterfactual and factual samples by disturbing numerical feature values and learns to rank their outputs using a contrastive loss."
  - [section 3.3.1]: "To address the monotonicity expectation... we add pairwise losses to contrast the (F, O) pair and the (O, C) pair... where l_P denotes the pairwise loss, and hinge loss is adopted."
- **Break condition:** The mechanism fails if the data distribution violates the monotonicity assumption (e.g., a feature where mid-range values are optimal), leading to conflicting gradients between the main task and the contrastive loss.

### Mechanism 2
- **Claim:** Prioritizing the perturbation of high-importance features enhances the efficiency of the contrastive learning process.
- **Mechanism:** Instead of selecting features randomly, CCSS calculates the probability of disturbing a feature based on its importance (quantified via Shapley Value). By focusing perturbations on features that significantly influence the output, the generated contrastive pairs provide a stronger learning signal for the model's decision boundary.
- **Core assumption:** Shapley values accurately reflect the "true" importance of a feature in a way that aligns with the desired monotonicity constraints.
- **Evidence anchors:**
  - [section 3.2.1]: "...we introduce feature importance as the specific metric to quantize the probability to be disturbed for each numerical feature. We interpret feature importance... using Shapley Value."
  - [table 7]: Shows that the "Equal Probability Random Disturb" strategy underperforms compared to the importance-weighted strategy.
- **Break condition:** If feature importance calculations are noisy or if the explainer (Shapley) fails to capture the semantic relationship, the model may waste capacity contrasting non-informative features.

### Mechanism 3
- **Claim:** Treating synthesized "factual" samples as labeled data augments the training set and improves generalization.
- **Mechanism:** When a sample is perturbed to increase a feature value (assuming increasing monotonicity), the resulting "factual" sample is assigned the same label as the original. These samples are added to the standard point-wise loss function, effectively acting as data augmentation.
- **Core assumption:** The local manifold assumption holds: if feature $x$ increases and the relationship is monotonic increasing, the label remains positive (or the score should increase).
- **Evidence anchors:**
  - [section 3.3.2]: "In this paper, we exploit these synthesized factual sample with known labels to augment training data... our loss function can be rewritten as: L = l(O) + l(F) +..."
  - [table 7]: Ablation shows that removing the point-wise loss for factual samples ("Only Factual Pairwise loss") results in lower AUC compared to the full model.
- **Break condition:** Excessive augmentation could bias the model if the synthesized values fall into out-of-distribution regions of the feature space.

## Foundational Learning

- **Concept: Monotonicity Constraints**
  - **Why needed here:** The core premise of CCSS is encoding prior knowledge that "more is better" (or worse) for specific numerical features (e.g., click counts). Without understanding monotonicity, the "Factual" vs. "Counterfactual" logic is unintelligible.
  - **Quick check question:** If a user's historical click count increases, should the predicted CTR increase, decrease, or stay the same? Why might a standard DNN fail to learn this naturally?

- **Concept: Contrastive Learning (Triplet/Paired Loss)**
  - **Why needed here:** The model learns not by memorizing absolute values, but by learning the relative ordering between the Original, Factual, and Counterfactual samples.
  - **Quick check question:** How does a pairwise hinge loss $max(0, margin - (Score_F - Score_O))$ differ from a standard cross-entropy loss in terms of what feedback signal it provides the model?

- **Concept: Feature Discretization**
  - **Why needed here:** The synthesizer relies on "buckets" (discretization) to generate neighbors (moving a value to the "left neighbor bucket").
  - **Quick check question:** How does the size of the discretization bucket affect the quality of the synthesized counterfactual? (e.g., too wide vs. too narrow).

## Architecture Onboarding

- **Component map:** Base Encoder -> Importance Calculator -> Counterfactual Synthesizer -> Multi-Head Loss
- **Critical path:**
  1.  **Input:** Batch of Original samples $O$.
  2.  **Synthesis:** Generate $F$ and $C$ using the Synthesizer (See Table 2 logic for direction).
  3.  **Forward Pass:** Pass $O, F, C$ through the *shared* base encoder weights.
  4.  **Loss Calculation:**
      - Compute Binary Cross Entropy on $O$ and $F$.
      - Compute Hinge Loss on pairs $(F, O)$ and $(O, C)$.
  5.  **Backprop:** Update weights.

- **Design tradeoffs:**
  - **Random vs. Importance Sampling:** Random is faster/implementation-free but lowers AUC (Table 7). Importance requires maintaining/updating feature stats.
  - **Hyperparameter $\alpha$:** Balances prediction accuracy vs. monotonicity. Figure 3 suggests a sweet spot; too high $\alpha$ may force monotonicity at the expense of fitting complex interactions.
  - **One vs. Multiple Disturbances:** The paper disturbs *only one* feature per sample to keep samples "realistic."

- **Failure signatures:**
  - **Training Instability:** Loss oscillates if the monotonicity assumption is wrong (e.g., forcing positive correlation on a negative feature).
  - **Low Mono_rate:** If the metric `Mono_rate` (Eq 13) doesn't increase, the contrastive loss weight $\alpha$ may be too low, or the bucket width is too small to create a distinguishable difference.
  - **AUC Drop:** If data augmentation adds noise (Factual samples are unrealistic), generalization may suffer.

- **First 3 experiments:**
  1.  **Sanity Check (Table 2 logic):** Verify the synthesizer logic. For a positive sample with a value in bucket 5, ensure the "Factual" sample moves to bucket 6 (for increasing features) and "Counterfactual" moves to bucket 4.
  2.  **Ablation (Table 7 replication):** Run three variants to isolate gains: (A) CCSS with only Pairwise Loss, (B) CCSS with only Data Augmentation, (C) Full CCSS. Confirm that (C) > (B) > (A).
  3.  **Hyperparameter Sensitivity (Figure 3 replication):** Sweep $\alpha$ (e.g., [0.1, 0.5, 1.0, 2.0]) to find the inflection point where Mono_rate improves without degrading AUC.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the CCSS framework be extended to automatically infer the direction of monotonicity (increasing vs. decreasing) for specific features, rather than requiring it as a pre-defined hyperparameter?
- Basis: [inferred] Section 3.2.2 and Table 2 require the monotonic relationship to be known a priori to determine the direction of the disturbance (neighbor bucket selection).
- Why unresolved: The current method relies on manual definition or domain knowledge to label features as increasing or decreasing.
- What evidence would resolve it: A modified framework that learns the directionality dynamically and performs equivalently to the manually configured version.

### Open Question 2
- Question: Is the computational overhead of using Shapley Values for feature selection justified compared to simpler, more scalable importance metrics?
- Basis: [inferred] Section 3.2.1 justifies the use of Shapley Values by their "solid theoretical foundation," but does not analyze the cost/benefit ratio against simpler metrics.
- Why unresolved: Calculating Shapley Values can be resource-intensive, and the paper does not provide an ablation study against low-cost alternatives like gradient-based importance.
- What evidence would resolve it: Experiments comparing convergence speed and model performance when using Shapley Values versus simpler feature importance proxies.

### Open Question 3
- Question: Does the enforcement of strict monotonicity constraints degrade model performance on numerical features that naturally exhibit non-monotonic or complex non-linear relationships with the target?
- Basis: [inferred] Section 1 asserts monotonicity is "critical" and Section 3.3.1 enforces it via contrastive loss, assuming this prior knowledge is always correct.
- Why unresolved: While beneficial for interpretability, enforcing monotonicity on non-monotonic data (e.g., a feature with a "Goldilocks" zone) could introduce bias.
- What evidence would resolve it: Evaluation of CCSS on synthetic datasets where the ground-truth relationship between features and labels is non-monotonic.

## Limitations

- **Monotonicity assumption limitation:** The framework assumes all numerical features have strictly monotonic relationships with the target, which may not hold in real-world recommendation scenarios where feature interactions are complex.
- **Discretization details unspecified:** The discretization strategy (bucket width, boundaries) significantly impacts counterfactual quality but is not detailed in the paper.
- **Computational overhead:** The reliance on Shapley values for feature importance introduces computational overhead and potential estimation errors, though the paper does not discuss convergence or stability of these estimates during training.

## Confidence

- **High Confidence:** The general effectiveness of the contrastive learning approach and the observed improvements in AUC, GAUC, and Mono_rate metrics across both public and industrial datasets.
- **Medium Confidence:** The specific mechanisms by which importance-weighted feature sampling and data augmentation contribute to the performance gains, as the ablation studies provide supporting evidence but don't isolate all interaction effects.
- **Low Confidence:** The generalizability of the approach to features with non-monotonic relationships and the sensitivity of results to the unspecified discretization parameters and Shapley value computation details.

## Next Checks

1. **Monotonicity Assumption Validation:** Systematically test the framework on datasets where certain numerical features are known to have non-monotonic relationships with the target (e.g., inverted U-shaped curves). Measure whether the contrastive loss actively degrades performance on these features.

2. **Discretization Sensitivity Analysis:** Conduct experiments varying the number of discretization buckets (e.g., 10, 50, 100) and boundary calculation methods (equal width vs. equal frequency). Track how these changes affect Mono_rate, AUC, and training stability.

3. **Feature Importance Method Comparison:** Replace the Shapley value-based importance with simpler alternatives (e.g., variance-based sampling or gradient-based importance) to determine whether the computational overhead of Shapley values is justified by performance gains.