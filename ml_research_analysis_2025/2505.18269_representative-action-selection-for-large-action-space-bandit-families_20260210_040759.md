---
ver: rpa2
title: Representative Action Selection for Large Action Space Bandit Families
arxiv_id: '2505.18269'
source_url: https://arxiv.org/abs/2505.18269
tags:
- action
- regret
- afull
- full
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of selecting a small, representative
  subset of actions from a large action space that is shared by a family of bandits.
  The goal is to achieve performance nearly matching that of using the full action
  space, leveraging correlations between rewards of different actions.
---

# Representative Action Selection for Large Action Space Bandit Families

## Quick Facts
- arXiv ID: 2505.18269
- Source URL: https://arxiv.org/abs/2505.18269
- Reference count: 40
- Primary result: Proposes an algorithm that selects representative action subsets from large action spaces by sampling optimal actions from a bandit family, achieving near-optimal regret bounds under correlation structure assumptions.

## Executive Summary
This paper addresses the challenge of selecting a small, representative subset of actions from a large action space shared by a family of bandits. The proposed algorithm samples bandit instances and adds their optimal actions to build a representative set without requiring prior knowledge of correlation structure. Theoretical analysis shows that when the action space exhibits clustering or low-dimensional manifold structure, the expected regret of the selected subset is bounded in terms of covering numbers. Empirical validation demonstrates superior performance compared to Thompson Sampling and UCB methods, particularly in settings with varying correlation structures.

## Method Summary
The method iteratively samples bandit instances θ ~ P and identifies their optimal actions a*(θ) = argmax_a μ_a(θ), adding these to a representative subset A until K distinct actions are collected. The approach constructs a measure-theoretic ε-net by implicitly sampling from an "importance measure" q over the action space. Theoretical guarantees show that regret depends on the geometric complexity of action clusters rather than the full action space size. An oracle-free variant approximates the exact argmax using Thompson Sampling as the inner solver. The algorithm is evaluated on synthetic datasets with RBF kernel-correlated rewards and compared against standard bandit baselines.

## Key Results
- Algorithm 1 outperforms Thompson Sampling and UCB methods in identifying near-optimal action subsets
- Expected regret approaches zero as subset size increases under clustering assumptions
- Oracle-free variant (EpsilonNet+TS) achieves comparable performance to exhaustive search with fewer evaluations
- Regret bounds depend on cluster diameters rather than full action space diameter

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sampling optimal actions from the bandit family distribution constructs a representative subset capable of covering high-probability regions of the action space.
- **Mechanism:** Algorithm 1 samples a bandit instance θ and identifies its optimal action a*. This process implicitly draws samples from an "importance measure" q over the action space. If a cluster of actions is frequently optimal (high measure q), the algorithm is statistically likely to select a representative from it, satisfying measure-theoretic ε-net conditions.
- **Core assumption:** The learner can sample bandit instances θ ~ P and solve them to find a*; the underlying action space possesses a correlation structure where q concentrates on manageable clusters.
- **Evidence anchors:**
  - [abstract] "algorithm... iteratively samples bandit instances and adds their optimal actions... without requiring prior knowledge of the correlation structure."
  - [section 3, p.3] Defines the "importance measure q" and the probability distribution of optimal actions.
  - [corpus] Contextual support from "Representative Action Selection..." papers, though specific mechanism validation is internal to this work.
- **Break condition:** If the distribution of optimal actions q is diffuse (uniform over a massive set) or if the sampler cannot access θ, the net construction fails.

### Mechanism 2
- **Claim:** The regret of the reduced action set is bounded by the geometric complexity (diameter) of action clusters rather than the full action space size.
- **Mechanism:** The paper argues that if the full action space A_full can be partitioned into clusters r_ℓ with small diameter ε, substituting any action with a representative from the same cluster incurs a regret proportional to ε. Algorithm 1 ensures these representatives are found by hitting clusters with high "importance."
- **Core assumption:** The reward process {μ_a} is Gaussian (or sub-Gaussian), inducing a metric structure where "similarity" implies similar rewards (small diameter ⟹ small regret).
- **Evidence anchors:**
  - [section 4, p.4] "regret bound... depends not on the diameter of the entire action space... but rather on the diameters of the individual clusters."
  - [theorem 4.5, p.5] Bounds regret by max E[max μ_a - μ_{a_ℓ}] (cluster diameter term) and sampling error.
- **Break condition:** If the action space lacks correlation (i.e., i.i.d. rewards, diameter is large), the cluster diameter term dominates, and reducing the action space causes high regret.

### Mechanism 3
- **Claim:** An exact oracle for finding optimal actions is not strictly necessary; a standard bandit solver (like Thompson Sampling) can approximate the oracle effectively.
- **Mechanism:** In the "Oracle-Free" variant, the algorithm replaces the exact argmax with a short run of Thompson Sampling (e.g., 300 steps). As long as this inner loop identifies a near-optimal action for the sampled instance, the outer loop continues to build a valid representative set.
- **Core assumption:** The inner bandit algorithm has sufficient signal-to-noise ratio and iterations to converge to the optimal action for the specific instance θ.
- **Evidence anchors:**
  - [section 5.2, p.8] "EpsilonNet+TS... oracle argmax step is approximated by running TS for 300 rounds."
  - [figure 3, p.8] Shows EpsilonNet+TS outperforming Combinatorial TS/UCB.
- **Break condition:** If the inner budget (e.g., 300 steps) is too small for the problem difficulty, the approximated optimal actions are random noise, breaking the importance measure sampling.

## Foundational Learning

- **Concept: Measure-Theoretic ε-Nets**
  - **Why needed here:** The core theoretical proof relies on the algorithm constructing an ε-net that covers "heavy" clusters (high measure q) while ignoring "light" ones. Understanding the difference between geometric covering (distance) and measure-theoretic covering (probability) is key to the regret bounds.
  - **Quick check question:** Can you explain why a set might be a valid ε-net in a geometric sense (covers the space) but fail to be a measure-theoretic net for a specific distribution P?

- **Concept: Gaussian Processes (GP) & Kernels**
  - **Why needed here:** The paper assumes the reward function is a sample from a GP to define "similarity" via covariance (RBF kernel). The diameter of clusters is defined by this kernel-induced metric.
  - **Quick check question:** How does the length-scale parameter l in an RBF kernel affect the "diameter" of action clusters and the resulting regret?

- **Concept: Bayesian Bandit Regret**
  - **Why needed here:** The objective minimizes *Bayesian regret* (expectation over the distribution of bandits P), rather than worst-case regret. This assumption is critical for the "importance sampling" logic to hold.
  - **Quick check question:** If we evaluated the algorithm on a fixed, adversarial bandit instance outside the support of P, would the theoretical guarantees still hold?

## Architecture Onboarding

- **Component map:** Source Distribution (P) -> Solver (Inner Loop) -> Accumulator -> Evaluator
- **Critical path:** The **Solver (Inner Loop)**. If the solver is slow or inaccurate, the entire subset selection process degrades. The paper replaces an O(|A_full|) exact search with a O(T_inner) approximate search.
- **Design tradeoffs:**
  - **Exact vs. Approximate Oracle:** Using an exact oracle (Line 6, Algo 1) provides theoretical guarantees but is computationally expensive. Using TS (Section 5.2) is faster/practical but adds estimation noise.
  - **Subset Size (K):** Higher K reduces the probability of missing important clusters (exponential decay of error term) but increases pre-computation time.
- **Failure signatures:**
  - **High Variance in Subset:** If A fluctuates wildly between runs, the inner solver budget is likely too low.
  - **High Regret on Validation:** If regret remains high despite increasing K, the action space likely lacks the assumed correlation structure (clusters are too wide/diameter is large).
  - **Uniform Selection:** If Algorithm 1 selects actions uniformly at random, the "importance measure" q is likely near-uniform (actions are independent), suggesting the method is ill-suited for the dataset.
- **First 3 experiments:**
  1. **Synthetic Validation (RBF Kernel):** Replicate Figure 2. Generate actions on a grid, sample rewards via RBF kernel, and plot Expected Regret vs. Length Scale to verify that high correlation (large scale) lowers regret.
  2. **Ablation on Inner Loop Budget:** Implement "EpsilonNet+TS" and vary the inner loop steps (e.g., 50, 100, 300, 1000) to find the point of diminishing returns for the solver accuracy.
  3. **Sensitivity to Clustering:** Construct a synthetic dataset with distinct, varying cluster diameters (as in Figure 4) to empirically validate the relationship between cluster diameter and regret bounds.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can a data-dependent stopping rule be developed to automatically determine the optimal subset size K?
- **Basis in paper:** [explicit] "Natural directions for future work include developing data-dependent stopping rules for choosing the subset size K..."
- **Why unresolved:** Algorithm 1 currently requires K as a fixed input parameter, and the theoretical bounds (Theorem 4.5) describe performance given a specific K rather than an adaptive convergence criterion.
- **What evidence would resolve it:** A modified algorithm that terminates based on the variance or coverage of the sampled optimal actions, along with corresponding theoretical guarantees.

### Open Question 2
- **Question:** Can the representative action selection framework be extended to Markov Decision Processes (MDPs) to capture sequential decision making?
- **Basis in paper:** [explicit] "Natural directions for future work include... extending the setting from bandits to Markov decision processes to capture sequential decision making."
- **Why unresolved:** The current theoretical analysis relies on the structure of single-step bandit instances; MDPs introduce temporal dependencies, state transitions, and policy complexities not covered by the current definitions of regret or importance measures.
- **What evidence would resolve it:** A formalization of the problem for MDPs and a regret bound for the selected action subset in a sequential setting.

### Open Question 3
- **Question:** What are the theoretical performance guarantees when the exact argmax oracle is replaced by an approximate solver (e.g., Thompson Sampling)?
- **Basis in paper:** [inferred] Section 5.2 empirically replaces the oracle with TS (EpsilonNet+TS) to create an "oracle-free" variant, but the main theoretical results (Theorem 4.5) assume exact identification of the optimal action a*(θ).
- **Why unresolved:** It is unstated how the approximation error from an iterative solver propagates to the quality of the epsilon-net or the final regret bound.
- **What evidence would resolve it:** A theoretical bound on the expected regret that explicitly accounts for the probability of error in the inner maximization step.

## Limitations

- **Assumption Dependence:** The method's effectiveness hinges critically on the action space exhibiting exploitable correlation structure (clustering, low-dimensional manifold). Performance degrades significantly when this assumption fails, though the paper doesn't quantify this degradation threshold.
- **Computational Scalability:** While the approach reduces action space size, the inner bandit solver (even with TS approximation) still requires multiple evaluations per sampled instance. The paper doesn't analyze how this scales with problem dimensionality or computational budget constraints.
- **Theorem Generality:** The theoretical bounds (Theorem 4.9) assume Gaussian rewards and specific cluster diameter properties. Extension to non-Gaussian rewards or different correlation structures remains unproven.

## Confidence

- **High Confidence:** The core mechanism of sampling optimal actions to build representative subsets is well-supported by empirical results and theoretical analysis under stated assumptions.
- **Medium Confidence:** The theoretical regret bounds hold under idealized conditions (exact oracle, known cluster structure). The approximation error introduced by the TS-based oracle-free variant lacks formal analysis.
- **Low Confidence:** Claims about performance on highly correlated vs. weakly correlated action spaces rely on limited synthetic experiments. Real-world applicability to complex correlation structures remains uncertain.

## Next Checks

1. **Correlation Structure Sensitivity:** Systematically vary the correlation strength in synthetic experiments and measure the exact performance degradation point where the method becomes ineffective.

2. **Non-Gaussian Reward Testing:** Validate the algorithm's performance when rewards follow heavy-tailed distributions or other non-Gaussian processes to assess theoretical bound applicability.

3. **High-Dimensional Scaling:** Test the method on action spaces with dimensionality beyond 2D (as shown in current experiments) to evaluate computational and performance scaling.