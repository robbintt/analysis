---
ver: rpa2
title: 'Machine Learning for Climate Policy: Understanding Policy Progression in the
  European Green Deal'
arxiv_id: '2510.16233'
source_url: https://arxiv.org/abs/2510.16233
tags:
- policy
- text
- features
- metadata
- climate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study uses machine learning to analyze the progression of
  climate policies within the European Green Deal, aiming to predict policy adoption
  stages from announcement to completion. A dataset of 165 policies with text and
  metadata was used, comparing text representation methods (TF-IDF, BERT, ClimateBERT)
  and regression models.
---

# Machine Learning for Climate Policy: Understanding Policy Progression in the European Green Deal

## Quick Facts
- arXiv ID: 2510.16233
- Source URL: https://arxiv.org/abs/2510.16233
- Reference count: 14
- Best model (BERT + metadata) achieves RMSE = 0.16, R² = 0.38 on predicting policy progression stages

## Executive Summary
This study applies machine learning to analyze and predict the progression of climate policies within the European Green Deal, mapping 165 policies from announcement through adoption. The research compares text representation methods (TF-IDF, BERT, ClimateBERT) and regression models, finding that combining BERT embeddings with metadata features delivers the strongest predictive performance. Explainability analysis reveals that political party representation and rapporteur information are dominant predictors beyond policy text content. The work demonstrates ML's potential for climate policy analysis while highlighting the importance of interpretability and ethical considerations in sensitive legislative contexts.

## Method Summary
The research employs regression models to predict policy progression stages mapped to a 0-1 continuous scale, using both text and metadata features from 165 European Green Deal policies. Text preprocessing includes cleaning and lemmatization, with embeddings generated via TF-IDF, BERT, and ClimateBERT. Metadata features include rapporteur country/party, procedure type, and spotlight status. Models compared include Bayesian Ridge, SVR, Random Forest, and CatBoost, with 80:20 train-test splits. Feature importance is analyzed using permutation importance and SHAP values to understand prediction drivers.

## Key Results
- BERT with metadata features achieves best performance (RMSE = 0.16, R² = 0.38)
- ClimateBERT excels on text alone (RMSE = 0.17, R² = 0.29), reflecting climate-specific pretraining
- Political party representation and rapporteur information dominate feature importance
- Text-only models show significantly lower performance than combined text-metadata approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific language model pretraining improves text-only policy prediction performance.
- Mechanism: ClimateBERT, pretrained on climate-related texts, captures domain semantics that general-purpose BERT misses when metadata is unavailable. This allows the model to extract predictive signals directly from policy language.
- Core assumption: Climate-specific vocabulary and phrasing patterns correlate with policy progression stages.
- Evidence anchors:
  - [abstract] "On text features alone, ClimateBERT outperforms other approaches (RMSE = 0.17, R^2 = 0.29)"
  - [section 3.1] "ClimateBERT demonstrated smaller performance improvements with the addition of metadata features. This may reflect the pretraining of ClimateBERT on climate texts, such that it was already capable of capturing relevant information from the policy texts"
  - [corpus] Weak direct corpus support for this specific mechanism; related NLP-climate work (e.g., "Analyzing Regional Impacts of Climate Change using Natural Language Processing Techniques") uses different task formulations.
- Break condition: If policy text length or complexity exceeds model context windows without chunking, or if domain vocabulary has shifted post-pretraining, this advantage may degrade.

### Mechanism 2
- Claim: Metadata features—particularly political party representation and rapporteur information—provide predictive signal beyond text content alone.
- Mechanism: Legislative context (who sponsors, which party, which country) encodes institutional dynamics that influence adoption probability. These features are orthogonal to text semantics and thus complementary.
- Core assumption: Political dynamics are relatively stable across the observation period and correlate with progression outcomes.
- Evidence anchors:
  - [abstract] "BERT achieves superior performance with the addition of metadata features (RMSE = 0.16, R^2 = 0.38)"
  - [section 3.2] "The 'no party' metadata feature value was observed as the most important feature to the prediction model, by a large margin"
  - [section 3.2] "Feature importance analysis illustrated the dominance of political party representation as a predictor for policy progression"
  - [corpus] No direct corpus validation for this specific EU Green Deal finding; related climate policy RL work ("Exploring Equity of Climate Policies using Multi-Agent Multi-Objective Reinforcement Learning") addresses different institutional mechanisms.
- Break condition: If rapporteur assignments or party compositions change (e.g., post-election), feature importance patterns may shift, requiring model retraining.

### Mechanism 3
- Claim: Explainability methods (SHAP, permutation importance) reveal that non-content features can dominate text features in policy prediction.
- Mechanism: Permutation importance measures performance degradation when features are shuffled; SHAP decomposes predictions into feature contributions. Both expose that metadata—especially party affiliation—drives predictions more than specific policy wording.
- Core assumption: Feature importance reflects genuine causal influence on the prediction target, not merely correlation with omitted variables.
- Evidence anchors:
  - [section 3.2] "The 'no party' metadata feature directed policy prediction away from advanced progression stages, as indicated by the positive feature values associated with negative SHAP values"
  - [section 3.2] "High values for the 'MEPs' metadata feature directed policy prediction towards advanced progression stages"
  - [corpus] No corpus papers validate this specific explainability finding for EU climate policy.
- Break condition: If class imbalance (e.g., <5% "Blocked"/"Withdrawn") biases importance estimates toward majority-class patterns, interpretations may mislead advocacy decisions.

## Foundational Learning

- Concept: **Transformer embeddings vs. bag-of-words representations**
  - Why needed here: Understanding why BERT/ClimateBERT outperform TF-IDF on semantic tasks, but TF-IDF remains more interpretable for SHAP analysis.
  - Quick check question: Given a policy document, would you expect TF-IDF or BERT to better capture that "agreement" appears more in later-stage policies? Why?

- Concept: **Regression metrics for ordinal classification**
  - Why needed here: The paper maps policy stages to 0–1 scale and uses RMSE/R² rather than classification accuracy to preserve ordinal relationships.
  - Quick check question: If predicting "Announced" (0.25) when truth is "Adopted" (1.0), should the penalty differ from predicting "Tabled" (0.5)? How does RMSE encode this?

- Concept: **Feature permutation importance and SHAP**
  - Why needed here: These methods identify which features drive predictions; permutation is model-agnostic, SHAP provides per-instance contributions.
  - Quick check question: If shuffling a feature causes RMSE to increase by 0.05, what does that imply about the feature's importance? What if SHAP values are large but permutation importance is small?

## Architecture Onboarding

- Component map: Raw policy text + 62 metadata features -> Text cleaning + metadata encoding -> TF-IDF/BERT/ClimateBERT embeddings -> Regression models (CatBoost, RF, BR, SVR) -> RMSE/R² evaluation + permutation importance + SHAP

- Critical path:
  1. Collect policy text + metadata from European Green Deal tracker
  2. Preprocess text and encode metadata
  3. Extract features using TF-IDF, BERT, or ClimateBERT
  4. Train regression model with 80:20 train-test split
  5. Evaluate with RMSE/R²; apply explainability methods to interpret drivers

- Design tradeoffs:
  - **Performance vs. interpretability**: BERT + metadata achieves best RMSE (0.16) but sentence-level embeddings limit text explainability; TF-IDF + CatBoost enables SHAP on words but lower R² (0.20)
  - **Text-only vs. text+metadata**: ClimateBERT strong on text alone; BERT benefits more from metadata augmentation
  - **Small dataset constraints**: 165 policies with class imbalance (<5% "Blocked"/"Withdrawn") may bias predictions toward majority classes

- Failure signatures:
  - **Overfitting to majority classes**: Model predicts "Adopted" accurately but misses "Blocked"/"Withdrawn"
  - **Metadata leakage**: Rapporteur features may encode post-hoc assignment (not available at prediction time for new policies)
  - **Temporal drift**: Post-election changes in party representation break learned associations

- First 3 experiments:
  1. **Baseline replication**: Implement TF-IDF + Bayesian Ridge on text-only; verify RMSE ≈ 0.17, R² ≈ 0.28
  2. **Ablation study**: Train BERT + metadata with/without rapporteur party features; quantify performance drop to assess feature contribution
  3. **Temporal holdout**: Train on pre-2023 policies, test on 2023–2024 policies; evaluate whether importance patterns hold under political shifts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the trained models effectively predict the progression of ongoing policies where the final status is not yet known?
- Basis in paper: [explicit] The authors state a "possible extension... is to evaluate the predictive utility of the trained models on current or ongoing policies whose final status is not yet known."
- Why unresolved: The current study relies on retrospective analysis of completed or static policy stages; prospective validation is untested.
- What evidence would resolve it: A time-series evaluation testing the model's predictions against the actual future outcomes of policies introduced after the training data cutoff.

### Open Question 2
- Question: Do the identified predictive features (e.g., party affiliation) generalize to legislative domains outside of climate policy?
- Basis in paper: [explicit] The authors note that "applying these models to different legislative areas will help evaluate whether findings generalise or are specific to climate policy texts."
- Why unresolved: The dataset is restricted to the European Green Deal, potentially capturing domain-specific correlations not present in other legislative topics.
- What evidence would resolve it: Comparative studies applying the same methodology to non-climate EU legislative datasets to see if political metadata remains the dominant predictor.

### Open Question 3
- Question: Does the high importance of the "no party" metadata feature reflect confounding factors verifiable by domain experts?
- Basis in paper: [explicit] The analysis identified "no party" as the most important feature, but the authors state "confounding factors" may exist and "further work, with domain experts, should be carried out."
- Why unresolved: Machine learning explainability identifies correlation (SHAP values), but cannot distinguish between causation and confounding variables without qualitative domain knowledge.
- What evidence would resolve it: Qualitative validation from legislative experts confirming whether the absence of party affiliation proxies for specific policy types or political strategies.

## Limitations

- Small dataset (165 policies) with severe class imbalance (<5% "Blocked"/"Withdrawn") may bias predictions toward majority classes
- Political feature importance may not generalize across election cycles or legislative domains
- Unclear definitions of some metadata features (e.g., "ClimateBERT Predicted score") affect reproducibility

## Confidence

- **High Confidence**: Domain-specific pretraining advantage of ClimateBERT over general BERT
- **Medium Confidence**: Metadata features (particularly party representation) as stronger predictors than text content
- **Low Confidence**: Specific explainability interpretations requiring external validation for policy advocacy applications

## Next Checks

1. **Temporal Validation**: Retrain the model using pre-2023 policies and test on 2023-2024 policies to assess whether feature importance patterns hold under political shifts, particularly after elections.

2. **Ablation Study**: Systematically remove metadata features (starting with rapporteur party information) to quantify performance degradation and confirm their contribution beyond text alone.

3. **Alternative Evaluation Metrics**: Implement ordinal classification metrics (e.g., Mean Absolute Error on ordinal scale) and compare against the current RMSE/R² approach to validate that the continuous regression formulation appropriately captures policy progression semantics.