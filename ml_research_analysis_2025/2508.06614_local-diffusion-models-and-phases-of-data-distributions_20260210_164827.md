---
ver: rpa2
title: Local Diffusion Models and Phases of Data Distributions
arxiv_id: '2508.06614'
source_url: https://arxiv.org/abs/2508.06614
tags:
- local
- diffusion
- phase
- data
- denoisers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel perspective on phases of data distributions
  in diffusion models, defining two distributions as belonging to the same phase if
  they can be mutually connected via spatially local operations. The authors prove
  that during the denoising process, there exists a phase transition where local denoisers
  fail and global information becomes necessary.
---

# Local Diffusion Models and Phases of Data Distributions

## Quick Facts
- **arXiv ID**: 2508.06614
- **Source URL**: https://arxiv.org/abs/2508.06614
- **Reference count**: 0
- **Primary result**: Introduces phases of data distributions in diffusion models, proving local denoising is possible when Conditional Mutual Information (CMI) decays exponentially with distance, and validates this through MNIST experiments showing phase transition around t_c ≈ 0.3-0.4

## Executive Summary
This paper introduces a novel perspective on phases of data distributions in diffusion models, defining two distributions as belonging to the same phase if they can be mutually connected via spatially local operations. The authors prove that during the denoising process, there exists a phase transition where local denoisers fail and global information becomes necessary. They establish a theoretical framework using conditional mutual information (CMI) to diagnose phase transitions, proving that local denoising is possible when CMI decays exponentially with distance. The authors validate their theory through numerical experiments on the MNIST dataset, demonstrating a phase transition around t_c ≈ 0.3-0.4 where local denoisers with small receptive fields begin to fail. This work suggests that diffusion models can be made more efficient by using small local neural networks for computing score functions far from phase transitions, with global networks only needed around the narrow time interval of phase transitions.

## Method Summary
The method introduces a hybrid architecture combining local and global denoisers based on CMI analysis. A linear noise schedule is used with flow matching formulation (X_t = (1-t)X_0 + tZ). The authors train local U-Nets with constrained receptive fields by removing pooling layers and using small kernels (size 3), while global U-Nets maintain full context. CMI is estimated using Mutual Information Neural Estimation (MINE) with a CNN architecture. During inference, local denoisers handle timesteps far from the critical time t_c, while global denoisers are activated only during the narrow transition window where CMI spikes. The approach is validated on MNIST with 60,000 training images, using AdamW optimizer (LR 1e-3, weight decay 1e-3), batch size 512, and 15 training epochs.

## Key Results
- Theoretical proof that local denoising is possible when CMI decays exponentially with distance along the entire diffusion path
- Experimental validation of phase transition on MNIST around t_c ≈ 0.3-0.4 where local denoisers with small receptive fields begin to fail
- Demonstration that using global networks only in the critical window [0.2, 0.5] matches the performance of fully global denoising
- Establishment of fundamental classical-quantum correspondence between diffusion models and quantum Petz maps

## Why This Works (Mechanism)

### Mechanism 1: Local Reversibility via CMI Decay
- Claim: A local denoiser can accurately reverse a diffusion step if the Conditional Mutual Information (CMI) between spatially separated regions decays exponentially with distance.
- Mechanism: The paper leverages the classical Fawzi-Renner inequality to bound the recovery error (KL divergence) by the CMI. Low CMI implies approximate spatial conditional independence (P_ABC ≈ P_AB·P_C|B), meaning the local score function (∂_x_A ln P_AB) approximates the global score (∂_x_A ln P) effectively.
- Core assumption: The data distribution at a given time step possesses a finite "Markov length" ξ, meaning correlations vanish sufficiently fast over distance.
- Evidence anchors: [abstract] "...proving that local denoising is possible when CMI decays exponentially with distance." [Page 3, col. 2] "We prove that local denoising is possible if CMI decays exponentially with distance, along the whole diffusion path."

### Mechanism 2: Phase Transition during Denoising
- Claim: The reverse denoising process traverses distinct "phases" (trivial and data) separated by a narrow phase transition where local processing fails.
- Mechanism: Early in denoising (high noise), the distribution is essentially Gaussian (trivial phase, CMI ≈ 0). Late in denoising (low noise), it resembles the dataset (data phase). Between these, correlations span the system, causing a spike in CMI and requiring global information integration.
- Core assumption: The dataset exhibits sufficient spatial structure to create a distinct critical point rather than a gradual shift.
- Evidence anchors: [abstract] "...show that the reverse denoising process consists of an early trivial phase and a late data phase, sandwiching a rapid phase transition where local denoisers must fail." [Page 5, col. 1] "At t_c ≈ 0.3 ∼ 0.4, we observe a significant CMI barrier..."

### Mechanism 3: Hybrid Efficiency via Critical Time Selection
- Claim: Computational efficiency can be optimized by restricting computationally expensive global networks to the narrow time window of the phase transition.
- Mechanism: Since CMI is low (and local denoisers work) everywhere except near the critical time t_c, small neural networks with limited receptive fields can handle the majority of the denoising steps. Global networks are deployed only when the CMI barrier is active.
- Core assumption: The phase transition window is sufficiently narrow relative to the total diffusion steps to yield significant savings.
- Evidence anchors: [Page 4, col. 2] "...global neural networks are only necessary around the narrow time interval of phase transitions." [Page 6, Fig 4] Shows that using global denoisers only in [0.2, 0.5] matches the performance of fully global denoising.

## Foundational Learning

- **Concept: Conditional Mutual Information (CMI)**
  - Why needed here: CMI serves as the primary diagnostic metric to determine if local recovery is possible (I(X_A:X_C|X_B)) and to detect phase transitions.
  - Quick check question: If CMI between region A and C (given B) is zero, what does that imply about the spatial dependencies in the data? (Answer: Markovianity / conditional independence).

- **Concept: Fokker-Planck Equation (Forward/Backward)**
  - Why needed here: This governs the evolution of probability distributions in diffusion models; the paper defines "k-local" versions of this operator to formalize locality.
  - Quick check question: In the backward SDE, what term acts as the "steering" force to reverse the noise? (Answer: The score function, ∇ ln P_t(x)).

- **Concept: Receptive Field**
  - Why needed here: This translates the theoretical "locality" (buffer width r) into a concrete architectural constraint for the neural networks used in experiments.
  - Quick check question: If the Markov length ξ increases, how must the receptive field of the local denoiser change to maintain low error? (Answer: It must grow linearly with ξ).

## Architecture Onboarding

- **Component map:**
  - MINE Estimator -> CMI Monitoring -> Scheduler -> (Local Denoiser for t ∉ [t_c - δ, t_c + δ]) and (Global Denoiser for t ≈ t_c)

- **Critical path:**
  1. Identify the critical time window (t_c) for the specific dataset (e.g., via CMI estimation or grid search on fidelity).
  2. Train/Fine-tune local denoisers with small receptive fields.
  3. Schedule the inference pipeline to switch from local → global → local denoisers based on t_c.

- **Design tradeoffs:**
  - **Receptive Field (r) vs. Efficiency:** Increasing r improves local denoiser performance but increases computational cost.
  - **Transition Width vs. Quality:** Narrowing the global window saves compute but risks missing the critical transition, leading to artifacts.

- **Failure signatures:**
  - **Local-only failure:** Output images look like noisy blobs or unrecognizable shapes if the local denoiser is applied across the phase transition (Fig 4b).
  - **Missed Transition:** If the global window is too small or shifted, global structure (e.g., digit shape in MNIST) fails to form.

- **First 3 experiments:**
  1. **CMI Profiling:** Estimate CMI I(X_A : X_C | X_B) over the diffusion timeline on a subset of data to locate t_c.
  2. **Receptive Field Ablation:** Train local denoisers with varying r and plot fidelity (MSE) vs. time t to verify the failure point corresponds to t_c.
  3. **Hybrid Inference Test:** Run generation using global networks only in the interval [t_c - ε, t_c + ε] and local networks elsewhere; compare FID scores against a fully global baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the phase transitions defined by spatial Markov length coincide with symmetry-breaking phase transitions defined by energy landscapes in real-world data?
- Basis in paper: [explicit] The authors state, "An intriguing open question is whether these two types of phase transitions coincide in real-world data, and if so, whether they are driven by the same mechanism."
- Why unresolved: Spatial Markov length relies on spatial locality (CMI), whereas symmetry-breaking transitions rely on the concentration of the score function in data space without spatial structure. The relationship between these distinct theoretical frameworks is unknown.
- What evidence would resolve it: Numerical studies on complex datasets analyzing the correlation between critical times (t_c) derived from CMI measurements and those derived from energy landscape analysis.

### Open Question 2
- Question: Can alternative diffusion paths be designed to completely bypass the Markov length divergence, similar to bypassing a critical point in a liquid-vapor transition?
- Basis in paper: [explicit] The authors suggest, "There may exist two paths... such that one path has a Markov length divergence, but the Markov length is always finite along the other path."
- Why unresolved: While the theory suggests such paths exist (implying fully local reversibility is possible), finding the specific noise schedules or intermediate distributions required to construct these paths in high-dimensional generative models remains an unsolved design challenge.
- What evidence would resolve it: The experimental demonstration of a noise schedule or flow matching path that maintains finite Markov length throughout the entire generative process, allowing for successful generation using only strictly local denoisers.

### Open Question 3
- Question: Do the specific phase transition behaviors (e.g., the narrow time window of failure) hold for complex, high-resolution datasets beyond MNIST?
- Basis in paper: [inferred] The paper validates its theory using the MNIST dataset (28x28 pixels). It is not demonstrated whether the transition width scales with image size or if the "narrow time interval" assumption holds for high-resolution data.
- Why unresolved: The Markov length ξ scales as ξ ∝ 1/|t-t_c|^ν; in finite systems, this peak is broadened. It is unclear if the transition remains "rapid" enough for the proposed efficiency gains (using small local networks) to persist in large-scale models like Stable Diffusion.
- What evidence would resolve it: Empirical measurement of CMI and local denoiser failure rates on high-resolution datasets (e.g., ImageNet) to determine if the global denoising requirement remains restricted to a small fraction of the diffusion steps.

## Limitations
- The theoretical framework assumes the data distribution has a well-defined finite Markov length ξ, which is not rigorously characterized for complex, high-dimensional datasets beyond MNIST.
- The CMI estimation via MINE is subject to optimization challenges and bias, though the paper reports stable estimates without providing statistical error bounds.
- The phase transition window width is empirically determined but not theoretically predicted, and the exact critical time t_c may vary with architecture choices and dataset specifics.

## Confidence

- **High Confidence**: The core theoretical result that CMI decay enables local denoising, and the MNIST experimental validation showing phase transition around t_c ≈ 0.3-0.4.
- **Medium Confidence**: The general applicability of the framework to other datasets and diffusion models beyond MNIST, as the paper doesn't extensively test these cases.
- **Low Confidence**: The quantum-classical correspondence claim, which appears in the abstract but receives minimal development in the main text.

## Next Checks

1. **Dataset Generalization**: Apply the hybrid local-global architecture to CIFAR-10 or CelebA to verify if the phase transition phenomenon and efficiency gains generalize beyond MNIST's simpler structure.

2. **CMI Estimation Robustness**: Implement and compare multiple CMI estimation methods (e.g., variational approaches, kernel-based estimators) on the same MNIST data to assess the stability and reproducibility of the reported phase transition timing.

3. **Architecture Sensitivity Analysis**: Systematically vary the U-Net architecture parameters (channel dimensions, layer depth) and measure how this affects both the critical time t_c and the efficiency gains from the hybrid approach.