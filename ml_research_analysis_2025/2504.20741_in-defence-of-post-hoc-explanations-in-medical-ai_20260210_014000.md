---
ver: rpa2
title: In defence of post-hoc explanations in medical AI
arxiv_id: '2504.20741'
source_url: https://arxiv.org/abs/2504.20741
tags:
- explanations
- post-hoc
- black
- systems
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# In defence of post-hoc explanations in medical AI

## Quick Facts
- arXiv ID: 2504.20741
- Source URL: https://arxiv.org/abs/2504.20741
- Reference count: 18
- Primary result: Defends post-hoc explanations against critiques that they are "fool's gold" for medical AI

## Executive Summary
This paper argues that post-hoc explanations in medical AI—often criticized as unreliable approximations—are nonetheless valuable for improving human-AI team performance and supporting clinical decision-making. The authors distinguish between inference opacity (cannot explain why input x produces output y) and training opacity (cannot trace how parameters emerged from data), asserting that post-hoc methods can address the former. They contend that saliency masks, LIME, and SHAP can help clinicians discriminate between correct and incorrect AI outputs, predict model behavior changes, and justify AI-informed decisions when combined with institutional explanations.

## Method Summary
This is a philosophical defense paper that synthesizes existing literature rather than presenting original empirical methods. The authors review critiques of post-hoc explanations (that they are mere approximations that can mislead) and counter with evidence from studies like Senoner et al. (2024), which found radiologist-AI teams were 4.7 percentage points more accurate when using saliency masks. The paper distinguishes between model-specific (saliency maps) and model-agnostic (LIME, SHAP) explanation approaches, and discusses how post-hoc explanations can be combined with institutional context (design decisions, training data provenance, reliability metrics) to enhance their utility.

## Key Results
- Post-hoc explanations can improve human-AI team accuracy by helping clinicians discriminate between correct and incorrect AI outputs
- Visual overlays like saliency masks can enhance users' ability to predict how model outputs change with input modifications
- Combining post-hoc explanations with institutional explanations can support clinical justification of AI-informed decisions
- Users with sufficient domain expertise can meaningfully interpret highlighted features to calibrate their reliance on AI predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Post-hoc explanations can improve human-AI team accuracy by enabling better discrimination between correct and incorrect AI outputs.
- Mechanism: Visual overlays (e.g., saliency masks) highlight influential input regions, allowing domain experts to calibrate their reliance—following accurate predictions while overruling erroneous ones.
- Core assumption: Users possess sufficient domain expertise to meaningfully interpret highlighted features against their clinical knowledge.
- Evidence anchors:
  - [abstract] "Empirical evidence shows that post-hoc explanations like saliency masks can improve human-AI team performance by 4.7 percentage points in identifying lung lesions."
  - [section 5] "Senoner et al. (2024) found that radiologist-AI teams were 4.7 percentage points more accurate... domain experts supported by explainable AI were more likely to follow AI predictions when they were accurate and more likely to overrule them when they were wrong."
  - [corpus] Neighbor paper on SHAP for multi-contrast medical image segmentation (arXiv:2504.04645) suggests similar attribution methods are being actively explored for clinical imaging tasks.
- Break condition: If users lack domain expertise, saliency highlights may be misinterpreted or ignored, negating accuracy gains.

### Mechanism 2
- Claim: Post-hoc explanations can improve users' functional understanding by enabling prediction of how model outputs change with input modifications.
- Mechanism: By revealing which features influence outputs, users develop a causal mental model of system behavior, allowing them to anticipate responses to novel inputs—even without knowing the true internal reasoning.
- Core assumption: The explanation method provides sufficient signal about feature-output relationships for users to generalize to new cases.
- Evidence anchors:
  - [abstract] "post-hoc explanations help users predict model behavior changes with input modifications"
  - [section 5] "Wang and Yin (2021) found that post-hoc explanations improved users' ability to predict how the outputs of a model may change in response to changes in input data."
  - [corpus] No direct corpus support; neighbor papers focus on technical explanation methods rather than behavioral prediction tasks.
- Break condition: If post-hoc explanations are systematically misleading (e.g., fail to capture spurious correlations), predictive understanding will be incorrect.

### Mechanism 3
- Claim: Post-hoc explanations combined with institutional explanations can support clinical justification of AI-informed decisions.
- Mechanism: Post-hoc feature attributions are contextualized with metadata about system design, training data, known limitations, and reliability metrics—enabling clinicians to construct reasoned justifications even without access to true model internals.
- Core assumption: Institutional context is accurate and honestly represented; users integrate both information sources appropriately.
- Evidence anchors:
  - [abstract] "post-hoc explanations... assist in justifying AI-informed decisions"
  - [section 5] "post-hoc explanations can be combined with 'institutional explanations' to make the former more meaningful... These explanations include information about the design decisions that went into the making of the [black box] system."
  - [corpus] Weak support; corpus papers focus on technical explanation generation rather than hybrid justification frameworks.
- Break condition: If institutional metadata is incomplete, outdated, or misleading, justifications may be unfounded.

## Foundational Learning

- Concept: **Inference opacity vs. training opacity**
  - Why needed here: The paper distinguishes these two forms of black-box opacity; understanding this clarifies what post-hoc methods can and cannot address. Inference opacity (cannot explain why input x produces output y) is the target of post-hoc explanations; training opacity (cannot trace how parameters emerged from data) is not.
  - Quick check question: Can you explain why a post-hoc explanation cannot resolve training opacity?

- Concept: **Model-specific vs. model-agnostic explanations**
  - Why needed here: Saliency maps (model-specific) work only for gradient-based models; LIME/SHAP (model-agnostic) work across model types. Selecting the appropriate method depends on deployment constraints.
  - Quick check question: Your deployment uses an ensemble of random forests and neural networks—which explanation approach is more suitable?

- Concept: **Automation bias**
  - Why needed here: The paper explicitly warns that post-hoc explanations can increase automation bias (over-reliance on automated cues) rather than reduce it; understanding this risk is essential for safe deployment.
  - Quick check question: What cognitive forcing technique could you implement to mitigate automation bias when presenting explanations?

## Architecture Onboarding

- Component map:
  - Black-box model -> Post-hoc explainer -> Explanation interface -> Clinician review
  - (Optional) Institutional context store -> (Optional) Cognitive forcing layer

- Critical path:
  1. Input data -> Black-box model -> Prediction output
  2. Input + prediction -> Post-hoc explainer -> Feature attribution
  3. Attribution + institutional context -> Explanation interface -> Clinician review
  4. (Optional) Cognitive forcing intervention before explanation display

- Design tradeoffs:
  - **Fidelity vs. interpretability**: Post-hoc explanations approximate, not replicate, model reasoning; higher fidelity may require more complex explanations that are harder to interpret
  - **Latency vs. thoroughness**: Real-time saliency generation may be faster but less accurate than slower perturbation-based methods
  - **Automation bias mitigation vs. workflow efficiency**: Cognitive forcing adds friction, potentially reducing throughput

- Failure signatures:
  - **Explanation bias**: Post-hoc methods can exhibit their own biases (e.g., demographic disparities in feature importance)
  - **Spurious correlation insensitivity**: Explanations may fail to highlight when models rely on irrelevant features
  - **Over-reliance patterns**: Users accepting AI recommendations without critical evaluation, especially with "placebic" explanations
  - **Misaligned saliency**: Highlighted regions not matching clinically relevant anatomy

- First 3 experiments:
  1. **Baseline calibration study**: Measure clinician accuracy with and without post-hoc explanations on a held-out test set; quantify discrimination improvement (correct vs. incorrect AI outputs).
  2. **Automation bias probe**: Compare acceptance rates for AI recommendations with genuine vs. placebo explanations; assess whether explanations increase uncritical acceptance.
  3. **Explanation bias audit**: Run fairness metrics on post-hoc feature attributions across demographic subgroups to detect systematic disparities in explanation quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific conditions determine whether post-hoc explanations improve or hinder clinician-AI team performance?
- Basis in paper: [explicit] The authors state, "More research is needed to better understand the conditions under which post-hoc explanations improve clinician-AI performance," noting that studies show mixed results.
- Why unresolved: Conflicting empirical evidence exists; while some studies show performance gains (e.g., lung lesion identification), others show no improvement or increased automation bias.
- What evidence would resolve it: Comparative user studies that isolate variables such as clinical task type, explanation format, and clinician expertise to identify factors that predict performance success.

### Open Question 2
- Question: How effective are "cognitive forcing" and "frictional AI" techniques at mitigating automation bias without compromising clinical workflow efficiency?
- Basis in paper: [inferred] The paper suggests these interventions can minimize overreliance, but does not provide empirical validation of their success in real-world medical environments.
- Why unresolved: While theoretically sound, these methods introduce delays or complexity (e.g., waiting 30 seconds) that may face resistance or unintended consequences in high-pressure clinical settings.
- What evidence would resolve it: Empirical trials in clinical simulations measuring overreliance rates and decision latency in systems with and without these friction-based interventions.

### Open Question 3
- Question: Can combining post-hoc explanations with "institutional explanations" reliably assist clinicians in justifying AI-informed decisions to patients?
- Basis in paper: [inferred] The paper argues this combination can bolster justificatory power, but acknowledges the distinction between explaining a model and justifying a decision remains a challenge.
- Why unresolved: It is unclear if providing meta-data about design and reliability effectively bridges the gap between technical explanation and ethical justification in practice.
- What evidence would resolve it: Qualitative studies analyzing the quality and coherence of clinician justifications when provided with both technical (saliency maps) and institutional (reliability metrics) information.

## Limitations
- Relies heavily on a small number of empirical studies to support claims about post-hoc explanation benefits
- Behavioral effects on clinical decision-making remain understudied and may vary across contexts
- Potential for automation bias amplification when explanations are presented
- Effectiveness depends on users' domain expertise for meaningful interpretation

## Confidence
- Mechanism 1 (accuracy improvement via discrimination): Medium confidence - supported by single empirical study with specific conditions
- Mechanism 2 (predictive understanding): Low confidence - based on limited literature and untested in medical contexts
- Mechanism 3 (combined justification support): Low confidence - largely theoretical with minimal empirical validation

## Next Checks
1. Conduct systematic review of all empirical studies on post-hoc explanation effectiveness in medical AI, not just supportive cases
2. Test automation bias empirically: compare acceptance rates for AI recommendations with genuine vs. placebo explanations in clinical workflows
3. Evaluate explanation method fairness: assess whether saliency maps and other post-hoc methods exhibit demographic disparities in feature importance attribution