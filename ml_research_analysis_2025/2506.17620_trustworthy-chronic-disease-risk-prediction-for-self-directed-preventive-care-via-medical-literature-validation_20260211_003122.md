---
ver: rpa2
title: Trustworthy Chronic Disease Risk Prediction For Self-Directed Preventive Care
  via Medical Literature Validation
arxiv_id: '2506.17620'
source_url: https://arxiv.org/abs/2506.17620
tags:
- chronic
- disease
- learning
- risk
- diseases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops trustworthy machine learning models for chronic
  disease risk prediction using only non-medical features. The key idea is to use
  deep learning with SHAP-based explainability, then validate the most influential
  features identified by the model against established medical literature.
---

# Trustworthy Chronic Disease Risk Prediction For Self-Directed Preventive Care via Medical Literature Validation

## Quick Facts
- arXiv ID: 2506.17620
- Source URL: https://arxiv.org/abs/2506.17620
- Authors: Minh Le; Khoi Ton
- Reference count: 40
- Primary result: Machine learning models predict 13 chronic diseases using only non-medical features with 65-75% accuracy; SHAP-identified influential features align with medical literature

## Executive Summary
This paper develops trustworthy machine learning models for chronic disease risk prediction using only non-medical features. The key innovation is validating SHAP-based explainability against established medical literature to provide evidence of model trustworthiness for self-directed preventive care. Using the 2023 BRFSS dataset, the models achieve accuracy and recall values between 65% and 75%, comparable to state-of-the-art approaches that use clinical data. Critically, the paper demonstrates that the model's most influential features align with medical literature across all 13 diseases, providing strong evidence of the models' trustworthiness for lay users making health decisions.

## Method Summary
The approach uses deep learning (ResNet architecture) to predict 13 chronic diseases from 38 non-medical features including demographics, lifestyle factors, and healthcare access indicators. Models are trained with weighted cross-entropy loss to handle class imbalance. SHAP (Shapley Additive Explanations) is applied to identify the most influential features for each disease prediction. These top-3 features per disease are then validated against established medical literature to assess whether the model has learned medically plausible relationships. The methodology is tested on the 2023 BRFSS dataset with 154,475 samples after removing missing values.

## Key Results
- Deep learning models achieve 65-75% accuracy and recall across 13 chronic diseases using only non-medical features
- SHAP-identified top features align with medical literature for all 13 diseases, providing evidence of trustworthy predictions
- Models show minimal overfitting with test loss only negligibly larger than train loss
- Employment and marital status emerge as strong predictors, interpreted as age proxies rather than direct causes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** SHAP-based feature importance, when validated against medical literature, provides evidence of model trustworthiness for lay users.
- **Mechanism:** SHAP quantifies each input feature's contribution to predictions via expected difference when that feature is varied. The top influential features are then compared to established medical literature—if they align (e.g., weight for diabetes, smoking for chronic bronchitis), this consistency suggests the model has learned medically plausible relationships rather than spurious correlations.
- **Core assumption:** Alignment between model-identified influential features and medical literature causality implies the model's internal reasoning is trustworthy for predictions on new data.
- **Evidence anchors:**
  - [abstract] "we use SHAP-based explainability to identify the most influential model features and validate them against established medical literature"
  - [section IV.C] "We find that the medical literature agrees that all of the most influential factors of the machine learning models... are indeed significant causes of their respective chronic diseases"
  - [corpus] Related work on explainable disease surveillance (arXiv:2501.15969) uses explainability for clinical insight but does not validate against literature—this paper's novelty.
- **Break condition:** If SHAP identifies a top predictor with no medical literature support, or if the model achieves alignment via proxy features that don't generalize (e.g., "employment" proxies age but fails for non-standard employment patterns), trustworthiness claims weaken.

### Mechanism 2
- **Claim:** Non-medical features alone can achieve predictive performance comparable to models using clinical test data for chronic disease risk assessment.
- **Mechanism:** Lifestyle and personal factors (smoking, exercise, weight, age-proxies like employment status) capture upstream risk signals that manifest before clinical biomarkers become abnormal. These 38 features aggregate across multiple risk dimensions, compensating for lack of single high-signal medical tests.
- **Core assumption:** Chronic diseases have sufficiently long prodromal phases where behavioral/demographic signals are detectable before clinical diagnosis.
- **Evidence anchors:**
  - [abstract] "deep learning models that predict the risk of developing 13 chronic diseases using only personal and lifestyle factors"
  - [section IV.A] "most models have accuracy and recall values between 65% - 75%. This is comparable to other similar research that uses datasets without medical information"
  - [corpus] "Medical Test-free Disease Detection Based on Big Data" (arXiv:2512.07856) explores similar non-invasive detection—suggests emerging research direction, not yet proven generalizable.
- **Break condition:** Performance degrades significantly for diseases where early-stage risk is primarily biomarker-driven (e.g., certain cancers detected via blood markers before lifestyle signals emerge).

### Mechanism 3
- **Claim:** ResNet architecture with weighted cross-entropy loss enables stable training on imbalanced healthcare survey data.
- **Mechanism:** Residual blocks mitigate vanishing gradients in deeper networks. Weighted cross-entropy (weights inversely proportional to class frequency) prevents the model from defaulting to majority-class predictions when disease prevalence is low (e.g., stroke at ~2% in dataset).
- **Core assumption:** The survey-derived features have meaningful nonlinear interactions that deeper networks can exploit better than shallow models.
- **Evidence anchors:**
  - [section III.A] "We choose a ResNet architecture... to address the issue of vanishing gradients"
  - [section III.A] "weighted cross-entropy loss during model training, where the weight for each class is inversely proportional to its frequency"
  - [section IV.A] "test loss is only negligibly larger than the train loss. This shows that our models do not overfit"
  - [corpus] No direct architectural comparison in neighbors—this is a design choice, not a proven mechanism.
- **Break condition:** If simpler architectures (e.g., gradient boosting) achieve equivalent or better performance with less computational cost, the ResNet choice becomes unjustified complexity.

## Foundational Learning

- **Concept: SHAP (Shapley Additive Explanations)**
  - **Why needed here:** This is the core explainability mechanism. SHAP assigns each feature an importance value for individual predictions by computing marginal contributions across all possible feature coalitions.
  - **Quick check question:** Given a model where Feature A has SHAP value +0.3 and Feature B has SHAP value -0.1 for a specific prediction, which feature increased the predicted risk more?

- **Concept: Class Imbalance Handling (Weighted Loss)**
  - **Why needed here:** Chronic diseases have low prevalence in general populations. Without reweighting, models optimize for accuracy by predicting the majority class (no disease) almost always.
  - **Quick check question:** If a disease occurs in 5% of samples, what weight should the positive class receive relative to the negative class to balance the loss contribution?

- **Concept: Proxy Features and Confounding**
  - **Why needed here:** Employment status and marital status are identified as top predictors, but the paper reveals they proxy for age. Understanding this distinction is critical for interpreting model behavior correctly.
  - **Quick check question:** If a model uses "zip code" as a top predictor for disease risk, what confounding variable might it actually be capturing?

## Architecture Onboarding

- **Component map:**
  Input (38 features) → Residual Block 1 → Residual Block 2 → Residual Block N → Linear Layer → Softmax → Risk Score (0-1)

- **Critical path:**
  1. Data cleaning: Handle BRFSS special codes (88 = zero days, 99 = refused, empty ≠ missing in skip patterns)
  2. Feature preprocessing: Convert mixed units to single scale (e.g., "times per week" and "times per month" to common unit)
  3. Model training with weighted cross-entropy (class weights = 1/frequency)
  4. SHAP explanation on 500 random samples with 100 k-means background points
  5. Literature validation: Match top-3 SHAP features per disease to published risk factors

- **Design tradeoffs:**
  - **SHAP computational cost:** Kernel Explainer is model-agnostic but slow. TreeSHAP would be faster but requires tree-based models. Paper uses k-means summarization (100 background points) to make computation tractable.
  - **ResNet vs. simpler models:** Not benchmarked against XGBoost or logistic regression. Deeper model may be unnecessary for 38-tabular features.
  - **Self-directed care vs. clinical accuracy:** 65-75% accuracy is acceptable for personal risk awareness but insufficient for clinical diagnosis. This is an intentional tradeoff for accessibility.

- **Failure signatures:**
  - Model predicts near-constant risk (~50%) across all inputs: Likely class imbalance not properly weighted, or learning rate too high causing divergence.
  - SHAP values near-zero for all features: Model may be collapsed or Explainer misconfigured.
  - Top SHAP features contradict medical literature for multiple diseases: Data quality issue or model learned spurious correlations—investigate feature encoding and missing value handling.
  - Large train-test loss gap: Overfitting; reduce model depth or add dropout.

- **First 3 experiments:**
  1. **Baseline comparison:** Train logistic regression and XGBoost on same data. Compare accuracy, recall, and SHAP feature alignment. If simpler models perform equally well, ResNet is overkill.
  2. **Ablation on SHAP sampling:** Test explanation stability by varying SHAP sample size (100, 500, 1000 points) and background data size (50, 100, 200). Check if top-3 features remain consistent.
  3. **Proxy feature intervention:** Remove employment and marital status (age proxies) and retrain. Observe whether the model learns direct age-related patterns from remaining features or if performance drops significantly—this reveals how much the model depends on proxies vs. genuine predictors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would the model's explanations and literature validation hold when applied to datasets from countries outside the United States?
- Basis in paper: [explicit] "Future work can apply this method to datasets from other countries and explore whether the model explanations can still be validated with medical literature."
- Why unresolved: The study exclusively uses the 2023 BRFSS dataset, which is limited to U.S. residents; population characteristics, healthcare access patterns, and lifestyle factors may differ substantially across countries.
- What evidence would resolve it: Replicating the methodology on equivalent health survey datasets from other countries (e.g., European Health Interview Survey, China Kadoorie Biobank) and confirming SHAP-derived influential features align with medical literature.

### Open Question 2
- Question: How can self-directed predictive tools avoid perpetuating existing demographic biases in chronic disease risk assessment?
- Basis in paper: [explicit] The authors note that "a model's alignment with medical literature doesn't guarantee trustworthiness, as it might reinforce existing chronic disease risk patterns, potentially causing over- or under-caution in health decisions," citing gender bias in heart disease recognition as an example.
- Why unresolved: Validating against medical literature may inadvertently reinforce systemic biases present in historical medical research and clinical practice; the paper does not propose mechanisms to detect or mitigate such bias propagation.
- What evidence would resolve it: Studies comparing model-influenced health decisions across demographic groups, or developing bias-aware validation frameworks that distinguish between legitimate risk factors and artifacts of historical healthcare disparities.

### Open Question 3
- Question: Would incorporating additional trustworthiness dimensions—such as robustness to data distribution shifts and adversarial attacks—complement or conflict with literature-validated explainability?
- Basis in paper: [explicit] "Future work can explore these different approaches to develop more trustworthy models," specifically mentioning "ensuring robustness to data shifts and to adversarial attacks" as alternative trustworthiness approaches.
- Why unresolved: The paper focuses solely on explainability-based trustworthiness; it remains unknown whether models optimized for adversarial robustness or distributional stability would produce different influential features that may or may not align with medical literature.
- What evidence would resolve it: Comparative studies training models with adversarial training or domain adaptation techniques, then evaluating whether SHAP explanations remain consistent and medically valid.

### Open Question 4
- Question: To what extent do the proxy features (employment status, marital status) capture age-related risk versus independent socioeconomic or occupational health effects?
- Basis in paper: [inferred] The authors interpret employment and marital status as age indicators (e.g., retirees show higher disease prevalence) but acknowledge these could also reflect "inability to work" as a consequence of disease; the analysis does not disentangle these confounded relationships.
- Why unresolved: Without direct age adjustment or causal analysis, it remains unclear whether the model learns genuine age-independent socioeconomic risk factors or merely proxies for age; this affects both model interpretability and the validity of literature comparisons.
- What evidence would resolve it: Ablation studies including/excluding direct age information, or causal mediation analysis quantifying how much employment/marital status effects operate through age versus independent pathways.

## Limitations
- Limited scope of non-medical features excludes clinical biomarkers that may be critical for early detection of certain diseases
- Literature validation completeness only checks top-3 SHAP features without assessing full feature set or temporal stability
- Temporal validity relies on cross-sectional survey data without validation across different years or populations

## Confidence
- SHAP-literature alignment as trustworthiness evidence: Medium confidence
- Non-medical features achieving comparable performance: Medium confidence  
- ResNet architecture necessity: Low confidence

## Next Checks
1. **Cross-year validation:** Retrain models on 2022 BRFSS data and compare SHAP feature importance rankings to assess temporal stability
2. **Proxy feature ablation:** Remove employment and marital status from features and retrain to test whether model learns direct age patterns
3. **Clinical biomarker comparison:** Obtain dataset with both non-medical features and basic clinical biomarkers to directly test performance claims