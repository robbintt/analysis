---
ver: rpa2
title: Reasoning Models Can be Accurately Pruned Via Chain-of-Thought Reconstruction
arxiv_id: '2509.12464'
source_url: https://arxiv.org/abs/2509.12464
tags:
- reasoning
- calibration
- pruning
- accuracy
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of compressing reasoning language
  models like DeepSeek-R1, which generate long chain-of-thought traces that make them
  costly to deploy at scale. Standard pruning techniques that focus on input reconstruction
  fail for these models because reasoning tasks are dominated by self-generated decoding
  tokens rather than prompts.
---

# Reasoning Models Can be Accurately Pruned Via Chain-of-Thought Reconstruction
## Quick Facts
- arXiv ID: 2509.12464
- Source URL: https://arxiv.org/abs/2509.12464
- Reference count: 10
- Primary result: RAC pruning achieves 66.4% accuracy vs 35.6% with C4 calibration on MATH-500 at 50% sparsity

## Executive Summary
This paper addresses the challenge of compressing reasoning language models like DeepSeek-R1, which generate long chain-of-thought traces that make them costly to deploy at scale. Standard pruning techniques that focus on input reconstruction fail for these models because reasoning tasks are dominated by self-generated decoding tokens rather than prompts. The authors propose Reasoning-Aware Compression (RAC), which augments calibration data with on-policy chain-of-thought activations collected from the dense model during generation. RAC integrates seamlessly into existing pruning workflows like SparseGPT. Experiments on MATH-500 and LiveCodeBench show that RAC significantly outperforms standard C4 calibration and task-specific prompt calibration, especially at higher sparsity levels.

## Method Summary
The key insight is that reasoning models spend most of their computational budget on self-generated tokens during chain-of-thought generation, making prompt-based calibration ineffective. RAC addresses this by generating chain-of-thought traces from the dense model and using these as augmented calibration data for the pruning process. The method works by having the dense model solve reasoning problems while recording its internal activations, then using this on-policy data to inform which weights are most important for reasoning performance. This approach is integrated into existing sparse training pipelines like SparseGPT, requiring minimal modification to current workflows while delivering significant accuracy improvements at high sparsity levels.

## Key Results
- At 50% sparsity on 1.5B model, RAC achieves 66.4% accuracy versus 35.6% with C4 calibration on MATH-500
- RAC significantly reduces decoding errors on held-out test problems compared to standard calibration methods
- Combines effectively with structured pruning and quantization for improved runtime efficiency

## Why This Works (Mechanism)
Reasoning models like DeepSeek-R1 generate extensive chain-of-thought traces during problem solving, with the majority of tokens being self-generated rather than prompt-derived. Standard pruning calibration methods that rely on reconstruction of natural language data (like C4) fail because they don't capture the importance of weights used during the self-generated reasoning process. RAC works by collecting on-policy data from the dense model's actual reasoning traces, which provides a more accurate signal for identifying which weights are critical for reasoning performance. This approach recognizes that the computational cost in reasoning models comes primarily from decoding these long self-generated sequences, making traditional calibration approaches fundamentally misaligned with the model's actual usage patterns.

## Foundational Learning
**Chain-of-Thought Reasoning**: The process where models generate intermediate reasoning steps before producing final answers, crucial for complex problem-solving but computationally expensive.

*Why needed*: Understanding that reasoning models spend most computational resources on self-generated tokens rather than prompt processing.
*Quick check*: Verify that reasoning traces contain significantly more tokens than prompts across multiple reasoning benchmarks.

**Weight Importance Calibration**: The process of determining which neural network weights are most critical for preserving task performance during pruning.

*Why needed*: Standard calibration methods fail for reasoning models because they don't account for self-generated decoding tokens.
*Quick check*: Compare weight importance scores from different calibration datasets to identify misalignment with actual reasoning usage.

**On-Policy Data Collection**: Gathering training or calibration data by running the target model itself, rather than using pre-collected datasets.

*Why needed*: Provides more accurate importance signals for reasoning tasks where self-generated tokens dominate.
*Quick check*: Measure correlation between dense model's self-generated tokens and pruning importance scores.

**Sparse Neural Network Training**: Techniques for reducing model size by removing connections while maintaining performance.

*Why needed*: Enables deployment of large reasoning models on resource-constrained hardware.
*Quick check*: Verify that pruned models maintain accuracy within acceptable bounds at target sparsity levels.

## Architecture Onboarding
**Component Map**: Dense Reasoning Model -> Chain-of-Thought Generation -> Activation Recording -> Calibration Dataset -> SparseGPT Pruning -> Pruned Model

**Critical Path**: The most important sequence is Dense Model generation of reasoning traces → Activation recording → Calibration data creation → Weight importance determination → Sparse pruning, as this captures the self-generated token importance that traditional methods miss.

**Design Tradeoffs**: RAC requires access to the dense model for data generation (increased upfront cost) but enables much higher sparsity levels with better accuracy preservation. The tradeoff favors deployment scenarios where inference cost savings outweigh initial data collection overhead.

**Failure Signatures**: Standard calibration methods will show rapid accuracy degradation at moderate sparsity levels (30-50%) for reasoning tasks, while RAC maintains performance. Models pruned without chain-of-thought data will fail on complex reasoning problems that require extensive intermediate reasoning.

**First Experiments**:
1. Compare accuracy at 40% sparsity between RAC and C4 calibration on a held-out reasoning subset
2. Measure chain-of-thought length distribution in dense vs pruned models to verify reasoning capability preservation
3. Profile inference latency improvements from combined pruning and quantization on target hardware

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation to DeepSeek-R1 model family and small model sizes (1.5B, 7B parameters)
- Requires access to dense model activations for calibration data generation
- Performance gains may not generalize to larger models or different reasoning architectures
- Runtime efficiency improvements claimed but not thoroughly validated with comprehensive measurements

## Confidence
- RAC significantly outperforms standard calibration methods: **High confidence** (consistent experimental improvements across tasks and sparsity levels)
- RAC reduces decoding errors on held-out test problems: **Medium confidence** (focused on specific benchmarks, may not generalize to all reasoning task types)
- RAC improves runtime efficiency with structured pruning and quantization: **Low confidence** (claimed but not thoroughly validated with latency measurements)

## Next Checks
1. Test RAC on additional reasoning model families beyond DeepSeek-R1, including both open-source and proprietary models, to assess generalizability across different reasoning architectures.

2. Evaluate performance degradation when RAC is applied to larger model sizes (14B-70B parameters) where the ratio of self-generated tokens to prompt tokens may scale differently.

3. Conduct ablation studies comparing RAC against task-specific fine-tuning approaches that generate synthetic calibration data without requiring dense model activations, to quantify the cost-benefit tradeoff of RAC's data collection requirements.