---
ver: rpa2
title: Is ChatGPT-5 Ready for Mammogram VQA?
arxiv_id: '2508.11628'
source_url: https://arxiv.org/abs/2508.11628
tags:
- gpt-5
- malignancy
- performance
- breast
- bi-rads
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluated the zero-shot performance of ChatGPT-5 and
  its variants on mammogram visual question answering (VQA) tasks using four public
  datasets (EMBED, InBreast, CMMD, CBIS-DDSM). GPT-5 outperformed other GPT variants
  in most tasks but still lagged behind human experts and specialized fine-tuned models.
---

# Is ChatGPT-5 Ready for Mammogram VQA?

## Quick Facts
- arXiv ID: 2508.11628
- Source URL: https://arxiv.org/abs/2508.11628
- Reference count: 25
- Key outcome: GPT-5 outperforms other GPT variants on mammogram VQA tasks but remains insufficient for clinical imaging without domain adaptation

## Executive Summary
This study evaluated ChatGPT-5 and its variants for zero-shot mammogram visual question answering across four public datasets. While GPT-5 showed superior performance compared to other GPT models, achieving up to 69.3% BI-RADS accuracy on CBIS-DDSM, it significantly underperformed human experts with sensitivity of 63.5% versus 86.9% and specificity of 52.3% versus 88.9%. The results demonstrate that while GPT-5 has potential for mammogram analysis, it requires targeted domain adaptation before clinical deployment due to inconsistent generalization across datasets and unacceptably high false positive and false negative rates.

## Method Summary
The study conducted zero-shot evaluation of ChatGPT-5 and its variants on mammogram visual question answering tasks using four public datasets: EMBED, InBreast, CMMD, and CBIS-DDSM. Tasks included mass classification, malignancy detection, BI-RADS assessment, and abnormality detection. Performance was compared against human expert benchmarks where available, and results were analyzed across different imaging modalities and populations to assess generalization capabilities.

## Key Results
- GPT-5 achieved up to 64.5% accuracy for mass classification and 52.8% for malignancy detection on EMBED dataset
- On CBIS-DDSM, GPT-5 reached 69.3% BI-RADS accuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy
- Compared to human experts, GPT-5 showed significantly lower sensitivity (63.5% vs. 86.9%) and specificity (52.3% vs. 88.9%)

## Why This Works (Mechanism)
Not applicable - the study focuses on zero-shot evaluation rather than explaining underlying mechanisms.

## Foundational Learning
- Visual Question Answering (VQA) - understanding how AI systems interpret images and answer questions about them; why needed to evaluate GPT-5's multimodal capabilities; quick check: review VQA benchmark datasets and task formulations
- Mammogram interpretation - knowledge of breast imaging standards, BI-RADS classification, and clinical significance of findings; why needed to contextualize performance metrics; quick check: examine BI-RADS lexicon and clinical guidelines
- Zero-shot learning - understanding AI model performance without task-specific training; why needed to assess GPT-5's general capabilities; quick check: review zero-shot learning literature and evaluation methods

## Architecture Onboarding
Component map: Image input -> Multimodal encoder -> Text generation model -> Output response
Critical path: Image processing pipeline → Feature extraction → Question understanding → Answer generation → Clinical interpretation
Design tradeoffs: Zero-shot evaluation vs. fine-tuning (flexibility vs. performance), general knowledge vs. domain expertise (broad vs. deep understanding)
Failure signatures: Inconsistent performance across datasets, low sensitivity/specificity compared to humans, potential reasoning quality issues
First experiments: 1) Compare zero-shot vs. fine-tuned GPT-5 performance on same datasets, 2) Analyze GPT-5's reasoning explanations for correct/incorrect answers, 3) Test GPT-5 on edge cases and rare findings in mammography

## Open Questions the Paper Calls Out
None

## Limitations
- Zero-shot evaluation inherently constrains performance compared to fine-tuned models
- Performance variability across datasets (36.9% vs. 69.3% BI-RADS accuracy) indicates inconsistent generalization
- Study focuses on accuracy metrics without examining reasoning quality or clinical decision support capabilities

## Confidence
- GPT-5 outperforms other GPT variants on mammogram VQA tasks: High
- GPT-5 remains insufficient for clinical imaging without domain adaptation: High
- Performance variability across datasets indicates inconsistent generalization: Medium
- Zero-shot performance represents true clinical capability limits: Low

## Next Checks
1. Conduct a head-to-head comparison between zero-shot GPT-5 and domain-specific models fine-tuned on mammography data to establish the performance gap and determine optimal adaptation strategies.
2. Perform bias and fairness analysis across demographic subgroups to identify potential disparities in GPT-5's mammogram interpretation accuracy.
3. Evaluate GPT-5's clinical reasoning and explanation quality through radiologist review of responses, not just accuracy metrics, to assess its utility as a decision support tool.