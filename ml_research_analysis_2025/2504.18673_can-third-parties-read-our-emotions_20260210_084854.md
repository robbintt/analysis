---
ver: rpa2
title: Can Third-parties Read Our Emotions?
arxiv_id: '2504.18673'
source_url: https://arxiv.org/abs/2504.18673
tags:
- annotators
- emotion
- third-party
- annotations
- first-party
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study directly compares third-party annotations (human and
  LLM-based) with first-party (author-provided) emotion labels, finding that both
  types of third-party annotations poorly capture authors' intended emotions. LLMs
  consistently outperform human annotators across most emotions, but still achieve
  only low to fair alignment scores (Cohen's kappa 0-0.45; F1 scores 0.2-0.6).
---

# Can Third-parties Read Our Emotions?

## Quick Facts
- arXiv ID: 2504.18673
- Source URL: https://arxiv.org/abs/2504.18673
- Authors: Jiayi Li; Yingfan Zhou; Pranav Narayanan Venkit; Halima Binte Islam; Sneha Arya; Shomir Wilson; Sarah Rajtmajer
- Reference count: 40
- Primary result: Third-party annotations (human and LLM-based) poorly capture authors' intended emotions, with demographic similarity and demographic prompting providing marginal improvements.

## Executive Summary
This study directly compares third-party annotations (human and LLM-based) with first-party (author-provided) emotion labels, finding that both types of third-party annotations poorly capture authors' intended emotions. LLMs consistently outperform human annotators across most emotions, but still achieve only low to fair alignment scores (Cohen's kappa 0-0.45; F1 scores 0.2-0.6). Human annotators who share demographic traits (age, gender, race) with authors show significantly better performance than those who don't, with in-group annotators achieving median F1 scores of 0.29 versus 0.00 for out-group. Including first-party demographic information in LLM prompts provides a marginal but statistically significant improvement in performance. The research reveals that misalignment stems from both a lack of explicit linguistic cues and subjective interpretation differences, with emotions like joy and love being more consistently recognized than nuanced emotions like relief or realization.

## Method Summary
The study recruited 123 participants to submit social media posts and self-label emotions using a 28-category taxonomy (27 emotions + neutral). For each post, 6 human annotators (3 in-group matching all 3 demographics, 3 out-group differing on â‰¥2 traits) provided multi-label annotations. Five LLMs (GPT-4 Turbo, GPT-4o, Gemini 1.5 Pro/Flash, Claude 3.5 Sonnet) were also used to annotate posts, with demographic prompting as an experimental condition. Third-party labels were aggregated via majority voting and compared to first-party labels using Cohen's kappa, F1, precision, and recall. Statistical tests (Wilcoxon signed-rank, mixed linear models) evaluated performance differences between in-group/out-group annotators and human/LLM performance.

## Key Results
- LLMs outperform human annotators on emotion recognition (F1, recall, kappa) but achieve only low to fair alignment scores (kappa 0-0.45; F1 0.2-0.6)
- Demographic similarity between annotators and authors significantly improves performance (in-group median F1 0.29 vs out-group 0.00, p=0.001)
- Including first-party demographic information in LLM prompts provides statistically significant but practically minimal improvement
- Emotions like joy and love are more consistently recognized than nuanced emotions like relief or realization
- Misalignment stems from both lack of explicit linguistic cues and subjective interpretation differences

## Why This Works (Mechanism)

### Mechanism 1: Demographic similarity improves annotation alignment
- Shared demographic traits (age, gender, race) correlate with shared cultural and experiential reference frames, reducing subjective interpretation gaps when inferring private states from text
- Assumes demographic similarity proxies for shared interpretive frameworks; may not hold across all contexts
- Evidence: In-group annotators achieve significantly higher recall (Median_in-group = 0.25, Median_out-group = 0.00, p = 0.001)

### Mechanism 2: Explicit linguistic cues enable higher accuracy
- Third-party annotators rely on surface-level lexical signals; unambiguous expressions (e.g., "thank you" for gratitude) yield straightforward inference
- Assumes emotional expression maps to observable linguistic markers
- Evidence: Posts with near-perfect F1 scores contained explicit and unambiguous emotional language

### Mechanism 3: LLMs outperform humans but remain fundamentally limited
- LLMs aggregate patterns from large-scale training data, providing more consistent predictions across instances
- Lacks access to author-internal context and defaults to plausible but not necessarily correct interpretations
- Evidence: LLMs perform significantly better in terms of F1, recall, and Cohen's kappa, but their advantage does not extend to precision

## Foundational Learning

**Private states** (internal experiences like emotions, opinions, speculations that are not directly observable)
- Why needed: This paper's central question is whether third parties can infer private states from text
- Quick check: Can you name two NLP tasks that attempt to infer private states? (Answer: emotion classification, sentiment analysis, sarcasm detection, stance detection)

**Cohen's kappa** (inter-rater agreement metric that accounts for chance agreement)
- Why needed: The paper uses kappa to quantify alignment between third-party annotations and first-party labels
- Quick check: If two annotators agree 80% of the time on a binary task where each label appears 50% of the time, is kappa necessarily 0.8? (Answer: No; kappa accounts for expected chance agreement)

**Demographic prompting** (including author demographic information in LLM prompts to guide predictions)
- Why needed: RQ3 tests whether this technique improves LLM alignment
- Quick check: If demographic prompting improves F1 from 0.40 to 0.40 with p=0.0095, what does this suggest about practical utility? (Answer: Statistical significance does not imply practical significance; the effect size is small)

## Architecture Onboarding

**Component map**: First-party data collection -> Third-party annotation layer (human + LLM) -> Aggregation -> Evaluation

**Critical path**:
1. Define emotion taxonomy (27 emotions + neutral) with clear definitions
2. Ensure demographic metadata collection for both authors and annotators
3. Assign annotators to in-group vs. out-group conditions systematically
4. Use identical prompts/instructions for human and LLM annotators
5. Compute agreement metrics per emotion and aggregated

**Design tradeoffs**:
- Fine-grained (28 categories) vs. coarse-grained (7 groups): Fine-grained captures nuance but yields lower agreement
- Number of annotators per post: More annotators improve reliability but increase cost (6 chosen here)
- LLM ensemble vs. single model: Ensemble (5 LLMs) reduces variance but introduces aggregation complexity

**Failure signatures**:
- Cohen's kappa near 0 indicates third-party annotations perform no better than random guessing
- Large gaps between in-group and out-group F1 suggest demographic mismatch is degrading annotation quality
- Near-zero neutral labels from LLMs (0.1% vs. 4%+ for humans) suggests calibration bias

**First 3 experiments**:
1. Replicate demographic similarity analysis on professional reviews vs. social media
2. Systematically add/remove emotion words from posts and measure accuracy changes
3. Test alternative prompting strategies beyond demographics to determine if any yield practically significant LLM improvement

## Open Questions the Paper Calls Out

**Open Question 1**: How can first-party feedback and explanations be systematically integrated into annotation protocols to improve alignment between third-party labels and authors' actual private states?
- Basis: The Conclusion states future research should "explore methods, such as incorporating first-party feedback and explanation, to achieve a more truthful representation of the emotions actually expressed by authors."
- Why unresolved: The current study focused on quantifying the misalignment gap rather than developing interactive annotation frameworks
- Evidence needed: Experiments comparing standard annotation workflows against those augmented with author-provided rationales

**Open Question 2**: Do the limitations of third-party annotations and the "in-group" advantage generalize to non-English languages and cultures outside the United States?
- Basis: The Limitations section notes that findings "may not generalize to broader populations, cultures, or languages."
- Why unresolved: The study restricted recruitment to US participants using English social media posts
- Evidence needed: Replication with participants from distinct cultural backgrounds and different languages

**Open Question 3**: How do granular, intersectional identity traits influence annotation performance, which remains unclear due to lack of statistical power within specific demographic subgroups?
- Basis: The authors state they "lack sufficient data points within each demographic subgroup to conduct statistically robust analyses"
- Why unresolved: The study analyzed broad categories but could not determine if specific intersectional identities significantly alter accuracy
- Evidence needed: Large-scale data collection ensuring statistically significant sample sizes for specific intersectional subgroups

## Limitations

- Small sample size (729 posts from 123 participants) limits generalizability to broader populations and contexts
- Demographic categories (age brackets, gender binary, three racial groups) may oversimplify complex identity intersections
- Assumption that first-party labels represent "ground truth" emotions may be problematic as authors may be uncertain about their emotional states
- LLM evaluation uses screenshot-based posts, introducing variability from processing visual content alongside text

## Confidence

**High Confidence**: LLMs consistently outperform human annotators on emotion recognition tasks but achieve only low to fair alignment scores (Cohen's kappa 0-0.45). This finding is well-supported by multiple metrics and statistical tests across various emotions.

**Medium Confidence**: Demographic similarity between third-party annotators and first-party authors improves annotation performance. While statistically significant differences are demonstrated, the practical significance is questionable given the small effect sizes.

**Low Confidence**: The claim that misalignment stems from both lack of explicit linguistic cues and subjective interpretation differences is based on qualitative analysis of a subset of posts rather than systematic quantitative validation across the full dataset.

## Next Checks

1. **Cross-domain replication**: Replicate the demographic similarity analysis on a different domain (e.g., professional reviews or forum discussions) to test whether in-group advantages generalize beyond social media contexts, controlling for both domain effects and demographic matching.

2. **Linguistic cue manipulation study**: Systematically manipulate emotional posts by adding or removing explicit emotion words, then measure how annotation accuracy changes. This would quantify the relationship between linguistic cue strength and third-party annotation reliability.

3. **Prompt engineering comparison**: Test alternative prompting strategies beyond demographic information (such as context provision, author intent clarification, or multi-turn dialogue) to determine whether any approach yields practically significant improvements in LLM alignment beyond the marginal gains reported.