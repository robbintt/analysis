---
ver: rpa2
title: Geometric Flow Models over Neural Network Weights
arxiv_id: '2504.03710'
source_url: https://arxiv.org/abs/2504.03710
tags:
- flow
- neural
- weights
- arxiv
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of designing effective generative
  models in weight-space by incorporating the symmetries of neural network weights,
  which have been largely ignored in previous work. The authors propose three different
  flow models (Euclidean, Normalized, and Geometric) that handle the scaling symmetries
  of neural networks in varying ways, while using graph neural networks to process
  the weights.
---

# Geometric Flow Models over Neural Network Weights

## Quick Facts
- arXiv ID: 2504.03710
- Source URL: https://arxiv.org/abs/2504.03710
- Reference count: 0
- Key result: Geometric flows learn straighter transport paths on weight manifolds and generate functional neural network weights that generalize across architectures

## Executive Summary
This paper addresses the problem of generative modeling in neural network weight space by incorporating the scaling symmetries inherent to neural network architectures. The authors propose three flow models—Euclidean, Normalized, and Geometric—that handle these symmetries differently while using graph neural networks to process weights. Through flow matching training and extensive evaluation on classification tasks, they demonstrate that their flows can generate high-quality samples competitive with optimized weights and generalize to different architectures. The Geometric flow, which models vector fields over the product geometry of normalized weights, achieves the best performance by learning straighter transport paths.

## Method Summary
The method involves training flow models to learn the posterior distribution of neural network weights by predicting target weights from intermediate samples along a probability path. The core approach uses flow matching with a relational transformer architecture that treats weights as graph-structured data. Three variants are proposed: Euclidean flow operates in standard parameter space, Normalized flow removes scaling symmetries through canonicalization, and Geometric flow models the vector field directly on the product manifold of hyperspheres. Training involves collecting weight checkpoints from independently trained networks, aligning them through permutation-invariant methods, and regressing the flow vector field using conditional flow matching objectives.

## Key Results
- Geometric and Normalized flows learn straighter transport paths than Euclidean flows, requiring fewer integration steps for comparable quality
- Generated weights achieve competitive performance with optimized weights on classification tasks (UCI Breast Cancer, MNIST)
- Flows trained on smaller architectures successfully generalize to generate weights for larger unseen architectures
- Performance scales with model size, showing improved quality when trained on larger networks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Removing scaling symmetries reduces effective dimensionality of weight space, enabling more efficient posterior learning
- **Mechanism:** Canonicalization procedure normalizes incoming weights of intermediate neurons to unit length (hypersphere) and inversely scales outgoing weights, projecting unconstrained space onto product manifold
- **Core assumption:** Scaling symmetries are nuisance variances that don't affect function but clutter learning signal
- **Evidence anchors:** Abstract mentions scaling symmetries "hold potential to make weight-space generative modeling more efficient"; Section 6.2.2 describes normalization procedure
- **Break condition:** Fails for activation functions with undefined scaling properties beyond ReLU

### Mechanism 2
- **Claim:** Geometric flow's manifold modeling produces straighter transport paths than Euclidean alternatives
- **Mechanism:** Uses Riemannian Flow Matching with geodesics (exponential/logarithmic maps) instead of linear interpolation on hyperspheres
- **Core assumption:** Optimal transport between neural network states follows geodesics on weight manifold, not Euclidean straight lines
- **Evidence anchors:** Abstract notes Geometric/Normalized flows learn straighter paths; Section 7.2 shows plateau in quality after fewer steps than Euclidean
- **Break condition:** Incorrect manifold geometry specification causes invalid trajectory paths

### Mechanism 3
- **Claim:** GNNs enable permutation symmetry respect and cross-architecture generalization
- **Mechanism:** Represents networks as graphs (nodes=neurons, edges=weights), processes with relational transformer sharing parameters across layers/architectures
- **Core assumption:** Connectivity structure contains sufficient information to disambiguate weights without fixed positional indices
- **Evidence anchors:** Abstract mentions GNNs respecting permutation symmetries; Table 7.4 shows generalization to different architectures
- **Break condition:** Generalization fails for architectures with topological features absent from training set

## Foundational Learning

- **Concept: Flow Matching (Conditional)**
  - **Why needed here:** Core training objective avoiding expensive Jacobian calculations by regressing time-dependent vector field
  - **Quick check question:** Can you explain why we sample $x_t$ on the path between $x_0$ and $x_1$ rather than just predicting the step direction directly?

- **Concept: Re-basin / Linear Mode Connectivity**
  - **Why needed here:** Weight space has equivalent modes (permutations); without alignment, flow attempts to connect functionally identical but spatially distinct points
  - **Quick check question:** Why is the "loss barrier" a critical metric when deciding if two trained networks are aligned?

- **Concept: Riemannian Manifolds (Hyperspheres)**
  - **Why needed here:** Geometric flow variant explicitly restricts weights to sphere surface; requires understanding geodesic computation
  - **Quick check question:** On a sphere, is the straight line (Euclidean chord) connecting two points a valid path for probability flow? Why or why not?

## Architecture Onboarding

- **Component map:** Flattened weights → Graph Construction (Nodes=Neurons, Edges=Weights, Bias) → Node/Edge features projected to $d_E$ dimensions + Time embedding → Relational Transformer (GNN) layers with edge updates → Projects final states back to weight space (velocity prediction)

- **Critical path:**
  1. Train standard NNs on task → Collect weights
  2. Align weights to reference using Re-basin
  3. Canonicalize weights to remove scaling symmetries (if using Geometric/Normalized flows)
  4. Train Relational Transformer to predict $x_1$ from $x_t$
  5. Sample: Initialize from Gaussian prior → Solve ODE (Euler steps) → Denormalize/Un-align (optional) → Evaluate

- **Design tradeoffs:**
  - Euclidean vs. Geometric: Euclidean simpler but needs more integration steps; Geometric complex but learns straighter paths
  - Independent vs. OT coupling: OT results in straighter paths and better stability but adds computational overhead

- **Failure signatures:**
  - High Loss Barriers: Alignment (Re-basin) failure; flow tries to average distinct functional modes
  - Deterministic Collapse: Flow generates low-variance samples; fix by adding noise or checking coupling diversity
  - Scale Explosion: Weights grow unbounded during sampling; fix by enforcing Geometric constraints or scaling output velocity

- **First 3 experiments:**
  1. **Sanity Check (Toy Gaussian):** Train flow to transport $N(0,I) \to N(1,I)$ on small MLP; verify captures variance vs. mean
  2. **Ablation (Geometry):** Train Euclidean vs. Geometric flow on UCI Breast Cancer; plot accuracy vs. ODE steps to confirm Geometric learns straighter paths
  3. **Generalization Check:** Train flow on weights from hidden dim 10; attempt to sample weights for hidden dim 32 without retraining; verify minimal accuracy drop

## Open Questions the Paper Calls Out

- **Question 1:** Can geometric flow framework extend to handle permutation and scaling symmetries of non-MLP architectures like CNNs or Transformers?
  - **Basis:** Authors explicitly state future work involves capturing "different architectural choices such as convolutions... or transformer blocks" inducing different symmetries
  - **Why unresolved:** Current implementation restricted to ReLU MLPs
  - **Evidence needed:** Successful application to CNN/Transformer using appropriate graph construction respecting specific layer symmetries

- **Question 2:** Is it feasible to train weight-space generative models effectively without pre-optimized checkpoint datasets?
  - **Basis:** Authors suggest relaxing "requirement of sample-based training" by utilizing recent work on generative modeling without samples
  - **Why unresolved:** Current methodology requires collecting target weights via independent gradient-based optimization trajectories
  - **Evidence needed:** Energy-based training loop using only task loss gradient achieving comparable posterior approximation

- **Question 3:** Can a single flow model learn to map multiple source distributions to different targets as a weight-space "foundation model"?
  - **Basis:** Authors propose exploring "weight-space 'foundation models'" trained over diverse tasks and adapted via fine-tuning
  - **Why unresolved:** Current flows trained on "single architecture and dataset to push-forward a single source distribution"
  - **Evidence needed:** Unified model trained on heterogeneous weights successfully generating functional weights for unseen tasks

## Limitations
- Heavy reliance on precise weight alignment (Re-basin) as prerequisite for successful flow training
- Performance benefits contingent on assumption that neural network weights reside on well-defined product manifold
- Scalability to very deep or highly complex networks remains unverified

## Confidence

- **High Confidence:** Core mechanism of using GNNs to process weight graphs and respect permutation symmetries (supported by empirical results showing cross-architecture generalization)
- **Medium Confidence:** Claim that removing scaling symmetries reduces effective dimensionality (plausible but lacks direct experimental validation)
- **Medium Confidence:** Assertion that Geometric flows learn "straighter" paths (supported by fewer integration steps but theoretical justification not rigorously proven)

## Next Checks

1. **Ablation Study on Alignment:** Train flows with and without proper Re-basin alignment on same dataset; measure CFM loss and sample quality to isolate impact of alignment quality

2. **Manifold Geometry Validation:** For small MLP, generate large set of aligned weights; compute pairwise distances using Euclidean vs. Riemannian metrics; visualize embedding to confirm weights approximately lie on product manifold of hyperspheres

3. **Activation Function Robustness:** Repeat main experiments using non-ReLU activation function (e.g., tanh); evaluate if Geometric flow's performance degrades significantly, indicating sensitivity to assumed scaling symmetry