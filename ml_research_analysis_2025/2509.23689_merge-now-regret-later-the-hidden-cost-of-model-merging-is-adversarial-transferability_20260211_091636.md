---
ver: rpa2
title: 'Merge Now, Regret Later: The Hidden Cost of Model Merging is Adversarial Transferability'
arxiv_id: '2509.23689'
source_url: https://arxiv.org/abs/2509.23689
tags:
- attack
- attacks
- adversarial
- test
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Model merging is increasingly used as an alternative to multi-task
  learning, but its security against adversarial transfer attacks remains underexplored.
  This study investigates the transferability of adversarial examples across merged
  models using 8 merging methods, 7 datasets, and 6 attack strategies across 336 attack
  settings.
---

# Merge Now, Regret Later: The Hidden Cost of Model Merging is Adversarial Transferability

## Quick Facts
- arXiv ID: 2509.23689
- Source URL: https://arxiv.org/abs/2509.23689
- Reference count: 40
- Primary result: Model merging increases vulnerability to adversarial transfer attacks despite assumptions of inherent robustness

## Executive Summary
Model merging is increasingly adopted as an alternative to multi-task learning, but its security implications remain underexplored. This study systematically investigates the transferability of adversarial examples across merged models using 8 merging methods, 7 datasets, and 6 attack strategies across 336 attack settings. The authors challenge the prevailing assumption that model merging provides free adversarial robustness by demonstrating that merged models are highly vulnerable to transfer attacks, with over 95% relative transfer attack success rates. Key findings include that stronger merging methods increase vulnerability to transfer attacks, and that weight averaging—typically considered the weakest method—exhibits the highest vulnerability to transfer attacks. The study provides explanations for these findings and suggests potential mitigation strategies, such as adding penalty terms to minimize transfer threat during merging.

## Method Summary
The study conducts an extensive empirical analysis of adversarial transferability in model merging across multiple dimensions. Researchers evaluate 8 different merging methods including weight averaging, fixed-weights, sigmoid-weighted averaging, and representation surgery. They test across 7 datasets (CIFAR-10, CIFAR-100, Tiny ImageNet, Caltech-UCSD Birds, Stanford Cars, German Traffic Signs, and Food-101) using 6 attack strategies including PGD, CW, and DeepFool. The experiments span 336 attack settings to comprehensively assess transfer attack success rates. The study compares merged models against single-task and multi-task learning baselines, and investigates the relationship between merging method strength and vulnerability to transfer attacks. Additionally, the researchers explore mitigation strategies by examining the impact of penalty terms during the merging process.

## Key Results
- Merged models exhibit over 95% relative transfer attack success rates, demonstrating high vulnerability
- Stronger merging methods (representation surgery, selective learning) increase vulnerability to transfer attacks
- Weight averaging, despite being the weakest merging method, shows the highest vulnerability to transfer attacks
- Mitigating representation bias through representation surgery increases vulnerability when the surrogate is a fine-tuned model

## Why This Works (Mechanism)
The increased vulnerability of merged models to transfer attacks stems from the fundamental nature of how merging methods combine knowledge from multiple models. When merging models trained on different tasks, the process creates complex decision boundaries that, while effective for clean data, can inadvertently amplify vulnerabilities that are transferable across architectures. Stronger merging methods that explicitly optimize for task performance create more consistent and exploitable gradients that adversaries can leverage. The study suggests that representation surgery, while effective at mitigating representation bias, creates representations that are more susceptible to fine-tuned model attacks. Weight averaging, despite its simplicity, creates averaged decision boundaries that are easier to approximate and attack across different model architectures, explaining its paradoxically high vulnerability.

## Foundational Learning
- **Model Merging Methods**: Understanding different approaches (weight averaging, fixed-weights, sigmoid-weighted averaging, representation surgery, selective learning) is essential because each method creates different vulnerability profiles against transfer attacks. Quick check: Compare gradient distributions across different merging methods.
- **Adversarial Transferability**: The phenomenon where adversarial examples crafted for one model successfully attack another model, crucial for understanding real-world attack scenarios. Quick check: Measure attack success rates across different model pairs.
- **Representation Surgery**: A merging technique that modifies representations to mitigate bias, needed to understand how representation-level interventions affect transfer vulnerability. Quick check: Analyze representation similarity before and after surgery.
- **Multi-task vs Single-task Learning**: The baseline comparison framework, important for contextualizing the security implications of model merging. Quick check: Compare clean accuracy and adversarial robustness across learning paradigms.
- **White-box vs Black-box Attacks**: Different attack scenarios that affect transferability assessment, necessary for understanding practical attack surfaces. Quick check: Evaluate attack success under varying knowledge assumptions.

## Architecture Onboarding

**Component Map**: Dataset Preprocessing -> Model Merging Methods -> Adversarial Attack Generation -> Transferability Evaluation -> Security Analysis

**Critical Path**: The study follows a systematic workflow: (1) Prepare datasets and pre-trained models, (2) Apply 8 different merging methods to create merged models, (3) Generate adversarial examples using 6 attack strategies, (4) Test transferability across merged models, (5) Analyze results and propose mitigation strategies.

**Design Tradeoffs**: The study prioritizes comprehensive empirical coverage (336 attack settings) over theoretical depth, trading off computational efficiency for robustness of conclusions. The choice to focus on computer vision tasks provides clear experimental results but limits generalizability to other domains.

**Failure Signatures**: High transfer attack success rates (>95%) indicate merged models are vulnerable. Unexpected findings like weight averaging showing highest vulnerability suggest that simpler methods may create more exploitable decision boundaries. Increased vulnerability when representation surgery is applied to fine-tuned models indicates potential interactions between merging methods and model initialization.

**First Experiments**: 
1. Replicate the core finding that merged models show >95% transfer attack success rates across different dataset combinations
2. Test the counterintuitive result that weight averaging exhibits highest vulnerability to transfer attacks
3. Evaluate the proposed penalty-term mitigation strategy during the merging process

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses exclusively on computer vision tasks, limiting generalizability to other domains like NLP or speech
- The proposed penalty-term mitigation strategy remains untested in practice
- The analysis assumes white-box access to surrogate models, which may not reflect all real-world attack scenarios
- The finding that weight averaging shows highest vulnerability contradicts conventional wisdom and requires deeper theoretical explanation

## Confidence
- High confidence in the overall vulnerability of merged models to transfer attacks, supported by extensive empirical evidence across multiple methods and datasets
- Medium confidence in the specific mechanisms explaining why certain merging methods increase vulnerability, as these explanations remain largely theoretical
- Medium confidence in the finding that representation surgery increases vulnerability when the surrogate is fine-tuned, given limited theoretical grounding

## Next Checks
1. Test the proposed penalty-term mitigation strategy across different merging methods and datasets to verify its effectiveness
2. Extend experiments to non-vision domains (NLP, speech) to assess domain transferability of findings
3. Evaluate black-box attack scenarios where the adversary has limited knowledge of the merged model architecture