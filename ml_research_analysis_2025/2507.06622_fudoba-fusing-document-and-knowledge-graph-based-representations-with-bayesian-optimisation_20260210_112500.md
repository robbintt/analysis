---
ver: rpa2
title: 'FuDoBa: Fusing Document and Knowledge Graph-based Representations with Bayesian
  Optimisation'
arxiv_id: '2507.06622'
source_url: https://arxiv.org/abs/2507.06622
tags:
- knowledge
- representations
- fudoba
- embeddings
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of high-dimensional, computationally
  expensive embeddings from large language models (LLMs) that are often too generic
  or inefficient for domain-specific applications. The proposed FuDoBa method integrates
  LLM-based embeddings with domain-specific structured knowledge from local and external
  knowledge graphs (e.g., WikiData) using Bayesian optimization.
---

# FuDoBa: Fusing Document and Knowledge Graph-based Representations with Bayesian Optimisation

## Quick Facts
- arXiv ID: 2507.06622
- Source URL: https://arxiv.org/abs/2507.06622
- Reference count: 40
- Primary result: FuDoBa fuses LLM and KG embeddings via Bayesian optimization, reducing dimensionality (e.g., 1536→112) while matching or exceeding proprietary LLM baselines in classification tasks.

## Executive Summary
This paper addresses the challenge of high-dimensional, computationally expensive embeddings from large language models (LLMs) that are often too generic or inefficient for domain-specific applications. The proposed FuDoBa method integrates LLM-based embeddings with domain-specific structured knowledge from local and external knowledge graphs (e.g., WikiData) using Bayesian optimization. This fusion produces low-dimensional, task-relevant representations while reducing training complexity and yielding interpretable early-fusion weights for enhanced classification performance. Experiments on six datasets across two domains (sentiment analysis and news genre classification) demonstrate that FuDoBa performs on par with or surpasses proprietary LLM-based embedding baselines when paired with AutoML classifiers. The approach significantly reduces feature dimensionality (e.g., from 1536 to 112 dimensions) while maintaining or improving classification accuracy, offering a practical, compute-efficient alternative to resource-intensive LLM fine-tuning.

## Method Summary
FuDoBa fuses LLM-based embeddings with structured knowledge from local and global knowledge graphs using Bayesian optimization. The method extracts domain-specific triplets from text to construct local KGs, embeds both local and global KGs using RotatE, and applies Truncated SVD to reduce dimensionality of each modality before fusion. Bayesian optimization tunes early fusion weights and projection dimensions to maximize classification performance, producing interpretable, low-dimensional representations that outperform or match proprietary LLM baselines while significantly reducing computational overhead.

## Key Results
- FuDoBa achieves performance on par with or exceeding proprietary LLM baselines across six datasets in sentiment analysis and news genre classification.
- The method reduces feature dimensionality by over 90% (e.g., from 1536 to 112 dimensions) while maintaining or improving classification accuracy.
- Bayesian optimization identifies optimal early fusion weights, with LLM embeddings showing highest importance (0.78) compared to Local KGs (0.50), demonstrating the effectiveness of adaptive, modality-specific integration.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Independent dimensionality reduction of modalities prior to fusion appears to preserve task-relevant variance better than compressing a concatenated high-dimensional vector.
- **Mechanism:** The paper proposes that applying Truncated SVD to each modality (LLM, Global KG, Local KG) *before* concatenation captures the "principal directions of variation" specific to that signal source. This contrasts with "concat-then-project" approaches where dominant high-dimensional features (like raw LLM embeddings) might obscure weaker but critical signals from lower-dimensional KGs during the single projection step.
- **Core assumption:** It is assumed that the top-$k$ singular values of each modality independently contain the most predictive signal for the downstream task, and that linear combination via scaling ($\alpha$) is sufficient to model interactions between these compressed subspaces.
- **Evidence anchors:**
  - [abstract]: "fusion produces low-dimensional, task-relevant representations"
  - [section 3.3]: "We argue that by doing so information was weighted equally... different signals might be important for different tasks."
  - [corpus]: Neighbor paper "HiBBO" (arXiv:2510.08965) notes that Bayesian Optimisation effectiveness diminishes in high-dimensional spaces due to sparse data, supporting the need for pre-fusion compression.
- **Break condition:** If the signal-to-noise ratio in a specific modality is low (e.g., noisy relation extraction), SVD might amplify noise as a "principal component," degrading the fused representation.

### Mechanism 2
- **Claim:** Constructing domain-specific "Local" Knowledge Graphs (LocKG) compensates for coverage gaps in Global KGs (like WikiData) for niche or short-text domains.
- **Mechanism:** The method extracts triplets directly from the dataset text using relation extractors. This ensures that entities and relations specific to the corpus (which might not exist or be linked in a global graph) are embedded. The paper suggests this is particularly useful for short, informal text where global entity linkers fail.
- **Core assumption:** The Relation Extraction (RE) models are accurate enough to extract meaningful triplets, and the subsequent embedding (RotatE) of these local graphs creates a vector space useful for classification.
- **Evidence anchors:**
  - [abstract]: "integrates... domain-specific structured knowledge, sourced both locally and from external repositories"
  - [section 5.3]: "LocKG reduces the proportion of documents with no extracted information... robustness in extracting meaningful information from short, unstructured texts."
  - [corpus]: Neighbor paper "Fusing Knowledge and Language" (arXiv:2509.09272) supports the general utility of KGs in enhancing LLMs, though FuDoBa specifically adds the local extraction layer.
- **Break condition:** If the relation extractor hallucinates or extracts trivial relations (e.g., "user", "HasProperty", "text"), the Local KG becomes noise, potentially diluting the stronger LLM signals.

### Mechanism 3
- **Claim:** Bayesian Optimization (BO) can identify an optimal "early fusion" weighting ($\alpha$) that acts as a task-specific feature selector, outperforming equal-weight baselines.
- **Mechanism:** BO treats the fusion weights and projection dimensions as hyperparameters to maximize a proxy objective (5-fold CV Macro F1). By using a Gaussian Process with an Expected Improvement acquisition function, the system learns which modalities matter (e.g., assigning high $\alpha$ to LLM for sentiment, or KG for news) without training the underlying embeddings.
- **Core assumption:** The optimal fusion configuration found via 5-fold CV on the AutoGluon proxy generalizes to the test set, and the search space (discrete weights 0.0–1.0) is sufficiently granular.
- **Evidence anchors:**
  - [section 3.4]: "BO iteratively builds a probabilistic surrogate model... to guide the selection of subsequent hyper-parameters."
  - [section 5.2]: "LLM embeddings have the highest importance (0.78)... compared to Local KGs (0.50)... highlighting the need for adaptive, modality-specific integration."
  - [corpus]: No direct corpus evidence explains the specific BO acquisition function used, relying solely on the paper's text for this mechanism.
- **Break condition:** If the BO budget (e.g., 50 trials) is insufficient for the search space size, the optimizer may converge to a local optimum, failing to find the true best weighting.

## Foundational Learning

- **Concept:** Truncated Singular Value Decomposition (SVD)
  - **Why needed here:** This is the compression engine. You must understand how SVD reduces dimensions by keeping only the top singular values (variance) to grasp how FuDoBa reduces a 1536-dim vector to 64-dim without losing the "essence" of the data.
  - **Quick check question:** If you have a 1000x100 matrix, and you apply Truncated SVD with $k=10$, what are the dimensions of the resulting matrix, and what does the value $k$ represent?

- **Concept:** Bayesian Optimization (BO) with Gaussian Processes
  - **Why needed here:** BO is the "brain" finding the best settings. You need to know that BO uses a probabilistic model (surrogate) to guess where the optimal parameters are, balancing exploring new areas vs. exploiting known good areas (Expected Improvement).
  - **Quick check question:** Why is Bayesian Optimization preferred over Grid Search for expensive-to-evaluate functions (like training an AutoML model)?

- **Concept:** Knowledge Graph Embeddings (RotatE)
  - **Why needed here:** The paper converts symbolic knowledge (triplets) into vectors using RotatE. You need to know that this allows geometric operations (like vector addition/averaging) on semantic relationships.
  - **Quick check question:** How does a Knowledge Graph Embedding model like RotatE represent a triplet like (Entity1, Relation, Entity2) in vector space?

## Architecture Onboarding

- **Component map:**
  1. **Embedding Branches:** Three parallel paths for Input Text: (1) LLM Encoder (OpenAI), (2) Global KG Linker (BabelFy + WikiData), (3) Local KG Extractor (RE model).
  2. **Projection Layer:** Per-branch Elastic Net Normalization → Truncated SVD (dim $l$) → Scaling (weight $\alpha$).
  3. **Fusion Layer:** Concatenation of scaled projections → Final Normalization.
  4. **Optimization Loop:** Bayesian Optimizer (Configures $l, \alpha$) → Trains AutoGluon Classifier → Returns F1 Score → Updates Optimizer.

- **Critical path:** The Bayesian Optimization loop is the bottleneck. The system does not just "run once"; it iteratively trains classifiers to evaluate different fusion configurations.

- **Design tradeoffs:**
  - **Dimensionality vs. Granularity:** The search space for dimensions is limited to $\{16, 32, 64\}$. This saves compute but might miss optimal intermediate dimensions (e.g., 48 or 128).
  - **Cost vs. Performance:** Using GPT-4o-mini for Local KG extraction provides high-quality triplets but incurs API costs. The paper notes smaller open models (Rebel/ReLiK) can be substituted with marginal performance drops (Appendix C).
  - **Interpretability vs. Power:** The linear weighting ($\alpha$) is interpretable but may fail to capture complex non-linear interactions between modalities that a neural "late fusion" network might catch.

- **Failure signatures:**
  - **Low Entity Coverage:** If the text has no entities (e.g., "wow this is great"), the KG branches output zero vectors. The optimizer should set $\alpha_{KG} \approx 0$, relying solely on LLM.
  - **Cost Overrun:** If the relation extraction prompt is too verbose or the dataset is massive, the Local KG construction step will dominate the runtime and cost before the AutoML training even begins.

- **First 3 experiments:**
  1. **Baseline Validation:** Run the pipeline with only the LLM branch active ($\alpha_{LLM}=1, \alpha_{KG}=0, \alpha_{Loc}=0$) to establish the raw LLM performance floor.
  2. **Ablation on Local KG:** Compare performance on a short-text dataset (e.g., HateSpeech) with and without the Local KG branch to verify if it improves coverage over the Global KG.
  3. **Sensitivity Analysis:** Fix the projection dimensions ($l$) and only optimize weights ($\alpha$) to see how sensitive the model is to dimensionality vs. weighting.

## Open Questions the Paper Calls Out
None

## Limitations
- **Cost and Scalability:** The use of GPT-4o-mini for Local KG extraction, while improving triplet quality, introduces ongoing API costs that may not be feasible for large-scale or enterprise deployments.
- **Domain Generalization:** The experiments are limited to sentiment analysis and news genre classification. The effectiveness of Local KG extraction and Bayesian fusion for other domains (e.g., biomedical, legal) remains untested.
- **Knowledge Graph Dependency:** The method relies on the availability and quality of external KGs (e.g., WikiData) and robust entity linking (BabelFy). In domains where entities are not well-covered or disambiguation is difficult, the Global KG branch may contribute little, reducing the fusion benefit to the LLM signal alone.

## Confidence
- **High Confidence:** The core experimental results showing FuDoBa's performance parity or improvement over LLM baselines (with reduced dimensionality) are well-supported by the ablation studies and error analysis in the paper.
- **Medium Confidence:** The claim that Local KG extraction "compensates for coverage gaps" is plausible given the short-text results, but the quality of triplets from the relation extractor is not independently verified.
- **Low Confidence:** The exact choice of Bayesian Optimization acquisition function (Expected Improvement) and its hyperparameters are not deeply justified.

## Next Checks
1. **Ablation on Knowledge Graph Sources:** Conduct experiments on a dataset with minimal entity coverage (e.g., tweets or short reviews) to measure the individual and combined contributions of Global KG, Local KG, and no KG.
2. **Cross-Domain Robustness Test:** Apply FuDoBa to a domain with a different entity distribution (e.g., biomedical literature) and compare performance using both the proprietary GPT-4o-mini and the open-source Rebel model for Local KG extraction.
3. **Bayesian Optimization Sensitivity Analysis:** Vary the BO budget (number of trials) and the dimensionality search space granularity to determine if the reported fusion weights and performance are stable or highly dependent on these hyperparameters.