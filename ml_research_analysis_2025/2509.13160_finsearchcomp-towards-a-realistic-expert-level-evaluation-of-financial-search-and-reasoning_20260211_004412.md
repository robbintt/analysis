---
ver: rpa2
title: 'FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial Search
  and Reasoning'
arxiv_id: '2509.13160'
source_url: https://arxiv.org/abs/2509.13160
tags:
- answer
- financial
- data
- price
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "FinSearchComp is the first open-domain benchmark for financial\
  \ search and reasoning, designed to assess whether LLM-based agents can retrieve\
  \ and synthesize real-time and historical financial data as analysts do. It contains\
  \ 635 expert-curated questions across three task types\u2014Time-Sensitive Data\
  \ Fetching, Simple Historical Lookup, and Complex Historical Investigation\u2014\
  covering global and Greater China markets."
---

# FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial Search and Reasoning

## Quick Facts
- **arXiv ID**: 2509.13160
- **Source URL**: https://arxiv.org/abs/2509.13160
- **Reference count**: 40
- **Primary result**: First open-domain benchmark for financial search and reasoning with 635 expert-curated questions across global and Greater China markets

## Executive Summary
FinSearchComp is the first open-domain benchmark for financial search and reasoning, designed to assess whether LLM-based agents can retrieve and synthesize real-time and historical financial data as analysts do. It contains 635 expert-curated questions across three task types—Time-Sensitive Data Fetching, Simple Historical Lookup, and Complex Historical Investigation—covering global and Greater China markets. The benchmark includes rigorous multi-stage quality control, uses LLM-as-a-judge with tolerance bands, and is fully open-source. Evaluation of 21 models (web-enabled products and APIs) shows that even top-performing systems such as Grok 4 (web) and GPT-5-Thinking (web) lag behind human experts, though they approach expert-level accuracy on the global subset. Web search and financial plugins substantially improve performance, especially for time-sensitive and complex tasks. Models' country origin strongly affects results, with US models excelling on global assets and Chinese models on domestic data. Persistent gaps in freshness awareness, multi-source reconciliation, and temporal reasoning indicate current agents remain fragile in realistic financial search and reasoning contexts.

## Method Summary
The benchmark comprises 635 questions across three task types (T1: time-sensitive data fetching, T2: simple historical lookup, T3: complex historical investigation) covering global and Greater China markets. Questions are expert-curated with multi-stage quality control and evaluated using LLM-as-a-judge with tolerance bands. 21 models were tested including both web-enabled products and API-based systems, with evaluations covering tool usage patterns, geographic performance differences, and task-specific challenges.

## Key Results
- Top-performing models (Grok 4 web, GPT-5-Thinking web) achieve human-level accuracy on global subset but lag behind on Greater China questions
- Web search and financial plugins provide substantial improvements, especially for time-sensitive queries (40.8 point gain on T1)
- Model geographic origin strongly influences performance: US models excel on global assets, Chinese models on domestic data
- Performance declines monotonically from T1 to T3, with complex historical investigations remaining the primary bottleneck

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Web search and financial plugins substantially improve agent performance on financial search tasks, with the largest gains on time-sensitive queries.
- Mechanism: External tools bypass parametric memory limitations by providing real-time data access and structured financial databases. Without search, models score 0 on T1 (time-sensitive) since they cannot retrieve current prices; with search, average gains of 40.8, 29.0, and 8.1 points are observed on T1, T2, and T3 respectively.
- Core assumption: The improvement stems from retrieval augmentation rather than model reasoning improvements alone.
- Evidence anchors:
  - [abstract] "Experimental analyses show that equipping agents with web search and financial plugins substantially improves results on FinSearchComp"
  - [Section 4.1/4.2] DeepSeek-R1 shows 31.9pp improvement on T1 when using financial plugins via YuanBao vs. official DeepSeek interface
  - [corpus] EconWebArena (arXiv:2506.08136) similarly shows autonomous agents require realistic web environments for economic tasks
- Break condition: If models memorize pre-training cutoff data that matches benchmark answers, gains may reflect retrieval rather than genuine search capability.

### Mechanism 2
- Claim: Task difficulty scales monotonically from T1 (time-sensitive fetching) to T2 (simple historical) to T3 (complex historical), requiring progressively more multi-hop reasoning and source reconciliation.
- Mechanism: T1 requires single-point real-time retrieval; T2 requires point-in-time fidelity with reporting calendar alignment; T3 requires multi-period synthesis across heterogeneous sources, corporate action adjustments, and provenance reconciliation. Performance declines monotonically across all models from T1→T2→T3.
- Core assumption: The difficulty progression reflects genuine cognitive demands rather than benchmark construction artifacts.
- Evidence anchors:
  - [Section 2.2/Table 2] Defines three task types with retrieval depth (1→1→>1), temporal span (1 day→1 day→184 months), and reasoning complexity (Easy→Medium→Hard)
  - [Section 3.2/Finding 1] "Across models, performance declines monotonically from T1 to T2 to T3"
  - [corpus] FinMR (arXiv:2510.07852) similarly emphasizes knowledge-intensive multimodal reasoning complexity in finance
- Break condition: If T3 questions are ambiguous or have multiple valid interpretations, difficulty may reflect benchmark noise rather than reasoning demands.

### Mechanism 3
- Claim: Model geographic origin strongly influences performance on regional financial data, with US models excelling on global assets and Chinese models on Greater China domestic data.
- Mechanism: Performance differences arise from training corpus geography (English/SEC filings vs. CN/HK/TW disclosures), linguistic conventions, and retrieval infrastructure integration with regional data sources.
- Core assumption: The pattern reflects corpus and tooling alignment rather than data leakage or memorization.
- Evidence anchors:
  - [Section 4.3/Figure 8] US models show asset origin ratio >100% on T1/T2; Chinese models show ratio <100%
  - [Section 3.1] "Rankings differ between the global and Greater China subsets, likely reflecting differences in training-corpus coverage, language/domain alignment, and retrieval infrastructure"
  - [corpus] MultiFinBen (arXiv:2506.14028) confirms multilingual/multimodal financial evaluation reveals cross-market generalization gaps
- Break condition: If benchmark questions overlap with training data in region-specific ways, performance may reflect memorization rather than search capability.

## Foundational Learning

- Concept: **Fiscal calendar vs. calendar year alignment**
  - Why needed here: Many companies (e.g., NVIDIA, Apple) have fiscal years offset from calendar years. T2 questions require precise point-in-time lookups aligned to company reporting periods.
  - Quick check question: If Apple's fiscal year ends in September, what date range does FY2024 cover?

- Concept: **Data provenance reconciliation**
  - Why needed here: T3 tasks require comparing data across sources that may use different adjustment methods (forward-adjusted vs. backward-adjusted prices), reporting conventions (GAAP vs. non-GAAP), or currency denominations.
  - Quick check question: Why might two financial databases report different PE-TTM ratios for the same company on the same date?

- Concept: **Freshness awareness and temporal validity**
  - Why needed here: T1 tasks require knowing when markets close, handling timezone differences, and understanding data latency from APIs vs. web sources.
  - Quick check question: If evaluating "IBM latest close price" at 3pm ET on a trading day, what time constraints apply?

## Architecture Onboarding

- Component map: User Query → Query Understanding → Tool Selection (Web Search / Financial Plugin / API) → Multi-source Retrieval → Temporal Alignment → Unit/Currency Normalization → Evidence Synthesis → Response Generation

- Critical path: For T3 tasks, the bottleneck is multi-source reconciliation—models must retrieve from >1 source, align timelines, handle corporate actions, and synthesize without hallucinating. Current top models (Grok 4, GPT-5-Thinking) achieve ~51% on T3 vs. human 76.7% on Greater China subset.

- Design tradeoffs:
  - LLM-as-Judge with tolerance bands vs. exact match: Tolerance bands accommodate legitimate rounding/revisions but may mask systematic errors
  - Web search vs. financial plugins: Plugins provide structured real-time data but require activation; web search is broader but retrieves outdated content
  - API access vs. web products: APIs allow controlled evaluation but lack proprietary tooling; web products reflect real deployment but vary in transparency

- Failure signatures:
  - Non-activation of financial plugins (models default to parametric memory)
  - Retrieval of stale web content instead of current prices
  - Confusion between opening/closing prices, adjusted/unadjusted values
  - Misalignment of fiscal vs. calendar year boundaries
  - Source conflict without reconciliation (two sources, different values, no disambiguation)

- First 3 experiments:
  1. **Tool ablation study**: Compare same model (e.g., DeepSeek-R1) with/without financial plugins on T1 to quantify plugin contribution (expect ~30pp gain based on Section 4.2)
  2. **Freshness degradation test**: Evaluate T1 queries at different times relative to market close to measure data latency sensitivity
  3. **Cross-market generalization**: Take top US model and top Chinese model, evaluate both on opposite-region subset to isolate corpus/tooling effects from reasoning capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can agentic frameworks be modified to dynamically suppress deep reasoning chains for low-complexity tasks to mitigate the "overthinking" penalty observed in Time-Sensitive Data Fetching?
- Basis in paper: [explicit] Section 4.5 notes a 7.0-point average performance decline for reasoning models on Task 1 (T1), attributing it to "potential overthinking of reasoning models" where complexity is unnecessary.
- Why unresolved: The paper identifies the phenomenon but does not propose or test mechanisms for "reasoning calibration" (knowing when *not* to reason).
- What evidence would resolve it: A study testing models equipped with a "complexity classifier" that bypasses Chain-of-Thought (CoT) processing for T1-type queries, showing restored or improved accuracy.

### Open Question 2
- Question: To what extent does the "home-field advantage" (performance bias based on model origin) stem from proprietary retrieval infrastructures versus training corpus dominance?
- Basis in paper: [explicit] The abstract and Section 4.3 note that US models excel on global assets while Chinese models excel on domestic data, citing "corpus geography" and "linguistic and market conventions" as likely causes without isolating the variable.
- Why unresolved: The study compares existing products with different stacks; it does not ablate whether swapping the search tool (e.g., giving a US model a Chinese search API) neutralizes the bias.
- What evidence would resolve it: An ablation study swapping the search backends of the top-performing US and Chinese models to measure the delta caused purely by the retrieval layer versus the base model.

### Open Question 3
- Question: Can multi-source verification mechanisms be integrated into search agents to resolve the "freshness awareness" and "cross-unit/currency aggregation" failures prevalent in Complex Historical Investigations?
- Basis in paper: [inferred] The conclusion identifies "persistent gaps in freshness awareness, multi-source reconciliation, and temporal reasoning," and Section 4.4 notes that T3 failures often stem from an inability to perform "structured data retrieval" or reconcile conflicting evidence.
- Why unresolved: The paper evaluates current off-the-shelf agents but does not test custom architectures designed specifically for financial provenance verification.
- What evidence would resolve it: Implementation of a dedicated "reconciliation module" in an agent pipeline that cross-checks retrieved data against primary filings (e.g., SEC EDGAR) before synthesis, demonstrating improved T3 scores.

## Limitations

- The tolerance bands used in LLM-as-judge evaluation may mask systematic errors in financial reasoning by accepting responses that are numerically within bounds but conceptually incorrect
- Web-enabled product evaluations face reproducibility challenges since product interfaces and capabilities change frequently
- The benchmark's focus on expert-level questions may not generalize to typical retail investor use cases

## Confidence

**High Confidence**:
- Models equipped with web search and financial plugins substantially outperform those without tools, particularly on time-sensitive queries
- Task difficulty scales predictably from T1 to T3, with performance declining across all models
- Geographic origin influences regional performance patterns, with US models excelling on global assets and Chinese models on China data

**Medium Confidence**:
- Top-performing models (Grok 4 web, GPT-5-Thinking web) approach but do not match human expert accuracy
- Multi-source reconciliation remains the primary bottleneck for complex historical tasks
- Freshness awareness limitations affect all models, particularly in handling time zone differences and data latency

**Low Confidence**:
- The exact contribution of parametric knowledge vs. retrieval augmentation is difficult to isolate
- Whether tolerance bands appropriately balance precision with practical usability
- The extent to which current performance gaps reflect fundamental limitations vs. available tool quality

## Next Checks

1. **Tool Ablation Replication**: Conduct controlled experiments comparing identical models with/without financial plugins across all task types to quantify plugin-specific contributions beyond general web search benefits.

2. **Freshness Sensitivity Analysis**: Systematically evaluate model performance on time-sensitive queries at multiple timepoints relative to market close to measure data latency impact and identify optimal retrieval strategies.

3. **Cross-Corpus Generalization Test**: Evaluate top US and Chinese models on reversed regional subsets (US models on Greater China, Chinese models on global) to isolate corpus coverage effects from genuine reasoning capability differences.