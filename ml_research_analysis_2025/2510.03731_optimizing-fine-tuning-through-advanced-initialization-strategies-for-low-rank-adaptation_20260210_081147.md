---
ver: rpa2
title: Optimizing Fine-Tuning through Advanced Initialization Strategies for Low-Rank
  Adaptation
arxiv_id: '2510.03731'
source_url: https://arxiv.org/abs/2510.03731
tags:
- inilora
- lora
- low-rank
- initialization
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces IniLoRA, a parameter-efficient fine-tuning\
  \ method that improves LoRA by initializing low-rank matrices to better approximate\
  \ original model weights using gradient descent. Unlike LoRA\u2019s zero initialization,\
  \ IniLoRA minimizes the mean squared error between the product of decomposed matrices\
  \ and the original weights to find optimal initialization, thereby fully activating\
  \ and leveraging the pretrained model\u2019s capacity."
---

# Optimizing Fine-Tuning through Advanced Initialization Strategies for Low-Rank Adaptation

## Quick Facts
- arXiv ID: 2510.03731
- Source URL: https://arxiv.org/abs/2510.03731
- Authors: Yongfu Xue
- Reference count: 34
- Primary result: IniLoRA improves LoRA performance across multiple benchmarks by initializing low-rank matrices to better approximate original model weights

## Executive Summary
This paper introduces IniLoRA, a parameter-efficient fine-tuning method that enhances LoRA by initializing low-rank matrices to better approximate original model weights using gradient descent. Unlike LoRA's zero initialization, IniLoRA minimizes the mean squared error between the product of decomposed matrices and the original weights to find optimal initialization, thereby fully activating and leveraging the pretrained model's capacity. The approach demonstrates consistent performance gains over LoRA, PiSSA, and MiLoRA across RoBERTa and LLaMA family models on GLUE, GSM8K, MATH, MMLU, and HumanEval benchmarks.

## Method Summary
IniLoRA pre-computes optimal initialization for low-rank matrices A and B by running gradient descent to minimize MSE between BA and original weights W₀, then freezes the residual R = W₀ - BA during fine-tuning. Two variants, IniLoRA-α (broader initialization variance) and IniLoRA-β (Kaiming distribution), further improve performance. The method requires a one-time weight approximation phase (typically 4000-20000 iterations) that is cached for reuse, making it efficient for repeated fine-tuning tasks.

## Key Results
- Consistently outperforms LoRA, PiSSA, and MiLoRA across RoBERTa and LLaMA family models on multiple benchmarks
- IniLoRA-α and IniLoRA-β variants achieve further improvements over standard IniLoRA
- Demonstrates robustness to learning rates and scalability with data size
- Weight approximation phase is efficient (26 minutes, 16GB memory for LLaMA2-7B) and results are cacheable

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Based Weight Approximation for Initialization
- Claim: Pre-approximating original model weights with low-rank matrices before fine-tuning improves convergence over zero initialization.
- Mechanism: IniLoRA initializes matrices A and B using global weight statistics (μ, σ computed across all layers), then optimizes them via gradient descent to minimize MSE between the product BA and original weights W₀. The residual R = W₀ − BA is frozen during fine-tuning, while only A and B are updated.
- Core assumption: Weight adjustments during the initial training phase should closely resemble those of full parameter fine-tuning.
- Evidence anchors:
  - [abstract]: "IniLoRA minimizes the mean squared error between the product of decomposed matrices and the original weights to find optimal initialization"
  - [Section 3, Equation 3]: Defines the gradient descent update rules for A and B
  - [Section 4.3.2, Figure 2]: Shows correlation between weight approximation quality (MSE) and downstream task accuracy—plateau occurs around 4000 iterations
  - [corpus]: NLoRA (arxiv:2502.14482) similarly addresses slow convergence via improved initialization strategies

### Mechanism 2: Enhanced Initialization Variance (IniLoRA-α)
- Claim: Broader initialization distributions with larger standard deviation enable better parameter space exploration and improve performance.
- Mechanism: IniLoRA-α samples from N(0, 0.5²) instead of the narrower distribution derived from original weights. Larger variance introduces greater randomness, generates larger gradients, and accelerates learning.
- Core assumption: There exists an optimal initialization scale that balances exploration capability against training stability.
- Evidence anchors:
  - [Section 3, Equation 4]: Defines IniLoRA-α with σα = 0.5
  - [Section 4.3.3, Figure 3]: Demonstrates performance increases as σ rises to 0.5, then drops sharply beyond this threshold
  - [corpus]: HRP (arxiv:2502.07739) theoretically analyzes LoRA sensitivity to initialization

### Mechanism 3: Kaiming Distribution for Gradient Stability (IniLoRA-β)
- Claim: Kaiming initialization better aligns with deep network gradient propagation requirements than standard normal or Xavier methods.
- Mechanism: IniLoRA-β initializes A and B using Kaiming distribution (normal or uniform variants), which stabilizes input-output variance across layers during training.
- Core assumption: Variance preservation across layers is critical for effective gradient flow in LoRA fine-tuning.
- Evidence anchors:
  - [Section 3]: "employs low-rank matrices initialized using the Kaiming distribution"
  - [Section 4.3.4, Figure 4]: Kaiming initialization consistently outperforms xavier, orthogonal, uniform, and other methods
  - [corpus]: The Primacy of Magnitude (arxiv:2507.06558) establishes update magnitude as the primary driver of LoRA performance

## Foundational Learning

- **Concept: Low-Rank Matrix Decomposition (W ≈ BA where r ≪ min(d,k))**
  - Why needed here: Understanding how rank constraints enable parameter-efficient updates is essential for interpreting IniLoRA's approximation strategy.
  - Quick check question: Explain why rank r = 8 requires fewer parameters than updating the full weight matrix W ∈ ℝ^(d×k).

- **Concept: Gradient Descent for Matrix Factorization**
  - Why needed here: IniLoRA uses iterative GD (not SVD) to minimize ||W − BA||²_F; understanding convergence behavior is critical.
  - Quick check question: What is the key difference between SVD-based initialization (PiSSA, MiLoRA) and GD-based initialization (IniLoRA)?

- **Concept: Initialization Distributions and Variance Scaling**
  - Why needed here: IniLoRA-α and IniLoRA-β demonstrate that initialization distribution choice significantly impacts fine-tuning outcomes.
  - Quick check question: Why does Kaiming initialization specifically stabilize variance in networks with ReLU activations?

## Architecture Onboarding

- **Component map:**
  - Low-rank matrices: A ∈ ℝ^(r×k), B ∈ ℝ^(d×r) (r = 8, 16, or 32 typical)
  - Residual matrix: R = W₀ − BA (frozen during fine-tuning)
  - Target modules: Query and Value projections in self-attention layers
  - Cached weights: Pre-computed A^(T) and B^(T) stored for reuse

- **Critical path:**
  1. Compute global μ and σ across all layer weight matrices (Equations 1-2)
  2. Initialize A and B using N(μ, σ)
  3. Run gradient descent for 4000-20000 steps (LR = 5e-4, Adam optimizer, StepLR scheduler)
  4. Cache optimized matrices for future fine-tuning sessions
  5. During fine-tuning: freeze R, update only A and B

- **Design tradeoffs:**
  - More approximation iterations → better MSE but longer setup time (convergence typically at ~4000 steps)
  - Higher rank (r = 32 vs 8) → better performance but more trainable parameters
  - IniLoRA-α → superior performance on complex tasks but risk of collapse if σ > 0.5
  - IniLoRA-β (Kaiming) → more stable gradient propagation but requires ReLU-compatible architecture

- **Failure signatures:**
  - Approximation loss plateaus early → initialization μ/σ may be miscomputed
  - Performance degrades sharply → initialization variance exceeds 0.5 (IniLoRA-α)
  - No convergence within 4000 steps → verify learning rate (~5e-4) and optimizer settings
  - Memory overflow during approximation → reduce concurrency level (default 64) in Table 5

- **First 3 experiments:**
  1. **Baseline validation**: Run standard LoRA (zero initialization) on your target task with rank r = 8 to establish reference performance.
  2. **IniLoRA core test**: Apply IniLoRA with r = 8, LR = 5e-4, 4000 approximation steps; compare loss curves against baseline.
  3. **Variant comparison**: Test IniLoRA-α (σ = 0.5) and IniLoRA-β (Kaiming-normal) to determine best initialization strategy for your task.

**Evidence-backed efficiency note:** Weight approximation runs once per model and matrices are cached—Table 5 reports ~26 minutes and 16GB memory for LLaMA2-7B on NVIDIA 4090.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does IniLoRA performance scale with training datasets significantly larger than 100K samples or with model architectures exceeding 13B parameters?
- Basis in paper: [explicit] Section 4.3.1 explicitly states, "Further work could explore its broader scalability."
- Why unresolved: The experiments in Table 4 test data sizes only up to 100K, and the largest model tested is LLaMA2-13B.
- Evidence: Evaluations on common crawl-scale datasets or 70B+ parameter models showing whether the initialization advantage is maintained or saturates.

### Open Question 2
- Question: Why does IniLoRA-α (random high-variance initialization) outperform the gradient-descent-based weight approximation of standard IniLoRA in certain high-rank settings?
- Basis in paper: [inferred] Table 3 shows IniLoRA-α surpassing IniLoRA on LLaMA2-13B, suggesting that mimicking original weights (MSE minimization) is not always the optimal strategy compared to simply increasing initialization variance.
- Why unresolved: The paper establishes the empirical result but does not theoretically explain why random noise yields better local optima than weight approximation in these specific cases.
- Evidence: An analysis of the loss landscape geometry or gradient flow dynamics comparing the α variant against the standard approximation method.

### Open Question 3
- Question: Can the computational cost of the weight approximation phase be reduced via early stopping without compromising downstream task performance?
- Basis in paper: [inferred] Section 4.3.2 notes that approximation loss plateaus at roughly 4,000 iterations, yet the methodology (Section 4.1) runs for 20,000 steps.
- Why unresolved: It is unclear if the marginal reduction in approximation error between 4,000 and 20,000 steps provides statistically significant benefits for the final fine-tuning task.
- Evidence: A comparison of downstream accuracy (e.g., GSM8K) when initializing from checkpoints saved at 4k vs. 20k iterations.

## Limitations

- Weight approximation phase requires significant computational resources (16GB memory, ~26 minutes for LLaMA2-7B) that may limit applicability to smaller hardware
- Performance gains beyond 4000 approximation steps are not thoroughly explored despite running for 20,000 steps
- Claims about learning rate robustness are based on a limited range of learning rates tested
- Concurrency strategy for multi-layer weight approximation remains unclear, potentially affecting reproducibility

## Confidence

- **High confidence**: The core mechanism of gradient-based weight approximation for initialization is well-supported by experimental evidence and aligns with established principles of optimization
- **Medium confidence**: The claims about IniLoRA-α's variance sensitivity (optimal at σ=0.5) and IniLoRA-β's superiority over other initialization methods are supported by experiments but lack theoretical justification
- **Medium confidence**: The scalability claims regarding data size and learning rate robustness are demonstrated but not extensively tested across the full parameter space

## Next Checks

1. **Approximation convergence validation**: Systematically measure the relationship between weight approximation MSE and downstream task performance across the full 0-20,000 iteration range to identify the optimal stopping point and quantify diminishing returns

2. **Extreme learning rate testing**: Test IniLoRA across a broader range of learning rates (including very low and very high values) to rigorously validate the claim of learning rate robustness and identify any hidden sensitivity thresholds

3. **Cross-module performance comparison**: Evaluate IniLoRA when applied to different LoRA-eligible modules (attention output, MLP layers) beyond query and value projections to determine if the initialization benefits generalize across all parameter-efficient fine-tuning locations