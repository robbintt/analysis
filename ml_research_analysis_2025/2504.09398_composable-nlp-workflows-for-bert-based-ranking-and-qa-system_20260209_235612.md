---
ver: rpa2
title: Composable NLP Workflows for BERT-based Ranking and QA System
arxiv_id: '2504.09398'
source_url: https://arxiv.org/abs/2504.09398
tags:
- pipeline
- ranking
- system
- have
- re-ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents an end-to-end BERT-based ranking and QA system
  built using the Forte toolkit for composable NLP pipelines. The system combines
  BM25-based full-ranking, BERT-based re-ranking, and BERT-based QA to retrieve and
  answer questions from large document corpora.
---

# Composable NLP Workflows for BERT-based Ranking and QA System
## Quick Facts
- arXiv ID: 2504.09398
- Source URL: https://arxiv.org/abs/2504.09398
- Authors: Gaurav Kumar; Murali Mohana Krishna Dandu
- Reference count: 15
- This work presents an end-to-end BERT-based ranking and QA system built using the Forte toolkit for composable NLP pipelines. The system combines BM25-based full-ranking, BERT-based re-ranking, and BERT-based QA to retrieve and answer questions from large document corpora. Evaluated on MS-MARCO and Covid-19 datasets, the pipeline achieves MRR@10 of 0.34 and Recall@10 of 58% for re-ranking, and QA metrics (BLEU-1, ROUGE-L, F1) around 0.32 on MS-MARCO. On the Covid-19 dataset, QA metrics are slightly lower (BLEU-1, ROUGE-L, F1 â‰ˆ 0.22), partly due to the use of a generic re-ranker not tuned to the domain. Results demonstrate that the modular, composable pipeline enables easy construction of complex NLP applications with competitive performance.

## Executive Summary
This paper presents a BERT-based ranking and question-answering system built using the Forte toolkit, which enables composable NLP workflows. The system combines traditional BM25 retrieval with BERT-based re-ranking and question-answering modules to process queries over large document collections. Evaluated on both the MS-MARCO and Covid-19 datasets, the pipeline demonstrates competitive performance with MRR@10 of 0.34 and Recall@10 of 58% for re-ranking on MS-MARCO, while achieving reasonable QA metrics around 0.32 BLEU-1 and ROUGE-L.

## Method Summary
The system uses a modular pipeline architecture where documents are first processed through BM25-based full-ranking to retrieve candidate passages, followed by BERT-based re-ranking to refine the results, and finally BERT-based QA to extract answers from the top-ranked passages. The pipeline leverages the Forte toolkit's composable components, allowing for flexible assembly of different NLP modules. The approach processes queries by first retrieving relevant passages using BM25, then applying a BERT-based re-ranker to improve result quality, and finally using a BERT-based QA model to generate answers from the re-ranked passages.

## Key Results
- MRR@10 of 0.34 and Recall@10 of 58% for BERT-based re-ranking on MS-MARCO dataset
- QA metrics (BLEU-1, ROUGE-L, F1) around 0.32 on MS-MARCO dataset
- QA metrics (BLEU-1, ROUGE-L, F1) approximately 0.22 on Covid-19 dataset
- Performance degradation on Covid-19 dataset attributed to use of generic re-ranker not domain-tuned

## Why This Works (Mechanism)
The pipeline's effectiveness stems from combining complementary retrieval approaches: BM25 provides efficient initial candidate selection based on lexical matching, while BERT-based re-ranking captures semantic similarity and contextual relevance that traditional methods miss. The modular design allows each component to specialize in its function while maintaining flexibility to swap or upgrade individual modules without affecting the entire system. The BERT-based QA component leverages contextual understanding to extract precise answers from the top-ranked passages, improving over simple keyword matching approaches.

## Foundational Learning
- **BERT-based ranking**: Why needed - to capture semantic similarity beyond exact keyword matches; Quick check - compare MRR@10 with and without BERT re-ranking
- **BM25 retrieval**: Why needed - provides efficient initial candidate selection from large document collections; Quick check - measure recall@100 before and after BM25 filtering
- **Composable pipeline architecture**: Why needed - enables modular development and easy swapping of components; Quick check - measure time to replace QA module with alternative implementation
- **BERT-based QA**: Why needed - extracts precise answers using contextual understanding; Quick check - compare F1 scores with rule-based extraction methods
- **Multi-stage ranking**: Why needed - combines efficiency of BM25 with accuracy of semantic ranking; Quick check - measure latency and accuracy trade-off at each stage
- **Domain adaptation**: Why needed - performance on Covid-19 dataset suggests need for domain-specific fine-tuning; Quick check - fine-tune models on Covid-19 corpus and measure performance improvement

## Architecture Onboarding
- **Component map**: Document Collection -> BM25 Full-Ranking -> BERT Re-Ranking -> BERT QA -> Answer Output
- **Critical path**: The sequence from initial BM25 retrieval through BERT re-ranking to final QA extraction represents the critical performance path, with each stage building on the previous one's output
- **Design tradeoffs**: The modular approach sacrifices some end-to-end optimization for flexibility and maintainability, allowing individual components to be upgraded without system-wide changes
- **Failure signatures**: Poor re-ranking performance manifests as low MRR@10 scores, while QA failures show up as low BLEU/ROUGE/F1 metrics; domain mismatch causes performance degradation on specialized datasets
- **First experiments**: 1) Run the pipeline end-to-end on MS-MARCO to establish baseline metrics, 2) Test with synthetic queries to validate each component independently, 3) Compare performance with and without the BERT re-ranking stage to quantify its contribution

## Open Questions the Paper Calls Out
None

## Limitations
- Performance on Covid-19 dataset is lower due to use of generic re-ranker not tuned to the domain
- Reliance on Forte toolkit may introduce computational overhead compared to specialized implementations
- Domain-general models used throughout the pipeline limit performance on specialized datasets

## Confidence
- Medium confidence in MS-MARCO results due to established benchmarks and known baselines
- Low-Medium confidence in Covid-19 results due to acknowledged domain mismatch and lack of comparison to existing systems
- Claim about modular pipeline ease of construction supported by results but needs ablation studies

## Next Checks
1. Fine-tune the BERT re-ranker and QA models on the Covid-19 dataset to establish an upper bound for domain-adapted performance
2. Conduct ablation studies removing the BM25 full-ranking step to quantify its contribution to overall performance
3. Benchmark against specialized Covid-19 retrieval systems like those from the TREC-COVID challenges to contextualize the performance gap