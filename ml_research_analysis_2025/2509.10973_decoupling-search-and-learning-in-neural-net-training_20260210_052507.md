---
ver: rpa2
title: Decoupling Search and Learning in Neural Net Training
arxiv_id: '2509.10973'
source_url: https://arxiv.org/abs/2509.10973
tags:
- search
- representations
- space
- block
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes decoupling search and learning in neural network
  training by performing evolutionary search in representation space (intermediate
  activations) rather than parameter space, then training networks via gradient descent
  to produce these searched representations. The approach addresses gradient descent's
  limitation of converging to single minima without exploring diverse solutions that
  may generalize better.
---

# Decoupling Search and Learning in Neural Net Training

## Quick Facts
- arXiv ID: 2509.10973
- Source URL: https://arxiv.org/abs/2509.10973
- Authors: Akshay Vegesna; Samip Dahal
- Reference count: 10
- One-line primary result: Evolutionary search in representation space yields test accuracies within 1% of SGD baselines on CIFAR-10/100 without data augmentation

## Executive Summary
This paper proposes decoupling search and learning in neural network training by performing evolutionary search in representation space (intermediate activations) rather than parameter space, then training networks via gradient descent to produce these searched representations. The approach addresses gradient descent's limitation of converging to single minima without exploring diverse solutions that may generalize better. Results show competitive performance: on CIFAR-10 and CIFAR-100 without data augmentation, the method achieves test accuracies within 1% of SGD baselines (90.6-88.3% vs 93.0-89.1%). With data augmentation, a variant that skips supervision on the first layer performs within 1-3% of SGD.

## Method Summary
The method works by first running evolutionary search to discover high-fitness representations at intermediate layers, then training network parameters to match these representations using layer-wise MSE losses combined with KL divergence on the final logits. Critically, the convolutional body learns exclusively from representation targets rather than from classification gradients. The approach uses a two-phase architecture: an untrained initialized network undergoes layer-wise evolutionary search with population 240 and per-image top-k selection, caching high-fitness representations; then an expanded network (6 conv layers per block) is trained with SGD using MSE regression to these cached targets plus KL on logits with stop-gradient on the convolutional body.

## Key Results
- Search-based learning achieves 99.0%, 88.3%, 61.6% test accuracy on MNIST, CIFAR-10, CIFAR-100 vs. SGD's 99.1%, 89.1%, 62.3% without data augmentation
- With data augmentation, skipping supervision on the first layer improves performance, achieving 90.6%, 88.3%, 58.9% vs. SGD's 93.0%, 89.1%, 61.5%
- Spatial smoothing during search is crucial for learnability; without it, the convolutional body cannot match searched representations
- The method demonstrates qualitatively different optimization trajectories compared to standard training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Representation space is tractable for evolutionary search in ways parameter space is not.
- Mechanism: The space of intermediate activations has dramatically lower dimensionality than parameter space, allowing random perturbations to make meaningful progress toward high-fitness solutions.
- Core assumption: Good parameters must produce good intermediate representations; the mapping from representations to parameters is learnable via gradient descent.
- Evidence anchors:
  - [abstract] "search in a tractable representation space (the space of intermediate activations) to find diverse representational solutions"
  - [section 2.2] "Parameter space fails both search requirements. First, it is far too large for random sampling to be effective"
- Break condition: If representations found through search are not learnable by gradient descent, the decoupling fails.

### Mechanism 2
- Claim: Layer-wise sequential evolution enables compositional optimization with controlled information flow.
- Mechanism: Evolution proceeds from early to late layers, fixing optimized representations at each stage before evolving the next. This ensures later layers build on earlier improvements.
- Core assumption: Optimal representations at layer ℓ can be found independently when layers 0 through ℓ−1 are fixed.
- Evidence anchors:
  - [section 3.1] "This ensures that each layer's search builds upon the optimized solutions found for previous layers, creating a compositional optimization process"
- Break condition: If strong inter-layer dependencies mean early-layer choices irreversibly constrain later-layer optima, sequential fixation may converge to poor local solutions.

### Mechanism 3
- Claim: Networks can learn exclusively from searched representations via regression without backpropagating classification loss.
- Mechanism: The training objective uses MSE loss at intermediate layers plus KL divergence at logits with stop-gradient on the network body.
- Core assumption: Searched representations contain sufficient task-relevant information that regressing to them generalizes to unseen data.
- Evidence anchors:
  - [section 4.2] "The convolutional representations are thus shaped entirely by the MSE regression targets, avoiding collapse to standard backpropagation"
- Break condition: If searched representations overfit to training examples, regression will memorize rather than generalize.

## Foundational Learning

- Concept: **Evolutionary search basics (population, mutation, selection, fitness)**
  - Why needed here: The entire search phase uses genetic operators—crossover, Gaussian mutation, top-k selection—to evolve representations.
  - Quick check question: Can you explain why top-k selection per image (vs. batch-wide) might accelerate convergence?

- Concept: **Neural network representations/activations**
  - Why needed here: Search operates directly on intermediate feature maps (e.g., 256×15×15 tensors), not parameters.
  - Quick check question: What does a 256×15×15 feature map represent at a convolutional block output?

- Concept: **Target propagation / layer-wise loss**
  - Why needed here: The learning phase regresses to searched targets rather than using end-to-end backpropagation.
  - Quick check question: Why does stop-gradient prevent "collapse to standard backpropagation"?

## Architecture Onboarding

- Component map:
  - Search phase: Untrained initialized network → Layer-wise evolution (population 240, top-k=20) → Cached representations {Ĥ^(ℓ)} for each training example
  - Learning phase: Expanded network (6 conv layers per block) → MSE regression to cached representations + KL on logits with stop-gradient
  - Four search layers: Block 0 output (256×15×15), Block 1 output (256×7×7), Block 2 output (256×3×3), final logits

- Critical path:
  1. Initialize network with Dirac (conv) / Kaiming Uniform (other)
  2. Run evolutionary search with layer-specific hyperparameters (Layer 0: 300 gens, α=0.3; Layer 1-2: 100 gens, α=0.2/0.1; Logits: 10 gens)
  3. Cache best representations per training example
  4. Train expanded network with SGD (lr=4.0, momentum=0.85) using MSE + KL objective (λ=0.03)
  5. Validate and select best checkpoint

- Design tradeoffs:
  - Capacity: Search uses 2 conv layers per block; learning requires 6 conv layers to fit searched targets
  - Supervision scope: "All layers" vs. "Skip block 0" variant—skipping early supervision helps with data augmentation
  - Search compute: More generations/populations improve fitness and diversity but with diminishing returns (saturation observed)

- Failure signatures:
  - Diversity collapse: Insufficient exploratory samples (C_exp) or too-strong selection pressure
  - Unlearnable representations: High-frequency artifacts in searched representations (mitigated by spatial smoothing/blur passes)
  - Generalization gap: Overfitting in search phase produces representations that don't transfer

- First 3 experiments:
  1. **Minimal reproduction**: Run search on a small subset (e.g., 1000 CIFAR-100 images) with reduced population/generations to verify fitness scaling and cache representations correctly.
  2. **Ablate spatial smoothing**: Remove blur passes during mutation and measure both search convergence and downstream regression loss—expect degraded learnability.
  3. **Compare supervision variants**: Train "All layers" vs. "Skip block 0" on CIFAR-10 with augmentation to reproduce the ~2% gap observed in Table 2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can implementing iterative feedback loops between search and learning phases improve performance over the current one-shot caching approach?
- Basis in paper: [explicit] The conclusion states, "Future work should implement tight feedback loops—networks learn searched representations, then trained networks inform the next search iteration."
- Why unresolved: The current architecture caches representations once; it does not update the search targets based on how well the network learns them.
- What evidence would resolve it: A comparison of test accuracy between the current method and a variant where search is re-run using features from the partially trained network.

### Open Question 2
- Question: Does the diversity discovered in searched representations reliably translate into diverse parameter-space solutions (distinct models)?
- Basis in paper: [explicit] The authors note, "more research is needed on how diverse representations translate to diverse model solutions—the ultimate goal."
- Why unresolved: While Section 3.2 measures representation diversity, the paper does not quantify if the resulting learned parameters span meaningfully different regions of parameter space.
- What evidence would resolve it: Measuring parameter-space distances or ensemble performance of multiple models trained on independently searched representations.

### Open Question 3
- Question: How can the performance gap between search-based learning and standard SGD be closed, particularly when using data augmentation?
- Basis in paper: [explicit] The limitations section states, "Future work must bridge this gap for the method to be viable in practice."
- Why unresolved: Search-based learning currently trails SGD (e.g., 2.6% on CIFAR-100 with augmentation), partly because early layers are difficult to supervise effectively.
- What evidence would resolve it: Identifying architectural changes or regularization techniques that allow search-based methods to match or exceed SGD accuracy.

## Limitations

- Computational overhead: Evolutionary search requires multiple generations of per-image evolution, making it significantly slower than standard SGD training
- Performance gap: Search-based learning trails SGD by 1-3% on CIFAR-100 with data augmentation, limiting practical viability
- Scalability uncertainty: Layer-wise sequential search may not scale well to deeper networks or more complex modalities

## Confidence

- **High confidence**: Spatial smoothing during search is crucial for learnability; stop-gradient prevents collapse to standard backpropagation
- **Medium confidence**: Networks can learn exclusively from searched representations; evolutionary search discovers diverse representational solutions
- **Medium confidence**: The decoupling approach achieves competitive generalization, though limited to standard vision datasets

## Next Checks

1. Test representation generalization under distribution shift (e.g., CIFAR-10-C) to verify searched representations encode robust features rather than dataset-specific artifacts.
2. Scale to deeper architectures (e.g., ResNet-18) to evaluate whether layer-wise fixation remains tractable and whether search time scales linearly or exponentially.
3. Compare search-based regression against alternative representation learning methods (e.g., contrastive learning pretraining) to isolate the benefit of evolutionary search versus the two-phase decoupling structure.