---
ver: rpa2
title: 'ProtoGCD: Unified and Unbiased Prototype Learning for Generalized Category
  Discovery'
arxiv_id: '2504.03755'
source_url: https://arxiv.org/abs/2504.03755
tags:
- classes
- learning
- protogcd
- samples
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ProtoGCD introduces a unified and unbiased prototype learning framework
  for generalized category discovery (GCD), addressing the imbalanced accuracy between
  old and new classes in prior methods. It jointly models old and new classes using
  learnable prototypes and unified learning objectives, enabling balanced modeling.
---

# ProtoGCD: Unified and Unbiased Prototype Learning for Generalized Category Discovery

## Quick Facts
- **arXiv ID**: 2504.03755
- **Source URL**: https://arxiv.org/abs/2504.03755
- **Reference count**: 40
- **One-line primary result**: Achieves state-of-the-art performance on GCD tasks, with significantly improved accuracy balance between old and new classes compared to prior methods.

## Executive Summary
ProtoGCD introduces a unified and unbiased prototype learning framework for generalized category discovery (GCD), addressing the imbalanced accuracy between old and new classes in prior methods. It jointly models old and new classes using learnable prototypes and unified learning objectives, enabling balanced modeling. A dual-level adaptive pseudo-labeling mechanism dynamically adjusts pseudo-label types and proportions based on sample confidence and model performance, mitigating confirmation bias. Two regularization terms—entropy maximization and prototype separation—help learn suitable, unbiased representations. ProtoGCD also estimates the number of new classes and extends to detect unseen outliers, achieving task-level unification.

## Method Summary
ProtoGCD uses a unified prototypical modeling approach where old and new classes are jointly modeled within a shared feature space using learnable von Mises-Fisher (vMF) prototypes. The framework employs a dual-level adaptive pseudo-labeling (DAPL) mechanism that assigns hard or soft labels based on sample confidence and dynamically adjusts the proportion of hard labels during training. Two regularization terms (entropy maximization and prototype separation) prevent trivial solutions and enforce inter-class separation. The method extends to estimate the number of new classes and detect outliers, achieving unification across multiple GCD tasks.

## Key Results
- Achieves state-of-the-art performance on standard GCD benchmarks (CIFAR, ImageNet-100, fine-grained datasets)
- Significantly improves accuracy balance between old and new classes compared to parametric baselines
- Effectively handles both generic and fine-grained datasets with consistent performance gains
- Successfully extends to unknown class number estimation and outlier detection tasks

## Why This Works (Mechanism)

### Mechanism 1: Unified Prototypical Modeling
- **Claim:** Joint modeling of old and new classes within a shared prototype space prevents the accuracy imbalance observed in methods using separate classification heads.
- **Mechanism:** Instead of maintaining distinct classifiers for labeled (old) and unlabeled (new) data, the system initializes a single set of learnable prototypes representing von Mises-Fisher (vMF) distributions on a hypersphere. All samples, regardless of labeled status, are mapped to this unified space.
- **Core assumption:** Old and new classes share a compatible geometric structure in the feature space, allowing a single feature extractor to serve both without catastrophic interference.
- **Evidence anchors:** [abstract] "wherein old and new classes are modeled with joint prototypes and unified learning objectives, enabling unified modeling between old and new classes." [section] Section 3.1.2 describes the "Unified Feature Space and Prototypes" and the formulation of prototypes as vMF distributions.
- **Break condition:** If new classes possess semantic features entirely orthogonal to old classes, the shared feature extractor may fail to separate them, leading to collapsed clusters.

### Mechanism 2: Confidence-Gated Pseudo-Labeling (DAPL)
- **Claim:** Adapting the "hardness" of pseudo-labels based on sample confidence and training progress mitigates confirmation bias.
- **Mechanism:** The "Dual-Level Adaptive Pseudo-Labeling" (DAPL) operates on two axes. First (Level-1), it assigns hard one-hot pseudo-labels to high-confidence samples (determined by the ratio of top-1 vs. top-2 prototype similarity) and soft labels to low-confidence ones. Second (Level-2), it linearly increases the proportion of samples receiving hard labels over the first `eramp` epochs.
- **Core assumption:** The model's predictions become more reliable as training progresses, justifying a shift from soft exploration to hard exploitation.
- **Evidence anchors:** [abstract] "...dual-level adaptive pseudo-labeling mechanism dynamically adjusts pseudo-label types and proportions... mitigating confirmation bias." [section] Section 3.3 details the "Prototype Confidence" definition (Eq. 8) and the ramp-up function (Eq. 10).
- **Break condition:** If the initial feature extractor is poor, even "high confidence" samples might be wrong, causing the ramp-up to reinforce errors early.

### Mechanism 3: Geometric and Entropic Regularization
- **Claim:** Regularization prevents trivial clustering solutions (collapse) and enforces inter-class separation.
- **Mechanism:** The system uses two losses: (1) **Marginal Entropy Maximization** ($L_{entropy}$) forces the model to predict a uniform distribution across the batch, acting as a soft constraint against class collapse; (2) **Prototype Separation** ($L_{sep}$) explicitly maximizes the angular distance between distinct prototypes in the feature space.
- **Core assumption:** The data does not follow an extreme long-tailed distribution that would fundamentally conflict with the uniformity prior of entropy maximization.
- **Evidence anchors:** [abstract] "...two regularization terms—entropy maximization and prototype separation—help learn suitable, unbiased representations." [section] Section 3.4 defines $L_{entropy}$ and $L_{sep}$; Table 8 shows performance degradation when entropy is removed (Row d vs Row g).
- **Break condition:** On heavily skewed datasets, strong entropy maximization may force the model to over-split dominant classes or merge rare ones to satisfy uniformity.

## Foundational Learning

**Concept:** von Mises-Fisher (vMF) Distribution
- **Why needed here:** Unlike standard linear classifiers, ProtoGCD models class prototypes as distributions on a hypersphere. Understanding vMF is required to grasp why the method uses cosine similarity and how it calculates posterior probabilities (Eq. 2).
- **Quick check question:** How does normalizing features and prototypes to unit length change the loss landscape compared to standard Euclidean distance?

**Concept:** Confirmation Bias in Semi-Supervised Learning
- **Why needed here:** The paper explicitly positions DAPL as a solution to confirmation bias (error reinforcement). You must understand that early errors in pseudo-labeling can snowball if not checked by soft labels or uncertainty thresholds.
- **Quick check question:** Why is assigning a hard pseudo-label to a low-confidence sample dangerous in the early epochs of training?

**Concept:** Clustering Accuracy (Hungarian Algorithm)
- **Why needed here:** GCD evaluation uses "All," "Old," and "New" accuracy, calculated via Hungarian matching rather than standard Top-1 accuracy, because cluster indices are arbitrary and must be aligned to ground truth.
- **Quick check question:** Why can't we use standard classification accuracy directly to evaluate the discovery of novel classes?

## Architecture Onboarding

**Component map:** Encoder (ViT-B/16) -> Projection Head -> Prototypes (vMF distributions) -> DAPL Module

**Critical path:**
1. **Batching:** Mix labeled ($D_l$) and unlabeled ($D_u$) data
2. **Contrastive View:** Generate two augmentations; compute $L_{con}$ (SupCon + SimCLR)
3. **Prototype View:** Compute feature-prototype similarities
4. **Pseudo-Labeling:** Apply DAPL logic (Hard/Soft assignment based on confidence and epoch)
5. **Optimization:** Jointly update Encoder, Head, and Prototypes using $L = L_{con} + L_{cls} + L_{reg}$

**Design tradeoffs:**
- **Entropy Weight ($\lambda_{entropy}$):** Set to ~2.0 generally, but may need reduction for long-tailed data to avoid enforcing false uniformity
- **Ramp-up Epochs ($e_{ramp}$):** Set to ~100. Too short leads to premature hard-labeling (bias); too long slows convergence

**Failure signatures:**
- **Collapse:** "All" accuracy is high, but "New" accuracy is near 0% (all samples map to one cluster). *Check:* Is $L_{entropy}$ active?
- **Imbalance:** "Old" accuracy > 90%, "New" < 20%. *Check:* Is the unified classifier actually sharing weights, or is the model overfitting labeled data?

**First 3 experiments:**
1. **Baseline Reproduction:** Run on CIFAR-100 with standard splits. Verify that the gap between "Old" and "New" accuracy is smaller than parametric baselines like UNO+.
2. **Ablation on DAPL:** Disable the ramp-up (fix hard-label ratio at 100% from epoch 0). Observe the drop in "New" class accuracy to confirm the role of adaptive bias mitigation (Fig 6).
3. **Hyper-parameter Sensitivity:** Vary $\lambda_{entropy}$ (e.g., 0.0, 0.5, 2.0) on a long-tailed dataset (e.g., Herbarium19) to observe the trade-off between avoiding collapse and fitting the true distribution (Fig 9).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can GCD frameworks be adapted to effectively handle long-tailed data distributions without relying on strong uniformity assumptions?
- **Basis in paper:** [explicit] Section 7 explicitly identifies "design[ing] more suitable methods for long-tailed distributions" as future work; [inferred] Appendix F discusses the conflict where the entropy regularization ($L_{entropy}$) imposes a uniform prior that degrades performance on the long-tailed Herbarium19 dataset.
- **Why unresolved:** The current method relies on a uniform prior for clustering stability, which contradicts real-world data skews, requiring manual weight tuning ($\lambda_{entropy}$) that may not generalize across different imbalance factors.
- **What evidence would resolve it:** A mechanism that dynamically adjusts the regularization prior based on the estimated distribution skew, maintaining performance on Herbarium19 without manual weight tuning.

### Open Question 2
- **Question:** How can the confidence gap between labeled old classes and unlabeled new classes be systematically calibrated to improve pseudo-label quality?
- **Basis in paper:** [explicit] Section 7 calls for future work to "calibrate the confidence for both old and new classes."
- **Why unresolved:** As noted in Appendix F, a disparity exists between the model's marginal probabilities ($p$) and the actual class ratios ($r$); the model tends to be over-confident on old classes due to partial labeling, potentially biasing the Dual-level Adaptive Pseudo-Labeling (DAPL) mechanism.
- **What evidence would resolve it:** Demonstrating that a calibration technique reduces the gap between predicted confidence and empirical accuracy for old classes, thereby improving the "New" class accuracy in DAPL.

### Open Question 3
- **Question:** Can the framework be extended to actively filter outliers from the unlabeled training set during the discovery process?
- **Basis in paper:** [explicit] Section 7 lists "filtering out outliers in training data" as a specific future research direction.
- **Why unresolved:** While ProtoGCD successfully extends to OOD detection at inference (Section 5), the current method assumes the unlabeled training set $D_u$ consists solely of in-distribution samples ($C_{old} \cup C_{new}$), lacking a mechanism to clean $D_u$ of semantic outliers during training.
- **What evidence would resolve it:** An extension of the method that successfully rejects semantic outliers present in $D_u$ while maintaining clustering performance on the valid new classes.

## Limitations

- The unified prototype framework assumes old and new classes share compatible geometric structure in feature space, which may not hold for datasets with entirely novel semantic features or extreme domain shifts
- DAPL's effectiveness critically depends on accurate confidence estimation; poor initial feature extractor quality can lead to early error reinforcement during ramp-up
- Entropy maximization regularization assumes non-extreme long-tailed distributions; on heavily skewed data it may force over-splitting of dominant classes or merging of rare ones
- Implementation details for unknown class number estimation and outlier detection extensions are not fully specified, requiring additional 3-epoch training loops with unclear weight handling

## Confidence

- **High Confidence**: The core mechanism of unified prototype modeling (jointly modeling old and new classes with shared prototypes) is well-defined and supported by the formulation in Section 3.1.2. The reported performance gains over parametric baselines like UNO+ on standard benchmarks (CIFAR, ImageNet-100, fine-grained datasets) are specific and verifiable.
- **Medium Confidence**: The DAPL (Dual-Level Adaptive Pseudo-Labeling) mechanism is explicitly detailed in Section 3.3 with equations for confidence calculation and the ramp-up function. However, its critical dependence on the quality of the initial feature extractor and the potential for early error reinforcement introduce uncertainty in its universal applicability.
- **Medium Confidence**: The geometric regularization (entropy maximization and prototype separation) is formally defined in Section 3.4 with clear loss functions. The effectiveness is supported by ablation studies (e.g., Table 8), but its interaction with highly imbalanced or long-tailed data distributions is not fully explored.

## Next Checks

1. **Baseline Imbalance Reproduction**: Run ProtoGCD on CIFAR-100 with the standard GCD split. Verify that the gap between "Old" and "New" accuracy is significantly smaller than parametric baselines like UNO+, confirming the paper's claim of improved balance.

2. **DAPL Ablation Confirmation**: Disable the ramp-up function in DAPL, fixing the hard-label ratio at 100% from epoch 0. Observe and document the drop in "New" class accuracy to empirically confirm the role of the adaptive mechanism in mitigating confirmation bias.

3. **Hyper-parameter Sensitivity Test**: Conduct experiments on a long-tailed dataset (e.g., Herbarium19) by varying $\lambda_{entropy}$ (e.g., 0.0, 0.5, 2.0). Analyze the trade-off between avoiding trivial clustering solutions and fitting the true class distribution, particularly focusing on the impact on rare class discovery.