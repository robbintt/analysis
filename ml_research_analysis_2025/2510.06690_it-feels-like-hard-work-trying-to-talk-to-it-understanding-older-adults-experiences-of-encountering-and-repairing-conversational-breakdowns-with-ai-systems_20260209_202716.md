---
ver: rpa2
title: '"It feels like hard work trying to talk to it": Understanding Older Adults''
  Experiences of Encountering and Repairing Conversational Breakdowns with AI Systems'
arxiv_id: '2510.06690'
source_url: https://arxiv.org/abs/2510.06690
tags:
- conversational
- older
- systems
- adults
- breakdowns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study identifies four types of conversational breakdowns between
  older adults and AI systems, analyzing 844 interactions from a 20-week in-home deployment.
  Participants experienced breakdowns in language/semantics, flow, historical understanding,
  and explanations, with 38% of interactions affected.
---

# "It feels like hard work trying to talk to it": Understanding Older Adults' Experiences of Encountering and Repairing Conversational Breakdowns with AI Systems

## Quick Facts
- arXiv ID: 2510.06690
- Source URL: https://arxiv.org/abs/2510.06690
- Authors: Niharika Mathur; Tamara Zubatiy; Agata Rozga; Elizabeth Mynatt
- Reference count: 40
- Primary result: 38% of 844 voice assistant interactions with older adults experienced breakdowns across four categories

## Executive Summary
This study investigates conversational breakdowns between older adults and AI systems during medication management tasks through a 20-week in-home deployment. Analyzing 844 interactions and post-deployment interviews with 14 participants, researchers identified four breakdown types: language/semantics, flow, historical understanding, and explanations. Older adults employed various repair strategies, often drawing on environmental resources, but found the cognitive effort taxing. The findings highlight the need for AI systems to better integrate contextual knowledge and memory while aligning explanations with user routines and expectations.

## Method Summary
The study employed a qualitative analysis of interaction logs and interview transcripts from a 20-week deployment of a voice-based AI system (MATCHA) with 7 older adult dyads. Researchers collected 844 interaction logs from Google Assistant and conducted thematic analysis to identify breakdown types and user-initiated repair strategies. The analysis used inductive coding for breakdown identification and deductive coding based on literature-derived strategies, with thematic analysis of interviews providing additional context for user experiences and repair approaches.

## Key Results
- 38% of all interactions experienced conversational breakdowns
- Four breakdown categories identified: language/semantics (27%), flow (17%), historical understanding (22%), and explanations (35%)
- Users employed 19 distinct repair strategies, often involving environmental resources like pillboxes
- Cognitive effort required for repairs was significant, with users describing the experience as "hard work"

## Why This Works (Mechanism)

### Mechanism 1: The Breakdown-Repair Cascade
When conversational misalignment occurs, older adults engage in a specific, escalating sequence of repair strategies (e.g., code-switching, repetition, external scaffolding) before abandoning the task. The interaction fails when the AI response violates user expectations (a "seam"). Users attempt to patch this seam first by modifying their own speech (hyper-articulation), then by simplifying commands, and finally by introducing external resources (checking a physical pillbox), as seen in Table 3.

### Mechanism 2: Distributed Memory Offloading
Effective medication management in this demographic relies on a "sociotechnical network" (caregivers, pillboxes, calendars) rather than isolated AI memory; the AI must bridge to these external nodes. The paper argues that "Historical Understanding" breakdowns occur because the AI cannot access the distributed cognition of the home. Participants explicitly desired AI integration with physical tools (e.g., a "smart pillbox" mentioned by CG2) to serve as external memory aids.

### Mechanism 3: Anthropomorphic Expectation Mismatch
Trust and perceived usability are mediated by the gap between the AI's social presentation (voice, persona) and its functional rigidity (lack of memory/flow). Older adults tend to anthropomorphize voice assistants ("Speaking like a Human," Section 7.3.4). When the AI speaks naturally but acts "mechanically" (e.g., ignoring context), it creates a dissonance that users interpret as "disrespect" or "hard work," leading to abandonment.

## Foundational Learning

- **Concept: Seamful Design (Vertesi)**
  - Why needed here: The paper uses this framework to reframe "breakdowns" not as errors to be hidden, but as "seams" that reveal where users need support. Understanding this helps designers see user struggles as signals for missing context.
  - Quick check question: When the AI fails to answer "Did I take my meds?", is the fix to improve the speech recognition, or to connect the AI to the physical pillbox?

- **Concept: The Gulf of Execution/Evaluation (Norman/Luger)**
  - Why needed here: Explains the cognitive load described in the paper. The "Gulf" is the distance between the user's intent ("I want to know if I took my pill") and the system's obscure state ("I don't know how to help").
  - Quick check question: Does the system provide feedback that bridges the gap between what the user said and what the system understood?

- **Concept: Contextual Integrity**
  - Why needed here: Crucial for understanding the "Explanation" breakdown. Users expect the AI to adhere to the context of the home (privacy, routine, history).
  - Quick check question: If the AI suggests a medication change, does it explain *why* based on the specific context (e.g., "You missed yesterday") or generic logic?

## Architecture Onboarding

- **Component map:** Input Layer: Voice Recognition (Google Home) + Physical State (Smart Pillbox/Calendar integration - *Proposed*) -> State Manager: User Profile + Interaction History (Long-term memory - *Missing in study*) -> Dialog Manager: Policy Engine (handles the 6 scenarios) + Repair Handler (detects breakdown loops) -> Output Layer: Natural Language Generation (Tone/Persona modulation)

- **Critical path:** The "Unknown" scenario path. The study highlights that 38% of interactions were breakdowns, many falling into the "Unknown" or error state. The system's ability to gracefully exit or repair from "Unknown" is the primary failure point.

- **Design tradeoffs:**
  - *Rigidity vs. Flexibility:* The system used a rigid policy (6 scenarios) for safety (medication). This reduced "hallucinations" but increased "Flow" breakdowns.
  - *Privacy vs. Context:* Storing "Historical Understanding" requires keeping interaction logs, which creates privacy risks in a home setting.

- **Failure signatures:**
  - **The Repetition Loop:** User repeats the same query ("Did I take it?") 3+ times without new information.
  - **The "I don't know" Dead End:** System falls back to generic error handling when context (history) is missing.
  - **Code-Switching Signal:** User shifts from natural language ("Have I taken my heart pill?") to robotic keywords ("Medication status").

- **First 3 experiments:**
  1. **Wizard-of-Oz Memory Injection:** Have a human researcher inject "historical context" (e.g., "You took it at 8 AM") into the AI response in real-time to measure if "Historical Understanding" repair reduces cognitive load.
  2. **Smart Pillbox Mockup:** Deploy a fake "sensor" on the pillbox that lights up when the AI asks about meds. Test if the visual cue reduces "Explanation" breakdowns (giving the user tangible proof).
  3. **Persona Variation:** A/B test the "Apology" mechanism. Does a "polite/social" apology ("I'm sorry, I forgot") vs. a "functional" one ("Data not found") change the repair strategy or abandonment rate?

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do interaction approaches and repair strategies differ for older adults with no prior exposure to the AI system compared to experienced users?
- **Basis in paper:** The authors state in the Limitations section that "Interaction approaches for participants with no prior exposure to the AI remains an open area for investigation."
- **Why unresolved:** The study sample consisted entirely of participants recruited from a clinical program who were already familiar with the AI system from prior studies, potentially biasing their willingness and ability to perform repairs.
- **What evidence would resolve it:** Comparative data from a deployment study involving AI-naive older adults to analyze differences in repair frequency and strategy complexity.

### Open Question 2
- **Question:** Which specific types of AI-driven explanations are most preferred by older adults across varying contexts of use within the home?
- **Basis in paper:** The authors outline a goal to "understanding which types of explanations are most preferred in varying contexts of use" in their future work section.
- **Why unresolved:** While the study identifies "Breakdowns in Explanations" as a category, it does not determine which specific explanatory frameworks successfully align with older adults' sociotechnical contexts and cognitive needs.
- **What evidence would resolve it:** Evaluation results from a user study testing different explanation styles (e.g., context-aware vs. algorithmic) on user confidence and task success rates.

### Open Question 3
- **Question:** To what extent do Large Language Models (LLMs) mitigate identified conversational breakdowns versus introducing new types of "social misattribution"?
- **Basis in paper:** The discussion section reflects on the need to examine whether "Generative AI's sophisticated language abilities will inherently resolve conversational breakdowns" or introduce new ones.
- **Why unresolved:** The study was conducted using a rule-based Google Conversational Action; it is unknown if the fluidity of LLMs solves flow/historical issues or exacerbates trust issues through hallucinations or "role-playing."
- **What evidence would resolve it:** Comparative interaction analysis between older adults using standard voice assistants versus LLM-powered agents to measure breakdown recovery rates and trust calibration.

## Limitations

- Findings based on a single AI platform (Google Assistant) with a specific medication management design
- Analysis relies on text transcripts that cannot capture non-verbal cues, tone, or full physical interaction context
- Study population already familiar with AI from prior studies, potentially biasing repair strategy effectiveness

## Confidence

- **High Confidence**: The identification of four distinct breakdown types is well-supported by the 844 interaction logs and consistent with established HCI literature on conversational agents.
- **Medium Confidence**: The characterization of user-initiated repair strategies is robust, but the mapping between specific strategies and breakdown types could benefit from additional validation across different user populations.
- **Medium Confidence**: The claim that 38% of interactions experienced breakdowns is statistically valid for this dataset, but may not generalize to systems with different architectures or user interfaces.

## Next Checks

1. **Cross-System Replication**: Deploy the same medication management task on a different voice platform (e.g., Amazon Alexa) with 20+ older adult participants for 8+ weeks to test if breakdown patterns persist across systems.

2. **Wizard-of-Oz Memory Injection**: Conduct a controlled experiment where a human confederate injects contextual information (e.g., "You took it at 8 AM") into AI responses during live interactions to measure the impact on "Historical Understanding" breakdowns and cognitive load.

3. **Multi-Modal Integration Test**: Implement a smart pillbox prototype that provides visual feedback when queried by the AI, then test whether this external memory aid reduces "Explanation" breakdowns compared to voice-only interactions.