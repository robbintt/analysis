---
ver: rpa2
title: 'Facts Do Care About Your Language: Assessing Answer Quality of Multilingual
  LLMs'
arxiv_id: '2506.03051'
source_url: https://arxiv.org/abs/2506.03051
tags:
- language
- languages
- llama
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the factuality of the Llama3.1 model family
  in answering middle and high school level factual questions across 12 languages.
  The researchers created a benchmark of 54 factual questions with hand-curated target
  keywords, then evaluated responses from Llama3.1-8B and 70B models using both manual
  bilingual evaluation and automated keyword analysis.
---

# Facts Do Care About Your Language: Assessing Answer Quality of Multilingual LLMs

## Quick Facts
- arXiv ID: 2506.03051
- Source URL: https://arxiv.org/abs/2506.03051
- Reference count: 6
- Major result: Low-resource languages receive significantly poorer quality responses from multilingual LLMs compared to high-resource languages

## Executive Summary
This study evaluates the factuality of Llama3.1 models across 12 languages using a benchmark of 54 factual questions. The researchers found substantial disparities in response quality, with English achieving 94.44% keyword coverage while low-resource languages like Tulu and Māori showed only 31.77% and 40.71% coverage respectively. Statistical analysis revealed significant positive correlations between language speaker population and both target word detection and overall response quality, indicating systematic biases against underrepresented languages.

## Method Summary
The researchers created a benchmark of 54 factual questions with hand-curated target keywords, then evaluated responses from Llama3.1-8B and 70B models using both manual bilingual evaluation and automated keyword analysis. Manual evaluation was conducted by 3 raters who scored responses for incorrectness and extraneous information, while automated analysis measured keyword coverage. The study covered 12 languages ranging from high-resource languages like English to low-resource languages like Tulu and Māori.

## Key Results
- English responses achieved 94.44% keyword coverage with low incorrectness (1.04) and extraneous information (1.35) scores
- Low-resource languages Tulu and Māori showed significantly worse performance at 31.77% and 40.71% keyword coverage respectively
- Statistical analysis revealed significant positive correlations (p < 0.05) between language speaker population and both target word detection and overall response quality

## Why This Works (Mechanism)
The disparity in LLM performance across languages stems from fundamental imbalances in training data availability and quality. High-resource languages benefit from abundant web data, structured knowledge bases, and community contributions, while low-resource languages suffer from limited digital content and representation. This creates a feedback loop where better-performing languages receive more attention and resources, further widening the quality gap.

## Foundational Learning
1. **Language resource disparity**: Understanding the fundamental differences in digital content availability across languages is crucial for contextualizing LLM performance gaps
   - Why needed: Provides baseline understanding of why certain languages perform better
   - Quick check: Compare Wikipedia article counts across languages

2. **Multilingual model architecture**: Knowledge of how LLMs handle multiple languages through shared representations and tokenization
   - Why needed: Essential for understanding cross-language transfer capabilities
   - Quick check: Examine tokenizer vocabulary distribution across languages

3. **Evaluation methodology**: Understanding both automated keyword analysis and manual evaluation approaches
   - Why needed: Critical for interpreting study results and potential biases
   - Quick check: Compare keyword-based vs. semantic evaluation approaches

## Architecture Onboarding
Component map: Training data collection -> Tokenization -> Model training -> Inference -> Evaluation
Critical path: Data → Model → Response → Evaluation
Design tradeoffs: Model size vs. language coverage vs. response quality
Failure signatures: Keyword mismatch, hallucination, incomplete responses
First experiments:
1. Test individual language performance in isolation
2. Compare keyword coverage across model sizes
3. Analyze correlation between training data quantity and response quality

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Manual evaluation conducted by only 3 raters, potentially introducing subjective bias
- Keyword-based evaluation may not capture nuanced aspects of answer quality
- Benchmark questions may not represent full complexity of real-world queries
- Correlation between speaker population and quality doesn't establish causation

## Confidence
High confidence in major claim of performance disparity between high-resource and low-resource languages.
Medium confidence in specific correlation between speaker population and answer quality due to potential confounding factors.

## Next Checks
1. Expand manual evaluation to include more raters and different cultural backgrounds
2. Develop comprehensive evaluation framework beyond keyword matching
3. Conduct ablation studies to isolate effects of training data quantity vs. quality