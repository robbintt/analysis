---
ver: rpa2
title: 'From Conversation to Orchestration: HCI Challenges and Opportunities in Interactive
  Multi-Agentic Systems'
arxiv_id: '2506.20091'
source_url: https://arxiv.org/abs/2506.20091
tags:
- systems
- agent
- agents
- user
- multi-agentic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies design challenges for end-user interaction
  with multi-agentic systems featuring hierarchical architectures, where a supervisor
  agent coordinates specialized sub-agents. Through analysis of existing frameworks
  and interfaces, the authors present six key challenges: reducing opaqueness in agent
  teamwork, supporting parallel agent interactions, managing emergent complexity,
  resolving agent conflicts, supporting appropriate mental models, and navigating
  trust and explainability.'
---

# From Conversation to Orchestration: HCI Challenges and Opportunities in Interactive Multi-Agentic Systems

## Quick Facts
- arXiv ID: 2506.20091
- Source URL: https://arxiv.org/abs/2506.20091
- Reference count: 40
- Primary result: Identifies six HCI design challenges for hierarchical multi-agent systems with supervisor agents coordinating specialized sub-agents.

## Executive Summary
This paper identifies key human-computer interaction challenges in hierarchical multi-agentic systems, where a supervisor agent coordinates specialized sub-agents. Through analysis of existing frameworks and interfaces, the authors present six critical challenges: reducing opaqueness in agent teamwork, supporting parallel agent interactions, managing emergent complexity, resolving agent conflicts, supporting appropriate mental models, and navigating trust and explainability. Using a hypothetical marketplace scenario, the paper proposes design considerations including group cards for mental models, conflict resolution mechanisms, and orchestration interfaces. The work establishes a research agenda for HCI as these systems become accessible to non-technical users.

## Method Summary
The paper employs qualitative analysis of six existing multi-agent frameworks (LangGraph, CrewAI, AutoGen, OpenAI Agents, MetaGPT, Magentic-One) combined with a hypothetical online marketplace scenario to identify HCI design challenges. The approach extrapolates from current tool capabilities to conceptualize future interface requirements, proposing solutions like group cards and orchestration panels without implementing them. The methodology relies on conceptual scenario extrapolation and literature review rather than empirical user studies or technical implementation.

## Key Results
- Hierarchical architectures introduce six distinct HCI challenges including opaqueness, parallel interactions, emergent complexity, and conflict resolution
- Group cards may support appropriate mental models better than individual agent cards in high-density systems
- Visualization of inter-agent negotiation (roundtable views) is likely necessary for effective conflict resolution
- The paper establishes a research agenda for designing user-centered interfaces as multi-agent systems become more accessible

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Abstraction via Orchestration
A hierarchical architecture reduces user cognitive load by abstracting inter-agent complexity into a single "Orchestrator" interface, provided the supervisor agent accurately interprets user intent. The system inserts a supervisor agent between the user and specialized sub-agents, parsing high-level goals and delegating tasks while presenting a unified control panel.

### Mechanism 2: Mental Model Formation via Group Cards
Grouping agents into functional categories via "group cards" supports appropriate mental models better than individual agent cards in high-density systems. Users infer capabilities from group labels (e.g., "Finance Team"), reducing information processing burden.

### Mechanism 3: Conflict Visibility via Roundtable Views
Visualizing inter-agent negotiation through "roundtable views" is necessary to resolve conflicts where agents have competing goals. This transparency allows users to act as final arbiters when specialized agents disagree.

## Foundational Learning

- **Concept: Supervisor Agent (Orchestrator)**
  - Why needed: This is the central component of the hierarchical architecture discussed in the paper
  - Quick check: Can you distinguish between a single general-purpose agent and a supervisor agent that delegates to specialized sub-agents?

- **Concept: Emergent Complexity (Cascading Failures)**
  - Why needed: The paper emphasizes that multi-agent systems introduce risks where one agent's error can propagate through the network
  - Quick check: If a "Data Retrieval" agent passes corrupted information to an "Analysis" agent, is the resulting error localized or systemic?

- **Concept: Human-in-the-Loop (HITL) for Alignment**
  - Why needed: The paper argues that full autonomy is risky in multi-agent setups
  - Quick check: Why might a user need to pause a system execution even if the agents are functioning correctly?

## Architecture Onboarding

- **Component map:** User Interface -> Orchestrator Agent -> Specialized Sub-Agents -> Shared Memory/Context

- **Critical path:**
  1. User inputs high-level goal
  2. Orchestrator decomposes goal into sub-tasks
  3. Orchestrator delegates sub-tasks to Specialized Agents (potentially in parallel)
  4. Agents execute; Conflict Resolution or Interrupt triggers if defined or if errors occur
  5. Orchestrator synthesizes outputs
  6. User reviews final result (or intermediate steps if visibility is enabled)

- **Design tradeoffs:**
  - Transparency vs. Cognitive Load: Displaying all agent logs vs. showing only Orchestrator's summary
  - Autonomy vs. Control: Allowing agents to resolve conflicts automatically vs. requiring user approval
  - Abstraction vs. Debugging: Hiding multi-agent nature behind single chat vs. exposing node graph

- **Failure signatures:**
  - Silent Failure: Orchestrator reports success, but sub-agent logic was flawed
  - Conflict Loop: Two agents repeatedly rejecting each other's outputs without resolution
  - Rogue Behavior: Agent creates unauthorized sub-agent or accesses tools outside scope
  - Stale Context: Agents working in parallel use outdated information shared by Orchestrator

- **First 3 experiments:**
  1. **Orchestration Latency Test:** Measure time delay and error rate when Orchestrator manages 3 agents vs. 10 agents
  2. **Conflict Visibility A/B Test:** Compare user ability to identify and fix errors when seeing only final result vs. negotiation process
  3. **Interrupt Injection:** Test if system state is preserved accurately enough for user to modify prompt and resume without restarting

## Open Questions the Paper Calls Out

### Open Question 1
How can orchestration interfaces be designed to balance transparency and user control without introducing overwhelming complexity? The authors ask how to design orchestration interfaces without high complexity given various agents and scaffolding needs. This remains unresolved because current interfaces force a trade-off between hiding agent teamwork (causing opaqueness) and exposing all internal states (causing cognitive overload).

### Open Question 2
How does system trust evolve when errors occur at different layers of the hierarchy, particularly when supervisor agents abstract or hide sub-agent failures? The paper explicitly asks about trust evolution when errors arise at different hierarchy layers. This is unresolved because trust research has focused on single-agent systems, and the impact of hierarchical delegation on trust calibration is unknown.

### Open Question 3
What interface mechanisms allow users to effectively prioritize or interrupt parallel tasks across agents without losing control of the system? The authors pose how to allow users to prioritize or interrupt parallel tasks without losing overall system control. This remains unresolved because existing pause features do not scale to scenarios with multiple agents executing tasks concurrently.

## Limitations
- All six challenges and proposed solutions remain theoretical extrapolations without empirical validation
- Critical technical details for reproducing the hierarchical orchestration architecture are unspecified
- The "group cards" mental model mechanism lacks validation for how users actually infer capabilities from functional labels

## Confidence
- **High confidence**: The identification of opaqueness in hierarchical agent systems as a genuine user experience challenge
- **Medium confidence**: The parallel interaction challenge - logically sound but specific failure modes remain speculative
- **Low confidence**: The proposed conflict resolution mechanisms (roundtable views) - conceptual sketches without evidence of effectiveness

## Next Checks
1. **Orchestration Latency Measurement**: Implement a hierarchical multi-agent system with 3-10 specialized agents performing parallel tasks. Measure end-to-end latency and error rates compared to equivalent single-agent approaches.
2. **Conflict Visibility Experiment**: Create a controlled scenario where two agents must negotiate conflicting objectives. Test two interface conditions: (A) users see only final output, (B) users see the negotiation process. Measure user ability to identify root causes and correct errors.
3. **Mental Model Formation Study**: Develop two interface variants for a multi-agent system: one showing individual agent cards with detailed capabilities, another using grouped functional categories. Test whether users can accurately predict system behavior and make appropriate trust decisions in each condition.