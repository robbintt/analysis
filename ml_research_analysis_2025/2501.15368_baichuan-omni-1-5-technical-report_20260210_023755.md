---
ver: rpa2
title: Baichuan-Omni-1.5 Technical Report
arxiv_id: '2501.15368'
source_url: https://arxiv.org/abs/2501.15368
tags:
- arxiv
- audio
- data
- zhang
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Baichuan-Omni-1.5 is an omni-modal model that achieves seamless,
  high-quality interaction across text, image, audio, and video modalities without
  compromising performance in any modality. The model integrates a comprehensive data
  cleaning and synthesis pipeline yielding about 500B high-quality multimodal data,
  an 8-layer Residual Vector Quantization (RVQ) audio tokenizer designed to capture
  both semantic and acoustic information at 12.5 Hz frame rate, and a multi-stage
  training strategy that progressively integrates multimodal alignment and multitask
  fine-tuning.
---

# Baichuan-Omni-1.5 Technical Report

## Quick Facts
- arXiv ID: 2501.15368
- Source URL: https://arxiv.org/abs/2501.15368
- Reference count: 40
- Baichuan-Omni-1.5 achieves seamless, high-quality interaction across text, image, audio, and video modalities without compromising performance in any modality.

## Executive Summary
Baichuan-Omni-1.5 is an omni-modal model achieving state-of-the-art performance across text, image, audio, and video understanding and generation. The model integrates a comprehensive data cleaning and synthesis pipeline yielding about 500B high-quality multimodal data, an 8-layer Residual Vector Quantization (RVQ) audio tokenizer designed to capture both semantic and acoustic information at 12.5 Hz frame rate, and a multi-stage training strategy that progressively integrates multimodal alignment and multitask fine-tuning. Baichuan-Omni-1.5 leads contemporary models including GPT4o-mini and MiniCPM-o 2.6 on comprehensive omni-modal capabilities, achieving an average score of 73.3 across ten image-understanding benchmarks and 83.8% on the OpenMM-Medical benchmark. The model also demonstrates strong performance in pure language tasks, scoring 72.2 on MMLU compared to Llama3-Instruct's 67.1%, while maintaining robust audio understanding and generation capabilities.

## Method Summary
Baichuan-Omni-1.5 employs a four-stage training strategy: (1) Image-text pretraining with visual projector alignment and gradual LLM unfreezing while maintaining 40% pure text ratio; (2) Image-audio-text alignment using interleaved data to mitigate modality conflict, training audio adapters and head with frozen LLM; (3) Omni-modal pretraining with all parameters and 64k context, including video at 1fps; (4) SFT split between understanding (freeze audio head) and generation (audio head only). The model uses a Qwen2-VL NaViT encoder, an 8-layer RVQ audio tokenizer at 12.5 Hz frame rate, and a separate audio head with 3-layer depth transformer. Training leverages approximately 500B tokens from diverse multimodal datasets including 887k hours of audio, 71.3M images, and 31M video samples.

## Key Results
- Average score of 73.3 across ten image-understanding benchmarks, surpassing GPT-4o-mini by 6 points
- OpenMM-Medical benchmark score of 83.8%, surpassing Qwen2-VL-72B's 80.7%
- MMLU score of 72.2, outperforming Llama3-Instruct's 67.1%
- Robust audio understanding and generation capabilities while maintaining strong text performance

## Why This Works (Mechanism)

### Mechanism 1
Interleaved audio-text pretraining mitigates modality conflict better than paired-only data. By presenting alternating audio and text segments within single sequences (segmented by punctuation), the model learns cross-modal correspondences without forcing one modality to dominate the representation space. This preserves textual knowledge while integrating audio. Core assumption: Modality conflict arises from feature distribution mismatch between speech and text; interleaving creates smoother transitions in shared embedding space.

### Mechanism 2
Multi-stage progressive unfreezing preserves base LLM capabilities while adding modalities. Each modality is first aligned via frozen LLM (training only adapters/projectors), then gradually unfrozen with lower learning rates. Pure text data maintained at 40% ratio throughout prevents catastrophic forgetting. Core assumption: Frozen LLM provides stable semantic anchor; adapter layers can map modalities into this space without disrupting learned representations.

### Mechanism 3
8-layer RVQ at 12.5 Hz captures both semantic and acoustic information for end-to-end audio I/O. Whisper Large Encoder extracts high-level semantic features; residual convolution downsamples to 12.5 Hz; 8-layer RVQ quantizes into discrete tokens fed to both LLM (for transcript prediction) and audio decoder (for Mel reconstruction). Multi-objective training forces tokens to serve both purposes. Core assumption: A single token sequence can simultaneously encode sufficient semantic detail for LLM understanding AND acoustic detail for reconstruction.

## Foundational Learning

- **Residual Vector Quantization (RVQ)**: Core to audio tokenizer; understanding how residual layers progressively refine quantization explains why 8 layers capture both semantics and acoustics. Quick check: Can you explain why each additional RVQ layer captures finer residual details rather than redundant information?

- **Flow Matching for Generative Modeling**: Audio decoder uses flow matching (not diffusion); understanding continuous normalizing flows explains how Mel spectrograms are generated. Quick check: How does flow matching differ from diffusion in terms of ODE trajectory and sampling efficiency?

- **Catastrophic Forgetting in Multimodal LLMs**: Explains why 40% pure text ratio and progressive unfreezing are necessary; without this, multimodal training degrades text performance. Quick check: Why does adding new modalities degrade original text task performance, and what training interventions mitigate this?

## Architecture Onboarding

- Component map: Input → [Visual: NaViT encoder → 2-layer MLP projector] → [Audio: Whisper Encoder → Residual Conv → 8-layer RVQ → Audio Embed Layer] → [Text: Standard tokenizer] → LLM Decoder (7B, pretrained) → [LM Head → Text tokens] → [Audio Head → 3-layer Depth Transformer + 8 classification heads] → [Flow Matching Decoder → HiFi-GAN Vocoder → Audio output]

- Critical path: 1. Audio tokenizer training: Whisper encoder frozen; train residual conv + RVQ with multi-objective loss (Mel reconstruction + transcript prediction via frozen LLM) 2. Vision alignment: Stage 1.1 projector-only, Stage 1.2 full unfreeze with 40% text 3. Audio integration: Stage 2.1 audio adapters only, Stage 2.2 partial unfreeze 4. Omni-modal pretrain: All parameters, 64k context, cross-modal data 5. SFT split: Understanding (freeze audio head) → Generation (audio head only)

- Design tradeoffs: 12.5 Hz vs higher frame rate: Lower rate improves efficiency but may lose acoustic detail for music/rapid speech; 40% text ratio: Higher preserves text capability but dilutes multimodal signal; lower risks text degradation; Separate audio head vs unified: Separate enables modular training but increases parameters

- Failure signatures: Text benchmarks drop significantly after audio integration → Check text ratio, increase pure text sampling; Audio sounds robotic/whispery → RVQ undertrained; increase reconstruction loss weight or RVQ layers; Cross-modal tasks fail but unimodal works → Interleaved data insufficient; add more cross-modal interaction samples

- First 3 experiments: 1. Ablate text ratio: Train with 20%, 40%, 60% pure text; measure MMLU/CMMLU drop after Stage 2 2. RVQ layer sweep: Test 4, 8, 12 layers; evaluate ASR WER and audio reconstruction MOS 3. Interleaved vs paired: Compare Stage 2 with interleaved data vs. purely paired ASR/TTS data; measure modality conflict on text benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
How can the model's audio processing be extended to robustly interpret non-speech environmental sounds? The Conclusion states a need to "improve audio understanding and generation to not only recognize human voices but also natural environmental sounds such as flowing water, bird songs, and collision noises." The current Baichuan-Audio-Tokenizer and training data are heavily optimized for human speech (ASR, TTS, dialogue), leaving the domain of general acoustic events largely unexplored.

### Open Question 2
What mechanisms are required to close the performance gap between processing raw audio versus text transcripts in omni-modal reasoning? Section 4.5 notes that across all models, including Baichuan-Omni-1.5, "results of using audio transcripts are better than those of using the original audio" on OmniBench, indicating a significant loss of fidelity or understanding when using the audio modality directly.

### Open Question 3
How can the architecture be adapted to support longer video sequences without compromising the detailed understanding of visual content? The Conclusion lists "support longer video frame understanding" as a necessary improvement, while Section 3.3.3 notes the current limitation to 1 frame per second with a maximum of 32 frames.

## Limitations
- Limited evaluation scope without comprehensive human evaluation of audio generation quality or cross-modal understanding in real-world scenarios
- Lack of direct comparative ablation studies for key design choices like 8-layer RVQ and 12.5 Hz frame rate
- No sensitivity analysis of the 40% pure text data ratio hyperparameter

## Confidence

- **High Confidence**: The architectural components (NaViT encoder, Whisper-based audio tokenizer, multi-stage training framework) are technically sound and follow established practices in the field. The benchmark results showing competitive performance against GPT-4o-mini and MiniCPM-o 2.6 are verifiable through the stated evaluation protocols.

- **Medium Confidence**: The claim that interleaved audio-text pretraining better mitigates modality conflict than paired-only data is supported by the training methodology but lacks direct comparative ablation studies. The assertion that the 8-layer RVQ captures both semantic and acoustic information at 12.5 Hz is plausible given the multi-objective training but unverified through systematic evaluation.

- **Low Confidence**: The claim of "seamless, high-quality interaction across all modalities without compromising performance" is difficult to verify given the limited evaluation scope. The paper does not provide detailed analysis of modality-specific failures or edge cases where the model might struggle.

## Next Checks

1. **Ablation study on text ratio**: Systematically vary the pure text data ratio from 20% to 60% during Stage 1.2 and 2.2 to quantify the relationship between text preservation and multimodal capability. Measure MMLU, CMMLU, and audio generation quality across different ratios.

2. **RVQ configuration sweep**: Test 4, 8, and 12-layer RVQ configurations with the same 12.5 Hz frame rate. Evaluate ASR word error rate, audio reconstruction quality (using objective metrics like MOSNet), and downstream task performance to determine if the 8-layer choice is optimal.

3. **Interleaved vs paired data comparison**: Train identical models using purely interleaved audio-text data versus purely paired ASR/TTS data during Stage 2. Measure modality conflict on text benchmarks, cross-modal understanding capabilities, and audio generation quality to validate the interleaving approach's effectiveness.