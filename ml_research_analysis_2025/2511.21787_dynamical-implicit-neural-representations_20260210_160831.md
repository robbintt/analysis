---
ver: rpa2
title: Dynamical Implicit Neural Representations
arxiv_id: '2511.21787'
source_url: https://arxiv.org/abs/2511.21787
tags:
- neural
- dinr
- rank
- inrs
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses spectral bias in implicit neural representations
  (INRs), where standard MLPs preferentially learn low-frequency signal components,
  limiting high-frequency reconstruction accuracy. The proposed Dynamical Implicit
  Neural Representations (DINR) framework mitigates this issue by modeling latent
  feature evolution as a continuous-time dynamical system rather than a single static
  transformation.
---

# Dynamical Implicit Neural Representations

## Quick Facts
- arXiv ID: 2511.21787
- Source URL: https://arxiv.org/abs/2511.21787
- Reference count: 40
- Primary result: DINR mitigates spectral bias in implicit neural representations through continuous-time latent dynamics, achieving up to 97% MSE reduction and improved generalization over static INR baselines.

## Executive Summary
Dynamical Implicit Neural Representations (DINR) addresses spectral bias in implicit neural representations by replacing static MLP transformations with continuous-time latent dynamics. Instead of directly approximating target functions, DINR evolves features along a learnable trajectory defined by an ODE, enabling richer intermediate representations and more adaptive frequency content. The method demonstrates significant improvements across diverse datasets including turbulence, weather, cloud, and cryo-EM data, with up to 97% MSE reduction and better generalization properties.

## Method Summary
DINR implements continuous-time latent dynamics by replacing the static MLP transformation in implicit neural representations with an ODE solver. The method maps input coordinates through an embedding function to an initial latent state, then iteratively evolves this state using a learnable vector field over multiple integration steps. A kinetic energy regularizer penalizes unnecessarily complex trajectories to improve generalization. The framework is compatible with existing INR backbones (FFNet, SIREN) and demonstrates improved expressivity through increased NTK rank and Rademacher complexity.

## Key Results
- Achieved up to 97% MSE reduction compared to static INR baselines across turbulence, weather, cloud, and cryo-EM datasets
- Improved PSNR and SSIM metrics consistently across all tested modalities
- Demonstrated better generalization under noise and data scarcity conditions
- Parameter-efficient architecture that maintains or reduces parameter count while improving performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing static feedforward transformations with continuous-time latent dynamics expands the representable function space exponentially without increasing network depth.
- **Mechanism:** A simple ODE (e.g., dz/dt = z²) integrates to an infinite power series (z/(1-tz) = z + tz² + t²z³ + ...), generating functions far more complex than the governing dynamics themselves.
- **Core assumption:** The vector field f is Lipschitz continuous; integration time T is bounded.
- **Evidence anchors:**
  - [Section 3.2]: "Even with f(z) = z², we obtain the analytic solution z(t;x) = x/(1-tx) = x + tx² + t²x³ + ..., which expands into an infinite power series."
  - [Theorem 4.1 / Proposition B.3]: Rademacher complexity bound scales as e^(Lf·T) for DINR versus L₀^ℓ for static INRs—exponential vs. polynomial growth.
- **Break condition:** If the vector field has zero Lipschitz constant (f is constant), or if integration time T → 0, the mechanism collapses to static INR behavior.

### Mechanism 2
- **Claim:** The cumulative Jacobian along the latent trajectory increases NTK rank, providing more diverse gradient directions and accelerating convergence on high-frequency modes.
- **Mechanism:** Each integration step contributes a local Jacobian J_k. The product P = ∏(I + Δt·J_k) composes transformations; unless all J_k are aligned, P spans a higher-dimensional subspace than any individual J_k, yielding rank(Θ_DINR) > rank(Θ_INR).
- **Core assumption:** Jacobians are non-zero and at least one J_k (k≥1) has row space not contained in J₀.
- **Evidence anchors:**
  - [Section 4.2 / Theorem 4.2]: "Unless all local Jacobians J_k are aligned, the product P spans a higher-dimensional space than any individual J_k."
  - [Figure 5(A-C)]: DINR exhibits higher effective NTK rank, lower condition number, and slower eigenvalue decay than static baselines across datasets.
- **Break condition:** If all Jacobians J_k are identical and low-rank, or if Δt → 0 (discretization vanishes), gradient diversity collapses.

### Mechanism 3
- **Claim:** Kinetic energy regularization bounds trajectory complexity, controlling Rademacher complexity and improving generalization under noise or data scarcity.
- **Mechanism:** The KE penalty L_KE = Σ‖f(z_k, t_k)‖²Δt constrains the reachable set radius to R = B_ϕ + √(TE). Bounded trajectories yield bounded covering numbers, tightening generalization bounds.
- **Core assumption:** Input embedding ϕ is bounded; output decoder ψ is Lipschitz.
- **Evidence anchors:**
  - [Section 3.4]: "This penalty discourages unnecessarily complex or circuitous trajectories in the feature space, which empirically improves convergence."
  - [Figure 6(A-B)]: KE regularization reduces MSE by up to 55% under 30% noise and improves generalization with only 25% training data.
- **Break condition:** If energy bound E is too restrictive, expressivity degrades; if E is unbounded, overfitting occurs on noisy/scarce data.

## Foundational Learning

- **Concept: Spectral Bias in Neural Networks**
  - **Why needed here:** DINR is explicitly designed to mitigate spectral bias—the tendency of MLPs to preferentially learn low-frequency components. Understanding this explains why the dynamical formulation matters.
  - **Quick check question:** Given a signal with both smooth gradients and sharp edges, which component will a standard MLP learn first during gradient descent?

- **Concept: Neural Tangent Kernel (NTK)**
  - **Why needed here:** The paper's theoretical contribution relies on NTK analysis to prove gradient diversity and expressivity gains. You need to interpret NTK spectra (effective rank, condition number, eigenvalue decay) to validate DINR's advantages.
  - **Quick check question:** If the NTK has low effective rank, what does this imply about the model's ability to fit diverse signal components simultaneously?

- **Concept: Numerical ODE Solvers (Euler, Runge-Kutta)**
  - **Why needed here:** DINR implements continuous dynamics via discrete integration. Step size Δt trades off accuracy vs. compute; solver choice affects gradient flow through the ODE.
  - **Quick check question:** In the Euler scheme z_{k+1} = z_k + Δt·f(z_k, t_k), what happens to gradient diversity if Δt is too large?

## Architecture Onboarding

- **Component map:** Input coordinates → Input embedding ϕ → Latent dynamics (ODE solver) → Output decoder ψ → Signal output

- **Critical path:**
  1. Embed input coordinates → z_0 = ϕ(x)
  2. Iterate dynamics: for k = 0 to N-1: z_{k+1} = z_k + Δt·f(z_k, t_k)
  3. Accumulate KE loss during iteration
  4. Decode output: ŷ = ψ(z_N)
  5. Backpropagate through unrolled trajectory

- **Design tradeoffs:**
  - **Integration steps N / step size Δt:** More steps → richer function space but higher memory/compute. Paper uses N such that N·Δt = T with T fixed.
  - **Hidden dimension d_z:** Larger dimensions increase expressivity but reduce parameter efficiency gains.
  - **KE regularization weight:** Controls expressivity-generalization tradeoff. Too high → underfitting; too low → noise sensitivity.
  - **Activation choice:** ReLU (FFNet) vs. sine (SIREN). SIREN is more expressive but sensitive to initialization ω₀; DINR stabilizes SIREN training.

- **Failure signatures:**
  - **Exploding trajectories:** ‖z_k‖ grows unbounded → check Lipschitz constant of f, reduce Δt, increase KE regularization.
  - **No improvement over static baseline:** NTK rank similar to INR → increase integration time T or check that vector field f is learning non-trivial dynamics (‖f‖ should be non-zero).
  - **Overfitting to noise:** Reconstruction captures noise artifacts → increase KE regularization weight; verify noise level in data.
  - **SIREN divergence:** Training fails or outputs wrong value range → DINR should stabilize this; if still failing, reduce initialization scale or check learning rate.

- **First 3 experiments:**
  1. **Ablation on integration steps:** Train DINR with N ∈ {1, 5, 10, 20} on a 2D image (e.g., turbulence slice). Plot PSNR vs. N and compare parameter counts. Expect diminishing returns beyond some N.
  2. **KE regularization sweep:** On noisy data (e.g., 10-20% Gaussian noise), sweep regularization weight λ_KE ∈ {0, 0.01, 0.1, 1.0}. Plot reconstruction error vs. noise level for each λ. Identify optimal λ that balances denoising and detail preservation.
  3. **NTK analysis replication:** Compute empirical NTK (Jacobian of outputs w.r.t. parameters) and check effective rank and condition number for DINR vs. static baseline on a held-out coordinate set.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can adaptive step-size or higher-order ODE solvers optimize the trade-off between expressivity and computational cost in DINR?
  - **Basis in paper:** [explicit] Conclusion section.
  - **Why unresolved:** The authors currently use a fixed implementation and explicitly list exploring adaptive step-sizes and higher-order solvers as a future direction to balance cost and performance.
  - **What evidence would resolve it:** A comparative study benchmarking DINR's convergence speed and accuracy using solvers like Dormand-Prince (adaptive) vs. fixed-step Euler/Runge-Kutta methods.

- **Open Question 2:** Does the dynamical formulation provide additive benefits when combined with high-frequency positional encoding strategies (e.g., Fourier features or Hash encodings)?
  - **Basis in paper:** [inferred] The Introduction claims DINR is "orthogonal to existing remedy strategies" like positional embeddings, but experiments are restricted to FFNet and SIREN backbones without combining DINR with explicit positional encoding layers.
  - **What evidence would resolve it:** Experiments integrating DINR with positional encoding methods (e.g., HashGrid or Wire) to see if performance gains are compounding or redundant.

- **Open Question 3:** Under what specific data or initialization conditions do the local Jacobians $J_k$ become aligned, causing the theoretical benefits of gradient diversity to vanish?
  - **Basis in paper:** [inferred] Theorem 4.2 proves the NTK rank increase "Unless all local Jacobians... are aligned," but the paper does not characterize when such alignment failure might occur in practice.
  - **Why unresolved:** The theoretical guarantee has a condition that is mathematically possible but empirically uncharacterized regarding its frequency or triggers.
  - **What evidence would resolve it:** An analysis of the Jacobian alignment distribution across different data modalities (e.g., natural images vs. turbulence) and initialization seeds.

- **Open Question 4:** How does the memory footprint and wall-clock training time of DINR scale relative to static INRs when processing higher-dimensional inputs (e.g., 3D video)?
  - **Basis in paper:** [inferred] Section 5.1 notes that batch sizes were adjusted to fit GPU memory for the utilized datasets, and the Conclusion lists "computational cost" as a key trade-off to be optimized.
  - **What evidence would resolve it:** Detailed profiling of memory usage and latency on 4D datasets (e.g., video or time-varying volumes) to quantify the efficiency gap compared to static baselines.

## Limitations

- Key architectural hyperparameters (hidden layer counts, embedding dimensions, integration step count N and size Δt) are unspecified, making exact replication difficult
- Performance improvements measured against specific baselines (FFNet, SIREN) without comparison to other spectral bias mitigation methods
- NTK theoretical analysis assumes idealized continuous dynamics and ignores discretization effects

## Confidence

- **High confidence**: Empirical MSE/PSNR improvements on all tested datasets; mechanistic validity of continuous dynamics expanding function space (Proposition B.3); KE regularization improving noise robustness
- **Medium confidence**: NTK rank and condition number improvements (theoretical derivation assumes idealized conditions; empirical NTK computation may differ); parameter efficiency claims (depends on unspecified architecture sizes)
- **Low confidence**: Claims about stabilizing SIREN training (not quantified beyond "stabilizes"); generalization improvements with limited data (test set details unclear)

## Next Checks

1. Implement DINR with varying integration steps N and measure diminishing returns in PSNR to verify optimal step count claimed in ablation studies
2. Systematically vary KE regularization weight λ_KE on noisy data (0%, 10%, 20%, 30% Gaussian noise) to reproduce the 55% MSE reduction at 30% noise
3. Compute empirical NTK eigenspectrum for DINR vs. static baseline on a held-out coordinate set to verify higher effective rank and lower condition number as shown in Figure 5