---
ver: rpa2
title: Scalable Engine and the Performance of Different LLM Models in a SLURM based
  HPC architecture
arxiv_id: '2508.17814'
source_url: https://arxiv.org/abs/2508.17814
tags:
- inference
- engine
- scalable
- slurm
- architecture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates a SLURM-based HPC architecture for scalable
  LLM inference, integrating dynamic resource scheduling, containerized microservices,
  and RESTful APIs to manage CPU, GPU, and memory efficiently. The architecture supports
  concurrent deployment of heterogeneous models (Llama 3.2 and 3.1 variants) with
  minimal overhead from scheduling and containerization.
---

# Scalable Engine and the Performance of Different LLM Models in a SLURM based HPC architecture

## Quick Facts
- arXiv ID: 2508.17814
- Source URL: https://arxiv.org/abs/2508.17814
- Authors: Anderson de Lima Luiz; Shubham Vijay Kurlekar; Munir Georges
- Reference count: 18
- Primary result: SLURM-based HPC architecture for scalable LLM inference with dynamic resource scheduling, containerized microservices, and RESTful APIs

## Executive Summary
This work presents a SLURM-based HPC architecture for deploying and scaling heterogeneous LLM inference workloads. The system dynamically allocates CPU, GPU, and memory resources using SLURM batch scheduling, containerized microservices, and RESTful APIs to manage concurrent requests across multiple model sizes. Experimental results demonstrate that smaller models (1B, 3B) handle up to 128 concurrent users with sub-50 ms latency, while larger models (70B) saturate at two concurrent users with over 2-second latency. The architecture supports advanced workflows like multi-step "tribunal" refinement and proves effective for real-world applications such as retrieval-augmented chatbots.

## Method Summary
The method involves deploying heterogeneous LLMs (Llama 3.2: 1B/3B; Llama 3.1: 8B/70B) on a SLURM-managed HPC cluster with 4Ã— NVIDIA A100 GPUs. The scalable engine programmatically generates SLURM batch scripts based on model requirements, then parses job logs to discover dynamic endpoint addresses for load balancing via NGINX. TGI or vLLM inference engines run in containers on allocated GPU nodes, with FastAPI providing the REST API layer. Stress testing uses 1024-token Lorem Ipsum prompts at varying concurrency levels to measure latency, throughput, and saturation points across different model sizes.

## Key Results
- Small models (1B, 3B) handle up to 128 concurrent requests at sub-50 ms latency
- Large models (70B) saturate at two concurrent users with over 2-second latency
- Throughput scales reliably before saturation points are reached
- Advanced "tribunal" refinement workflows enhance response quality

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Resource Translation Layer
The architecture bridges high-level model requirements and low-level hardware allocation by programmatically generating SLURM batch scripts. The scalable engine accepts abstract parameters and writes them into a `.slurm` file template, delegating hardware partitioning to the SLURM scheduler. This ensures the inference engine receives isolated, dedicated resources. The core assumption is that the HPC cluster has a pre-configured SLURM environment capable of parsing these dynamic scripts and allocating exclusive GPU resources on demand.

### Mechanism 2: Log-Parsed Endpoint Discovery
The system enables network communication with ephemeral HPC jobs by treating job logs as a service registry. Instead of pre-assigning static IPs, the system allows SLURM to spawn jobs on any available node. The inference servers log their dynamic IP/port combinations to a "hosts file" (HPC job log). The scalable engine parses this file to programmatically configure an NGINX load balancer, creating temporary REST endpoints for job duration. The core assumption is that inference servers successfully write to standard output/logs and the engine has file-system permission to read these logs near real-time.

### Mechanism 3: Adaptive Workflow Degradation (Tribunal)
The system maintains responsiveness during peak load by conditionally disabling complex, multi-step inference workflows. The "tribunal" system (generate-critique-revise) runs multiple LLM calls per user request. The architecture appears to monitor capacity/saturation and bypasses tribunal steps when under heavy load, relying on the base model's single-pass output. This trades response quality for latency/throughput stability. The core assumption is that the system has accurate real-time metrics on queue depth or GPU utilization to trigger this fallback before latency spikes violate SLAs.

## Foundational Learning

- **Concept: SLURM Resource Allocation**
  - Why needed: This is the underlying operating system of the HPC cluster. You cannot deploy models without understanding how to request CPUs, GPUs, and memory in a batch script.
  - Quick check: Can you explain the difference between `--ntasks` and `--cpus-per-task` when requesting resources for a GPU-heavy inference server?

- **Concept: Asynchronous Python (FastAPI/Uvicorn)**
  - Why needed: The scalable engine acts as a web server. Handling concurrent user requests while waiting for slow HPC job scheduling or GPU inference requires non-blocking I/O (`async`/`await`).
  - Quick check: Why would using a standard synchronous `requests.post` call inside a FastAPI endpoint cause the entire service to freeze under load?

- **Concept: NGINX Reverse Proxying**
  - Why needed: The architecture relies on NGINX to abstract away the dynamic IPs of the HPC nodes.
  - Quick check: How does NGINX handle a request if one of the upstream HPC nodes (inferred from the log file) suddenly crashes or finishes its job time limit?

## Architecture Onboarding

- **Component map:** Client Layer (FastAPI/Streamlit) -> Orchestrator (Scalable Engine) -> Resource Manager (SLURM Scheduler) -> Compute Layer (HPC Nodes with TGI/vLLM) -> Network Layer (NGINX container)

- **Critical path:** The sequence from Job Submission -> SLURM Scheduling -> Hosts File Parsing -> NGINX Reload. If any step fails here, the API endpoint returns a 504 Gateway Timeout even if the GPUs are free.

- **Design tradeoffs:** Small models (1B/3B) handle high concurrency (128 users) at low latency; large models (70B) saturate immediately (2 users), requiring serial execution. Startup latency is 750ms for 70B models, acceptable for batch jobs but problematic for interactive cold starts.

- **Failure signatures:**
  - Starvation: Valid inference requests sit in the SLURM queue because larger jobs monopolize GPUs
  - Orphaned Processes: If the orchestrator crashes after submitting a SLURM job but before linking it to NGINX, the GPU node runs an inference server that is inaccessible to the outside world

- **First 3 experiments:**
  1. **Latency Baseline:** Run a single request against the 1B and 70B models to confirm the startup overhead and inference latency match Figure 3
  2. **Saturation Test:** Use a load testing tool to send 10 concurrent requests to the 3B model and verify if the system maintains sub-50ms latency or if it degrades unexpectedly
  3. **Recovery Test:** Manually kill the SLURM job while the NGINX load balancer is active, and verify if the orchestrator detects the failure and stops routing traffic to the dead IP

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can auto-parallelization techniques be extended to support extreme-scale models (larger than 70B parameters) without degrading the sub-50 ms latency standards achieved by smaller models?
- Basis: The conclusion states that "Further efforts will focus on extending auto-parallelization for extreme-scale models."
- Why unresolved: The current study only benchmarks models up to 70B parameters, which exhibit significant latency (2s) and low concurrency saturation (2 users).
- What evidence would resolve it: Successful deployment and throughput/latency benchmarking of models with 100B+ parameters within the existing SLURM architecture.

### Open Question 2
- Question: To what degree can advanced caching and smarter scheduling algorithms reduce the queue-derived latency observed during system saturation?
- Basis: The discussion notes the architecture "can be enhanced by smarter scheduling, advanced caching, [and] fine-grained resource partitioning."
- Why unresolved: The current implementation relies on standard FIFO policies, resulting in "composite lag" and "wild" throughput behavior once the concurrency limit is reached.
- What evidence would resolve it: Comparative analysis of latency tails under saturation before and after implementing the proposed caching and scheduling enhancements.

### Open Question 3
- Question: What is the quantitative overhead of the NGINX load balancer and container spin-up compared to raw inference engine performance?
- Basis: The paper claims "minimal overhead from container and scheduling activities" but provides no direct A/B comparison against a non-containerized or bare-metal baseline.
- Why unresolved: Isolating the orchestration latency is necessary to determine if the "unified orchestration" is the bottleneck for the 70B model's 2-second latency.
- What evidence would resolve it: Benchmarks isolating the latency contribution of the NGINX/FastAPI layers separate from the GPU inference time.

## Limitations
- Dynamic Resource Allocation Precision: Specific SLURM parameters that ensure low-latency allocation are not detailed, limiting reproducibility on different HPC clusters.
- Log-Parsing Latency: The real-time discovery of inference endpoints relies on parsing job logs without specified handling of log buffering delays.
- Tribunal System Implementation: The adaptive workflow degradation lacks implementation details and clear metrics for when and how the fallback is triggered.

## Confidence
- **High Confidence:** The fundamental architecture of using SLURM for job scheduling and TGI/vLLM for inference is sound and reproducible. Saturation trends are consistent with known GPU constraints.
- **Medium Confidence:** Performance numbers are credible but specific hardware configuration and model loading times are not fully specified, making exact replication difficult.
- **Low Confidence:** The "tribunal" workflow degradation mechanism is the least verified component without metrics on when and how the fallback is triggered.

## Next Checks
1. **Baseline Latency Verification:** Run a single inference request for the 1B and 70B models to measure startup overhead and first-token latency. Compare these numbers against the paper's Figure 3 to confirm fundamental performance characteristics.
2. **Saturation Point Confirmation:** Use a load-testing tool to send controlled concurrent requests (e.g., 16, 32, 64) to the 3B model. Monitor if the system maintains reported sub-50ms latency or degrades earlier than expected.
3. **Failure Mode Testing:** Simulate a node failure by manually terminating a SLURM job while active. Observe if the orchestrator detects the missing log entry and updates NGINX configuration to stop routing traffic to the dead endpoint.