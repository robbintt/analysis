---
ver: rpa2
title: Reinforcement Learning is all You Need
arxiv_id: '2503.09512'
source_url: https://arxiv.org/abs/2503.09512
tags:
- reasoning
- answer
- base
- learning
- think
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that pure reinforcement learning can significantly
  improve reasoning capabilities in language models. Training a 3B parameter model
  on the Countdown Game with GRPO leads to strong performance gains on four of five
  benchmarks, showing effective generalization beyond the training domain.
---

# Reinforcement Learning is all You Need

## Quick Facts
- arXiv ID: 2503.09512
- Source URL: https://arxiv.org/abs/2503.09512
- Reference count: 32
- A 3B parameter model trained on Countdown Game with GRPO achieves strong performance gains on four of five benchmarks

## Executive Summary
This paper demonstrates that pure reinforcement learning can significantly enhance reasoning capabilities in language models. By training a 3B parameter Qwen2.5-Instruct model on the Countdown Game using Group Relative Policy Optimization (GRPO), the study achieves substantial improvements across multiple reasoning benchmarks. The work reveals that longer responses don't guarantee better reasoning, and while "aha moments" of self-correction emerge during training, they don't always lead to correct answers. These findings suggest RL has substantial potential for driving reasoning development and highlight the importance of refining reward structures to bridge the gap between emergent insights and accuracy.

## Method Summary
The method involves fine-tuning Qwen2.5-3B-Instruct using GRPO on the Countdown Game, an arithmetic puzzle where the model must reach a target number using given integers and basic operations. The RL training employs a dual-component rule-based reward: a format reward verifying the correct structured output format and a binary answer reward checking numerical correctness. Training proceeds for 850 steps with a KL penalty of 0.04, learning rate of 1e-6, and batch size of 2. The model is evaluated zero-shot on five benchmarks (GSM8K, MATH, BBH, MMLU-Pro, IFEval) using Language Model Evaluation Harness with R1-style prompts.

## Key Results
- Zero-shot performance on GSM8K improves from 55.0% to 64.8% (Strict-Match)
- MATH benchmark performance increases from 13% to 27%
- Four of five evaluated benchmarks show strong performance gains
- Few-shot prompting underperforms zero-shot prompting for the trained model

## Why This Works (Mechanism)

### Mechanism 1: Group-Relative Advantage Normalization
GRPO eliminates the need for an explicit value function by using group-based reward normalization. For each query, it generates G responses and computes advantage as Ai = (ri - mean(rewards)) / std(rewards), creating a dynamic baseline that identifies which responses outperform the group average without requiring a separate critic network. This reduces computational complexity while maintaining relative quality assessment.

### Mechanism 2: Dual-Component Rule-Based Reward
The reward system separates format compliance from answer correctness, providing structured guidance for reasoning development. Format Reward (regex-based verification of casiong...<answer> structure) ensures the model articulates reasoning before concluding, while Answer Reward (binary: 1 if target matched, 0 otherwise) provides sparse but unambiguous correctness signal. Together, they scaffold the model toward verifiable chain-of-thought.

### Mechanism 3: Cross-Domain Reasoning Transfer from Controllable Tasks
Training on verifiable, structured tasks like Countdown Game develops reasoning patterns that generalize to unrelated domains. The Countdown Game provides deterministic correctness verification and requires multi-step arithmetic planning, optimizing policies that explore, backtrack, and verify—all general-purpose reasoning sub-skills that transfer because the underlying cognitive operations are domain-agnostic.

## Foundational Learning

- **Proximal Policy Optimization (PPO) fundamentals**
  - Why needed: GRPO modifies PPO's core mechanics; understanding clipping objective and KL penalty is prerequisite to grasping what GRPO removes (value function) and preserves (policy constraint)
  - Quick check: Explain why PPO uses both a clipping term and a KL divergence penalty—what failure mode does each prevent?

- **Sparse vs. dense reward signals**
  - Why needed: The paper uses binary answer rewards (sparse), creating credit assignment challenges that explain why "aha moments" emerge unpredictably
  - Quick check: Why might a model trained on binary correctness rewards develop longer reasoning chains even without explicit length rewards?

- **Zero-shot vs. few-shot evaluation**
  - Why needed: The paper finds zero-shot outperforms few-shot for the trained model—counterintuitive and requires understanding how RL fine-tuning shifts in-context learning distribution
  - Quick check: What hypothesis explains why adding exemplars to a prompt might degrade performance for an RL-tuned model?

## Architecture Onboarding

- **Component map:**
  Base Model (Qwen2.5-3B-Instruct) -> Training Data (Countdown Game) -> RL Algorithm (GRPO) -> Reward Model (Format + Answer) -> Evaluation (5 benchmarks)

- **Critical path:**
  1. Prepare Countdown Game prompts with target/numbers format
  2. Configure GRPO with β=0.04 (KL penalty), lr=1e-6, batch_size=2
  3. Implement format reward (regex for ings/on<answer> structure) and answer reward (numerical equality check)
  4. Train for 850 steps, monitoring completion length and reward curves
  5. Evaluate checkpoints on GSM8K, MATH, BBH, MMLU-Pro, IFEval using zero-shot prompts

- **Design tradeoffs:**
  - GRPO vs. PPO: GRPO reduces per-step time ~60% but may increase length-hacking susceptibility
  - Binary vs. process rewards: Current design uses sparse correctness; PRMs could provide denser signal but require annotation infrastructure
  - Group size G=2: Minimal computational overhead but potentially high variance in advantage estimates

- **Failure signatures:**
  - Rule violation in early training: Multiple ings/ blocks, missing <answer> tags
  - Brute-force without reflection: Systematic enumeration without strategic pruning
  - Correct format, incorrect answer: "Aha moments" with self-correction language but final errors
  - Format regressions: Duplicated <answer> blocks even at training end

- **First 3 experiments:**
  1. **Ablate group size**: Run G=2, 4, 8 to measure variance in advantage estimates and final benchmark performance
  2. **Add process reward component**: Implement partial credit for intermediate arithmetic correctness
  3. **Control for prompt contamination**: Test whether GSM8K/MATH improvements derive from arithmetic skill transfer vs. prompt format familiarity

## Open Questions the Paper Calls Out

- **Why does few-shot prompting underperform zero-shot prompting in RL-trained models, and can this be remediated?**
  - The authors document that few-shot performance is worse than zero-shot but offer no causal explanation, noting it aligns with DeepSeek R1 findings

- **How does the GRPO sample size (G in Eq. 1) affect training stability and final reasoning performance?**
  - The paper used only 2 samples per step but did not systematically vary this hyperparameter, despite noting it may significantly affect performance and stability

- **What reward mechanisms can reliably convert "aha moment" self-correction behaviors into correct final answers?**
  - The paper observes that aha moments "do not always yield correct answers" and calls for refining reward structures to bridge emergent insights with accuracy

## Limitations

- Transfer is asymmetric with no improvement on Temporal Sequences despite gains on arithmetic-heavy tasks
- Binary answer rewards may limit the granularity of feedback, constraining the model's ability to learn nuanced reasoning patterns
- The paper doesn't explain why few-shot prompting underperforms zero-shot for RL-trained models

## Confidence

- GRPO mechanics and implementation: **High** - Well-documented, standard configuration
- Dual reward design effectiveness: **Medium** - Logical structure but sparse reward limitations
- Cross-domain reasoning transfer: **Medium-Low** - Strong on some benchmarks, absent on others
- "Aha moments" as genuine reasoning: **Low-Medium** - Descriptive observation, causal mechanism unclear

## Next Checks

1. **Ablation study on reward components**: Remove format reward while keeping answer reward, and vice versa, to isolate whether structured reasoning or correctness signal drives improvements

2. **Extended evaluation on non-arithmetic reasoning tasks**: Test transfer to logical reasoning, commonsense inference, and multi-hop reasoning tasks that don't involve numerical computation

3. **Analysis of "aha moments" quality**: Examine cases where the model produces self-correction language but arrives at incorrect answers, implementing process rewards to determine if intermediate reasoning quality correlates with final answer accuracy