---
ver: rpa2
title: 'gHAWK: Local and Global Structure Encoding for Scalable Training of Graph
  Neural Networks on Knowledge Graphs'
arxiv_id: '2512.08274'
source_url: https://arxiv.org/abs/2512.08274
tags:
- node
- ghawk
- training
- graph
- transe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: gHAWK addresses the scalability challenge of training graph neural
  networks on large knowledge graphs by introducing a preprocessing step that encodes
  local and global structural features before GNN training begins. The method uses
  Bloom filters to compactly represent each node's local 1-hop neighborhood and TransE
  embeddings to capture global structural position and semantic context.
---

# gHAWK: Local and Global Structure Encoding for Scalable Training of Graph Neural Networks on Knowledge Graphs

## Quick Facts
- **arXiv ID**: 2512.08274
- **Source URL**: https://arxiv.org/abs/2512.08274
- **Reference count**: 40
- **Primary result**: Achieves SOTA accuracy on OGB benchmarks (75.04% on MAG240M, 75.74% MRR on OGB-WikiKG2) while reducing training time

## Executive Summary
gHAWK addresses the scalability challenge of training graph neural networks on large knowledge graphs by introducing a preprocessing step that encodes local and global structural features before GNN training begins. The method uses Bloom filters to compactly represent each node's local 1-hop neighborhood and TransE embeddings to capture global structural position and semantic context. These structural features are fused with domain-specific features through learned MLPs to create compact, informative node representations that can be incorporated into any GNN architecture. Extensive experiments on OGB benchmarks demonstrate that gHAWK achieves state-of-the-art accuracy while reducing training time, enabling shallow message passing to achieve high performance.

## Method Summary
gHAWK introduces a preprocessing framework that encodes both local and global structural information for knowledge graph nodes. Local structure is captured using Bloom filters that represent each node's 1-hop neighborhood in a compact binary format, while global structure is encoded using TransE embeddings that capture semantic context and positional information. These structural features are then fused with domain-specific features (such as RoBERTa or Word2Vec embeddings) through learned MLPs to create enriched node representations. The fused features serve as input to standard GNN architectures, enabling effective learning with shallow message passing (1-2 layers) while achieving state-of-the-art performance. The preprocessing is computationally efficient, with Bloom filters constructed in linear time and TransE providing scalable global structure encoding.

## Key Results
- Achieves 75.04% accuracy on MAG240M, ranking first on the OGB leaderboard
- Achieves 75.74% MRR on OGB-WikiKG2, ranking first on the OGB leaderboard
- Consistently improves accuracy by 4.8-17.5 percentage points across different GNN backbones and tasks
- Enables shallow message passing (1-2 layers) to achieve high performance, reducing training time

## Why This Works (Mechanism)
The key insight behind gHAWK is that structural information in knowledge graphs can be efficiently encoded before GNN training begins, eliminating the need for expensive message passing operations to learn this information. By using Bloom filters for local structure and TransE embeddings for global structure, gHAWK captures essential topological and semantic information in a compact form. The fusion of these structural features with domain-specific features creates rich node representations that provide GNNs with the necessary context to make accurate predictions without requiring deep architectures. This approach is particularly effective for knowledge graphs where structural patterns are crucial for understanding node properties and relationships.

## Foundational Learning
- **Bloom Filters**: Probabilistic data structures for set membership testing with compact memory footprint; needed to efficiently encode local 1-hop neighborhoods without expensive neighbor aggregation
- **TransE Embeddings**: Knowledge graph embedding method that models entities as vectors and relations as translations; needed to capture global structural position and semantic context
- **Heterogeneous Graph Processing**: Handling graphs with multiple node and edge types; needed for MAG240M's academic paper citation network with papers, authors, and institutions
- **Knowledge Graph Completion**: Link prediction task for inferring missing edges; needed for OGB-WikiKG2's triple completion objective
- **Hard Negative Sampling**: Selecting informative negative examples for training; needed to improve link prediction performance through TransE-guided sampling
- **Quick Check**: Verify that Bloom filter false positive rate remains below 5% for nodes at the 95th percentile degree

## Architecture Onboarding
- **Component Map**: Raw Graph -> TransE Embeddings + Bloom Filters -> Fusion MLP -> GNN Backbone -> Predictions
- **Critical Path**: Preprocessing (TransE + Bloom) -> Feature Fusion -> GNN Training
- **Design Tradeoffs**: Fixed-size Bloom filters vs. dynamic neighbor lists (memory vs. accuracy), shallow GNNs vs. deep GNNs (speed vs. expressiveness)
- **Failure Signatures**: Bloom saturation for high-degree nodes, TransE embedding collapse, fusion module ignoring structural features
- **3 First Experiments**:
  1. Benchmark Bloom filter construction time and memory usage on MAG240M
  2. Train TransE embeddings and evaluate reconstruction accuracy on validation triples
  3. Ablation study comparing GNN performance with only domain features vs. with structural features

## Open Questions the Paper Calls Out
None

## Limitations
- Bloom filters may lose information for very high-degree nodes due to fixed size constraints
- TransE embeddings capture only first-order proximity, potentially missing complex relational patterns
- The method requires careful hyperparameter tuning for the fusion MLP architecture and training rates

## Confidence
- **High confidence**: SOTA performance claims on OGB benchmarks, preprocessing efficiency claims
- **Medium confidence**: Exact fusion MLP architecture details and fine-tuning learning rates
- **High confidence**: Overall methodology and reproducibility of core components

## Next Checks
1. Implement and benchmark the Bloom filter construction on MAG240M to verify linear-time preprocessing and measure load factor for high-degree nodes
2. Conduct ablation studies removing either the Bloom filter or TransE components to isolate their individual contributions to accuracy improvements
3. Test gHAWK with varying fusion MLP hidden dimensions to determine sensitivity to this architectural choice and identify the optimal configuration