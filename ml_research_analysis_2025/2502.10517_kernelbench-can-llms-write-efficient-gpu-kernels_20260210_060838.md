---
ver: rpa2
title: 'KernelBench: Can LLMs Write Efficient GPU Kernels?'
arxiv_id: '2502.10517'
source_url: https://arxiv.org/abs/2502.10517
tags:
- kernels
- kernel
- torch
- pytorch
- level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces KernelBench, a framework for evaluating language
  models' ability to generate efficient GPU kernels for machine learning workloads.
  It includes 250 tasks spanning individual operations, operator sequences, and full
  ML architectures, with a new metric "fastp" measuring the percentage of kernels
  that are both correct and achieve a speedup threshold p over PyTorch baselines.
---

# KernelBench: Can LLMs Write Efficient GPU Kernels?

## Quick Facts
- **arXiv ID**: 2502.10517
- **Source URL**: https://arxiv.org/abs/2502.10517
- **Reference count**: 40
- **Primary result**: LLMs generate GPU kernels that outperform PyTorch Eager in less than 20% of cases; iterative refinement with feedback improves success rates.

## Executive Summary
This paper introduces KernelBench, a benchmark framework for evaluating language models' ability to generate efficient GPU kernels for machine learning workloads. The study tests state-of-the-art models on 250 tasks spanning individual operations, operator sequences, and full ML architectures, measuring both functional correctness and performance speedup over PyTorch baselines. Results show that even the best models struggle to generate correct and performant kernels, with iterative refinement and feedback loops being crucial for improvement. The authors conclude that KernelBench remains a challenging unsolved problem that will continue to evolve with new AI workloads and hardware platforms.

## Method Summary
KernelBench provides a framework where LLMs receive PyTorch `nn.Module` definitions and must generate optimized versions with custom CUDA kernels. Tasks are organized into three levels: Level 1 (100 single operations), Level 2 (100 operator sequences), and Level 3 (50 full architectures). The framework evaluates both correctness (comparing outputs on 5 random inputs) and performance (measuring wall-clock execution time with CUDA events). Models are tested in one-shot, repeated sampling, and iterative refinement modes, with the latter using compiler errors, execution results, and profiling data as feedback. The primary metric is `fast_p`, measuring the percentage of kernels that are both correct and achieve speedup threshold p over PyTorch Eager baselines.

## Key Results
- Even state-of-the-art models like OpenAI-o1 and DeepSeek-R1 achieve fast_p1 > 0.2 (20%) on only a small fraction of tasks
- Iterative refinement with execution and profiling feedback increases fast_p from 12% to 43% on Level 1 tasks
- Providing hardware-specific information or optimization examples can help models generate better kernels, but effectiveness varies by model type
- Functional correctness remains the primary challenge, with most failures due to execution errors or incorrect outputs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Providing execution and profiling feedback to LLMs during iterative refinement significantly improves the generation of functionally correct and performant GPU kernels.
- **Mechanism**: The KernelBench environment collects compiler errors, execution results, and profiling data. This ground-truth feedback is fed back to the model in a multi-turn process, allowing it to self-correct. The model uses this to primarily reduce execution failures and discover optimization opportunities guided by performance data.
- **Core assumption**: LLMs can effectively parse and act on structured error messages and performance metrics within their context window to modify code logic and optimization strategies.
- **Evidence anchors**: Abstract shows fast_p increasing from 12% to 43% on Level 1 tasks with iterative refinement. Section 5.1.2 shows DeepSeek-R1 generates functional kernels on >90% of tasks within 10 turns. Corpus papers like PRAGMA, STARK, and Astra propose similar feedback systems.

### Mechanism 2
- **Claim**: Repeated sampling with high temperature allows LLMs to explore a larger solution space, increasing the probability of discovering correct and performant kernels.
- **Mechanism**: The model generates k different kernel implementations for a single task. The best one is selected based on the fast_p metric. High temperature encourages diverse outputs.
- **Core assumption**: The correct or optimal solution exists within the model's probability distribution, even if it is in the tail.
- **Evidence anchors**: Section 5.1.1 shows DeepSeek-V3 reaches fast_p1 of 37% with k = 100 samples versus 4% in one-shot baseline. STARK and other papers focus on structured refinement rather than pure sampling.
- **Break condition**: Diminishing returns and hard capability wall when model's knowledge base is fundamentally insufficient, as evidenced by inability to solve 34 specific convolution tasks even with 100 samples.

### Mechanism 3
- **Claim**: Providing hardware-specific context and optimization examples (in-context learning) can nudge LLMs toward more aggressive and hardware-aware optimizations.
- **Mechanism**: The model's prompt is augmented with hardware specifications and optimization examples to prime it for specific hardware features.
- **Core assumption**: LLMs can generalize from provided examples and specifications to new problems, correctly applying demonstrated techniques.
- **Evidence anchors**: Section 5.2.1 shows in-context examples result in more execution failures but interesting optimizations on 77% of GEMM variants. Section 5.2.2 shows hardware information helps reasoning models achieve 1-3 outliers ≥ 2× faster. CUDA-LLM and QiMeng-Kernel papers focus on architecture-aware paradigms.
- **Break condition**: Highly model-dependent effectiveness and potential to backfire by increasing execution failures despite better successful kernel performance.

## Foundational Learning

- **Concept: GPU Kernel Fusion**
  - **Why needed here**: Primary optimization target, especially for Level 2 tasks which are sequences of operations.
  - **Quick check question**: How does fusing multiple point-wise operators (e.g., ReLU, Bias Add) after a matrix multiplication into a single kernel reduce memory bandwidth pressure?

- **Concept: CUDA Memory Hierarchy (Global vs. Shared Memory)**
  - **Why needed here**: Fundamental for writing efficient kernels; successful LLM-generated kernels often use shared memory.
  - **Quick check question**: Why is accessing data from shared memory significantly faster than accessing it from global (HBM) memory, and what programming construct is used to declare a variable in shared memory?

- **Concept: Functional Correctness vs. Execution Errors**
  - **Why needed here**: Paper distinguishes these as key failure modes; different feedback strategies needed for each.
  - **Quick check question**: If a kernel compiles and runs without crashing but produces a result that is numerically different from the PyTorch baseline, is this an execution failure or a correctness failure?

## Architecture Onboarding

- **Component map**: Task Specification (PyTorch Model + get_inputs) -> Model Output (optimized ModelNew with CUDA C++) -> Evaluation Engine (Correctness Verifier + Performance Profiler) -> Feedback Loop (compiler output, execution results, profiler data) -> LLM

- **Critical path**: Model (reference) -> ModelNew (candidate) -> (Correctness Check & Profiling) -> Feedback. A kernel must pass correctness check on 5 randomized inputs before performance is measured.

- **Design tradeoffs**:
  - Flexibility vs. Guidance: Full freedom leads to high failure rates; in-context examples increase guidance but can negatively impact overall fast_p by encouraging overly complex attempts.
  - One-shot vs. Iterative Refinement: One-shot is faster but shows raw capability; iterative refinement is far more effective but requires more inference calls and complex state management.

- **Failure signatures**:
  - Execution Failure: Code fails to compile, segfaults, or times out due to syntax errors, incorrect API usage, or illegal memory accesses.
  - Functional Incorrectness: Code runs but produces wrong values; harder to debug remotely and primary remaining failure mode after 10 turns of refinement.
  - Slow Correct Kernel: Code is correct but slower than PyTorch baseline; LLM failed to identify or implement effective optimization.

- **First 3 experiments**:
  1. Baseline Capability Assessment: Run one-shot prompt across subsets of Level 1, 2, and 3 tasks using different models, recording fast_0 and fast_1.
  2. Iterative Refinement Loop: Implement basic loop with compiler and execution feedback for 5 turns on Level 1 tasks to observe self-correction from common execution errors.
  3. Hardware-Specific Prompting: Compare model outputs with and without hardware specifications to measure impact on aggressive or hardware-specific instructions.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can generating code in intermediate abstractions (e.g., Triton, CUTLASS, ThunderKittens) rather than raw CUDA significantly improve functional correctness and optimization success rates of LLMs?
  - Basis: Section 6.3 suggests exploring alternative programming abstractions to simplify the generation problem.
  - Why unresolved: Current experiments rely on raw CUDA generation with high execution failure rates and struggles with complex hardware instructions.
  - Evidence needed: Comparative evaluation of Triton vs raw CUDA generation on KernelBench measuring delta in fast_p scores and error rates.

- **Open Question 2**: To what extent does the scarcity of high-quality, open-source kernel data contribute to the trade-off between attempting complex optimizations and maintaining functional correctness?
  - Basis: Section 6.3 notes CUDA is a low-resource language and suggests open-sourcing more high quality data to address performance issues.
  - Why unresolved: Paper hypothesizes data scarcity causes struggles with niche instructions but doesn't empirically prove via fine-tuning experiments.
  - Evidence needed: Fine-tuning baseline model on curated dataset of verified, optimized kernels and measuring if trade-off between complexity and correctness shifts positively.

- **Open Question 3**: How can models be improved to generate kernels that maintain performance portability across diverse GPU architectures without requiring explicit, per-device hardware specifications in the prompt?
  - Basis: Section 4.4 shows large performance variations across GPUs (e.g., 11% between L40S and A10G), suggesting lack of robust generalization.
  - Why unresolved: Authors observe poor generalization across hardware but only suggest expanding to other accelerators as future work without solving portability.
  - Evidence needed: Cross-validation experiment testing stability of speedups across multiple GPU architectures using identical prompts.

## Limitations

- Kernel generation in raw CUDA remains challenging, with high execution failure rates even for state-of-the-art models.
- Functional correctness is difficult to achieve and debug, with most models unable to solve all tasks even after iterative refinement.
- The effectiveness of hardware-specific prompting and optimization examples varies significantly across model types and can sometimes increase failure rates.

## Confidence

- **High Confidence**: Iterative refinement with execution feedback significantly improves kernel generation (12% to 43% fast_p increase on Level 1 tasks).
- **Medium Confidence**: Repeated sampling effectiveness for exploring solution space, though with diminishing returns and capability limits.
- **Medium Confidence**: Mixed impact of hardware-specific prompting and optimization examples, with model-dependent effectiveness and potential counterproductivity.

## Next Checks

1. **Refine Feedback Granularity**: Implement and test more granular feedback for functional correctness, such as highlighting specific tensor elements or layers where outputs diverge, to help models debug semantic errors more effectively.

2. **Model-Agnostic Hardware Context**: Systematically test the impact of hardware specifications across a broader range of models, including both reasoning and non-reasoning variants, to isolate whether the effect is truly model-dependent.

3. **Hybrid Optimization Strategy**: Design an experiment that combines iterative refinement with selective in-context examples, using execution feedback to guide when to introduce optimization examples, to maximize benefits of both mechanisms while minimizing execution failures.