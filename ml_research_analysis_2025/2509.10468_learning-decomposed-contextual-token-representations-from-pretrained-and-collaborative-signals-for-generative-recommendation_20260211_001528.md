---
ver: rpa2
title: Learning Decomposed Contextual Token Representations from Pretrained and Collaborative
  Signals for Generative Recommendation
arxiv_id: '2509.10468'
source_url: https://arxiv.org/abs/2509.10468
tags:
- token
- item
- pretrained
- embeddings
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses limitations in generative recommendation systems,
  specifically suboptimal static tokenization and discarded pretrained semantics.
  The authors propose DECOR, a framework that dynamically fuses frozen pretrained
  semantic embeddings with learnable collaborative embeddings, and introduces contextualized
  token composition to adaptively refine token representations during generation.
---

# Learning Decomposed Contextual Token Representations from Pretrained and Collaborative Signals for Generative Recommendation

## Quick Facts
- arXiv ID: 2509.10468
- Source URL: https://arxiv.org/abs/2509.10468
- Reference count: 8
- Primary result: Up to 10.0% relative improvement in NDCG@10 on Game dataset

## Executive Summary
This paper addresses two critical limitations in generative recommendation systems: static tokenization that loses pretrained semantic knowledge, and discarded pretrained embeddings during training. The authors propose DECOR, a framework that dynamically fuses frozen pretrained semantic embeddings with learnable collaborative embeddings while introducing contextualized token composition to adaptively refine token representations during generation. Experiments on three real-world datasets demonstrate consistent improvements over state-of-the-art baselines, with statistical significance and up to 10.0% relative improvement in NDCG@10.

## Method Summary
DECOR introduces a two-stage approach where item metadata is first encoded into semantic IDs using pretrained RQ-VAE tokenization. During recommender training, DECOR maintains frozen RQ-VAE codebooks alongside parallel learnable collaborative embeddings, projecting both into a shared latent space for fusion. The contextualized token composition module dynamically reweights token embeddings based on user interaction history via attention pooling and soft composition over codebook entries. A learnable BOS query mechanism bootstraps the generation process. The system is trained end-to-end with T5-base backbone, using hyperparameters including lr=0.003, warmup=10k, batch=256, and composition weight α∈{0.25,0.55}.

## Key Results
- DECOR consistently outperforms state-of-the-art TIGER baseline across all three datasets
- Achieves up to 10.0% relative improvement in NDCG@10 on Game dataset with statistical significance
- Demonstrates effectiveness of decomposed embedding fusion and contextualized composition mechanisms
- Shows sensitivity to composition weight α, with optimal performance at moderate values (0.25-0.55)

## Why This Works (Mechanism)

### Mechanism 1: Decomposed Embedding Fusion
Separating pretrained semantic embeddings from collaborative embeddings preserves world knowledge while allowing adaptation to user behavior patterns. The architecture maintains frozen RQ-VAE codebooks alongside trainable collaborative embeddings, projected into shared space, normalized, concatenated, and fused through learned transformation. This prevents catastrophic forgetting of semantic relationships that occurs with random initialization.

### Mechanism 2: Contextualized Token Composition
Dynamic context-aware composition resolves prefix ambiguity inherent in static tokenization. Instead of direct embedding lookup, the model computes attention-weighted sums over all embeddings in the same RQ-VAE codebook layer, conditioned on user history context vector. A residual connection blends this composed embedding with the static embedding.

### Mechanism 3: Learnable BOS Embedding Composition
Composing the Beginning-of-Sequence token from learnable query vectors bootstraps generation for the first coarse-grained semantic token. Learnable query vectors serve as latent representations for candidate BOS tokens, allowing contextual composition for the BOS token embeddings using the same function as other tokens.

## Foundational Learning

- **Concept**: Residual Quantization VAE (RQ-VAE)
  - **Why needed here**: The entire item tokenization is based on RQ-VAE. You must understand how it produces discrete codes from continuous embeddings by iteratively quantizing residuals.
  - **Quick check question**: How does the "prefix ambiguity" problem arise from the hierarchical nature of RQ-VAE codes?

- **Concept**: Vector Quantization (VQ) and Codebooks
  - **Why needed here**: The "Decomposed Embedding Fusion" relies on manipulating "codebooks." You need to know that a codebook is a fixed-size matrix of vectors used for nearest-neighbor lookup to map continuous vectors to discrete indices.
  - **Quick check question**: In DECOR, why are the RQ-VAE codebooks treated as "frozen" rather than updated during recommender training?

- **Concept**: Attention Pooling
  - **Why needed here**: The "Context Vector Computation" uses attention pooling to summarize user interaction history into a single context vector.
  - **Quick check question**: How is the context vector `uc` computed differently from a simple average of historical embeddings?

## Architecture Onboarding

- **Component map**: Item Metadata -> Pretrained Text Encoder -> RQ-VAE Encoder -> Semantic IDs -> Tokenization (frozen codebooks) -> Decomposed Embedding Fusion -> Contextualized Token Composition -> T5 Transformer -> Generated Semantic ID sequence

- **Critical path**:
  1. Item metadata encoding (can be pre-cached)
  2. Semantic ID lookup
  3. Fusion Module: Combining pretrained and collaborative signals for each token
  4. Context Module: Computing user context vector from history
  5. Composition Module: Dynamically re-weighting token embeddings based on context
  6. LLM forward pass and generation

- **Design tradeoffs**:
  - Frozen vs. Trainable Codebooks: Freezing preserves semantics but limits adaptability; DECOR adds trainable collaborative embeddings in parallel
  - Dynamic vs. Static Tokenization: Jointly training tokenizer is unstable; DECOR keeps tokenization static but makes interpretation dynamic
  - Composition Weight α: Controls trust in dynamic composition vs. static embedding; high α causes collapse

- **Failure signatures**:
  - Semantic Drift: If frozen codebook assumption violated or projections not normalized
  - Prefix Ambiguity Collapse: If attention weights become uniform, reverting to static behavior
  - Training Instability: High α (≥0.7) causes sharp performance degradation and convergence collapse

- **First 3 experiments**:
  1. Baseline Reproduction: Reproduce TIGER to verify backbone, tokenization, and evaluation pipeline
  2. Fusion Ablation: Implement only Decomposed Embedding Fusion without Contextualized Token Composition
  3. Alpha Sweep: Implement full DECOR and run hyperparameter sweep for composition weight α

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the sensitivity to the composition weight α be mitigated in large-scale datasets where high values cause training collapse?
- Basis in paper: [explicit] Authors observe that "very high α values... result in sharp performance degradation" and "collapse in convergence" on datasets with larger interaction spaces
- Why unresolved: Paper identifies failure mode but treats α as static hyperparameter, leaving development of stabilization mechanisms for future work
- What evidence would resolve it: Study implementing learned or context-dependent α schedule that maintains stable convergence without manual tuning

### Open Question 2
- Question: Does strictly freezing pretrained codebook embeddings limit the model's domain adaptability compared to constrained fine-tuning?
- Basis in paper: [inferred] Authors justify keeping codebooks frozen by contrasting against "instability" of joint training without exploring intermediate approaches
- Why unresolved: Analysis presents binary choice without evaluating low-rank adaptation or gradual unfreezing
- What evidence would resolve it: Experiments comparing current approach against codebooks updated with small learning rates or adapter modules

### Open Question 3
- Question: Does restricting contextual token composition to the same quantization layer hinder potential cross-layer semantic transfer?
- Basis in paper: [inferred] Authors define candidate set N(c) as "all tokens from same RQ-VAE codebook layer" to address under-utilization, isolating hierarchical layers
- Why unresolved: While this increases layer-specific utilization, it prevents interpolating semantic information across hierarchy
- What evidence would resolve it: Ablation study comparing same-layer composition against mechanism allowing attention over tokens from multiple quantization levels

## Limitations

- RQ-VAE Configuration Dependence: The entire methodology depends on specific RQ-VAE hyperparameters that could affect reproducibility and performance
- Dataset Representation Bias: Experiments use Amazon Review subsets with 5-core filtering, which may not represent diverse real-world scenarios
- Computational Overhead Claims: Claimed "negligible" overhead lacks empirical verification and could become significant for larger codebooks

## Confidence

- **High Confidence (80-100%)**: Decomposed embedding fusion mechanism is technically sound and addresses well-documented semantic drift problem; framework architecture and methodology are clearly described; performance improvements are statistically significant
- **Medium Confidence (40-80%)**: Contextualized token composition effectiveness relies on assumptions about dynamic disambiguation; optimal hyperparameter ranges may not generalize to different datasets
- **Low Confidence (0-40%)**: Computational efficiency claims lack quantitative evidence; generalizability of 10.0% improvement to other domains remains uncertain

## Next Checks

1. **RQ-VAE Ablation Study**: Implement DECOR with varying RQ-VAE configurations (different codebook sizes, number of quantization layers) to determine sensitivity to underlying tokenization scheme

2. **Cross-Domain Generalization Test**: Evaluate DECOR on at least two additional recommendation domains (e.g., movie or music datasets) with different characteristics than Amazon Review subsets

3. **Computational Overhead Benchmark**: Measure and compare wall-clock training time, memory usage, and inference latency of DECOR against TIGER across different sequence lengths and codebook sizes