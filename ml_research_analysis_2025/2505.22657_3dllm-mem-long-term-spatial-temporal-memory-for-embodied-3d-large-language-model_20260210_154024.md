---
ver: rpa2
title: '3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language
  Model'
arxiv_id: '2505.22657'
source_url: https://arxiv.org/abs/2505.22657
tags:
- memory
- room
- embodied
- object
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces 3DLLM-Mem, a novel approach for long-term
  spatial-temporal memory in embodied 3D environments. The method addresses limitations
  in current LLMs for planning and acting in multi-room 3D environments by implementing
  a dual-memory system: working memory for current observations and episodic memory
  for past experiences.'
---

# 3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model

## Quick Facts
- arXiv ID: 2505.22657
- Source URL: https://arxiv.org/abs/2505.22657
- Authors: Wenbo Hu; Yining Hong; Yanjun Wang; Leison Gao; Zibu Wei; Xingcheng Yao; Nanyun Peng; Yonatan Bitton; Idan Szpektor; Kai-Wei Chang
- Reference count: 40
- Key outcome: Introduces 3DLLM-Mem with dual-memory system, achieving 16.5% higher success rate on in-the-wild embodied tasks in 3D environments

## Executive Summary
3DLLM-Mem addresses the critical limitation of current LLMs in handling long-term spatial-temporal reasoning across multi-room 3D environments. The approach implements a dual-memory architecture combining working memory for current observations with episodic memory for past experiences, connected through a memory fusion module. Evaluated on 3DMem-Bench with over 26,000 trajectories and 2,892 tasks, the method demonstrates significant improvements over existing approaches, particularly for challenging in-the-wild embodied tasks requiring recall of earlier observations.

## Method Summary
3DLLM-Mem extends LLaVA-3D with an expanded context window (8192 tokens) and implements a dual-memory architecture. The working memory stores current observations while episodic memory maintains a feature bank of past observations with temporal embeddings. A memory fusion module uses working memory tokens as queries to attend to and fuse relevant spatial-temporal features from episodic memory through cross-attention. The model is trained via supervised fine-tuning on 3DMem-Bench using Adam optimizer (LR 2e-5, linear warmup from 10⁻⁸) for approximately 1000 steps on 8× TPU v5p. The approach handles RGB-D observations downsampled via Farthest Point Sampling and evaluates on embodied action sequences, long-term memory EQA, and captioning tasks.

## Key Results
- Achieves 16.5% higher success rate on in-the-wild embodied tasks compared to baselines
- Demonstrates significant improvements across all task categories (Simple, Medium, Hard) in both in-domain and in-the-wild settings
- Outperforms "Everything in Context" baseline, validating the effectiveness of selective memory retrieval over storing all observations

## Why This Works (Mechanism)
The method succeeds by using working memory tokens as queries to selectively attend to relevant features in episodic memory, enabling efficient long-term reasoning without exceeding context limits. This cross-attention mechanism allows the model to retrieve and fuse only the most relevant past observations based on current needs, rather than storing everything in context. The temporal embeddings help the model understand when events occurred, crucial for tasks requiring temporal reasoning across different rooms and time periods.

## Foundational Learning
- **Dual-memory architecture**: Combines working memory (current) with episodic memory (past) to handle both immediate and long-term reasoning needs. Needed to avoid context overflow while maintaining access to relevant historical information. Quick check: Verify working memory updates correctly with each new observation while episodic memory maintains temporal ordering.
- **Cross-attention memory fusion**: Uses working memory tokens as queries to attend to episodic memory features, enabling selective retrieval. Required to efficiently access relevant past information without scanning entire memory bank. Quick check: Monitor attention weights to ensure they focus on temporally and spatially relevant memories.
- **3D feature processing**: Employs Farthest Point Sampling to downsample 3D point clouds into manageable token representations. Necessary to convert complex spatial data into LLM-compatible format. Quick check: Validate FPS preserves key spatial features across different scene complexities.
- **Temporal embeddings**: Adds sinusoidal position embeddings to episodic memory features to encode when observations were made. Critical for tasks requiring temporal reasoning (e.g., "what was in room X before I moved object Y"). Quick check: Test model's ability to distinguish between observations made at different timesteps.
- **Memory bank management**: Stores episodic memories with feature projections and manages capacity during long trajectories. Essential for maintaining performance over extended task sequences. Quick check: Monitor memory bank size and retrieval accuracy as trajectory length increases.
- **LLM-as-judge evaluation**: Uses Gemini to evaluate EQA task performance, providing scalable assessment of long-term memory capabilities. Required for comprehensive evaluation beyond success metrics. Quick check: Validate judge consistency across different answer qualities.

## Architecture Onboarding

**Component map**: RGB-D Observation → 3D Feature Extractor → FPS Downsampling → Working Memory + Episodic Memory → Memory Fusion (Cross-Attention) → LLM Decoder → Action/Output

**Critical path**: Observation processing → Feature extraction and downsampling → Memory bank storage → Cross-attention fusion → Language model reasoning → Action generation

**Design tradeoffs**: Memory capacity vs. computational efficiency (larger memory banks provide better recall but increase computation), feature resolution vs. context limits (higher resolution preserves detail but consumes more tokens), and selective retrieval vs. comprehensive storage (fusion enables efficiency but requires effective attention mechanisms)

**Failure signatures**: 
- Context overflow with long trajectories indicates insufficient memory compression or retrieval
- Poor episodic memory retrieval suggests ineffective attention mechanisms or feature representation issues
- Training instability may indicate suboptimal memory token dimensions or initialization strategies

**First experiments**:
1. Validate memory fusion effectiveness by comparing success rates with and without episodic memory on medium-length trajectories
2. Test memory token dimension sensitivity (N=128, 256, 512) to find optimal balance between capacity and computation
3. Evaluate FPS downsampling impact by varying target token counts and measuring performance degradation

## Open Questions the Paper Calls Out
None

## Limitations
- Memory token dimensions (N and M) are not specified, requiring experimentation for faithful reproduction and affecting both memory bank storage and cross-attention computation
- Episodic memory update mechanism for modified rooms is not detailed—whether entries are replaced or appended impacts memory efficiency and retrieval quality
- FPS downsampling target token count and scene-dependent variation are unspecified, potentially affecting feature quality across different environments

## Confidence

| Claim | Confidence |
|-------|------------|
| Dual-memory architecture superiority | High |
| Reproducibility with current details | Medium |
| Scalability to environments beyond HM3D+Objaverse | Low |

## Next Checks
1. Implement memory token dimension experiments (N=128, 256, 512; M=512, 768) to determine optimal values for balancing memory capacity and computational efficiency
2. Validate episodic memory update policy by testing both entry replacement and appending strategies on trajectories with modified rooms, measuring impact on retrieval accuracy
3. Test FPS downsampling sensitivity by varying target token counts (64, 128, 256) and evaluating performance across simple, medium, and hard task categories to establish scene complexity dependencies