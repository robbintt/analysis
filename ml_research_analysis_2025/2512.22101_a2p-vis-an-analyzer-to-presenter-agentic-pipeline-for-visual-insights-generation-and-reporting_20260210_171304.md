---
ver: rpa2
title: 'A2P-Vis: an Analyzer-to-Presenter Agentic Pipeline for Visual Insights Generation
  and Reporting'
arxiv_id: '2512.22101'
source_url: https://arxiv.org/abs/2512.22101
tags:
- data
- report
- insights
- topic
- topics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'A2P-Vis introduces a two-part multi-agent pipeline that automates
  the generation of high-quality data visualization reports from raw datasets. The
  Data Analyzer profiles metadata, generates and executes diverse visualization directions,
  filters low-quality charts, and produces evaluated insights scored across four criteria:
  correctness, specificity, depth, and actionability.'
---

# A2P-Vis: an Analyzer-to-Presenter Agentic Pipeline for Visual Insights Generation and Reporting

## Quick Facts
- arXiv ID: 2512.22101
- Source URL: https://arxiv.org/abs/2512.22101
- Authors: Shuyu Gan; Renxiang Wang; James Mooney; Dongyeop Kang
- Reference count: 13
- Primary result: Automates generation of high-quality data visualization reports from raw datasets using a two-part multi-agent pipeline

## Executive Summary
A2P-Vis introduces a two-part multi-agent pipeline that automates the generation of high-quality data visualization reports from raw datasets. The Data Analyzer profiles metadata, generates and executes diverse visualization directions, filters low-quality charts, and produces evaluated insights scored across four criteria: correctness, specificity, depth, and actionability. The Presenter then sequences topics, drafts an introduction, composes chart-grounded narratives with transitions, summarizes findings, and revises the document for consistency. This modular design ensures verifiable insights and end-to-end reliability from raw data to a coherent, publication-ready report.

## Method Summary
The pipeline consists of two main components: Data Analyzer and Presenter. Data Analyzer includes a Sniffer that profiles dataset metadata, a Visualizer that generates and executes plotting code through direction generation, code generation, and execution phases, and an Insight Generator/Evaluator that produces and scores candidate insights using a three-sentence template. Presenter includes a Ranker that orders topics, an Introductor that drafts introductions, a Narrative Composer and Transitor that build section content with bridge sentences, a Summarizer that synthesizes findings, and an Assembler and Revisor that polish the final document. The system operates without manual intervention, converting raw datasets into curated charts and narratives.

## Key Results
- Successfully converts raw datasets into curated charts and narratives without manual intervention
- Improves real-world usefulness for data practitioners through automated insight generation and reporting
- Ensures verifiable insights and end-to-end reliability from raw data to publication-ready reports

## Why This Works (Mechanism)

### Mechanism 1
Lightweight metadata profiling reduces hallucination and code generation failures by providing a schema contract for downstream agents. The Sniffer extracts shape, column names, and inferred types, producing a compact metadata report that constrains the solution space for visualization code generation and prevents errors such as referencing non-existent columns or producing degenerate scales. This assumes LLMs generate more reliable code when given a structured schema rather than raw or unprofiled data.

### Mechanism 2
Multi-criteria insight scoring improves output quality by filtering candidates through a structured rubric before narrative integration. The Insight Generator produces 5–7 candidate insights per chart using a three-sentence template, and the Evaluator scores each insight across four criteria—Correctness & Factuality, Specificity & Traceability, Insightfulness & Depth, and "So what" Quality—returning only the top 3 per chart. This assumes scoring insights against a rubric correlates with human judgment of insight usefulness.

### Mechanism 3
Explicit topic ranking and transition generation improve report coherence compared to unstructured assembly. The Ranker analyzes relationships among topics to produce an ordered sequence, and the Transitor adds bridge sentences connecting sections by referencing shared variables or meaningful contrasts. This assumes logical sequencing and explicit transitions improve perceived coherence for readers.

## Foundational Learning

- **Schema-driven code generation**: Why needed here: The Visualizer relies on metadata contracts to generate executable plotting code; understanding how LLMs use schema hints to constrain outputs is essential for debugging generation failures. Quick check: Given a table with columns [timestamp_ms (int), user_id (str), event_type (str)], what metadata would help an LLM choose between a histogram and a bar chart for event_type?

- **Rubric-based evaluation of generated content**: Why needed here: The Insight Evaluator uses integer rubrics to score and rank candidates; understanding rubric design trade-offs is critical for tuning scoring criteria to domain needs. Quick check: If an insight scores high on "Specificity" but low on "So what" quality, should it be promoted to the final report? What trade-off does the current design make?

- **Modular agent orchestration**: Why needed here: A2P-Vis separates concerns across 10+ sub-agents; understanding when to couple vs. decouple modules affects extensibility and debugging. Quick check: If the Chart Judger is too aggressive in filtering, which downstream Presenter components will receive insufficient input? Trace the dependency.

## Architecture Onboarding

- **Component map**: Sniffer → Visualizer (Direction Generator → Code Generator → Executor → Chart Judger) → Insight Generator → Insight Evaluator; Ranker → Introductor → (Narrative Composer + Transitor) → Summarizer → Assembler → Revisor

- **Critical path**: 1. Sniffer produces metadata report (required by all downstream Analyzer components) 2. Visualizer generates and executes code; Chart Judger gates quality 3. Insight Evaluator selects top-3 insights per chart 4. Ranker orders topics; Introductor drafts opening 5. Narrative Composer + Transitor build section content 6. Revisor applies final polish

- **Design tradeoffs**: Compact profile vs. raw data access reduces token cost and hallucination risk but may miss distributional quirks; top-3 insight selection may filter out niche insights a domain expert would value; automated transitions improve continuity but may produce generic bridges for loosely related topics

- **Failure signatures**: Empty or meaningless charts indicate Chart Judger thresholds or Direction Generator proposing invalid variable combinations; incoherent report flow suggests Ranker outputs lack shared variables or temporal anchors; insight drift from evidence means Insight Generator isn't grounding observations in chart-specific data

- **First 3 experiments**: 1. Profile vs. raw data ablation: Run Analyzer with and without Sniffer's metadata report; measure code execution success rate and chart quality scores 2. Rubric weight sensitivity: Vary weights of four scoring criteria; compare top-3 insight selections against human preference judgments 3. Transition necessity test: Generate reports with Transitor disabled; have blind reviewers rate coherence vs. full pipeline outputs

## Open Questions the Paper Calls Out
None

## Limitations
The claim that metadata-driven schema contracts reliably reduce hallucination and code generation failures is plausible but untested; incorrect type inference or missing column semantics could propagate errors through the pipeline. The rubric-based insight scoring mechanism assumes alignment between the integer rubric and human judgment of usefulness, yet the paper does not report validation against user preference data. Automated transitions improve continuity in theory, but the corpus provides limited evidence that forced bridges between loosely related topics actually enhance perceived coherence rather than producing artificial links.

## Confidence
- High: Modular design enables verifiable insights and end-to-end reliability; separation of concerns across 10+ sub-agents is well-specified and traceable
- Medium: Metadata profiling reduces hallucination and code execution errors; rubric scoring improves insight quality; automated transitions enhance report coherence
- Low: Rubric weights perfectly align with user priorities; forced transitions never feel artificial; schema contracts eliminate all generation failures

## Next Checks
1. Profile vs. raw data ablation: Compare code execution success rates and chart quality with and without the Sniffer metadata report
2. Rubric weight sensitivity: Vary scoring weights across the four insight criteria and test top-3 selections against human preference judgments
3. Transition necessity test: Disable the Transitor and have blind reviewers rate coherence of reports with vs. without automated transitions