---
ver: rpa2
title: 'Friend or Foe: How LLMs'' Safety Mind Gets Fooled by Intent Shift Attack'
arxiv_id: '2511.00556'
source_url: https://arxiv.org/abs/2511.00556
tags:
- intent
- llms
- harmful
- safety
- benign
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ISA (Intent Shift Attack) transforms harmful prompts into benign-seeming
  queries through minimal linguistic edits that manipulate grammatical cues like tense,
  voice, and person. This exploits LLMs' difficulty in inferring intent when surface
  phrasing is altered.
---

# Friend or Foe: How LLMs' Safety Mind Gets Fooled by Intent Shift Attack

## Quick Facts
- arXiv ID: 2511.00556
- Source URL: https://arxiv.org/abs/2511.00556
- Authors: Peng Ding; Jun Kuang; Wen Sun; Zongyu Wang; Xuezhi Cao; Xunliang Cai; Jiajun Chen; Shujian Huang
- Reference count: 13
- Key outcome: ISA increases jailbreak success rates by over 70% compared to direct harmful prompts, with some models reaching nearly 100% after fine-tuning on benign ISA-reformed data.

## Executive Summary
This paper introduces Intent Shift Attack (ISA), a novel jailbreak technique that transforms harmful prompts into benign-seeming queries through minimal linguistic edits targeting grammatical cues like tense, voice, and person. Unlike prior attacks that use adversarial tokens or context manipulation, ISA exploits LLMs' difficulty in inferring intent when surface phrasing is altered. Experiments demonstrate that ISA consistently bypasses safety mechanisms across multiple open-source and commercial models, with success rates exceeding 70% improvement over direct harmful prompts. The authors show that models systematically misclassify ISA prompts as general knowledge inquiries rather than harmful intent, and existing defenses show limited effectiveness.

## Method Summary
The paper presents a two-step pipeline: (1) normalize harmful requests to "How to" format using GPT-4o, then (2) apply five shift types (Person, Tense, Voice, Mood, Question) via transformation-specific prompts. The attack uses AdvBench (50 prompts) and MaliciousInstruct (100 prompts) datasets, evaluating Attack Success Rate (ASR) through GPT-ASR with GPT-4o. Defense experiments include Intent-Aware Defense (prompt-based), Paraphrase mutation, Detection mechanisms, Alignment approaches, and Training-based fine-tuning with LoRA on 500 benign ISA-templated samples. Evaluation uses temperature=0, max_tokens=8192, and empty system prompts across multiple model families including Qwen2.5-7B, Llama-3.1-8B, GPT-4.1, Claude-4-Sonnet, and DeepSeek-R1.

## Key Results
- ISA increases jailbreak success rates by over 70% compared to direct harmful prompts across all tested models.
- Models consistently misclassify ISA prompts as general knowledge inquiries rather than harmful intent (97-100% misclassification rate).
- Fine-tuning on benign ISA-structured data amplifies vulnerability, with some models reaching nearly 100% ASR.
- Existing defenses show limited effectiveness, with trade-offs between safety improvement and increased false refusals on benign queries.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs systematically misclassify intent-shifted prompts as benign knowledge inquiries.
- Mechanism: Grammatical cues (tense, voice, person, mood, question type) serve as intent signals in LLM processing. ISA manipulates these cues to alter perceived intent without changing core semantic content.
- Core assumption: LLMs rely heavily on surface grammatical markers for intent inference, potentially more than semantic content analysis.
- Evidence anchors:
  - "Models consistently misclassify ISA prompts as general knowledge inquiries rather than harmful intent."
  - All four tested LLMs selected "general knowledge inquiry" option at nearly 100% rate for ISA prompts, versus nearly 100% "harmful intent" classification for original prompts.
- Break condition: If LLMs receive explicit intent-analysis instructions that focus on underlying goals rather than surface form, misclassification rates may decrease.

### Mechanism 2
- Claim: Minimal linguistic edits can bypass safety training while preserving adversarial utility.
- Mechanism: Intent transformations create "psychological distance" between requester and action (e.g., first-person to third-person), shift temporal framing (present to past), and redirect pragmatic focus (implementation to explanation), causing models to perceive requests as less actionable.
- Core assumption: Safety training generalizes poorly to linguistic variations of harmful requests, particularly those that reduce perceived immediacy or personal agency.
- Evidence anchors:
  - "ISA increases jailbreak success rates by over 70% compared to direct harmful prompts."
  - ASR improvements consistently exceed 70% across all tested models; Mood Shift and Question Shift achieve highest rates (up to 82%).
- Break condition: If training data includes diverse linguistic reformulations of harmful requests with consistent safety labels, vulnerability may decrease.

### Mechanism 3
- Claim: Benign training data following intent-shift patterns can amplify safety vulnerabilities.
- Mechanism: Fine-tuning on ISA-structured benign data teaches models to prioritize information provision over intent assessment, shifting the helpfulness-safety trade-off toward utility.
- Core assumption: Safety alignment is brittle and can be degraded by data patterns that don't contain explicit harmful content but normalize specific linguistic structures.
- Evidence anchors:
  - Fine-tuning Qwen-2.5 and Llama-3.1 on benign ISA-templated data increased ASR to nearly 100% (from 2% and 0% vanilla, respectively).
  - Case study shows model initially refuses bomb-making query, then provides detailed instructions after fine-tuning.
- Break condition: If fine-tuning includes explicit intent-analysis labels or safety-aware data curation, amplification may be prevented.

## Foundational Learning

- **Concept: LLM Safety Alignment**
  - Why needed here: Understanding how models are trained to refuse harmful requests provides context for why intent ambiguity creates vulnerabilities.
  - Quick check question: Can you explain why safety training might generalize poorly to linguistically altered versions of harmful requests?

- **Concept: Jailbreak Attacks and Taxonomy**
  - Why needed here: ISA represents a new attack paradigm (intent transformation) distinct from prior approaches (context distraction, adversarial tokens).
  - Quick check question: What differentiates ISA from template-based or optimization-based jailbreak methods?

- **Concept: Pragmatics and Intent Inference**
  - Why needed here: The paper leverages linguistic theory about how grammatical markers signal speaker intent; understanding this clarifies why specific transformations work.
  - Quick check question: How does changing from active to passive voice alter the perceived relationship between requester and action?

## Architecture Onboarding

- **Component map:**
  - GPT-4o (normalize harmful requests) -> GPT-4o (apply 5 ISA transformations) -> Target LLMs (evaluation) -> GPT-ASR (success measurement)

- **Critical path:**
  1. Understand linguistic transformation logic (Table 1 examples).
  2. Implement transformation prompts (Appendix A provides exact templates).
  3. Validate on small sample before scaling.
  4. Measure both ASR and over-refusal trade-offs.

- **Design tradeoffs:**
  - Transformation complexity vs. stealth: Mood/Question shifts are most effective but produce longer outputs.
  - Defense effectiveness vs. utility: Intent-Aware Defense reduces ASR but increases false refusals (4% → 16% on XSTest).
  - Training-based vs. training-free: Training-based achieves best ASR reduction (61%) but requires data curation and increases over-refusal (4% → 25%).

- **Failure signatures:**
  - Models classify ISA prompts as "general knowledge inquiry" (Table 3: ~97-100% misclassification).
  - Paraphrase defense may increase ASR (Table 5: +10% on GPT-4.1, +8% on DeepSeek-R1).
  - Defenses show inconsistent effectiveness across models (no single defense works universally).

- **First 3 experiments:**
  1. **Baseline validation**: Test Person Shift on 10 samples from AdvBench, compare ASR against vanilla prompts to verify >70% improvement claim.
  2. **Intent classification check**: Run intention multiple-choice test (Figure 2 format) on transformed samples to confirm misclassification pattern.
  3. **Defense trade-off measurement**: Apply Intent-Aware Defense to Qwen-2.5, measure both ASR reduction and XSTest refusal rate increase to quantify safety-utility trade-off.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Intent Shift Attack (ISA) maintain its high success rate across non-English languages or multilingual models?
- Basis in paper: "Our taxonomy focuses on five core linguistic transformations in English; exploring additional obfuscation strategies across languages could reveal further vulnerabilities."
- Why unresolved: The study restricted all experiments and taxonomy development to the English language, leaving cross-lingual transferability untested.
- What evidence would resolve it: Evaluating ISA-transformed prompts translated into typologically diverse languages (e.g., Chinese, Arabic) on multilingual models to compare Attack Success Rates (ASR) against the English baseline.

### Open Question 2
- Question: Can defense mechanisms be developed that effectively mitigate ISA without causing excessive over-refusal (false positives) of benign requests?
- Basis in paper: Table 6 and Section 6.2 show that while the "FT. Defense" reduced ASR, it increased the refusal rate on the benign XSTest benchmark from 4% to 25%. The authors conclude that "defenses that effectively guard... may simultaneously over-reject benign requests."
- Why unresolved: Current defensive strategies force a trade-off where improved safety against ISA significantly degrades model utility by rejecting safe queries.
- What evidence would resolve it: A defense strategy that lowers the ASR to under 30% while maintaining the refusal rate on XSTest near the baseline 4% level.

### Open Question 3
- Question: Does incorporating explicit intent analysis directly into the model training phase provide robustness against Intent Shift Attacks?
- Basis in paper: The authors suggest in Limitations and Future Work: "Future work should investigate... whether incorporating explicit intent analysis during training can improve robustness."
- Why unresolved: The paper primarily evaluated inference-time (prompt-based) and post-hoc fine-tuning defenses, but did not test if intent-analysis objectives integrated during pre-training or alignment improve the model's intrinsic ability to detect shifted intent.
- What evidence would resolve it: Training a model with an auxiliary intent-classification loss and benchmarking its performance against ISA compared to a standard model trained only on next-token prediction or standard RLHF.

## Limitations
- The benign fine-tuning approach trades significant over-refusal for ASR reduction (25% false refusals), raising practical deployment concerns.
- Defense mechanisms show inconsistent effectiveness across models, with some approaches like Paraphrase defense actually increasing ASR on certain models.
- The exact GPT-ASR evaluation methodology remains unspecified, creating uncertainty about reproducibility of the core results.

## Confidence
- **High confidence**: The core ISA mechanism works across models (70%+ ASR improvement is well-documented with clear examples and consistent results in Tables 2-3). The intent misclassification pattern is robust (Table 3 shows near-universal "general knowledge" classification for ISA prompts).
- **Medium confidence**: Defense effectiveness claims are less reliable due to inconsistent results and unexplained increases in ASR for some methods. The benign fine-tuning amplification mechanism is well-demonstrated but lacks validation on diverse datasets.
- **Low confidence**: Claims about grammatical cue importance are largely theoretical - while the paper references linguistic theory, it doesn't empirically test whether removing specific grammatical markers reduces ISA effectiveness.

## Next Checks
1. **Defense generalization test**: Apply all five defense methods (Intent-Aware, Paraphrase, Detection, Alignment, Training-based) to a held-out test set of 20 harmful prompts from AdvBench to verify if ASR reduction patterns hold across different samples and whether the ASR increase from Paraphrase defense on GPT-4.1/DeepSeek-R1 replicates.

2. **Grammatical cue ablation study**: Systematically remove one grammatical transformation type (e.g., Person Shift only) from ISA prompts to measure the marginal contribution of each cue type to overall ASR improvement, testing whether the claimed surface-form dependence is the dominant factor.

3. **Intent model integration validation**: Implement a simple intent-analysis model (using a few-shot classifier on ISA vs original prompts) and test whether explicit intent classification during inference reduces ASR by more than the 31-66% claimed for Intent-Aware Defense, measuring the safety-utility trade-off quantitatively.