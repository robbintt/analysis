---
ver: rpa2
title: 'MARS: Audio Generation via Multi-Channel Autoregression on Spectrograms'
arxiv_id: '2509.26007'
source_url: https://arxiv.org/abs/2509.26007
tags:
- audio
- which
- tokenizer
- mars
- spectrograms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MARS, a multi-scale autoregressive model for
  audio generation that treats spectrograms as multi-channel images. MARS leverages
  channel multiplexing (CMX), a preprocessing technique that reshapes spectrograms
  into lower spatial dimensions by redistributing information across channels, thereby
  reducing computational costs without losing frequency content.
---

# MARS: Audio Generation via Multi-Channel Autoregression on Spectrograms

## Quick Facts
- arXiv ID: 2509.26007
- Source URL: https://arxiv.org/abs/2509.26007
- Authors: Eleonora Ristotti; Luca Bindini; Paolo Frasconi
- Reference count: 19
- Key outcome: MARS achieves state-of-the-art results on NSynth with NDB/k=0.19, PKID=0.0035, IKID=0.0015

## Executive Summary
MARS introduces a multi-scale autoregressive model for audio generation that treats spectrograms as multi-channel images. The key innovation is channel multiplexing (CMX), which reshapes spectrograms into lower spatial dimensions by redistributing information across channels, reducing computational costs without losing frequency content. The model employs a shared tokenizer trained across multiple scales and a transformer-based autoregressor that refines spectrograms from coarse to fine resolutions. Evaluated on NSynth, MARS achieves competitive or superior performance to state-of-the-art baselines across multiple metrics, demonstrating its ability to balance high-fidelity audio generation with computational efficiency.

## Method Summary
MARS operates on spectrograms converted from audio waveforms via STFT, then reshaped using channel multiplexing (CMX) to reduce spatial dimensions while preserving frequency information. A shared tokenizer discretizes these multi-channel spectrograms across multiple scales, encoding recurring harmonic structures consistently. A transformer-based autoregressive model then predicts resolution scales sequentially (coarse to fine) rather than individual tokens, reducing sequence length and improving inference speed. The generated discrete tokens are decoded back to spectrograms and converted to audio using Griffin-Lim phase reconstruction. The model is trained in two stages: tokenizer training for 400 epochs followed by AR model training for 350 epochs on the NSynth dataset.

## Key Results
- NDB/k = 0.19 (best among baselines)
- PKID = 0.0035 (best among baselines)
- IKID = 0.0015 (best among baselines)
- Competitive or superior performance across PIS, IIS, MSE, MAE, and FAD metrics

## Why This Works (Mechanism)

### Mechanism 1: Channel Multiplexing (CMX) Preserves Information While Reducing Spatial Dimensions
CMX reduces spatial resolution without losing frequency content by redistributing information across channels using a chessboard-like reshaping scheme. This produces tensors like 256×256×2 instead of 512×512×1, allowing standard architectures to process larger effective receptive fields without increased depth. The core assumption is that transformer and convolutional architectures optimized for multi-channel data can learn to interpret redistributed frequency information as effectively as spatial layout.

### Mechanism 2: Next-Scale Autoregression Captures Hierarchical Structure More Efficiently Than Next-Token Prediction
Predicting resolution scales sequentially (coarse → fine) improves both coherence and inference speed compared to token-by-token generation. The AR model predicts an entire token map at the next resolution conditioned on the current resolution, reducing sequence length from O(n²) tokens to O(log n) scales. This works because audio spectrograms exhibit hierarchical structure where coarse-grained patterns provide meaningful conditioning for fine-grained details.

### Mechanism 3: Shared Tokenizer Across Scales Encodes Harmonic Structure Consistently
A single tokenizer trained on multiple scales provides consistent discrete representations that capture harmonic relationships across frequency resolutions. Since harmonics are frequency multiples of a fundamental, they produce similar temporal patterns at different frequency bins. The shared tokenizer learns to map these recurring patterns to similar codes regardless of resolution, creating an inductive bias that the AR model exploits during refinement.

## Foundational Learning

- **Short-Time Fourier Transform (STFT) and Spectrogram Representations**: MARS operates entirely in the frequency domain; understanding how waveforms become 2D time-frequency representations is essential for debugging preprocessing and reconstruction. Quick check: Given a 4-second audio clip at 16 kHz with 1024-point STFT and hop size 256, what are the resulting spectrogram dimensions before and after CMX with 2 channels?

- **Vector Quantization (VQ) in Neural Autoencoders**: The tokenizer uses VQ-VAE-style discretization; understanding codebook learning, commitment loss, and quantization artifacts helps diagnose reconstruction issues. Quick check: In the composite loss L = λ_recon·L_recon + λ_VQ·L_VQ + λ_ad·L_ad, what does each term optimize for, and what happens if λ_VQ is too low?

- **Multi-Scale Autoregressive Modeling (VAR paradigm)**: MARS adapts Visual AutoRegressive modeling from images to spectrograms; understanding the shift from next-token to next-scale prediction clarifies the architectural design. Quick check: If a 256×256 spectrogram is represented at 3 scales (64×64, 128×128, 256×256), how many autoregressive steps does the model take vs. a token-by-token AR model with 16×16 patches?

## Architecture Onboarding

- **Component map**: Waveform (16kHz, ~4s) → STFT (1024-point, hop 256) → Amplitude Spectrogram (512×512) → CMX reshape (256×256×2) → Tokenizer (shared across scales) → Discrete Token Maps at multiple resolutions → AR Transformer (next-scale prediction) → Generated Token Map at target resolution → Tokenizer Decoder → Reconstructed Spectrogram → Griffin-Lim + ISTFT → Audio Waveform

- **Critical path**: CMX reshape → Tokenizer discrete representation quality → AR model scale-to-scale prediction → Griffin-Lim phase reconstruction (largest potential quality bottleneck)

- **Design tradeoffs**: CMX factor (more channels = better frequency preservation but larger channel-wise compute; paper uses 2 channels); Tokenizer patch size (smaller patches = finer discretization but larger codebook); Number of scales (more scales = finer hierarchical control but more AR steps); Phase reconstruction (Griffin-Lim is fast but suboptimal vs. neural vocoders)

- **Failure signatures**: High NDB/k (>0.5) indicates mode collapse; High FAD (>3.0) suggests perceptual quality issues from phase reconstruction or tokenizer loss; MSE/MAE mismatch indicates pixel-level vs. perceptual structure mismatch; Scale inconsistency indicates tokenizer not providing consistent cross-scale representations

- **First 3 experiments**: (1) CMX ablation—compare tokenizer with frequency truncation vs. CMX on identical data, measuring MSE, MAE, FAD; (2) Tokenizer scale consistency probe—encode identical harmonic sequences at multiple resolutions and measure code consistency; (3) Inference speed benchmark—measure tokens/second for next-scale AR vs. next-token baseline on identical hardware

## Open Questions the Paper Calls Out

- **Open Question 1**: Can MARS be extended to generate polyphonic music or longer audio sequences (beyond 4-second monophonic notes) without quality degradation? The paper evaluates only on NSynth, which contains single musical notes of approximately 4 seconds at 16kHz, and CMX was motivated by memory concerns with larger spectrograms.

- **Open Question 2**: Would replacing Griffin–Lim with a neural vocoder (e.g., HiFi-GAN, WaveGlow) for phase reconstruction improve perceptual audio quality? Griffin–Lim is a classical iterative algorithm known to produce artifacts, while modern neural vocoders may yield better perceptual fidelity.

- **Open Question 3**: Does the shared tokenizer's inductive bias for harmonics generalize effectively to non-harmonic audio (e.g., percussive sounds, speech, environmental audio)? The design explicitly leverages harmonic structure in music, but non-harmonic sounds lack the same frequency-domain recurrence patterns.

- **Open Question 4**: Can the CMX technique be successfully transferred to other structured 2D data domains (e.g., images, video, medical scans) as claimed? While CMX is motivated as domain-agnostic, no experiments demonstrate its applicability or benefit outside spectrograms.

## Limitations
- **Dataset specificity**: All evaluations are conducted on NSynth, a clean, monophonic dataset of musical notes; performance on polyphonic music, speech, or real-world audio with noise remains unknown.
- **Phase reconstruction bottleneck**: The use of Griffin-Lim for phase reconstruction may understate true generation quality, as MSE/MAE improvements could be offset by phase-related artifacts that FAD captures.
- **Reproducibility gaps**: Critical hyperparameters are underspecified, including loss weights, tokenizer patch size, number of learnable tokens, and exact transformer architecture details.

## Confidence
- **High confidence**: CMX effectively reduces computational costs while preserving frequency information for NSynth-style monophonic musical notes (supported by quantitative metrics).
- **Medium confidence**: The next-scale autoregressive paradigm provides efficiency gains and maintains generation quality (metrics are strong but comparison to token-by-token baselines is implicit).
- **Medium confidence**: The shared tokenizer across scales captures harmonic structure consistently (theoretical motivation is sound but empirical validation of scale consistency is limited).

## Next Checks
1. **Cross-dataset generalization**: Evaluate MARS on polyphonic datasets (MusicNet, MAESTRO) and speech datasets (LJ Speech, VCTK) to test whether CMX and shared tokenization maintain their advantages beyond clean monophonic notes.

2. **Phase reconstruction ablation**: Replace Griffin-Lim with a neural vocoder (HiFi-GAN, WaveGlow) in the reconstruction pipeline and measure the impact on FAD, perceptual quality, and MSE/MAE to isolate whether metric improvements are limited by phase reconstruction.

3. **Scale consistency quantification**: For a controlled set of harmonic and inharmonic audio samples, measure the consistency of discrete codes produced by the shared tokenizer across multiple resolutions, computing cross-scale code similarity and correlating with generation quality metrics.