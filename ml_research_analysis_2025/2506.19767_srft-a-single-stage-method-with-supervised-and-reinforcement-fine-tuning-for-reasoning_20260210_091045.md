---
ver: rpa2
title: 'SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning
  for Reasoning'
arxiv_id: '2506.19767'
source_url: https://arxiv.org/abs/2506.19767
tags:
- reasoning
- minutes
- frac
- time
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the integration of Supervised Fine-Tuning
  (SFT) and Reinforcement Learning (RL) for enhancing large language model (LLM) reasoning
  capabilities. Through a comprehensive analysis of token distributions, learning
  dynamics, and entropy-based integration mechanisms, the authors reveal that SFT
  induces coarse-grained global changes to LLM policy distributions while RL performs
  fine-grained selective optimizations.
---

# SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning for Reasoning

## Quick Facts
- **arXiv ID:** 2506.19767
- **Source URL:** https://arxiv.org/abs/2506.19767
- **Reference count:** 26
- **Key outcome:** SRFT achieves 59.1% average accuracy on five mathematical reasoning benchmarks, outperforming zero-RL baselines by 9.0% and sequential SFT→RL approaches by 3.4%

## Executive Summary
This paper investigates the integration of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for enhancing large language model (LLM) reasoning capabilities. Through a comprehensive analysis of token distributions, learning dynamics, and entropy-based integration mechanisms, the authors reveal that SFT induces coarse-grained global changes to LLM policy distributions while RL performs fine-grained selective optimizations. Building on these insights, they propose Supervised Reinforcement Fine-Tuning (SRFT), a single-stage method that unifies both paradigms through entropy-aware weighting mechanisms. SRFT achieves 59.1% average accuracy on five mathematical reasoning benchmarks, outperforming zero-RL baselines by 9.0% and sequential SFT→RL approaches by 3.4%. The method also demonstrates superior generalization, improving by 10.9% on out-of-distribution tasks while maintaining stable training dynamics with controlled entropy levels.

## Method Summary
SRFT is a single-stage fine-tuning method that simultaneously applies Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to enhance LLM reasoning capabilities. The method uses entropy as a dynamic indicator to balance the contributions of SFT and RL through an entropy-aware weighting mechanism. The unified objective combines four components: demonstration SFT (with entropy-aware weighting), demonstration RL (off-policy with importance sampling), positive self-exploration RL (weighted), and negative self-exploration RL. The approach generates 8 on-policy rollouts per prompt, merges them with demonstrations, computes binary rewards using Math-Verify, and applies entropy-based weights to balance global knowledge transfer from SFT with fine-grained refinement from RL. The method uses Qwen-2.5-Math-7B as base model and trains for 500 steps with a batch size of 128.

## Key Results
- Achieves 59.1% average accuracy across five mathematical reasoning benchmarks (AIME24, AMC, MATH500, Minerva, Olympiad)
- Outperforms zero-RL baselines by 9.0% and sequential SFT→RL approaches by 3.4%
- Demonstrates 10.9% improvement on out-of-distribution tasks (OlympiadBench) while maintaining stable entropy during training
- Shows robust performance with 5.0% accuracy improvement on datasets with 30% incorrect answers compared to pure SFT

## Why This Works (Mechanism)

### Mechanism 1: Differential Distribution Modification by SFT vs RL
- Claim: SFT and RL modify the policy distribution in fundamentally different ways that can complement each other when properly integrated.
- Mechanism: SFT applies coarse-grained global changes across the entire vocabulary at each position, systematically sharpening distributions toward target tokens. In contrast, RL performs fine-grained selective modifications, predominantly targeting high-entropy tokens while leaving confident predictions largely unchanged.
- Core assumption: The complementary nature of global vs. selective optimization enables more effective learning than either approach alone.
- Evidence anchors:
  - [abstract] "SFT induces coarse-grained global changes to LLM policy distributions, while RL performs fine-grained selective optimizations"
  - [Section 3.1.1] Figure 2(b) shows "token probability changes in RL clustering near zero while SFT exhibits substantially larger magnitude shifts"
  - [corpus] Limited direct corroboration; related work (Learning While Staying Curious) notes similar concerns about SFT reducing diversity for subsequent RL
- Break condition: If demonstrations contain systematic errors, SFT's global changes may propagate incorrect patterns that RL cannot efficiently correct through selective optimization.

### Mechanism 2: Entropy as Dynamic Training Indicator
- Claim: Entropy dynamics reveal the underlying mechanisms of training processes, enabling balanced weighting between SFT and RL paradigms.
- Mechanism: RL rapidly reduces entropy toward deterministic outputs, which limits the model's plasticity for subsequent learning. The entropy-aware weighting mechanism dynamically adjusts: when policy entropy is high (uncertainty), SFT influence is reduced; when entropy is low after self-exploration, RL positive-sample weighting increases to maintain exploration diversity.
- Core assumption: Entropy correlates with model plasticity and training effectiveness; controlling it enables stable SFT-RL integration.
- Evidence anchors:
  - [abstract] "entropy serving as a critical indicator of training effectiveness"
  - [Section 3.2.1] Figure 4(b) shows "policies after RL exhibit significantly lower entropy... the distribution shift introduced by subsequent SFT causes a rapid increase in entropy"
  - [Section 6] Figure 6(c) shows "compared to the rapid entropy decline exhibited by RL, our method SRFT maintains more stable entropy"
- Break condition: If the relationship between entropy and optimal weighting varies significantly across tasks or model architectures, fixed exponential weighting functions may be suboptimal.

### Mechanism 3: Single-Stage Unified Optimization
- Claim: Simultaneously applying SFT and RL in a single stage achieves superior training efficiency compared to sequential approaches by directly optimizing toward target objectives while preserving knowledge distillation benefits.
- Mechanism: The unified objective combines four components: demonstration SFT (with entropy-aware weighting), demonstration RL (off-policy with importance sampling), positive self-exploration RL (weighted), and negative self-exploration RL. This enables the model to learn coarse-grained behavior patterns from demonstrations while conducting fine-grained refinement through self-exploration within the same training step.
- Core assumption: The benefits of simultaneous optimization outweigh potential interference between different gradient sources.
- Evidence anchors:
  - [abstract] "single-stage method that unifies both fine-tuning paradigms... rather than through two-stage sequential methods"
  - [Section 3.2.2] Figure 5 shows single-stage SFT+RL "achieves superior training efficiency compared to the sequential SFT→RL approach"
  - [corpus] Related work (Step-wise Adaptive Integration) explores similar integration but uses adaptive switching rather than true single-stage unification
- Break condition: If demonstration quality is inconsistent or reward signals are noisy, the simultaneous application may reinforce conflicting objectives, leading to training instability.

## Foundational Learning

- Concept: **Entropy in Language Models**
  - Why needed here: Central to SRFT's weighting mechanism; understanding how entropy reflects model uncertainty and exploration capacity is essential.
  - Quick check question: Can you explain why reducing entropy too quickly during RL might harm subsequent learning capacity?

- Concept: **Importance Sampling in Off-Policy RL**
  - Why needed here: Required for understanding how SRFT incorporates demonstration data from different behavior policies into on-policy GRPO training.
  - Quick check question: Why does setting π_β = 1 simplify integration with off-the-shelf datasets, and what approximation does this make?

- Concept: **GRPO (Group Relative Policy Optimization)**
  - Why needed here: Base RL algorithm that SRFT extends; understanding its advantage computation and clipping mechanism is prerequisite.
  - Quick check question: How does GRPO compute advantages without a learned value function, and why does this matter for memory efficiency?

## Architecture Onboarding

- Component map:
  - **Demonstration Buffer**: Stores expert reasoning traces (e.g., DeepSeek-R1 outputs) with prompts
  - **Rollout Generator**: Produces G=8 on-policy trajectories per prompt using current policy π_θ
  - **Reward Function**: Binary verification (1 if correct, 0 otherwise) using Math-Verify
  - **Augmented Group Combiner**: Merges rollouts with demonstrations for unified advantage computation
  - **Entropy Calculator**: Computes H(π_θ) at each step for weighting mechanisms
  - **Loss Combiner**: Aggregates four loss terms with entropy-aware weights

- Critical path:
  1. Sample prompts from training dataset
  2. Generate rollouts (8 per prompt, max 8192 tokens)
  3. Sample demonstrations (matched or random)
  4. Compute rewards for all samples
  5. Calculate group-relative advantages across augmented batch
  6. Compute entropy and derive w_SFT and w_RL weights
  7. Backpropagate combined loss: L_demo^SFT + L_demo^RL + L_self-rollout^RL

- Design tradeoffs:
  - **w_SFT = 0.5 × exp(-H)**: Reduces SFT influence when uncertain, but may slow knowledge acquisition
  - **w_RL = 0.1 × exp(H)**: Increases positive-sample RL weight when entropy is high, but 0.1 coefficient limits magnitude
  - **π_β = 1 for importance sampling**: Simplifies implementation but ignores true demonstration distribution

- Failure signatures:
  - Rapid entropy collapse (→0.1) early in training suggests overfitting; check if w_RL mechanism is functioning
  - Training rewards plateau while validation drops indicates demonstration overfitting; verify w_SFT is reducing appropriately
  - Response length continuously decreasing suggests over-conservative RL; check positive/negative sample balance

- First 3 experiments:
  1. **Baseline comparison**: Train pure SFT, pure RL (GRPO), sequential SFT→RL, and SRFT on same 46k dataset with identical hyperparameters; plot entropy trajectories and accuracy curves
  2. **Weighting ablation**: Set w_SFT and w_RL to fixed constants (1.0) separately to validate each mechanism's contribution; expect -4.0 and -2.9 point drops respectively per Table 3
  3. **Demonstration quality test**: Train with filtered vs. unfiltered demonstrations to verify robustness to imperfect expert traces; monitor if entropy-aware weighting mitigates negative transfer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can more sophisticated entropy-based control mechanisms (e.g., adaptive entropy scheduling, multi-timescale entropy analysis) improve upon the basic exponential weighting functions used in SRFT?
- Basis in paper: [explicit] The Limitations section states: "our current utilization of entropy dynamics remains relatively simple with basic exponential weighting functions... Future work could explore adaptive entropy scheduling or multi-timescale entropy analysis to better capture the interplay between SFT and RL signals."
- Why unresolved: The paper uses fixed entropy-aware weighting formulas (w_SFT = 0.5 × exp(-H(π_θ)) and w_RL = 0.1 × exp(H(π_θ))) that do not adapt to temporal patterns or task-specific dynamics.
- What evidence would resolve it: Experiments comparing the current weighting scheme against learned/adaptive schedulers across multiple training phases and task domains, measuring both final performance and training stability.

### Open Question 2
- Question: How robust is SRFT when demonstrations contain errors or originate from less capable models rather than high-quality expert sources like DeepSeek-R1?
- Basis in paper: [explicit] The Limitations section notes: "our approach assumes access to high-quality demonstrations, and future research could investigate the potential for training with imperfect demonstrations to enhance the method's applicability."
- Why unresolved: All experiments use demonstrations from DeepSeek-R1 with verified correctness. The entropy-aware weighting mechanisms were designed assuming high-quality demonstration distributions, and their behavior under noisy/imperfect conditions is unknown.
- What evidence would resolve it: Systematic experiments varying demonstration quality (introducing noise, using weaker generator models) and measuring SRFT's performance degradation curves compared to baselines.

### Open Question 3
- Question: Would SRFT's single-stage integration approach remain effective when applied to larger model scales or different architectural families beyond Qwen2.5-Math-7B?
- Basis in paper: [inferred] The paper only evaluates on Qwen2.5-Math-7B (a 7B parameter model). The entropy dynamics and SFT/RL balance may differ significantly at larger scales (e.g., 70B+ models) or different architectures, where RL dynamics and capacity for exploration vary.
- Why unresolved: The entropy-based observations about SFT's "coarse-grained global changes" versus RL's "fine-grained selective optimizations" were derived from experiments on one specific model architecture and size.
- What evidence would resolve it: Replication of SRFT across multiple model families (Llama, Mistral) and scales (1B to 70B+ parameters), analyzing whether entropy dynamics and optimal weighting parameters transfer.

## Limitations

- Assumes access to high-quality demonstrations, limiting applicability when expert traces are unavailable or noisy
- Uses fixed entropy-aware weighting formulas without theoretical justification for the specific exponential functions chosen
- Only evaluated on mathematical reasoning tasks, with unknown transferability to other domains like code generation or multi-step planning

## Confidence

**High Confidence** (Multiple corroborating sources, direct evidence):
- Differential distribution modification by SFT vs RL (Section 3.1.1, Figure 2)
- Baseline performance improvements (Table 1, multiple benchmarks)
- Entropy stability benefits (Section 6, Figure 6)

**Medium Confidence** (Single study, partial evidence):
- Single-stage efficiency claims (Section 3.2.2, Figure 5)
- Generalization to out-of-distribution tasks (Table 2, OlympiadBench)
- Robustness to noisy demonstrations (Table 4)

**Low Confidence** (Extrapolated, under-specified):
- Theoretical optimality of exponential weighting functions
- Transferability to non-mathematical domains
- Long-term training stability beyond 500 steps

## Next Checks

1. **Ablation of weighting mechanisms**: Systematically test linear, quadratic, and learned alternatives to the exponential weighting functions. Replace w_SFT = 0.5 × exp(-H) with w_SFT = 1.0 × (1-H) and w_RL = 0.1 × H, then compare performance degradation. This would validate whether the exponential form is critical or if any entropy-monotonic function suffices.

2. **Cross-domain transfer evaluation**: Apply SRFT to code generation (HumanEval, MBPP) and multi-step planning tasks (ALFWorld, ALFWorld+). Use the same hyperparameters and entropy-aware weighting, measuring whether the method's advantages persist outside mathematical reasoning. Success would validate the generality of the SFT-RL integration principles.

3. **Demonstration corruption robustness**: Intentionally inject systematic errors (10-30% incorrect reasoning steps) into the training demonstrations. Compare SRFT's performance degradation against pure SFT and sequential SFT→RL baselines. This would test whether the entropy-aware mechanism genuinely mitigates negative transfer from noisy supervision, as claimed.