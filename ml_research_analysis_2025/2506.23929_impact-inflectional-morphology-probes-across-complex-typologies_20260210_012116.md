---
ver: rpa2
title: 'IMPACT: Inflectional Morphology Probes Across Complex Typologies'
arxiv_id: '2506.23929'
source_url: https://arxiv.org/abs/2506.23929
tags:
- llms
- plurality
- verb
- table
- gender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces IMPACT, a synthetically generated evaluation
  framework for assessing Large Language Models'' (LLMs) understanding of inflectional
  morphology across five morphologically rich languages: Arabic, Russian, Finnish,
  Turkish, and Hebrew. The framework includes unit-test-style cases covering shared
  and language-specific phenomena, from basic verb inflections to unique features
  like Arabic''s reverse gender agreement and vowel harmony in Finnish and Turkish.'
---

# IMPACT: Inflectional Morphology Probes Across Complex Typologies

## Quick Facts
- **arXiv ID:** 2506.23929
- **Source URL:** https://arxiv.org/abs/2506.23929
- **Reference count:** 20
- **Primary result:** IMPACT benchmark reveals LLMs' persistent gaps in inflectional morphology across Arabic, Russian, Finnish, Turkish, and Hebrew, with asymmetric performance between grammatical recognition and ungrammatical rejection.

## Executive Summary
IMPACT is a synthetic evaluation framework that systematically assesses Large Language Models' understanding of inflectional morphology across five morphologically rich languages. The framework uses controlled templates with placeholders filled from UniMorph wordlists to generate both grammatical and ungrammatical utterances targeting specific morphological features. Eight multilingual LLMs were evaluated across Generation (predicting correct inflections) and Judgement (assessing grammaticality) scenarios. Results demonstrate that while models excel at recognizing grammatical sentences, they struggle significantly with ungrammatical examples and complex language-specific phenomena like vowel harmony and dual agreement. Chain-of-Thought and Thinking Modes show inconsistent effects without reliable improvements.

## Method Summary
The framework generates synthetic utterances using morphological agreement logic applied to template placeholders ([NAME], [VERB], [ADJ], [NOUN]) filled from UniMorph wordlists. Each template targets specific morphosyntactic features (gender, number, case, mood), creating evaluation units—the smallest testable combinations. The evaluation includes Generation prompts (fill-in-the-blank with base form hints) and Judgement prompts (Yes/No grammaticality assessment). Perturbation creates ungrammatical negatives by mismatching inflections. Models are evaluated zero-shot using Direct and CoT prompting, with scores aggregated via harmonic mean at template and model levels.

## Key Results
- LLMs consistently perform better on grammatical (JY) than ungrammatical (JN) judgment tasks across all languages
- Chain-of-Thought and Thinking Modes do not reliably improve performance and sometimes degrade it
- Language-specific features (Arabic dual agreement, Finnish city case, Turkish vowel harmony) show near-universal model failure
- Models exhibit asymmetric competence: strong statistical fluency but weak explicit grammatical rule application

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Synthetic template-based probing can reveal morphological blind spots that downstream benchmarks miss.
- **Mechanism:** The IMPACT framework generates controlled utterances using morphological agreement logic (templates with placeholders for [NAME], [VERB], [ADJ], [NOUN]) filled from UniMorph wordlists. Each template targets specific morphosyntactic features (gender, number, case, mood), creating "evaluation units"—the smallest testable combinations. By systematically varying these features and testing both generation (fill-in-the-blank) and judgment (grammaticality assessment), the framework isolates whether models can apply rules rather than rely on surface pattern matching.
- **Core assumption:** Assumption: Errors on controlled synthetic utterances generalize to real-world morphological handling in natural text.
- **Evidence anchors:**
  - [abstract]: "IMPACT includes unit-test-style cases covering both shared and language-specific phenomena, from basic verb inflections... to unique features like Arabic's reverse gender agreement and vowel harmony in Finnish and Turkish."
  - [Section 3.1]: "Each unique placeholder combination, called an 'Evaluation Unit', is the smallest evaluated element."
  - [corpus]: Related work on adversarial evaluation (arXiv:2505.07856) shows inflectional perturbations degrade model behavior, supporting the probing approach.
- **Break condition:** If models performed equally well on grammatical and ungrammatical judgment, the probing would fail to distinguish surface fluency from rule knowledge.

### Mechanism 2
- **Claim:** LLMs exhibit asymmetric competence: better at recognizing grammatical forms than rejecting ungrammatical ones.
- **Mechanism:** The paper evaluates two judgment scenarios—positive (JY, grammatical utterances) and negative (JN, ungrammatical utterances created by perturbing inflections). Results show LLMs consistently score higher on JY than JN across all five languages. This asymmetry suggests models learn statistical regularities of well-formed text but lack explicit grammatical rule representations needed to detect violations. Generation performance falls between the two, indicating partial rule knowledge insufficient for reliable rejection.
- **Core assumption:** Assumption: The asymmetry reflects genuine gaps in morphological understanding rather than task framing effects.
- **Evidence anchors:**
  - [Section 4.2]: "LLMs excel at recognizing grammatical sentences, struggle more with ungrammatical ones, and perform moderately on generation."
  - [Table 1]: Shows consistent JY > Gen > JN score ordering across templates and models.
  - [Section 4.4]: "About 72% [of failing units] related to negative judgements (JN)."
- **Break condition:** If perturbation methods introduced semantic or lexical confounds rather than purely morphological violations, the asymmetry could be artifact-driven.

### Mechanism 3
- **Claim:** Chain-of-Thought and Thinking Modes do not reliably improve morphological reasoning and can introduce new errors.
- **Mechanism:** The paper compares Direct prompting (output immediately) against CoT (reasoning before answer) and Qwen3's "Thinking Mode" (extended test-time compute). CoT showed variable effects: sometimes boosting JY scores but simultaneously reducing JN scores on the same template. Analysis of reasoning chains revealed models correctly identifying rules but failing at final verification steps—longer reasoning introduces more failure points. Thinking Mode showed similar inconsistency, with strong losses on specific templates like fin-1.
- **Core assumption:** Assumption: Observed degradations stem from reasoning chain failures rather than prompt formatting differences.
- **Evidence anchors:**
  - [Section 4.3]: "CoT boosted the JY score of Qwen2.5 on tur-2, but simultaneously reduced the JN score for the same template."
  - [Section 4.3]: "Models consistently failed to detect grammatical errors involving case and plurality, often misclassifying incorrect examples as correct."
  - [corpus]: Related work (Sprague et al., 2025; Ma et al., 2025) confirms CoT inconsistency on non-mathematical reasoning tasks.
- **Break condition:** If CoT prompts were optimized per-template rather than uniformly applied, benefits might emerge that the current study misses.

## Foundational Learning

- **Concept: Inflectional vs. Derivational Morphology**
  - **Why needed here:** IMPACT explicitly scopes to inflectional morphology (word changes expressing grammatical features like tense, case, gender) while excluding derivational morphology (word formation creating new lexemes). Understanding this distinction is necessary to interpret what the framework tests and what it leaves unexplored.
  - **Quick check question:** Given "happiness" derived from "happy" versus "walked" from "walk"—which exemplifies inflectional morphology?

- **Concept: Fusional vs. Agglutinative Languages**
  - **Why needed here:** The paper evaluates both language types—Finnish/Turkish (agglutinative: single affix = single function) and Arabic/Russian/Hebrew (fusional: single affix blends multiple functions). This typological difference affects how morphological features interact and what errors are possible.
  - **Quick check question:** In Turkish "yürüyorum" (I am walking), can you identify the separate morphemes for verb root, progressive aspect, and person marker?

- **Concept: Harmonic Mean Aggregation**
  - **Why needed here:** Template scores aggregate evaluation unit scores via harmonic mean, heavily penalizing poor performance on any unit. This prevents strong averages from masking specific weaknesses—a critical choice for diagnostic evaluation.
  - **Quick check question:** Why would arithmetic mean mask a model that fails completely on dual forms while succeeding on singular/plural?

## Architecture Onboarding

- **Component map:**
  1. **Template Definitions** (Tables 5-6, Section 3.1): Blueprints with placeholders targeting morphological features
  2. **Placeholder Wordlists**: Language-specific lists from UniMorph with inflected forms
  3. **Agreement Logic Engine** (Section 3.2): Rules generating grammatically correct utterances per language
  4. **Perturbation Module**: Creates ungrammatical variants by mismatching inflections
  5. **Prompt Generator** (Section 3.3, Appendix A): Formats for Generation (fill-blank) and Judgment (Yes/No)
  6. **Evaluation Scorer**: Accuracy per evaluation unit, harmonic mean aggregation per template

- **Critical path:** Template identification → Placeholder population → Agreement-logic utterance generation → (branch: keep correct OR perturb for negative) → Prompt formatting → LLM inference → Response validation → Accuracy computation → Harmonic mean aggregation

- **Design tradeoffs:**
  - Synthetic templates enable controlled probing but may not capture naturalistic morphological patterns
  - Sampling 50 prompts per evaluation unit balances coverage vs. compute; downsampling negative examples when units exceed 100 addresses class imbalance
  - Zero-shot evaluation tests inherent capabilities but may underestimate few-shot potential
  - CoT prompts in English for all languages standardizes reasoning language but may disadvantage non-English morphological reasoning

- **Failure signatures:**
  - **High JY + Low JN:** Model has statistical fluency but lacks rule-based grammaticality detection (observed across all models)
  - **CoT degrading JN:** Reasoning chains correctly identify rules but fail verification (observed in GF2 on fin-2)
  - **Template-specific collapse:** Near-zero scores on language-specific templates (e.g., all models on fin-1 allative case generation) indicate missing knowledge
  - **Dual/plural confusion:** Arabic/Hebrew models accepting plural verbs with dual nouns signals incomplete number system representation

- **First 3 experiments:**
  1. **Baseline establishment:** Run IMPACT Generation and Judgment (JY + JN) on your target model across all 5 languages using Direct prompting; compute template-level harmonic means and identify failing evaluation units (>40% failure rate).
  2. **CoT comparison:** Re-run with CoT prompting on failing templates only; analyze reasoning chains to classify error types (rule misidentification vs. verification failure vs. output formatting).
  3. **Language-specific failure analysis:** For templates with universal failure across models (e.g., Arabic dual agreement, Finnish lexical casing, Turkish vowel harmony), examine training data representation and test whether few-shot examples with explicit rule descriptions improve performance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What reasoning strategies could consistently improve morphological judgment without introducing the errors that Chain-of-Thought and Thinking Modes sometimes cause?
- **Basis in paper:** [explicit] Section 4.3 concludes that "CoT and 'Thinking Mode' do not consistently enhance the performance of LLMs and may introduce new errors, highlighting the need for more robust reasoning strategies."
- **Why unresolved:** The paper documents the inconsistency but does not propose alternative approaches.
- **What evidence would resolve it:** Development of prompting methods that yield monotonic improvement across all templates, particularly for ungrammatical judgment tasks.

### Open Question 2
- **Question:** Does extending evaluation to derivational morphology reveal similar or distinct failure patterns compared to inflectional morphology?
- **Basis in paper:** [explicit] Section 1 states the work focuses "on inflectional morphology, with derivational morphology left for future work." Limitations reiterates this constraint.
- **Why unresolved:** No systematic evaluation of derivational morphology exists in this framework.
- **What evidence would resolve it:** An IMPACT-style benchmark covering derivational phenomena (e.g., nominalization, causative formation) across the same languages.

### Open Question 3
- **Question:** What training interventions could close the persistent gap between positive judgment (JY) and negative judgment (JN) performance?
- **Basis in paper:** [inferred] The paper documents consistent JY > Gen > JN patterns across all models and languages (Tables 1, 9), with models struggling especially to reject ungrammatical examples, but offers no remediation strategies.
- **Why unresolved:** The study focuses on evaluation; the underlying cause and potential fixes for the asymmetry remain unexplored.
- **What evidence would resolve it:** Experiments showing that targeted training (e.g., contrastive negative examples, explicit grammar supervision) reduces the JY-JN gap.

## Limitations
- Synthetic templates may not fully capture complex morphological interactions present in natural language
- Perturbation-based ungrammatical examples may not perfectly replicate authentic morphological errors
- Zero-shot evaluation leaves open whether targeted few-shot examples could substantially improve performance
- Focus on controlled utterances rather than fully composed semantic sentences limits generalization assessment

## Confidence
**High Confidence:** The asymmetry between grammatical recognition (JY) and ungrammatical rejection (JN) across all models and languages; the specific pattern of near-universal failure on language-specific templates; the observation that CoT/Thinking Mode does not consistently improve performance and sometimes degrades it.

**Medium Confidence:** The claim that IMPACT reveals morphological blind spots missed by downstream benchmarks; the assertion that models lack explicit grammatical rule representations; the interpretation that error patterns reflect genuine knowledge gaps rather than task-specific artifacts.

**Low Confidence:** The generalizability of synthetic template performance to real-world morphological handling; the conclusion that current evaluation frameworks inadequately assess morphological competence; the claim that few-shot prompting would substantially improve identified weaknesses.

## Next Checks
1. **Natural Text Validation:** Apply IMPACT's morphological probes to naturally occurring text samples (news articles, literature, social media) from the same five languages. Compare model performance on synthetic vs. natural instances of identical morphological phenomena to assess ecological validity.

2. **Few-Shot Rule Prompting:** For templates showing consistent failure (e.g., Arabic dual agreement, Turkish vowel harmony), design targeted few-shot prompts containing explicit morphological rule descriptions. Evaluate whether performance improves beyond the zero-shot baseline reported.

3. **Cross-Lingual Transfer Analysis:** Test whether performance on one language's morphological features correlates with performance on typologically related languages (e.g., Finnish-Turkish agglutination, Arabic-Hebrew Semitic features). This would reveal whether models develop abstract morphological representations or rely on language-specific patterns.