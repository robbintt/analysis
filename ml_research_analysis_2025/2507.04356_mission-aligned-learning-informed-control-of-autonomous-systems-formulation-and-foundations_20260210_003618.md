---
ver: rpa2
title: 'Mission-Aligned Learning-Informed Control of Autonomous Systems: Formulation
  and Foundations'
arxiv_id: '2507.04356'
source_url: https://arxiv.org/abs/2507.04356
tags:
- state
- control
- learning
- planning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel hierarchical optimization framework
  that integrates classical planning at the high level with Model Predictive Control
  (MPC) at the low level, incorporating reinforcement learning for model and reward
  learning. The approach aims to address safety and interpretability concerns in autonomous
  systems, particularly in applications like robotic care for movement and health-impaired
  humans.
---

# Mission-Aligned Learning-Informed Control of Autonomous Systems: Formulation and Foundations

## Quick Facts
- arXiv ID: 2507.04356
- Source URL: https://arxiv.org/abs/2507.04356
- Reference count: 40
- Primary result: Hierarchical optimization framework integrating classical planning with MPC and RL for safe, interpretable autonomous control in safety-critical domains

## Executive Summary
This paper proposes a novel hierarchical optimization framework that integrates classical planning at the high level with Model Predictive Control (MPC) at the low level, incorporating reinforcement learning for model and reward learning. The approach aims to address safety and interpretability concerns in autonomous systems, particularly in applications like robotic care for movement and health-impaired humans. The framework formulates a bilevel optimization problem where the upper level uses discrete planning to select tasks while the lower level uses continuous MPC for physical execution, with learned models of dynamics and rewards.

## Method Summary
The approach uses a three-level hierarchy: a scheduler optimizes task sequences over a 16-hour horizon, a classical planner generates discrete action sequences per task using learned transition models, and an RL-based MPC executes continuous control with hard safety constraints. Fuzzy membership functions bridge continuous control states to discrete symbolic states, enabling gradient-based learning across the symbolic-continuous boundary while preserving logical interpretability. The framework learns three key components: transition models p^S_ψ, cost models θ_C, and value functions θ_V, using observed rewards and state transitions.

## Key Results
- Formulates a bilevel optimization problem integrating classical planning with MPC
- Demonstrates how fuzzy logic provides differentiable bridges between continuous and symbolic states
- Shows integration of learned models at planning level while maintaining safety through MPC constraints
- Addresses safety and interpretability concerns in autonomous systems through hierarchical structure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding MPC at the lower control layer provides hard safety constraint enforcement that pure RL cannot guarantee.
- Mechanism: MPC solves a constrained optimization problem at each time step, where collision avoidance, actuator limits, and safety bounds are encoded as explicit constraints h(x,u) ≤ 0. The optimizer cannot produce actions violating these constraints, unlike RL policies which must learn safety from data.
- Core assumption: The safety-critical constraints are known and can be expressed as differentiable functions of state and control.
- Evidence anchors: [section 3]: "MPC ensures explainable policies with guarantees... RLMPC can encode explicit safety constraints to provide a safe-RL framework"

### Mechanism 2
- Claim: The bilevel structure enables learning at both levels while maintaining interpretability through symbolic planning.
- Mechanism: Upper-level planning operates on discrete symbolic states using classical planning (e.g., A* search) over learned transition models p^S_ψ(θψ; s, a). Lower-level MPC operates on continuous dynamics f(z, v). Learning signals flow upward (rewards observed during control execution update planning value estimates) and downward (planned actions parameterize MPC cost functions).
- Core assumption: The symbolic abstraction is sufficiently coarse-grained that planning decisions remain interpretable, yet sufficiently detailed to capture task-relevant structure.
- Evidence anchors: [section 6.2, Eq. 51]: Action selection via learned action-value function V(θV, a, s) or classical planning algorithm C

### Mechanism 3
- Claim: Fuzzy membership functions provide differentiable bridges between continuous control states and discrete planning symbols.
- Mechanism: Each logical atom Lm (e.g., "At(Robot, Kitchen)") is associated with a membership function μLm(z) mapping continuous state z ∈ R^{d_z} to [0,1]. This enables gradient-based learning across the symbolic-continuous boundary while preserving logical interpretability.
- Core assumption: The fuzzy membership functions can be designed or learned to accurately capture the semantic meaning of logical predicates.
- Evidence anchors: [section 6.4, Eq. 54-56]: Fuzzy state representation s^μ = T_s(μL1(z), ..., μLM(z); a, s)

## Foundational Learning

- Concept: **Model Predictive Control (receding horizon)**
  - Why needed here: The entire lower control layer depends on understanding how MPC formulates and solves constrained finite-horizon optimization at each timestep.
  - Quick check question: Given dynamics x_{k+1} = f(x_k, u_k) and constraint h(x_k, u_k) ≤ 0, can you write the MPC optimization problem and explain why only u*_0 is applied?

- Concept: **Bellman Equations and Value Function Decomposition**
  - Why needed here: The paper treats both MPC and planning as approximations to dynamic programming; understanding the recursive value structure is essential for the learning updates.
  - Quick check question: Can you derive Q^π(s,a) = E[R(s,a) + γV^π(s')] from the Bellman equation?

- Concept: **First-Order Logic and Predicate Semantics**
  - Why needed here: The planning layer uses relational logic representations; interpreting the symbolic state requires understanding predicates, terms, and satisfaction semantics.
  - Quick check question: Given a state s mapping predicates to truth values and an action a with pre(a) and eff(a), what is the resulting state s' after applying a?

## Architecture Onboarding

- Component map: Scheduler (task sequencing, 16-hour horizon) → Planner (PDDL/classical planning, per-task) → Fuzzy State Mapper (μ_L: z → [0,1]^M) → MPC Controller (real-time, h ≪ 1 timestep) → Physical System (robot dynamics) → Learning Module (updates θ_ψ, θ_C, θ_V)

- Critical path:
1. Task selection by scheduler
2. Planning problem formulation from task
3. Action selection (classical planner OR learned policy)
4. MPC problem construction from action
5. Real-time iteration (SQP solve, apply first control, observe, repeat)
6. State inference from continuous to discrete
7. Model and value function updates

- Design tradeoffs:
1. **Planning horizon vs. computational budget**: Longer MPC horizons improve optimality but may violate real-time constraints. Paper suggests RTI (single QP iteration per timestep) as solution.
2. **Symbolic granularity vs. learning complexity**: More symbolic predicates improve interpretability but increase θ_ψ dimensionality and sample complexity.
3. **Exploration vs. safety**: Paper proposes adding perturbation d^T u_0 to MPC cost (Eq. 26) for exploration while maintaining constraint feasibility.

- Failure signatures:
1. **MPC infeasibility**: No solution satisfies constraints → check if constraints are over-tight or initial state is already violated.
2. **Planning-control state mismatch**: Observed z_{t+1} maps to unexpected s_{t+1} → fuzzy membership functions may be poorly calibrated.
3. **Learning instability**: θ updates cause performance collapse → check timescale separation between θ_ψ, θ_C, θ_V updates.

- First 3 experiments:
1. **Validate MPC safety alone**: Implement lower-layer MPC with known dynamics and hard constraints; verify constraint satisfaction under perturbations before enabling learning.
2. **Test fuzzy grounding accuracy**: For a simple domain (e.g., robot in grid rooms), measure precision/recall of μL(z) matching ground-truth symbolic state across trajectory samples.
3. **Isolate learning signal**: Fix planning to use oracle transitions (no learning) and verify MPC action execution produces expected rewards; then enable θ updates and monitor convergence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can MPC-based RL be extended to problems with degenerate constraints that do not satisfy the Linear Independence Constraint Qualification (LICQ)?
- Basis in paper: [explicit] "extending optimization methods that perform MPC for problems with degenerate constraints, those that do not satisfy the LICQ, like [45], to incorporate RL, remains an important open problem."
- Why unresolved: Degenerate constraints create discontinuities in the policy that break standard policy gradient methods, and current approaches only work when SOSC and LICQ hold.
- What evidence would resolve it: A theoretical framework extending RL-based MPC to handle degenerate constraint structures, with convergence proofs and demonstrated performance on benchmark problems with such constraints.

### Open Question 2
- Question: What are the theoretical convergence guarantees for the full hierarchical bilevel learning scheme integrating scheduling, planning, MPC, and RL?
- Basis in paper: [explicit] The paper states "ongoing works analyzing the theoretical properties and testing a simulation of this stylized example" and presents the formulation as "foundations" without proving convergence of the integrated system.
- Why unresolved: The coupling between levels introduces a "coupled Dynamic Programming Fixed Point Problem" where upper and lower level solutions depend on each other, complicating standard convergence analysis.
- What evidence would resolve it: Formal proofs establishing conditions under which the hierarchical scheme converges, including analysis of the interplay between timescales and learning rates across layers.

### Open Question 3
- Question: How can safe exploration be guaranteed for RL-based MPC in partially unknown systems without requiring explicit worst-case models?
- Basis in paper: [explicit] "Safe exploration is difficult to produce without a model of the system in the form (29), which can predict the worst-case evolution of the system, and assess the impact of the exploration on the system safety."
- Why unresolved: Current safe exploration approaches in RL-based MPC rely on robust MPC formulations that require set-membership identification of worst-case models, which may be unavailable or computationally prohibitive.
- What evidence would resolve it: Algorithms that provably maintain safety constraints during exploration using only partial model knowledge, validated on safety-critical benchmarks without requiring explicit robust models.

## Limitations

- Lack of specific robot dynamics equations, particularly the "unknown nonlinear gyroscopic effect"
- Incomplete reward function specifications for the learning components
- Fuzzy logic grounding mechanism lacks empirical validation in the literature for complex robotic tasks
- No theoretical convergence guarantees for the full hierarchical bilevel learning scheme

## Confidence

- **High confidence**: MPC safety guarantees when constraints are known
- **Medium confidence**: Bilevel learning structure maintaining interpretability
- **Low confidence**: Fuzzy membership functions accurately bridging symbolic-continuous states

## Next Checks

1. Implement a simplified version with known dynamics (e.g., double integrator) to verify MPC safety before introducing learned models
2. Test fuzzy state mapping on a grid-world domain with ground-truth symbolic states to measure accuracy and consistency
3. Perform ablation studies isolating each learning component to identify sources of instability or performance degradation