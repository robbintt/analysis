---
ver: rpa2
title: Quantifying Memory Utilization with Effective State-Size
arxiv_id: '2504.19561'
source_url: https://arxiv.org/abs/2504.19561
tags:
- total
- accuracy
- memory
- state
- state-size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes "effective state-size" (ESS), a new metric to
  quantify how much of a sequence model's memory capacity is actually used during
  processing. ESS is derived from the rank of operator submatrices in linear input-varying
  systems and provides a quantitative measure of memory utilization beyond raw capacity.
---

# Quantifying Memory Utilization with Effective State-Size

## Quick Facts
- **arXiv ID:** 2504.19561
- **Source URL:** https://arxiv.org/abs/2504.19561
- **Reference count:** 40
- **Primary result:** Proposes ESS metric to quantify actual memory utilization in sequence models, revealing two failure modes invisible to capacity-only metrics

## Executive Summary
The paper introduces Effective State-Size (ESS) as a metric to quantify how much of a sequence model's memory capacity is actually utilized during processing. Unlike raw capacity metrics, ESS measures the effective dimensionality of the internal state space that contributes to computation. The authors demonstrate that ESS strongly correlates with model performance on memory-intensive tasks, revealing two distinct failure modes—state saturation and state collapse—that capacity-only metrics miss. This framework enables targeted interventions like regularization and distillation to improve model memory utilization.

## Method Summary
The method computes ESS by extracting operator submatrices H_i from sequence models during forward passes, then measuring their rank via SVD. ESS can be calculated using either tolerance-based (counting singular values above threshold) or entropy-based (spectral perplexity) approaches. The framework is applied to analyze and optimize various architectures including attention, convolutions, and state space models across tasks like associative recall. Regularization techniques and distillation strategies are proposed to prevent state collapse and guide model design.

## Key Results
- ESS/kv strongly correlates with accuracy across task-model space, outperforming TSS/kv correlation
- State collapse occurs in gated architectures (GLA/WLA) when A matrices decay toward zero during training
- Input-dependent state transitions (SA, GLA) show critical state modulation at delimiter tokens for language tasks
- ESS predicts distillation efficiency—higher teacher ESS requires larger student models for lossless transfer

## Why This Works (Mechanism)

### Mechanism 1
ESS quantifies memory utilization by measuring the effective dimensionality of the internal state space that actually contributes to computation. The paper derives ESS from the rank of operator submatrices (H_i = T_{i:,:i-1}) in linear input-varying systems. Since any sequence model can be expressed as y = T u where T encodes all transformations, the rank of submatrices determines the minimal state-size needed for an equivalent recurrent realization (Theorem 3.2). When rank(H_i) < TSS (theoretical state-size), the model is underutilizing its available memory.

### Mechanism 2
ESS reveals two distinct failure modes in recurrent models that capacity-only metrics miss: state saturation (insufficient capacity) and state collapse (trainability issues). State saturation occurs when ESS ≈ TSS, meaning the model cannot expand its utilized memory despite task demands. State collapse occurs when ESS remains low despite sufficient TSS, often due to learnable A matrices decaying toward zero during training. The paper shows that for GLA/WLA, ESS/kv and ||∏A_i||_F decrease with sequence length, while LA (with fixed identity A) maintains ESS.

### Mechanism 3
State modulation—the ability to dynamically adjust ESS in response to input tokens—distinguishes architectures on language recall tasks. The paper observes that ESS dips at separator tokens (EOS, periods) in language models. Architectures with input-dependent state transitions (SA, GLA) show strong modulation, while input-invariant ones (LA) show none. This modulation appears necessary for "resetting" context, correlating with bigram recall perplexity.

## Foundational Learning

- **Concept: Linear Systems and Operator Representations**
  - **Why needed here:** The entire ESS framework relies on expressing sequence models as linear operators T where y = T u. Understanding how attention, convolutions, and recurrences can be unified under this representation is essential.
  - **Quick check question:** Can you write the operator form for linear attention (T_ij = C_i B_j) and explain why it's input-varying?

- **Concept: Singular Value Decomposition and Matrix Rank**
  - **Why needed here:** ESS is computed via SVD of H_i submatrices. The distinction between numerical rank (tolerance-based) and effective rank (entropy-based) determines how you interpret memory utilization.
  - **Quick check question:** Given singular values [10, 1, 0.1, 0.01], what's the tolerance-ESS at τ=0.5? What's the entropy-ESS?

- **Concept: Recurrent Realization and Minimal State-Space**
  - **Why needed here:** The paper draws from control theory to show that the rank of H_i determines the minimal state-size for an equivalent recurrence. This is the theoretical justification for ESS as a memory metric.
  - **Quick check question:** If rank(H_i) = 8 but the model's recurrent formulation uses state-size 64, what does this imply about memory utilization?

## Architecture Onboarding

- **Component map:** Input sequence u → Model (produces operator T via forward pass) → Extract H_i = T_{i:,:i-1} for each position i → Compute SVD: H_i = U diag(Σ_i) V^T → ESS via: tolerance-ESS (count σ > τ) OR entropy-ESS (exp(-Σ p log p)) → Aggregate across layers, channels, batch → analyze or optimize

- **Critical path:**
  1. **For analysis:** Compute ESS on validation data during training; correlate with accuracy to identify saturation/collapse
  2. **For distillation:** Use teacher ESS to predict student model size requirements (higher teacher ESS → larger student needed)
  3. **For regularization:** Monitor ESS/TSS ratio; add λ||A - I||_F to loss if ratio is low and performance is poor
  4. **For architecture selection:** Check if model can modulate ESS at delimiter tokens (process text with <sep> tokens and visualize ESS over sequence)

- **Design tradeoffs:**
  - **Tolerance-ESS vs entropy-ESS:** Tolerance-ESS is interpretable (minimum state size for approximation within error τ) but requires choosing τ. Entropy-ESS is parameter-free but less intuitive and may overestimate rank when singular values are uniformly small
  - **Per-channel vs total ESS:** Per-channel ESS enables efficient O(dℓ³) computation for SISO models; total ESS (summing across channels) is necessary for architectures like softmax attention where TSS scales with sequence position
  - **Aggregation strategy:** Averaging ESS across layers/batch smooths noise but may hide layer-specific issues (e.g., one collapsed layer). Minimum or per-layer analysis reveals localized failures

- **Failure signatures:**
  - **Low ESS-accuracy correlation during training:** May indicate the task doesn't require memory utilization (too easy) or model is in early training phase where ESS decreases before accuracy improves
  - **ESS/TSS near 1.0 with poor performance:** State saturation—increase TSS or simplify task
  - **ESS/TSS near 0 with high TSS:** State collapse—check A matrix norms, try regularization toward identity
  - **No ESS modulation at separators:** May explain poor recall on language tasks; consider architectures with input-dependent state transitions

- **First 3 experiments:**
  1. **Baseline correlation check:** Train GLA, LA, WLA models on MQAR with varying kv pairs and TSS. Compute ESS every 10 epochs. Verify that ESS/kv correlates more strongly with accuracy than TSS/kv (should replicate Figure 2)
  2. **State collapse intervention:** Train GLA on high-kv MQAR (kv=128, seqlen=4096) with and without A→I regularization (λ=2^-3 to 2^1). Compare final accuracy and ESS/TSS ratios (should replicate Figure 9b)
  3. **Distillation feasibility:** Train teacher GLA (TSS=256) on MQAR, checkpoint at intervals. Distill to students (TSS=16,32,64,128). Plot teacher ESS vs distillation loss to verify ESS as compressibility predictor (should replicate Figure 5)

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** What are the mechanistic causes of the transient decrease in Effective State-Size (ESS) observed during the very early phases of training (epochs 0-10) in recurrent models?
**Basis in paper:** Section F.1.5 notes a "phase at the start of training in which ESS tends to decrease... that we have yet to characterize."
**Why unresolved:** The paper observes the phenomenon (a sharp drop in ESS before it rises with accuracy) but does not provide a theoretical explanation for why models discard state capacity initially or how this phase influences final convergence.
**What evidence would resolve it:** A fine-grained analysis of the singular value dynamics of the operator submatrices (H_i) and the gradients of the recurrent weights (A) during the first few epochs, correlated with the onset of performance gains.

### Open Question 2
**Question:** Can per-layer ESS analysis be leveraged to construct novel hybrid architectures that outperform fixed-ratio hybridization policies (such as the 1:7 attention-to-SSM ratio in Jamba)?
**Basis in paper:** Section F.4.2 states the authors hope to "leverage these insights to construct novel ESS-informed hybridization policies in future work" after observing that fixed policies sometimes suffer from state collapse.
**Why unresolved:** The paper demonstrates that ESS can diagnose failure modes in hybrids (e.g., state collapse in specific layers) but has not yet formalized this diagnostic into a generative design algorithm for new architectures.
**What evidence would resolve it:** The development of an automated search or training procedure that selects layer types based on local ESS constraints, yielding a hybrid model that achieves higher accuracy or efficiency than baselines with fixed hybrid topologies.

### Open Question 3
**Question:** Why does regularizing only the second layer of a network lead to better performance than regularizing both layers when using ESS-based regularization to mitigate state collapse?
**Basis in paper:** Section F.3 discusses the hypothesis that regularizing both layers may create "conflicting objectives" or cause reversion to a less expressive regime, but states this needs further exploration.
**Why unresolved:** The paper empirically identifies that targeted regularization is superior but does not theoretically validate whether the issue is optimization interference (gradient conflict) or a loss of expressivity (over-decay of singular values) in specific layers.
**What evidence would resolve it:** An ablation study visualizing the gradient cosine similarity between layers and the singular value distributions of the A matrices when applying the regularization term to single vs. multiple layers simultaneously.

## Limitations
- The framework assumes linear operator approximations faithfully represent nonlinear model behavior, which may break for models with strong element-wise nonlinearities
- State collapse regularization (λ||A - I||_F) assumes decay of A matrices is the primary cause, but other factors like gradient flow issues may also contribute
- ESS dips at delimiter tokens could reflect artifacts of token distributions rather than intentional context management

## Confidence
- **High confidence:** ESS correlation with accuracy (strong empirical validation across multiple architectures and tasks)
- **Medium confidence:** State collapse mechanism (A matrix decay identified but other causes possible)
- **Medium confidence:** State modulation importance (observed correlation but causal mechanism not fully established)

## Next Checks
1. Reproduce ESS/kv vs TSS/kv correlation comparison on MQAR task across multiple architectures
2. Verify state collapse intervention works by training GLA with and without A→I regularization
3. Test ESS as distillation predictor by comparing teacher ESS to student distillation loss