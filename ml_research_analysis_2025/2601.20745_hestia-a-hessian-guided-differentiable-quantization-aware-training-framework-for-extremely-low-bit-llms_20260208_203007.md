---
ver: rpa2
title: 'HESTIA: A Hessian-Guided Differentiable Quantization-Aware Training Framework
  for Extremely Low-Bit LLMs'
arxiv_id: '2601.20745'
source_url: https://arxiv.org/abs/2601.20745
tags:
- quantization
- training
- arxiv
- llms
- hestia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HESTIA tackles optimization stagnation in extremely low-bit LLM
  quantization by replacing the non-differentiable STE with a temperature-controlled
  softmax relaxation, maintaining gradient flow early in training. It further employs
  a Hessian-trace metric to adaptively anneal discretization rates per tensor, harmonizing
  the optimization trajectory with local curvature.
---

# HESTIA: A Hessian-Guided Differentiable Quantization-Aware Training Framework for Extremely Low-Bit LLMs

## Quick Facts
- **arXiv ID**: 2601.20745
- **Source URL**: https://arxiv.org/abs/2601.20745
- **Reference count**: 18
- **Primary result**: HESTIA improves zero-shot accuracy by 5.39% (1B) and 4.34% (3B) over state-of-the-art ternary QAT baselines.

## Executive Summary
HESTIA addresses optimization stagnation in extremely low-bit LLM quantization by replacing the non-differentiable Straight-Through Estimator with a temperature-controlled softmax relaxation, maintaining gradient flow during early training. It further employs a Hessian-trace metric to adaptively anneal discretization rates per tensor, harmonizing the optimization trajectory with local curvature. On LLaMA-3.2-1B and 3B models, HESTIA demonstrates significant improvements in zero-shot accuracy over ternary QAT baselines, validating that prioritizing gradient fidelity and curvature-aware annealing effectively recovers representational capacity in 1.58-bit models.

## Method Summary
HESTIA implements a differentiable quantization-aware training framework for extremely low-bit LLMs through a two-stage training process. First, it computes per-tensor sensitivity scores using the Hutch++ algorithm to estimate Hessian traces from calibration data. During training, it uses a temperature-controlled softmax relaxation to maintain gradient flow, with temperatures adaptively scheduled based on the sensitivity scores. The method employs a convex interpolation between full-precision weights and relaxed weights, with pressure gradually increasing during an initial compress stage and temperatures decaying during an annealing stage. This approach enables smooth transition from full-precision to ternary representations while preserving representational capacity.

## Key Results
- HESTIA improves zero-shot accuracy by 5.39% on LLaMA-3.2-1B and 4.34% on LLaMA-3.2-3B compared to state-of-the-art ternary QAT baselines
- Hessian-aware temperature scheduling improves average scores from 0.540 to 0.547 (+1.30%) on 1B models in ablation studies
- The framework successfully quantizes models to 1.58 bits while maintaining competitive performance on zero-shot benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Variance-Modulated Gradient Flow via Softmax Relaxation
HESTIA replaces the non-differentiable step function with a temperature-controlled softmax surrogate that maintains gradient flow during early training. The softmax kernel πτ(q|w) assigns soft probabilities over ternary states {-1, 0, +1}, with the effective weight being the expectation ˜w = γ·Σq·πτ(q|w). The Jacobian ∂H/∂w = (2/τ)·Vτ(w) shows gradients are modulated by assignment uncertainty—amplified when assignments are ambiguous, attenuated when confident. As τ→0⁺, the surrogate converges to hard quantization, addressing the dead zones where STE-based updates fail to induce discrete state transitions.

### Mechanism 2: Hessian-Trace Guided Per-Tensor Annealing Schedules
Tensor-wise Hessian trace metrics predict quantization sensitivity, enabling fine-grained temperature schedules that preserve critical layers longer while accelerating insensitive tensors. Sensitivity score s_i = Sigmoid(κ·(log h_i - μ_h)/σ_h + ε) normalizes per-tensor Hessian traces estimated offline via Hutch++. Temperature schedule τ_i(t) = τ̄(t)·e^{αs_i} scales a base cosine decay by e^{αs_i}, so high-curvature tensors maintain elevated temperatures longer, preserving their representational integrity.

### Mechanism 3: Two-Stage Compress-Then-Anneal Training Trajectory
HESTIA separates "when to quantize" from "how hard to quantize" through a convex interpolation Weff(t) = (1-p_t)·W + p_t·H(W;τ). During the compress stage (first ρ fraction of training), p_t ramps linearly from 0→1 while τ remains at τ_init, allowing full-precision weights to guide the soft surrogate. During annealing, p_t=1 and τ decays, hardening the quantizer. This continuation path smoothly transfers representational responsibility, preventing abrupt representational collapse.

## Foundational Learning

- **Concept: Straight-Through Estimator (STE) and its gradient mismatch problem**
  - Why needed here: HESTIA's entire motivation stems from STE's limitation—treating quantization as identity during backprop creates mismatch between discrete forward signals and continuous weight updates, causing dead zones where updates don't trigger state transitions.
  - Quick check question: Given a ternary quantizer Q(w) ∈ {-γ, 0, +γ}, explain why a weight w=0.3γ with gradient ∂L/∂ŵ = 0.5 might fail to transition states under STE but succeed under HESTIA's softmax surrogate at τ=0.5.

- **Concept: Hessian trace as sensitivity metric**
  - Why needed here: HESTIA repurposes Hessian trace (traditionally used for mixed-precision allocation) as a temporal annealing scheduler. Understanding why curvature correlates with quantization sensitivity is essential for interpreting the adaptive schedule.
  - Quick check question: If layer A has Hessian trace 10× higher than layer B, how would HESTIA's temperature schedules differ between them, and what does this imply about their respective annealing rates?

- **Concept: Softmax temperature annealing and Gumbel-softmax connection**
  - Why needed here: HESTIA's πτ(q|w) = Softmax(-(w/γ - q)²/τ) is a distance-aware softmax; understanding how τ controls the soft-to-hard transition and connects to stochastic gradient estimation is prerequisite.
  - Quick check question: As τ→0, what does πτ(q|w) converge to, and why does the Jacobian ∂H/∂w approach a sum of Dirac deltas at decision boundaries?

## Architecture Onboarding

- **Component map**: Sensitivity Estimator (offline) -> Temperature Scheduler -> Softmax Quantizer -> Pressure Controller -> Weight Blender
- **Critical path**: 1) Run Hutch++ on all linear layers → store sensitivity scores s_i; 2) Initialize τ_init by minimizing MSE between full-precision and relaxed weights; 3) Training loop: compute p_t, look up per-tensor τ_i(t), compute H(W_i; τ_i), blend to Weff, run forward/backward, update latent W; 4) End state: τ→0 ensures H(W;τ)→Q(W), yielding true ternary weights for inference
- **Design tradeoffs**: Hutch++ sketch rank r vs. calibration samples (higher r improves accuracy but increases pre-computation cost); Compress ratio ρ vs. training efficiency (longer compress stage improves stability but delays discretization learning); Temperature scaling strength α (controls differentiation between schedules); Group size (group-wise provides modest gain but isn't main driver)
- **Failure signatures**: Gradient explosion early in training (τ_init too small); No accuracy improvement over STE baseline (Hessian estimation failed, sensitivity scores uniform); Inference weights not truly ternary (τ didn't reach 0 at training end); Per-layer performance variance (some layers over-quantized, others under-quantized)
- **First 3 experiments**: 1) Ablation: Softmax relaxation alone (w/o Hessian) - implement uniform temperature schedule with softmax quantizer to isolate gradient-flow benefit; 2) Sensitivity score validation - visualize distribution of s_i across layers and compare to layer-wise reconstruction error from naive ternarization; 3) Compress ratio ρ sweep - train with ρ ∈ {0.1, 0.2, 0.3, 0.5} and monitor training loss curve smoothness and final accuracy

## Open Questions the Paper Calls Out
- Can the temperature-controlled Softmax relaxation be effectively adapted for activation quantization, or is it fundamentally limited by the dynamic input distributions of activations?
- Is the specific Sigmoid-based normalization (Eq. 13) for sensitivity scores theoretically optimal for all architectures, or is it a heuristic that requires tuning?
- Does the variance introduced by the Hutch++ approximation of the Hessian trace lead to instability in the temperature schedule for tensors with low sensitivity?

## Limitations
- Weak corpus validation for Hessian-guided annealing as the primary novelty mechanism
- Sensitive to pre-training hyperparameters (ρ=0.2, α=0.4) without sensitivity analysis
- Effectiveness depends on accurate Hessian trace estimation from representative calibration data

## Confidence
- **High confidence**: Softmax relaxation replacing STE maintains gradient flow and mitigates dead zones
- **Medium confidence**: Two-stage compress-then-anneal training trajectory prevents representational collapse
- **Low confidence**: Hessian-trace guided per-tensor annealing schedules are the primary novelty

## Next Checks
1. Cross-model sensitivity correlation validation: Run Hutch++ on different architectures (Mistral or Phi-2) and verify whether Hessian traces still correlate with ternarization reconstruction error
2. Compress ratio ablation on downstream tasks: Systematically vary ρ ∈ {0.1, 0.2, 0.3, 0.5} while keeping other hyperparameters fixed, measuring accuracy, training stability, and convergence speed
3. Stochastic vs deterministic Hessian estimation: Compare Hutch++ results using 20 samples vs 50 samples, evaluating whether temperature schedules derived from high-variance vs low-variance sensitivity estimates lead to different quantization quality