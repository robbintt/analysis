---
ver: rpa2
title: Making, not Taking, the Best of N
arxiv_id: '2510.00931'
source_url: https://arxiv.org/abs/2510.00931
tags:
- wang
- fusor
- zhang
- data
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a simple yet effective method for aggregating
  multiple LLM-generated candidates into a single, higher-quality output. Instead
  of selecting the best candidate via scoring (Best-of-N), the proposed Fusion-of-N
  method uses a generative LLM judge to synthesize a new response that integrates
  the strengths of all candidates.
---

# Making, not Taking, the Best of N

## Quick Facts
- **arXiv ID:** 2510.00931
- **Source URL:** https://arxiv.org/abs/2510.00931
- **Reference count:** 40
- **Primary result:** Fusion-of-N consistently outperforms Best-of-N across test-time scaling and synthetic data generation, improving win rates, translation quality, and reasoning accuracy.

## Executive Summary
This work introduces Fusion-of-N, a method that synthesizes a single high-quality output by combining multiple LLM-generated candidates rather than selecting the best via scoring. The approach uses a generative LLM judge (fusor) to integrate the strengths of all candidates, showing consistent gains over Best-of-N across 11 languages, 3 tasks, and varying model scales. Extensive experiments demonstrate that treating LLM generations as collaborators—rather than competitors—unlocks higher-quality outputs and better downstream performance, with benefits in sample efficiency and robustness to weaker teacher pools.

## Method Summary
Fusion-of-N replaces the standard Best-of-N approach of selecting the best candidate via scoring with a generative synthesis step. Given N LLM-generated samples, a strong fusor LLM is prompted to produce a new output that integrates the strengths of all candidates. This is evaluated in two settings: test-time scaling (generating and fusing multiple samples from a single model) and synthetic data generation (fusing samples from diverse teacher models for fine-tuning). The fusor’s model size is critical, with 27B+ parameters recommended for effective synthesis. The method is evaluated on multilingual benchmarks, including translation, reasoning, and open-ended instruction following, using win rates, translation quality metrics, and reasoning accuracy.

## Key Results
- Fusion-of-N consistently outperforms Best-of-N in win rates, translation quality, and reasoning accuracy across 11 languages and 3 tasks.
- The method is more sample-efficient and robust to weaker teacher pools, sometimes surpassing the oracle baseline.
- Fusor model size matters: models smaller than 27B parameters show a sharp drop in performance.
- Diversity in the candidate pool is key to Fusion-of-N’s success; smaller 7B students are more sensitive to data quality.

## Why This Works (Mechanism)
Fusion-of-N leverages the generative capabilities of a strong LLM to synthesize a new response by integrating the diverse strengths of multiple candidate outputs. Instead of merely selecting the best, the fusor can combine correct reasoning, factual accuracy, and language fluency from different samples, resulting in a more robust and higher-quality final output. This approach is especially effective for open-ended tasks where multiple valid solutions exist.

## Foundational Learning
- **Best-of-N:** Selecting the highest-scoring candidate from multiple LLM samples. Needed for comparison; check by verifying candidate scoring is based on a reward model.
- **Fusion-of-N:** Synthesizing a new output by integrating multiple LLM candidates via a generative fusor. Needed for understanding the core contribution; check by confirming the fusor prompt includes all N candidates.
- **Fusor model scale:** The importance of fusor size (27B+ recommended). Needed to explain performance thresholds; check by referencing ablation studies on fusor scale.
- **Synthetic data generation:** Using fused outputs to train student models. Needed for understanding fine-tuning applications; check by confirming student models are fine-tuned on fused data.
- **Reward model (RM):** Scoring candidates or outputs, used for Best-of-N comparison. Needed for benchmarking; check by noting the RM is proprietary and non-public.
- **Diversity in candidate pool:** The benefit of sampling from multiple, diverse teacher models. Needed for understanding robustness; check by confirming diversity improves fusion quality.

## Architecture Onboarding

**Component Map:**
Single model (or multiple teachers) -> Generate N candidates -> Format with prompt -> Fusor (27B+) -> Fused output -> (Optional) Fine-tune student model

**Critical Path:**
Generate N diverse candidates → Format all candidates with the prompt → Feed to strong fusor (27B+) → Produce fused output

**Design Tradeoffs:**
- Fusor scale vs. computational cost: 27B+ fusors are needed for strong performance, increasing resource requirements.
- Fixed N vs. adaptive compute: Current method uses fixed N; adaptive strategies could improve efficiency but are unexplored.
- English-only prompt vs. multilingual fusor: English prompt may limit language correctness; multilingual fusors could improve cross-lingual performance.

**Failure Signatures:**
- FusioN underperforms on tightly constrained tasks (e.g., math) where outcome-based selection is superior.
- Small fusors (4B-12B) fail to synthesize effectively.
- 7B student models may regress on open-ended tasks after fine-tuning on FusioN data.

**First Experiments:**
1. Generate N=5 candidates from a single model at T=0.7 and fuse with a 27B+ fusor.
2. Evaluate win rate vs Gemini 2.5-Pro/Flash using Arena with GPT-4o judge.
3. Fine-tune a 7B student on fused data and evaluate on GeoFactX and WMT.

## Open Questions the Paper Calls Out
- **Open Question 1:** Why does generative synthesis underperform on tightly constrained tasks like mathematical reasoning, and what modifications could mitigate this? (Basis: Section 5 and E.2; unresolved due to lack of specialized training experiments.)
- **Open Question 2:** How does FusioN performance scale with adaptive compute strategies rather than fixed N? (Basis: Section 6; unresolved as current method uses fixed N.)
- **Open Question 3:** Does translating the fusion prompt or using a multilingually-aligned fusor improve language correctness in low-resource languages? (Basis: Section 5; unresolved as authors leave this to future work.)

## Limitations
- Fusor model size is a major barrier: models smaller than 27B show sharp performance drops.
- On tightly constrained reasoning tasks, Fusion-of-N underperforms specialized Best-of-N methods.
- 7B student models are sensitive to data quality and may regress on open-ended tasks after fine-tuning.
- Oracle baseline relies on a non-public, proprietary multilingual reward model.

## Confidence
- **High confidence:** Fusion-of-N consistently outperforms Best-of-N on open-ended, subjective tasks and improves fine-tuning outcomes with large fusors.
- **Medium confidence:** Claims about robustness to weak teacher pools and better generalization are supported but less extensively validated.
- **Low confidence:** Claims that Fusion-of-N can surpass the Oracle are difficult to verify due to reliance on a non-public benchmark.

## Next Checks
1. Replicate ablation studies on fusor scale (Figure 5) using publicly available fusor models (e.g., 7B, 13B, 33B) to confirm the sharp threshold effect.
2. Test Fusion-of-N on constrained reasoning tasks (e.g., GSM8K, MATH) to verify limitations and compare with outcome-based Best-of-N methods.
3. Evaluate robustness of 7B student models across multiple fine-tuning runs with varying dataset sizes and hyperparameters to confirm sensitivity to data quality.