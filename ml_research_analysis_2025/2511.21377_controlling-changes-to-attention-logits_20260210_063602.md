---
ver: rpa2
title: Controlling changes to attention logits
arxiv_id: '2511.21377'
source_url: https://arxiv.org/abs/2511.21377
tags:
- learning
- arxiv
- attention
- norm
- logits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a method to stabilize transformer training\
  \ by controlling changes in attention logits rather than their magnitude. The core\
  \ idea is to assign parameter-dependent learning rates to query and key weights\
  \ based on the norms of their corresponding matrices, inspired by Maximal Update\
  \ Parametrization (\xB5P) principles."
---

# Controlling changes to attention logits

## Quick Facts
- arXiv ID: 2511.21377
- Source URL: https://arxiv.org/abs/2511.21377
- Reference count: 9
- Primary result: Controls attention logit changes via parameter-dependent learning rates for stable training at high learning rates

## Executive Summary
This paper addresses training instability in transformers by controlling changes in attention logits rather than their magnitude. The proposed method, QuacK, assigns parameter-dependent learning rates to query and key weights based on the norms of their corresponding matrices. Inspired by Maximal Update Parametrization principles, this approach bounds the worst-case change in logits independently of weight size, enabling stable training at high learning rates (up to 3e-2) while being computationally cheaper than alternatives like QK norm.

## Method Summary
The method controls the worst-case change in attention logits during training by assigning parameter-dependent learning rates to query and key weights. Specifically, the learning rate for query weights is set inversely proportional to the norm of corresponding key weights, and vice versa. This is derived from a first-order analysis where the change in logits is bounded by keeping terms "order 1." The approach maintains stability without constraining logit magnitude and is particularly valuable for Multi-Latent Attention (MLA) where QK normalization is not applicable.

## Key Results
- Enables stable training at high learning rates (3e-2) in both MHA and MLA settings
- Achieves performance competitive with QK norm in standard Multi-Head Attention
- Outperforms QK clip in Multi-Latent Attention settings
- Provides approximately 10% faster training compared to QK norm by removing two RMS norm computations per attention block

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bounding the worst-case change in attention logits during training improves stability without constraining logit magnitude.
- Mechanism: The method assigns parameter-dependent learning rates to query and key weights. Specifically, the learning rate for query weights is set inversely proportional to the norm of corresponding key weights, and vice versa. This is derived from a first-order analysis where the change in logits is $\Delta L \approx Q(\Delta K)^T + (\Delta Q)K^T$. By setting $\eta_Q \propto \|W_K\|^{-1}$ and $\eta_K \propto \|W_Q\|^{-1}$, the terms $Q(\Delta K)^T$ and $(\Delta Q)K^T$ are kept "order 1," bounding the worst-case change.
- Core assumption: The weight norms $\|W_Q\|$ and $\|W_K\|$ are lower-bounded by a constant $c > 0$, and the gradients are bounded by a constant $D$ (valid for Adam and Muon).
- Evidence anchors:
  - [abstract] "We show that these changes are controllable by assigning parameter-dependent learning rates to the query and key weights."
  - [section 3] Lemma 1 proves that with the proposed learning rate scheme, "the worst-case change in logits is bounded above independently of the weight size."
  - [corpus] No directly relevant corpus evidence found for this specific mechanism.

### Mechanism 2
- Claim: The method enables higher base learning rates while maintaining stability comparable to QK normalization.
- Mechanism: By directly controlling the step-to-step change in logits, the method prevents large, destabilizing updates that can occur at high learning rates. QK normalization controls logit magnitude, while this method controls logit *change*, which the authors argue is the key to stability during training.
- Core assumption: Controlling the *change* in logits is sufficient for stability, even if the magnitude of the logits themselves grows.
- Evidence anchors:
  - [abstract] "Our cheap intervention allows us to increase the base learning rate of the network."
  - [section 4, Figure 2] Shows that at a high base learning rate ($\eta=3e-2$), the proposed method (QuacK) remains stable and performant in both MHA and MLA settings, where QK clip fails and the simple ablation underperforms.
  - [corpus] Related work on Muon (corpus paper 61882) discusses optimizer stability, which is related but not directly about logit change.

### Mechanism 3
- Claim: This approach is uniquely suitable for Multi-Latent Attention (MLA) where QK normalization is not applicable.
- Mechanism: QK normalization requires full materialization of queries and keys at inference time for normalization, which MLA avoids for efficiency. The proposed method modifies learning rates based on weight norms, which can be computed regardless of whether the keys/queries are materialized. This allows it to be applied to the MLA architecture, including its multiple parameter matrices ($W_{uq}, W_{uk}, W_{qr}, W_{dq}, W_{dkv}, W_{kr}$).
- Core assumption: The first-order analysis bounding logit change can be extended to the more complex weight structure of MLA.
- Evidence anchors:
  - [abstract] "...outperform other methods in the MLA setting..."
  - [section 1] "QK norm is not compatible with Multi Latent Attention (MLA)... In this paper we suggest that controlling the changes to logits is important for stability."
  - [corpus] Paper 93822 discusses implementing MLA in existing LLMs, reinforcing its relevance and the need for compatible techniques.

## Foundational Learning

### Concept: Maximal Update Parametrization (µP)
- Why needed here: This work is inspired by µP's goal of controlling the magnitude of activations and their updates to enable stable scaling. Understanding µP provides the context for the "order 1-like" change desideratum.
- Quick check question: What is the core principle of µP regarding activation updates that this paper adapts for attention logits?

### Concept: Attention Logits and Instability
- Why needed here: The problem being solved is training instability caused by exploding query/key weights and logits. Knowing that large logits can lead to entropy collapse is crucial.
- Quick check question: Why can large attention logits lead to training instability in transformers?

### Concept: QK Normalization vs. QK Clip
- Why needed here: These are the key baselines. QK norm normalizes activations, QK clip constrains logit magnitude. This paper offers an alternative by controlling logit *change*.
- Quick check question: What is the fundamental operational difference between QK norm and the proposed method for controlling attention stability?

## Architecture Onboarding

### Component map
Query/Value matrices (W_Q, W_K) in attention heads -> Learning rate scaling factors -> Optimizer

### Critical path
1. At initialization, calculate and store the Frobenius norm of each query and key weight matrix ($W_{Q,h}^{\ell}, W_{K,h}^{\ell}$ for all layers $\ell$ and heads $h$).
2. During training (before each optimizer step), re-calculate the current norms of the weight matrices.
3. Adjust learning rates: Modify the learning rate for each weight using the formula: $W_{Q,h}^{\ell}.lr \leftarrow \tau \eta \cdot \frac{W_{K,h}^{\ell}.init\_norm}{\|W_{K,h}^{\ell}\|}$ and the inverse for the key weights. Use $\eta$ as the base learning rate and $\tau$ as a hyperparameter.

### Design tradeoffs
- Pros: Simple to implement as a "drop-in" with no specialized kernels. Computationally cheaper than QK norm (~10% faster in experiments). Applicable to MLA.
- Cons: Introduces a new hyperparameter $\tau$. In MHA, performance is competitive but slightly below QK norm. Relies on a first-order approximation which may not hold perfectly.
- Norm Choice: The paper recommends Frobenius norm for computational efficiency, as the theoretical benefit of spectral norm was minimal in preliminary tests.

### Failure signatures
- Instability at high LR: If $\tau$ is set too high, logit changes will be too large, leading to the same instability the method aims to prevent.
- Stagnation at low LR: If $\tau$ is set too low, the model may train too slowly and underperform.

### First 3 experiments
1. Baseline Reproduction: Implement the method in a standard MHA transformer and train on a small-scale task. Sweep $\tau \in \{10^{-2}, 10^{-1}, 1, 10\}$ and base LR $\eta \in \{3e-4, 3e-3, 3e-2\}$ to verify stability and performance compared to a QK norm baseline.
2. Ablation vs. Full Method: Compare the full QuacK method against the "Ablation" baseline described in the paper (which uses a fixed reduced LR for Q/K weights). This isolates the benefit of the *dynamic* learning rate adjustment.
3. MLA Integration: Integrate the method into an MLA architecture following Algorithm 2 in the paper. Compare its performance and stability against QK clip to validate its primary use case.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does QuacK maintain its stability benefits at larger model scales (e.g., 7B+ parameters) and longer training runs (100K+ steps)?
- Basis in paper: [explicit] Section 5 states "we were unable to execute longer training runs and with larger models to demonstrate that our method is widely applicable."
- Why unresolved: All experiments used ~1B parameter models trained for only 5K steps due to compute constraints.
- What evidence would resolve it: Scaling experiments with larger models and longer training durations comparing QuacK against baselines.

### Open Question 2
- Question: Does QuacK generalize across different architectures, datasets, and optimizers beyond Muon?
- Basis in paper: [explicit] Section 5 notes "results are limited by... a single dataset and model architecture" (Qwen3-based with Cosmopedia-V2).
- Why unresolved: No experiments tested alternative optimizers like AdamW, different architectures, or diverse datasets.
- What evidence would resolve it: Systematic ablations across architectures (LLaMA, Mistral), optimizers, and datasets.

### Open Question 3
- Question: Can the slight performance gap between QuacK and QK norm in MHA settings be eliminated?
- Basis in paper: [inferred] Section 4 reports QuacK achieves "similar but slightly worse performance compared to QK norm in the MHA setting."
- Why unresolved: The paper doesn't investigate the source of this gap or potential remedies.
- What evidence would resolve it: Analysis of what causes the gap and method modifications that close it.

## Limitations
- The theoretical analysis relies on first-order approximations that may not capture higher-order dynamics during training
- Extension to MLA lacks the same theoretical rigor as the MHA case and depends on more complex learning rate assignments
- All experiments used ~1B parameter models trained for only 5K steps, limiting scalability validation

## Confidence

**High Confidence**: The empirical demonstration that QuacK enables stable training at high learning rates (3e-2) in both MHA and MLA settings is well-supported by the experimental results. The computational efficiency claim (10% faster than QK norm) is directly measured and reported.

**Medium Confidence**: The theoretical proof that learning rate scaling based on weight norms bounds logit change is sound but depends on assumptions about weight behavior during training. The claim that controlling logit *change* rather than magnitude is the key to stability is logically consistent with the analysis but would benefit from more extensive ablation studies.

**Low Confidence**: The claim about QuacK being "uniquely suitable" for MLA over QK norm is supported by the incompatibility argument but lacks extensive comparative analysis across diverse MLA architectures and scales.

## Next Checks
1. **Stability Boundary Analysis**: Systematically vary the hyperparameter τ across multiple orders of magnitude (e.g., {0.001, 0.01, 0.1, 1, 10, 100}) and identify the precise stability boundaries where training diverges. This would quantify the sensitivity to τ and validate the boundedness claims.

2. **Higher-Order Effect Study**: Design experiments to test whether second-order effects of weight updates impact stability predictions. This could involve comparing training trajectories when using exact vs. first-order approximations of logit changes.

3. **Cross-Architecture Generalization**: Implement and test QuacK on architectures beyond the 1B-parameter models studied, including both smaller models (to test sensitivity) and larger models (to test scalability). Include architectures with different attention mechanisms (e.g., gated attention, multi-query attention) to assess generalizability.