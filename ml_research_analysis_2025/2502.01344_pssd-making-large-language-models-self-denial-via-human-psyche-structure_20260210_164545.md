---
ver: rpa2
title: 'PSSD: Making Large Language Models Self-denial via Human Psyche Structure'
arxiv_id: '2502.01344'
source_url: https://arxiv.org/abs/2502.01344
tags:
- llms
- pssd
- reasoning
- answer
- role
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of improving reasoning accuracy\
  \ in large language models (LLMs) by focusing on self-denial\u2014the ability to\
  \ identify and correct mistakes within the model's own reasoning. Current approaches,\
  \ such as multi-agent debates, often rely on extensive resource competition and\
  \ lack mechanisms for effective self-correction."
---

# PSSD: Making Large Language Models Self-denial via Human Psyche Structure

## Quick Facts
- arXiv ID: 2502.01344
- Source URL: https://arxiv.org/abs/2502.01344
- Reference count: 40
- Key result: PSSD improves reasoning accuracy by 4.55% EM on AdvHotpotQA over CoT-SC baseline

## Executive Summary
PSSD introduces a human psyche-inspired framework to improve LLM reasoning accuracy by implementing self-denial mechanisms. The method decomposes reasoning into three interconnected roles based on Freudian psychology: the intuition-based id generates multiple reasoning attempts, the rule-driven superego provides methodological guidance through pre-summarized rules, and the script-centric ego synthesizes information into executable scripts for final answers. Extensive experiments show PSSD outperforms existing methods across textual and mathematical reasoning tasks while reducing resource usage compared to multi-agent debate approaches.

## Method Summary
PSSD implements a three-role sequential framework where an id agent generates multiple reasoning paths, a superego agent retrieves rules and provides key points, and an ego agent synthesizes these into an executable script. The method can be implemented as a multi-agent system or through single-model fine-tuning (PSSD-SFT) using LoRA adapters. Rule generation involves contrastive analysis between high-quality (GPT-4) and low-quality (LLaMA-2) reasoning to extract patterns. The ego role executes the generated script to produce the final answer, forcing concrete refinement paths rather than vague self-reflection.

## Key Results
- PSSD achieves 4.55% higher exact match on AdvHotpotQA compared to CoT-SC baseline
- The method reduces resource usage (5 calls vs 7.8 for Self-Contrast) while maintaining accuracy
- PSSD is compatible with external tools like ReAct and VE, further enhancing performance
- Superego role contributes 0.64% EM improvement, while Ego role adds 1.63% EM gain

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Methodological regularization via pre-summarized rules may enhance the reliability of self-correction.
- **Mechanism:** The framework contrasts high-quality reasoning (GPT-4) against low-quality reasoning (LLaMA-2) to extract patterns and summarize "rules." These rules guide the "Superego" agent to generate specific "key points" rather than generic critiques, providing persuasive internal references for denial.
- **Core assumption:** Reasoning errors in LLMs follow identifiable patterns that can be codified into generalizable rules.
- **Evidence anchors:** Section 3.3.1 describes "Pattern Extraction" and "Rule Summary" phases using contrastive analysis. Table 3 shows 0.64% EM drop when summarized rules are removed.
- **Break condition:** If reasoning errors are stochastic or highly domain-specific, static rule banks may fail to provide relevant guidance.

### Mechanism 2
- **Claim:** Translating abstract critique into an executable script improves rectification rates.
- **Mechanism:** The "Ego" agent synthesizes the "Id's" initial attempts and the "Superego's" key points into a step-by-step script (e.g., "Verify the adaptation occurred"). It then "executes" this script to generate the final answer, forcing a concrete refinement path rather than vague self-reflection.
- **Core assumption:** LLMs can more accurately correct reasoning when following a procedural plan than when attempting direct answer revision.
- **Evidence anchors:** Abstract mentions the "script-centric ego role that absorbs all procedural information to generate executable script." Table 3 indicates Ego role contributes 1.63% EM improvement.
- **Break condition:** If the "Ego" script generation inherits hallucinations from the "Id," the execution step may formalize incorrect logic.

### Mechanism 3
- **Claim:** Decoupling generation, regulation, and execution reduces resource competition found in standard multi-agent debates.
- **Mechanism:** By assigning fixed responsibilities (Id generates, Superego regulates, Ego executes), the system avoids the iterative, unstructured "tit-for-tat" arguing typical of debates. This limits API calls to a linear flow (Id -> Superego -> Ego).
- **Core assumption:** A single pass of structured, role-based interaction is sufficient to identify and correct errors, eliminating the need for multi-round consensus building.
- **Evidence anchors:** Abstract claims the method avoids being "stuck in a state of resource competition demanding significant time." Table 5 reports PSSD uses 5 calls vs. 7.8 for Self-Contrast.
- **Break condition:** Complex logic requiring true backtracking may fail if the linear Id-Superego-Ego flow cannot loop back to the Id for fresh attempts.

## Foundational Learning

- **Concept: Chain-of-Thought Self-Consistency (CoT-SC)**
  - **Why needed here:** The "Id" role in PSSD is fundamentally a CoT-SC implementation that generates multiple reasoning paths ($l=5$) to create a diverse candidate set. Understanding sampling variability is crucial.
  - **Quick check question:** Can you explain why CoT-SC sometimes fails to improve over standard CoT (as seen in Table 1 for AdvHotpotQA)?

- **Concept: LoRA (Low-Rank Adaptation)**
  - **Why needed here:** PSSD-SFT relies on LoRA to merge the three distinct role-specific adapters ($W_1, W_2$) into a single base model efficiently, avoiding full fine-tuning costs.
  - **Quick check question:** How does merging LoRA weights ($W = W_0 + W_1 + W_2$) allow a single model to perform sequential, multi-role tasks?

- **Concept: Prompt Engineering Role-Playing**
  - **Why needed here:** The non-fine-tuned PSSD version relies entirely on precise prompts ($M_{Id}, M_{Superego}, M_{Ego}$) to enforce the Freudian roles (Id, Superego, Ego).
  - **Quick check question:** What is the risk of "role bleed" where the Id agent ignores its intuition prompt and attempts to self-correct prematurely?

## Architecture Onboarding

- **Component map:** Input Q -> [Id] Generate Attempts -> [Superego] Rules + Critique -> Key Points -> [Ego] Synthesize -> Script -> Execution -> Final Answer

- **Critical path:** `Input Q` -> `[Id] Generate Attempts` -> `[Superego] Rules + Critique -> Key Points` -> `[Ego] Synthesize -> Script -> Execution` -> `Final Answer`

- **Design tradeoffs:**
  - PSSD vs. PSSD-SFT: PSSD (multi-agent) offers modularity but higher latency/API costs. PSSD-SFT (single merged model) offers speed (2 calls) but requires a complex data construction and fine-tuning phase.
  - Rule Specificity: Highly specific rules may improve accuracy but fail to generalize to out-of-distribution questions.

- **Failure signatures:**
  - Stuck Id: If the Id generates 5 identical wrong paths, Superego has no diversity to analyze.
  - Hallucinated Rules: If the Superego retrieves irrelevant rules, the Ego's script will solve the wrong problem.
  - Silent Execution: In PSSD-SFT, if the merged model skips the "Key Point" generation step, the self-denial mechanism is bypassed.

- **First 3 experiments:**
  1. **Baseline Validation:** Implement the "Id" role using CoT-SC ($k=5$) to verify if the base model's diversity is sufficient for the dataset (check PM metric).
  2. **Rule Ablation:** Run the "Superego" with and without the retrieved "Rules" (using just the prompt) to quantify the value of the pre-summarized rule bank.
  3. **PSSD-SFT data Gen:** Generate a small batch (e.g., 50 samples) of the training template (Question + Attempts + Key Points + Script + Answer) to test the LoRA fine-tuning pipeline.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the high confidence levels associated with incorrect predictions in PSSD be effectively reduced or calibrated? The paper notes incorrect samples often exhibit a bimodal distribution with high confidence and leaves this as future work.

- **Open Question 2:** How can the PSSD framework be enhanced to increase the rate of correct hits for questions where the initial "id" role fails to generate the correct answer in its attempts? The paper explicitly states it leaves efforts on increasing correct hits to future work.

- **Open Question 3:** What is the optimal strategy for integrating PSSD with external retrieval tools beyond the current consistency-threshold-based triggering? The paper observes that combining PSSD with tools yields improvements and suggests this as a future direction.

## Limitations
- The method relies heavily on specific prompt templates that are not fully specified in the main text, creating uncertainty about exact replication
- The static rule bank may not generalize well to out-of-distribution questions or different reasoning domains
- Resource efficiency claims don't account for the significant upfront cost of rule generation using GPT-4

## Confidence
- **High Confidence:** Core architectural claim supported by consistent EM improvements across multiple datasets (4.55% on AdvHotpotQA)
- **Medium Confidence:** Script execution mechanism plausible but relies on assumptions about LLMs' procedural reasoning abilities
- **Low Confidence:** Resource efficiency claims lack direct empirical support and don't include full cost analysis

## Next Checks
1. **Prompt Ablation Study:** Systematically remove or simplify components of the Ego prompt to determine which elements are essential for performance gains
2. **Cross-Domain Rule Transfer:** Apply the same rule bank to a distinctly different reasoning dataset to quantify degradation and test generalization limits
3. **Cost-Benefit Analysis:** Measure total resource consumption including one-time rule generation costs across 100+ queries to validate claimed efficiency improvements