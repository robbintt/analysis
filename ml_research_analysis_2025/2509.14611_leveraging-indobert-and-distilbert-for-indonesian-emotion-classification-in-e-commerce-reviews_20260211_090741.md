---
ver: rpa2
title: Leveraging IndoBERT and DistilBERT for Indonesian Emotion Classification in
  E-Commerce Reviews
arxiv_id: '2509.14611'
source_url: https://arxiv.org/abs/2509.14611
tags:
- indobert
- emotion
- classification
- distilbert
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study improves emotion classification in Indonesian e-commerce
  reviews by leveraging advanced NLP models IndoBERT and DistilBERT, combined with
  data augmentation techniques such as back-translation and synonym replacement. The
  authors process and balance an Indonesian emotion dataset (PRDECT-ID) using undersampling
  and augmentation, achieving an 80% accuracy with IndoBERT after hyperparameter tuning.
---

# Leveraging IndoBERT and DistilBERT for Indonesian Emotion Classification in E-Commerce Reviews

## Quick Facts
- **arXiv ID:** 2509.14611
- **Source URL:** https://arxiv.org/abs/2509.14611
- **Reference count:** 40
- **Primary result:** Achieved 80% accuracy on Indonesian emotion classification using IndoBERT with data augmentation and hyperparameter tuning.

## Executive Summary
This study improves emotion classification in Indonesian e-commerce reviews by leveraging advanced NLP models IndoBERT and DistilBERT, combined with data augmentation techniques such as back-translation and synonym replacement. The authors process and balance an Indonesian emotion dataset (PRDECT-ID) using undersampling and augmentation, achieving an 80% accuracy with IndoBERT after hyperparameter tuning. While combining multiple IndoBERT models via bagging showed marginal improvements, IndoBERT consistently outperformed DistilBERT, demonstrating its suitability for Indonesian emotion classification. Data augmentation was identified as crucial for improving model performance. The results highlight the potential of IndoBERT for Indonesian NLP tasks and suggest future work focus on addressing overfitting and exploring alternative architectures for low-resource datasets.

## Method Summary
The authors fine-tuned IndoBERT and DistilBERT on the PRDECT-ID Indonesian emotion dataset (5,400 samples, 5 classes). They balanced the dataset by undersampling the majority class and augmenting minority classes using back-translation (ID-EN-ID, ID-AR-ID) and synonym replacement until all classes had 1,770 samples. Text preprocessing included lowercasing and removing non-alphabetic characters while retaining stopwords. Hyperparameter tuning identified optimal settings: 10 epochs, batch size 8, weight decay 0.3, and dropout 0.1. An ensemble bagging approach combining multiple IndoBERT models was also tested but showed minimal performance gains.

## Key Results
- IndoBERT achieved 80% accuracy on Indonesian emotion classification, outperforming DistilBERT at 75.14%
- Data augmentation improved performance by approximately 10% compared to models trained without augmentation
- Retaining stopwords during preprocessing was critical, with stopword removal causing 6-8% accuracy degradation
- Bagging ensemble of multiple IndoBERT models provided only marginal improvements (0.2% gain)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data augmentation via back-translation and synonym replacement substantially improves emotion classification accuracy for low-resource Indonesian datasets.
- Mechanism: Augmentation increases training sample diversity and balances class distribution, reducing overfitting to minority classes and providing more varied linguistic patterns for the model to learn from.
- Core assumption: Augmented examples preserve emotional semantics of the original text despite translation perturbations.
- Evidence anchors:
  - [abstract] "A key component of our approach was data processing, specifically data augmentation... These methods played a significant role in boosting the model's performance."
  - [section] Table 4 shows IndoBERT accuracy jumping from ~68% (without augmentation) to ~78% (with augmentation)—a 10% improvement.
  - [corpus] Weak direct corpus evidence; neighbor papers focus on sentiment analysis rather than emotion classification augmentation.
- Break condition: If augmented samples introduce semantic drift (e.g., "anger" becomes "annoyance" or neutral), the signal-to-noise ratio degrades, and accuracy gains may reverse.

### Mechanism 2
- Claim: IndoBERT consistently outperforms DistilBERT for Indonesian emotion classification due to language-specific pre-training and larger model capacity.
- Mechanism: IndoBERT is pre-trained on Indonesian corpora, encoding language-specific morphology, syntax, and cultural-emotional expressions that DistilBERT (trained primarily on English) lacks despite distillation efficiency.
- Core assumption: The performance gap stems from language-specific knowledge rather than solely from model size differences.
- Evidence anchors:
  - [abstract] "IndoBERT consistently outperformed DistilBERT, demonstrating its suitability for Indonesian emotion classification."
  - [section] Table 5: IndoBERT achieves 80% accuracy after tuning; DistilBERT peaks at 75.14% under comparable conditions.
  - [corpus] Neighbor paper "Sentiment Analysis Of Shopee Product Reviews Using Distilbert" shows DistilBERT applied to Indonesian reviews but does not benchmark against IndoBERT.
- Break condition: If computational resources are severely constrained, DistilBERT's efficiency advantage may outweigh IndoBERT's accuracy edge in production deployments.

### Mechanism 3
- Claim: Retaining stopwords preserves contextual integrity critical for emotion detection in Indonesian text.
- Mechanism: Indonesian stopwords (e.g., "ada," "dia") carry discourse-level context that helps models distinguish between emotionally similar but pragmatically different utterances.
- Core assumption: The performance drop from stopword removal is due to context loss rather than increased noise from other preprocessing steps.
- Evidence anchors:
  - [section] Table 6 shows IndoBERT accuracy dropping to ~71-74% after stopword removal, compared to ~78-80% with stopwords retained.
  - [section] "Removing stopwords appears to degrade the contextual integrity of sentences, ultimately impairing the model's ability to understand and classify them effectively."
  - [corpus] No direct corpus evidence; this finding may be specific to Indonesian morphology and requires validation.
- Break condition: If stopwords introduce task-irrelevant noise in other Indonesian NLP tasks (e.g., topic classification), this pattern may not generalize.

## Foundational Learning

- Concept: **Transformer fine-tuning for classification**
  - Why needed here: The entire methodology relies on fine-tuning pre-trained IndoBERT/DistilBERT models on the PRDECT-ID dataset. Understanding how tokenization, attention mechanisms, and classification heads interact is essential.
  - Quick check question: Can you explain why a [CLS] token representation is used for sentence-level classification in BERT-style models?

- Concept: **Data augmentation for low-resource NLP**
  - Why needed here: The paper's key intervention is augmentation (back-translation, synonym replacement) to address class imbalance and limited data. Without this foundation, the 10% accuracy gain is opaque.
  - Quick check question: What is the risk of label preservation failure when applying back-translation to emotionally charged text?

- Concept: **Overfitting detection and regularization**
  - Why needed here: The paper explicitly notes overfitting concerns (validation loss increasing with epochs) and uses weight decay, dropout, and early stopping as mitigations.
  - Quick check question: If validation loss increases while accuracy improves, what does this indicate about model generalization?

## Architecture Onboarding

- Component map:
Raw text (PRDECT-ID reviews) → Preprocessing (alphabet filtering, lowercase; stopwords KEPT) → Data augmentation (back-translation: ID→EN→ID, ID→AR→ID; synonym replacement) → Tokenization (IndoBERT/DistilBERT tokenizer) → Model fine-tuning (pre-trained transformer + classification head) → Evaluation (accuracy, F1, precision, recall) → Optional: Bagging ensemble (multiple IndoBERT models on resampled subsets)

- Critical path:
  1. **Data balancing** is the highest-impact step—without augmentation, models plateau at ~68%.
  2. **Stopword retention** is non-negotiable for this task; removal causes ~6-8% accuracy drop.
  3. **Hyperparameter tuning** (especially weight decay and epochs) provides the final 1-2% gain to reach 80%.

- Design tradeoffs:
  - IndoBERT vs. DistilBERT: Accuracy vs. inference speed. IndoBERT wins on accuracy (~80% vs. ~75%); DistilBERT may be preferred for real-time applications.
  - Bagging ensemble: Marginal accuracy improvement (~0.2%) with 5x-10x computational cost. The paper found it "not successful to achieve higher performance" significantly.
  - Epoch count: More epochs improve accuracy but increase validation loss (overfitting risk). The optimal range appears to be 10 epochs with weight decay regularization.

- Failure signatures:
  - **Validation loss climbing while accuracy improves**: Classic overfitting signal. The paper shows this in Figures 4-5. Mitigation: Add weight decay, reduce epochs, or increase dropout.
  - **Performance drop after stopword removal**: Indicates context loss. Do NOT apply standard English NLP preprocessing blindly to Indonesian text.
  - **Ensemble not improving over single model**: Suggests models are not sufficiently diverse. Bagging only helps when base models have uncorrelated errors.

- First 3 experiments:
  1. **Baseline replication**: Fine-tune IndoBERT on PRDECT-ID without augmentation. Target: ~67-68% accuracy. Confirms data quality and tokenization pipeline.
  2. **Augmentation ablation**: Add back-translation and synonym replacement. Target: ~77-79% accuracy. Validates the paper's core claim.
  3. **Stopword removal test**: Repeat experiment 2 with stopword removal. Target: ~71-74% accuracy. Confirms contextual integrity hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the persistent overfitting observed in fine-tuned IndoBERT models on the PRDECT-ID dataset be effectively mitigated?
- Basis in paper: [explicit] The conclusion states, "Future works should focus on fixing the overfitting issues."
- Why unresolved: Although hyperparameter tuning (weight decay, dropout) achieved 80% accuracy, the analysis notes that validation loss increased alongside accuracy, indicating the model is "overfitting to the training data."
- What evidence would resolve it: A modified training regime or regularization technique that achieves competitive accuracy (>78%) while maintaining a stable or decreasing validation loss curve.

### Open Question 2
- Question: Which alternative deep learning architectures can outperform IndoBERT on low-resource Indonesian datasets without succumbing to overfitting?
- Basis in paper: [explicit] The conclusion advises to "experiment with different model that can excel more on lower size of dataset while not have high overfitting."
- Why unresolved: The study established IndoBERT's superiority over DistilBERT, but both struggled with generalization errors on the 5,400-sample dataset, suggesting current transformer models may be too parameter-heavy.
- What evidence would resolve it: Benchmarking diverse architectures (e.g., lighter RNNs or efficient transformers) that achieve higher accuracy with lower generalization error on the same augmented dataset.

### Open Question 3
- Question: Can alternative ensemble configurations yield significant performance gains beyond the marginal improvements observed with bagging?
- Basis in paper: [explicit] The authors suggest "using different combination of model and configuration to get better results."
- Why unresolved: The bagging approach failed to significantly outperform single models (79.77% vs 80%), likely because combining a strong model (IndoBERT) with a weaker one (DistilBERT) diluted the ensemble's predictive power.
- What evidence would resolve it: A stacking or voting ensemble utilizing multiple high-performing IndoBERT variants or distinct architectures that surpasses the 80% single-model accuracy threshold.

## Limitations

- The specific augmentation implementation details (tools/APIs for back-translation and synonym replacement) are not disclosed, affecting reproducibility
- Bagging ensemble approach provided only marginal accuracy improvements (0.2% gain) despite 5-10x computational cost
- Findings regarding stopword retention are based on Indonesian-specific preprocessing and may not generalize to other languages or tasks

## Confidence

- **High confidence:** IndoBERT outperforms DistilBERT for Indonesian emotion classification (80% vs. 75% accuracy); data augmentation significantly improves performance (10% gain)
- **Medium confidence:** Retaining stopwords is critical for contextual integrity in Indonesian emotion classification; overfitting occurs with more than 10 epochs
- **Low confidence:** Ensemble bagging provides meaningful accuracy improvements; augmentation methods preserve emotional semantics without drift

## Next Checks

1. **Reproduce baseline accuracy without augmentation:** Fine-tune IndoBERT on PRDECT-ID using the exact preprocessing pipeline (lowercase, alphabet filtering, stopword retention) to confirm the ~68% baseline
2. **Validate stopword retention impact:** Train two models—one with stopwords retained, one with stopwords removed—and measure accuracy drop to confirm the ~6-8% degradation
3. **Test augmentation semantic preservation:** Sample 50 augmented examples via back-translation and synonym replacement, manually verify if emotional labels remain consistent with originals