---
ver: rpa2
title: Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts
arxiv_id: '2506.22343'
source_url: https://arxiv.org/abs/2506.22343
tags:
- proportion
- 'true'
- watermark
- where
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of estimating the proportion of
  watermarked text in mixed-source documents that combine human-written and LLM-generated
  content. The authors formulate this as a mixture model estimation problem using
  pivotal statistics, which quantify evidence of watermark presence at each token
  level.
---

# Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts

## Quick Facts
- arXiv ID: 2506.22343
- Source URL: https://arxiv.org/abs/2506.22343
- Reference count: 40
- This paper addresses the problem of estimating the proportion of watermarked text in mixed-source documents that combine human-written and LLM-generated content.

## Executive Summary
This paper addresses the problem of estimating the proportion of watermarked text in mixed-source documents that combine human-written and LLM-generated content. The authors formulate this as a mixture model estimation problem using pivotal statistics, which quantify evidence of watermark presence at each token level. They prove that for green-red list watermarks (with binary statistics), the proportion is not identifiable and standard estimators suffer from inherent bias. However, for watermarks with continuous pivotal statistics like Gumbel-max and inverse transform methods, they establish identifiability under mild conditions. The authors propose three estimators: an initial simple estimator, a refined estimator incorporating additional watermarked data, and an optimal estimator using a theoretically derived weight function. Experiments on synthetic data and open-source LLMs (OPT-1.3B, OPT-13B, LLaMA3.1-8B) demonstrate that their optimal estimator consistently achieves the lowest mean absolute errors (often 10-30× better than baselines), with strong robustness to post-processing modifications like substitution, deletion, and insertion.

## Method Summary
The authors develop a mixture model approach where each token's pivotal statistic follows either a null distribution (human-written) or an alternative distribution (watermarked). For continuous pivotal statistics, they establish identifiability of the watermark proportion ε through the mixture CDF equation. They propose three estimators: (1) an initial estimator using empirical CDF ratios with threshold δ, (2) a refined estimator that incorporates external watermarked data to estimate the density ratio between null and alternative distributions, and (3) an optimal estimator using a theoretically derived weight function that minimizes variance. The optimal estimator solves a fixed-point equation using pre-computed density ratios from reference watermarked corpora. The method leverages the analytically-known null distribution under the watermarking scheme to separate the mixture components.

## Key Results
- Proved that watermark proportion ε is identifiable for continuous pivotal statistics (Gumbel-max, inverse transform) but not for binary statistics (green-red list)
- Proposed three estimators with the optimal estimator achieving minimax-optimal performance
- Demonstrated 10-30× lower mean absolute errors compared to baselines across multiple open-source LLMs
- Showed strong robustness to post-processing modifications including substitution, deletion, and insertion

## Why This Works (Mechanism)

### Mechanism 1: Identifiability via Continuous Pivotal Statistics
- **Claim**: Watermark proportion ε becomes identifiable when pivotal statistics are continuous but not for binary statistics
- **Mechanism**: Continuous pivotal statistics satisfy lim(x→0) F̄_P(x)/F₀(x) = 0, enabling unique recovery of ε from the mixture CDF. Binary statistics yield multiple (ε, μ) combinations producing identical distributions.
- **Core assumption**: NTP distributions are non-singular; alternative CDF differs uniformly from null CDF
- **Evidence anchors**: Formal proof in [section 3.2, Lemma 3.2]; abstract states identifiability for continuous statistics
- **Break condition**: Pivotal statistics become discrete/binary, or NTP distributions become singular

### Mechanism 2: Mixture Model Separation via Known Null Distribution
- **Claim**: The analytically-known null distribution F₀ enables decomposition of observed mixture into human and AI-generated components
- **Mechanism**: Each pivotal statistic follows Y_t|P_t ∼ (1-ε)μ₀ + εμ_{1,P_t}. Since μ₀ is known, the mixture equation can be solved for ε by exploiting that F̄_P(x)/F₀(x) → 0 as x → 0.
- **Core assumption**: Assumption 3.1 holds; verifier has access to watermark scheme parameters
- **Evidence anchors**: Formal mixture model in [section 3.1, Model 1]; core equation in [section 3.3, Equation 2]
- **Break condition**: Null distribution unknown/mis-specified; NTP distributions vary too rapidly

### Mechanism 3: Variance-Minimizing Optimal Weight Function
- **Claim**: The derived weight function v_opt(x) = (1-g(x))/((1-ε)+εg(x)) achieves minimax-optimal estimation
- **Mechanism**: By Cauchy-Schwarz inequality, this weight minimizes estimator variance, achieving variance τ*ₙ ≤ σ*ₙ (optimal beats indicator-based refined estimator).
- **Core assumption**: Density ratio g(x) accurately estimated; D_TV(F₀, bF_P) > 0; moment condition E_F₀[g]⁻² < ∞
- **Evidence anchors**: Formal derivation in [section 3.3, Lemma 3.4]; error bounds in [section 4.2, Theorem 4.2]
- **Break condition**: Density ratio estimation fails; moment conditions violated; insufficient separation between F₀ and F̄_P

## Foundational Learning

- **Concept**: Mixture model identifiability
  - **Why needed here**: Central theoretical contribution is determining when ε is estimable; must understand why binary statistics fail vs. continuous statistics succeed
  - **Quick check question**: Given i.i.d. samples from (1-ε)Ber(γ) + εBer(μ) with both ε and μ unknown, can you uniquely recover ε? What if samples were from a continuous mixture?

- **Concept**: Pivotal statistics in watermark detection
  - **Why needed here**: Entire framework builds on Y_t following μ₀ under null (human) and μ_{1,P} under alternative (watermarked); must distinguish watermark schemes by their pivotal statistic distributions
  - **Quick check question**: For Gumbel-max watermark, what is Y_t's distribution when token is human-written vs. LLM-generated? How does this differ from green-red list?

- **Concept**: Minimax lower bounds (Le Cam's method)
  - **Why needed here**: Theoretical optimality established via minimax bounds; understanding τ*ₙ/√n and σ*ₙ/√n as fundamental limits
  - **Quick check question**: What does inf_{ε̂} sup_{ε₀∈B(ε)} E|ε̂-ε₀| ≥ τ*ₙ/(√2n) imply about achievable estimation accuracy?

## Architecture Onboarding

- **Component map**:
  1. **Pivotal statistic extractor**: Computes Y_t = Y(w_t, ζ_t) per watermark scheme
  2. **Null distribution F₀**: Uniform(0,1) after probability integral transform; analytically known
  3. **Density ratio estimator**: Histogram (N=500 bins) or KDE computing b_g(x) = dbF_P(x)/dF₀(x)
  4. **Three-estimator pipeline**: ε_ini (Equation 4), ε_rfn (Equation 5), ε_opt (fixed-point Equation 8)
  5. **Fixed-point solver**: L-BFGS-B optimization minimizing |ε - bT(ε)|

- **Critical path**:
  1. Collect watermarked-only pivotal statistics from comparable LLM → estimate b_g(x), bF_P(x) offline
  2. Extract pivotal statistics Y_{1:n} from query document → compute empirical CDF bF(x)
  3. Solve fixed-point equation ε = P(bT(ε)) using pre-computed b_g → output ε_opt

- **Design tradeoffs**:
  1. **δ threshold (ε_ini/ε_rfn)**: Smaller δ reduces bias but increases variance; paper uses {10⁻¹, 10⁻², 10⁻³}
  2. **ε_min stability constant**: Larger values prevent numerical instability but truncate extreme estimates; paper uses 10⁻³ or log(n)/√n
  3. **Density ratio method**: Histogram scales to n=10⁶ (O(n) time) but piecewise constant; KDE smoother but O(n²) prohibitive
  4. **Reference corpus source**: Similar LLM/task yields better b_g but may mismatch deployment

- **Failure signatures**:
  1. **ε estimates clustered at 0 or 1**: NTP distribution mismatch between reference and target; insufficient watermarked reference data
  2. **High variance across runs**: Density ratio b_g unstable (histogram bins too sparse); check sample count per bin
  3. **Green-red list watermark failure**: Binary Y_t causes non-identifiability; all estimators return 0 or 1 regardless of true ε
  4. **Fixed-point divergence**: ε_min too small or b_g(x) near zero causing denominator blowup

- **First 3 experiments**:
  1. **Synthetic validation**: Generate NTP distributions via Algorithm 1 with Δ ∈ {0.1, 0.3, 0.5, 0.7}; verify ε_opt MAE < ε_rfn < ε_ini across 200 ε values
  2. **Density ratio ablation**: Compare histogram (N=500) vs. KDE vs. parametric exponential fit for b_g estimation; measure MAE degradation
  3. **Post-processing robustness**: Apply random substitution/deletion/insertion to fully watermarked text at proportions ε ∈ {0.1, 0.3, 0.5, 0.7}; verify ε_opt tracks modified ground truth

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can watermark proportion estimation be extended to high-dimensional pivotal statistics, such as those used in watermarking schemes that require neural network-based detection?
- **Basis in paper**: [explicit] The discussion section explicitly states: "while our paper focuses on scalar pivotal statistics, some watermarking schemes, such as [8], use high-dimensional pivotal statistics. It even requires training neural networks for detection due to their complexity. Estimating the watermark proportion in such high-dimensional settings remains an open challenge."
- **Why unresolved**: The theoretical framework relies on one-dimensional CDFs and density ratios, which do not directly generalize to high-dimensional spaces where distributions may not have tractable forms.
- **What evidence would resolve it**: Derivation of analogous identifiability conditions and efficient estimators for multivariate pivotal statistics, with convergence guarantees comparable to Theorem 4.2.

### Open Question 2
- **Question**: What are the theoretical limits (including phase transitions) for precisely identifying AI-generated segments within mixed-source texts?
- **Basis in paper**: [explicit] The authors state: "Another direction is to investigate the theoretical limits of identifying AI-generated segments, including possible phase transitions, similar to classic work [10, 5]."
- **Why unresolved**: Segment identification is inherently harder than aggregate proportion estimation—the paper notes that "signals can be statistically detectable even when it is information-theoretically impossible to locate each instance accurately."
- **What evidence would resolve it**: Characterization of conditions (e.g., minimum segment length, signal strength) under which segment boundaries can be consistently estimated, analogous to change-point detection theory.

### Open Question 3
- **Question**: How can token semantics or entropies be incorporated to enable importance-weighted proportion estimation in mixed-source texts?
- **Basis in paper**: [explicit] The discussion notes: "This work measures the extent of AI-generated content through pivotal statistics but does not account for its importance within mixed-source texts. For instance, 10% AI-generated content might carry critical significance in some contexts but be less relevant in others."
- **Why unresolved**: The current formulation estimates a uniform proportion ε without distinguishing tokens by semantic importance or information content.
- **What evidence would resolve it**: A modified estimator that weights pivotal statistics by token-level importance metrics (e.g., entropy, semantic role) and theoretical analysis showing improved utility for downstream applications.

### Open Question 4
- **Question**: Can the non-identifiability of proportion parameters for binary pivotal statistics (e.g., green-red list watermarks) be overcome through alternative modeling assumptions or auxiliary information?
- **Basis in paper**: [inferred] Lemma 3.1 and Corollary 1 establish that ε is not identifiable for green-red list watermarks when NTP distributions are unknown. Theorem 4.5 demonstrates inherent MLE bias that persists even with large samples.
- **Why unresolved**: Binary statistics provide insufficient information to disentangle ε from the unknown NTP-dependent parameter μ̄t, and no estimator can uniquely recover ε from observed data alone.
- **What evidence would resolve it**: Identification of additional observable quantities (e.g., partial NTP information, multi-sample scenarios) that restore identifiability, or proof that non-identifiability is fundamental regardless of auxiliary information.

## Limitations
- The theoretical analysis relies on specific differentiability and limit conditions (Assumption 3.1) that may not hold for all watermarking schemes
- Minimax lower bounds assume access to true watermarking parameters and NTP distributions, but real-world deployment would require estimating these from data
- Experiments are limited to open-source models and synthetic data, not testing on proprietary models like GPT-4 or Claude
- Density ratio estimation using 500-bin histograms may not scale well to very large vocabularies or may miss important structure in tail regions
- Fixed-point iteration for optimal estimator could be computationally expensive for very long documents

## Confidence
- **High Confidence**: The fundamental identifiability result for continuous pivotal statistics (Lemma 3.2) - the mathematical proof is rigorous and the intuition about binary vs continuous statistics is sound. The minimax optimality of the optimal estimator (Theorem 4.2) - the variance minimization derivation is clear and the error bounds are proven.
- **Medium Confidence**: The practical performance claims - while experiments are thorough, they are limited to open-source models and synthetic data. The robustness to post-processing modifications is demonstrated but only for specific types and levels of modification. The density ratio estimation method using histograms is practical but may not be optimal for all scenarios.
- **Low Confidence**: The claims about green-red list watermark non-identifiability being fundamental rather than methodological - while Lemma 3.1 proves this mathematically, there may be alternative approaches not considered. The assumption that reference watermarked data can be easily obtained from similar LLMs is practical but not theoretically justified.

## Next Checks
1. **Cross-model transfer validation**: Generate watermarked data from OPT-1.3B, then test the estimators on LLaMA3.1-8B without retraining the density ratio estimator. This would validate whether the method truly generalizes across model architectures or is overfitting to specific NTP distributions.

2. **Continuous pivotal statistic robustness**: Systematically vary the Gumbel-max parameter α and measure how estimation accuracy degrades as the NTP distributions become closer to the null distribution. This would test the theoretical assumption that D_TV(F₀, F_P) > 0 is necessary for identifiability.

3. **Real-world mixed-source test**: Create hybrid documents by combining actual human-written text (from ArXiv or C4) with LLM-generated text, varying the watermark proportion ε. This would validate whether the synthetic mixture model assumptions hold for real-world data distributions.