---
ver: rpa2
title: 'CoheMark: A Novel Sentence-Level Watermark for Enhanced Text Quality'
arxiv_id: '2504.17309'
source_url: https://arxiv.org/abs/2504.17309
tags:
- text
- sentence
- cohemark
- watermarking
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CoheMark, a novel sentence-level watermarking
  technique designed to improve text quality in AI-generated content. Traditional
  sentence-level watermarking methods often degrade text quality by randomly selecting
  valid sentences, disrupting semantic coherence.
---

# CoheMark: A Novel Sentence-Level Watermark for Enhanced Text Quality

## Quick Facts
- arXiv ID: 2504.17309
- Source URL: https://arxiv.org/abs/2504.17309
- Reference count: 18
- Primary result: Sentence-level watermarking that preserves text quality via semantic coherence clustering, achieving over 97% TPR@1% while outperforming baseline methods in perplexity and robustness against paraphrasing.

## Executive Summary
CoheMark introduces a novel sentence-level watermarking technique that embeds watermarks into AI-generated text while preserving semantic coherence and text quality. Unlike traditional sentence-level methods that degrade quality by randomly selecting valid sentences, CoheMark leverages fuzzy c-means clustering on sentence embeddings to select semantically coherent sentences for watermark embedding. The method uses a three-module architecture: an Embedder (Sentence-BERT), a FuzzyClusterer (8 clusters), and a CoheSampler (rejection sampling with NSSCv1/NSSCv2 rules and a Switching Rule). Experiments on OpenGen and LFQA datasets demonstrate that CoheMark achieves strong watermark detection (TPR@1% > 97%) while significantly outperforming baselines in text quality metrics and robustness against paraphrasing attacks.

## Method Summary
CoheMark operates through a three-module pipeline. First, the Embedder uses Sentence-BERT to convert sentences into embeddings. Second, the FuzzyClusterer applies fuzzy c-means clustering with 8 clusters to these embeddings, trained on domain-specific sentences. Third, the CoheSampler performs rejection sampling using Next Sentence Selection Criteria (NSSC) rules: NSSCv1 accepts sentences if their Primary Membership Cluster is 1st or 3rd in the prior Membership Index, NSSCv2 accepts if 2nd/4th/5th/6th, and a Switching Rule toggles between criteria after 5 cumulative Primary Membership matches. Generation uses temperature 0.9 and repetition penalty 1.05. Detection computes the watermark ratio r = SV/ST using the same Embedder and clusters. The method aims to balance watermark detectability with text quality preservation through semantic coherence.

## Key Results
- Achieves TPR@1% over 97% on OpenGen and LFQA datasets
- Outperforms baseline methods in text quality as measured by perplexity, BertScore, and LLM-based pairwise evaluations
- Demonstrates higher robustness against paraphrasing attacks compared to other methods
- Significantly reduces quality degradation compared to random sentence selection watermarking approaches

## Why This Works (Mechanism)
CoheMark works by replacing random sentence selection in watermarking with semantically coherent sentence selection through clustering. By training fuzzy c-means on domain sentence embeddings, the method identifies natural sentence groupings that preserve topic coherence. The NSSC rules ensure selected watermark sentences maintain narrative flow by considering cluster membership patterns across consecutive sentences. The Switching Rule provides flexibility to pivot topics while maintaining overall coherence. This approach embeds watermarks into text without the quality degradation typical of random selection methods, as detected sentences remain contextually appropriate within the generated passage.

## Foundational Learning
- Sentence-BERT embeddings: Vector representations capturing semantic meaning of sentences; needed to measure semantic similarity for clustering; quick check: verify embeddings cluster similar sentences together using t-SNE visualization.
- Fuzzy c-means clustering: Soft clustering algorithm allowing sentences to belong to multiple clusters with membership degrees; needed to capture semantic ambiguity and gradual topic transitions; quick check: confirm membership distributions show smooth transitions between topics.
- Rejection sampling with NSSC: Selective generation approach that only accepts sentences meeting coherence criteria; needed to enforce semantic constraints during generation; quick check: measure acceptance rate and generation latency compared to standard sampling.

## Architecture Onboarding
- Component map: Embedder -> FuzzyClusterer -> CoheSampler -> LLM Generator
- Critical path: Sentence generation requires embedding computation, cluster membership evaluation, NSSC rule checking, and potential rejection loops
- Design tradeoffs: Higher TPR and quality achieved through semantic clustering versus computational overhead from rejection sampling and clustering requirements
- Failure signatures: Infinite loops in generation if acceptance criteria too strict; low TPR if clusters poorly aligned with test domain; quality degradation if Switching Rule triggers too frequently
- First experiments: 1) Train fuzzy c-means on 10,000 domain sentences and visualize cluster assignments; 2) Generate text with CoheSampler and measure acceptance rate per sentence; 3) Compare TPR@1% and perplexity against random sentence selection baseline

## Open Questions the Paper Calls Out
- Can the generation latency of CoheMark be reduced to approach the speed of token-level watermarks without sacrificing text quality? The current sentence-level rejection sampling creates a computational bottleneck, making CoheMark approximately seven times slower than token-level methods.
- What is the optimal configuration for the Next Sentence Selection Criteria (NSSC) and the Switching Rule? The current rules were chosen heuristically and may not be mathematically optimal for balancing coherence and watermark detectability.
- How does CoheMark compare to the k-SemStamp baseline on the C4 and BookSum datasets? The comparison was omitted due to lack of clustering outcomes from k-SemStamp authors for the datasets used in this paper.

## Limitations
- Generation speed is approximately seven times slower than token-level watermarking methods due to sentence-level rejection sampling
- Several critical hyperparameters (Sentence-BERT variant, fuzzy c-means settings, rejection limits) are underspecified, affecting reproducibility
- Current robustness claims based only on text simplification attacks, not comprehensive attack surface evaluation
- Scalability to longer documents and more complex domains has not been demonstrated

## Confidence
- High confidence: Watermark detectability via cluster-based sentence selection is technically sound; TPR@1% > 97% is achievable with proper clustering
- Medium confidence: Quality preservation relative to baselines is demonstrated, but dependent on exact cluster training and sampling hyperparameters
- Low confidence: Robustness to paraphrasing attacks generalizes beyond the single simplification attack tested; scalability to longer documents is unproven

## Next Checks
1. Replicate TPR@1% and PPL/BertScore metrics on OpenGen and LFQA using the same base models and training sentences; verify detection on paraphrased outputs
2. Compare CoheMark to SimMark and DERMark using identical datasets, base models, and evaluation protocols to isolate quality and robustness differences
3. Test CoheMark with multiple Sentence-BERT variants and clustering configurations to quantify sensitivity to embedding and hyperparameter choices