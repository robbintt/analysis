---
ver: rpa2
title: 'Proxy-FDA: Proxy-based Feature Distribution Alignment for Fine-tuning Vision
  Foundation Models without Forgetting'
arxiv_id: '2505.24088'
source_url: https://arxiv.org/abs/2505.24088
tags:
- proxy-fda
- feature
- fine-tuning
- forgetting
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Proxy-FDA, a novel regularization method
  that addresses concept forgetting in vision foundation model fine-tuning. The method
  performs Feature Distribution Alignment (FDA) by matching nearest neighbor graphs
  between pre-trained and fine-tuned feature spaces, with an enhanced version called
  Proxy-FDA that generates synthetic proxy features to increase data diversity.
---

# Proxy-FDA: Proxy-based Feature Distribution Alignment for Fine-tuning Vision Foundation Models without Forgetting

## Quick Facts
- arXiv ID: 2505.24088
- Source URL: https://arxiv.org/abs/2505.24088
- Reference count: 40
- Primary result: Proxy-FDA achieves average ΔLP of 1.50, significantly outperforming LP-FT (-2.59) and LDIFS (0.29) in preserving pre-trained concepts during fine-tuning

## Executive Summary
Proxy-FDA addresses concept forgetting in vision foundation model fine-tuning through structure-aware feature distribution alignment. The method transfers nearest neighbor graphs from pre-trained to fine-tuned feature spaces using a noise-resistant Sigmoid loss, preserving local neighborhood topology. An enhanced version generates synthetic proxy features via a lightweight attention-based generator, increasing data diversity and improving alignment quality. Extensive experiments across 10 datasets with CLIP demonstrate superior performance in preventing forgetting while maintaining downstream task accuracy.

## Method Summary
Proxy-FDA performs Feature Distribution Alignment (FDA) by matching nearest neighbor graphs between pre-trained and fine-tuned feature spaces. The core FDA loss transfers kNN relationships using weighted similarities with a Sigmoid loss. Proxy-FDA enhances this by generating synthetic proxy features through a lightweight generator (attention + conv layers, 23.6k parameters) that creates convex combinations of true features. These proxies are constrained to their respective manifolds and regularized for diversity. The method uses hard class mining for batch construction, ensuring diverse-yet-related class distributions within each batch to capture meaningful cross-class attribute sharing.

## Key Results
- Proxy-FDA achieves average ΔLP of 1.50 compared to 0.29 for LDIFS and -2.59 for LP-FT, indicating superior concept preservation
- Proxy-FDA outperforms FDA in all 10 tested datasets, demonstrating the effectiveness of synthetic proxy generation
- Strong performance in few-shot (average harmonic mean AH), continual, and cross-task fine-tuning settings
- Proxy diversity metric of 3.14×10^-2 versus 2.89×10^-2 for random interpolation baselines

## Why This Works (Mechanism)

### Mechanism 1
Structure-wise feature alignment reduces concept forgetting more effectively than point-wise matching by transferring nearest neighbor graphs (indices + similarities) from pre-trained to fine-tuned feature space using a noise-resistant Sigmoid loss, preserving local neighborhood topology across the distribution manifold rather than enforcing rigid point-to-point correspondence.

### Mechanism 2
Dynamically generated proxies improve FDA quality by synthesizing informative unseen data points that regularize neighborhood boundaries. A lightweight proxy generator produces convex combinations of true features via learned pooling weights, generating positive/negative proxies constrained to their respective manifolds while maximizing diversity through a variance loss.

### Mechanism 3
Hard class mining enables meaningful kNN graph construction within mini-batches for effective structure transfer. Batch construction greedily selects classes with highest FDA loss relative to already-selected classes, ensuring local neighborhoods contain semantically similar but distinct class concepts.

## Foundational Learning

- **Concept: Nearest Neighbor Graphs in Feature Space**
  - Why needed here: FDA's core operation transfers kNN relationships; understanding how similarity graphs encode manifold structure is essential
  - Quick check question: Given a batch of features X ∈ R^(d×B), can you compute the K-nearest neighbors for each point and explain what structural properties this reveals?

- **Concept: Optimal Transport / Wasserstein Distance**
  - Why needed here: OTDD metric validates FDA quality; OT naturally handles distribution alignment with geometric structure awareness
  - Quick check question: Why does Wasserstein distance capture distributional structure better than KL divergence when supports don't overlap?

- **Concept: Sigmoid Loss for Contrastive Learning**
  - Why needed here: FDA uses Sigmoid loss (not Softmax) to handle variable positive/negative counts per sample without global normalization
  - Quick check question: How does Sigmoid loss differ from InfoNCE when handling an imbalanced ratio of positives to negatives?

## Architecture Onboarding

- **Component map:** Input batch → Pre-trained encoder → Fine-tuned encoder → kNN graph construction → Hard class mining → Proxy generation → FDA loss → Combined with L_task

- **Critical path:** Hard class mining → kNN indices + similarities → proxy generation → FDA loss computation. Errors in batch construction propagate through entire pipeline.

- **Design tradeoffs:**
  - Batch size B vs. GPU memory: B < 64 hurts performance; B ≥ 64 is robust but requires memory
  - K selection: Fixed K=2n works across datasets; per-dataset tuning adds ~1-2% AH
  - Proxy scale s: s=0.4 balances diversity vs. compute overhead
  - Assumption: Hard mining adds ~10% training time overhead but is non-optional

- **Failure signatures:**
  - ΔLP remains negative despite training: Check K > n constraint, verify hard mining is active
  - Proxies collapse to duplicates: Monitor variance loss term; increase α if diversity metric < 0.02
  - OTDD increases while L2 decreases: This is expected—structure-wise alignment may increase point distances
  - Training instability: τ and b initialization per Zhai et al. 2023 is critical; defaults rarely need adjustment

- **First 3 experiments:**
  1. **Sanity check:** Run FDA-only (no proxies) on CIFAR-10 with B=64, m=16, n=4, K=8. Verify ΔLP > 0 on held-out datasets (e.g., CIFAR-100). Expected: +1.0 to +1.5 ΔLP.
  2. **Proxy validation:** Add proxy generator with s=0.4. Compare proxy diversity metric (Table 4) against random interpolation baseline. Expected: 3.14 vs 2.89 (×10^-2).
  3. **Ablation sequence:** Test (a) without neighbor similarities, (b) without hard mining, (c) without attention in proxy generator. Quantify AH drop on base-to-new setting. Expected drops: ~2%, ~3%, ~1% respectively.

## Open Questions the Paper Calls Out

### Open Question 1
Can sampling suitable external data for Feature Distribution Alignment (FDA) outperform or complement the current proxy-based generation approach? The paper notes that using external data suffers from costs and distributional shifts, stating this should be left to future work.

### Open Question 2
Can the optimal neighborhood size (K) be determined adaptively based on feature density rather than manual tuning? The paper acknowledges K must be varied as a function of dataset distribution and is currently picked per dataset.

### Open Question 3
Does structure-wise feature alignment in Proxy-FDA inadvertently amplify or mitigate inherited social biases compared to point-wise regularization? The Impact Statement warns that when pre-trained concepts reflect unintentional biases, the regularization method could inherit or amplify those biases.

## Limitations
- Dependence on tuned hyperparameters (K, λ, proxy scale s) that may require dataset-specific adjustment
- Potential inheritance and amplification of social biases present in pre-trained models
- Additional computational overhead from proxy generation and hard mining procedures

## Confidence

### High Confidence Claims
- Proxy-FDA significantly outperforms baseline regularization methods (LP-FT, LDIFS) in preserving pre-trained concepts across diverse datasets
- Structure-wise alignment via FDA is more effective than point-wise matching for preventing concept forgetting
- Synthetic proxy generation provides meaningful diversity improvements over random interpolation baselines

### Medium Confidence Claims
- The specific architectural choices for proxy generator (attention + conv layers) are optimal for this task
- Hard class mining is necessary and optimal for batch construction in FDA
- The current proxy generation approach is superior to all potential external data sampling strategies

### Low Confidence Claims
- The method's behavior on fairness metrics and bias propagation remains untested
- Dynamic K selection based on feature density would perform comparably to manual tuning
- The approach generalizes equally well to non-CLIP vision models beyond DINOv2

## Next Checks

1. **Reproduce core FDA behavior:** Implement FDA loss (Eq. 2) and verify ΔLP > 0 on CIFAR-10 → CIFAR-100 transfer with B=64, m=16, n=4, K=8

2. **Validate proxy generator diversity:** Implement proxy generator (Fig. 6) and measure diversity metric (Table 4: should be ~3.14×10^-2) compared to random interpolation baseline (2.89×10^-2)

3. **Test hard mining necessity:** Run ablation without hard mining and measure AH drop on base-to-new setting; expect ~3% degradation from ~74% to ~71% AH