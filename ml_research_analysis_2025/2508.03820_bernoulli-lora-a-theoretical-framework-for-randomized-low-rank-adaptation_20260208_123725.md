---
ver: rpa2
title: 'Bernoulli-LoRA: A Theoretical Framework for Randomized Low-Rank Adaptation'
arxiv_id: '2508.03820'
source_url: https://arxiv.org/abs/2508.03820
tags:
- where
- convergence
- gradient
- optimization
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Bernoulli-LoRA, a novel theoretical framework
  that extends Low-Rank Adaptation (LoRA) methods through a probabilistic Bernoulli
  mechanism for selecting which matrix to update during fine-tuning. The framework
  unifies and generalizes existing LoRA approaches while maintaining theoretical tractability.
---

# Bernoulli-LoRA: A Theoretical Framework for Randomized Low-Rank Adaptation

## Quick Facts
- arXiv ID: 2508.03820
- Source URL: https://arxiv.org/abs/2508.03820
- Authors: Igor Sokolov; Abdurakhmon Sadiev; Yury Demidovich; Fawaz S Al-Qahtani; Peter Richtárik
- Reference count: 40
- Primary result: Introduces Bernoulli-LoRA framework with probabilistic selection between LoRA matrices, achieving convergence guarantees for non-convex optimization with variance reduction and federated learning extensions.

## Executive Summary
This paper introduces Bernoulli-LoRA, a novel theoretical framework that extends Low-Rank Adaptation (LoRA) methods through a probabilistic Bernoulli mechanism for selecting which matrix to update during fine-tuning. The framework unifies and generalizes existing LoRA approaches while maintaining theoretical tractability. Under standard non-convex optimization assumptions, the authors analyze several algorithmic variants including Bernoulli-LoRA-GD, Bernoulli-LoRA-SGD, Bernoulli-LoRA-PAGE, and Bernoulli-LoRA-MVR, establishing convergence guarantees for each. The framework is extended to federated learning with methods like Fed-Bernoulli-LoRA-QGD, Fed-Bernoulli-LoRA-MARINA, and Fed-Bernoulli-LoRA-EF21. Additionally, the analysis is extended to convex non-smooth functions with both constant and adaptive Polyak-type stepsizes. Experiments validate the theoretical findings across various tasks including linear regression with non-convex regularization and MLP training on MNIST, demonstrating practical efficacy.

## Method Summary
The Bernoulli-LoRA framework randomly selects which low-rank matrix (A or B) to update at each iteration using a Bernoulli trial with probability p. The method reformulates LoRA updates as projected gradient steps to maintain smoothness properties needed for convergence analysis. The framework supports multiple base gradient estimators including full gradient, SGD, PAGE, and MVR, each with specific convergence guarantees. For federated learning, the framework extends to handle multiple clients with communication-efficient variants. The theoretical analysis covers both non-convex and convex non-smooth optimization settings with appropriate stepsize strategies.

## Key Results
- Establishes convergence guarantees for Bernoulli-LoRA variants (GD, SGD, PAGE, MVR) under non-convex optimization assumptions
- Achieves O(1/T) convergence rates for variance-reduced variants (PAGE, MVR) with projection eigenvalue factors
- Extends framework to federated learning with communication-efficient methods (Fed-Bernoulli-LoRA-QGD, MARINA, EF21)
- Demonstrates practical efficacy on linear regression and MNIST classification tasks with rank-1 LoRA adapters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Randomly selecting which low-rank matrix to update at each iteration maintains theoretical tractability while generalizing existing update strategies (alternating A-only, B-only, or both).
- **Mechanism**: At each step, a Bernoulli trial with probability p selects whether to update matrix A (keeping B fixed via sampling from D_B^S) or matrix B (keeping A fixed). This can be reformulated as a projected gradient step: W^{t+1} = W^t - γH^t G^t, where H^t is a random projection matrix determined by the Bernoulli outcome.
- **Core assumption**: Assumption 1 (Positive Expected Projection)—the smallest eigenvalue of E[H] must be strictly positive. For Gaussian-sampled sketches, E[H] = (r/n)I, so λ_min = r/n > 0.
- **Evidence anchors**:
  - [abstract]: "probabilistic Bernoulli mechanism for selecting which matrix to update"
  - [section 6.1]: Shows the unified update W^{t+1} = W^t - γĜ^t where Ĝ^t is the projected gradient estimator (Equation 7-8)
  - [corpus]: Related work on asymmetric LoRA initialization (arXiv:2506.14530) empirically observes inherent asymmetry in factor initialization, which Bernoulli-LoRA addresses probabilistically
- **Break condition**: If sketch distributions D_B^S, D_A^S yield E[H] with zero eigenvalues, convergence guarantees fail. Symptoms: optimization stagnates despite small stepsizes.

### Mechanism 2
- **Claim**: Reformulating LoRA updates as projected gradient steps circumvents the non-smoothness introduced by the LoRA reparameterization.
- **Mechanism**: Direct optimization of f(W_0 + BA) loses L-smoothness in {B,A}. By instead taking gradient steps in W-space and projecting onto low-rank subspaces via H^t, the framework preserves the smoothness properties needed for standard convergence analysis.
- **Core assumption**: Assumption 3 (L-smooth gradient)—the base function f must be L-smooth in W. Section 3 explicitly notes that LoRA reparameterization breaks this property.
- **Evidence anchors**:
  - [section 3]: "the LoRA re-parameterization inherently transforms a smooth Lipschitz loss into a non-smooth one"
  - [section 6.1]: "This reformulation reveals that both Left and Right sketch updates are equivalent to applying a standard gradient-based update, but projected onto a randomly chosen low-rank subspace"
  - [corpus]: Weak/missing—no corpus papers address this projected gradient equivalence
- **Break condition**: If f is not L-smooth, the descent lemma (Lemma 5) proof strategy fails. Verify smoothness before applying convergence bounds.

### Mechanism 3
- **Claim**: Variance-reduced estimators (PAGE, MVR) integrate with Bernoulli projection while preserving optimal non-convex convergence rates, up to projection-eigenvalue factors.
- **Mechanism**: Base gradient estimators G^t from PAGE (probabilistic full/mini-batch switching) or MVR (momentum-based VR) are combined with Bernoulli projection via Ĝ^t. Convergence is proven using Lyapunov functions Φ_t = f(W^t) - f* + correction terms tracking estimator error.
- **Core assumption**: For SGD: Assumption 4 (Expected Smoothness). For MVR/PAGE: Assumption 5 (Bounded Variance σ²). Stepsize bounds depend on both L and the eigenvalue ratio λ_p^max/λ_p^min.
- **Evidence anchors**:
  - [section 6.2, Table 2]: Lists base estimator definitions for each variant
  - [section 7, Table 1]: Convergence rates show PAGE and MVR achieve O(1/T) rates with additional λ_p^max/λ_p^min factors
  - [corpus]: Related LoRA literature does not address variance reduction for non-convex optimization
- **Break condition**: Poorly tuned hyperparameters (q for PAGE, b for MVR) relative to λ_p^max/λ_p^min can prevent Lyapunov decrease. Monitor gradient norm plateauing.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA) parameterization**
  - Why needed here: Bernoulli-LoRA extends standard LoRA (W = W_0 + (α/r)BA); you must understand why this reduces parameters from O(mn) to O(r(m+n)) and the role of initialization asymmetry.
  - Quick check question: Why does LoRA typically initialize B=0 and A=Gaussian, and how does this relate to ΔW=0 at initialization?

- **Concept: Projection matrices and expected eigenvalues**
  - Why needed here: Convergence rates scale with 1/λ_p^min; understanding why E[H] can have positive eigenvalues even though individual H matrices have eigenvalues in {0,1} is essential.
  - Quick check question: For Gaussian-sampled sketches, what is E[H] and why does this satisfy Assumption 1?

- **Concept: Variance reduction in stochastic optimization**
  - Why needed here: Advanced variants use PAGE and MVR to eliminate the noise floor that prevents SGD from converging to stationary points.
  - Quick check question: Why does PAGE achieve optimal O(1/ε²) oracle complexity for non-convex optimization, and what role does the probability q play?

## Architecture Onboarding

- **Component map**:
  - Pre-trained weights W_0 ∈ R^{m×n}: Fixed throughout
  - Low-rank factors A ∈ R^{r×n}, B ∈ R^{m×r}: One trainable per iteration
  - Bernoulli selector c_t ~ Be(p): Controls Left vs. Right sketch
  - Sketch matrices B_S^t ~ D_B^S, A_S^t ~ D_A^S: Sampled for the fixed matrix
  - Base gradient estimator G^t: Full gradient, SGD, PAGE, or MVR depending on variant
  - Projection matrices H_B^t, H_A^t: Defined via pseudoinverse (Equation 21)

- **Critical path**:
  1. Sample Bernoulli c_t to select update direction
  2. Sample sketch matrix for the fixed factor
  3. Compute gradient estimator G^t (depends on variant)
  4. Project: Ĝ^t = H_B^t G^t (if c_t=1) or G^t H_A^t (if c_t=0)
  5. Update: W^{t+1} = W^t - γĜ^t
  6. Update estimator state (for PAGE/MVR)
  7. Repeat for chain length T

- **Design tradeoffs**:
  - Rank r: Higher r increases λ_min = r/n (faster convergence) but more parameters
  - Probability p: Controls expected trainable parameters; experiments show p≈1 favoring B-updates worked best (Table 3)
  - Sketch distribution: Gaussian gives isotropic E[H]; structured distributions may yield different eigenvalue spectra
  - Chain length T: Longer chains accumulate higher effective rank but require more iterations

- **Failure signatures**:
  - Convergence plateau with non-zero gradient: VR hyperparameters (q, b) misconfigured or variance bound violated
  - Divergence with reasonable stepsizes: λ_p^min too small (try higher rank) or function not L-smooth
  - Performance gap vs. full fine-tuning: Chain length insufficient or rank too low

- **First 3 experiments**:
  1. Reproduce Bernoulli-LoRA-GD convergence on synthetic regression; verify gradient norm decreases as O(1/T) per Theorem 1
  2. Compare Bernoulli-LoRA-SGD vs. Bernoulli-LoRA-PAGE on stochastic setting; confirm PAGE breaks through SGD noise floor (Figure 1 pattern)
  3. Sweep probability p ∈ {0.1, 0.5, 0.9, 0.99} on MNIST transfer; verify asymmetric preference for one matrix direction (Table 3 shows p=0.99 optimal)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Why does a deterministic assignment of fixed and trainable matrices at initialization yield better performance than the theoretically prescribed probabilistic selection?
- **Basis in paper**: [explicit] Table 3 (footnote 1) notes that despite the framework prescribing probabilistic selection, "a deterministic assignment... yielded better performance" in their MNIST experiments.
- **Why unresolved**: The paper establishes the theoretical framework based on probabilistic updates but does not offer a theoretical explanation for why violating this randomness at initialization improves empirical results.
- **What evidence would resolve it**: An ablation study comparing optimization trajectories and convergence rates for deterministic vs. probabilistic initialization, or a theoretical extension analyzing the "burn-in" period.

### Open Question 2
- **Question**: Does the Positive Expected Projection (Assumption 1) hold for non-Gaussian sketch distributions, such as sparse or Rademacher matrices?
- **Basis in paper**: [inferred] Remark 1 and Lemma 2 prove Assumption 1 ($\lambda_{min}(E[H]) > 0$) specifically for i.i.d. Gaussian sampling, stating it is satisfied by "common practical choices" without verifying others.
- **Why unresolved**: The convergence proofs rely entirely on $\lambda_{min} > 0$, but the paper restricts the theoretical verification of this property to Gaussian distributions.
- **What evidence would resolve it**: A derivation of the expected eigenvalues for alternative sketch distributions, or an empirical analysis showing whether the convergence rates hold when using non-Gaussian sketches.

### Open Question 3
- **Question**: To what extent does the Polyak-Łojasiewicz (PŁ) condition actually hold for the loss landscapes of models fine-tuned with LoRA?
- **Basis in paper**: [inferred] Theorems 8, 9, and 12 derive linear convergence rates by assuming the PŁ condition (Assumption 6), a strong generalization of strong convexity.
- **Why unresolved**: While the theory relies on this condition for its strongest guarantees, the paper provides no empirical verification that modern neural network fine-tuning tasks satisfy this inequality locally.
- **What evidence would resolve it**: Empirical measurements of the gradient norm squared ($\|\nabla f(W)\|^2_F$) versus the function suboptimality gap ($f(W) - f^*$) during standard fine-tuning to validate the inequality.

## Limitations

- **Convex extension limitations**: While the paper claims convergence for convex non-smooth functions with Polyak stepsizes, the non-convex results are more thoroughly validated experimentally. The extension to federated learning with multiple clients adds theoretical complexity but lacks comprehensive empirical validation across diverse network conditions.

- **Sketch distribution sensitivity**: The theoretical analysis assumes Gaussian sketches for tractability, but real-world performance may vary with alternative sketching distributions. The paper doesn't systematically explore how different sketch distributions affect convergence rates or practical performance.

- **Hyperparameter sensitivity**: The variance-reduced variants (PAGE, MVR) require careful tuning of hyperparameters (q, b) relative to projection eigenvalue ratios, which may limit practical applicability in real-world scenarios.

## Confidence

- **High confidence**: The core convergence guarantees for Bernoulli-LoRA-GD/SGD under non-convex optimization (Theorems 1-4). The mathematical framework is internally consistent and the proof techniques follow established non-convex optimization theory.
- **Medium confidence**: The variance-reduced variants (PAGE, MVR) achieving optimal rates with Bernoulli projection. While theoretically sound, these require careful hyperparameter tuning that may limit practical applicability.
- **Medium confidence**: The federated learning extensions. Theoretical convergence is established but empirical validation is limited to synthetic scenarios rather than real federated datasets.

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary the Bernoulli probability p, sketch rank r, and stepsize to map the complete convergence landscape and identify regimes where the framework breaks down.

2. **Distribution robustness testing**: Replace Gaussian sketches with alternative distributions (Rademacher, countSketch) to verify whether the convergence guarantees extend beyond the Gaussian case and how this affects practical performance.

3. **Real federated deployment**: Implement Fed-Bernoulli-LoRA variants on actual federated datasets with heterogeneous clients to validate whether the theoretical convergence bounds translate to practical performance gains in realistic federated settings.