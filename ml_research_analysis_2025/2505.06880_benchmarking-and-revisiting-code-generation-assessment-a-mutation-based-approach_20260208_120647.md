---
ver: rpa2
title: 'Benchmarking and Revisiting Code Generation Assessment: A Mutation-Based Approach'
arxiv_id: '2505.06880'
source_url: https://arxiv.org/abs/2505.06880
tags:
- code
- pass
- prompt
- input
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a significant bias in current code generation
  benchmarks, where single prompts fail to represent the variety of real-world problem
  descriptions. To address this, the authors propose 10 mutation strategies (e.g.,
  typos, paraphrases, synonym substitution) to generate diverse prompt variants and
  three new metrics: Correctness Variability (CV), Mutation Bias (MB), and Best Pass@k
  (Pass@k b).'
---

# Benchmarking and Revisiting Code Generation Assessment: A Mutation-Based Approach

## Quick Facts
- arXiv ID: 2505.06880
- Source URL: https://arxiv.org/abs/2505.06880
- Reference count: 24
- Key outcome: Single-prompt benchmarks introduce significant bias; mutation-based evaluation with new metrics (CV, MB, Pass@k_b) reveals substantial performance variation across prompt variants.

## Executive Summary
This paper identifies a critical bias in current code generation benchmarks where single prompts fail to capture the variety of real-world problem descriptions. The authors propose a mutation-based approach using 10 strategies (typos, paraphrases, synonym substitution, etc.) to generate diverse prompt variants and introduce three new metrics to evaluate their impact. Testing five popular code LLMs on 12,834 mutated prompts reveals that mutations substantially affect performance—sometimes improving results, sometimes worsening them. The findings demonstrate that existing benchmarks do not fairly reflect model capabilities and recommend expanding benchmarks with diverse prompt variants for more robust, unbiased evaluations.

## Method Summary
The authors apply 10 mutation strategies to HumanEval problems, targeting function signatures, problem descriptions, and examples. Mutations include typos (via Augly), synonyms (via WordNet), and LLM-based paraphrases/summaries (via GPT-4o). For each original and mutated prompt, they generate 10 code samples per model using temperature 0.8, extract the first complete function, and validate against unit tests. They compute Pass@1 (baseline), Pass@k_b (best Pass@1 across variants), Mutation Bias (MB = mean |CV|), and Correctness Variability (CV = difference in correct samples between mutated and original prompt).

## Key Results
- Single-prompt benchmarks introduce significant bias in code generation evaluation
- Mutation Bias (MB) is consistently non-zero across all models, indicating performance sensitivity to prompt variations
- Pass@k_b consistently outperforms original Pass@1, suggesting benchmarks underestimate model potential
- Different models show varying sensitivity to mutations in different prompt components (signatures, descriptions, examples)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Slight adjustments to problem descriptions, without changing semantics, can significantly affect the code generation performance of CLLMs.
- Mechanism: Input prompt mutation introduces semantic-preserving variations that a specific CLLM may interpret differently, leading to changes in functional correctness of generated code.
- Core assumption: Mutation strategies preserve underlying semantic intent, and observed performance changes are due to model's sensitivity to prompt formulation.
- Evidence anchors: Abstract states mutations impact performance; Finding 2 shows Pass@k_b outperforms original Pass@1; Table II shows non-zero MB values.
- Break condition: If mutation alters semantic intent, performance change cannot be attributed solely to prompt sensitivity.

### Mechanism 2
- Claim: Different parts of an input prompt have varying impacts on code generation correctness across different CLLMs.
- Mechanism: Models have distinct architectures and training objectives, leading them to weight signals from different prompt components differently.
- Core assumption: Variation in MB scores across mutation categories reflects differential reliance on those components.
- Evidence anchors: Finding 4 shows different models affected differently by prompt part mutations; Finding 5-6 show variable name and description impacts vary by model.
- Break condition: If mutation impact is uniform across all prompt parts for all models, differential reliance would be unsupported.

### Mechanism 3
- Claim: A benchmark using only a single prompt per problem provides a biased and incomplete assessment of a CLLM's true capabilities.
- Mechanism: Single prompt represents one point in possible problem descriptions; models may perform well on specific phrasing but poorly on semantically equivalent variations.
- Core assumption: "Mutated benchmark" with diverse variants is more representative of real-world scenarios.
- Evidence anchors: Abstract states various descriptions lead to biased evaluation; Finding 2 shows Pass@k_b consistently outperforms original Pass@1.
- Break condition: If model rankings are perfectly consistent between standard Pass@k and Pass@k_b, claim of "significant bias" would be weakened.

## Foundational Learning

- Concept: **Pass@k Metric**
  - Why needed here: Standard baseline metric the paper critiques; essential to understand why new metrics are proposed.
  - Quick check question: If a model generates 10 samples for a problem and 3 are correct, what is the Pass@1?

- Concept: **Semantic-Preserving Mutation**
  - Why needed here: Core technique of the paper; distinguishing it from semantic-altering changes is critical for interpreting results.
  - Quick check question: A prompt for function `add(a, b)` is paraphrased from "sums a and b" to "concatenates a and b". Is this a valid mutation for this paper's methodology?

- Concept: **Benchmark Bias**
  - Why needed here: Paper's central argument is that current benchmarks are biased; this concept frames the problem being solved.
  - Quick check question: If a model scores 90% on a benchmark but fails on a semantically identical prompt, what form of bias does this indicate?

## Architecture Onboarding

- Component map: HumanEval problems -> Mutation Engine (10 strategies) -> Execution Harness (10 samples per prompt) -> Evaluation Module (unit tests) -> Analysis Layer (metrics calculation)

- Critical path: Generate mutated prompts → Run inference on CLLMs → Execute unit tests → Calculate MB and Pass@k_b → Compare model rankings

- Design tradeoffs:
  - LLM-based mutation: High quality/diversity but costly and introduces mutator model variability
  - Rule-based mutation: Low cost and controllable but may produce less natural variants
  - Pass@k_b: Provides ceiling for performance but may overestimate capability

- Failure signatures:
  - High Mutation Bias (MB): Model is not robust to prompt variations
  - Large gap between Pass@1 and Pass@k_b: Original benchmark significantly underestimates model potential
  - Negative Correctness Variability (CV): Mutation was detrimental to model's comprehension

- First 3 experiments:
  1. Baseline Reproduction: Run 5 CLLMs on original HumanEval and reproduce Pass@1 scores
  2. Single-Strategy Mutation Impact: Apply only Description Paraphrase mutations to one model and calculate CV per prompt
  3. Metric Correlation Analysis: Compute both Pass@1 and Pass@k_b for all models and check if rankings change

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an iteration-based evaluation approach, requiring models to clarify understanding before coding, effectively mitigate the prompt bias identified in single-turn benchmarks?
- Basis in paper: Discussion section suggests implementing "iteration-based evaluation approach" to refine prompts and ensure model comprehension.
- Why unresolved: Current study focuses on single-turn mutations without implementing multi-turn clarification interactions.
- What evidence would resolve it: Empirical results from framework allowing model-led clarifications, showing reduced CV compared to static prompts.

### Open Question 2
- Question: Do Mutation Bias (MB) and Correctness Variability (CV) metrics generalize to larger proprietary models (e.g., GPT-4) and complex multi-file benchmarks beyond HumanEval?
- Basis in paper: Evaluation limited to five open-weight models and HumanEval dataset, leaving behavior of state-of-the-art proprietary models unexplored.
- Why unresolved: Unclear if larger models with different training data exhibit same sensitivity to typos and paraphrases.
- What evidence would resolve it: Applying 10 mutation strategies to proprietary models and larger datasets like MBPP or DS-1000.

### Open Question 3
- Question: What underlying mechanisms cause non-semantic perturbations, such as typos in variable names, to occasionally improve the code generation performance of specific models like DeepSeek?
- Basis in paper: Finding 5 notes counter-intuitive result that "Typos in variable names can sometimes improve the model's evaluation performance."
- Why unresolved: Paper quantifies performance shift but does not offer theoretical explanation for why introducing errors would aid synthesis.
- What evidence would resolve it: Analysis of attention heads or token probability distributions to determine how typos alter model's latent reasoning process.

## Limitations

- Mutation Quality Control: Paper relies on GPT-4o for paraphrase/summarize mutations without reporting quality metrics or controlling for semantic drift, which could invalidate semantic-preservation assumption.

- Model Selection Bias: Evaluation covers only five 7B parameter models, excluding larger and more capable code LLMs like GPT-4o or Claude, potentially limiting generalizability of findings.

- Dataset Representativeness: HumanEval represents specific slice of programming problems (primarily function implementation), and mutation sensitivity findings may not extend to other code generation tasks.

## Confidence

- **High Confidence**: Core claim that single-prompt benchmarks create biased evaluations is well-supported by substantial performance variation observed across mutation variants (MB values consistently non-zero).

- **Medium Confidence**: Claim that different prompt components affect models differently is supported by aggregate MB scores but lacks per-model mechanistic analysis to rule out random variation.

- **Low Confidence**: Claim that mutations "significantly impact" performance is based on aggregate statistics but lacks statistical significance testing or confidence intervals.

## Next Checks

1. **Manual Semantic Validation**: Have human annotators rate a stratified sample of 50 mutated prompts for semantic equivalence to originals. Calculate false positive rate where mutations actually changed the problem.

2. **Model Size Scaling Analysis**: Repeat mutation analysis for at least two larger models (e.g., CodeLlama 13B, DeepSeek-Coder 6.7B-32K) to test whether mutation sensitivity decreases with model scale.

3. **Statistical Significance Testing**: Apply paired t-tests (or non-parametric equivalents) on Pass@1 scores between original and mutated prompts for each model-strategy pair. Report effect sizes and p-values.