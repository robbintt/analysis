---
ver: rpa2
title: 'Better with Less: Small Proprietary Models Surpass Large Language Models in
  Financial Transaction Understanding'
arxiv_id: '2509.25803'
source_url: https://arxiv.org/abs/2509.25803
tags:
- transaction
- proprietary
- data
- transactions
- decoder-only
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of understanding financial transaction
  data, which is crucial for regulatory compliance, fraud detection, and decision-making.
  The authors explore the potential of Transformer-based models, including Encoder-Only,
  Decoder-Only, and Encoder-Decoder models, to improve transaction understanding.
---

# Better with Less: Small Proprietary Models Surpass Large Language Models in Financial Transaction Understanding

## Quick Facts
- **arXiv ID:** 2509.25803
- **Source URL:** https://arxiv.org/abs/2509.25803
- **Reference count:** 26
- **Primary result:** Small proprietary models (1.7M params) outperform large LLMs (8B+ params) on financial transaction understanding tasks while achieving faster inference and lower costs.

## Executive Summary
This paper addresses the challenge of understanding financial transaction data for regulatory compliance, fraud detection, and decision-making. The authors compare three Transformer-based architectures (Encoder-Only, Decoder-Only, and Encoder-Decoder) across three options each: pretrained LLMs, fine-tuned LLMs, and small proprietary models developed from scratch. Their primary finding is that while pretrained LLMs perform well on general NLP tasks, they do not significantly outperform small proprietary models tailored specifically for transaction data. The proprietary models achieve faster processing times, lower operational costs, and sufficient accuracy for real-time financial applications.

## Method Summary
The study evaluates financial transaction understanding using a synthetic dataset containing transaction text, zipcodes, and merchant IDs. Three architectures are tested: Encoder-Only (for fixed merchant databases), Decoder-Only (for raw/complex transactions), and Encoder-Decoder (for both). The winning approach is a proprietary Decoder-Only model with 8 layers, 128 embedding dimension, and 1.7M parameters, trained with BPE tokenizer (vocab size 500) using Cross Entropy loss. The model generates clean merchant names from messy input, which are then matched to a merchant database via Lucene string search. Implementation uses PyTorch TransformerDecoderLayer with Ray Tune/Optuna for hyperparameter optimization.

## Key Results
- Proprietary Decoder-Only model achieves 72.07% accuracy with just 1.7M parameters, compared to Llama3's 72.89% with 8B parameters
- Proprietary models demonstrate faster processing times and lower operational costs than LLMs for real-time applications
- Implementation of proprietary decoder model resulted in 14% increase in transaction coverage and over $13 million annual savings
- Encoder-Only models achieve best performance with vocabulary size of 1000, while Decoder-Only models perform optimally with vocabulary size of 500

## Why This Works (Mechanism)

### Mechanism 1: Vocabulary Compression for Noisy Domains
Restricting vocabulary size (500-1000 tokens) improves performance on noisy, structured financial strings compared to standard large vocabularies (30k+). Standard tokenizers segment transaction codes into inefficient fragments, while smaller, trained vocabularies force the model to learn domain-specific subword units, reducing output space complexity and filtering out noise.

### Mechanism 2: Task-Specific Parameter Efficiency
Architectures trained from scratch (1.5M-11M params) can match or exceed generalist LLMs (8B+ params) on narrow mapping tasks by discarding irrelevant linguistic capacity. General LLMs store vast world knowledge not required for simple string-to-entity mapping, while restricted models minimize overfitting to irrelevant noise and reduce inference latency.

### Mechanism 3: Generative De-noising over Similarity
Generative (Decoder) architectures outperform Embedding (Encoder) architectures because they can reconstruct clean entity names from messy input. Encoder models map messy input to fixed vectors that may blur distinctions between similar merchants, while Decoder models explicitly generate the "clean" name token-by-token, stripping noise during generation.

## Foundational Learning

- **Subword Tokenization (BPE vs. WordPiece)**
  - Why needed: Tokenizer choice significantly impacts accuracy on transaction codes
  - Quick check: Does the tokenizer split "SWA*EARLYBRD" into meaningful chunks or garbage tokens?

- **Contrastive Learning (Triplet Loss)**
  - Why needed: Essential for understanding Encoder-Only baseline using contrastive loss to distinguish similar merchants
  - Quick check: How do you select "negative" samples to ensure the model learns to distinguish "Target" from "Target Cafe"?

- **Autoregressive Generation**
  - Why needed: Required to understand winning Proprietary Decoder-Only model that predicts merchant name sequence one token at a time
  - Quick check: Why does the model stop generating? (It must predict an End-of-Sequence token)

## Architecture Onboarding

- **Component map:** Raw Transaction String + Zipcode -> Tokenizer (BPE/WordPiece) -> Transformer (Enc/Dec/Both) -> Search (Lucene Vector or String Match) -> Output: Merchant ID

- **Critical path:** Tokenizer Configuration (vocab size 500-1000 is highest leverage hyperparameter) -> Model Selection (Proprietary Decoder for raw/complex data, Proprietary Encoder for known/simple data)

- **Design tradeoffs:** Latency vs. Coverage (Encoder-Only: 11ms, 60% Acc vs. Decoder-Only: 95ms, 72% Acc) / Indexing (Encoder models require 80+ hours of indexing time and must be rebuilt on updates; Decoder models do not need vector indexing)

- **Failure signatures:** Over-tokenization (if vocab is too large, model sees transaction codes as incomprehensible sub-character noise) / Hallucination (Decoder generates plausible but non-existent merchant name) / High Latency (using Llama3 fails real-time requirement)

- **First 3 experiments:**
  1. Tokenizer Sweep: Train BPE and WordPiece tokenizers with vocab sizes [100, 500, 1000, 5000] on transaction corpus
  2. Architecture Sizing: Train Decoder-Only model varying embedding dimensions [64, 128, 256] and layers [2, 4, 8]
  3. Hard Negative Mining: Compare random negatives vs. Jaccard-similarity negatives (>0.75) for Encoder model

## Open Questions the Paper Calls Out

### Open Question 1
Why does fine-tuning degrade performance in specific encoder-only architectures like SBERT for financial transaction data, and can this be mitigated? The paper notes SBERT performance decreased by 4% after fine-tuning but does not validate the hypothesis or propose a solution to adapt encoder-only LLMs without performance loss.

### Open Question 2
Can small proprietary models maintain their advantage over LLMs in zero-shot scenarios involving entirely new transaction patterns or languages? The study evaluates models on specific test sets but does not test ability to handle structural shifts in data format or languages without retraining.

### Open Question 3
Is it possible to consolidate the proposed hybrid system (Rules + ESD + Neural Model) into a single end-to-end small model without compromising coverage? The paper demonstrates Decoder-Only handles "Rawcleansed" data well but does not explore if a single small model could learn rules-based patterns and string distances effectively to replace the entire hybrid pipeline.

## Limitations
- The paper relies on a proprietary dataset that is not publicly available, making independent verification impossible
- Critical implementation details like exact Lucene search configuration and data synthesis procedures remain unspecified
- Economic claims ($13M annual savings) are difficult to verify without understanding full operational context

## Confidence
- **High Confidence:** General finding that smaller, specialized models can outperform larger LLMs on narrow, domain-specific tasks
- **Medium Confidence:** Specific architectural choices (BPE vocab size 500, 8-layer decoder) and their impact on performance
- **Low Confidence:** Economic claims and their real-world applicability without full operational context

## Next Checks
1. **Tokenizer Sensitivity Analysis:** Train the same decoder architecture with varying BPE vocabulary sizes (100-5000) on a publicly available financial transaction dataset to verify the 500-token optimum claim.

2. **Latency Benchmarking:** Implement the inference pipeline using the described Lucene search and measure end-to-end latency across different beam widths to confirm the <100ms requirement is consistently met.

3. **Generalization Test:** Evaluate the trained model on a held-out set of merchant names not present in training to assess the frequency of plausible but incorrect generations that might pass the Lucene similarity filter.