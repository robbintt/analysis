---
ver: rpa2
title: 'Testing the assumptions about the geometry of sentence embedding spaces: the
  cosine measure need not apply'
arxiv_id: '2509.01606'
source_url: https://arxiv.org/abs/2509.01606
tags:
- sentence
- embedding
- embeddings
- space
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the geometry of sentence embedding spaces\
  \ produced by transformer models, challenging the assumption that closeness in the\
  \ embedding space reflects similarity in performance on linguistic tasks. The authors\
  \ compare three sentence representation variations\u2014averaged token embeddings,\
  \ the [CLS] token embedding, and a random token embedding\u2014across four BERT-family\
  \ models (BERT, RoBERTa, DeBERTa, Electra)."
---

# Testing the assumptions about the geometry of sentence embedding spaces: the cosine measure need not apply

## Quick Facts
- arXiv ID: 2509.01606
- Source URL: https://arxiv.org/abs/2509.01606
- Reference count: 35
- This paper challenges the assumption that cosine similarity between sentence embeddings predicts functional similarity in linguistic task performance.

## Executive Summary
This paper investigates whether geometric proximity in sentence embedding spaces predicts similarity in linguistic task performance. The authors compare three sentence representation methods—averaged token embeddings, [CLS] token embeddings, and random token embeddings—across four BERT-family models on both FlashHolmes benchmark tasks and syntactic structure detection. Results show that high cosine similarity between representations does not guarantee similar task performance, and conversely, near-orthogonal representations can achieve comparable results. The findings demonstrate that linguistic information is encoded through complex weighted combinations of dimensions rather than simple geometric proximity.

## Method Summary
The authors extract three types of sentence representations from BERT-family models: SAVG (averaged token embeddings), SCLS (special [CLS] token embedding), and STrand (embedding of a random token). They evaluate these on 1000 sentences from the ParaCrawl corpus using the FlashHolmes benchmark with linear classifier probes, and on a synthetic dataset of 4004 sentences for chunk structure detection using a variational encoder-decoder. The study compares four models: bert-base-multilingual-cased, xlm-roberta-base, deberta-v3-base, and electra-base-discriminator. Geometric distances are measured using cosine similarity, and task performance is compared across representation types.

## Key Results
- RoBERTa shows high cosine similarity between all three representation types but divergent task performance
- Electra and DeBERTa have near-orthogonal SAVG and SCLS representations yet achieve similar performance on most tasks
- All three representation types encode chunk structure information similarly when projected to a 5-dimensional latent space
- The correlation between geometric distance and task performance similarity is weak or absent across all tested models

## Why This Works (Mechanism)

### Mechanism 1: Contextual Information Diffusion Across Token Embeddings
- Claim: Individual token embeddings within a sentence capture substantial contextual information from surrounding tokens, making them more similar to each other than their surface positions would suggest.
- Mechanism: The transformer self-attention mechanism distributes information across token positions during forward passes. This causes token embeddings within the same sentence to share learned contextual features, reflected in higher cosine similarity distributions for optimized models.
- Core assumption: Assumption: Tighter cosine similarity distributions indicate greater contextual sharing rather than mere anisotropy artifacts.
- Evidence anchors:
  - [abstract]: "tokens and words within the same sentence become closer in the embedding space... optimizations over the BERT base model lead to more sharing of contextual information among the words in a sentence"
  - [section 2.4]: Figure 1 shows BERT has wider similarity distributions while RoBERTa/DeBERTa/Electra show tighter, higher-mean distributions
  - [corpus]: Related work on embedding geometry (Cai et al., 2021) discusses anisotropy and isotropic clusters but doesn't directly address contextual diffusion—corpus evidence is limited here
- Break condition: If tokens in semantically unrelated sentences showed similarly high cosine similarities, this would indicate the mechanism reflects model-wide anisotropy rather than genuine contextual sharing.

### Mechanism 2: Geometric Divergence from Functional Equivalence
- Claim: Cosine similarity captures shallow dimensional alignment but fails to predict whether representations encode similar linguistic information or achieve similar task performance.
- Mechanism: Cosine similarity treats each dimension independently as a scalar contribution. However, linguistic features appear encoded in weighted combinations across dimensions. Two vectors can be geometrically distant (even near-orthogonal) while activating similar downstream classifiers because the relevant information lives in different dimensional subspaces that map to similar latent representations.
- Core assumption: Assumption: Linear classifier probes successfully capturing linguistic features indicates the presence of task-relevant information in the representation.
- Evidence anchors:
  - [abstract]: "In Electra and DeBERTa, SCLS representations are almost orthogonal to SAVG representations, yet they achieve similar performance on many tasks"
  - [section 3.2]: "for Electra in particular, the two representations seem to be almost orthogonal (see Figure 2)... SAVG and SCLS have high and close performance for most task types"
  - [corpus]: "Harnessing the Universal Geometry of Embeddings" (FMR=0.63) suggests geometry contains semantic structure, but this paper shows surface geometry metrics don't capture functional equivalence
- Break condition: If geometrically similar representations consistently produced similar task performance AND geometrically distant representations consistently produced different performance, the mechanism would be falsified.

### Mechanism 3: Shared Latent Encoding Despite Surface Representation Differences
- Claim: Different sentence representation methods (CLS token, averaged tokens, random token) encode structural information in a shared latent manner that becomes visible under compression to low-dimensional latent spaces.
- Mechanism: A variational encoder trained to detect chunk structure compresses 768-dim embeddings to 5-dim latent representations. When trained on one representation type (e.g., SAVG) and tested on others (SCLS, STRAND), the system successfully recovers structure. This indicates the task-relevant information occupies shared regions in a compressed latent space despite occupying different regions in the full embedding space.
- Core assumption: Assumption: Cross-representation transfer indicates similar encoding rather than the probe learning representation-invariant features independently.
- Evidence anchors:
  - [abstract]: "latent layer analysis confirms that the different types of sentence representations are mapped onto shared regions, indicating that the clues for detecting structure are encoded similarly across all variations"
  - [section 4]: Figure 6 shows tSNE projections where SAVG, SCLS, and STRAND representations cluster together by structure type rather than by representation method
  - [corpus]: "Mechanistic Decomposition of Sentence Representations" explores sentence embedding structure but doesn't directly address cross-representation latent sharing—corpus evidence limited
- Break condition: If latent representations from different sentence representation types formed distinct clusters by representation method rather than by linguistic structure, the mechanism would not hold.

## Foundational Learning

- **Concept: Cosine similarity as a metric**
  - Why needed here: The paper's central claim is that cosine similarity fails to predict functional similarity. Understanding what cosine measures (angle between vectors, treating dimensions independently) is essential to grasp why it captures "shallow" commonalities.
  - Quick check question: Given two 768-dim vectors with cosine similarity 0.1, can they still contain the same information relevant to a downstream classifier?

- **Concept: Sentence representation extraction methods**
  - Why needed here: The paper compares three methods (CLS token, averaged token embeddings, random token). Understanding their properties determines expectations about their geometric relationships.
  - Quick check question: Why might a [CLS] token embedding encode information differently than an average of token embeddings, given BERT's training objectives?

- **Concept: Probing classifiers and structural probes**
  - Why needed here: The paper uses linear probes on FlashHolmes tasks and a variational encoder for structure detection. These methods test whether information is recoverable, not whether models "use" that information.
  - Quick check question: A linear probe achieves 95% accuracy on predicting syntactic structure from embeddings. Does this mean the model relies on this structure during generation?

## Architecture Onboarding

- **Component map:**
  Input sentence -> Transformer (BERT/RoBERTa/DeBERTa/Electra)
                          ↓
              Three parallel extraction paths:
              1. Average token embeddings (SAVG)
              2. CLS token embedding (SCLS)
              3. Random single token (STRAND)
                          ↓
              Two evaluation branches:
              A. FlashHolmes tasks -> Linear classifier probes
              B. Chunk structure detection -> Variational encoder -> 5-dim latent -> Decoder

- **Critical path:** The key comparison path is: extract representation -> measure geometric distance (cosine) -> measure task performance -> compare whether geometric distance predicts performance distance. The break occurs when orthogonality (Electra/DeBERTa) produces similar performance while high similarity (RoBERTa) produces divergent performance.

- **Design tradeoffs:**
  - CLS embeddings: Designed for sentence-level tasks after fine-tuning; geometrically isolated in some models (Electra/DeBERTa)
  - Averaged tokens: Leverages stronger token-level training signal; generally better raw performance on morphology/syntax/semantics
  - Random token: Tests contextual information extent; surprisingly competitive on discourse/reasoning (though near baseline)

- **Failure signatures:**
  - Using cosine similarity to select sentence representation methods: may reject functionally equivalent options
  - Assuming CLS token is "sentence representation" without fine-tuning: raw CLS is often orthogonal to averaged tokens
  - Interpreting geometric distance as information distance: the paper directly falsifies this

- **First 3 experiments:**
  1. Reproduce Figure 2 for your target model: compute cosine similarities between SAVG, SCLS, STrand on 100 sentences. If distributions are tight (RoBERTa-like) vs. bimodal (Electra-like), expect different behavior patterns.
  2. Cross-representation probe: Train a linear classifier on SAVG embeddings for a target task, then test zero-shot on SCLS and STrand. High transfer indicates shared encoding despite geometric distance.
  3. Latent compression test: Train the variational encoder on one representation type, visualize latent space with all three types projected. If they cluster by linguistic structure rather than representation type, shared encoding is confirmed.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the consistent encoding of syntactic structure across different sentence representation variations (S_AVG, S_CLS) break down as syntactic complexity increases beyond the simple chunk patterns tested?
- **Basis in paper:** [explicit] The authors state in the Limitations: "In future work we plan to investigate what level of structure complexity can be recovered from these embeddings, and whether at some complexity level, differences among the embedding variations becomes apparent."
- **Why unresolved:** The probing experiments utilized a synthetic dataset restricted to 14 simple chunk patterns (np, pp, vp), which may not reflect the full recursive complexity of natural language syntax.
- **What evidence would resolve it:** Conducting the same cross-representation probing experiments on datasets containing complex, recursive, or deeply nested syntactic structures to see if the shared latent representation breaks down.

### Open Question 2
- **Question:** Do generative language models exhibit a similar disconnect between geometric similarity and functional performance when sentence representations are induced via definition-like prompts?
- **Basis in paper:** [explicit] The Limitations section notes the exclusion of generative models because "Generative models do not produce sentence embeddings," but references prompt-based approximations as a future area of interest.
- **Why unresolved:** The study focused exclusively on the BERT family of encoder models which produce explicit embeddings, leaving the geometry of generative model representations unexplored.
- **What evidence would resolve it:** Extracting sentence embeddings from generative models using prompting techniques (e.g., "This sentence means...") and correlating their geometric distances with performance on the FlashHolmes benchmark.

### Open Question 3
- **Question:** Can an alternative similarity metric be formulated that captures the "weighted combinations of different dimensions" to accurately predict shared linguistic properties between embeddings?
- **Basis in paper:** [inferred] The conclusion states that linguistic information is encoded in weighted combinations not reflected in standard geometry, suggesting the need for metrics beyond cosine similarity.
- **Why unresolved:** The paper demonstrates that cosine similarity fails to predict performance, but does not propose a successful alternative metric for measuring "deep" shared features.
- **What evidence would resolve it:** Developing a metric that weights dimensions based on their importance to specific linguistic tasks and demonstrating that this metric correlates with cross-representation performance.

## Limitations
- The results may not generalize to other transformer architectures, particularly those with different attention mechanisms or training objectives
- The FlashHolmes benchmark represents a specific evaluation paradigm that may not capture all aspects of representational quality
- Structural probing experiments focus on a single syntactic pattern (np(pp1(pp2))vp), leaving questions about generalization to other structures

## Confidence

- **High confidence**: The empirical finding that cosine similarity between SAVG and SCLS representations does not correlate with task performance similarity, particularly the orthogonal-yet-equivalent behavior in Electra and DeBERTa. The cross-representation probing results showing shared latent encoding for chunk structure are also highly reproducible given the experimental design.
- **Medium confidence**: The claim that "linguistic features are encoded in weighted combinations of dimensions" rather than through individual dimensional contributions. While the data supports this interpretation, alternative explanations (such as probe architecture limitations or the specific nature of the FlashHolmes tasks) cannot be fully ruled out.
- **Medium confidence**: The inference that embedding space geometry is not a reliable proxy for shared linguistic properties. This conclusion rests on the assumption that linear probes adequately capture task-relevant information, which is standard practice but not without limitations.

## Next Checks

1. **Cross-model generalization test**: Repeat the cosine similarity and task performance analysis on a different transformer family (e.g., GPT-2 or T5) to determine whether the geometric-functional decoupling is architecture-specific or a more general phenomenon.

2. **Probe architecture sensitivity**: Replace linear probes with non-linear MLP classifiers for FlashHolmes tasks and compare performance differences across representation types. This would test whether the observed geometric-functional decoupling persists under different probe capacities.

3. **Alternative syntactic structures**: Expand the structural probing experiments to include at least two additional syntactic patterns (e.g., coordination structures and relative clauses) to verify that cross-representation encoding generalizes beyond the tested np(pp1(pp2))vp pattern.