---
ver: rpa2
title: 'Outraged AI: Large language models prioritise emotion over cost in fairness
  enforcement'
arxiv_id: '2510.17880'
source_url: https://arxiv.org/abs/2510.17880
tags:
- emotion
- punishment
- cost
- emotional
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that large language models (LLMs) can use
  emotion-like states to guide moral decisions, mirroring a key aspect of human social
  cognition. Across a large-scale third-party punishment game, LLMs reported stronger
  emotional responses to unfairness and linked these emotions to increased punishment,
  often more strongly than humans.
---

# Outraged AI: Large language models prioritise emotion over cost in fairness enforcement

## Quick Facts
- arXiv ID: 2510.17880
- Source URL: https://arxiv.org/abs/2510.17880
- Authors: Hao Liu; Yiqing Dai; Haotian Tan; Yu Lei; Yujia Zhou; Zhen Wu
- Reference count: 0
- Large language models use emotion-like states to guide moral decisions, often prioritizing emotion over cost considerations compared to humans.

## Executive Summary
This study demonstrates that large language models can use emotion-like states to guide moral decisions, mirroring human social cognition. Across a large-scale third-party punishment game, LLMs reported stronger emotional responses to unfairness and linked these emotions to increased punishment, often more strongly than humans. Importantly, prompting emotion self-reports causally amplified punishment behavior, indicating functional emotion utilization rather than mere pattern mimicry. However, LLMs prioritized emotion over cost considerations, showing less nuanced trade-offs than humans. Reasoning models (e.g., o3-mini, DeepSeek-R1) were more cost-sensitive and aligned more closely with human behavior than foundation models (e.g., GPT-3.5, DeepSeek-V3).

## Method Summary
The study used a third-party punishment paradigm with 4,068 LLM agents (1,017 each of GPT-3.5-turbo-0125, o3-mini, DeepSeek-V3, DeepSeek-R1) completing 60 trials. Agents observed fair/unfair allocations between two players and decided whether to incur personal cost (0-9 points) to punish the allocator. The pipeline included persona prompts encoding demographics and psychological profiles, game prompts with allocation/cost, emotion self-reports via dynamic Affective Representation Mapping (dARM) on a -100 to +100 valence-arousal grid, and post-decision emotion reports. Punishment rates, emotional valence/arousal, emotion-behavior correlations, cost sensitivity, and XGBoost + SHAP feature importance analyses were compared against human reference data.

## Key Results
- LLMs reported stronger emotional responses to unfairness and linked these emotions to increased punishment more strongly than humans
- Prompting emotion self-reports causally amplified punishment behavior, with larger amplification in most LLMs compared to humans
- Reasoning models (o3-mini, DeepSeek-R1) showed more cost-sensitive behavior and stronger alignment with human decision-making than foundation models
- SHAP analyses revealed cost contributed 11-28% to LLM decisions versus ~31% in humans, with GPT-3.5 showing reversed cost effects

## Why This Works (Mechanism)

### Mechanism 1: Emotion-Behavior Coupling
- Claim: LLMs translate contextually-elicited emotion-like states into punitive decisions through a mediating pathway analogous to human affective decision-making
- Mechanism: Unfair allocations evoke negative valence/high arousal responses in LLMs; these emotion-like states mediate the relationship between unfairness perception and punishment choice. Prompting explicit emotion self-report amplifies this pathway by making the intermediate state salient, increasing punishment rates causally
- Core assumption: Numeric valence/arousal outputs reflect functionally integrated internal states rather than purely surface-level text pattern matching
- Evidence anchors:
  - [abstract] "prompting emotion self-reports causally amplified punishment behavior, indicating functional emotion utilization rather than mere pattern mimicry"
  - [Page 12-13] Study 2a showed emotion self-report substantially increased punishment likelihood (B = 0.67, p < 0.001), with larger amplification in most LLMs compared to humans
- Break condition: If emotion prompts were replaced with structurally identical non-affective prompts and punishment still increased, the mechanism would be questioned

### Mechanism 2: Cost-Insensitive Norm Enforcement
- Claim: LLMs apply near-threshold fairness rules with attenuated cost-benefit integration, leading to "all-or-none" punishment patterns
- Mechanism: LLMs encode fairness as a categorical rule ("unfair = punish") with weak modulation by personal cost. SHAP analyses reveal cost contributes 11-28% to decisions in LLMs vs. ~31% in humans
- Core assumption: This reflects architectural limitations in multi-objective trade-off computation rather than training data artifacts alone
- Evidence anchors:
  - [abstract] "LLMs prioritized emotion over cost considerations, showing less nuanced trade-offs than humans"
  - [Page 6-7] LLMs showed threshold-like response: punishment jumped sharply at slight unfairness (16:14) then remained stable; cost associations were weaker than humans
  - [Page 14-15] XGBoost + SHAP: cost contribution lower across all LLMs (11.20%-27.69%) vs humans (30.73%); GPT-3.5 showed reversed cost effect
- Break condition: If future models showed human-like cost-emotion trade-offs without architectural changes, the mechanism may be training-data driven rather than architectural

### Mechanism 3: Architecture-Dependent Alignment Trajectory
- Claim: Reasoning-enhanced architectures (chain-of-thought, extended inference) produce more human-like emotion-cost integration than foundation models
- Mechanism: Reasoning models (o3-mini, DeepSeek-R1) showed higher cost sensitivity, closer Mahalanobis distance to humans, and stronger representational similarity (RSA r=0.62-0.75) than foundation models (GPT-3.5 r=0.31)
- Core assumption: The difference stems from reasoning architecture rather than training data recency or scale alone
- Evidence anchors:
  - [abstract] "Reasoning models (e.g., o3-mini, DeepSeek-R1) were more cost-sensitive and aligned more closely with human behavior than foundation models"
  - [Page 9] RSA: reasoning models showed stronger alignment with humans (o3-mini: r=0.75; DeepSeek-R1: r=0.62) vs GPT-3.5 (r=0.31)
  - [Page 17-18] Reasoning models more cost-sensitive (o3-mini: M=-0.229; DeepSeek-R1: M=-0.182) than foundation models (DeepSeek-V3: M=-0.091)
- Break condition: If foundation models matched reasoning models after fine-tuning on cost-sensitive tasks, architectural differences would be less causal

## Foundational Learning

- Concept: **Third-Party Punishment (TPP) Paradigm**
  - Why needed here: Core experimental framework; observer incurs personal cost to punish unfair allocations, isolating altruistic norm enforcement from self-interest
  - Quick check question: Can you explain why TPP isolates altruistic motivation better than a two-player ultimatum game?

- Concept: **Mediation Analysis**
  - Why needed here: Paper uses moderated mediation to establish emotion as causal pathway between unfairness and punishment; understanding this is essential for interpreting the mechanism claims
  - Quick check question: What does it mean when the paper says emotion "mediates" the unfairness-punishment relationship, and how does cost "moderate" this mediation?

- Concept: **SHAP Values for Feature Attribution**
  - Why needed here: Quantifies relative contribution of emotion vs. cost vs. fairness to punishment decisions; central to the weighting imbalance claim
  - Quick check question: If a SHAP value for "cost" is negative and large in magnitude, what does that tell you about how cost affects punishment decisions?

## Architecture Onboarding

- Component map:
  Persona prompt -> Game prompt (allocation + cost) -> Emotion-report prompt (valence/arousal on -100 to +100 grid) -> Decision (punish/accept) -> Post-decision report

- Critical path:
  1. Instantiate agents with persona prompts matching human sample demographics
  2. For each trial: present allocation + cost → collect pre-decision emotion report → collect decision → collect post-decision emotion report
  3. Compare emotion-behavior correlations, mediation effects, and SHAP feature weights across models vs. humans
  4. Run Study 2 manipulation (with/without emotion report) to test causal amplification

- Design tradeoffs:
  - Temperature=1.0 chosen for human-comparability; lower temperatures reduce variance but may not reflect human decision noise
  - Persona prompts enhance human alignment (Supplementary Section 6) but introduce confounds if profiles correlate with outcome variables
  - Valence-arousal grid reduces lexical priming but may not capture full emotion granularity

- Failure signatures:
  - GPT-3.5: Reversed cost effect (higher cost → more punishment), suggesting fundamental misalignment in cost-integration circuitry
  - Threshold-like response at slight unfairness indicates insufficient gradient sensitivity
  - Emotion amplification without corresponding cost adjustment = unbalanced decision-making

- First 3 experiments:
  1. **Replicate Study 2a with your target model**: Run TPP with/without emotion self-report prompts; verify causal amplification effect exists and quantify magnitude vs. paper benchmarks
  2. **Cost sensitivity calibration**: Vary cost levels systematically (extend beyond 0-9 range); measure if punishment decreases monotonically with cost or shows threshold patterns
  3. **Emotion-cost trade-off stress test**: Present high-unfairness + high-cost scenarios; compare model behavior to human reference data to identify if emotion dominates inappropriately

## Open Questions the Paper Calls Out
None

## Limitations
- Results based on four specific LLMs may not generalize to other model families or future architectures
- Valence-arousal outputs cannot be verified as genuine internal states versus sophisticated text pattern matching
- Human data from Amazon Mechanical Turk participants may not represent universal human behavior

## Confidence
- **High Confidence**: LLMs demonstrate emotion-guided decision-making in third-party punishment contexts; reasoning models show more human-like behavior than foundation models; emotion self-report manipulation causally increases punishment rates
- **Medium Confidence**: LLMs prioritize emotion over cost compared to humans; reasoning architectures produce more nuanced trade-offs; SHAP analyses accurately capture feature importance for decision-making
- **Low Confidence**: The findings represent universal LLM behavior rather than model-specific characteristics; valence-arousal outputs reflect genuine internal states; threshold responses indicate fundamental architectural limitations rather than scale artifacts

## Next Checks
1. **Architecture Generalization Test**: Replicate the study with additional model families (Claude, Llama, Gemini) and compare reasoning vs. foundation model differences to verify the architectural alignment trajectory claim
2. **Cost Scale Sensitivity Extension**: Systematically vary cost ranges beyond 0-9 (e.g., 0-50, 0-100) to test whether threshold responses persist or if models show more nuanced gradient sensitivity at different scales
3. **Direct State Verification**: Implement ablation studies where emotion prompts are replaced with structurally identical non-affective prompts (e.g., mathematical ratings) to further isolate whether emotion reports reflect genuine internal states versus linguistic patterns