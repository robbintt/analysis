---
ver: rpa2
title: Contrastive Retrieval Heads Improve Attention-Based Re-Ranking
arxiv_id: '2510.02219'
source_url: https://arxiv.org/abs/2510.02219
tags:
- heads
- re-ranking
- retrieval
- attention
- core
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CoRe heads are a set of attention heads identified through a contrastive
  scoring metric that explicitly rewards attention directed toward relevant documents
  while penalizing attention to irrelevant ones. This method outperforms existing
  approaches by isolating the most discriminative heads for re-ranking.
---

# Contrastive Retrieval Heads Improve Attention-Based Re-Ranking

## Quick Facts
- arXiv ID: 2510.02219
- Source URL: https://arxiv.org/abs/2510.02219
- Authors: Linh Tran; Yulong Li; Radu Florian; Wei Sun
- Reference count: 21
- Primary result: Using <1% of attention heads (typically 8) selected via contrastive scoring improves re-ranking accuracy while enabling up to 50% layer pruning with 20% latency reduction and 40% memory savings

## Executive Summary
This paper introduces CoRe heads, a method for identifying the most discriminative attention heads for list-wise document re-ranking in large language models. The approach uses a contrastive scoring metric that rewards attention directed toward relevant documents while penalizing attention to irrelevant ones, outperforming existing methods that rely on absolute attention values. Experiments with three LLMs show that selecting fewer than 1% of all heads consistently improves re-ranking accuracy across BEIR and MLDR benchmarks while enabling significant computational efficiency gains through layer pruning.

## Method Summary
The method involves detecting discriminative attention heads through a contrastive scoring mechanism that compares attention to relevant documents against attention to hard negatives. Using a subset of NQ training data (1000 samples), the system computes scores for all attention heads, selects the top 8, and uses only these heads for re-ranking. The approach incorporates contextual calibration and can be combined with layer pruning, as CoRe heads concentrate in middle transformer layers. The method consistently achieves state-of-the-art list-wise re-ranking performance while reducing computational overhead.

## Key Results
- Using <1% of attention heads (typically 8) improves re-ranking accuracy over using all heads
- CoRe heads are concentrated in middle layers, enabling up to 50% of final layers to be pruned without accuracy loss
- The method achieves 20% latency reduction and 40% memory savings while maintaining performance
- Consistently achieves state-of-the-art list-wise re-ranking performance across BEIR and MLDR benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A contrastive scoring metric isolates attention heads that discriminate relevant from irrelevant documents more effectively than absolute attention alone.
- Mechanism: For each head h, compute S_CoRe(h) = exp(s^h_pos/τ) / (exp(s^h_pos/τ) + Σ_i exp(s^h_neg,i/τ)), where s^h_d is aggregated attention from query tokens to document tokens. This rewards heads that attend strongly to the gold document while penalizing attention to hard negatives, implementing a relative ranking criterion rather than absolute magnitude.
- Core assumption: Heads that discriminate well on a small labeled set (NQ subset) generalize across datasets and languages.
- Evidence anchors:
  - [abstract] "contrastive scoring metric that explicitly rewards high attention heads that correlate with relevant documents, while downplaying nodes with higher attention that correlate with irrelevant documents"
  - [section 4] Eq. 5 defines the InfoNCE-style score; Section 4 describes using 1000 NQ samples with 49 hard negatives.
  - [corpus] Related work (QR heads) uses absolute attention without contrastive penalization, which the paper shows can select misleading heads (Fig. 1).
- Break condition: If temperature τ is too low, selection becomes overly narrow; if too high, sensitivity to hard negatives diminishes (Section 4). Temperature requires tuning per model.

### Mechanism 2
- Claim: Aggregating attention from fewer than 1% of heads (typically 8) improves re-ranking accuracy over using all heads.
- Mechanism: Replace the full aggregation in ICR (Eq. 1) with sparse aggregation over H* (Eq. 6). Most heads contribute noise or redundancy; the top CoRe heads carry the discriminative signal.
- Core assumption: The head ranking on the detection set reflects true re-ranking utility on held-out tasks.
- Evidence anchors:
  - [abstract] "constituting less than 1% of all heads, substantially improve re-ranking accuracy"
  - [section 5.2, Fig. 2] DBPedia peaks at 9 heads; Fig. 3 shows CoRe-R consistently outperforms ICR and QR-R across head counts.
  - [corpus] Prior work (Michel et al., 2019; Voita et al., 2019) shows many heads are unnecessary for translation; this extends the insight to retrieval.
- Break condition: If the detection data distribution diverges sharply from the target task (e.g., counter-evidence retrieval in ArguAna), selected heads may underperform (Section 5.4).

### Mechanism 3
- Claim: CoRe heads concentrate in middle transformer layers, enabling late-layer pruning without accuracy loss.
- Mechanism: Since attention-based re-ranking does not require token generation, layers primarily responsible for output decoding can be skipped. Pruning 50% of final layers preserves accuracy while reducing memory and latency.
- Core assumption: The middle layers contain sufficient relevance signal; late layers are task-agnostic for generation.
- Evidence anchors:
  - [abstract] "concentrated in middle layers, allowing up to 50% of the final layers be pruned"
  - [section 5.3, Fig. 4] Heatmaps show high S_CoRe in middle layers across Mistral, Llama, Phi-4. Fig. 5–6 show 50% pruning yields negligible accuracy drop with 20% latency reduction and 40% memory savings.
  - [corpus] Corpus signals do not directly address layer pruning for retrieval heads; this is a novel contribution of this paper.
- Break condition: Pruning beyond 50% degrades performance (Fig. 5), as remaining CoRe heads in earlier layers may lack full discriminative capacity.

## Foundational Learning

- Concept: **Multi-head attention and head specialization**
  - Why needed here: Understanding that not all heads are equally useful; some specialize in retrieval-like patterns while others handle syntax or generation.
  - Quick check question: Can you explain why aggregating over all heads might introduce noise in a re-ranking task?

- Concept: **Contrastive learning (InfoNCE)**
  - Why needed here: The scoring metric adapts InfoNCE without training; understanding the numerator/denominator structure clarifies why the metric emphasizes relative differences.
  - Quick check question: What happens to the score if a head attends equally to the positive and all negative documents?

- Concept: **List-wise vs. point-wise vs. pair-wise re-ranking**
  - Why needed here: The method is list-wise (considers all candidates jointly); this affects prompt construction and cross-document attention patterns.
  - Quick check question: Why might list-wise re-ranking better capture global ranking quality than pair-wise methods?

## Architecture Onboarding

- Component map:
  - Head detector: Computes S_CoRe for all heads on labeled detection data, averages across samples, selects top-k (e.g., 8). Outputs H* (layer, head indices).
  - Re-ranker inference: Constructs prompt (instruction + k documents + query), runs single forward pass, aggregates attention only from H*, applies contextual calibration with content-free query.
  - Layer-pruning wrapper (optional): Stops computation after middle layers; skips remaining transformer blocks.

- Critical path:
  1. Mine hard negatives (49 per query from top-100 retriever candidates, filter false negatives).
  2. Run forward pass on detection prompts, compute S_CoRe per head, average, select top-k.
  3. At inference, aggregate attention from H* only, calibrate, sum token scores per document.

- Design tradeoffs:
  - Temperature τ: Lower values are more selective but may over-prune useful heads; higher values tolerate hard negatives but may include noisier heads.
  - Number of heads: Peak performance at 8–9 heads (Fig. 2); more heads reintroduce noise.
  - Layer pruning threshold: 50% is safe across tested models; 60% starts to degrade.

- Failure signatures:
  - Accuracy drops below retriever baseline: Check for domain mismatch (e.g., counter-evidence datasets like ArguAna) or inappropriate universal prompts.
  - Selected heads vary wildly across runs: Detection sample size may be insufficient; increase to >1000 or verify hard-negative quality.
  - Pruning causes sudden collapse: Verify that top CoRe heads are not in late layers (model-specific; Granite shows some late-layer CoRe heads).

- First 3 experiments:
  1. Reproduce S_CoRe head detection on NQ subset (1000 samples, 50 docs each), verify top-8 head indices match reported locations for Llama-3.1 8B or Mistral 7B.
  2. Ablate temperature τ (e.g., {0.0001, 0.001, 0.01, 0.1}) on a held-out NQ split; plot nDCG@10 vs. τ to confirm optimal range.
  3. Implement layer pruning at 30%, 50%, 60% on BEIR subset; measure nDCG@10, latency, and peak GPU memory to validate efficiency claims.

## Open Questions the Paper Calls Out
None

## Limitations
- Detection Set Domain Dependence: CoRe head selection relies entirely on NQ detection data, with no theoretical guarantee that heads optimal for NQ will transfer to domains with fundamentally different relevance patterns.
- Temperature Sensitivity: The contrastive scoring depends critically on the temperature parameter τ, which requires model-specific tuning and can significantly affect head selection quality.
- Hard Negative Quality Dependency: The method requires mining 49 hard negatives per query, and the quality of these negatives directly impacts the contrastive scoring reliability.

## Confidence
- High Confidence: The core mechanism of contrastive head selection and the empirical demonstration that <1% of heads can match or exceed full-head performance.
- Medium Confidence: The layer concentration claims and associated pruning efficiency benefits, as the 50% threshold appears safe but may be conservative for some models.
- Medium Confidence: The cross-lingual generalization claims, as the paper demonstrates performance on BEIR but doesn't systematically vary language pairs.

## Next Checks
1. **Temperature Sensitivity Analysis**: Systematically vary τ across {0.0001, 0.001, 0.01, 0.1, 1.0} on a held-out NQ validation set, measuring how nDCG@10 varies with temperature and plotting head selection stability across temperature values.

2. **Domain Transfer Robustness**: Design a controlled experiment where detection data comes from a different domain than evaluation data (e.g., detect on NQ, evaluate on SciDocs or ArguAna), measuring performance degradation compared to in-domain detection and testing whether fine-tuning the detection set on target-domain samples improves transfer.

3. **Negative Quality Impact**: Create controlled variations in hard negative quality by perturbing the embedding model similarity scores or using progressively weaker negative sets, measuring how S_CoRe scores and final re-ranking performance vary with negative quality to establish degradation thresholds.