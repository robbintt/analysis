---
ver: rpa2
title: 'Evaluating LLM Abilities to Understand Tabular Electronic Health Records:
  A Comprehensive Study of Patient Data Extraction and Retrieval'
arxiv_id: '2501.09384'
source_url: https://arxiv.org/abs/2501.09384
tags:
- task
- retrieval
- tasks
- llms
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates large language models' (LLMs) abilities
  to understand and process tabular electronic health records (EHRs) for patient data
  extraction and retrieval tasks. The research explores the impact of prompt structure,
  instruction types, context serialization methods, and in-context learning on task
  performance using the MIMICSQL dataset with Llama2 and Meditron models.
---

# Evaluating LLM Abilities to Understand Tabular Electronic Health Records: A Comprehensive Study of Patient Data Extraction and Retrieval

## Quick Facts
- arXiv ID: 2501.09384
- Source URL: https://arxiv.org/abs/2501.09384
- Reference count: 40
- Primary result: Optimal serialization and feature selection improve EHR task performance by up to 26.79% over naive approaches

## Executive Summary
This study systematically evaluates large language models' capabilities for processing tabular electronic health records (EHRs) across two fundamental tasks: patient data extraction and retrieval. The research investigates how prompt engineering components—specifically instruction types, context serialization methods, feature selection strategies, and in-context learning—affect model performance using the MIMICSQL dataset with Llama2 and Meditron models. The authors develop new benchmarks (MIMICask and MIMICsearch) and provide evidence-based guidelines for optimizing LLM prompting on EHR-related tasks.

## Method Summary
The study employs Llama2-7B and Meditron-7B models to process tabular EHR data converted into text through three serialization methods: simple templates (txt), HTML-tagged separation (xsep), and LLM-generated descriptions (sgen). Feature selection varies between using all features or averaged representations, with 60% random sampling as a baseline. The evaluation uses two newly created datasets derived from MIMIC-III: MIMICask for extraction (100 patients) and MIMICsearch for retrieval (4000 patients). In-context learning experiments employ a dense retriever (Dragon+ encoder) with FAISS indexing to select relevant demonstrations based on query similarity.

## Key Results
- Optimal feature selection and serialization methods enhance task performance by up to 26.79% compared to naive approaches
- Query-based in-context learning with relevant example selection improves data extraction performance by 5.95%
- Patient data retrieval is more challenging than extraction for LLMs, with retrieval benefiting more from zero-shot setups
- Self-generated EHR descriptions and query-based demonstration selection yield the best results

## Why This Works (Mechanism)

### Mechanism 1: Context Serialization Enhances Task Performance
The paper demonstrates that effective context serialization—converting tabular EHR data into linear text—improves LLM comprehension by reducing information loss during format conversion. Methods like self-generated descriptions (sgen) provide semantically rich representations that better align with LLM language understanding capabilities. This works by creating more structured, meaningful signals that help the model parse and reason about complex medical data. The mechanism may break when serialization introduces hallucinations or when verbose descriptions exceed model context windows.

### Mechanism 2: Query-Based In-Context Learning (ICL) Improves Data Extraction
ICL with query-based demonstration selection improves extraction by providing contextually relevant examples that guide the model's attention to pertinent features. When demonstrations are selected based on their similarity to the input query rather than randomly or by patient similarity, the model learns task-specific patterns more effectively. This is particularly valuable for fine-grained extraction tasks requiring precise information retrieval. The benefit diminishes when demonstrations are irrelevant, when too many examples cause context dilution, or when the model struggles with following in-context patterns.

### Mechanism 3: Task-Specific Challenges in EHR Retrieval vs. Extraction
Retrieval proves more challenging than extraction because it requires broader comprehension of EHRs to match queries against multiple patient profiles, demanding implicit relationship reasoning across sparse features. Extraction focuses on finding specific information within a known profile, which is more straightforward for LLMs. Retrieval performance benefits more from zero-shot approaches, as ICL examples can introduce confusing heterogeneous information that degrades performance. This task difference reflects the varying cognitive demands of matching versus locating information.

## Foundational Learning

- **Prompt Engineering for Tabular Data**: Understanding how to format and structure prompts is essential since the entire study examines how prompt components affect LLM performance on EHR tasks. Quick check: How does the order of prompt elements like `<Instruction [Demonstration] Context>` potentially influence model output?

- **In-Context Learning (ICL)**: ICL is central to the study's methodology for evaluating how demonstration quality and selection impact performance. Quick check: Why might query-based demonstration selection be more effective than patient-based selection for an extraction task?

- **Serialization of Structured Data (EHR to Text)**: Converting tabular EHRs to linear text is a critical preprocessing step tested across multiple methods. Quick check: What are the potential trade-offs between simple template-based serialization (txt) and more complex LLM self-generated descriptions (sgen)?

## Architecture Onboarding

- **Component map**: Raw EHR -> Feature Selection -> Serialization -> Combine with Query & Demonstrations -> LLM -> Answer
- **Critical path**: For extraction: Raw EHR -> Feature Selection -> Serialization (sgen) -> Combine with Query & Query-Based Demonstrations -> LLM -> Extracted Answer. For retrieval: Candidate EHRs -> Serialization -> Combine with Query (Zero-Shot or Re-ranking) -> LLM -> Relevance Score.
- **Design tradeoffs**:
  - Serialization: sgen provides rich context but is computationally expensive; txt is fast but may lose semantic nuance
  - Feature Selection: all features maximize context but risk token limits; averaging reduces length but loses temporal detail
  - ICL vs Zero-Shot: ICL improves extraction but often hurts retrieval due to potential confusion from heterogeneous examples
- **Failure signatures**:
  - Hallucination in sgen serialization creating fabricated features
  - Context truncation for patients with long longitudinal histories
  - ICL overload causing "No relevant" bias in retrieval tasks
- **First 3 experiments**:
  1. Establish zero-shot baseline using simple txt serialization and all features for both tasks
  2. Ablate serialization methods (txt, xsep, sgen) with fixed feature selection to identify performance gains
  3. Tune ICL for extraction using query-based demonstration retriever with varying example counts (k=1-3)

## Open Questions the Paper Calls Out

- Whether guidelines for feature selection and serialization generalize to LLM architectures beyond Llama2-based models or significantly larger parameter counts
- If optimal prompting strategies for extraction and retrieval can be effectively applied to predictive EHR-related tasks
- The extent to which extraction and retrieval tasks can serve as control tasks for assessing data privacy protection in LLMs
- The causal relationships between specific prompt elements when deviating from optimal settings

## Limitations

- The self-generated serialization method may introduce hallucinations or omit critical features, though hallucination rates aren't reported
- The study focuses exclusively on English-language EHRs from MIMIC-III, limiting generalizability to other languages and clinical contexts
- The comparison between Llama2 and Meditron conflates model architecture differences with fine-tuning effects, making it difficult to isolate performance sources

## Confidence

- **High Confidence**: The core finding that optimal serialization methods improve performance by up to 26.79% is well-supported by controlled ablation studies
- **Medium Confidence**: The 5.95% ICL improvement for extraction is demonstrated but may be task-specific with variable magnitude
- **Low Confidence**: The claim that retrieval is inherently more challenging than extraction may reflect implementation choices rather than fundamental task difficulty

## Next Checks

1. **Hallucination Audit**: Run the sgen serialization method on 100 random EHRs and have clinical experts verify whether generated descriptions accurately represent source data without fabrication

2. **Cross-Domain Transfer**: Test the optimal prompt configuration (Llama2 with all features + sgen serialization) on a non-ICU EHR dataset like eICU to assess generalizability beyond MIMIC-III's critical care context

3. **Fine-Tuning Isolation**: Compare zero-shot Llama2 performance against Meditron to determine baseline architecture difference, then measure how much of Meditron's advantage comes from clinical fine-tuning versus pretraining