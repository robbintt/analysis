---
ver: rpa2
title: 'Surgical-MambaLLM: Mamba2-enhanced Multimodal Large Language Model for VQLA
  in Robotic Surgery'
arxiv_id: '2509.16618'
source_url: https://arxiv.org/abs/2509.16618
tags:
- surgical
- mamba2
- visual
- arxiv
- scanning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Surgical-MambaLLM, the first method combining
  Mamba2 with LLM for Visual Question Localized-Answering (VQLA) in robotic surgery.
  The model addresses limitations in current approaches by using Mamba2's ability
  to capture cross-modal dependencies and perceive spatial information in surgical
  scenes.
---

# Surgical-MambaLLM: Mamba2-enhanced Multimodal Large Language Model for VQLA in Robotic Surgery

## Quick Facts
- arXiv ID: 2509.16618
- Source URL: https://arxiv.org/abs/2509.16618
- Reference count: 37
- Achieves 69.64% accuracy on EndoVis18-VQLA benchmark

## Executive Summary
This paper introduces Surgical-MambaLLM, the first method combining Mamba2 with a Large Language Model (LLM) for Visual Question Localized-Answering (VQLA) in robotic surgery. The model addresses limitations in current approaches by using Mamba2's ability to capture cross-modal dependencies and perceive spatial information in surgical scenes. The proposed Cross-modal Bidirectional Mamba2 Integration (CBMI) module leverages a novel Surgical Instrument Perception (SIP) scanning mode that scans surgical images radially from center outward, enhancing spatial understanding. Experimental results show the model outperforms state-of-the-art methods on both EndoVis18-VQLA and EndoVis17-VQLA datasets.

## Method Summary
Surgical-MambaLLM uses a two-stage training approach for VQLA in robotic surgery. Stage 1 freezes the InternLM-7B LLM and trains the vision encoder (CLIP-ViT-B/32), CBMI module (Mamba2-130M, 24 layers), projector, answer head, and location head. Stage 2 performs LoRA fine-tuning of the LLM only. The CBMI module employs a novel Surgical Instrument Perception (SIP) scanning mode that processes surgical images radially from center to periphery, improving spatial understanding. The model predicts both textual answers and bounding box locations for surgical questions. Training uses AdamW optimizer with learning rate 1e-5 for stage 1 and 1e-6 for stage 2, batch size 8 per GPU, and dropout 0.1 across 6× RTX 3090 GPUs.

## Key Results
- Achieves 69.64% accuracy, 41.10% F-score, and 80.27% mIoU on EndoVis18-VQLA
- Achieves 51.91% accuracy, 44.06% F-score, and 76.48% mIoU on EndoVis17-VQLA
- Outperforms state-of-the-art methods on both benchmark datasets
- Ablation studies confirm effectiveness of CBMI module, SIP scanning mode, and Mamba2 model

## Why This Works (Mechanism)
The model leverages Mamba2's selective state space capabilities to efficiently capture cross-modal dependencies between visual and textual modalities. The SIP scanning mode exploits the geometric structure of surgical scenes where instruments typically enter from periphery toward central surgical field, providing an inductive bias for spatial reasoning. The CBMI module performs bidirectional fusion of visual and textual features, allowing the model to better align spatial information with question semantics.

## Foundational Learning
- **Cross-modal dependency capture**: Mamba2 processes sequences with linear complexity, making it efficient for multimodal fusion compared to attention-based methods. Needed for real-time surgical applications.
- **Spatial scanning patterns**: SIP mode uses radial center-to-periphery scanning based on surgical scene geometry. Quick check: Verify instrument trajectories in dataset follow expected radial pattern.
- **Selective state spaces**: Mamba2 uses input-dependent state transitions unlike RNNs. Quick check: Compare feature evolution with and without Mamba2 parameters.
- **Two-stage training**: Freezing LLM during vision encoder training prevents catastrophic forgetting. Quick check: Monitor LLM outputs before and after stage 2.
- **LoRA fine-tuning**: Low-rank adaptation reduces parameters during LLM adaptation. Quick check: Verify memory savings versus full fine-tuning.
- **VQLA task formulation**: Joint prediction of answers and bounding boxes requires multi-task learning. Quick check: Monitor both task losses independently.

## Architecture Onboarding

Component map: Input Image + Question -> CLIP-ViT-B/32 -> Visual Features -> CBMI (Mamba2) -> Projector -> Answer Head + Location Head -> Output

Critical path: Visual feature extraction → CBMI fusion → Multi-task prediction. The CBMI module is the innovation point where Mamba2 processes cross-modal features.

Design tradeoffs: Mamba2 offers linear complexity vs Transformer attention's quadratic complexity, trading some modeling flexibility for efficiency. SIP scanning assumes radial instrument entry which may not hold for all surgical scenarios.

Failure signatures: CBMI convergence issues manifest as degraded feature alignment; LoRA instability shows as unstable LLM outputs during stage 2; SIP mode failure appears when instruments don't follow radial trajectories.

First experiments: 1) Validate CBMI output distributions match input modalities before and after fusion. 2) Test SIP scanning on images with non-radial instrument paths. 3) Compare Mamba2 vs Transformer CBMI using identical feature dimensions.

## Open Questions the Paper Calls Out
- How can the model's grounding capability be enhanced to match its high classification accuracy, particularly on external validation datasets where localization performance (mIoU) currently lags?
- Does the Surgical Instrument Perception (SIP) scanning mode, which relies on a radial "periphery-to-center" geometric assumption, perform effectively in surgical scenarios where instruments do not enter the frame radially from the edges?
- Does the integration of Mamba2 in the CBMI module provide computational efficiency or latency benefits over standard Transformer-based attention mechanisms in this specific multimodal setup?

## Limitations
- Missing LoRA configuration details (rank, alpha, target modules) prevent exact replication
- No computational efficiency metrics comparing Mamba2 vs Transformer alternatives
- Limited validation on diverse surgical scenarios that may violate SIP scanning assumptions

## Confidence
High: Novel CBMI module architecture and SIP scanning mode
Medium: Reported performance improvements over SOTA
Low: Exact reproducibility due to missing LoRA and training details

## Next Checks
1. Verify SIP scanning mode implementation produces the intended radial center-to-periphery sequence and validate its impact on spatial feature learning through controlled ablation
2. Test Stage 1 training convergence with different LoRA ranks (e.g., 8, 16, 32) to determine optimal configuration for the 7B LLM fine-tuning
3. Compare performance using alternative vision backbones (ViT vs ConvNeXt) while keeping Mamba2 and SIP scanning constant to isolate architectural contributions