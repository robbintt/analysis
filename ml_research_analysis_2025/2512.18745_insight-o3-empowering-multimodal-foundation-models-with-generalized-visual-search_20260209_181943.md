---
ver: rpa2
title: 'InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual
  Search'
arxiv_id: '2512.18745'
source_url: https://arxiv.org/abs/2512.18745
tags:
- fitted
- centre
- four
- three
- five
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: O3-Bench addresses the gap in multimodal benchmarks by focusing
  on reasoning-oriented tasks that require gathering and synthesizing information
  from multiple distinct image areas, such as analyzing composite charts and navigating
  maps. To solve these complex visual tasks, the authors propose InSight-o3, a multi-agent
  framework that splits the workload between a high-level visual reasoning agent (vReasoner)
  and a specialized visual search agent (vSearcher).
---

# InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search

## Quick Facts
- arXiv ID: 2512.18745
- Source URL: https://arxiv.org/abs/2512.18745
- Reference count: 40
- Primary result: InSight-o3-vS improves GPT-5-mini accuracy on O3-Bench from 39.0% to 61.5%

## Executive Summary
InSight-o3 addresses the challenge of multimodal reasoning tasks that require gathering and synthesizing information from multiple distinct image areas. The framework proposes a two-agent system where a high-level visual reasoning agent (vReasoner) delegates visual search tasks to a specialized agent (vSearcher). The vSearcher is trained via hybrid reinforcement learning to locate fuzzy or conceptual regions described in free-form language, going beyond simple object detection. When integrated as a plug-and-play component, InSight-o3 significantly improves the performance of state-of-the-art multimodal models across multiple benchmarks.

## Method Summary
The InSight-o3 framework trains a visual search agent (vSearcher) based on Qwen2.5-VL-7B-Instruct using hybrid reinforcement learning. The training combines out-of-loop RL with pre-generated (image, region description, bbox) triplets from InfographicVQA, and in-loop RL with dynamic region descriptions generated by a visual reasoning agent during training on image collages. The vSearcher is trained to respond to the `image_zoom_in_tool` by returning precise bounding boxes for free-form language descriptions. The hybrid reward function balances IoU-based localization accuracy with proper tool usage. The trained vSearcher is then integrated with various vReasoner models (GPT-5-mini, Gemini-2.5-Flash) to evaluate performance on the O3-Bench benchmark.

## Key Results
- InSight-o3-vS improves GPT-5-mini accuracy on O3-Bench from 39.0% to 61.5%
- Hybrid RL training (in-loop + out-of-loop) outperforms either component alone (86.9% vs 41.2% vs 61.5% across benchmarks)
- vSearcher generalizes to medical and UI domains despite being trained on infographics and collages
- The framework shows consistent improvements across different vReasoner models including Gemini-2.5-Flash and open-source alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling visual search from high-level reasoning via specialized agents improves performance on complex multimodal tasks that require gathering information from multiple distinct image areas.
- Mechanism: The InSight-o3 framework splits the workload between a vReasoner (handles task decomposition, planning, and synthesis) and a vSearcher (specializes in locating regions described in free-form language). The vReasoner iteratively issues natural-language region descriptions, and the vSearcher returns precise bounding boxes and cropped views. This separation allows each component to specialize—vReasoner focuses on reasoning logic while vSearcher focuses on spatial localization.
- Core assumption: The cognitive operations of visual search and reasoning are sufficiently distinct that specialization yields net gains despite coordination overhead.
- Evidence anchors:
  - [abstract] "As a plug-and-play agent, our vSearcher empowers frontier multimodal models (as vReasoners), significantly improving their performance... notably increasing GPT-5-mini's accuracy on O3-Bench from 39.0% to 61.5%."
  - [section] Section 4: "To address the issue, we propose INSIGHT-O3, a two-agent system that largely decouples the aforementioned burden by a visual reasoning agent (vReasoner) and a visual search agent (vSearcher)."
  - [corpus] "Rethinking Information Synthesis in Multimodal Question Answering A Multi-Agent Perspective" explores similar multi-agent approaches for multimodal QA, indicating this is a recognized design pattern.
- Break condition: The mechanism fails if coordination overhead dominates (many back-and-forth exchanges without progress) or if vSearcher cannot reliably interpret free-form descriptions.

### Mechanism 2
- Claim: A hybrid RL approach combining in-loop and out-of-loop training enables vSearcher to generalize across diverse visual search tasks.
- Mechanism: The training combines two components: (1) Out-of-loop RL uses pre-generated (image, region description, bbox) triplets with direct IoU supervision, which is computationally efficient. (2) In-loop RL dynamically generates region descriptions via vReasoner during training on image collages, using "pseudo IoU" rewards based on vReasoner feedback and final answer correctness. The combined reward function incentivizes both proper tool use and accurate localization.
- Core assumption: Combining static, pre-generated data with dynamic, reasoning-aligned data produces better generalization than either alone.
- Evidence anchors:
  - [section] Section 4.1: "For the out-of-loop RL, we use the following reward function: r = I[n_tool > 0] · (λ_format · r_format + λ_IoU · r_IoU)" and "For the in-loop RL component, we replace the IoU reward r_IoU with a pseudo IoU reward r̂_IoU ∈ {0, 1}."
  - [section] Table 5: Hybrid training (in-loop + out-of-loop) achieves 86.9% on V*-Bench, 41.2% on VisualProbe-Hard, and 61.5% on O3-Bench, outperforming either component alone.
  - [corpus] The approach builds on GRPO from DeepSeek-R1, but the specific hybrid design appears novel to this paper.
- Break condition: The mechanism fails if vReasoner provides noisy or inconsistent feedback during in-loop training, leading to unreliable pseudo-IoU rewards.

### Mechanism 3
- Claim: Training on "generalized visual search" enables vSearcher to locate fuzzy, conceptual, or relational regions that go beyond traditional object detection.
- Mechanism: Unlike prior work focused on locating discrete objects, this approach trains vSearcher on free-form language descriptions including relational regions ("the area to the left of the wooden chair"), conceptual regions ("the chart showing the company's revenue in the last decade"), and fuzzy boundaries. Training data includes high-density infographics (charts, tables, paragraphs) and image collages with embedded targets, forcing the model to reason semantically rather than relying on object-category detectors.
- Core assumption: The base vision-language model (Qwen2.5-VL-7B-Instruct) has sufficient grounding capabilities that RL training can sharpen for precise localization of abstract descriptions.
- Evidence anchors:
  - [abstract] "...generalized visual search—locating relational, fuzzy, or conceptual regions described in free-form language, beyond just simple objects or figures in natural images."
  - [section] Figure 13 shows example descriptions: "a small table-like block showing quarterly visitor arrival numbers, located at the lower-left portion of the image."
  - [corpus] "VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search" addresses complex visual reasoning, but the specific focus on generalized visual search appears unique here.
- Break condition: The mechanism fails if training data lacks diversity in description types or if the base model lacks sufficient visual-language grounding.

## Foundational Learning

- Concept: **Reinforcement Learning with Policy Optimization (GRPO)**
  - Why needed here: vSearcher is trained using a variant of GRPO. Understanding how policy gradient methods work, how advantages are estimated, and why GRPO uses group-relative normalization is essential to understand the training pipeline.
  - Quick check question: How does GRPO estimate advantages differently from standard REINFORCE-style policy gradient methods?

- Concept: **Visual Grounding in Multimodal Models**
  - Why needed here: The entire approach depends on connecting natural language to specific image regions. Understanding what visual grounding means, why it's harder for "generalized" vs. "object-centric" descriptions, and how multimodal models encode spatial information is fundamental.
  - Quick check question: What makes "the chart showing revenue trends" harder to localize than "the red car"?

- Concept: **Multi-Agent Coordination and Credit Assignment**
  - Why needed here: The framework is explicitly multi-agent. Understanding how agents communicate, how task decomposition reduces complexity, and why credit assignment is challenging when multiple agents contribute to a final outcome helps explain design choices.
  - Quick check question: Why is credit assignment harder when both vReasoner and vSearcher contribute to a correct/incorrect final answer?

## Architecture Onboarding

- Component map:
  - User query + image -> vReasoner -> (if visual search needed) region description -> vSearcher -> bbox + crop -> vReasoner evaluates -> (if needed) more searches -> final answer

- Critical path:
  1. User provides query + image to vReasoner
  2. vReasoner analyzes and decides if visual search needed; issues region description
  3. vSearcher localizes region, returns cropped view
  4. vReasoner evaluates crop, provides feedback, decides if more search needed
  5. Loop until sufficient evidence or tool-call limit (max 6 in experiments)
  6. vReasoner outputs final answer

- Design tradeoffs:
  - Multi-agent vs. single-agent: Specialization benefits vs. coordination overhead
  - Pre-generated vs. on-the-fly training data: Efficiency vs. distributional realism
  - Tool complexity: Only `image_zoom_in_tool` currently; richer toolkits would increase flexibility but complicate training
  - vReasoner choice: Strong proprietary models yield best results; open alternatives enable fully open systems at potential performance cost

- Failure signatures:
  - **vReasoner hallucination**: Pretends to see details not present or overrides visual evidence with pre-trained knowledge (Figure 20, Failure Case 1)
  - **vReasoner visual misperception**: Misreads visual information even with correct crops (Figure 20, Failure Case 2)
  - **vSearcher localization failure**: Cannot locate described region with complex structures (Figure 21, Failure Case 4)
  - **Improper delegation**: vReasoner attempts direct localization instead of delegating to vSearcher (Figure 21, Failure Case 3)

- First 3 experiments:
  1. **Baseline comparison**: Run vReasoner alone → vReasoner + untrained vSearcher → vReasoner + trained InSight-o3-vS on O3-Bench. Compare accuracy to quantify RL training benefit.
  2. **Ablation of hybrid RL**: Train separate vSearchers with only in-loop, only out-of-loop, and both combined. Evaluate to validate the hybrid design's contribution.
  3. **Cross-domain generalization**: Train on paper's data, evaluate on different domain (medical images, UI screenshots). Test the claim that vSearcher generalizes out-of-distribution.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the multi-turn interaction traces generated by the InSight-o3 framework effectively train a single, unified model to perform both visual reasoning and search?
  - Basis in paper: [Explicit] The paper notes that the resulting system "may help synthesize multi-turn supervised-finetuning (SFT) traces ..., paving the way for a larger, potentially unified model."
  - Why unresolved: The authors explicitly decouple the agents to simplify optimization (avoiding credit assignment issues) and use a frozen vReasoner. The efficacy of a unified architecture trained on these traces remains untested.
  - What evidence would resolve it: Training a single, large-scale multimodal model on the synthesized SFT traces and comparing its performance on O3-Bench against the multi-agent InSight-o3 system.

- **Open Question 2**: Does incorporating additional visual tools (beyond simple image cropping) significantly enhance the vSearcher's ability to perform generalized visual search?
  - Basis in paper: [Explicit] The authors state, "For simplicity, we only allow vSearcher to use the most essential image cropping tool. Our framework, however, does not impose such a constraint."
  - Why unresolved: The study isolates the cropping tool to demonstrate core capability, leaving the potential performance gains or latency costs of a richer toolkit (e.g., OCR, detection) unquantified.
  - What evidence would resolve it: An ablation study integrating a diverse set of vision tools into vSearcher and measuring the accuracy and efficiency changes on high-information-density tasks in O3-Bench.

- **Open Question 3**: How can the vReasoner be trained or prompted to robustly suppress internal world knowledge that conflicts with visual evidence retrieved by the vSearcher?
  - Basis in paper: [Inferred] Appendix D.4 identifies "Knowledge–Perception Conflict" as a failure mode where the vReasoner (GPT-5-mini) hallucinates visual details based on priors (e.g., adding a non-existent subway station) rather than trusting the visual crop.
  - Why unresolved: The current framework trains the vSearcher via RL but relies on a fixed proprietary vReasoner, which still exhibits significant hallucination errors despite receiving accurate crops.
  - What evidence would resolve it: A fine-tuning or RL alignment process for the vReasoner that specifically penalizes outputs contradicting the visual evidence provided in the context, followed by a reduction in the observed conflict-based error rate.

## Limitations
- Heavy reliance on proprietary models (GPT-5-mini, GPT-5-nano) as vReasoners limits reproducibility
- Out-of-distribution generalization claims based on only two domains with small sample sizes
- Hybrid RL training complexity may not generalize well to domains with different visual characteristics
- vReasoner hallucination remains a significant failure mode despite accurate visual crops

## Confidence

**High confidence**: Core mechanism of decoupling visual search from reasoning improves performance (39.0% → 61.5% on O3-Bench)

**Medium confidence**: Hybrid RL training design effectiveness, though specific design choices may be dataset-specific

**Low confidence**: Generalization claims beyond tested domains due to limited testing scope

## Next Checks

1. **Reproduce with open-source vReasoners**: Replace GPT-5-mini with open alternatives (e.g., Qwen2.5-VL-7B-Instruct) as the vReasoner to isolate vSearcher's contribution and enable full open-source reproduction.

2. **Cross-domain robustness testing**: Systematically evaluate vSearcher on 5+ diverse domains (medical imaging, UI screenshots, satellite imagery, etc.) with larger sample sizes to validate generalization claims beyond the two domains tested.

3. **Long-horizon task evaluation**: Test vSearcher's performance on tasks requiring >6 tool calls or complex multi-step reasoning chains to identify failure modes and determine if the current max tool-call limit is appropriate.