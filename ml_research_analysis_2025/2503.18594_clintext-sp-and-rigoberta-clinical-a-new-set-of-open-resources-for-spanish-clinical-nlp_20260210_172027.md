---
ver: rpa2
title: 'ClinText-SP and RigoBERTa Clinical: a new set of open resources for Spanish
  Clinical NLP'
arxiv_id: '2503.18594'
source_url: https://arxiv.org/abs/2503.18594
tags:
- clinical
- cases
- spanish
- corpus
- rigoberta
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ClinText-SP, the largest publicly available
  Spanish clinical corpus, and RigoBERTa Clinical, a state-of-the-art clinical encoder
  language model. The corpus was curated from diverse sources including medical journals,
  shared task datasets, and clinical knowledge repositories, containing 26M tokens
  across 37,077 samples.
---

# ClinText-SP and RigoBERTa Clinical: a new set of open resources for Spanish Clinical NLP

## Quick Facts
- arXiv ID: 2503.18594
- Source URL: https://arxiv.org/abs/2503.18594
- Authors: Guillem García Subies; Álvaro Barbero Jiménez; Paloma Martínez Fernández
- Reference count: 40
- Primary result: Introduces ClinText-SP corpus (26M tokens, 37K samples) and RigoBERTa Clinical model achieving state-of-the-art performance on Spanish clinical NLP benchmarks

## Executive Summary
This paper presents ClinText-SP, the largest publicly available Spanish clinical corpus, and RigoBERTa Clinical, a state-of-the-art clinical encoder language model. The corpus was curated from diverse sources including medical journals, shared task datasets, and clinical knowledge repositories. RigoBERTa Clinical was developed through domain-adaptive pretraining on this corpus, achieving significant improvements over existing models on multiple clinical NLP benchmarks. The model demonstrates superior performance compared to both Spanish-only and multilingual clinical models, ranking first in Nemenyi plot evaluations. Both the dataset and model are made publicly available to support further research in Spanish clinical NLP.

## Method Summary
The method involves creating a comprehensive Spanish clinical corpus (ClinText-SP) from multiple sources including medical journals, shared task datasets, and clinical knowledge repositories. The corpus was preprocessed with fuzzy deduplication, language filtering, and tokenization using the RigoBERTa 2 tokenizer with 128-token stride for long documents. RigoBERTa Clinical was developed through domain-adaptive pretraining using masked language modeling on this corpus, initialized from RigoBERTa 2 weights. The model was fine-tuned on downstream clinical NLP tasks including named entity recognition and multilabel classification, with checkpoint selection based on validation performance across multiple datasets.

## Key Results
- ClinText-SP contains 26M tokens across 37,077 samples from diverse clinical sources
- RigoBERTa Clinical achieves an average performance increase of 0.01 over existing models
- Ablation study confirms combined dataset provides best results, outperforming models trained on either clinical cases or shared task data alone
- Model ranks first in Nemenyi plot evaluations compared to both Spanish-only and multilingual clinical models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-adaptive pretraining from a strong general-language model outperforms training clinical models from scratch when clinical data is limited.
- Mechanism: Initialize with RigoBERTa 2 weights (already capturing Spanish linguistic structure), then continue masked language modeling on ClinText-SP. This preserves general language competence while shifting token distributions toward clinical vocabulary and document structure.
- Core assumption: The base model's general Spanish knowledge transfers to clinical Spanish; the 26M token corpus is sufficient for domain shift without catastrophic forgetting.
- Evidence anchors: [abstract] "RigoBERTa Clinical, developed through domain-adaptive pretraining on this comprehensive dataset, significantly outperforms existing models"; [Section 4.1] "Prior research indicates that adapting general-purpose models through domain-specific pretraining often yields better results than training specialized models from scratch with small data"; [corpus] No direct comparison to from-scratch training in this paper; claim relies on cited prior work [2]
- Break condition: If pretraining loss diverges or downstream validation metrics degrade consistently across tasks, the learning rate may be too high for stable adaptation.

### Mechanism 2
- Claim: Combining long-form clinical cases with shorter annotated corpora creates complementary learning signals.
- Mechanism: Journal clinical cases average 700-4000 tokens, enabling learning of narrative structure and complex clinical language; shared task corpora provide task-relevant entity patterns. Together they balance depth and breadth.
- Core assumption: Both document types share underlying clinical vocabulary but differ in register; the model can integrate both without interference.
- Evidence anchors: [Section 6] Ablation study shows RigoBERTa Clinical (combined data) outperforms models trained on either subset alone on Meddocan and LivingNER1; [Section 3.4] Table 1 shows token-per-sample variance: clinical cases (1364-4102) vs. shared tasks (52-1582); [corpus] Ablation only tested on 3 datasets; generalization to other task types is not validated
- Break condition: If performance degrades on short-document tasks after training, the long clinical cases may dominate gradient updates.

### Mechanism 3
- Claim: Downstream task validation selects better checkpoints than pretraining loss.
- Mechanism: Pretraining loss poorly predicts downstream performance; instead, fine-tune candidate checkpoints on validation datasets (livingner1, meddocan, socialdisner) and rank using weighted combination of average performance, stability, and tail metrics.
- Core assumption: The three validation datasets are representative of the broader benchmark distribution.
- Evidence anchors: [Section 4.4] "Given that pretraining loss does not reliably predict downstream performance [53], the best model was selected based on performance on downstream tasks"; [Section 4.4] Describes weighted ranking formula (0.8 weight on average performance, higher weights on stability metrics); [corpus] No analysis of whether validation set choice biases selection toward certain task types
- Break condition: If selected model underperforms on held-out task types not represented in validation, expand validation set diversity.

## Foundational Learning

- Concept: **Masked Language Modeling (MLM)**
  - Why needed here: Domain-adaptive pretraining uses MLM to adjust token representations without labeled data; understanding this clarifies why the model learns clinical vocabulary.
  - Quick check question: Can you explain why MLM is preferred over causal language modeling for encoder-style domain adaptation?

- Concept: **Subword Tokenization with Stride**
  - Why needed here: Clinical cases exceed 512 tokens; the 128-token stride strategy creates overlapping windows to preserve context. Engineers must understand this when debugging tokenization artifacts.
  - Quick check question: What information is potentially lost when a 4000-token document is chunked into 512-token segments with 128-token stride?

- Concept: **Fuzzy Deduplication (MinHash)**
  - Why needed here: Shared task corpora reuse foundational datasets; deduplication prevents train/validation leakage. Critical to understand before adding new data sources.
  - Quick check question: If two documents share 80% of text but differ in diagnosis, would MinHash-based deduplication remove them incorrectly?

## Architecture Onboarding

- Component map: RigoBERTa 2 weights -> ClinText-SP tokenization (stride=128) -> MLM pretraining (batch=32, lr=2e-5, 1.8 epochs) -> Downstream fine-tuning -> Task-specific classifiers
- Critical path: 1. Load RigoBERTa 2 weights -> 2. Tokenize ClinText-SP with stride=128 -> 3. Run MLM pretraining (batch=32, lr=2e-5, 1.8 epochs) -> 4. Validate on downstream tasks -> 5. Select checkpoint via weighted ranking -> 6. Fine-tune per task
- Design tradeoffs:
  - **Stride vs. truncation**: Stride=128 preserves context but increases compute; truncation loses information but is faster.
  - **Validation set size**: 3 datasets enable fast selection but may not represent full task distribution.
  - **Epochs (2) vs. overfitting risk**: Early stopping at 1.8 epochs prevents over-adaptation; assumes limited data requires conservative training.
- Failure signatures:
  - **SocialDisNER degradation**: Model loses social media language competence after clinical adaptation (observed in ablation: RigoBERTa 2 baseline outperforms adapted models). Mitigation: Consider mixed-domain continued pretraining.
  - **High variance across fine-tuning runs**: Check learning rate stability; ensure weight decay (0.1) is applied correctly.
  - **OOV token spikes**: Verify tokenizer handles clinical abbreviations; subword tokenization should split unknown terms.
- First 3 experiments:
  1. **Reproduce benchmark**: Fine-tune RigoBERTa Clinical on Meddocan using Appendix A hyperparameters; compare F1 to reported 0.982 to validate setup.
  2. **Ablation sanity check**: Train two mini-models using only journal cases vs. only shared task data; verify combined model outperforms both on at least 2/3 validation datasets.
  3. **Tokenizer stress test**: Run inference on a sample with heavy clinical abbreviations; inspect subword splits to confirm OOV handling matches expectations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can generative LLMs with few-shot learning strategies surpass fine-tuned encoder-based models like RigoBERTa Clinical on Spanish clinical NLP benchmarks while maintaining lower operational costs for real-world healthcare deployment?
- Basis in paper: [explicit] The authors state that "generative LLMs suggests that in the near future they may surpass fine-tuned encoder-based models in many clinical NLP tasks, especially with effective few-shot learning strategies. However, the high computational cost of these models remains a major limitation for real-world deployment in healthcare settings."
- Why unresolved: No comparative evaluation between RigoBERTa Clinical and generative LLMs was conducted in this study.
- What evidence would resolve it: A direct benchmark comparison between RigoBERTa Clinical and generative LLMs (e.g., Med-PaLM, MedAlpaca) on the same Spanish clinical NLP tasks, measuring both performance metrics and computational costs.

### Open Question 2
- Question: Would architectural innovations such as ModernBERT improve the efficiency and performance of Spanish clinical encoder models beyond the current diminishing returns observed with RigoBERTa-based approaches?
- Basis in paper: [explicit] The authors note "we are reaching a point of diminishing returns" and that "Recent advances, such as those proposed in ModernBERT, suggest promising modifications that could enhance the efficiency and adaptability of encoder-based models for domain-specific applications."
- Why unresolved: The current study only evaluated domain-adaptive pretraining without exploring architectural modifications.
- What evidence would resolve it: Training a Spanish clinical model using ModernBERT-style architecture and comparing its performance-efficiency trade-off against RigoBERTa Clinical on the same benchmarks.

### Open Question 3
- Question: What specific biases exist in ClinText-SP across data sources, and how do they affect downstream model performance on underrepresented clinical specialties or patient demographics?
- Basis in paper: [explicit] The authors acknowledge "there may still be biases in the data sources and gaps in domain coverage" and call for "expanding the dataset to include more diverse clinical texts."
- Why unresolved: No bias analysis or demographic breakdown of the corpus was conducted.
- What evidence would resolve it: A systematic analysis of specialty distribution, patient population representation, and geographic coverage in ClinText-SP, followed by stratified performance evaluation across these dimensions.

### Open Question 4
- Question: Can multimodal approaches integrating structured medical data (e.g., lab results, imaging metadata) with clinical text improve Spanish clinical NLP model robustness and applicability beyond text-only pretraining?
- Basis in paper: [explicit] The authors suggest "exploring multimodal approaches that integrate structured medical data, could further improve the robustness and applicability of Spanish clinical NLP models."
- Why unresolved: ClinText-SP and RigoBERTa Clinical are text-only resources with no structured data integration.
- What evidence would resolve it: Developing a multimodal Spanish clinical model that jointly processes clinical narratives and structured EHR data, then evaluating on tasks requiring integration of both modalities.

## Limitations

- The claim that domain-adaptive pretraining outperforms training from scratch has not been directly validated in this paper, relying instead on cited prior work.
- The ablation study showing complementary benefits from combining data sources only tested 3 datasets, raising questions about generalizability to other clinical NLP tasks.
- The model selection process using 3 validation datasets may introduce bias toward specific task types, as no analysis demonstrates these are representative of the broader benchmark distribution.

## Confidence

**High Confidence Claims:**
- The technical implementation of domain-adaptive pretraining using MLM on ClinText-SP corpus
- The quantitative results showing RigoBERTa Clinical outperforming existing Spanish clinical models on the tested benchmarks
- The corpus statistics and composition details (26M tokens, 37K samples from specified sources)

**Medium Confidence Claims:**
- The mechanism that combining long-form clinical cases with shorter annotated corpora provides complementary learning signals (supported by ablation but limited dataset scope)
- The claim that downstream task validation provides better checkpoint selection than pretraining loss (reasonable based on literature [53] but not directly validated)
- The ranking of RigoBERTa Clinical first in Nemenyi plot evaluations (statistically sound but dependent on benchmark choice)

**Low Confidence Claims:**
- The generalizability of performance improvements to clinical NLP tasks beyond the tested benchmarks
- The assumption that the 26M token corpus is sufficient for effective domain adaptation without catastrophic forgetting
- The long-term stability of the model's clinical domain competence across diverse clinical subdomains

## Next Checks

1. **Benchmark Expansion Validation**: Evaluate RigoBERTa Clinical on at least 3 additional Spanish clinical NLP tasks not used in model selection (e.g., clinical relation extraction, clinical question answering) to test generalizability beyond the current benchmark set.

2. **Pretraining Strategy Comparison**: Conduct a controlled experiment training a clinical model from scratch on ClinText-SP versus domain-adaptive pretraining from RigoBERTa 2 to directly validate the claimed superiority of the adaptation approach.

3. **Long-Document Context Analysis**: Systematically evaluate model performance on documents of varying lengths (short: <512 tokens, medium: 512-1024 tokens, long: >1024 tokens) to quantify information loss from the 128-token stride chunking strategy and identify potential context boundary issues.