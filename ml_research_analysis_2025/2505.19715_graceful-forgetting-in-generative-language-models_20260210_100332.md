---
ver: rpa2
title: Graceful Forgetting in Generative Language Models
arxiv_id: '2505.19715'
source_url: https://arxiv.org/abs/2505.19715
tags:
- forgetting
- learning
- unlearning
- task
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses negative transfer in generative language models
  by proposing a method to selectively unlearn harmful knowledge during fine-tuning.
  The approach, called Learning With Forgetting (LWF), uses self-generated texts to
  represent the knowledge to be forgotten, evaluates forgetting confidence using Fisher
  Information Matrix-weighted parameter updates, and periodically unlearns high-confidence
  data via gradient ascent.
---

# Graceful Forgetting in Generative Language Models

## Quick Facts
- arXiv ID: 2505.19715
- Source URL: https://arxiv.org/abs/2505.19715
- Authors: Chunyang Jiang; Chi-min Chan; Yiyang Cai; Yulong Liu; Wei Xue; Yike Guo
- Reference count: 12
- Primary result: LWF improves fine-tuning performance by selectively unlearning harmful knowledge in domain-specific QA tasks

## Executive Summary
This paper introduces Learning With Forgetting (LWF), a method to address negative transfer during fine-tuning of generative language models by selectively unlearning detrimental knowledge. The approach uses self-generated texts to represent knowledge to be forgotten, evaluates forgetting confidence through Fisher Information Matrix-weighted parameter updates, and periodically applies gradient ascent to unlearn high-confidence data. Experiments on five domain-specific QA tasks demonstrate consistent performance improvements across most settings, with particular success in mixed forgetting scenarios. The method also alters semantic and lexical properties of forgotten tasks and outperforms structural regulation methods when adapted to generative models.

## Method Summary
LWF operates through three main phases: representation of knowledge to be forgotten using self-generated texts, confidence evaluation via Fisher Information Matrix-weighted parameter updates, and periodic unlearning through gradient ascent. The method first generates texts that embody the unwanted knowledge, then assesses which parameters are most responsible for this knowledge using the Fisher Information Matrix. During fine-tuning, it periodically identifies high-confidence forgetting candidates and applies gradient ascent updates to reduce their influence. This selective approach aims to preserve useful knowledge while removing detrimental information that causes negative transfer during domain adaptation.

## Key Results
- LWF consistently improves fine-tuning performance across five domain-specific QA tasks
- Superior performance demonstrated in mixed forgetting scenarios where multiple knowledge types must be selectively removed
- Outperforms structural regulation methods (BSS, SRS) when adapted to generative model settings

## Why This Works (Mechanism)
LWF leverages the Fisher Information Matrix to identify which model parameters are most responsible for specific knowledge, allowing targeted unlearning rather than indiscriminate weight updates. By using self-generated texts as representations of knowledge to be forgotten, the method creates a proxy for the target forgetting data, enabling selective modification without requiring explicit negative examples. The periodic forgetting cycles prevent catastrophic forgetting of useful knowledge while maintaining the ability to remove harmful information that would otherwise cause negative transfer during domain adaptation.

## Foundational Learning
- **Fisher Information Matrix**: Quantifies parameter sensitivity to data; needed for identifying which parameters encode specific knowledge to target unlearning
- **Gradient ascent for unlearning**: Inverse of standard fine-tuning; needed to actively remove rather than add knowledge
- **Self-generated proxy data**: Creates representations of knowledge to forget without requiring explicit negative examples; needed when forgetting data is unavailable
- **Periodic forgetting cycles**: Balances unlearning with retention of useful knowledge; needed to prevent catastrophic forgetting
- **Confidence-weighted updates**: Prioritizes removal of high-confidence unwanted knowledge; needed for selective rather than indiscriminate unlearning
- **Domain adaptation transfer**: Addresses negative transfer where source task knowledge interferes with target task performance

## Architecture Onboarding

**Component Map**
Input -> Self-Generation Module -> Fisher Matrix Calculator -> Confidence Evaluator -> Gradient Ascent Unlearner -> Fine-tuned Model

**Critical Path**
1. Generate proxy texts representing knowledge to forget
2. Calculate Fisher Information Matrix to identify responsible parameters
3. Evaluate confidence scores for forgetting candidates
4. Apply gradient ascent updates to unlearn high-confidence data
5. Continue fine-tuning with remaining parameters

**Design Tradeoffs**
- Self-generation vs explicit forgetting data: Trade computation for flexibility when forgetting data is unavailable
- Periodic vs continuous unlearning: Trade precision for stability in knowledge removal
- Confidence weighting vs uniform forgetting: Trade computational overhead for selective precision

**Failure Signatures**
- Degraded generation quality from compounding errors in self-generated forgetting proxies
- Over-forgetting of useful knowledge when confidence thresholds are too low
- Under-forgetting leading to persistent negative transfer when confidence thresholds are too high
- Computational bottlenecks from repeated Fisher Matrix calculations

**3 First Experiments**
1. Compare LWF performance against standard fine-tuning on single-domain QA tasks
2. Test forgetting effectiveness when self-generated proxies contain errors or hallucinations
3. Measure parameter stability before and after forgetting cycles using parameter similarity metrics

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation scope focused primarily on domain-specific QA tasks without testing open-ended generation or non-QA text generation
- Reliance on self-generated data could introduce compounding errors if model quality degrades during multiple forgetting iterations
- Computational overhead from periodic forgetting cycles and Fisher Information Matrix calculations not thoroughly quantified
- Experiments assume access to target forgetting data, which may not be available in real-world scenarios

## Confidence
- Core forgetting mechanism effectiveness: High - Supported by consistent performance improvements across five QA tasks
- Method superiority over baselines: Medium - Demonstrated in specific settings but limited comparative analysis on broader tasks
- Generalization to non-QA generative tasks: Low - Not explicitly tested beyond QA benchmarks

## Next Checks
1. Evaluate LWF on open-ended text generation tasks (story completion, summarization) to assess cross-task generalizability
2. Measure computational overhead and wall-clock time compared to standard fine-tuning across different model scales
3. Test forgetting effectiveness when only partial or noisy examples of target knowledge are available