---
ver: rpa2
title: Text Simplification with Sentence Embeddings
arxiv_id: '2510.24365'
source_url: https://arxiv.org/abs/2510.24365
tags:
- sentence
- simplification
- text
- texts
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of sentence embeddings for text simplification,
  demonstrating that reconstructed embeddings preserve complexity levels. The authors
  learn a transformation between embeddings of complex and simple sentences using
  a small feed-forward neural network, achieving competitive results compared to larger
  Seq2Seq and LLM-based approaches.
---

# Text Simplification with Sentence Embeddings

## Quick Facts
- arXiv ID: 2510.24365
- Source URL: https://arxiv.org/abs/2510.24365
- Reference count: 0
- This paper explores using sentence embeddings for text simplification, achieving competitive results with a small feed-forward network compared to larger Seq2Seq and LLM-based approaches.

## Executive Summary
This paper presents a novel approach to text simplification by learning transformations between sentence embeddings representing complex and simple sentences. Using a small feed-forward neural network trained on sentence embeddings from the SONAR model, the authors demonstrate that they can effectively simplify text while preserving semantic content. The method shows competitive performance against larger models on multiple datasets and exhibits promising cross-lingual transfer capabilities, suggesting that learning transformations in sentence embedding space is a promising direction for developing compact yet powerful models for text simplification and potentially other natural language generation tasks.

## Method Summary
The method uses sentence embeddings from the SONAR model to represent text complexity levels, then learns a transformation between embeddings of complex and simple sentences using a small feed-forward neural network. The approach operates in fixed-size vector space, encoding input text into 1024-dimensional embeddings, applying a learned transformation via a 2-layer MLP, and decoding back to text. Training uses MSE loss on paired complex-simple sentence embeddings from WikiAuto, with evaluation on multiple datasets including an unseen medical simplification dataset (MedEASI) and cross-lingual applications in German and Spanish.

## Key Results
- The reconstruction test shows that encode-decode cycles preserve complexity levels (FKGL delta < 0.15, BLEURT > 0.8)
- The TSSE method achieves competitive FKGL scores (8.303 on ASSET test) compared to larger Seq2Seq (7.126) and LLM (9.022) approaches
- The method demonstrates partial zero-shot cross-lingual transfer, working better for German (SARI 37.198) than Spanish (SARI 25.996)

## Why This Works (Mechanism)

### Mechanism 1
- Sentence embeddings preserve complexity levels through encode-decode cycles
- When text passes through f(T) → E → f⁻¹(E) → T', the reconstructed text maintains similar FKGL, ARI, and CEFR scores to the original
- Core assumption: The SONAR encoder-decoder captures sufficient lexical and syntactic information to reconstruct readability-correlated features
- Evidence: FKGL delta between original and reconstructed complex sentences is only -0.075 (Asset) and -0.131 (WikiAuto); BLEURT scores 0.923 and 0.824 indicate high semantic preservation
- Break condition: If reconstruction quality degrades sharply for longer sentences or domain-specific vocabulary (medical, legal), the preserved complexity signal may not transfer to those domains

### Mechanism 2
- A small feed-forward network can learn a transformation g(Ec) ≈ Es that maps complex-to-simple embeddings
- The 1024-dimensional SONAR space is sufficiently structured that a 2-layer MLP (0.5–8M parameters) trained with MSE loss can approximate the embedding shift corresponding to simplification
- Core assumption: The complex→simple relationship is approximately continuous and smooth in embedding space, not requiring discrete symbolic operations
- Evidence: Final MSE loss decreases from 4.712×10⁻⁵ (K=256) to 3.618×10⁻⁵ (K=4096), showing the mapping is learnable; TSSE achieves FKGL 8.303 on Asset test vs. reference 8.154
- Break condition: If the embedding space were re-initialized or replaced with a non-multilingual encoder, the learned g(x) would likely fail to generalize

### Mechanism 3
- The learned transformation shows partial zero-shot cross-lingual transfer via SONAR's shared multilingual embedding space
- SONAR maps multiple languages into a common 1024-dimensional space. The g(x) learned on English data can be applied to German/Spanish embeddings, then decoded back to those languages
- Core assumption: Complexity-reduction directions in embedding space are at least partially language-independent
- Evidence: DEPlain (German) achieves SARI 37.198, comparable to English results; CLARA-MeD (Spanish) achieves only 25.996
- Break condition: If SONAR's multilingual alignment is weak for a target language, or if simplification conventions differ radically (e.g., agglutinative morphology), the transfer will degrade

## Foundational Learning

- **Concept: Sentence Embeddings**
  - Why needed here: The entire method operates in fixed-size vector space (1024-dim SONAR). Without understanding that embeddings compress semantic and syntactic information into continuous vectors, the g(x) transformation approach will not make sense
  - Quick check question: Can you explain why two sentences with similar meaning but different words might have similar embeddings?

- **Concept: Encoder-Decoder Reconstruction**
  - Why needed here: The method relies on the invertibility of embeddings—f⁻¹(f(T)) ≈ T. If this property didn't hold, the decoded simplifications would be incoherent
  - Quick check question: What happens to FKGL scores when you encode and decode the same sentence through SONAR?

- **Concept: Mean Squared Error Loss for Embedding Regression**
  - Why needed here: The MLP is trained to minimize MSE between predicted embeddings and reference simple-sentence embeddings. Understanding this objective clarifies what the model is optimizing
  - Quick check question: Why might MSE in embedding space not perfectly correlate with human judged simplification quality?

## Architecture Onboarding

- **Component map:** SONAR Encoder (frozen) → 1024-dim embedding → MLP g(x) → 1024-dim embedding → SONAR Decoder (frozen) → reconstructed text
- **Critical path:** 1) Pre-compute SONAR embeddings for all WikiAuto/ASSET complex-simple pairs 2) Train MLP to map Ec → Es with MSE loss 3) At inference: encode input → apply g(x) → decode output 4) Evaluate with FKGL, ARI, CEFR (complexity) and BLEURT, SARI, LENS (semantic)
- **Design tradeoffs:** Larger K (more hidden units) reduces loss but increases parameters (8M params at K=4096); GPU RAM limits K to 4096 in experiments; frozen encoder/decoder means you cannot adapt to new domains or languages without retraining SONAR (expensive); MLP simplicity vs. Seq2Seq expressiveness: TSSE is 1000x smaller but underperforms on semantic metrics (BLEURT 0.59 vs. 0.73 for LLM)
- **Failure signatures:** Hallucinations: Decoder generates plausible but factually incorrect text (Table 6 shows examples); Neural text degeneration: Repetitive or incoherent outputs for long sentences or many named entities; Cross-lingual transfer gap: Spanish results (SARI 25.996) much worse than German (37.198)
- **First 3 experiments:** 1) Replicate the complexity preservation test: Encode-decode 2000 ASSET sentences and verify FKGL/CEFR deltas match Table 1 (ΔFKGL < 0.15) 2) Train MLP with K=2048 on WikiAuto for 10K epochs; confirm validation MSE reaches ~3.9×10⁻⁵ as in Table 2 3) Run zero-shot inference on MedEASi (unseen medical domain); expect FKGL reduction from 13.736 → ~10.953 per Table 3, but note semantic metric gap (BLEURT 0.47 vs. 0.58 for Seq2Seq)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the cross-lingual transferability of the learned simplification transformation $g(x)$ correlate with the ethnolinguistic similarity between the source (English) and target languages?
- Basis in paper: The authors observe better performance on German (DE) than Spanish (ES) and hypothesize this is due to German being more ethnolinguistically similar to English, but state they "do not have sufficient data to fully investigate this claim"
- Why unresolved: The study only evaluated two non-English languages, providing an insufficient sample size to confirm if linguistic distance dictates the effectiveness of the transfer
- What evidence would resolve it: Applying the English-trained transformation to a diverse set of target languages with varying linguistic distances from English and correlating performance scores with ethnolinguistic metrics

### Open Question 2
- Question: Can increasing the neural network capacity beyond the 4096 hidden node limit significantly improve semantic preservation and reduce the performance gap with larger models?
- Basis in paper: The authors note they were limited by GPU RAM to a maximum of 4096 hidden nodes and "suspect that a larger hidden layer or a more complex neural architecture may have been suitable for further improving the performance"
- Why unresolved: Hardware constraints prevented the authors from exploring the scaling laws of the feed-forward network used to learn the embedding transformation
- What evidence would resolve it: Training the model with larger hidden dimensions or deeper architectures on high-memory GPUs and comparing the resulting BLEURT and SARI scores against the current baselines

### Open Question 3
- Question: Does the reduction in complexity achieved by the TSSE method translate to improved utility and readability for human readers compared to baseline models?
- Basis in paper: The Discussion section states: "We have not explicitly evaluated the degree to which simplified texts produced via sentence embeddings (or other means) are useful for a reader"
- Why unresolved: The evaluation relies solely on automated metrics (BLEURT, SARI, FKGL) which serve as proxies for quality but may not fully capture the human experience of the simplification or the impact of observed hallucinations
- What evidence would resolve it: Conducting human evaluation studies to assess the fluency, adequacy, and ease of reading of the TSSE outputs relative to the source texts and baseline simplifications

## Limitations

- The method depends on a frozen, proprietary embedding model (SONAR) and has only been evaluated on a single domain (WikiAuto/ASSET)
- Cross-lingual transfer results are uneven, with Spanish performance notably worse than German, suggesting limited zero-shot generalization
- The method may generate hallucinations and text degeneration, with these failure modes acknowledged but not quantified

## Confidence

- **High Confidence**: The ability to preserve complexity levels through encode-decode cycles (Table 1 FKGL/ARI deltas < 0.15; BLEURT > 0.8)
- **Medium Confidence**: The claim that a small MLP can learn the complex→simple embedding transformation with competitive results vs. larger models (BLEURT 0.59 vs. 0.73 for LLM)
- **Low Confidence**: The assertion that the method is broadly applicable to unseen domains (MedEASI) and languages (German/Spanish) without retraining (Spanish SARI 25.996 vs. German 37.198)

## Next Checks

1. **Reproduce the complexity preservation test (Section 4)**: Encode-decode 2000 ASSET sentences and verify FKGL/CEFR deltas match Table 1 (ΔFKGL < 0.15)
2. **Train MLP with K=2048 on WikiAuto for 10K epochs**: Confirm validation MSE reaches ~3.9×10⁻⁵ as in Table 2
3. **Run zero-shot inference on MedEASI (unseen medical domain)**: Expect FKGL reduction from 13.736 → ~10.953 per Table 3, but note semantic metric gap (BLEURT 0.47 vs. 0.58 for Seq2Seq)