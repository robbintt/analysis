---
ver: rpa2
title: 'Eidoku: A Neuro-Symbolic Verification Gate for LLM Reasoning via Structural
  Constraint Satisfaction'
arxiv_id: '2512.20664'
source_url: https://arxiv.org/abs/2512.20664
tags:
- cost
- structural
- verification
- context
- violation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to detecting hallucinations
  in LLM reasoning by reformulating verification as a Constraint Satisfaction Problem
  (CSP) rather than a probability-based confidence estimation. The key insight is
  that hallucinations often manifest as "smooth falsehoods" - statements that are
  semantically plausible but structurally disconnected from the context, which traditional
  likelihood-based methods cannot reliably detect.
---

# Eidoku: A Neuro-Symbolic Verification Gate for LLM Reasoning via Structural Constraint Satisfaction

## Quick Facts
- arXiv ID: 2512.20664
- Source URL: https://arxiv.org/abs/2512.20664
- Reference count: 25
- One-line primary result: Perfect detection of structurally unsupported but semantically plausible LLM conclusions (0% FTAR, 100% TTAR) on the Reasoning Gap Dataset

## Executive Summary
This paper introduces Eidoku, a lightweight System-2 verification gate that detects LLM hallucinations by reformulating verification as a Constraint Satisfaction Problem (CSP) over semantic structure. The key insight is that hallucinations often manifest as "smooth falsehoods"—semantically plausible statements that are structurally disconnected from context—which traditional likelihood-based methods cannot reliably detect. Eidoku achieves perfect performance on the Reasoning Gap Dataset by evaluating candidates based on accumulated semantic Violation Cost across three independent proxies: structural connectivity, geometric consistency, and logical entailment, using context-calibrated rejection thresholds.

## Method Summary
Eidoku implements verification through a lightweight System-2 gate that evaluates LLM reasoning candidates based on accumulated semantic Violation Cost across three independent proxies: graph connectivity (τstruct), embedding geometry (τcurv), and logical entailment (τlogic). The total cost J(S) is computed as the sum of normalized violation costs at each junction, and candidates are rejected if any junction exceeds a context-calibrated threshold τc. This threshold is derived from the observed Violation Cost distribution within each context, avoiding ad hoc hyperparameters. The approach treats verification as a feasibility problem (constraint satisfaction) rather than a probability estimation problem (likelihood maximization).

## Key Results
- Achieved 0% False Target Acceptance Rate (FTAR) and 100% True Target Acceptance Rate (TTAR) on the Reasoning Gap Dataset
- Outperformed baseline probability-based methods that struggle to maintain both metrics simultaneously
- Demonstrated that structural constraint satisfaction provides a complementary verification axis essential for robust reasoning
- Perfect performance indicates the approach effectively detects high-likelihood errors that violate contextual entailment

## Why This Works (Mechanism)

### Mechanism 1: Structural Violation Cost as Constraint Satisfaction
- **Claim:** Verification can be reformulated as constraint satisfaction (feasibility) rather than probability estimation (plausibility).
- **Mechanism:** Accumulates semantic violation cost across three independent proxies—graph connectivity, geometric consistency, and logical entailment. Total cost J(S) = Σ τ(Si,Sj|C). A candidate is rejected if any local junction exceeds the calibrated threshold.
- **Core assumption:** Hallucinations manifest as structural disconnection from context even when semantically plausible (high likelihood).
- **Evidence anchors:** [abstract]: "reformulate the verification of LLM reasoning as a Constraint Satisfaction Problem (CSP) operating independently of the generation likelihood"; [section 3.2]: "τ(Si,Sj|C) = 1/σL τstruct + 1/σG τcurv + 1/σI τlogic" with variance-balanced normalization; [corpus]: Neuro-Symbolic Verification (arxiv 2601.17789) addresses instruction-following violations but lacks the unified CSP cost formulation.

### Mechanism 2: Context-Calibrated Threshold Without Learning
- **Claim:** Per-context calibration of rejection threshold avoids ad hoc hyperparameters while adapting to local constraint rigidity.
- **Mechanism:** τc = min(τmax, max(τmin, Percentile_p(T(C)) × (1+δ))). Derived from observed violation cost distribution T(C) within each context, bounded by absolute safety ceiling and stability floor.
- **Core assumption:** The empirical violation cost distribution within a context meaningfully defines "structural outliers."
- **Evidence anchors:** [abstract]: "context-calibrated rejection threshold derived from the observed Violation Cost distribution, avoiding ad hoc hyperparameters"; [section 4.4]: Dual-stage thresholding with τmax safety ceiling and τmin stability floor; [corpus]: Weak/no direct corpus precedent for this specific percentile-based calibration approach.

### Mechanism 3: Optimization-Independent System-2 Gate
- **Claim:** A verification axis independent of the generation objective can detect "smooth falsehoods" that probability-based methods cannot.
- **Mechanism:** System 2 evaluates candidates post-hoc via J(S) ≤ τc (feasibility) separately from Pθ(y|x) optimization (plausibility). No gradient flow to System 1. Rejection is first-class outcome; if all candidates exceed τc, return null set.
- **Core assumption:** High-probability outputs can still violate structural constraints—hallucination is not a low-confidence phenomenon.
- **Evidence anchors:** [abstract]: "hallucinations often manifest as 'smooth falsehoods'—statements that are semantically plausible but structurally disconnected"; [section 2]: "hallucinations are not necessarily low-probability events... persist under temperature scaling or beam search"; [corpus]: Semantic Entropy (Farquhar et al., Nature 2024, cited in paper) captures geometric dispersion but lacks structural constraint enforcement.

## Foundational Learning

- **Constraint Satisfaction Problems (CSP)**
  - Why needed here: Core formalism replaces probability maximization with feasibility checking under structural constraints.
  - Quick check question: Can you explain why a feasibility check (J(S) ≤ τc) is fundamentally different from optimizing P(y|x)?

- **Knowledge Graph Connectivity & Reachability**
  - Why needed here: τstruct uses shortest-path distance; disconnection implies infinite cost barrier.
  - Quick check question: Given context "A→B, B→C" as a graph, what is τstruct for candidate "A→D" where D is unreachable?

- **Embedding Geometry (PCA Reconstruction, Mahalanobis Distance)**
  - Why needed here: τcurv penalizes candidates deviating from local tangent space; Mahalanobis scaling captures context-specific rigidity.
  - Quick check question: Why does high reconstruction error from a local PCA subspace indicate semantic inconsistency with context?

## Architecture Onboarding

- **Component map:**
  - System 1 (LLM Decoder): Generates K candidate sequences—unchanged, no fine-tuning required
  - Graph Extractor: Dependency parsing / OpenIE → contextual structure Γ(C)
  - Embedding Module: all-MiniLM-L6-v2 (d=384) → φ(·) for geometric cost
  - NLI Module: cross-encoder/nli-deberta-v3-small → entailment/contradiction scores
  - Cost Aggregator: Computes τ = w·(τstruct, τcurv, τlogic) with variance normalization
  - Threshold Calibrator: Derives τc from observed T(C) per context
  - Rejection Gate: Filters candidates; selects argmin J(S) among accepted

- **Critical path:**
  1. Extract context graph Γ(C) from input
  2. For each candidate, compute local τ at all junctions (sliding window W=10 for O(K·n·W) scalability)
  3. Calibrate τc from pooled violation costs
  4. Apply rejection rule; return lowest-cost accepted candidate or null set

- **Design tradeoffs:**
  - Sliding window trades full-context verification for linear scalability
  - Graph extraction failures → missing edges → conservative rejection (preserves safety/FTAR, hurts utility/TTAR)
  - τmax ceiling enables principled refusal rather than "least bad" selection
  - Static cost evaluation (vs. dynamic reasoning chain evolution)—accumulated J(S) serves as discrete proxy

- **Failure signatures:**
  - Null set returned: all candidates exceeded τmax—correct behavior under mode collapse
  - Over-rejection: τc too tight; check τmin floor is preventing collapse
  - Under-rejection: τc too permissive; verify τmax ceiling is set appropriately
  - Context error propagation: System enforces coherence with flawed context (by design—distinguish from knowledge oracle)

- **First 3 experiments:**
  1. **Reproduce RGD baseline:** Clone https://github.com/ShinobuMiya/Eidoku, run on N=1000 RGD samples, verify 0% FTAR / 100% TTAR.
  2. **Proxy ablation:** Disable each proxy independently (set corresponding weight to 0) to measure individual contribution; verify low pairwise correlation (|r|<0.3 per paper).
  3. **Threshold sensitivity:** Grid search p∈[85,99], δ∈[0.0,0.3]; confirm performance invariance for δ≥0.1 as reported in Figure 4.

## Open Questions the Paper Calls Out
None

## Limitations
- The approach depends critically on the empirical variance of each proxy being well-behaved across contexts—if σk→0 in low-variance scenarios, the normalized cost function becomes unstable.
- The independence assumption across τstruct, τcurv, and τlogic is asserted without validation—high correlation would cause double-counting of the same structural failure.
- The Graph Extractor component's failure modes are underspecified: while missing edges cause conservative rejection (preserving FTAR), false positive edges would enable undetected hallucinations.

## Confidence

- **High confidence**: The core insight that structural constraint satisfaction provides verification complementary to likelihood estimation is well-supported by the perfect RGD performance (FTAR=0.00, TTAR=1.00) and the clear theoretical distinction between plausibility (P(y|x)) and feasibility (J(S)≤τc).
- **Medium confidence**: The context-calibrated thresholding mechanism appears sound in principle, but the absolute bounds τmax/τmin are only conceptually described without concrete values, making implementation uncertain.
- **Low confidence**: The independence assumption across the three proxies lacks empirical validation, and the Graph Extractor's robustness to ambiguous parsing is not characterized.

## Next Checks

1. **Proxy correlation analysis**: Measure pairwise correlation coefficients across all τ values in RGD. If |r| > 0.3 for any pair, the additive cost formulation may be overweighting correlated failure modes.

2. **Graph extraction error characterization**: For a random sample of RGD false targets, manually verify that dΓ(A,D)=∞ (no path exists) to confirm the Graph Extractor is not creating spurious edges. Document the rate of extraction failures and their impact on rejection decisions.

3. **Coefficient sensitivity sweep**: Vary the proxy weights (αstruct, αcurv, αlogic) independently across a reasonable range (0.1 to 10.0) while holding other parameters fixed. Verify that performance remains stable and that the variance normalization adequately compensates for weight changes.