---
ver: rpa2
title: 'ClusterUCB: Efficient Gradient-Based Data Selection for Targeted Fine-Tuning
  of LLMs'
arxiv_id: '2506.10288'
source_url: https://arxiv.org/abs/2506.10288
tags:
- data
- training
- samples
- selection
- influence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ClusterUCB, an efficient gradient-based data
  selection framework for targeted fine-tuning of LLMs. The key idea is to leverage
  clustering and a modified UCB algorithm to reduce computational costs while maintaining
  selection effectiveness.
---

# ClusterUCB: Efficient Gradient-Based Data Selection for Targeted Fine-Tuning of LLMs

## Quick Facts
- arXiv ID: 2506.10288
- Source URL: https://arxiv.org/abs/2506.10288
- Reference count: 22
- One-line primary result: Achieves comparable accuracy to full-budget gradient-based methods using only 20% of computing budget through clustering and UCB-based selection

## Executive Summary
This paper introduces ClusterUCB, a gradient-based data selection framework for targeted fine-tuning of large language models. The method reduces computational costs by clustering training samples based on gradient similarities and using a modified UCB algorithm to efficiently allocate a limited computing budget across clusters. Experiments demonstrate that ClusterUCB achieves comparable results to full-budget methods (LESS, Dynamic) while using only 20% of the computing budget, with significant improvements over random selection and baseline reranking methods across four benchmarks (MMLU, TydiQA, GSM8k, HumanEval) using LLaMA-2-7B and Qwen2.5-3B models.

## Method Summary
ClusterUCB performs targeted fine-tuning data selection by first computing LoRA gradients for all training samples after a warmup checkpoint, then projecting these gradients to 8192-dimensional vectors via random projection. K-means clustering (k=150) groups samples by gradient cosine similarity, concentrating high-influence samples into clusters. A modified UCB algorithm with 5% cold-start budget allocation selects clusters to sample from, balancing exploration and exploitation. Influence is computed as normalized gradient inner product with validation gradients. After exhausting the 20% budget, samples are sorted by influence and the top 5% are selected for fine-tuning with 4 epochs, AdamW optimizer, and LoRA rank=128/α=512.

## Key Results
- Achieves comparable accuracy to full-budget gradient-based methods using only 20% computing budget
- Outperforms random selection and baseline reranking methods by significant margins
- 5% cold-start budget allocation improves sample-level and influence-level recall rates
- Optimal cluster count is K≥100, with K=150 used in main experiments
- Budget sweep shows 20% achieves best trade-off, with GSM8k/HumanEval peaking at 10% and degrading at 30%

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Based Clustering Concentrates High-Influence Samples
Clustering training data by gradient cosine similarity concentrates high-influence samples into a subset of clusters, enabling targeted sampling. Data influence is computed as cosine similarity between training gradients and validation gradients. Samples with similar gradients have similar influence potential, so K-means clustering groups them accordingly. This assumes gradient relationships remain stable through training.

### Mechanism 2: Modified UCB for Budget-Constrained Cluster Selection
A modified Upper Confidence Bound algorithm allocates limited computing budget across clusters by treating each cluster as a bandit arm. Instead of classic UCB (mean reward), ClusterUCB uses Uc = μ̂c + β·σ̂c to approximate a threshold where influence probability is equal across clusters. This balances exploration and exploitation through historical influence data.

### Mechanism 3: Cold Start Reduces Early-Stage Regret
Allocating 5% of budget proportionally across all clusters before UCB-driven selection improves final data quality. Pure UCB would exploit based on noisy initial estimates, while cold start forces proportional exploration across all clusters, providing initial distribution estimates for better UCB decisions.

## Foundational Learning

- **Gradient-Based Data Influence Approximation**: Needed because ClusterUCB builds on I(xᵢᵗʳ, xⱼᵛ) ≈ ⟨∇L(xⱼᵛ; θ), Γ(xᵢᵗʳ; θ)⟩. Quick check: Given training gradient g_tr and validation gradient g_val, how do you compute their influence score?

- **Multi-Armed Bandit / Upper Confidence Bound (UCB)**: Needed because inter-cluster selection is framed as a bandit problem. Understanding exploration-exploitation tradeoffs explains cold start and modified UCB importance. Quick check: In classic UCB, why does the algorithm sometimes select arms that haven't performed well historically?

- **K-Means Clustering on High-Dimensional Vectors**: Needed because gradients are projected to 8192-dim vectors via random projection before K-means clustering on cosine similarity. Quick check: Given two gradient vectors, how would you compute their cosine similarity? What does 0.9 vs 0.1 mean practically?

## Architecture Onboarding

- **Component map**: Warmup training -> Compute ALL training gradients -> Random projection (8192-dim) -> K-means clustering (k=150) -> Modified UCB scheduler -> Influence calculator -> Final selector

- **Critical path**: 1) Warmup training (20 steps) -> checkpoint 2) Compute ALL training gradients once 3) Cluster samples by gradient similarity 4) Cold start: draw 5% samples proportionally from each cluster 5) UCB loop: select cluster by max Uc, draw sample, compute influence, update stats 6) Sort all samples by influence, return top 5%

- **Design tradeoffs**: K (clusters): Too few → poor separation; too many → unreliable statistics. Cold start ratio: 0% → poor estimates; 100% → no exploitation. Budget B: 10-20% sufficient; more doesn't always help. β: Set to 1; controls weight on distribution spread vs mean.

- **Failure signatures**: Low Rinf → cold start ratio too low or clustering invalid. High variance across seeds → budget too low or validation set unrepresentative. No improvement over random → gradients not informative for target; wrong features; bad validation data.

- **First 3 experiments**: 1) Validate gradient-influence correlation: On 1000 samples, verify high-gradient-similarity samples have similar influence scores. 2) Ablate cold start: Run pcs% ∈ {0%, 5%, 25%} on single task. Plot Rs and Rinf. 3) Budget sweep: Run B ∈ {10%, 20%, 30%} end-to-end. Compare to random and full-budget baseline.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the ClusterUCB framework be adapted to account for mutual influences within data subsets rather than evaluating individual samples independently? The authors note gradient-based methods typically neglect mutual influences and that considering groups of data samples as arm-drawing rewards is the "next step of our work."

- **Open Question 2**: Can more advanced clustering algorithms significantly improve the performance of ClusterUCB compared to the K-means baseline? The paper notes that K-means is the "simplest clustering algorithm" used in this study, and "Better clustering might also improve the performance."

- **Open Question 3**: Does dynamically updating clusters during training provide sufficient performance gains to justify the additional computational cost? Appendix B notes that recall rates decline as training progresses, suggesting updating clusters could help, but the authors chose not to implement this to avoid extra consumption.

## Limitations
- Gradient-based influence correlations may not remain stable across the full training trajectory, though Appendix B shows declining but acceptable recall rates
- The modified UCB heuristic (μ̂ + βσ̂) assumes influence distributions are approximately symmetric and unimodal, which may not hold for all tasks
- Generalizability across different model sizes, pretraining domains, and fine-tuning objectives remains untested beyond the four benchmarks and specific model sizes evaluated

## Confidence
- **High Confidence**: Experimental results showing 20% budget achieving comparable accuracy to full methods, effectiveness of clustering (K≥100 optimal), and cold start improvement (5% optimal)
- **Medium Confidence**: UCB mechanism's theoretical justification and stability of gradient-based clustering over training
- **Low Confidence**: Generalizability claim across all LLM fine-tuning scenarios beyond tested benchmarks and model sizes

## Next Checks
1. **Gradient Stability Validation**: Track correlation decay between early and late training gradients across all clusters to quantify relationship between gradient drift and accuracy degradation.
2. **UCB Distribution Validation**: For each cluster, plot the empirical influence distribution and compare against the UCB's μ̂ + βσ̂ threshold to verify the heuristic approximates the target tail probability.
3. **Budget-Aware Scalability Test**: Run experiments with B ∈ {10%, 40%, 60%} on all four benchmarks to identify whether 20% is truly optimal or if certain tasks benefit from more aggressive sampling.