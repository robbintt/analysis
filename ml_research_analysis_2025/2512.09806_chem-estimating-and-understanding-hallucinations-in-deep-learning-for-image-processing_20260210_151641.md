---
ver: rpa2
title: 'CHEM: Estimating and Understanding Hallucinations in Deep Learning for Image
  Processing'
arxiv_id: '2512.09806'
source_url: https://arxiv.org/abs/2512.09806
tags:
- image
- hallucination
- hallucinations
- networks
- u-shaped
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new method called CHEM to measure hallucinations
  in deep learning image processing. CHEM uses wavelet and shearlet transforms to
  detect texture-level inconsistencies, and conformalized quantile regression to quantify
  hallucination levels without assuming specific data distributions.
---

# CHEM: Estimating and Understanding Hallucinations in Deep Learning for Image Processing

## Quick Facts
- **arXiv ID:** 2512.09806
- **Source URL:** https://arxiv.org/abs/2512.09806
- **Reference count:** 40
- **Key outcome:** Introduces CHEM method to measure hallucinations in DL image processing using wavelet/shearlet transforms and conformal quantile regression, revealing accuracy-hallucination tradeoffs

## Executive Summary
This paper introduces CHEM (Conformal Hallucination Estimation Metric), a novel method for detecting and quantifying hallucinations in deep learning image processing. CHEM uses wavelet and shearlet transforms to detect texture-level inconsistencies in the transform domain, combined with conformalized quantile regression to quantify hallucination levels without assuming specific data distributions. Tested on astronomical image deconvolution, CHEM reveals a fundamental tradeoff between accuracy (MSE) and hallucination levels, particularly in U-shaped networks. The study provides both practical detection tools and theoretical insights into why deep learning models generate unrealistic textures.

## Method Summary
CHEM detects hallucinations by analyzing transform-domain coefficients rather than pixel space. It applies Discrete Wavelet Transform (DWT) or Discrete Shearlet Transform (DST) to decompose images, then uses Conformalized Quantile Regression (CQR) with a calibration set to construct confidence intervals for each coefficient. The hallucination score $H(\Phi)$ measures deviations beyond these intervals. The method is tested on astronomical image deconvolution using U-Net, SwinUNet, and Learnlets architectures trained on CANDELS dataset with Tikhonet preprocessing. Training uses 500 epochs with cosine annealing learning rate schedule, and CHEM validation uses separate calibration splits.

## Key Results
- CHEM effectively identifies texture-level hallucinations that MSE metrics miss, revealing accuracy-hallucination tradeoffs
- Shearlets outperform wavelets for detecting elongated structures due to directional sensitivity
- U-shaped networks show higher hallucination rates due to approximation theory limits and finite capacity constraints
- ℓ₁ loss reduces hallucinations compared to ℓ₂ but may trade off pixel accuracy
- Hallucination levels correlate with image complexity and network depth

## Why This Works (Mechanism)

### Mechanism 1
Texture-level inconsistencies (hallucinations) are more effectively detected in the transform domain (wavelet/shearlet) than in pixel space. The method applies DWT or DST to decompose images into directional subbands, calculating hallucination scores by comparing predicted coefficients against calibrated confidence intervals in this domain, isolating implausible high-frequency details. Core assumption: hallucinations manifest as structural or textural artifacts deviating from valid coefficient distributions. Evidence: [Abstract] and [Section 3.3] define the metric in transform space. Break condition: if hallucinations occur as low-frequency distortions rather than localized texture artifacts.

### Mechanism 2
Distribution-free uncertainty quantification (via Conformalized Quantile Regression) allows rigorous hallucination detection without assuming specific data distributions. The algorithm uses a calibration set to learn mapping $\hat{R}(X)$ defining confidence interval radius for each transform coefficient, guaranteeing true value lies within interval $B_\alpha$ with probability $1-\alpha$. Core assumption: calibration dataset is exchangeable with test data. Evidence: [Section 3.3] provides the coverage guarantee equation. Break condition: if calibration set is small or unrepresentative, intervals become unreliable.

### Mechanism 3
Hallucinations in U-shaped networks are theoretically driven by the gap between model's finite capacity and complexity of target mapping (approximation error). Theoretical analysis shows error bounded by model parameters, depth, and image smoothness. Limited capacity forces networks to "hallucinate" features to minimize loss on complex mappings. Core assumption: underlying mapping satisfies Lipschitz continuity. Evidence: [Section 4] presents the approximation theory bounds. Break condition: if network is severely over-parameterized relative to data dimensionality, capacity-related hallucination term approaches zero.

## Foundational Learning

- **Concept:** Wavelets and Shearlets (Sparse Representations)
  - **Why needed here:** CHEM operates entirely in wavelet/shearlet domain. Understanding how transforms separate images into frequency subbands is required to interpret texture-level detection.
  - **Quick check question:** If an image has a smooth gradient, would this appear in high-frequency "detail" coefficients or low-frequency "approximation" coefficients?

- **Concept:** Conformal Prediction
  - **Why needed here:** CHEM uses Conformalized Quantile Regression to construct confidence ball $B_\alpha$. Without this concept, statistical guarantee and calibration role are unclear.
  - **Quick check question:** Does conformal prediction require data to follow Gaussian distribution to provide valid confidence intervals?

- **Concept:** Approximation Theory (Modulus of Continuity)
  - **Why needed here:** Section 4 links hallucinations to smoothness of input function via $\omega_f(r)$. Grasping this explains why "complex" inputs trigger higher hallucination bounds.
  - **Quick check question:** Does larger modulus of continuity $\omega_f(r)$ indicate smoother function or more complex/rough one?

## Architecture Onboarding

- **Component map:** Input X -> Backbone (U-Net/SwinUNet/Learnlets) -> Transform (DWT/DST) -> Calibrator (CQR) -> CHEM Score
- **Critical path:** Method requires held-out calibration set. Do not train CQR step on training set. Flow: Train Model -> Extract Coefficients on Calibration Set -> Compute Quantiles $\lambda_j$ -> Apply to Test Set.
- **Design tradeoffs:**
  - Dictionary Choice: Haar is fast; Shearlets are anisotropic (better for edges) but heavier
  - Loss Function: $\ell_1$ loss may lower hallucinations vs $\ell_2$ for U-Nets, but might trade off pixel-accuracy
  - Threshold $\theta$: Clipping hallucination score prevents outliers from skewing average metric but may hide severe local hallucinations
- **Failure signatures:**
  - High MSE / Low CHEM: Model failing to reconstruct but not "hallucinating" (e.g., outputting blur)
  - Low MSE / High CHEM: "Super-resolution" pathology where model invents plausible textures
  - CHEM Saturation: Insufficient calibration data makes $\hat{R}(X)$ uninformative
- **First 3 experiments:**
  1. Baseline Calibration: Implement CHEM on trained U-Net using calibration split. Verify coverage guarantee holds.
  2. Dictionary Ablation: Run CHEM using Haar vs. db8 vs. Shearlets on same predictions to visualize which textures are flagged.
  3. Robustness Test: Perturb input PSF and plot MSE vs. CHEM curve to confirm accuracy-hallucination tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
Can an adaptive, data-driven dictionary selection mechanism outperform fixed wavelet/shearlet bases for hallucination detection in diverse image domains? Basis in paper: [explicit] Authors state plan to enable dictionaries to automatically adjust to relevant images. Why unresolved: Current implementation relies on predefined dictionaries limiting efficiency for images with features poorly represented by these bases. What evidence would resolve it: Comparative benchmarks on diverse datasets showing higher detection fidelity using learned dictionaries versus fixed bases.

### Open Question 2
What specific learning dynamics cause the observed trade-off where minimizing training loss leads to increased hallucination levels? Basis in paper: [explicit] Limitations section notes need to expand analysis of causes behind performance-hallucination tradeoff phenomenon. Why unresolved: While paper empirically observes inverse correlation, it does not isolate theoretical mechanism driving this during optimization. What evidence would resolve it: Framework or ablation study identifying if trade-off is driven by loss function properties, model capacity constraints, or spectral bias.

### Open Question 3
Do approximation theory error bounds hold for complex, non-astronomical data like medical imaging? Basis in paper: [inferred] Paper validates only on CANDELS but claims applicability to medical imaging. Theoretical bounds rely on Assumptions regarding Lipschitz continuity and image smoothness. Why unresolved: Uncertain if bounds accurately predict hallucination behavior in medical images which may violate assumed smoothness or texture properties. What evidence would resolve it: Application to medical datasets to verify predicted error terms align with observed hallucinations.

## Limitations
- Reliance on specific transform representations (wavelet/shearlet) may not capture all artifact types, particularly global distortions
- Theoretical bounds assume Lipschitz continuity which may not hold for all image processing tasks
- Calibration set requirements could be prohibitive for small datasets
- Computational overhead of transform operations and conformal calibration may limit scalability

## Confidence
- Mechanism 1 (Transform-domain detection): High - well-supported by transform theory and empirical results
- Mechanism 2 (Conformal calibration): Medium - theoretically sound but sensitive to calibration data quality
- Mechanism 3 (U-shaped architecture limitations): High - grounded in established approximation theory

## Next Checks
1. Test CHEM's sensitivity to distribution shift by evaluating on astronomical images from different survey than CANDELS
2. Compare CHEM scores across different transform dictionaries (Haar, db8, shearlets) on same predictions to validate dictionary-dependence
3. Evaluate whether CHEM correlates with downstream task performance degradation rather than just coefficient deviation