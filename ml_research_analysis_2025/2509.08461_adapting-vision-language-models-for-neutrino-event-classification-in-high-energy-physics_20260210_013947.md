---
ver: rpa2
title: Adapting Vision-Language Models for Neutrino Event Classification in High-Energy
  Physics
arxiv_id: '2509.08461'
source_url: https://arxiv.org/abs/2509.08461
tags:
- vision
- neutrino
- llama
- classification
- event
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the application of vision-language models
  (VLMs) for classifying neutrino interactions in high-energy physics experiments,
  specifically comparing a fine-tuned LLaMA 3.2 Vision model against conventional
  CNN architectures. Using simulated pixelated detector data from a liquid argon time
  projection chamber, the research demonstrates that VLMs can outperform CNNs in classification
  accuracy while providing interpretable, physics-grounded explanations for predictions.
---

# Adapting Vision-Language Models for Neutrino Event Classification in High-Energy Physics

## Quick Facts
- arXiv ID: 2509.08461
- Source URL: https://arxiv.org/abs/2509.08461
- Reference count: 40
- Primary result: LLaMA 3.2 Vision model achieved 0.87 accuracy vs CNN's 0.68 on neutrino event classification, with interpretable explanations

## Executive Summary
This study investigates vision-language models (VLMs) for classifying neutrino interactions in high-energy physics experiments. Using simulated pixelated detector data from liquid argon time projection chambers, the research compares a fine-tuned LLaMA 3.2 Vision model against conventional CNN architectures. The VLMs achieved superior classification accuracy while providing interpretable, physics-grounded explanations for predictions, demonstrating better generalization under degraded detector conditions.

## Method Summary
The study fine-tuned LLaMA 3.2 Vision 11B with QLoRA adapters (rank 8) on simulated neutrino event pixel maps from liquid argon time projection chambers. The model processed paired 512×512 grayscale XZ and YZ views of particle interactions, using constrained decoding to enforce structured classification outputs with natural language explanations. A CNN baseline using a Siamese MobileNetV2-style architecture was trained separately for comparison. Both models were evaluated on 3-class classification (νe CC, νµ CC, NC) using standard metrics including accuracy, precision, recall, and AUC-ROC.

## Key Results
- LLaMA 3.2 Vision achieved accuracy of 0.87 versus CNN's 0.68
- VLMs demonstrated better generalization at half resolution (0.85 vs 0.49 accuracy)
- VLMs generated interpretable physics explanations referencing event topology features

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** VLMs transfer pretrained visual-linguistic representations to physics detector data with minimal adaptation.
- **Mechanism:** The LLaMA 3.2 Vision model's vision encoder (ViT-style) has learned hierarchical visual features from large-scale image-text pretraining. When fine-tuned via QLoRA on neutrino pixel maps, these features adapt to detect event topologies while preserving general visual reasoning capabilities.
- **Core assumption:** Pretrained visual representations contain transferable structures applicable to sparse, domain-specific detector images.
- **Evidence anchors:** [abstract] "VLMs can outperform CNNs, while also providing greater flexibility in integrating auxiliary textual or semantic information"
- **Break condition:** If detector images have fundamentally different statistical structure from natural images, transfer benefits may degrade significantly.

### Mechanism 2
- **Claim:** Constrained decoding with phrasal constraints produces reliable classification while enabling natural language explanations.
- **Mechanism:** The inference pipeline uses constrained beam search to force output format ("I classify the pixel maps as [label]"), ensuring machine-readable predictions. The model then generates free-form explanations referencing physics concepts learned during instruction tuning.
- **Core assumption:** The VLM's internal representations align language tokens with visual features in a way that produces causally meaningful explanations.
- **Evidence anchors:** [section 2.2.3] "we enforced that the output must begin with a fixed phrase... followed by a token sequence corresponding exclusively to one of the target class labels"
- **Break condition:** If explanations are post-hoc rationalizations uncorrelated with actual decision pathways, interpretability benefits are illusory.

### Mechanism 3
- **Claim:** Large-scale pretraining induces robustness to input degradation that specialized CNNs lack.
- **Mechanism:** VLMs pretrained on diverse visual data develop feature hierarchies tolerant to resolution changes, noise, and distribution shifts. When tested on half-resolution images, the fine-tuned VLM maintained 0.85 accuracy vs. CNN's 0.49.
- **Core assumption:** Robustness stems from pretraining diversity rather than architecture alone.
- **Evidence anchors:** [section 3] "generalization testing by running inference... on neutrino event pixel maps downsampled to half the original resolution... LLaMA 3.2 Vision... achieved an accuracy, precision, and recall of 0.85, compared to 0.49 for the CNN"
- **Break condition:** If degradation type differs qualitatively from pretraining corruptions, robustness may not transfer.

## Foundational Learning

- **Concept: Liquid Argon Time Projection Chamber (LArTPC) Data Representation**
  - **Why needed here:** Input data consists of 2D projections (XZ and YZ views) of 3D particle interactions, downsampled to 512×512 grayscale pixel maps. Understanding that brightness represents energy deposition and that track morphology distinguishes event types is essential for interpreting model outputs.
  - **Quick check question:** Given two orthogonal 2D projections of a muon track, how would you identify the distinguishing visual features versus an electron shower?

- **Concept: Parameter-Efficient Fine-Tuning (QLoRA)**
  - **Why needed here:** The 11B parameter model is quantized to 4-bit precision with LoRA adapters (rank 8) injected into attention and MLP modules. This reduces memory from prohibitive levels to ~25GB, enabling training on 4×A6000 GPUs.
  - **Quick check question:** If LoRA rank were increased from 8 to 64, what trade-offs would you expect in memory usage, training time, and risk of catastrophic forgetting?

- **Concept: Constrained Decoding for Classification**
  - **Why needed here:** VLMs are autoregressive text generators; without constraints, they may produce verbose or variable outputs. Phrasal constraints enforce structured output ("I classify the pixel maps as [label]") while confidence extraction uses log-probabilities at the label token position.
  - **Quick check question:** Why extract confidence from the first token of the class label rather than averaging across all tokens in the label phrase?

## Architecture Onboarding

- **Component map:** Paired 512×512 grayscale images (XZ and YZ views) + system prompt (physics context) + user prompt (classification instruction) → Vision Encoder (ViT-style) → Language Decoder (Transformer 11B, 4-bit quantized) with QLoRA adapters → Constrained beam search → Class label + optional explanation → Confidence extraction (temperature-scaled softmax)

- **Critical path:** Data preprocessing: GENIE simulation → GEANT4 → voxel smearing → 2D projection → downsampling → 512×512 crops → Training: QLoRA fine-tuning (1 epoch, lr=2e-4, batch=8 effective) → Inference: Load base model + adapters → constrained decoding → confidence extraction

- **Design tradeoffs:** VLM vs CNN: 25× memory (25GB vs 1GB), 165× slower inference (3.3s vs 20ms), but +19% accuracy and natural language explanations; Quantization: 4-bit reduces memory but may limit precision; LoRA rank 8: Low rank limits adaptation capacity but preserves pretrained knowledge

- **Failure signatures:** Low confidence on clear events (adapter weights not loaded correctly or temperature scaling misconfigured); Verbose/malformed outputs (constrained decoding not applied); Poor NC discrimination (neutral current events lack distinctive topology)

- **First 3 experiments:** 1) Baseline reproduction: Train CNN and VLM on same data split, verify accuracy gap (0.68 vs 0.87) and confirm constrained decoding produces parseable outputs; 2) Resolution sensitivity: Test both models at 256×256 to reproduce generalization gap (0.49 vs 0.85); 3) Explanation faithfulness: Manually inspect 50 predictions where VLM is correct and CNN is wrong; verify explanations reference actual visual features

## Open Questions the Paper Calls Out

- **Question:** Can VLMs transfer to real experimental neutrino data with comparable performance, or do they overfit to simulation artifacts?
  - **Basis in paper:** [inferred] The entire evaluation uses simulated GENIE/GEANT4 data with specific smearing assumptions and idealized detector conditions. No validation on real detector data is presented.
  - **Why unresolved:** Simulations inevitably differ from real data due to unmodeled noise, calibration uncertainties, and detector effects not captured in the simulation pipeline.
  - **What evidence would resolve it:** Evaluation on real LArTPC data from experiments like MicroBooNE or DUNE with systematic uncertainty quantification comparing simulation-trained vs real-data performance.

- **Question:** Can VLMs be compressed or distilled to achieve inference speeds suitable for real-time trigger applications while retaining interpretability?
  - **Basis in paper:** [explicit] The conclusion states: "promising research directions include compressing large models through quantization and pruning, distilling VLMs into compact architectures that retain interpretability."
  - **Why unresolved:** Current 3.3 second inference time and 25.4 GB memory footprint are impractical for online filtering or edge deployment where CNNs excel (20ms, 1GB).
  - **What evidence would resolve it:** Demonstrating quantized, pruned, or distilled variants achieving <100ms inference while maintaining accuracy within 2-3% and preserving meaningful textual explanations.

- **Question:** Can a single VLM serve as a reusable foundation model across different neutrino experiments with minimal adaptation?
  - **Basis in paper:** [explicit] The paper states results "suggest it would be possible to establish a reusable HEP foundation model, where future adaptations can be achieved even across experiments with minimal further fine-tuning."
  - **Why unresolved:** The study only tests generalization to lower resolution within the same simulation framework, not transfer across fundamentally different detector geometries, readout schemes, or physics processes.
  - **What evidence would resolve it:** Zero-shot or few-shot evaluation on datasets from different experiments (NOvA, DUNE far detector, MicroBooNE) demonstrating cross-experiment transfer efficiency.

## Limitations

- Lack of open-source data and code prevents exact reproduction and limits community validation
- CNN baseline has significantly fewer parameters (3.4M vs 11B), making architectural advantages difficult to isolate
- Explanation quality assessment relies on qualitative evaluation rather than systematic faithfulness metrics
- Computational requirements (25GB memory, 165× slower inference) present practical barriers for widespread adoption

## Confidence

**High Confidence:** VLM outperforms CNN on standard classification metrics (accuracy 0.87 vs 0.68, AUC-ROC 0.96 vs 0.93) when tested on same-resolution simulated data. The methodology for constrained decoding and confidence extraction is clearly specified and reproducible.

**Medium Confidence:** Generalization under degraded conditions (0.85 vs 0.49 accuracy at half resolution) demonstrates pretraining benefits, but the exact nature of degradation and pretraining data diversity are not fully characterized. Explanation quality claims are supported by qualitative inspection but lack systematic evaluation metrics.

**Low Confidence:** Claims about physics-grounded explanations are based on manual inspection of 50 samples without quantitative faithfulness metrics. The assumption that pretrained visual-linguistic representations transfer effectively to sparse, domain-specific detector images lacks direct validation through ablation studies.

## Next Checks

1. **Explanation Faithfulness Validation:** Implement a systematic evaluation protocol comparing VLM explanations against ground truth event features across 1000 predictions. Calculate precision and recall using automated feature extraction from pixel maps.

2. **Architecture Ablation Study:** Compare VLM performance with and without QLoRA adapters, and test different LoRA ranks (8, 32, 64) to quantify the trade-off between adaptation capacity and catastrophic forgetting.

3. **Domain Shift Robustness:** Test models on data from different simulation configurations (alternative physics generators, detector geometries, noise models) to assess true generalization beyond resolution changes. Evaluate sim-to-real transfer if real detector data becomes available.