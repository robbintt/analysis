---
ver: rpa2
title: Improving Minimax Group Fairness in Sequential Recommendation
arxiv_id: '2501.18117'
source_url: https://arxiv.org/abs/2501.18117
tags:
- group
- groups
- training
- methods
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper applies Distributionally Robust Optimization (DRO)\
  \ methods to sequential recommendation models to address minimax group fairness.\
  \ The authors explore three DRO approaches\u2014Group DRO, Streaming DRO, and CVaR\
  \ DRO\u2014on SASRec to improve recommendation quality across diverse user groups."
---

# Improving Minimax Group Fairness in Sequential Recommendation

## Quick Facts
- arXiv ID: 2501.18117
- Source URL: https://arxiv.org/abs/2501.18117
- Authors: Krishna Acharya; David Wardrope; Timos Korres; Aleksandr Petrov; Anders Uhrenholt
- Reference count: 34
- Key outcome: CVaR DRO outperforms group-dependent methods (GDRO, SDRO) on sequential recommendation fairness without requiring group annotations

## Executive Summary
This paper addresses minimax group fairness in sequential recommendation by applying Distributionally Robust Optimization (DRO) methods to SASRec. The authors compare three DRO approaches—Group DRO, Streaming DRO, and CVaR DRO—on two real-world datasets. CVaR DRO emerges as the superior method, achieving the highest NDCG@20 scores both overall and across user groups without requiring explicit group annotations. The key insight is that CVaR dynamically up-weights high-loss examples within each mini-batch, effectively identifying and prioritizing underperforming samples (often from minority groups) without knowing group membership a priori.

## Method Summary
The authors apply DRO methods to SASRec for sequential recommendation fairness. CVaR DRO computes the worst α-fraction of losses per batch and optimizes for those, requiring no group annotations and naturally handling overlapping groups. GDRO and SDRO maintain distributions over predefined groups and update them via exponentiated gradient ascent. All methods are evaluated using NDCG@20 with full item corpus ranking (not negative sampling). The paper demonstrates that CVaR outperforms both group-dependent methods and standard ERM across various group definitions based on popularity and sequence length.

## Key Results
- CVaR DRO achieves the highest NDCG@20 scores both overall and across user groups on Retailrocket and Movielens-1M
- CVaR naturally handles overlapping groups while GDRO/SDRO are sensitive to group choice and struggle with intersections
- Group-dependent methods (GDRO, SDRO) show inconsistent performance depending on whether popularity or sequence-length groups are used in loss computation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** CVaR improves worst-case group performance by dynamically up-weighting high-loss examples within each mini-batch, without requiring explicit group annotations.
- **Mechanism:** CVaR computes the worst α-fraction of losses per batch and optimizes for those, effectively identifying and prioritizing underperforming samples (which often correlate with minority groups) without knowing group membership a priori.
- **Core assumption:** High-loss examples in training meaningfully correlate with underperforming user groups at inference time.
- **Evidence anchors:**
  - [abstract] "CVaR does not require such annotations and can naturally handle overlapping groups"
  - [section 2.3.4] "the CVaR loss is the average loss over the m samples that incur the highest losses"
  - [corpus] CTC-DRO paper notes group DRO "fails when group losses misrepresent performance differences" — CVaR bypasses this by not relying on group definitions
- **Break condition:** If loss distribution during training does not correlate with downstream group-level metrics, CVaR's implicit prioritization provides no fairness benefit.

### Mechanism 2
- **Claim:** Group-dependent DRO methods (GDRO, SDRO) are sensitive to the choice of groups used in loss computation, leading to inconsistent results when groups intersect or are misspecified.
- **Mechanism:** GDRO and SDRO maintain a distribution ω over predefined groups and update it via exponentiated gradient ascent. If the chosen groups don't align with actual underperformance patterns, the reweighting targets the wrong subpopulations.
- **Core assumption:** The practitioner knows which group definitions matter for fairness a priori.
- **Evidence anchors:**
  - [section 3.3] "using popularity groups in the loss calculation yields worse relative improvements than using sequence length groups"
  - [figure 1] Shows GDROpop/SDROpop underperform GDROseq/SDROseq on the same data
  - [corpus] Cross-domain recommendation paper shows knowledge transfer can "heighten group-level unfairness" — group specification matters
- **Break condition:** If groups are unknown, protected (GDPR), or intersecting, group-dependent methods cannot be correctly specified and may underperform or amplify bias.

### Mechanism 3
- **Claim:** DRO methods improve both group-level and overall NDCG by reducing overfitting to majority patterns, though the magnitude of improvement varies with group imbalance.
- **Mechanism:** Standard ERM uniformly weights all examples, causing models to overfit majority groups. DRO methods shift optimization toward harder examples, improving generalization on underrepresented groups while often maintaining or improving overall metrics.
- **Core assumption:** The base model (SASRec) has sufficient capacity to learn minority patterns when appropriately weighted.
- **Evidence anchors:**
  - [table 2] CVaR improves overall NDCG from 0.225→0.239 (Gpop33) while also improving niche/diverse/popular subgroups
  - [section 3.2] "DRO methods obtain higher NDCGs than standard training across group sizes"
  - [corpus] Corpus shows limited direct replications; no external validation of this specific claim yet
- **Break condition:** If minority group patterns are fundamentally different (not just undersampled), reweighting alone may not close performance gaps.

## Foundational Learning

- **Concept: Minimax group fairness**
  - Why needed here: This paper targets minimax fairness (minimize worst-group loss), not equalized metrics across groups. Misunderstanding this leads to wrong evaluation expectations.
  - Quick check question: If group A achieves NDCG 0.30 and group B achieves 0.20, does minimax fairness require equalizing them or improving group B's score?

- **Concept: Distributionally Robust Optimization (DRO)**
  - Why needed here: DRO is the mathematical framework underlying all three methods. Without this, CVaR's loss formula will appear arbitrary.
  - Quick check question: What distribution does DRO optimize over — the empirical training distribution or a worst-case distribution within some uncertainty set?

- **Concept: Transformer-based sequential recommendation (SASRec)**
  - Why needed here: The paper applies DRO specifically to SASRec. Understanding the base architecture (positional embeddings, causal attention, softmax over full item corpus) is prerequisite to debugging DRO interactions.
  - Quick check question: Why does the paper use softmax cross-entropy over all items instead of binary cross-entropy with negative sampling?

## Architecture Onboarding

- **Component map:** SASRec (transformer decoder, L=200) -> DRO loss computation (ERM, CB/IPW, GDRO/SDRO, CVaR) -> Cross-entropy over full item corpus
- **Critical path:**
  1. Preprocess data → define groups (for GDRO/SDRO only) → train SASRec with chosen loss
  2. For CVaR: tune α ∈ {0.1, 0.2, ..., 0.95} on validation NDCG@20
  3. For GDRO/SDRO: tune η ∈ {1e-3, 5e-3, 1e-2, 5e-2, 0.1} and select group type
  4. Evaluate on held-out test set using leave-one-out split

- **Design tradeoffs:**
  - CVaR vs. GDRO/SDRO: CVaR requires no group labels but offers less interpretability about which groups benefit; GDRO/SDRO provide explicit group control but fail with unknown/intersecting groups
  - α parameter: Lower α = more conservative (optimizes fewer worst-case examples); higher α = closer to ERM
  - Computational cost: CVaR requires sorting batch losses; GDRO/SDRO require maintaining group-level statistics

- **Failure signatures:**
  - CVaR underperforms ERM → α may be too low (optimizing for noise/outliers) or too high (degenerates to ERM)
  - GDRO/SDRO underperform → group definition may not align with actual performance disparities; try alternative groupings
  - Overall NDCG drops significantly → model capacity insufficient or hyperparameters misconfigured

- **First 3 experiments:**
  1. **Replicate baseline**: Train SASRec with ERM on Retailrocket, verify NDCG@20 ≈ 0.225. Confirm data preprocessing and evaluation pipeline.
  2. **Ablate α sensitivity**: Train CVaR with α ∈ {0.2, 0.5, 0.8} on same data. Plot validation NDCG vs. α to find optimal region before full grid search.
  3. **Intersecting groups test**: Create a setup where users belong to both popularity and sequence-length groups. Compare CVaR vs. GDROseq vs. GDROpop on the same test set to observe sensitivity to group choice.

## Open Questions the Paper Calls Out
- The paper identifies that important user attributes such as age and ethnicity are commonly protected and unknown, rendering group-dependent methods inapplicable, and argues that CVaR is the solution. However, the experiments exclusively validate the methods on "proxy" groups derived from interaction history rather than on actual sensitive/protected attributes.

## Limitations
- The CVaR approach shows strong empirical performance but lacks theoretical guarantees about which groups benefit from the implicit prioritization
- The claim that CVaR naturally handles overlapping groups is demonstrated but not rigorously explained mechanistically
- The computational cost of full softmax ranking over large item catalogs may limit scalability to industrial-scale recommendation systems

## Confidence
- **High confidence:** CVaR outperforms GDRO/SDRO empirically on tested datasets; group-dependent methods are sensitive to group choice
- **Medium confidence:** CVaR's implicit prioritization meaningfully correlates with minority group performance; full softmax ranking is necessary (vs negative sampling)
- **Low confidence:** CVaR's mechanism for handling intersecting groups is well-understood; theoretical guarantees exist for CVaR's minimax fairness properties

## Next Checks
1. **Cross-dataset validation:** Apply CVaR to a third sequential recommendation dataset (e.g., Amazon Books) to test generalizability beyond Retailrocket and Movielens
2. **Group identification experiment:** After CVaR training, analyze which user groups have the highest losses to verify the implicit correlation between high-loss examples and minority groups
3. **Computational scaling test:** Measure runtime and memory usage as item catalog size increases from 10k to 100k items to quantify practical scalability limits