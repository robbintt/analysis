---
ver: rpa2
title: 'A Survey of Self-Evolving Agents: What, When, How, and Where to Evolve on
  the Path to Artificial Super Intelligence'
arxiv_id: '2507.21046'
source_url: https://arxiv.org/abs/2507.21046
tags:
- arxiv
- learning
- agents
- agent
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey presents the first comprehensive and systematic review
  of self-evolving agents, which can continuously adapt and improve their capabilities
  through experience. The paper organizes the field around three fundamental dimensions:
  what to evolve (model, context, tools, architecture), when to evolve (intra-test-time
  vs.'
---

# A Survey of Self-Evolving Agents: What, When, How, and Where to Evolve on the Path to Artificial Super Intelligence

## Quick Facts
- arXiv ID: 2507.21046
- Source URL: https://arxiv.org/abs/2507.21046
- Reference count: 40
- Primary result: First comprehensive survey organizing self-evolving agents around what, when, and how to evolve, providing a roadmap for advancing more adaptive, robust, and versatile agentic systems toward Artificial Super Intelligence.

## Executive Summary
This survey presents the first comprehensive and systematic review of self-evolving agents, which can continuously adapt and improve their capabilities through experience. The paper organizes the field around three fundamental dimensions: what to evolve (model, context, tools, architecture), when to evolve (intra-test-time vs. inter-test-time), and how to evolve (reward-based, imitation, population-based methods). It analyzes evolutionary mechanisms across agent components, categorizes adaptation methods by stages, and examines algorithmic and architectural designs that guide evolution. The survey reviews evaluation metrics and benchmarks, highlights applications in coding, education, and healthcare, and identifies critical challenges and research directions in safety, scalability, and co-evolutionary dynamics. By providing a structured framework for understanding and designing self-evolving agents, this work establishes a roadmap for advancing more adaptive, robust, and versatile agentic systems, ultimately contributing to the realization of Artificial Super Intelligence where agents evolve autonomously and perform beyond human-level intelligence across tasks.

## Method Summary
The survey employs a systematic literature review methodology, examining 40+ papers to construct a comprehensive taxonomy of self-evolving agents. The authors define the self-evolving agent framework through a formal POMDP-based model and identify three core dimensions: what components to evolve (model, context, tools, architecture), when evolution occurs (intra-test vs. inter-test time), and how evolution is implemented (reward-based, imitation, population-based methods). They analyze evolutionary mechanisms across agent components, categorize adaptation methods by stages, and examine algorithmic and architectural designs that guide evolution. The survey also reviews evaluation metrics and benchmarks, highlights applications in coding, education, and healthcare, and identifies critical challenges and research directions in safety, scalability, and co-evolutionary dynamics.

## Key Results
- Self-evolving agents can autonomously improve through experience by modifying internal states based on self-generated trajectories and feedback signals
- Evolution operates across four distinct loci (models, context/memory, tools, architecture) with different update pathways
- Timing of evolution (intra-test vs. inter-test) determines the learning paradigm and data availability constraints
- The survey establishes a structured framework for understanding and designing self-evolving agents as a roadmap toward Artificial Super Intelligence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Agents can improve performance by modifying internal states based on self-generated trajectories and feedback signals.
- Mechanism: A self-evolving strategy f transforms agent system Π to Π' by consuming trajectory τ and feedback r, then updating components (Γ, ψ, C, W) according to the objective of maximizing cumulative utility over sequential tasks.
- Core assumption: Feedback signals—whether textual, scalar rewards, or internal confidence—contain actionable information that can be mapped to specific component modifications.
- Evidence anchors:
  - [abstract] "agents can autonomously learn from new data, interactions, and experiences in real-time"
  - [section 2.1] Formal definition: "f(Π, τ, r) = Π′" with objective "maximize cumulative utility over tasks"
  - [corpus] Limited direct validation; MemSkill paper notes static memory operations are "rigid under diverse interaction patterns"
- Break condition: Feedback is sparse, noisy, or adversarial such that mapping to useful modifications becomes unreliable; or reward hacking emerges (survey Section 8.3.1 notes "alignment tipping process" risk).

### Mechanism 2
- Claim: Evolution operates across four loci (models, context, tools, architecture), each with distinct update pathways.
- Mechanism: Models update via self-generated supervision (e.g., SCA generates Code-as-Task problems, filters trajectories for SFT); context evolves through memory operations (ADD/UPDATE/DELETE as in Mem0, Memory-R1); tools expand via autonomous creation (Voyager, SkillWeaver); architecture restructures through search (AFlow uses MCTS, ADAS defines Turing-complete code search).
- Core assumption: Each locus is independently modifiable without destabilizing other components; credit assignment across loci is tractable.
- Evidence anchors:
  - [section 3] Table 2 shows which methods evolve which loci (e.g., RAGEN evolves model + context + architecture)
  - [section 3.1] "SCA...alternates roles between challenger and executor...fine-tunes using trajectories from successful solutions"
  - [corpus] LabOS claims "self-evolving agents" but no quantitative validation of multi-locus evolution efficacy
- Break condition: Cross-locus interference (e.g., memory growth destabilizes tool selection accuracy); or computational cost of coordinated updates exceeds practical limits (survey notes architecture search is "offline over extended periods").

### Mechanism 3
- Claim: Timing of evolution (intra-test vs. inter-test) determines the learning paradigm and data availability constraints.
- Mechanism: Intra-test-time uses online ICL/SFT/RL with immediate feedback during task execution (e.g., Reflexion stores verbal critiques in episodic memory); inter-test-time leverages accumulated trajectories offline (e.g., STaR generates reasoning chains post-hoc, RAGEN uses multi-turn RL).
- Core assumption: Intra-test adaptation can occur within task latency constraints; inter-test consolidation preserves knowledge without catastrophic forgetting.
- Evidence anchors:
  - [section 4] "Intra-test-time is characterized by its online nature...learning data emerges dynamically"
  - [section 4.2] "Inter-test-time SFT methods establish a paradigm of iterative self-improvement through synthetic data generation"
  - [corpus] "Learning on the Job" paper notes LLM agents "cannot learn from experience, lacking" this capability—suggesting current implementations remain limited
- Break condition: Real-time latency constraints prevent meaningful intra-test updates; or inter-test consolidation fails due to distribution shift between collected and deployment data.

## Foundational Learning

- Concept: **Partially Observable Markov Decision Process (POMDP)**
  - Why needed here: The paper models agent environments as POMDPs (S, A, T, R, Ω, O, γ), making this the foundational abstraction for understanding state, observation, and reward structures.
  - Quick check question: Can you sketch why an agent might receive different observations from the same underlying state?

- Concept: **Credit Assignment in Multi-Step Trajectories**
  - Why needed here: Section 3.3 explicitly identifies "determining which line of code or parameter was responsible for a failure" as a core challenge in tool mastery.
  - Quick check question: Given a failed 10-step tool chain, how would you attribute blame across steps?

- Concept: **Stability-Plasticity Tradeoff**
  - Why needed here: Section 2.2 contrasts lifelong learning (mitigating catastrophic forgetting) with self-evolving agents (active exploration); retention metrics (FGT, BWT) directly measure this tension.
  - Quick check question: What happens to old task performance when an agent aggressively adapts to new domains?

## Architecture Onboarding

- Component map:
Environment (POMDP) -> Agent System Π -> Architecture Γ (control flow / multi-agent topology) -> LLM/MLLM nodes {ψ_i} -> Context {C_i} = {Prompt P_i, Memory M_i} -> Tools {W_i} (APIs, skills)

- Critical path: (1) Define which locus evolves first—paper suggests starting with context/memory (lower risk) before model weights or architecture; (2) Establish feedback signal type (textual vs. scalar) and collection pipeline; (3) Set evolution timing constraints (intra-test latency budget vs. inter-test consolidation window).

- Design tradeoffs:
  - Intra-test vs. inter-test: Real-time adaptation vs. deeper consolidation
  - On-policy vs. off-policy: Fresh trajectory data vs. sample efficiency from replay buffers
  - Outcome vs. process rewards: Sparse but clear signals vs. dense but noisy step-level feedback

- Failure signatures:
  - Reward hacking: Agent discovers shortcuts (e.g., unnecessary refunds correlated with satisfaction ratings in Section 8.3.1)
  - Context overflow: Memory replay buffers exceed context window limits (Section 7.1.2)
  - Architecture drift: Automated topology changes degrade interpretability without improving performance

- First 3 experiments:
  1. **Baseline context evolution**: Implement simple memory operations (ADD/UPDATE/DELETE) on a single-task domain; measure success-by-iteration curves per Section 7.2.2.
  2. **Feedback signal ablation**: Compare textual feedback (Reflexion-style) vs. scalar rewards on same task; track adaptation speed and final performance.
  3. **Timing constraint test**: Enforce intra-test latency budget (e.g., 500ms max per update); measure whether meaningful adaptation persists or degrades to no-op.

## Open Questions the Paper Calls Out
None

## Limitations
- The feedback-signal-to-component-modification mapping remains largely theoretical with limited empirical validation across diverse domains
- The multi-locus evolution model assumes components can be independently modified without destabilizing the agent system, but cross-locus interference is acknowledged as an open challenge
- The timing distinction between intra-test and inter-test evolution oversimplifies real-world constraints where latency budgets and data availability often conflict

## Confidence
- **High Confidence**: The organizational framework (what/when/how to evolve) and taxonomy of evolution loci are well-supported by the surveyed literature. The survey's structure and classification systems demonstrate strong internal consistency.
- **Medium Confidence**: The claim that self-evolving agents represent a distinct paradigm from traditional lifelong learning is reasonable but requires more empirical differentiation studies. The assertion that this work "establishes a roadmap" is appropriately qualified given the field's nascent state.
- **Low Confidence**: Specific claims about Artificial Super Intelligence emergence from self-evolving agents are speculative. The survey appropriately flags safety and alignment as critical challenges but doesn't resolve these fundamental concerns.

## Next Checks
1. **Cross-locus interference experiment**: Implement a controlled study where model updates, memory operations, and tool additions are applied simultaneously vs. sequentially to measure interference effects and identify optimal update ordering.

2. **Feedback signal robustness test**: Systematically introduce noise and adversarial feedback patterns into existing self-evolving agent implementations to measure breakdown points and identify signal quality thresholds for reliable adaptation.

3. **Real-time constraint evaluation**: Establish concrete latency budgets (e.g., 100ms, 500ms, 2000ms) for intra-test evolution and measure the trade-off between adaptation speed and quality across tasks of varying complexity to determine practical limits.