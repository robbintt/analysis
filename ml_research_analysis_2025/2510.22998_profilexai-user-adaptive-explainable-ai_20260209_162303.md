---
ver: rpa2
title: 'ProfileXAI: User-Adaptive Explainable AI'
arxiv_id: '2510.22998'
source_url: https://arxiv.org/abs/2510.22998
tags:
- dataset
- available
- https
- online
- shap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ProfileXAI is a model- and domain-agnostic framework that couples
  post-hoc explainers (SHAP, LIME, Anchor) with retrieval-augmented LLMs to produce
  explanations tailored to three user profiles: machine learning experts, domain experts,
  and non-technical users. The system indexes a multimodal knowledge base, selects
  an explainer per instance via quantitative metrics, and generates grounded narratives
  with interactive chat prompting.'
---

# ProfileXAI: User-Adaptive Explainable AI

## Quick Facts
- arXiv ID: 2510.22998
- Source URL: https://arxiv.org/abs/2510.22998
- Reference count: 33
- Primary result: ProfileXAI achieves model- and domain-agnostic user-adaptive explanations by dynamically selecting post-hoc explainers and generating profile-conditioned narratives grounded in RAG-augmented knowledge bases.

## Executive Summary
ProfileXAI is a framework that produces explainable AI outputs tailored to three user profiles: machine learning experts, domain experts, and non-technical users. It couples post-hoc explainers (SHAP, LIME, Anchor) with retrieval-augmented LLMs to generate grounded, natural-language explanations. The system indexes a multimodal knowledge base, selects an explainer per instance via quantitative metrics, and generates narratives with interactive chat prompting. On Heart Disease and Thyroid Cancer datasets, ProfileXAI achieved LIME's best fidelity-robustness trade-off (Infidelity ≤0.30, Lipschitz <0.7), Anchor's sparsest rules (3-4 features), and SHAP's highest satisfaction score (average 4.1/5). Profile conditioning stabilized token use (σ ≤13%) and maintained high perceived quality across profiles (average ≥3.7/5).

## Method Summary
ProfileXAI ingests a multimodal knowledge base, chunks and embeds it into a vector database, then at inference runs SHAP, LIME, and Anchor explainers in parallel on each instance. It computes Infidelity, Lipschitz stability, and Effective Complexity to select the optimal explainer per instance, retrieves relevant knowledge base fragments via RAG, and generates a profile-conditioned narrative using an LLM. The system supports interactive chat for follow-up questions and is evaluated on two medical datasets with simulated user satisfaction scores.

## Key Results
- LIME achieves best fidelity-robustness trade-off (Infidelity ≤0.30, Lipschitz <0.7 on Heart Disease)
- Anchor yields sparsest explanations (3-4 features) for non-technical users
- SHAP attains highest satisfaction score (average 4.1/5)
- Profile conditioning stabilizes token consumption (σ ≤13%) and maintains satisfaction ≥3.7/5 across all user profiles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic explainer selection based on quantitative metrics improves explanation quality compared to using a single method.
- Mechanism: The Explanation Engine computes Infidelity, Lipschitz stability, and Effective Complexity for each explainer on a per-instance basis, then selects the method that optimizes the trade-off for that specific prediction context.
- Core assumption: The optimal explainer varies by instance rather than by domain or user preference alone.
- Evidence anchors:
  - [abstract] "LIME achieves the best fidelity-robustness trade-off (Infidelity ≤0.30, L<0.7 on Heart Disease); Anchor yields the sparsest, low-token rules; SHAP attains the highest satisfaction (4.1/5)"
  - [section III] "No explainer dominates: LIME offers the best balance, Anchor excels when brevity is paramount, and SHAP is preferable when capturing rich feature interactions outweighs stability considerations"
  - [corpus] Weak direct support; related work (PLEX, IXAII) confirms multi-method XAI interfaces but does not validate dynamic selection criteria.
- Break condition: If feature space exceeds training distribution significantly, metric reliability may degrade (observed: complexity and robustness worsen from 13 to 16 features).

### Mechanism 2
- Claim: Profile-conditioned prompting stabilizes cognitive load while maintaining perceived quality across heterogeneous audiences.
- Mechanism: The LLM receives a profile tag (ML engineer / domain expert / non-technical) that modulates output verbosity, terminology density, and example usage, enforcing consistent token budgets per profile.
- Core assumption: Users within a profile category have sufficiently homogeneous comprehension needs that a single prompt template suffices.
- Evidence anchors:
  - [abstract] "Profile conditioning stabilizes tokens (σ≤13%) and maintains positive ratings across profiles (x̄≥3.7)"
  - [section II.A] "ML engineer: technical details... Domain expert: translation into domain terminology... Non-technical user: accessible language with illustrative examples"
  - [corpus] IXAII (interactive XAI interface) similarly segments users but does not quantify token stabilization.
- Break condition: If a user's expertise is misclassified, output may be either patronizingly simple or impenetrably technical.

### Mechanism 3
- Claim: Retrieval-augmented grounding reduces hallucination risk by anchoring LLM narratives in indexed domain literature.
- Mechanism: A multimodal knowledge base is chunked, embedded, and stored in a vector database; at inference, relevant fragments are retrieved and injected into the generation prompt, constraining the LLM to domain-consistent phrasing.
- Core assumption: The knowledge base is sufficiently comprehensive and current to cover the explanation space.
- Evidence anchors:
  - [section II.A] "The information-extraction module processes the knowledge base in a multimodal manner... stores them in a vector database... retrieves relevant fragments to compose the generation prompt"
  - [section II.B] "The knowledge base comprised the articles [23], [24]" (domain literature for Heart Disease and Thyroid Cancer)
  - [corpus] Privacy-Preserving Explainable AIoT via SHAP Entropy Regularization uses SHAP but does not address RAG grounding; corpus provides no direct validation of this mechanism.
- Break condition: If retrieval fails (e.g., query drift, sparse index), the LLM may generate plausible but ungrounded statements.

## Foundational Learning

- **Post-hoc explanation methods (SHAP, LIME, Anchor)**
  - Why needed here: ProfileXAI dynamically selects among these three; understanding their output formats (Shapley values vs. local surrogates vs. rule anchors) is prerequisite to interpreting metrics and debugging selection logic.
  - Quick check question: Given a tabular prediction, which method would you expect to produce the sparsest explanation for a non-technical user?

- **RAG system architecture (embedding, vector search, context injection)**
  - Why needed here: The framework's grounding mechanism depends on correctly indexing multimodal documents and retrieving context at inference; misconfiguration here propagates hallucinations.
  - Quick check question: If retrieved chunks are irrelevant to the instance being explained, what failure mode would you expect in the final narrative?

- **Evaluation metrics for explanations (infidelity, Lipschitz stability, effective complexity)**
  - Why needed here: The dynamic selection engine uses these metrics to rank explainers; practitioners must understand what each metric captures to interpret trade-offs.
  - Quick check question: A low Infidelity score but high Lipschitz constant suggests what property of the explanation?

## Architecture Onboarding

- **Component map:**
  1. Knowledge Base Ingestion → Multimodal document processor → Chunker/Embedder
  2. Vector Database → Stores embedded chunks with metadata
  3. Explanation Engine → Parallel execution of SHAP, LIME, Anchor on input instance
  4. Metric Calculator → Computes Infidelity, Lipschitz, Effective Complexity per explainer
  5. Explainer Selector → Ranks methods by weighted metric criteria
  6. RAG Retriever → Retrieves top-k relevant chunks from vector DB
  7. Profile Prompt Assembler → Injects profile tag + retrieved context + explainer output
  8. LLM Generator → Produces natural-language explanation
  9. Interactive Chat Module → Maintains dialogue history for follow-up queries

- **Critical path:**
  Instance input → Explanation Engine (parallel SHAP/LIME/Anchor) → Metric Calculator → Explainer Selector → RAG Retriever → Profile Prompt Assembler → LLM Generator → User output

- **Design tradeoffs:**
  - Explainer selection vs. latency: Running three explainers per instance increases compute; paper does not report timing—assumption: acceptable for offline/low-throughput contexts.
  - Token budget vs. detail: Profile conditioning caps verbosity (σ≤13%) but may truncate nuanced explanations for domain experts.
  - Retrieval depth vs. context window: More chunks improve grounding but increase prompt size and latency.

- **Failure signatures:**
  1. Empty or irrelevant retrieval → LLM generates generic explanation lacking domain specificity.
  2. Metric tie or near-tie → Selector may arbitrarily pick; observed behavior not specified.
  3. Profile mismatch → User receives explanation at wrong technical depth (satisfaction drops).
  4. High Lipschitz on selected explainer → Explanation changes dramatically under small input perturbations; low robustness.

- **First 3 experiments:**
  1. Baseline explainer comparison: Run SHAP, LIME, Anchor on 50 instances from a held-out dataset; compute Infidelity, Lipschitz, Effective Complexity manually to validate reported metric ranges.
  2. Ablation on RAG grounding: Generate explanations with RAG enabled vs. disabled; have domain experts blind-rate factual accuracy and relevance to detect grounding contribution.
  3. Profile swap test: Deliver ML-engineer explanations to domain experts and vice versa on 30 instances; measure satisfaction score degradation to quantify profile-conditioning benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do human users rate ProfileXAI explanations consistently with the LLM-simulated Hoffman satisfaction scores?
- Basis in paper: [explicit] "Future work will... validate with human participants to refine the simulated assessments."
- Why unresolved: All satisfaction ratings (mean ≥3.7/5) were generated by an LLM simulating user responses, not actual humans. The correlation between simulated and real user satisfaction remains unknown.
- What evidence would resolve it: A user study with real ML engineers, domain experts, and non-technical users rating the same explanations using the Hoffman questionnaire.

### Open Question 2
- Question: How does ProfileXAI performance generalize to non-medical domains and higher-dimensional feature spaces?
- Basis in paper: [inferred] Experiments were limited to two medical datasets with only 13–16 features. The paper claims "model- and domain-agnostic" capability but provides no evidence beyond tabular healthcare data.
- Why unresolved: SHAP's robustness degradation from 13 to 16 features (L≈1.7→2.0) suggests scalability issues. Performance on image, text, or high-dimensional tabular data is untested.
- What evidence would resolve it: Evaluation on datasets from finance, legal, or computer vision domains with substantially larger feature spaces.

### Open Question 3
- Question: What is the optimal strategy for dynamically selecting among explainers when fidelity, robustness, and parsimony conflict?
- Basis in paper: [inferred] The paper concludes "no single explainer dominates" and selects explainers "via quantitative criteria," but the specific selection algorithm and its trade-off weights are not specified or justified.
- Why unresolved: Users prioritizing different metrics (e.g., brevity vs. completeness) may require different selection logic; a one-size-fits-all approach may be suboptimal.
- What evidence would resolve it: Ablation studies comparing fixed vs. user-preference-weighted selection strategies on downstream task performance.

### Open Question 4
- Question: How robust are profile-conditioned explanations to knowledge base incompleteness or errors in the RAG pipeline?
- Basis in paper: [inferred] The framework relies on a multimodal knowledge base indexed in a vector database, yet no analysis examines how KB quality affects explanation grounding or factual accuracy.
- Why unresolved: Retrieval failures or outdated medical literature could produce plausible-sounding but incorrect justifications, undermining trust.
- What evidence would resolve it: Controlled experiments with corrupted or ablated knowledge bases measuring factual consistency scores.

## Limitations
- Metric thresholds for explainer selection are referenced but not explicitly defined, making the selection logic non-reproducible without inference from cited literature.
- The exact LLM configuration (model, temperature, max tokens) and prompt templates for each profile are unspecified, introducing potential variability in narrative quality and token stability.
- The method assumes explainer metrics (Infidelity, Lipschitz, Effective Complexity) are reliable indicators of explanation quality, but the paper does not validate this assumption against user comprehension or task performance.

## Confidence
- **High confidence** in the conceptual framework and overall approach (coupling post-hoc explainers with RAG-augmented LLMs for user-adaptive explanations).
- **Medium confidence** in the reported evaluation results (satisfaction scores, token stability, explainer trade-offs), as the methodology is clear but some implementation details are missing.
- **Low confidence** in the exact reproducibility of the explainer selection logic and LLM-generated narratives due to unspecified thresholds and prompts.

## Next Checks
1. Validate explainer selection thresholds: Manually compute Infidelity, Lipschitz, and Effective Complexity on 50 held-out instances; compare against Table I ranges and infer decision thresholds for selection.
2. Ablate RAG grounding: Generate explanations with RAG enabled vs. disabled on 30 instances; have domain experts blind-rate factual accuracy and relevance to quantify grounding impact.
3. Profile swap test: Deliver ML-engineer explanations to domain experts and vice versa on 30 instances; measure satisfaction score degradation to quantify profile-conditioning benefit.