---
ver: rpa2
title: A Methodological and Structural Review of Parkinsons Disease Detection Across
  Diverse Data Modalities
arxiv_id: '2505.00525'
source_url: https://arxiv.org/abs/2505.00525
tags:
- parkinson
- disease
- data
- dataset
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This comprehensive review examines Parkinson's Disease (PD) detection
  across diverse data modalities including MRI, gait analysis, handwriting, speech,
  EEG, and multimodal fusion approaches. The study analyzed over 347 articles from
  2014-2025, identifying significant advancements in machine and deep learning techniques
  for PD diagnosis, staging, and biomarker identification.
---

# A Methodological and Structural Review of Parkinsons Disease Detection Across Diverse Data Modalities

## Quick Facts
- arXiv ID: 2505.00525
- Source URL: https://arxiv.org/abs/2505.00525
- Reference count: 40
- Analyzes over 347 articles from 2014-2025 on PD detection methods

## Executive Summary
This comprehensive review examines Parkinson's Disease (PD) detection across six data modalities including MRI, gait analysis, handwriting, speech, EEG, and multimodal fusion approaches. The study systematically analyzed over 347 articles published between 2014-2025, identifying significant advancements in machine and deep learning techniques for PD diagnosis, staging, and biomarker identification. The research highlights how multimodal approaches can achieve up to 97.14% accuracy by combining complementary pathological markers from different data sources, with deep learning models demonstrating superior performance compared to traditional machine learning methods.

## Method Summary
The review employed a systematic literature review methodology, searching databases including Scopus, IEEE Xplore, Web of Science, Science Direct, MDPI, Springer Link, ResearchGate, and Google Scholar using specific keywords like "MRI-based PD," "Gait sensory," and "Multimodal fusion." The authors filtered results to include journal and conference proceedings from 2014-2024, excluding brief mentions, review articles, unavailable text, and opinion pieces. Remaining articles were categorized into six modalities: MRI, Video/Pose, Sensor, Handwriting, Speech, and EEG. The methodology focused on identifying detailed experimental information and classifying papers based on their primary data modality.

## Key Results
- Multimodal approaches achieve up to 97.14% accuracy by fusing complementary pathological markers
- Deep learning models demonstrate superior performance compared to traditional machine learning methods
- Current limitations include small datasets, data variability, and model interpretability challenges

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fusing heterogeneous data modalities (e.g., combining gait with speech) may mitigate the diagnostic limitations inherent in single-modality systems by capturing complementary pathological markers.
- **Mechanism:** Multimodal systems aggregate distinct clinical features—such as motor impairments from gait sensors and cognitive/phonatory deficits from speech—into a joint feature space. This redundancy allows the model to maintain accuracy even if one symptom domain is ambiguous or noisy.
- **Core assumption:** Assumes that different modalities provide non-redundant information and that effective fusion strategies (feature-level or decision-level) can align these diverse data streams.
- **Evidence anchors:**
  - [abstract] "Key findings show multimodal approaches achieving up to 97.14% accuracy... emphasizing the need for larger, standardized datasets."
  - [section IX] "The combination of different data types—such as genetics, behaviour, and sensor data—offers a more holistic view of PD symptoms..."
  - [corpus] "Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion..." validates the efficacy of RGB-D fusion for gait analysis specifically.
- **Break condition:** Performance degrades if modalities are asynchronous or if the fusion logic introduces artifacts (e.g., aligning a high-frequency sensor stream with a low-frequency video frame), leading to "noisy" joint representations.

### Mechanism 2
- **Claim:** Deep Learning (DL) architectures, specifically Convolutional Neural Networks (CNNs), automate the extraction of complex, non-linear diagnostic features from raw signals, outperforming traditional handcrafted feature engineering.
- **Mechanism:** DL models exploit hierarchical representation learning. For example, CNNs applied to MRI or spectrograms automatically detect structural abnormalities or vocal frequency perturbations that would require manual calculation (e.g., HOG, MFCC) in traditional Machine Learning (ML).
- **Core assumption:** Assumes that "deeper" features correlate strongly with pathological biomarkers and that sufficient labeled data exists to train these high-capacity models without severe overfitting.
- **Evidence anchors:**
  - [abstract] "...deep learning models demonstrating superior performance compared to traditional machine learning methods."
  - [section VI.D] "Deep learning techniques... leverage neural networks... to classify speech patterns... eliminating the need for manual feature extraction."
  - [corpus] "Improvement of Performance in Freezing of Gait... using Transformer networks" supports the shift toward advanced deep architectures (Transformers) over basic baselines.
- **Break condition:** In "small dataset" regimes (common in PD research), deep models tend to overfit to training subjects rather than learning generalizable disease markers, resulting in poor cross-dataset validation.

### Mechanism 3
- **Claim:** Hybrid Spatio-Temporal architectures (e.g., CNN-RNN, Graph Convolutional Networks) are necessary to decode the dynamic nature of PD motor symptoms like gait and handwriting.
- **Mechanism:** These models decouple spatial dependencies (e.g., skeleton joint structure in gait, spatial pressure in handwriting) from temporal dependencies (e.g., gait cycles, in-air pen velocity). CNNs/Graph networks extract the structural features, while LSTMs/Transformers model the sequential progression of the symptom over time.
- **Core assumption:** Assumes that PD symptoms manifest as time-dependent anomalies (e.g., "freezing of gait") rather than static defects.
- **Evidence anchors:**
  - [section III.C] "Gait is highly individualistic, but PD patients show notable deviations... enabling early detection..."
  - [section IV.D] "...hybrid models, combining CNNs with temporal architectures like Recurrent Neural Networks (RNNs) to better capture sequential gait patterns..."
  - [corpus] "Linguistic Changes in Spontaneous Speech... Using Large Language Models" implies capturing complex sequential dependencies (though for speech, the temporal mechanism is analogous).
- **Break condition:** The mechanism fails if the temporal window is too short to capture a full gait cycle or handwriting stroke, or if the sample rate is too low, causing the model to miss high-frequency tremor features.

## Foundational Learning

- **Concept: Clinical Feature Heterogeneity**
  - **Why needed here:** PD manifests differently across patients (tremor-dominant vs. postural instability). Understanding that no single modality captures all symptoms explains *why* the review focuses on "diverse data modalities" rather than a "silver bullet" dataset.
  - **Quick check question:** Can a purely audio-based system detect "postural instability"? (Answer: No, Section VI.E notes postural instability has no direct impact on speech).

- **Concept: The "Small Data" Problem in Medical AI**
  - **Why needed here:** The paper repeatedly cites "small datasets" and "data variability" as limitations (Section II.E, Section X.A). Standard DL practices (like massive pre-training) often do not apply directly without transfer learning or augmentation strategies.
  - **Quick check question:** Why might a high-accuracy model (99%) on a 30-patient dataset be clinically useless? (Answer: High risk of overfitting to specific subjects/conditions rather than the disease pathology).

- **Concept: Preprocessing Alignment (Signal vs. Image)**
  - **Why needed here:** The review details vastly different preprocessing pipelines—skull stripping for MRI (Section II.B) vs. MFCC extraction for Speech (Section VI.B). An engineer must know these pipelines are not interchangeable.
  - **Quick check question:** What is the primary preprocessing goal for MRI vs. Sensor data? (Answer: MRI requires noise reduction/artifact removal; Sensors often require normalization/filtering to isolate movement dynamics).

## Architecture Onboarding

- **Component map:**
  Input Layer -> Preprocessing Block -> Feature Extractor -> Fusion Layer -> Classifier

- **Critical path:**
  1. **Dataset Selection:** Choosing the right benchmark (e.g., PPMI for MRI vs. PaHaW for Handwriting) is the first critical constraint.
  2. **Preprocessing:** If raw data isn't cleaned (e.g., removing "in-air" noise from handwriting or eye-blink artifacts from EEG), the DL model will learn noise.
  3. **Validation Strategy:** Implementing rigorous Cross-Validation (k-fold) to overcome small dataset bias.

- **Design tradeoffs:**
  - **Interpretability vs. Accuracy:** The paper notes DL models are "black boxes" (Section X.A). You trade clinical trust for higher accuracy compared to SVM/Random Forest.
  - **Generality vs. Specificity:** Models trained on specific tasks (e.g., spiral drawing) are robust but narrow; multimodal models are broad but computationally complex and data-hungry.

- **Failure signatures:**
  - **Data Leakage:** High training accuracy but near-random test accuracy often indicates subject-specific features (e.g., background noise) were learned rather than disease features.
  - **Modality Dropout:** In fusion models, if one modality (e.g., speech) is corrupted, the entire model might fail unless robust fusion (e.g., attention) is used.
  - **Overfitting to Lab Conditions:** Models working on the "Daphnet" dataset (lab-based) failing on "mPower" (smartphone/home-based) due to environmental noise.

- **First 3 experiments:**
  1. **Baseline Establishments:** Implement a Random Forest classifier on the "HandPD" dataset using handcrafted features (kinematics) to set a performance baseline (Section V.C).
  2. **Modality Comparison:** Train a simple CNN on the "PPMI" MRI dataset vs. a CNN on the "Oxford" Speech dataset to compare the signal-to-noise ratio inherent in different modalities.
  3. **Multimodal Fusion Test:** Combine the feature embeddings from Experiment 2 (MRI + Speech) using simple concatenation to verify if accuracy improves over the single best modality, replicating the "97.14%" multimodal finding mentioned in the abstract.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can data fusion techniques be optimized to efficiently synchronize and integrate speech patterns with motion sensor data (e.g., gait, IMU) to capture the full spectrum of motor symptoms, including rigidity and postural instability?
- **Basis in paper:** [explicit] The authors state in Section IX.E that current systems struggle with "synchronizing these diverse sensor streams" and that speech data alone may fail to detect rigidity or postural instability, necessitating integration with motion sensors for a complete assessment.
- **Why unresolved:** Existing reviews often focus on single modalities, and the paper notes that "optimizing data fusion techniques" remains a critical research gap due to data heterogeneity and processing speed challenges.
- **What evidence would resolve it:** Development of a robust, real-time framework that achieves higher diagnostic accuracy by successfully correlating voice modulation changes with specific gait irregularities across diverse patient datasets.

### Open Question 2
- **Question:** Which Explainable AI (XAI) techniques (e.g., attention mechanisms, saliency maps) can most effectively demystify the "black-box" nature of deep learning models to ensure clinical trust and adoption?
- **Basis in paper:** [explicit] Section X.A identifies the "lack of explainability" in current deep learning models as a significant limitation that "reduces trust among medical professionals." Section V.F further suggests future systems will likely incorporate XAI to improve transparency.
- **Why unresolved:** Deep learning models achieve high accuracy but are currently difficult to interpret, creating a barrier to clinical utility where transparency is required for diagnosis.
- **What evidence would resolve it:** A study demonstrating a high-accuracy PD detection model where XAI outputs align with known clinical diagnostic criteria (e.g., MDS-UPDRS), validated by medical professionals.

### Open Question 3
- **Question:** How can AI models be developed and validated to identify prodromal biomarkers and early-stage PD in asymptomatic individuals before significant neurodegeneration occurs?
- **Basis in paper:** [inferred] Section X.A notes that many available datasets are collected from patients already exhibiting clinical symptoms, which limits the ability of models to detect early-stage PD. The authors explicitly call for identifying "early biomarkers before significant neurodegeneration occurs" as a critical need.
- **Why unresolved:** There is a lack of large-scale, standardized datasets containing data from pre-symptomatic stages (prodromal phase), making it difficult to train models for early intervention.
- **What evidence would resolve it:** A longitudinal study utilizing multimodal data (e.g., sleep EEG, DaTscan) that successfully predicts the onset of PD in a currently healthy cohort.

## Limitations
- Publication bias toward successful models, with negative results rarely published
- Heterogeneous datasets with varying sample sizes (dozens to hundreds of subjects) making direct comparisons problematic
- Cannot verify reproducibility of individual experimental claims without access to source code or raw data

## Confidence
- **High Confidence:** Structural organization of PD detection modalities and associated preprocessing requirements
- **Medium Confidence:** Claims about deep learning outperforming traditional ML methods
- **Low Confidence:** Specific accuracy figures (e.g., "97.14% for multimodal approaches") due to selective reporting of best-case scenarios

## Next Checks
1. **Dataset Verification:** Cross-reference claimed performance metrics against actual sample sizes and demographic distributions to assess overfitting risks
2. **Code Reproducibility:** Implement and validate at least three representative models from different modalities using publicly available datasets
3. **Clinical Translation Assessment:** Evaluate whether highest-performing models have been validated in real-world clinical settings or remain academic benchmarks