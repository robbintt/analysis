---
ver: rpa2
title: Type and Complexity Signals in Multilingual Question Representations
arxiv_id: '2510.06304'
source_url: https://arxiv.org/abs/2510.06304
tags:
- complexity
- languages
- representations
- metrics
- linguistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how multilingual transformer models encode
  morphosyntactic properties of questions by introducing the Question Type and Complexity
  (QTC) dataset with 9,000 annotated questions across seven languages. The dataset
  includes categorical labels for question type (polar vs.
---

# Type and Complexity Signals in Multilingual Question Representations

## Quick Facts
- arXiv ID: 2510.06304
- Source URL: https://arxiv.org/abs/2510.06304
- Reference count: 19
- Primary result: Neural probes consistently outperform statistical baselines for question type classification across seven languages, with distinct encoding profiles revealed through layer-wise analysis

## Executive Summary
This study investigates how multilingual transformer models encode morphosyntactic properties of questions by introducing the Question Type and Complexity (QTC) dataset with 9,000 annotated questions across seven languages. The research employs selectivity-controlled probing methods to compare frozen Glot500-m representations against subword TF-IDF baselines and a fine-tuned model. Neural probes demonstrate superior performance for question type classification, particularly in languages requiring contextual integration, while statistical methods show better selectivity on most individual complexity metrics. Layer-wise analysis reveals three distinct encoding profiles: flat performance indicating surface-feature dependence, moderate variation suggesting partial structural encoding, and high oscillations reflecting unstable representation of certain properties.

## Method Summary
The study introduces a novel Question Type and Complexity (QTC) dataset containing 9,000 annotated questions across seven diverse languages (English, French, German, Japanese, Korean, Finnish, and Arabic). The dataset includes categorical labels for question type (polar vs. content) and continuous complexity metrics including token count, lexical density, dependency length, tree depth, verbal arity, and subordination patterns. Using selectivity-controlled probing methods, the research compares frozen Glot500-m representations against subword TF-IDF baselines and a fine-tuned model. The neural probes use two-layer MLPs with selectivity control to mitigate spurious correlations, while statistical baselines employ TF-IDF representations. Layer-wise analysis examines how different complexity metrics are encoded across transformer layers, and fine-tuning experiments assess the impact of adaptation on representation quality.

## Key Results
- Neural probes consistently outperform statistical baselines for question type classification across most languages, especially those requiring contextual integration (Japanese, Korean, English, Finnish)
- Statistical methods demonstrate better selectivity on most individual complexity metrics compared to neural approaches
- Layer-wise analysis reveals three distinct encoding profiles: flat performance (surface-feature dependence), moderate variation (partial structural encoding), and high oscillations (unstable representation)
- Fine-tuning compensates for unstable neural encoding but degrades performance on metrics with stable layer-wise representations, revealing a trade-off between adaptation and preservation of pre-trained knowledge

## Why This Works (Mechanism)
The success of neural probes for question type classification stems from their ability to capture contextual dependencies and complex interaction patterns that statistical methods cannot represent. The selectivity-controlled approach mitigates spurious correlations by comparing probe performance against random baseline controls. The three distinct encoding profiles observed in layer-wise analysis reflect different degrees of structural vs. surface-level representation: flat profiles indicate that models rely primarily on surface features, moderate variation suggests partial integration of structural information, and high oscillations reveal unstable or inconsistent encoding patterns that may require fine-tuning to stabilize.

## Foundational Learning
- **Selectivity-controlled probing**: Measures whether probe performance exceeds what can be achieved by chance, distinguishing genuine linguistic knowledge from spurious correlations. Needed to validate that neural probes capture real linguistic patterns rather than memorization. Quick check: Compare probe accuracy against random baseline performance.
- **Layer-wise representation analysis**: Examines how linguistic information is distributed across transformer layers, revealing encoding patterns and potential information loss. Needed to understand where and how different complexity metrics are represented. Quick check: Plot performance across layers to identify encoding profiles.
- **Fine-tuning trade-offs**: Investigates the balance between adaptation to downstream tasks and preservation of pre-trained linguistic knowledge. Needed to understand how model modification affects representation quality. Quick check: Compare frozen vs. fine-tuned performance on both stable and unstable metrics.

## Architecture Onboarding
- **Component map**: QTC dataset -> Selectivity-controlled probing (neural vs. statistical) -> Layer-wise analysis -> Fine-tuning experiments -> Performance comparison
- **Critical path**: Dataset creation → Probe training with selectivity control → Layer-wise performance evaluation → Fine-tuning adaptation → Trade-off analysis
- **Design tradeoffs**: Neural probes offer superior contextual modeling but risk spurious correlations; statistical methods provide better selectivity but limited contextual integration; fine-tuning improves unstable encodings but may degrade stable ones
- **Failure signatures**: Poor selectivity indicating spurious correlations; flat layer-wise profiles suggesting surface-level dependence; performance degradation during fine-tuning indicating catastrophic forgetting
- **3 first experiments**:
  1. Train neural probe with selectivity control on question type classification
  2. Compare layer-wise performance across all complexity metrics
  3. Evaluate frozen vs. fine-tuned model performance on stable metrics

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Probing methodology limitations may capture spurious correlations despite selectivity control, particularly given oscillation patterns in layer-wise representations
- Dataset construction choices and annotation schemes could influence results, with continuous complexity measures potentially not capturing full multidimensional nature of question difficulty
- Study doesn't explore intermediate fine-tuning stages or investigate whether observed trade-offs between adaptation and preservation can be optimized through alternative strategies

## Confidence
- **High confidence**: Neural probes consistently outperform statistical baselines for question type classification across most languages
- **Medium confidence**: Layer-wise encoding profiles and their interpretation as surface-feature dependence vs. structural encoding
- **Medium confidence**: Trade-off between fine-tuning adaptation and preservation of pre-trained knowledge

## Next Checks
1. Conduct adversarial testing with controlled perturbations to distinguish genuine linguistic encoding from surface-level correlations in neural probes
2. Expand dataset to include additional languages from underrepresented families and test whether observed encoding patterns generalize
3. Implement intermediate fine-tuning experiments with regularization techniques to investigate whether trade-off between adaptation and preservation can be optimized