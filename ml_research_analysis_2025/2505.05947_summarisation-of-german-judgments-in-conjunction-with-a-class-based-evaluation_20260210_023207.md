---
ver: rpa2
title: Summarisation of German Judgments in conjunction with a Class-based Evaluation
arxiv_id: '2505.05947'
source_url: https://arxiv.org/abs/2505.05947
tags:
- legal
- summaries
- guiding
- principles
- judgments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses automated summarization of German legal judgments\
  \ by fine-tuning a decoder-based language model (LeoLM) to generate guiding principles\u2014\
  concise summaries of court decisions. The authors enrich judgments with legal entity\
  \ tags before training and evaluate outputs using ROUGE, BERTScore, and custom classes\
  \ assessing language, pertinence, completeness, and correctness."
---

# Summarisation of German Judgments in conjunction with a Class-based Evaluation

## Quick Facts
- **arXiv ID**: 2505.05947
- **Source URL**: https://arxiv.org/abs/2505.05947
- **Reference count**: 40
- **Primary result**: Fine-tuned LeoLM with legal entity enrichment produces more relevant summaries than baseline, though quality remains inconsistent

## Executive Summary
This paper addresses automated summarization of German legal judgments by fine-tuning a decoder-based language model (LeoLM) to generate guiding principles—concise summaries of court decisions. The authors enrich judgments with legal entity tags before training and evaluate outputs using ROUGE, BERTScore, and custom classes assessing language, pertinence, completeness, and correctness. Results show the model outperforms a baseline, with legal entity enrichment improving relevance detection. Manual evaluation reveals high-quality summaries exist but are inconsistent, often containing hallucinations or unnatural phrasing. While automated metrics improve slightly, qualitative analysis indicates room for improvement. The study contributes a structured evaluation framework for legal summaries and demonstrates the potential of legal entity-aware models, though current outputs remain insufficient for practical deployment without further refinement.

## Method Summary
The authors fine-tune LeoLM-Mistral-7B on 3,556 German Federal Court of Justice civil judgments (2003-2022) to generate abstractive guiding principles. Judgments are enriched with legal entity tags using bert-german-ler before training. Two conditions are tested: plain judgments and judgments with legal entity tags. The model is trained for 10 epochs with lr=2e-4, batch_size=1, and greedy decoding (max 750 tokens). Evaluation uses ROUGE-1/2/L, BERTScore, and a 7-class manual framework covering language, pertinence, completeness, correctness, and other quality dimensions. A baseline LexRank extractive summarizer is compared against the generated outputs.

## Key Results
- LeoLM with legal entity enrichment achieves ROUGE-1 0.2997 vs LexRank 0.2597 baseline
- BERTScore improves from 0.6458 (baseline) to 0.6724 (LerLeoLM)
- Class-based evaluation shows LeoLM fulfills ~3.38 classes vs LexRank's ~2.55 classes
- Legal entity enrichment improves Main Focus class fulfillment from 32% to 42%
- Hallucinations persist despite legal entity enrichment; model does not create more correct entities on average

## Why This Works (Mechanism)

### Mechanism 1: Legal Entity Enrichment for Attention Guidance
- Claim: Inserting legal entity tags around domain-specific entities (norms, court decisions, literature) improves the model's ability to identify relevant content for summarization.
- Mechanism: Tags (e.g., `<GS> § 125 BGB </GS>`) are added as special tokens to the model's vocabulary during fine-tuning. This provides explicit structural signals that demarcate legally salient information, potentially improving attention allocation to entity-dense regions.
- Core assumption: Legal entities correlate with summarization-relevant content in guiding principles.
- Evidence anchors:
  - [abstract] "Our results show that employing legal entities helps the generative model to find the relevant content"
  - [Section 5.2, Table 8] Class 5 (Main Focus: 3/4 aspects covered) improved from 32% (LeoLM) to 42% (LerLeoLM)
  - [corpus] Related work on legal summarization (AugAbEx, LegalViz) does not explicitly test entity tagging as an enrichment strategy, limiting external validation.
- Break condition: If hallucinated entities increase proportionally with enrichment, the mechanism may be amplifying entity detection without improving factual grounding. The paper notes this was not observed to reduce hallucinations.

### Mechanism 2: Domain-Specific Decoder Fine-Tuning
- Claim: Fine-tuning a German-native decoder LLM (LeoLM-Mistral-7B) on court judgments produces higher-quality guiding principles than extractive baselines.
- Mechanism: Full fine-tuning (not QLoRA in final experiments) on 3,556 judgment–guiding-principle pairs enables the model to learn the rhetorical and stylistic conventions of German legal summarization.
- Core assumption: The training distribution (BGH civil judgments 2003–2022) is representative of target deployment cases.
- Evidence anchors:
  - [Section 5.2, Table 6] LeoLM achieves ROUGE-1 0.2997 vs LexRank 0.2597; BERTScore 0.6724 vs 0.6458
  - [Section 5.2, Table 8] LeoLM fulfills ≈3.38 evaluation classes vs LexRank's ≈2.55 classes (majority-voted)
  - [corpus] Comparable work (Glaser et al. 2021, cited in paper) achieves similar ROUGE scores on German court rulings, suggesting this performance level is current state-of-the-art for the domain.
- Break condition: If generated summaries consistently fail class 6 (Correctness) due to legal reasoning errors, domain fine-tuning alone is insufficient—requiring explicit reasoning supervision.

### Mechanism 3: Class-Based Human Evaluation Framework
- Claim: A structured multi-class evaluation (Language, Pertinence, Completeness, Correctness) captures quality dimensions missed by n-gram overlap metrics.
- Mechanism: Three legal experts independently rate each summary against 7 binary classes; majority vote determines fulfillment. Fleiss' Kappa measures inter-rater reliability.
- Core assumption: Legal experts can consistently operationalize subjective criteria (e.g., "necessity" of information).
- Evidence anchors:
  - [Section 4.1, Table 2] Seven classes derived from C.H. Beck editorial guidelines for guiding principles
  - [Section 4.2, Table 4] Pairwise reviewer agreement: 0.64–0.77 (substantial by Landis & Koch)
  - [Section 4.2, Table 5] Class 3 (Pertinence) and Class 2 (Language) show lowest agreement (0.11, 0.16), indicating subjectivity challenges
  - [corpus] Related evaluation frameworks (e.g., nugget-based methods in legal deposition summarization) address similar subjectivity but for different document types.
- Break condition: If specific classes consistently yield <0.20 Fleiss' Kappa, those categories require operationalization refinement or should be collapsed.

## Foundational Learning

- **Legal Entity Recognition (LER) as a preprocessing step**
  - Why needed here: The enrichment mechanism depends on accurate entity tagging before training. Errors propagate to model inputs.
  - Quick check question: Can you explain why inserting `<GS>` tags requires vocabulary expansion rather than tokenization as subwords?

- **Abstractive vs. Extractive Summarization in Legal Contexts**
  - Why needed here: The paper notes only 77 of 5,081 guiding principles are extractive; the model must learn paraphrasing, not sentence selection.
  - Quick check question: What evaluation failure would you expect if an extractive model were applied to an abstractive summarization task?

- **Limitations of ROUGE for Semantic Evaluation**
  - Why needed here: The paper uses BERTScore and class-based evaluation because ROUGE cannot detect synonyms or semantic equivalence.
  - Quick check question: Why would ROUGE-1 recall be insufficient to measure "Completeness" of legal reasoning?

## Architecture Onboarding

- **Component map:**
  ```
  Raw judgment → Section extraction (Entscheidungsgründe, exclude §I)
              → LER tagging (bert-german-ler)
              → Tokenization with special entity tokens
              → LeoLM-Mistral-7B fine-tuning
              → Greedy decoding (max 750 tokens)
              → Output: Guiding principles
              ↓
  Evaluation: ROUGE/BERTScore (automated) + 7-class human review
  ```

- **Critical path:**
  1. Section extraction quality—omitting non-summaries-relevant sections
  2. LER tag accuracy—incorrect tags misguide attention
  3. Fine-tuning hyperparameters (10 epochs, lr=2e-4, batch_size=1 due to memory)
  4. Human evaluation recruitment (3 reviewers per sample, legal expertise required)

- **Design tradeoffs:**
  - Greedy decoding chosen for reproducibility; sacrifices diversity vs. nucleus sampling
  - Truncating >32k-token judgments (<1% of data) risks losing dispositive content
  - Manual evaluation is labor-intensive; not scalable for iterative development
  - Assumption: German-native LeoLM outperforms multilingual alternatives—untested in paper

- **Failure signatures:**
  - Hallucinated citations: References to non-existent BGH decisions or literature
  - Unnatural legal phrasing: Synonymous but non-legal terms substituted for established terminology
  - Incomplete coverage: Models struggle with judgments combining procedural + substantive law
  - Extraction fallback: Summaries closely resemble first sentences rather than synthesizing key points

- **First 3 experiments:**
  1. **Baseline replication:** Run LexRank (2-sentence extractive) on test set; compute ROUGE/BERTScore and manually evaluate 20 samples to calibrate reviewer expectations.
  2. **Ablation on enrichment:** Train identical LeoLM models with and without LER tags on same data; compare class 5 (Main Focus) fulfillment rates to isolate entity signal contribution.
  3. **Hallucination audit:** Extract all cited legal entities from generated summaries; verify existence against source documents to quantify entity hallucination rate before/after enrichment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can alternative methods reduce hallucination rates for legal citations in generated summaries, given that legal entity enrichment did not reduce hallucinations?
- Basis in paper: [explicit] "We also analyse whether using the text enriched with legal entities... reduces the hallucinations for these entities. This does not seem to be the case. Although the model trained with the enriched texts generates summaries containing more legal entities, it apparently does not create more correct entities on average."
- Why unresolved: The authors tested one approach (tag-based enrichment) which failed to address hallucination; no alternative mitigation strategies were explored.
- What evidence would resolve it: Experiments comparing hallucination rates across methods such as constrained decoding, retrieval-augmented generation, or entity-aware loss functions on the same test set.

### Open Question 2
- Question: Why do models struggle specifically with judgments combining procedural and substantive law, and can targeted training data or architectural changes improve performance on such cases?
- Basis in paper: [explicit] "the models appear to struggle with the creation of guiding principles that consist of a combination of procedural law and substantive law, which oftentimes leads to considerable loss in quality. We suspect that this might be due to the models' tendency to only include aspects regarding substantive law."
- Why unresolved: The authors identify the problem and hypothesize a cause (preference for substantive law content) but do not test interventions.
- What evidence would resolve it: Error analysis on procedural/substantive mixed judgments, plus experiments with stratified sampling or specialized attention mechanisms during training.

### Open Question 3
- Question: How can evaluation classes 2 (Language) and 3 (Pertinence) be refined to achieve higher inter-rater agreement among legal experts?
- Basis in paper: [explicit] "For future improvements of our evaluation classes, we intend to address the classes with a considerable disagreement between the reviewers, such as class 2 and class 3."
- Why unresolved: Fleiss' Kappa scores were only 0.1585 (class 2) and 0.1107 (class 3), indicating fair to slight agreement; the paper does not propose specific refinements.
- What evidence would resolve it: Revised class definitions with concrete examples and annotation guidelines, followed by re-evaluation measuring improved Kappa scores with the same reviewer pool.

### Open Question 4
- Question: Does truncating judgments that exceed the model's context window cause significant information loss, and how do long-context models compare on this task?
- Basis in paper: [explicit] "Another aspect we intend to address in future work is the length of the texts. Due to the small number of judgments exceeding our model's context window size in our data set (<1%), we neglect this aspect and simply truncate the judgments, which potentially leads to a loss of important information."
- Why unresolved: The authors acknowledge this limitation but defer investigation; it remains unknown whether critical content is lost.
- What evidence would resolve it: Comparison of summary quality for long judgments using truncation versus long-context models, analyzing which judgment sections are most commonly truncated and their relevance to guiding principles.

## Limitations

- Hallucinated legal citations persist despite entity enrichment, with no tested mitigation strategies
- Modest performance improvements (ROUGE-1 +0.04, BERTScore +0.03) suggest limited practical impact
- Evaluation classes 2 and 3 show poor inter-rater agreement (Kappa 0.11-0.16), undermining reliability
- Model struggles specifically with judgments combining procedural and substantive law
- Exact train/validation/test split and legal entity tag vocabulary are unspecified, preventing exact replication

## Confidence

**High Confidence**: The mechanism of legal entity enrichment providing structural signals for attention guidance is well-supported by the 10-percentage-point improvement in class 5 (Main Focus) fulfillment rates. The class-based evaluation framework is rigorously implemented with documented inter-rater reliability metrics.

**Medium Confidence**: Domain-specific fine-tuning improvements over the extractive baseline are statistically significant but practically modest. The claim that LeoLM-Mistral-7B is superior to multilingual alternatives remains untested. The generalization to other German court domains (beyond BGH civil judgments 2003-2022) is unproven.

**Low Confidence**: The assertion that generated summaries are "of high quality" based on manual evaluation is undermined by the paper's own admission that "only a few summaries... are of high quality" and that hallucinations persist. The practical deployment readiness claim overstates the current capabilities.

## Next Checks

1. **Hallucination Audit**: Extract all legal citations and entity references from 100 generated summaries, verify existence against source judgments, and calculate hallucination rate. Compare pre- and post-enrichment hallucination frequencies to validate the enrichment mechanism's safety.

2. **Cross-Domain Generalization**: Apply the LerLeoLM model to a different German court domain (e.g., labor or administrative court judgments) and evaluate using the same class framework. Measure performance degradation to assess domain dependency.

3. **Metric Sensitivity Analysis**: Conduct ablation studies removing each of the seven evaluation classes to determine which criteria drive the majority of observed differences between models. This will reveal whether improvements are distributed across quality dimensions or concentrated in specific areas.