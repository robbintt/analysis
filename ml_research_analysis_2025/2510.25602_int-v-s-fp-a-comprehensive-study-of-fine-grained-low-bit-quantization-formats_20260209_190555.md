---
ver: rpa2
title: 'INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats'
arxiv_id: '2510.25602'
source_url: https://arxiv.org/abs/2510.25602
tags:
- quantization
- qsnr
- formats
- arxiv
- scale
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive comparison of fine-grained
  low-bit quantization formats, focusing on integer (INT) and floating-point (FP)
  approaches. The authors analyze the trade-offs between INT and FP quantization across
  different granularities, revealing that while FP excels in coarse-grained quantization,
  INT formats become highly competitive at fine-grained (block-wise) levels.
---

# INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats

## Quick Facts
- arXiv ID: 2510.25602
- Source URL: https://arxiv.org/abs/2510.25602
- Reference count: 40
- Primary result: Fine-grained INT formats outperform FP formats at 8-bit, challenging the industry trend toward FP quantization.

## Executive Summary
This paper presents a comprehensive comparison of fine-grained low-bit quantization formats, focusing on integer (INT) and floating-point (FP) approaches. The authors analyze the trade-offs between INT and FP quantization across different granularities, revealing that while FP excels in coarse-grained quantization, INT formats become highly competitive at fine-grained (block-wise) levels. For 8-bit formats, MXINT8 consistently outperforms MXFP8 in both accuracy and hardware efficiency. For 4-bit formats, FP formats like MXFP4 and NVFP4 hold an accuracy advantage, but NVINT4 can surpass NVFP4 when combined with outlier-mitigation techniques like Hadamard rotation. The paper also introduces a symmetric clipping method that resolves gradient bias in fine-grained low-bit INT training, enabling nearly lossless performance for MXINT8. Hardware cost analysis shows that fine-grained INT formats are significantly more area and energy-efficient than their FP counterparts at matched throughput. These findings challenge the current industry trend toward FP formats and advocate for prioritizing fine-grained INT formats to achieve a better balance of accuracy and efficiency in future AI accelerators.

## Method Summary
The study systematically compares fine-grained INT vs FP quantization formats for LLM inference and training. The authors evaluate 12 LLMs (Qwen3 series 0.6B–235B, Llama-3.1/3.2) using direct-cast inference and low-bit training with block-wise quantization (MX: block=32, NV: block=16). Formats include MXINT8/6/4, MXFP8/6/4, NVINT4, and NVFP4 with symmetric clipping for INT formats. Training uses 1B and 3B Llama-style models on OLMo2-Mix-1124 dataset with AdamW optimizer. The evaluation metrics include KL divergence vs BF16 baseline (top-25 logits restriction), QSNR, training loss, and downstream task accuracy.

## Key Results
- MXINT8 consistently outperforms MXFP8 in both accuracy and hardware efficiency at 8-bit fine-grained quantization across tested models
- FP formats maintain accuracy advantages for 4-bit quantization, but Hadamard rotation enables INT formats to match or exceed FP performance
- Symmetric clipping resolves gradient bias in INT training, enabling nearly lossless performance for MXINT8
- Fine-grained INT formats are significantly more area and energy-efficient than FP counterparts at matched throughput

## Why This Works (Mechanism)
The key mechanism is block-wise granularity reducing local crest factor (κ = max/|RMS|) within each block. Fine-grained quantization isolates outliers into separate blocks, lowering the local κ and enabling INT formats with uniform precision to outperform FP formats. Theoretical QSNR crossover points show MXINT8 outperforms MXFP8 when κ < 7.55, but MXINT4 needs κ < 2.04. Real data shows κ≈2.96 for block-32, favoring INT8 but not INT4. This shifts the traditional INT vs FP trade-off in favor of INT at fine-grained levels.

## Foundational Learning
**Crest Factor (κ)**: Ratio of maximum value to RMS in a signal block. Critical for determining INT vs FP performance - lower κ favors INT.
- Why needed: Determines the crossover point where INT formats outperform FP formats
- Quick check: Compute κ per block in your tensor; if < 7.55, INT8 likely superior to FP8

**Symmetric Clipping**: Scaling method that forces INT range to be symmetric around zero, eliminating gradient bias in training.
- Why needed: Prevents systematic bias in gradient updates that would otherwise degrade INT training convergence
- Quick check: Verify that your INT quantization range [-127,127] for INT8 maintains symmetric coverage

**Hadamard Rotation**: Random orthogonal transformation that redistributes outliers across transform domain.
- Why needed: Enables INT4 formats to handle sparse outliers by spreading them across multiple coefficients
- Quick check: Apply random Hadamard rotation to high-κ blocks and verify crest factor reduction

## Architecture Onboarding
**Component map**: Model weights -> Block-wise quantization -> Scale computation -> Symmetric clipping (INT) or FP encoding -> MAC unit -> Activation -> Quantization -> Next layer
**Critical path**: Quantization → Scale computation → MAC operation → Activation
**Design tradeoffs**: Block size vs. crest factor isolation; precision vs. hardware efficiency; FP dynamic range vs. INT uniform precision
**Failure signatures**: High KL divergence indicates κ > crossover threshold; training divergence indicates asymmetric clipping issues; accuracy loss suggests inadequate outlier mitigation
**First experiments**:
1. Measure crest factor distribution across model layers to identify optimal block size
2. Compare QSNR of INT vs FP formats at different κ values using synthetic data
3. Validate symmetric clipping implementation by checking gradient bias in training loss

## Open Questions the Paper Calls Out
**Cross-architecture generalization**: Do accuracy and efficiency advantages of fine-grained INT formats generalize to non-Transformer architectures such as CNNs or ViTs?
- Basis: Study evaluates exclusively on LLMs with specific outlier characteristics
- Resolution needed: Comparative benchmarks on computer vision tasks (ImageNet with ResNet/ViT)

**Sub-8-bit training feasibility**: Can symmetric clipping be extended to enable stable, lossless sub-8-bit training (e.g., INT4)?
- Basis: Paper demonstrates "nearly lossless" MXINT8 training but restricts 4-bit to inference
- Resolution needed: Training curves for LLMs using MXINT4/NVINT4 with symmetric clipping

**Silicon implementation overhead**: What are practical silicon implementation overheads for fine-grained scaling logic?
- Basis: Hardware analysis uses theoretical models excluding quantizer overhead and routing costs
- Resolution needed: Post-synthesis area/power reports from RTL implementation of fine-grained INT MAC vs FP unit

## Limitations
- Conclusions rely heavily on sparsity of outliers in typical LLM weight distributions, which may not generalize across all model architectures
- Hadamard rotation effectiveness requires validation across diverse model architectures and rotation types
- Hardware cost model simplifications may not capture implementation-specific overheads like memory bottlenecks
- Symmetric clipping effectiveness at scale (3B+ models) remains to be validated across different learning rates

## Confidence
**High confidence**: MXINT8 consistently outperforms MXFP8 in both accuracy and hardware efficiency at 8-bit fine-grained quantization
**Medium confidence**: FP formats maintain accuracy advantages for 4-bit quantization, but Hadamard rotation can enable INT formats to match or exceed FP performance
**Low confidence**: Symmetric clipping method fully resolves gradient bias in INT training for all scenarios; exact crossover points universally applicable

## Next Checks
1. **Cross-architecture crest factor analysis**: Measure crest factor distribution across vision transformers, multimodal models, and encoder-only models to validate κ < 7.55 threshold for INT8 dominance
2. **Hadamard rotation ablation study**: Systematically evaluate effectiveness across different block sizes, rotation types, and model layers to determine when and why it succeeds or fails
3. **Large-scale training validation**: Replicate symmetric clipping training experiments on 3B+ parameter models with varying learning rates and batch sizes to confirm scalability and identify potential instabilities