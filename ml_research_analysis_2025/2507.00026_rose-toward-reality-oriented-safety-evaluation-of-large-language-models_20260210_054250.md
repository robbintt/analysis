---
ver: rpa2
title: 'ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models'
arxiv_id: '2507.00026'
source_url: https://arxiv.org/abs/2507.00026
tags:
- adversarial
- prompts
- safety
- arxiv
- rose
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ROSE introduces a reality-oriented safety evaluation framework
  for large language models that addresses the limitations of static benchmarks and
  existing automated methods by using multi-objective reinforcement learning to generate
  adversarial prompts that are both topically diverse and contextually rich. The framework
  incorporates a topic-level diversity metric based on LLM-based safety guard embeddings
  and a consistency reward to ensure prompts are grounded in realistic scenarios.
---

# ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models

## Quick Facts
- arXiv ID: 2507.00026
- Source URL: https://arxiv.org/abs/2507.00026
- Reference count: 40
- Primary result: ROSE framework achieves over 30% improvement in integrated metrics for generating adversarial prompts

## Executive Summary
ROSE introduces a reality-oriented safety evaluation framework for large language models that addresses the limitations of static benchmarks and existing automated methods. The framework uses multi-objective reinforcement learning to generate adversarial prompts that are both topically diverse and contextually rich, overcoming the brittleness of single-prompt attacks. By incorporating a topic-level diversity metric based on LLM-based safety guard embeddings and a consistency reward to ensure prompts are grounded in realistic scenarios, ROSE significantly outperforms baseline methods in generating more varied and realistic adversarial prompts for safety evaluation.

## Method Summary
ROSE employs a multi-objective reinforcement learning framework to generate adversarial prompts for LLM safety evaluation. The system uses a generator to create prompts and an evaluator to provide rewards based on two key objectives: topic-level diversity and contextual consistency. The diversity objective uses LLM-based embeddings to ensure prompts cover varied safety-relevant topics, while the consistency reward ensures prompts are grounded in realistic scenarios. The framework is trained through iterative interaction between the generator and evaluator, with the RL objective balancing the trade-off between prompt diversity and contextual coherence to produce more effective adversarial prompts for safety testing.

## Key Results
- ROSE achieves over 30% improvement in integrated metrics compared to baseline methods
- The framework generates more topically diverse adversarial prompts than existing approaches
- Prompts produced by ROSE demonstrate better contextual richness and realism in safety evaluations

## Why This Works (Mechanism)
ROSE works by addressing the fundamental limitations of static safety benchmarks through dynamic prompt generation that maintains both diversity and realism. The multi-objective reinforcement learning framework enables the system to explore the trade-off space between generating varied adversarial prompts and ensuring they remain contextually grounded. By using LLM-based embeddings for topic diversity metrics, ROSE can automatically assess and optimize for topical coverage without manual annotation, while the consistency reward mechanism ensures generated prompts reflect realistic scenarios rather than artificial edge cases.

## Foundational Learning

**Multi-objective Reinforcement Learning**
- Why needed: To balance competing objectives of diversity and consistency in prompt generation
- Quick check: Verify that the reward function properly weights both diversity and consistency terms

**LLM-based Safety Guard Embeddings**
- Why needed: To automatically assess topic diversity without manual annotation
- Quick check: Confirm embedding space captures meaningful safety-relevant distinctions

**Adversarial Prompt Generation**
- Why needed: To stress-test LLM safety systems beyond static benchmark limitations
- Quick check: Ensure generated prompts actually trigger safety violations in target models

## Architecture Onboarding

**Component Map**
ROSE Generator -> Multi-objective RL Engine -> LLM-based Evaluator -> Reward Signals -> Generator Update

**Critical Path**
Generator produces prompts → Evaluator computes diversity and consistency rewards → RL agent updates policy → Improved prompts generated

**Design Tradeoffs**
- Diversity vs consistency: Balancing prompt variety against realistic scenarios
- Computational cost vs evaluation coverage: More iterations improve quality but increase runtime
- LLM embedding quality vs metric reliability: Depends on embedding model performance

**Failure Signatures**
- Mode collapse: Generator produces repetitive prompts despite diversity objective
- Unrealistic prompts: High diversity but low consistency scores
- Reward hacking: System optimizes metrics without improving actual safety evaluation

**First Experiments**
1. Ablation study removing diversity objective to measure its contribution
2. Comparison against static benchmark performance
3. Human evaluation of prompt realism and diversity

## Open Questions the Paper Calls Out
None

## Limitations

- Reliance on LLM-based embeddings introduces potential subjectivity and model-specific biases
- Limited discussion of how well generated prompts translate to real-world attack scenarios
- Performance against state-of-the-art defense mechanisms not thoroughly explored

## Confidence

**High confidence:** The technical implementation of the multi-objective RL framework with its diversity and consistency objectives is well-defined and reproducible.

**Medium confidence:** The reported performance improvements over baselines are plausible given the framework's design, but the absolute impact on real-world safety evaluation needs more validation.

**Low confidence:** The claim that ROSE-generated prompts are "topically diverse" and "contextually rich" in ways that meaningfully improve safety evaluation beyond existing methods, as this requires extensive human evaluation and real-world testing.

## Next Checks

1. Conduct blind human evaluation studies comparing ROSE-generated prompts against manually crafted adversarial prompts to verify their realism and diversity claims.

2. Test the framework's effectiveness against multiple deployed LLM safety systems (not just models) to validate practical utility.

3. Perform ablation studies to quantify the relative contributions of the diversity metric and consistency reward components to the overall performance improvement.