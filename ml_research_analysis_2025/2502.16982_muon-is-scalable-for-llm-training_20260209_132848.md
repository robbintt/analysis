---
ver: rpa2
title: Muon is Scalable for LLM Training
arxiv_id: '2502.16982'
source_url: https://arxiv.org/abs/2502.16982
tags:
- muon
- training
- adamw
- weight
- update
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces key techniques to scale up the Muon optimizer
  for large language model (LLM) training. The authors identified two crucial improvements:
  (1) adding weight decay to stabilize training and (2) adjusting per-parameter update
  scales to maintain consistent update magnitudes across different parameter shapes.'
---

# Muon is Scalable for LLM Training

## Quick Facts
- arXiv ID: 2502.16982
- Source URL: https://arxiv.org/abs/2502.16982
- Authors: Jingyuan Liu; Jianlin Su; Xingcheng Yao; Zhejun Jiang; Guokun Lai; Yulun Du; Yidao Qin; Weixin Xu; Enzhe Lu; Junjie Yan; Yanru Chen; Huabin Zheng; Yibo Liu; Shaowei Liu; Bohong Yin; Weiran He; Han Zhu; Yuzhi Wang; Jianzhou Wang; Mengnan Dong; Zheng Zhang; Yongsheng Kang; Hao Zhang; Xinran Xu; Yutao Zhang; Yuxin Wu; Xinyu Zhou; Zhilin Yang
- Reference count: 40
- Key outcome: Muon optimizer achieves ~2× computational efficiency compared to AdamW for LLM training when properly scaled with weight decay and per-parameter update scaling

## Executive Summary
This paper introduces key techniques to scale up the Muon optimizer for large language model (LLM) training. The authors identified two crucial improvements: (1) adding weight decay to stabilize training and (2) adjusting per-parameter update scales to maintain consistent update magnitudes across different parameter shapes. These modifications allow Muon to work out-of-the-box without hyperparameter tuning. The authors developed an efficient distributed implementation with reduced communication overhead and demonstrated that Muon achieves approximately 2× computational efficiency compared to AdamW under compute-optimal training settings. Based on these improvements, they trained Moonlight, a 3B/16B-parameter MoE model on 5.7T tokens, which advances the performance frontier compared to prior models while requiring fewer training FLOPs. The authors also released their distributed Muon implementation and model checkpoints.

## Method Summary
The authors scaled Muon by adding decoupled weight decay (0.1) to prevent numerical instability during long training runs, and by adjusting per-parameter update scales using a factor of 0.2·√max(A,B) to normalize update RMS across matrix shapes. The core Muon update uses Newton-Schulz orthogonalization (5 iterations) on gradient momentum with momentum coefficient 0.95. The implementation uses ZeRO-1 distributed optimization with an additional DP Gather operation to reconstruct full gradient matrices for orthogonalization. Muon handles matrix parameters while AdamW handles embeddings, RMSNorm, and LM head. The authors trained Moonlight (3B/16B MoE) on 5.7T tokens using a 3-stage learning rate schedule, achieving 2× compute efficiency compared to AdamW baselines.

## Key Results
- Muon achieves ~2× computational efficiency compared to AdamW under compute-optimal training settings
- Moonlight (3B/16B MoE) advances performance frontier with fewer training FLOPs than comparable models
- SVD entropy analysis shows Muon maintains higher spectral diversity across weight matrices, especially in MoE router weights
- Weight decay is critical for Muon stability, preventing weight/activation explosion in long training runs
- Distributed Muon implementation achieves 1.0–1.25× communication overhead relative to AdamW

## Why This Works (Mechanism)

### Mechanism 1: Matrix Orthogonalization via Newton-Schulz Iteration
- Claim: Orthogonalizing gradient momentum prevents optimization from collapsing onto a few dominant directions, improving spectral diversity of learned weight matrices.
- Mechanism: Given momentum matrix $M_t$, Newton-Schulz iteration approximates $(M_t M_t^T)^{-1/2} M_t = U V^T$, which orthogonalizes the update. This constrains optimization under a spectral norm rather than element-wise norms.
- Core assumption: Spectral norm constraints are more appropriate for weight matrices acting as operators on Euclidean spaces than Adam's per-element adaptive scaling.
- Evidence anchors:
  - [abstract] "Muon optimizer based on matrix orthogonalization has demonstrated strong results"
  - [Section 2.1] "orthogonalization can ensure that the update matrices are isomorphic, preventing the weight from learning along a few dominant directions"
  - [Section 3.4] SVD entropy analysis shows Muon maintains higher spectral diversity across weight matrices, with strongest effects in MoE router weights
  - [corpus] "On the Convergence Analysis of Muon" provides theoretical treatment; most corpus papers extend/validate Muon but remain early-stage (0 citations)
- Break condition: If weight matrices are not full-rank or gradient momentum has very small singular values, Newton-Schulz may converge slowly or produce inaccurate orthogonalization.

### Mechanism 2: Weight Decay for Numerical Stability at Scale
- Claim: Adding weight decay prevents unbounded growth of weight magnitudes and layer output RMS, which otherwise exceed bf16 precision range and degrade performance in long training runs.
- Mechanism: Standard decoupled weight decay (as in AdamW) subtracts $\lambda W_{t-1}$ from updates, regularizing weight magnitude growth over extended training.
- Core assumption: The bf16 precision ceiling is the binding constraint on performance in over-trained regimes without decay.
- Evidence anchors:
  - [abstract] "adding weight decay" identified as crucial scaling technique
  - [Section 2.2] "both the weight and the layer output's RMS keep growing to a large scale, exceeding the high-precision range of bf16"
  - [Figure 2] Validation loss curves show Muon without weight decay initially converges faster but underperforms in over-train regime
  - [corpus] "Iterative Orthogonalization Scaling Laws" discusses weight decay hyperparameter transfer; corpus evidence is nascent
- Break condition: Excessive weight decay may over-regularize and limit model capacity; too little allows numerical instability to recur.

### Mechanism 3: Per-Parameter Update Scaling for Shape-Consistent Updates
- Claim: Scaling each matrix's update by $\sqrt{\max(A, B)}$ counteracts shape-dependent RMS variation, ensuring consistent update magnitudes across layers with different parameter dimensions.
- Mechanism: By Lemma 1, Muon's theoretical update RMS is $\sqrt{1/\max(A, B)}$. Multiplying by $\sqrt{\max(A, B)}$ normalizes to consistent ~0.2 RMS, matching AdamW's empirical range.
- Core assumption: Consistent update RMS across parameter shapes improves optimization, and matching AdamW's 0.2–0.4 RMS range enables hyperparameter transfer.
- Evidence anchors:
  - [abstract] "adjusting the per-parameter update scale" enables out-of-the-box use
  - [Section 2.2] "When max(A, B) is too large... updates become too small; When max(A, B) is too small... updates become too large"
  - [Table 1] "Adjusted LR" method achieves better training/validation loss than baseline
  - [corpus] Limited direct validation; concurrent works noted in paper footnote
- Break condition: Non-matrix parameters (embeddings, norms) still require AdamW; inconsistent scaling between matrix/non-matrix params may cause subtle optimization pathologies.

## Foundational Learning

- Concept: **Newton-Schulz Iteration for Matrix Orthogonalization**
  - Why needed here: This is the core computational primitive Muon uses to orthogonalize momentum. Understanding convergence properties helps diagnose when iterations are insufficient.
  - Quick check question: Given a matrix $X$ with singular values [0.1, 0.5, 0.9], how many Newton-Schulz iterations would you need to approximate its orthogonal component within 1% error?

- Concept: **Singular Value Decomposition (SVD) and Spectral Norms**
  - Why needed here: Muon operates on spectral properties of weight matrices. SVD entropy analysis is used to validate the mechanism.
  - Quick check question: If a weight matrix has singular values [10, 1, 0.1], what is its spectral norm, and how would orthogonalization change the update direction?

- Concept: **ZeRO-1 Distributed Optimization**
  - Why needed here: Scaling Muon requires partitioning optimizer states across devices while maintaining matrix-wholeness for orthogonalization.
  - Quick check question: In ZeRO-1, what is partitioned across workers? Why does Muon require additional "DP Gather" compared to AdamW?

## Architecture Onboarding

- Component map: Gradient computation -> Reduce-scatter on DP group -> Momentum update -> **DP Gather** to reconstruct full matrix -> Newton-Schulz orthogonalization -> Extract local partition -> Scaled update with weight decay -> All-gather updated parameters

- Critical path:
  1. Gradient computation → reduce-scatter on DP group
  2. Momentum update with local partitioned buffer
  3. **DP Gather** to reconstruct full gradient matrix (critical addition)
  4. Newton-Schulz orthogonalization on full matrix
  5. Extract local partition, apply scaled update
  6. All-gather updated parameters

- Design tradeoffs:
  - Memory: Muon uses 1 momentum buffer vs. AdamW's 2 → ~50% optimizer memory reduction
  - Communication: 1.0–1.25× AdamW's communication volume (bf16 gather adds ~50% overhead)
  - Latency: Additional Newton-Schulz compute + gather, but only 1–3% of forward-backward time
  - Hyperparameters: Can reuse AdamW learning rates; weight decay critical; momentum fixed at 0.95

- Failure signatures:
  - **Exploding attention logits**: Max attention logit >100 in early training (monitor ratio of logits >100 per batch)
  - **RMS drift**: Weight RMS or layer output RMS growing beyond bf16 range without weight decay
  - **Shape-dependent divergence**: Small matrices (e.g., GQA heads) training unstably → check update scaling
  - **Optimizer mismatch penalty**: Pretraining with AdamW + SFT with Muon (or vice versa) underperforms consistent optimizer use

- First 3 experiments:
  1. **Small-scale validation**: Train 100M parameter model on 1B tokens with Muon vs. AdamW; verify loss curves and SVD entropy of weight matrices
  2. **Update RMS sweep**: Test controlled update RMS values [0.05, 0.1, 0.2, 0.4, 0.8] on 800M model; confirm 0.2–0.4 range matches AdamW baseline
  3. **Distributed overhead benchmark**: Profile end-to-end step time with Distributed Muon vs. Distributed AdamW at target scale; verify latency overhead <3%

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the underlying mechanisms causing the pretraining-finetuning optimizer mismatch, where models pretrained with AdamW underperform when fine-tuned with Muon (and vice versa)?
- Basis in paper: [explicit] Section 4 states: "A notable phenomenon observed in practice is the suboptimal performance of models pretrained with AdamW when fine-tuned with Muon, and vice versa... A precise understanding of the underlying mechanisms is essential for devising robust and effective solutions."
- Why unresolved: The paper identifies the problem through SFT ablations (Tables 6-7) but provides no theoretical explanation for why optimizer switching degrades performance.
- What evidence would resolve it: A theoretical analysis connecting optimizer-specific weight matrix structure to fine-tuning dynamics, plus experiments showing how to transfer AdamW checkpoints to Muon effectively.

### Open Question 2
- Question: Why does Muon achieve particularly strong gains on mathematical reasoning and code generation benchmarks?
- Basis in paper: [explicit] Section 3.3 states: "We observed that Muon especially excels on Math and Code related tasks, and we encourage the research community to further investigate this phenomena."
- Why unresolved: The paper documents the empirical finding (e.g., +8.9 HumanEval over Moonlight-A) but offers no hypothesis for this domain-specific advantage.
- What evidence would resolve it: Analysis correlating task-specific gradient structures with Muon's orthogonalization properties, or controlled experiments isolating whether the benefit stems from training data characteristics.

### Open Question 3
- Question: Can Muon be extended to optimize all parameter types (embeddings, RMSNorm, scalars) without relying on AdamW as a hybrid component?
- Basis in paper: [explicit] Section 4: "Currently, the Muon optimizer is utilized in conjunction with the Adam optimizer, where certain parameters remain under the purview of Adam optimization... The integration of the optimization of all parameters exclusively within the Muon framework is a topic of significant research interest."
- Why unresolved: Muon operates on matrix-structured parameters; non-matrix parameters like embeddings and normalization weights currently require AdamW.
- What evidence would resolve it: A principled extension of orthogonalization or Schatten-norm constraints to vector/scalar parameters, validated on full-model training without hybrid optimization.

## Limitations

- The theoretical convergence guarantees of Muon in practical deep learning settings remain partially open questions
- The 2× compute efficiency claim relies on a proprietary baseline dataset, limiting reproducibility
- The distributed implementation overhead figures are specific to the reported cluster configuration and may not generalize
- Muon currently requires hybrid optimization with AdamW for non-matrix parameters like embeddings and normalization weights

## Confidence

**High Confidence**: The core mechanism of Newton-Schulz orthogonalization for gradient momentum is mathematically sound and well-established. The empirical demonstration that Muon without weight decay exhibits numerical instability (weight/activation explosion) is clearly shown in Figure 2. The per-parameter update scaling based on √max(A,B) to normalize update RMS is mathematically derived from Lemma 1 and validated through ablation studies.

**Medium Confidence**: The 2× compute efficiency claim relative to AdamW is supported by strong empirical evidence on the target architecture but relies on a proprietary baseline dataset. The generalization of scaling law findings to other architectures and data distributions remains to be fully established. The distributed implementation overhead figures are specific to the reported cluster configuration.

**Low Confidence**: Theoretical convergence properties of Muon in practical deep learning settings are not fully characterized. The interaction between Muon's spectral norm constraints and modern architectural innovations (attention mechanisms, normalization layers) is not systematically explored. The optimal iteration count for Newton-Schulz (fixed at N=5) is heuristic rather than theoretically derived.

## Next Checks

1. **Cross-Architecture Transfer**: Validate Muon's compute efficiency claim on a different MoE architecture (e.g., Mixtral 8x7B) and a dense transformer variant (e.g., Llama-3 8B) using publicly available datasets. Compare validation loss curves and training throughput to establish architecture-agnostic benefits.

2. **Hyperparameter Sensitivity Analysis**: Systematically vary the Newton-Schulz iteration count (N=3, 5, 7, 10) and momentum coefficient (µ=0.9, 0.95, 0.98) to identify optimal settings across different parameter scales. Measure convergence speed, final validation loss, and numerical stability.

3. **Distributed Overhead Characterization**: Profile Muon's communication and compute overhead across different cluster configurations (varying node counts, network bandwidths). Identify scaling regimes where gather operations become bottlenecks and evaluate optimization strategies (e.g., hierarchical gathers, compression).