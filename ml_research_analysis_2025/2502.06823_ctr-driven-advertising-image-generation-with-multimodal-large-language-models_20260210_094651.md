---
ver: rpa2
title: CTR-Driven Advertising Image Generation with Multimodal Large Language Models
arxiv_id: '2502.06823'
source_url: https://arxiv.org/abs/2502.06823
tags:
- product
- image
- advertising
- images
- background
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CAIG, a method using Multimodal Large Language
  Models (MLLMs) for CTR-driven advertising image generation. The authors address
  the limitation of existing methods focusing on aesthetics without optimizing for
  Click-Through Rate (CTR).
---

# CTR-Driven Advertising Image Generation with Multimodal Large Language Models

## Quick Facts
- **arXiv ID:** 2502.06823
- **Source URL:** https://arxiv.org/abs/2502.06823
- **Reference count:** 40
- **Primary result:** 2% CTR improvement in online A/B tests with over 60 million impressions

## Executive Summary
This paper introduces CAIG, a method using Multimodal Large Language Models (MLLMs) for CTR-driven advertising image generation. The authors address the limitation of existing methods focusing on aesthetics without optimizing for Click-Through Rate (CTR). They propose a two-stage approach: first, pre-training MLLMs on a large e-commerce dataset with targeted tasks to incorporate domain knowledge; second, fine-tuning the models through Reinforcement Learning using a novel reward model that simulates user click preferences. A product-centric preference optimization strategy ensures generated backgrounds align with product characteristics. Experiments on both public and commercial datasets show state-of-the-art performance, with Pair Accuracy reaching 58.6% on commercial data and 56.2% on public data. Online A/B tests demonstrate a 2% CTR improvement with over 60 million impressions.

## Method Summary
CAIG employs a two-stage approach to CTR-driven advertising image generation. The first stage involves pre-training MLLMs on a large e-commerce dataset with targeted tasks to incorporate domain knowledge. The second stage fine-tunes the models through Reinforcement Learning using a novel reward model that simulates user click preferences. A product-centric preference optimization strategy ensures generated backgrounds align with product characteristics. The method addresses the limitation of existing approaches that focus on aesthetics without optimizing for CTR, resulting in more effective advertising images.

## Key Results
- Pair Accuracy reaches 58.6% on commercial data and 56.2% on public data
- 2% CTR improvement in online A/B tests with over 60 million impressions
- State-of-the-art performance compared to existing methods

## Why This Works (Mechanism)
The method works by integrating CTR optimization directly into the image generation process through a reward-based fine-tuning approach. By pre-training on e-commerce data, the model learns product-specific knowledge that informs background generation. The reinforcement learning stage with click preference simulation allows the model to iteratively improve based on what drives user engagement rather than just visual appeal. The product-centric optimization ensures that generated images maintain coherence between products and their backgrounds, which is critical for effective advertising.

## Foundational Learning
- **Multimodal Large Language Models**: Models that can process and generate both text and images, essential for understanding product descriptions and generating corresponding visuals
- **Reinforcement Learning for image generation**: Technique that uses reward signals to guide the generation process toward desired outcomes (higher CTR)
- **CTR optimization**: Focus on click-through rate as the primary metric rather than traditional aesthetic quality measures
- **Product-centric background generation**: Ensuring visual coherence between products and their surroundings to enhance advertising effectiveness
- **Preference optimization**: Techniques for aligning generated content with user preferences through iterative refinement
- **E-commerce domain adaptation**: Tailoring general MLLM capabilities to the specific needs and characteristics of online retail advertising

## Architecture Onboarding

**Component Map:** E-commerce dataset -> Pre-training tasks -> MLLM backbone -> Reward model -> RL fine-tuning -> CTR-optimized image generation

**Critical Path:** The sequence from reward model evaluation through RL fine-tuning directly impacts the final CTR performance, making this the most critical path for optimization and debugging.

**Design Tradeoffs:** The approach balances between leveraging pre-trained MLLM capabilities and adapting them for CTR optimization through RL. The product-centric preference optimization adds complexity but ensures better alignment with advertising goals.

**Failure Signatures:** Poor performance on Pair Accuracy metrics indicates issues with either the pre-training domain knowledge incorporation or the reward model's ability to capture click preferences. Low CTR improvements in A/B tests suggest misalignment between simulated preferences and actual user behavior.

**First Experiments:**
1. Evaluate pre-trained MLLM performance on e-commerce product description understanding tasks
2. Test reward model accuracy in predicting click preferences on held-out data
3. Conduct ablation study removing product-centric optimization to measure its contribution

## Open Questions the Paper Calls Out
None

## Limitations
- The two-stage approach relies heavily on the quality and representativeness of the pre-training e-commerce dataset
- The reinforcement learning setup depends on a simulated reward model that approximates user click preferences
- Online A/B test results lack detailed statistical significance analysis and control for potential confounding factors

## Confidence
- **MLLM-based CTR optimization methodology**: High confidence
- **Performance on benchmark datasets**: Medium confidence
- **Online A/B test results**: Low confidence

## Next Checks
1. Conduct ablation studies to quantify the individual contributions of pre-training domain knowledge, reward modeling, and product-centric optimization to overall CTR performance
2. Implement human preference studies comparing images generated by CAIG against baselines, measuring not just click-through rates but also subjective quality assessments
3. Perform extended online A/B tests across multiple time periods and product categories to verify the robustness of CTR improvements and establish confidence intervals for the reported 2% gain