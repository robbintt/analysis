---
ver: rpa2
title: Model Predictive Control is almost Optimal for Heterogeneous Restless Multi-armed
  Bandits
arxiv_id: '2511.08097'
source_url: https://arxiv.org/abs/2511.08097
tags:
- policy
- problem
- proof
- optimal
- will
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proves that the LP-update policy is almost asymptotically
  optimal for the infinite-horizon heterogeneous restless multi-armed bandit problem
  under an easily verifiable ergodicity assumption. The LP-update policy solves a
  finite-horizon linear program and uses randomized rounding to produce decisions,
  and the authors show it achieves an $O(\log N / \sqrt{N})$ optimality gap as the
  number of arms $N$ grows.
---

# Model Predictive Control is almost Optimal for Heterogeneous Restless Multi-armed Bandits

## Quick Facts
- arXiv ID: 2511.08097
- Source URL: https://arxiv.org/abs/2511.08097
- Reference count: 40
- Primary result: LP-update policy achieves O(log N / √N) optimality gap for heterogeneous RMABs without requiring uniform global attractor assumptions

## Executive Summary
This paper introduces the LP-update policy for infinite-horizon heterogeneous restless multi-armed bandits, proving it is almost asymptotically optimal with an O(log N / √N) gap. The policy solves a finite-horizon linear program and uses randomized rounding to produce decisions, overcoming limitations of classical index policies that require uniform global attractor properties. The key innovation uses dissipativity from model predictive control to show that a finite-horizon LP can approximate the infinite-horizon optimal policy. The approach is validated through numerical experiments demonstrating strong performance, particularly in cases where standard LP-priority policies fail.

## Method Summary
The LP-update policy operates at each time step by solving a finite-horizon linear program (Eq. 7-10) with horizon τ and initial state x(s(t)). The solution provides a fractional action vector y^FL(t), which is converted to binary decisions via randomized rounding. The algorithm maintains a budget constraint of αN arms pulled per step. The LP formulation relaxes binary constraints to continuous values, making the problem tractable while preserving near-optimal performance through careful analysis of the gap between fluid and stochastic solutions.

## Key Results
- LP-update policy achieves O(log N / √N) optimality gap as N grows
- Performance is robust to heterogeneous arm distributions without requiring uniform global attractor assumptions
- Finite horizon τ=4-5 is typically sufficient for near-optimal performance
- LP-priority policy (τ=1) can fail dramatically when UGAP is violated
- Theoretical analysis leverages dissipativity from model predictive control

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A finite-horizon Linear Program (LP) can effectively approximate the infinite-horizon optimal policy without requiring uniform global attractor (UGAP) assumptions.
- **Mechanism:** The paper utilizes **dissipativity**, a concept from control theory, to define a "rotated cost" function $\tilde{c}(x,u)$. This function acts as a surrogate loss that constrains the system's trajectory. If the finite-horizon solution minimizes this rotated cost, the system remains "close" to the optimal fixed point $y^*$ in value, effectively stitching finite plans into an infinite horizon policy.
- **Core assumption:** The system must satisfy **uniform ergodicity** (Assumption 1), specifically $\rho_k > 0$ for some integer $k$, ensuring the transition kernel under the 0-action is ergodic.
- **Evidence anchors:** [abstract] "Our theoretical results draw on techniques from the model predictive control literature by invoking the concept of dissipativity..."; [Section 5.2] "Dissipativity... provides us with an equivalent surrogate loss function..."
- **Break condition:** If the system is not dissipative or the ergodicity constant $\rho_k$ is too small, the horizon $\tau$ required to bound the error $\epsilon$ may grow intractably large.

### Mechanism 2
- **Claim:** The "Jensen Gap" allows the algorithm to handle heterogeneous arms where standard concentration bounds fail.
- **Mechanism:** Standard methods often rely on homogeneity to use concentration of measure. Because arms are heterogeneous (statistically different), the authors bound the difference between the deterministic fluid value $W^{FL}(t,x)$ and its stochastic expectation $\mathbb{E}[W^{FL}(t, X)]$ using the **Jensen Gap**. Since the value function is concave, the Jensen inequality implies this gap is manageable ($O(t \sqrt{\log N / N})$), allowing deterministic planning to guide stochastic execution.
- **Core assumption:** Lipschitz continuity of the bias function $h^\star(\cdot)$ and the finite-horizon value function $W^{FL}$.
- **Evidence anchors:** [Section 5.3] "Due to the heterogeneity... we are unable to directly leverage concentration results... our third result stems from a 'inverse' Jensen inequality."
- **Break condition:** If the value function $W^{FL}$ were not sufficiently smooth or Lipschitz, the gap between fluid and stochastic realization would diverge as $N$ grows.

### Mechanism 3
- **Claim:** Randomized rounding preserves the optimality gap while satisfying strict budget constraints.
- **Mechanism:** The LP-update policy solves for a fractional "fluid" action $y^{FL}(t)$ (e.g., "pull arm $i$ with probability 0.7"). A **randomized rounding** procedure converts this to a binary decision vector $A(t)$. Because the marginal probabilities match the fluid solution, the expected error is bounded by $O(1/N)$, ensuring the constraint $\alpha N$ is met in expectation and violated by at most 1 with high probability.
- **Core assumption:** Independence of arm transitions and a valid coupling scheme for the rounding.
- **Evidence anchors:** [Section 3.2] "The LP-update policy... uses a randomized rounding procedure... l1 norm distance... is bounded in expectation."
- **Break condition:** If the rounding procedure introduced a systematic bias or if variance exploded (violating McDiarmid's inequality conditions), the budget constraint would be frequently violated.

## Foundational Learning

- **Concept:** Restless Multi-Armed Bandits (RMAB) & Indexability
  - **Why needed here:** The paper frames its solution against the classical Whittle Index. Understanding that standard Whittle indices require "indexability" and "UGAP" (Uniform Global Attractor Property) is necessary to appreciate why the LP-update is superior for complex (heterogeneous) environments.
  - **Quick check question:** Why does the LP-priority policy (which corresponds to $\tau=1$) fail in the counter-examples provided in the paper?

- **Concept:** Model Predictive Control (MPC)
  - **Why needed here:** The algorithm is fundamentally MPC-based: it re-optimizes at every time step $t$ over a finite horizon $\{t \dots t+\tau\}$ rather than computing a stationary policy once. Understanding the "receding horizon" concept is crucial.
  - **Quick check question:** What happens to the computational complexity if the horizon $\tau$ is set to the total time horizon $T$ vs. a constant like 5?

- **Concept:** Linear Programming Relaxation
  - **Why needed here:** The "LP" in LP-update comes from relaxing the hard binary constraints ($a_i \in \{0,1\}$) into soft, fractional constraints ($y_{s,a} \in [0,1]$). This converts a PSPACE-hard problem into a tractable LP that provides an upper bound on the optimal value.
  - **Quick check question:** How does the paper guarantee that the solution to the relaxed LP (fluid solution) translates back to a feasible integer solution for the original problem?

## Architecture Onboarding

- **Component map:**
  State Encoder -> LP Solver (Core) -> Lagrange Multiplier Manager -> Randomized Rounding Engine -> Execute & Transition

- **Critical path:**
  Observe State -> Encode to $x(t)$ -> Solve $\tau$-horizon LP -> Extract First Action Vector $y^{FL}(0)$ -> Round to $A(t)$ -> Execute & Transition

- **Design tradeoffs:**
  - **Horizon $\tau$:** The paper suggests $\tau=4$ or $5$ is often sufficient. Lower $\tau$ is faster but may fail on non-UGAP instances; higher $\tau$ tightens the bound but increases computation linearly.
  - **Rounding Strategy:** The paper uses independent randomized rounding. One could swap this for a specific "water-filling" implementation if strict adherence to the budget $\alpha N$ is required at every single step (not just in expectation).

- **Failure signatures:**
  - **UGAP Violation ($\tau$ too small):** If the system has multiple attractors and $\tau$ is set to 1 (mimicking LP-priority), the policy oscillates or converges to a suboptimal fixed point (see "Counter-example-Yan").
  - **Ergodicity Break:** If Assumption 1 fails (e.g., absorbing states that cannot be exited under action 0), the Lipschitz constants of the value function blow up, and the optimality guarantee is void.

- **First 3 experiments:**
  1. **Benchmark $\tau$ Sensitivity:** Replicate Figure 3 using the "Counter-example-Hong" matrices (Appendix H) to verify the threshold behavior of $\tau$ (specifically the jump from $\tau=3$ to $\tau=4$).
  2. **Scaling Verification:** Generate $N=10$ to $N=100$ random heterogeneous arms and plot the "Normalized Average Reward" to confirm the $O(\log N / \sqrt{N})$ convergence rate empirically.
  3. **Rounding Variance:** Implement the rounding logic and measure the standard deviation of the budget usage $\sum A(t)$ over time to verify it stays within the $O(1/N)$ bound claimed in Lemma 2.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Is the planning horizon $\tau(\epsilon)$ independent of the number of arms $N$, depending only on the maximum state space size, budget, and ergodicity constants?
- **Basis in paper:** [explicit] Conjecture 1 (Page 9) posits this independence, and the Conclusion (Page 19) identifies resolving this conjecture as "the most direct question" arising from the analysis.
- **Why unresolved:** The current theoretical analysis only guarantees the existence of a horizon $\tau$ for a given $\epsilon$ but does not characterize its dependence on $N$, whereas simulations suggest a small, constant $\tau$ suffices.
- **What evidence would resolve it:** A formal proof demonstrating that $\tau(\epsilon)$ can be bounded independently of $N$, or a counter-example showing the horizon must scale with system size.

### Open Question 2
- **Question:** Can the planning horizon $\tau$ be optimized for the LP-update policy to balance computational cost and asymptotic optimality?
- **Basis in paper:** [explicit] The Conclusion (Page 19) explicitly asks, "can one optimize $\tau$ for the LP-update?" following the discussion on Conjecture 1.
- **Why unresolved:** While simulations indicate good performance with small fixed values (e.g., $\tau=5$), there is currently no theoretical framework for selecting the optimal horizon dynamically.
- **What evidence would resolve it:** An algorithm or heuristic that selects $\tau$ based on system parameters to minimize regret or computational complexity while maintaining optimality guarantees.

### Open Question 3
- **Question:** Can a computational method be derived that avoids solving a linear program at every time step while maintaining the properties of the LP-update policy?
- **Basis in paper:** [explicit] Remark 5 (Page 16) notes that the fixed priority order of the LP-priority policy suggests the possibility of avoiding per-step LP computation, stating, "This is a line of future work for us."
- **Why unresolved:** The LP-update policy currently requires solving a finite-horizon LP at each decision epoch, which is computationally intensive compared to static priority indices.
- **What evidence would resolve it:** The derivation of a closed-form or lookup-table policy that replicates the LP-update decisions without requiring real-time optimization solvers.

### Open Question 4
- **Question:** Can the LP-update policy framework be effectively extended to the reinforcement learning setting where model parameters (transition kernels and rewards) are unknown?
- **Basis in paper:** [explicit] The Conclusion (Page 19) states that optimizing the horizon and computational method are questions "highly relevant... in the more complex learning setting."
- **Why unresolved:** The current proof assumes full knowledge of model parameters to solve the finite-horizon LP, and the interplay between estimation error (in learning) and the planning horizon $\tau$ is not analyzed.
- **What evidence would resolve it:** Theoretical bounds on the regret of an "LP-update-learning" algorithm that estimates parameters online, or an empirical study demonstrating the policy's robustness to estimation errors.

## Limitations

- The paper relies on uniform ergodicity (Assumption 1), which is stated as easily verifiable but lacks a practical test method
- The randomized rounding mechanism depends on external references, creating incomplete specification for faithful reproduction
- The O(log N/√N) optimality gap assumes ergodicity holds, but degradation when ergodicity is weak is not quantified

## Confidence

- **High:** Dissipativity enabling MPC-style finite-horizon planning (Sections 5.2-5.3 proofs are well-supported)
- **Medium:** Randomized rounding preserving feasibility (depends on external references [GN25] Appendix B.1 and [IY16])
- **High:** Jensen-gap-based error analysis for heterogeneous arms (well-supported by proofs in Section 5.3)

## Next Checks

1. Verify the threshold behavior for τ by replicating Figure 3 with Counter-example-Hong matrices to confirm the jump from τ=3 to τ=4.
2. Implement the randomized rounding procedure and measure budget constraint violations across 1000 trials to verify the O(1/N) bound.
3. Test the algorithm on a non-ergodic instance (e.g., absorbing states) to observe how the optimality gap behaves when Assumption 1 fails.