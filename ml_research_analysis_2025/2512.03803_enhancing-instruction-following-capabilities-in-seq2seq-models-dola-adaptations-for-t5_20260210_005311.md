---
ver: rpa2
title: 'Enhancing Instruction-Following Capabilities in Seq2Seq Models: DoLA Adaptations
  for T5'
arxiv_id: '2512.03803'
source_url: https://arxiv.org/abs/2512.03803
tags:
- dola
- layers
- steering
- decoder
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates why encoder-decoder models such as FLAN-T5\
  \ often fail to follow instructions when they conflict with memorized training continuations.\
  \ The authors adapt DoLa to FLAN-T5 to study how decoder representations evolve\
  \ with depth and develop a gradient-based activation steering method that injects\
  \ an \u201Cinstruction-compliance\u201D direction into mid-decoder layers."
---

# Enhancing Instruction-Following Capabilities in Seq2Seq Models: DoLA Adaptations for T5

## Quick Facts
- **arXiv ID:** 2512.03803
- **Source URL:** https://arxiv.org/abs/2512.03803
- **Reference count:** 40
- **Primary result:** Activation steering dramatically improves MemoTrap accuracy (52% to 99.7%) in FLAN-T5 by injecting instruction-compliance directions into mid-decoder layers.

## Executive Summary
This work investigates why encoder-decoder models like FLAN-T5 often fail to follow instructions when they conflict with memorized training continuations. The authors adapt DoLa to FLAN-T5 to study how decoder representations evolve with depth and develop a gradient-based activation steering method that injects an "instruction-compliance" direction into mid-decoder layers. DoLa performs inconsistently across tasks, improving on some instruction types but failing on others due to volatile intermediate token preferences. In contrast, targeted activation steering dramatically improves MemoTrap performance by steering in mid-depth layers where representations are informative but not yet committed. The findings demonstrate that mechanistic steering succeeds where contrastive decoding fails in Seq2Seq architectures.

## Method Summary
The authors adapt DoLa to FLAN-T5 to analyze decoder representation evolution and develop gradient-based activation steering. For steering, they compute contrastive loss gradients between instruction-compliant and memorized tokens using 100 mining examples, extract a steering vector from mid-decoder layer hidden states, and inject it during inference with a strength parameter α=1000. DoLa selects a premature layer based on maximum Jensen-Shannon divergence and applies contrastive decoding with repetition penalty. The study evaluates on MemoTrap Proverb Ending (300 test examples) and IFEval benchmarks using FLAN-T5 models of various sizes.

## Key Results
- Activation steering improves MemoTrap Proverb Ending accuracy from 52% to 99.7% on FLAN-T5-Large
- DoLa performs inconsistently across tasks, sometimes failing due to volatile intermediate token preferences in T5's cross-attention mechanism
- Mid-decoder layers (layer 10 for Large, layer 5 for Base) are optimal for steering, balancing informativeness and malleability
- Steering strength α=1000 provides substantial improvements without degrading fluency

## Why This Works (Mechanism)
The method works by injecting a learned "instruction-compliance" direction into decoder representations at layers where they are still malleable but contain sufficient semantic information. In Seq2Seq models, cross-attention from the encoder to decoder brings instruction information into decoder layers, but intermediate representations can become volatile and revert to memorized patterns. By computing gradients that contrast instruction-compliant tokens against memorized traps, the steering vector explicitly pushes representations toward following instructions rather than defaults.

## Foundational Learning
- **DoLa adaptation for Seq2Seq:** Required to handle cross-attention and sequential decoding; why needed to apply contrastive decoding to T5; quick check: verify layer selection JSD curves differ from decoder-only models.
- **Contrastive loss computation:** Essential for mining steering vectors that distinguish compliant vs. trap behaviors; why needed to create meaningful gradient directions; quick check: confirm loss surface shows clear separation between target and competing tokens.
- **Activation injection at mid-layers:** Critical for balancing representational stability and malleability; why needed because early layers lack information and late layers are committed; quick check: sweep across layers 2-10 to confirm performance peak.
- **Gradient averaging across examples:** Necessary to create a general steering direction rather than task-specific; why needed to avoid overfitting to individual examples; quick check: validate vector stability across different mining set sizes.
- **L2 normalization of steering vectors:** Important for controlling injection magnitude; why needed to prevent arbitrary scaling effects; quick check: compare performance with and without normalization.
- **Contrastive strength λ tuning:** Key parameter for DoLa's effectiveness; why needed to balance original and contrasting logits; quick check: measure accuracy sensitivity across λ values.

## Architecture Onboarding
- **Component map:** Input text → Encoder → Cross-attention → Decoder layers (0,2,4,6,8,10 for Base) → LM Head → Output tokens
- **Critical path:** Encoder representation → Cross-attention at each decoder layer → Hidden state evolution → Token prediction
- **Design tradeoffs:** Steering in decoder vs. encoder (decoder chosen due to cross-attention flow), layer selection (mid-depth balances information and malleability), gradient computation scope (full decoder vs. limited)
- **Failure signatures:** DoLa fails with volatile intermediate logits, activation steering fails if applied too early/late or with wrong strength, both methods may degrade fluency if over-applied
- **First experiments:** 1) Layer-wise ablation of steering injection (2-10) on MemoTrap, 2) Strength sweep of α (100-5000) on small validation set, 3) Comparison of encoder vs. decoder steering injection

## Open Questions the Paper Calls Out
- **Generalization to complex instructions:** Can activation steering generalize to multi-step reasoning or dialogue beyond single-sentence conflicts? The current study's narrow MemoTrap scope leaves complex instruction efficacy unknown.
- **Encoder-side interventions:** How do encoder-side interventions compare to decoder-side injections for establishing instruction compliance? The paper exclusively focuses on decoder-side due to cross-attention but hasn't tested encoder manipulation.
- **Automated steering selection:** Can automated, metric-driven procedures replace manual search for optimal layers and strengths? Current results depend on manual sweeps that are computationally expensive.
- **General behavior vectors:** Is it possible to formulate steering vectors for general behaviors rather than task-specific token contrasts? Current vectors are task-specific and risk overfitting to lexical items.

## Limitations
- Steering vector computation relies on only 100 examples, raising robustness concerns
- Procedure for identifying instruction-compliant versus trap tokens in MemoTrap is not specified
- Gradient computation scope is unclear regarding cross-attention inclusion and position specificity
- Steering optimality based on qualitative observations rather than rigorous ablation studies

## Confidence
- MemoTrap accuracy improvements: Medium confidence (strong empirical gains but narrow training set and unclear steering target selection)
- DoLa failure mode characterization: Medium confidence (plausible explanation but lacks quantitative analysis of token preference volatility)
- Mid-layer steering optimality: Medium confidence (based on qualitative observations rather than systematic ablation)

## Next Checks
1. Verify steering vector computation by reproducing gradients on a subset of MemoTrap examples and confirming L2 normalization
2. Perform layer-wise steering ablation on FLAN-T5-Large to confirm layer 10 is indeed optimal for instruction compliance
3. Test steering robustness by varying the mining set size (50, 100, 200 examples) and measuring MemoTrap accuracy stability