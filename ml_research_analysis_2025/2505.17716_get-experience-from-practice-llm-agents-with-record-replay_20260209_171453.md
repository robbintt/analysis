---
ver: rpa2
title: 'Get Experience from Practice: LLM Agents with Record & Replay'
arxiv_id: '2505.17716'
source_url: https://arxiv.org/abs/2505.17716
tags:
- arxiv
- agent
- experience
- https
- replay
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AgentRR, a novel paradigm that addresses
  the fundamental challenges of modern AI agents through an experience-based record-and-replay
  approach. By decoupling an agent's intelligence from its execution, AgentRR offers
  practical solutions to the core challenges of reliability, privacy, operational
  cost, and execution performance.
---

# Get Experience from Practice: LLM Agents with Record & Replay

## Quick Facts
- **arXiv ID:** 2505.17716
- **Source URL:** https://arxiv.org/abs/2505.17716
- **Reference count:** 40
- **Primary result:** Introduces AgentRR, a record-and-replay paradigm that significantly improves reliability, privacy, and efficiency of LLM agents by decoupling intelligence from execution.

## Executive Summary
This paper presents AgentRR, a novel paradigm that addresses fundamental challenges in modern AI agents through an experience-based record-and-replay approach. By capturing interaction traces and summarizing them into multi-level experiences, AgentRR achieves reliable execution while reducing reliance on expensive LLM computation. The key innovation lies in the hierarchical experience design and the introduction of check functions as a trusted computing base for safety and reliability.

## Method Summary
AgentRR operates through three core phases: Record (captures user/agent traces and environment state), Summary (converts traces into multi-level experiences and generates check functions), and Replay (executes actions with verification). The system records raw interaction traces using tools like Chrome Recorder, summarizes them into low-level precise behavioral operations and high-level generalized summaries, and replays them using stored experiences with safety verification through check functions.

## Key Results
- Multi-level experience design enables balance between execution speed (low-level) and environmental adaptability (high-level)
- Check functions serve as a trusted computing base that verifies execution flow integrity, state preconditions, and safety invariants
- Demonstrated effectiveness on real-world form-filling tasks with significantly improved accuracy and efficiency compared to existing LLM-based agents

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-level experience abstraction allows agents to balance execution speed (low-level) with environmental adaptability (high-level).
- **Mechanism:** The system captures raw traces and summarizes them. Low-level experiences retain concrete UI coordinates and API calls for rapid replay in static environments. High-level experiences abstract these into procedural knowledge requiring a local LLM to interpret the intent against the current UI during replay.
- **Core assumption:** Tasks contain reusable patterns, and the environment changes are detectable to trigger switches between experience levels.
- **Evidence anchors:**
  - [abstract] "...multi-level experience design where lower-level experiences provide precise behavioral operations... while high-level experiences offer more generalized summaries..."
  - [Section 3.2.1] "Low-level experiences offer more precise behavioral descriptions... In contrast, high-level experiences represent more generalized summaries..."
  - [corpus] Related work in "Contextual Experience Replay" supports the value of leveraging past experiences for decision-making.

### Mechanism 2
- **Claim:** Check functions serve as a Trusted Computing Base (TCB) to enforce safety and reliability, preventing "unrecoverable errors" typical of pure LLM agents.
- **Mechanism:** During the Summary phase, constraints (flow integrity, preconditions, data bounds) are codified into executable check functions. During Replay, these functions act as guardrails, verifying state and parameters before actions are executed.
- **Core assumption:** Safety invariants can be explicitly defined or generated from successful traces, and the check function code itself is trusted/auditable.
- **Evidence anchors:**
  - [abstract] "...check functions as a trusted computing base that verifies execution flow integrity, state preconditions, and safety invariants..."
  - [Section 3.2.2] "These check functions delineate the boundaries of the generalization capability... verifying Execution Flow Integrity, State Preconditions..."
  - [corpus] General agent safety literature highlights the need for boundary enforcement.

### Mechanism 3
- **Claim:** Decoupling intelligence (Record/Summary) from execution (Replay) reduces operational costs and latency while preserving privacy.
- **Mechanism:** Heavy LLM inference is front-loaded to the Record/Summary phase (done once). The Replay phase relies on cheaper, local retrieval and execution (or small models), minimizing cloud calls and data exposure.
- **Core assumption:** The "amortized cost" of creating the experience is lower than repeated LLM calls for the same task class.
- **Evidence anchors:**
  - [Section 1] "...decoupling an agent's intelligence from its execution... reduces reliance on LLM computation... improves privacy."
  - [Section 5] "In contrast, AgentRR enables users to simply describe the task... LLM autonomously and quickly fills out the form..."
  - [corpus] "How Memory Management Impacts LLM Agents" empirically supports the efficiency gains of memory/experience use.

## Foundational Learning

- **Concept: State Transition Diagrams**
  - **Why needed here:** AgentRR models all tasks as trajectories through a state space (nodes) connected by actions (edges). Understanding this abstraction is critical to grasp how "Experiences" are essentially sub-graphs or paths through this diagram.
  - **Quick check question:** If an agent clicks a button and a popup appears, is the popup part of the *Action* or the new *State*?

- **Concept: Trusted Computing Base (TCB)**
  - **Why needed here:** The paper explicitly positions "Check Functions" as the TCB. You must understand that the security of the system relies entirely on these functions being correct and immutable, distinct from the probabilistic LLM.
  - **Quick check question:** Why is the LLM excluded from the TCB in this architecture?

- **Concept: Determinism vs. Probabilistic Execution**
  - **Why needed here:** Traditional R&R is deterministic; LLMs are probabilistic. AgentRR blends them. You need to distinguish when the system behaves like a script (Low-level) vs. a reasoning engine (High-level).
  - **Quick check question:** Which experience level would you use for a stable banking API vs. a dynamic e-commerce site?

## Architecture Onboarding

- **Component map:**
  - Record Module (traces + state) -> Summary Module (experiences + checks) -> Experience Store -> Replay Module (task -> match -> execute + verify)

- **Critical path:**
  1. **Ingest:** Raw Trace -> **Summarize** -> Experience + Checks
  2. **Store:** Validated Experience -> Experience Store
  3. **Execute:** User Task -> Query Store -> Select Level -> Run Action -> **Verify Check**

- **Design tradeoffs:**
  - **Specificity vs. Generality:** Low-level is fast but brittle; High-level is robust but slower/costlier
  - **Automation vs. Safety:** Auto-generated Check Functions scale well but may miss edge cases; Manual checks are safe but labor-intensive

- **Failure signatures:**
  - **"Locator Failure":** Low-level replay fails because UI coordinates changed. (System should auto-escalate to High-level)
  - **"Check Violation":** Action blocked because precondition (e.g., "User logged in") was false
  - **"Incomplete Trace":** Record phase missed a dynamic popup, causing Summary to generate invalid flow

- **First 3 experiments:**
  1. **Basic R&R Baseline:** Record a simple form-filling task. Replay it identically. Verify speedup vs. zero-shot LLM
  2. **Generalization Test:** Change the UI layout (move buttons) or input data. Verify if Low-level fails and High-level succeeds
  3. **Check Function Injection:** Manually inject a "forbidden action" into the replay stream. Verify the Check Function catches and blocks it

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can a system ensure the completeness of recorded information to guarantee reliable replay?
- Basis in paper: [explicit] The introduction explicitly asks, "How to ensure the completeness of recorded information so that the experience captures everything needed to reliably replay later?"
- Why unresolved: Dynamic HTML elements and inconsistent UI layouts make it difficult to capture all necessary state deterministically

### Open Question 2
- Question: How can experiences be generalized to new scenarios without compromising safety or reliability?
- Basis in paper: [explicit] The text asks, "How to improve the generalizability of experiences so that they can apply to a broader range of scenarios?"
- Why unresolved: Summary complexity and the trade-off between abstract plans and concrete safety constraints remain a fundamental tension

### Open Question 3
- Question: What defines the boundary of tasks suitable for Record & Replay versus purely model-based approaches?
- Basis in paper: [explicit] The authors ask, "How to delineate the scope of tasks for which R&R is most suitable versus where a more flexible approach is needed?"
- Why unresolved: The current evaluation is limited to repetitive tasks like form-filling, leaving performance on novel, complex tasks uncertain

## Limitations
- Evaluation focuses narrowly on form-filling tasks, leaving uncertainty about performance on more complex, multi-modal agent scenarios
- Generation of reliable check functions remains a manual process in the current implementation, limiting scalability
- Approach may struggle with highly dynamic or context-dependent tasks where environmental state changes in unpredictable ways

## Confidence
- **High confidence:** Core mechanism of decoupling intelligence from execution and multi-level experience abstraction are technically sound
- **Medium confidence:** Claimed efficiency improvements are plausible but lack rigorous comparison to state-of-the-art approaches
- **Low confidence:** Broad claims about generalizability and scalability without sufficient empirical support

## Next Checks
1. **Cross-task generalization:** Test AgentRR on a diverse set of web tasks beyond form-filling (e.g., e-commerce checkout, data analysis workflows) to evaluate the claimed adaptability of the multi-level experience system
2. **Check function robustness:** Systematically attempt to bypass or invalidate check functions through adversarial inputs or edge cases to assess the true strength of the TCB implementation
3. **Cost-benefit analysis:** Conduct a comprehensive study comparing the amortized cost of experience recording and storage against the operational savings in replay scenarios across varying task frequencies and complexity levels