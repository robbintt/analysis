---
ver: rpa2
title: 'StarFlow: Generating Structured Workflow Outputs From Sketch Images'
arxiv_id: '2503.21889'
source_url: https://arxiv.org/abs/2503.21889
tags:
- workflow
- flow
- arxiv
- generation
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: STARFLOW is a framework that converts hand-drawn and computer-generated
  workflow sketches into structured JSON representations using vision-language models.
  It addresses the challenge of translating ambiguous visual diagrams into executable
  workflows by curating a diverse dataset of synthetic, manually annotated, and real-world
  workflow diagrams, then finetuning vision-language models on this data.
---

# StarFlow: Generating Structured Workflow Outputs From Sketch Images

## Quick Facts
- arXiv ID: 2503.21889
- Source URL: https://arxiv.org/abs/2503.21889
- Authors: Patrice Bechard; Chao Wang; Amirhossein Abaskohi; Juan Rodriguez; Christopher Pal; David Vazquez; Spandana Gella; Sai Rajeswar; Perouz Taslakian
- Reference count: 17
- Primary result: Finetuned VLMs achieve 91.9% Flow Similarity vs 43.5% baseline on workflow diagram JSON generation

## Executive Summary
StarFlow addresses the challenge of converting hand-drawn and computer-generated workflow sketches into executable JSON representations. The framework curates a diverse dataset of synthetic, manually annotated, and real-world workflow diagrams, then finetunes vision-language models on this data. Experiments demonstrate that finetuned models significantly outperform both open-weight and proprietary vision-language models on sketch-to-workflow generation, with particular challenges noted for handwritten annotations and out-of-distribution samples.

## Method Summary
StarFlow finetunes vision-language models on a curated dataset of workflow diagrams generated via pattern-based heuristics and rendered in multiple visual styles. The approach freezes the vision encoder during training while updating language model and connector components, enabling efficient domain adaptation. Synthetic workflows follow 18 common automation patterns, annotated using LLMs, and rendered as Graphviz diagrams, UI screenshots, hand-drawn variants, and whiteboard sketches. The model outputs structured JSON containing triggers, components, flow logic, and annotations, evaluated using Flow Similarity metrics based on normalized tree edit distance.

## Key Results
- Finetuned Llama 3.2 11B achieves 91.9% FlowSim vs 43.5% baseline on test set
- End-to-end sketch-to-workflow (0.952 FlowSim) outperforms decomposed pipelines (0.834)
- Synthetic-only training achieves 78.7-86.0 avg FlowSim vs 91.6-91.9 with full diverse training
- Handwritten samples consistently score 5-10% lower than synthetic/digital variants

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Finetuning Enables Precise Component Grounding
- Claim: Finetuning VLMs on workflow-specific data substantially improves structured JSON generation accuracy compared to general-purpose models.
- Mechanism: During finetuning, models learn precise naming conventions for domain-specific actions, triggers, and flow logic components, reducing hallucination of plausible-but-incorrect component names.
- Core assumption: The training data covers the component vocabulary and naming conventions encountered at inference time.
- Evidence anchors:
  - [abstract] "Experiments show that finetuned models significantly outperform both baseline open-weight and proprietary vision-language models...with Flow Similarity scores improving from 43.5% to 91.9% for Llama 3.2 11B"
  - [section 5] "The model predicts the appropriate flow execution logic along with all relevant components. It is also able to properly predict the right tables for the task as it has seen some data from the same domain during the finetuning phase."
  - [corpus] Weak/missing - no direct corpus evidence for this specific workflow domain grounding mechanism.
- Break condition: If target workflows use components, tables, or naming conventions absent from training data, grounding fails and hallucination increases.

### Mechanism 2: Pattern-Based Synthetic Data Generation Captures Real-World Workflow Structure
- Claim: Programmatically generating workflows using common design patterns produces effective training data for VLMs.
- Mechanism: Real workflows follow repeatable patterns (CRUD, Scheduled Loop, Integration). By implementing heuristics that compose triggers, actions, and flow logic according to these templates, synthetic data captures structural regularities without manual annotation at scale.
- Core assumption: The set of implemented patterns sufficiently represents real-world automation workflows.
- Evidence anchors:
  - [section 3.2] "Real world workflows are often built using a distinct set of design patterns. To build our synthetic workflow generation pipeline, we implemented a heuristic that can build workflows using a set of flow logic elements (e.g. IF, ELSE, FOREACH) along with actions and subflows sampled either deterministically or stochastically"
  - [Table 5] Shows 18 pattern types with CRUDLOOP (2,148) and CRUDSINGLE (2,144) as dominant categories.
  - [corpus] Weak/missing - no corpus papers validate pattern-based workflow synthesis specifically.
- Break condition: If real workflows diverge from defined patterns (novel control structures, unconventional logic), the model will misinterpret or incorrectly structure the output.

### Mechanism 3: Multi-Style Visual Training Reduces Domain Shift at Inference
- Claim: Training across diverse diagram rendering styles improves robustness to visual variation.
- Mechanism: By exposing the VLM to multiple visual representations of the same underlying workflow logic, the model learns style-invariant features—recognizing that a hand-sketched arrow and a Graphviz edge both represent control flow.
- Core assumption: The five style categories (SYNTHETIC, MANUAL, DIGITAL, WHITEBOARD, USERINTERFACE) span the visual variation space encountered at deployment.
- Evidence anchors:
  - [section 3.3] Describes five diagram types created from same underlying workflows.
  - [section 4.4.1] Shows performance gap: MANUAL samples drop ~6-10% FlowSim vs. SYNTHETIC/USERINTERFACE for finetuned models, indicating style-dependent difficulty.
  - [corpus] [O3SLM] "LVLMs...struggle to comprehend hand-drawn sketches" - corroborates that sketch interpretation is a known VLM weakness, validating multi-style training rationale.
- Break condition: Out-of-distribution visual styles will cause accuracy degradation.

## Foundational Learning

- **Tree Edit Distance (Zhang-Shasha Algorithm)**
  - Why needed: The FlowSim metric normalizes tree edit distance by tree size to produce a 0-1 similarity score. Understanding this clarifies why structurally similar but component-different workflows score lower.
  - Quick check question: Given two workflow trees, can you identify what operations (insert, delete, relabel) transform one to the other?

- **Vision-Language Model Connector Architecture**
  - Why needed: The paper freezes the vision encoder during finetuning. Understanding what components are trainable clarifies why finetuning is efficient but may not adapt to radically different visual domains.
  - Quick check question: If the vision encoder is frozen, what happens when encountering a diagram style with visual features outside the pre-training distribution?

- **Workflow Semantics (Trigger → Action → Flow Logic)**
  - Why needed: The JSON schema has specific fields (trigger, components, inputs, annotation) that must be populated correctly. Misunderstanding the semantics leads to misinterpreting failure modes.
  - Quick check question: What is the semantic difference between a subflow and a workflow, and how does the model's prediction change for each?

## Architecture Onboarding

- **Component map:**
  Pattern Templates (18 types) -> Synthetic Workflow Generator (Algorithm 1) -> Natural Language Annotator (Llama 3.1 70B) -> Diagram Renderers (5 styles) -> VLM Finetuning Pipeline (LM + connectors, frozen vision encoder) -> Evaluation Suite (FlowSim, TreeBLEU, Trigger Match, Component Match)

- **Critical path:**
  1. Define/extend pattern templates for target domain (currently 18 patterns - see Table 5)
  2. Generate synthetic JSON workflows with LLM annotations
  3. Create multi-style diagram variants for each workflow
  4. Finetune VLM with early stopping on validation loss (learning rate: 2×10⁻⁵, warmup: 30 steps)
  5. Evaluate on held-out test split with stratified sampling across diagram styles

- **Design tradeoffs:**
  - **Synthetic scale vs. real-world diversity**: SYNTHETIC provides 12K+ samples but may miss edge cases present in 3K MANUAL samples. Section 4.5 shows models trained on SYNTHETIC-only achieve 78.7-86.0 avg FlowSim vs. 91.6-91.9 with full diverse training.
  - **End-to-end vs. task decomposition**: Table 4 shows end-to-end sketch→workflow (0.952 FlowSim) outperforms decomposed sketch→summary→outline→workflow (0.834), suggesting error compounding in pipelines outweighs modularity benefits.
  - **Model size vs. compute**: 3B models achieve 0.941 FlowSim after finetuning vs. 0.957 for 7B models - diminishing returns above ~7B parameters for this task.

- **Failure signatures:**
  - **Handwritten text recognition errors**: MANUAL/WHITEBOARD samples consistently score 5-10% lower than DIGITAL/SYNTHETIC
  - **Component name hallucination**: Non-finetuned models predict generic names (`create_user`) instead of domain-specific names (`create_a_user`)
  - **Control flow misinterpretation**: Confusing DOUNTIL loops with IF conditions, missing ELSE branches
  - **Trigger/component confusion**: Treating TIMER or look_up_records as triggers instead of components
  - **Orientation bias**: Landscape diagrams score slightly lower than portrait across models

- **First 3 experiments:**
  1. **Baseline reproduction**: Run inference with base Llama 3.2 11B and finetuned checkpoint on test split. Verify FlowSim improvement from ~43.5% → ~91.9%. This validates the training pipeline.
  2. **Ablation on training data composition**: Train separate models on (a) SYNTHETIC-only, (b) SYNTHETIC + MANUAL, (c) full dataset. Evaluate on each diagram type separately to quantify contribution of visual diversity. Expect MANUAL/WHITEBOARD test performance to improve only when those styles appear in training.
  3. **OOD stress test**: Collect 20 real workflow diagrams from platforms not in training data (e.g., Zapier, n8n). Compare finetuned vs. base vs. proprietary models. Expect all models to score <50%, but finetuned should still outperform base. Identify specific failure patterns for dataset expansion.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating execution-based evaluation metrics better predict the functional correctness of generated workflows than structural similarity scores?
- Basis in paper: [explicit] The authors state that current metrics "do not capture whether the generated workflow would execute correctly" and suggest future work should "incorporate execution-based or behavior-level evaluation."
- Why unresolved: The current study relies on syntactic metrics like FlowSim and TreeBLEU, which measure structural alignment but fail to verify if the logic actually executes without errors.
- What evidence would resolve it: A benchmark integrating a sandboxed execution environment to measure runtime success rates of the generated JSON workflows against the current structural scores.

### Open Question 2
- Question: Does integrating retrieval-augmented generation (RAG) or function calling significantly reduce hallucination rates for specific component definitions?
- Basis in paper: [explicit] The authors hypothesize that "augmenting models with external information via retrieval-augmented generation or function calling might help better ground the models" to mitigate naming convention errors.
- Why unresolved: Finetuned models currently rely on memorized domain knowledge, leading to zero scores when they hallucinate incorrect component definition names.
- What evidence would resolve it: A comparative analysis measuring hallucination frequency between purely finetuned models and models augmented with access to live API schemas or component registries.

### Open Question 3
- Question: What specific data augmentation or visual pre-training strategies are required to close the performance gap on noisy, handwritten diagrams?
- Basis in paper: [explicit] The paper identifies "particular challenges noted for handwritten annotations" and states the approach "remains sensitive to noisy inputs such as messy handwriting," necessitating improved robustness.
- Why unresolved: There is a distinct performance drop on MANUAL and WHITEBOARD samples compared to SYNTHETIC ones (e.g., ~39% vs ~45% for base Llama), indicating current visual parsing is brittle.
- What evidence would resolve it: Ablation studies showing performance improvements on the MANUAL test split after training on datasets specifically augmented with noise injection, simulated ink bleeds, or varied writing styles.

## Limitations

- **Synthetic pattern coverage**: The approach depends on 18 predefined workflow patterns, with OOD performance dropping to ~50% when encountering novel control structures
- **Handwritten annotation brittleness**: MANUAL and WHITEBOARD samples score 5-10% lower than synthetic/digital variants due to recognition errors
- **Frozen vision encoder constraint**: The architecture only finetunes language and connector components, potentially limiting adaptation to radically different visual styles

## Confidence

- **High Confidence**: Finetuning VLMs on domain-specific synthetic data improves workflow JSON generation accuracy compared to base models (supported by 30-50% absolute FlowSim improvements across multiple model sizes)
- **Medium Confidence**: The five-diagram-style training strategy meaningfully improves robustness to visual variation (supported by style-stratified evaluation showing 5-10% performance gaps between training-matched and mismatched styles)
- **Low Confidence**: The synthetic pattern generation approach captures sufficient real-world workflow diversity (limited by absence of corpus validation and the sharp OOD performance drop observed)

## Next Checks

1. **Cross-platform Generalization Test**: Collect 50 real workflow diagrams from Zapier, n8n, and Power Automate platforms not represented in training data. Evaluate finetuned vs. base vs. proprietary models on structural correctness and component naming accuracy. Expect finetuned models to outperform base but fall short of in-distribution performance.

2. **Pattern Coverage Analysis**: Identify workflows from target deployment domains that use control structures outside the 18 synthetic patterns (e.g., parallel branches, complex nested loops). Generate corresponding sketches and evaluate model performance to quantify pattern coverage gaps.

3. **Visual Style Transfer Robustness**: Systematically degrade test images (blur, noise, compression, perspective distortion) and measure performance degradation curves. Compare degradation rates between finetuned and base models to quantify robustness gains from multi-style training.