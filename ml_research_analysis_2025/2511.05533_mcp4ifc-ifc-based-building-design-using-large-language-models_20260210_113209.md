---
ver: rpa2
title: 'MCP4IFC: IFC-Based Building Design Using Large Language Models'
arxiv_id: '2511.05533'
source_url: https://arxiv.org/abs/2511.05533
tags:
- tools
- code
- framework
- building
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MCP4IFC, an open-source framework enabling
  large language models (LLMs) to directly manipulate Industry Foundation Classes
  (IFC) building models through standardized tool calls. The system combines predefined
  BIM tools with dynamic code generation powered by in-context learning and retrieval-augmented
  generation (RAG), allowing LLMs to handle tasks beyond fixed tool sets.
---

# MCP4IFC: IFC-Based Building Design Using Large Language Models

## Quick Facts
- arXiv ID: 2511.05533
- Source URL: https://arxiv.org/abs/2511.05533
- Reference count: 21
- Primary result: LLM framework achieving 83.1% accuracy on IFC scene querying tasks

## Executive Summary
MCP4IFC introduces an open-source framework that enables large language models to directly manipulate Industry Foundation Classes (IFC) building models through standardized tool calls. The system combines predefined BIM tools with dynamic code generation powered by in-context learning and retrieval-augmented generation (RAG), allowing LLMs to handle tasks beyond fixed tool sets. Experiments demonstrate that GPT-5 mini achieves competitive accuracy on IFC querying tasks, while Claude Sonnet 4.5 successfully performs complex modeling operations such as generating walls, doors, and roofs from natural language prompts.

## Method Summary
The MCP4IFC framework operates through a Model Context Protocol (MCP) server that exposes standardized tools for LLM interaction with IFC building models. The system integrates predefined BIM tools with dynamic code generation capabilities, enabling LLMs to both use existing tools and generate custom code when needed. The framework employs RAG-enhanced code generation to retrieve relevant examples and improve code quality for complex modeling tasks. Visual reasoning experiments utilize multimodal models to analyze spatial relationships within building models, though performance remains moderate on these tasks.

## Key Results
- GPT-5 mini achieves 83.1% accuracy on IFC scene querying tasks
- Claude Sonnet 4.5 successfully generates walls, doors, and roofs from natural language prompts
- LLMs demonstrate ability to understand, query, and modify IFC data through tool-driven workflows without manual GUI interaction

## Why This Works (Mechanism)
The framework's effectiveness stems from its dual approach combining predefined BIM tools with dynamic code generation. By providing LLMs with standardized tool interfaces for common BIM operations while also enabling custom code generation through RAG-enhanced mechanisms, the system bridges the gap between natural language understanding and precise BIM manipulation. The in-context learning capabilities allow models to adapt to new modeling scenarios by leveraging retrieved examples from relevant BIM contexts.

## Foundational Learning
- **Industry Foundation Classes (IFC)**: Open standard for BIM data exchange; needed for interoperability across different BIM software platforms; quick check: verify IFC schema compliance in test models
- **Model Context Protocol (MCP)**: Standardized interface for tool integration; needed to provide consistent LLM access to BIM tools; quick check: test tool call format adherence
- **Retrieval-Augmented Generation (RAG)**: Technique for enhancing LLM outputs with external knowledge; needed to improve code generation quality for complex modeling tasks; quick check: measure retrieval relevance scores
- **In-Context Learning**: LLM capability to learn from examples within prompts; needed to adapt to new modeling scenarios without fine-tuning; quick check: test performance across varying context lengths
- **Multimodal Visual Reasoning**: Ability to analyze spatial relationships from visual inputs; needed for understanding building geometry and spatial arrangements; quick check: evaluate accuracy on spatial comprehension tasks

## Architecture Onboarding

**Component Map**: LLM Client -> MCP Server -> BIM Tools + Code Generator -> IFC Model Database

**Critical Path**: Natural Language Input -> Tool Selection/Code Generation -> BIM Operation Execution -> IFC Model Update -> Response Generation

**Design Tradeoffs**: The framework prioritizes flexibility through dynamic code generation at the cost of potential reliability issues, while maintaining standardized tool interfaces for predictable operations.

**Failure Signatures**: Tool call failures indicate schema mismatches or missing tool implementations; code generation failures suggest insufficient context or retrieval quality issues; visual reasoning errors point to limitations in multimodal model capabilities.

**First Experiments**:
1. Test basic tool calls for simple IFC queries to verify MCP server connectivity
2. Execute predefined BIM operations (create wall, add door) to validate tool functionality
3. Attempt complex modeling tasks requiring dynamic code generation to assess RAG effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- Multimodal models show only moderate performance on spatial reasoning tasks
- Framework evaluation focuses primarily on GPT-5 mini with limited cross-model comparison
- Dependency on specific tool implementations may limit generalizability to other BIM data formats

## Confidence

**High Confidence**: MCP4IFC successfully enables LLM interaction with IFC models through standardized tool calls; achieves competitive 83.1% accuracy on IFC scene querying; Claude Sonnet 4.5 performs basic modeling operations from natural language prompts.

**Medium Confidence**: Combination of predefined tools and dynamic code generation improves task coverage; RAG-enhanced code generation contributes to handling complex modeling tasks; visual reasoning enables basic spatial understanding.

**Low Confidence**: Framework effectiveness across diverse building design scenarios; long-term reliability of LLM-generated IFC modifications; performance with larger, more complex building models.

## Next Checks
1. Conduct extensive cross-model validation using multiple LLM architectures (GPT, Claude, open-source models) across varying parameter sizes to assess framework robustness
2. Test the framework on complex, real-world building projects with diverse architectural styles and structural requirements to evaluate practical applicability
3. Perform longitudinal studies measuring accuracy and consistency of LLM-generated IFC modifications over extended design iterations and team collaborations