---
ver: rpa2
title: '"Yeah Right!" -- Do LLMs Exhibit Multimodal Feature Transfer?'
arxiv_id: '2501.04138'
source_url: https://arxiv.org/abs/2501.04138
tags:
- basic
- zero-shot
- few-shot
- gpt-4o
- gpt-4-turbo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models exhibit multimodal
  feature transfer when detecting covert deceptive communication, such as sarcasm,
  irony, and condescension. The authors compare speech+text models (GPT-4o) against
  text-only models (GPT-4-Turbo) and conversation-tuned models (Llama-2-70B-conversational)
  against standard text models (Llama-2-70B-chat).
---

# "Yeah Right!" -- Do LLMs Exhibit Multimodal Feature Transfer?

## Quick Facts
- arXiv ID: 2501.04138
- Source URL: https://arxiv.org/abs/2501.04138
- Reference count: 8
- This paper investigates whether large language models exhibit multimodal feature transfer when detecting covert deceptive communication, such as sarcasm, irony, and condescension.

## Executive Summary
This paper investigates whether large language models (LLMs) exhibit multimodal feature transfer when detecting covert deceptive communication like sarcasm, irony, and condescension. The authors compare speech+text models (GPT-4o) against text-only models (GPT-4-Turbo) and conversation-tuned models (Llama-2-70B-conversational) against standard text models (Llama-2-70B-chat). Using four prompt types across four datasets, they test each model's ability to detect deceptive communication. Results show that with basic prompting, speech+text models outperform text-only models by 2.2% accuracy, 1.9% precision, and 3.3% recall. Similarly, conversation-tuned models achieve 12.7% better accuracy, 11.8% better precision but lower recall compared to standard models.

## Method Summary
The study evaluates four datasets from Big-Bench (TalkDown, Irony Identification, Snarks, and a combined "Deceptions" dataset) using binary classification for covert deceptive communication. Four models are compared: GPT-4o (speech+text), GPT-4-Turbo (text-only), Llama-2-70B-conversational (conversation-tuned), and Llama-2-70B-chat (standard). Four prompt types are tested: basic, speech-features, conversational-features, and chain-of-thought, each with zero-shot and few-shot variants. Temperature is set to 0, and accuracy, precision, recall, and F1-score are computed for all experimental conditions.

## Key Results
- With basic prompting, speech+text models (GPT-4o) outperform text-only models (GPT-4-Turbo) by 2.2% accuracy, 1.9% precision, and 3.3% recall.
- Conversation-tuned models (Llama-2-70B-conversational) achieve 12.7% better accuracy and 11.8% better precision than standard models, but with lower recall.
- Chain-of-thought prompting reverses the multimodal advantage, with text-only models outperforming speech+text models in CoT settings.

## Why This Works (Mechanism)

### Mechanism 1
Models trained on audio-text pairs may acquire implicit representations of prosody and tone that remain accessible during text-only inference. During pre-training, the model learns to correlate textual markers with audio features. When presented with text-only inputs, the model leverages these correlated text features as proxies for the missing audio signal, improving detection of connotative intent. The architecture allows features learned in the speech modality to bind to text token representations rather than remaining isolated in a modality-specific encoder.

### Mechanism 2
Fine-tuning on human-to-human conversation data improves precision in detecting covert deception by shifting the model's prior probability distribution toward pragmatic, interactive language patterns. Conversation-heavy datasets contain higher densities of sarcasm, irony, and condescension than standard web scrapes. Fine-tuning adjusts the model's weights to be more sensitive to the subtle textual patterns typical of dialogue, making it more conservativeâ€”missing true positives to avoid false positives.

### Mechanism 3
Explicitly prompting a model to "imagine" or "think about" speech features degrades performance compared to basic prompting. Explicit instructions may force the model to generate hallucinated prosodic descriptions which then confound the classification task. The paper suggests the transfer is implicit; making it explicit disrupts the direct mapping from text to the learned latent feature.

## Foundational Learning

- **Concept: Covert vs. Overt Deception**
  - **Why needed here:** The paper specifically excludes lies (overt) and focuses on sarcasm/irony (covert), where text denotation contradicts connotation. Without this distinction, the performance metrics (detecting "intent" vs. detecting "facts") are uninterpretable.
  - **Quick check question:** If a user says "Great job" after a failure, is this overt or covert deception, and which signal (text vs. intent) must the model prioritize?

- **Concept: Multimodal Feature Transfer (Cross-Modal)**
  - **Why needed here:** The central hypothesis is that skills learned in Modality A (Speech) transfer to Modality B (Text). Understanding this requires distinguishing between a model merely accepting multiple inputs vs. a model using modality A to alter its weights for processing modality B.
  - **Quick check question:** Does a multimodal model process text differently than a text-only model if the text input is identical? (This paper suggests Yes).

- **Concept: The Precision/Recall Trade-off in Safety/Deception**
  - **Why needed here:** The results show conversation-tuned models gain precision but lose recall. In a production system, this determines if you flag too much (annoying user) or miss too much (allowing toxic content).
  - **Quick check question:** In a content moderation system, would you prefer the model that catches 95% of sarcasm but flags 50% of normal text as deceptive, or one that catches 60% but almost never flags normal text?

## Architecture Onboarding

- **Component map:**
  - Input: Text-only string (e.g., "Yeah, right")
  - Encoder (Experimental): Path A: Text-Image model (GPT-4-Turbo); Path B: Text-Image-Audio model (GPT-4o); Path C: Text model + Conversation Fine-tuning (Llama-2-Conv)
  - Prompt Interface: Basic vs. Feature-Emphasis vs. Chain-of-Thought
  - Classifier Head: Binary (Deceptive/Not) or Multi-class (Sarcasm/Irony/Condescension)

- **Critical path:**
  1. Select a model with audio-native training (not just an adapter) if relying on implicit transfer.
  2. Use **Basic Prompts**. Do not use CoT or "imagine the voice" prompts for this specific task.
  3. If using conversation-tuned models, calibrate thresholds to handle the drop in recall.

- **Design tradeoffs:**
  - Speech-Models (GPT-4o): Better raw detection (Recall) out-of-the-box. Fragile to CoT prompting.
  - Conversation-Models (Llama-Conv): High confidence/precision (fewer false alarms), but misses many instances. Stable with CoT.
  - Prompting: Basic prompting is the "local maximum" for multimodal transfer; CoT disrupts the transfer but aids logical reasoning tasks.

- **Failure signatures:**
  - The "Literalist" Failure: Model interprets "Yeah, right" as affirmative agreement (High precision/Low recall on deception).
  - The "Over-Thinker" Failure: Model generates a long CoT explanation about prosody and concludes incorrectly (CoT degradation observed in Table 1).
  - The "Hallucinated Tone" Failure: Model invents a sarcastic tone for a neutral statement because the prompt asked it to consider speech features (Table 2 drop).

- **First 3 experiments:**
  1. **Baseline Validation:** Run GPT-4o vs. GPT-4-Turbo on the Snark dataset using *only* basic prompting (Zero-shot). Verify the 2-3% delta exists in your environment.
  2. **CoT Ablation:** Apply Zero-Shot CoT to both models. Confirm that the Speech-model advantage collapses or flips (as per Table 1).
  3. **Conversation Tuning Test:** Compare a base Llama model against a conversation-tuned variant on the "Condescension" dataset. Measure the specific drop in Recall to see if the precision gain is worth the missed detections for your use case.

## Open Questions the Paper Calls Out

### Open Question 1
Can trained approaches explicitly leverage multimodal features to improve accuracy on unimodal tasks? The paper's conclusion states, "Future work will explore trained approaches that explicitly take advantage of multimodal features to improve accuracy on unimodal tasks." This study focused on evaluating existing models via prompting strategies rather than developing new training methodologies.

### Open Question 2
Why does Chain-of-Thought (CoT) prompting negate the performance advantage of speech+text models over text-only models? Table 1 shows that while GPT-4o leads with basic prompting, GPT-4-Turbo (text-only) becomes superior when CoT is applied. The paper reports this reversal but does not explain why explicit reasoning steps would disrupt or fail to utilize the latent multimodal features.

### Open Question 3
Do performance gains in closed-source models stem specifically from multimodal features or from confounding architectural differences? Section 6 notes that because models like GPT-4o are closed-source, it is "difficult to know if the performance differences... stem from the addition of a modality, or from a difference in how the model was trained." Without access to training data or model weights, it is impossible to isolate the "speech modality" variable from other optimizations.

## Limitations

- The central claim about multimodal feature transfer relies on indirect interpretation of performance differences between model families rather than explicit validation of the mechanism.
- Results are based on a limited set of Big-Bench datasets and may not generalize to other domains of covert deceptive communication.
- The conversational fine-tuning approach lacks transparency about the specific datasets and objectives used, making it difficult to determine the source of performance improvements.

## Confidence

- **High confidence**: The empirical observation that different model architectures yield different performance on covert deception detection tasks. The quantitative results (accuracy, precision, recall differences) are clearly reported and reproducible.
- **Medium confidence**: The interpretation that these differences reflect multimodal feature transfer rather than other architectural or data-related factors. The conclusion follows logically from the data but isn't directly proven.
- **Medium confidence**: The claim that conversation-tuning improves precision for detecting covert deception, given the lack of transparency about the fine-tuning process and datasets used.

## Next Checks

1. **Architectural ablation study**: Test whether the performance gap between GPT-4o and GPT-4-Turbo persists when both models are given identical text inputs, controlling for any architectural differences beyond modality handling.

2. **Cross-dataset generalization**: Evaluate the same model pairs on additional datasets of covert deceptive communication (e.g., Twitter sarcasm datasets, Reddit irony detection) to determine if the observed advantages transfer across domains or are dataset-specific.

3. **Prompt sensitivity analysis**: Systematically vary the basic prompt structure (word order, phrasing, instruction detail) to determine whether the multimodal advantage is robust to prompt engineering or specific to the exact formulation used in the paper.