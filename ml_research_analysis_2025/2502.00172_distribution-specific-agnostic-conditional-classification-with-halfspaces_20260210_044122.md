---
ver: rpa2
title: Distribution-Specific Agnostic Conditional Classification With Halfspaces
arxiv_id: '2502.00172'
source_url: https://arxiv.org/abs/2502.00172
tags:
- classi
- cation
- algorithm
- errd
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies selective/conditional classification in the
  agnostic setting, where the goal is to model feature-category relationships only
  on a subset of data defined by a selector function. The authors focus on selectors
  defined by homogeneous halfspaces under Gaussian feature distributions.
---

# Distribution-Specific Agnostic Conditional Classification With Halfspaces

## Quick Facts
- arXiv ID: 2502.00172
- Source URL: https://arxiv.org/abs/2502.00172
- Authors: Jizhou Huang; Brendan Juba
- Reference count: 40
- One-line primary result: First polynomial-time algorithm achieving O(√opt) approximation for conditional classification with homogeneous halfspace selectors under Gaussian distributions.

## Executive Summary
This paper studies selective/conditional classification in the agnostic setting, where the goal is to model feature-category relationships only on a subset of data defined by a selector function. The authors focus on selectors defined by homogeneous halfspaces under Gaussian feature distributions. They develop a polynomial-time algorithm that achieves O(√opt) approximation for conditional classification with homogeneous halfspace selectors, where opt is the optimal conditional classification error. The paper also establishes a hardness result showing that approximating conditional classification loss within small additive error is computationally hard under cryptographic assumptions, even under Gaussian distributions.

## Method Summary
The algorithm reduces the conditional classification problem to a "one-sided" classification problem using a label transformation technique. For each classifier in the class C, it maps labels via y → 1{c(x) ≠ y} to form new distributions D^(c), then applies projected stochastic gradient descent (SGD) with a convex surrogate loss (ReLU) on the unit sphere. The method works by exploiting the geometric properties of Gaussian distributions and homogeneous halfspaces, where the probability of being in any halfspace is exactly 1/2. For infinite classifier classes, the approach uses a list learning subroutine that enumerates sparse linear classifier candidates and solves linear programs to find consistent weight vectors.

## Key Results
- Polynomial-time algorithm achieving O(√opt) approximation for conditional classification with homogeneous halfspace selectors
- First provable approximation guarantees for this problem setting
- Hardness result showing conditional classification is at least as hard as agnostic classification under cryptographic assumptions
- Extension to sparse linear classifiers via list learning approach

## Why This Works (Mechanism)

### Mechanism 1: One-Sided Classification Reduction via Label Mapping
- Claim: Conditional classification can be reduced to a "one-sided" classification problem through a label transformation.
- Mechanism: For each classifier c ∈ C, the algorithm maps labels y → 1{c(x) ≠ y} to form distribution D^(c). This transforms the conditional loss Pr{c(x) ≠ y | x ∈ h(w)} into minimizing Pr{c(x) ≠ y ∩ x ∈ h(w)}. For homogeneous halfspaces under standard normal marginals, Pr{x ∈ h(w)} = 1/2, making these objectives equivalent.
- Core assumption: Standard normal x-marginal distribution; selectors are homogeneous halfspaces (passing through origin).
- Evidence anchors: [abstract] "The algorithm works by reducing the conditional classification problem to a 'one-sided' classification problem using a mapping technique"; [Section 3.1] "the mapping step (line 7) for each c ∈ C essentially just creates another adversarial distribution D^(c)".

### Mechanism 2: Projected SGD Convergence via Gradient-Optimal Direction Alignment
- Claim: Projected SGD on the ReLU surrogate loss finds halfspaces where the projected gradient norm is small, which corresponds to approximate optimality.
- Mechanism: The surrogate loss L_D(w) = E[y·max(0, ⟨x, w⟩)] has gradient structure g_w(x, y) = y·x^w⊥·1{x ∈ h(w)}. Proposition 3.2 establishes that when h(w) is sub-optimal, the negative gradient has positive projection onto the direction of the optimal halfspace v. This ensures gradient descent "points toward" v. The projection step keeps w normalized, bounding the loss.
- Core assumption: The optimal halfspace v satisfies θ(v, w^(0)) ∈ [0, π/2); ε sufficiently small (≤ 1/e); gradient is "almost Lipschitz" due to anti-concentration of Gaussian measure.
- Evidence anchors: [Section 3.2] "the projected negative gradient E[-g_w] of the surrogate loss must have non-negligible projection on the normal vector of the optimal halfspace"; [Lemma B.1] Proves relative smoothness of ∇_w L_D(w) under Gaussian marginals.

### Mechanism 3: List Learning Extension for Infinite Classifier Classes
- Claim: Sparse linear classifiers with s = O(1) nonzero coefficients can be list-learned, enabling extension from finite C to infinite classes.
- Mechanism: Algorithm 4 enumerates all s-tuples of feature indices and samples, solving linear programs to find weight vectors consistent with inlier labels. The returned list of size O((md)^s) contains at least one classifier with error ≤ 2ε on the target distribution.
- Core assumption: The true classifier is s-sparse; at least α fraction of samples are inliers labeled correctly; VC-dimension of s-sparse linear classifiers is s log d.
- Evidence anchors: [Theorem A.1] "Algorithm 4 solves robust list-learning of linear classifiers with s = O(1) nonzero coefficients"; [Section 3.3] "we can generalize our approach to work with infinite classes of classifiers whenever they are list-learnable".

## Foundational Learning

- **Concept: Agnostic vs. Realizable Learning**
  - Why needed here: The paper explicitly targets the agnostic setting where no perfect classifier-selector pair exists; optimal error "opt" may be nonzero. Prior work largely addressed realizable settings.
  - Quick check question: Can you explain why an O(√opt) approximation is meaningful when opt > 0 but small?

- **Concept: Halfspaces and Homogeneous Halfspaces**
  - Why needed here: Selectors are defined as homogeneous halfspaces h(w) = {x | ⟨x, w⟩ ≥ 0}. The restriction to homogeneous (no bias term) is critical for Pr{x ∈ h(w)} = 1/2 under symmetric marginals.
  - Quick check question: Why does the homogeneous assumption simplify the conditional classification objective?

- **Concept: Projected Gradient Descent and Stationary Points**
  - Why needed here: The algorithm seeks points where ∥E[g_w]∥_2 is small (approximate stationary points), not global minima of L_D. Convergence relies on gradient-norm decrease.
  - Quick check question: How does the projection step in Algorithm 2 (normalization) affect the loss landscape?

## Architecture Onboarding

- **Component map:** CCFC (Algorithm 1) -> PSGD (Algorithm 2) -> SPARSE_LIST (Algorithm 4) -> CCSLC (Algorithm 3)
- **Critical path:**
  1. Generate classifier list via SPARSE_LIST (if C is infinite).
  2. For each c, transform labels: y ← 1{c(x) ≠ y}.
  3. Run PSGD from w^(0) and -w^(0); collect T candidates.
  4. Select w with minimum empirical conditional error on held-out set.
  5. Return best (c, w) pair.

- **Design tradeoffs:**
  - Sample complexity: Õ(d/ε^6) is polynomial but high-degree; limits scalability for very small ε.
  - Approximation factor: Õ(√ε) is sub-optimal compared to constant-factor results for standard agnostic classification.
  - Selector restriction: Homogeneous halfspaces limit subset selection to half of the data; cannot select arbitrary minoritized subsets.

- **Failure signatures:**
  - θ(v, w^(0)) outside [0, π/2) causes gradient updates to move away from optimum.
  - Non-Gaussian marginals break the Pr{h(w)} = 1/2 property and anti-concentration arguments.
  - Inhomogeneous halfspaces (nonzero thresholds) invalidate the one-sided reduction equivalence.

- **First 3 experiments:**
  1. **Sanity check on synthetic Gaussian data:** Generate data from known halfspace selector v and sparse classifier c; verify PSGD recovers v up to Õ(√ε) error where ε is injected noise rate.
  2. **Ablation on initialization angle:** Test w^(0) at various angles θ(v, w^(0)); confirm convergence only when θ ∈ [0, π/2).
  3. **Distribution shift stress test:** Evaluate on log-concave or heavy-tailed marginals to assess brittleness of Gaussian assumption.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the algorithmic results for conditional classification be extended to general (non-homogeneous) halfspaces?
- Basis in paper: [explicit] Section 5 states that "moving from homogeneous halfspaces to general halfspaces would constitute a significant advance," noting that the current homogeneous restriction limits the ability to select minority subsets.
- Why unresolved: Homogeneous halfspaces are constrained to selecting exactly half of the probability mass, whereas general halfspaces allow for more flexible selection rules which are currently computationally hard to learn.
- What evidence would resolve it: A polynomial-time algorithm with provable approximation guarantees for selectors defined by general halfspaces, even with a weaker error bound than $\tilde{O}(\sqrt{\mathrm{opt}})$.

### Open Question 2
- Question: Can the positive results be generalized to other distribution classes, specifically log-concave distributions?
- Basis in paper: [explicit] Section 5 highlights that real-world data rarely has standard normal marginals and suggests "it's worth trying to extend our result to more general classes of distributions, such as log-concave distributions."
- Why unresolved: The current convergence analysis relies heavily on the specific geometric and anti-concentration properties of Gaussian distributions.
- What evidence would resolve it: A modified projected SGD analysis or new algorithm that provably minimizes the conditional classification error under log-concave marginal distributions.

### Open Question 3
- Question: Can the $\tilde{O}(\sqrt{\mathrm{opt}})$ approximation factor be improved for homogeneous halfspaces?
- Basis in paper: [explicit] Section 5 identifies the current error guarantee as a limitation, stating "the error guarantee $O(\sqrt{\epsilon})$ appears sub-optimal."
- Why unresolved: There is a gap between the upper bound provided by the algorithm and the known lower bounds for this specific setting.
- What evidence would resolve it: An algorithm achieving a constant factor approximation or a lower bound proving that $O(\sqrt{\mathrm{opt}})$ is computationally necessary.

## Limitations

- **Gaussian Assumption Fragility**: The entire algorithmic approach critically depends on standard normal marginals. While the paper proves hardness even under Gaussian distributions, the tractability gap may not extend to other natural distributions.
- **Initial Vector Dependency**: The algorithm requires the initial vector w^(0) to satisfy θ(v, w^(0)) ∈ [0, π/2) for correctness. No practical initialization strategy is provided, creating a circular dependency where the optimal halfspace v must be approximately known.
- **List Size Explosion**: The SPARSE_LIST approach generates O((md)^s) candidates for s-sparse classifiers. This becomes computationally prohibitive even for modest sparsity s ≥ 3, limiting practical applicability.

## Confidence

- **High**: The polynomial-time O(√opt) approximation result for homogeneous halfspace selectors under Gaussian marginals. The reduction to one-sided classification and projected SGD analysis appear sound.
- **Medium**: The hardness result showing conditional classification is at least as hard as agnostic classification. The reduction from cLWE is technically involved and depends on cryptographic assumptions.
- **Low**: Practical implementation details, particularly initialization strategy and handling of the θ(v, w^(0)) constraint. The paper provides theoretical guarantees but limited guidance for real-world deployment.

## Next Checks

1. **Initialization Sensitivity Test**: Systematically vary the initialization angle θ(v, w^(0)) from 0 to π/2 radians and measure algorithm performance. This would quantify how sensitive the approach is to the initialization constraint.
2. **Distribution Robustness Evaluation**: Replace Gaussian marginals with log-concave or heavy-tailed distributions while keeping the homogeneous halfspace selector structure. Measure whether the O(√opt) approximation guarantee breaks down.
3. **Scalability Benchmark**: Implement the SPARSE_LIST approach for increasing sparsity levels (s = 1, 2, 3, 4) on synthetic data with d = 50, 100, 200 features. Track runtime and memory usage to identify practical limits.