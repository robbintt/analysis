---
ver: rpa2
title: 'MKG-Rank: Enhancing Large Language Models with Knowledge Graph for Multilingual
  Medical Question Answering'
arxiv_id: '2503.16131'
source_url: https://arxiv.org/abs/2503.16131
tags:
- medical
- knowledge
- nerve
- palsy
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MKG-Rank is a knowledge graph-enhanced framework that enables English-centric
  LLMs to perform multilingual medical question answering. It uses word-level translation
  to integrate external medical knowledge graphs into LLM reasoning at low cost, mitigating
  cross-lingual semantic distortion.
---

# MKG-Rank: Enhancing Large Language Models with Knowledge Graph for Multilingual Medical Question Answering

## Quick Facts
- arXiv ID: 2503.16131
- Source URL: https://arxiv.org/abs/2503.16131
- Reference count: 40
- Primary result: MKG-Rank improves multilingual medical QA accuracy by up to 35.03% using word-level translation and knowledge graph integration

## Executive Summary
MKG-Rank is a framework that enables English-centric LLMs to perform multilingual medical question answering by integrating external medical knowledge graphs through word-level translation. The system extracts medical entities from non-English questions, translates them to English, retrieves relevant UMLS triplets, and converts them to natural language before reasoning. Extensive evaluations show consistent improvements across Chinese, Japanese, Korean, and Swahili medical QA benchmarks, achieving up to 35.03% accuracy gains while maintaining sub-millisecond retrieval times through caching.

## Method Summary
The framework operates through a pipeline that first extracts medical entities from questions using an LLM, then translates only these entities to English rather than full sentences. It queries the Unified Medical Language System (UMLS) for relevant triplets, ranks them using a two-stage process (UMLS-BERT similarity followed by MedCPT cross-encoder reranking), and converts the structured triplets to natural language declarative statements. When retrieval yields low-quality matches, the system falls back to BM25-based self-knowledge mining. The framework employs caching to reduce retrieval time from 14 seconds to 0.0009 seconds per query.

## Key Results
- Achieves up to 35.03% improvement in accuracy over zero-shot LLMs on multilingual medical QA
- Maintains average retrieval time of 0.0009 seconds through caching optimization
- Consistently outperforms baseline models across Chinese, Japanese, Korean, and Swahili medical QA benchmarks
- Ablation studies show declarative conversion improves performance on most models except LLaMA-3.1 70B

## Why This Works (Mechanism)

### Mechanism 1
Word-level entity translation preserves medical semantics better than sentence-level translation when bridging non-English queries to English-centric knowledge graphs. The framework extracts up to 3 medical entities from the question and 1 from each option, translates only these terms to English, then queries UMLS. This avoids the semantic distortion that occurs when entire medical questions are machine-translated.

### Mechanism 2
Two-stage multi-angle ranking (semantic similarity + cross-encoder reranking) filters noisy multilingual KG triplets to relevant medical knowledge. First, UMLS-BERT embeddings compute similarity between question and triplets. Second, MedCPT Cross Encoder further filters based on question-option context. This handles the fact that UMLS returns multilingual triplets with varying relevance.

### Mechanism 3
Declarative conversion transforms structured triplets into natural language, reducing multilingual noise interference and aligning with LLM's preferred input format. Raw triplets (multilingual, potentially redundant) are converted to English declarative statements by the LLM itself, with compression to remove low-information content.

## Foundational Learning

- **Concept: Unified Medical Language System (UMLS)**
  - Why needed here: Source of English-centric medical knowledge graphs; understanding triplet structure (entity-relation-entity) is required to implement retrieval and declarative conversion
  - Quick check question: Can you explain what a UMLS triplet looks like and why the same medical concept might appear in multiple languages within results?

- **Concept: Retrieval-Augmented Generation (RAG) with Knowledge Graphs**
  - Why needed here: MKG-Rank is a graph-based RAG variant; distinguishing flat document retrieval from graph-structured retrieval clarifies why multi-angle ranking matters
  - Quick check question: How does retrieving structured triplets differ from retrieving document chunks, and what additional filtering does this require?

- **Concept: Cross-Encoder vs Bi-Encoder Ranking**
  - Why needed here: The two-stage ranking uses bi-encoder (UMLS-BERT) for retrieval then cross-encoder (MedCPT) for reranking; understanding this distinction is critical for reproducing the pipeline
  - Quick check question: Why would you use a bi-encoder first and a cross-encoder second, rather than only one?

## Architecture Onboarding

- **Component map:** Entity Extractor (LLM) -> Word Translator -> Local KG Cache -> Remote UMLS Query -> UMLS-BERT Ranker -> MedCPT Cross-Encoder -> Declarative Converter (LLM) -> Final LLM Reasoner -> Self-Knowledge Miner (BM25) fallback
- **Critical path:** Entity extraction → translation → cache/UMLS retrieval → UMLS-BERT ranking → MedCPT reranking → declarative conversion → LLM reasoning. Self-knowledge mining activates only when retrieved triplets score below threshold.
- **Design tradeoffs:** Caching: 14s → 0.0009s per query, but requires periodic sync for incremental UMLS updates. Declarative conversion: Helps GPT/Claude but can hurt LLaMA models due to context length sensitivity. Word-level vs sentence translation: Lower cost and distortion, but fails on questions with minimal medical entities.
- **Failure signatures:** Invalid retrieval (Scenario 2): Medical entities like "Power" return irrelevant triplets ("Cylindrical power", "Heater power") → triggers BM25 self-mining fallback. Negative improvement on CMMLU with Qwen-2.5 72B: Model already strong in Chinese; English knowledge injection interferes.
- **First 3 experiments:**
  1. Replicate the cache hit/miss timing: Query 10 entities with empty cache, then repeat—should see ~14s → 0.0009s
  2. Ablate declarative conversion: Compare raw triplets vs declarative form on SW MMLU with GPT-4o-mini (expect degradation without conversion)
  3. Test break condition: Run medical-adjacent questions with few medical entities (like the marathon example), verify BM25 fallback activates and check answer quality

## Open Questions the Paper Calls Out

None

## Limitations

- Entity extraction reliability remains uncertain with no quantitative accuracy analysis or error rate reporting
- Cross-lingual semantic preservation claims lack direct empirical validation comparing word-level vs sentence-level translation
- Ranking mechanism effectiveness is not independently evaluated with triplet recall or correlation studies
- Model-specific behaviors show inconsistent results (declarative conversion helps GPT/Claude but harms LLaMA-3.1 70B) without systematic analysis

## Confidence

- **High confidence** in caching mechanism performance (14s → 0.0009s) - straightforward engineering optimization with clear quantitative reporting
- **Medium confidence** in overall framework architecture and multilingual medical QA accuracy improvements - supported by consistent improvements across benchmarks
- **Low confidence** in specific mechanisms for entity extraction reliability, cross-lingual semantic preservation, and ranking effectiveness - insufficient empirical validation and ablation studies

## Next Checks

1. **Entity extraction accuracy study**: Systematically evaluate the LLM-based entity extractor across diverse medical question types, measuring precision/recall and identifying failure modes. Test with questions containing minimal medical entities to quantify fallback activation frequency.

2. **Cross-lingual translation ablation**: Compare word-level vs sentence-level translation on a held-out set of multilingual medical questions, measuring semantic drift and downstream QA accuracy. Include language pairs beyond those in the main evaluation.

3. **Ranking mechanism dissection**: Analyze the two-stage ranking pipeline independently—measure triplet recall before/after each ranking stage, compute correlation between ranking scores and answer utility, and test whether irrelevant triplets actually degrade LLM reasoning through controlled experiments.