---
ver: rpa2
title: 'From Clicks to Preference: A Multi-stage Alignment Framework for Generative
  Query Suggestion in Conversational System'
arxiv_id: '2508.15811'
source_url: https://arxiv.org/abs/2508.15811
tags:
- reward
- suggestion
- user
- query
- suggestions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-stage framework to align generative
  query suggestion models with user preferences in conversational systems. The framework
  progresses from prompt engineering and supervised fine-tuning to Gaussian reward
  modeling and reinforcement learning with composite rewards and OOD regularization.
---

# From Clicks to Preference: A Multi-stage Alignment Framework for Generative Query Suggestion in Conversational System

## Quick Facts
- arXiv ID: 2508.15811
- Source URL: https://arxiv.org/abs/2508.15811
- Reference count: 40
- Primary result: 34% relative CTR increase in live A/B tests with Gaussian reward modeling and RL optimization

## Executive Summary
This paper presents a multi-stage alignment framework for generative query suggestion in conversational search systems. The framework progresses from prompt engineering and supervised fine-tuning to Gaussian reward modeling and reinforcement learning with composite rewards and OOD regularization. A key innovation is the Gaussian Reward Model (GaRM), which models user preferences as probability distributions to capture uncertainty, achieving better performance than deterministic reward models. The approach demonstrates significant gains over baseline methods in both automatic and human evaluations, with a 34% relative increase in click-through rate in live A/B tests.

## Method Summary
The framework consists of four progressive stages: (1) prompt engineering to collect high-quality click logs from user interactions, (2) supervised fine-tuning using a teacher-student distillation approach with diversity-aware assembly, (3) Gaussian reward modeling that represents preferences as distributions rather than point estimates, and (4) reinforcement learning with GRPO optimization using composite rewards including the GaRM, rule-based rewards, perplexity-based OOD regularization, and fusion of multiple reward models. The Gaussian formulation captures uncertainty in user preferences and enables better reward model robustness during RL optimization.

## Key Results
- 34% relative increase in click-through rate in live A/B tests
- 80.0% gain over the reference model in GSB (Generation Success Rate with Bias)
- Gaussian Reward Model (GaRM-8B) outperforms BTRM-32B despite being smaller, demonstrating the value of uncertainty modeling
- OOD regularization via perplexity prevents reward hacking, with ablation showing 40 GSB point drop when removed

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Modeling preferences as Gaussian distributions improves reward model robustness to noisy user click signals.
- **Mechanism:** GaRM outputs both mean preference score and uncertainty estimate, using variance to identify samples where model is less confident. During RL, negative standard deviation is incorporated into rewards, steering policy toward high-confidence predictions.
- **Core assumption:** User preferences exhibit stochastic variability that deterministic scalar rewards cannot capture; uncertainty correlates with prediction quality.
- **Evidence anchors:** Abstract states GaRM represents preferences as probability distributions; Figure 4 shows accuracy increasing from ~0.55 to ~0.80 as confidence rises; ECE scores of 0.0302 (in-distribution) vs. 0.0886 (OOD) validate calibration.
- **Break condition:** If uncertainty estimates do not correlate with prediction accuracy, the probabilistic formulation adds complexity without benefit.

### Mechanism 2
- **Claim:** Logged perplexity under reference model serves as proxy for OOD detection, constraining RL policy exploration to reliable regions.
- **Mechanism:** High perplexity indicates samples outside reward model's training distribution where scores are unreliable. Using this as regularization term prevents reward hacking.
- **Core assumption:** Reward model's accuracy degrades on samples with high perplexity under reference model; correlation holds across policy updates.
- **Evidence anchors:** Section 3.6.2 states logged perplexity serves as approximate indicator of in-distribution samples; Table 4 shows ablation removal causes largest performance drop (-40 GSB points); qualitative evidence shows reward hacking with malformed outputs.
- **Break condition:** If perplexity does not correlate with reward model accuracy degradation, regularization term may unnecessarily constrain exploration.

### Mechanism 3
- **Claim:** Progressive distillation from click logs creates higher-quality SFT data than direct training on noisy user interactions.
- **Mechanism:** Two-stage SFT: (1) train large teacher model (Qwen3-32B) on single-suggestion data to synthesize high-quality candidates; (2) use diversity-aware assembly with embedding similarity and n-gram filtering to create diverse 3-suggestion training instances.
- **Core assumption:** Larger offline teacher model can denoise click signals better than direct training; diversity within suggestion groups improves user engagement.
- **Evidence anchors:** Section 3.4 describes progressive data construction methodology; Table 3 shows distillation with Qwen3-32B yields +39 GSB/+24.73% CTR gain vs. +29 GSB/+11.27% without distillation.
- **Break condition:** If teacher model generations diverge from actual user preferences, distillation amplifies distribution shift.

## Foundational Learning

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - **Why needed here:** Framework follows standard RLHF paradigm (SFT → Reward Model → RL optimization). Understanding reward modeling, policy optimization, and reward hacking is essential.
  - **Quick check question:** Can you explain why reward models trained on preference pairs can be exploited by RL policies, and what symptoms indicate reward hacking?

- **Concept: Out-of-Distribution (OOD) Detection in Neural Networks**
  - **Why needed here:** OOD regularization via logged perplexity is central to training stability. Engineers must understand why reward models fail on unseen inputs and how to detect them.
  - **Quick check question:** Why does the policy's exploration during RL generate samples outside the reward model's training distribution, and what metric does this paper use to detect such samples?

- **Concept: Probabilistic / Distributional Reward Modeling**
  - **Why needed here:** GaRM's innovation is representing preferences as distributions. Understanding uncertainty quantification, calibration (ECE), and relationship between variance and confidence is critical.
  - **Quick check question:** If a reward model outputs μ = 2.0, σ = 0.5 for suggestion A and μ = 1.8, σ = 1.5 for suggestion B, which should be preferred and why might the uncertainty matter during RL?

## Architecture Onboarding

- **Component map:** Prompt engineering -> Click log collection -> Teacher model (Qwen3-32B) -> Diversity filtering -> SFT student model -> Click log pairs -> GaRM training -> GRPO RL with composite rewards -> Two-stage fusion (logistic regression -> Pareto refinement)

- **Critical path:**
  1. Prompt quality determines initial data quality -> affects all downstream stages
  2. SFT teacher model size directly impacts distillation quality (Table 3: 32B > MoE-30B)
  3. GaRM's variance regularization (λ in Eq. 5) is critical for stable training; improper tuning causes gradient explosion
  4. PPL reward weight is single most important RL hyperparameter (-40 GSB if removed)

- **Design tradeoffs:**
  - GaRM-8B vs. BTRM-32B: Smaller GaRM achieves better downstream RL performance (+80 GSB vs. +74) despite lower accuracy on IID test sets, suggesting uncertainty signal matters more than raw accuracy
  - Multi-component reward increases complexity but provides redundancy; ablation shows all components contribute (7-40 GSB drops)
  - Two-stage SFT adds latency but yields +13.5% CTR improvement over direct training

- **Failure signatures:**
  - Reward hacking: Model generates malformed outputs (e.g., "1. 2. 3." with no content) to maximize reward without completing task -> indicates PPL regularization failure
  - Position bias in RM: Disproportionate scoring of position-1 suggestions -> check if position filtering was applied during pair construction
  - σ explosion during GaRM training: Gradient norms reaching 10⁴ -> variance regularization coefficient λ too low
  - Diversity collapse: All three suggestions semantically identical -> diversity reward weight insufficient or deduplication pipeline failure

- **First 3 experiments:**
  1. **Validate OOD-perplexity correlation:** Plot reward model accuracy vs. perplexity bins on held-out test set. Confirm monotonic degradation as PPL increases. If flat, mechanism is broken.
  2. **Ablate reward components incrementally:** Start with GaRM-only, add PPL, add rule-based, add Skywork. Measure GSB at each step to verify Table 4 magnitudes reproduce. Largest jump should occur at PPL addition.
  3. **Stress test GaRM uncertainty calibration:** Generate 1000 samples with varying μ-σ combinations, compute ECE on Week 4 vs. Week 4-RFT splits. If ECE > 0.15 on either, variance regularization may need retuning.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on single commercial search platform's click logs constrains external validity
- OOD-perplexity correlation assumption lacks direct quantitative verification (no plots showing accuracy degradation as function of perplexity bins)
- RL policy exploration bounded by reference model, potentially limiting discovery of novel query patterns users might prefer

## Confidence

- **High confidence:** Stage 1 prompt engineering, Stage 2 SFT distillation (robust ablation results, clear CTR gains), Stage 4 RL fusion performance (consistent GSB improvements across ablations)
- **Medium confidence:** Gaussian Reward Model's uncertainty benefits (calibration evidence is strong but mechanism validation indirect), OOD-perplexity regularization (strong ablation effect but weak mechanistic evidence)
- **Low confidence:** Long-term generalization of RL policy (no data beyond 6-week training period), cross-domain transferability (single-platform deployment)

## Next Checks

1. **OOD-perplexity correlation validation:** Create 10 perplexity bins on held-out test set, plot reward model accuracy vs. average perplexity per bin. Verify monotonic degradation; if absent, regularization mechanism is unsupported.

2. **GaRM calibration stress test:** Generate 2000 samples spanning μ ∈ [0,5], σ ∈ [0.1,3.0]. Compute Expected Calibration Error (ECE) on both IID and OOD subsets. If OOD ECE > 0.15, variance regularization λ needs adjustment.

3. **Diversity ablation under production load:** Deploy system with and without diversity reward component in live A/B tests. Measure not just CTR but also suggestion diversity (distinct n-grams per session) and user query reformulation rates. If diversity drops >20% without diversity reward, mechanism is critical.