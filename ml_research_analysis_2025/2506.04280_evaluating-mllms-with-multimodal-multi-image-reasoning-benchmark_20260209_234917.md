---
ver: rpa2
title: Evaluating MLLMs with Multimodal Multi-image Reasoning Benchmark
arxiv_id: '2506.04280'
source_url: https://arxiv.org/abs/2506.04280
tags:
- reasoning
- multi-image
- arxiv
- multimodal
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMRB, the first benchmark for evaluating
  structured visual reasoning across multiple images. It comprises 92 sub-tasks with
  4,750 samples and 68,882 reasoning steps, annotated using GPT-4o and refined by
  human experts.
---

# Evaluating MLLMs with Multimodal Multi-image Reasoning Benchmark

## Quick Facts
- arXiv ID: 2506.04280
- Source URL: https://arxiv.org/abs/2506.04280
- Reference count: 40
- 40+ MLLMs evaluated, showing open-source models lag significantly behind commercial ones in multi-image reasoning tasks

## Executive Summary
This paper introduces MMRB, the first benchmark specifically designed to evaluate structured visual reasoning across multiple images. The benchmark comprises 92 sub-tasks with 4,750 samples and 68,882 reasoning steps, covering spatial, temporal, and semantic reasoning with multi-solution annotations. A key innovation is the sentence-level matching framework using open-source LLMs (Qwen3-32B) for fast and scalable evaluation of Chain-of-Thought reasoning. Experiments reveal that while commercial MLLMs significantly outperform open-source alternatives, multimodal reward models struggle particularly with multi-image ranking tasks, highlighting MMRB's challenge and significance for advancing the field.

## Method Summary
The MMRB benchmark is constructed by curating 22 source datasets and filtering for multi-image, semantically driven tasks. GPT-4o generates three distinct Chain-of-Thought solution paths per sample, which human experts then correct and refine. The evaluation uses a two-tier approach: a rule-based extractor evaluates final answers, while Qwen3-32B performs sentence-level matching to assess reasoning quality against the multi-solution ground truths. The benchmark includes a dedicated subset for evaluating multimodal reward models on ranking tasks. Inference is performed with both direct answering and step-by-step CoT prompting to measure inference-time scaling effects.

## Key Results
- Commercial MLLMs (GPT-4o, GPT-4o-mini) significantly outperform open-source alternatives on multi-image reasoning tasks
- Inference-time scaling via CoT is effective for models >8B parameters but can degrade performance for smaller models
- Multimodal reward models exhibit severe order sensitivity in multi-image ranking tasks, with performance varying dramatically based on sample ordering
- Open-source models larger than 8B achieve an average 6.4% performance improvement from CoT prompting

## Why This Works (Mechanism)

### Mechanism 1: Sentence-Level Matching for Process Evaluation
The framework decomposes model outputs and ground-truth annotations into individual reasoning steps, using Qwen3-32B to match these against defined cognitive operations. This automated approach offers a cost-efficient proxy for human evaluation while maintaining reliability through semantic understanding of reasoning equivalence.

### Mechanism 2: Multi-Solution Annotation Tolerance
By providing three distinct valid reasoning paths per sample, the evaluation allows taking the maximum score across these paths. This prevents penalizing models for valid alternative reasoning strategies and acknowledges the non-deterministic nature of complex reasoning tasks.

### Mechanism 3: Inference-Time Scaling via CoT
Explicit step-by-step prompting forces models to allocate more compute tokens to decompress visual context and model relationships before concluding. This is particularly effective for larger models (>8B) or those not already fine-tuned for reasoning, improving multi-image reasoning accuracy.

## Foundational Learning

- **Concept:** Multimodal Reward Models (MRM)
  - **Why needed here:** MRMs act as "critics" to score or rank output quality, crucial for RLHF applications
  - **Quick check question:** Why does the paper report that MRMs struggle with "order sensitivity" when evaluating multi-image reasoning paths?

- **Concept:** Visual Grounding in Multi-Image Contexts
  - **Why needed here:** Core failure mode identified is "Information Grounding" - attributing specific details to correct images in a sequence
  - **Quick check question:** If a model correctly describes a red car but claims it is in Image 2 instead of Image 3, does it fail at the "Logical Reasoning" step or the "Information Grounding" step?

- **Concept:** Efficacy Score vs. Accuracy
  - **Why needed here:** "Efficacy" measures the delta provided by reasoning strategies, not just absolute accuracy
  - **Quick check question:** If a model has 50% accuracy without CoT and 50% accuracy with CoT, what is its Efficacy Score, and what does that imply about the model's internal state?

## Architecture Onboarding

- **Component map:** 4,750 samples × Avg 6.17 images → Subject Model → CoT text + Final Answer → Evaluator Pipeline (rule-based extractor + Qwen3-32B) → Scores
- **Critical path:** The Manual Inspection and Correction phase, where 355 incorrect ground truths and 1,198 corrected reasoning paths (~25% of data) are identified
- **Design tradeoffs:** Qwen3-32B vs. GPT-4o for evaluation (cost vs. potential oracle capability); excluding hard-math questions to focus on general multi-image reasoning
- **Failure signatures:** Reward Model Instability (extreme performance drop with order swap), Small Model Instruction Drift (generic text output instead of structured CoT)
- **First 3 experiments:**
  1. Sanity Check the Evaluator: Run Qwen3-32B on GPT-4o generated answers to verify Process Score correlation with human intuition
  2. Reproduce the Scaling Curve: Evaluate InternVL3-1B, 8B, and 38B on 100 samples to reproduce model size vs. CoT efficacy correlation
  3. Probe Reward Model Bias: Run LLaVA-Critic on Reward Subset twice (swapped order) to confirm "Order Instability" finding

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the underlying causes of the extreme positional bias in multimodal reward models when evaluating multi-image inputs?
- **Basis in paper:** [explicit] Section 7.2 notes that reversing the order of accepted and rejected samples causes drastic performance drops (e.g., GPT-4o-mini dropping from 85.69% to 10.20%), stating this "warrants further investigation in future research"
- **Why unresolved:** The paper identifies annotation scarcity and potential model bias as factors but doesn't isolate specific attention or architectural mechanisms causing order sensitivity
- **What evidence would resolve it:** Ablation studies on attention mechanisms during pair-wise ranking, or training data designed to neutralize positional bias in multi-image contexts

### Open Question 2
- **Question:** How does the sentence-level matching framework using Qwen3-32B compare to human evaluation in assessing complex multi-image reasoning chains?
- **Basis in paper:** [inferred] Section 5.2 introduces the evaluation protocol but provides no direct quantitative correlation analysis against human or GPT-4o annotations for final results
- **Why unresolved:** While the method is claimed efficient, equivalence to nuanced grading for complex reasoning steps remains unverified
- **What evidence would resolve it:** Correlation score (e.g., Cohen's kappa) comparing Qwen3-32B evaluator judgments against randomized human expert panel on same reasoning chains

### Open Question 3
- **Question:** Can specific multi-image Chain-of-Thought training strategies bridge the performance gap between open-source and commercial MLLMs?
- **Basis in paper:** [explicit] Conclusion states open-source MLLMs "still lag significantly behind commercial MLLMs" and exhibit "limited multi-image reasoning capabilities"
- **Why unresolved:** The paper demonstrates scaling effectiveness but doesn't determine if lag is due to data scarcity, architecture limitations, or lack of specific multi-image CoT fine-tuning
- **What evidence would resolve it:** Training experiments where open-source baseline is fine-tuned on MMRB training subset to measure performance delta against commercial baselines

## Limitations
- Evaluation framework heavily depends on quality of multi-solution annotations generated by GPT-4o and human-corrected, with 25% correction rate suggesting potential systematic bias
- Qwen3-32B as evaluation judge may introduce distributional bias despite outperforming GPT-4o on general tasks
- The 92 sub-tasks and 4,750 samples, while substantial, may not fully capture the diversity of real-world multi-image reasoning scenarios

## Confidence

- **High Confidence:** Commercial MLLMs significantly outperform open-source alternatives on multi-image reasoning tasks (consistent performance gaps across 40+ models)
- **Medium Confidence:** Efficacy of inference-time scaling via CoT for models >8B parameters (trend clear but mechanism may be confounded by instruction-following capability)
- **Medium Confidence:** Multimodal reward models struggle with order sensitivity in multi-image ranking tasks (magnitude well-demonstrated but underlying cause requires investigation)

## Next Checks

1. **Cross-Evaluator Validation:** Re-run Process Score evaluation using Llama-3-70B on a random 10% sample to test Qwen3-32B scoring system robustness

2. **Small Model CoT Analysis:** Manually inspect 50 CoT outputs from 1-2B parameter models to quantify exact failure modes causing negative efficacy scores

3. **Annotation Quality Audit:** Evaluate 100 samples with both original GPT-4o-generated paths and human-corrected versions, comparing Process Score distributions to quantify annotation noise impact