---
ver: rpa2
title: Deep operator network for surrogate modeling of poroelasticity with random
  permeability fields
arxiv_id: '2509.11966'
source_url: https://arxiv.org/abs/2509.11966
tags:
- permeability
- surrogate
- network
- training
- deeponet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a DeepONet-based surrogate model for poroelasticity
  problems with spatially variable permeability fields. The framework learns the solution
  operator mapping permeability realizations to transient displacement and pressure
  fields.
---

# Deep operator network for surrogate modeling of poroelasticity with random permeability fields

## Quick Facts
- **arXiv ID:** 2509.11966
- **Source URL:** https://arxiv.org/abs/2509.11966
- **Reference count:** 40
- **Primary result:** DeepONet surrogate achieves high accuracy (RMSE 10⁻³–10⁻²) for poroelasticity with random permeability, delivering orders-of-magnitude speedup over FEM.

## Executive Summary
This study develops a DeepONet-based surrogate model for poroelasticity problems with spatially variable permeability fields. The framework learns the solution operator mapping permeability realizations to transient displacement and pressure fields. Key methodological enhancements include nondimensionalization of governing equations, dimensionality reduction via truncated Karhunen–Loéve expansion, and a two-step training procedure that decouples branch and trunk network optimization. The approach is evaluated on soil consolidation and ground subsidence problems, achieving high predictive accuracy across various permeability statistics and delivering orders-of-magnitude speedup over finite element simulations.

## Method Summary
The framework uses a DeepONet architecture with a two-step training procedure. First, the permeability field is represented as a truncated Karhunen–Loéve expansion of orthogonal eigenfunctions and Gaussian random variables. Physical variables are nondimensionalized to stabilize training. The trunk network is trained first to learn optimal spatiotemporal bases, followed by QR decomposition to orthogonalize these bases. The branch network is then trained separately to map the low-dimensional K-L coefficients to the projected targets. Separate DeepONets are trained for displacement and pressure outputs. The method is implemented in PyTorch and validated against FEM solutions from FEniCSx.

## Key Results
- Test RMSE values for soil consolidation range from 2.56×10⁻³ to 5.36×10⁻²
- Test RMSE values for ground subsidence range from 4.60×10⁻³ to 2.07×10⁻²
- Crossover simulation counts (where surrogate becomes more efficient than FEM) range from 8,442–9,246 for consolidation and 9,351–9,432 for subsidence
- K-L truncation order M=40 or 60 found optimal; M=400 shows overfitting without accuracy gains

## Why This Works (Mechanism)

### Mechanism 1: Input Dimensionality Reduction
- **Claim:** K-L expansion renders high-dimensional random permeability fields tractable for the neural operator
- **Mechanism:** Log-permeability field represented as truncated series of orthogonal eigenfunctions and Gaussian random variables; DeepONet branch network maps these low-dimensional coefficients rather than full discretized field
- **Core assumption:** Permeability field energy concentrated in leading eigenmodes (rapid spectral decay)
- **Evidence anchors:** Section 3.2 describes K-L coefficient mapping; abstract mentions dimensionality reduction; neighbor papers frequently employ this approach
- **Break condition:** Slow spectral decay (e.g., exponential covariance) requires large M, negating surrogate efficiency

### Mechanism 2: Nondimensionalization
- **Claim:** Rescaling physical variables stabilizes training by normalizing scales
- **Mechanism:** Variables rescaled using characteristic quantities (reference length, permeability) to reduce numerical stiffness and magnitude disparities
- **Core assumption:** Valid characteristic scales can be derived from problem physics to generalize across parametric regimes
- **Evidence anchors:** Section 3.1 defines scaling parameters; abstract lists nondimensionalization as key strategy
- **Break condition:** Loading conditions or material properties vary by orders of magnitude outside training distribution

### Mechanism 3: Two-Step Training
- **Claim:** Decoupling branch and trunk network optimization improves accuracy over joint optimization
- **Mechanism:** Trunk network trained first to learn optimal spatiotemporal bases, then QR decomposed; branch network trained separately to map coefficients to projected targets
- **Core assumption:** Solution space can be spanned by trunk network's basis functions independent of specific input coefficients
- **Evidence anchors:** Section 3.3 details loss functions and QR decomposition; abstract cites two-step procedure
- **Break condition:** Output fields require input-dependent basis functions that fixed trunk network cannot express

## Foundational Learning

- **Concept:** Karhunen–Loéve (K-L) Expansion
  - **Why needed here:** Data compression gatekeeper; cannot feed dense random field directly into network
  - **Quick check question:** Given Gaussian covariance kernel, how to determine number of modes M for 95% field variance?

- **Concept:** DeepONet Architecture (Branch & Trunk)
  - **Why needed here:** Core functional approximator; must distinguish branch (input function) from trunk (spatiotemporal domain)
  - **Quick check question:** Why is output computed as dot product ⟨c, φ⟩ rather than sequential layer pass?

- **Concept:** Poroelasticity (u–p formulation)
  - **Why needed here:** Provides physical constraints and nondimensionalization context; need coupling between displacement and pressure
  - **Quick check question:** How does characteristic time scale T* relate to permeability k* and viscosity μ_f in consolidation?

## Architecture Onboarding

- **Component map:** Random Field → K-L Expansion → Coefficients ξ_M (Branch Input) → Branch Net → Coefficients → Dot Product → Prediction; Spatiotemporal points (x,t) (Trunk Input) → Trunk Net → Bases → Dot Product → Prediction

- **Critical path:** 1) Generate FEM data and nondimensionalize; 2) Compute K-L eigenmodes and truncate; 3) Train Trunk + coefficient matrix A to minimize reconstruction error; 4) QR decompose trunk output → compute projected targets B*; 5) Train Branch network to map ξ_M → B*

- **Design tradeoffs:**
  - Truncation Order (M): Higher M captures more detail but increases branch input dimension and data requirements
  - Basis Size (K): Larger K allows more complex output fields but increases training time
  - Separate vs. Shared Networks: Separate DeepONets for u_z and p provide flexibility but trade parameter efficiency

- **Failure signatures:**
  - High RMSE on pressure: Pressure sensitive to fine-scale permeability features; may need higher M or deeper trunk
  - Stagnant Loss in Step 1: Trunk failing to learn bases; check learning rate decay or basis size K
  - Generalization Gap: Often caused by insufficient K-L modes or training data diversity relative to permeability statistics

- **First 3 experiments:**
  1. Baseline Reproduction: Replicate soil consolidation case with (σ_κ, l_x, l_z) = (1.5, 0.25, 0.125); verify M=40 minimizes RMSE for u_z
  2. Ablation on Training Method: Compare two-step training against standard end-to-end Adam optimization
  3. Generalization Test: Train on low variance (σ_κ=0.5) and test on high variance (σ_κ=1.5) to determine robustness outside training distribution

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can DeepONet framework handle permeability fields with slow-decaying spectra (exponential covariance kernels) where K-L truncation becomes inefficient?
- **Basis in paper:** Explicit statement in Conclusion that K-L truncation "may be less suitable for input permeability fields characterized by slow-decaying spectra"
- **Why unresolved:** Current strategy relies on rapid spectral decay assumption; slow decay would require retaining high-dimensional inputs
- **What evidence would resolve it:** Evaluation on datasets generated using exponential vs. Gaussian covariance kernels

### Open Question 2
- **Question:** Can framework extend to generalize across varying mechanical parameters and statistical configurations without retraining?
- **Basis in paper:** Paper notes limitation that "framework assumes fixed mechanical parameters and single statistical configuration per network"
- **Why unresolved:** Current architecture maps K-L coefficients for specific statistics; doesn't accept physical parameters or statistical hyperparameters as inputs
- **What evidence would resolve it:** Successful implementation of multi-input DeepONet taking parameters as distinct inputs across unseen settings

### Open Question 3
- **Question:** Does incorporating physics-informed training objectives improve robustness and physical consistency?
- **Basis in paper:** Authors suggest "incorporating architecture-level constraints or physics-informed training objectives may further improve robustness"
- **Why unresolved:** Current training relies entirely on data-driven supervised learning without explicit conservation law constraints
- **What evidence would resolve it:** Comparative analysis of standard vs. physics-informed DeepONet training measuring governing equation violations

## Limitations

- K-L expansion effectiveness contingent on rapid spectral decay of permeability covariance, which may not hold for all geological formations
- Nondimensionalization strategy assumes characteristic scales can be consistently derived across varying loading conditions
- Current evaluation limited to two specific problem types (soil consolidation and ground subsidence) with limited exploration of complex boundary conditions

## Confidence

- **High Confidence:** Methodological framework (DeepONet architecture, two-step training, K-L expansion) is technically sound and well-documented; error metrics and computational speedup claims clearly specified
- **Medium Confidence:** Generalizability to different poroelasticity problems and permeability statistics is demonstrated but not exhaustively explored; hyperparameter choices empirically justified but may not be optimal for all configurations
- **Low Confidence:** Robustness to extreme parametric variations outside training distribution and scalability to larger three-dimensional problems remain untested

## Next Checks

1. **Input-Dependent Basis Test:** Evaluate whether fixed trunk network can adequately represent solutions for permeability fields with very different spatial correlation structures (exponential vs. Gaussian covariance) without retraining

2. **Distribution Shift Evaluation:** Train surrogate on low-variance permeability fields (σ_κ=0.5) and test on high-variance fields (σ_κ=1.5) to quantify performance degradation and determine extrapolation capability

3. **Three-Dimensional Extension:** Extend framework to simplified 3D consolidation problem to assess computational efficiency gains and identify architectural modifications required for higher-dimensional spatiotemporal outputs