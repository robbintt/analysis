---
ver: rpa2
title: Expert-Free Online Transfer Learning in Multi-Agent Reinforcement Learning
arxiv_id: '2501.15495'
source_url: https://arxiv.org/abs/2501.15495
tags:
- agent
- learning
- transfer
- agents
- thesis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis introduces Expert-Free Online Transfer Learning (EF-OnTL),
  a novel framework for online transfer learning in multi-agent reinforcement learning.
  EF-OnTL enables agents to share selected experiences to expedite the learning process,
  overcoming the need for a fixed expert.
---

# Expert-Free Online Transfer Learning in Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2501.15495
- Source URL: https://arxiv.org/abs/2501.15495
- Authors: Alberto Castagna
- Reference count: 0
- This thesis introduces Expert-Free Online Transfer Learning (EF-OnTL), a novel framework for online transfer learning in multi-agent reinforcement learning

## Executive Summary
This thesis introduces Expert-Free Online Transfer Learning (EF-OnTL), a novel framework for online transfer learning in multi-agent reinforcement learning. EF-OnTL enables agents to share selected experiences to expedite the learning process, overcoming the need for a fixed expert. The framework dynamically selects a temporary expert based on real-time performance metrics and filters experiences using uncertainty and expected surprise. The core method involves agents estimating epistemic uncertainty using State Action Reward Next-state Random Network Distillation (sars-RND), an extension of Random Network Distillation. This allows for more accurate uncertainty estimation by considering action, reward, and next state information. The selected expert shares a batch of experiences, which target agents filter based on delta confidence and expected surprise.

## Method Summary
EF-OnTL is a peer-to-peer transfer learning framework where agents dynamically select a temporary "expert" based on performance metrics and share experience batches. The key innovation is sars-RND, which extends Random Network Distillation to estimate uncertainty from state-action-reward-next-state tuples rather than just states. Agents use this uncertainty signal to filter incoming experiences based on the delta confidence (gap between source confidence and target uncertainty) and expected surprise (approximated by TD-error). The framework supports both discrete (Dueling DQN) and continuous (PA-DDPG) action spaces, with transfer occurring at fixed intervals using configurable budgets.

## Key Results
- EF-OnTL doubled the number of goals scored in Half Field Offense compared to independent agents
- In sparse reward environments like MT-PP, high transfer budgets (5000) degraded performance while low budgets (500) improved win rates
- EF-OnTL eventually caught up to expert-based methods like RCMP, demonstrating viability without a fixed expert

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Expert Selection via Real-Time Metrics
The framework overcomes the need for a fixed, pre-trained expert by dynamically selecting a temporary "teacher" agent at each transfer step based on local performance metrics. At every transfer interval, agents share metadata (average uncertainty or cumulative reward). A Source Selection Criteria (SS) algorithm identifies the agent with the lowest average uncertainty or highest performance. This agent is designated as the source and shares its experience buffer with others. Agents in a multi-agent system learn at different rates, creating temporary "islands of expertise" that can be exploited for peer-to-peer transfer.

### Mechanism 2: sars-RND for Epistemic Uncertainty Estimation
Extending Random Network Distillation (RND) to include State, Action, Reward, and Next-state (sars-RND) provides a finer-grained estimate of epistemic uncertainty than state-only RND. Traditional RND estimates uncertainty based solely on state visitation. sars-RND trains a predictor network to match a random target network using the input tuple $(s, a, r, s')$. The prediction error serves as the uncertainty signal. This allows the agent to distinguish between familiar states where a specific *action* was previously unexplored. Uncertainty in RL is action-dependent; an agent may be certain about a state but uncertain about the outcome of a specific action taken in that state.

### Mechanism 3: Target-Side Filtering via Delta Confidence
Effective transfer is achieved not just by sending data, but by the *target* filtering incoming data based on the discrepancy between the source's confidence and its own uncertainty (Delta Confidence). The target agent estimates its own uncertainty on the source's experience tuples. It calculates $\Delta\text{-conf} = u_{target} - u_{source}$. It prioritizes experiences where the source is confident (low $u_{source}$) but the target is uncertain (high $u_{target}$). Transfer is most beneficial when filling specific "knowledge gaps" where the target lacks data that the source has mastered.

## Foundational Learning

- **Concept: Random Network Distillation (RND)**
  - **Why needed here:** The core uncertainty estimator (sars-RND) is built on top of RND principles. You must understand how prediction error in a fixed random network acts as a novelty detector.
  - **Quick check question:** Can you explain why the target network in RND is kept fixed (untrained) while the predictor network is trained?

- **Concept: Temporal Difference (TD) Error**
  - **Why needed here:** One of the filtering criteria (Transfer Content Selection) uses "expected surprise," which is approximated by the TD-error of the learning model.
  - **Quick check question:** If an agent observes a transition $(s, a, r, s')$ where the predicted value of $s$ differs significantly from the observed reward plus the predicted value of $s'$, what does a high TD-error imply about the agent's current policy?

- **Concept: Dueling Network Architectures**
  - **Why needed here:** The implementation uses Dueling DQN for discrete environments. Understanding the split between Value and Advantage streams is necessary to interpret the network outputs.
  - **Quick check question:** Why might a Dueling network learn faster than a standard DQN in states where the choice of action is irrelevant?

## Architecture Onboarding

- **Component map:** RL Agent -> sars-RND Module -> Transfer Buffer -> Transfer Core Engine
- **Critical path:** Collection: Agent interacts with env → stores tuple in local buffer with estimated uncertainty → Selection (SS): At transfer interval, agents broadcast average uncertainty → the agent with lowest average is selected as Source → Filtering (TCS): Non-source agents calculate Δ-conf for every tuple in the Source's buffer → they select the top B tuples where Δ-conf is highest → Update: The filtered batch is used to update the RL Agent's weights
- **Design tradeoffs:**
  - **Source Selection:** `Average Uncertainty (U)` vs `Best Performance (BP)`. *Tradeoff:* U is more stable in complex environments (MT-PP); BP may select lucky agents. *Recommendation:* Default to U.
  - **Transfer Budget ($B$):** Larger $B$ isn't always better. In sparse reward environments (MT-PP), large $B$ introduced noise and hurt performance. *Recommendation:* Start with smaller budgets (e.g., 500) for sparse rewards.
  - **Filtering:** `high ∆-conf` vs `loss & conf`. `high ∆-conf` (uncertainty gap) was generally more robust than `loss & conf` (TD-error).
- **Failure signatures:**
  - **Negative Transfer / Performance Drop:** Occurs if the transfer budget is too high in sparse reward settings, flooding the buffer with low-value experiences.
  - **Budget Exhaustion (in baselines like RCMP):** If the threshold for advice is too low, the agent stops learning prematurely. EF-OnTL avoids this by using peer data rather than a fixed budget for *advice*.
  - **Stagnant Uncertainty:** If the sars-RND predictor overfits immediately, uncertainty drops to zero, and no transfer occurs.
- **First 3 experiments:**
  1. **Hyperparameter Sweep (Encoder Size):** Run the sensitivity analysis described in Section 3.4 for sars-RND encoder size (Figure 3.9) to ensure the uncertainty curve decays appropriately (not too fast, not too slow).
  2. **Ablation on Source Selection:** Compare `SS: U` vs `SS: BP` in a competitive environment (MT-PP) to verify that uncertainty-based selection leads to more consistent win rates.
  3. **Budget Stress Test:** Test high budget ($B=5000$) vs low budget ($B=500$) in the Cart-Pole environment to replicate the result that high budgets benefit dense-reward tasks but may harm sparse-reward tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would enabling dynamic criteria selection for identifying the source of transfer and filtering incoming knowledge yield superior performance compared to the fixed criteria evaluated?
- Basis in paper: [explicit] The author states in Section 7.3 (Future Work), "we propose enabling dynamic criteria selection to choose both the source of transfer and the interactions to be integrated into a target agent's learning process."
- Why unresolved: The current EF-OnTL framework relies on fixed metrics (e.g., Average Uncertainty or Best Performance for source selection) that do not adapt based on the changing status of the system.
- What evidence would resolve it: Comparative experiments showing improved convergence speed or asymptotic performance in a system that dynamically switches selection criteria based on system status versus the static methods tested in the thesis.

### Open Question 2
- Question: Does dynamically adjusting the transfer budget $B$ or varying the frequency of transfer improve the trade-off between performance and communication cost?
- Basis in paper: [explicit] Section 7.3 identifies the "investigation of dynamic adjustment of the transfer budget $B$ and its impact on the final performance" and the assessment of "the effect of increased transfer frequency with a smaller budget against lower transfer frequency" as necessary future steps.
- Why unresolved: The evaluation in the thesis utilizes fixed transfer budgets (e.g., 100, 500, 5000) and frequencies, leaving the impact of adaptive budgeting unstudied.
- What evidence would resolve it: Empirical results demonstrating the performance trajectory of agents using an adaptive budget mechanism compared to those using the static fixed-budget configurations.

### Open Question 3
- Question: Can the computational efficiency of sars-RND be improved by reducing its input to state-action pairs without compromising the quality of uncertainty estimation?
- Basis in paper: [explicit] Section 7.3 outlines plans to "refine sars-RND by reducing its input to state-action pairs and assess the impact that this adjustment might have on both the estimation of epistemic uncertainty and the transfer effect."
- Why unresolved: The current sars-RND model uses the full interaction tuple (state, action, reward, next-state), which increases the computational overhead and contributes to the increased training time limitation noted in Section 7.2.
- What evidence would resolve it: A comparative analysis of training time and transfer learning efficacy between the current sars-RND and a simplified version utilizing only state-action inputs.

## Limitations

- The framework assumes homogeneous agent architectures and similar state representations, which may not hold in heterogeneous multi-agent systems
- The computational overhead of maintaining both predictor and target networks for uncertainty estimation across multiple agents could become prohibitive in large-scale MARL applications
- Scalability claims to large multi-agent systems are based on theoretical arguments rather than empirical validation

## Confidence

- **High Confidence:** The dynamic expert selection mechanism (SS algorithm) is well-validated across multiple environments, showing consistent improvement over fixed-expert baselines. The filtering mechanism using delta confidence is also well-supported by empirical results, particularly in competitive environments like MT-PP where it demonstrably improves win rates.
- **Medium Confidence:** The sars-RND uncertainty estimation shows promise but requires more rigorous ablation studies to quantify its advantage over standard RND. While the thesis provides evidence of better action sensitivity, the computational trade-offs and performance in more complex continuous domains need further validation.
- **Low Confidence:** The scalability claims to large multi-agent systems are based on theoretical arguments rather than empirical validation. The performance in environments with more than 5 agents or with highly asymmetric agent capabilities remains speculative.

## Next Checks

1. **Continuous Action Space Validation:** Implement sars-RND in a continuous control benchmark (e.g., Multi-Agent Particle Environment) and compare uncertainty estimation quality against both standard RND and ensembles. Measure both accuracy and computational overhead.

2. **Heterogeneous Agent Test:** Design a MARL scenario with agents having different state/action spaces and reward structures. Evaluate whether the delta confidence filtering mechanism breaks down when agents cannot meaningfully compare uncertainties.

3. **Large-Scale Stress Test:** Scale up the framework to 10+ agents in a benchmark like SMAC (StarCraft Multi-Agent Challenge). Measure the communication overhead and evaluate whether the source selection criteria remains effective when agent performances are tightly clustered.