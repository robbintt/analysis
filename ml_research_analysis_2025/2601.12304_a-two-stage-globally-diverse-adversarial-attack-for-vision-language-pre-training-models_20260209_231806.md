---
ver: rpa2
title: A Two-Stage Globally-Diverse Adversarial Attack for Vision-Language Pre-training
  Models
arxiv_id: '2601.12304'
source_url: https://arxiv.org/abs/2601.12304
tags:
- adversarial
- attack
- s-gda
- text
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes 2S-GDA, a two-stage globally-diverse adversarial
  attack framework for vision-language pre-training models. The method improves transferability
  by introducing a globally-aware textual perturbation mechanism and a visual perturbation
  module with multi-scale resizing and block-shuffle rotation.
---

# A Two-Stage Globally-Diverse Adversarial Attack for Vision-Language Pre-training Models

## Quick Facts
- arXiv ID: 2601.12304
- Source URL: https://arxiv.org/abs/2601.12304
- Authors: Wutao Chen; Huaqin Zou; Chen Wan; Lifeng Huang
- Reference count: 0
- Key outcome: Proposes 2S-GDA, achieving up to 11.17% improvement in black-box attack success rates on VLP models

## Executive Summary
This paper introduces 2S-GDA, a two-stage adversarial attack framework designed specifically for vision-language pre-training (VLP) models. The method enhances adversarial transferability through a globally-aware textual perturbation mechanism and a visual perturbation module incorporating multi-scale resizing and block-shuffle rotation. Experiments demonstrate consistent superiority over state-of-the-art baselines, with significant improvements in black-box attack success rates. The framework's modular design allows integration with existing methods to further boost adversarial effectiveness.

## Method Summary
2S-GDA operates in two stages: first, it applies globally-aware textual perturbations to generate diverse adversarial inputs across the semantic space; second, it employs a visual perturbation module with multi-scale resizing and block-shuffle rotation to enhance visual diversity. This dual approach addresses the challenge of generating transferable adversarial examples against black-box VLP models. The framework is designed to be modular, allowing combination with existing attack methods to further improve transferability performance.

## Key Results
- Achieves up to 11.17% improvement in black-box attack success rates compared to state-of-the-art baselines
- Demonstrates consistent performance gains across various VLP model architectures
- Shows modular design capability to enhance existing adversarial attack methods

## Why This Works (Mechanism)
The framework's effectiveness stems from its two-stage approach that separately optimizes textual and visual components. The globally-aware textual perturbation generates diverse semantic variations that explore a broader adversarial space, while the multi-scale visual perturbations with block-shuffle rotation create complementary visual diversity. This separation allows each component to specialize in its domain while maintaining compatibility, resulting in more transferable adversarial examples that are robust across different model architectures.

## Foundational Learning

**Vision-Language Pre-training Models**: Jointly trained models that learn cross-modal representations from image-text pairs. Needed because 2S-GDA targets VLP models specifically, requiring understanding of their dual-input nature. Quick check: Verify the model accepts both image and text inputs simultaneously.

**Adversarial Transferability**: The ability of adversarial examples generated against one model to fool different models. Critical for black-box attacks where the target model is unknown. Quick check: Test whether attacks transfer between different VLP architectures.

**Multi-scale Perturbations**: Applying transformations at different spatial resolutions. Important for capturing both fine-grained and global visual features. Quick check: Confirm perturbations work across varying image sizes and aspect ratios.

**Block-shuffle Rotation**: Rearranging image blocks and applying rotations. Creates structured visual noise that preserves some spatial relationships. Quick check: Validate that block-shuffle maintains recognizable image content.

**Globally-aware Textual Perturbation**: Generating textual variations that explore semantic space broadly rather than locally. Essential for creating diverse adversarial inputs. Quick check: Ensure perturbed text maintains relevance to the original image.

## Architecture Onboarding

Component map: TextualPerturbation -> VisualPerturbation -> CombinedAttack

Critical path: The framework first generates textual perturbations, then applies visual perturbations to the resulting image-text pairs, combining both to create the final adversarial example. This sequential dependency ensures each stage builds upon the previous one's output.

Design tradeoffs: The two-stage approach introduces computational overhead compared to single-stage methods but provides superior transferability. The separation of concerns allows specialized optimization but requires careful coordination between stages to maintain attack effectiveness.

Failure signatures: Poor textual perturbations may result in semantically inconsistent image-text pairs, reducing attack effectiveness. Overly aggressive visual perturbations can destroy image content, making the attack detectable or ineffective. Insufficient diversity in either stage limits transferability.

First experiments:
1. Validate individual component performance by testing textual perturbations alone and visual perturbations alone
2. Measure transferability gains when combining both stages versus using either stage independently
3. Test robustness against various VLP model architectures to establish baseline transferability

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on standard VLP benchmarks without testing diverse real-world scenarios
- Visual perturbation module effectiveness across varying image scales and domains requires further validation
- Computational overhead of the two-stage approach is not thoroughly analyzed

## Confidence

High: Core methodology and reported improvements (up to 11.17% success rate gains)
Medium: Generalizability claim regarding combination with existing methods
Low: Robustness and practical applicability claims due to limited testing beyond standard benchmarks

## Next Checks

1. Evaluate 2S-GDA on additional vision-language tasks beyond standard classification, such as visual question answering or image captioning, to assess broader applicability
2. Conduct ablation studies isolating the contributions of the globally-aware textual perturbation and multi-scale visual perturbation components to quantify their individual impacts on transferability
3. Test the framework's performance under varying computational constraints and compare its efficiency against single-stage alternatives to establish practical viability