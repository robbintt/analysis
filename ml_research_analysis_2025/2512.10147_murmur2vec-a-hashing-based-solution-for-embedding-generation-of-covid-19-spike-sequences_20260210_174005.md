---
ver: rpa2
title: 'Murmur2Vec: A Hashing Based Solution For Embedding Generation Of COVID-19
  Spike Sequences'
arxiv_id: '2512.10147'
source_url: https://arxiv.org/abs/2512.10147
tags:
- hash
- sequence
- embedding
- sequences
- collision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of scalable embedding generation
  for large-scale COVID-19 spike sequence analysis. Existing phylogenetic tree-based
  methods are computationally intensive and do not scale well, while current embedding-based
  techniques often rely on aligned sequences or exhibit suboptimal predictive performance
  and high runtime costs.
---

# Murmur2Vec: A Hashing Based Solution For Embedding Generation Of COVID-19 Spike Sequences

## Quick Facts
- arXiv ID: 2512.10147
- Source URL: https://arxiv.org/abs/2512.10147
- Reference count: 27
- Classification accuracy up to 86.4% with 99.81% runtime reduction

## Executive Summary
This paper addresses the challenge of scalable embedding generation for large-scale COVID-19 spike sequence analysis. Existing phylogenetic tree-based methods are computationally intensive and do not scale well, while current embedding-based techniques often rely on aligned sequences or exhibit suboptimal predictive performance and high runtime costs. The authors propose Murmur2Vec, a hashing-based method that generates compact, low-dimensional representations of spike sequences by converting unaligned sequences into k-mers, counting their frequencies, and applying Murmur hash to create fixed-length embeddings. This approach is alignment-free and avoids expensive sequence alignment steps.

Experimental results on 7,000 spike sequences from 22 SARS-CoV-2 lineages show that Murmur2Vec achieves up to 86.4% classification accuracy while reducing embedding generation time by as much as 99.81% compared to state-of-the-art methods. The method outperforms existing approaches across multiple evaluation metrics including accuracy, precision, recall, F1 scores, and ROC-AUC, demonstrating its potential as a fast, effective, and scalable solution for large-scale viral sequence analysis.

## Method Summary
Murmur2Vec generates embeddings through a three-step process: (1) Extract k-mers using a sliding window approach from unaligned spike sequences, (2) Count k-mer frequencies using a dictionary, and (3) Apply Murmur hash to each k-mer to bin counts into a fixed-length embedding table. The method uses MurmurHash3 with seed=0 and allows controlled collision rates through table size selection. The approach is trained and evaluated using 70/30 train-test splits with 5-fold cross-validation repeated 5 times. Multiple classifiers (SVM, Naive Bayes, KNN, Random Forest, MLP, Logistic Regression, Decision Tree) are tested to assess the quality of the generated embeddings.

## Key Results
- Achieved up to 86.4% classification accuracy on 22 SARS-CoV-2 lineages
- Reduced embedding generation time by up to 99.81% compared to baselines
- Outperformed existing methods (Spike2Vec, PWM2Vec, Spaced k-mers) across accuracy, precision, recall, F1 scores, and ROC-AUC metrics
- Demonstrated scalability on 40,000 sequences with consistent performance

## Why This Works (Mechanism)
The method works by converting variable-length biological sequences into fixed-length numerical representations through hashing. By using k-mers as features and Murmur hash for dimensionality reduction, the approach preserves important sequence patterns while eliminating the need for expensive alignment steps. The hashing process creates a form of locality-sensitive hashing where similar sequences produce similar embeddings, enabling effective classification while maintaining computational efficiency.

## Foundational Learning
- **k-mer extraction**: Breaking sequences into overlapping substrings of length k - needed to capture local sequence patterns; quick check: verify sliding window implementation
- **Frequency counting**: Creating histograms of k-mer occurrences - needed to represent sequence composition; quick check: validate dictionary counting logic
- **Murmur hashing**: Using non-cryptographic hash functions for fast, uniform distribution - needed for efficient dimensionality reduction; quick check: test hash collision rates
- **Fixed-length embedding**: Converting variable-length sequences to consistent vector sizes - needed for machine learning compatibility; quick check: verify all embeddings have same dimension
- **Alignment-free representation**: Avoiding sequence alignment computationally - needed for scalability; quick check: confirm no alignment steps in pipeline
- **Controlled collision hashing**: Managing hash table size to balance accuracy and speed - needed for practical deployment; quick check: test different table sizes

## Architecture Onboarding

**Component Map**: Sequence -> K-mer extraction -> Frequency counting -> Murmur hashing -> Fixed-length embedding -> Classification

**Critical Path**: The most compute-intensive step is k-mer extraction and counting, followed by hashing operations. The embedding generation is the bottleneck for large datasets, while classification is relatively fast.

**Design Tradeoffs**: The method trades some classification accuracy for dramatic runtime improvements. Using hashing introduces controlled information loss through collisions, but this is offset by the ability to process millions of sequences efficiently. The choice of k-mer length and hash table size represents the primary hyperparameters balancing accuracy and speed.

**Failure Signatures**: Excessive collisions will degrade classification accuracy. Class imbalance (evident in the dataset with B.1.1.7 having 100x more samples than R.1) may skew performance metrics. Inconsistent hash implementations can produce non-deterministic embeddings.

**First Experiments**:
1. Generate embeddings for a small test set (100 sequences) and verify consistency across runs
2. Compare embedding generation time against a baseline alignment-based method
3. Test classification accuracy with increasing hash table sizes to find the accuracy-speed tradeoff point

## Open Questions the Paper Calls Out
- Can Murmur2Vec generalize to viral genomes with different mutational profiles, such as Zika or Rabies?
- How does Murmur2Vec perform on multi-million-scale sequence datasets?
- Can Murmur2Vec embeddings identify novel lineages without prior supervised training?

## Limitations
- Accuracy (86.4% max) lags behind deep learning approaches achieving 90-95% on similar tasks
- Method validated only on SARS-CoV-2 spike sequences, limiting generalizability claims
- Potential loss of fine-grained evolutionary information through hashing process

## Confidence
- Runtime improvement claims: **High** confidence - Well-documented through direct timing comparisons
- Classification accuracy claims: **Medium** confidence - Specific metrics reported but performance gap with modern methods noted
- Scalability claims: **Medium** confidence - Demonstrated on 40,000 sequences but multi-million scale untested

## Next Checks
1. Replicate the embedding generation pipeline with publicly available SARS-CoV-2 spike sequences to verify the reported 99.81% runtime reduction
2. Test the method's performance on sequences from non-SARS-CoV-2 coronaviruses to assess generalizability
3. Compare classification results against deep learning approaches (CNN/LSTM) on the same dataset to benchmark practical utility