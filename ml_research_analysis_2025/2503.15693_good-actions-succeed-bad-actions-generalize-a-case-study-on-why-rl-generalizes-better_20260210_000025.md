---
ver: rpa2
title: 'Good Actions Succeed, Bad Actions Generalize: A Case Study on Why RL Generalizes
  Better'
arxiv_id: '2503.15693'
source_url: https://arxiv.org/abs/2503.15693
tags:
- generalization
- training
- agent
- unseen
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares the zero-shot generalization capabilities of
  reinforcement learning (RL) and supervised learning (SL) in the Habitat visual navigation
  task. Using PPO as an RL representative and BC as an SL representative, the authors
  evaluate both algorithms on state-goal pair generalization within seen environments
  and generalization to unseen environments.
---

# Good Actions Succeed, Bad Actions Generalize: A Case Study on Why RL Generalizes Better

## Quick Facts
- arXiv ID: 2503.15693
- Source URL: https://arxiv.org/abs/2503.15693
- Authors: Meng Song
- Reference count: 5
- Key outcome: PPO outperforms BC in zero-shot generalization across seen/unseen state-goal pairs and unseen environments in Habitat visual navigation.

## Executive Summary
This paper investigates why reinforcement learning (RL) demonstrates superior zero-shot generalization compared to supervised learning (SL) in visual navigation tasks. Through systematic experiments in Habitat using PPO (RL) and BC (SL) on Gibson scenes, the authors demonstrate that RL consistently outperforms SL in both success rate and SPL across zero-shot settings. The key insight is that RL generalizes by combinatorial experience stitching—reusing fragments from diverse (often failed) training trajectories—while SL is limited to pattern matching successful demonstrations. This fundamental difference explains why RL can discover novel solutions beyond the training distribution while SL remains constrained to learned patterns.

## Method Summary
The study uses Habitat's point-goal navigation task where agents navigate from random start positions to relative target locations using egocentric RGB observations. The experimental setup includes 4 training scenes and 14 validation scenes from Gibson dataset. PPO uses a shared RNN encoder for actor and critic, while BC uses the same architecture without the critic. Both methods condition on full action history and relative goal vectors. The reward function includes a small distance-change bonus. Training uses 2000 state-goal pairs per scene, with evaluation on seen pairs, unseen pairs, and unseen scenes. Success rate and SPL are measured, with episodes terminating at 500 steps or 200 collisions.

## Key Results
- PPO achieves higher success rates than BC in both seen and unseen state-goal pair settings
- PPO demonstrates superior generalization to unseen scenes compared to BC
- While augmenting BC with optimal training data can match PPO's SPL, it falls significantly behind in success rate
- PPO's lower SPL relative to success rate indicates it finds feasible but not always shortest paths

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** TD-based RL generalizes via combinatorial experience stitching, recombining trajectory fragments from training (mostly failed ones) to solve unseen tasks.
- **Mechanism:** Temporal difference learning performs implicit dynamic programming, building a value function that captures reachable states across many trajectories. When facing a new (state, goal) pair, the agent stitches together previously visited sub-paths—even from failed episodes—to compose novel solutions.
- **Core assumption:** The state space has overlapping structure; fragments from different trajectories share connectable states.
- **Evidence anchors:**
  - [abstract] "TD-based RL-trained models generalize through combinatorial experience stitching—leveraging fragments of past trajectories (mostly failed ones) to construct solutions for new tasks."
  - [section 1] "RL solves unseen tasks by stitching together experiences collected during training, which are often suboptimal or failed trajectories for the original tasks."
  - [corpus] Related work on goal-conditioned RL (Horizon Generalization) supports that training on nearby goals enables longer-horizon composition, but does not directly validate the stitching mechanism here.
- **Break condition:** If environments lack overlapping reachable states across trajectories, or if the task requires solving completely disconnected regions, stitching fails.

### Mechanism 2
- **Claim:** Behavior cloning generalizes by extracting common patterns from successful demonstrations, limiting it to solution classes present in training data.
- **Mechanism:** BC maximizes likelihood of expert actions given observations. It learns statistical correlations between visual features and optimal actions. Generalization occurs by pattern matching to the nearest training distribution support.
- **Core assumption:** Unseen tasks share feature-level similarities with training demonstrations; optimal solutions follow learned patterns.
- **Evidence anchors:**
  - [abstract] "BC-trained models generalize by imitating successful trajectories."
  - [section 1] "SL extracts features from the training data and generates an adapted solution for the unseen task. Its generalization is limited to the specific class of solutions defined by the training distribution."
  - [corpus] No direct corpus validation; gradient coupling barriers in agentic RL suggest SL-style learning may face optimization interference, but this is speculative.
- **Break condition:** When unseen tasks require solutions outside the training pattern class (e.g., navigating around novel obstacles), BC fails to find feasible paths.

### Mechanism 3
- **Claim:** Learning from failed trajectories provides RL with exponentially more combinatorial solution candidates than learning solely from optimal demonstrations.
- **Mechanism:** Trial-and-error exploration generates diverse state visitation. Even when failing to reach goal g₀, reaching g₁ instead creates useful experience for future tasks requiring traversal through g₁. TD learning preserves all reachable state values.
- **Core assumption:** Failed trajectories contain useful sub-skills; exploration coverage is sufficient to reach diverse states.
- **Evidence anchors:**
  - [section 1, Figure 1] Shows agent failing to reach g₀ but reaching g₁—these trajectories become useful for composing skills.
  - [section 6.2] "PPO's SPL is lower than its success rate, indicating that the path it finds is not always the shortest. In contrast, BC and augmented BC have equal SPL and success rate."
  - [corpus] Weak direct evidence; no corpus papers explicitly validate failure-trajectory utility.
- **Break condition:** If exploration is too sparse, or if failures provide no transferable sub-skills (e.g., getting stuck in dead ends with no reusable fragments), this advantage disappears.

## Foundational Learning

- **Concept:** Temporal Difference (TD) Learning
  - **Why needed here:** The paper attributes RL's combinatorial generalization to TD learning's dynamic programming nature, which propagates value estimates backward through visited states.
  - **Quick check question:** Can you explain why TD learning enables "stitching" across trajectory fragments while supervised learning does not?

- **Concept:** Behavior Cloning as Imitation Learning
  - **Why needed here:** BC serves as the SL baseline; understanding its objective (maximum likelihood on expert actions) clarifies why it's limited to training-distribution solutions.
  - **Quick check question:** Why does BC achieve higher SPL (shortest-path accuracy) on seen pairs but lower success rate on unseen pairs compared to PPO?

- **Concept:** POMDP and Partial Observability
  - **Why needed here:** Habitat navigation uses egocentric visual observations, requiring recurrent policies that maintain belief states over history.
  - **Quick check question:** How does the recurrent architecture in this paper differ from a feedforward policy, and why is history necessary?

## Architecture Onboarding

- **Component map:** Visual Encoder (CNN) → RNN (GRU/LSTM) with goal concatenation → Actor head (policy π(a|h, g))
                                                    ↘ Critic head (value V(h, g)) [PPO only]

- **Critical path:**
  1. Egocentric RGB observation → CNN encoder → visual embedding
  2. Goal vector + visual embedding + previous action → RNN input
  3. RNN hidden state → actor (softmax over 4 actions: stop, forward, left, right)
  4. For PPO: RNN hidden state → critic (scalar value estimate)

- **Design tradeoffs:**
  - **Relative vs. absolute goals:** Relative goals make the task more instruction-following-like, easier to generalize. The paper notes a 40% success rate drop when switching to absolute goals in single-room tests.
  - **Shared vs. separate encoder:** PPO shares the RNN encoder between actor and critic; BC uses actor-only. Shared encoder may improve sample efficiency but risks interference.
  - **Training scenes vs. pairs:** More training scenes improve visual generalization but hurt geometric memorization (conflicting layouts across scenes cause confusion).

- **Failure signatures:**
  - BC: High SPL on seen pairs (>95%) but sharp degradation on unseen (s₀, g) pairs—indicates memorization without compositional understanding.
  - PPO: Lower SPL than success rate—finds feasible paths but not always shortest; indicates exploration favors coverage over optimality.
  - Both: Performance drops when training on more scenes due to geometric-visual trade-off.

- **First 3 experiments:**
  1. **Replicate the BC vs. PPO comparison on 4 training scenes:** Measure success rate and SPL on seen (s₀, g), unseen (s₀, g), and unseen scenes. Confirm that PPO leads in success rate while BC leads in seen-pair SPL.
  2. **Ablate the distance-change reward term in PPO:** The paper claims removing this causes "complete failure." Verify and document the degradation magnitude.
  3. **Test augmented BC with suboptimal (non-shortest-path) demonstrations:** The paper suggests BC needs noisy/suboptimal data to improve success rate. Evaluate whether adding 20% longer-path demonstrations closes the success-rate gap with PPO.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does training Supervised Learning (SL) on diverse, sub-optimal data (including failed trajectories) effectively close the success rate gap with RL?
- **Basis in paper:** [explicit] The authors propose that to improve generalization, "Supervised learning methods need to be trained on data of varying quality... to generate diverse solutions for new tasks."
- **Why unresolved:** The paper demonstrates that augmenting BC with *more optimal* data matches SPL but fails to close the success rate gap. The utility of *noisy* data for SL remains a theoretical suggestion in the conclusion, not an empirically tested result in the study.
- **What evidence would resolve it:** An experiment comparing PPO against a BC agent trained on a mixture of optimal and sub-optimal trajectories to see if the success rate generalization gap disappears.

### Open Question 2
- **Question:** Can scaling model size and training data volume resolve the observed trade-off between visual generalization and geometric generalization?
- **Basis in paper:** [explicit] The authors observe that training on more scenes improves visual generalization but degrades geometric generalization (unseen state-goal pairs). They state scaling "could potentially help reconcile the trade-off."
- **Why unresolved:** The paper only presents results on a fixed architecture with limited dataset sizes (1 to 4 scenes). It is unknown if the "confusion" introduced by conflicting layouts in different scenes persists or diminishes with larger model capacities.
- **What evidence would resolve it:** A scaling study varying model width/depth and dataset size (scenes and pairs) to analyze the correlation between visual and geometric performance metrics.

### Open Question 3
- **Question:** Does maximum entropy regularization fundamentally enhance the combinatorial generalization capabilities of RL agents?
- **Basis in paper:** [explicit] The authors suggest incorporating "a maximum entropy regularization term... to further improve its generalization ability," hypothesizing it preserves diverse solutions better than standard PPO.
- **Why unresolved:** The paper establishes that standard PPO benefits from combinatorial stitching, but it does not provide empirical evidence that explicitly maximizing entropy further improves this specific mechanism or final zero-shot performance.
- **What evidence would resolve it:** Comparative experiments between standard PPO and Maximum Entropy PPO (or SAC) on the same unseen state-goal pairs to measure differences in solution diversity and success rates.

## Limitations

- The core stitching mechanism remains theoretically underspecified without formal characterization
- Architectural details (visual encoder, RNN type/hidden size) are unspecified, complicating exact reproduction
- The generalization advantage is only demonstrated for point-goal navigation, not more complex geometric reasoning tasks

## Confidence

- **High Confidence:** The empirical observation that PPO outperforms BC on unseen (state, goal) pairs and unseen scenes. The success rate vs. SPL patterns are consistently measured and explained.
- **Medium Confidence:** The attribution of RL's advantage to combinatorial experience stitching. While intuitively compelling and consistent with observations, the mechanism lacks formal proof or deeper analysis.
- **Low Confidence:** The claim that failed trajectories specifically enable better generalization. The paper shows PPO uses diverse experiences but doesn't directly prove that failed trajectories contribute more than successful ones to generalization.

## Next Checks

1. **Ablation of trajectory quality:** Train PPO on datasets containing only successful trajectories versus only failed trajectories, then measure generalization performance to determine if failed trajectories provide unique benefits.
2. **Formal mechanism analysis:** Implement a synthetic environment where states can be explicitly partitioned into disjoint regions, then test whether PPO can still generalize when no overlapping states exist between training and test tasks.
3. **Cross-task transfer experiment:** Train PPO on one type of navigation task (e.g., point-goal) and test zero-shot transfer to a different task type (e.g., object-goal navigation) to assess whether the stitching mechanism generalizes across task families.