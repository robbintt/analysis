---
ver: rpa2
title: A Multimodal Fusion Model Leveraging MLP Mixer and Handcrafted Features-based
  Deep Learning Networks for Facial Palsy Detection
arxiv_id: '2503.10371'
source_url: https://arxiv.org/abs/2503.10371
tags:
- facial
- palsy
- images
- data
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of automated facial palsy detection
  to improve upon current labor-intensive and subjective clinical assessments. The
  authors propose a multimodal fusion-based deep learning approach that leverages
  both unstructured data (RGB images or images with facial line segments processed
  via an MLP Mixer) and structured data (facial landmark coordinates, expression features,
  or handcrafted features processed via a feed-forward neural network).
---

# A Multimodal Fusion Model Leveraging MLP Mixer and Handcrafted Features-based Deep Learning Networks for Facial Palsy Detection

## Quick Facts
- **arXiv ID:** 2503.10371
- **Source URL:** https://arxiv.org/abs/2503.10371
- **Reference count:** 32
- **Primary result:** Multimodal fusion achieved 96.00 F1 score, significantly outperforming single-modality approaches (82.80 F1 for handcrafted features alone, 89.00 F1 for MLP Mixer on RGB images).

## Executive Summary
This paper addresses the challenge of automated facial palsy detection to improve upon current labor-intensive and subjective clinical assessments. The authors propose a multimodal fusion-based deep learning approach that leverages both unstructured data (RGB images or images with facial line segments processed via an MLP Mixer) and structured data (facial landmark coordinates, expression features, or handcrafted features processed via a feed-forward neural network). The study evaluates diverse data modalities and model architectures using the YouTube Facial Palsy (YFP) dataset and CK+ emotion dataset, applying leave-one-patient-out cross-validation across 20 facial palsy patients and 20 healthy subjects. The multimodal fusion model achieved an F1 score of 96.00, significantly outperforming single-modality approaches. This demonstrates the advantage of integrating diverse data modalities to enhance detection accuracy and robustness.

## Method Summary
The approach processes facial palsy detection as a binary classification task using static video frames. Two data streams are employed: unstructured RGB images processed by a pretrained MLP Mixer (B-16) with frozen parameters except the final block, and structured 29-dimensional handcrafted asymmetry features derived from facial landmarks. A deep feed-forward neural network (15 hidden layers) processes the structured features. The multimodal fusion combines embeddings from both streams using early concatenation, followed by a fusion head of 4 fully connected layers. Training uses SGD with specific learning rates (0.01 for visual models, 0.2045 for FNN), batch sizes (256 for unimodal, 128 for fusion), and cross-validation via leave-one-patient-out (21 folds).

## Key Results
- Multimodal fusion model achieved 96.00 F1 score, outperforming handcrafted features alone (82.80 F1) and MLP Mixer on RGB images alone (89.00 F1).
- MLP Mixer architecture achieved 89.00 F1, outperforming ResNet50 (85.00 F1) due to its ability to capture long-range spatial dependencies relevant to facial asymmetry.
- Early fusion (feature concatenation) achieved 96.00 F1, while late fusion (probability averaging) achieved only 83.00 F1, demonstrating the importance of learning non-linear interactions between modalities.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The MLP Mixer architecture appears more effective than standard CNNs for this specific task because its patch-mixing mechanism captures global spatial dependencies relevant to facial asymmetry.
- **Mechanism:** Unlike CNNs that rely on local convolutional windows, the MLP Mixer splits images into patches and applies token-mixing layers. This allows the model to communicate information across distant facial regions (e.g., comparing left vs. right hemiface) directly, rather than building up context through deep layers.
- **Core assumption:** The conditional assumption is that facial palsy detection relies heavily on long-range spatial relationships (asymmetry between eyes and mouth) rather than just local texture.
- **Evidence anchors:**
  - [section 5]: The paper reports the MLP Mixer achieved an F1 of 89.00, outperforming ResNet50 (85.00), and suggests this is due to "spatial token mixing enabling the model to capture long-range dependencies."
  - [corpus]: The *KAN-Mixers* and *MRI-Based Brain Tumor Detection* papers in the corpus similarly utilize MLP-Mixer architectures for medical image analysis, suggesting the architecture has general utility in domains requiring spatial context.

### Mechanism 2
- **Claim:** Fusing explicit geometric features (handcrafted) with implicit image features (RGB embeddings) resolves the "precision-recall trade-off" observed in single modalities.
- **Mechanism:** The handcrafted features (based on landmark ratios) explicitly encode clinical knowledge about symmetry, providing high recall (88.30). The RGB embeddings capture subtle textures and lighting, providing high precision (96.60). Early fusion allows the classifier to use the structured geometry to "gate" or validate the visual features, reducing false positives without sacrificing detection rates.
- **Core assumption:** The handcrafted features are sufficiently robust to noise (e.g., head pose) to serve as a reliable anchor for the visual model.
- **Evidence anchors:**
  - [abstract]: The multimodal fusion model achieved 96.00 F1, "significantly higher" than handcrafted alone (82.80) or RGB alone (89.00).
  - [section 5]: Notes that handcrafted features achieved the highest recall (88.30), while MLP Mixer on RGB had the highest precision (96.60); the fusion model maximized both.

### Mechanism 3
- **Claim:** Early fusion (feature concatenation) outperforms late fusion (probability averaging) because it allows the model to learn non-linear interactions between visual and geometric data.
- **Mechanism:** Late fusion averages the predictions of independent models, assuming the errors are uncorrelated. Early fusion concatenates the feature vectors *before* the final classification head, allowing the network to learn complex rules (e.g., "Flag as palsy only if lip asymmetry > X *and* specific texture is present").
- **Core assumption:** The features from the two modalities exist in a compatible latent space where their interaction provides signal rather than just noise.
- **Evidence anchors:**
  - [section 5]: Table 1 shows Early Fusion achieved 96.00 F1, while Late Fusion dropped to 83.00 F1, indicating that simple averaging was insufficient.

## Foundational Learning

- **Concept:** **MLP Mixer Architecture (Token vs. Channel Mixing)**
  - **Why needed here:** You cannot debug or optimize the core visual encoder without understanding that it replaces convolutions with matrix transposes and MLPs applied to patches.
  - **Quick check question:** Does the model apply mixing along the *spatial* dimension (patches) or *channel* dimension first, and why does this remove the need for explicit attention mechanisms?

- **Concept:** **Handcrafted Asymmetry Features**
  - **Why needed here:** The "structured" branch relies entirely on these 29 specific metrics. Understanding their definition is required to diagnose failure cases (e.g., did the model fail because the landmark detection failed?).
  - **Quick check question:** How does the model calculate "asymmetry" from raw (x,y) landmark coordinates, and is this calculation invariant to head rotation?

- **Concept:** **Early vs. Late Fusion**
  - **Why needed here:** The paper claims a massive win for early fusion. To replicate this, you must know the difference between concatenating *features* vs. averaging *probabilities*.
  - **Quick check question:** In the early fusion architecture, at what point are the output vectors of the MLP Mixer and the Feed-Forward Network combinedâ€”before or after the final classification layer?

## Architecture Onboarding

- **Component map:** Video Frame (RGB) -> **Branch A (Unstructured):** Mediapipe Landmarks -> Crop/Augment -> **MLP Mixer B-16** (Pre-Trained, frozen except last block) -> **Embedding Vector**.
- **Component map:** Video Frame (RGB) -> **Branch B (Structured):** Mediapipe Landmarks (478 points) -> **Feature Engineering** (29 asymmetry metrics) -> **Feed-Forward Network** (15 layers) -> **Embedding Vector**.
- **Component map:** Concatenate(Branch A, Branch B) -> **Fusion Head** (4 FC layers) -> Sigmoid -> Probability.

- **Critical path:** The extraction of facial landmarks is the critical dependency. If the landmark model fails to detect a face, *both* the structured input (coordinates) and the unstructured input (if cropped based on landmarks) are compromised.

- **Design tradeoffs:**
  - **ResNet50 vs. MLP Mixer:** ResNet is safer and better studied; MLP Mixer is more parameter-efficient for global context but requires careful patch management.
  - **Frozen vs. Fine-tuned:** The authors freeze most of the pretrained backbones. This reduces training time and overfitting risk but limits the model's ability to adapt to the specific "look" of palsy textures in the YFP dataset.

- **Failure signatures:**
  - **High Precision / Low Recall:** Model is too conservative; likely relying too heavily on RGB textures and ignoring the geometric recall signals (adjust fusion layer weights or loss function).
  - **Random Guessing (F1 ~50):** Check landmark extraction; if the dataset contains profile views not handled by the landmark model, the handcrafted features will be garbage.

- **First 3 experiments:**
  1. **Baseline Validation:** Reproduce the single-modality results (Handcrafted FNN and MLP-Mixer RGB) to ensure the data pipeline is correctly sorting the "Leave-One-Patient-Out" splits.
  2. **Ablation Study:** Run the Early Fusion model but replace the MLP Mixer with the ResNet50 encoder to quantify exactly how much performance gain comes from the *fusion* vs. the *architecture*.
  3. **Feature Importance:** Retrain the fusion model with shuffled handcrafted features to prove the model is actually using the geometric data and not just learning from the RGB branch.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can video-based temporal approaches further enhance detection accuracy and robustness compared to the current static frame-based multimodal model?
- Basis in paper: [explicit] The authors state in the Future Work section that they will focus on "exploring video-based, temporal approaches to further enhance detection accuracy and robustness."
- Why unresolved: The current study processes data as static image frames (or sampled frames), potentially missing dynamic information regarding the progression or timing of facial movements.
- What evidence would resolve it: A comparative study evaluating the performance of architectures incorporating temporal dynamics (e.g., 3D CNNs or Recurrent Neural Networks) against the current static MLP-Mixer baseline on the same dataset.

### Open Question 2
- Question: How can the model's predictions be made explainable to support clinical decision-making and improve trust?
- Basis in paper: [explicit] The authors explicitly list as future work the need to "study how to make an AI output explainable to clinicians."
- Why unresolved: While the model achieves high accuracy (96.00 F1), the paper does not currently provide mechanisms for clinicians to understand *why* a specific classification was made.
- What evidence would resolve it: The integration and validation of interpretable machine learning techniques (e.g., attention maps or feature importance scores) that correlate with clinical markers of facial palsy.

### Open Question 3
- Question: Can the fusion techniques be refined to outperform the current early concatenation and late averaging methods?
- Basis in paper: [explicit] The authors note that future work will focus on "refining fusion techniques."
- Why unresolved: The current implementation relies on simple early concatenation of embeddings or late averaging of probabilities, which may not capture complex cross-modal interactions optimally.
- What evidence would resolve it: Implementation of advanced fusion strategies (e.g., cross-attention or gating mechanisms) demonstrating statistically significant performance improvements over the reported early fusion F1 score of 96.00.

## Limitations
- Small dataset (21 patients, 20 healthy subjects) may limit generalizability to broader clinical populations.
- Use of YouTube videos introduces potential biases in lighting, pose, and image quality not representative of clinical settings.
- Performance heavily depends on Mediapipe landmark detection accuracy, which may fail on extreme poses or occlusions.
- Lack of comparison to other transformer-based architectures (ViT, Swin) makes it unclear if MLP Mixer is optimal.

## Confidence
- **High Confidence:** The core finding that multimodal fusion improves performance over single modalities (F1 96.00 vs. 82.80-89.00) is well-supported by the experimental results.
- **Medium Confidence:** The specific superiority of the MLP Mixer architecture over ResNet50 is plausible but requires further validation on different datasets.
- **Low Confidence:** The generalizability of these findings to clinical practice remains uncertain due to the small sample size and use of YouTube data.

## Next Checks
1. **Cross-Dataset Validation:** Test the trained fusion model on a separate, clinically-acquired facial palsy dataset to assess real-world applicability.
2. **Ablation Architecture Study:** Replace the MLP Mixer with ResNet50 and ViT architectures in the fusion framework to isolate whether performance gains come from the architecture or fusion approach.
3. **Landmark Robustness Testing:** Systematically degrade landmark detection quality to quantify the impact of the handcrafted feature branch on overall model performance.