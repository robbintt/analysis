---
ver: rpa2
title: 'C-SHAP for time series: An approach to high-level temporal explanations'
arxiv_id: '2504.11159'
source_url: https://arxiv.org/abs/2504.11159
tags:
- time
- series
- shap
- concept
- c-shap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces C-SHAP for time series, a method that explains
  model predictions using high-level temporal concepts rather than low-level input
  features. While existing XAI methods for time series focus on local patterns, C-SHAP
  determines the contribution of higher-level patterns (e.g., trend, seasonality)
  to model outcomes.
---

# C-SHAP for time series: An approach to high-level temporal explanations

## Quick Facts
- arXiv ID: 2504.11159
- Source URL: https://arxiv.org/abs/2504.11159
- Reference count: 25
- Method provides concept-level explanations for time series models rather than point-based attributions

## Executive Summary
This paper introduces C-SHAP, a method that extends SHAP to provide high-level temporal concept explanations for time series models. Rather than explaining predictions using individual input features, C-SHAP attributes model outputs to interpretable patterns like trend, daily seasonality, and weekly seasonality. The approach treats concepts as features in SHAP computation and uses training sample masking to simulate concept absence. Applied to energy consumption forecasting, C-SHAP reveals that growth and weekly patterns most strongly influence predictions while providing both global and local explanations.

## Method Summary
C-SHAP adapts the SHAP framework by treating high-level temporal concepts (e.g., trend, seasonality) as features rather than individual time points. The method first decomposes time series using Prophet into additive components: growth, daily seasonality, weekly seasonality, and residuals. For each test sample, SHAP values are computed by masking excluded concepts through replacement with corresponding components from randomly sampled training data. The approach provides both global explanations (mean absolute SHAP per concept) and local explanations (additive contributions per sample). The method is validated on energy consumption forecasting using a GRU model trained on 168-hour windows to predict hour 169.

## Key Results
- C-SHAP successfully identifies growth and weekly seasonality as the most influential concepts in energy consumption forecasting
- Global explanations show concept-level impacts while local explanations reveal additive contributions per sample
- The masking approach using training samples effectively approximates background distributions for concept absence
- Mean absolute SHAP values provide interpretable measures of concept importance across the dataset

## Why This Works (Mechanism)

### Mechanism 1: Concept-Level Shapley Attribution
- **Claim**: Treating high-level temporal concepts as SHAP features produces explanations that align better with human understanding of time series patterns
- **Mechanism**: Redefines the feature set in Shapley value computation from raw data points to concepts C = {c₁, c₂, ..., cₘ}. Each concept represents an interpretable temporal pattern. SHAP formula computes weighted marginal contributions across all coalitions of concepts
- **Core assumption**: Concepts are semantically meaningful and can be independently "toggled off" without corrupting remaining signal structure
- **Evidence anchors**: Abstract states C-SHAP provides attributions for concepts rather than low-level patterns; Section 2 defines concepts as higher-level abstraction than input features
- **Break condition**: If concepts are highly correlated or overlapping, marginal contributions become unstable and attribution scores lose interpretability

### Mechanism 2: Additive Decomposition for Independent Concept Manipulation
- **Claim**: Decomposing time series into additive components enables independent masking of concepts, ensuring SHAP values are order-invariant
- **Mechanism**: Decompose y(t) = y_growth(t) + Σ y_season(t) + y_other(t). Because components add linearly, replacing yi(t) with sampled yj_i(t) doesn't affect other components' values
- **Core assumption**: Decomposition captures all meaningful patterns; residual "Other" component is small or interpretable
- **Evidence anchors**: Section 3.1 explains additive advantage for independent adjustment; Section 4.2/Figure 2 shows "Other" concept has non-negligible mean absolute SHAP
- **Break condition**: If decomposition produces correlated components or leaves substantial unexplained structure in "Other," explanations become dominated by residual variance

### Mechanism 3: Training Sample Masking for Background Distribution Approximation
- **Claim**: Replacing masked concepts with corresponding components from randomly sampled training data approximates expected model output when concept is absent
- **Mechanism**: For N training samples, precompute decompositions. When concept ci is excluded, replace yi(t) with yj_i(t) from sample j. Average model outputs to simulate "concept absence"
- **Core assumption**: Training data spans concept space sufficiently to provide representative background values
- **Evidence anchors**: Section 3.2/Algorithm 1 provides explicit pseudocode for component masking via training sample substitution; Section 4.1.3 uses N=100 training samples without replacement
- **Break condition**: If training data lacks diversity or contains outliers, masking produces unrealistic inputs and SHAP estimates become unreliable or high-variance

## Foundational Learning

- **Shapley Values and SHAP**:
  - Why needed: Entire method builds on SHAP's coalition-based marginal contribution framework
  - Quick check question: For features A, B, C, why does Shapley require averaging A's marginal contribution over all subsets {∅, {B}, {C}, {B,C}} rather than just computing f(A) − f(∅)?

- **Time Series Decomposition (Additive vs. Multiplicative)**:
  - Why needed: Prophet uses additive decomposition; understanding why additive masking works is critical for extending to other domains
  - Quick check question: If your data followed y(t) = trend × seasonality × noise, why would replacing the seasonality component by subtraction produce invalid inputs?

- **Concept-Based XAI (TCAV, ConceptSHAP)**:
  - Why needed: Paper positions C-SHAP against TCAV/ConceptSHAP; understanding their limitations clarifies design tradeoffs
  - Quick check question: Why might TCAV's requirement to train classifiers on model activations be problematic for recurrent time series models with high-dimensional hidden states?

## Architecture Onboarding

- **Component map**: Time series y(t) -> Prophet decomposition -> component vectors {y_growth, y_daily, y_weekly, y_other} -> Preprocess training samples -> Mask excluded components with sampled training components -> Model prediction -> SHAP calculation -> Explanation rendering

- **Critical path**: Preprocess: Decompose all training samples (one-time cost, O(|train| × Prophet_cost)). Per test sample: Decompose -> enumerate coalitions -> for each coalition: mask (O(N)) -> model predict -> aggregate. Total per sample: O(2^m × N × model_forward)

- **Design tradeoffs**:
  - Fewer concepts (m↓): Faster SHAP computation but risk incomplete coverage -> high "Other" SHAP
  - More masking samples (N↑): Better background approximation but linear cost increase
  - Prophet vs. custom decomposition: Prophet handles fixed-period seasonality; fails on variable-period sensor patterns

- **Failure signatures**:
  1. "Other" SHAP dominates: Decomposition incomplete -> add concepts (outliers, variance shifts, multi-scale seasonality)
  2. SHAP correlates strongly with last observed value: For forecasting tasks, may reflect autoregressive data structure rather than concept influence
  3. SHAP values unstable across runs: Masking sample variance too high -> increase N or check training data quality

- **First 3 experiments**:
  1. Decomposition sanity check: Plot original vs. reconstructed signal (sum of components); if reconstruction error is large, decomposition is inadequate before any SHAP computation
  2. Masking visualization: For highest-SHAP concept in a sample, plot masked signal (concept removed) vs. original; confirm visual alignment with semantic intuition
  3. Convergence test on N: Run C-SHAP with N=10, 25, 50, 100, 200 on held-out subset; if SHAP values don't stabilize by N=100, training data may lack coverage or contain outliers

## Open Questions the Paper Calls Out

- **How can concept construction be expanded to capture patterns with fluctuating periodicity (e.g., sensor data), beyond the fixed-period seasonality captured by Prophet decomposition?**
  - Basis: "Future work will aim to expand the concept construction to sensor data... Specifically in sensor data, for repeating patterns, there are often fluctuations in period over time. Such repetitions cannot be captured by Prophet."
  - Why unresolved: Prophet decomposition only handles fixed-periodicity patterns, limiting applicability to domains with irregular or drifting periodic patterns
  - What evidence would resolve it: Development and validation of concept construction methods (e.g., filters capturing frequency variability) on sensor datasets with known irregular periodicities

- **Can a hybrid approach combining concept-based and point-based explanations effectively reveal both which concepts and which temporal regions influence model predictions?**
  - Basis: "In future research, a hybrid approach could be explored, in which the influence of concepts over time is determined."
  - Why unresolved: Current C-SHAP provides concept-level attributions but no temporal localization; integrating both granularities requires methodological development
  - What evidence would resolve it: A unified framework producing temporally-localized concept attributions, validated against synthetic data with known concept-region interactions

- **How does C-SHAP generalize to multivariate time series, and what challenges arise in defining and masking cross-variable concepts?**
  - Basis: "The C-SHAP for definition can be expanded to multivariate series."
  - Why unresolved: Paper only demonstrates univariate cases; multivariate settings introduce inter-variable dependencies that complicate concept definition and masking strategies
  - What evidence would resolve it: Application of multivariate C-SHAP to benchmark datasets with multiple correlated channels, comparing explanation fidelity against univariate baselines

## Limitations

- **Single Decomposition Method**: Only Prophet-based additive decomposition is tested; generalizability to non-additive patterns (multiplicative seasonality, irregular events) remains unverified
- **Static Concept Set**: Concepts are fixed per dataset; method lacks automatic concept discovery for novel domains
- **Forecasts Only**: Applied exclusively to one-step-ahead energy forecasting; unclear how well it scales to multi-step or classification tasks

## Confidence

- **High**: Concept-level Shapley attribution framework and mathematical formulation
- **Medium**: Prophet decomposition masking approach and experimental results on energy data
- **Low**: Claims about general applicability across all time series domains and decomposition methods

## Next Checks

1. **Concept Completeness Test**: Run on synthetic data with known concept patterns; verify all concepts are detected and "Other" remains minimal
2. **Decomposition Robustness**: Compare C-SHAP results using Prophet vs. alternative decompositions (STL, wavelet) on same dataset
3. **Model Architecture Transfer**: Apply method to non-GRU models (transformer, ARIMA) on same energy dataset; check SHAP stability