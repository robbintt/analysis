---
ver: rpa2
title: Benign Overfitting in Linear Classifiers with a Bias Term
arxiv_id: '2511.12840'
source_url: https://arxiv.org/abs/2511.12840
tags:
- theorem
- conditions
- term
- inhomogeneous
- hashimoto
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the analysis of benign overfitting from Hashimoto
  et al. (2025) to linear classifiers with a bias term, addressing a critical gap
  since practical classifiers typically include intercepts.
---

# Benign Overfitting in Linear Classifiers with a Bias Term

## Quick Facts
- arXiv ID: 2511.12840
- Source URL: https://arxiv.org/abs/2511.12840
- Reference count: 40
- Primary result: Extends benign overfitting analysis to linear classifiers with bias term, proving benign overfitting persists while requiring stronger dimensionality conditions due to bias-induced perturbations

## Executive Summary
This paper extends the analysis of benign overfitting from Hashimoto et al. (2025) to linear classifiers with a bias term, addressing a critical gap since practical classifiers typically include intercepts. The key contribution is proving that benign overfitting persists in the inhomogeneous case while identifying new constraints on data dimensionality introduced by the bias term. The core technical insight involves analyzing how the bias term perturbs the normalized Gram matrix, requiring stronger conditions on the trace of the covariance matrix (tr(Σ)) relative to the sample size.

## Method Summary
The paper analyzes maximum margin linear classifiers with bias terms under non-sub-gaussian mixture distributions (Model EM). The key technical advance involves extending the homogeneous analysis to inhomogeneous classifiers by analyzing how the bias term perturbs the normalized Gram matrix. This requires bounding the perturbation terms that arise when adding a constant feature to all samples, and verifying that concentration events still hold under the perturbed matrix. The analysis yields explicit dimensionality requirements scaling as tr(Σ) ≿ n^{3/2+2/k+1/l} in the noiseless case and tr(Σ) ≿ (1/η) n^{3/2+2/k+1/l} in the noisy case, where k bounds negative moments of noise scaling and η is the noise level.

## Key Results
- Benign overfitting persists in linear classifiers with bias terms under the same distributional assumptions as homogeneous case
- The bias term introduces a shared component across all samples, increasing correlation and moving the Gram matrix away from identity
- Dimensionality requirements are substantially stronger in the noisy case, scaling with 1/η factor
- In the isotropic case, new inhomogeneous requirements are dominated by existing homogeneous requirements

## Why This Works (Mechanism)

### Mechanism 1: Gram Matrix Perturbation via Shared Bias Component
- Claim: The bias term introduces a shared component across all training samples, systematically increasing inter-sample correlation and perturbing the normalized Gram matrix away from the identity structure.
- Mechanism: Appending a constant feature '1' to each noise vector z_i creates tilde{z}_i = (z_i, 1). This shared component adds a positive perturbation P_ij to off-diagonal entries of the normalized Gram matrix, where |P_ij| ≤ εA_ij + B_ij. The perturbation is magnified when noise norms ||z_i|| are small, as terms involve 1/||z_i||^2.
- Core assumption: Model (EM) assumptions hold—specifically, the noise scaling g has bounded negative moments E[g^{-k}] < ∞ for k ∈ (2, 4], and noise directions ξ have bounded r-th moments.
- Evidence anchors:
  - [abstract] "The analysis reveals that the bias term introduces a shared component across all samples, increasing correlation and moving the Gram matrix away from the identity"
  - [section 4.1] "The inhomogeneous model appends a constant feature ('1') to all noise vectors z_i. This adds a shared component to the noise, which systematically increases the correlation between different samples (the perturbation P_ij analyzed in Appendix A.1.1)"
  - [corpus] Related work on benign overfitting universality (Hashimoto et al., 2501.10538) establishes baseline conditions for homogeneous case; this paper extends with perturbation analysis
- Break condition: If tr(Σ) is insufficient relative to n, perturbations become O(1) rather than o(1), destroying near-orthogonality and causing the analysis to fail.

### Mechanism 2: Dimensionality as a "Dilution" Buffer
- Claim: Higher trace of covariance tr(Σ) relative to sample size n "dilutes" the perturbation effect of the bias term, restoring near-orthogonality conditions required for generalization.
- Mechanism: The perturbation bound in Lemma 5 scales as T_bound ≈ (n/δ)^{2/k} / tr(Σ). By requiring tr(Σ) ≿ n^{3/2} · (n/δ)^{2/k+1/l}, the perturbation term becomes sufficiently small relative to the identity structure. In noisy settings, the tolerance for perturbation shrinks with noise level η, yielding tr(Σ) ≿ (1/η) n^{3/2+2/k+1/l}.
- Core assumption: The concentration events E_1 through E_5 from the homogeneous case hold with high probability; the perturbation analysis accurately bounds how tilde{E}_i deviates from E_i.
- Evidence anchors:
  - [abstract] "necessitating higher dimensionality to maintain the near-orthogonality crucial for generalization"
  - [section 3.1, Theorem 1] "T_Inhom = (n/δ)^{2/k+1/l} n^{3/2}" with requirement "tr(Σ) ≥ C · max{T_Hom, T_Inhom}"
  - [section 4.1, Table 1] Explicit comparison showing inhomogeneous requirements dominate in noisy case with 1/η scaling
  - [corpus] Corpus evidence is weak for this specific mechanism—no direct comparison papers analyze bias term effects on dimensionality requirements
- Break condition: If noise level η approaches 1/2, the 1/η scaling makes dimensionality requirements impractically large; benign overfitting guarantees break down.

### Mechanism 3: Parameter k Controls Perturbation Sensitivity
- Claim: The parameter k, which bounds the negative moments E[g^{-k}] of the noise scaling factor, critically determines how strongly the bias term perturbs concentration events—smaller k amplifies perturbation effects.
- Mechanism: Perturbation terms involve max_k 1/||z_k||^2. When k is small (close to 2), the probability of small norms is higher (heavy lower tail on g), making max_k g_k^{-2} larger. This amplifies both the Gram matrix perturbation (Lemma 5) and the inverse norm perturbation β' (Lemma 8). The inhomogeneous requirements scale as (n/δ)^{2/k}, which diverges as k → 2.
- Core assumption: Assumption: The negative moment bound k reflects true data distribution properties—real noise doesn't have lighter lower tails than assumed.
- Evidence anchors:
  - [section 4.1] "If k is small (close to 2), the probability of small norms is higher, necessitating stronger conditions on tr(Σ) to counteract the resulting perturbations"
  - [section A.1.1, Lemma 5] Bound involves (n·E[g^{-k}]/δ)^{2/k}, showing explicit k-dependence
  - [section A.3.1] Isotropic case analysis shows exponent E_3 = 3/2 + 2/k + 1/l, demonstrating k's role in dimensionality scaling
  - [corpus] No corpus papers directly address k-dependence in bias term analysis
- Break condition: If the true distribution has k < 2 (heavier lower tails than assumed), the moment bounds fail and perturbation analysis does not apply.

## Foundational Learning

- Concept: **Maximum Margin Classifier and Implicit Bias of Gradient Descent**
  - Why needed here: The entire analysis concerns the generalization of the maximum margin classifier, which is what gradient descent converges to on separable data (Soudry et al., 2018). Understanding this connection explains why the paper studies this specific classifier.
  - Quick check question: Why does gradient descent on logistic loss converge to the maximum margin solution rather than just any interpolating classifier?

- Concept: **Near-Orthogonality in High Dimensions**
  - Why needed here: The core technical mechanism depends on the normalized Gram matrix ŽŽ^⊤ being close to the identity matrix. This near-orthogonality is what allows benign overfitting—the noise vectors are almost orthogonal, so memorizing them doesn't corrupt the signal direction.
  - Quick check question: If n random vectors in R^p have normalized Gram matrix close to I_n, what does this imply about their pairwise angles?

- Concept: **Homogeneous vs. Inhomogeneous Linear Classifiers**
  - Why needed here: The paper's central contribution is extending homogeneous (no bias: sgn(⟨w,x⟩)) analysis to inhomogeneous (with bias: sgn(⟨w,x⟩ + b)) classifiers. The homogeneous case assumes decision boundary passes through origin; adding bias allows arbitrary offset, which is standard in practice.
  - Quick check question: What geometric constraint does a homogeneous linear classifier impose that an inhomogeneous one does not?

## Architecture Onboarding

- Component map:
  - Extended feature space: Original features x ∈ R^p mapped to tilde{x} = (x, 1) ∈ R^{p+1}
  - Extended noise vectors: z → tilde{z} = (z, 1), adding constant dimension
  - Extended signal: μ → tilde{μ} = (μ, 0), bias doesn't change true signal direction
  - Normalized Gram matrix: ŽŽ^⊤ → Ž̃Ž̃^⊤, with perturbation P analyzed in Lemma 5
  - Concentration events: E_1 through E_5 → tilde{E}_1 through tilde{E}_5 with modified parameters (ε̃, β̃, γ̃, etc.)

- Critical path:
  1. Verify Model (EM) assumptions on your data distribution (estimate k from negative moments of noise scaling, r from noise direction tails)
  2. Check dimensionality condition: tr(Σ) ≿ max{T_Hom, T_Inhom} where T_Inhom = (n/δ)^{2/k+1/l} n^{3/2} (scaled by 1/η if noisy)
  3. Verify signal strength: ||μ||^2 ≥ C δ^{-1/2} ||Σ^{1/2}μ|| for noiseless; ||μ||^2 ≥ C max{1/η, 1/(1-2η)} δ^{-1/2} ||Σ^{1/2}μ|| for noisy
  4. In isotropic case (Σ = I_p), new inhomogeneous requirements are dominated by homogeneous ones—simpler verification

- Design tradeoffs:
  - **Isotropic vs. anisotropic covariance**: Isotropic (Σ = I_p) case is more forgiving—inhomogeneous requirements are dominated by homogeneous ones. Anisotropic case may require explicit verification of T_Inhom.
  - **Noise level η**: Higher noise dramatically strengthens dimensionality requirements (1/η scaling). Trade-off between collecting more samples (increases n) vs. ensuring cleaner labels.
  - **Sample size n**: Larger n helps with concentration but also increases dimensionality requirements (polynomial in n). Assumption: There exists a "sweet spot" regime where both conditions can be satisfied.

- Failure signatures:
  - **Perturbation too large**: If tr(Σ) is insufficient, ε̃ exceeds threshold (1/4 in noiseless, min{η, 1-2η}/8 in noisy)—near-orthogonality fails, test error bound does not hold
  - **β̃ ≥ 1/2**: Inverse norm concentration fails, maximum margin classifier analysis breaks down
  - **Signal too weak**: ||μ|| below threshold means classifier cannot reliably separate classes even with benign overfitting
  - **Isotropic assumption violated**: If Σ is far from identity, the "domination" result (Corollary 4) does not apply and T_Inhom may become the binding constraint

- First 3 experiments:
  1. **Validate isotropic domination**: Generate synthetic data with Σ = I_p under Model (EM), vary p/n ratio, compare test error of homogeneous vs. inhomogeneous classifiers—verify that both achieve benign overfitting under same p requirements (Corollary 4 prediction)
  2. **Stress-test dimensionality bounds**: Generate anisotropic Σ (e.g., spiked covariance), systematically reduce tr(Σ) while holding n fixed—identify the threshold where test error degrades, compare to theoretical T_Inhom bound
  3. **Noise sensitivity analysis**: Fix p, n, Σ; vary label noise η from 0.01 to 0.4—plot test error gap (inhomogeneous - homogeneous) vs. η; verify 1/η scaling of required tr(Σ) in noisy regime

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis relies on specific moment conditions for the noise distribution (Model EM), which may not capture all realistic data distributions
- The 1/η scaling in the noisy regime makes benign overfitting practically difficult for high noise levels (η > 0.4)
- The perturbation analysis assumes the extended noise vectors' Gram matrix can be bounded relative to the homogeneous case, but this may not hold for highly anisotropic covariance structures

## Confidence
- **High Confidence**: The mechanism by which the bias term perturbs the normalized Gram matrix away from identity (Mechanism 1) is well-supported by the perturbation analysis in Lemma 5 and the explicit construction of the extended feature space.
- **Medium Confidence**: The dimensionality scaling requirements (Mechanism 2) are mathematically rigorous but may be overly conservative in practice.
- **Low Confidence**: The dependence on parameter k (Mechanism 3) is derived from the perturbation analysis, but the sensitivity to k approaching 2 may be overstated.

## Next Checks
1. **Distribution Sensitivity Analysis**: Systematically test the robustness of benign overfitting guarantees across different noise distributions with varying k values (2.1, 2.5, 3, 4) to identify the practical impact of the k-dependence and potential failure modes when assumptions are violated.

2. **Anisotropic Covariance Stress Test**: Generate synthetic data with spiked or otherwise non-isotropic covariance structures to empirically verify whether the required dimensionality tr(Σ) ≿ n^{3/2+2/k+1/l} is necessary, or if the isotropic domination result (Corollary 4) overstates the impact in practice.

3. **High-Noise Regime Validation**: For η ∈ [0.3, 0.45], empirically measure the gap between theoretical bounds and actual test error, and assess whether the 1/η scaling accurately predicts the breakdown of benign overfitting guarantees as noise approaches 0.5.