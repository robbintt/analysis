---
ver: rpa2
title: Do Chatbot LLMs Talk Too Much? The YapBench Benchmark
arxiv_id: '2601.00624'
source_url: https://arxiv.org/abs/2601.00624
tags:
- openai
- category
- prompt
- minimal
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The YapBench benchmark quantifies unnecessary verbosity in large
  language models by measuring excess output length beyond a minimal sufficient baseline
  on brevity-ideal prompts. The benchmark evaluates 76 assistant models across three
  categories: minimal/ambiguous inputs, short factual Q&A, and one-line coding tasks.'
---

# Do Chatbot LLMs Talk Too Much? The YapBench Benchmark

## Quick Facts
- arXiv ID: 2601.00624
- Source URL: https://arxiv.org/abs/2601.00624
- Reference count: 40
- Key outcome: YapBench benchmark quantifies unnecessary verbosity in LLMs, with models showing order-of-magnitude spread in YapIndex (22.7 to 1427), revealing distinct failure modes across three prompt categories.

## Executive Summary
This paper introduces YapBench, a benchmark for measuring unnecessary verbosity (over-generation) in large language models. The benchmark evaluates 76 assistant models across 304 prompts spanning three categories: ambiguous/minimal inputs, short factual Q&A, and one-line coding tasks. Using the YapIndex metric, which aggregates median excess characters beyond minimal sufficient baselines, the study reveals that newer models are not necessarily more verbose, and that verbosity appears to be a controllable policy shaped by post-training rather than an inherent capability consequence. The work also introduces YapTax, a cost-oriented metric estimating the marginal overhead of over-generation in token-priced terms.

## Method Summary
YapBench measures LLM verbosity by comparing response lengths to curated minimal-sufficient baselines on brevity-ideal prompts. The benchmark uses 304 English prompts across three categories (A: ambiguous/minimal, B: factual Q&A, C: coding), with YapScore computed as excess characters beyond baseline. YapIndex aggregates category medians, while YapTax estimates cost overhead. Models are evaluated via OpenRouter API with temperature=0 where available, no system prompt, and markdown stripped from responses. The approach enables tokenizer-agnostic comparison across providers.

## Key Results
- Models show an order-of-magnitude spread in YapIndex, from gpt-3.5-turbo (22.7) to glm-4.5 (1427)
- Brevity-optimal behavior is not aligned with model recency or frontier capability
- Category-level analysis reveals distinct failure modes: vacuum-filling on ambiguous inputs and explanation overhead on technical requests
- YapTax estimates marginal token-priced overhead ranging from $1.14 to $2.71 per 1,000 prompts across models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preference-based post-training systematically favors longer outputs, inducing verbosity even when brevity is optimal.
- Mechanism: RLHF and DPO reward models learn to associate response length with helpfulness; longer outputs receive higher scores at comparable correctness, creating a training signal that rewards over-generation.
- Core assumption: The reward model or human annotators during preference collection conflate length with quality.
- Evidence anchors:
  - [abstract]: "Prior work suggests that preference-based post-training and LLM-judged evaluations can induce systematic length bias, where longer answers are rewarded even at comparable quality."
  - [Section 2]: Cites Shen et al. (2023) showing reward models learn shortcuts where longer responses receive systematically higher scores; Saito et al. (2023) and Dubois et al. (2024) find GPT-4 as proxy judge favors longer answers.
  - [corpus]: Limited direct corpus support; neighboring papers focus on chatbot trustworthiness and applications rather than verbosity mechanisms.
- Break condition: If reward models are explicitly length-normalized or trained with brevity-weighted preferences, the mechanism should attenuate.

### Mechanism 2
- Claim: Models generate verbose responses as a compensation strategy under uncertainty or underspecification.
- Mechanism: When input is ambiguous (Category A) or the model is uncertain, it produces additional hedging, caveats, or exploratory content rather than issuing a minimal clarification or admitting insufficient context.
- Core assumption: Verbosity correlates with internal uncertainty signals (e.g., perplexity).
- Evidence anchors:
  - [Section 2]: Zhang et al. (2025) show verbosity correlates with higher perplexity and uncertainty metrics; verbose answers are often less accurate.
  - [Section 5.3]: "Category A can be a major source of excess length... consistent with a tendency to 'fill the vacuum' with unsolicited content rather than issuing a minimal clarification."
  - [corpus]: No direct corpus evidence on uncertainty-verbosity linkage; weak external validation.
- Break condition: If models are calibrated to express uncertainty explicitly (e.g., "I don't know") or trained to request clarification on ambiguous inputs, this mechanism should diminish.

### Mechanism 3
- Claim: Verbosity is a controllable output policy influenced by assistant-style training priors, not an inherent consequence of model scale or capability.
- Mechanism: Post-training shapes response style independently of base model capability; newer/capable models can be more verbose because their fine-tuning encourages thoroughness or proactivity.
- Core assumption: Response length is orthogonal to task performance on brevity-ideal prompts.
- Evidence anchors:
  - [Section 6]: "Brevity-optimal behavior is not aligned with model recency or perceived frontier capability. GPT-3.5-turbo achieves the best overall YapIndex (23) in our evaluation... outperforming a range of newer frontier assistant models."
  - [Section 6]: "Verbosity is not monotonic in model generation or perceived frontier capability... appears to be a behavioral property that is influenced by post-training and assistant-style response priors."
  - [corpus]: No corpus papers address the recency-verbosity relationship directly.
- Break condition: If models are explicitly fine-tuned with brevity constraints or inference-time length penalties, verbosity should decrease without harming capability.

## Foundational Learning

- Concept: Minimal sufficient baseline
  - Why needed here: YapScore is defined relative to a per-prompt baseline representing the shortest response that remains correct and clear; understanding this anchor is essential to interpreting excess length.
  - Quick check question: For the prompt "What is the freezing point of water in Celsius?", what is a minimal sufficient baseline?

- Concept: Length bias in preference learning
  - Why needed here: The paper attributes verbosity to systematic reward for longer outputs during RLHF/DPO; grasping this helps diagnose why newer models may yap more.
  - Quick check question: Why might a reward model trained on human preferences assign higher scores to longer responses even when quality is equal?

- Concept: Tokenizer-agnostic evaluation
  - Why needed here: YapScore uses characters rather than tokens to enable fair comparison across models with different tokenizers; this design choice affects metric interpretation.
  - Quick check question: Why does measuring excess length in characters rather than tokens make YapBench comparable across providers?

## Architecture Onboarding

- Component map: YapBench consists of (1) 304 prompts across 3 categories (A: ambiguous/minimal, B: factual Q&A, C: one-line code), (2) curated minimal-sufficient baselines per prompt, (3) YapScore (per-prompt excess characters), (4) YapIndex (category-median aggregate), and (5) YapTax (cost in USD from excess tokens).

- Critical path: Load prompts → query model via API (single-turn, no system prompt, temperature=0 where supported) → strip markdown from response → compute YapScore_i = max(0, L_i - B_i) → aggregate medians per category → compute weighted YapIndex.

- Design tradeoffs: Character-based YapScore is tokenizer-agnostic but ignores semantic redundancy; median aggregation reduces outlier sensitivity but may underrepresent worst-case verbosity bursts; baselines are curated (subjective) rather than crowdsourced.

- Failure signatures: High Cat A scores indicate vacuum-filling on ambiguous inputs; high Cat C scores indicate explanation/formatting overhead around one-line code; high variance across categories suggests inconsistent policies rather than global verbosity.

- First 3 experiments:
  1. Run baseline evaluation on 3–5 target models (e.g., GPT-3.5-turbo, GPT-4, Claude, Gemini) to establish category-level YapScores and identify which category drives excess.
  2. Apply inference-time length penalty or explicit "be concise" system prompt and measure YapIndex reduction; check whether correctness on Cat B/C degrades.
  3. Sample 20 high-YapScore responses per category for manual review to classify failure modes (hedging, restatement, formatting, unsolicited context) and inform targeted mitigation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does optimizing for a low YapIndex lead to "pathological brevity," where models fail to provide necessary detail in complex, non-brevity-ideal contexts?
- Basis in paper: [explicit] The authors identify the need for a "Legitimate verbosity (control)" prompt set in future work to ensure the benchmark does not penalize helpful elaboration.
- Why unresolved: The current benchmark evaluates only brevity-ideal prompts; the inverse relationship between YapScore and performance on tasks requiring detail has not been measured.
- What evidence would resolve it: A correlation analysis between YapIndex scores and performance on a new category of prompts explicitly requiring step-by-step explanations or drafting.

### Open Question 2
- Question: Which specific components of modern post-training pipelines (e.g., DPO, RLHF reward functions) are responsible for the observed trend of increasing verbosity in newer model generations?
- Basis in paper: [inferred] The paper observes that GPT-3.5-turbo (2023) outperforms newer models, concluding verbosity is not tied to scale but likely to "post-training and assistant-style response priors."
- Why unresolved: The study quantifies the *what* (verbosity levels across models) but does not ablate the *why* (specific training data or reward mechanisms driving the behavior).
- What evidence would resolve it: Controlled training experiments where identical base models are aligned with different reward model configurations to observe the impact on YapIndex.

### Open Question 3
- Question: Does high performance on YapBench (low YapScore) correlate with a tendency to briefly agree with false premises rather than providing necessary corrections?
- Basis in paper: [explicit] The authors propose adding "Misconception traps" (prompts inviting agreement with falsehoods) to test if brevity-optimized models fail to correct users.
- Why unresolved: It is currently unclear if the failure mode of "vacuum-filling" in ambiguous inputs is distinct from or related to sycophantic agreement in misleading inputs.
- What evidence would resolve it: Evaluating top-ranking YapBench models on a dataset of false-premise prompts to measure the rate of brief agreement versus corrective refutation.

### Open Question 4
- Question: Is the "explanation overhead" failure mode observed in technical tasks (Category C) consistent in safety contexts, leading to verbose refusals?
- Basis in paper: [explicit] The authors list "Safety / Refusal" as a future category to test if models can output "short, firm refusals" rather than "pages of boilerplate."
- Why unresolved: The paper demonstrates that models add unnecessary text to code and factual answers, but it has not verified if this same mechanism drives over-explanation in safety refusals.
- What evidence would resolve it: Extending YapBench to harmful prompts and comparing the YapScores (excess characters beyond a minimal refusal) against the existing Category C scores.

## Limitations

- The baseline curation process is inherently subjective, with no detailed inter-annotator agreement or validation procedures reported
- The character-based YapScore does not account for semantic redundancy, treating repetitive content the same as equally valuable elaboration
- The benchmark's single-turn, no system prompt protocol may not reflect real-world usage where conversational context could modulate verbosity
- The correlation between YapScore and user dissatisfaction or cost burden is intuitive but not directly validated through user studies

## Confidence

**High Confidence**: The observation that models exhibit substantial variation in verbosity (YapIndex spread of ~60×) is robustly demonstrated across 76 models and three prompt categories. The methodology for measuring excess characters relative to baselines is clearly specified and reproducible.

**Medium Confidence**: The claim that verbosity is a controllable policy rather than an inherent capability consequence is supported by the GPT-3.5-turbo result outperforming newer models, but this could reflect other factors like base model architecture differences or dataset shifts in preference learning rather than post-training alone. The attribution to preference-based post-training inducing length bias is mechanistically plausible given cited work but not directly proven within this paper.

**Low Confidence**: The assertion that Category A prompts are "major sources of excess length" due to vacuum-filling is based on qualitative interpretation of high YapScores rather than systematic analysis of response content. The YapTax cost estimates assume current token pricing remains stable and that excess tokens would be eliminated rather than repurposed.

## Next Checks

1. **Baseline Validation Study**: Conduct a crowdsourcing experiment where 10+ independent annotators rate sufficiency for a random sample of 50 prompts (10 per category). Compute inter-annotator agreement (e.g., Fleiss' kappa) and assess whether YapIndex rankings are stable under alternative baseline selections.

2. **Content Analysis of High-YapScore Responses**: For the top 25% YapScore responses in each category (N≈50), perform systematic content coding to classify verbosity types: hedging, restatement, unsolicited context, formatting overhead, etc. Test whether Category A responses indeed contain more vacuum-filling content versus Category C responses showing more explanation overhead.

3. **Brevity Intervention Experiment**: Select 5 high-YapIndex models and evaluate them under three conditions: (a) baseline (no system prompt), (b) explicit brevity instruction ("respond as concisely as possible while remaining accurate"), (c) length-penalized decoding (e.g., -1 token penalty per generation step). Measure YapIndex reduction and assess correctness degradation on Category B/C prompts to determine if verbosity can be controlled without harming capability.