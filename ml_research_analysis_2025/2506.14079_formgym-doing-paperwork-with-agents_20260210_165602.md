---
ver: rpa2
title: 'FormGym: Doing Paperwork with Agents'
arxiv_id: '2506.14079'
source_url: https://arxiv.org/abs/2506.14079
tags:
- field
- text
- form
- user
- fieldfinder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "FormGym benchmarks end-to-end form filling, where agents must\
  \ populate PDF-style document fields using external persona data. Baseline vision-language\
  \ agents score below 1% accuracy due to poor field localization, while GUI agents\
  \ achieve 10.6\u201368.0% accuracy but suffer from high latency and cost."
---

# FormGym: Doing Paperwork with Agents

## Quick Facts
- arXiv ID: 2506.14079
- Source URL: https://arxiv.org/abs/2506.14079
- Reference count: 31
- Primary result: FieldFinder improves VLA form-filling accuracy from <1% to 56% while reducing latency and cost versus GUI agents.

## Executive Summary
FormGym benchmarks end-to-end form filling, where agents must populate PDF-style document fields using external persona data. Baseline vision-language agents score below 1% accuracy due to poor field localization, while GUI agents achieve 10.6–68.0% accuracy but suffer from high latency and cost. To address this, we introduce FieldFinder, an open-vocabulary object detection model that locates input fields by name. With FieldFinder, all models achieve equal or better performance, with maximum gains from 2% to 56%. FieldFinder improves accuracy across all scenarios with minimal latency and memory overhead, demonstrating a scalable method for equipping zero-shot VLAs with field localization capabilities.

## Method Summary
The authors benchmark end-to-end form filling using the FormGym dataset, which includes 25,466 training and 3,889 test field examples across multiple domains. They fine-tune Florence 2 Large (0.77B parameters) as FieldFinder to locate form fields by name. The system integrates FieldFinder with VLAs (GPT-5, Claude 4, LLaVA 7B, etc.) via an API that places text at detected coordinates. They compare this approach against baseline VLAs using direct coordinate placement and against GUI agents like Claude Computer Use. The evaluation measures field accuracy (whether predicted text center falls within ground truth bounding box), latency, and cost.

## Key Results
- Baseline VLAs score <1% accuracy due to poor field localization
- GUI agents achieve 10.6–68.0% accuracy but require 100× more cost and 84× more time
- FieldFinder improves accuracy from 2% to 56% maximum, with 80.5% on sparse forms and 6.9% on dense forms
- FieldFinder provides minimal latency and memory overhead while improving accuracy across all scenarios

## Why This Works (Mechanism)

### Mechanism 1: Spatial Decoupling via Specialized Tooling
Offloading spatial localization from the reasoning model to a specialized detector significantly improves form-filling accuracy. Vision-Language Agents (VLAs) struggle to output precise Cartesian coordinates for text placement due to a lack of pixel-level training. By replacing the coordinate-based API with a semantic API where the agent outputs a field name, the system decouples "what to write" (reasoning) from "where to write" (localization). The lightweight FieldFinder model handles the coordinate regression.

### Mechanism 2: Hierarchical Semantic Context Injection
Prepending hierarchical structural context to field names improves localization accuracy in dense or complex layouts. Forms often contain duplicate or ambiguous field names. FieldFinder mitigates this by training on input strings that include hierarchical paths (e.g., `Section 1 | Applicant | Name`), allowing the model to disambiguate based on spatial relationships relative to headers and sections.

### Mechanism 3: Efficiency via Lightweight Model Augmentation
Augmenting a frontier VLA with a small, fine-tuned vision model is more cost-effective than using a Generalist GUI Agent. Instead of using an expensive, high-latency generalist model to navigate pixels via mouse/keyboard simulation, the system uses a 0.77B parameter model for the "heavy lifting" of visual grounding. The VLA only performs high-level planning.

## Foundational Learning

- **Open-Vocabulary Object Detection**
  - Why needed: FieldFinder is explicitly described as an "open-vocabulary object detection model" that detects objects based on natural language descriptions at inference time.
  - Quick check: How does FieldFinder handle a field name it has never seen during training? (Answer: It generalizes based on text-image alignment, though the paper notes out-of-distribution struggles).

- **Vision-Language Agents (VLAs) vs. GUI Agents**
  - Why needed: The paper distinguishes performance between agents that operate via API and agents that operate via pixel-based mouse/keyboard simulation.
  - Quick check: Why does the paper argue that GUI Agents are less efficient for this specific task?

- **Coordinate Regression vs. Classification**
  - Why needed: The paper identifies that VLAs fail at predicting specific (x,y) coordinates but succeed at identifying semantic entities.
  - Quick check: What specific metric defines a "correct" placement in the paper's evaluation? (Answer: Center point of predicted text within ground truth bounding box).

## Architecture Onboarding

- **Component map:** VLA (Planner) -> FieldFinder (Localizer) -> Editor API (Executor)
- **Critical path:** Load persona data and form image → VLA generates action plan with field names and values → FieldFinder attempts to locate bounding box for each field name → Text is rendered at calculated centroid → (Optional) VLA receives feedback on success/failure
- **Design tradeoffs:** Performance vs. Field Density (accuracy degrades with more fields), Annotation Depth vs. Scalability (manual vs. automatic dataset creation)
- **Failure signatures:** Leftward Bias (training on left-aligned text), Early Termination (VLAs call Terminate immediately), Lexical Confusion (similar field names cause transposition errors)
- **First 3 experiments:**
  1. Sanity Check: Run Claude/GPT-4o on baseline task to confirm <1% accuracy and identify horizontal/vertical localization failures
  2. Ablation: Test FieldFinder with and without section headers on Auto Loans dataset to quantify hierarchical context impact
  3. Stress Test: Plot localization accuracy against "Fields per Form" to determine operational density limits

## Open Questions the Paper Calls Out

- How can agents be improved to utilize iterative feedback loops for error correction rather than terminating prematurely? (Frontier models call "Terminate" immediately after first pass, lacking self-awareness to doubt accuracy)
- Can field localization models be made robust to high field density and multilingual layouts? (FieldFinder accuracy shows negative linear trend with fields per form and drops significantly on multilingual XFUND)
- What causes the approximate 50% performance drop when agents must retrieve persona information from images versus text? (GPT-5 and Claude struggle with chained reasoning in image-based persona scenarios)

## Limitations

- Benchmark focuses on static form-filling rather than interactive scenarios requiring dynamic decision-making
- FieldFinder's accuracy degrades significantly with form density (88.6 fields/form → 6.9% accuracy)
- Reliance on hierarchical context assumes consistent form structure across domains
- Cross-lingual generalization shows significant performance gaps (24.9% on XFUND vs. higher English performance)

## Confidence

- **High confidence:** Core mechanism demonstrating improved accuracy over baseline VLAs (<1% → up to 56% with FieldFinder)
- **Medium confidence:** Efficiency claims comparing GUI vs. API approaches (latency/cost differences clearly demonstrated but single GUI implementation)
- **Low confidence:** Cross-lingual generalization (significantly lower performance on non-English forms with minimal analysis)

## Next Checks

1. **Field Density Stress Test:** Systematically evaluate FieldFinder accuracy across forms with varying field counts (5, 20, 50, 100+ fields) to establish operational limits

2. **Cross-Lingual Ablation:** Fine-tune FieldFinder exclusively on non-English XFUND data and evaluate performance on held-out multilingual forms

3. **Iterative Correction Analysis:** Implement instrumentation to track whether VLAs successfully utilize iterative correction opportunities when initial field placement fails