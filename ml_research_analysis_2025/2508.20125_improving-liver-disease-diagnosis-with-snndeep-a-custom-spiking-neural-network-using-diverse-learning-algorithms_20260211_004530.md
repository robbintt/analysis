---
ver: rpa2
title: 'Improving Liver Disease Diagnosis with SNNDeep: A Custom Spiking Neural Network
  Using Diverse Learning Algorithms'
arxiv_id: '2508.20125'
source_url: https://arxiv.org/abs/2508.20125
tags:
- learning
- liver
- neural
- spiking
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces SNNDeep, a custom spiking neural network\
  \ optimized for binary liver health classification from CT imaging. The model employs\
  \ biologically inspired learning rules\u2014Surrogate Gradient, Tempotron, and Bio-Inspired\
  \ Active Learning\u2014across three implementations: a handcrafted low-level design\
  \ and two framework-based versions using snnTorch and SpikingJelly."
---

# Improving Liver Disease Diagnosis with SNNDeep: A Custom Spiking Neural Network Using Diverse Learning Algorithms

## Quick Facts
- arXiv ID: 2508.20125
- Source URL: https://arxiv.org/abs/2508.20125
- Reference count: 0
- Primary result: Custom SNNDeep achieves 98.35% validation accuracy for binary liver health classification from CT imaging, outperforming framework-based alternatives.

## Executive Summary
This study introduces SNNDeep, a custom spiking neural network optimized for binary liver health classification from CT imaging. The model employs biologically inspired learning rules—Surrogate Gradient, Tempotron, and Bio-Inspired Active Learning—across three implementations: a handcrafted low-level design and two framework-based versions using snnTorch and SpikingJelly. All models were trained and evaluated on the Task03_Liver dataset with Optuna-based hyperparameter optimization. The custom SNNDeep achieved a maximum validation accuracy of 98.35%, outperforming framework-based variants (95.19%) and significantly reducing training time. The results demonstrate that low-level, tunable SNNs surpass standard frameworks in medical imaging, particularly in data-limited, temporally constrained settings, opening new pathways for neuro-inspired AI in precision medicine.

## Method Summary
The method employs a three-layer spiking neural network architecture built on Leaky Integrate-and-Fire (LIF) neurons. The model uses temporal spike encoding of CT-derived feature vectors, converting them into spike trains via Poisson or rate-based encoding over 5-20 simulation steps. Three learning rules are benchmarked: Surrogate Gradient Learning (best performer), Tempotron, and Bio-Inspired Active Learning. Hyperparameter optimization is performed using Optuna across membrane time constants, spike thresholds, layer sizes, and encoding parameters. The custom implementation provides explicit control over neuronal dynamics, enabling precise tuning of spike generation and threshold mechanisms. The study evaluates performance on the Task03_Liver dataset from the Medical Segmentation Decathlon, achieving binary classification between healthy and diseased liver tissue.

## Key Results
- Custom SNNDeep with Surrogate Gradient Learning achieves maximum validation accuracy of 98.35%
- Framework-based implementations (snnTorch, SpikingJelly) achieve lower accuracy at 95.19%
- Custom implementation reduces training time by 52% compared to SpikingJelly variant
- Surrogate Gradient outperforms Tempotron (+3% accuracy) and Bio-Inspired Active Learning (+1% accuracy)

## Why This Works (Mechanism)

### Mechanism 1
Fine-grained control over neuronal dynamics in custom implementations yields higher accuracy than framework-based alternatives for specialized medical imaging tasks. The custom SNNDeep allows explicit tuning of membrane time constants, spike thresholds, and reset mechanisms per layer, enabling precise alignment between network dynamics and liver CT feature distributions. Framework abstractions constrain this configurability.

### Mechanism 2
Surrogate Gradient Learning outperforms Tempotron and Bio-Inspired Active Learning when gradient-based weight updates can leverage deeper architectures. SGL replaces the non-differentiable Heaviside spike function with a smooth surrogate, enabling backpropagation through time across both hidden layers, whereas Tempotron operates primarily on single-layer classification boundaries.

### Mechanism 3
Temporal spike encoding of CT-derived features enables event-driven computation without sacrificing diagnostic accuracy. Morphological and intensity-based feature vectors are converted to spike trains over 5-20 simulation steps, allowing the network to leverage inter-spike intervals as additional discriminative signals beyond static feature vectors.

## Foundational Learning

- **Leaky Integrate-and-Fire (LIF) Neuron Model**: The entire SNNDeep architecture is built on LIF dynamics; understanding τm·dVm/dt = −Vm(t) + Rm·I(t) is prerequisite for interpreting hyperparameter choices and threshold behavior. *Quick check*: Can you explain why a smaller membrane time constant (τm) would make a neuron more sensitive to recent inputs versus accumulated history?

- **Surrogate Gradient Descent**: The best-performing configuration uses SGL; without understanding how the Heaviside derivative is approximated, weight update mechanics remain opaque. *Quick check*: If α in σ′(V) = α·exp(−α(V−1)²) is increased, does the surrogate become sharper or smoother near the threshold?

- **Temporal Spike Coding**: CT features are encoded into spike trains; understanding Poisson vs. rate coding determines how input preprocessing affects downstream learning. *Quick check*: Given a feature value f, how would a Poisson encoder differ from a rate encoder in generating spike patterns over 10 timesteps?

## Architecture Onboarding

- **Component map**: Input encoding layer → Hidden layer 1 (64-128 LIF neurons) → Hidden layer 2 (32-64 LIF neurons) → Output layer (2 logits)

- **Critical path**: 1) Load Task03_Liver CT volumes, extract feature vectors from (Vi, Gi) pairs; 2) Convert feature vectors → temporal spike trains via selected encoding scheme; 3) Forward pass through LIF layers: membrane integration → threshold comparison → spike/reset; 4) Compute loss; apply selected learning rule; 5) Optuna optimizes τm, Vth, layer sizes, encoding parameters over validation accuracy

- **Design tradeoffs**: Custom vs. framework: Custom offers 3% higher accuracy but requires manual implementation of all neuron dynamics and learning rules; SGL vs. Tempotron: SGL achieves +3% accuracy but costs ~2× training time (108,870s vs. 58,878s for custom); Simulation steps (5-20): More steps increase temporal resolution but linearly increase compute

- **Failure signatures**: Accuracy plateaus at ~95% across all learning rules → likely using framework implementation with constrained neuron tunability; Training time exceeds 300,000s with no accuracy gain → likely using SpikingJelly without optimized batch processing; Validation accuracy drops below 90% → check that binary labels correctly distinguish lesion-present vs. healthy; verify feature extraction from CT masks

- **First 3 experiments**: 1) Replicate Table 1: Implement custom SNNDeep with SGL on Task03_Liver with Optuna optimization; target ≥98% validation accuracy within 110,000s training; 2) Ablate temporal encoding: Compare 5-step vs. 20-step simulation to verify whether temporal resolution contributes to accuracy or merely increases compute; 3) Cross-validate framework gap: Train snnTorch variant with identical hyperparameters to isolate whether performance difference stems from optimization constraints vs. implementation bugs

## Open Questions the Paper Calls Out

- Can SNNDeep maintain high diagnostic accuracy when extended to multiclass classification tasks involving specific liver pathologies (e.g., differentiating hepatocellular carcinoma from metastases)?
- Does SNNDeep generalize effectively to multimodal inputs and external clinical environments?
- What are the empirical energy and latency improvements when deploying SNNDeep on neuromorphic hardware compared to GPU-based simulations?
- Would integrating unsupervised learning rules, such as STDP variants, improve the model's feature extraction capabilities compared to the currently tested supervised methods?

## Limitations
- Feature extraction pipeline from CT volumes is not specified, making exact replication impossible without assumptions about the input representation
- Performance comparisons between custom and framework implementations may be influenced by unaccounted optimization differences in data loading or preprocessing
- Claim that temporal spike encoding provides discriminative information lacks validation through direct ablation studies comparing temporal vs. rate encoding

## Confidence
- **High confidence**: Custom SNNDeep architecture design and LIF neuron implementation details are well-specified and reproducible
- **Medium confidence**: Performance superiority of custom implementation over frameworks is credible but could be influenced by optimization differences
- **Low confidence**: Temporal encoding's contribution to accuracy lacks direct validation through ablation studies

## Next Checks
1. Implement an ablation study comparing 5-step vs. 20-step temporal encoding to quantify the contribution of temporal resolution to diagnostic accuracy
2. Replicate the snnTorch framework implementation with identical hyperparameters to the custom version to isolate whether performance differences stem from architectural constraints or implementation details
3. Cross-validate the feature extraction methodology by applying standard radiomics pipelines to Task03_Liver and measuring the impact on SNNDeep performance to assess whether input representation drives the accuracy gains