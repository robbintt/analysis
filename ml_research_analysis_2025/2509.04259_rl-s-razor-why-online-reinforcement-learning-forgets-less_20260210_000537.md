---
ver: rpa2
title: 'RL''s Razor: Why Online Reinforcement Learning Forgets Less'
arxiv_id: '2509.04259'
source_url: https://arxiv.org/abs/2509.04259
tags:
- forgetting
- arxiv
- task
- learning
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Online reinforcement learning (RL) exhibits less catastrophic forgetting
  than supervised fine-tuning (SFT) when adapting models to new tasks, despite achieving
  similar new-task performance. The key insight is that forgetting is strongly predicted
  by the KL divergence between the fine-tuned and base model on the new task, regardless
  of the training algorithm.
---

# RL's Razor: Why Online Reinforcement Learning Forgets Less

## Quick Facts
- arXiv ID: 2509.04259
- Source URL: https://arxiv.org/abs/2509.04259
- Reference count: 40
- Primary result: Online RL exhibits less catastrophic forgetting than SFT when adapting models to new tasks, despite achieving similar new-task performance.

## Executive Summary
This paper reveals that catastrophic forgetting during fine-tuning is strongly predicted by the KL divergence between the fine-tuned and base model, measured on the new task distribution. Online reinforcement learning (RL) exhibits less forgetting than supervised fine-tuning (SFT) when adapting models to new tasks, despite achieving similar new-task performance. The key insight is that forgetting is strongly predicted by the KL divergence between the fine-tuned and base model on the new task, regardless of the training algorithm. On-policy RL updates are inherently biased toward KL-minimal solutions among those solving the new task, while SFT can converge to arbitrary distant distributions. Experiments across large language models and simulated robotics confirm RL's advantage, and an oracle SFT distribution that minimizes KL divergence shows even better preservation of prior knowledge. The findings reveal that to reduce forgetting, training algorithms should explicitly minimize KL divergence from the base model, suggesting a new design principle for continual learning systems.

## Method Summary
The study compares RL (GRPO) and SFT fine-tuning on catastrophic forgetting. Experiments use Qwen 2.5 3B-Instruct on Math, Science Q&A, and Tool use tasks, plus OpenVLA 7B on robotic manipulation. For controlled experiments, ParityMNIST+FashionMNIST with small MLPs. The method tracks forward KL(π₀||π) on new-task inputs during training and evaluates prior-task performance at checkpoints. Hyperparameter sweeps across learning rates, batch sizes, and epochs construct Pareto frontiers of new-task accuracy vs prior-task retention. RL uses binary success rewards with on-policy sampling; SFT uses cross-entropy against fixed annotations.

## Key Results
- Forgetting is strongly predicted by forward KL divergence between fine-tuned and base model on new-task inputs (R² = 0.96 in MNIST, R² = 0.71 in LLMs).
- On-policy RL methods achieve similar new-task accuracy with significantly less forgetting than SFT across both LLM and robotics experiments.
- The critical factor distinguishing RL from SFT is the sampling distribution (on-policy vs offline), not the presence of negative gradients.
- Oracle SFT distributions that minimize KL divergence show even better preservation of prior knowledge than RL.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Catastrophic forgetting is predicted by forward KL divergence between the fine-tuned and base model, measured on the new task distribution.
- Mechanism: The distributional shift from the base policy quantifies how much the model has moved in probability space. Larger KL shifts on the new task correlate with greater interference with prior knowledge, likely because broader distributional changes affect more of the model's capability surface.
- Core assumption: The KL measured on new-task inputs generalizes as a proxy for distributional shift affecting prior tasks.
- Evidence anchors:
  - [abstract] "We find that the degree of forgetting is determined by the distributional shift, measured as the KL-divergence between the fine-tuned and base policy evaluated on the new task."
  - [section 4] ParityMNIST experiments show R² = 0.96 for quadratic fit between forgetting and KL; LLM experiments show R² = 0.71.
  - [corpus] Weak direct validation. Related work (Lai et al., 2025) reports RL forgets less than SFT but attributes this to negative examples rather than KL.

### Mechanism 2
- Claim: On-policy RL methods are implicitly biased toward KL-minimal solutions among all policies that solve the new task.
- Mechanism: Policy gradient methods sample from the model's own distribution, then reweight samples by reward. This produces incremental shifts toward higher-reward regions rather than pulling toward an arbitrary external distribution.
- Core assumption: The representable policy class is sufficiently rich that optimal solutions exist within it; the theoretical result assumes e-flat (exponential family) structure that neural networks only approximately satisfy.
- Evidence anchors:
  - [abstract] "Our analysis reveals that on-policy RL is implicitly biased towards KL-minimal solutions among the many that solve the new task."
  - [section 5.2] Theorem 5.2 states policy gradient converges to argmin over optimal policies of KL(π||π₀).
  - [corpus] Weak validation. Corpus neighbors discuss RL benefits for generalization and capability but do not address KL-minimal bias.

### Mechanism 3
- Claim: The critical difference between RL and SFT is the sampling distribution (on-policy vs offline), not the presence of negative gradients.
- Mechanism: SFT pulls the model toward an external annotation distribution that may be arbitrarily far in KL from the base model. RL shifts the model incrementally by reweighting its own samples.
- Core assumption: The SFT label distribution is not already KL-minimal.
- Evidence anchors:
  - [section 5.1] Figure 4 shows 1-0 Reinforce and GRPO cluster together (lower KL at matched accuracy), while SFT and SimPO cluster together (higher KL).
  - [section 5.1] "Thus, the critical factor is not the presence of negative gradients but the use of on-policy data."
  - [corpus] Conflicting evidence. Lai et al. (2025) attribute RL's advantage to negative examples.

## Foundational Learning

- Concept: **KL Divergence (Forward vs Reverse)**
  - Why needed here: The paper uses forward KL (E[log(π₀/π)]) measured on new-task inputs as the predictor variable.
  - Quick check question: Given a base policy π₀ and fine-tuned policy π, which KL direction penalizes placing mass where π₀ has none?

- Concept: **Policy Gradient and On-Policy Sampling**
  - Why needed here: The mechanism hinges on how policy gradient updates reweight the model's own samples rather than matching external targets.
  - Quick check question: In the GRPO loss L = −E[A(x,y) log π(y|x)], where does the sample y come from, and what happens if A(x,y) is negative?

- Concept: **I-Projection and M-Projection in Information Geometry**
  - Why needed here: The theoretical analysis frames RL updates as alternating projections: I-projection finds the closest (in KL) distribution satisfying the reward constraint, and M-projection finds the closest representable policy to that target.
  - Quick check question: Given a target distribution q and a constraint set of representable policies Π, what does minimizing KL(q||π) vs. KL(π||q) imply about which direction "projects" onto Π?

## Architecture Onboarding

- Component map:
  Base model π₀ -> Fine-tuning algorithm (RL/SFT) -> KL tracker -> Evaluation harness (new-task accuracy + prior-task benchmarks)

- Critical path:
  1. Initialize from base model π₀
  2. For RL: sample outputs from π, compute rewards, update with policy gradient (GRPO or REINFORCE). For SFT: minimize cross-entropy against fixed labels
  3. Track KL(π₀||π) on new-task validation set throughout training
  4. Evaluate prior-task performance at checkpoints to confirm KL-forgetting correlation

- Design tradeoffs:
  - On-policy RL requires online sampling, which is computationally expensive and may be infeasible for some deployed systems. SFT is cheaper but may cause more forgetting unless labels are carefully constructed.
  - Explicit KL regularization (e.g., adding β·KL term) can reduce forgetting but may cap new-task performance; the paper shows implicit KL minimization from on-policy sampling achieves good trade-offs without explicit regularization.
  - Hyperparameter sweeps (learning rate, batch size, epochs) significantly affect the learning-forgetting frontier; practitioners should report Pareto curves rather than single points.

- Failure signatures:
  - If RL causes as much forgetting as SFT, check whether the reward signal is binary and sparse—dense/shaped rewards may pull the policy further in KL.
  - If KL-forgetting correlation breaks down, verify that KL is computed on the new-task distribution (not prior tasks) and that prior-task benchmarks genuinely measure retained capabilities.
  - If SFT matches RL, inspect whether the label distribution is already close to π₀'s outputs (e.g., distillation from an RL teacher).

- First 3 experiments:
  1. Replicate the ParityMNIST + FashionMNIST experiment with a small MLP to confirm the KL-forgetting correlation (R² > 0.9) and the RL-SFT gap in your setup.
  2. On a production LLM (e.g., Qwen 2.5 3B), fine-tune on a new task with both SFT and GRPO across 5+ learning rates; plot the Pareto frontier of new-task accuracy vs. average prior-benchmark score to verify RL's advantage.
  3. Ablate on-policy vs. offline sampling: compare GRPO, 1-0 Reinforce, SFT, and SimPO (or similar offline contrastive method) on the same task, confirming that on-policy methods cluster on lower-KL, higher-retention trajectories.

## Open Questions the Paper Calls Out

- **Question**: What is the precise mechanistic link between KL divergence on the new task and the degradation of prior capabilities?
  - Basis: The Discussion states, "we still lack a mechanistic account of why larger KL shifts on the new task disrupt prior knowledge—whether through representational interference, implicit capacity limits, or other dynamics."
  - Why unresolved: The paper establishes an empirical correlation but does not isolate the specific representational or functional changes that cause the KL shift to damage old skills.
  - What evidence would resolve it: Causal interventions or detailed representational analysis identifying whether forgetting is driven by overwriting features, capacity saturation, or distributional collision.

- **Question**: Does the bias toward KL-minimal solutions hold for online, off-policy reinforcement learning algorithms?
  - Basis: The Conclusion notes, "we didn't study online but off-policy algorithms, which are popular in RL."
  - Why unresolved: The theoretical justification relies on on-policy sampling, and the experiments focus on GRPO/REINFORCE. Off-policy methods sample from a replay buffer, potentially breaking the "staying close to the current policy" dynamic.
  - What evidence would resolve it: Evaluating the KL trajectory and forgetting rates of online off-policy algorithms compared to on-policy baselines.

- **Question**: Does the convergence to the KL-minimal optimal policy persist with dense or continuous reward functions?
  - Basis: The theoretical analysis relies on a binary reward function to characterize the optimal set.
  - Why unresolved: Real-world RL often involves dense or continuous rewards. If the optimal policy set is not defined by a simple linear constraint, the proof for the "RL's Razor" property may not hold.
  - What evidence would resolve it: Theoretical extension of the proof to general reward functions or empirical verification showing that policy gradient methods under dense rewards still converge to solutions with lower KL than SFT equivalents.

## Limitations

- The paper's core finding is empirically robust but mechanistically incomplete—the theory assumes e-flat policy classes that neural networks only approximately satisfy.
- The experimental support for the on-policy vs offline sampling mechanism is weakened by conflicting evidence in the broader literature.
- The strong correlation (R²=0.96) in controlled MNIST experiments degrades to R²=0.71 in LLMs, suggesting that KL estimates become noisier with larger models and real-world data distributions.

## Confidence

- **High confidence**: KL-forgetting correlation holds empirically across multiple experimental domains; RL achieves similar new-task performance with less forgetting than SFT.
- **Medium confidence**: On-policy sampling is the primary driver of RL's forgetting advantage (the mechanism is plausible but contested by conflicting corpus evidence).
- **Medium confidence**: The theoretical result that policy gradient converges to KL-minimal optimal policies (depends on assumptions about policy class richness and e-flat structure).

## Next Checks

1. Replicate the ParityMNIST controlled experiment with varying policy classes (from simple linear to deep networks) to test whether the KL-forgetting correlation holds as the representational capacity changes.
2. Conduct an ablation study comparing on-policy vs offline RL methods with identical reward functions and architectures to isolate the sampling mechanism's contribution to forgetting.
3. Design an experiment where SFT is trained with labels generated from a KL-minimal oracle and test whether it matches or exceeds RL's forgetting performance on real LLM tasks.