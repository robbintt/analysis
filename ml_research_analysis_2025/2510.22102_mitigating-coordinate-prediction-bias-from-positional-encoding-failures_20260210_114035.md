---
ver: rpa2
title: Mitigating Coordinate Prediction Bias from Positional Encoding Failures
arxiv_id: '2510.22102'
source_url: https://arxiv.org/abs/2510.22102
tags:
- positional
- vpsg
- arxiv
- coordinate
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Multimodal large language models excel at vision-language tasks
  but struggle with precise coordinate prediction, especially at high resolutions
  where positional encodings degrade and introduce directional biases. This work investigates
  how perturbing visual positional encodings induces predictable, non-random biases,
  and shows that similar patterns occur in natural high-resolution data.
---

# Mitigating Coordinate Prediction Bias from Positional Encoding Failures

## Quick Facts
- arXiv ID: 2510.22102
- Source URL: https://arxiv.org/abs/2510.22102
- Reference count: 40
- Multimodal LLMs struggle with precise coordinate prediction at high resolutions due to positional encoding degradation

## Executive Summary
Multimodal large language models excel at vision-language tasks but struggle with precise coordinate prediction, especially at high resolutions where positional encodings degrade and introduce directional biases. This work investigates how perturbing visual positional encodings induces predictable, non-random biases, and shows that similar patterns occur in natural high-resolution data. To address this, Vision-PE Shuffle Guidance (VPSG) is proposed—a training-free test-time method that runs auxiliary decoding with shuffled positional encodings to isolate position-unconditioned tendencies, then uses this as negative evidence to guide digit prediction via a lightweight finite-state machine. Experiments on ScreenSpot-Pro demonstrate consistent accuracy gains: VPSG improves Qwen2.5-VL-3B from 11.6% to 13.3% and Qwen2.5-VL-7B from 18.5% to 19.1% correct coordinate predictions, validating the effectiveness of suppressing spurious position-unconditioned biases for spatial reasoning.

## Method Summary
Vision-PE Shuffle Guidance (VPSG) is a training-free test-time method that improves coordinate prediction accuracy by identifying and suppressing position-unconditioned biases. The method runs one main decoding pass with normal visual positional encodings and S auxiliary passes with shuffled positional encodings. The auxiliary outputs are aggregated to estimate position-unconditioned bias, which is then subtracted from digit logits in the main pass. A finite-state machine tracks whether decoding x or y coordinates, and a position-aware coefficient decay prevents over-regularization of later digits. This approach isolates spurious numeric tendencies that emerge when positional encodings degrade at high resolutions, particularly affecting digit-level predictions.

## Key Results
- VPSG improves Qwen2.5-VL-3B coordinate prediction accuracy from 11.6% to 13.3%
- VPSG improves Qwen2.5-VL-7B coordinate prediction accuracy from 18.5% to 19.1%
- Shuffled positional encodings cause predictions to collapse to specific numbers (e.g., 1024) with normalized pairwise distance ~0.16 vs ~0.40-0.44 for normal PEs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: When visual positional encodings degrade at high resolutions, MLLMs exhibit predictable directional biases rather than random coordinate errors.
- Mechanism: The model relies on position-unconditioned spurious correlations when the X→Y causal pathway weakens due to unreliable positional signals P. This manifests as systematic clustering toward preferred coordinate patterns.
- Core assumption: Positional encoding failure amplifies a non-causal S→Y pathway, but this causal graph is proposed rather than empirically validated through interventional studies.
- Evidence anchors: Shuffled PE conditions show normalized pairwise distance ~0.16 vs ~0.40-0.44 for normal PEs; perturbations induce predictable, non-random coordinate biases.

### Mechanism 2
- Claim: Contrasting position-conditioned and position-unconditioned outputs isolates spurious numeric tendencies that can be suppressed during decoding.
- Mechanism: VPSG runs auxiliary decoding with shuffled PEs to estimate position-unconditioned bias, then subtracts scaled negative evidence from main route digit logits.
- Core assumption: Assumes the shuffled PE distribution approximates the true position-unconditioned marginal and provides stable bias estimation.
- Evidence anchors: Removing seeds aggregation drops accuracy from 13.3%→13.0% (3B) and 19.1%→18.6% (7B); classifier-free guidance provides theoretical foundation.

### Mechanism 3
- Claim: Position-aware coefficient decay focuses correction on high-order digits where positional grounding matters most.
- Mechanism: The FSM tracks whether decoding x or y coordinates, and guidance coefficient α_t decays geometrically within each coordinate.
- Core assumption: Assumes early digits carry more positional information and later digits have narrower logit margins making them sensitive to over-penalization.
- Evidence anchors: Removing coefficient decay causes larger accuracy drops (3B: 13.3%→11.9%, Δ-1.4; 7B: 19.1%→18.2%, Δ-0.9) than removing seeds aggregation.

## Foundational Learning

- **Structural Causal Models (SCMs)**: The paper frames coordinate prediction as Y = g(X, P, S) where X=input, P=positional encodings, S=spurious correlations. Understanding this decomposition is essential to grasp why shuffling PEs exposes the S→Y pathway. Quick check: If P were perfectly reliable, would the S→Y pathway disappear entirely or just become relatively weaker?

- **Classifier-Free Guidance (CFG)**: VPSG adapts CFG from diffusion models to token decoding. Understanding that CFG contrasts conditional vs unconditional distributions helps explain why p_A(v|c_t) and p_B(v) are combined through log-space subtraction. Quick check: Why does the paper use negative evidence subtraction rather than positive amplification of the conditional alone?

- **Positional Encoding Extrapolation Failure**: High-resolution inputs push models beyond training distribution, causing RoPE and 2D encodings to weaken. This is the root cause VPSG addresses. Quick check: What happens to sinusoidal or RoPE encodings when sequence length exceeds the training maximum?

## Architecture Onboarding

- **Component map**: Main route (normal PEs) -> p_A(v|c_t) -> digit logits -> FSM tracker -> coefficient scheduler -> negative-evidence scorer -> argmax selection. Auxiliary routes (S shuffled PEs) -> {p_B^(s)(v)} -> log-space aggregator -> ℓ̃_B(v) -> negative-evidence scorer.

- **Critical path**: 1) Forward pass with normal PEs → digit logits; 2) S forward passes with shuffled PEs → auxiliary digit distributions; 3) Aggregate auxiliary log-probs across seeds; 4) FSM determines current coordinate position; 5) Compute α_t based on position; 6) Subtract scaled negative evidence from main route digit logits; 7) Argmax selection, advance FSM, repeat until EOS.

- **Design tradeoffs**: More seeds → more stable bias estimate but higher inference cost (S+1 forward passes total); Higher α → stronger bias suppression but risk of over-correction; Faster decay → protects later digits but may under-correct systematic errors in mid-position digits; Digit-only guidance → preserves format but ignores potential spurious patterns in non-digit tokens.

- **Failure signatures**: Predictions cluster around specific numbers (e.g., 1024) across diverse inputs → positional encoding failure; Removing coefficient decay causes larger accuracy drops than removing seeds aggregation → model has digit-position confidence hierarchy; Single-seed VPSG shows high variance → need more seeds for stable bias estimation; Normalized pairwise distance under shuffled PEs approaches random baseline (~0.52) → model lacks directional bias.

- **First 3 experiments**: 1) Bias validation: Run base model on ScreenSpot-Pro with normal vs shuffled PEs. Compute diagonal-normalized pairwise distances. Confirm shuffled condition shows d̃≈0.16 (collapsed) vs normal d̃≈0.40+ (dispersed). 2) Ablation sweep: Test VPSG with α∈{0.3,0.55,0.8}, decay∈{0.2,0.4,0.6}, S∈{1,3,5}. 3) Cross-model transfer: Apply VPSG hyperparameters to a different MLLM (e.g., different vision encoder or LLM backbone). Measure whether gains transfer.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does VPSG interact with non-greedy decoding strategies like beam search or nucleus sampling? The paper exclusively evaluates VPSG using greedy decoding, leaving the stability of the negative-evidence subtraction under stochastic or beam-based search unexplored.

- **Open Question 2**: Can the finite-state machine (FSM) logic be generalized to support bounding box or segmentation mask formats without structural errors? The current FSM resets coefficients specifically for single-point x and y digits; it is unclear if this logic holds or requires re-engineering for more complex spatial outputs.

- **Open Question 3**: Does the observed "position-unconditioned bias" (e.g., the frequency of the number 1024) stem from dataset priors or architectural inductive biases? While the method mitigates the bias, the root cause of the specific numeric "priors" remains observational rather than mechanistically explained.

## Limitations
- The causal explanation linking PE degradation to spurious numeric pathways is plausible but not definitively proven through interventional studies
- VPSG requires S+1 forward passes per decoding step, resulting in significant inference slowdown (6x with S=5)
- Claims about model-agnostic applicability across different MLLM architectures are not empirically validated beyond the two Qwen2.5-VL variants tested

## Confidence
- **High Confidence**: The empirical demonstration that VPSG improves coordinate prediction accuracy on ScreenSpot-Pro (3B: 11.6%→13.3%, 7B: 18.5%→19.1%) is well-supported with statistical significance
- **Medium Confidence**: The causal explanation linking PE degradation to spurious numeric pathways is plausible but not definitively proven; the assumption that shuffled PE outputs approximate true position-unconditioned distributions is reasonable but untested
- **Low Confidence**: Claims about model-agnostic applicability across different MLLM architectures are not empirically validated beyond the two Qwen2.5-VL variants tested

## Next Checks
1. **Intervention Study**: Design an experiment that systematically varies positional encoding reliability and measures the resulting bias strength and VPSG effectiveness to validate the proposed causal mechanism
2. **Cross-Architecture Transfer**: Apply VPSG to at least two additional MLLM architectures with different vision encoders and LLM backbones to test whether the same hyperparameters achieve similar relative improvements
3. **Real-time Feasibility Assessment**: Benchmark VPSG's inference latency on commodity hardware and compare against accuracy-latency tradeoffs of alternative approaches like fine-tuning or curriculum learning to determine if the 6x slowdown is acceptable for practical deployment scenarios