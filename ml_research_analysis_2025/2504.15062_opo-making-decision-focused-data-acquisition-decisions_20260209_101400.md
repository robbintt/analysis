---
ver: rpa2
title: 'OPO: Making Decision-Focused Data Acquisition Decisions'
arxiv_id: '2504.15062'
source_url: https://arxiv.org/abs/2504.15062
tags:
- problem
- prediction
- which
- optimisation
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OPO, a model for decision-focused data acquisition
  in contextual stochastic optimization problems. The key idea is to use differentiable
  optimization to learn a surrogate linear objective function that guides data acquisition
  decisions to optimize downstream decision quality.
---

# OPO: Making Decision-Focused Data Acquisition Decisions

## Quick Facts
- arXiv ID: 2504.15062
- Source URL: https://arxiv.org/abs/2504.15062
- Reference count: 3
- Primary result: OPO achieves 17% lower relative regret than best non-learned data acquisition method

## Executive Summary
OPO introduces a model for decision-focused data acquisition (DA) in contextual stochastic optimization problems. The key innovation is using differentiable optimization to learn a surrogate linear objective function that guides DA decisions to optimize downstream decision quality. The approach addresses the challenge where DA decisions are typically treated as separate and fixed, but can significantly impact downstream prediction and decision performance. OPO is demonstrated on a drone reconnaissance problem for a shortest path task, where the goal is to determine which image segments to capture to provide optimal inputs for a travel cost prediction model.

## Method Summary
OPO learns a parameter vector π that parameterizes a surrogate linear objective function for data acquisition decisions. This surrogate objective approximates the true (unknown) mapping from acquisition decisions to downstream value. The method uses differentiable optimization layers to compute gradients through both the DA layer (computing ∂s*/∂π using perturbation-based methods) and the downstream decision layer (computing ∂x*/∂θ̂ using counterfactual gradients). A masked-input prediction model with learnable mask tokens handles arbitrary missing segments during inference. The approach is demonstrated on a drone reconnaissance problem where a drone must visit tiles on a grid to capture image segments for travel cost prediction, with the ultimate goal of finding an optimal shortest path.

## Key Results
- OPO achieves 17% lower relative regret compared to the best non-learned data acquisition method
- Outperforms random search strategies for data acquisition parameter initialization
- Best performance achieved with prediction-focused learning (PFL) rather than decision-focused learning (DFL) when combined with warm-start pretraining

## Why This Works (Mechanism)

### Mechanism 1: Surrogate Linear Objective for Intractable DA Mapping
A learned linear surrogate can approximate optimal solutions to the true (unknown) data acquisition objective under reasonable geometric assumptions. The true DA problem arg max p(s) has an unknown mapping p(·) from acquisition decisions to downstream value. OPO learns a parameter vector π such that arg max ⟨π, s⟩ subject to A(s) ≤ 0 approximates the true optimum. This works when the optimal solution lies on the boundary of the convex hull of the feasible set.

### Mechanism 2: Differentiable Optimization with Counterfactual Gradients
Gradients can flow through both DA and CDT optimization layers via perturbation-based differentiable optimizers, enabling end-to-end training. Two non-sequential implicit layers require DO: (1) DA layer computes ∂s*/∂π using Pogancic et al. 2020's method—gradient estimated as difference between perturbed and initial solution; (2) CDT layer computes ∂x*/∂θ̂ using Perturbed Fenchel-Young Loss. The critical "counterfactual gradient" ∂I/∂s* = t(z)i - Λ enables backpropagation through the masking operation by requiring full context z during training.

### Mechanism 3: Masked-Input Prediction Model for DA-Robust Inference
A prediction model with learnable mask tokens Λ can produce stable predictions across diverse DA strategies, enabling reuse across acquisition patterns. The tokenization I(z, s*; Λ) = diag(s*)t(z) + (1 - s*) ⊗ Λ copies observed tokens and replaces unobserved positions with learnable mask token Λ. The class token provides a stable anchor. Pretraining with random masking at rates matching DA constraints improves generalization.

## Foundational Learning

- **Differentiable Optimization / Decision-Focused Learning**
  - Why needed: OPO extends DFL principles to data acquisition; understanding gradient flow through optimization operators is prerequisite
  - Quick check: Can you explain why ∂x*/∂θ̂ requires perturbation-based approximation for combinatorial solvers?

- **Masked Input Modeling (Vision Transformers)**
  - Why needed: The prediction model must handle arbitrary missing segments; mask token design directly affects counterfactual gradient computation
  - Quick check: How does a learnable mask token differ from zero-imputation for gradient flow?

- **Orienteering Problem and Routing Constraints**
  - Why needed: The DA problem is formulated as orienteering (maximize rewards under travel budget); understanding constraint structure is essential for solver selection
  - Quick check: Why can't the orienteering problem be solved with standard MIP solvers for large instances?

## Architecture Onboarding

- **Component map**: π (learned surrogate) → Orienteering Solver → s* (binary mask) → z (full context) → Masked Tokenization I(z,s*;Λ) → ViT Encoder → θ̂ (predicted costs) → Shortest Path Solver → x* (path decision)

- **Critical path**: The DO layer for DA (∂s*/∂π) dominates compute—it requires two solver calls per batch (forward + backward pass). With 10-second heuristic budgets, this adds ~20 seconds per batch.

- **Design tradeoffs**:
  - Warm-start vs joint training: Joint training risks local optima from narrow DA diversity; warm-start with pretrained prediction model stabilizes but requires extra pretraining phase
  - PFL vs DFL loss: Paper finds PFL outperforms DFL for this specific task (contrary to some DFL literature), possibly due to missing-value effects on DFL being underexplored
  - Fixed vs learned π: Learning π provides ~17% regret reduction but adds optimization complexity and instability

- **Failure signatures**:
  - High variance across random initializations of π indicates DA strategy sensitivity (see Figure 2 spread)
  - Validation-test selection inconsistency suggests overfitting to specific DA patterns
  - DFL-pretrained models with warm-start DA showed degradation, suggesting overfitting to pretraining masks

- **First 3 experiments**:
  1. **Random search baseline**: Sample 1000 random π initializations, evaluate downstream decision quality on validation set to establish performance distribution and identify warm-start candidates
  2. **Ablation grid**: Test fixed vs learned π × PFL vs DFL × joint vs fine-tuning (8 configurations) to identify best training modality without hyperparameter tuning
  3. **Learning rate sweep for π**: On best initialization, test 8 learning rates (logarithmic spacing) to stabilize π optimization and quantify improvement over untuned baseline

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the OPO framework be effectively extended to natural language tasks where reasoning or token selection is formulated as a combinatorial optimization problem?
- **Basis in paper**: [explicit] Section 2.2 states, "We leave it to future work to examine how this could be extended to language tasks, but given that reasoning can be expressed as a combinatorial optimisation problem we hypothesise that it is possible."
- **Why unresolved**: The current implementation and experiments are restricted to a computer vision task (drone reconnaissance), and the tokenization mechanism is designed for image segments
- **What evidence would resolve it**: Successful application of the OPO architecture to a text-based reasoning task, demonstrating improved decision quality over standard heuristic selection methods

### Open Question 2
- **Question**: How can the Full Observation Assumption be relaxed to allow OPO to train in environments where the training data contains partially observed contextual variables?
- **Basis in paper**: [explicit] Section 5 suggests that online Informative Path Planning (IPP) approaches "may contain insight on how to extend OPO to environments without the full observation assumption."
- **Why unresolved**: The model currently relies on Assumption 3.1, which requires fully observed vectors z to compute the counterfactual gradient (Equation 10) during the backward pass
- **What evidence would resolve it**: A modified gradient estimation technique that converges without access to the ground truth values of the unobserved contextual variables during training

### Open Question 3
- **Question**: Can dynamic learning rates mitigate the sensitivity and instability observed when optimizing the surrogate data acquisition parameters (π)?
- **Basis in paper**: [explicit] The Conclusion notes, "We believe dynamic learning rates could be leveraged to further improve performance due to the observed sensitivity of the surrogate DA problem."
- **Why unresolved**: The empirical evaluation (Section 4.3) highlighted performance variance and instability, noting that some training runs showed no improvement, suggesting the optimization landscape is sensitive to the step size
- **What evidence would resolve it**: Experiments showing that an adaptive learning rate schedule for π results in lower variance and consistently lower regret compared to the fixed learning rates tested

### Open Question 4
- **Question**: Can solver-free differentiable optimization layers be developed to reduce the significant computational overhead caused by repeated calls to heuristic solvers?
- **Basis in paper**: [explicit] Section 4.3 identifies the computational cost of solver calls as a bottleneck, stating, "The development of solver-free approaches which work as a layer would be of immense benefit."
- **Why unresolved**: The current method requires solving the surrogate DA problem twice per batch (forward and backward pass) using a heuristic solver (OR-Tools), which limits scalability
- **What evidence would resolve it**: An architecture that approximates the solver with a neural network layer (amortized optimization) while maintaining the decision quality achieved by the explicit solver

## Limitations
- The primary limitation is the unverified assumption that optimal data acquisition solutions lie on the boundary of the feasible set's convex hull, which is critical for the surrogate linear objective to work
- The counterfactual gradient computation requires full observation of context z during training, restricting applicability to domains with comprehensive training data
- Significant computational overhead from DO layers requiring two solver calls per batch, adding ~20 seconds per batch with 10-second heuristic budgets

## Confidence
- **High Confidence**: The masked-input prediction model architecture is well-established and validated by related work like SimMIM
- **Medium Confidence**: The effectiveness of learned surrogate objectives for data acquisition decisions (17% regret reduction) is demonstrated on the drone reconnaissance task but may not generalize to domains with different constraint structures
- **Low Confidence**: The claim that optimal DA solutions lie on the convex hull boundary is asserted without proof or extensive empirical validation across different problem types

## Next Checks
1. **Convex Hull Boundary Validation**: Systematically test whether optimal DA solutions for different problem instances (beyond the drone reconnaissance task) lie on the convex hull boundary. Vary constraint structures and objective functions to assess robustness of this assumption.

2. **Counterfactual Gradient Sensitivity**: Evaluate performance when training data lacks full context observations. Compare against imputation baselines and measure degradation in decision quality to quantify the impact of the Full Observation Assumption.

3. **Scalability Assessment**: Measure wall-clock training time and decision quality degradation as problem size increases. Test with larger grid sizes and more complex routing constraints to identify computational bottlenecks and performance limits.