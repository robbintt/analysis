---
ver: rpa2
title: 'Prompt Sentiment: The Catalyst for LLM Change'
arxiv_id: '2503.13510'
source_url: https://arxiv.org/abs/2503.13510
tags:
- sentiment
- prompt
- prompts
- language
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how prompt sentiment influences LLM-generated
  outputs across six domains, revealing significant effects on coherence, factuality,
  and bias. Using both lexicon-based and transformer-based sentiment analysis, 500
  prompts were categorized and evaluated with five leading LLMs (Claude, DeepSeek,
  GPT-4, Gemini, LLaMA).
---

# Prompt Sentiment: The Catalyst for LLM Change

## Quick Facts
- arXiv ID: 2503.13510
- Source URL: https://arxiv.org/abs/2503.13510
- Authors: Vishal Gandhi; Sagar Gandhi
- Reference count: 17
- Negative prompts reduce factual accuracy by 8.4% and amplify bias, while positive prompts increase verbosity by 8.1% and sentiment propagation

## Executive Summary
This study systematically investigates how prompt sentiment influences LLM-generated outputs across six domains. Using both lexicon-based and transformer-based sentiment analysis, the authors categorized 500 prompts and evaluated them with five leading LLMs. The findings reveal that negative prompts significantly reduce factual accuracy and amplify bias, while positive prompts increase verbosity and sentiment propagation. Neutral prompts consistently yielded the most reliable and balanced outputs across all domains, highlighting the importance of sentiment-aware prompt engineering for ensuring fair, factual, and unbiased AI-generated content.

## Method Summary
The study employed a mixed-method approach combining lexicon-based (VADER, TextBlob) and transformer-based (BERT, RoBERTa fine-tuned on SST-2) sentiment analysis to classify 500 prompts across six domains. Each prompt was transformed into positive, neutral, and negative variants. Five LLMs (ChatGPT v4, Claude v1.3, DeepSeek v2.0, Gemini v1, LLaMA v2) generated responses to all variants. Outputs were evaluated using composite metrics: coherence (perplexity + semantic similarity), factuality (FEVER + Google FactCheck API), bias (sentiment drift + fairness metrics), and sentiment propagation ratio. The composite quality score Q = λ₁C + λ₂F − λ₃B − λ₄Sp weighted these components, though specific λ values were not disclosed.

## Key Results
- Negative prompts reduced factual accuracy by 8.4% and amplified bias across all domains
- Positive prompts increased response verbosity by 8.1% and enhanced sentiment propagation
- Neutral prompts consistently produced the most reliable and balanced outputs
- Domain effects varied: creative writing showed strongest sentiment amplification, while technical documentation showed strongest neutralization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs propagate and often amplify the affective tone of input prompts, with strength varying by domain.
- Mechanism: As statistical approximators of human language, LLMs internalize sentiment cues present in the input and reflect them in generated text. Subjective domains (creative writing, content generation) show stronger amplification; objective domains (legal, technical documentation) show neutralization.
- Core assumption: The model's training data and fine-tuning procedures encode domain-specific expectations about appropriate emotional expression.
- Evidence anchors:
  - [abstract] "positive prompts tend to increase verbosity and sentiment propagation"
  - [section 3.1] "Content generation and creative writing tasks exhibit the strongest sentiment propagation... domain-specific models default to a more formal, neutral tone"
  - [corpus] "Exploring the Impact of Personality Traits on LLM Bias and Toxicity" examines related affective influence on model behavior, though not directly validating propagation ratios.

### Mechanism 2
- Claim: Negative prompt sentiment reduces factual accuracy by inducing speculative, exaggerated, or alarmist responses.
- Mechanism: Negative framing shifts model behavior toward hedging, speculation, or emotionally charged language, interfering with precise information retrieval and reasoning.
- Core assumption: The correlation between negative sentiment and reduced factuality reflects a causal relationship rather than confounding factors (e.g., prompt complexity).
- Evidence anchors:
  - [abstract] "negative prompts often reducing factual accuracy and amplifying bias"
  - [section 3.2] "Prompts with negative sentiment result in the largest factual accuracy decline (~8.4%)... likely stems from the model shifting toward speculative, exaggerated, or alarmist responses"
  - [corpus] No direct corpus validation of the 8.4% figure; related work on bias measurement exists but does not confirm this specific mechanism.

### Mechanism 3
- Claim: Prompt sentiment asymmetrically affects response verbosity—positive prompts increase length, negative prompts decrease it more substantially.
- Mechanism: Models mirror human conversational patterns: expansiveness with positive input, terseness or disengagement with negative input.
- Core assumption: This asymmetric response reflects learned associations from human-written training data rather than architectural artifacts.
- Evidence anchors:
  - [section 3.4] "essays and blog-style responses from positive prompts are 8.1% longer... responses to negative prompts are 17.6% shorter"
  - [section 3.4] "The greater magnitude of length reduction for negative prompts compared to length increase for positive prompts suggests an asymmetric response"
  - [corpus] Weak support; corpus papers focus on bias and alignment rather than verbosity effects.

## Foundational Learning

- Concept: **Sentiment Analysis Methods (Lexicon-based vs. Transformer-based)**
  - Why needed here: The paper uses both VADER/TextBlob (lexicon) and BERT/RoBERTa (transformer) to categorize prompts; understanding their differences is essential for replicating the methodology.
  - Quick check question: Given the prompt "This policy is an absolute disaster for everyone involved," would a lexicon-based method capture the full negative intensity, or would context-aware transformer classification be more reliable?

- Concept: **LLM Evaluation Metrics (Perplexity, Factuality, Bias)**
  - Why needed here: The study's composite quality score (Q = λ₁C + λ₂F − λ₃B − λ₄Sp) requires understanding how coherence, factuality, and bias are operationalized.
  - Quick check question: If a model produces a fluent but factually incorrect response, which metrics would score high and which would score low?

- Concept: **Sentiment Propagation Ratio (Sp)**
  - Why needed here: This novel metric quantifies how much LLM outputs inherit or amplify input sentiment; understanding it is critical for interpreting Figure 1.
  - Quick check question: If a neutral prompt about healthcare receives a response with strongly empathetic language, would Sp increase or decrease?

## Architecture Onboarding

- Component map: 500 base prompts → Sentiment transformation (positive/neutral/negative variants) → VADER + TextBlob + BERT/RoBERTa → Human annotation validation (92% agreement) → 5 LLMs (ChatGPT v4, Claude v1.3, DeepSeek v2.0, Gemini v1, LLaMA v2) → Coherence (perplexity + semantic similarity), Factuality (FEVER + Google FactCheck API), Bias (sentiment drift + fairness metrics), Sentiment Propagation ratio → Composite quality score Q

- Critical path:
  1. Curate domain-specific prompts across 6 applications
  2. Transform each prompt into 3 sentiment variants
  3. Validate sentiment classification via dual methods + human annotation
  4. Generate responses from each LLM
  5. Compute composite quality score Q with weighted λ constants
  6. Analyze domain-specific and sentiment-specific patterns

- Design tradeoffs:
  - **Lexicon vs. Transformer sentiment analysis**: Lexicon methods are faster but miss context; transformers capture nuance but require more compute.
  - **Automated vs. Human fact-checking**: FEVER API enables scale but may miss domain-specific inaccuracies; human evaluation is more reliable but costly.
  - **Composite score weighting**: λ values prioritize certain metrics over others; the paper does not disclose specific values, limiting reproducibility.

- Failure signatures:
  - **Low inter-rater agreement (<85%)**: Indicates ambiguous sentiment classification; prompt transformations may not cleanly isolate sentiment effects.
  - **Domain cross-contamination**: If prompts from different domains share semantic structure, domain-specific conclusions become unreliable.
  - **Model version drift**: Results from Claude v1.3 or GPT-4 may not generalize to current versions.

- First 3 experiments:
  1. **Baseline validation**: Replicate sentiment classification on a 50-prompt subset using both VADER and RoBERTa; verify ≥90% agreement with human labels before scaling.
  2. **Single-domain deep-dive**: Run full pipeline on healthcare AI prompts only; measure whether negative sentiment amplification (noted in Section 3.1) replicates with current model versions.
  3. **Neutral prompt robustness test**: Generate 10 neutral-variant prompts for the same underlying query; assess variance in factuality scores to determine whether "neutral" reliably yields consistent outputs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does chain-of-thought (CoT) prompting mitigate or exacerbate the factual degradation caused by negative prompt sentiment?
- Basis in paper: [explicit] The authors explicitly propose "exploring the interaction between prompt sentiment and other prompt engineering techniques, such as chain-of-thought prompting."
- Why unresolved: The current study isolated sentiment variables but did not test how reasoning strategies interact with emotional framing to affect factuality.
- What evidence would resolve it: Comparative experiments evaluating factual accuracy scores (F) on negative prompts with and without CoT augmentation across different LLMs.

### Open Question 2
- Question: Do the observed patterns of bias amplification and factual accuracy loss persist across different languages and cultural contexts?
- Basis in paper: [explicit] The authors state that future work should consider "cross-lingual and multicultural contexts, to fully understand the global implications of sentiment dynamics."
- Why unresolved: The study relied on a dataset of 500 prompts which may not represent the full complexity of sentiment expression in non-English languages.
- What evidence would resolve it: Replicating the sentiment transformation methodology on multilingual benchmarks and analyzing cross-lingual sentiment propagation ratios.

### Open Question 3
- Question: Can real-time sentiment adaptation mechanisms effectively neutralize bias amplification without compromising the model's responsiveness?
- Basis in paper: [explicit] The paper calls for research into "dynamic sentiments, where LLMs adjust their responses in real time based on evolving emotional contexts."
- Why unresolved: The current methodology relied on static prompt classifications (positive, neutral, negative) and did not evaluate dynamic feedback loops.
- What evidence would resolve it: Developing and testing adaptive model architectures that detect and modulate internal sentiment states during generation to maintain neutrality.

## Limitations
- The 8.4% factual accuracy decline for negative prompts relies on a single evaluation framework without cross-validation against human fact-checkers
- Composite quality score Q uses unspecified λ weights, preventing exact replication of reported domain-specific rankings
- Asymmetric verbosity effects (17.6% vs 8.1%) lack model-level breakdowns, making it unclear whether this pattern holds across all five tested LLMs

## Confidence
- **High confidence**: The general relationship between positive prompts and increased sentiment propagation (Section 3.1), supported by multiple evaluation methods and consistent across domains
- **Medium confidence**: The 8.4% factual accuracy decline for negative prompts (Section 3.2), given the single-fact-checker methodology and lack of human validation
- **Low confidence**: The asymmetry of verbosity effects (Section 3.4), due to missing model-level granularity and unclear prompt transformation methodology

## Next Checks
1. **Cross-validation of factual accuracy measurements**: Replicate the negative prompt factual accuracy assessment using both FEVER API and human expert evaluation on a 50-prompt subset to determine if the 8.4% degradation holds
2. **Model-specific verbosity analysis**: Generate identical prompt sets for all five LLMs and measure response length changes separately to confirm whether the 17.6% vs 8.1% asymmetry persists across models
3. **Neutral prompt stability test**: Create 10 neutral variants for the same underlying queries and measure variance in composite quality scores to establish whether neutral prompts consistently yield the most reliable outputs as claimed