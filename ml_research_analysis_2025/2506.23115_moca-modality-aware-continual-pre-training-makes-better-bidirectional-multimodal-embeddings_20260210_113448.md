---
ver: rpa2
title: 'MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal
  Embeddings'
arxiv_id: '2506.23115'
source_url: https://arxiv.org/abs/2506.23115
tags:
- multimodal
- learning
- arxiv
- contrastive
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of multimodal embedding models
  built on causal vision-language models (VLMs), specifically suboptimal bidirectional
  attention, scalability issues due to reliance on labeled paired data, and lack of
  diverse training objectives. To overcome these, the authors propose MoCa, a two-stage
  framework that first applies modality-aware continual pre-training using joint reconstruction
  objectives (masked language modeling and masked autoencoding) on interleaved multimodal
  inputs, then fine-tunes with heterogeneous contrastive learning on diverse multimodal
  pairs beyond simple image-caption data.
---

# MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings

## Quick Facts
- **arXiv ID:** 2506.23115
- **Source URL:** https://arxiv.org/abs/2506.23115
- **Reference count:** 40
- **Primary result:** MoCa achieves state-of-the-art performance on MMEB and ViDoRe-v2 benchmarks

## Executive Summary
This paper introduces MoCa, a framework designed to improve bidirectional multimodal embeddings by addressing limitations in existing causal vision-language models. The authors identify three key issues: suboptimal bidirectional attention due to causal constraints, scalability challenges from reliance on labeled paired data, and lack of diverse training objectives. MoCa tackles these through a two-stage approach combining modality-aware continual pre-training with heterogeneous contrastive fine-tuning, resulting in strong performance across diverse multimodal tasks.

## Method Summary
MoCa operates in two stages: first, modality-aware continual pre-training uses joint reconstruction objectives (masked language modeling and masked autoencoding) on interleaved multimodal inputs to learn bidirectional attention patterns. Second, heterogeneous contrastive learning fine-tunes the model on diverse multimodal pairs beyond simple image-caption data. This approach enables the model to handle varied multimodal relationships while maintaining strong retrieval and alignment capabilities. The framework is tested across different model scales (3B and 7B parameters), demonstrating consistent improvements over state-of-the-art baselines.

## Key Results
- MoCa achieves state-of-the-art performance on MMEB and ViDoRe-v2 benchmarks
- The 3B MoCa model matches or surpasses competitive 7B baselines after pre-training on only 30B tokens
- Scaling from 3B to 7B parameters yields substantial performance gains, demonstrating strong scalability

## Why This Works (Mechanism)
MoCa's success stems from addressing the fundamental limitations of causal VLMs through bidirectional attention learning and diverse training objectives. The modality-aware continual pre-training stage allows the model to learn bidirectional relationships between vision and language modalities by reconstructing masked inputs, rather than being constrained by causal attention patterns. The heterogeneous contrastive fine-tuning stage exposes the model to diverse multimodal pairs beyond simple captions, enabling better generalization to varied real-world scenarios. This two-stage approach effectively combines reconstruction-based learning with contrastive objectives, creating a more robust bidirectional multimodal embedding model.

## Foundational Learning

**Bidirectional vs Causal Attention**
- *Why needed:* Causal attention limits context to past tokens, preventing true bidirectional understanding of multimodal relationships
- *Quick check:* Compare performance on tasks requiring future context understanding between MoCa and causal baseline models

**Masked Language Modeling (MLM)**
- *Why needed:* Forces the model to understand context and relationships between modalities to reconstruct missing information
- *Quick check:* Measure reconstruction accuracy on masked inputs across different modalities

**Masked Autoencoding (MAE)**
- *Why needed:* Enables learning of spatial-visual relationships by reconstructing masked image patches
- *Quick check:* Evaluate image reconstruction quality and downstream visual understanding tasks

**Contrastive Learning**
- *Why needed:* Aligns multimodal representations in a shared embedding space for better retrieval and comparison
- *Quick check:* Measure retrieval accuracy and embedding space similarity metrics

## Architecture Onboarding

**Component Map**
- Input Layer -> Modality Encoder -> Modality-Aware Continual Pre-training -> Heterogeneous Contrastive Fine-tuning -> Output Layer

**Critical Path**
The critical path involves the modality encoder processing interleaved multimodal inputs, followed by the continual pre-training stage where reconstruction objectives are applied. The heterogeneous contrastive fine-tuning stage then aligns representations across diverse multimodal pairs. The output layer provides the final bidirectional multimodal embeddings for downstream tasks.

**Design Tradeoffs**
The framework trades increased computational complexity during pre-training for improved bidirectional understanding and scalability. The interleaving strategy requires careful data curation but enables more diverse training than simple paired data. The two-stage approach adds training overhead but provides clear separation between reconstruction learning and contrastive alignment.

**Failure Signatures**
- Poor reconstruction performance on masked inputs indicates issues with bidirectional attention learning
- Degraded performance on out-of-distribution multimodal pairs suggests insufficient heterogeneous contrastive learning
- Inconsistent gains across model scales may indicate optimization or architectural issues

**First 3 Experiments**
1. Compare bidirectional vs causal attention performance on a simple multimodal understanding task
2. Ablation study isolating the contribution of masked autoencoding vs masked language modeling
3. Test heterogeneous contrastive learning on increasingly diverse multimodal pairs

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on English-language benchmarks limits assessment of multilingual capabilities
- Reliance on specific data curation strategies may not generalize to all domains
- Potential domain specificity of heterogeneous contrastive learning approach may limit broader applicability

## Confidence
- Benchmark performance claims: **High**
- Efficiency claims (30B tokens): **Medium**
- Scalability with model size: **Medium**
- Generalization to diverse multimodal tasks: **Low**

## Next Checks
1. Conduct ablation studies that isolate the contribution of modality-aware continual pre-training versus heterogeneous contrastive fine-tuning on downstream tasks
2. Test model performance on multilingual multimodal benchmarks to assess language generalization beyond English
3. Evaluate the model on truly out-of-distribution multimodal pairs (e.g., medical imaging with clinical notes, satellite imagery with geospatial data) not seen during pre-training