---
ver: rpa2
title: 'SRAS: A Lightweight Reinforcement Learning-based Document Selector for Edge-Native
  RAG Pipelines'
arxiv_id: '2601.01785'
source_url: https://arxiv.org/abs/2601.01785
tags:
- sras
- document
- reward
- bertscore
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SRAS, a lightweight reinforcement learning-based
  document selector for edge-native RAG pipelines. SRAS addresses the limitations
  of static top-k retrieval mechanisms by learning a compact policy that optimizes
  document selection for downstream generation quality under strict compute and latency
  constraints.
---

# SRAS: A Lightweight Reinforcement Learning-based Document Selector for Edge-Native RAG Pipelines

## Quick Facts
- arXiv ID: 2601.01785
- Source URL: https://arxiv.org/abs/2601.01785
- Reference count: 18
- Primary result: RL-based document selector achieving sub-1MB size and sub-1s CPU latency while matching larger models on QA accuracy

## Executive Summary
SRAS introduces a lightweight reinforcement learning approach for document selection in edge-native RAG pipelines, addressing the limitations of static top-k retrieval mechanisms. The method learns a compact policy using Proximal Policy Optimization that optimizes document selection for downstream generation quality under strict compute and latency constraints. By combining Relaxed F1 and BERTScore in a hybrid reward function, SRAS achieves competitive answer accuracy while maintaining ultra-lightweight properties suitable for on-device deployment. The work demonstrates that RL-based document selection can be made both efficient and effective for edge RAG systems.

## Method Summary
SRAS employs a compact neural scorer that projects query and document embeddings through learned weight matrices, combines them with a non-linearity, and produces selection scores. The method uses PPO for policy optimization with a hybrid reward combining Relaxed F1 and BERTScore metrics. Training incorporates supervised warmup, reward normalization, advantage estimation, and curriculum learning to stabilize learning. The approach operates on frozen MiniLM embeddings and selects top-k documents from candidate pools, optimizing for downstream QA quality under strict latency and memory constraints.

## Key Results
- Achieves sub-1MB model size and sub-1s CPU latency on Intel i5
- Competitive answer accuracy on synthetic QA benchmark and SQuAD v2 zero-shot test (BERTScore F1 of 0.8546)
- Outperforms both supervised and random selectors in edge RAG pipeline evaluation
- Demonstrates strong generalization without domain-specific tuning

## Why This Works (Mechanism)
The method works by learning a discriminative scoring function that captures complex relevance patterns beyond simple cosine similarity. Through RL optimization with a hybrid reward that balances precision (Relaxed F1) and semantic similarity (BERTScore), the policy learns to select documents that improve downstream answer quality. The curriculum learning approach gradually exposes the model to harder selection scenarios, while supervised warmup provides stable initialization. The compact architecture ensures edge compatibility without sacrificing selection quality.

## Foundational Learning
- **Reinforcement Learning with PPO**: Needed for learning document selection policies that optimize for downstream task performance rather than retrieval metrics. Quick check: Verify clipped objective implementation and advantage estimation.
- **Hybrid Reward Design**: Required to balance multiple quality metrics (precision vs. semantic similarity) in document selection. Quick check: Validate reward weighting (α=0.6) and normalization.
- **Curriculum Learning**: Essential for stable policy training by gradually increasing task difficulty. Quick check: Define difficulty metric and progression schedule.
- **Compact Neural Scoring**: Critical for edge deployment while maintaining selection quality. Quick check: Verify parameter count (~197K) and inference latency.

## Architecture Onboarding
**Component Map**: Corpus → Embedding Generation → Candidate Sampling → SRAS Scorer → Top-k Selection → Frozen T5 Generator → Answer Generation
**Critical Path**: Query/Document Embedding → SRAS Scoring → Top-k Selection → Answer Generation
**Design Tradeoffs**: Compact architecture (197K params) vs. selection quality; frozen embeddings vs. adaptability; RL optimization vs. training stability
**Failure Signatures**: Reward plateaus near 0.02-0.03 indicate reward shaping issues; high early variance suggests insufficient warmup; poor generalization indicates overfitting
**First Experiments**:
1. Verify embedding generation and candidate sampling pipeline
2. Test SRAS scorer with random weights on synthetic data
3. Validate hybrid reward computation with known inputs

## Open Questions the Paper Calls Out
- **Scalability to Real-World Corpus Sizes**: The paper restricts to 100 documents for tractability, acknowledging this may limit real-world scale. Benchmarking on MS MARCO or Natural Questions with larger candidate pools would resolve this.
- **End-to-End Integration**: Current retriever is decoupled and fixed; integrating SRAS into end-to-end retriever-generator frameworks remains future work. Joint training with a trainable retriever could improve selection but may violate edge constraints.
- **Static vs. Learned Performance**: Top-k Cosine baseline outperforms SRAS on Relaxed F1, raising questions about RL's value for certain metrics. Analysis of failure cases and evaluation on datasets where lexical overlap is less correlated with embeddings would help.
- **Embedded Hardware Validation**: Claims edge-suitability but only validates on desktop CPU. Deployment benchmarks on ARM-based devices measuring latency, memory, and power would confirm real-world applicability.

## Limitations
- Restricted to small corpus (100 documents) and candidate pools (n=8), limiting real-world scalability
- Only validated on desktop CPU, not actual embedded hardware
- Static retriever remains fixed rather than end-to-end trainable
- Performance gap with static methods on certain metrics (Relaxed F1)

## Confidence
- **High confidence**: Core RL methodology, model architecture specifications, deployment constraints
- **Medium confidence**: Synthetic data generation, zero-shot generalization, hybrid reward implementation
- **Medium confidence**: Curriculum learning schedule and supervised warmup hyperparameters

## Next Checks
1. Implement and validate the curriculum learning schedule with a defined difficulty metric
2. Systematically test supervised warmup duration impact on PPO stability
3. Conduct ablation studies on hybrid reward components to quantify individual contributions