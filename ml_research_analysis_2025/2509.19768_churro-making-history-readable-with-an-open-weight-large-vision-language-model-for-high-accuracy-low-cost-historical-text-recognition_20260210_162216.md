---
ver: rpa2
title: 'CHURRO: Making History Readable with an Open-Weight Large Vision-Language
  Model for High-Accuracy, Low-Cost Historical Text Recognition'
arxiv_id: '2509.19768'
source_url: https://arxiv.org/abs/2509.19768
tags:
- https
- license
- description
- link
- zenodo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CHURRO introduces a 3B-parameter open-weight vision-language model
  specialized for historical text recognition, trained on CHURRO-DS, the largest historical
  OCR dataset to date. The dataset unifies 155 corpora comprising 99,491 pages across
  46 language clusters spanning 22 centuries.
---

# CHURRO: Making History Readable with an Open-Weight Large Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition

## Quick Facts
- arXiv ID: 2509.19768
- Source URL: https://arxiv.org/abs/2509.19768
- Reference count: 40
- Primary result: 3B-parameter VLM trained on 99,491 pages achieves 82.3% normalized Levenshtein similarity on printed historical documents, outperforming larger models while being 15.5x more cost-effective

## Executive Summary
CHURRO is an open-weight vision-language model designed specifically for historical text recognition across 46 language clusters spanning 22 centuries. Trained on CHURRO-DS, the largest historical OCR dataset to date (99,491 pages from 155 corpora), CHURRO achieves state-of-the-art performance on historical documents while maintaining cost-effectiveness through its 3B parameter architecture. The model addresses the challenge of making historical documents readable by learning from diverse historical artifacts including degraded printed texts and handwritten manuscripts, achieving 82.3% normalized Levenshtein similarity on printed documents and 70.1% on handwritten ones.

## Method Summary
CHURRO uses supervised fine-tuning of the Qwen 2.5 VL (3B) base model on the CHURRO-DS dataset, which unifies 155 historical corpora with diplomatic transcriptions. The training procedure employs 32 NVIDIA H100 GPUs for approximately 25 hours, using 5 epochs with an effective batch size of 128 and learning rate of 5e-5 with cosine scheduling. Images are resized to fit within 2500×2500 pixels and limited to 5,120 visual patches (28×28 pixels each), with pages containing fewer than 30 tokens filtered out. The model generates page-level text directly from images without requiring explicit layout detection or bounding boxes.

## Key Results
- Achieves 82.3% normalized Levenshtein similarity on printed historical documents
- Achieves 70.1% normalized Levenshtein similarity on handwritten historical documents
- Outperforms second-best model (Gemini 2.5 Pro) by 1.4% and 6.5% respectively
- 15.5 times more cost-effective than larger commercial VLMs
- Significantly reduces reading order errors and hallucinations compared to baseline models

## Why This Works (Mechanism)

### Mechanism 1: Domain Alignment via Unified Historical Curation
General VLMs are biased toward modern, standardized layouts and clean text. By training on CHURRO-DS specifically curated for historical features—degradation, irregular spacing, and archaic typography—the model aligns its visual-textual mapping to historical document distributions. The performance gain is primarily driven by domain shift in training data distribution rather than architectural changes, assuming sufficient model capacity.

### Mechanism 2: End-to-End Sequence Learning vs. Pipeline Segmentation
Pipeline systems (Azure OCR) rely on explicit bounding box detection and reading order heuristics. If segmentation fails on complex layouts, text recognition fails. The VLM generates text auto-regressively, implicitly learning to resolve reading order and layout structure as part of sequence generation. The attention mechanism successfully attends to correct spatial regions in proper order during generation.

### Mechanism 3: "Diplomatic" Grounding Reduces Stereotype Hallucinations
Standard VLMs often prioritize semantic fluency or modernized text, leading them to "correct" historical spelling or generate contextually plausible but non-existent text. Training on strict diplomatic transcriptions forces the model to ground output strictly in visual signal rather than language priors. The visual features of degraded text are mapped to specific diplomatic characters.

## Foundational Learning

- **Vision-Language Models (VLMs)**: CHURRO uses VLM architecture merging visual encoder with language model. Essential to understand how image pixels transform into text tokens. *Quick check: Can you explain how a model processes an image of a page differently than plain text input?*

- **Supervised Fine-Tuning (SFT)**: Transition from base model to CHURRO via SFT on CHURRO-DS. Must understand SFT adapts general pre-trained model to specific task by minimizing loss over labeled pairs. *Quick check: How does updating weights on specific dataset differ from zero-shot inference used for baseline models?*

- **Levenshtein Similarity (CER)**: Paper uses Normalized Levenshtein Similarity as primary metric. Mathematically related to Character Error Rate. Crucial for interpreting "82.3% similarity" results as edit distance measure. *Quick check: If model outputs "aple" for ground truth "apple", how would Levenshtein distance be calculated?*

## Architecture Onboarding

- **Component map**: Image Resizing -> Base VLM (Qwen 2.5 VL) -> Text Generation -> Normalization
- **Critical path**: Data Unification -> Image Resizing -> Model Inference (Fine-tuned CHURRO) -> Text Normalization -> Metric Calculation
- **Design tradeoffs**:
  - Diplomatic vs. Modernized: Chooses faithful transcription, trading modern readability for historical accuracy and reduced hallucination
  - End-to-End vs. Hybrid: Rejects pipeline/hybrid approaches in favor of pure VLM, requiring high-quality page-level training data
  - Model Size: Chooses 3B model over larger ones (72B), prioritizing cost-efficiency (15.5x cheaper) and accessibility
- **Failure signatures**:
  1. Reading Order Reversal: Generating text in wrong column order in multi-column layouts
  2. Repetitive Degeneration: Getting stuck in loops generating same word/phrase when visual confidence low
  3. Stereotype Hallucination: Generating plausible context when text illegible
  4. Script Confusion: Defaulting to dominant script when processing transitional scripts
- **First 3 experiments**:
  1. Language Ablation: Evaluate performance on specific low-resource language cluster to determine under-fitting due to data scarcity
  2. Hybrid Comparison: Run "Azure OCR + Gemini" hybrid baseline on CHURRO failure cases to identify if explicit bounding boxes aid specific complex layouts
  3. Inference Thresholding: Test model with different confidence thresholds to see if "Repetitive Degeneration" can be mitigated without SFT retraining

## Open Questions the Paper Calls Out

1. **Advanced Training Methods**: Can reinforcement learning or semi-supervised learning improve performance over standard supervised fine-tuning? The paper notes future work should explore advanced training methods, as only standard SFT was experimented with.

2. **Hybrid Architecture Performance**: Why do hybrid OCR-VLM architectures consistently underperform compared to individual end-to-end models? The hybrid Azure OCR + Gemini system trails at least one component in every language despite theoretical benefits of task division.

3. **Extreme Data Scarcity Impact**: How does extreme training data scarcity (fewer than 10 samples) impact model's ability to generalize versus retaining pre-trained knowledge? Authors attribute low performance on Chinese printed text to only 6 training samples but don't quantify minimum data requirements for stable fine-tuning.

## Limitations

- Language-specific performance gaps exist, with printed Chinese showing notably low accuracy (2.7%) due to only 6 training samples
- 5,120 token limit constrains ability to process large historical documents, potentially forcing segmentation that breaks reading order context
- Dataset coverage gaps exist across historical scripts and languages, with certain rare scripts underrepresented

## Confidence

- **High Confidence**: CHURRO outperforms baseline VLMs on both printed (82.3%) and handwritten (70.1%) historical documents with statistically significant improvements
- **Medium Confidence**: 15.5× cost-effectiveness advantage based on computational efficiency assumptions and estimated inference costs for larger models
- **Medium Confidence**: End-to-end VLMs avoid error propagation claim supported by qualitative analysis but lacks systematic ablation studies
- **Low Confidence**: "None of the predictions from CHURRO exhibit hallucinations" based on manual inspection of only 50 samples

## Next Checks

1. **Language-Specific Ablation Study**: Systematically evaluate CHURRO's performance across language clusters with varying training sample sizes to quantify relationship between training data quantity and accuracy

2. **Long Document Processing Evaluation**: Test CHURRO's performance on documents requiring segmentation due to 5,120 token limit, measuring how reading order accuracy degrades when documents must be split

3. **Hallucination Detection Benchmark**: Implement automated hallucination detection metrics to systematically evaluate "no hallucinations" claim across larger sample set, comparing against baseline models using identical protocols