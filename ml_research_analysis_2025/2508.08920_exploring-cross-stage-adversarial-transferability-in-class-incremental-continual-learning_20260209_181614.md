---
ver: rpa2
title: Exploring Cross-Stage Adversarial Transferability in Class-Incremental Continual
  Learning
arxiv_id: '2508.08920'
source_url: https://arxiv.org/abs/2508.08920
tags:
- adversarial
- learning
- stage
- attacks
- transferability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the vulnerability of class-incremental continual
  learning (Class-IL) models to stage-transferred adversarial attacks, where adversarial
  examples crafted using earlier-stage models are effective against later-stage models.
  The study finds that Class-IL methods are highly susceptible to these attacks, with
  attack success rates comparable to direct attacks.
---

# Exploring Cross-Stage Adversarial Transferability in Class-Incremental Continual Learning

## Quick Facts
- arXiv ID: 2508.08920
- Source URL: https://arxiv.org/abs/2508.08920
- Reference count: 33
- Primary result: Class-IL models are highly vulnerable to stage-transferred adversarial attacks, with attack success rates comparable to direct attacks, and current adversarial training defenses are insufficient.

## Executive Summary
This paper investigates the vulnerability of class-incremental continual learning (Class-IL) models to stage-transferred adversarial attacks, where adversarial examples crafted using earlier-stage models are effective against later-stage models. The study finds that Class-IL methods are highly susceptible to these attacks, with attack success rates comparable to direct attacks. This vulnerability is attributed to model similarity between stages and gradual robustness degradation as models become more complex during incremental learning. While three state-of-the-art adversarial training-based defense strategies were evaluated, they proved insufficient in mitigating stage-transferred attacks. The findings highlight critical security concerns in Class-IL and underscore the need for specialized defense mechanisms.

## Method Summary
The study uses Split-MNIST (5 stages, 2 classes/stage) with MLP and Split-CIFAR100 (10 stages, 10 classes/stage) with ResNet-based architectures, all implemented using the Avalanche framework. Class-IL methods include iCaRL (replay + nearest-mean), GDumb (greedy replay), ER-ACE/ER-AML (experience replay with asymmetric losses). Attacks include FGSM (single-step), PGD (iterative), and AutoAttack (ensemble). Perturbation bounds are ε=0.3 for MNIST and ε=8/255 for CIFAR100. Model similarity is measured via cosine similarity and CKA, while model complexity is quantified using Lipschitz constant and Hessian spectral norm. Defenses tested include TABA, FLAIR, and FLAIR+.

## Key Results
- Stage-transferred attacks achieve ASR comparable to direct attacks, with success rates exceeding 0.5 even with defenses.
- Model similarity between stages increases as stage proximity increases, correlating positively with ASR.
- Later-stage models show increased vulnerability due to progressive decision boundary complexity, with asymmetric transferability (4→5 higher than 5→4).
- Current adversarial training-based defenses (TABA, FLAIR, FLAIR+) are insufficient to mitigate stage-transferred attacks.

## Why This Works (Mechanism)

### Mechanism 1: Cross-Stage Model Similarity Enables Perturbation Transfer
Adversarial examples transfer across stages because model parameters and representations remain partially similar through incremental updates. Class-IL methods update models iteratively rather than training from scratch, creating parameter and representation overlap between stages. Since adversarial perturbations exploit gradient directions that cross decision boundaries, shared model structure means shared vulnerable directions. Core assumption: Adversarial transferability correlates positively with model similarity. Evidence: Pearson correlation between CKA/cosine similarity and ASR ratio; similarity increases as stage proximity increases.

### Mechanism 2: Progressive Decision Boundary Complexity Degrades Robustness
Later-stage models become inherently more vulnerable to perturbations because decision boundaries must accommodate more classes, increasing local loss surface curvature. As new classes are added, the feature space requires finer-grained partitions, steepening loss gradients (higher Lipschitz constant) and increasing parameter-space curvature (higher Hessian spectral norm). Core assumption: Complexity metrics (Lipschitz constant, Hessian spectral norm) correlate with adversarial vulnerability. Evidence: Monotonic increase in both complexity metrics across stages; asymmetric transferability indicates robustness degradation.

### Mechanism 3: Adversarial Training Defenses Fail Under Catastrophic Forgetting Dynamics
Standard adversarial training methods provide incomplete protection because robustness learned at one stage is forgotten or diluted as new classes are added. Adversarial training embeds robust features into model parameters, but subsequent stage updates overwrite or interfere with these parameters. Core assumption: Robustness is subject to the same forgetting dynamics as standard accuracy in continual learning. Evidence: ASR remains high (~50% for FGSM even with defenses) despite TABA/FLAIR/FLAIR+.

## Foundational Learning

- **Concept: Adversarial Transferability**
  - Why needed: The entire paper builds on the premise that adversarial examples generated on one model can fool another. Without this, stage-transferred attacks wouldn't exist.
  - Quick check: Given two models trained on the same task with different initializations, would you expect FGSM attacks to transfer between them? Why or why not?

- **Concept: Class-Incremental Continual Learning (Class-IL)**
  - Why needed: The vulnerability is specific to the iterative, class-expanding training paradigm. Understanding how Class-IL differs from standard training is essential.
  - Quick check: In Class-IL with 5 stages adding 2 classes each, what information does the Stage 3 model have access to that the Stage 1 model does not? What constraints does it face?

- **Concept: Model Complexity Metrics (Lipschitz Constant, Hessian Spectral Norm)**
  - Why needed: These quantify why later stages are more vulnerable. The paper uses them to explain asymmetric transferability.
  - Quick check: If a model's Lipschitz constant doubles between Stage 1 and Stage 5, what does this imply about the sensitivity of its predictions to input perturbations?

## Architecture Onboarding

- **Component map:**
  Attack module (FGSM/PGD/AutoAttack) -> Class-IL methods (iCaRL/GDumb/ER-ACE/ER-AML) -> Evaluation pipeline (ASR computation) -> Defense layer (TABA/FLAIR/FLAIR+)

- **Critical path:**
  1. Train Class-IL model through all stages, saving checkpoints at each stage
  2. Generate adversarial examples using Stage a model (attacker)
  3. Evaluate ASR on Stage T model (target, typically final stage)
  4. Compare cross-stage ASR to direct attack ASR (attacker = target)

- **Design tradeoffs:**
  - Memory vs. robustness: Larger replay buffers improve accuracy but don't inherently reduce transferability
  - Defense cost: FLAIR+ adds RandAugment and distillation—higher compute, still insufficient protection
  - Attack strength vs. transferability: AutoAttack achieves highest direct ASR but shows more variance in transfer

- **Failure signatures:**
  - ASR > 0.5 for stage-transferred attacks even with defenses
  - Asymmetric transfer (4→5 ≠ 5→4) indicates robustness degradation, not just similarity
  - High direct ASR but low transferred ASR suggests low model similarity

- **First 3 experiments:**
  1. Train iCaRL on Split-CIFAR100, compute ASR for Stage 1→10, 5→10, 9→10 using PGD
  2. Modify iCaRL to use architectural separation (separate heads per stage), measure if cosine similarity drops and ASR decreases
  3. Apply FLAIR at only Stage 10 vs. all stages, compare ASR to determine if robustness forgetting is the failure mode

## Open Questions the Paper Calls Out

### Open Question 1
How can specialized defense mechanisms be developed to effectively mitigate stage-transferred attacks without compromising the plasticity of class-incremental learning models?
- Basis: The authors conclude that "existing adversarial training-based defense strategies are not sufficient" and explicitly state the "need for future research to develop effective defense mechanisms against stage-transferred attacks."
- Why unresolved: Tested TABA, FLAIR, and FLAIR+ none adequately reduced Attack Success Rates (ASR), indicating that current robustness techniques cannot handle the unique vulnerability of cross-stage transferability.
- What evidence would resolve it: A new defense strategy that significantly lowers ASR for stage-transferred attacks while maintaining classification accuracy on newly added classes comparable to undefended baselines.

### Open Question 2
How do stage-transferred adversarial vulnerabilities interact with other threat vectors, such as data poisoning or backdoor insertions, in a unified security framework?
- Basis: The conclusion states: "Future works should integrate these various attack scenarios into a unified evaluation framework and quantify how their interactions amplify cumulative vulnerabilities."
- Why unresolved: This study isolated evasion attacks (FGSM, PGD, AutoAttack) to analyze transferability, leaving the combined or cumulative effect of different attack types on continual learning models unexplored.
- What evidence would resolve it: An evaluation framework that measures the success rate and robustness degradation of Class-IL models when subjected to simultaneous stage-transferred evasion attacks and data poisoning.

### Open Question 3
Does the vulnerability to stage-transferred attacks persist if the adversary lacks knowledge of the specific classes added in each stage?
- Basis: The authors state: "We assume that the classes added in each stage are known to the adversary," establishing a strong threat model assumption regarding class labels.
- Why unresolved: It is unclear if the high transferability relies on the attacker knowing the specific class boundaries of the target stage, which might not be available in strict black-box scenarios.
- What evidence would resolve it: Experiments measuring ASR where adversarial perturbations are crafted using earlier-stage models without access to the labels or data distributions of the later incremental stages.

## Limitations
- Exact architectures (MLP layer sizes, ResNet configurations) and Class-IL hyperparameters (buffer size, learning rate, epochs) are not specified, relying on unlisted tuning decisions.
- The failure of adversarial training defenses is demonstrated but not deeply explained—the paper shows the symptom (high ASR) without fully characterizing the mechanism (parameter interference patterns, forgetting curves of robustness).
- The complexity-robustness degradation hypothesis lacks direct corpus support; it is theoretically plausible but not empirically established in the continual learning context.

## Confidence
- High: Cross-stage transferability exists and correlates with model similarity (supported by CKA/cosine similarity analysis).
- Medium: Progressive robustness degradation due to decision boundary complexity (supported by complexity metrics but no ablation studies).
- Medium: Adversarial training defenses fail due to forgetting dynamics (supported by empirical results but not mechanistic analysis).

## Next Checks
1. Modify iCaRL to use separate model components per stage (e.g., per-stage heads). Measure if similarity drops and ASR decreases accordingly.
2. Apply FLAIR only at final stage vs. all stages. Compare ASR to isolate whether robustness forgetting is the failure mode.
3. Apply explicit Lipschitz regularization during incremental learning. Measure if later-stage models maintain robustness and reduce asymmetric transferability.