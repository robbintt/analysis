---
ver: rpa2
title: 'Ravan: Multi-Head Low-Rank Adaptation for Federated Fine-Tuning'
arxiv_id: '2506.05568'
source_url: https://arxiv.org/abs/2506.05568
tags:
- clients
- lora
- parameter
- parameters
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Ravan introduces a multi-head LoRA method for federated fine-tuning
  that improves rank expressivity without increasing communication cost. By reparameterizing
  weight updates as a sum of low-rank heads (si Bi Hi Ai) with only core matrices
  Hi and scaling factors si trained, Ravan maintains exact aggregation while enabling
  clients with varying computational resources to fine-tune subsets of heads.
---

# Ravan: Multi-Head Low-Rank Adaptation for Federated Fine-Tuning

## Quick Facts
- arXiv ID: 2506.05568
- Source URL: https://arxiv.org/abs/2506.05568
- Reference count: 40
- Multi-head LoRA improves rank expressivity without increasing communication cost in federated fine-tuning

## Executive Summary
Ravan introduces a multi-head LoRA method for federated fine-tuning that improves rank expressivity without increasing communication cost. By reparameterizing weight updates as a sum of low-rank heads with only core matrices and scaling factors trained, Ravan maintains exact aggregation while enabling clients with varying computational resources to fine-tune subsets of heads. Experiments on vision and language tasks show Ravan improves test accuracy by 2-8% over prior parameter-efficient baselines, demonstrating robustness to both data and computational heterogeneity in federated settings.

## Method Summary
Ravan reparameterizes LoRA weight updates as a sum of multiple low-rank heads: $\sum_i s_i B_i H_i A_i$. The key innovation is freezing the base matrices $B_i$ and $A_i$ at initialization while training only the core matrices $H_i$ (rank×rank) and scaling factors $s_i$. This enables exact aggregation in federated learning since all clients share identical frozen bases. Clients can select subsets of heads based on scoring functions, allowing resource-constrained devices to participate without breaking the aggregation mechanism. The method leverages the sub-additivity of matrix rank to achieve higher effective rank within the same trainable parameter budget.

## Key Results
- Improves test accuracy by 2-8% over prior parameter-efficient federated fine-tuning baselines
- Maintains exact aggregation while enabling head subset selection for computational heterogeneity
- Shows robustness to both data heterogeneity (Dirichlet α=0.3) and skewed client compute distributions

## Why This Works (Mechanism)

### Mechanism 1: Multi-Head Rank Expansion via Sub-Additivity
Replacing a single LoRA head with multiple augmented heads increases effective rank within the same trainable parameter budget. With parameter budget N and h heads, each head has rank √(N/h). By the sub-additivity of matrix rank, the combined effective rank is bounded by √(Nh), which exceeds the single-head rank of √N by factor √h. This matters in FL because non-I.I.D. client data produces higher effective rank in true weight updates. Core assumption: frozen bases B_i and A_i span mutually orthogonal column and row spaces, ensuring heads capture distinct directions. Break condition: if bases are not orthogonal (e.g., shared subspace initialization), heads collapse into redundant directions and rank gains vanish.

### Mechanism 2: Exact Aggregation via Frozen Shared Bases
Freezing B and A matrices at initialization and training only H_i and s_i preserves exact aggregation during server averaging. Since all clients share identical frozen B_i and A_i, server aggregation satisfies: avg(Σ s_i B_i H_i A_i) = Σ B_i avg(s_i H_i) A_i. This avoids the inexactness problem of vanilla LoRA FL where avg(B) × avg(A) ≠ avg(BA). Core assumption: all clients initialize with identical B_i and A_i, and these matrices are never modified during training. Break condition: if any client modifies B_i or A_i during local training, exact aggregation is lost.

### Mechanism 3: Trainable Scaling Factors for Head Importance Weighting
Lightweight trainable scalars s_i allow optimization to emphasize useful heads and suppress less informative ones, improving accuracy without increasing communication. Each head contributes s_i B_i H_i A_i to the update. Gradient updates to s_i amplify or attenuate entire subspaces. At upload, s_i H_i is transmitted as a unit, preserving exactness while implicitly weighting heads during aggregation. Core assumption: not all orthogonal subspaces contribute equally to the downstream task. Break condition: if all heads contribute equally, scaling factors provide no benefit.

### Mechanism 4: Head Subset Selection for Computational Heterogeneity
Clients with limited resources can train only a subset of heads, selected via scoring functions, without breaking aggregation. At round start, client c evaluates scoring function ρ on each head (random, weight-magnitude, or gradient-magnitude). Top-K_c heads form training set H_c. Server aggregates each head only from clients who trained it. Core assumption: heads vary in importance; selecting higher-utility heads compensates for training fewer parameters. Break condition: if all clients select identical head subsets, heads become undertrained and aggregation variance increases.

## Foundational Learning

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: Ravan builds on LoRA's reparameterization of weight updates as BA. Understanding that B and A are trainable low-rank matrices (typically B∈R^{d×r}, A∈R^{r×d}) and that r≪d enables parameter-efficient training.
  - Quick check question: Given weight matrix W∈R^{1024×1024} and LoRA rank r=8, how many trainable parameters does standard LoRA introduce? (Answer: 2×1024×8=16,384)

- Concept: Federated Averaging (FedAvg) Aggregation
  - Why needed here: Ravan operates within FedAvg-style FL where clients perform local training and server aggregates updates. The inexactness problem arises because avg(B)×avg(A)≠avg(BA) in standard LoRA FL.
  - Quick check question: If 3 clients have LoRA updates B_1A_1=[1,2], B_2A_2=[3,4], B_3A_3=[5,6], what is the exact aggregated update? Does averaging B and A separately give the same result?

- Concept: Matrix Rank and Sub-Additivity
  - Why needed here: Ravan's key theoretical justification is that rank(Σ X_i) ≤ Σ rank(X_i). With h heads each of rank r, effective rank can reach h×r, but only if heads span different subspaces.
  - Quick check question: If matrices A and B each have rank 4 and share the same column space, what is rank(A+B)? What if their column spaces are orthogonal?

## Architecture Onboarding

- Component map:
  Server -> Broadcasts {H_i} -> Client -> Local training -> Uploads {s_i·H_i} -> Server -> Aggregates per-head

- Critical path:
  1. Orthogonal initialization of B, A (Gram-Schmidt or random normal in high dimensions)
  2. Per-round: server broadcasts H; clients select heads via scoring; local training; upload s×H
  3. Server aggregates per-head products; reinitializes all s_i=1 for next round

- Design tradeoffs:
  - Number of heads h: More heads increase √(Nh) effective rank but diminish individual head rank (√(N/h)). Saturation when √(Nh)≥d.
  - Initialization: Gram-Schmidt guarantees orthogonality but costs O(d²) at initialization (one-time). Random normal is cheap but only probabilistically orthogonal.
  - Scoring function: Gradient-based most robust to heterogeneity but requires extra forward pass. Random is free but no adaptivity. Weight-based may over-select same heads.
  - Aggregation: Uploading s_i H_i vs H_i alone. Trainable s_i helps (up to 2.3%) but adds negligible communication.

- Failure signatures:
  - Accuracy degrades to baseline LoRA levels: Check if B, A are accidentally unfrozen or modified locally
  - No improvement from multi-head: Verify orthogonal initialization; constant/shared subspaces cause collapse
  - Training instability or NaN: Scaling factors s_i may explode; consider gradient clipping or learning rate reduction for scalars
  - Aggregation appears inexact: Verify server aggregates s_i×H_i products, not H_i alone; reinitialize s_i=1 each round
  - Weak clients underperform severely: Head selection may be biased; try gradient-based or random scoring instead of weight-based

- First 3 experiments:
  1. Orthogonal vs shared-subspace initialization on CIFAR-100 with 20 non-I.I.D. clients: Implement both Gram-Schmidt and constant B_i=B_j, A_i=A_j initialization. Expected: orthogonal wins by 15-20%.
  2. Head count sweep at fixed parameter budget: Train with h∈{1,4,8,12,16} heads, measuring test accuracy and effective rank. Expected: accuracy peaks when √(Nh)≈d then saturates/declines.
  3. Computational heterogeneity robustness test: Assign clients trainable parameter budgets from skewed-right distribution (25% at 25%, 50% at 50%, 25% at 75% of max). Compare Ravan with gradient-based scoring vs FlexLoRA and HetLoRA baselines. Expected: Ravan loses <2% accuracy from homogeneous baseline; baselines lose >10%.

## Open Questions the Paper Calls Out

- Can cross-layer head selection improve performance by selecting different numbers of active heads for different layers?
- How does Ravan perform under differential privacy constraints, particularly regarding the interaction between gradient clipping/noise and the trainable scaling factors s_i?
- Can data-aware initialization of the frozen matrices B_i and A_i improve the effective rank and final accuracy beyond the current random or Gram-Schmidt methods?

## Limitations
- Sub-additivity rank expansion relies on strict orthogonality of B_i, A_i bases, which may degrade under random initialization in low dimensions
- The scaling factor mechanism lacks strong empirical justification beyond observed performance gains
- Computational heterogeneity results depend on specific skewed-right client distributions that may not generalize to other settings

## Confidence
- Mechanism 1 (Rank expansion): Medium - theoretical derivation is sound but empirical validation depends heavily on initialization quality
- Mechanism 2 (Exact aggregation): High - mathematical proof is straightforward and implementation follows naturally
- Mechanism 3 (Scaling factors): Low - performance improvements are observed but theoretical motivation is weak
- Mechanism 4 (Head selection): Medium - robustness shown across datasets but optimal scoring function varies

## Next Checks
1. Test orthogonal initialization robustness by varying random seed and measuring effective rank degradation
2. Conduct ablation study removing scaling factors to quantify their true contribution
3. Evaluate head selection mechanisms under uniform compute distribution to assess necessity claims