---
ver: rpa2
title: 'TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in
  Tabular Data'
arxiv_id: '2511.02219'
source_url: https://arxiv.org/abs/2511.02219
tags:
- table
- data
- answer
- question
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TabDSR tackles complex numerical reasoning over tabular data by
  decomposing questions into sub-questions, sanitizing noisy table content, and using
  program-of-thoughts to generate executable code for precise calculations. Experiments
  on TAT-QA, TableBench, and the new CalTab151 dataset show consistent state-of-the-art
  performance with 8.79%, 6.08%, and 19.87% accuracy improvements respectively.
---

# TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data

## Quick Facts
- arXiv ID: 2511.02219
- Source URL: https://arxiv.org/abs/2511.02219
- Reference count: 40
- Primary result: 8.79% accuracy improvement on TAT-QA, 6.08% on TableBench, 19.87% on CalTab151

## Executive Summary
TabDSR is a three-agent framework that addresses complex numerical reasoning over tabular data by decomposing questions, sanitizing noisy tables, and using program-of-thoughts to generate executable code. The framework achieves state-of-the-art performance across multiple benchmarks by breaking down multi-hop queries, normalizing table content, and offloading calculations to deterministic Python execution. A new CalTab151 dataset was constructed to provide a more reliable evaluation by mitigating data leakage concerns present in existing benchmarks.

## Method Summary
The framework employs three specialized agents: a Query Decomposer that splits complex questions into sub-questions using only textual cues, a Table Sanitizer that normalizes multi-level headers, removes non-numeric characters, and standardizes null values, and a PoT-based Reasoner that generates and executes pandas code for precise numerical calculations. The approach uses Qwen2.5-7B-Instruct with constrained decoding and operates on three datasets: TAT-QA (736 filtered examples), TableBench (493 samples), and the newly constructed CalTab151 (151 samples).

## Key Results
- 8.79% accuracy improvement over baselines on TAT-QA
- 6.08% accuracy improvement on TableBench
- 19.87% accuracy improvement on CalTab151
- Sanitizer reduces execution errors from 30.72% to 1.28% on noisy tables
- Executor shows 15-17% failure rate on code generation and execution

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing complex queries into sub-questions improves accuracy by reducing context complexity and enabling focused reasoning.
- **Mechanism:** Query Decomposer parses questions using conjunctions ("and," "or") and punctuation to split them into simpler sub-questions, reducing prompt length and attention dilution.
- **Core assumption:** LLM reasoning is bounded by context complexity; isolating sub-problems improves coverage and accuracy.
- **Evidence anchors:** Abstract states "query decomposer that breaks down complex questions"; section 3.1 describes feeding only the question to reduce prompt length; neighbor paper questions LLM robustness on tabular queries.

### Mechanism 2
- **Claim:** Sanitizing noisy tabular data prevents runtime errors and improves execution success rates.
- **Mechanism:** Table Sanitizer normalizes multi-level headers, removes non-numeric characters from numerical columns, and standardizes null values to produce clean, machine-readable DataFrames.
- **Core assumption:** LLM-generated code failures stem primarily from inconsistent data types and structural irregularities rather than algorithmic logic errors.
- **Evidence anchors:** Abstract mentions "sanitizing noisy table content"; section 3.2 details structural optimization and content cleaning; neighbor paper "TabularMath" underscores need for clean numerical inputs.

### Mechanism 3
- **Claim:** Using PoT approach with external code execution guarantees numerical precision.
- **Mechanism:** PoT-based Reasoner generates pandas code for computations and executes it in a deterministic environment, ensuring calculation accuracy.
- **Core assumption:** LLMs are better at generating code logic than performing arithmetic internally; deterministic executors guarantee precision.
- **Evidence anchors:** Abstract states "using program-of-thoughts to generate executable code for precise calculations"; section 3.3 describes loading data into pandas and generating code with format validation; neighbor papers discuss LLM reliability and need for externalized execution.

## Foundational Learning

- **Concept: Multi-hop Reasoning**
  - **Why needed here:** Understanding multi-hop reasoning is essential to grasp why decomposition improves accuracy for sequential or parallel reasoning steps.
  - **Quick check question:** Given "What is the average revenue and which year had the highest growth?", identify the two distinct sub-questions and their potential dependency.

- **Concept: Pandas DataFrame Operations**
  - **Why needed here:** Understanding pandas operations is crucial for auditing generated code and debugging execution failures in the Reasoner agent.
  - **Quick check question:** How would you filter rows where column 'Revenue' > 1000 and compute the mean of a 'Profit' column?

- **Concept: Prompt Engineering for Structured Output**
  - **Why needed here:** Each agent uses carefully designed prompts to enforce JSON or code output formats; understanding prompt constraints helps diagnose issues like JSONDecodeError or malformed code.
  - **Quick check question:** What prompt instruction would you add to ensure a model outputs a valid JSON object with keys 'subQueryCount' and 'subQueries'?

## Architecture Onboarding

- **Component map:** User Query & Raw Table → Decomposer → Sanitizer → PoT Reasoner → Code Execution → Final Answer

- **Critical path:** Query and table enter Decomposer (parallel with Sanitizer), producing sub-questions and sanitized table, which feed into PoT Reasoner for code generation and execution, yielding final answer.

- **Design tradeoffs:**
  - Decomposer excludes table context to reduce prompt length but may miss table-dependent cues
  - Sanitizer retains full table content for completeness, trading off context window efficiency
  - Single iteration limit on sanitizer reflection prevents infinite loops but may leave errors uncorrected

- **Failure signatures:**
  - Decomposer: Zero reported failures, but irrelevant sub-questions possible with ambiguous queries
  - Sanitizer: JSONDecodeError or ValueError (1-3% failure rate)
  - Executor: ValueError, KeyError, TypeError (15-17% failure rate)

- **First 3 experiments:**
  1. Run ablation study: Disable Decomposer and measure accuracy drop on CalTab151 to quantify decomposition contribution
  2. Inject controlled noise into tables and compare Sanitizer success rates with and without reflection loop
  3. Profile Executor failures on held-out set, classify error types, and test whether fallback logic improves accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can hybrid approaches incorporating table signals improve Query Decomposer performance compared to current context-agnostic method?
- **Basis in paper:** Explicit statement in Limitations section about exploring hybrid approaches that selectively incorporate table signals
- **Why unresolved:** Current design ignores table content to avoid distraction, occasionally degrading performance when questions rely heavily on table context
- **What evidence would resolve it:** Comparative study showing improved accuracy on complex tables using hybrid decomposer versus current TabDSR decomposer

### Open Question 2
- **Question:** What fallback mechanisms can effectively mitigate Table Sanitizer failures when it cannot repair tables?
- **Basis in paper:** Explicit statement in Limitations about implementing call monitoring and fallback mechanisms
- **Why unresolved:** Sanitizer relies on base model's reflection capabilities, which can fail on complex noise leading to redundant calls or unclean tables
- **What evidence would resolve it:** Reduction in Sanitizer's 1-3% failure rate through implementation of specific default resolution strategies

### Open Question 3
- **Question:** Can constrained decoding strategies or runtime feedback loops reduce the 15-17% failure rate of PoT-based Reasoner?
- **Basis in paper:** Inferred from Failure Analysis identifying Executor as most failure-prone module (15.1-17.2% error rate)
- **Why unresolved:** Framework lacks mechanism to correct common runtime errors during code generation, leading to high failure frequencies
- **What evidence would resolve it:** Experimental results showing decreased execution error counts when constrained decoding is applied to Reasoner agent

## Limitations
- Decomposer may produce incomplete sub-questions for queries with heavy table context dependency
- Sanitizer's single-iteration reflection loop may leave certain data irregularities uncorrected
- Executor shows significant failure rates (15-17%) due to code generation and execution errors

## Confidence

- **High:** Decomposition mechanism improves accuracy by reducing context complexity and isolating reasoning steps
- **Medium:** Sanitizer effectively prevents runtime errors, though single-iteration limit may leave edge cases unaddressed
- **Medium:** PoT approach guarantees numerical precision through deterministic execution, but code generation failures remain significant

## Next Checks

1. Run ablation study on CalTab151 comparing full TabDSR vs. Sanitizer+Reasoner-only performance to quantify decomposition contribution
2. Profile Executor failures on held-out test set to classify error types and test whether simple fallback logic improves accuracy
3. Inject controlled noise into tables (multi-level headers, mixed data types) and measure Sanitizer success rates with and without reflection loop to assess robustness limits