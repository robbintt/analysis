---
ver: rpa2
title: 'MHQA: A Diverse, Knowledge Intensive Mental Health Question Answering Challenge
  for Language Models'
arxiv_id: '2502.15418'
source_url: https://arxiv.org/abs/2502.15418
tags:
- question
- mental
- health
- dataset
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MHQA, a large-scale mental health question-answering
  dataset derived from PubMed abstracts, targeting four key domains: anxiety, depression,
  trauma, and obsessive-compulsive disorders. The dataset contains ~58.6k QA pairs
  with four answer choices each, including a gold-standard subset of 2,475 expert-verified
  instances.'
---

# MHQA: A Diverse, Knowledge Intensive Mental Health Question Answering Challenge for Language Models

## Quick Facts
- **arXiv ID:** 2502.15418
- **Source URL:** https://arxiv.org/abs/2502.15418
- **Reference count:** 12
- **Primary result:** Introduces MHQA, a 58.6k QA dataset from PubMed abstracts covering anxiety, depression, trauma, and OCD domains with expert-verified subset

## Executive Summary
MHQA is a large-scale mental health question-answering dataset containing ~58.6k QA pairs derived from PubMed abstracts spanning 2000-2024. The dataset covers four key mental health domains (anxiety, depression, trauma, OCD) and four question types (factoid, diagnostic, prognostic, preventive), with a gold-standard subset of 2,475 expert-verified instances. Using a rigorous LLM-based pipeline with post-hoc validation, MHQA ensures high-quality, medically grounded questions for evaluating language models' mental health knowledge. The dataset is designed to address the need for domain-specific evaluation benchmarks in mental health NLP.

## Method Summary
The dataset construction uses GPT-4o-mini to generate multiple-choice questions from PubMed abstracts, with each question having four answer choices. A post-hoc validation step filters generated questions based on similarity scores (≥0.7 threshold) to ensure quality. The MHQA-Gold subset contains 2,475 expert-verified QA pairs, while MHQA-B contains ~56.1k pseudo-labeled pairs. Evaluation uses F1 score and accuracy metrics across various models including GPT-4o, BioBERT, and Mental-BERT variants. The SFT approach formats each QA as four input sequences (question+[SEP]+option_i) with softmax over output tokens.

## Key Results
- GPT-4o achieves up to 79.8% F1 score on MHQA-Gold, outperforming smaller models
- BioBERT with SFT training reaches 81.0% F1, slightly exceeding GPT-4o's zero-shot performance
- Factoid questions show the lowest performance (~10-15% below prognostic/diagnostic) across all models
- Larger models like GPT-4o can degrade with few-shot CoT prompting compared to zero-shot

## Why This Works (Mechanism)
The dataset's effectiveness stems from its multi-domain coverage of core mental health conditions and diverse question types that test different reasoning capabilities. The expert-verified gold subset ensures reliable evaluation, while the larger pseudo-labeled set enables model training. The rigorous validation pipeline with similarity thresholds filters low-quality generations, and the PubMed source material provides medically grounded content. The multiple-choice format with four options creates a challenging evaluation setup that requires precise understanding rather than vague matching.

## Foundational Learning
- **Mental health domain knowledge**: Understanding anxiety, depression, trauma, and OCD symptoms, treatments, and research contexts - needed to interpret questions and evaluate model responses; quick check: verify model can distinguish between similar symptoms across different disorders
- **PubMed literature structure**: Familiarity with medical abstract formatting, terminology, and research conventions - needed to understand source material and question generation context; quick check: assess model's ability to extract key findings from abstract snippets
- **Question type reasoning**: Distinguishing factoid recall from diagnostic, prognostic, and preventive reasoning - needed to evaluate model's comprehensive mental health understanding; quick check: analyze per-type performance to identify reasoning gaps
- **LLM-based validation**: Understanding similarity-based filtering and quality control for generated content - needed to assess dataset reliability; quick check: examine distribution of similarity scores across validated vs. rejected questions
- **SFT methodology for QA**: Multi-sequence input formatting and softmax-based answer selection - needed to reproduce reported performance; quick check: verify classification head correctly handles four-option softmax

## Architecture Onboarding

**Component Map**: PubMed abstracts -> GPT-4o-mini generation -> Similarity validation -> MHQA-B/MHQA-Gold -> Zero-shot/Few-shot/SFT evaluation

**Critical Path**: Dataset construction (generation + validation) → Training/evaluation pipeline (SFT or prompting) → Performance measurement (F1/accuracy)

**Design Tradeoffs**: Expert verification provides quality but limits dataset size; pseudo-labeling enables scale but may introduce noise; multiple question types ensure diversity but complicate model training

**Failure Signatures**: Factoid questions consistently underperform (10-15% gap); few-shot CoT can degrade large model performance; pseudo-label noise may affect SFT convergence

**First Experiments**:
1. Load MHQA-Gold and evaluate GPT-4o with exact benchmarking prompt from Appendix C.3
2. Train BioBERT-base SFT on MHQA-B with 4-sequence input format for 5 epochs
3. Compare zero-shot vs few-shot CoT performance on Llama-3-8B using provided examples

## Open Questions the Paper Calls Out

**Open Question 1**: Why do language models perform significantly worse on factoid-type questions compared to prognostic and diagnostic questions in mental health QA? The authors observe performance gaps but don't investigate whether knowledge sparsity, domain-specific terminology, or model limitations cause the deficit.

**Open Question 2**: What is the causal mechanism behind performance degradation when applying few-shot CoT prompting to larger models like GPT-4o? The authors hypothesize bias from limited examples but provide no empirical validation of whether prompt-induced bias, overfitting, or interference with reasoning patterns causes the drop.

**Open Question 3**: Why do Mental-BERT and Mental-RoBERTa underperform compared to their base counterparts despite prior pretraining on mental health corpora? The paper identifies the gap but doesn't determine whether Reddit-based pretraining creates domain mismatch, introduces noise, or fails to capture clinical/medical grounding.

## Limitations
- Dataset construction relies on LLM-based generation and pseudo-labeling, potentially introducing systematic biases
- 0.7 similarity threshold for validation lacks theoretical justification for mental health QA contexts
- SFT methodology lacks critical hyperparameters (learning rate, batch size, optimizer settings)
- Focus on PubMed abstracts may overrepresent academic perspectives while underrepresenting diverse cultural contexts
- Proprietary API evaluation (GPT-4o, GPT-4o-mini) creates reproducibility barriers

## Confidence
- **High confidence**: Dataset creation methodology (PubMed sourcing, four-domain structure, four-question types, expert verification process for MHQA-Gold)
- **Medium confidence**: Benchmark results (F1 scores and accuracy metrics reported, but few-shot CoT examples not shared and SFT hyperparameters unspecified)
- **Medium confidence**: Dataset quality claims (post-hoc validation improves reliability, but validation set size is relatively small and criteria may not capture all generation errors)

## Next Checks
1. Conduct inter-annotator agreement analysis on a random sample of 100 MHQA-Gold questions to quantify expert validation reliability and identify systematic disagreement patterns
2. Analyze the dataset for demographic and cultural representation gaps by examining PubMed source distribution and evaluating whether certain mental health perspectives are systematically underrepresented
3. Test whether reported performance differences between zero-shot and few-shot CoT prompting persist across multiple small language models using exact few-shot examples to determine if the strategy is universally effective or model-specific