---
ver: rpa2
title: 'The Pitfalls of Growing Group Complexity: LLMs and Social Choice-Based Aggregation
  for Group Recommendations'
arxiv_id: '2505.05016'
source_url: https://arxiv.org/abs/2505.05016
tags:
- group
- llms
- complexity
- social
- aggregation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how well large language models (LLMs) can
  correctly apply social choice-based aggregation strategies in group recommender
  systems as group complexity increases. It tests four different LLMs on 1,000 randomly
  generated group scenarios ranging from 10 to 400 total ratings.
---

# The Pitfalls of Growing Group Complexity: LLMs and Social Choice-Based Aggregation for Group Recommendations

## Quick Facts
- arXiv ID: 2505.05016
- Source URL: https://arxiv.org/abs/2505.05016
- Authors: Cedric Waterschoot; Nava Tintarev; Francesco Barile
- Reference count: 40
- Primary result: LLM accuracy in social choice aggregation degrades sharply above 100 ratings, but In-Context Learning can mitigate this

## Executive Summary
This study investigates how well large language models (LLMs) can correctly apply social choice-based aggregation strategies in group recommender systems as group complexity increases. Testing four different LLMs on 1,000 randomly generated group scenarios ranging from 10 to 400 total ratings, the research finds that while LLMs perform well at lower complexity, accuracy drops sharply when considering more than 100 ratings. The study reveals that In-Context Learning significantly improves performance at higher complexity levels, while prompting for explanations or adding domain cues does not. Smaller models can achieve good results under the right conditions, supporting their use for efficiency and privacy. The study also shows that the formatting of group data (per-user vs. per-item) impacts LLM accuracy.

## Method Summary
The study employs a systematic experimental approach using four different LLMs tested across 1,000 randomly generated group scenarios with varying complexity levels (10 to 400 total ratings). The research evaluates social choice-based aggregation strategies through controlled conditions, comparing different prompting techniques including In-Context Learning, explanation prompts, and domain cue prompts. The experiments use binary classification to assess whether LLMs correctly identify the appropriate aggregation strategy for each scenario, allowing for quantitative analysis of performance degradation as complexity increases.

## Key Results
- LLM accuracy in social choice aggregation remains high at low complexity but drops sharply above 100 ratings
- In-Context Learning significantly improves performance at higher complexity levels
- Data formatting (per-user vs. per-item) impacts LLM accuracy in identifying correct aggregation strategies

## Why This Works (Mechanism)
The effectiveness of LLMs in social choice-based aggregation depends on their ability to process and reason about group preferences as complexity scales. At lower complexity levels, LLMs can effectively parse individual preferences and apply aggregation rules. However, as the number of ratings increases beyond 100, the models struggle to maintain accuracy due to increased cognitive load and potential context window limitations. In-Context Learning helps by providing concrete examples that guide the model's reasoning process, effectively scaffolding the complex decision-making required for accurate aggregation strategy selection.

## Foundational Learning
1. **Social Choice Theory** - Mathematical framework for aggregating individual preferences into collective decisions; needed to understand how group preferences should be combined
2. **Recommender Systems** - Algorithms that predict user preferences; needed as the application domain for group recommendation scenarios
3. **Large Language Models** - AI models trained on vast text data that can perform reasoning tasks; needed as the computational approach for implementing aggregation strategies
4. **In-Context Learning** - Technique where models learn from examples provided in the prompt; needed to improve performance without fine-tuning
5. **Prompt Engineering** - Designing effective input prompts to guide model behavior; needed to optimize LLM performance for specific tasks
6. **Complexity Scaling** - How system performance changes with increasing input size; needed to understand limitations of LLM approaches

## Architecture Onboarding
Component map: User Ratings -> LLM Model -> Aggregation Strategy -> Recommendation Output

Critical path: The LLM receives formatted group preference data, processes it through its reasoning capabilities, and outputs the selected aggregation strategy that determines the final recommendation.

Design tradeoffs: The study balances model size and privacy concerns against accuracy requirements, finding that smaller models can be effective with proper prompting techniques, though at the cost of potentially lower performance ceilings compared to larger models.

Failure signatures: Sharp accuracy drops above 100 ratings indicate context window limitations or processing bottlenecks; formatting sensitivity suggests importance of data representation in model performance.

First experiments:
1. Test LLM performance on synthetic group scenarios with varying complexity levels (10-400 ratings)
2. Compare In-Context Learning effectiveness against baseline prompting strategies
3. Evaluate the impact of data formatting (per-user vs. per-item) on aggregation accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic data may not capture real-world group dynamics and nuanced preferences
- Binary classification oversimplifies the complex nature of group recommendation quality
- Controlled environment may not reflect diverse prompting strategies and real-world constraints

## Confidence
High: LLM accuracy degradation above 100 ratings, In-Context Learning effectiveness
Medium: Prompting for explanations or domain cues not helping, smaller models' practical utility

## Next Checks
1. Test the same experimental setup with real-world group recommendation datasets to validate whether synthetic data patterns hold in practical scenarios
2. Evaluate model performance using a more granular success metric that captures degrees of recommendation quality rather than binary correctness
3. Conduct experiments with additional prompt engineering techniques and larger variety of group compositions to better understand the boundaries of effective prompting strategies