---
ver: rpa2
title: Riemannian Denoising Diffusion Probabilistic Models
arxiv_id: '2505.04338'
source_url: https://arxiv.org/abs/2505.04338
tags:
- where
- diffusion
- data
- process
- riemannian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Riemannian Denoising Diffusion Probabilistic
  Models (RDDPMs) for generative modeling on submanifolds, particularly those that
  are level sets of smooth functions. The method leverages a projection scheme from
  Monte Carlo sampling, allowing it to be applied to general manifolds without requiring
  extensive geometric information like geodesics or eigenfunctions of the Laplace-Beltrami
  operator.
---

# Riemannian Denoising Diffusion Probabilistic Models

## Quick Facts
- arXiv ID: 2505.04338
- Source URL: https://arxiv.org/abs/2505.04338
- Reference count: 40
- Primary result: Introduces RDDPMs for generative modeling on submanifolds using projection-based diffusion steps that only require first-order derivatives

## Executive Summary
This paper introduces Riemannian Denoising Diffusion Probabilistic Models (RDDPMs) for generative modeling on submanifolds, particularly those that are level sets of smooth functions. The method leverages a projection scheme from Monte Carlo sampling, allowing it to be applied to general manifolds without requiring extensive geometric information like geodesics or eigenfunctions of the Laplace-Beltrami operator. The method only needs the value and first-order derivatives of the defining function of the submanifold.

## Method Summary
The method defines a diffusion process on submanifolds by projecting steps from the ambient Euclidean space back onto the manifold using Newton's method to solve the constraint equation. The projection scheme creates a bijection between Gaussian tangent vectors and manifold points, enabling tractable density calculations for variational training. In the continuous-time limit, the method converges to a score-matching objective, establishing a theoretical connection to Riemannian score-based generative models while maintaining practical computational advantages.

## Key Results
- Achieves competitive NLL results: -3.57 ±1.05 for volcano dataset and 0.77 ± 0.00 for Spot the Cow dataset with k=100
- Successfully demonstrates on high-dimensional manifolds including SO(10) and alanine dipeptide configuration space
- Provides a solution to isolated data points by including them in training, improving NLL values
- Shows the method only requires first-order derivatives of the defining function, unlike prior approaches

## Why This Works (Mechanism)

### Mechanism 1: Projection-Based Constraint Solving
Enables diffusion steps to remain on the manifold without requiring geodesic curves by proposing steps in ambient space and solving constraint equation $\xi(y)=0$ via Newton's method to project back onto manifold.

### Mechanism 2: Explicit Transition Densities via Change of Variables
Allows tractable training without heat kernel approximations by calculating exact transition density $q(y|x)$ using the Jacobian determinant of the projection map.

### Mechanism 3: Equivalence to Score Matching in Continuous Limit
The discrete variational loss converges to a Riemannian score-matching objective, linking RDDPMs to RSGMs as the number of discretization steps increases.

## Foundational Learning

- **Level Sets & Submanifolds**: Data lies on $M = \{x \in \mathbb{R}^n | \xi(x) = 0\}$. You must define constraint function $\xi$ for your specific data geometry.
  - Quick check: For a torus in $\mathbb{R}^3$, what is output dimension of $\xi$? (Answer: 1)

- **Tangent Spaces & Projection**: Noise $v$ must be sampled in tangent space $T_xM$ before projection.
  - Quick check: How is projection matrix $P(x)$ calculated using only $\nabla\xi$? (Answer: $P = I - \nabla\xi (\nabla\xi^\top \nabla\xi)^{-1} \nabla\xi^\top$)

- **Variational Inference (ELBO)**: Training objective minimizes KL divergence between forward and reverse processes.
  - Quick check: Does maximizing ELBO directly minimize NLL? (Answer: No, it minimizes upper bound)

## Architecture Onboarding

- **Component map**: Manifold Interface -> Forward Sampler -> Denoiser ($s_\theta$) -> Trainer

- **Critical path**: 
  1. Pre-processing: Define $\xi$ and verify $\nabla\xi$ rank
  2. Trajectory Generation: Sample dataset → Run Forward Process → Store trajectories (Update every $l_f$ epochs)
  3. Training: Minimize $\mathbb{E}[||G(x_k, x_{k+1}) - s_\theta||^2]$
  4. Inference: Sample prior → Run Reverse Process (Newton step at every denoising step)

- **Design tradeoffs**: 
  - Trajectory Update Frequency ($l_f$): Every epoch is accurate but slow (67% time in simulation for Volcano); less frequent ($l_f=100$) speeds training but risks overfitting
  - Step Size ($\sigma_k$): Large steps speed diffusion but increase Newton solver failure rate

- **Failure signatures**: 
  - "NaN" or divergence: Likely Newton solver failure or exploding gradients
  - High failure rate: Step size too large for local curvature
  - Mode collapse: If $l_f$ too large, overfitting to early noise trajectories

- **First 3 experiments**: 
  1. Sphere ($S^2$): Implement $\xi(x) = |x| - 1$ with closed-form solution
  2. Simple Mesh (Bunny): Train network to learn $\xi$ first, then run RDDPM
  3. SO(3) Rotation: Test on matrix manifold $S^\top S = I$

## Open Questions the Paper Calls Out

1. What is the optimal protocol for evaluating generative models on manifolds with sparse data distributions, specifically regarding the handling of isolated data points?

2. What are the theoretical non-asymptotic error bounds for the RDDPM algorithm when using a finite number of discretization steps $N$?

3. How robust is the projection scheme to approximation errors in the constraint function $\xi$ and its gradient $\nabla \xi$, particularly when $\xi$ is represented by a learned neural network?

## Limitations

- Geometric approximations may deviate from true manifold diffusion, especially on high-curvature manifolds
- Computational scalability issues with high-dimensional manifolds due to expensive Newton solver
- Isolated point handling through training inclusion is a workaround that may not generalize to truly out-of-manifold samples

## Confidence

- **High**: Variational bound derivation and continuous-time limit connection are mathematically rigorous
- **Medium**: Experimental results are reproducible but depend on unspecified binning strategy for isolated points
- **Low**: Claims of competitive NLL results lack extensive comparative analysis and ablation studies

## Next Checks

1. For sphere experiment, compare projected trajectories from Algorithm 3 against ground-truth geodesic Brownian motion to quantify approximation quality

2. Systematically vary $l_f$ (trajectory update frequency) and $\gamma_{max}$ (maximum step size) on volcano dataset to identify Pareto-optimal tradeoff curve

3. Create synthetic datasets with varying proportions of isolated points (0%, 5%, 10%) and measure NLL and sample quality with/without including points in training