---
ver: rpa2
title: 'Analyzing Uncertainty of LLM-as-a-Judge: Interval Evaluations with Conformal
  Prediction'
arxiv_id: '2509.18658'
source_url: https://arxiv.org/abs/2509.18658
tags:
- interval
- score
- prediction
- conformal
- coverage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work quantifies the uncertainty of LLM-as-a-judge in rating-based
  evaluations by constructing prediction intervals using conformal prediction. The
  proposed framework uses LLM output logits as input, applies boundary adjustment
  to align intervals with discrete rating scales, and leverages interval midpoints
  as calibrated scores.
---

# Analyzing Uncertainty of LLM-as-a-Judge: Interval Evaluations with Conformal Prediction

## Quick Facts
- arXiv ID: 2509.18658
- Source URL: https://arxiv.org/abs/2509.18658
- Reference count: 40
- Primary result: Conformal prediction constructs prediction intervals for LLM judges with target 90% coverage, where interval midpoints serve as less biased calibrated scores than raw outputs.

## Executive Summary
This work quantifies uncertainty in LLM-as-a-judge rating-based evaluations by constructing prediction intervals using conformal prediction. The framework extracts LLM output logits for rating tokens, applies boundary adjustment to align intervals with discrete rating scales, and uses interval midpoints as calibrated scores. Experiments across summarization and reasoning tasks demonstrate that conformal prediction methods achieve coverage rates close to the target 90% after boundary adjustment, with R2CCP providing the best balance between coverage and interval width. The results show that LLM judges can reliably express uncertainty, offering a practical reference for trustworthy automated evaluation.

## Method Summary
The framework uses conformal prediction to construct prediction intervals for LLM-as-a-judge evaluations. It extracts token-level logits for rating tokens (e.g., 1-5) from LLM outputs, applies non-conformity score functions on a calibration set, and computes quantiles to construct intervals. Boundary adjustment transforms continuous intervals to discrete ones aligned with rating scales, and the midpoint of the interval serves as a calibrated score. The method uses a 50/50 train/test split with 30 random seeds, applying various conformal prediction methods including R2CCP, LVD, and CQR across summarization and reasoning benchmarks.

## Key Results
- Conformal prediction methods achieve coverage rates close to target 90% after boundary adjustment
- R2CCP provides the best balance between coverage and interval width among tested methods
- Interval midpoints reduce MSE by up to 88.7% compared to raw LLM scores
- Boundary adjustment consistently improves coverage rates across all tasks and methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Conformal prediction transforms raw LLM logits into prediction intervals with statistically guaranteed coverage rates.
- **Mechanism:** The framework extracts token-level logits for rating tokens, applies non-conformity score functions on calibration data, computes quantiles, and constructs intervals with formal coverage guarantees under exchangeability assumptions.
- **Core assumption:** Calibration and test samples are exchangeable.
- **Evidence anchors:** Coverage guarantee proof in Section 3.1; empirical validation showing rates close to 90% target.
- **Break condition:** Coverage degrades under distribution shift or small calibration sets.

### Mechanism 2
- **Claim:** Boundary adjustment aligns continuous prediction intervals with discrete rating scales while preserving or improving coverage.
- **Mechanism:** Continuous intervals [l, u] are transformed to discrete intervals via rounding rules, redefining non-conformity scores for integer labels only.
- **Core assumption:** Rating scales are discrete and ordinal.
- **Evidence anchors:** Theorem 1 proves non-decreasing coverage; Table 2 shows consistent coverage improvements after adjustment.
- **Break condition:** Fails with non-integer rating scales that cannot map to integers.

### Mechanism 3
- **Claim:** The interval midpoint serves as a less biased estimate of ground-truth ratings compared to raw LLM scores.
- **Mechanism:** Under uniform or symmetric distribution assumptions within intervals, the midpoint is the best linear unbiased estimator.
- **Core assumption:** Ground-truth distribution within intervals is approximately symmetric or uniform.
- **Evidence anchors:** Table 3 shows 40-80% MSE reduction; empirical validation across multiple tasks.
- **Break condition:** Biased if true distribution within interval is highly skewed.

## Foundational Learning

- **Conformal Prediction Basics**
  - Why needed here: The entire framework rests on split conformal prediction's coverage guarantees and exchangeability assumptions.
  - Quick check question: Given a calibration set of 100 samples with non-conformity scores, how do you compute the 90% coverage threshold q̂?

- **Logits vs. Probabilities in LLMs**
  - Why needed here: The framework uses token logits (not probabilities) as features for regression-based CP methods due to multicollinearity issues with probabilities.
  - Quick check question: Why would token probabilities be unsuitable for regression-based conformal prediction methods?

- **Ordinal vs. Nominal Classification**
  - Why needed here: Ratings are ordinal, so prediction intervals must respect order. Standard classification CP methods produce unordered sets that are uninterpretable for ratings.
  - Quick check question: Why is a prediction set {1, 5} problematic for a 5-point Likert scale evaluation?

## Architecture Onboarding

- **Component map:** LLM Judge Module -> Logit Extractor -> Calibration Engine -> Interval Constructor -> Boundary Adjuster -> Midpoint Calculator
- **Critical path:** Collect calibration data with human ratings → run LLM judge → extract logits → fit CP method → construct interval → adjust boundaries → output midpoint
- **Design tradeoffs:** R2CCP vs. LVD vs. CQR (coverage vs. width); calibration set size (stability vs. efficiency); boundary adjustment strictness (coverage vs. continuous outputs)
- **Failure signatures:** Coverage < 85% (calibration issues); width > 3 (uncertainty/heteroscedasticity); ordinal methods fail (irregular distributions)
- **First 3 experiments:**
  1. Replicate SummEval results with GPT-4o mini + R2CCP; target coverage ≥90%, width ~1-3
  2. Vary calibration set size (25%-100%) on subset; plot coverage vs. size convergence
  3. Compare midpoint vs. raw score MSE/MAE on held-out test set; expect 40-80% MSE reduction

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed framework maintain valid coverage guarantees when applied to evaluation tasks beyond summarization and reasoning, such as machine translation or multimodal generation? The study restricts validation to SummEval, DialSumm, and ROSCOE benchmarks, leaving behavior in other domains untested.

### Open Question 2
To what extent can uncertainty estimates (interval width) be utilized to improve downstream processes like reinforcement learning from AI feedback or active learning? The paper focuses on quantifying uncertainty rather than consuming it for training loops.

### Open Question 3
Does enabling continuous or non-integer scoring in reprompting enhance the judge's ability to self-correct compared to discrete Likert-scale constraints? Current experiments force integer-only outputs during reprompting, with limited analysis of continuous outputs.

## Limitations
- Relies on exchangeability between calibration and test data, which may fail under domain shift
- Coverage guarantees are distribution-free but not robust to covariate shift
- Boundary adjustment improves coverage but increases interval width, creating fundamental trade-offs
- Midpoint heuristic assumes symmetric uncertainty within intervals, which may not hold

## Confidence

- **High confidence** in core conformal prediction mechanism (formal coverage proofs and empirical validation)
- **Medium confidence** in boundary adjustment benefits (consistent empirical improvements but theoretical assumptions may not fully apply)
- **Medium confidence** in midpoint as calibrated score (strong empirical performance but relies on heuristic symmetry assumptions)

## Next Checks

1. **Distribution shift robustness**: Run conformal prediction on out-of-distribution test sets to measure coverage degradation; compare R2CCP vs. LVD performance under shift.

2. **Calibration set sensitivity**: Systematically vary calibration set size from 10% to 90% of available data; measure coverage convergence and midpoint calibration quality; identify minimum viable calibration size.

3. **Ordinal vs. nominal validation**: Compare ordinal CP methods against nominal methods on datasets with known rating distribution shapes (symmetric vs. skewed) to validate claims about ordinal method instability.