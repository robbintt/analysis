---
ver: rpa2
title: 'Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners'
arxiv_id: '2502.20339'
source_url: https://arxiv.org/abs/2502.20339
tags:
- distilled
- time
- https
- inference
- compute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether lower-complexity models can outperform
  Transformer-based models in reasoning tasks by leveraging faster generation speeds.
  The authors distill pure Mamba and hybrid Mamba-in-Llama models from Llama teacher
  models to enable efficient inference for test-time compute scaling.
---

# Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners

## Quick Facts
- **arXiv ID**: 2502.20339
- **Source URL**: https://arxiv.org/abs/2502.20339
- **Reference count**: 35
- **One-line primary result**: Subquadratic Mamba models distilled from Transformers can match or exceed teacher performance in reasoning tasks when optimized for inference speed, achieving 2.8-4.2× faster generation with 8B tokens of training data.

## Executive Summary
This paper demonstrates that lower-complexity subquadratic architectures (pure Mamba and hybrid Mamba-Transformer models) can outperform Transformer-based models in mathematical reasoning tasks by leveraging faster inference speeds. The authors develop specialized distillation techniques to transfer reasoning capabilities from Llama teacher models to Mamba student models, enabling efficient test-time compute scaling. Under fixed time budgets, these distilled models generate more completions, achieving better coverage and accuracy than their Transformer counterparts despite having lower per-sample accuracy.

## Method Summary
The authors distill Llama-3.2-1B/3B-Instruct models into two types of subquadratic architectures: pure Mamba (Llamba) using a modified 3-stage MOHAWK approach, and hybrid Mamba-Transformer (MambaInLlama) using attention-to-Mamba replacement with reverse KL fine-tuning. Training uses 8B tokens total (4B FineMath-4+ and 4B OpenMathInstruct-2). Evaluation focuses on MATH-500 and GSM8K-500 benchmarks, measuring coverage (pass@k), majority voting accuracy, and weighted Best-of-N accuracy with a process reward model verifier. The approach exploits Mamba's constant memory property to achieve 2.6-4.2× faster generation at large batch sizes, enabling more completions under fixed time budgets.

## Key Results
- Distilled Mamba models achieve 2.8-4.2× faster token generation than Transformer counterparts while matching or exceeding accuracy
- Under fixed time budgets, distilled models generate more completions, improving pass@k coverage and overall accuracy
- Pure and hybrid Mamba models outperform Llama-1B on coverage and accuracy while being faster
- Larger distilled models (3B scale) outperform smaller Transformers on the Pareto front
- Performance is highly sensitive to data selection, with >10pp accuracy difference between OpenMathInstruct-2 and OpenHermes-2.5

## Why This Works (Mechanism)

### Mechanism 1: Throughput-Compensated Coverage Scaling
- **Claim**: Subquadratic models achieve superior coverage under fixed time budgets by generating more completions faster despite lower per-sample accuracy
- **Mechanism**: Mamba architectures maintain constant memory during inference vs. linear KV cache in Transformers, enabling 2.6-4.2× faster generation at large batch sizes
- **Core assumption**: Memory bandwidth, not compute, is the dominant bottleneck for batched generation with long sequences
- **Evidence anchors**: 2.5× less time to reach same performance; MambaInLlama-3B achieves 3.7× speedup at batch=256; Llama-3B OOMs at batch=512 while distilled models succeed

### Mechanism 2: Cross-Architecture Reasoning Distillation
- **Claim**: Mathematical reasoning capabilities can be transferred from Transformer teachers to subquadratic students with controlled degradation
- **Mechanism**: Modified MOHAWK for pure Mamba (3 stages: matrix orientation, hidden state alignment, knowledge distillation) and MambaInLlama for hybrids (reuse Q,K,V,O projections, train only Δ and dynamic A, reverse KL loss)
- **Core assumption**: Reasoning patterns are not intrinsically tied to quadratic attention; SSMs can approximate relevant state dependencies
- **Evidence anchors**: Stage-wise training with matrix orientation and hidden alignment; reverse KL behaves more like mode-seeking and better mimics peak values

### Mechanism 3: Aggregation Strategy Selection
- **Claim**: Weighted Best-of-N with process reward models outperforms majority voting for accuracy when scaling completions
- **Mechanism**: PRMs provide step-level scores that aggregate to solution-level scores; weighted Best-of-N sums rewards across samples sharing same answer, selecting highest combined score
- **Core assumption**: PRM generalizes well and correctly identifies correct reasoning steps; correct answers cluster in high-reward regions
- **Evidence anchors**: Weighted Best-of-N often superior to Best-of-N, identifying highly-rated but common solutions; weighted RM accuracy curves show distilled models dominating Pareto front

## Foundational Learning

- **Concept: Subquadratic architectures (SSMs/Mamba)**
  - **Why needed**: Understanding why these models are faster—selective state spaces maintain constant-size hidden states vs. growing KV cache; requires grasping RNN-style sequential processing with learned state dynamics
  - **Quick check**: Can you explain why Mamba's inference memory is O(1) in sequence length while Transformers are O(n)?

- **Concept: Knowledge distillation (logit matching, KL divergence)**
  - **Why needed**: The paper uses both forward and reverse KL; understanding mode-covering vs. mode-seeking behavior explains why reverse KL works better here
  - **Quick check**: Why would reverse KL (student || teacher) produce more peaked distributions than forward KL (teacher || student)?

- **Concept: Test-time compute scaling (pass@k, Best-of-N, majority voting)**
  - **Why needed**: Coverage vs. accuracy distinction is central; pass@k measures probability of at least one correct answer, while majority voting requires consensus
  - **Quick check**: For a model with 30% single-sample accuracy, how does pass@8 compare to pass@1?

## Architecture Onboarding

- **Component map**: Llama-3.2-1B/3B-Instruct -> Llamba-1B/4B (pure Mamba-2), MambaInLlama-1B/3B (hybrid) -> MATH-500, GSM8K-500 benchmarks
- **Critical path**: Data preparation (chat template, packing to 8192 context) -> Distillation (pure Mamba: 3-stage MOHAWK; hybrid: single-round weight initialization + reverse KL) -> Optional SFT post-distillation -> Evaluation with multiple temperatures, sample N≥k completions, compute pass@k and accuracy metrics
- **Design tradeoffs**: Pure Mamba (Llamba) vs. hybrid (MambaInLlama): pure is slightly faster due to smaller SSM state; hybrid achieves better accuracy Pareto front; Data selection: OpenMathInstruct-2 vs. OpenHermes-2.5 caused >10 percentage point MATH accuracy difference; Reverse KL vs. forward KL: reverse KL better for reasoning distillation (mode-seeking)
- **Failure signatures**: Low coverage per k despite fast generation → distillation quality issue, check data quality and KL convergence; OOM at moderate batch sizes → verify Mamba implementation, check state size configuration; Majority voting accuracy plateaus while coverage grows → model lacks self-consistency, may need PRM guidance
- **First 3 experiments**:
  1. **Benchmark inference speed**: Measure generation time for 512 tokens from 512-token prompt across batch sizes [1, 32, 128, 256]; confirm 3×+ speedup and identify OOM threshold for baselines
  2. **Coverage scaling validation**: Sample N=256 completions per problem on MATH-500 subset; plot pass@k vs. k for k∈[1, 8, 32, 128, 256]; verify distilled models match teacher coverage at high k
  3. **Ablate distillation data**: Train Llamba-1B with OpenHermes-2.5 vs. OpenMathInstruct-2 for Stage 3; measure MATH acc@1 delta to quantify data sensitivity

## Open Questions the Paper Calls Out

- **Can test-time compute scaling strategies be effectively applied to subjective or conversational tasks?**: The paper states it's not yet clear how to evaluate and exploit test-time compute for more subjective, conversational tasks, as current evaluations rely on verifiable ground truths (MATH, GSM8K) that are difficult to define for open-ended, subjective outputs

- **How can distillation techniques be standardized to be less sensitive to specific data mixtures?**: Performance remains highly sensitive to both data and distillation techniques, with a 10-point drop in MATH accuracy simply by switching the Stage 3 dataset, without identifying precise data attributes necessary for robust reasoning transfer

- **How would the performance Pareto front shift if Transformers utilized advanced memory-management optimizations?**: The paper notes that optimizations like PagedAttention (vLLM) exist for Transformers to enable faster generation but were not included in the baseline comparison, leaving open how architectural advantages would compare under production-like conditions

## Limitations

- Knowledge distillation from Transformers to subquadratic models is empirically validated but mechanistically incomplete, with theoretical basis for why SSMs can approximate attention-based reasoning remaining unclear
- Inference speedup claims depend heavily on specific serving conditions, with real-world deployment potentially showing different performance profiles depending on KV cache optimization strategies
- Process reward model quality critically affects final accuracy but is not thoroughly validated, with PRM accuracy, calibration, and generalization across problem types not extensively characterized

## Confidence

**High Confidence**: Inference speed comparisons between distilled Mamba models and Transformer baselines are directly measurable and reproducible, supported by systematic benchmarking across batch sizes and the architecturally fundamental constant memory property of SSMs during inference

**Medium Confidence**: The distillation methodology produces measurable improvements in reasoning accuracy and coverage with clear performance gains over smaller Transformer models, though the optimal distillation recipe appears sensitive to hyperparameter choices not fully explored

**Low Confidence**: Claims about fundamental architectural advantages of subquadratic models for reasoning tasks extend beyond empirical results, as the paper shows these models can match Transformer performance when optimized for inference speed but does not establish that SSMs are intrinsically better suited for reasoning than attention mechanisms

## Next Checks

- **Benchmark against optimized Transformer serving**: Reproduce inference speed comparisons using state-of-the-art Transformer serving optimizations (PagedAttention, continuous batching, speculative decoding) to determine if Mamba advantage persists under production-like conditions

- **Ablate process reward model quality**: Train and evaluate with PRMs of varying quality levels (different model sizes, different training data) to quantify how sensitive weighted Best-of-N accuracy gains are to PRM reliability

- **Test architectural generalization beyond mathematics**: Apply the same distillation methodology to reasoning tasks in different domains (code, commonsense reasoning, multi-step planning) to determine if Transformer-to-Mamba transfer capability is task-specific or represents broader architectural compatibility