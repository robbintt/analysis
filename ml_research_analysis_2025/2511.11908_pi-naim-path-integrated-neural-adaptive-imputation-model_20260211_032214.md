---
ver: rpa2
title: 'PI-NAIM: Path-Integrated Neural Adaptive Imputation Model'
arxiv_id: '2511.11908'
source_url: https://arxiv.org/abs/2511.11908
tags:
- imputation
- data
- missing
- task
- missingness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PI-NAIM introduces a dynamic dual-path architecture for handling
  missing data in multimodal learning, addressing the efficiency-expressiveness trade-off
  in existing imputation methods. It routes samples to either a statistical MICE branch
  (for low missingness complexity) or a deep GAIN-based branch (for high missingness
  complexity), determined by a learned Missingness Rate criterion.
---

# PI-NAIM: Path-Integrated Neural Adaptive Imputation Model

## Quick Facts
- **arXiv ID**: 2511.11908
- **Source URL**: https://arxiv.org/abs/2511.11908
- **Reference count**: 40
- **Primary result**: Achieves state-of-the-art imputation RMSE of 0.108, outperforming baselines by 28-29%

## Executive Summary
PI-NAIM introduces a dynamic dual-path architecture for handling missing data in multimodal learning, addressing the efficiency-expressiveness trade-off in existing imputation methods. It routes samples to either a statistical MICE branch (for low missingness complexity) or a deep GAIN-based branch (for high missingness complexity), determined by a learned Missingness Rate criterion. Cross-path attention fusion and task-supervised adaptive weighting are used to combine outputs effectively. Evaluated on MIMIC-III and CIFAR-10/100 benchmarks, PI-NAIM achieves state-of-the-art imputation accuracy with RMSE of 0.108, outperforming baselines (0.119–0.152). It also improves downstream task performance, notably achieving AUROC of 0.812 for mortality prediction.

## Method Summary
PI-NAIM implements a dynamic dual-path architecture that routes data based on missingness complexity. The method uses a Gating Network to evaluate Missingness Rate (MR) and route samples: if MR < 0.2, data flows to the efficient MICE branch; if MR ≥ 0.2, it flows to the expressive GAIN branch with temporal analysis. Both branches execute in parallel, and their outputs are fused via cross-path attention using missingness embeddings. The system employs curriculum masking training (MCAR→MAR→MNAR) and joint optimization with homoscedastic uncertainty weighting. The architecture processes multimodal data from clinical records (MIMIC-III) and visual datasets (CIFAR), handling various missingness mechanisms while maintaining computational efficiency.

## Key Results
- Achieves imputation RMSE of 0.108, outperforming baselines by 28-29% (0.119–0.152)
- Improves downstream mortality prediction AUROC to 0.812 from baseline 0.748
- Maintains strong performance across diverse missingness patterns (MCAR, MAR, MNAR)

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Routing via Missingness Rate
The Gating Network utilizes Missingness Rate (MR) and temporal embeddings to route data. If MR < 0.2, data flows to the MICE path (efficient, statistical); if MR ≥ 0.2, it flows to the GAIN path (expressive, neural). This optimizes the trade-off between computational efficiency and representational capacity. The core assumption is that MR and temporal structure serve as sufficient proxies for imputation complexity.

### Mechanism 2: Cross-Path Attention Fusion
The mechanism projects Missingness Embedding (E) as Query (Q_{NHM}) against concatenated imputed outputs (K_{imp}, V_{imp}) to compute attention weights (α). This refines the initial routing decision, mixing signals based on feature-level context. The core assumption is that E successfully captures the context required to weigh the two paths correctly.

### Mechanism 3: Curriculum Masking
Training follows a schedule: Phase 1 (MCAR - random), Phase 2 (MAR - feature correlated), Phase 3 (MNAR - value dependent). This prevents early convergence and overfitting to difficult structured patterns. The core assumption is that real-world missingness follows a hierarchy where MNAR is structurally dependent on learned representations of MAR/MCAR.

## Foundational Learning

- **Concept: Multiple Imputation by Chained Equations (MICE)**
  - **Why needed here**: This is the "statistical branch" of the architecture. It handles low-complexity missingness efficiently but assumes linear relationships.
  - **Quick check question**: Can you explain why MICE might fail to capture non-linear temporal dependencies in ICU time-series data?

- **Concept: Generative Adversarial Imputation Networks (GAIN)**
  - **Why needed here**: This is the "neural branch." It uses a Generator to fill missing values and a Discriminator to verify realism.
  - **Quick check question**: How does the masking vector M in GAIN differ from standard GAN noise injection?

- **Concept: Missingness Mechanisms (MCAR vs MAR vs MNAR)**
  - **Why needed here**: The model is explicitly trained on a curriculum of these mechanisms. Understanding the difference is required to configure the curriculum masking schedule.
  - **Quick check question**: If a sensor fails only when temperature exceeds 100°C, is this MCAR, MAR, or MNAR?

## Architecture Onboarding

- **Component map**: Input Processor -> Embedding Generation -> Gating Decision (MR Threshold) -> Specific Branch Execution -> Attention Fusion
- **Critical path**: Input → Embedding Generation → Gating Decision (MR Threshold) → Specific Branch Execution → Attention Fusion
- **Design tradeoffs**: 
  - Efficiency vs. Overhead: While MICE is fast, the architecture implies computational overhead due to parallel execution of both paths.
  - Static vs. Dynamic: The threshold MR=0.2 is cited but the Gating Network is learned, reducing adaptability compared to fully learned routing.
- **Failure signatures**:
  - Mode Collapse: If GAIN branch fails to generate diverse outputs, Fusion layer will rely heavily on MICE regardless of complexity.
  - Router Bias: If gating network learns constant outputs, dual-path system becomes single-path.
  - Curriculum Mismatch: If Phase 3 (MNAR) is under-trained, performance on real-world clinical data will degrade.
- **First 3 experiments**:
  1. Router Validation: Ablate the dynamic router to confirm the 0.108 RMSE drops, proving routing logic adds value.
  2. Threshold Sensitivity: Vary the MR routing threshold (0.1, 0.3, 0.5) to verify if 0.2 is optimal.
  3. Curriculum Ablation: Train with standard random masking vs. full curriculum schedule to isolate contribution of MAR/MNAR phases.

## Open Questions the Paper Calls Out

### Open Question 1
How can PI-NAIM be adapted for fully unsupervised real-world scenarios where complete ground-truth data is unavailable for training? The current optimization relies on comparing imputed values against ground truth, which must be removed for unsupervised adaptation.

### Open Question 2
To what extent does performance degrade when curriculum masking schedule creates distribution mismatch with inference-time missingness patterns? The paper demonstrates effectiveness using a specific 30%/50%/20% schedule but doesn't analyze sensitivity to deviations.

### Open Question 3
How can interpretability of the Dynamic Path Selection mechanism be improved to explain why specific samples are routed to MICE vs. GAIN? The gating network's choices are hardly interpretable, which may hinder trust in healthcare settings.

### Open Question 4
Can computational overhead of the parallel architecture be reduced without compromising gains from the dual-path design? The paper identifies routing computational overhead as a limitation but doesn't provide detailed efficiency analysis.

## Limitations
- Dynamic routing threshold selection lacks sensitivity analysis across different missingness distributions
- Curriculum masking effectiveness is claimed but not empirically isolated through ablation studies
- Computational overhead from parallel execution of both branches is not quantified against single-path alternatives

## Confidence

- **High confidence**: The dual-path routing concept is well-defined and RMSE results (0.108) are clearly stated
- **Medium confidence**: Cross-path attention fusion mechanism is described but lacks empirical validation showing its necessity
- **Low confidence**: Specific impact of curriculum masking schedule is claimed but not empirically isolated from other components

## Next Checks

1. **Router ablation test**: Force all samples through only MICE or only GAIN to quantify the performance contribution of dynamic routing mechanism
2. **Threshold sensitivity analysis**: Vary the MR routing threshold across a range (0.1, 0.2, 0.3, 0.5) to determine optimal values for different missingness patterns
3. **Curriculum schedule ablation**: Train with only MCAR corruption versus the full MAR/MNAR curriculum to isolate the contribution of progressive missingness complexity