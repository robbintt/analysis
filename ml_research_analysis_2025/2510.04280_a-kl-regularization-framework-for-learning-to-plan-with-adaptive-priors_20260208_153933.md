---
ver: rpa2
title: A KL-regularization framework for learning to plan with adaptive priors
arxiv_id: '2510.04280'
source_url: https://arxiv.org/abs/2510.04280
tags:
- policy
- planning
- learning
- prior
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PO-MPC, a unified framework for model-based
  reinforcement learning that combines MPPI planning with KL-regularized policy optimization.
  The key insight is to use the planner's action distribution as a prior for policy
  updates, addressing the distribution mismatch between the sampling policy and planner.
---

# A KL-regularization framework for learning to plan with adaptive priors

## Quick Facts
- arXiv ID: 2510.04280
- Source URL: https://arxiv.org/abs/2510.04280
- Reference count: 40
- Primary result: PO-MPC achieves superior sample efficiency and final performance compared to TD-MPC2 and BMPC on 21 continuous control tasks by using the planner's action distribution as a prior for policy updates.

## Executive Summary
This paper introduces PO-MPC, a unified framework that combines MPPI planning with KL-regularized policy optimization in model-based reinforcement learning. The key innovation is using the planner's action distribution as a prior for policy updates, which addresses the distribution mismatch between the sampling policy and planner. The method introduces an adaptive intermediate prior policy that shields the sampling policy from outdated planner statistics stored in the replay buffer, reducing variance in updates. Experiments on 21 continuous control tasks show that PO-MPC significantly improves sample efficiency and final performance compared to state-of-the-art baselines.

## Method Summary
PO-MPC operates on a Plan→Infer→Regularize loop where MPPI planning generates a Gaussian action distribution that serves as a prior for KL-regularized policy updates. The framework uses an adaptive intermediate prior policy trained via forward or reverse KL to stabilize learning by filtering stale planner statistics from the replay buffer. The sampling policy is optimized with a KL-regularized objective that trades off return maximization against staying close to the planner's distribution. The method allows tuning this trade-off through the regularization strength λ, with intermediate values often yielding the best results. The framework also demonstrates that training the intermediate prior with different KL directions (forward vs. reverse) can embed different properties in the sampling policy, affecting exploration and convergence behavior.

## Key Results
- PO-MPC significantly outperforms TD-MPC2 and BMPC baselines on 21 continuous control tasks in terms of sample efficiency and final performance
- Intermediate values of KL regularization strength (λ) often yield the best results, balancing return maximization and planner alignment
- Forward KL for prior training helps exploration-heavy tasks while reverse KL accelerates convergence in precision-dominated tasks
- The adaptive intermediate prior reduces variance in KL divergence updates compared to using direct planner samples from the replay buffer

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: KL-regularizing the sampling policy toward a planner-induced prior reduces distribution mismatch between training and execution, improving value estimation accuracy.
- **Mechanism**: The sampling policy π_θ^s is optimized to maximize a KL-regularized objective J(π) = E[Σ γ^t r(z_t, a_t)] - λ KL[π_θ^s || π^p], where π^p is the planning policy. When λ > 0, the policy update is constrained to stay near the planner's action distribution. This alignment ensures the value function Q is trained on state-action pairs that the planner actually visits, reducing bootstrap error—especially critical for short horizons H where terminal Q dominates trajectory scoring.
- **Core assumption**: The MPPI planner produces higher-quality action distributions than the current sampling policy, particularly near high-reward regions.
- **Evidence anchors**: [abstract] "aligning the learned policy with the planner's behavior, PO-MPC allows more flexibility in the policy updates to trade off Return maximization and KL divergence minimization"; [Section 4.1] "This decoupling creates a distribution mismatch: the value function is trained under states and actions induced by the planner, but the policy update optimizes a different objective"
- **Break condition**: If the planner is poorly trained (early episodes), cloning it may propagate bad priors; λ must be tuned or annealed.

### Mechanism 2
- **Claim**: Distilling the planner into an adaptive intermediate prior π_θ^p reduces update variance caused by stale planner statistics stored in the replay buffer.
- **Mechanism**: The replay buffer stores planner distributions (π^P = N(ā, σ²I)) from many training stages. Sampling from this buffer yields a time-varying Gaussian mixture rather than a unimodal distribution. By training a single learned prior π_θ^p to match π^P (via forward or reverse KL), PO-MPC provides a stable, unimodal target that the sampling policy can track reliably, reducing gradient variance.
- **Core assumption**: The planner's action distribution can be adequately approximated by a unimodal Gaussian.
- **Evidence anchors**: [abstract] "adaptive intermediate prior policy that shields the sampling policy from outdated planner statistics stored in the replay buffer, reducing variance in updates"; [Section 4.2] "the sampled planning distribution behaves like a Gaussian mixture instead of the unimodal distribution resulting from MPPI under the current sampling policy"; [Figure 6] Shows significantly lower mean and standard deviation of KL divergence when using intermediate prior vs. direct planner samples.
- **Break condition**: If the true planner distribution is highly multimodal, a Gaussian prior may lose critical modes; more expressive priors would be needed.

### Mechanism 3
- **Claim**: Training the intermediate prior with forward KL encourages exploration (mode-covering), while reverse KL accelerates convergence (mode-seeking).
- **Mechanism**: Forward KL (KL[π^P || π_θ^p]) forces the prior to cover the full support of planner samples, preventing premature collapse and aiding exploration-heavy tasks. Reverse KL (KL[π_θ^p || π^P]) allows the prior to match a single mode, speeding convergence in precision-dominated tasks. The choice of KL direction thus embeds different inductive biases into the sampling policy via the KL-regularized update.
- **Core assumption**: Task structure determines whether exploration or precision is more valuable; the practitioner can select the appropriate KL direction.
- **Evidence anchors**: [abstract] "forward KL helping exploration-heavy tasks and reverse KL accelerating convergence in precision-dominated tasks"; [Section 4.2] "minimizing the reverse KL-divergence... will bias the sampling policy towards distributions that match one of the modes... minimizing the forward KL-divergence... will bias the policy towards a Gaussian distribution that includes the support of all sampled planning distributions"; [Figure 3, 7] Forward KL improves Stair (exploration-heavy); reverse KL improves Balance Simple (precision-dominated).
- **Break condition**: If the task requires both exploration and precision at different phases, a fixed KL direction may be suboptimal; adaptive switching could help.

## Foundational Learning

- **Concept**: KL-regularized reinforcement learning
  - **Why needed here**: The entire PO-MPC framework is built on maximizing return while regularizing toward a prior; understanding the λ trade-off is essential for tuning.
  - **Quick check question**: Can you explain why λ → 0 recovers standard return maximization and λ → ∞ recovers pure prior cloning?

- **Concept**: Model Predictive Path Integral (MPPI) control
  - **Why needed here**: MPPI generates the planning policy π^P that serves as the prior; understanding its sampling, weighting, and update rules is necessary to debug planner quality.
  - **Quick check question**: Given a set of sampled trajectories with costs S(τ_i), how does MPPI compute the weights w_i and update the nominal action sequence?

- **Concept**: Forward vs. reverse KL divergence
  - **Why needed here**: The choice of KL direction for training the intermediate prior determines exploration vs. convergence behavior.
  - **Quick check question**: Why does forward KL encourage mode-covering and reverse KL encourage mode-seeking?

## Architecture Onboarding

- **Component map**: Environment → Encoder → Latent State z → MPPI Planner (using π_θ^s, Q_θ^Q) → Planning Distribution π^P → Execute Action → Replay Buffer → Update π_θ^p (via KL), Q_θ^Q (TD), π_θ^s (KL-regularized)

- **Critical path**:
  1. Environment step → encode state z
  2. MPPI planning using π_θ^s and Q_θ^Q → produces π^P = N(ā, σ²I)
  3. Execute action, store (s, a, r, s', ā, σ) in replay buffer
  4. Update world model, then train π_θ^p to match π^P samples
  5. Update Q_θ^Q (standard TD), Q̃_θ^Q (KL-regularized TD), and π_θ^s (KL-regularized policy gradient)

- **Design tradeoffs**:
  - λ tuning: low λ prioritizes return (riskier), high λ prioritizes planner alignment (more stable but potentially suboptimal)
  - Forward vs. reverse KL for prior: forward for exploration, reverse for precision
  - Horizon H: short horizons increase reliance on bootstrap Q, amplifying distribution mismatch; longer horizons reduce this but increase model error
  - Pretraining steps N_s: insufficient pretraining leads to poor planner quality, harming prior quality

- **Failure signatures**:
  - High variance in KL term during policy updates → check if intermediate prior is undertrained or if replay buffer contains highly diverse planner distributions
  - Policy collapses to local optimum early → λ may be too high; reduce or anneal
  - Poor exploration on sparse-reward tasks → consider forward KL for prior training
  - Value function estimates unstable → verify Q_θ^Q is trained before enabling KL-regularized updates

- **First 3 experiments**:
  1. **λ sweep on a single DMControl task**: Test λ ∈ {0.1, 0.5, 1.0, 5.0, 10.0} with reverse KL prior; plot learning curves and final performance to identify the sweet spot
  2. **Ablate intermediate prior**: Compare using π_θ^p vs. direct replay buffer π^P samples; measure mean and variance of KL divergence term (replicate Figure 6)
  3. **Forward vs. reverse KL prior on contrasting tasks**: Run PO-MPC on Stair (exploration-heavy) and Balance Simple (precision-dominated) with both KL directions; confirm directional effect on convergence speed and final performance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the performance of PO-MPC be improved by replacing Gaussian policy representations with more expressive distribution classes?
- **Basis in paper**: [explicit] The authors state in the Conclusion that current Gaussian assumptions are "very restrictive since the Planning policy... is not necessarily Gaussian," and suggest future work focus on "extending the distribution of the policies used to more expressive classes."
- **Why unresolved**: The current implementation assumes unimodal Gaussian distributions for both the sampling policy and the adaptive prior, which limits the ability to fully capture the complex, multi-modal nature of the MPPI planning posterior.
- **What evidence would resolve it**: Implementing the framework using mixture density networks or normalizing flows and demonstrating improved alignment with the planner distribution and higher task returns.

### Open Question 2
- **Question**: Can the optimal KL-regularization strength (λ) be learned dynamically during training rather than tuned as a hyperparameter?
- **Basis in paper**: [explicit] The Limitations section notes that tuning λ is essential and suggests that "A similar approach might be taken as in SAC... where the appropriate value of λ would be learned during training."
- **Why unresolved**: Currently, λ is a fixed hyperparameter that must be tuned depending on the environment complexity and prior training, acting as a manual trade-off between return maximization and planner alignment.
- **What evidence would resolve it**: A mechanism that adjusts λ automatically (e.g., via dual gradient descent or constrained optimization) and achieves equal or better performance without manual per-task tuning.

### Open Question 3
- **Question**: Can the computational efficiency and sample efficiency of PO-MPC be improved by reusing the simulated transitions generated during MPPI planning?
- **Basis in paper**: [explicit] The Conclusion identifies "increasing the computational efficiency by leveraging the simulated transition data generated during planning for action value learning" as a direction for future work.
- **Why unresolved**: Currently, the many trajectories simulated during the planning phase are used solely to select an action and are then discarded, which the authors note is "computationally inefficient."
- **What evidence would resolve it**: A modified training loop that successfully incorporates planning rollouts into the value function training data without introducing distribution bias, resulting in faster convergence.

## Limitations
- **Gaussian policy restrictions**: The framework assumes unimodal Gaussian distributions for both sampling policy and intermediate prior, which may not capture the true multimodal nature of MPPI planning distributions
- **Computational inefficiency**: The method requires significant computational resources due to repeated MPPI planning and KL regularization calculations
- **Hyperparameter sensitivity**: Performance depends critically on tuning the KL regularization strength λ and the choice between forward and reverse KL for prior training

## Confidence

- **High confidence**: The mechanism of KL-regularizing the sampling policy toward a planner-induced prior to reduce distribution mismatch (Mechanism 1) is well-supported by theoretical derivation and experimental results
- **Medium confidence**: The claim that distilling the planner into an adaptive intermediate prior reduces update variance (Mechanism 2) is supported by Figure 6 showing reduced KL divergence variance, though the multimodal distribution assumption needs further validation
- **Medium confidence**: The claim that forward vs. reverse KL for prior training affects exploration vs. convergence behavior (Mechanism 3) is supported by task-specific results, but the generalization across diverse tasks needs broader validation

## Next Checks

1. **Ablate intermediate prior**: Compare PO-MPC with and without the learned prior π_θ^p, measuring KL divergence mean and variance during training to confirm variance reduction effect
2. **Hyperparameter sensitivity**: Systematically test λ ∈ {0.1, 1.0, 5.0, 10.0} on multiple DMControl tasks to identify the optimal trade-off and verify the robustness of the sweet spot
3. **Forward vs. reverse KL prior**: Run PO-MPC with both KL directions on a diverse set of tasks (exploration-heavy, precision-dominated, and balanced) to confirm the directional effects on convergence speed and final performance generalize beyond the two tasks presented