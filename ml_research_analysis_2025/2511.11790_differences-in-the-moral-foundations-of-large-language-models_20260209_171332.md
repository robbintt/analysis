---
ver: rpa2
title: Differences in the Moral Foundations of Large Language Models
arxiv_id: '2511.11790'
source_url: https://arxiv.org/abs/2511.11790
tags:
- moral
- foundations
- human
- each
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uses moral foundations theory (MFT) to evaluate and
  compare the ethical judgments of various large language models against a human baseline.
  By applying the Moral Foundations Vignettes (MFV) survey to multiple models from
  different providers, the study reveals that models diverge systematically from human
  moral reasoning.
---

# Differences in the Moral Foundations of Large Language Models

## Quick Facts
- arXiv ID: 2511.11790
- Source URL: https://arxiv.org/abs/2511.11790
- Reference count: 8
- Primary result: LLMs systematically diverge from human moral baselines, with provider-specific profiles and capability-dependent shifts

## Executive Summary
This paper evaluates LLM moral reasoning using Moral Foundations Theory (MFT), comparing 16 models from 8 providers against a human baseline. The study reveals that models consistently deviate from human moral patterns, placing greater emphasis on care and fairness while devaluing loyalty, authority, and sanctity. These differences are systematic, provider-specific, and tend to increase with model capability. The findings suggest MFT is a useful lens for characterizing LLM moral biases, with implications for alignment and policy.

## Method Summary
The study administers the Moral Foundations Vignettes (MFV) survey to 16 LLM models across 8 providers. Each vignette asks models to rate the "wrongness" of scenarios on a 0-4 Likert scale. For OpenAI and xAI models, responses are scored using weighted averages of top-3 logprobs; other models use mean scores from 10 independent queries. Results are aggregated by foundation and compared to a human baseline from Clifford et al. [2015], with Spearman correlations and PCA biplots used for analysis.

## Key Results
- Models systematically diverge from human moral baselines, with most providers placing greater emphasis on care and fairness while devaluing loyalty, authority, and sanctity
- Provider-level clustering indicates systematic differences in moral foundation profiles that are not random
- As model capabilities increase, deviation from human baselines typically increases, particularly in the devaluation of binding foundations
- Text analysis of model justifications confirms distinct moral vocabularies across providers

## Why This Works (Mechanism)

### Mechanism 1
Provider-level clustering of moral foundation profiles likely stems from post-training alignment procedures rather than pre-training data alone. Different providers apply SFT and RLHF with distinct reward landscapes and annotation guidelines, shaping which moral foundations are reinforced versus suppressed.

### Mechanism 2
Increased model capability correlates with amplified deviation from human moral baselines, particularly through devaluation of binding foundations (loyalty, authority, sanctity). Larger models may develop broader "moral circles" emphasizing universalizability and harm reduction over rule adherence.

### Mechanism 3
MFT's individualizing/binding foundation structure partially generalizes to LLMs, enabling meaningful cross-model comparison. The rank correlation analysis shows expected clustering of binding foundations and some predicted weak correlations, though the care-fairness correlation is weaker than in human studies.

## Foundational Learning

- **Moral Foundations Theory (MFT) taxonomy**: The six foundations (care, fairness, loyalty, authority, sanctity, liberty) and individualizing vs. binding distinction form the analytical framework. Quick check: Can you explain why care and fairness are classified as "individualizing" while loyalty, authority, and sanctity are "binding"?

- **RLHF/SFT alignment pipeline**: Provider-level variance is attributed to post-training practices. Quick check: How might a toxicity classifier used in RLHF inadvertently penalize loyalty-related language?

- **PCA biplot interpretation**: The primary visualization uses PCA scores and foundation loadings. Quick check: If loyalty vectors point left on the x-axis and OpenAI models cluster left of the human baseline, what does that imply about their loyalty valuation?

## Architecture Onboarding

- **Component map**: MFV survey → Expected Parrot tool → API calls per question → Likert scoring → Foundation aggregation → Correlation analysis → PCA visualization

- **Critical path**: Vignette selection → Model response extraction → Foundation-level aggregation → Cross-model correlation → PCA projection → Text analysis

- **Design tradeoffs**: MFV vs. MFQ (different constructs), Logprobs vs. repeated sampling (different uncertainty capture), US baseline vs. cross-cultural baselines

- **Failure signatures**: Non-numeric responses, foundation label leakage, temperature/variance differences, small sample size

- **First 3 experiments**: 1) Controlled RLHF ablation on open-source base models, 2) Cross-cultural baseline validation, 3) Loyalty-honesty conflict probe

## Open Questions the Paper Calls Out

1. Do the observed deviations in moral foundations stem primarily from pre-training data composition or post-training alignment techniques (RLHF/SFT)?
2. Does the systematic devaluation of the "loyalty" foundation result from a conflict between loyalty vignettes and the optimization for "honesty"?
3. Does the aggressive filtering of toxic outputs cause the theoretical correlation between "care" and "fairness" foundations to break down in LLMs?

## Limitations

- Causal attribution ambiguity: Provider-level variance is correlated with alignment procedures but not definitively proven
- Human baseline representativeness: Single US sample may not capture universal moral baselines
- Survey construct validity for LLMs: MFT was developed for humans; weaker care-fairness correlation suggests incomplete fit

## Confidence

- **High confidence**: Empirical finding that LLMs systematically diverge from human moral baselines with provider-specific patterns
- **Medium confidence**: Hypothesis that alignment procedures drive provider-level differences
- **Low confidence**: Claim that capability scaling inherently increases deviation from human baselines

## Next Checks

1. Controlled RLHF ablation study: Apply different SFT/RLHF procedures to a single base model to isolate causal effects
2. Cross-cultural baseline validation: Administer MFV to multiple human populations to test baseline universality
3. Loyalty-honesty conflict probe: Design systematic vignette sets varying loyalty-honesty tradeoffs to test the hypothesis