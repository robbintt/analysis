---
ver: rpa2
title: 'DualCap: Enhancing Lightweight Image Captioning via Dual Retrieval with Similar
  Scenes Visual Prompts'
arxiv_id: '2510.24813'
source_url: https://arxiv.org/abs/2510.24813
tags:
- visual
- image
- arxiv
- retrieval
- dualcap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of generating detailed image captions
  with lightweight models, which often struggle to capture fine-grained visual details
  due to reliance on static global features. The authors propose DualCap, a novel
  approach that enhances lightweight image captioning by employing a dual retrieval
  mechanism.
---

# DualCap: Enhancing Lightweight Image Captioning via Dual Retrieval with Similar Scenes Visual Prompts

## Quick Facts
- **arXiv ID**: 2510.24813
- **Source URL**: https://arxiv.org/abs/2510.24813
- **Reference count**: 40
- **Primary result**: DualCap achieves 123.6 CIDEr on COCO with only 11M trainable parameters

## Executive Summary
DualCap addresses the challenge of generating detailed image captions with lightweight models that struggle to capture fine-grained visual details. The approach employs a dual retrieval mechanism that combines image-to-text retrieval for contextual text prompts with image-to-image retrieval to source visually similar scenes. Salient keywords are extracted from these similar scenes and encoded to create visual prompts that enrich the original image features through a lightweight Feature Fusion Network. Experiments demonstrate competitive performance with fewer trainable parameters than prior methods, achieving strong cross-domain generalization while maintaining faster inference speeds.

## Method Summary
DualCap implements a dual retrieval-augmented generation approach for lightweight image captioning. The method pre-computes CLIP features for an external datastore containing COCO training images and captions. For a given input image, the model performs two retrieval operations: image-to-text (I2T) retrieves top-4 semantically similar captions for contextual prompting, while image-to-image (I2I) retrieves top-3 visually similar images. Scene-keywords are extracted from I2I-retrieved captions using NLTK POS tagging and chunking, then encoded by CLIP's text encoder. These keyword embeddings are fused with original image patch features through a single-layer Transformer (SFN) using cross-attention, where visual features attend to keyword embeddings. The enhanced features are added to frozen CLIP features via residual connection and condition a frozen GPT-2 decoder alongside the text prompt. Only the SFN and GPT-2 cross-attention layers are trained (11M parameters total).

## Key Results
- Achieves 123.6 CIDEr on COCO test set, outperforming baselines SmallCap and ViPCap
- Maintains strong cross-domain generalization on NoCaps out-of-domain evaluation
- Requires only 11M trainable parameters while frozen models handle encoding and decoding
- Demonstrates faster inference than full fine-tuning approaches while maintaining competitive quality

## Why This Works (Mechanism)

### Mechanism 1: Dual Retrieval Decouples Contextual and Visual Evidence
DualCap separates image-to-text and image-to-image retrieval paths to provide both high-level semantic context and fine-grained visual details. The I2T path retrieves semantically similar captions as text prompts for GPT-2, while the I2I path retrieves visually similar images and extracts scene-keywords from their captions to enhance original image features via SFN. This addresses the visual-text semantic gap left by text-only retrieval.

### Mechanism 2: Keyword-Driven Visual Prompt Enhances Feature Specificity
Keywords/phrases extracted from I2I-retrieved captions using NLTK (POS tagging, chunking) capture essential semantics better than full sentences. These keywords are encoded by frozen CLIP text encoder and fused with original image patch features using cross-attention in SFN, where patches attend to keywords. This produces targeted visual prompts that add discriminative power to the feature representation.

### Mechanism 3: Lightweight Fusion with Frozen Backbones Maximizes Efficiency
By freezing large pre-trained models (CLIP encoder, GPT-2 decoder) and training only the lightweight SFN and cross-attention layers (~11M params), DualCap achieves competitive performance with minimal parameters. The visual prompt generated by SFN is added to frozen CLIP features via residual connection, and enhanced features condition the frozen GPT-2 decoder alongside the text prompt.

## Foundational Learning

- **Cross-Attention in Transformer Architectures**: Essential for understanding how SFN fuses visual and textual features, with image patches as Query and keyword embeddings as Key/Value
- **Retrieval-Augmented Generation (RAG)**: Critical context for understanding DualCap's dual RAG mechanism and motivation for adding visual retrieval beyond standard text-only RAG
- **CLIP Joint Vision-Language Embeddings**: Fundamental to DualCap's use of CLIP for image encoding, text encoding, and retrieval via cosine similarity in shared embedding space

## Architecture Onboarding

- **Component map**: Input image -> CLIP ViT-B/32 (frozen) -> global feature v_I + patch features V -> I2T retrieval (top-4 captions) -> text prompt X; I2I retrieval (top-3 images) -> caption processing (NLTK) -> keywords Kp -> CLIP text encoder (frozen) -> keyword embeddings E_kp -> SFN (trainable) -> visual prompt Z_kp -> enhanced features V' = V + Z_kp -> frozen GPT-2 decoder + text prompt X -> generated caption

- **Critical path**: Input image encoded by CLIP vision encoder to global feature v_I and patch features V; v_I queries datastore for top-k captions (I2T) formatted into text prompt X; v_I queries datastore for top-M similar images (I2I), captions processed to extract Kp; Kp encoded to E_kp; SFN takes V (Query) and E_kp (Key/Value), outputs Z_kp; V' = V + Z_kp; X and V' condition frozen GPT-2 decoder to generate caption

- **Design tradeoffs**: Dual retrieval improves accuracy but increases inference latency vs. single-stream models (0.42s vs. SmallCap 0.25s); keyword extraction reduces noise vs. full captions but may miss nuanced descriptions; freezing encoder/decoder limits adaptability vs. full fine-tuning but drastically reduces trainable parameters (11M)

- **Failure signatures**: Low CIDEr on in-domain data indicates poor I2I retrieval quality or irrelevant keyword extraction; poor generalization on NoCaps suggests datastore lacks relevant visual examples or text prompt style mismatch; slow inference suggests non-optimized FAISS indexing

- **First 3 experiments**: 1) Ablate dual retrieval by running I2T-only and I2I-only variants to measure individual contributions; 2) Vary fusion method by comparing SFN vs. direct sum, concat, or concat+MLP to validate SFN design; 3) Swap decoders by testing GPT-2, OPT-125M, and XGLM-564M to verify architecture robustness

## Open Questions the Paper Calls Out
The paper explicitly identifies extending the framework to Visual Question Answering as a promising avenue for future work, suggesting the dual retrieval mechanism could benefit visual reasoning tasks beyond captioning.

## Limitations
- Keyword extraction process using NLTK's POS tagging and chunking lacks specification of exact POS tag sets and chunking rules, introducing implementation variability
- Evaluation focuses exclusively on English datasets, leaving multilingual performance and cross-linguistic generalization untested
- Computational cost analysis is incomplete, lacking detailed latency measurements across hardware configurations and total inference time comparisons

## Confidence
- **High Confidence**: DualCap achieves state-of-the-art performance among lightweight models on COCO (123.6 CIDEr) with only 11M trainable parameters, well-supported by experimental results
- **Medium Confidence**: Dual retrieval mechanism (I2T + I2I) outperforms single retrieval approaches, supported by ablation studies but lacking confidence intervals
- **Low Confidence**: Keyword-driven visual prompts are superior to full caption retrieval, lacking direct experimental validation through ablation comparison

## Next Checks
1. **Keyword Extraction Sensitivity Analysis**: Systematically vary POS tag sets and chunking rules, then measure impact on downstream captioning performance to quantify sensitivity to this preprocessing step
2. **Retrieval Quality Validation**: Implement human evaluation protocol where annotators rate visual similarity between input images and top-3 I2I retrievals, then correlate similarity scores with captioning performance
3. **Cross-Lingual Generalization Test**: Evaluate DualCap on multilingual image captioning dataset or non-English COCO images with machine-translated captions to test linguistic diversity handling