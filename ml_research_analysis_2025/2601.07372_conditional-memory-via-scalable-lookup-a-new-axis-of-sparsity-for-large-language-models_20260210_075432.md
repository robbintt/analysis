---
ver: rpa2
title: 'Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language
  Models'
arxiv_id: '2601.07372'
source_url: https://arxiv.org/abs/2601.07372
tags:
- engram
- memory
- language
- arxiv
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces conditional memory as a complementary sparsity\
  \ axis to the prevailing conditional computation paradigm (MoE) in large language\
  \ models. The key innovation is Engram, a module that leverages scalable \U0001D441\
  -gram embeddings with O(1) lookup to offload static pattern reconstruction from\
  \ the Transformer backbone."
---

# Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models

## Quick Facts
- arXiv ID: 2601.07372
- Source URL: https://arxiv.org/abs/2601.07372
- Reference count: 40
- Conditional memory module achieves 3-5% gains on reasoning and code tasks over MoE baselines

## Executive Summary
This paper introduces conditional memory as a novel sparsity axis complementary to existing conditional computation approaches like MoE in large language models. The authors propose Engram, a scalable N-gram embedding lookup module that can reconstruct static patterns with O(1) complexity, effectively offloading this burden from the Transformer backbone. Through a systematic analysis of sparsity allocation between experts and memory, they discover a U-shaped scaling law demonstrating that hybrid models outperform pure MoE baselines. The resulting 27B parameter model shows significant improvements across reasoning, code, math, and long-context retrieval tasks.

## Method Summary
The authors develop Engram, a conditional memory module that uses scalable N-gram embeddings with constant-time lookup to reconstruct static patterns. This module is integrated alongside MoE experts in a hybrid architecture where the Sparsity Allocation problem determines the optimal balance between expert and memory parameters. The key insight is that conditional memory and conditional computation represent orthogonal axes of sparsity that can be combined for improved efficiency and performance. The Engram module operates by learning to store and retrieve static patterns rather than having the Transformer backbone learn to reconstruct them, effectively freeing up model capacity for more dynamic processing.

## Key Results
- Achieves 5.0 BBH and 3.7 ARC-Challenge improvements on reasoning tasks
- Improves HumanEval by 3.0 and MATH by 2.4 over iso-parameter MoE baselines
- Boosts NIAH long-context retrieval from 84.2 to 97.0 with the same model size
- Demonstrates superior performance over iso-FLOPs MoE baselines

## Why This Works (Mechanism)
The mechanism works by leveraging the observation that many patterns in language are static and predictable, making them ideal candidates for memorization rather than reconstruction. By offloading these static patterns to Engram's N-gram embeddings, the Transformer backbone is relieved from learning to reconstruct them, effectively deepening the network's capacity for dynamic processing. The deterministic addressing of Engram enables runtime prefetching, which minimizes overhead. The U-shaped scaling law suggests there's an optimal balance between expert and memory parameters that maximizes performance, with pure MoE on either extreme underperforming the hybrid approach.

## Foundational Learning

**Conditional Computation**: Why needed - Enables selective activation of model components to reduce compute; Quick check - Verify that only relevant experts activate per token

**N-gram Embeddings**: Why needed - Captures local context patterns efficiently; Quick check - Confirm lookup complexity remains O(1) regardless of vocabulary size

**Sparsity Allocation**: Why needed - Balances parameter count between experts and memory; Quick check - Plot performance vs allocation ratio to verify U-shaped curve

**Hybrid Architectures**: Why needed - Combines complementary strengths of different sparsity axes; Quick check - Compare hybrid vs pure expert/memory baselines

## Architecture Onboarding

**Component Map**: Input -> Tokenizer -> Engram (N-gram Memory) + MoE Experts -> Transformer Backbone -> Output

**Critical Path**: Token sequence flows through Engram for pattern matching, then MoE routing, then through Transformer layers with selective expert activation

**Design Tradeoffs**: Memory vs compute efficiency (Engram reduces FLOPs but adds parameters), model depth vs width (relieving static reconstruction deepens effective network), routing complexity (deterministic vs learned routing)

**Failure Signatures**: Performance degradation when static patterns dominate dynamic ones, routing overhead exceeding gains, memory capacity bottlenecks limiting pattern storage

**First Experiments**: 1) Measure baseline performance with pure MoE vs hybrid; 2) Sweep sparsity allocation ratio to find optimal balance; 3) Profile memory access patterns to verify O(1) lookup

## Open Questions the Paper Calls Out

None

## Limitations

- Evaluation primarily against standard MoE models without testing against recent sparse architectures like SAM
- Benefits appear most pronounced in specific domains (reasoning, code, long-context) with unclear generalization
- Runtime prefetching optimization lacks empirical validation of practical latency impact
- Training dynamics and convergence properties of hybrid models remain uncharacterized

## Confidence

- **High** for empirical observation that Engram improves performance in tested reasoning and code domains
- **Medium** for theoretical framing of conditional memory as distinct sparsity axis orthogonal to MoE
- **Medium** for U-shaped scaling law finding, though more hyperparameter sweeps would strengthen this
- **Low** for claims about practical deployment benefits without latency measurements

## Next Checks

1. Benchmark against recent sparse architectures (SAM, Switch Transformers) on the same tasks to establish relative performance gains
2. Measure end-to-end inference latency with and without the proposed prefetching optimization across different batch sizes
3. Conduct ablation studies removing Engram from early layers only to isolate the mechanism by which it "deepens" the network