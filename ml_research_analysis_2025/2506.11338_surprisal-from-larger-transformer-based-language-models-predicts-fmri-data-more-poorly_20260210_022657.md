---
ver: rpa2
title: Surprisal from Larger Transformer-based Language Models Predicts fMRI Data
  More Poorly
arxiv_id: '2506.11338'
source_url: https://arxiv.org/abs/2506.11338
tags:
- surprisal
- data
- language
- fmri
- estimates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates the relationship between the size of Transformer-based
  language models and their ability to predict human brain activity during language
  comprehension. Using 17 pre-trained models from three different families (GPT-2,
  GPT-Neo, OPT), surprisal estimates were calculated for two fMRI datasets: Shain
  et al.'
---

# Surprisal from Larger Transformer-based Language Models Predicts fMRI Data More Poorly

## Quick Facts
- arXiv ID: 2506.11338
- Source URL: https://arxiv.org/abs/2506.11338
- Reference count: 19
- Key outcome: Larger Transformer-based language models (more parameters) show significantly worse predictive power for fMRI brain activity during language comprehension

## Executive Summary
This study investigates how the size of pre-trained Transformer-based language models affects their ability to predict human brain activity during language comprehension. Using 17 models from three families (GPT-2, GPT-Neo, OPT) ranging from 124M to 66B parameters, the authors computed surprisal estimates for two fMRI datasets. The surprisal estimates were temporally aligned with BOLD signals via hemodynamic response function convolution and used as predictors in linear mixed-effects models. Results show a significant inverse scaling relationship: larger models provide poorer fits to brain activity patterns, extending previous findings from reading times to neural data.

## Method Summary
The study uses surprisal estimates from 17 pre-trained Transformer models to predict fMRI BOLD signals. For each model, token-level surprisal is computed and aggregated to word-level, with whitespace-trailing decoding applied to prevent probability > 1. For the Natural Stories dataset, surprisal is convolved with a canonical HRF to align with the 4-6 second hemodynamic lag before entering linear mixed-effects models with baseline predictors. For the Pereira dataset, sentence-final word surprisal is used directly. Model predictive power is evaluated via held-out log-likelihood, with statistical significance assessed through permutation tests.

## Key Results
- Inverse scaling: Larger models (more parameters) show significantly worse predictive power for fMRI data (p < 0.01 by permutation test)
- Extends previous findings: The inverse scaling effect previously observed with reading times also holds for neural data
- Frequency-driven mechanism: The effect is primarily driven by larger models assigning lower surprisal to rare open-class words (nouns, adjectives)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Surprisal estimates from language models predict BOLD signals when temporally aligned via hemodynamic response function convolution.
- Mechanism: Token-level surprisal is computed from autoregressive LMs → aggregated to word-level (handling whitespace trailing) → convolved with a canonical HRF to match the delayed BOLD response → entered into linear mixed-effects models with baseline predictors (word position, length). The HRF models the 4-6 second lag between neural activity and measurable blood oxygenation changes.
- Core assumption: Processing difficulty (operationalized as surprisal) generates neural activity that follows a predictable hemodynamic response curve.
- Evidence anchors:
  - [abstract] "surprisal estimates were convolved with a hemodynamic response function and used as predictors in linear mixed-effects models"
  - [section 3.2] Provides explicit HRF formula: "f(x) = 5.2*(5.4^5.4)*x^(5.4-1)*e^(-5.2x)/Γ(5.4) − 0.35*(7.35^10.8)*x^(10.8-1)*e^(-7.35x)/Γ(10.8)"
  - [corpus] "Vectors from Larger Language Models Predict Human Reading Time and fMRI Data More Poorly" confirms the surprisal-to-brain pipeline generalizes across representation types.
- Break condition: If the HRF parameters do not match the actual hemodynamic response timing in the sampled population, temporal alignment fails and predictive power drops arbitrarily.

### Mechanism 2
- Claim: Larger transformer models (more parameters, lower perplexity) yield surprisal estimates that are progressively worse predictors of human neural activity.
- Mechanism: As models scale, they improve at predicting rare words through extended training on larger corpora, producing lower surprisal values. This improved statistical fit diverges from human processing, which appears calibrated to different frequency expectations—possibly reflecting cognitive resource constraints or memory limitations not present in transformers.
- Core assumption: Human language processing involves surprisal expectations that are systematically different from those of large models optimized purely for next-token prediction.
- Evidence anchors:
  - [abstract] "Results show a significant inverse scaling relationship between model size (parameter count) and predictive power for both datasets"
  - [section 3.4] "significant decrease in predictive power when parameter count (model size) increases (p < 0.01 by a permutation test with 1,000 permutations)"
  - [corpus] "The Inverse Scaling Effect of Pre-Trained Language Model Surprisal Is Not Due to Data Leakage" addresses a key alternative explanation, strengthening the claim.
- Break condition: If model scaling behavior changes (e.g., emergent abilities at extreme scale alter rare-word statistics) or if human frequency sensitivity shifts with task demands, the inverse relationship may not hold.

### Mechanism 3
- Claim: Word frequency drives the inverse scaling effect—larger models lower surprisal on rare open-class words (nouns, adjectives), creating divergence from human neural responses.
- Mechanism: During training, larger models receive more exposure to rare tokens and optimize perplexity by assigning them higher probability (lower surprisal). Humans, by contrast, may retain higher surprisal for rare items due to working memory constraints, limited exposure, or adaptive surprisal calibration. The mismatch is most pronounced in open-class content words.
- Core assumption: Human surprisal expectations do not scale with corpus exposure the way model probabilities do—cognitive architecture imposes a ceiling.
- Evidence anchors:
  - [section 2] "Oh and Schuler (2023)...observing that the poorer fit to reading times of larger LMs is mainly driven by the lower surprisal values those LMs assign to open-class words such as nouns and adjectives"
  - [section 5] "larger LMs get better at predicting rare words, resulting in lower surprisal which diverges from human reading times"
  - [corpus] Weak direct evidence for frequency-driven mechanisms specifically in fMRI; corpus neighbors focus on reading-time validation rather than neural data.
- Break condition: If frequency effects are confounded by another variable (e.g., contextual predictability, syntactic complexity) that scales with model size independently, the attributed mechanism would need revision.

## Foundational Learning

- Concept: **Surprisal Theory (Information-Theoretic Processing)**
  - Why needed here: The entire methodology assumes processing difficulty is proportional to surprisal (negative log probability). Without this foundation, the rationale for using LM probabilities to predict brain activity is opaque.
  - Quick check question: In the context "The doctor prescribed ___", would "medication" or "elephant" have higher surprisal, and what does theory predict about neural activation at that word?

- Concept: **Hemodynamic Response Function (HRF) and BOLD Signal Timing**
  - Why needed here: fMRI does not measure neural activity directly—it measures delayed blood oxygenation changes. Understanding the 4-6 second lag and the canonical HRF shape is essential for correctly aligning surprisal predictors with observed BOLD responses.
  - Quick check question: If a high-surprisal word appears at t=2s, when should you expect the peak BOLD response, and why must convolution be applied before regression?

- Concept: **Linear Mixed-Effects Models with Random Effects Structure**
  - Why needed here: The analysis uses maximal random effects (by-subject, by-item) to control for participant variability and stimulus-specific effects. Convergence failures are common, and improper simplification can invalidate inference.
  - Quick check question: Why does a maximal random-effects structure often fail to converge, and what is the principled order for pruning random slopes?

## Architecture Onboarding

- Component map:
  - Tokenization Layer: Model-specific BPE tokenizer (GPT-2, GPT-Neo, OPT) → token-level surprisal extraction
  - Aggregation Layer: Token surprisal → word-level surprisal (sum over subwords) with whitespace-trailing decoding correction
  - Temporal Alignment Layer: Word surprisal → HRF convolution (canonical double-gamma) → BOLD-aligned predictors
  - Regression Layer: Convolved surprisal + baseline predictors (word position, length) → LME model → held-out log-likelihood evaluation
  - Model Zoo: 17 variants across three families (GPT-2: 124M–1.6B; GPT-Neo: 125M–20B; OPT: 125M–66B)

- Critical path:
  1. Use model-specific tokenizer (not shared across families)
  2. Apply whitespace-trailing decoding to prevent word probabilities > 1
  3. Convolve with HRF matching dataset TR (2s for Natural Stories; sentence-level for Pereira)
  4. Fit LME starting from maximal structure; prune by-item random effects before by-subject

- Design tradeoffs:
  - **Continuous BOLD vs sentence-final surprisal**: Natural Stories uses full time-series with HRF; Pereira uses only sentence-final word surprisal—loses granularity but matches dataset structure.
  - **Token vs word granularity**: Token-level is standard; Oh and Schuler (2025) shows small effect on scaling relationship, so word-level aggregation is acceptable.
  - **Model family diversity vs scale ceiling**: Three families improve generalization, but 66B ceiling (OPT) leaves billion-parameter models untested.

- Failure signatures:
  - **LME non-convergence**: Random effects structure too complex; prune iteratively (by-item first, then by-subject)
  - **Temporal misalignment**: HRF not applied or parameters incorrect; verify 4-6s peak shift in predictors
  - **Tokenization leakage**: Using wrong tokenizer for model variant; check vocabulary match
  - **Probability > 1**: Whitespace-leading tokens misallocated; ensure whitespace-trailing decoding

- First 3 experiments:
  1. **Scaling replication**: Run GPT-2 small/medium/large/XL on Natural Stories fMRI fit set; plot log-likelihood vs log(parameter count) on held-out set to confirm inverse slope (expected p < 0.01).
  2. **Whitespace handling validation**: Compare word surprisal with and without whitespace-trailing decoding on 100 sentences; quantify probability > 1 occurrences and effect on regression fit.
  3. **Random effects sensitivity**: Fit both maximal and simplified LME models; verify inverse scaling direction and significance are robust to random effects structure choice.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the inverse scaling relationship between model size and predictive power generalize to languages other than English?
- Basis in paper: [explicit] The authors state in the Limitations section that because models were trained on English and datasets were from English speakers, findings may or may not be replicated in other languages.
- Why unresolved: Morphological and syntactic differences across languages could alter how model capacity affects surprisal estimates relative to human neural processing.
- What evidence would resolve it: Replication of this study using multilingual fMRI datasets and non-English language models.

### Open Question 2
- Question: To what extent does word frequency drive the inverse scaling effect observed in fMRI data?
- Basis in paper: [inferred] The authors speculate the deviation is due to larger LMs predicting rare words better (as seen in reading times), but this specific mechanism is not directly tested on the neural data in this study.
- Why unresolved: While frequency effects explain inverse scaling in behavioral data, it is unconfirmed whether the neurobiological response to surprisal diverges from model estimates for the same reasons.
- What evidence would resolve it: An analysis of fMRI model fits controlling for word frequency or specific lexical classes to isolate the driver of the poor fit.

### Open Question 3
- Question: Why does surprisal exhibit inverse scaling while model representations (vectors) often show positive scaling with brain activity?
- Basis in paper: [inferred] The introduction notes that recent work using vectors directly from LLMs observes positive scaling on brain imaging data, contrasting with the negative scaling found here using surprisal.
- Why unresolved: This discrepancy suggests that as models grow, their internal activations become more brain-like, but their output probabilities become less human-like.
- What evidence would resolve it: A comparative study evaluating both surprisal and vector embeddings from identical model sets on the same fMRI datasets.

## Limitations
- The study is limited to English-language models and datasets, leaving open whether findings generalize to other languages
- The 66B parameter ceiling leaves untested whether the inverse scaling trend continues at trillion-parameter scales
- The mechanism driving the inverse scaling (frequency vs. other factors) is not definitively established in the neural data context

## Confidence
- **High Confidence**: The basic experimental design (HRF convolution, LME modeling, permutation testing) is sound and follows established neuroimaging encoding analysis protocols. The negative correlation between model size and predictive power is statistically robust within the tested datasets.
- **Medium Confidence**: The attribution of the effect to frequency-driven mechanisms in open-class words is plausible but not definitively established in the fMRI context. The claim that this extends behavioral reading-time findings to neural data is supported but would benefit from additional neural datasets.
- **Low Confidence**: Whether the inverse scaling will persist at scales beyond 66B parameters, or under different experimental paradigms (visual vs. auditory presentation, different comprehension tasks), remains speculative without further testing.

## Next Checks
1. **Cross-dataset replication**: Apply the same analysis pipeline to an independent fMRI language dataset with different acquisition parameters (e.g., different TR, field strength, or task design) to test the robustness of the inverse scaling relationship.
2. **Frequency effect decomposition**: Conduct a post-hoc analysis isolating the contribution of rare word surprisal to the inverse scaling effect by comparing predictive power on high-frequency vs. low-frequency word subsets within the same models.
3. **Scale extrapolation test**: Fit the inverse scaling relationship to models up to 66B parameters, then use the fitted curve to predict fMRI predictive power at 175B and 1T parameters, comparing against actual measurements when available or using model-based simulations.