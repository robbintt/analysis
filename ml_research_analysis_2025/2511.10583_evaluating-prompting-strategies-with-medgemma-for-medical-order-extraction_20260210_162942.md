---
ver: rpa2
title: Evaluating Prompting Strategies with MedGemma for Medical Order Extraction
arxiv_id: '2511.10583'
source_url: https://arxiv.org/abs/2511.10583
tags:
- order
- clinical
- medical
- extraction
- orders
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluated three prompting strategies for extracting
  medical orders from doctor-patient conversations using MedGemma, a domain-specific
  language model. The strategies compared were one-shot prompting, ReAct reasoning,
  and an agentic workflow approach.
---

# Evaluating Prompting Strategies with MedGemma for Medical Order Extraction

## Quick Facts
- arXiv ID: 2511.10583
- Source URL: https://arxiv.org/abs/2511.10583
- Reference count: 7
- Primary result: One-shot prompting outperformed ReAct and agentic workflows for medical order extraction from doctor-patient transcripts

## Executive Summary
This paper evaluates three prompting strategies—one-shot prompting, ReAct reasoning, and an agentic workflow—for extracting structured medical orders from clinical conversations using MedGemma, a domain-specific language model. Experiments on manually annotated clinical transcripts showed that the simple one-shot method consistently outperformed the more complex frameworks, achieving the highest scores across multiple metrics including order type classification and description extraction. The findings suggest that for well-annotated clinical data, complex reasoning chains can introduce noise and reduce accuracy compared to direct extraction approaches.

## Method Summary
The study compared three prompting strategies using MedGemma-4B and 27B models on the SIMORD dataset of doctor-patient conversations. One-shot prompting used a single example to generate structured JSON outputs, while ReAct employed iterative reasoning-action cycles, and the agentic workflow used a 4-agent pipeline for detection, mapping, structuring, and validation. The evaluation measured ROUGE-1 F1 for description/reason extraction, strict F1 for order type classification, and multi-label F1 for provenance tracking.

## Key Results
- One-shot prompting achieved 0.602 F1 for order type classification and 0.516 Rouge1-F1 for description extraction with the 4B model
- The 27B model improved one-shot performance to 0.549 average score across all metrics
- ReAct and agentic frameworks consistently underperformed the one-shot approach, with the 27B agentic workflow scoring 0.465 average

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific pre-training improves clinical extraction accuracy over general-purpose models
- Mechanism: MedGemma's medical domain adaptation provides stronger baseline understanding of clinical terminology, enabling better in-context learning for order extraction tasks without task-specific fine-tuning
- Core assumption: Domain corpus alignment between pre-training and evaluation data transfers to structured extraction tasks
- Evidence anchors: Abstract states "MedGemma consistently outperformed the base model across all evaluation metrics"; section 6 reports significant improvements with domain-specific fine-tuning

### Mechanism 2
- Claim: Simpler prompting strategies can outperform complex reasoning chains on well-structured clinical data
- Mechanism: Complex frameworks like ReAct and agentic workflows generate intermediate reasoning steps that can fabricate relationships or misinterpret nuances ("analytical over-processing"), introducing noise. One-shot prompting maintains direct constrained generation, reducing error propagation
- Core assumption: The SIMORD dataset's clean annotations and structured format mean additional inference layers add variance without compensatory signal gain
- Evidence anchors: Abstract posits that "complex reasoning chains can lead to 'overthinking' and introduce noise"; section 6 reports ReAct and Agentic frameworks consistently underperformed relative to simpler 1-Shot approach

### Mechanism 3
- Claim: Increased model scale improves extraction performance across all prompting strategies while preserving relative strategy rankings
- Mechanism: Larger parameter count (27B vs 4B) provides greater capacity for capturing medical terminology variations and contextual relationships, improving both text generation and classification metrics proportionally
- Core assumption: Model scaling benefits are additive to prompting strategy effects rather than interactive
- Evidence anchors: Abstract states "The larger 27B model further improved performance"; Table 3 shows MedGemma-27B one-shot reaching 0.549 average score versus 4B's 0.436 average

## Foundational Learning

- **In-Context Learning (Few-Shot Prompting)**
  - Why needed here: One-shot prompting relies entirely on the model's ability to generalize task structure from a single example without weight updates
  - Quick check question: Can you explain why increasing the number of examples might not improve performance if examples are too similar to each other?

- **ReAct (Reasoning + Acting) Paradigm**
  - Why needed here: The ReAct approach structures extraction as iterative thought-action-observation cycles; understanding this helps diagnose where "overthinking" errors originate
  - Quick check question: In a ReAct loop, what happens if the observation validation step has false positives (accepting incorrect extractions)?

- **Multi-Agent Pipeline Decomposition**
  - Why needed here: The agentic workflow separates detection, mapping, structuring, and validation into specialized "agents"; understanding task decomposition helps identify failure propagation points
  - Quick check question: If Agent 1 (Identifier) misses an order, can downstream agents recover? Why or why not?

## Architecture Onboarding

- **Component map:** Input transcript → Prompt construction → Model inference → JSON validation → Structured output
- **Critical path:** 1) Transcript preprocessing (turn segmentation, physician utterance identification) → 2) Prompt construction (example selection for one-shot, constraint specification) → 3) Model inference (direct generation vs. iterative reasoning) → 4) JSON validation and normalization (order type mapping, duplicate removal)
- **Design tradeoffs:** Simplicity vs. robustness (one-shot is computationally efficient but may struggle with noisy inputs; ReAct/Agentic add overhead but provide interpretable reasoning traces); Model size vs. cost (27B improves accuracy but makes agentic workflows expensive)
- **Failure signatures:** Hallucinated orders (model generates orders not present in transcript); Example replication (model copies orders from few-shot example instead of extracting from target transcript); Reason-order misalignment (mapper agent pairs wrong clinical justification with order)
- **First 3 experiments:** 1) Baseline reproduction: Run one-shot prompting with MedGemma-4B on 5 sample transcripts; verify JSON output structure matches schema and calculate per-field accuracy against manual labels; 2) Noise sensitivity test: Inject synthetic ASR errors into 3 transcripts; compare one-shot vs. ReAct performance degradation; 3) Ablation on example quality: Swap the one-shot example for a different clinical scenario; measure impact on order type classification F1

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the relative performance of prompting strategies shift in favor of complex frameworks when processing noisy, real-world ASR transcripts compared to clean, manually annotated transcripts? Basis: Authors explicitly state finding favoring 1-Shot is dataset-dependent and future work should explore noisier data.
- **Open Question 2:** How does the performance of the Agentic workflow change when implemented as a decentralized multi-agent system with independent model instances compared to the single-context simulation used in this study? Basis: Authors note their Agentic workflow was "implemented within a single model" and admit a true multi-agent system could behave differently.
- **Open Question 3:** Can specific constraints or fine-tuning on reasoning steps mitigate the "analytical over-processing" and hallucinations observed in the ReAct and Agentic frameworks? Basis: Authors infer complex frameworks fail due to "analytical over-processing" where models "fabricate relationships" but don't investigate if this is controllable.

## Limitations

- Dataset and model accessibility: Neither SIMORD dataset nor MedGemma model weights are directly specified, preventing independent validation
- Evaluation framework ambiguity: Multi-label provenance F1 implementation details unclear, particularly for overlapping or nested provenance spans
- Limited testing scope: Results based on clean, manually annotated transcripts without testing on noisy real-world clinical data

## Confidence

- **High Confidence:** Domain-specific pre-training improves baseline performance (supported by direct comparisons between MedGemma and base Gemma models)
- **Medium Confidence:** Simpler prompting strategies outperform complex ones on clean clinical data (strong empirical results but limited testing across diverse transcript qualities)
- **Low Confidence:** Model scaling benefits are additive to prompting strategy effects (insufficient data to rule out interaction effects between model size and prompt complexity)

## Next Checks

1. **Dataset Verification:** Obtain and run exact evaluation scripts on SIMORD to confirm reported metric calculations, particularly multi-label provenance F1 implementation
2. **Model Availability Test:** Verify MedGemma-4B and 27B models are publicly accessible and reproduce baseline one-shot performance on 5 sample transcripts
3. **Noise Sensitivity Validation:** Systematically introduce synthetic transcription errors into transcripts to empirically test whether complex reasoning chains become necessary under degraded input quality, probing the stated break condition