---
ver: rpa2
title: Stochastic Linear Bandits with Parameter Noise
arxiv_id: '2601.23164'
source_url: https://arxiv.org/abs/2601.23164
tags:
- regret
- bound
- lemma
- algorithm
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies stochastic linear bandits under the parameter
  noise model, where the reward is a linear function of an action vector and a parameter
  vector drawn from a fixed distribution. The authors provide algorithms with variance-dependent
  regret bounds that can outperform the general minimax bounds for specific action
  sets.
---

# Stochastic Linear Bandits with Parameter Noise

## Quick Facts
- **arXiv ID**: 2601.23164
- **Source URL**: https://arxiv.org/abs/2601.23164
- **Reference count**: 40
- **Primary result**: Achieves variance-dependent regret bounds for stochastic linear bandits under parameter noise model, outperforming general minimax bounds for specific action sets.

## Executive Summary
This paper studies stochastic linear bandits under the parameter noise model, where the reward is a linear function of an action vector and a parameter vector drawn from a fixed distribution. The authors provide algorithms with variance-dependent regret bounds that can outperform the general minimax bounds for specific action sets. For general finite action sets, they present an optimal design-based successive elimination algorithm with regret O(d² + √(dT log(K/δ) σ²max)), where σ²max is the maximal variance. For ℓp unit balls with p ≤ 2, they introduce a simple explore-exploit algorithm achieving O(d + √(dT σ²q)) regret when the covariance matrix is known, and O(d^(2/3+2/3q)T^(1/3) + √(dT σ²q)) when unknown. The authors also provide matching lower bounds showing these regret bounds are tight up to logarithmic factors.

## Method Summary
The paper presents two main algorithms: VASE for general finite action sets using G-optimal design and variance-weighted least squares, and VALEE for ℓp unit balls (p ≤ 2) using coordinate-wise exploration. VASE computes an optimal design to minimize maximum variance, then uses weighted least squares with exploration counts proportional to estimated variance. VALEE explores only standard basis vectors using median-of-means estimation, then exploits the computed optimal action based on the ℓq dual norm geometry. Both algorithms leverage the key insight that parameter noise allows the learner to control reward variance through action selection, enabling tighter concentration bounds than additive noise models.

## Key Results
- VASE achieves O(d² + √(dT log(K/δ) σ²max)) regret for general finite action sets
- VALEE achieves O(d + √(dT σ²q)) regret for ℓp unit balls (p ≤ 2) with known covariance
- VALEE achieves O(d^(2/3+2/3q)T^(1/3) + √(dT σ²q)) regret for ℓp unit balls with unknown covariance
- Matching lower bounds prove these regret bounds are tight up to logarithmic factors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parameter noise model enables variance-dependent regret bounds through learner-controlled variance.
- Mechanism: When the learner selects action a_t, the reward variance equals a_t^T Σ a_t. This differs from additive noise where Var(X_t) is environment-fixed. The learner can choose low-variance actions to accelerate estimation, and the linear structure of reward realizations (not just expectations) yields tighter concentration than additive noise for equivalent sample budgets.
- Core assumption: The reward vector θ_t is sampled i.i.d. from a fixed distribution ν with bounded support on the ℓq unit ball.
- Evidence anchors:
  - [abstract] "reward of action a is a^T θ where θ is sampled i.i.d."
  - [section 1, page 2] "in the parameter noise model, the learner can influence the variance of the reward... Var(X_t) = a_t^T Σ a_t"
  - [corpus] No directly comparable variance-control mechanism in neighbor papers; most assume additive noise.
- Break condition: If θ_t is adversarially chosen (not i.i.d.), variance-awareness breaks and regret reverts to O~(√dT) adversarial bounds.

### Mechanism 2
- Claim: Weighted least squares with variance-weighted exploration reduces sample complexity while maintaining estimation error.
- Mechanism: VASE computes a G-optimal design π_ℓ on the active set, estimates per-action variance σ̂²_ℓ(a), and sets exploration counts as T_ℓ(a) ∝ σ̂²_ℓ(a)π_ℓ(a)/ε²_ℓ. The weighted least squares estimator θ̂^(ℓ) = V_ℓ^(-1) Σ σ̂^(-2)_ℓ(a_s) a_s X_s maintains the same covariance structure as unweighted regression despite fewer samples per action.
- Core assumption: The action set spans R^d (or a subspace thereof) and variances can be estimated within multiplicative constants.
- Evidence anchors:
  - [section 3.1, page 5] "T_ℓ(a) ← 49d/ε² · log(1/δ) · σ²(a)π(a)"
  - [lemma 3.6, page 5] "with probability 1−δ: |a^T(θ̂^(ℓ) - θ*)| ≤ ε_ℓ"
  - [corpus] Neighbor papers on contextual bandits use similar design-based exploration but without variance-weighting.
- Break condition: If variances are estimated poorly (e.g., adversarial noise in variance estimation), the weighted estimator may have inflated error, causing premature arm elimination.

### Mechanism 3
- Claim: For ℓp balls with p ≤ 2, simple coordinate-wise exploration achieves minimax optimal regret.
- Mechanism: VALEE explores only standard basis vectors e_i, using median-of-means to estimate each θ*_i with error √(Σ_ii · ε̃_j). The action set geometry (curvature for p=2, sparsity-inducing for p<2) ensures that the optimal action a* has a simple form: a*_i = sign(θ*_i)|θ*_i|^(q-1)/||θ*||^(q-1)_q. Estimating θ* suffices to identify a*.
- Core assumption: The action set contains standard basis vectors and has ℓp unit ball geometry with p ∈ (1, 2].
- Evidence anchors:
  - [section 3.2, page 7] "exploration phase plays only the standard basis vectors"
  - [lemma B.5, page 17] proves optimal action formula a(θ)_i = sign(θ_i)|θ_i|^(q-1)/||θ||^(q-1)_q
  - [corpus] Related work on constrained/safe bandits often requires second-order cone programs; VALEE is comparatively simple.
- Break condition: If p > 2, the action set geometry changes fundamentally; lower bounds show O~(d√Tσ²) regret is unavoidable, and coordinate-wise exploration is suboptimal.

## Foundational Learning

- Concept: **G-optimal design**
  - Why needed here: VASE uses G-optimal designs to select a small support set (≤ d(d+1)/2 actions) that minimizes maximum variance of the least squares estimator.
  - Quick check question: Can you explain why g(π*) = d for an optimal design, and how this relates to covering-based approaches for infinite action sets?

- Concept: **Median-of-means estimation**
  - Why needed here: VALEE uses MoM with κ = O(log(d log(T)/δ)) batches to achieve sub-Gaussian concentration even when reward distributions may have heavier tails than Gaussian.
  - Quick check question: Given κ independent estimators where each succeeds with probability ≥ 3/4, why does taking the median succeed with probability ≥ 1 - e^(-κ/8)?

- Concept: **Dual norms and ℓp-ℓq duality**
  - Why needed here: The regret bounds involve σ²_q defined via ℓ_(q/2) norms of diagonal covariance entries. Understanding why 1/p + 1/q = 1 matters for the optimal action characterization.
  - Quick check question: For p = 2 (the ℓ2 unit ball), what is q, and what does the optimal action formula simplify to?

## Architecture Onboarding

- Component map:
  - Variance Estimation (Algorithm 1) -> VASE (Algorithm 2) -> Optimal Action Selection
  - Variance Estimation (Algorithm 1) -> VALEE (Algorithm 3) -> Optimal Action Selection

- Critical path:
  1. Implement stopping rule (Algorithm 1) correctly—the threshold τ must be set to balance estimation cost vs. regret penalty. For unknown Σ, τ ≈ d^(1/3)T^(-1/3) is optimal.
  2. Ensure design computation uses Frank-Wolfe for efficiency (O(d log log d) support) rather than exact Kiefer-Wolfowitz (O(d^2) support).
  3. Verify elimination criterion in VASE uses the correct tolerance (2ε_ℓ) and doesn't eliminate the optimal arm (union bound over all phases).

- Design tradeoffs:
  - Known vs. unknown covariance: With known Σ, VALEE achieves O~(d + √dTσ²_q). Unknown Σ adds O~(d^(2/3+2/3q)T^(1/3)).
  - VASE vs. VALEE for ℓ2 ball: VALEE is simpler (no design computation) and achieves better dimension dependence, but VASE generalizes to arbitrary finite action sets.
  - Support size vs. regret: Larger design support increases d^2 term in VASE regret but may reduce variance term.

- Failure signatures:
  - Regret scaling as d√T on ℓ2 ball: Indicates VALEE's variance estimation failed or τ was misconfigured. Check that σ̂²_q estimates are within multiplicative constants of true σ²_q.
  - Optimal arm eliminated in VASE: Elimination tolerance was too aggressive or design didn't cover action space. Verify E_lss holds (Lemma A.9) and check union bound parameters.
  - Exploration phase never terminates: Norm estimate ||θ̂^(j)||_q never drops below threshold. Check median-of-means batch count κ and verify Σ_ii estimates are bounded.

- First 3 experiments:
  1. Sanity check on synthetic ℓ2 ball: Generate θ_t ~ N(θ*, σ²I) with small σ, run VALEE with known Σ, verify regret ≈ O~(√dTσ) beats adversarial O~(√dT) baseline.
  2. Ablation on variance estimation: Compare VALEE with true Σ vs. estimated Σ̂ on same problem. Measure the O~(dT^(1/3)) gap to confirm theoretical penalty.
  3. Geometry stress test: Run VASE on discretized ℓ3 ball (where p > 2) and verify regret scales as O~(d√T), confirming the lower bound regime. Compare to VALEE (which should fail or underperform).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an algorithm achieve optimal regret for ℓp unit balls (p ≤ 2) when the covariance matrix is unknown?
- Basis: [explicit] Section 5 states that finding an algorithm which is optimal when the covariance matrix is unknown is "left to future work."
- Why unresolved: The proposed algorithm (VALEE) incurs an additive regret term of d^(2/3+2/3q)T^(1/3) when the covariance is unknown, which is suboptimal compared to the O~(√dTσ²_q) lower bound.
- What evidence would resolve it: An algorithm achieving regret of O~(√dTσ²_q) on ℓp balls (p ≤ 2) without prior knowledge of Σ.

### Open Question 2
- Question: Does there exist an algorithm for ℓp unit balls with p > 2 that matches the d√Tσ²_q lower bound?
- Basis: [explicit] Section 5 identifies finding an algorithm matching the lower bound for p > 2 as a "natural and interesting open question."
- Why unresolved: The paper establishes a lower bound of O~(d√Tσ²_q) for this geometry but does not provide an algorithm achieving a matching upper bound.
- What evidence would resolve it: An algorithm with a proven upper bound of O~(d√Tσ²_q) for action sets that are ℓp unit balls with p > 2.

### Open Question 3
- Question: Can the regret bound for general finite action sets be tightened when the relationship between log(K) and dimension d deviates significantly?
- Basis: [inferred] The abstract states the provided lower bound is tight "whenever log(K) ≈ d," implicitly suggesting the bounds may not be tight for all action set sizes relative to the dimension.
- Why unresolved: The upper bound scales as O~(√dT log K) while the lower bound is O~(d√T), leaving a gap when log K is much smaller or larger than d.
- What evidence would resolve it: A refined lower bound or algorithm proving the minimax regret is independent of the specific log(K) ≈ d condition.

## Limitations

- The assumption of i.i.d. parameter noise is essential—adversarial parameter selection would break the variance control mechanism
- Algorithm performance depends critically on accurate variance estimation, which may be challenging in practice with limited samples
- Logarithmic factors in the regret bounds are not optimized and may be loose

## Confidence

- **High confidence**: The mechanism of learner-controlled variance through parameter noise selection, and the finite action set regret bound O~(d² + √dT log(K/δ) σ²max)
- **Medium confidence**: The ℓp ball results with coordinate-wise exploration for p ≤ 2, as the transition at p = 2 requires careful verification of the optimal action characterization
- **Low confidence**: The exact constants and logarithmic factors in the regret bounds, which are not optimized in the analysis

## Next Checks

1. Verify the G-optimal design implementation produces support size O(d log log d) rather than O(d²) to confirm the Frank-Wolfe approximation
2. Test VALEE on a synthetic ℓ3 ball problem to confirm the O~(d√T) lower bound regime
3. Implement ablation experiments comparing weighted vs. unweighted least squares in VASE to quantify the variance estimation benefit