---
ver: rpa2
title: 'R-GenIMA: Integrating Neuroimaging and Genetics with Interpretable Multimodal
  AI for Alzheimer''s Disease Progression'
arxiv_id: '2512.18986'
source_url: https://arxiv.org/abs/2512.18986
tags:
- disease
- genetic
- alzheimer
- multimodal
- brain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: R-GenIMA is an interpretable multimodal AI framework that integrates
  structural MRI and genetic SNP data for Alzheimer's disease progression analysis.
  The core innovation is a ROI-wise vision transformer (RiT) that decomposes 3D MRI
  into anatomically parcellated regions, preserving fine-grained neuroanatomical detail,
  combined with genetic prompting to encode SNPs as structured text for joint cross-modal
  attention with the LLM.
---

# R-GenIMA: Integrating Neuroimaging and Genetics with Interpretable Multimodal AI for Alzheimer's Disease Progression

## Quick Facts
- arXiv ID: 2512.18986
- Source URL: https://arxiv.org/abs/2512.18986
- Reference count: 40
- Primary result: 99.01% accuracy and 98.50% F1 for four-way AD stage classification

## Executive Summary
R-GenIMA is an interpretable multimodal AI framework that integrates structural MRI and genetic SNP data for Alzheimer's disease progression analysis. The core innovation is a ROI-wise vision transformer (RiT) that decomposes 3D MRI into anatomically parcellated regions, preserving fine-grained neuroanatomical detail, combined with genetic prompting to encode SNPs as structured text for joint cross-modal attention with the LLM. This design overcomes limitations of whole-brain 3D CNNs and late-fusion methods by maintaining regional specificity and enabling biologically meaningful genotype-phenotype associations. Evaluated on the ADNI cohort, R-GenIMA achieves state-of-the-art four-way classification accuracy of 99.01% and F1 of 98.50% on the Mixture Dataset, surpassing unimodal and conventional multimodal baselines. Attention-based attribution reveals stage-specific neuroanatomical signatures—shared vulnerability hubs across disease stages with stage-specific patterns in striatal, frontotemporal, and cerebellar regions—and consistent enrichment for established GWAS-supported AD risk loci (APOE, BIN1, CLU, RBFOX1).

## Method Summary
R-GenIMA uses a two-stage training approach: first training a ROI-wise vision transformer (RiT) on 4-way AD stage classification, then fine-tuning a multimodal large language model (LLM) with frozen visual encoders. T1 MRI data is parcellated into 109 anatomically defined ROIs using the Desikan-Killiany atlas, each resampled to 96³ voxels and processed by a 3D convolutional patch encoder to produce single visual tokens. SNPs from 105 candidate genes (1,079 SNPs after QC) are serialized into structured text and encoded via the LLM's tokenizer. The RiT outputs and genetic text embeddings are projected to a common space through a connector layer, enabling cross-modal attention fusion within the LLM backbone (Llama3-8B or Qwen2.5-7B). The model is trained on three configurations: Gene-only, Image-Gene (paired), and Mixture (50% paired + 50% SNP-only), using AdamW optimizer with bfloat16 precision on 8×A100 GPUs.

## Key Results
- Achieves 99.01% accuracy and 98.50% F1 on four-way AD stage classification (NC, SMC, MCI, AD)
- Outperforms unimodal and conventional multimodal baselines, with cross-modal attention fusion significantly better than late-fusion (concatenation)
- Identifies stage-specific neuroanatomical signatures with consistent enrichment for established GWAS-supported AD risk loci
- Demonstrates superior performance of LLM-based genetic encoders (81% accuracy) compared to MLP baselines (38.82%)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: ROI-level decomposition preserves fine-grained neuroanatomical detail lost by whole-brain 3D CNNs.
- **Mechanism**: Each T1 MRI is parcellated into 109 anatomically defined ROIs (96³ voxels each). Each ROI becomes a single visual token via 3D convolution, then refined by a lightweight ViT to model inter-ROI dependencies. This retains regional structural fidelity while capturing global brain organization.
- **Core assumption**: Disease-relevant structural changes are regionally localized rather than globally distributed; the Desikan-Killiany atlas boundaries align with disease-relevant anatomy.
- **Evidence anchors**:
  - [abstract]: "core innovation is a ROI-wise vision transformer (RiT) that decomposes 3D MRI into anatomically parcellated regions, preserving fine-grained neuroanatomical detail"
  - [Section 4.2.1]: "Each 96³ ROI patch is embedded using a 3D convolutional patch encoder whose kernel spans the entire spatial extent of the ROI... producing a single visual token per region"
  - [corpus]: Limited direct comparison; neighbor papers discuss regional brain experts and multimodal fusion but not the ROI-token approach specifically.
- **Break condition**: If anatomical boundaries in the atlas do not capture disease-relevant regions (e.g., subcortical microstructures), tokenization will blur meaningful signals. Performance would degrade when tested on populations with different brain morphologies.

### Mechanism 2
- **Claim**: Encoding SNPs as structured text enables the LLM to apply biomedical priors for cross-modal gene-brain alignment.
- **Mechanism**: SNP sequences are serialized into natural-language text and embedded via the LLM's tokenizer. This allows the model to treat genetic variants as structured semantic inputs rather than unordered feature vectors, leveraging pretrained biomedical knowledge. Cross-modal attention then links genetic tokens to ROI visual tokens.
- **Core assumption**: LLMs pretrained on text have transferable biomedical knowledge relevant to SNP interpretation; gene order permutation invariance preserves biological meaning.
- **Evidence anchors**:
  - [abstract]: "genetic prompting to encode SNPs as structured text for joint cross-modal attention with the LLM"
  - [Section 3.1]: "the marked performance gap between LLM-based genetic encoders (≈81% accuracy) and MLP baselines (38.82%) underscores the value of treating SNPs sequences as structured semantic inputs"
  - [corpus]: Weak corpus support—neighbor papers focus on neuroimaging fusion, not genetic text encoding strategies.
- **Break condition**: If SNP textual representation loses critical interaction patterns (e.g., epistatic effects between non-adjacent variants), the model may fail to capture polygenic risk. Check whether permutation augmentation actually achieves invariance.

### Mechanism 3
- **Claim**: Bootstrap-validated attention stability reveals reproducible ROI–gene associations that track disease progression.
- **Mechanism**: For each ROI–gene pair, attention weights are aggregated across subjects and resampled 1,000 times. A stability metric normalizes mean attention by confidence interval width. High-stability pairs are interpreted as disease-relevant associations.
- **Core assumption**: Attention weights reflect meaningful biological relationships rather than spurious correlations; bootstrap resampling adequately controls for sampling noise.
- **Evidence anchors**:
  - [abstract]: "Attention-based attribution reveals stage-specific neuroanatomical signatures... and consistent enrichment for established GWAS-supported AD risk loci"
  - [Section 2.2]: "Fisher's Exact Test yielded P = 0.027; odds ratio = 5.58" for enrichment of model-prioritized genes vs. GWAS loci
  - [Section 4.3]: "Stability metric: ¯μr,g / (CI97.5% − CI2.5%), normalizes average attention strength by its uncertainty"
  - [corpus]: NeuroPathX (arxiv 2508.18303) proposes explainable imaging-genetics associations using attention mechanisms—partial support for attention-based interpretability.
- **Break condition**: If attention captures dataset-specific artifacts rather than biology, external validation will fail. The enrichment test against GWAS loci provides partial validation, but generalization to non-ADNI cohorts remains unproven.

## Foundational Learning

- **Concept: Cross-modal attention in Transformers**
  - Why needed here: R-GenIMA uses cross-modal attention to let genetic tokens "query" image tokens, learning which brain regions each genetic variant attends to. This is the core fusion mechanism.
  - Quick check question: Given genetic features F_gene and image features F_img, write the cross-attention formula. Which modality provides Queries?

- **Concept: Bootstrap resampling for stability estimation**
  - Why needed here: Raw attention weights are noisy. The paper uses 1,000 bootstrap iterations to identify ROI–gene pairs that are reproducibly high-attention, not just artifacts.
  - Quick check question: Why normalize stability by confidence interval width rather than using raw mean attention?

- **Concept: ROI-based brain parcellation (Desikan-Killiany atlas)**
  - Why needed here: The RiT encoder requires anatomically consistent regions across subjects. The choice of atlas determines what "fine-grained" means.
  - Quick check question: What happens if an atlas region spans two functionally distinct areas—one affected by disease, one not?

## Architecture Onboarding

- **Component map**:
  - T1 MRI → FreeSurfer parcellation → 109 ROI volumes (96³) → RiT encoder (Conv3D + ViT) → 109 visual tokens
  - SNPs → QC filtering → text serialization → LLM tokenizer → genetic embeddings
  - Visual tokens + genetic embeddings → Connector (linear projection) → LLM backbone (Llama3-8B or Qwen2.5-7B)
  - LLM → Cross-modal attention fusion → Classification output
  - Inference → Attention rollout → Bootstrap stability analysis

- **Critical path**:
  1. Preprocess MRI → FreeSurfer parcellation → 109 ROI volumes (96³)
  2. Preprocess SNPs → QC filtering (MAF > 0.05, HWE p > 10⁻⁶) → serialize to text
  3. Stage 1: Train RiT and Med3D on 4-way classification (frozen afterward)
  4. Stage 2: Fine-tune Connector + LLM with frozen visual encoders
  5. Inference → Attention rollout → Bootstrap stability analysis

- **Design tradeoffs**:
  - ROI-patched vs. whole-brain: ROI preserves detail but assumes atlas alignment; whole-brain is simpler but loses regional specificity.
  - Candidate gene panel (105 genes) vs. genome-wide: Targeted approach enables interpretability but limits novel discovery.
  - Mixture Dataset (SNPs-only + paired samples) vs. paired-only: Heterogeneous training improves accuracy but complicates evaluation.

- **Failure signatures**:
  - Accuracy drops on external cohorts: Likely atlas mismatch or population-specific genetic architecture.
  - Attention heatmap looks random: Connector may not have converged; check learning rate and gradient flow.
  - ROI–gene associations don't generalize: Bootstrap stability may be overfitting to ADNI-specific noise.

- **First 3 experiments**:
  1. **Unimodal baselines**: Train MLP, BERT, Llama, Qwen on SNPs-only data. Verify LLM advantage (~81% vs. 38% for MLP) before adding imaging.
  2. **Ablate fusion strategy**: Compare late-fusion (concat) vs. cross-modal attention using BERT + Med3D. Confirm cross-modal attention improves F1.
  3. **ROI vs. whole-brain**: Replace RiT with Med3D in the full R-GenIMA pipeline. Quantify accuracy drop (expected ~2–3% based on Table 1).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Will R-GenIMA's identified neuroimaging-genetic associations generalize to populations with diverse ancestry and socioeconomic backgrounds?
- **Basis in paper**: [explicit] The authors note in Section 3.6 that the ADNI cohort is predominantly non-Hispanic White with high educational attainment, and they explicitly call for external validation across diverse ancestry groups.
- **Why unresolved**: There is currently a lack of large-scale, paired imaging-genetic datasets outside of ADNI to test generalizability.
- **What evidence would resolve it**: Successful replication of the model's ROI-gene attention maps in independent, multi-ethnic cohorts.

### Open Question 2
- **Question**: How does expanding the input to genome-wide SNPs affect the model's computational tractability and attention stability?
- **Basis in paper**: [explicit] Section 3.6 states that the predefined 105-gene panel restricts the discovery of novel loci, and future work must address the "increased dimensionality and multiple testing concerns" of genome-wide data.
- **Why unresolved**: The current architecture optimizes for a restricted token set; scaling to millions of variants may introduce noise or memory bottlenecks.
- **What evidence would resolve it**: A modified architecture processing genome-wide variants that maintains stable, interpretable attention rollouts without diluting feature specificity.

### Open Question 3
- **Question**: Does the framework maintain high diagnostic performance when applied to clinical-grade data with heterogeneous acquisition protocols?
- **Basis in paper**: [explicit] The authors identify "robustness to variation in scanner hardware and acquisition parameters" as a challenge that must be addressed for translation to clinical implementation.
- **Why unresolved**: The study utilized the standardized ADNI protocol; real-world clinical MRI data often suffers from lower resolution and higher artifact rates.
- **What evidence would resolve it**: Prospective validation on raw clinical MRI scans showing robust performance despite noise and protocol variance.

## Limitations

- ROI tokenization assumes Desikan-Killiany atlas boundaries capture disease-relevant anatomy; atlas mismatch could blur meaningful signals.
- SNP text encoding relies on unproven assumption that LLM biomedical priors transfer to genotype-phenotype mapping.
- Attention-based interpretability depends on bootstrap stability, but generalization to non-ADNI cohorts remains unproven.

## Confidence

- **High Confidence**: Four-way classification accuracy (99.01%) and F1 (98.50%) on the Mixture Dataset are well-documented through cross-validation and compared against established baselines.
- **Medium Confidence**: Stage-specific neuroanatomical signatures and ROI-gene associations are reproducible within ADNI but require external validation. Enrichment test against GWAS loci (P=0.027) provides partial support.
- **Low Confidence**: Claim that structured text encoding enables LLM biomedical knowledge transfer is weakly supported; the specific mechanism (biomedical knowledge vs. better optimization) is not isolated.

## Next Checks

1. **External Cohort Validation**: Test R-GenIMA on an independent AD cohort (e.g., AIBL, OASIS) to verify that ROI-level patterns and genetic associations generalize beyond ADNI's specific population characteristics.

2. **Ablation of Genetic Encoding**: Replace the text-encoding approach with a standard MLP on the same SNP features to isolate whether the LLM's biomedical knowledge provides measurable advantage over conventional genetic encoders.

3. **Atlas Sensitivity Analysis**: Repeat the analysis using alternative parcellation schemes (e.g., Destrieux, subcortical atlases) to quantify how sensitive the model's attention patterns and accuracy are to anatomical boundaries.