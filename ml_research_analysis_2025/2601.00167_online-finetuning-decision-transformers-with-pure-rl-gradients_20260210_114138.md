---
ver: rpa2
title: Online Finetuning Decision Transformers with Pure RL Gradients
arxiv_id: '2601.00167'
source_url: https://arxiv.org/abs/2601.00167
tags:
- learning
- online
- grpo
- decision
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of online finetuning Decision
  Transformers (DTs) using pure reinforcement learning (RL) gradients. Existing online
  DT methods rely heavily on supervised objectives and face instability when applying
  importance-sampling-based RL algorithms like GRPO and PPO due to hindsight return
  relabeling.
---

# Online Finetuning Decision Transformers with Pure RL Gradients

## Quick Facts
- arXiv ID: 2601.00167
- Source URL: https://arxiv.org/abs/2601.00167
- Authors: Junkai Luo; Yinglun Zhu
- Reference count: 23
- Key outcome: This work addresses the challenge of online finetuning Decision Transformers (DTs) using pure reinforcement learning (RL) gradients. Existing online DT methods rely heavily on supervised objectives and face instability when applying importance-sampling-based RL algorithms like GRPO and PPO due to hindsight return relabeling. The authors propose removing hindsight relabeling and adapting GRPO to DTs (GRPO-DT) with three key modifications: sub-trajectory optimization for improved credit assignment, sequence-level importance ratios for stability, and active state sampling for better exploration. They also adapt PPO to DTs (PPO-DT). Experiments across 17 D4RL datasets show that GRPO-DT achieves new state-of-the-art performance, significantly outperforming baselines including ODT and ODT+TD3. PPO-DT is competitive but fails in some environments. The results demonstrate that pure-RL-based online finetuning is both viable and effective for DTs.

## Executive Summary
This paper addresses a critical limitation in online reinforcement learning for Decision Transformers: the incompatibility between hindsight return relabeling and importance-sampling-based RL algorithms. The authors identify that relabeling returns during training creates a mismatch in the importance sampling ratios, leading to instability. They propose GRPO-DT, which removes relabeling and introduces three key modifications: sub-trajectory optimization for better credit assignment, sequence-level importance ratios for stability, and active state sampling for exploration. Extensive experiments on 17 D4RL datasets demonstrate that GRPO-DT achieves state-of-the-art performance, outperforming existing online DT methods and showing that pure RL gradients can effectively finetune DTs online.

## Method Summary
The authors develop GRPO-DT, a pure-RL algorithm for online finetuning Decision Transformers that addresses the instability caused by hindsight return relabeling in importance-sampling-based methods. The approach involves three key modifications to standard GRPO: (1) sub-trajectory optimization, where rollouts are segmented at reset points to improve credit assignment, (2) sequence-level importance ratios that align with sequence-level advantages for stability, and (3) active state sampling based on action variance for better exploration. The algorithm collects full trajectories, samples reset points, generates sub-trajectories with advantages computed via group normalization, and updates the policy using sequence-level clipped objectives with KL penalty and adaptive entropy constraints. A Q-guided variant avoids environment resetting by using a learned critic for state initialization.

## Key Results
- GRPO-DT achieves new state-of-the-art performance across 17 D4RL datasets, significantly outperforming baselines including ODT and ODT+TD3.
- Removing hindsight return relabeling is critical for stability, as demonstrated by ablation studies showing relabeling causes importance ratio explosion.
- Sub-trajectory optimization with sequence-level importance ratios provides substantial improvements over full-trajectory RL objectives and token-level ratios.
- PPO-DT is competitive but fails in specific environments like Door-cloned-v1, highlighting the importance of GRPO's KL penalty and entropy constraints.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing hindsight return relabeling enables stable importance sampling-based RL finetuning of Decision Transformers.
- Mechanism: Hindsight return relabeling replaces intended RTG tokens ($g_{online}$) with actual achieved returns ($g_{actual}$). This creates a conditioning mismatch: actions sampled from $\pi_{\theta_{old}}(a|s, g_{online})$ are later trained as if drawn from $\pi_{\theta_{old}}(a|s, g_{actual})$. The importance sampling ratio $\frac{\pi_\theta(a|s,g)}{\pi_{\theta_{old}}(a|s,g)}$ becomes unreliable because the conditioning variable $g$ differs between rollout and training. Without relabeling, consistency is restored, enabling stable PPO/GRPO optimization.
- Core assumption: The policy is sufficiently conditioned on RTG tokens that changing them alters the action distribution meaningfully.
- Evidence anchors:
  - [abstract] "hindsight return relabeling...is fundamentally incompatible with importance sampling-based RL algorithms such as GRPO, leading to unstable training"
  - [section 3.1] "relabeling RTGs used during rollout with returns observed afterward introduces a mismatch in importance ratios, leading to unstable optimization"
  - [corpus] Weak direct support; related DT works focus on architecture innovations rather than this specific instability mechanism.
- Break condition: If policies are not meaningfully RTG-conditioned (i.e., $\pi(a|s,g) \approx \pi(a|s)$), the relabeling mismatch becomes negligible and removal yields minimal benefit.

### Mechanism 2
- Claim: Sub-trajectory optimization improves credit assignment over full-trajectory RL objectives.
- Mechanism: Full-trajectory GRPO assigns a single trajectory-level return uniformly to all timesteps, conflating credit across potentially hundreds of actions. Sub-trajectory optimization segments rollouts at $K$ reset points, generating $G$ sub-trajectories of length $L_{traj}$ each. Rewards are computed over $L_{traj} + L_{eval}$ steps (with expected actions during evaluation to reduce variance). This localizes advantage computation $\tilde{A}_{ki}$ to short, temporally coherent segments, improving gradient signal-to-noise.
- Core assumption: Actions' effects on returns are temporally localized enough that sub-trajectories capture meaningful causal structure.
- Evidence anchors:
  - [section 3.2] "a straightforward adaptation...would aggregate all stepwise rewards in a rollout...such a formulation performs poorly in RL environments"
  - [figure 2a] Sub-trajectory converges faster and higher than full trajectory on Ant-medium-v2 and Hopper-medium-v2
  - [corpus] No direct corpus evidence on sub-trajectory credit assignment in DTs; related works address different mechanisms.
- Break condition: If environments have extremely delayed rewards with no local structure, sub-trajectories may fail to capture relevant signal regardless of length.

### Mechanism 3
- Claim: Sequence-level importance ratios align better with sequence-level advantages, improving training stability.
- Mechanism: Standard GRPO computes token-level importance ratios $\frac{\pi_\theta(o_{i,h}|q,o_{i,<h})}{\pi_{\theta_{old}}(o_{i,h}|q,o_{i,<h})}$, but advantages $\tilde{A}_{ki}$ are computed at sequence level (Eq. 2). This creates a granularity mismatch. Sequence-level importance ratios $\frac{\pi_\theta(\tau^{sub}_{ki}|s_k,g_k)}{\pi_{\theta_{old}}(\tau^{sub}_{ki}|s_k,g_k)}$ align the optimization unit with the advantage signal, yielding more stable gradient estimates. The geometric mean normalization is removed to allow more aggressive updates for approximately on-policy sub-trajectories.
- Core assumption: Likelihoods across sequence elements are not so correlated that the product becomes unstable.
- Evidence anchors:
  - [section 3.2] "computing importance weights at the sequence level...significantly improves model performance"
  - [figure 2c] Sequence likelihood outperforms token likelihood on Door-cloned-v1
  - [corpus] Concurrent work (Zheng et al., 2025) independently proposes sequence-level ratios for LLMs, supporting the general principle.
- Break condition: If sequence lengths are very long (making likelihood products numerically unstable) or very short (providing no benefit over token-level), the advantage diminishes.

## Foundational Learning

- **Concept: Decision Transformers as conditional sequence models**
  - Why needed here: GRPO-DT builds on DT's autoregressive policy $\pi(a_h|s_h, g_h)$ conditioned on RTG tokens. Understanding RTG conditioning is essential to grasp why relabeling breaks importance sampling.
  - Quick check question: Can you explain why changing the RTG token $g$ between rollout and training affects the importance ratio $\frac{\pi_\theta(a|s,g)}{\pi_{\theta_{old}}(a|s,g)}$?

- **Concept: Importance sampling in policy gradient methods**
  - Why needed here: Both GRPO and PPO rely on importance ratios to reuse samples across policy updates. The relabeling issue is fundamentally about corrupted importance weights.
  - Quick check question: When does the importance sampling ratio $\frac{\pi_\theta}{\pi_{\theta_{old}}}$ become unreliable, and what clipping strategy does PPO use to mitigate this?

- **Concept: Credit assignment in reinforcement learning**
  - Why needed here: Sub-trajectory optimization addresses RL's credit assignment problem—determining which actions deserve credit/blame for observed returns. This is more acute in continuous control than language modeling.
  - Quick check question: Why might assigning a trajectory-level return uniformly to all actions perform worse than sub-trajectory-level assignment in a continuous control task?

## Architecture Onboarding

- **Component map:**
  - Policy network $\pi_\theta$: Transformer (4 layers, 4 heads, 512 dim) processing $(g, s, a)$ sequences
  - Full trajectory buffer $\mathcal{T}_{replay}$: FIFO, stores up to 32 complete trajectories
  - Sub-trajectory buffer $\mathcal{T}_{sub}$: FIFO, stores up to 2048 sub-trajectories with computed advantages
  - Reference policy $\pi_{ref}$: Slow-updated copy for KL penalty (updated every 4 iterations)
  - Optional Q-function $Q_\phi$: TD3-trained critic for Q-guided variant when resetting is infeasible

- **Critical path:**
  1. Roll out full trajectory with $\pi_\theta(\cdot|s_1, g_{online})$
  2. Sample $K$ reset points per trajectory using action-variance-based active sampling
  3. For each reset point, generate $G$ sub-trajectories of length $L_{traj}$, evaluate for $L_{eval}$ steps
  4. Compute advantages via normalized group comparison (Eq. 2)
  5. Update policy using sequence-level clipped objective with KL penalty (Eq. 3)

- **Design tradeoffs:**
  - $L_{traj}$: Shorter = finer credit assignment but more homogeneous groups; longer = more context but coarser credit. Sweet spot: 10-15 for MuJoCo
  - $L_{eval}$: Larger = more reliable reward estimates but more computation. Required: $L_{eval} \geq 30$, typically 100-400
  - Group size $G$: Larger = more stable advantage estimates but higher sample cost. Default: 8
  - Entropy constraint $\rho$: Higher = more exploration but potentially slower convergence

- **Failure signatures:**
  - Importance ratio explosion: Check if relabeling is accidentally enabled (ratios > 1000 indicate mismatch)
  - No learning from random data: Policy may have collapsed to a narrow mode; increase initial entropy dual variable $\kappa_{initial}$
  - Instability on Adroit: Reduce learning rate to $3 \times 10^{-5}$, increase batch size to 512
  - Sub-trajectory buffer starvation: Ensure $K \times G$ per trajectory fills buffer adequately

- **First 3 experiments:**
  1. **Sanity check—relabeling ablation**: Run GRPO-DT on Hopper-medium-v2 with and without hindsight relabeling. Expected: relabeling version shows ratio instability and lower final reward (per Fig. 1). If not observed, verify importance ratio logging.
  2. **Hyperparameter sweep—sub-trajectory length**: On Ant-medium-v2, test $L_{traj} \in \{5, 15, 50, 100\}$ with fixed $L_{eval}=200$. Expected: intermediate values (15) perform best (per Fig. 4c).
  3. **Architecture generalization—Reinformer**: Apply GRPO-DT to Reinformer (Zhuang et al., 2024) on Ant-medium-replay-v2. Expected: similar improvements over baseline, validating method independence from specific DT architecture (per Fig. 4a).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the failure of PPO-DT in specific environments be resolved by adopting sequence-level importance ratios instead of token-level ratios?
- Basis in paper: [inferred] The authors demonstrate that GRPO-DT succeeds using sequence-level ratios, while noting PPO-DT fails in environments like Door-cloned-v1 despite using token-level ratios (pp. 7, 9).
- Why unresolved: The paper attributes PPO-DT's failure to general instability but does not experimentally isolate the token-level importance ratio as the specific cause or test sequence-level alternatives for PPO.
- What evidence would resolve it: Ablation studies applying sequence-level importance ratios to PPO-DT on the specific environments where it currently fails.

### Open Question 2
- Question: Can the GRPO-DT framework scale to high-dimensional domains such as vision-based reinforcement learning or large language models?
- Basis in paper: [inferred] The introduction draws a parallel to LLM finetuning success (p. 1), yet the experimental validation is restricted to low-dimensional D4RL control tasks (p. 8).
- Why unresolved: The computational efficiency and stability of sub-trajectory resetting and active sampling are unproven for high-dimensional observation spaces or larger transformer backbones.
- What evidence would resolve it: Evaluation of GRPO-DT on vision-based benchmarks (e.g., Atari) or language tasks to assess scalability.

### Open Question 3
- Question: What are the precise trade-offs in sample complexity and asymptotic performance between the reset-based GRPO-DT and the Q-guided variant?
- Basis in paper: [inferred] The authors introduce Q-guided GRPO-DT to avoid environment resetting (p. 7), but Figure 4b suggests a performance gap compared to the reset-based method (p. 11).
- Why unresolved: The paper describes Q-guided GRPO-DT as "competitive" but does not quantify the performance loss or sample efficiency reduction relative to the reset mechanism.
- What evidence would resolve it: A comprehensive comparison of learning curves and final returns across all 17 datasets between the reset-based and Q-guided variants.

## Limitations
- Performance degradation on Adroit tasks requires reduced learning rate and increased batch size, indicating sensitivity to environment difficulty.
- Q-guided variant sacrifices some performance compared to reset-based method, suggesting a fundamental trade-off between computational efficiency and final returns.
- The method's effectiveness in high-dimensional domains (vision-based RL, LLMs) remains unproven despite theoretical parallels to language model finetuning.

## Confidence

**High Confidence Claims:**
- Removing hindsight return relabeling enables stable importance sampling-based RL finetuning of Decision Transformers.
- Sub-trajectory optimization improves credit assignment over full-trajectory RL objectives.
- Sequence-level importance ratios align better with sequence-level advantages, improving training stability.

**Medium Confidence Claims:**
- GRPO-DT achieves new state-of-the-art performance across 17 D4RL datasets.
- PPO-DT fails in specific environments due to general instability rather than specific design choices.
- Active state sampling based on action variance provides better exploration than random sampling.

**Low Confidence Claims:**
- The method's effectiveness in high-dimensional domains (vision-based RL, LLMs) remains unproven.
- The Q-guided variant's performance degradation is fundamental rather than a result of suboptimal hyperparameter tuning.
- The adaptive entropy dual variable update rule is robust across different environment types.

## Next Checks
1. **Implementation verification**: Implement GRPO-DT with the exact hyperparameters specified (4-layer transformer, 4 heads, 512 dim, LAMB optimizer, no relabeling) and verify importance ratio stability on Hopper-medium-v2.
2. **Ablation study**: Run controlled ablations removing each of the three key modifications (sub-trajectory optimization, sequence-level ratios, active sampling) on Ant-medium-v2 to quantify their individual contributions.
3. **Reproducibility check**: Replicate the full 17-dataset benchmark using the exact D4RL datasets and report normalized final rewards with 3 random seeds to verify claimed state-of-the-art performance.