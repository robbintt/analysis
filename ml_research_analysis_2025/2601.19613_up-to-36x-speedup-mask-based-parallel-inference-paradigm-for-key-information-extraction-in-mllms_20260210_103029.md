---
ver: rpa2
title: 'Up to 36x Speedup: Mask-based Parallel Inference Paradigm for Key Information
  Extraction in MLLMs'
arxiv_id: '2601.19613'
source_url: https://arxiv.org/abs/2601.19613
tags:
- mask
- inference
- parallel
- arxiv
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the efficiency bottleneck in Key Information
  Extraction (KIE) from visually-rich documents by proposing a parallel inference
  paradigm for Multi-Modal Large Language Models (MLLMs). The core method, PIP (Parallel
  Inference Paradigm), reformulates KIE by replacing target values with "[mask]" tokens,
  enabling simultaneous generation of all outputs in a single forward pass.
---

# Up to 36x Speedup: Mask-based Parallel Inference Paradigm for Key Information Extraction in MLLMs

## Quick Facts
- arXiv ID: 2601.19613
- Source URL: https://arxiv.org/abs/2601.19613
- Reference count: 0
- Key result: Achieves 5-36x inference speedup for KIE tasks with negligible performance degradation

## Executive Summary
This paper addresses the efficiency bottleneck in Key Information Extraction (KIE) from visually-rich documents by proposing a parallel inference paradigm for Multi-Modal Large Language Models (MLLMs). The core method, PIP (Parallel Inference Paradigm), reformulates KIE by replacing target values with "[mask]" tokens, enabling simultaneous generation of all outputs in a single forward pass. To enable this approach, the authors develop a mask pre-training strategy and construct large-scale supervised datasets. Experiments demonstrate that PIP-Models achieve a 5-36x inference speedup with negligible performance degradation compared to autoregressive baselines. The approach sets new state-of-the-art results on SROIE (97.0 ANLS) and CORD (97.3 ANLS) datasets while maintaining competitive performance on FUNSD (79.3 ANLS). The method is validated across different base models and scales, showing consistent efficiency gains with limited memory overhead.

## Method Summary
The paper introduces a parallel inference paradigm for KIE tasks that replaces the traditional autoregressive generation approach with masked token prediction. The method involves three key components: (1) Mask Pre-training, where models learn to predict masked target values from document images and keys, (2) Parallel Inference, where all target values are replaced with "[mask]" tokens and generated simultaneously in a single forward pass, and (3) Supervised Fine-tuning on constructed datasets. The approach leverages the ability of MLLMs to process multiple masked tokens in parallel, dramatically reducing inference time while maintaining accuracy. The authors also develop specialized datasets and training strategies to optimize the mask-based generation process.

## Key Results
- Achieves 5-36x inference speedup compared to autoregressive baselines
- Sets new state-of-the-art results on SROIE (97.0 ANLS) and CORD (97.3 ANLS) datasets
- Maintains competitive performance on FUNSD (79.3 ANLS) while being significantly faster
- Validated across different base models and scales (up to 30B parameters)

## Why This Works (Mechanism)
The method works by reformulating the KIE task from a sequential prediction problem into a parallel masked token generation problem. Traditional autoregressive approaches generate each target value sequentially, requiring multiple forward passes. By replacing all target values with "[mask]" tokens and training the model to predict these masks simultaneously, the entire output can be generated in a single forward pass. This leverages the parallel processing capabilities of modern transformer architectures while maintaining the same underlying understanding of document structure and content. The mask pre-training strategy ensures the model learns to effectively predict masked values, and the supervised fine-tuning on specialized datasets optimizes performance for the KIE task.

## Foundational Learning
- **Multi-Modal Large Language Models (MLLMs)**: AI models that process both visual and textual information through unified architectures. Why needed: Enables processing of document images alongside text for KIE tasks. Quick check: Verify model can handle both image and text inputs simultaneously.
- **Key Information Extraction (KIE)**: Task of identifying and extracting specific information from documents. Why needed: Core application domain for the proposed method. Quick check: Confirm task definition matches extraction of predefined fields from structured documents.
- **Parallel vs Autoregressive Generation**: Parallel generates all outputs simultaneously; autoregressive generates sequentially. Why needed: Understanding the fundamental difference in inference approaches. Quick check: Compare token generation patterns between methods.
- **Mask-based Training**: Technique where target values are replaced with special mask tokens during training. Why needed: Core mechanism enabling parallel inference. Quick check: Verify model learns to predict correct values from masked positions.
- **ANLS (Average Normalized Levenshtein Similarity)**: Evaluation metric for text similarity. Why needed: Primary metric for measuring extraction accuracy. Quick check: Calculate ANLS scores for sample predictions.

## Architecture Onboarding
- **Component Map**: Document Image -> Visual Encoder -> Cross-modal Fusion -> Masked Token Predictor -> Target Values
- **Critical Path**: Image preprocessing → Visual feature extraction → Cross-modal attention → Parallel mask prediction → Output decoding
- **Design Tradeoffs**: Speed vs. accuracy balance achieved through parallel generation, memory overhead vs. efficiency gains, and pre-training vs. fine-tuning resource allocation
- **Failure Signatures**: Degraded performance on documents with complex layouts, potential accuracy drops when scaling beyond tested model sizes, and memory constraints during parallel generation
- **First Experiments**: 1) Benchmark baseline autoregressive performance on standard KIE datasets, 2) Implement and test parallel inference on small-scale model, 3) Conduct ablation study on mask pre-training effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- Mask pre-training scalability to extremely large-scale datasets or models beyond 30B parameters remains untested
- Applicability to non-document structured data formats requires further investigation
- Memory overhead characterization lacks thorough analysis for edge deployment scenarios
- Performance on documents with highly irregular layouts or poor image quality not extensively validated

## Confidence
- **High Confidence**: Core parallel inference mechanism, 5-36x speedup claims, baseline performance comparisons
- **Medium Confidence**: Mask pre-training effectiveness, generalization across different base models
- **Low Confidence**: Memory overhead characterization, scalability beyond tested model sizes

## Next Checks
1. Conduct detailed memory consumption analysis comparing PIP-Models against autoregressive baselines across different batch sizes and document complexities
2. Evaluate PIP-Models on non-document structured data (e.g., scientific tables, medical forms) to assess cross-domain applicability
3. Create and evaluate on a dataset of documents with varying quality levels to measure robustness to real-world document variations