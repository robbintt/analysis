---
ver: rpa2
title: 'HASARD: A Benchmark for Vision-Based Safe Reinforcement Learning in Embodied
  Agents'
arxiv_id: '2503.08241'
source_url: https://arxiv.org/abs/2503.08241
tags:
- agent
- safety
- cost
- learning
- thresholdsafety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The HASARD benchmark introduces six novel 3D environments built
  on ViZDoom to evaluate vision-based safe reinforcement learning. These tasks require
  strategic navigation, spatial reasoning, and short-term prediction under safety
  constraints.
---

# HASARD: A Benchmark for Vision-Based Safe Reinforcement Learning in Embodied Agents

## Quick Facts
- arXiv ID: 2503.08241
- Source URL: https://arxiv.org/abs/2503.08241
- Reference count: 40
- Key outcome: PPOPID consistently meets safety thresholds with competitive performance across six novel 3D ViZDoom environments.

## Executive Summary
HASARD introduces six 3D vision-based safe reinforcement learning environments built on ViZDoom to evaluate strategic navigation under safety constraints. These tasks require spatial reasoning, short-term prediction, and safety-aware decision-making in visually rich settings. The benchmark includes environments like Remedy Rush (collect items while avoiding harm), Volcanic Venture (navigate lava hazards), and Detonator's Dilemma (push barrels away from enemies). Experiments with six baseline methods show that PPO ignores safety costs for higher rewards, while PPOPID achieves better safety-performance trade-offs using a PID controller to dynamically adjust the Lagrangian multiplier.

## Method Summary
The benchmark uses ViZDoom to create 160×120 egocentric RGB observations with 90° FOV, downsampled to 128×72 for processing. Agents employ a CNN encoder with ELU activations followed by two 512-dense layers and a 512-hidden GRU for partial observability handling. The action space is simplified to discrete combinations of movement and actions (6-54 actions depending on environment). Training uses Sample-Factory with 32 parallel workers and 10 environments per worker, processing 500M environment steps over 5 seeds. Six baseline algorithms are evaluated: PPO (unsafe), PPOCost, PPOLag, PPOSauté, PPOPID, and P3O, with PPOPID showing consistent constraint satisfaction across all environments.

## Key Results
- PPOPID consistently meets safety thresholds while achieving competitive rewards across all six environments and three difficulty levels
- Visual augmentations (segmentation maps, depth buffers) improve learning efficiency by 21-34% on specific environments
- Sequential training across difficulty levels provides implicit curriculum learning, yielding nearly threefold performance increase in Remedy Rush
- PPO ignores safety costs entirely, achieving higher rewards but violating constraints, while PPOLag oscillates around thresholds due to multiplier instability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PPOPID achieves better safety-performance trade-offs than Lagrangian methods on HASARD tasks
- Mechanism: PID controller dynamically adjusts the Lagrangian multiplier using proportional, integral, and derivative terms, smoothing the optimization trajectory compared to naive gradient-based multiplier updates that cause oscillations
- Core assumption: The cost feedback signal is sufficiently informative and temporally consistent for the PID controller to track
- Evidence anchors:
  - [abstract] "PPOPID consistently meets safety thresholds with competitive performance"
  - [section 5.1] "PPOPID consistently meets the constraints, most often outperforming other baselines"
  - [corpus] Weak corpus support; no direct comparison in neighbor papers
- Break condition: When cost signals are extremely sparse or delayed beyond the PID response window

### Mechanism 2
- Claim: Visual augmentations reduce learning complexity by simplifying the observation space
- Mechanism: Segmentation maps each pixel to a fixed semantic label, reducing texture noise; depth buffers provide explicit distance information that would otherwise require implicit learning from RGB patterns
- Core assumption: The agent's CNN encoder struggles to disentangle task-relevant features from visual distractors without augmentation
- Evidence anchors:
  - [section 5.4] "Training PPOPID on segmented observations in Armament Burden Level 2 yields a 21% reward increase"
  - [section 5.4] "Augmenting the RGB input with depth information in Precipice Plunge Level 2 results in a 34% reward increase"
  - [corpus] Not addressed in corpus neighbors
- Break condition: When visual complexity is already low or when augmentation removes critical perceptual cues

### Mechanism 3
- Claim: Sequential training across difficulty levels provides implicit curriculum learning
- Mechanism: Easier levels develop foundational competencies (navigation, basic safety awareness) that transfer to harder levels with additional mechanics, reducing exploration burden
- Core assumption: Skills learned at lower difficulty levels are reusable and not counterproductive at higher levels
- Evidence anchors:
  - [abstract] "Implicit curriculum across difficulty levels enhances policy development"
  - [section 5.3] "Nearly threefold performance increase in Remedy Rush and a 33% improvement in Collateral Damage"
  - [corpus] Weak corpus support; curriculum learning not discussed in neighbors
- Break condition: When higher difficulty levels introduce contradictory mechanics that invalidate previously learned strategies

## Foundational Learning

- Concept: Constrained Markov Decision Processes (CMDPs)
  - Why needed here: HASARD formulates safe RL as reward maximization subject to cost thresholds; understanding the Lagrangian formulation and constraint satisfaction is essential for interpreting baseline results
  - Quick check question: Can you explain why PPOPID's PID controller adjusts the Lagrangian multiplier differently than PPOLag's gradient-based approach?

- Concept: Partial Observability in Vision-Based RL
  - Why needed here: Agents receive only 160×120 egocentric views with 90° FOV; must maintain memory via GRU to infer off-screen hazards and predict short-term futures
  - Quick check question: Why does the paper use a GRU with 512 hidden units rather than a feedforward network?

- Concept: Visual Representation Learning for Control
  - Why needed here: The paper shows that raw RGB is suboptimal; segmentation and depth channels improve sample efficiency by reducing the perceptual learning burden
  - Quick check question: What does the 21% reward increase from segmentation suggest about the CNN encoder's ability to handle texture noise?

## Architecture Onboarding

- Component map: ViZDoom environment → 160×120 RGB + HUD → CNN encoder (ELU activations) → Two 512-dense layers → GRU (512 hidden) → Actor head (action distribution) + Critic head (value estimate)
- Critical path:
  1. Environment wrapper converts ViZDoom observations to tensors (downsample to 128×72)
  2. Policy inference every 4 frames (action repeated)
  3. Rollout collection (32 timesteps per batch)
  4. PPO update with safety-specific loss terms (varies by baseline)
  5. For PPOPID: PID controller updates Lagrangian multiplier based on cost constraint violation

- Design tradeoffs:
  - Simplified action space (6–54 discrete actions) vs full action space (864 discrete + 2 continuous): simplified accelerates learning but reduces expressivity; full space shows 40–70% reward drops (Table 12)
  - Soft constraints (cumulative cost budget) vs hard constraints (episode termination on violation): hard constraints cause overly conservative behavior with near-zero rewards
  - Segmentation/depth augmentations: improve learning but require engine-level access to labels/depth buffers

- Failure signatures:
  - PPO ignores costs entirely (baseline upper bound on reward and cost)
  - PPOCost becomes overly cautious or reckless depending on cost scaling factor (Table 9)
  - PPOLag oscillates around cost threshold due to Lagrangian multiplier instability
  - Hard constraint setting: agents learn to output NO-OP exclusively

- First 3 experiments:
  1. Reproduce PPOPID vs PPOLag comparison on Level 1 of all six environments with default hyperparameters (Table 3) to validate setup
  2. Ablate visual augmentation by training PPOPID on Armament Burden Level 2 with and without segmentation; expect ~21% reward gap
  3. Test curriculum transfer by loading Level 1 checkpoint and fine-tuning on Level 3; compare against training from scratch to quantify curriculum benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a curriculum that progressively reduces the safety budget to zero enable agents to learn effective policies under hard safety constraints in HASARD environments?
- Basis in paper: [explicit] Appendix C.3 states: "A potential approach to developing a successful policy involves using a curriculum that progressively reduces the cost budget until it reaches zero. We leave this open for future research."
- Why unresolved: Current methods (PPOCost, PPOLag) exhibit overly cautious behaviors under hard constraints, achieving near-zero rewards due to the safe exploration problem.
- What evidence would resolve it: Demonstration of a curriculum-based approach achieving non-trivial rewards while maintaining zero safety violations in hard-constraint HASARD environments.

### Open Question 2
- Question: Can methods be developed that learn strategic behaviors such as discarding weapons in Armament Burden, seeking night vision goggles in Remedy Rush, or pushing barrels in Detonator's Dilemma?
- Basis in paper: [inferred] Appendix F.1 documents that human players outperform agents through strategies none of the evaluated baselines learned: discarding heavy weapons, seeking goggles before item collection, and pushing barrels to safer locations.
- Why unresolved: These strategies require credit assignment over longer horizons since actions (discarding, seeking goggles, pushing barrels) provide no immediate reward signal.
- What evidence would resolve it: An agent exhibiting these strategic behaviors during evaluation, achieving human-comparable or superhuman performance on the relevant environments.

### Open Question 3
- Question: What effective methods exist for integrating curriculum learning with safe RL to leverage HASARD's implicit difficulty-based curriculum?
- Basis in paper: [explicit] Section 5.3 states: "Research on integrating curriculum learning with safe RL remains sparse. HASARD's inherent curriculum across difficulty levels opens new avenues for exploration in this field."
- Why unresolved: While the paper demonstrates that sequential training across levels improves performance, the principled integration of curriculum learning with safety constraints remains unexplored.
- What evidence would resolve it: A systematic comparison of curriculum strategies for safe RL, showing improved sample efficiency or final performance over the simple sequential training demonstrated in Table 4.

### Open Question 4
- Question: How can HASARD be extended to support multi-constraint safe RL where agents must satisfy multiple simultaneous safety thresholds?
- Basis in paper: [explicit] Appendix G states: "In the current HASARD framework, each environment only has a single safety constraint. Given the focus on Safe RL, incorporating multiple safety constraints presents a compelling avenue."
- Why unresolved: Real-world applications often involve multiple safety requirements, but current HASARD environments combine different cost sources into a single scalar.
- What evidence would resolve it: Modified HASARD environments with decoupled constraints (e.g., separate thresholds for harming neutrals vs. self-damage in Detonator's Dilemma) and baseline evaluations showing how methods handle the multi-constraint setting.

## Limitations

- PID controller effectiveness depends heavily on cost signal quality and temporal consistency, potentially failing with sparse or delayed feedback
- Visual augmentation benefits assume CNN encoders struggle with texture noise, which may not hold for more sophisticated architectures
- Curriculum learning claims rest on skill transfer assumptions that could break when higher levels introduce contradictory mechanics

## Confidence

- **High confidence:** PPOPID outperforms Lagrangian methods on HASARD benchmark tasks
- **Medium confidence:** Visual augmentations improve learning efficiency
- **Medium confidence:** Curriculum learning enhances policy development

## Next Checks

1. **PID Controller Robustness Test:** Evaluate PPOPID performance when cost signals are artificially delayed by 2-4 timesteps to test mechanism 1's break condition
2. **Visual Augmentation Ablation:** Conduct comprehensive study across all six environments testing segmentation, depth, and combined augmentations to quantify learning benefits more precisely
3. **Curriculum Learning Transfer Analysis:** Measure skill retention and transfer by testing agents trained on easier levels against agents trained from scratch on harder levels, isolating specific skill contributions