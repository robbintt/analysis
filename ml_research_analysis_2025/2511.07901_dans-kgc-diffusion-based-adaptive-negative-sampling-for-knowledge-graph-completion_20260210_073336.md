---
ver: rpa2
title: 'DANS-KGC: Diffusion Based Adaptive Negative Sampling for Knowledge Graph Completion'
arxiv_id: '2511.07901'
source_url: https://arxiv.org/abs/2511.07901
tags:
- negative
- knowledge
- graph
- sampling
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DANS-KGC is a novel negative sampling framework for knowledge graph
  completion that addresses the limitations of existing methods in terms of false
  negatives, generalization, and lack of control over sample hardness. The method
  introduces a Difficulty Assessment Module (DAM) that quantifies entity-specific
  learning difficulty by integrating semantic and structural features.
---

# DANS-KGC: Diffusion Based Adaptive Negative Sampling for Knowledge Graph Completion

## Quick Facts
- arXiv ID: 2511.07901
- Source URL: https://arxiv.org/abs/2511.07901
- Authors: Haoning Li; Qinghua Huang
- Reference count: 10
- Primary result: State-of-the-art MRR of 0.972 on UMLS and 0.572 on YAGO3-10

## Executive Summary
DANS-KGC introduces a novel negative sampling framework for knowledge graph completion that addresses key limitations of existing methods. The approach combines a Difficulty Assessment Module that quantifies entity-specific learning difficulty using semantic and structural features, with an Adaptive Negative Sampling Module that employs conditional diffusion models for generating negative samples of varying hardness. A Dynamic Training Mechanism further enhances learning through curriculum-style progression of sample difficulty throughout training.

The framework demonstrates significant performance improvements across six benchmark datasets, achieving state-of-the-art results on both UMLS (MRR: 0.972) and YAGO3-10 (MRR: 0.572). By generating entity-specific negatives with controlled hardness levels and incorporating semantic constraints during generation, DANS-KGC effectively reduces false negatives while improving generalization compared to traditional and diffusion-based baselines.

## Method Summary
DANS-KGC implements a three-module architecture for adaptive negative sampling in knowledge graph completion. The Difficulty Assessment Module (DAM) computes entity-specific difficulty scores by combining semantic embeddings with structural graph features through an MLP. The Adaptive Negative Sampling Module (ANS) uses these difficulty scores to control noise injection in a conditional diffusion model, generating negatives at multiple timesteps with varying hardness levels while incorporating semantic type and neighborhood constraints. The Dynamic Training Mechanism (DTM) progressively adjusts the hardness distribution during training using curriculum scheduling, with stage-aware weighted margins. The framework is trained on six benchmark datasets using pre-trained TransE embeddings and various structural features, with performance evaluated using MRR, Hits@1, and Hits@10 metrics.

## Key Results
- Achieved state-of-the-art MRR of 0.972 on UMLS dataset
- Achieved state-of-the-art MRR of 0.572 on YAGO3-10 dataset
- Significant performance improvements across all evaluation metrics compared to traditional and diffusion-based baselines
- Ablation studies confirm contributions of all three modules (DAM, CCD, DTM)

## Why This Works (Mechanism)

### Mechanism 1: Difficulty-Adaptive Noise Injection
The DAM evaluates entity learning difficulty by integrating semantic and structural features, outputting a difficulty score ζ(e) ∈ (0,1). This score modulates the maximum noise variance β_max(x) during forward diffusion, with harder entities receiving stronger perturbations and easier entities receiving lighter corruption. The core assumption is that structural graph position and semantic similarity correlate with entity learning difficulty, allowing the framework to generate more informative negatives for difficult entities while avoiding trivial samples for easier ones.

### Mechanism 2: Condition-Constrained Reverse Denoising
During the reverse denoising process, the framework conditions on semantic type clusters (from K-means) and neighborhood information (entity + relation), guiding generation toward semantically plausible yet discriminatively useful negatives. This conditioning reduces false negatives by ensuring generated triples are coherent while maintaining discriminative power. The core assumption is that K-means clusters capture meaningful semantic types and that neighborhood structure generalizes to valid negative candidates.

### Mechanism 3: Curriculum-Based Hardness Progression
The DTM dynamically adjusts the hardness distribution of negative samples throughout training by using time-varying sampling weights across four difficulty bands. This curriculum-style progression stabilizes early training by starting with easier negatives and gradually introducing harder ones, forcing later-stage boundary refinement. The core assumption is that smooth curriculum scheduling prevents gradient destabilization from early hard negatives while ensuring comprehensive learning across difficulty levels.

## Foundational Learning

- **Concept: Denoising Diffusion Probabilistic Models (DDPMs)**
  - Why needed here: The entire ANS module builds on forward/reverse diffusion. Understanding how noise schedules control sample quality is essential.
  - Quick check question: Can you explain why sampling at earlier reverse timesteps yields "easier" negatives?

- **Concept: Knowledge Graph Embeddings (TransE, RotatE)**
  - Why needed here: DAM requires pre-trained semantic features; the loss function uses triple scoring S(·) from KGC models.
  - Quick check question: Given a triple ⟨h, r, t⟩, how would TransE score it vs. a corrupted triple ⟨h, r, t'⟩?

- **Concept: Curriculum Learning**
  - Why needed here: DTM implements curriculum scheduling; understanding the easy-to-hard progression logic is critical for tuning.
  - Quick check question: What happens if you reverse the curriculum (hard-to-easy) during training?

## Architecture Onboarding

- **Component map:**
  Input Triples → DAM (compute ζ(e)) → ANS (forward diffusion with β_t(x)) → Reverse denoising with (x_type, x_e+x_r) → Multi-timestep sampling → G⁻ (4 hardness bands) → DTM (curriculum weights w(k)_e) → Mini-batch training → L_KGC + L_Diff → Parameter update

- **Critical path:**
  1. Pre-train embeddings (TransE or similar) to get semantic features
  2. Compute structural features and train DAM MLP to output ζ(e)
  3. Run forward diffusion with entity-specific β_max(x)
  4. Execute reverse denoising with type/neighborhood conditioning
  5. Sample negatives from 4 timesteps, apply DTM weights during loss computation

- **Design tradeoffs:**
  - μ (noise influence): Higher values increase difficulty differentiation but risk over-corrupting easy entities
  - η (loss balance): Controls DANS vs. random NS contribution; too low wastes generated negatives, too high ignores random samples
  - Number of diffusion steps T: More steps yield finer hardness granularity but increase compute

- **Failure signatures:**
  - MRR plateaus early → curriculum schedule too slow; increase λ
  - High false negative rate → type constraints too weak; increase K-means clusters or check cluster quality
  - Training instability → β_global too high or μ too aggressive for difficult entities

- **First 3 experiments:**
  1. Sanity check: Run DANS-KGC on UMLS with default hyperparameters; verify MRR ≈ 0.97
  2. Ablation: Disable CCD and confirm performance drop similar to Table 5 (MRR: 0.994 → 0.989)
  3. Hyperparameter sweep: Vary μ ∈ {0.5, 1.0, 1.5, 2.0} on Family dataset; identify optimal region per Figure 2 sensitivity analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the training efficiency and convergence speed of DANS-KGC compare to non-diffusion baselines, given the computational overhead of the iterative denoising process?
- Basis in paper: The methodology utilizes a multi-step diffusion process (T steps) and a denoising MLP to generate negatives, which is computationally more intensive than single-step random or adversarial sampling methods.
- Why unresolved: The experimental results report link prediction accuracy but do not provide wall-clock training times or convergence curves relative to baseline methods.
- Evidence: A comparative analysis of training epochs per second and total time-to-target-accuracy on large-scale datasets like FB15k-237 or YAGO3-10.

### Open Question 2
- Question: To what extent does the quality of the pre-trained embeddings used in the Difficulty Assessment Module (DAM) impact the final performance of the framework?
- Basis in paper: The DAM relies on semantic features derived from pre-trained embeddings (e.g., TransE) to quantify entity learning difficulty, assuming these features accurately represent difficulty.
- Why unresolved: The paper does not ablate the source or quality of these input features; it is unclear if poor pre-training would degrade the adaptive noise scheduling mechanism.
- Evidence: An ablation study comparing DAM performance when initialized with random embeddings versus high-quality pre-trained embeddings.

### Open Question 3
- Question: Does the Condition-Constrained Reverse Denoising (CCD) effectively eliminate false negatives, or does the generative process risk reconstructing valid but unobserved facts?
- Basis in paper: The paper claims to address the "vulnerability to false negatives" by using semantic type and neighborhood constraints to guide generation.
- Why unresolved: While performance gains imply better sample quality, the paper does not quantitatively measure the false negative rate of the generated samples.
- Evidence: A statistical analysis of generated negative samples to calculate the percentage that correspond to missing facts in the validation or test sets.

## Limitations
- Core diffusion hyperparameters (T, β_global, β_low) are unspecified, making exact replication impossible without extensive tuning
- Structural feature computation details are missing (Appendix A reference unavailable), requiring assumptions about implementation
- Triple scoring function S(·) in the margin loss is not explicitly defined, though likely follows TransE conventions

## Confidence
- **High Confidence**: Performance improvements over baselines (UMLS MRR: 0.972, YAGO3-10 MRR: 0.572) and ablation results showing DAM/CCD/DTM contributions are well-documented with clear experimental evidence
- **Medium Confidence**: The theoretical framework connecting difficulty scores to noise schedules and curriculum progression is sound, but practical effectiveness depends heavily on hyperparameter tuning that isn't fully specified
- **Low Confidence**: Claims about false negative reduction through semantic constraints lack direct quantitative evidence beyond indirect performance gains

## Next Checks
1. **Hyperparameter Sensitivity**: Replicate the μ sensitivity analysis from Figure 2 on Family dataset to verify optimal range and confirm performance scaling behavior
2. **False Negative Rate Measurement**: Implement a controlled experiment comparing false negative rates between DANS-KGC and traditional NS on a dataset with known negative sampling quality (e.g., FB15k-237)
3. **Curriculum Schedule Robustness**: Test DTM with reversed curriculum (hard-to-easy) and aggressive/slow schedules to validate the claimed benefits of the proposed progression