---
ver: rpa2
title: 'ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific
  Reasoning'
arxiv_id: '2511.14366'
source_url: https://arxiv.org/abs/2511.14366
tags:
- reasoning
- scientific
- wang
- atlas
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ATLAS, a high-difficulty, multidisciplinary
  benchmark for evaluating frontier scientific reasoning in AI. The authors created
  ~800 expert-designed, original problems spanning seven scientific fields to address
  benchmark saturation and data contamination issues.
---

# ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning

## Quick Facts
- arXiv ID: 2511.14366
- Source URL: https://arxiv.org/abs/2511.14366
- Reference count: 40
- Leading models achieve only 21.7-43.8% accuracy on ATLAS

## Executive Summary
ATLAS is a benchmark of ~800 expert-designed, original problems spanning seven scientific fields to evaluate frontier scientific reasoning in AI. The benchmark addresses benchmark saturation and data contamination issues by emphasizing complex, open-ended answers requiring multi-step reasoning. A rigorous construction pipeline combines expert review with adversarial filtering using state-of-the-art LLMs to ensure high quality and difficulty. Preliminary results show leading models achieve only 21.7-43.8% accuracy, demonstrating its effectiveness in differentiating advanced reasoning capabilities.

## Method Summary
ATLAS evaluates Large Reasoning Models on ~800 high-difficulty, expert-authored scientific problems requiring multi-step reasoning and LaTeX formatting. The process uses adversarial filtering where problems must fail state-of-the-art models at ≤40% accuracy, expert peer review, and LRM-as-Judge evaluation for complex answers. Models generate 4 predictions per question (temperature=0.6, max tokens=32k/64k), which are evaluated by judge models using strict grading protocols with tolerances for numerical answers. The benchmark includes validation (300 questions) and test (500 questions) sets with bilingual support.

## Key Results
- Leading models achieve only 21.7-43.8% accuracy on ATLAS
- Judge model inconsistencies show 2-5% score differences between different judges
- 27% of errors stem from numerical discrepancies beyond tolerance
- Grok-4 shows 10.38% truncation rate at 32k tokens

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Difficulty Filtering
A dual-filter system where problems must first pass expert review, then survive adversarial testing where SOTA LLMs achieve ≤40% accuracy over 10 attempts. This ensures genuine difficulty rather than superficial complexity by requiring models to fail at high rates.

### Mechanism 2: Originality-Based Contamination Resistance
Expert-authored original problems with RAG-based deduplication prevent data contamination. All problems are newly created by PhD-level experts and screened against web content, academic papers, and benchmarks using semantic similarity scoring.

### Mechanism 3: LRM-as-Judge for Complex Answer Evaluation
Large Reasoning Models substitute for human graders when evaluating multi-step, open-ended scientific answers. Judge models apply grading protocols including validity checks and equivalence comparisons with tolerances (±0.1 relative error for numerical answers).

## Foundational Learning

- **Benchmark Saturation Phenomenon**
  - Why needed here: ATLAS exists because existing benchmarks show >90% accuracy for SOTA models, eliminating their discriminative power.
  - Quick check question: If a model scores 95% on a benchmark, can you distinguish whether it has genuine capability or memorized the test set?

- **Data Contamination vs. Genuine Reasoning**
  - Why needed here: The core problem ATLAS solves is that models may score highly on benchmarks by encountering similar problems during training.
  - Quick check question: A model solves an Olympiad problem correctly. What evidence would distinguish memorization from reasoning?

- **Generative vs. Recognition-Based Evaluation**
  - Why needed here: ATLAS uses short-answer and fill-in-the-blank formats (71.4% calculation/derivation) rather than multiple-choice.
  - Quick check question: Why does a 4-option multiple-choice question provide less signal about reasoning capability than a LaTeX equation derivation?

## Architecture Onboarding

- **Component map**: Question Submission Layer -> Pre-screening Pipeline -> Adversarial Filter -> Human Review -> Answer Refinement -> Evaluation Runtime
- **Critical path**: Question submission → Offline deduplication → Adversarial LLM filtering → Expert peer review → Meta-review → Final ingestion
- **Design tradeoffs**: Scale vs. Quality (~800 problems with rigorous validation vs. larger benchmarks), Generality vs. Focus (7 core fields vs. broader coverage), Judge Automation vs. Accuracy (scalability vs. model-dependent variance)
- **Failure signatures**: Judge model inconsistency (2-5% score differences), truncation errors (8-10% for Grok-4), numerical precision drift (27% of errors)
- **First 3 experiments**:
  1. Judge calibration study: Compare LRM-as-Judge scores against human expert grading on 50-question sample
  2. Contamination probe: Test whether models show suspiciously high accuracy on problems with high semantic similarity to existing benchmarks
  3. Cross-model judge consistency: Evaluate same predictions using multiple judge models to measure inter-judge agreement

## Open Questions the Paper Calls Out

### Open Question 1
How robust is the "LRM-as-Judge" evaluation paradigm for complex scientific answers compared to human expert grading? The authors demonstrate specific failure cases where judge models failed to recognize mathematically equivalent expressions, potentially skewing accuracy metrics.

### Open Question 2
Does high performance on ATLAS predict a model's capability for autonomous scientific discovery? While ATLAS tests multi-step problem solving, true scientific discovery requires open-ended hypothesis generation and experimental design not captured by current format.

### Open Question 3
Does the adversarial filtering pipeline bias the dataset toward "anti-model" artifacts rather than genuine reasoning difficulty? Selecting questions because current models fail them may inadvertently select for linguistic ambiguities rather than problems requiring deep scientific insight.

## Limitations
- LRM-as-Judge evaluation reliability degrades for problems requiring nuanced semantic understanding
- Adversarial filtering may select for domain-specific complexity rather than general reasoning difficulty
- Data contamination resistance depends on effectiveness of semantic similarity detection

## Confidence
- **High Confidence**: Benchmark construction methodology and technical implementation details
- **Medium Confidence**: LRM-as-Judge framework works for straightforward answers but shows systematic scoring biases
- **Low Confidence**: Assumption that adversarial filtering produces genuine reasoning difficulty lacks extensive validation

## Next Checks
1. Judge model calibration study comparing LRM-as-Judge scores against human expert grading
2. Cross-model judge consistency analysis measuring inter-judge agreement
3. Adversarial filter validation analyzing whether passed problems genuinely test reasoning capabilities