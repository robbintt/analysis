---
ver: rpa2
title: Order-Optimal Sample Complexity of Rectified Flows
arxiv_id: '2601.20250'
source_url: https://arxiv.org/abs/2601.20250
tags:
- rectified
- flow
- error
- complexity
- velocity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper establishes the first sample complexity bounds for rectified\
  \ flow generative models, proving that they achieve the order-optimal O(\u03B5\u207B\
  \xB2) sample complexity for learning a target distribution in Wasserstein distance.\
  \ This improves upon the best known O(\u03B5\u207B\u2074) bounds for general flow\
  \ matching models and matches the information-theoretic lower bound."
---

# Order-Optimal Sample Complexity of Rectified Flows

## Quick Facts
- arXiv ID: 2601.20250
- Source URL: https://arxiv.org/abs/2601.20250
- Reference count: 40
- The paper establishes the first sample complexity bounds for rectified flow generative models, proving they achieve order-optimal O(ε⁻²) sample complexity for learning a target distribution in Wasserstein distance.

## Executive Summary
This paper provides the first theoretical analysis of sample complexity for rectified flow generative models, proving they achieve the information-theoretically optimal O(ε⁻²) rate for learning distributions in Wasserstein distance. The key insight is that the linear interpolation geometry of rectified flows, combined with the squared-loss objective and Polyak-Łojasiewicz (PL) condition, enables a localized Rademacher complexity analysis that yields faster statistical rates than general flow matching methods. This improves upon the best known O(ε⁻⁴) bounds for general flow matching models and matches the lower bound, establishing rectified flows as an order-optimal framework for generative modeling.

## Method Summary
The method learns a velocity field vθ(x,t) for rectified flows that transport a base distribution π₀ to a target distribution π₁ via linear interpolation paths Xt = (1-t)X₀ + tX₁. Training minimizes squared loss L(θ) = E[||vθ(Xt,t) - (X₁-X₀)||²] using SGD with diminishing step sizes. The key technical innovation is a localized Rademacher complexity analysis that exploits the PL condition to achieve O(1/n) statistical rates. During inference, samples are generated by solving the ODE dZt/dt = vθ(Zt,t) from Z₀~π₀ to Z₁.

## Key Results
- Proves rectified flows achieve O(ε⁻²) sample complexity for Wasserstein-2 distance, improving upon prior O(ε⁻⁴) bounds for general flow matching
- Introduces localized Rademacher complexity analysis specifically adapted to rectified flow structure
- Establishes that SGD achieves O(1/n) optimization error under smoothness and PL conditions
- Shows the PL condition enables Bernstein-type variance bounds that are critical for fast rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear interpolation paths reduce the effective hypothesis class complexity, enabling faster statistical rates than general flow matching.
- Mechanism: Rectified flow constrains trajectories to straight lines between source and target samples via Xt = (1-t)X₀ + tX₁. This structural bias limits the velocity field to modeling conditional expectations of displacements, which admits polynomial covering numbers in network parameters rather than exponential growth in data dimension.
- Core assumption: The true velocity field can be expressed as v*(x,t) = E[X₁ - X₀ | Xt = x] (paper's equation 4), and the network class is sufficiently expressive to approximate this (Assumption 6).
- Evidence anchors:
  - [Abstract] "constrain transport trajectories to be linear from the base distribution to the data distribution"
  - [Section 5.1] "Two features of the rectified flow setting make learning significantly easier...linear interpolation path"
  - [Corpus] Related work on rectified flows (Liu et al. 2023) confirms straight trajectories enable one-step sampling, though sample complexity was previously unstudied.
- Break condition: If data distributions have highly curved optimal transport maps (e.g., multimodal targets requiring path-crossing), the linear assumption may increase approximation error substantially.

### Mechanism 2
- Claim: The squared-loss objective combined with the PL condition yields a Bernstein-type variance bound, enabling localized Rademacher analysis with O(1/n) rates instead of O(1/√n).
- Mechanism: Lemma 2 shows ||vθ - vθ*||² ≤ B|L(θ) - L(θ*)| where B = 2L²/μ. This variance-excess-risk relationship allows restricting analysis to a localized function class Fr = {vθ : E(vθ - vθ*)² ≤ r} rather than the full hypothesis space.
- Core assumption: Assumption 4 (PL condition with constant μ > 0) and Lipschitz continuity of vθ in parameters (Corollary 4).
- Evidence anchors:
  - [Section 5] "Using this structure, we construct an explicit localized fixed-point bound, Theorem 3, which yields a rate of O(1/n)"
  - [Lemma 5] Shows the localized Rademacher result from Bartlett et al. (2005) is applicable
  - [Corpus] Gaur et al. (2025a) on flow matching uses global Rademacher bounds achieving only O(ε⁻⁴); this paper's localization is the key differentiator.
- Break condition: If the loss landscape lacks the PL property (e.g., flat regions far from optima, or highly non-convex landscapes without gradient dominance), the Bernstein bound fails and rates revert to O(1/√n).

### Mechanism 3
- Claim: SGD achieves optimization error O(1/n) under smoothness and PL conditions, ensuring optimization does not dominate statistical error.
- Mechanism: Theorem 4 uses diminishing step sizes ηk = c/(k+γ) with the PL inequality to establish linear recursion on excess loss. Unrolling yields δn = O(1/n) after n iterations. Combined with Lemma 2, this bounds velocity field error.
- Core assumption: κ-smoothness of population loss and bounded gradient variance σ² (Assumption 5).
- Evidence anchors:
  - [Section 6] "choosing appropriate variable stepsize ηk yields an O(1/n) excess empirical loss after n iterations"
  - [Theorem 5 proof] Shows explicit recursion: δk+1 ≤ (1 - μηk)δk + κσ²c² / 2(k+γ)²
  - [Corpus] Standard in optimization theory (Karimi et al. 2016); the contribution here is verifying it applies to the rectified-flow objective.
- Break condition: If stochastic gradients have unbounded variance (e.g., heavy-tailed data without sub-Gaussian assumption), or if step sizes are poorly tuned, convergence degrades.

## Foundational Learning

- Concept: **Localized Rademacher Complexity**
  - Why needed here: This is the central technical tool distinguishing the O(ε⁻²) rate from prior O(ε⁻⁴) results. Standard Rademacher bounds measure worst-case complexity over the full function class; localized bounds restrict to functions with small excess risk.
  - Quick check question: Can you explain why bounding complexity over {f : E(f-f*)² ≤ r} yields faster rates than bounding over all f ∈ F?

- Concept: **Polyak-Łojasiewicz (PL) Condition**
  - Why needed here: Enables both the Bernstein variance bound (Lemma 2) and SGD convergence guarantees (Theorem 4) without requiring convexity. Critical bridge between optimization and statistical rates.
  - Quick check question: How does ||∇L(θ)||² ≥ 2μ(L(θ) - L*) differ from strong convexity, and why is it sufficient for linear convergence?

- Concept: **Wasserstein Distance and Flow Error Decomposition**
  - Why needed here: Theorem 1 (from Benton et al. 2024) translates L² velocity error to Wasserstein distribution error via W₂ ≤ ε · exp(∫ Lt dt). The three-way error decomposition (approximation + statistical + optimization) isolates each contribution.
  - Quick check question: Why does velocity error in L² translate to distribution error, and what role does the Lipschitz constant Lt play in the bound?

## Architecture Onboarding

- Component map: Data sampling -> Velocity network training -> ODE integration -> Distribution generation
- Critical path: Velocity field estimation accuracy → ODE integration quality → distributional fidelity. The paper bounds each stage: statistical error (Theorem 3), optimization error (Theorem 4), and distribution error via velocity error (Theorem 1).
- Design tradeoffs:
  - Network capacity vs. generalization: More parameters P increase approximation power but scale statistical error as O(BP/n) (Theorem 3). Paper suggests P should grow with d for expressiveness but notes dimension dependence is "at most logarithmic" for non-P terms.
  - Sample count vs. accuracy: n = O(BP/ε²) samples needed for ε Wasserstein error. Order-optimal; cannot be improved information-theoretically (lower bound in Section D).
  - Assumption strictness: PL constant μ appears as B = 2L²/μ throughout. Smaller μ (weaker PL) increases required samples quadratically.
- Failure signatures:
  - Blowup in velocity magnitude: May indicate violation of boundedness assumptions or training instability; check gradient norms and consider gradient clipping.
  - Wasserstein error not decreasing with samples: Suggests approximation error dominates; network may be underparameterized for target distribution complexity.
  - ODE divergence at inference: Lipschitz constant Lt too large; may need smaller step sizes or architectural changes to smooth velocity field.
- First 3 experiments:
  1. Validate sample complexity scaling: Train rectified flow on synthetic Gaussian mixtures with varying n ∈ {100, 1000, 10000, 100000}; plot Wasserstein error vs. n on log-log scale. Expect slope approximately -1/2 (since error ~ 1/√n).
  2. Compare to general flow matching: Implement both rectified flow and standard flow matching (with optimal transport or variance-preserving paths) on same data; verify rectified flow achieves target accuracy with ~100× fewer samples as predicted by ε⁻² vs. ε⁻⁴ scaling.
  3. Ablate PL assumption: Monitor ||∇L(θ)||² / (L(θ) - L*) during training. If ratio is small or inconsistent, the PL assumption may not hold empirically; investigate network initialization or architecture changes that improve gradient dominance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what precise conditions on network width, initialization, and data distribution does the Polyak-Łojasiewicz (PL) condition provably hold for the rectified flow objective?
- Basis in paper: [inferred] The paper assumes the PL condition (Assumption 4) and notes it "appears frequently in neural network theory to characterize the optimization landscapes of overparameterized models, where the loss may be locally or approximately gradient-dominant under suitable width or initialization assumptions."
- Why unresolved: The PL condition is treated as an assumption rather than derived from network architecture or data properties specific to rectified flows.
- What evidence would resolve it: A theorem characterizing explicit width/data conditions guaranteeing PL; empirical verification across architectures and initializations.

### Open Question 2
- Question: Can the O(ε⁻²) sample complexity be extended to heavy-tailed distributions (sub-exponential or polynomial tails)?
- Basis in paper: [inferred] The analysis relies critically on sub-Gaussian tails (Assumption 1), and the paper explicitly notes this "rules out heavy tails and provides the concentration properties needed for our analysis."
- Why unresolved: The truncation extension argument depends on sub-Gaussian decay rates; alternative concentration machinery would be required.
- What evidence would resolve it: Modified analysis using alternative concentration inequalities; or lower bounds showing worse rates are unavoidable for heavy-tailed data.

### Open Question 3
- Question: Does the sample complexity for k-rectified flow (k>1 reflow iterations) differ from the 1-RF O(ε⁻²) bound when accounting for error propagation across iterations?
- Basis in paper: [inferred] The paper states analysis is "restricted to the 1-Rectified Flow (1-RF) setting" and claims "reflow...does not introduce any additional statistical estimation problem," but provides no formal analysis of multi-round error accumulation.
- Why unresolved: Error in the initial velocity field propagates through subsequent reflow steps; whether this accumulates or remains controlled is unstudied.
- What evidence would resolve it: A formal bound for k-RF sample complexity; empirical study of sample requirements across reflow iterations.

### Open Question 4
- Question: Can the localized Rademacher complexity framework yield fast rates for flow matching with non-linear interpolation paths?
- Basis in paper: [inferred] The paper emphasizes that "the linear interpolation geometry and squared-loss training objective of rectified flows enable sharper statistical guarantees than general flow matching," suggesting the path structure is essential.
- Why unresolved: The Bernstein-type variance bound (Lemma 2) relies on properties tied to linear interpolation; whether analogous structure exists for other paths is unclear.
- What evidence would resolve it: Extension of localized analysis to optimal transport or diffusion bridge paths; characterization of which path geometries admit fast rates.

## Limitations

- Empirical validation gap: The paper presents comprehensive theoretical analysis but lacks empirical experiments demonstrating the predicted O(ε⁻²) sample complexity scaling.
- Assumption stringency: Analysis relies on multiple technical assumptions (PL condition, Lipschitz and bounded activations, sub-Gaussian data) that may be difficult to verify in practice.
- Distribution dependence: The bound shows sample complexity depends on B ~ L²/μ, but the relationship between target distribution geometry and these constants is not characterized.

## Confidence

**High confidence**: The localized Rademacher complexity analysis (Section 5) and SGD convergence proof (Section 6) are mathematically rigorous given the stated assumptions. The order-optimal sample complexity claim (O(ε⁻²)) follows logically from the error decomposition and is supported by the information-theoretic lower bound.

**Medium confidence**: The mechanism connecting rectified flow structure to reduced complexity (Mechanism 1) is well-justified theoretically, but the practical magnitude of this advantage depends on the true optimal transport map's geometry. The PL condition's empirical prevalence in neural network training for generative modeling is theoretically established but may vary with architecture and initialization.

**Low confidence**: The practical translation of the bound to specific network architectures and hyperparameters lacks empirical grounding. The paper provides a framework but not implementation guidance for achieving the theoretical rates.

## Next Checks

1. **Empirical sample complexity verification**: Implement the complete rectified flow pipeline (data generation, network training, ODE sampling) on synthetic Gaussian mixtures and real image datasets. Plot Wasserstein error vs. training samples on log-log scale to verify the predicted O(ε⁻²) scaling, comparing against standard flow matching baselines.

2. **PL condition monitoring**: During training, track the ratio ||∇L(θ)||² / (L(θ) - L*). Verify it remains bounded below by 2μ consistently, and investigate architectural modifications (initialization, activation functions) if the condition fails empirically.

3. **Architecture scaling study**: Systematically vary network width W and depth D while measuring achieved Wasserstein error for fixed sample budget. Confirm that increasing capacity improves approximation error until statistical error dominates, validating the error decomposition and identifying the optimal capacity regime for different data complexities.