---
ver: rpa2
title: 'Catch Your Breath: Adaptive Computation for Self-Paced Sequence Production'
arxiv_id: '2510.13879'
source_url: https://arxiv.org/abs/2510.13879
tags:
- pause
- pauses
- tokens
- token
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors introduce a family of training objectives, called\
  \ CYB losses, that enable language models to dynamically allocate additional compute\
  \ steps to individual tokens during sequence processing. The core method allows\
  \ the model to emit a <don\u2019t know token to request extra compute via inserted\
  \ <pause tokens, with the loss function designed to train the model to calibrate\
  \ when to pause based on potential accuracy gains."
---

# Catch Your Breath: Adaptive Computation for Self-Paced Sequence Production

## Quick Facts
- arXiv ID: 2510.13879
- Source URL: https://arxiv.org/abs/2510.13879
- Reference count: 4
- Primary result: CYB-AP loss enables Gemma1-2B to reduce perplexity on C4 using one-third the training data by learning when to pause for additional compute steps

## Executive Summary
The paper introduces CYB (Catch Your Breath) losses, a family of training objectives that enable language models to dynamically allocate compute steps during sequence processing. The method allows models to emit a `<don't know>` token to request extra compute via inserted `<pause>` tokens, with the loss function training the model to calibrate when pauses improve accuracy. Through fine-tuning experiments on Gemma1-2B, CYB-AP (anytime prediction with no early stopping or discounting) achieved the best performance, reducing perplexity on C4 while requiring significantly less training data than baselines. The model demonstrated adaptive pause behavior, pausing more for complex tokens and less for simple ones.

## Method Summary
CYB introduces a sequential decision framework where token prediction becomes a process with time costs. The model outputs either content tokens or `<DON'T KNOW>` with probability d_i at each step i, with stopping time S determined by both model decisions and an external stop-time distribution ω. The expected loss E_{i∼Pr(S|d,ω)}[γ_i·t_i] creates a speed-accuracy tradeoff, training the model to pause only when accuracy improvements justify the computational cost. Three variants were studied: CYB-AP (anytime prediction), CYB-VA (variational approach), and CYB-DP (distribution penalization). The best-performing variant, CYB-AP, uses ω=[0:0:0:1] (no early stopping) and γ=1.0 (no discounting), allowing the model to self-calibrate pause usage.

## Key Results
- CYB-AP achieved the lowest perplexity on C4 dataset among all CYB variants
- The model required only one-third the training data compared to baseline to reach equivalent performance
- Adaptive pause behavior emerged naturally: more pauses for complex tokens (plural nouns, ambiguous words) and fewer for simple/unambiguous tokens
- Spearman correlation between d_i and accuracy improvement was 0.383, 0.157, 0.271 for steps 0-2, indicating meaningful calibration

## Why This Works (Mechanism)

### Mechanism 1
Framing token prediction as a sequential decision problem enables adaptive compute allocation without degenerate pause behavior. At each step i, the model outputs content tokens or `<DON'T KNOW>` with probability d_i. The stopping time S becomes a random variable influenced by both model decisions and external stop-time distribution ω. The expected loss E_{i∼Pr(S|d,ω)}[γ_i·t_i] creates a speed-accuracy tradeoff: the model learns to pause only when γ_{i+1}·t_{i+1} ≥ γ_i·t_i. This works because the model can learn to predict its own future accuracy improvements and calibrate d_i accordingly.

### Mechanism 2
CYB-AP (anytime prediction) with no forced early stopping and no discounting yields the best perplexity by allowing the model to self-calibrate pause usage. The best-performing variant sets ω = [0:0:0:1] (no early stopping) and γ = 1.0 (no discounting). The model still learns to use pauses judiciously because the CYB loss implicitly trains calibration: the Spearman correlation between d_i and accuracy improvement (t_{i+1} - t_i) is 0.383, 0.157, 0.271 for steps 0-2. This works because implicit regularization from the loss structure prevents the model from exploiting unlimited pauses to maximize compute.

### Mechanism 3
Assigning pause tokens the same position index as the preceding real token (with unique token codes per pause) enables effective fine-tuning of pretrained models without retraining position embeddings. Standard RoPE position encoding would shift indices unpredictably with variable pauses. Instead, `<PAUSE1>`, `<PAUSE2>`, `<PAUSE3>` all share the position index of the preceding non-pause token but have distinct token embeddings. This allows the model to distinguish pause steps while maintaining consistent position semantics from pretraining, working because the model can learn pause-specific computations by attending to unique token embeddings even when position indices are shared.

## Foundational Learning

- **Concept: Sequential Decision Processes / Markov Decision Processes**
  - **Why needed here:** The CYB loss treats each token prediction as a sequential decision problem with states, actions, and rewards. Understanding expected discounted rewards is essential to grasp why the loss structure incentivizes calibrated pausing.
  - **Quick check question:** Given a discount factor γ=0.9 and accuracy improving from 0.6 to 0.7 with one extra pause step, is it beneficial to pause? (Answer: 0.9 × 0.7 = 0.63 > 0.6, so yes.)

- **Concept: Variational Inference and the ELBO**
  - **Why needed here:** CYB-VA uses the Evidence Lower Bound to train both the stopping distribution s and predictions t. The derivation shows that optimal s_i ∝ ρ_i · γ_i · t_i.
  - **Quick check question:** Why does the ELBO include a KL divergence term? (Answer: To regularize the learned stopping distribution toward a prior ρ.)

- **Concept: Position Encodings in Transformers (RoPE)**
  - **Why needed here:** The paper modifies standard RoPE behavior for pause tokens. Without understanding how position encodings affect attention patterns, the rationale for shared position indices across pauses is unclear.
  - **Quick check question:** What would happen if pause tokens received sequential position indices? (Answer: Variable pause insertion would create irregular, unpredictable position shifts, confusing the model.)

## Architecture Onboarding

- **Component map:** Input sequence with pause tokens -> Token Embedding Layer (extended vocab) -> RoPE Position Encoding (pauses share position with preceding real token) -> Standard Transformer Layers -> Output Head (vocab + `<DON'T KNOW>`) -> CYB Loss computation

- **Critical path:**
  1. Extend vocabulary: repurpose unused input tokens for `<PAUSE1/2/3>`, unused output token for `<DON'T KNOW>`
  2. Modify position encoding logic: assign pause tokens the same position index as preceding real token
  3. Renormalize output logits with prior on `<DON'T KNOW>` (ψ'_DK=0.9) to encourage initial exploration
  4. Insert W_max pause tokens after each real token during training
  5. Compute CYB loss using Equation 3, summing over all steps weighted by Pr(S=i|d,ω)

- **Design tradeoffs:**
  - **Recipe 1 (constant pauses) vs Recipe 2 (variable pauses):** Recipe 1 allows parallel processing during training; Recipe 2 requires autoregressive sampling but is more compute-efficient at inference
  - **ω distribution:** [0:0:0:1] (no early stopping) performed best; uniform or bimodal distributions harm performance
  - **γ discount factor:** γ=1.0 (no discounting) best; even γ=0.99 harms perplexity
  - **W_max:** Experiments used 3 pauses; higher values increase context window usage

- **Failure signatures:**
  - **Degenerate pause usage:** Model always outputs `<DON'T KNOW>` or never pauses → check that ψ'_DK prior is set correctly and loss is implemented with proper gradient flow
  - **No calibration:** d_i uncorrelated with accuracy improvement → verify loss is being applied at each step, not just final step
  - **Worse than baseline:** Position encoding issue or pause tokens not properly repurposed

- **First 3 experiments:**
  1. **Baseline validation:** Fine-tune Gemma1-2B on C4 subset without pauses (context window 2048) to establish perplexity baseline
  2. **TBYS comparison:** Fine-tune with 3 pause tokens per real token using standard cross-entropy loss (only final step contributes to loss) to replicate Goyal et al.
  3. **CYB-AP sweep:** Fine-tune with CYB-AP loss, sweeping ω ∈ {[0:0:0:1], [1:1:1:1], [4:1:1:4]} and γ ∈ {1.0, 0.99} to confirm paper's finding that ω=[0:0:0:1], γ=1.0 is optimal

## Open Questions the Paper Calls Out

- **Open Question 1:** Does conditioning attention-head weights on the pause index enhance the model's ability to utilize compute during pause steps? The authors suggest future work could involve "conditioning some of the attention-head weights on the pause index" to provide a more direct means of compute modulation. This remains unresolved because the current architecture relies on pause-index embeddings to distinguish steps, which limits the model's ability to compute distinct features when the input stream is nearly identical to the initial step.

- **Open Question 2:** Can CYB losses be effectively integrated into reinforcement learning (RL) alignment stages to enable variable, autoregressive pause insertion? The discussion notes that variable pause insertion (Recipe 2) would "make most sense... at the post-training (RL) alignment stage where autoregressive updating is performed." This remains untested because the current study relies on Recipe 1 (fixed pauses) to enable parallel training.

- **Open Question 3:** Does the improved micro-inference from CYB losses translate to performance gains on complex macro-inference and reasoning tasks? The introduction differentiates micro-inference (token integration) from macro-inference (reasoning/planning), but experimental results are limited to C4 perplexity. It is unclear if the data efficiency and adaptive computation learned for next-token prediction scale to multi-step logical reasoning or planning problems.

## Limitations

- The computational overhead and efficiency claims are relative rather than absolute, and the actual inference-time compute overhead from pause tokens is not thoroughly quantified
- The position encoding modifications for pause tokens are critical design choices not validated through ablation studies, with no empirical evidence they don't introduce subtle biases
- All experiments use Gemma1-2B (2B parameters), with no demonstration of whether the CYB framework generalizes to larger models or different architectures

## Confidence

**High Confidence Claims:**
- The CYB framework can be implemented and trained successfully on Gemma1-2B
- CYB-AP with ω=[0:0:0:1] and γ=1.0 achieves lower perplexity than other CYB variants
- The model demonstrates adaptive pause behavior correlated with token complexity
- The basic architectural modifications (token repurposing, position encoding sharing) are feasible

**Medium Confidence Claims:**
- CYB-AP requires less training data than the no-pauses baseline for equivalent performance
- The correlation between d_i and accuracy improvement indicates meaningful calibration
- The three CYB variants (AP, VA, DP) have distinct theoretical properties that manifest in practice

**Low Confidence Claims:**
- The efficiency gains are practically significant for real-world deployment
- The position encoding modification has no negative side effects
- The approach generalizes to larger models or different tasks beyond C4

## Next Checks

1. **Ablation Study on Position Encoding:** Systematically test the impact of different position encoding strategies for pause tokens (sequential positions vs shared positions vs no position encoding) to validate that the chosen approach is optimal and doesn't introduce hidden performance costs.

2. **Efficiency Benchmarking:** Measure actual inference-time latency and FLOPs for sequences with variable pause counts, comparing against both the no-pauses baseline and fixed-pause approaches. Quantify the trade-off between accuracy gains and computational overhead.

3. **Cross-Model Generalization:** Implement CYB on a larger model (e.g., Gemma2-7B or LLaMA-2-7B) and test whether the same hyperparameters (ω=[0:0:0:1], γ=1.0) remain optimal, or if model scaling requires different configurations.