---
ver: rpa2
title: 'Connectivity-Guided Sparsification of 2-FWL GNNs: Preserving Full Expressivity
  with Improved Efficiency'
arxiv_id: '2511.12838'
source_url: https://arxiv.org/abs/2511.12838
tags:
- node
- graph
- interactions
- gnns
- connected
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Co-Sparsify, a connectivity-aware sparsification
  framework for 2-FWL GNNs that eliminates only expressivity-redundant computations
  while preserving full 2-FWL expressive power. The key insight is that 3-node interactions
  are necessary only within biconnected components, while 2-node interactions matter
  only within connected components.
---

# Connectivity-Guided Sparsification of 2-FWL GNNs: Preserving Full Expressivity with Improved Efficiency

## Quick Facts
- **arXiv ID**: 2511.12838
- **Source URL**: https://arxiv.org/abs/2511.12838
- **Reference count**: 31
- **Primary result**: Co-Sparsify achieves 13-60% runtime reduction and 12-52% memory reduction while maintaining or improving accuracy on substructure counting and real-world benchmarks.

## Executive Summary
This paper introduces Co-Sparsify, a connectivity-aware sparsification framework for 2-FWL GNNs that eliminates only expressivity-redundant computations while preserving full 2-FWL expressive power. The key insight is that 3-node interactions are necessary only within biconnected components, while 2-node interactions matter only within connected components. Co-Sparsify restricts message passing accordingly, removing provably redundant operations without approximation or sampling. The framework achieves significant computational efficiency improvements (13-60% runtime reduction, 12-52% memory reduction) while maintaining or improving accuracy on substructure counting tasks and real-world benchmarks (ZINC, QM9). Theoretical analysis proves Co-Sparsified GNNs are as expressive as the 2-FWL test, making it the first sparsification method for HOGNNs with guaranteed expressivity preservation. The method demonstrates that high expressivity and scalability can be achieved simultaneously through principled, topology-guided sparsification.

## Method Summary
Co-Sparsify is a connectivity-guided sparsification framework that preserves the full expressive power of 2-FWL GNNs while eliminating redundant computations. The method uses block-cut tree decomposition to identify biconnected components and connected components in graphs, then restricts 3-node interactions to within biconnected components and 2-node interactions to within connected components. This creates sparse neighborhood sets (N_sp) that guide message passing, reducing computational complexity from O(n³) to O(∑n_bj³) where n_bj are biconnected component sizes. The framework integrates with existing HOGNN architectures (PPGN, GT-SMP) and uses standard training procedures with AdamW optimization and cosine learning rate decay.

## Key Results
- Achieves 13-60% runtime reduction and 12-52% memory reduction compared to standard PPGN
- Maintains or improves accuracy on substructure counting tasks and real-world benchmarks (ZINC, QM9)
- First sparsification method for HOGNNs with guaranteed 2-FWL expressivity preservation
- Outperforms existing methods (e.g., SPPGNN) on molecular property prediction tasks
- Demonstrates significant scalability improvements while preserving theoretical guarantees

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Restricting 3-node interactions to biconnected components preserves full 2-FWL expressivity.
- **Mechanism:** Within a biconnected component, any two nodes have at least two internally disjoint paths (Menger's theorem). Detecting such path multiplicity—essential for distinguishing graphs like 4-cycles from path chains—requires aggregating over triplets (u, t, v). Outside biconnected components, all paths between nodes must pass through a cut node, decomposing any (u, t, v) interaction into separable pairwise information already captured by (u, c) and (c, v).
- **Core assumption:** Aggregation is injective (e.g., sum or MLP-based), and initialization is consistent across pairs.
- **Evidence anchors:**
  - [abstract]: "3-node interactions are expressively necessary only within biconnected components — maximal subgraphs where every pair of nodes lies on a cycle."
  - [section 4.1, Lemma 1 & 2]: Formal necessity and redundancy proofs via case analysis on cut node configurations.
  - [corpus]: Weak direct evidence; neighbor papers discuss expressivity/efficiency tradeoffs but not this specific connectivity-based decomposition.
- **Break condition:** If aggregation is not injective (e.g., mean with collisions), the decomposition guarantee may fail since pairwise information cannot be uniquely reconstructed.

### Mechanism 2
- **Claim:** 2-node interactions across connected components are expressively redundant for connected substructure detection.
- **Mechanism:** Disconnected node pairs share no paths; the absence of edges itself encodes disconnection. Component-level readouts capture macro-topological features (component count, sizes), while graph-level readouts aggregate these. Explicit inter-component pair modeling adds no discriminative information for any connected query subgraph.
- **Core assumption:** Hierarchical readout functions are sufficiently expressive to capture global topology.
- **Evidence anchors:**
  - [abstract]: "Outside these components, structural relationships can be fully captured via 2-node message passing or global readout."
  - [section 4.1, Lemma 3]: Formal proof that disconnected queries' detection depends only on intra-component structure.
  - [corpus]: No direct counter-evidence; neighbor papers focus on expressivity bounds rather than readout design.
- **Break condition:** If the task requires distinguishing graphs by inter-component patterns not captured by size/count (e.g., specific component adjacency configurations), expressivity may be lost.

### Mechanism 3
- **Claim:** Block-cut tree preprocessing enables O(n+m) identification of sparse interaction neighborhoods without approximation.
- **Mechanism:** Tarjan's algorithm decomposes the graph into connected components and biconnected blocks in linear time. The co-sparsified neighbor set N_sp(u, v) is constructed by: (1) returning empty set for disconnected pairs, (2) including only intra-block triplets for biconnected regions, (3) propagating self-pair and edge-pair information for connectivity within components. No sampling or learned pruning is used.
- **Core assumption:** Graph connectivity structure is available and meaningful for the task.
- **Evidence anchors:**
  - [abstract]: "Co-Sparsify restricts message passing accordingly, removing provably redundant operations without approximation or sampling."
  - [section 4.2]: Detailed N_sp construction rules; preprocessing overhead reported as ~1ms per graph.
  - [corpus]: Neighbor paper "Large-Scale Spectral GNNs via Laplacian Sparsification" uses sampling-based sparsification with different guarantees, highlighting the distinction.
- **Break condition:** On fully biconnected graphs (e.g., complete graphs), sparsification provides no benefit since N_sp(u, v) ≈ V for all pairs.

## Foundational Learning

- **Concept: 2-FWL (Folklore Weisfeiler-Leman) test**
  - **Why needed here:** Co-Sparsify modifies the 2-FWL message passing scheme; understanding the baseline update rule h^(l)(u,v) = Φ(h^(l-1)(u,v), AGG_t[φ(h^(l-1)(u,t), h^(l-1)(t,v))]) is prerequisite to grasping what's being sparsified.
  - **Quick check question:** Given a 3-node graph with edges (a,b) and (b,c), which triplets contribute to updating h(a,c) in standard 2-FWL?

- **Concept: Biconnected components and block-cut trees**
  - **Why needed here:** The core theoretical lever; you must understand why cut nodes decompose expressivity and how Tarjan's algorithm identifies them.
  - **Quick check question:** In a graph where removing node x disconnects it into three components, which 3-node interactions involving x are provably redundant?

- **Concept: Injective aggregation functions**
  - **Why needed here:** The expressivity preservation proof hinges on aggregation being injective (no information collision). Common choices: sum, MLP-sum concatenation.
  - **Quick check question:** Why would mean aggregation break the theoretical guarantee even if all other components are correct?

## Architecture Onboarding

- **Component map:** Preprocessing module (BFS + Tarjan's) -> Sparse neighborhood constructor -> Co-sparsified message passing -> Hierarchical readout
- **Critical path:** Block-cut decomposition correctness -> sparse mask construction -> masked matrix operations in forward pass. Errors in decomposition propagate directly to expressivity loss.
- **Design tradeoffs:**
  - Preprocessing vs. online computation: Block-cut tree computed once per graph; efficient for static graphs, overhead for dynamic graphs.
  - Sparsity level vs. hardware utilization: Highly fragmented graphs yield high sparsity but may underutilize GPU parallelism; batch graphs by similar sparsity patterns.
  - Expressivity vs. over-squashing: Paper notes degraded performance on long-range benchmarks (Peptides-struct); distance-aware variant (restricting to ≤4 hops) mitigates this but may lose expressivity for larger substructures.
- **Failure signatures:**
  - Training/validation gap on long-range tasks: Near-perfect training MAE with poor test MAE indicates over-squashing, not expressivity loss (Appendix B.3).
  - No speedup on dense graphs: Fully biconnected graphs yield minimal sparsification; verify biconnected component distribution before expecting gains.
  - Incorrect substructure counts: If cut nodes are misidentified, triplets may be incorrectly excluded; validate decomposition on small synthetic graphs.
- **First 3 experiments:**
  1. **Sanity check on synthetic graphs:** Construct graphs with known biconnected structure (e.g., two triangles sharing a cut node); verify CoSp-PPGN matches PPGN on cycle counting.
  2. **Ablation on sparsification level:** Measure runtime/memory/accuracy on ZINC-subset with varying biconnected component thresholds (e.g., artificially merge blocks); plot efficiency-accuracy frontier.
  3. **Long-range stress test:** Evaluate on Peptides-struct with and without distance-aware restriction; confirm over-squashing hypothesis and document the depth-performance curve.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the connectivity-guided sparsification principle be generalized to Higher-Order GNNs (HOGNNs) based on $k$-FWL for $k > 2$?
- **Basis in paper:** [explicit] The authors explicitly state: "Since HOGNNs beyond 2-FWL face prohibitive computational complexity, we aim to extend our sparsification framework to make them more scalable."
- **Why unresolved:** The current theoretical proof relies specifically on the relationship between 2-node/3-node interactions and graph biconnectivity. Extending this to $k$-tuples requires identifying new topological invariants that dictate redundancy in higher-dimensional interaction spaces.
- **What evidence would resolve it:** A formal extension of the redundancy lemmas (specifically Lemma 1 and 2) to $k$-tuple interactions, followed by empirical validation showing efficiency gains on $3$-FWL or $4$-FWL architectures without performance degradation.

### Open Question 2
- **Question:** How can the conflict between preserving full 2-FWL expressivity and mitigating the "over-squashing" phenomenon in deep architectures be resolved?
- **Basis in paper:** [explicit] The paper identifies a "key open problem: balancing expressivity and generalization," noting that standard Co-Sparsify underperforms on long-range benchmarks (Peptides-struct) due to information distortion, even while maintaining theoretical expressivity.
- **Why unresolved:** Preserving all theoretically relevant interactions (to guarantee expressivity) forces long-range dependencies into fixed-dimension node embeddings, which causes information distortion (over-squashing). It is unclear if provably lossless sparsification is compatible with deep long-range generalization.
- **What evidence would resolve it:** The identification of a sparsification criteria that removes "generalization-harmful" interactions in addition to "expressivity-redundant" ones, or the integration of a mechanism that controls information flow bottlenecks in the sparsified topology.

### Open Question 3
- **Question:** How can adaptive receptive fields be formalized within the Co-Sparsify framework to dynamically balance structural sensitivity and noise?
- **Basis in paper:** [explicit] The authors state in Future Work: "To address this [generalization], we plan to explore adaptive receptive fields."
- **Why unresolved:** The current method uses static, deterministic boundaries (biconnected components). Making these fields "adaptive" implies learning which interactions to keep, which risks violating the theoretical guarantees of the current fixed approach.
- **What evidence would resolve it:** A learnable sparsification module that can dynamically adjust the scope of 3-node interactions based on task requirements, ideally accompanied by bounds on the resulting expressivity loss or generalization gain.

## Limitations
- The theoretical guarantees assume injective aggregation functions and consistent initialization, which may not hold for all aggregation choices.
- The method provides minimal benefit on fully biconnected graphs where sparsification cannot significantly reduce the interaction space.
- The distance-aware variant for long-range tasks trades expressivity for performance, requiring careful task-specific configuration.
- Preprocessing overhead may be significant for dynamic graphs where connectivity structure changes frequently.

## Confidence
- **High confidence:** Core theoretical claims (2-FWL expressivity preservation via connectivity-guided sparsification) due to formal proofs in Section 4.1.
- **Medium confidence:** Practical efficiency gains, as results depend on graph structure distribution and hardware utilization patterns.
- **Low confidence:** General applicability to long-range tasks without the distance-aware variant.

## Next Checks
1. Verify expressivity preservation on synthetic graphs with controlled biconnected structure by comparing cycle counting accuracy between CoSp-PPGN and standard PPGN.
2. Characterize the efficiency-accuracy tradeoff by systematically varying biconnected component thresholds and measuring the resulting sparsity patterns and model performance.
3. Test the distance-aware variant on long-range benchmarks to confirm whether the reported over-squashing hypothesis explains degraded performance, and document the optimal distance threshold for different task types.