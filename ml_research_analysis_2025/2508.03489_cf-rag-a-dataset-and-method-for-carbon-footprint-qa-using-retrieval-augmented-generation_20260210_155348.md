---
ver: rpa2
title: 'CF-RAG: A Dataset and Method for Carbon Footprint QA Using Retrieval-Augmented
  Generation'
arxiv_id: '2508.03489'
source_url: https://arxiv.org/abs/2508.03489
tags:
- carbon
- data
- question
- footprint
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CarbonPDF-QA, a dataset of 18,000 carbon
  footprint questions derived from 1,735 real-world product reports in PDF format,
  and CarbonPDF, a retrieval-augmented QA system designed to handle the inconsistencies
  and unstructured data common in such documents. Unlike prior work, the method uses
  a critic model to select the most relevant retrieved documents and a program-based
  reasoner that generates executable Python code for numerical reasoning, improving
  accuracy over direct text generation.
---

# CF-RAG: A Dataset and Method for Carbon Footprint QA Using Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2508.03489
- Source URL: https://arxiv.org/abs/2508.03489
- Reference count: 40
- Key outcome: Introduces CarbonPDF-QA dataset and CarbonPDF RAG system for carbon footprint QA, achieving 93.70% Exact Match on numerical questions over unstructured PDF reports.

## Executive Summary
This paper introduces CarbonPDF-QA, a dataset of 18,000 carbon footprint questions derived from 1,735 real-world product reports in PDF format, and CarbonPDF, a retrieval-augmented QA system designed to handle the inconsistencies and unstructured data common in such documents. Unlike prior work, the method uses a critic model to select the most relevant retrieved documents and a program-based reasoner that generates executable Python code for numerical reasoning, improving accuracy over direct text generation. Experiments show CarbonPDF achieves state-of-the-art performance with RMSE of 0.78, MAE of 0.69, and Exact Match of 93.70%, outperforming baselines including GPT-4o and fine-tuned models. Ablation studies confirm the effectiveness of both the critic model and program-based reasoning. The dataset and model are available to support future research on QA over unstructured sustainability documents.

## Method Summary
The CarbonPDF system uses a two-stage RAG pipeline: first, TF-IDF retrieves top-k documents; then a fine-tuned Llama 3 8B critic selects the most relevant one. A program-based reasoner generates executable Python code for numerical calculations rather than direct text answers. The system is fine-tuned on CarbonPDF-QA, a dataset of 18k QA pairs from 1,735 real-world carbon footprint PDFs, using QLoRA with 4-bit quantization. This approach handles noisy PDF extraction and inconsistent reporting formats better than general-purpose models like GPT-4o.

## Key Results
- Achieves state-of-the-art performance: RMSE 0.78, MAE 0.69, Exact Match 93.70%
- Outperforms GPT-4o (EM 49.20% without programs, 62.18% with programs) and other baselines
- Ablation studies show critic model contributes ~10% EM improvement, program-based reasoning contributes ~27% EM improvement
- Successfully handles numerical reasoning over inconsistent and noisy PDF-extracted data

## Why This Works (Mechanism)

### Mechanism 1: Critic-Based Reranking
A dedicated "critic" model is necessary to select the correct reference document from a broad retrieval pool when dealing with inconsistent PDF data. The system uses TF-IDF to fetch top-k documents, then a fine-tuned Llama 3 critic evaluates these candidates against the question to select the single most relevant text segment. The core assumption is that the relevant answer context exists within the top-k retrieved candidates, even if not ranked first. Ablation studies show removing the critic drops Exact Match by approximately 10% (83.92% → 93.70%).

### Mechanism 2: Program-Based Reasoning
Offloading numerical calculation to a Python interpreter significantly reduces arithmetic errors compared to direct text generation. Instead of predicting the final answer token-by-token, the model generates a Python program that defines variables and executes calculation logic. This approach is more reliable than direct text generation for math problems. Ablation studies confirm that removing the program reasoner (generating text directly) causes a approximately 27% drop in Exact Match.

### Mechanism 3: Domain-Specific Fine-Tuning on Noisy Data
Fine-tuning a smaller model (Llama 3 8B) on specific, noisy PDF-extracted data enables it to outperform larger general-purpose models (GPT-4o). The model is trained on CarbonPDF-QA, which contains "raw" text extracted by PyMuPDF, forcing it to learn to ignore formatting artifacts and spurious text common in PDF parsing. This specialization in document noise tolerance is crucial for the domain. Fine-tuned CarbonPDF achieves 93.70% EM, vastly outperforming GPT-4o (49.20% EM without programs, 62.18% with programs).

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: The system cannot rely on the LLM's parametric memory for specific, recent product carbon reports. The mechanism depends on explicitly fetching a document context before reasoning.
  - Quick check question: If a user asks about a product released yesterday, why would a standard LLM fail but a RAG system potentially succeed?

- **Concept: Tool Use / Code Execution (PAL)**
  - Why needed here: The core innovation is generating executable code to solve problems, not just generating text. Understanding the difference between reasoning trace and execution is vital.
  - Quick check question: Why is `print(275 * 0.075)` generally more reliable than the LLM outputting "The answer is roughly 20.6"?

- **Concept: TF-IDF vs. Dense Retrieval**
  - Why needed here: The authors explicitly choose TF-IDF over neural retrievers for the initial search. Understanding keyword matching vs. semantic embedding is necessary to grasp the retrieval design.
  - Quick check question: Why might a sparse keyword match (TF-IDF) be preferable to a dense semantic search when looking for a very specific product ID or number in a technical report?

## Architecture Onboarding

- **Component map:** Parser (PyMuPDF) -> TF-IDF Vectorization -> Top-10 Retrieval -> Critic Selection (Top-1) -> Code Generation -> Execution -> Answer
- **Critical path:** Query → TF-IDF Vectorization → Top-10 Retrieval → Critic Selection (Top-1) → Code Generation → Execution → Answer
- **Design tradeoffs:**
  - TF-IDF vs. Neural Retrieval: TF-IDF achieves nearly 100% hit rate in top-10, whereas neural methods struggled. The tradeoff is lower precision at Top-1, which necessitates the added complexity of the Critic model.
  - Fine-tuning vs. Few-shot: Fine-tuning is required because few-shot prompting proved too brittle for the "spurious" data and complex math (RMSE > 800 in ablation).
- **Failure signatures:**
  - Spurious Data Injection: The LLM incorporates random numbers from hidden PDF text (e.g., page headers/footers) into its calculation logic.
  - Critic Bottleneck: The Critic selects a document that mentions the entity (e.g., "Laptop") but lacks the specific breakdown table required for the math.
  - Execution Error: The generated code attempts to divide by zero or calls a variable (e.g., `ssd_percent`) that was never defined in the context.
- **First 3 experiments:**
  1. Implement the TF-IDF retrieval pipeline and measure the "Top-k Hit Rate" to verify the retriever is good enough to support a critic.
  2. Run the full pipeline, then bypass the critic (feed the top-1 TF-IDF result directly to the reasoner) to quantify the critic's specific contribution to Exact Match.
  3. Prompt the fine-tuned model to generate a direct text answer vs. a Python program for a set of "Calculation" questions and compare the error distributions.

## Open Questions the Paper Calls Out

### Open Question 1
Can multimodal LLMs effectively reason over both textual and graphical content (pie charts, bar graphs) in sustainability reports? The paper states they plan to explore multimodal LLMs to perform reasoning on both text and visual data, noting current limitation that CarbonPDF primarily relies on text data and cannot effectively answer questions based on content that combines graphs and text.

### Open Question 2
Can LLMs be trained to handle synonym understanding and hierarchical component relationships when exact terminology doesn't appear in documents? The paper identifies this as key future research, noting CarbonPDF fails when asked about "processor" if only "mainboard" appears, lacking semantic understanding of component hierarchies.

### Open Question 3
How well does CarbonPDF generalize to sustainability reports from industries beyond computing products? The dataset is limited to 4 computing companies (HP, Dell, Acer, Lenovo) with 1,735 PDF reports, and reporting formats likely differ across industries.

### Open Question 4
What is the upper bound on retrieval quality needed to prevent critic model failures when relevant documents rank outside top-k? TF-IDF achieves ~87% top-1 hit rate; critic model can only select from top-k candidates. If correct document ranks below k, the pipeline fails with no recovery mechanism.

## Limitations
- The system's generalizability to unseen vendors or document formats is not tested, focusing only on four major PC manufacturers
- The effectiveness of the critic model depends heavily on TF-IDF retrieval quality, which claims "nearly 100% hit rate" without rigorous quantification across all vendors
- The paper does not explore alternative critic architectures or compare program-based reasoning against other code-generation approaches in the literature

## Confidence
- **High Confidence:** Dataset construction methodology and overall RAG architecture are well-documented and reproducible. Reported performance metrics (RMSE: 0.78, MAE: 0.69, EM: 93.70%) are specific and verifiable.
- **Medium Confidence:** Claim that GPT-4o struggles with data inconsistencies is supported by comparative results, but lacks detailed error analysis. Critic model design lacks ablation comparisons against simpler reranking methods.
- **Low Confidence:** Generalizability to unseen vendors or document formats is not tested. Model may overfit to specific noise patterns of training companies.

## Next Checks
1. **Retriever Robustness:** Conduct vendor-specific analysis of TF-IDF retrieval hit rate. Measure how often correct document appears in top-10 for each vendor and identify systematic retrieval failures.
2. **Critic Generalization:** Test critic model on held-out documents from new vendor not in training data. Evaluate whether critic can still select correct document context with unfamiliar formatting and terminology.
3. **Program vs. Direct Generation:** Implement direct text-generation baseline (fine-tuned Llama 3 8B generating answers without code) and compare performance on same "Calculation" questions. Analyze error distribution to determine if program-based reasoning consistently reduces arithmetic errors or introduces new failure modes.