---
ver: rpa2
title: 'See the past: Time-Reversed Scene Reconstruction from Thermal Traces Using
  Visual Language Models'
arxiv_id: '2510.05408'
source_url: https://arxiv.org/abs/2510.05408
tags:
- thermal
- scene
- heat
- image
- traces
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors propose a method for reconstructing past scenes from
  thermal traces using paired RGB and thermal images. The approach combines visual-language
  models (VLMs) with a constrained diffusion process: one VLM generates scene descriptions
  from multimodal inputs, and these descriptions guide a diffusion model to synthesize
  plausible past frames.'
---

# See the past: Time-Reversed Scene Reconstruction from Thermal Traces Using Visual Language Models

## Quick Facts
- arXiv ID: 2510.05408
- Source URL: https://arxiv.org/abs/2510.05408
- Authors: Kebin Contreras; Luis Toscano-Palomino; Mauro Dalla Mura; Jorge Bacca
- Reference count: 0
- Method reconstructs past scene states from thermal traces using VLMs and diffusion models

## Executive Summary
This work introduces a method for time-reversed scene reconstruction, enabling visualization of past states by leveraging fading thermal traces. The approach uses paired RGB and thermal images captured with an FLIR ONE camera, where heat signatures from human-object interactions serve as passive temporal codes. By combining visual-language models for scene description generation with a constrained diffusion process, the method synthesizes plausible past frames up to 120 seconds before the current observation. Experiments across three controlled scenarios demonstrate that incorporating thermal data and scene descriptions significantly improves reconstruction quality compared to RGB-only baselines.

## Method Summary
The method operates in two stages using frozen pretrained VLMs without fine-tuning. First, a VLM generates structured scene descriptions from multimodal inputs (current RGB, thermal, and a descriptive prompt) in past tense, describing the interaction that occurred. Second, another VLM performs constrained diffusion-based generation, using the scene description and input images to synthesize the past frame. The framework leverages the temporal decay of thermal traces as implicit coding of past events, with evaluation showing improved performance in both low-level metrics (PSNR, SSIM) and high-level metrics (pose estimation, semantic segmentation) when thermal and description inputs are included.

## Key Results
- Method successfully reconstructs past scenes up to 120 seconds earlier from thermal traces
- Including thermal data and scene descriptions improves reconstruction quality across all metrics
- Performance degrades with time delay, with best results at 5-30 seconds post-contact

## Why This Works (Mechanism)
The method exploits the physics of thermal dissipation as a natural temporal marker. When a person interacts with an object, heat transfer creates a visible thermal signature that fades predictably over time. This fading trace acts as a passive temporal code encoding when the interaction occurred. By capturing this signature alongside RGB imagery, the system can infer the past state. The VLMs bridge the gap between the multimodal thermal cues and coherent visual generation, translating the thermal decay pattern into structured descriptions that guide the diffusion model to reconstruct the missing past scene content.

## Foundational Learning

**Thermal Imaging Principles**: Understanding how LWIR cameras capture heat signatures and how materials retain/emit heat differently. *Why needed*: To interpret thermal traces as temporal codes and understand the method's operational limits. *Quick check*: Verify thermal sensor warming and emissivity settings produce visible traces for 30+ seconds.

**Visual Language Models**: How VLMs process multimodal inputs and generate structured outputs. *Why needed*: Core to both description generation and constrained image synthesis steps. *Quick check*: Test VLM with paired RGB-thermal inputs to confirm structured past-tense descriptions.

**Diffusion Models**: Principles of iterative denoising guided by conditioning signals. *Why needed*: Enables the constrained generation of past frames from scene descriptions. *Quick check*: Verify that adding scene description text to diffusion prompts changes generated output content.

## Architecture Onboarding

**Component Map**: RGB+Thermal Input -> VLM Description Generator -> Scene Description -> VLM Diffusion Generator -> Past Frame Output

**Critical Path**: The most timing-sensitive sequence is thermal trace capture (must occur while signature is visible) → VLM description generation → diffusion-based synthesis. Any delay in this pipeline risks missing the thermal evidence.

**Design Tradeoffs**: Uses frozen VLMs for zero-shot inference instead of training, sacrificing potential performance gains for flexibility and avoiding dataset requirements. The low-resolution thermal sensor (80×60) limits fine detail recovery but provides essential temporal coding.

**Failure Signatures**: Weak or noisy thermal traces produce vague/hallucinated descriptions. Generated frames may preserve environment but fail to render correct human pose/action. Temporal decay beyond 30s makes traces too faint for reliable reconstruction.

**First Experiments**:
1. Capture paired RGB-thermal dataset with controlled contact scenarios to establish baseline trace visibility and decay patterns
2. Implement scene description pipeline with practical VLM substitutes and test structured past-tense output generation
3. Evaluate constrained generation pipeline with explicit pose placement instructions to verify pose reconstruction capability

## Open Questions the Paper Calls Out
**Open Question 1**: Can the framework maintain reconstruction fidelity in uncontrolled, real-world environments with variable illumination and complex backgrounds? The authors state that future work will "extend the framework to real-world environments... and variable illumination." The current evaluation is restricted to three controlled scenarios (sitting, touching, leaning) with static lighting and simple backgrounds.

**Open Question 2**: How does the method perform when disentangling overlapping thermal traces from multiple subjects? The authors identify extending the framework to "multiple subjects" as a specific direction for future work. The current proof-of-concept only validates single-subject interactions, leaving the separation of complex or overlapping heat signatures unexplored.

**Open Question 3**: Does the low spatial resolution of the thermal sensor (80x60) fundamentally limit the recovery of fine-grained details? The paper notes the thermal sensor operates at 80x60 while the RGB is 1440x1080, and results show a loss of fine details at longer time intervals. It is unclear if the failure to capture fine details at 120s is due to thermal decay physics or the upsampling of low-resolution thermal inputs.

## Limitations
- Relies on proprietary VLMs (GPT-5, Gemini 2.5 Flash Image) not publicly available, requiring substitutions
- Limited temporal window (up to 120 seconds) dependent on thermal trace visibility and decay
- Current evaluation restricted to three controlled scenarios with simple backgrounds and static lighting

## Confidence
**High confidence in**: the overall framework design (using VLMs for description generation followed by diffusion-based synthesis), the general approach of leveraging thermal traces as temporal codes, and the qualitative improvement from including thermal data and scene descriptions.

**Medium confidence in**: the specific quantitative results (PSNR, SSIM, MPJPE, OA scores) due to potential differences in VLM availability and implementation details, the generalizability beyond the three tested scenarios, and the exact performance degradation timeline beyond 30 seconds.

**Low confidence in**: the exact cross-attention implementation details for multimodal conditioning, the precise dataset collection and preprocessing protocols, and the performance on real-world scenes with uncontrolled thermal conditions.

## Next Checks
1. Reproduce the baseline ablation (RGB-only, no scene description) using publicly available VLMs (GPT-4o, DALL-E 3) and compare against reported metrics to establish performance floor and identify VLM impact.

2. Systematically test thermal trace visibility thresholds by controlling room temperature, material emissivity, and contact duration to map the method's operational limits and validate the 120-second claim under varying conditions.

3. Implement and evaluate the constrained generation pipeline with explicit pose placement instructions (p4_edit variant) to verify whether the method can consistently reconstruct human poses and actions rather than just environmental features.