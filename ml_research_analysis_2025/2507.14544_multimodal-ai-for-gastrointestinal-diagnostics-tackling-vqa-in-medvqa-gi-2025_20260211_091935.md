---
ver: rpa2
title: 'Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI
  2025'
arxiv_id: '2507.14544'
source_url: https://arxiv.org/abs/2507.14544
tags:
- visual
- question
- dataset
- medical
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multimodal AI pipeline for visual question
  answering in gastrointestinal endoscopy, using the Florence-2 model as the core
  framework. The approach involves fine-tuning Florence-2 with domain-specific augmentations
  on a subset of the KASVIR dataset to improve generalization in medical VQA.
---

# Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025

## Quick Facts
- arXiv ID: 2507.14544
- Source URL: https://arxiv.org/abs/2507.14544
- Reference count: 31
- Primary result: Fine-tuned Florence-2 on 1% KASVIR-VQA subset achieved BLEU 0.15, ROUGE-L 0.80, METEOR 0.44 on GI endoscopy VQA

## Executive Summary
This paper presents a multimodal AI pipeline for visual question answering in gastrointestinal endoscopy using Florence-2 as the core framework. The approach fine-tunes Florence-2 with domain-specific augmentations on a stratified subset of the KASVIR dataset to improve generalization in medical VQA tasks. Experiments demonstrate that fine-tuned augmentations yield the best performance metrics, though the model performs better on spatial and binary questions than on procedural or abstract ones. The work provides a strong baseline for future research on interpretability, robustness, and clinical integration in medical VQA systems.

## Method Summary
The method involves fine-tuning the `microsoft/Florence-2-base-ft` model with a frozen ViT-L/14 vision encoder on the KASVIR-VQA dataset. A 1% stratified subset (~588 samples) is created from the full 58,849 image-question-answer triplets with 90/10 train/validation split. The model is trained using AdamW optimizer with learning rate 7.8e-6, weight decay 0.1, and cosine decay over 20 epochs. Domain-specific augmentations including random crop, flip, and color jitter are applied during training. Mixed precision training (FP16/FP32) is used on NVIDIA T4 GPU with gradient accumulation. The causal language modeling loss is configured with padding tokens set to -100 for proper handling of variable-length outputs.

## Key Results
- Validation metrics: BLEU 0.15, ROUGE-L 0.80, METEOR 0.44 on 1% KASVIR-VQA subset
- Performance gap: Spatial and binary questions outperformed procedural and abstract questions significantly
- Best augmentation: Fine-tuned augmentation approach showed superior performance over standard augmentation
- Computational setup: Trained on NVIDIA T4 GPU (16GB) with gradient accumulation and mixed precision

## Why This Works (Mechanism)
The approach leverages Florence-2's strong multimodal foundation by fine-tuning on domain-specific GI endoscopy data while preserving the frozen vision encoder's general visual capabilities. Domain-specific augmentations help the model generalize across variations in endoscopic imaging conditions. The stratified sampling ensures representation across different question types and difficulty levels. The causal language modeling framework allows the model to generate coherent answers conditioned on both image and question inputs. The frozen vision encoder maintains learned visual representations while the language decoder adapts to medical domain terminology and question-answer patterns.

## Foundational Learning
- **Visual Question Answering (VQA)**: Why needed - Core task combining image understanding with natural language processing; Quick check - Model can answer basic questions about endoscopic images
- **Fine-tuning vs. Training from Scratch**: Why needed - Efficient adaptation of pre-trained models to specialized domains; Quick check - Lower training cost and faster convergence compared to full training
- **Domain-Specific Data Augmentation**: Why needed - Improves generalization to real-world variations in medical imaging; Quick check - Performance improvement on augmented vs. non-augmented validation sets
- **Metric Evaluation (BLEU/ROUGE-L/METEOR)**: Why needed - Standard measures for assessing generated answer quality; Quick check - Consistent improvement across all three metrics indicates robust performance
- **Stratified Sampling**: Why needed - Ensures balanced representation of different question types during training; Quick check - Validation set maintains same distribution as full dataset
- **Mixed Precision Training**: Why needed - Reduces memory usage and speeds up training on GPUs; Quick check - Training completes without memory overflow on T4 GPU

## Architecture Onboarding

**Component Map:**
Florence-2 Base -> Frozen ViT-L/14 Vision Encoder -> Causal Language Decoder -> Answer Generation

**Critical Path:**
Image Input → Vision Encoder → Cross-modal Fusion → Language Decoder → Generated Answer

**Design Tradeoffs:**
- Frozen vision encoder: Preserves general visual capabilities but limits adaptation to GI-specific features
- 1% subset training: Enables experimentation but raises generalization concerns
- Domain augmentations: Improves robustness but requires careful parameter tuning
- Mixed precision: Saves memory but may affect training stability

**Failure Signatures:**
- OOM errors: Likely from insufficient batch size or missing gradient accumulation
- Degraded performance: May indicate over-aggressive augmentations or insufficient training data
- Inconsistent metrics: Could signal issues with padding token handling or loss configuration

**First Experiments:**
1. Verify basic VQA functionality on held-out validation samples from the 1% subset
2. Test augmentation ablations to confirm which augmentations contribute most to performance
3. Evaluate model's ability to handle different question types (spatial vs. procedural) systematically

## Open Questions the Paper Calls Out
**Open Question 1:** How can the Florence-2 pipeline be adapted to reliably detect and abstain from answering clinically unanswerable questions to prevent misinformation? The current study focused on standard VQA metrics for answerable questions, and the model's ability to reject mismatched or unanswerable queries was not evaluated.

**Open Question 2:** What specific architectural or training modifications are required to improve performance on procedural reasoning questions (e.g., "how") to match the levels of spatial queries? The results highlight a performance gap where procedural questions score significantly lower than spatial or binary questions.

**Open Question 3:** How does model performance scale when training data is increased from the 1% subset used in this study to the full Kvasir-VQA dataset? The authors acknowledge that experiments were conducted on a small subset due to limited infrastructure, framing this as a limitation.

## Limitations
- Performance metrics based on only 1% stratified subset may not generalize to full dataset
- Exact parameter settings for "fine-tuned augmentation" variant not specified, limiting precise replication
- Lower performance on procedural and abstract questions indicates significant limitations in complex medical reasoning
- Single GPU configuration (NVIDIA T4) limits assessment of scalability for larger deployments

## Confidence
- **High Confidence**: Core methodology (fine-tuning Florence-2 with frozen ViT-L/14 encoder) and basic experimental setup are clearly specified and reproducible
- **Medium Confidence**: Reported validation metrics are verifiable, but generalizability to full dataset remains uncertain due to 1% sampling approach
- **Low Confidence**: Claims about domain-specific augmentation benefits lack precise parameter specifications

## Next Checks
1. Re-run the fine-tuning pipeline on the complete KASVIR-VQA dataset (rather than 1% subset) to verify that validation metrics remain consistent and don't degrade significantly
2. Contact authors to obtain exact hyperparameter values for the "fine-tuned augmentation" approach (rotation degrees, color jitter magnitudes, etc.) to enable precise replication
3. Design and implement a systematic evaluation protocol specifically targeting procedural and abstract question types to quantify the model's limitations in complex medical reasoning scenarios