---
ver: rpa2
title: 'SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning
  Capabilities'
arxiv_id: '2502.12025'
source_url: https://arxiv.org/abs/2502.12025
tags:
- safety
- safe
- reasoning
- lrms
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a systematic safety evaluation of large reasoning
  models (LRMs) that use long chain-of-thought reasoning. The authors develop metrics
  to assess safety jointly across the reasoning trace and final answer, and find that
  state-of-the-art LRMs remain unsafe on StrongReject and WildJailbreak datasets despite
  their reasoning advances.
---

# SafeChain: Safety of Language Models with Long Chain-of-Thought Reasoning Capabilities

## Quick Facts
- arXiv ID: 2502.12025
- Source URL: https://arxiv.org/abs/2502.12025
- Authors: Fengqing Jiang, Zhangchen Xu, Yuetai Li, Luyao Niu, Zhen Xiang, Bo Li, Bill Yuchen Lin, Radha Poovendhan
- Reference count: 24
- Key outcome: SafeChain dataset improves safety of LRMs while preserving reasoning performance

## Executive Summary
This paper conducts a systematic safety evaluation of large reasoning models (LRMs) that use long chain-of-thought (CoT) reasoning. The authors develop metrics to assess safety jointly across the reasoning trace and final answer, finding that state-of-the-art LRMs remain unsafe on StrongReject and WildJailbreak datasets despite their reasoning advances. They observe that unsafe responses tend to be longer and that learning long CoT does not necessarily enhance safety. To improve LRM safety, the authors introduce SafeChain, a safety training dataset in CoT style, and demonstrate that fine-tuning LRMs with it improves safety while preserving performance on six reasoning benchmarks.

## Method Summary
The authors create SafeChain by sampling 50K instructions from WildJailbreak, generating 5 responses per instruction using R1-70B, filtering with Llama-Guard to keep only those where all 5 responses are safe, then selecting 1 response per instruction to create 40K CoT-style instruction-response pairs. They fine-tune R1-7B and R1-8B using supervised fine-tuning with LLaMA Factory (LR=1e-5, 2 epochs, cosine scheduler) on this dataset. Safety is evaluated using Llama-Guard on StrongReject (310 queries) and WildJailbreak (250 queries) with Safe@1, Safe@K, and ConsSafe@K metrics, while reasoning performance is measured on GSM8K, MATH-500, AIME 2024, HumanEval, MBPP, and LiveCodeBench.

## Key Results
- State-of-the-art LRMs remain unsafe on StrongReject and WildJailbreak despite reasoning advances
- Unsafe responses tend to be longer and use more tokens than safe ones
- Three decoding strategies (ZeroThink, LessThink, MoreThink) can improve model safety without additional training
- Fine-tuning LRMs with SafeChain improves safety while preserving performance on six reasoning benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Length-based safety correlation
- Claim: Longer, unconstrained chain-of-thought traces correlate with increased likelihood of unsafe content in LRM outputs
- Mechanism: Unsafe responses use more tokens and are longer than safe ones, providing more surface area for harmful content to emerge
- Core assumption: The correlation reflects that extended generation increases exposure to unsafe tokens/paths in the model's distribution
- Evidence anchors: Finding 3 shows unsafe responses skewed toward higher token counts; H-CoT studies hijacking CoT safety reasoning
- Break condition: If unsafe responses are longer primarily because they require more explanation, not because length causes unsafety

### Mechanism 2: Decoding strategy intervention
- Claim: Constrained reasoning trace length via decoding strategies can improve safety without retraining
- Mechanism: ZeroThink (empty CoT) showed best safety, suggesting models may have a "safety instinct" accessible without explicit reasoning
- Core assumption: Safety improvements stem from altering reasoning trace content/structure, not from unrelated changes
- Evidence anchors: ZeroThink achieving near-perfect safety on StrongReject for larger models; AdvChain explores adversarial CoT tuning
- Break condition: If safety gains from ZeroThink are primarily because the model defaults to a short refusal template

### Mechanism 3: Supervised fine-tuning on safety data
- Claim: Supervised fine-tuning on safety-aligned, long-CoT-style data can improve LRM safety while preserving reasoning capability
- Mechanism: SafeChain dataset provides instruction-response pairs with safe CoT traces, aligning model's CoT generation and final answers with safety constraints
- Core assumption: Safety and reasoning capabilities are sufficiently orthogonal that fine-tuning on well-matched distribution can improve one without degrading the other
- Evidence anchors: SafeChain vs. baselines on math/coding/safety benchmarks; UnsafeChain and AdvChain provide converging evidence
- Break condition: If reasoning performance degrades significantly or safety gains are brittle to new jailbreak types

## Foundational Learning

- Concept: Chain-of-Thought (CoT) Reasoning
  - Why needed here: The entire paper centers on LRMs that generate long CoT
  - Quick check question: Can you explain how a model's CoT trace differs from its final output token?

- Concept: Safety Alignment in LLMs
  - Why needed here: The paper evaluates and improves safety
  - Quick check question: What is the goal of safety alignment, and what are two common techniques?

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: The proposed intervention uses SFT on the SafeChain dataset
  - Quick check question: How does SFT differ from pretraining, and what is a key risk when fine-tuning for alignment?

## Architecture Onboarding

- Component map: WildJailbreak/StrongReject datasets -> Llama-Guard evaluation -> R1-70B generation -> SafeChain filtering -> SFT on R1-7B/R1-8B -> Safety and reasoning benchmark evaluation
- Critical path:
  1. Validate the safety evaluator (Llama-Guard) on your target model/data
  2. Establish a safety baseline on benchmarks using Safe@1 metrics
  3. Pilot decoding strategies (especially ZeroThink) to confirm they provide a safety boost
  4. Prepare SafeChain-like data: sample diverse instructions, generate multi-response with a safe LRM, filter with Llama-Guard
  5. Fine-tune LRM with SFT, then re-evaluate on both safety and reasoning benchmarks
- Design tradeoffs:
  - ZeroThink: High safety, but sacrifices CoT benefits; useful as a cheap baseline
  - MoreThink: Moderate safety gain, but high inference cost; may be viable only for high-stakes queries
  - SafeChain SFT: Balances safety and reasoning, but requires dataset creation and compute; risk of overfitting or capability loss
- Failure signatures:
  1. Safety gains from SFT are lost after one epoch (overfitting to refusal templates)
  2. Reasoning benchmark scores drop >10% after SafeChain fine-tuning (catastrophic forgetting)
  3. ZeroThink safety degrades on new jailbreak types (surface-level alignment)
- First 3 experiments:
  1. Reproduce the evaluator comparison (Llama-Guard vs. others) on your own LRM outputs to validate Llama-Guard selection
  2. Ablate SafeChain data: compare models fine-tuned on SafeChain vs. same instructions with non-CoT safe responses
  3. Test generalization: evaluate SafeChain-fine-tuned models on a held-out safety benchmark not used in the paper

## Open Questions the Paper Calls Out

- How can safety evaluation methods for long CoT reasoning be refined to better handle the unique challenges of extended reasoning traces? (Future work: refining safety evaluation methods for long CoT reasoning)
- How does the safety of LRMs perform in multi-turn interactions compared to the single-turn evaluations conducted in this study? (Future work: safety evaluation on multi-turn interaction)
- What mechanisms cause fine-tuning with long CoT to degrade safety performance in models that were previously well-aligned? (Paper shows degradation but does not isolate underlying cause)
- Can SafeChain's safety improvements generalize effectively to multilingual settings and diverse reasoning model architectures? (Future work: extending SAFE CHAIN to multilingual settings)

## Limitations

- The safety evaluation relies heavily on Llama-Guard as the sole safety metric, introducing potential bias since the same model is used for both dataset filtering and evaluation
- The SafeChain dataset creation process may introduce selection bias toward conservative, refusal-heavy responses by only keeping instructions where all 5 generated responses are safe
- The decoding strategy experiments lack mechanistic clarity about why these approaches work, with hypotheses remaining speculative

## Confidence

- High Confidence: The observation that unsafe responses tend to be longer and use more tokens is well-supported by empirical evidence
- Medium Confidence: The effectiveness of ZeroThink as a decoding strategy for improving safety is demonstrated empirically, though the underlying mechanism remains unclear
- Low Confidence: The claim that SafeChain fine-tuning preserves reasoning capabilities while improving safety relies on comparison with only one baseline

## Next Checks

1. **Human Evaluation Validation**: Conduct a human study comparing Llama-Guard safety judgments against human annotators on a stratified sample of Safe@1 predictions from StrongReject and WildJailbreak

2. **Ablation of CoT Format**: Create a variant of SafeChain where safe responses are formatted identically to WJ-40K (direct answers without CoT traces) but sourced from the same instruction pool

3. **Cross-Evaluator Consistency**: Evaluate the same model outputs using multiple safety classifiers (not just Llama-Guard) including those from different organizations or trained on different datasets