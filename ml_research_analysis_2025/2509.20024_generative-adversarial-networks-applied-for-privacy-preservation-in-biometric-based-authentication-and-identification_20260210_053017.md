---
ver: rpa2
title: Generative Adversarial Networks Applied for Privacy Preservation in Biometric-Based
  Authentication and Identification
arxiv_id: '2509.20024'
source_url: https://arxiv.org/abs/2509.20024
tags:
- images
- learning
- domain
- privacy
- faces
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using GANs to translate face images into visually
  private domains (e.g., flowers, shoes) for privacy-preserving biometric authentication.
  The key idea is to train a classifier on translated images so that real faces are
  never exposed during authentication.
---

# Generative Adversarial Networks Applied for Privacy Preservation in Biometric-Based Authentication and Identification

## Quick Facts
- arXiv ID: 2509.20024
- Source URL: https://arxiv.org/abs/2509.20024
- Reference count: 35
- One-line primary result: GAN-based face-to-flower translation preserves 99.76% authentication accuracy while preventing identity reconstruction.

## Executive Summary
This paper proposes using GANs to translate face images into visually private domains (e.g., flowers, shoes) for privacy-preserving biometric authentication. The key idea is to train a classifier on translated images so that real faces are never exposed during authentication. Experiments show that classifiers trained on flower-translated images achieve 99.76% accuracy and 86.11% F1 score, only slightly lower than those trained on real faces. TraVeLGAN outperforms other GAN frameworks, generating consistent and diverse translations. Inverse attacks were largely unsuccessful, especially when using asymmetric domains. The method balances privacy and utility effectively, requiring no server-side retraining and allowing flexible dataset or model choices.

## Method Summary
The system trains a TraVeLGAN model to translate face images into a visually private domain (flowers) without cycle-consistency constraints. Face images are first augmented with StarGAN to normalize pose and appearance variations. The translated images are then used to train a MobileNetV2 classifier for authentication. The approach avoids exposing raw face data while maintaining high classification accuracy through semantic preservation via a Siamese network. Inverse attacks were tested to evaluate privacy preservation, with most failing to recover identifiable facial features.

## Key Results
- TraVeLGAN with flower domain achieves 99.76% accuracy and 86.11% F1 score on authentication
- CycleGAN suffers from mode collapse, while TraVeLGAN generates consistent and diverse translations
- Inverse transformation attacks fail to reconstruct identifiable faces, though coarse features like sex/hair color may be inferred
- Asymmetric domains (e.g., shoes) provide stronger privacy protection than symmetric ones

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Preserving authentication utility during domain translation appears dependent on maintaining semantic distance metrics rather than pixel-level reconstruction.
- **Mechanism:** The architecture utilizes a Siamese network within the TraVeLGAN framework to capture high-level semantics. Instead of forcing the generator to recreate the exact input (cycle-consistency), it minimizes the difference between the "travel" of the vector in the latent space, ensuring the "flower" generated for User A is distinct from User B mathematically, even if visually similar.
- **Core assumption:** The assumption is that the latent space features required to distinguish faces can be successfully mapped to the feature space of the target domain (e.g., flowers) without loss of inter-class variance.
- **Evidence anchors:**
  - [section 2.1] Notes that TraVeLGAN "utilizes a third siamese network... to capture high-level semantics."
  - [section 5.2] States TraVeLGAN identified key features and translated them correctly, unlike CycleGAN which suffered mode collapse.
  - [corpus] Weak direct validation; neighboring papers focus on general GAN utility rather than this specific Siamese preservation logic.
- **Break condition:** If the Siamese loss fails to penalize semantic drift, different identities will map to similar flowers, collapsing the classifier's accuracy.

### Mechanism 2
- **Claim:** Privacy is provisionally achieved by eliminating cycle-consistency constraints, thereby preventing the learning of an invertible mapping function.
- **Mechanism:** By avoiding the "cycle-consistency" loss used in frameworks like CycleGAN, the generator learns a one-way transformation. The paper posits that removing the requirement to reconstruct the original face from the flower prevents the model from encoding sufficient facial data in the synthetic image for an attacker to reverse it.
- **Core assumption:** The assumption is that without an explicit reconstruction loss, the generator will discard sensitive biometric details (like facial geometry) that are unnecessary for the classification task.
- **Evidence anchors:**
  - [section 2.1] Argues that cycle-consistency "unnecessarily prefer[s] an easily invertible mapping function."
  - [section 5.4] Shows that reconstructed faces via inverse networks lacked identity, though high-level features like sex were sometimes retained.
  - [corpus] General support implies GANs can obfuscate data, but specific inversion resistance for TraVeLGAN is isolated to this text.
- **Break condition:** If an attacker trains a dedicated "Inverse Transformation Network" (ITN) with access to the specific seed or model weights, they may recover coarse features.

### Mechanism 3
- **Claim:** Authentication accuracy is likely stabilized by standardizing the input variance before translation.
- **Mechanism:** The authors use StarGAN to augment the input face dataset (normalizing poses/backgrounds) before feeding it to the TraVeLGAN. This reduces the chance that a change in the user's environment (e.g., turning their head) results in a completely different "flower" representation, ensuring the classifier receives a consistent signal.
- **Core assumption:** The assumption is that the translation network is sensitive to input variances (poses/hair) that may not be relevant to identity but affect the target domain's texture/color output.
- **Evidence anchors:**
  - [section 5.3] Explicitly states, "face images of a single individual... often do not resemble each other. Because of that, we had to augment the images with StarGAN."
  - [section 5.3] Notes that hairstyles affected translation significantly, leading to different flower shapes.
  - [corpus] Not mentioned in corpus neighbors.
- **Break condition:** If the input augmentation fails to normalize extreme variances, the system will generate inconsistent identities for the same user (high False Rejection Rate).

## Foundational Learning

### Concept: Generative Adversarial Networks (GANs)
- **Why needed here:** Understanding the adversarial game between the generator (creating the fake private image) and the discriminator (enforcing realism) is the baseline for this system.
- **Quick check question:** Can you explain why mode collapse (where the generator produces the same output for every input) would destroy the utility of an authentication system?

### Concept: Cycle-Consistency vs. Identity Loss
- **Why needed here:** The paper explicitly rejects cycle-consistency to favor privacy. You must understand what cycle-consistency is to understand why removing it protects the user.
- **Quick check question:** Why does forcing a network to reconstruct the original image (Cycle-Consistency) create a security risk in a privacy-preserving system?

### Concept: Transfer Learning (MobileNetV2)
- **Why needed here:** The downstream authentication relies on a pre-trained classifier adapted to the new "flower" domain.
- **Quick check question:** What is the trade-off between freezing the pre-trained layers of the classifier versus fine-tuning them on the synthetic data, as described in the results?

## Architecture Onboarding

### Component map:
CelebA Faces -> StarGAN (Augmentation/Pose Normalization) -> TraVeLGAN (Generator + Siamese Network) -> MobileNetV2 (Binary classifier)

### Critical path:
1. Train TraVeLGAN on unaligned Face and Flower datasets.
2. Validate that the Siamese loss preserves identity distance (check visually if same ID -> same Flower style).
3. Generate synthetic dataset (Augmented Faces -> Flowers).
4. Train MobileNetV2 on the synthetic dataset.

### Design tradeoffs:
- **Cycle-Consistency:** Including it stabilizes training and improves image quality but creates an invertible map (Privacy ↓, Utility ↑). The paper excludes it.
- **Classifier Freezing:** Freezing MobileNetV2 layers is faster and more private, but fine-tuning (Trainable MNv2) recovers significant accuracy (0.86 Recall vs 0.71 Recall).

### Failure signatures:
- **Mode Collapse:** All users map to the same flower texture (Discriminator loss -> 0).
- **Information Leakage:** Inverse transformation recovers facial geometry (check if reconstructed faces have eyes/noses).
- **Inconsistent Mapping:** Same user maps to different flower species depending on the day (indicates need for better input augmentation).

### First 3 experiments:
1. **Baseline Reproduction:** Train TraVeLGAN on Faces -> Flowers and measure the drop in classification accuracy compared to raw faces (Target: < 6% drop as per paper).
2. **Inversion Attack:** Attempt to train an inverse generator (Flower -> Face) to verify if identity can be recovered (Expectation: Only coarse features like sex/hair color should be visible).
3. **Domain Sensitivity:** Test the system on a highly asymmetric domain (e.g., Shoes vs. Flowers) to measure if the "semantic gap" affects the Siamese network's ability to preserve identity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the U-GAT-IT framework offer a superior balance of privacy and utility compared to TraVeLGAN for this application?
- Basis in paper: [explicit] The authors note U-GAT-IT is a "good candidate" but left experiments "for future work" due to high computational costs and memory requirements.
- Why unresolved: The architecture was excluded from the primary experimental results despite its potential for higher quality image-to-image translation.
- What evidence would resolve it: A comparative analysis of U-GAT-IT's classification accuracy and visual privacy protection against the established TraVeLGAN baseline.

### Open Question 2
- Question: How can the system efficiently update user templates to handle changes in physical appearance (visage) over time?
- Basis in paper: [explicit] The authors propose "implementing an update procedure allowing users to submit new synthetic images after changing the visage."
- Why unresolved: The current system is static; the proposed caching and threshold-based update mechanism is conceptualized but not implemented or tested.
- What evidence would resolve it: A functional protocol for automated re-enrollment that maintains authentication accuracy while handling temporal biometric variance.

### Open Question 3
- Question: Can the translation method be modified to prevent the leakage of soft biometric attributes (e.g., sex or hair style) during inverse transformation attacks?
- Basis in paper: [inferred] While robust against full identity reconstruction, the authors admit that "From the samples, it is possible to determine sex or hair style."
- Why unresolved: The current privacy preservation is incomplete as secondary personal traits remain recoverable by attackers using inverse transformation networks.
- What evidence would resolve it: An attack evaluation showing that reconstructed images reveal no statistically significant correlation with the input subject's soft biometric attributes.

## Limitations

- Privacy claims rely on avoiding cycle-consistency, but this creates tension as the system cannot verify perfect identity preservation
- Experiments use only 93 identities from CelebA and Oxford flowers, limiting generalization to larger, more diverse populations
- Inverse attacks show coarse features like sex and hair color can still be recovered, indicating partial rather than complete privacy protection

## Confidence

**High Confidence:** The core mechanism of using TraVeLGAN for cross-domain translation and achieving high classification accuracy on synthetic domains is well-supported by the experimental results and visual examples provided.

**Medium Confidence:** The privacy claims based on avoiding cycle-consistency are plausible but lack rigorous mathematical proof. The inverse attack results show some privacy protection, but the extent is unclear.

**Low Confidence:** The scalability claims and generalization to larger, more diverse datasets are speculative. The paper does not provide evidence that the method works beyond the specific 93-identity subset used in experiments.

## Next Checks

1. **Quantitative Semantic Consistency Test:** Implement a formal metric to measure whether the same identity consistently maps to similar flowers across different augmentations and runs. Calculate the intra-class variance in the flower domain and compare it to the inter-class variance to verify the Siamese network is preserving identity distinctions.

2. **Adversarial Inversion Attack:** Design a more sophisticated inverse attack using a dedicated neural network trained specifically to recover facial features from flowers, with access to the TraVeLGAN weights. Measure what specific facial attributes (beyond coarse features) can be recovered and quantify the information leakage.

3. **Cross-Dataset Generalization Study:** Test the system on a completely different face dataset (e.g., LFW or VGGFace) without retraining the TraVeLGAN. Measure the drop in accuracy and analyze whether the flower translation learned on CelebA transfers to new identities, validating the method's practical applicability.