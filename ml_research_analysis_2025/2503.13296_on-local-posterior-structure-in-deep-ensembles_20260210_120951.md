---
ver: rpa2
title: On Local Posterior Structure in Deep Ensembles
arxiv_id: '2503.13296'
source_url: https://arxiv.org/abs/2503.13296
tags:
- llla
- elpd
- swag
- de-bnns
- la-nf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether enriching deep ensembles with local
  posterior structure via Bayesian methods (DE-BNNs) improves performance. Surprisingly,
  the authors find that as ensemble size increases, standard deep ensembles (DEs)
  consistently outperform DE-BNNs on in-distribution data, despite DE-BNNs showing
  better out-of-distribution detection.
---

# On Local Posterior Structure in Deep Ensembles

## Quick Facts
- **arXiv ID:** 2503.13296
- **Source URL:** https://arxiv.org/abs/2503.13296
- **Reference count:** 40
- **Primary result:** Deep Ensembles consistently outperform DE-BNNs on in-distribution data for large ensemble sizes (K≥10), despite DE-BNNs showing better OOD detection.

## Executive Summary
This paper investigates whether enriching deep ensembles with local posterior structure via Bayesian methods (DE-BNNs) improves performance. Surprisingly, the authors find that as ensemble size increases, standard deep ensembles (DEs) consistently outperform DE-BNNs on in-distribution data, despite DE-BNNs showing better out-of-distribution detection. The study systematically evaluates SWAG, last-layer Laplace approximation (LLLA), and LLLA refined with normalizing flows across multiple datasets, architectures, and ensemble sizes. Results show DEs rank highest in in-distribution metrics for larger ensembles (K=10,20), while DE-BNNs only marginally outperform DEs on OOD detection at the cost of reduced in-distribution performance. The authors provide sensitivity analyses, open-source trained models, and recommend DEs for practitioners when ensemble size is not memory-constrained.

## Method Summary
The study compares standard Deep Ensembles (DEs) against Deep Ensembles of Bayesian Neural Networks (DE-BNNs) using three post-hoc Bayesian approximation methods: SWAG, last-layer Laplace approximation (LLLA), and LLLA refined with normalizing flows (LA-NF). The authors train 30 independent MAP models per dataset-architecture combination using SGD with cosine annealing. For DE-BNNs, they apply the Bayesian approximation methods to each MAP estimate to construct approximate posteriors, then form ensembles by sampling from these posteriors. They evaluate performance across ensemble sizes K∈{2,5,10,20} on CIFAR-10/100, SST-2, and QM9 datasets using ELPD as the primary in-distribution metric and AUROC for out-of-distribution detection.

## Key Results
- Deep Ensembles (DEs) consistently outperform DE-BNNs on in-distribution ELPD for larger ensemble sizes (K≥10)
- DE-BNNs show marginally better OOD detection (AUROC) than DEs, but this comes at the cost of reduced in-distribution performance
- SWAG and LLLA's Gaussian assumptions and LLLA's last-layer-only approach are identified as key limitations
- Covariance scaling ablation shows that shrinking the local structure often improves ELPD, suggesting the full local approximation is detrimental

## Why This Works (Mechanism)

### Mechanism 1
Standard Deep Ensembles capture multiple distinct modes of the posterior, which is more critical for in-distribution performance than modeling local structure around fewer modes. Neural network posteriors are highly multimodal, and DEs stochastically discover and average over these diverse modes. Bayesian approximation methods typically model a Gaussian distribution centered on a single mode, which may not span enough modes for optimal performance.

### Mechanism 2
The performance of DE-BNNs is limited by the imperfections of local posterior approximation, particularly the Gaussian assumption. SWAG and LLLA enforce Gaussianity, while LLLA ignores uncertainty in all but the last layer. These imperfect local structures introduce predictive variance that harms in-distribution ELPD more than it helps, unlike the "purer" mode averaging in standard DEs.

### Mechanism 3
There is a fundamental tradeoff where DE-BNNs achieve better out-of-distribution detection at the direct cost of degraded in-distribution performance. The local uncertainty modeled by BNN methods makes predictions more sensitive to inputs that deviate from the training data manifold, which is useful for OOD detection. However, this same sensitivity or added variance from the approximate posterior leads to less confident or accurate predictions on standard ID data.

## Foundational Learning

- **Bayesian Model Averaging vs. Deep Ensembles**: Understanding the difference between averaging over discrete modes (DEs) versus integrating over continuous distributions (BNNs) is crucial for interpreting the paper's central comparison.
  - Quick check: Explain the difference between the predictive distribution of a Deep Ensemble (DE) and a Deep Ensemble of Bayesian Neural Networks (DE-BNN) in terms of how they sample from the weight space.

- **The Laplace Approximation**: LLLA approximates the posterior as a Gaussian centered at the MAP estimate using the inverse Hessian as the covariance. Understanding this method's local, Gaussian, and often subnetwork-only nature is crucial for interpreting its results and limitations.
  - Quick check: What is the key simplifying assumption the Laplace Approximation makes about the shape of the posterior distribution?

- **Expected Log Predictive Density (ELPD)**: ELPD measures the log-probability assigned to the correct answer, rewarding both accuracy and well-calibrated confidence. A higher (less negative) ELPD is better.
  - Quick check: If Model A has higher accuracy than Model B but a lower (more negative) ELPD, what does that imply about Model A's confidence or calibration?

## Architecture Onboarding

- **Component map**: Base Model -> Post-Hoc BNN Wrapper (SWAG/LLLA/LA-NF) -> Ensemble Engine -> Evaluator

- **Critical path**:
  1. Train 30 base models from scratch with different random seeds to get MAP estimates
  2. Apply chosen post-hoc method to each MAP estimate to construct approximate posteriors
  3. Randomly sample K models from the pool to form a single ensemble instance, repeat for statistical analysis
  4. Make predictions on test data by sampling (for BNNs) or averaging (for DEs), compute final performance metrics

- **Design tradeoffs**:
  - DE vs. DE-BNN (K=large): Choose DE for superior in-distribution performance and simplicity; choose DE-BNN only if OOD detection is the absolute priority
  - BNN Method Choice: LLLA is most memory-efficient (last-layer only), SWAG captures more covariance but is expensive, LA-NF offers more flexibility but adds complexity
  - Ensemble Size (K): Small K favors DE-BNNs, while large K favors DEs; choice of K should determine whether to invest in BNN complexity

- **Failure signatures**:
  - Low ELPD with High Variance in DE-BNN may indicate posterior covariance is too large
  - Memory overflow with SWAG occurs when low-rank covariance matrix scales with O(KPR)
  - Minimal OOD improvement with large ELPD drop suggests local structure isn't providing useful uncertainty signal

- **First 3 experiments**:
  1. Baseline Comparison (CIFAR-10): Train 10 ResNets, compare ELPD and AUROC of DE (K=10) vs. DE-BNN using LLLA (K=10)
  2. Covariance Scaling Ablation: Take a single BNN model and multiply its posterior covariance by scaling factor lambda (0.01 to 1.0), plot ELPD vs. lambda
  3. OOD vs. ID Pareto Frontier: For fixed K=5, train DE-BNNs with LLLA, SWAG, and LA-NF, plot (ELPD, AUROC) pairs with DE baseline

## Open Questions the Paper Calls Out

- Can optimal combination strategies, such as non-uniform mixing weights or stacking, recover the in-distribution performance of DE-BNNs? (Section 6.2)
- Does joint tuning of posterior approximations across ensemble members improve performance compared to independent tuning? (Section 7)
- Can new multi-modal posterior approximations overcome limitations of Gaussian or last-layer assumptions to improve DE-BNN effectiveness? (Section 7)

## Limitations
- Focuses only on post-hoc methods applied to MAP models, missing potential of training BNNs from scratch with sophisticated priors
- Assumes Gaussian local structure is the bottleneck without testing non-Gaussian alternatives
- OOD datasets may not fully represent real-world distribution shift scenarios

## Confidence
- **High confidence**: In-distribution results showing DEs outperforming DE-BNNs for K≥10
- **Medium confidence**: Out-of-distribution findings showing marginal AUROC improvements for DE-BNNs
- **Medium-Low confidence**: Mechanism explanations relying on assumptions about Gaussian posterior limitations

## Next Checks
1. Replicate the WRN-16-4 CIFAR-10 experiment comparing DE vs. DE-BNN (LLLA) for K=10, verifying the FRN implementation
2. Run the covariance scaling ablation on a single BNN to confirm that shrinking the local structure improves ELPD
3. Test a full-network normalizing flow to see if more flexible local approximations can outperform DEs for large K