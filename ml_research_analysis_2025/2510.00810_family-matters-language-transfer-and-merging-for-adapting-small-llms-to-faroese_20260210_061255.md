---
ver: rpa2
title: 'Family Matters: Language Transfer and Merging for Adapting Small LLMs to Faroese'
arxiv_id: '2510.00810'
source_url: https://arxiv.org/abs/2510.00810
tags:
- language
- faroese
- languages
- lora
- full
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting small language models
  to Faroese, a low-resource North Germanic language, by leveraging transfer from
  related Scandinavian languages. The authors investigate the effects of continued
  pre-training on individual or merged Scandinavian languages before fine-tuning on
  Faroese, comparing full fine-tuning and LoRA (parameter-efficient fine-tuning).
---

# Family Matters: Language Transfer and Merging for Adapting Small LLMs to Faroese

## Quick Facts
- arXiv ID: 2510.00810
- Source URL: https://arxiv.org/abs/2510.00810
- Reference count: 22
- Primary result: Transfer from related languages is crucial for Faroese adaptation, with Icelandic enhancing linguistic accuracy and Danish boosting comprehension; full fine-tuning outperforms LoRA on comprehension tasks while LoRA excels at linguistic acceptability.

## Executive Summary
This paper addresses the challenge of adapting small language models to Faroese, a low-resource North Germanic language, by leveraging transfer from related Scandinavian languages. The authors investigate the effects of continued pre-training on individual or merged Scandinavian languages before fine-tuning on Faroese, comparing full fine-tuning and LoRA (parameter-efficient fine-tuning). They introduce two new minimal-pair evaluation benchmarks, FoBLiMP for linguistic acceptability and FoBCoMP for text comprehension, supplemented by human evaluation by Faroese linguists. The primary finding is that transfer from related languages is crucial, with Icelandic providing the best linguistic accuracy and mainland Scandinavian languages enhancing text comprehension. Full fine-tuning generally outperformed LoRA on comprehension tasks and downstream summarization, while LoRA showed advantages in linguistic acceptability. Model merging showed promise, especially when combining Icelandic and Danish, but benefits were inconsistent.

## Method Summary
The authors adapt SmolLM2 models (135M/360M parameters) to Faroese through sequential continued pre-training on Scandinavian languages followed by fine-tuning on Faroese. They use Fineweb-2 corpora with capped token counts (4B for Danish, Swedish, Norwegian-Bokmål; 1.6B for Icelandic; 495M for Norwegian-Nynorsk) and 95M tokens for Faroese. The methodology compares full fine-tuning (5 epochs, lr=5e-4) versus LoRA (rank=256, lr=8e-4) across transfer languages. They also explore TIES model merging of language-specific checkpoints using Mergekit, with configurations ranging from equal weighting to weighted combinations emphasizing Icelandic and Danish. Evaluation uses perplexity on Faroese validation data, two minimal-pair benchmarks (FoBLiMP for linguistic acceptability and FoBCoMP for comprehension), and human evaluation on fluency and task completion.

## Key Results
- Initializing Faroese models with Scandinavian transfer languages improves performance across all benchmarks compared to English-only models.
- Icelandic transfer yields the best linguistic accuracy while Danish enhances text comprehension.
- LoRA outperforms full fine-tuning on linguistic acceptability tasks, while full fine-tuning excels at comprehension and preserves capabilities during downstream fine-tuning.
- Model merging shows promise, particularly for full fine-tuning combining Icelandic and Danish, but benefits are inconsistent under LoRA.

## Why This Works (Mechanism)

### Mechanism 1: Transfer Language Initialization
Initializing from typologically related languages improves Faroese adaptation over English-only baselines across all evaluated tasks. Related languages share subword tokenization patterns, morphological structures, and syntactic features, providing better-initialized representations before target-language fine-tuning. Icelandic yields lowest zero-shot perplexity due to script overlap; Danish contributes comprehension gains likely through larger training corpora. Core assumption: Typological similarity predicts transfer success; shared surface features (orthography, morphology) matter more than corpus size for linguistic accuracy.

### Mechanism 2: Task-Arithmetic Model Merging with TIES
Merging models trained on different source languages can combine complementary strengths (Icelandic for linguistics, Danish for comprehension), but benefits are inconsistent under LoRA. TIES merging computes language vectors (fine-tuned minus base weights), retains top-magnitude parameters, resolves sign conflicts, and averages disjoint contributions. This allows controlled influence from multiple transfer languages without joint multilingual training. Core assumption: Language-specific adaptations occupy partially disjoint parameter regions; merging preserves complementary capabilities without catastrophic interference.

### Mechanism 3: Fine-Tuning Method Task-Specialization
LoRA outperforms full fine-tuning on linguistic acceptability (FoBLiMP), while full fine-tuning excels at comprehension (FoBCoMP) and preserves capabilities during downstream task adaptation. LoRA's low-rank constraint acts as implicit regularization, preventing overfitting to limited Faroese data on surface linguistic patterns. Full fine-tuning provides sufficient capacity for deeper semantic representations and knowledge retention, critical for comprehension and downstream tasks. Core assumption: Linguistic acceptability relies on surface patterns learnable in low-dimensional subspaces; comprehension requires distributed knowledge across full parameter space.

## Foundational Learning

- **Concept: Minimal-pair evaluation methodology**
  - Why needed here: Standard benchmarks don't exist for Faroese; perplexity is unreliable due to tokenizer issues. Minimal pairs (correct vs. corrupted sentences) test implicit grammatical/semantic preferences without requiring task-specific datasets.
  - Quick check question: Can you explain why comparing model-assigned probabilities to "The key is on the table" vs. "The key are on the table" tests syntactic knowledge without supervised labels?

- **Concept: Catastrophic forgetting in continued pre-training**
  - Why needed here: Adapting to Faroese risks losing general reasoning and knowledge. LoRA is hypothesized to preserve prior skills better than full fine-tuning, though results show task-dependent trade-offs.
  - Quick check question: Why might freezing most parameters (LoRA) hurt comprehension tasks that require broad world knowledge?

- **Concept: TIES merging (Trim, Elect Sign, Disjoint Mean)**
  - Why needed here: The paper uses TIES to combine language-specific models. Understanding sign conflict resolution and parameter selection is essential for debugging merge failures.
  - Quick check question: If two language models modify the same weight in opposite directions (+0.5 vs. -0.3), how does TIES resolve this versus simple averaging?

## Architecture Onboarding

- **Component map:** SmolLM2 (135M/360M) -> Scandinavian continued pre-training (1 epoch) -> TIES merging (optional) -> Faroese fine-tuning (5 epochs) -> FoBLiMP/FoBCoMP evaluation

- **Critical path:**
  1. Select transfer language(s) based on task priority (Icelandic→linguistics, Danish→comprehension)
  2. Run continued pre-training (1 epoch per source language)
  3. If merging: apply TIES with density=0.5, pre-normalization weights as specified
  4. Fine-tune on Faroese (5 epochs, LR=5e-4 full / 8e-4 LoRA)
  5. Evaluate on FoBLiMP/FoBCoMP minimal pairs

- **Design tradeoffs:**
  - Full fine-tuning: Better perplexity, comprehension, downstream stability; higher compute, potential forgetting
  - LoRA: Better linguistic acceptability for small models; parameter-efficient; weaker merging compatibility
  - Single transfer language vs. merge: Single is more predictable; merge can combine strengths but inconsistent under LoRA

- **Failure signatures:**
  - Exploded perplexity after merging (especially LoRA): Reduce merge density or number of merged models
  - Low semantic coherence with high fluency: Small model limitation—insufficient world knowledge regardless of adaptation method
  - Language mixing in outputs after downstream fine-tuning: Low-resource task data corrupted language boundaries; full fine-tuning more robust

- **First 3 experiments:**
  1. **Baseline replication:** Train English→Icelandic→Faroese (full and LoRA) on SmolLM2-360M, evaluate on FoBLiMP subset to validate transfer gains.
  2. **Ablate transfer language:** Compare Icelandic-only, Danish-only, and no-transfer (English→Faroese direct) on both FoBLiMP and FoBCoMP to confirm task-dependent patterns.
  3. **Merge sanity check:** Merge Icelandic+Danish via TIES (density=0.5), evaluate whether merge outperforms either single-language transfer on comprehension tasks; if LoRA merge fails, retry with full fine-tuning variant.

## Open Questions the Paper Calls Out

- Can adaptive or data-driven merging strategies (e.g., weighting transfer languages based on typological similarity, data size, or task-specific performance) yield more consistent improvements over single-language transfer than the fixed-weight merging explored here?
- Do the observed patterns—Icelandic favoring linguistic accuracy while Danish favors comprehension, and LoRA's relative strength on acceptability tasks—persist, weaken, or reverse at larger model scales (e.g., 1B+ parameters)?
- Why does LoRA outperform full fine-tuning on linguistic acceptability probes despite underperforming on comprehension, contrary to the hypothesis that full fine-tuning's greater capacity would benefit linguistic acquisition?
- To what extent do these findings generalize to other low-resource languages with typologically mixed profiles (sharing features with multiple higher-resource relatives), and does the optimal transfer strategy depend on the specific typological dimensions shared?

## Limitations

- Model merging under LoRA showed inconsistent benefits with several configurations producing dramatically higher perplexity than single-language transfers.
- The newly introduced minimal-pair benchmarks (FoBLiMP and FoBCoMP) lack external validation, raising concerns about benchmark reliability.
- Findings are limited to small models (135M/360M), making generalizability to larger architectures uncertain.
- The observed Icelandic linguistic advantage vs Danish comprehension benefit could be artifacts of corpus size differences (1.6B vs 4B tokens) rather than inherent language properties.

## Confidence

- **High Confidence:** Transfer language initialization improves Faroese adaptation over English baselines across all tasks.
- **Medium Confidence:** LoRA outperforms full fine-tuning on linguistic acceptability while full fine-tuning excels at comprehension and downstream task preservation.
- **Low Confidence:** Model merging consistently combines complementary strengths from different languages, particularly Icelandic and Danish.

## Next Checks

1. **TIES Merging Implementation Validation:** Reproduce the Icelandic+Danish merge under full fine-tuning with controlled density parameters (0.3, 0.5, 0.7) and document the exact sign conflict resolution process. Compare merged performance against weighted averaging of individual model outputs to isolate whether TIES provides benefits beyond simple ensemble methods.

2. **Corpus Size Control Experiment:** Create balanced corpora for Icelandic and Danish (e.g., 2B tokens each) and retrain transfer models to determine whether the observed task-specific advantages stem from inherent language properties or training data quantity. This would clarify whether the Icelandic linguistic advantage is genuine or an artifact of the 1.6B vs 4B token difference.

3. **Downstream Task Generalization Test:** Extend the summarization evaluation to include additional downstream tasks (e.g., translation, question answering) and measure language mixing rates across more diverse evaluation sets. Track parameter evolution through each fine-tuning stage to identify where language boundaries degrade and test whether constrained decoding or language-specific tokens can mitigate mixing without harming performance.