---
ver: rpa2
title: 'Revisiting Generalization Across Difficulty Levels: It''s Not So Easy'
arxiv_id: '2511.21692'
source_url: https://arxiv.org/abs/2511.21692
tags:
- difficulty
- generalization
- train
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how well large language models (LLMs) generalize
  across different task difficulties using a large-scale, model-based difficulty estimation
  approach. The authors use Item Response Theory (IRT) with responses from thousands
  of models to assign difficulty scores to examples in six datasets, finding that
  human-based difficulty metrics often poorly align with model performance.
---

# Revisiting Generalization Across Difficulty Levels: It's Not So Easy

## Quick Facts
- arXiv ID: 2511.21692
- Source URL: https://arxiv.org/abs/2511.21692
- Reference count: 40
- Primary result: Training on easy or hard data alone does not consistently improve performance across all difficulty levels; generalization is strongest for similar train-test difficulty pairs.

## Executive Summary
This paper investigates how well large language models (LLMs) generalize across different task difficulties using a large-scale, model-based difficulty estimation approach. The authors use Item Response Theory (IRT) with responses from thousands of models to assign difficulty scores to examples in six datasets, finding that human-based difficulty metrics often poorly align with model performance. They then train models on individual difficulty bins and evaluate across all bins, showing that cross-difficulty generalization is limited: training on easy or hard data alone does not consistently improve performance across the full range of difficulties. Generalization is strongest for similar train-test difficulty pairs and declines as the gap increases, even falling below zero-shot baselines. These findings challenge the assumption that training on either easy or hard data alone is sufficient for broad generalization, highlighting the need for difficulty-aware data curation and evaluation in LLM development.

## Method Summary
The study estimates example difficulty using Item Response Theory (IRT) applied to thousands of model responses from Open LLM Leaderboard across six datasets. Difficulty scores are obtained via a Rasch (1PL) model, then binned into ten equal-sized groups. For each bin, a base model is fine-tuned using supervised fine-tuning (SFT) for five epochs, and evaluated on all ten bins to measure cross-difficulty generalization. The key metric is improvement over zero-shot baseline, revealing that models generalize best when train-test difficulty levels are similar.

## Key Results
- Cross-difficulty generalization is strongest when training and testing difficulties are similar, with performance degrading as the gap increases.
- Training on easy or hard data alone does not consistently improve performance across all difficulty levels.
- Generalization can fall below zero-shot baselines when train-test difficulty gaps are large, indicating negative transfer.

## Why This Works (Mechanism)

### Mechanism 1: Difficulty-Specific Feature Learning
- **Claim**: Fine-tuning on a narrow difficulty band causes models to learn features that are predictive primarily for that difficulty range, with limited transfer to other ranges.
- **Mechanism**: The paper shows that when training on a single difficulty bin, models exhibit the strongest performance on test bins with similar difficulty (diagonal clustering in heatmaps), with performance degrading as the train-test gap increases. This suggests features learned at one difficulty level may not be optimal for others—e.g., "easy" examples may rely on surface patterns, while "hard" examples require deeper reasoning chains.
- **Core assumption**: Difficulty levels capture meaningful differences in the features or reasoning patterns required.
- **Evidence anchors**:
  - [Section 5]: "generalization is highest where training and testing difficulties are similar"
  - [Figure 4]: Heatmaps show strongest improvements clustered around the diagonal, with negative values when train-test difficulty gaps are large (e.g., training on bin 0, testing on bin 9 in MMLU-Pro shows -0.07 improvement)
  - [Corpus]: "Frontier LLMs Still Struggle with Simple Reasoning Tasks" supports that difficulty misalignment leads to performance degradation
- **Break condition**: If difficulty bins were not semantically meaningful (just random splits), we would not observe the diagonal pattern or systematic degradation with increasing gaps.

### Mechanism 2: Model-Centric Difficulty Estimation via IRT
- **Claim**: Using Item Response Theory with responses from thousands of LLMs provides difficulty estimates that better predict model behavior than human-defined metrics.
- **Mechanism**: IRT jointly estimates question difficulty (β) and model ability (θ) from observed response patterns across many models. This captures what is actually hard for models, not humans. The paper shows human metrics (grade level, reasoning steps, answer length) correlate weakly with IRT scores (Spearman correlations mostly <0.3), and IRT scores validate well on held-out Qwen3 models (accuracy decreases monotonically with IRT bin difficulty).
- **Core assumption**: Patterns across many models generalize to new models.
- **Evidence anchors**:
  - [Section 3.1]: "P(r_ij|θ_j, β_i) = 1 / (1 + e^(-(θ_j - β_i)))"
  - [Section 3.4]: "most datasets show very little to no positive correlation between IRT difficulty scores and human-based metrics"
  - [Corpus]: Weak corpus evidence directly comparing IRT to human metrics for LLM difficulty; the paper's contribution is precisely filling this gap
- **Break condition**: If the pool of models in Open LLM Leaderboard were not diverse enough, IRT estimates might not generalize to architectures outside that distribution.

### Mechanism 3: Cross-Difficulty Transfer Decay
- **Claim**: Cross-difficulty generalization decays predictably with train-test difficulty gap, sometimes yielding negative transfer below zero-shot baselines.
- **Mechanism**: Training on a single difficulty bin provides localized optimization. When the test distribution differs significantly, fine-tuned weights may be actively misaligned with what the test requires. In BBH, models trained on hard bins (8-9) show negative improvements on easy bins (0-2), suggesting overfitting to hard-example features harms easy-example performance.
- **Core assumption**: The zero-shot baseline represents a reasonable anchor for "unbiased" performance.
- **Evidence anchors**:
  - [Abstract]: "Generalization is strongest for similar train-test difficulty pairs and declines as the gap increases, even falling below zero-shot baselines"
  - [Section 5]: In BBH, training on bin 9 yields +0.06 on bin 9 but -0.22 on bin 0
  - [Corpus]: "What Makes a Good Curriculum?" finds curriculum effects are sensitive to difficulty metric choice, supporting the decay pattern
- **Break condition**: If training used a difficulty-aware curriculum (e.g., gradually increasing difficulty), rather than single-bin training, this decay pattern might be mitigated.

## Foundational Learning

- **Item Response Theory (IRT)**:
  - **Why needed here**: IRT is the core methodology for estimating difficulty from model response patterns, replacing human heuristics.
  - **Quick check question**: Given a matrix of binary responses (correct/incorrect) from 100 models on 50 questions, how would you explain why IRT jointly estimates both question difficulty and model ability rather than just computing per-question accuracy?

- **Supervised Fine-Tuning (SFT) with Distribution Shift**:
  - **Why needed here**: The experimental protocol relies on fine-tuning on narrow distributions (single difficulty bins) and testing for out-of-distribution generalization.
  - **Quick check question**: If a model fine-tuned on easy examples performs worse than its zero-shot version on hard examples, is this catastrophic forgetting, negative transfer, or both? How would you distinguish?

- **Spearman Correlation for Ordinal Alignment**:
  - **Why needed here**: The paper uses Spearman correlation to validate that IRT scores have weak alignment with human-defined difficulty metrics.
  - **Quick check question**: Why use Spearman (rank) correlation instead of Pearson (linear) correlation when comparing IRT difficulty bins to grade levels?

## Architecture Onboarding

- **Component map**: Web-scrape Open LLM Leaderboard responses -> IRT Estimation (1PL Rasch model) -> Difficulty Binning (10 equal bins) -> Single-bin SFT training (5 epochs) -> lm-evaluation-harness evaluation on all bins -> Compute improvement over zero-shot baseline

- **Critical path**:
  1. IRT difficulty estimation quality → validity of bin assignments
  2. Training data quantity per bin (must be sufficient; small datasets like GPQA failed)
  3. Evaluation alignment (same prompt format, same parsing logic as leaderboard)

- **Design tradeoffs**:
  - **1PL vs 2PL/3PL IRT models**: Paper chose 1PL (Rasch) for simplicity and interpretability; 2PL/3PL introduced instability (e.g., unanswerable questions placed mid-difficulty).
  - **10 bins vs coarser splits**: 10 bins provide fine-grained analysis but reduce examples per bin; datasets with <550 examples (IFEval, GPQA) showed no meaningful learning.
  - **Single-bin training vs curriculum**: Single-bin isolates difficulty effects but doesn't test curriculum strategies that might improve cross-difficulty transfer.

- **Failure signatures**:
  - **No learning**: Small bins (<55 examples) yield flat or random improvement patterns (see GPQA, IFEval in Appendix E).
  - **Negative transfer**: Training on extreme bins (0 or 9) yields negative improvements on opposite-extreme test bins (see BBH bin 9 → bin 0: -0.22).
  - **Dataset-specific**: ARC shows near-zero cross-difficulty generalization across all bins; GSM8K shows moderate generalization for Qwen but not Llama.

- **First 3 experiments**:
  1. **Replicate IRT estimation on a single dataset** (e.g., MMLU-Pro subset): Scrape responses from 50 models, fit 1PL model, verify that accuracy decreases monotonically across bins on a held-out Qwen3 model.
  2. **Single-bin fine-tuning pilot**: Fine-tune Qwen2.5-7B on bin 0 (easiest 10%) of GSM8K; evaluate on all 10 bins; plot improvement vs test bin to check for diagonal clustering.
  3. **Ablation on bin granularity**: Compare 5-bin vs 10-bin splits for MMLU-Pro; check if the diagonal pattern strengthens or weakens with coarser granularity.

## Open Questions the Paper Calls Out
None

## Limitations
- The study's reliance on model-based difficulty estimation via IRT introduces uncertainty about whether the estimates generalize to models outside the Open LLM Leaderboard distribution.
- Single-bin training, while isolating difficulty effects, may not reflect practical fine-tuning strategies like difficulty-aware curricula that could mitigate cross-difficulty generalization decay.
- Datasets with fewer than 550 examples showed no meaningful learning, suggesting the methodology may not scale down effectively.

## Confidence

- **High Confidence**: The observation that cross-difficulty generalization is strongest for similar train-test difficulty pairs and declines with increasing gaps (supported by systematic heatmap patterns and quantitative improvements/degradations across all six datasets). The IRT methodology's validity is also well-supported by held-out Qwen3 validation and the monotonic accuracy decline across IRT bins.

- **Medium Confidence**: The claim that human-based difficulty metrics poorly align with IRT scores (Spearman correlations mostly <0.3). While the paper provides these correlations, the weak relationship could also reflect limitations in the human metrics themselves rather than superiority of IRT. The negative transfer below zero-shot baselines is observed but may be dataset-specific (strongest in BBH).

- **Low Confidence**: The broader implication that training on easy or hard data alone is insufficient for broad generalization. This conclusion is based on single-bin training, which is an extreme case—real-world curricula or mixed-difficulty training might achieve better transfer. The study also doesn't explore whether architectural changes or different training objectives could overcome the observed limitations.

## Next Checks

1. **Curriculum Training Ablation**: Instead of single-bin training, implement a difficulty-aware curriculum where training difficulty gradually increases (or follows an optimal ordering). Compare cross-difficulty generalization against the single-bin results to determine if the decay pattern can be mitigated.

2. **IRT Model Ablation**: Repeat the IRT estimation using 2PL or 3PL models (despite the paper's stability issues) on a subset of datasets to quantify how much the 1PL simplification affects difficulty bin assignments and subsequent generalization patterns.

3. **Dataset Size Sensitivity**: Systematically subsample larger datasets to create conditions where each bin contains only 50-100 examples (matching IFEval/GPQA levels). Evaluate whether the diagonal generalization pattern persists or breaks down, confirming whether sufficient per-bin data is a prerequisite for meaningful difficulty-based learning.