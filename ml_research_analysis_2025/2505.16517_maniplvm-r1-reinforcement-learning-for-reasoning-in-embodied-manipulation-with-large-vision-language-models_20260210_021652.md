---
ver: rpa2
title: 'ManipLVM-R1: Reinforcement Learning for Reasoning in Embodied Manipulation
  with Large Vision-Language Models'
arxiv_id: '2505.16517'
source_url: https://arxiv.org/abs/2505.16517
tags:
- arxiv
- reward
- reasoning
- trajectory
- manipulation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ManipLVM-R1 introduces a reinforcement learning framework that
  replaces costly human annotations with rule-based verifiable rewards for robotic
  manipulation. It decomposes tasks into affordance perception and trajectory prediction,
  using structured rewards like IoU-based spatial localization and multi-metric trajectory
  similarity to provide immediate, task-aligned feedback.
---

# ManipLVM-R1: Reinforcement Learning for Reasoning in Embodied Manipulation with Large Vision-Language Models

## Quick Facts
- arXiv ID: 2505.16517
- Source URL: https://arxiv.org/abs/2505.16517
- Reference count: 15
- Primary result: Achieves 31.0 IoU in affordance perception and 110.87 average trajectory error using 50% training data, outperforming larger supervised models

## Executive Summary
ManipLVM-R1 introduces a reinforcement learning framework that replaces costly human annotations with rule-based verifiable rewards for robotic manipulation. It decomposes tasks into affordance perception and trajectory prediction, using structured rewards like IoU-based spatial localization and multi-metric trajectory similarity to provide immediate, task-aligned feedback. The method achieves strong in-domain performance while demonstrating improved sample efficiency and generalization to out-of-domain scenarios compared to supervised fine-tuning approaches.

## Method Summary
The framework employs Reinforcement Learning with Verifiable Rewards (RLVR) to optimize manipulation tasks without human annotations. It decomposes manipulation into two subtasks: affordance perception (spatial localization of interaction points) and trajectory prediction (path planning). Each subtask has specialized reward functions - affordance uses IoU-based spatial rewards, while trajectory prediction combines DFD, HD, RMSE, and endpoint distance metrics. The model uses Qwen2.5-VL-3B as base and employs Group Relative Policy Optimization (GRPO) for stable policy updates with KL divergence constraints.

## Key Results
- Achieves 31.0 IoU in affordance perception on in-domain tasks
- Reaches 110.87 average trajectory error using only 50% of training data
- Outperforms all baselines on out-of-domain tests with 34.65 Grasp-IoU and 131.99 trajectory error

## Why This Works (Mechanism)

### Mechanism 1: Verifiable Reward Substitution for Human Annotations
Replacing supervised fine-tuning with RLVR enables data-efficient learning while improving out-of-domain generalization. Instead of imitating human demonstrations, the model optimizes directly for task-aligned, automatically-computable outcomes using gradient signals derived from reward comparisons rather than label matching. Core assumption: designed reward functions sufficiently capture the underlying physical reasoning required for successful manipulation.

### Mechanism 2: Task Decomposition into Affordance + Trajectory Subtasks
Structured decomposition enables focused optimization per subtask with specialized rewards. Affordance perception receives format validation + IoU-based spatial rewards; trajectory prediction receives format validation + multi-metric path similarity + endpoint accuracy rewards. Each subtask trains separately, reducing credit assignment complexity. Core assumption: affordance localization and trajectory generation can be learned independently and later compose effectively.

### Mechanism 3: Multi-Metric Trajectory Reward Aggregation
Combining complementary geometric metrics (DFD, HD, RMSE, endpoint) yields more robust trajectory learning than any single metric. DFD captures shape/temporal alignment; HD captures worst-case deviation; RMSE captures average error; endpoint distance ensures goal-reaching. Weighted aggregation provides dense, multi-faceted feedback. Core assumption: each metric contributes orthogonal information about trajectory quality.

## Foundational Learning

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed: Traditional RL suffers from sparse, delayed rewards; RLVR provides immediate, rule-based feedback by defining rewards as deterministic functions of outputs
  - Quick check: Given a manipulation task, can you write a deterministic function that returns a scalar reward from (instruction, image, predicted_output) without human judgment?

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed: Standard RL requires separate value networks; GRPO samples multiple responses, computes relative advantages via group normalization, and optimizes policy directly
  - Quick check: How would advantage computation change if you sampled G=1 vs. G=8 responses per instruction?

- **Concept: Geometric Trajectory Metrics (DFD, HD, RMSE)**
  - Why needed: Each metric captures different trajectory aspects—DFD for shape alignment, HD for worst-case spatial deviation, RMSE for average error
  - Quick check: If a predicted trajectory has perfect endpoint but meanders wildly, which metric would penalize it most?

## Architecture Onboarding

- **Component map**: Input (image + text instruction) → Vision-Language Encoder (Qwen2.5-VL-3B base) → Policy Model π_θ → Generates G responses → Reward Functions → Advantage Computation → Policy Update via GRPO + KL constraint

- **Critical path**: Visual encoder quality → response format compliance → reward signal quality → advantage normalization stability → policy update magnitude (controlled by KL β)

- **Design tradeoffs**: 
  - 50% training data reduces annotation cost but assumes reward signals are sufficiently informative
  - Separate affordance/trajectory models simplify debugging but may miss cross-task synergies
  - 2D image-space trajectories enable leveraging existing vision models but require additional 3D lifting for real robots
  - Multi-metric rewards are comprehensive but increase compute per training step

- **Failure signatures**:
  - IoU stuck near 0 despite decreasing format errors → reward signal not reaching spatial reasoning
  - Trajectory metrics diverging during training → normalize metric scales or reduce learning rate
  - Model generates valid format but coordinates always in image center → R_aff/R_path too weak
  - OOD performance collapses while ID performance remains high → overfitting to dataset biases

- **First 3 experiments**:
  1. Train on 10% data with full rewards; verify IoU > 5 and trajectory avg error < 200
  2. Remove R_path, keep only R_end; observe if trajectories become jerky
  3. Evaluate on UMD affordance categories not in training; if Grasp-IoU < 15, investigate base model's zero-shot capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the ManipLVM-R1 framework be extended to directly predict 3D robot movements rather than 2D image-space trajectories?
- Basis: The authors state in Appendix A that "the translation of predicted 2D image-space trajectories to precise and safe 3D robot movements in physical environments also presents challenges"
- Why unresolved: Current model outputs 2D coordinates, relying on future 3D reconstruction advancements
- What evidence would resolve it: Successful deployment on physical robotic arms using 3D reconstruction module

### Open Question 2
- Question: How does RLVR framework perform on long-horizon tasks requiring fine-grained force control or manipulation of deformable objects?
- Basis: Appendix A identifies limitation where framework lacks skills for "fine-grained force control, manipulation of deformable objects, and dynamic interaction"
- Why unresolved: Current rewards are geometric and don't capture physical dynamics or tactile feedback
- What evidence would resolve it: Experiments on cloth folding or assembly tasks requiring force feedback

### Open Question 3
- Question: Can integrating external commonsense knowledge bases mitigate reasoning hallucinations in uncommon scenarios?
- Basis: Section 4.5 notes failures stem from "lack of commonsense knowledge," suggesting "future work could explore incorporating external commonsense knowledge bases"
- Why unresolved: Model lacks mechanism for verifying internal beliefs when encountering unfamiliar objects
- What evidence would resolve it: Ablation studies showing reduced error rates in OOD scenarios with retrieval module

## Limitations

- Task decomposition assumption: Assumes affordance perception and trajectory prediction can be learned independently and compose effectively
- 2D trajectory constraint: Predicts trajectories in 2D image space, requiring additional 3D lifting for real robot execution
- Reward function robustness: Success depends entirely on reward functions capturing true physical reasoning requirements

## Confidence

**High confidence**: In-domain performance claims (31.0 IoU, 110.87 trajectory error) are directly measured on UMD benchmark with clear methodology.

**Medium confidence**: Out-of-domain generalization results show improvement over baselines but lack comparison to alternative generalization strategies.

**Low confidence**: Claim that RLVR "enhances physical reasoning" is qualitative and not directly measured.

## Next Checks

1. **Cross-dataset generalization test**: Evaluate on completely different manipulation dataset (e.g., SAPIEN, RLBench) to verify whether 34.65 Grasp-IoU out-of-domain performance generalizes beyond UMD's distribution.

2. **Real-robot deployment validation**: Implement the 3D lifting pipeline and test on physical robot platform. Measure end-effector error, task completion rate, and compare against 2D simulation metrics.

3. **Joint training ablation**: Train model with affordance and trajectory prediction in single forward pass with shared representations. Compare against current decoupled approach on computational efficiency and task performance.