---
ver: rpa2
title: Predicting Training Re-evaluation Curves Enables Effective Data Curriculums
  for LLMs
arxiv_id: '2509.25380'
source_url: https://arxiv.org/abs/2509.25380
tags:
- training
- trec
- data
- loss
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces training re-evaluation curves (TRECs) to
  guide data placement in LLM training. TRECs measure how well a trained model retains
  data from different training stages, revealing that placing high-quality data at
  low TREC points improves performance.
---

# Predicting Training Re-evaluation Curves Enables Effective Data Curriculums for LLMs

## Quick Facts
- **arXiv ID:** 2509.25380
- **Source URL:** https://arxiv.org/abs/2509.25380
- **Reference count:** 40
- **Primary result:** Training Re-evaluation Curves (TRECs) predict optimal data placement in LLM training, with Pearson correlation exceeding 98% for TREC shape prediction.

## Executive Summary
This paper introduces Training Re-evaluation Curves (TRECs) as a principled framework for determining optimal placement of high-quality data in large language model (LLM) training. TRECs measure how well a trained model retains information from different training stages, revealing that placing high-quality data at low TREC points significantly improves performance. Critically, the authors show TRECs can be predicted in advance using AdamW's EMA coefficients combined with a training-fraction adjustment, enabling proactive curriculum design. Experiments across models from 111M to 3.9B parameters demonstrate that predicted TRECs reliably guide optimal data placement within learning rate schedules, offering a principled alternative to heuristic data ordering.

## Method Summary
The method computes TRECs by evaluating the final trained model on historical training batches to measure retrospective loss. TREC prediction uses the formula $\hat{L}_{re}(\hat{t}) = 1 - c(\hat{t})^p \cdot \hat{t}^m$, where $c(\hat{t})$ are AdamW EMA coefficients based on the learning rate schedule and weight decay, and $m$ is a training-fraction exponent predicted by a power law. The optimal placement point is identified as the training fraction minimizing predicted TREC. This approach is validated through experiments inserting high-quality data at predicted minima versus traditional end-of-training placement.

## Key Results
- TRECs can be predicted with >98% Pearson correlation using AdamW EMA coefficients and training-fraction adjustment
- Placing high-quality data at TREC minima improves validation performance compared to end-of-training placement
- TREC prediction generalizes across model sizes from 111M to 3.9B parameters
- The framework explains prior curriculum strategies and identifies suboptimal placements in step-decay schedules

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** AdamW optimizer functions as an implicit Exponential Moving Average (EMA) over weight updates, where EMA coefficients determine training batch influence on final model weights.
- **Mechanism:** AdamW updates rewrite as $y_t = (1-\alpha_t)y_{t-1} + \alpha_t x_t$. The timescale $\tau = \frac{1}{\eta \lambda}$ defines the optimizer's memory window. Larger $\tau$ retains earlier data influence longer; smaller $\tau$ focuses on recent data.
- **Core assumption:** Training uses AdamW; standard Adam or non-EMA optimizers may not exhibit same TREC dynamics.
- **Evidence:** TREC shapes align when sweeping learning rate, weight decay, or batch size while maintaining matching timescales (Section 3).

### Mechanism 2
- **Claim:** Gradient update effectiveness degrades over time due to "minimizer drift," modeled as a power law of normalized training fraction.
- **Mechanism:** Updates calculated at step $t$ become misaligned with final parameters $\theta_T$ as the batch-specific local minimizer drifts. Degradation follows $\hat{t}^m$ where $\hat{t} = \frac{t}{T}$.
- **Core assumption:** Loss landscape is locally quadratic with predictable minimizer drift based on normalized training progress.
- **Evidence:** TREC shape prediction combines EMA coefficients with training-fraction adjustment (Section 4).

### Mechanism 3
- **Claim:** High-quality data placed at TREC minimum yields better validation performance than end-of-training placement.
- **Mechanism:** TREC minimum indicates where final model $\theta_T$ is most aligned with data presented at that time. Inserting HQ data here retains more specific signal compared to high TREC loss points.
- **Core assumption:** HQ data segment is small enough not to fundamentally alter overall TREC shape of dominant training distribution.
- **Evidence:** Optimal placement is in TREC valley, not at end, for step-decay schedules (Figure 1).

## Foundational Learning

- **Concept:** AdamW EMA Timescale ($\tau$)
  - **Why needed here:** Primary control knob for TREC shape. Cannot predict or manipulate TREC without understanding how $\tau = \frac{B}{\eta \lambda D}$ dictates optimizer memory.
  - **Quick check question:** If you double batch size $B$ and want to keep TREC shape identical, what must you do to learning rate $\eta$ or weight decay $\lambda$? (Answer: Double $\eta$ or $\lambda$ to maintain constant $\tau$).

- **Concept:** Training Fraction ($\hat{t}$) vs. Wall-clock Time
  - **Why needed here:** Drift mechanism and TREC prediction operate on normalized progress $\hat{t} = \frac{tB}{D}$. Models of different sizes align only when compared on this scale.
  - **Quick check question:** Why does a 111M model trained for 20 TPP have similar TREC shape to a 3.3B model trained for 20 TPP, despite vastly different token counts?

- **Concept:** Minimizer Drift (The "Why" of Forgetting)
  - **Why needed here:** Explains why EMA coefficients alone are insufficient. Early updates are "forgotten" not just from EMA decay but because optimal solution for early batch has moved by training end.
  - **Quick check question:** In quadratic model, why does gradient update computed at $t=10$ potentially increase loss when re-evaluated at $T=1000$?

## Architecture Onboarding

- **Component map:** Learning Rate Schedule, Weight Decay, Batch Size, Total Tokens -> Compute EMA coefficients and Drift exponent -> Predicted TREC curve and optimal insertion point
- **Critical path:**
  1. Compute EMA Coefficients: Calculate $c(\hat{t})$ based on LR schedule using AdamW formulation
  2. Estimate Drift ($m$): Use fitted power law $m^* = C \cdot (TPP)^{\mu_1} \cdot (\tau)^{\mu_2}$ or small proxy run
  3. Predict TREC: Combine into $\hat{L}_{re}(\hat{t}) = 1 - c(\hat{t})^p \cdot \hat{t}^m$
  4. Identify Minima: Find $\hat{t}_{min}$ where predicted loss is lowest
  5. Curriculum Design: Schedule HQ data to fall at $\hat{t}_{min}$

- **Design tradeoffs:**
  - Precision vs. Cost: Generic power law for $m$ enables prediction without training, but proxy model fitting yields higher Pearson correlation
  - CPT Learning Rate: Higher CPT LR creates deeper TREC valley (better retention) but risks destabilizing model

- **Failure signatures:**
  - Flat TREC: Occurs if $\lambda \to 0$ (vanilla Adam) or LR schedule causes EMA coefficients to converge to LR shape
  - Divergent CPT: If CPT LR too high (e.g., 0.015), TREC loss drops but validation loss degrades

- **First 3 experiments:**
  1. Reproduce EMA Control: Train 610M model sweeping $\eta$ and $\lambda$ keeping $\tau$ constant. Verify TREC shapes align
  2. Validate Placement: Take model with Step-Decay schedule. Place HQ data at predicted TREC minimum vs. end. Compare validation loss
  3. Test Generalization: Predict TREC for 3.9B model using $m$ fitted on 111M model. Compare predicted curve to actual 3.9B TREC

## Open Questions the Paper Calls Out

- Can predictive TREC framework extend to optimizers lacking implicit EMA formulation, such as Adagrad, Adafactor, or SGD variants? (Section A)
- Can absolute TREC values be normalized to predict optimal placement across different learning rate schedules? (Section D.2, Section A)
- Does TREC-guided placement effectiveness vary between distinct data types, such as factual knowledge versus reasoning capabilities? (Section A)
- Why does correlation between low TREC loss and improved validation performance break down at high learning rates during Continual Pre-Training? (Section 5.3, Appendix I.2)

## Limitations

- The generic power law for drift exponent $m$ enables prediction without training, but schedule-specific $m$ fits yield higher Pearson correlation, suggesting limitations for complex schedules
- TREC mechanism relies on stability of training trajectory and AdamW EMA approximation; significantly different optimization strategies may exhibit different dynamics
- Placement strategy assumes HQ data segment is small enough not to alter dominant TREC shape; very large HQ data segments (>20% of total tokens) may break this assumption

## Confidence

- **TREC Prediction Accuracy:** High - Pearson correlations exceeding 98% demonstrated across multiple model sizes and schedules
- **Optimal Placement Efficacy:** High - Experiments show consistent improvements in validation performance with TREC-guided placement
- **Mechanism Validity:** Medium - AdamW EMA and minimizer drift mechanisms are theoretically sound but mathematical relationship could benefit from more rigorous proof

## Next Checks

1. **Schedule Generalization Test:** Train models with cyclic learning rate schedules and compare predicted TREC curves (using generic $m$) against actual TRECs. Measure Pearson correlation and identify systematic prediction errors.

2. **Optimizer Diversity Test:** Replicate TREC prediction and placement experiments using Adafactor or Lion optimizers instead of AdamW. Compare TREC shapes, prediction accuracy, and placement efficacy.

3. **Data Scale Stress Test:** Design experiments with HQ data segments ranging from 5% to 30% of total training tokens. Track how predicted TREC minimum shifts and whether placement strategy remains effective as HQ data becomes larger fraction of training corpus.