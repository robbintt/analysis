---
ver: rpa2
title: 'On Temperature-Constrained Non-Deterministic Machine Translation: Potential
  and Evaluation'
arxiv_id: '2601.13729'
source_url: https://arxiv.org/abs/2601.13729
tags:
- nd-mt
- metrics
- systems
- lexical
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically investigates non-deterministic machine
  translation (ND-MT), identifying temperature-constrained ND-MT as a distinct phenomenon.
  The authors demonstrate that ND-MT can address the multi-modality challenge by generating
  lexically diverse candidates while maintaining semantic equivalence under temperature
  constraints.
---

# On Temperature-Controlled Non-Deterministic Machine Translation: Potential and Evaluation

## Quick Facts
- **arXiv ID**: 2601.13729
- **Source URL**: https://arxiv.org/abs/2601.13729
- **Reference count**: 32
- **Primary result**: Temperature-constrained non-deterministic machine translation (ND-MT) generates lexically diverse candidates while maintaining semantic equivalence

## Executive Summary
This paper systematically investigates non-deterministic machine translation (ND-MT) as a distinct phenomenon, identifying temperature-constrained ND-MT as a valuable approach for addressing multi-modality challenges in translation. The authors demonstrate that ND-MT systems can generate lexically diverse candidates while maintaining semantic equivalence under temperature constraints. Through experiments across 22 systems in six language directions, they reveal that temperature critically affects ND-MT quality - lexical diversity increases while semantic equivalence decreases with rising temperature. The study also exposes a fundamental flaw in current deterministic MT evaluation frameworks when applied to ND-MT, termed the "Buckets effect," where the lowest-quality candidate determines system ranking across sampling sizes.

## Method Summary
The authors conducted comprehensive experiments comparing deterministic and non-deterministic MT systems across multiple language pairs. They evaluated systems using various sampling strategies and temperature settings, measuring both lexical diversity and semantic equivalence. The study introduced the ExpectoSample strategy to automatically assess evaluation metric reliability and identify robust ND-MT systems. Temperature was varied systematically to analyze its impact on translation quality metrics, and the authors examined how current evaluation frameworks perform when applied to non-deterministic outputs. They released their code, data, and evaluation results to support future research in this emerging field.

## Key Results
- ND-MT systems provide significant lexical variance with nearly identical semantic meanings compared to deterministic counterparts across 22 systems in six language directions
- Temperature critically affects ND-MT quality - lexical diversity increases while semantic equivalence decreases with rising temperature
- The current D-MT evaluation framework fails to yield consistent results when applied to ND-MT, revealing a "Buckets effect" where the lowest-quality candidate determines system ranking across sampling sizes

## Why This Works (Mechanism)
The paper demonstrates that non-deterministic machine translation leverages the inherent probabilistic nature of modern transformer-based models to generate multiple valid translations for the same source sentence. By controlling the sampling temperature parameter, systems can navigate the trade-off between diversity and quality. Higher temperatures allow exploration of the output probability space, yielding more diverse lexical choices, while lower temperatures concentrate probability mass around high-scoring candidates, maintaining semantic fidelity. This mechanism exploits the multi-modal nature of translation, where multiple valid expressions can convey the same meaning, a challenge that deterministic beam search approaches struggle to address effectively.

## Foundational Learning
- **Temperature sampling in neural networks**: Controls randomness in output generation by scaling logits before softmax; needed to understand diversity-quality trade-off in ND-MT
- **Multi-modality in machine translation**: Multiple valid translations exist for single source sentences; quick check: compare reference translations in parallel corpora
- **Evaluation metrics for MT**: BLEU, COMET, chrF; quick check: compute metrics on same translation with different references
- **Beam search limitations**: Deterministic search may miss valid translation alternatives; quick check: examine beam search outputs for diversity
- **Sampling strategies comparison**: Top-k, top-p, multinomial sampling; quick check: visualize probability distributions under different sampling methods
- **Semantic equivalence measurement**: Metrics like BERTScore, MoverScore; quick check: compute semantic similarity between paraphrases

## Architecture Onboarding

**Component map**: Input text -> Tokenizer -> Encoder -> Decoder (with temperature) -> Sampling strategy -> Output candidates

**Critical path**: The decoder with temperature scaling represents the critical path for ND-MT, where temperature adjustment directly controls the exploration-exploitation trade-off in the output probability space.

**Design tradeoffs**: Higher temperature increases diversity but reduces semantic consistency; multiple sampling strategies offer different exploration patterns but vary in computational efficiency; deterministic approaches ensure consistency but may miss valid alternatives.

**Failure signatures**: Temperature too high leads to semantically divergent outputs; inadequate sampling size fails to capture true diversity; applying deterministic evaluation metrics directly to ND-MT produces inconsistent rankings.

**First experiments**: (1) Vary temperature from 0.1 to 2.0 and measure diversity metrics; (2) Compare top-k vs top-p sampling with identical temperature settings; (3) Evaluate the same ND-MT system with different sampling sizes (1, 5, 10, 20) to observe the "Buckets effect"

## Open Questions the Paper Calls Out
None

## Limitations
- Investigation focuses primarily on sentence-level evaluation, leaving document-level coherence effects unexplored
- The "Buckets effect" raises questions about generalizability of current ranking methodologies across different task domains
- The ExpectoSample strategy shows promise but requires broader validation across additional language pairs and model architectures

## Confidence
- Temperature's inverse relationship with semantic equivalence: **High confidence**
- Lexical diversity benefits of ND-MT systems: **High confidence**
- ExpectoSample strategy effectiveness: **Medium confidence**
- Evaluation framework inadequacy: **High confidence**

## Next Checks
1. Test ExpectoSample methodology across diverse model families including encoder-decoder and decoder-only architectures to verify robustness beyond the current experimental scope
2. Conduct human evaluation studies to confirm automated metric findings about semantic equivalence degradation at higher temperatures, particularly for low-resource language pairs
3. Extend analysis to document-level evaluation to determine whether the identified temperature-lexical diversity trade-off holds when coherence and discourse consistency are considered