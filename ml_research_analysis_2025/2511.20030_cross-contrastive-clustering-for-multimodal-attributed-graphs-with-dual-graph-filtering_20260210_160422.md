---
ver: rpa2
title: Cross-Contrastive Clustering for Multimodal Attributed Graphs with Dual Graph
  Filtering
arxiv_id: '2511.20030'
source_url: https://arxiv.org/abs/2511.20030
tags:
- graph
- clustering
- node
- learning
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DGF, a novel clustering method for Multimodal
  Attributed Graphs (MMAGs). Existing multi-view clustering methods struggle with
  MMAGs due to low correlation and high noise in multimodal attributes from pre-trained
  models.
---

# Cross-Contrastive Clustering for Multimodal Attributed Graphs with Dual Graph Filtering

## Quick Facts
- **arXiv ID**: 2511.20030
- **Source URL**: https://arxiv.org/abs/2511.20030
- **Reference count**: 40
- **Primary result**: Achieves up to 10.35% improvement in Adjusted Rand Index on large graphs compared to state-of-the-art multimodal clustering methods.

## Executive Summary
This paper introduces DGF, a novel clustering method for Multimodal Attributed Graphs (MMAGs) that overcomes limitations of existing multi-view clustering approaches. Traditional methods struggle with MMAGs due to low correlation and high noise in multimodal attributes from pre-trained models. DGF addresses these challenges through dual graph filtering (node-domain and feature-domain denoising) and a tri-cross contrastive training strategy. The method achieves significant performance improvements across eight benchmark datasets, demonstrating both effectiveness and scalability for large-scale graph clustering.

## Method Summary
DGF combines dual graph filtering with tri-cross contrastive learning for multimodal attributed graph clustering. The dual filtering applies both node-domain propagation and feature-domain denoising simultaneously, acting as a low-pass filter to attenuate high-frequency noise. The tri-cross contrastive training includes cross-modality alignment, cross-neighborhood discrimination, and cross-community clustering. The method uses Attribute-Aware Sampling to prune spurious edges and Hard Positive Sampling for community-level contrastive learning. DGF operates by first projecting multimodal features to a shared hidden space, applying the dual filters to obtain denoised representations, then optimizing contrastive losses to align embeddings across modalities, neighborhoods, and communities.

## Key Results
- Achieves up to 10.35% improvement in Adjusted Rand Index on large graphs
- Outperforms state-of-the-art methods across all eight benchmark datasets
- Demonstrates scalability with linear complexity relative to graph size
- Shows significant gains in both accuracy and efficiency compared to existing approaches

## Why This Works (Mechanism)

### Mechanism 1: Dual Graph Filtering as Low-Pass Noise Attenuation
When node attributes contain high feature-wise noise from pre-trained models, dual graph filtering denoises more effectively than standard node-domain propagation. The method optimizes an objective combining Laplacian regularization with Feature-Domain Denoising (FDD), creating a low-pass filter in both node and feature spectral domains. This theoretically attenuates high-frequency noise present in attribute matrices, with the filter response $g(\omega_j) = (1 + \beta \omega_j)^{-1}$ proving its low-pass characteristics.

### Mechanism 2: Attribute-Aware Edge Pruning
When graph edges contain spurious links, pruning them based on cross-modal attribute similarity prevents contamination of node representations. DGF computes cross-modal similarity scores for edges and removes those below a threshold $\phi$ (derived from mean + std of random samples). Random walks for positive sampling then occur only on the pruned graph, ensuring neighborhood contrast uses structurally and semantically valid edges.

### Mechanism 3: Cross-Modality Contrastive Alignment
When multimodal attributes have low semantic correlation, explicit contrastive alignment across modalities is required to fuse them. DGF uses Cross-Modality Contrastive Loss with Masked Margin Softmax to enforce agreement between embeddings of the same node across different modalities, pulling them into a joint embedding space. This is particularly important for MMAGs where Attribute Distance Correlation is low (< 0.28) compared to standard multi-view data (> 0.89).

## Foundational Learning

- **Graph Signal Processing (GSP) and Spectral Filtering**: The paper frames dual filtering through GSP lens, proving it acts as a low-pass filter. Quick check: Can you explain why multiplying a signal $Z$ by powers of the normalized adjacency matrix $\hat{A}^t$ is considered a "low-pass" operation in the spectral domain?

- **Contrastive Learning (Instance-level)**: The training strategy relies entirely on Tri-Cross Contrastive Learning rather than reconstruction errors. Quick check: How does the "Hard Positive Sampling" strategy in the cross-community loss differ from standard positive sampling, and what specific problem does it solve?

- **Neumann Series Expansion**: The paper uses Neumann series to approximate matrix inverse in the closed-form solution. Quick check: Under what condition does the series $(I - M)^{-1} = \sum M^t$ converge, and how does Lemma 4.2 ensure this condition is met for the feature shift operator $S^{(i)}$?

## Architecture Onboarding

- **Component map**: Input MMAGs $\{X^{(i)}, A\}$ -> Linear projectors $W^{(i)}$ -> Hidden features $Z^{(i)}$ -> Feature Graph Constructor (builds $S^{(i)}$) -> Dual Filter (node + feature denoising) -> Representation $H$ -> Tri-Cross Heads (modality, neighborhood, community) -> K-Means clustering

- **Critical path**: The computation of $H$ in Algorithm 1 (lines 8-10). The multiplication order matters: $F_L \cdot Z \cdot F_R$. Errors in constructing $S^{(i)}$ (line 4) or the accumulation of filters (lines 8-9) will prevent the gradient flow required for the contrastive losses.

- **Design tradeoffs**: Truncation depth ($T$) balances receptive field vs. over-smoothing (peaks at $T=10$). AAS threshold ($\phi$) balances pruning spurious edges vs. removing valid structural edges.

- **Failure signatures**: Feature collapse if $L_{mod}$ dominates (monitor $H$ variance). Non-convergence if $S^{(i)}$ eigenvalues > 1. Over-smoothing if $T$ is too large (loss of neighborhood signal).

- **First 3 experiments**: 1) Validate FDD by comparing DGF with/without feature-domain denoising on high-noise dataset (Reddit-S), confirming ARI gap >10%. 2) Calculate Attribute Distance Correlation on target dataset - if ADC > 0.8, DGF may offer little benefit over simpler methods. 3) Sweep $T \in \{1, 5, 10, 20\}$ and plot NMI to verify the "elbow" at $T=10$.

## Open Questions the Paper Calls Out
The paper explicitly states its aim to investigate the applicability of DGF to other downstream multimodal graph learning tasks beyond clustering, such as node classification or link prediction, in future work.

## Limitations
- The method's effectiveness depends heavily on the quality and correlation of pre-trained embeddings, with performance potentially degrading if attributes come from incompatible or irrelevant encoders.
- The Attribute-Aware Sampling threshold selection assumes normal distribution of cross-modal similarities, which may not hold for all real-world graphs with skewed or heavy-tailed similarity distributions.
- The method is specifically optimized for clustering tasks, and its adaptation to other downstream tasks like node classification or link prediction requires further investigation.

## Confidence
- **High Confidence**: The theoretical framework of dual graph filtering as a low-pass filter is rigorously proven and well-grounded in graph signal processing principles.
- **Medium Confidence**: The mechanism of Attribute-Aware Sampling for spurious link pruning is supported by ablation studies but requires more investigation across diverse graph structures.
- **Medium Confidence**: The effectiveness of cross-modality contrastive learning is theoretically justified but requires more detailed ablation of the Masked Margin Softmax contribution.

## Next Checks
1. **Theoretical Validation**: Verify the low-pass filter property by computing the frequency response $g(\omega_j) = (1 + \beta \omega_j)^{-1}$ on synthetic graphs with known spectral properties to confirm attenuation of high-frequency components.

2. **Robustness Analysis**: Test the Attribute-Aware Sampling mechanism across graphs with varying sparsity levels and edge noise ratios to quantify the sensitivity of the $\phi$ threshold selection.

3. **Scaling Evaluation**: Measure the actual training time and memory consumption on graphs of increasing size (beyond the reported benchmarks) to validate the claimed scalability and identify potential bottlenecks in the dual filtering implementation.