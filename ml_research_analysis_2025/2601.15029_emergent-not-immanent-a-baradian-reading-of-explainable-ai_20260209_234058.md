---
ver: rpa2
title: 'Emergent, not Immanent: A Baradian Reading of Explainable AI'
arxiv_id: '2601.15029'
source_url: https://arxiv.org/abs/2601.15029
tags:
- which
- human
- these
- they
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper critiques the dominant assumptions in Explainable AI
  (XAI), which treat interpretability as a technical problem of uncovering pre-existing
  explanations within AI models. Drawing on Karen Barad's agential realism, the authors
  propose an alternative onto-epistemology where interpretations are material-discursive
  performances emerging from situated entanglements between AI models, humans, context,
  and explanatory tools.
---

# Emergent, not Immanent: A Baradian Reading of Explainable AI

## Quick Facts
- arXiv ID: 2601.15029
- Source URL: https://arxiv.org/abs/2601.15029
- Reference count: 40
- Primary result: Proposes an onto-epistemological alternative to dominant XAI approaches, treating explanations as emergent performances rather than immanent truths within models.

## Executive Summary
This paper critiques the prevailing assumptions in Explainable AI that treat interpretability as a technical problem of uncovering pre-existing explanations within AI models. Drawing on Karen Barad's agential realism, the authors propose an alternative framework where interpretations are material-discursive performances emerging from situated entanglements between AI models, humans, context, and explanatory tools. Through diffractive analysis of various XAI methods, they reveal how these methods operate under flawed assumptions about immanent explananda and human-machine commensurability. The paper argues for shifting from extracting truths to practicing explanations that embrace difference, interference, and accountability, proposing design directions for diffractive XAI interfaces that foreground multiple interpretations and situatedness.

## Method Summary
The paper employs diffractive analysis to examine XAI methods through the lens of agential realism. Rather than testing empirical hypotheses, it uses theoretical critique to expose unexamined onto-epistemological assumptions in mainstream XAI approaches. The method involves reading multiple XAI methods (saliency maps, LIME, SHAP, Grad-CAM, counterfactuals, mechanistic interpretability, etc.) through Baradian optics to surface assumptions about where explanations reside and how they relate to model internals. The analysis produces a taxonomy of XAI methods based on their implicit ontological commitments and proposes a framework for understanding explanations as emergent phenomena rather than immanent truths waiting to be discovered.

## Key Results
- Mainstream XAI methods implicitly assume explanations are immanent properties within models that can be extracted through technical means
- XAI methods enact agential cuts that make some interpretations possible while foreclosing others, with ethical implications for exclusion
- Meaning emerges through interference patterns when multiple explanatory performances are staged together and read through one another
- Design directions for diffractive XAI interfaces emphasize multiple interpretations, situatedness, and foregrounding uncertainty

## Why This Works (Mechanism)

### Mechanism 1: Intra-action Over Representation
Explanations do not pre-exist within models waiting to be extracted; they emerge through situated entanglements of model, human, context, and explanatory tools. Under agential realism, the observer, apparatus, and observed are mutually constituted through intra-action. An XAI method is not a neutral lens revealing pre-existing structure but an apparatus that participates in bringing forth what counts as an explanation. The explanandum, explainer, and explanation co-emerge.

### Mechanism 2: Agential Cuts Produce Exclusions
Every XAI method enacts an agential cut that makes some interpretations possible while foreclosing others. Apparatuses (XAI methods, visualization tools, evaluation metrics) draw provisional boundaries in sociomaterial reality. These cuts are not neutral—they encode assumptions about what counts as salient, who is a valid interpreter, and which features matter. The cut determines the phenomenon that materializes.

### Mechanism 3: Diffractive Signatures from Multiple Apparatuses
Meaning emerges through interference patterns when multiple explanatory performances are staged together and read through one another. Rather than seeking a single correct explanation, diffractive XAI juxtaposes outputs from different methods, contexts, and interpreters. The "diffractive signature" is the pattern of convergence and divergence across these readings. Meaning arises from contrast, not from isolating intrinsic properties.

## Foundational Learning

- **Agential Realism (Barad)**
  - Why needed here: This is the core theoretical framework the paper deploys. Without understanding intra-action, apparatuses, and agential cuts, the critique of mainstream XAI will seem like wordplay rather than a substantive alternative ontology.
  - Quick check question: Can you explain why "intra-action" differs from "interaction," and what this implies for the observer-object relationship in XAI?

- **Mainstream XAI Taxonomy**
  - Why needed here: The paper analyzes specific methods (Grad-CAM, LIME, SHAP, circuits, counterfactuals, etc.) through Baradian optics. Familiarity with what these methods claim to do is prerequisite to understanding the critique.
  - Quick check question: For a given XAI method (e.g., saliency maps), can you articulate what it assumes about where explanations reside and whether the method is neutral?

- **Immanent vs. Emergent Ontology**
  - Why needed here: The central argument turns on this distinction. Immanent = explanations pre-exist in the model, awaiting discovery. Emergent = explanations co-constituted through the interpretive act. Misunderstanding this distinction undermines the entire argument.
  - Quick check question: If you were told "the model knows why it made this prediction," which ontology is being assumed?

## Architecture Onboarding

- **Component map:**
  - Model (M) -> Apparatus (A) -> Human (H) -> Context (C) -> Entanglement configuration (δ) -> Interference pattern (Δ)

- **Critical path:**
  1. Identify the explanatory entanglement: which model, which apparatus(es), which interpreters, which context
  2. Make agential cuts visible: what does this configuration foreground and what does it exclude?
  3. Stage multiple entanglements: vary at least one component (different method, different interpreter background, different context)
  4. Read diffractively: analyze interference patterns—where do interpretations converge, diverge, cancel?
  5. Iterate: allow the interpreter to become a "different kind of knowing subject" through the process

- **Design tradeoffs:**
  - Single vs. multiple interpretations: Single interpretations offer clarity and actionability but risk false certainty and exclusion. Multiple interpretations surface ambiguity and complexity but may overwhelm users or fail to guide action.
  - Foregoing immanence: Accepting emergence means giving up on the goal of "opening the black box" to reveal pre-existing truth. This is intellectually honest but may not satisfy regulatory or user demands for definitive explanations.
  - Making cuts visible vs. invisible: Visible cuts support reflexivity and accountability but add cognitive load and may expose institutional politics.

- **Failure signatures:**
  - Fauxtomation: XAI outputs that appear explanatory but merely "offload ambiguity back to human users" without genuine sense-making
  - Simulacra: Simplified/surrogate models that replace rather than approximate the original, presenting human-friendly projections as if they were the model's actual logic
  - Explanatory unicity assumption: Believing there exists one correct explanation to be discovered; diffractive approach explicitly rejects this
  - Incommensurability unrecognized: Treating machine abstractions as if they were translatable to human concepts without remainder

- **First 3 experiments:**
  1. Multi-method comparison on same prediction: Take a single model output and apply 3+ XAI methods (e.g., Grad-CAM, LIME, counterfactual). Document where they agree and disagree. Interview users on whether juxtaposition helps or confuses.
  2. Context variation: Use the same XAI method on the same prediction with different contextual framings (e.g., legal compliance vs. scientific understanding vs. end-user trust). Document how interpretations shift.
  3. Exclusion mapping: For a given XAI pipeline, explicitly document who is excluded from the interpretive entanglement (which communities, which expertise, which data histories).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the theoretical design directions for diffractive XAI (e.g., foregrounding multiple interpretations) be translated into operational design patterns and guidelines?
- Basis in paper: [explicit] The conclusion states that future work must "translate these directions into concrete design patterns and guidelines" as the current proposal is primarily conceptual and speculative.
- Why unresolved: The paper offers a high-level framework and a speculative case study but lacks a structured design methodology for practitioners to implement these concepts.

### Open Question 2
- Question: Does a diffractive XAI approach improve user decision-making and appropriate reliance compared to traditional "immanent" methods?
- Basis in paper: [inferred] The limitations section notes the framework is "primarily conceptual" without a "full empirical case study," leaving the actual utility of the approach untested.
- Why unresolved: While theoretically robust, it is unknown if emphasizing ambiguity and situatedness aids users or induces harmful cognitive load compared to standard fidelity-based explanations.

### Open Question 3
- Question: How can the validity or quality of an explanation be evaluated if the explanandum is viewed as an emergent performance rather than a pre-existing truth?
- Basis in paper: [inferred] The paper rejects "explanatory unicity" and the assumption that explanations map to fixed internal structures, thereby destabilizing standard fidelity metrics.
- Why unresolved: If explanations are not "correct" representations of a model, current evaluation methods become insufficient.

## Limitations
- The theoretical framework, while philosophically grounded, lacks empirical validation in XAI contexts
- Design directions remain speculative without concrete implementation details or user testing
- The exclusion critique, though compelling philosophically, lacks systematic mapping of which communities are marginalized
- No operationalized metrics exist for evaluating whether an interface supports "emergent interpretation"

## Confidence
- **High**: The critique of immanent explananda assumptions is well-grounded in existing XAI literature
- **Medium**: Claims about emergence and diffractive signatures lack direct experimental support
- **Low**: Design directions remain speculative without implementation or user testing

## Next Checks
1. Implement a diffractive XAI interface comparing multiple explanation methods on identical predictions and measure whether users achieve better calibrated understanding of uncertainty
2. Systematically document which interpretive communities (disciplinary backgrounds, expertise levels, cultural contexts) are excluded from standard XAI pipelines
3. Conduct longitudinal studies tracking how repeated engagement with multiple explanations affects users' epistemological stances toward AI systems