---
ver: rpa2
title: Provable Scaling Laws of Feature Emergence from Learning Dynamics of Grokking
arxiv_id: '2509.21519'
source_url: https://arxiv.org/abs/2509.21519
tags:
- multiplier
- train
- test
- learning
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a mathematical framework called Li2 that explains
  the grokking phenomenon in neural network training. Grokking refers to the delayed
  generalization where a model initially memorizes training data then suddenly generalizes
  to unseen data after prolonged training.
---

# Provable Scaling Laws of Feature Emergence from Learning Dynamics of Grokking

## Quick Facts
- arXiv ID: 2509.21519
- Source URL: https://arxiv.org/abs/2509.21519
- Reference count: 40
- Primary result: Mathematical framework (Li2) explains grokking through three-stage dynamics governed by an energy function

## Executive Summary
This paper proposes Li2, a mathematical framework explaining the grokking phenomenon in neural network training. Grokking refers to the delayed generalization where models initially memorize training data then suddenly generalize after prolonged training. The framework identifies three distinct stages: lazy learning (output layer overfits), independent feature learning (hidden nodes learn via energy function gradient ascent), and interactive feature learning (hidden nodes refine features through interaction). The analysis reveals that sample size determines a phase transition between generalizable features and memorization solutions, with scaling laws showing the critical training ratio p ≈ M⁻¹log(M).

## Method Summary
The framework analyzes 2-layer nonlinear networks with weight decay and quadratic activation (σ(x)=x²). Training uses MSE loss with L2 regularization. The key mechanism involves backpropagated gradient GF carrying label information that drives hidden nodes to optimize an energy function E through gradient ascent. The paper establishes theoretical characterization of local maxima of E for group arithmetic tasks and proves scaling laws for the sample size threshold that determines generalization vs memorization outcomes.

## Key Results
- Identifies three-stage dynamics (lazy → independent → interactive feature learning) that explain grokking behavior
- Shows independent feature learning follows gradient ascent of energy function E whose local maxima correspond to emerging features
- Establishes scaling laws for sample size threshold determining generalization-memorization phase transition (p ≈ M⁻¹log(M))

## Why This Works (Mechanism)

### Mechanism 1: Three-Stage Learning Dynamics of Grokking
- **Claim**: Grokking emerges from three distinct learning stages where backpropagated gradient GF drives hidden node feature learning
- **Core assumption**: 2-layer nonlinear networks with weight decay η > 0 and finite hidden width K
- **Evidence**: Theorem 1 proves GF structure enables independent feature learning; alternative thermodynamics-based explanations exist but Li2 offers first-principles gradient dynamics
- **Break condition**: If K → ∞ (NTK regime) or initialization scale α too large without weight decay

### Mechanism 2: Energy Function E Governs Feature Emergence
- **Claim**: Local maxima of energy function E(w) = ½||Ỹ⊤σ(Xwj)||² correspond to emerging features for group arithmetic tasks
- **Core assumption**: Nonlinear activation required; linear activation reduces E to single global maximum
- **Evidence**: Theorem 2 characterizes local maxima as irreducible representations of group structure; E provides more efficient target reconstruction than memorization
- **Break condition**: Highly concentrated data distributions where only memorization solutions exist

### Mechanism 3: Sample Size Determines Generalization-Memorization Phase Transition
- **Claim**: Critical sample threshold exists where features remain stable and generalizable versus converging to memorization
- **Core assumption**: Uniform random sampling from H×H for group arithmetic tasks
- **Evidence**: Theorem 4 proves stability threshold n ≳ dk²M log(M/δ); scaling law p ≈ M⁻¹log(M)
- **Break condition**: Non-diverse samples lacking single-target diversity

## Foundational Learning

- **Concept: Backpropagated Gradient GF Structure**
  - Why needed: GF = P⊥₁(Y - FV)V⊤ carries label information proportional to ηỸỸ⊤F that initiates feature learning
  - Quick check: Explain why weight decay η acts as effective learning rate for feature learning process

- **Concept: Group Representation Theory (Irreducible Representations)**
  - Why needed: Local maxima of energy function E correspond to irreps of input group structure
  - Quick check: Why do irreducible representations matter for determining learned features in group arithmetic tasks?

- **Concept: Energy Landscapes and Local Maxima**
  - Why needed: E's landscape determines feature emergence; landscape stability governed by data size
  - Quick check: How does energy landscape change when transitioning from sufficient to insufficient training data?

## Architecture Onboarding

- **Component map**: Input Layer (W) → Hidden Activation (F) → Output Layer (V) → Backpropagated Gradient (GF)

- **Critical path**:
  1. Stage I: V converges to Vridge while W remains near random → GF becomes ηỸỸ⊤F
  2. Stage II: Each hidden node wj independently ascends E via GF → converges to local maxima (features/irreps)
  3. Stage III: Node interactions via B matrix induce repulsion of similar features; top-down modulation shifts GF

- **Design tradeoffs**:
  - Hidden width K: Large K reduces ||GF(+∞)|| → slower feature learning; too small limits feature diversity
  - Weight decay η: Controls signal strength in GF; higher η accelerates grokking but may push toward memorization
  - Learning rate: Small LR in Stage II helps stay in generalizable basins when data scarce
  - Top-layer initialization: Zero-init for V accelerates feature learning 10× in multi-layer settings

- **Failure signatures**:
  - No grokking, test accuracy ~0: GF too weak (small η or very large K)
  - Train→1, test stays low: Converged to memorization local maxima due to insufficient data
  - Ungrokking (test rises then falls): Continued training past generalization into higher-energy memorization

- **First 3 experiments**:
  1. Replicate grokking dynamics: Train 2-layer network (K=1024, M=71, n=40%) with η=0.0002; verify three-stage dynamics
  2. Sample size phase transition: Sweep training ratio 10%-50% on modular addition (M=41, 71, 127); observe p ~ M⁻¹log(M) scaling
  3. Muon optimizer validation: With limited hidden nodes (K=60-120, M=71), compare Adam vs Muon for feature diversity

## Open Questions the Paper Calls Out

- **Open Question 1**: Can Li2 framework be extended to characterize transition time between consecutive learning stages?
  - Why unresolved: Current framework characterizes each stage but provides no quantitative bounds on transition timing
  - What would resolve: Theoretical bounds on transition epoch as function of hyperparameters

- **Open Question 2**: Does energy function analysis generalize to inputs without group structure?
  - Why unresolved: Group representation theory enables complete characterization of local maxima but no analogous tool exists for arbitrary inputs
  - What would resolve: Identification of local maxima structure for non-group tasks or development of general techniques

- **Open Question 3**: Can interactive feature learning dynamics for multi-layer networks be fully characterized?
  - Why unresolved: Paper sketches multi-layer extension but leaves rigorous analysis of layer-wise interactions unresolved
  - What would resolve: Theorems characterizing how learned features at layer l affect gradient structure at layer l-1

## Limitations

- Theoretical framework relies on specific mathematical assumptions (2-layer networks, weight decay, quadratic activation) that may not generalize to practical deep architectures
- Group representation theory approach requires strong structural assumptions about input data that may not hold for arbitrary tasks
- Real-world grokking behavior in deep architectures (like LLMs) remains incompletely explained

## Confidence

- **High Confidence**: Three-stage learning dynamics mechanism and backpropagated gradient GF structure
- **Medium Confidence**: Energy function E characterization and local maxima for group arithmetic tasks
- **Medium Confidence**: Sample size phase transition scaling laws

## Next Checks

1. **Cross-architecture validation**: Test whether three-stage Li2 dynamics emerge in deeper networks (3+ layers) or transformers on group arithmetic tasks
2. **Activation function generalization**: Verify energy function E and local maxima characterization for activations beyond quadratic and ReLU (GeLU, Swish)
3. **Beyond group tasks**: Apply framework to non-group structured tasks (modular multiplication, natural language) to test universality of energy function approach and phase transition behavior