---
ver: rpa2
title: Can Constructions "SCAN" Compositionality ?
arxiv_id: '2509.20074'
source_url: https://arxiv.org/abs/2509.20074
tags:
- compositional
- data
- inproceedings
- training
- jump
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of compositionality in sequence-to-sequence
  models, which struggle to generalize systematically when faced with novel combinations
  of familiar elements. The authors propose that this limitation stems from the models'
  failure to internalize constructions - conventionalized form-meaning pairings that
  enable productive recombination.
---

# Can Constructions "SCAN" Compositionality ?

## Quick Facts
- arXiv ID: 2509.20074
- Source URL: https://arxiv.org/abs/2509.20074
- Authors: Ganesh Katrapati; Manish Shrivastava
- Reference count: 18
- The paper proposes unsupervised pseudo-construction mining that achieves 47.8% accuracy on ADD JUMP and 20.3% on AROUND RIGHT splits without architectural changes.

## Executive Summary
This paper addresses systematic compositionality in sequence-to-sequence models by proposing that their limitations stem from failing to internalize constructions - conventionalized form-meaning pairings. The authors introduce an unsupervised method for mining pseudo-constructions as variable-slot templates from training data, which enables models to recombine familiar elements productively. Applied to the SCAN dataset, their approach yields significant improvements on out-of-distribution splits, demonstrating both strong generalization and data efficiency without requiring architectural modifications.

## Method Summary
The method extracts recurring structural patterns as variable-slot templates from training data through unsupervised mining. It identifies token spans (up to 4 tokens), generates masked variants with slots, and ranks candidates by probability and alignment scores. Beam decoding segments input sentences into optimal pseudo-construction sequences. Slots are replaced with indexed tokens during training, and a bidirectional lexicon maps primitive mappings to targets. At inference, predicted slot tokens are swapped back using saved mappings, enabling compositional generalization without architectural changes.

## Key Results
- Achieves 47.8% accuracy on ADD JUMP split with no architectural changes
- Reaches 20.3% accuracy on harder AROUND RIGHT split
- Demonstrates strong data efficiency: 40.7% accuracy with only 39% of training data on ADD JUMP

## Why This Works (Mechanism)

### Mechanism 1
Extracting recurring structural patterns as variable-slot templates enables compositional generalization without architectural changes. The method mines pseudo-constructions by extracting token spans (≤4 tokens) from training data, generating masked variants with slots (e.g., "around twice" → "W1 around twice"), then ranking candidates by probability. Beam decoding segments input sentences into optimal sequences of pseudo-constructions. Core assumption: recurring structural patterns in source data correspond to compositional generalization points; masking variable positions preserves transferable structure. Evidence: [abstract] shows accuracy rises to 47.8% on ADD JUMP and 20.3% on AROUND RIGHT without architectural changes. Break condition: If novel test combinations require structural patterns absent from training data, mining cannot recover them.

### Mechanism 2
Penalizing pseudo-constructions with inconsistent target mappings improves slot quality. A misalignment score MS(P) computes the average length difference between target sequences sharing the same source pattern. Patterns where different slot fillers produce wildly different targets receive high penalties; patterns with consistent target structure receive low scores and are preferred during beam search. Core assumption: aligned source-target patterns indicate genuine compositional structure rather than surface coincidences. Evidence: [section 4.1] explicitly defines MS(P) = (1/|S(P)|) Σ|len(ti) − len(tj)| where (ti, tj) are targets of nearest-neighbor sources. Example: "look around left" produces consistent targets vs. "look opposite left" with variable targets—latter discouraged. Break condition: If target variability reflects legitimate semantic differences rather than poor pattern quality, the penalty may exclude valid constructions.

### Mechanism 3
Slot token abstraction at training time creates reusable representations that transfer to unseen combinations. After segmentation, slots are replaced with indexed tokens (W1, W2...). A bidirectional lexicon maps singleton rules (e.g., "jump" → "I JUMP") so target tokens are replaced with corresponding slot tokens during training. At inference, predicted slot tokens are swapped back using saved mappings. Core assumption: Models learn slot-filling operations better than memorizing surface sequences; the lexicon captures primitive mappings transferable to novel contexts. Evidence: [section 6] shows data efficiency results: 40.7% accuracy with only 39% of training data on ADD JUMP. Break condition: If test cases require slot fillers unseen in the lexicon, the model cannot map them correctly.

## Foundational Learning

- **Construction Grammar (CxG)**
  - Why needed here: The paper grounds its approach in CxG's notion of "conventionalized form-meaning pairs" (Goldberg 1995). Understanding that constructions can be partially filled (schemata with slots) vs. fully fossilized (idioms) clarifies why pseudo-constructions target productive patterns.
  - Quick check question: Can you distinguish why "kick the bucket" resists modification while "the Xer the Yer" permits slot-filling?

- **Beam Search Decoding**
  - Why needed here: The method uses beam search to segment sentences into optimal pseudo-construction sequences. Without understanding beam search (maintaining top-k hypotheses, scoring, pruning), the extraction pipeline is opaque.
  - Quick check question: Given candidates ["walk left", "W1 left"] with scores 0.3 and 0.7, which does beam search prefer?

- **Sequence-to-Sequence Transformer Basics**
  - Why needed here: The base model is a standard encoder-decoder transformer (4 layers, 4 heads, 256 dim). The innovation is input preprocessing, not architecture—distinguishing this from approaches requiring structural modifications.
  - Quick check question: What remains unchanged between baseline and proposed method—the model or the training data representation?

## Architecture Onboarding

- **Component map:**
  Training Source → Span Extraction → Masked Candidates → Alignment Scoring → Beam Segmentation → Slot Token Replacement → Transformer Training → Inference → Slot De-substitution → Output

- **Critical path:** Quality of pseudo-construction mining (extraction + alignment filtering) determines generalization capacity. Poor patterns propagate through training.

- **Design tradeoffs:**
  - Simpler splits (ADD JUMP) show strong data efficiency; harder splits (AROUND RIGHT with nested constructions) require more data
  - Span length limit (4 tokens) balances pattern granularity vs. overfitting to surface n-grams
  - Misalignment penalty may reject valid patterns with legitimate target variability

- **Failure signatures:**
  - Section 5 notes: "turn around right" and "walk around right" appear similar but produce different outputs—masking "turn" causes confusion
  - AROUND RIGHT accuracy (20.3%) far below ADD JUMP (47.8%) indicates nested constructions resist current mining approach
  - Data efficiency plateaus (10.2% at 60% data for AROUND RIGHT vs. 40.7% at 39% for ADD JUMP)

- **First 3 experiments:**
  1. **Baseline replication:** Train standard transformer on raw SCAN without preprocessing; verify baseline matches reported ~1-15% on target splits to confirm experimental setup
  2. **Ablation on alignment penalty:** Disable MS(P) scoring; train with pseudo-constructions selected only by frequency/probability to isolate alignment contribution
  3. **Span length sensitivity:** Vary max span length (2, 4, 6 tokens); measure impact on both splits to identify optimal granularity and potential over-extraction

## Open Questions the Paper Calls Out
None

## Limitations
- The method struggles with nested constructions (AROUND RIGHT at 20.3% vs 47.8% on ADD JUMP)
- Cannot generalize to slot fillers unseen in training lexicon
- Several implementation details unspecified (beam width, scoring function, scheduler parameters)

## Confidence

**High Confidence (★★★):** Core mechanism of mining variable-slot templates and using them to preprocess input sequences is sound and well-demonstrated. Data efficiency results on ADD JUMP (40.7% accuracy with 39% of training data) are robust.

**Medium Confidence (★★☆):** Specific accuracy numbers on both splits (47.8% on ADD JUMP, 20.3% on AROUND RIGHT) are well-documented but depend on unspecified implementation details.

**Low Confidence (★☆☆):** Generalization claims beyond SCAN are largely speculative. Success on SCAN's constrained nature doesn't necessarily translate to more complex language tasks.

## Next Checks
1. **Implement baseline replication with exact hyperparameters**: Recreate the standard transformer baseline on SCAN using the specified architecture (4-layer, 4-head, embedding=256) in JoeyNMT with 30 epochs and NOAM scheduler. Verify baseline accuracy matches reported values (~1-15%) to confirm experimental setup before testing the proposed method.

2. **Ablation study on alignment penalty sensitivity**: Disable the misalignment score (MS) component and train with pseudo-constructions selected only by frequency/probability. Compare results on both splits to isolate the contribution of alignment filtering. Vary the MS penalty weight to identify optimal thresholds.

3. **Cross-split construction transfer validation**: Train on the AROUND RIGHT split but evaluate on ADD JUMP (and vice versa) to test whether learned constructions transfer across compositional challenges. This reveals whether the method learns general compositional strategies versus split-specific patterns.