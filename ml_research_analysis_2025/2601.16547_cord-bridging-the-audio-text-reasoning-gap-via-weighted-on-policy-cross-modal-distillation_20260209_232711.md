---
ver: rpa2
title: 'CORD: Bridging the Audio-Text Reasoning Gap via Weighted On-policy Cross-modal
  Distillation'
arxiv_id: '2601.16547'
source_url: https://arxiv.org/abs/2601.16547
tags:
- cord
- reasoning
- alignment
- zhang
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the degradation of knowledge and reasoning
  capabilities in Large Audio Language Models (LALMs) compared to their text-based
  counterparts, hypothesizing that this stems from ineffective bridging of the acoustic-semantic
  gap. The authors propose CORD, a unified alignment framework that performs online
  cross-modal self-distillation without external teachers.
---

# CORD: Bridging the Audio-Text Reasoning Gap via Weighted On-policy Cross-modal Distillation

## Quick Facts
- **arXiv ID**: 2601.16547
- **Source URL**: https://arxiv.org/abs/2601.16547
- **Reference count**: 11
- **Key outcome**: Reduces audio-text performance gap by average 41.6% (Qwen2-Audio-7B) and 44.8% (Step-Audio2-mini) across reasoning benchmarks

## Executive Summary
This paper addresses the degradation of knowledge and reasoning capabilities in Large Audio Language Models (LALMs) compared to their text-based counterparts, hypothesizing that this stems from ineffective bridging of the acoustic-semantic gap. The authors propose CORD, a unified alignment framework that performs online cross-modal self-distillation without external teachers. CORD aligns audio-conditioned reasoning with text-conditioned behavior within a single model using two complementary objectives: token-level alignment with importance-aware and position-aware reverse KL divergence, and sequence-level alignment via judge-based global reward optimized through Group Relative Policy Optimization (GRPO). The method significantly reduces the audio-text performance gap across multiple reasoning benchmarks, demonstrating strong data efficiency and effectiveness in bridging cross-modal reasoning disparities.

## Method Summary
CORD is a unified alignment framework that bridges the audio-text reasoning gap in LALMs through on-policy cross-modal self-distillation. The method operates within a single model, using its own text-conditioned behavior as the target for audio-conditioned reasoning. CORD employs two complementary objectives: token-level alignment using weighted reverse KL divergence that emphasizes high-divergence and early-position tokens, and sequence-level alignment via judge-based global rewards optimized through GRPO. The framework is trained on paired audio-text data created by converting text questions to semantically equivalent audio using TTS. During training, the model samples rollouts from its current audio-conditioned policy and aligns these with text-conditioned behavior at both token and sequence levels, without requiring external teacher models.

## Key Results
- Reduces audio-text gap by average 41.6% on Qwen2-Audio-7B-Instruct and 44.8% on Step-Audio2-mini
- Maintains auxiliary audio capabilities (MMAU benchmark) while improving reasoning performance
- Demonstrates stronger performance gains on reasoning-heavy benchmarks (MMSU, OBQA) compared to GSM8K
- Ablation studies show both token-level and sequence-level alignment are necessary for stable training

## Why This Works (Mechanism)

### Mechanism 1: On-policy Cross-modal Self-Distillation
The model samples rollouts from its current audio-conditioned policy, comparing the audio-conditioned token distribution against the text-conditioned distribution over the same prefix using reverse KL divergence. This provides supervision at states the model actually encounters during inference, correcting modality-specific errors that accumulate during audio decoding.

### Mechanism 2: Importance-aware and Position-aware Token Weighting
Two-dimensional weighting scheme amplifies critical tokens: (1) Top-K KL-based weighting (α=2) amplifies ~20 tokens with highest cross-modal divergence, and (2) Sequential decay weighting (β=2) assigns higher weights to early tokens that disproportionately determine reasoning outcomes. The final weight combines both multiplicatively.

### Mechanism 3: Judge-based Sequence-level Alignment via GRPO
For each audio input, sample N=4 rollouts and use a judge model to evaluate whether each audio-conditioned output is semantically consistent with the text-conditioned reference. GRPO computes relative advantages against the group mean, optimizing to increase likelihood of higher-reward trajectories and ensuring global reasoning consistency beyond local token alignment.

## Foundational Learning

- **Concept**: Reverse KL vs Forward KL Divergence
  - Why needed: CORD uses reverse KL to emphasize high-probability tokens under text distribution, encouraging audio policy to "cover" teacher's modes rather than mode-averaging
  - Quick check: Given teacher p_text concentrated on ["A", "B"] and student p_audio spread over ["A", "B", "C", "D"], which KL direction penalizes student's failure to commit to high-probability tokens?

- **Concept**: On-policy vs Off-policy Distillation
  - Why needed: CORD uses student-generated trajectories to provide supervision at states the student actually visits, avoiding distribution mismatch
  - Quick check: If teacher always generates "The answer is A" but student under audio conditioning generates "The answer is...", which approach provides supervision at actual decision point?

- **Concept**: Group Relative Policy Optimization (GRPO)
  - Why needed: GRPO provides stable RL-style optimization by computing advantages relative to a group of sampled trajectories, avoiding need for separate value function
  - Quick check: Why might comparing trajectory's reward to group average improve training stability over absolute baseline?

## Architecture Onboarding

- **Component map**: Audio Encoder -> Modality Projector -> LLM Backbone -> Judge Model; TTS Module (for data creation)
- **Critical path**: 1) Text questions → TTS → semantically equivalent (audio, text) pairs, 2) Sample audio-conditioned rollout, 3) Generate text-conditioned reference, 4) Compute weighted reverse KL (token-level), 5) Sample N=4 rollouts for GRPO with judge rewards, 6) Joint optimization of L_CORD = L_tok + L_seq
- **Design tradeoffs**: Judge model quality vs training scalability (binary judge simpler but sparse rewards); group size N=4 (larger N better advantages but more compute); weighting intensity α=β=2 (bell-shaped sensitivity)
- **Failure signatures**: GRPO-only collapse below baseline after ~1000 steps; forward KL baseline loses auxiliary audio capabilities; uniform KL dilution fails to correct critical misalignments
- **First 3 experiments**: 1) KL distribution analysis on MMSU to verify heavy-tailed distribution and early-token concentration, 2) Ablation by component (GRPO only, GRPO+OPD, Full CORD) to confirm OPD prevents collapse, 3) Cross-domain transfer test (train on NuminaMath, evaluate on MMSU/OBQA/GSM8K) to verify gains transfer beyond training domain

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does CORD's cross-modal alignment effectiveness scale with training data diversity beyond math-only corpora?
- **Basis**: The authors state: "We expect that incorporating more diverse instruction data (e.g., general QA, knowledge-intensive reasoning, and non-speech acoustic tasks) would further strengthen cross-modal generalization, which we leave for future work."
- **Why unresolved**: CORD is trained exclusively on NuminaMath; its behavior with diverse domains, acoustic event types, or non-English speech remains untested.
- **What evidence would resolve it**: Experiments applying CORD to multi-domain training datasets and measuring transfer to held-out benchmarks across reasoning types and audio categories.

### Open Question 2
- **Question**: What is the relationship between base model text-conditioned reasoning quality and CORD's gap reduction effectiveness?
- **Basis**: The paper observes Step-Audio2-mini achieves larger gap reductions than Qwen2-Audio (44.8% vs 41.6%) and suggests "CORD naturally scales with the quality of the base LALM," but this relationship is not systematically investigated.
- **Why unresolved**: Only two backbone models tested; no controlled experiments isolating text-teacher quality as an independent variable.
- **What evidence would resolve it**: Controlled experiments with model families of varying sizes or capabilities, measuring how teacher quality modulates CORD gains.

### Open Question 3
- **Question**: How robust is CORD to judge model errors or misalignment with target evaluation criteria?
- **Basis**: The sequence-level GRPO objective depends entirely on a binary judge reward J(y, ŷ), but no ablation analyzes how judge inaccuracies propagate through optimization or affect final performance.
- **Why unresolved**: The judge's high self-evaluation accuracy (>99%) is reported, but sensitivity to judge quality is not examined.
- **What evidence would resolve it**: Ablations with synthetic judges of controlled accuracy rates, or analysis of training dynamics when judge signals are corrupted.

### Open Question 4
- **Question**: Does the optimal weighting intensity (α=β=2.0) generalize across different model architectures and task domains?
- **Basis**: Figure 4 shows α=β=2.0 is optimal for Qwen2-Audio on MMSU/OBQA/GSM8K, but this single-model, single-configuration finding may not transfer to other LALMs or task distributions.
- **Why unresolved**: No cross-model validation of hyperparameter sensitivity; the bell-shaped curve may shift with different token divergence distributions.
- **What evidence would resolve it**: Hyperparameter sweeps on additional backbones (e.g., Step-Audio) and task categories to validate generalization of the 2.0 optimum.

## Limitations
- **Opaque judge model**: Critical dependency on unspecified judge model (described as "distilled from proprietary models" with "over 99% accuracy") that cannot be independently verified
- **Text-conditioned target assumption**: Assumes text-conditioned behavior represents optimal reasoning, without validation that this holds across domains where audio provides additional information
- **Single-domain training**: Only trained on NuminaMath dataset, limiting assessment of cross-domain generalization capabilities

## Confidence
- **High Confidence**: On-policy cross-modal self-distillation and importance-aware/position-aware token weighting are well-supported by empirical evidence and ablation studies
- **Medium Confidence**: Sequence-level alignment via GRPO with judge-based rewards shows clear benefits when combined with token-level alignment, but dependency on unspecified judge model reduces confidence
- **Low Confidence**: Claims about "data-efficiency" and "significant reduction in audio-text gap" across multiple domains are supported by results but lack analysis of performance across different data scales or judge quality variations

## Next Checks
1. **Judge Model Validation**: Implement transparent judge model (e.g., preference-based reward model trained on human-annotated audio-text equivalence pairs) and retrain CORD to assess sensitivity to judge quality and verify binary classification sufficiency
2. **Cross-Domain Generalization Analysis**: Test CORD on reasoning tasks where audio provides non-textual information (tone, emotion, environmental context) to determine whether text-conditioned target is always appropriate
3. **On-policy vs Off-policy Trajectory Distribution Analysis**: Systematically compare token-level KL divergence distributions between on-policy rollouts and off-policy teacher-generated trajectories to quantify reduction in distribution mismatch and correlate with reasoning performance improvements