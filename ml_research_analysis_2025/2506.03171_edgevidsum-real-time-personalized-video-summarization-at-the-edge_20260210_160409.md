---
ver: rpa2
title: 'EdgeVidSum: Real-Time Personalized Video Summarization at the Edge'
arxiv_id: '2506.03171'
source_url: https://arxiv.org/abs/2506.03171
tags:
- video
- summarization
- while
- personalized
- ieee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EdgeVidSum addresses the challenge of real-time personalized video
  summarization on edge devices by introducing a thumbnail-based approach that processes
  lightweight thumbnail containers instead of full-resolution frames, achieving a
  97.8% reduction in data processing volume. The system employs a hierarchical analysis
  framework using a lightweight 2D CNN with a Triplet Attention Module to identify
  user-preferred content, generating personalized fast-forward summaries while preserving
  temporal continuity.
---

# EdgeVidSum: Real-Time Personalized Video Summarization at the Edge

## Quick Facts
- arXiv ID: 2506.03171
- Source URL: https://arxiv.org/abs/2506.03171
- Authors: Ghulam Mujtaba; Eun-Seok Ryu
- Reference count: 34
- Key outcome: Achieves 97.8% data reduction and 2-5 second processing for 90-minute videos using thumbnail-based approach on Jetson Nano

## Executive Summary
EdgeVidSum introduces a real-time personalized video summarization system that operates on edge devices by processing lightweight thumbnail containers instead of full-resolution frames. The system achieves a 97.8% reduction in data processing volume while maintaining semantic relevance for user-preferred content detection. Running on a Jetson Nano, it delivers personalized fast-forward summaries with processing times of 2-5 seconds for 90-minute videos, preserving temporal continuity and user privacy through complete on-device processing.

## Method Summary
The system uses pre-generated 160×90 pixel thumbnail containers (4KB each) extracted from long-form videos, reducing data volume by 97.8% compared to frame-by-frame processing. A lightweight 2D CNN with Triplet Attention Module (TAM) analyzes thumbnails to identify user-preferred content across categories like sports and movies. The architecture performs all preference analysis and summarization locally on edge devices, requesting only specific video segments from the provider for playback. The method preserves temporal continuity through variable-speed playback rather than discrete frame selection, maintaining narrative coherence while adapting to user preferences.

## Key Results
- Achieves 97.8% reduction in data processing volume compared to traditional frame-by-frame methods
- Processes 90-minute videos in 2-5 seconds on Jetson Nano, maintaining real-time performance
- Improves classification accuracy by 4.2% using Triplet Attention Module while adding only 0.3M parameters

## Why This Works (Mechanism)

### Mechanism 1: Thumbnail Container-Based Data Reduction
- Claim: Processing lightweight thumbnail containers instead of full-resolution frames enables real-time summarization on edge devices.
- Mechanism: Uses pre-generated 160×90 pixel thumbnails (4KB each) rather than full frames (1-3MB), reducing data volume by 97.8% and processing quantity by ~30× (6,734 thumbnails vs. 202,036 frames for a 2-hour video). The system transfers only thumbnail containers to the edge device, where a lightweight CNN performs all analysis.
- Core assumption: Thumbnails preserve sufficient semantic information (scene type, objects, actions) to identify user-preferred content accurately.
- Evidence anchors: [abstract] "uses thumbnail containers to significantly reduce computational complexity without sacrificing semantic relevance"; [section 3.1] "ach