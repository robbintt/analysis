---
ver: rpa2
title: Direct Semantic Communication Between Large Language Models via Vector Translation
arxiv_id: '2511.03945'
source_url: https://arxiv.org/abs/2511.03945
tags:
- semantic
- translation
- injection
- vector
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of semantic communication between
  large language models (LLMs) by proposing a vector translation approach that bypasses
  text-based token exchange. The authors develop a dual-encoder translator to learn
  bidirectional mappings between the latent representation spaces of two distinct
  LLMs (LLaMA-2-7B and Mistral-7B-Instruct), achieving an average cosine alignment
  of 0.538.
---

# Direct Semantic Communication Between Large Language Models via Vector Translation

## Quick Facts
- arXiv ID: 2511.03945
- Source URL: https://arxiv.org/abs/2511.03945
- Reference count: 9
- One-line primary result: Vector translation approach achieves 0.538 cosine alignment between LLaMA-2-7B and Mistral-7B-Instruct, enabling direct semantic communication between models without text-based token exchange.

## Executive Summary
This paper introduces a novel approach for direct semantic communication between large language models by translating their latent representations rather than exchanging text tokens. The authors develop a dual-encoder translator that learns bidirectional mappings between the 4096D hidden states of LLaMA-2-7B and Mistral-7B-Instruct models, achieving an average cosine alignment of 0.538. A conservative 30% vector injection mechanism at the final transformer layers steers target model generation without destabilizing logits. The approach demonstrates that cross-model latent communication is feasible while preserving computational stability, though with moderate semantic fidelity.

## Method Summary
The method employs a dual-encoder translator that learns bidirectional mappings between the latent representation spaces of two LLMs. The translator consists of a semantic feature extractor (reducing 4096D to 512D), cross-domain alignment with multi-head attention, and a target space generator (expanding back to 4096D). Training uses a composite loss function combining translation, cycle consistency, InfoNCE contrastive, and distribution preservation losses. Translated vectors are injected at 30% blending strength into the final transformer layers (-3, -2, -1) where high-level semantic processing occurs. The approach enables one model to influence another's generation by directly manipulating internal hidden states rather than through text-based communication.

## Key Results
- Dual-encoder translator achieves 0.538 average cosine alignment between LLaMA-2-7B and Mistral-7B-Instruct representation spaces
- 30% vector injection strength successfully steers target model generation without logit destabilization
- Bidirectional evaluation reveals 2.01:1 transfer asymmetry, with general-purpose models yielding more transferable representations than instruction-tuned variants

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Geometric Alignment Between Representation Spaces
- Claim: Learned linear-ish mappings can bridge semantically similar concepts across independently trained LLMs with different tokenizers and training objectives.
- Core assumption: Semantic content occupies comparable geometric regions across transformer models despite architectural differences.
- Evidence: 0.538 average cosine alignment achieved between LLaMA-2-7B and Mistral-7B-Instruct using composite loss function.

### Mechanism 2: Conservative Residual-Stream Blending at Semantic Layers
- Claim: Injecting translated vectors at 30% blend ratio into final transformer layers steers generation without logit destabilization.
- Core assumption: Final layers encode task-relevant semantics while earlier layers handle syntax/low-level processing.
- Evidence: Successful steering of target model generation while maintaining computational stability at 30% injection strength.

### Mechanism 3: Asymmetric Transfer from General-Purpose to Instruction-Tuned Models
- Claim: General-purpose pretrained models yield representations that transfer more readily than instruction-tuned variants (2.01:1 ratio).
- Core assumption: Instruction tuning creates more idiosyncratic, less universal semantic manifolds.
- Evidence: Forward translation (LLaMA→Mistral) achieves 0.758 similarity vs. 0.375 reverse translation.

## Foundational Learning

- **Concept: Latent/Hidden-State Representations**
  - Why needed here: The entire method operates on 4096D internal vectors, not tokens.
  - Quick check question: Can you explain why the final token position's hidden state encodes "next-token prediction ready" semantics?

- **Concept: Contrastive Learning (InfoNCE)**
  - Why needed here: The translator uses InfoNCE loss to pull semantically paired representations together while pushing unpaired ones apart.
  - Quick check question: Given two batches of vectors from different models, how would InfoNCE create positive vs. negative pairs?

- **Concept: Residual Stream and Layer Targeting**
  - Why needed here: Injection mechanism modifies hidden states at specific layers; understanding residual stream architecture explains why blending works without architectural changes.
  - Quick check question: Why might injecting at layer 0 produce different effects than injecting at layer -1?

## Architecture Onboarding

- **Component map:**
  Source LLM (4096D hidden) → Semantic Feature Extractor (→512D) → Cross-Domain Alignment (8-head attention, 512D) → Target Space Generator (→4096D) → Conservative Injection (α=0.3, layers -3 to -1, final 3 tokens) → Target LLM generation

- **Critical path:** Translator training quality → cosine alignment → injection effectiveness. If alignment < 0.4, semantic transfer weakens substantially.

- **Design tradeoffs:**
  - Higher α → stronger semantic influence but risk of destabilization
  - Earlier layer injection → more pervasive influence but potential syntactic disruption
  - Cycle consistency weight → smoother bidirectional mappings but slower convergence

- **Failure signatures:**
  - Random/low cosine similarity (< 0.3): Translator failed to learn meaningful mappings
  - Degraded precision in numerical/code tasks: Injection strength too high or wrong layer targeting
  - No semantic shift in outputs: Translation working but injection not activating

- **First 3 experiments:**
  1. Reproduce alignment metrics: Train translator on 5 domain pairs, verify 0.538±0.081 average cosine similarity.
  2. Ablate injection strength: Test α ∈ {0.1, 0.3, 0.5, 0.7} on held-out domain.
  3. Swap source/target roles: Test Mistral→LLaMA translation explicitly.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can this translation framework effectively bridge models with different hidden dimensions (e.g., 1024D vs. 4096D) without relying on simple projection layers?
- Basis in paper: Section 7 states current evaluation focuses on models with identical 4096D dimensionality.
- Why unresolved: Current dual-encoder reduces dimensions to a 512D bottleneck, but it is unclear if this sufficiently preserves semantics when up-scaling to vastly larger latent spaces.
- What evidence would resolve it: Successful translation and injection results between models with distinctly different native dimensionalities.

### Open Question 2
- Question: Does the observed 2.01:1 transfer asymmetry persist or invert when applying vector translation to models significantly larger than the 7B parameter scale?
- Basis in paper: Section 7 notes testing was restricted to 7B parameter models.
- Why unresolved: It remains uncertain if the "transferability" of general-purpose models scales linearly or if instruction-tuned representations stabilize in larger models.
- What evidence would resolve it: Bidirectional translation experiments on 70B+ parameter models.

### Open Question 3
- Question: Can the injection blending strength be optimized above the conservative 30% threshold for specific tasks to maximize semantic transfer without causing instability?
- Basis in paper: Section 7 states conservative parameters may limit semantic transfer potential.
- Why unresolved: The authors used a fixed 30% blend to ensure stability, but the upper bounds of injection tolerance for different reasoning tasks are unmapped.
- What evidence would resolve it: A hyperparameter sweep analyzing the trade-off between semantic alignment scores and logit stability across varying blend ratios.

## Limitations

- The 0.538 average cosine alignment represents only moderate semantic preservation, suggesting substantial information loss during translation.
- The conservative 30% injection strength may severely constrain practical utility, limiting the approach to coarse-grained semantic steering.
- The 2.01:1 transfer asymmetry finding is based on a single model pairing and may not generalize across different architectures or model sizes.

## Confidence

**High Confidence (3-4)**: Basic feasibility of cross-model latent communication is well-supported with clear quantitative metrics and reproducible experimental setups.

**Medium Confidence (2-3)**: The claim that 30% injection strength optimally balances semantic transfer and output quality is supported by ablation studies but limited by narrow testing range.

**Low Confidence (1-2)**: The generalizability of the 2.01:1 transfer asymmetry to other model pairs remains uncertain without replication across multiple model combinations.

## Next Checks

1. **Cross-Architecture Generalization Test**: Train translators between model pairs spanning different families (e.g., LLaMA-Mistral, GPT-Neo-OPT, Falcon-LLaMA) and different sizes (7B, 13B, 33B). Measure whether the 0.538±0.081 alignment range holds across combinations.

2. **Injection Strength Optimization Study**: Systematically explore injection strengths from 0.1 to 0.9 in 0.1 increments across three task categories: factual question answering, creative writing, and code generation. Identify the Pareto frontier where semantic transfer is maximized while task performance loss remains below 5%.

3. **Multilingual and Multi-Modal Extension**: Train translators between English and non-English LLMs (e.g., LLaMA-English and LLaMA-French) to test cross-linguistic semantic transfer. Additionally, extend the approach to multi-modal models by translating between text and image latent representations.