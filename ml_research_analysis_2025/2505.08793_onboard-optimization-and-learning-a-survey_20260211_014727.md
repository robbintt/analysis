---
ver: rpa2
title: 'Onboard Optimization and Learning: A Survey'
arxiv_id: '2505.08793'
source_url: https://arxiv.org/abs/2505.08793
tags:
- learning
- edge
- inference
- onboard
- ieee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically reviews optimization and learning techniques
  for deploying deep learning models on resource-constrained edge devices. It categorizes
  approaches across model compression, efficient inference, decentralized learning,
  security/privacy, and advanced deployment topics.
---

# Onboard Optimization and Learning: A Survey

## Quick Facts
- **arXiv ID**: 2505.08793
- **Source URL**: https://arxiv.org/abs/2505.08793
- **Reference count**: 40
- **Key outcome**: Survey of optimization and learning techniques for deploying deep learning models on resource-constrained edge devices

## Executive Summary
This survey systematically reviews optimization and learning techniques for deploying deep learning models on resource-constrained edge devices. It categorizes approaches across model compression, efficient inference, decentralized learning, security/privacy, and advanced deployment topics. Key findings include: structured pruning achieving up to 94% MAC reduction with 1-4% accuracy loss; post-training quantization reducing memory by up to 80% and latency by 23%; knowledge distillation improving accuracy from 94.3% to 97.9%; hardware-aware NAS delivering 35% speedup on Jetson Nano; federated learning reducing communication by 55% with 11% accuracy improvement; split learning reducing memory footprint by 95.5% and training time by 94.8%; and quantization-aware continual learning achieving 4× memory reduction with 0.26% accuracy loss. The survey highlights that successful onboard learning requires integrated co-design across compression, inference, learning, privacy, and hardware constraints rather than treating them as separate optimization problems.

## Method Summary
The authors conducted a systematic literature review using PRISMA flow methodology across 5 databases (WoS, IEEE Xplore, ACM Digital Library, ArXiv, SpringerLink) with 4 specific search query patterns for the 2016-2025 period. Starting with 970 records, they applied screening filters to exclude non-deep learning, non-edge, or cloud-based works, then quality filters (Q1 journals/CORE conferences), resulting in 293 final studies. The survey extracts quantitative trade-offs between accuracy, efficiency, and deployment feasibility across different optimization techniques.

## Key Results
- Structured pruning can reduce MAC operations by up to 94% with only 1-4% accuracy loss on edge hardware
- Post-training quantization achieves up to 80% memory reduction and 23% latency improvement while maintaining accuracy above 76.7% on ImageNet
- Federated learning with knowledge distillation reduces communication costs by 55-99% while improving accuracy by up to 11%
- Hardware-aware NAS delivers 35% inference speedup on Jetson Nano compared to non-optimized models

## Why This Works (Mechanism)

### Mechanism 1: Structured Pruning for Computational Reduction
- Claim: Structured pruning can reduce multiply-accumulate (MAC) operations by up to 94% with only 1-4% accuracy loss on edge hardware.
- Mechanism: By removing entire channels, filters, or neurons based on importance scores, structured pruning produces compact, hardware-friendly models that align with standard deep learning frameworks and accelerators. This eliminates redundant computation without introducing irregular sparsity patterns that require specialized hardware support.
- Core assumption: The removed structures are truly redundant and do not encode critical task-relevant features. This assumes the importance scoring metric accurately reflects each structure's contribution to model performance.
- Evidence anchors:
  - [Section II.A] "Structured pruning methods on ResNet architectures demonstrate 59.8-74.4% computational reduction with 1.0-1.12% accuracy loss while achieving 1.84-2.93× inference speedup on edge hardware including NVIDIA Jetson TX2, Snapdragon 855, and Google TPU."
  - [Section IV.A] Federated learning with adaptive pruning (PruneFL) dynamically resizes models to balance communication and computation overhead while maintaining accuracy.
  - [Corpus] "Neural Network Quantization for Microcontrollers" survey confirms pruning as a foundational compression technique for resource-constrained edge devices.
- Break condition: Pruning beyond the redundancy threshold leads to catastrophic accuracy degradation (>5% loss). This occurs when important features are mistakenly pruned, often due to poor importance metrics or dataset-specific sensitivity not captured during pruning.

### Mechanism 2: Hardware-Aware Quantization for Memory-Latency Trade-offs
- Claim: Post-training quantization (PTQ) can reduce memory footprint by up to 80% and latency by approximately 23% while maintaining accuracy above 76.7% on ImageNet.
- Mechanism: PTQ maps high-precision floating-point tensors (FP32) to low-bit integer formats (INT8/INT4) using affine transformation with scale and zero-point parameters. This reduces both storage requirements and enables energy-efficient integer arithmetic on edge hardware with dedicated NPU/DSP support.
- Core assumption: The quantization noise introduced by reduced precision does not exceed the model's inherent robustness. This assumes the model's activation and weight distributions remain within representable ranges of the target precision.
- Evidence anchors:
  - [Section II.B] "Reported results indicate that QAT attains between 4-12× compression and 2-10× improvement in inference throughput, while maintaining accuracy within 3% of full-precision models."
  - [Section VI.B] MLPerf Tiny benchmarking reveals that pre- and post-inference operations account for majority of energy consumption in quantized models, indicating deployment complexity beyond model conversion.
  - [Corpus] "From Tiny Machine Learning to Tiny Deep Learning" survey emphasizes quantization as critical for TinyDL deployment on microcontrollers with sub-256KB SRAM.
- Break condition: Quantization below 4-bit precision causes non-linear accuracy degradation as quantization noise exceeds gradient correction capacity. Dynamic range mismatch between calibration and deployment data distributions also breaks this mechanism.

### Mechanism 3: Federated Learning with Knowledge Distillation for Communication Efficiency
- Claim: Federated learning combined with knowledge distillation can reduce communication costs by 55-99% while improving accuracy by up to 11% compared to baselines.
- Mechanism: Federated learning enables collaborative training by exchanging only model updates (gradients or parameters) rather than raw data. Knowledge distillation transfers knowledge from a large teacher model to smaller student models via soft labels, preserving inter-class relationships. The combination allows devices to train compact models locally while benefiting from global knowledge.
- Core assumption: Client data distributions are sufficiently similar (moderate non-IID) such that local updates contribute meaningfully to global convergence. This assumes communication rounds are sufficient for knowledge transfer and that student models have adequate capacity to absorb teacher knowledge.
- Evidence anchors:
  - [Section II.C] "Itahara et al. demonstrated semi-supervised federated learning with knowledge distillation that deducted 99% communication cost while maintaining similar accuracy compared to the benchmark."
  - [Section IV.A] Decentralized FL with probabilistic peer-to-peer communication (CEDFL) achieves 11% accuracy improvement and 55% reduction in training completion time compared to deterministic baselines.
  - [Corpus] "Edge-Cloud Collaborative Computing" survey confirms FL as foundational for distributed intelligence with privacy preservation.
- Break condition: Extreme non-IID data distributions cause model divergence and catastrophic accuracy collapse (e.g., FedProx dropping to 13.32% accuracy under severe heterogeneity). Communication constraints limiting update frequency or compression artifacts destroying gradient information also break this mechanism.

## Foundational Learning

- **Pruning Fundamentals (Structured vs. Unstructured)**
  - Why needed here: Understanding pruning granularity determines deployment feasibility. Unstructured pruning achieves higher compression (93-98% sparsity) but requires specialized sparse computation support, while structured pruning (channel/filter removal) works on standard hardware with moderate compression (60-75%).
  - Quick check question: Can you explain why removing individual weights (unstructured) might not translate to speedup on standard GPUs/TPUs without sparse computation libraries?

- **Quantization-Training vs. Inference-Aware**
  - Why needed here: PTQ is fast but assumes deployment data matches calibration data; QAT integrates quantization into training for better accuracy but requires retraining. Understanding this trade-off is critical for selecting the right approach given edge constraints (memory, compute budget, available training data).
  - Quick check question: If your deployment data distribution differs significantly from calibration data, which quantization approach would you choose and why?

- **Knowledge Distillation Temperature Scheduling**
  - Why needed here: The temperature parameter (C) in softmax controls soft label "softness." Moderate values (C=2-3) preserve inter-class similarity while maintaining confidence, but extreme values degrade student performance. This is crucial for effectively training compact edge models.
  - Quick check question: What happens to knowledge transfer if you set temperature C=1 (hard labels) versus C=10 (uniform distribution)?

## Architecture Onboarding

- **Component map:**
  Input Data -> [Compression Layer] -> [Compressed Model] -> [Inference Engine] <-> [Partition Decision] -> [Local Inference] or [Offloading/Split] -> [Decentralized Learning Hub] -> [Security Layer] -> [Hardware Co-Design Interface] -> Optimized Edge Deployment

- **Critical path:**
  1. **Model compression first** - Reduces memory/computation baseline before other optimizations
  2. **Inference optimization second** - Partitioning and early exit decisions depend on compressed model characteristics
  3. **Decentralized learning configuration third** - Communication protocols (FL/SL) build upon compressed models
  4. **Security/privacy integration fourth** - Mechanisms must co-design with model structure and update protocols
  5. **Hardware mapping last** - Final deployment requires understanding all prior constraints

- **Design tradeoffs:**
  - **Accuracy vs. Efficiency**: Each 10% compression typically costs 0.5-2% accuracy; optimal operating point depends on application tolerance
  - **Communication vs. Computation**: More local computation (higher partition depth) reduces transmission but increases edge energy consumption
  - **Privacy vs. Performance**: Differential privacy noise reduces accuracy 3-27%; TEEs add 40% transmission latency but enable strong confidentiality
  - **Adaptability vs. Stability**: Continual learning with 4× memory reduction costs 0.26% accuracy loss; aggressive compression increases catastrophic forgetting risk

- **Failure signatures:**
  - **Accuracy collapse post-pruning**: Sudden >5% accuracy drop indicates pruning beyond redundancy threshold; requires recalibrating pruning ratios or switching to less aggressive method
  - **Latency spikes after partitioning**: 2×+ expected latency suggests partition point mismatch with hardware capabilities or network bandwidth overestimation
  - **Privacy leakage in FL**: Membership inference attack success >50% indicates insufficient DP noise or secure aggregation implementation flaws
  - **Catastrophic forgetting in CL**: Task accuracy drops >15% on previous tasks indicates insufficient replay/regularization for current compression level

- **First 3 experiments:**
  1. **Baseline compression profiling**: Apply PTQ (INT8) and structured pruning (30-50% ratio) to your target model on representative edge hardware (e.g., Jetson Nano/Raspberry Pi 4). Measure memory, latency, and accuracy degradation to establish compression operating region.
  2. **Inference partition analysis**: Profile layer-wise computation vs. communication costs under different network conditions (WiFi/LTE/3G). Identify optimal partition points and test early exit thresholds on validation data to quantify latency-accuracy trade-offs.
  3. **Decentralized learning pilot**: Implement federated averaging with 5-10 simulated edge clients using your compressed model. Test with both IID and non-IID (class-skewed) data distributions, measuring convergence speed, final accuracy, and communication volume per round to assess feasibility.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can resource-aware continual learning frameworks dynamically adjust model capacity, update frequency, and learning objectives based on real-time resource feedback under non-stationary conditions?
- Basis in paper: [explicit] The authors state that "Future systems must support learning under non-stationary conditions while respecting strict memory and energy constraints," calling for frameworks where these parameters are dynamically adjusted.
- Why unresolved: Existing continual learning methods struggle to balance long-term stability, memory overhead, and energy consumption, often treating learning as an extension of cloud training rather than a constrained optimization problem intrinsic to onboard operation.
- What evidence would resolve it: A learning framework deployed on a resource-constrained device (e.g., microcontroller) that successfully maintains accuracy while autonomously modulating update rates and model architecture in response to fluctuating energy and memory budgets.

### Open Question 2
- Question: How can adaptive protection mechanisms be co-designed with optimization and learning strategies to explicitly trade privacy, accuracy, and efficiency in response to mission context?
- Basis in paper: [explicit] The survey notes, "Future systems should explore adaptive protection mechanisms that explicitly trade privacy, accuracy, and efficiency in response to mission context and resource availability."
- Why unresolved: Current privacy-preserving methods impose static, often high computational overheads that struggle to balance security with the ultra-low latency demands of real-time onboard applications.
- What evidence would resolve it: An onboard system that dynamically modifies its privacy budget and noise injection levels during runtime to maintain a specified latency or energy cap without catastrophic accuracy degradation.

### Open Question 3
- Question: How can standardized evaluation protocols be established to capture system-level trade-offs—specifically extending beyond inference to include training-aware energy and latency measurements across heterogeneous hardware?
- Basis in paper: [explicit] The authors highlight that "The field also lacks standardized evaluation protocols that capture trade-offs between accuracy, energy, robustness, and latency" and specifically note that "standardized benchmarking must extend beyond inference to support phase-separated... measurement."
- Why unresolved: Existing benchmarks like MLPerf Tiny are inference-centric and conflate training-specific costs (e.g., gradient computation, model loading) with inference, failing to capture the full energy profile of on-device learning.
- What evidence would resolve it: A widely adopted benchmark suite offering reproducible, phase-separated measurements for training overhead and energy consumption across diverse edge platforms (e.g., MCUs, NPUs, FPGAs).

### Open Question 4
- Question: How can compression, inference, learning, and security be mathematically formulated as jointly constrained variables to ensure system stability under real-world variability?
- Basis in paper: [inferred] The conclusion asserts that "successful onboard learning requires integrated co-design across compression, inference, learning, privacy, and hardware constraints rather than treating them as separate optimization problems," noting that fragmentation leads to system fragility.
- Why unresolved: Current research predominantly optimizes these pillars in isolation (e.g., compression first, then security), which creates deployment risks where aggressive compression undermines privacy or learning stability.
- What evidence would resolve it: A unified optimization framework or compiler that automatically validates the interaction between compressed model parameters and security protocols, demonstrating stable performance under resource non-stationarity and distributional drift.

## Limitations
- Most reported quantitative results come from controlled experimental settings rather than real-world deployments, potentially overestimating achievable performance under actual edge constraints.
- The systematic literature review methodology likely missed emerging techniques published after the search cutoff or in non-indexed venues.
- The survey focuses primarily on individual optimization techniques rather than integrated co-design approaches, despite highlighting this as a key requirement.

## Confidence
- **High Confidence**: Model compression mechanisms (pruning/quantization/NAS) and their hardware impact are well-established with extensive experimental validation across multiple studies.
- **Medium Confidence**: Decentralized learning techniques show strong theoretical foundations but face significant practical challenges with non-IID data distributions and communication constraints.
- **Low Confidence**: Advanced topics like split learning and continual learning with compression are emerging areas with limited large-scale deployment evidence and mixed experimental results.

## Next Checks
1. **Hardware Benchmark Validation**: Implement the top 3 compression configurations (PTQ + structured pruning) on actual target hardware (Jetson Nano/Raspberry Pi) with representative models (ResNet-18, MobileNet) to verify claimed speedups and memory reductions.

2. **Non-IID Federated Learning Test**: Conduct controlled experiments with class-skewed data distributions (Dirichlet parameter α=0.1, 0.5, 1.0) to measure accuracy degradation and convergence behavior, comparing with the reported 11% improvement under moderate heterogeneity.

3. **Real-World Deployment Pilot**: Deploy a compressed model with federated learning on 5-10 physical edge devices in a realistic scenario (e.g., smart factory sensors or distributed camera systems) to validate communication overhead, privacy mechanisms, and accuracy retention under operational conditions.