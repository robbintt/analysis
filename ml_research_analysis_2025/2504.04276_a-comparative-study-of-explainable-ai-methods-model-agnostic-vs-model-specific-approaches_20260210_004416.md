---
ver: rpa2
title: 'A Comparative Study of Explainable AI Methods: Model-Agnostic vs. Model-Specific
  Approaches'
arxiv_id: '2504.04276'
source_url: https://arxiv.org/abs/2504.04276
tags:
- methods
- shap
- feature
- grad-cam
- backpropagation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compared model-agnostic (LIME, SHAP) and model-specific
  (Grad-CAM, Guided Backpropagation) XAI methods for interpreting ResNet50 image classifications
  across seven species. Each method revealed different aspects of model decision-making:
  model-agnostic techniques provided broader feature attribution across architectures,
  while model-specific approaches highlighted precise activation regions with greater
  computational efficiency.'
---

# A Comparative Study of Explainable AI Methods: Model-Agnostic vs. Model-Specific Approaches

## Quick Facts
- arXiv ID: 2504.04276
- Source URL: https://arxiv.org/abs/2504.04276
- Authors: Keerthi Devireddy
- Reference count: 12
- Key outcome: Model-agnostic (LIME, SHAP) and model-specific (Grad-CAM, Guided Backpropagation) XAI methods provide complementary interpretations of ResNet50 image classifications, with no single superior method; combining approaches offers most comprehensive understanding.

## Executive Summary
This study systematically compares model-agnostic (LIME, SHAP) and model-specific (Grad-CAM, Guided Backpropagation) XAI methods for interpreting ResNet50 image classifications across seven species. Each method reveals different aspects of model decision-making: model-agnostic techniques provide broader feature attribution across architectures, while model-specific approaches highlight precise activation regions with greater computational efficiency. The analysis found that LIME and SHAP show variable explanations across runs but work with any model, Grad-CAM provides class-discriminative localization but lacks fine-grained details, and Guided Backpropagation excels at edge detection but may be noisy. The study concludes that combining multiple XAI methods offers the most comprehensive understanding of complex models, particularly valuable in high-stakes domains requiring transparency.

## Method Summary
The study compares four XAI methods (LIME, SHAP, Grad-CAM, Guided Backpropagation) applied to pretrained ResNet50 for classifying images of seven species. Model-agnostic methods probe model behavior through input perturbations, while model-specific methods leverage internal CNN structures. The analysis evaluates qualitative visualization outputs showing feature importance regions, with no quantitative metrics defined. LIME generates perturbed input variants and fits local surrogate models, SHAP computes Shapley values from cooperative game theory, Grad-CAM computes gradient-weighted activation maps from convolutional features, and Guided Backpropagation modifies gradient flow to highlight edge information.

## Key Results
- Model-agnostic methods (LIME, SHAP) provide architecture-agnostic explanations but show variable outputs across runs
- Model-specific methods (Grad-CAM, Guided Backpropagation) offer computational efficiency and precise localization but are architecture-limited
- Grad-CAM provides class-discriminative heatmaps but lacks fine-grained detail resolution
- Guided Backpropagation excels at edge detection but produces potentially noisy visualizations
- No single method emerges as superior; combining approaches yields most comprehensive interpretability

## Why This Works (Mechanism)

### Mechanism 1: Perturbation-based Local Surrogate Approximation (LIME)
- Claim: Local linear approximations can capture feature importance for individual predictions by observing model response to input perturbations.
- Mechanism: LIME generates perturbed variants of input by modifying superpixel regions, passes them through the model, then fits an interpretable surrogate model g that minimizes L(f, g, πx) + Ω(g), where πx weights closer perturbations more heavily. The surrogate's coefficients reveal feature importance.
- Core assumption: The complex model's decision boundary is locally linear and can be approximated by a simpler interpretable model in the neighborhood of the input.
- Evidence anchors:
  - "LIME and SHAP showing variable explanations across runs but working with any model"
  - "LIME creates a local interpretable model g that approximates the complex function f in the vicinity of a given input x"
  - MUPAX paper mentions deterministic, model-agnostic approaches—contrasts with LIME's stochastic perturbation strategy
- Break condition: When decision boundaries are highly non-linear locally, or when perturbation strategy creates unrealistic samples that fall outside the data distribution.

### Mechanism 2: Cooperative Game Theory for Feature Attribution (SHAP)
- Claim: Shapley values from cooperative game theory provide theoretically grounded, consistent feature attribution by averaging marginal contributions across all feature coalitions.
- Mechanism: For each feature i, SHAP computes ϕi = Σ |S|!(|N|-|S|-1)!/|N|! × (f(S∪{i}) - f(S)), summing over all subsets S excluding i. This fairly distributes the prediction value among features based on their incremental contribution.
- Core assumption: Features can be meaningfully "removed" or "added" in coalition, and marginal contributions are the appropriate measure of importance.
- Evidence anchors:
  - "SHAP showing variable explanations across runs but working with any model"
  - "SHAP provides strong theoretical guarantees and is widely used to interpret deep learning predictions"
  - Limited direct corpus validation for SHAP's theoretical guarantees; neighboring papers focus on applications rather than theory verification
- Break condition: Exponential computational cost makes exact SHAP intractable for high-dimensional inputs (images); approximations may introduce error. Paper notes SHAP is "computationally intensive but highly reliable."

### Mechanism 3: Gradient-weighted Activation Mapping (Grad-CAM)
- Claim: Gradients of class scores with respect to convolutional feature maps can weight those maps to reveal spatially discriminative regions.
- Mechanism: Compute importance weights αkc = (1/Z)ΣiΣj(∂yc/∂Aijk) via global average pooling of gradients, then generate heatmap Lc = ReLU(Σk αkc Ak). Only positive influences are visualized.
- Core assumption: Gradient magnitude correlates with feature importance, and later convolutional layers capture semantically meaningful spatial information.
- Evidence anchors:
  - "Grad-CAM providing class-discriminative localization but lacking fine-grained details"
  - "Grad-CAM highlights image regions most relevant to a class prediction by computing the importance weights αkc"
  - Bioacoustics CNN explainability paper applies similar gradient-based XAI to spectrograms, suggesting cross-domain applicability of the mechanism
- Break condition: When the final convolutional layer has low spatial resolution, or when gradient signals suffer from saturation in deep networks. Paper notes Grad-CAM "lacks fine-grained details."

## Foundational Learning

- Concept: **Convolutional Feature Maps and Spatial Hierarchies**
  - Why needed here: Grad-CAM and Guided Backpropagation both operate on internal CNN activations; understanding how early layers capture edges/textures while deeper layers capture semantic concepts is essential for interpreting their outputs.
  - Quick check question: Given a ResNet50 architecture, which layer's feature maps would you expect to better localize a dog's face vs. its fur texture, and why?

- Concept: **Gradient Flow and Backpropagation in CNNs**
  - Why needed here: Grad-CAM requires computing ∂yc/∂Ak (gradients of class score w.r.t. feature maps); Guided Backpropagation modifies standard gradient flow by suppressing negative signals.
  - Quick check question: In Guided Backpropagation, what happens to gradients when passing through a ReLU activation during the backward pass, and how does this differ from standard backpropagation?

- Concept: **Perturbation-based vs. Gradient-based Explanation Paradigms**
  - Why needed here: The paper's central comparison contrasts perturbation methods (LIME/SHAP) that probe model behavior externally vs. gradient methods (Grad-CAM/Guided Backpropagation) that leverage internal structure.
  - Quick check question: If you needed to explain a proprietary model where you can only query predictions (no gradient access), which paradigm would apply, and what computational trade-off should you expect?

## Architecture Onboarding

- Component map:
  - ResNet50 backbone → outputs class logits and intermediate feature maps
  - LIME pipeline: Input image → superpixel segmentation → perturbation sampling → model queries → surrogate linear model training → coefficient visualization
  - SHAP pipeline: Input image → feature masking/coalition sampling → model queries → Shapley value computation → heatmap generation
  - Grad-CAM pipeline: Forward pass → extract target conv layer feature maps (Ak) → backward pass to compute ∂yc/∂Ak → compute weights αkc → weighted sum + ReLU → upsample and overlay
  - Guided Backpropagation pipeline: Forward pass → backward pass with modified ReLU gradient rule (zero for negative gradients) → relevance map Rl visualization

- Critical path:
  1. Establish baseline classification accuracy on ResNet50 for your target images
  2. For Grad-CAM: Identify target convolutional layer (typically last conv layer before classification)
  3. For LIME/SHAP: Define perturbation/masking strategy appropriate to image resolution
  4. Implement visualization normalization (colormap, overlay opacity) for consistent comparison

- Design tradeoffs:
  - **LIME**: Flexible (any model) vs. unstable across runs; requires careful perturbation parameter tuning
  - **SHAP**: Theoretically sound vs. computationally expensive (O(2^n) for exact); use KernelSHAP or DeepSHAP approximations
  - **Grad-CAM**: Efficient and class-discriminative vs. coarse spatial resolution; cannot localize fine details
  - **Guided Backpropagation**: High-resolution edge visualization vs. not class-discriminative; may highlight irrelevant edges
  - **Hybrid approach**: Combining methods (e.g., Grad-CAM × Guided Backpropagation = Guided Grad-CAM) can address individual limitations

- Failure signatures:
  - LIME explanations vary significantly across runs → check random seed, increase sample count
  - SHAP computation timeout on large images → reduce input resolution or use gradient-based approximation (DeepSHAP)
  - Grad-CAM highlights background/irrelevant regions → verify target layer selection; earlier layers may be more spatially precise but less semantic
  - Guided Backpropagation produces noisy/chaotic visualizations → check for gradient explosion; consider adding smoothing

- First 3 experiments:
  1. **Baseline reproducibility check**: Run LIME 5 times on the same image with fixed parameters; measure explanation variance (coefficient stability, region overlap) to quantify method instability
  2. **Layer sensitivity analysis for Grad-CAM**: Apply Grad-CAM at 3 different ResNet50 conv layers (early, middle, final); compare spatial resolution vs. semantic relevance of highlighted regions
  3. **Cross-method agreement quantification**: For a single prediction, compute intersection-over-union of top-10% important pixels across LIME, SHAP, and Grad-CAM; identify where methods agree vs. disagree to understand complementary coverage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can hybrid XAI approaches systematically combine model-agnostic and model-specific methods to mitigate individual limitations while preserving interpretability?
- Basis in paper: Future work section states: "Hybrid approaches that leverage both perturbation-based and gradient-based techniques can enhance interpretability while mitigating individual method limitations."
- Why unresolved: The paper concludes that combining methods is optimal but provides no framework for designing or validating such combinations.
- What evidence would resolve it: A systematic methodology for hybrid XAI with quantitative metrics showing improved interpretability over individual methods.

### Open Question 2
- Question: How can the run-to-run variability of model-agnostic explanations (LIME, SHAP) be reduced to ensure reproducible interpretations?
- Basis in paper: The paper identifies "LIME and SHAP showing variable explanations across runs" as a key weakness but does not investigate causes or solutions.
- Why unresolved: The stochastic nature of perturbation-based methods introduces instability that was characterized but not addressed.
- What evidence would resolve it: Modified algorithms or consensus techniques producing consistent explanations across multiple runs, validated via statistical reproducibility tests.

### Open Question 3
- Question: Do XAI-generated explanations align with human domain expertise in high-stakes applications like healthcare and autonomous systems?
- Basis in paper: Future work calls for "evaluating how well these explanations align with human intuition and domain expertise" for practical deployment.
- Why unresolved: This study relied solely on qualitative visual inspection without user studies involving domain experts.
- What evidence would resolve it: User studies with clinicians or engineers demonstrating that XAI visualizations meaningfully improve task performance or trust calibration.

### Open Question 4
- Question: Can model-specific interpretability techniques be effectively adapted for non-convolutional architectures such as transformers and graph neural networks?
- Basis in paper: Future work recommends "extending model-specific interpretability techniques to non-convolutional architectures, such as transformers and graph neural networks."
- Why unresolved: Grad-CAM and Guided Backpropagation leverage convolutional layer structures absent in transformer attention mechanisms.
- What evidence would resolve it: Attention-based or gradient-based explanation methods adapted for transformers, validated on benchmark datasets with comparison to existing XAI baselines.

## Limitations
- No quantitative metrics for method comparison; analysis relies on qualitative visualization assessment
- Missing dataset specification and exact hyperparameter settings for reproducible implementation
- No statistical validation of observed method differences or significance testing
- Limited to image classification; results may not generalize to other data modalities

## Confidence
- **High confidence**: The described mechanisms of LIME, SHAP, Grad-CAM, and Guided Backpropagation are technically accurate and align with established literature
- **Medium confidence**: The comparative observations (variable LIME/SHAP outputs, Grad-CAM localization vs. fine-grained detail tradeoff, Guided Backpropagation edge detection) are plausible but unverified without quantitative metrics
- **Low confidence**: The claim that "combining multiple XAI methods offers the most comprehensive understanding" lacks empirical validation through hybrid method testing

## Next Checks
1. **Quantitative agreement analysis**: Compute pixel-wise correlation coefficients and intersection-over-union between all four XAI methods on 50+ validation images to objectively measure agreement vs. disagreement
2. **Stability benchmarking**: Run each method 10 times per image with fixed vs. varying seeds; report coefficient/feature importance stability scores (standard deviation, coefficient correlation) to quantify LIME/SHAP variability claims
3. **Computational complexity profiling**: Measure wall-clock time and memory usage for each method across varying image resolutions (224×224, 448×448, 672×672) to empirically validate the computational efficiency claims, particularly for SHAP vs. model-specific approaches