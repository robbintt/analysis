---
ver: rpa2
title: Structure-Aware Fill-in-the-Middle Pretraining for Code
arxiv_id: '2506.00204'
source_url: https://arxiv.org/abs/2506.00204
tags:
- code
- arxiv
- ast-fim
- pretraining
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Structure-Aware Fill-in-the-Middle Pretraining for Code

## Quick Facts
- arXiv ID: 2506.00204
- Source URL: https://arxiv.org/abs/2506.00204
- Authors: Linyuan Gong; Alvin Cheung; Mostafa Elhoussi; Sida Wang
- Reference count: 22
- Key outcome: AST-aligned masking preserves L2R performance while improving infilling

## Executive Summary
This paper introduces AST-FIM, a fill-in-the-middle pretraining strategy that uses syntax-aware masking to improve code infilling performance while maintaining left-to-right generation capabilities. By masking complete AST subtrees rather than random character spans, the approach creates training examples with coherent semantic boundaries that better align with real-world code structures. The method achieves state-of-the-art results on both standard FIM benchmarks and a new Real-FIM-Eval benchmark based on actual GitHub commits, demonstrating that structural coherence in training data translates to practical editing capabilities.

## Method Summary
The core innovation is AST-FIM masking, which selects complete AST subtrees as the middle span to be predicted, rather than random character ranges. The pretraining uses a 70/30 mix of FIM and L2R training, with 90% AST-FIM and 10% random FIM within the FIM portion. Two masking strategies are employed: Single-Node Masking (sampling AST nodes weighted by size) and Aligned-Span Masking (finding the smallest enclosing subtree for a random span). Models are trained from scratch on 1B and 8B parameters using Llama-3 architectures, with training on 256 H100 GPUs and 8192 context length.

## Key Results
- AST-FIM preserves L2R performance, matching pure L2R models on HumanEval+ (15.9) and MBPP+ (35.3 vs 34.5)
- Outperforms random FIM baselines on SAFIM (Pass@1) and Real-FIM-Eval (perplexity)
- Single-Node Masking excels on Real-FIM-Eval-Add, validating the commit-representativeness assumption
- The 10% random FIM mix prevents overfitting to AST patterns while maintaining structural benefits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AST-aligned masking produces training examples with coherent semantic boundaries, reducing noise in the learning signal.
- Mechanism: By constraining masked spans to complete AST subtrees, the model receives prefix/suffix context that always aligns with syntactically valid boundaries. This avoids mid-expression or mid-statement splits that force the model to infer incomplete syntax patterns, which the paper notes "can break coherent code structures and introduce noisy boundaries" (Section 6.1).
- Core assumption: Syntactic coherence in training examples translates to better generalization on real-world infilling tasks.
- Evidence anchors:
  - [abstract]: "ensuring coherent training examples better aligned with universal code structures"
  - [section 6.1]: "character-level random FIM can break coherent code structures and introduce noisy boundaries"
  - [corpus]: Weak direct evidence; neighbor papers focus on FIM evaluation and instruction-tuning rather than syntax-aware masking mechanisms.
- Break condition: If code files are frequently syntactically invalid (the paper assumes 98.2% validity), AST parsing fails and falls back to random masking, diluting the signal.

### Mechanism 2
- Claim: AST-FIM reduces the L2R performance degradation typically caused by high FIM rates.
- Mechanism: Because AST-FIM preserves structural coherence, the training objective remains closer in difficulty to standard L2R than Rand-FIM. The paper observes that "AST-FIM training loss is at the middle of the higher Rand-FIM loss and the lower L2R loss" (Section 6.1), suggesting the model isn't forced into as divergent a distribution.
- Core assumption: Loss interpolation reflects task similarity; structured masking is an intermediate difficulty between random masking and unmasked L2R.
- Evidence anchors:
  - [section 6.1]: "AST-FIM model achieves L2R performance nearly identical to the baseline L2R model"
  - [table 1]: AST-FIM matches L2R on HumanEval+ (15.9) and slightly exceeds on MBPP+ (35.3 vs 34.5)
  - [corpus]: No direct corroboration; neighbor papers do not address L2R/FIM tradeoffs.
- Break condition: If the FIM rate exceeds some threshold (the paper used 0.7), degradation may still occur even with AST alignment.

### Mechanism 3
- Claim: Training on commit-derived masks improves real-world infilling because the span distribution matches developer edit patterns.
- Mechanism: Real-FIM-Eval extracts "middle" segments from actual git diffs (additions and edits), so models trained with AST-FIM—which similarly masks complete syntactic units—face a test distribution closer to training. The ablation (Table 5) shows Single-Node Masking excels on Real-FIM-Eval-Add, "because most real-world code insertions involves adding individual AST nodes."
- Core assumption: GitHub commits are representative of real-world code editing patterns; AST nodes correlate with what developers actually insert/modify.
- Evidence anchors:
  - [section 4]: "every example is a real commit diff, Real-FIM-Eval provides an unbiased, faithful view"
  - [section A.4]: Single-Node Masking outperforms on Real-FIM-Eval-Add because insertions align with single AST nodes
  - [corpus]: No direct validation of commit-representativeness; neighbor papers focus on evaluation methodology rather than data distribution alignment.
- Break condition: If pretraining data contamination exists (the paper claims temporal separation), benchmark scores may inflate.

## Foundational Learning

- Concept: Fill-in-the-Middle (FIM) as data transformation
  - Why needed here: FIM isn't a new loss function—it's a reordering of tokens (prefix, suffix, middle) into a sequence the model processes left-to-right. Understanding this explains why FIM can be "jointly trained" with L2R.
  - Quick check question: In the PSM format, which part does the model predict last?

- Concept: Abstract Syntax Trees (ASTs) and subtree masking
  - Why needed here: The core intervention is selecting mask spans that correspond to complete AST subtrees rather than arbitrary character ranges. Without understanding ASTs, the distinction between Rand-FIM and AST-FIM is opaque.
  - Quick check question: Why does masking a binary expression node differ from masking a random span that bisects it?

- Concept: Perplexity as an evaluation metric
  - Why needed here: Real-FIM-Eval uses character-level perplexity rather than execution-based metrics. Understanding why (scalability, noise reduction) clarifies what the benchmark actually measures.
  - Quick check question: Why might perplexity be preferred over Pass@1 for evaluating large-scale infilling?

## Architecture Onboarding

- Component map: Tokenizer -> Parser -> Masking Module -> Data Mixer -> Training Loop
- Critical path:
  1. Parse code -> AST (Tree-sitter)
  2. Apply masking strategy -> identify middle span
  3. Reorder into PSM or SPM format with special tokens
  4. Train with NLL loss
  5. Fallback to Rand-FIM if parsing fails

- Design tradeoffs:
  - Single-Node vs Aligned-Span: Single-Node better for isolated insertions; Aligned-Span handles multi-statement edits. Paper uses 50:50 mix for coverage.
  - FIM rate at 0.7: Higher than some prior work (DeepSeek-Coder caps at 0.5) but AST structure mitigates L2R degradation.
  - Perplexity vs execution for evaluation: Perplexity is scalable and low-noise but doesn't verify functional correctness.

- Failure signatures:
  - Parsing failures on malformed code -> fallback to Rand-FIM, reducing AST-FIM signal
  - Overfitting to AST patterns -> poor performance on random-span FIM tasks (paper notes AST-FIM "got there slower" on HumanEval random span without the 10% Rand-FIM mix)
  - Temporal contamination between training and Real-FIM-Eval -> inflated benchmark scores

- First 3 experiments:
  1. Ablation of masking strategies: Train separate 1B models with only Single-Node or only Aligned-Span masking; compare on SAFIM subtasks and Real-FIM-Eval splits to validate the 50:50 mix rationale.
  2. FIM rate sweep: Train 1B models at FIM rates 0.5, 0.7, 0.9 to confirm AST-FIM's degradation profile differs from Rand-FIM; measure L2R (HumanEval+/MBPP+) vs FIM (SAFIM) tradeoff.
  3. Cross-benchmark generalization: Evaluate AST-FIM and Rand-FIM models on existing FIM benchmarks (HumanEval Single-Line, CruxEval) to test whether Real-FIM-Eval improvements transfer or are benchmark-specific.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal FIM rate for pretraining, and does it differ between Rand-FIM and AST-FIM?
- Basis in paper: [explicit] Section 2 states "the optimal FIM rate is yet to be determined (with suggestions ranging from 0.5 to 0.9)" and the paper uses 0.7 experimentally.
- Why unresolved: Prior works report conflicting optimal rates, and the paper only tests a single rate without systematic ablation.
- What evidence would resolve it: Controlled experiments varying FIM rate (0.3-0.9) for both Rand-FIM and AST-FIM, measuring L2R and FIM benchmarks.

### Open Question 2
- Question: How does masked span length affect FIM performance across different masking strategies (random, token, line, AST)?
- Basis in paper: [explicit] Section 7 states "A more careful test of random FIM, token FIM, and line FIM with tuned lengths is needed to better understand the effects of mid length."
- Why unresolved: The paper focuses on AST-aligned boundaries but does not isolate the effect of span length distributions independent of syntactic alignment.
- What evidence would resolve it: Ablations matching length distributions across masking strategies while varying syntactic coherence.

### Open Question 3
- Question: Can syntax-aware pretraining be designed to improve—rather than merely retain—L2R generation performance?
- Basis in paper: [explicit] Section 7 states "Outperforming standard L2R training on L2R tasks would be the strongest evidence for the benefits of syntax annotations."
- Why unresolved: AST-FIM matches pure L2R on HumanEval+/MBPP+ but does not exceed it, suggesting structural priors do not transfer to sequential generation.
- What evidence would resolve it: Novel objectives combining syntax awareness with L2R training, evaluated against pure L2R baselines.

### Open Question 4
- Question: How well does perplexity on Real-FIM-Eval correlate with generative metrics like exact match or functional correctness?
- Basis in paper: [explicit] Section 4.2 notes "Future work could explore generative metrics such as exact match or edit distance" since execution-based evaluation is impractical at scale.
- Why unresolved: Perplexity captures probability but not whether generated code is functionally correct or syntactically well-formed.
- What evidence would resolve it: Correlation analysis between perplexity and exact match/edit distance on Real-FIM-Eval with human-validated ground truth.

## Limitations

- The paper's claims about commit-representativeness and data decontamination are not empirically validated
- Real-FIM-Eval uses perplexity rather than functional correctness metrics, limiting practical relevance
- The 98.2% parse success rate assumption lacks validation and may affect AST-FIM signal strength

## Confidence

- High confidence: AST-FIM's ability to preserve L2R performance relative to Rand-FIM, the effectiveness of the 10% Rand-FIM mix, and the correlation between Single-Node Masking and real-world insertion patterns
- Medium confidence: The claim that AST-FIM produces more coherent training examples with better-aligned semantic boundaries
- Low confidence: The assertion that AST-FIM's performance gains on Real-FIM-Eval generalize to other FIM benchmarks

## Next Checks

1. Cross-benchmark generalization test: Evaluate AST-FIM and Rand-FIM models on established FIM benchmarks (HumanEval Single-Line, CruxEval) to determine whether Real-FIM-Eval improvements transfer or are benchmark-specific artifacts.

2. Data contamination verification: Perform statistical analysis of token overlap between training data and Real-FIM-Eval to validate the claimed temporal separation and decontaminated training data, addressing the potential inflation of benchmark scores.

3. Syntax validity audit: Systematically sample code files that failed parsing to quantify the proportion of truly unparseable code versus implementation issues, and measure how often Rand-FIM fallback occurs in practice to assess signal dilution.