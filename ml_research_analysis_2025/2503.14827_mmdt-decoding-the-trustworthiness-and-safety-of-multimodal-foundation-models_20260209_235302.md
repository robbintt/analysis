---
ver: rpa2
title: 'MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models'
arxiv_id: '2503.14827'
source_url: https://arxiv.org/abs/2503.14827
tags:
- fairness
- mmfms
- images
- image
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MMDT introduces the first comprehensive and unified platform to
  evaluate the trustworthiness and safety of multimodal foundation models (MMFMs)
  across six critical perspectives: safety, hallucination, fairness, privacy, adversarial
  robustness, and out-of-distribution robustness. The platform constructs challenging
  evaluation datasets for both text-to-image and image-to-text models using red teaming
  algorithms, forming a high-quality benchmark.'
---

# MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models

## Quick Facts
- arXiv ID: 2503.14827
- Source URL: https://arxiv.org/abs/2503.14827
- Reference count: 40
- Primary result: Introduces first comprehensive platform evaluating MMFMs across safety, hallucination, fairness, privacy, adversarial robustness, and out-of-distribution robustness

## Executive Summary
MMDT presents the first unified platform to evaluate the trustworthiness and safety of multimodal foundation models across six critical dimensions. The platform constructs challenging evaluation datasets using red teaming algorithms, forming a high-quality benchmark for both text-to-image and image-to-text models. Comprehensive evaluations of state-of-the-art MMFMs reveal significant vulnerabilities across all trustworthiness dimensions, with no single model consistently excelling. The platform provides actionable insights for developing safer and more reliable MMFMs through detailed failure mode analysis.

## Method Summary
MMDT employs red-teaming algorithms to generate challenging evaluation data, filtering candidates through multiple surrogate models to identify systematic failures. The platform evaluates both text-to-image and image-to-text models using automated judges (GPT-4o) and detectors (GroundingDINO, EasyOCR, FairFace). Dimension-specific metrics include Harmful Content Generation Rate (HGR), Bypass Rate (BR), Group Unfairness (G), and task-specific accuracy. The evaluation pipeline runs inference on target models and assesses outputs against ground truth using automated evaluation modules.

## Key Results
- No single MMFM consistently excels across all trustworthiness dimensions
- Severe safety issues persist with significant decoupling between input-level and output-level resilience
- Hallucination problems affect all tasks with average non-hallucination accuracy below 50%
- Pronounced fairness and privacy risks including over-refusal and CLIP embedding similarity vulnerabilities
- Poor out-of-distribution robustness and susceptibility to adversarial attacks

## Why This Works (Mechanism)

### Mechanism 1: Surrogate-Guided Challenging Data Selection
Filtering evaluation data through multiple surrogate models identifies prompts where models systematically fail while humans might find them tractable. This works by generating large candidate pools, evaluating on surrogate models, selecting instances where surrogates fail but humans succeed, then manually verifying quality. The assumption is that failures transfer across model families. Evidence shows performance drops from original to challenging datasets (e.g., LLaVa drops from 77.9% to 0% on natural selection object recognition).

### Mechanism 2: Input-Level vs Output-Level Safety Decoupling
Measuring bypass rate (input-level) and harmful generation rate (output-level) separately reveals that safety filters do not guarantee safe outputs. This mechanism shows that transformed prompts may bypass filters more easily but trigger fewer harmful generations than vanilla prompts. The assumption is that safety alignment primarily occurs at the input/text interface. Evidence shows all T2I models exhibit worse input-level resilience but better output-level resilience under transformed prompts.

### Mechanism 3: Multi-Scenario Hallucination Stress Testing
Hallucination manifests differently across six scenarios (distraction, counterfactual, co-occurrence, misleading, OCR, natural selection), and models fail differently on each. This works by constructing prompts that exploit different failure modes - distracting symbols exploit code-comment heuristics, co-occurrence tests parametric vs contextual knowledge balance. The assumption is that hallucination is not unitary but emerges from distinct architectural weaknesses. Evidence shows average performance for all MMFMs in terms of non-hallucination accuracy is below 50%.

## Foundational Learning

- **Red-teaming in multimodal models**: Why needed - The entire evaluation infrastructure depends on generating adversarial inputs that exploit cross-modal vulnerabilities (e.g., hiding harmful intent in typography rather than text). Quick check: Can you explain why a harmful instruction embedded in an image might bypass a text-only safety filter?

- **Transfer attacks and surrogate models**: Why needed - The benchmark uses white-box attacks on open models to generate adversarial examples, then transfers them to black-box commercial models. Quick check: Why would an adversarial perturbation optimized against LLaVa affect GPT-4o?

- **CLIP embedding similarity for memorization detection**: Why needed - Privacy evaluation measures training data leakage via embedding-space similarity between generated and original images. Quick check: What does high CLIP similarity between a generated image and training data imply about privacy risk, and what are its limitations?

## Architecture Onboarding

- Component map:
  ```
  Benchmark Orchestration
  ├── Instance Generator (red-teaming algorithms: GCG, MMP, typography, style transforms)
  ├── Chat Adapters (API clients for GPT-4o, Gemini, etc.; vLLM for local models)
  ├── Evaluator Modules (LLM-as-judge, GroundingDINO, EasyOCR, FairFace classifier)
  └── Results Aggregator (metrics: BR, HGR, accuracy, unfairness scores)
  ```

- Critical path: 1. Define trustworthiness dimension → 2. Design red-teaming scenarios → 3. Generate candidate pool (LLM-assisted) → 4. Filter via surrogate models → 5. Manual verification → 6. Run evaluation across targets → 7. Aggregate metrics with LLM-as-judge

- Design tradeoffs:
  - Static vs dynamic benchmarks: Static benchmarks risk becoming obsolete via adversarial training; dynamic generation keeps evaluation current but requires private holdout data
  - LLM-as-judge vs human evaluation: GPT-4o used for harmfulness scoring; faster than human review but may have blind spots. Manual verification spot-checked at 93-98% agreement
  - Surrogate diversity: More surrogates increase transferability but raise computational cost. Paper uses 3 surrogates as a practical compromise

- Failure signatures:
  - Over-refusal: Llama-3.2 shows 85.2% refusal rate on fairness tasks, inflating apparent "fairness" while actually refusing to engage
  - Overkill fairness: Models generate historically inaccurate outputs (e.g., diverse founding fathers) in pursuit of diversity, scoring poorly on the O metric
  - Refusal bypass: Typography and illustration attacks circumvent text-based safety filters entirely

- First 3 experiments:
  1. Reproduce safety evaluation on a single model: Take GPT-4o, run the typography and jailbreak scenarios (390 prompts each from Table 45), compute BR and HGR
  2. Add a new hallucination scenario: Extend framework with "multilingual confusion" scenario where contradictory information appears in different languages, use 3 surrogates to filter, then evaluate on 2 target models
  3. Test temporal robustness: Re-evaluate same safety benchmark on model versions released 6 months apart (e.g., GPT-4V vs GPT-4o) to measure whether alignment improvements actually reduce HGR, not just BR

## Open Questions the Paper Calls Out

- **Balancing group vs overkill fairness**: How can multimodal models be optimized to balance group fairness objectives against "overkill fairness" (sacrificing historical/factual accuracy for diversity)? The paper notes that models optimizing for group fairness often produce historically inaccurate outputs, yet no current method effectively resolves this tension. A mitigation strategy that simultaneously reduces both Group Unfairness ($G$) and Overkill Fairness ($O$) scores would resolve this.

- **Bridging instance-level vs distribution-level fairness**: What mechanisms can bridge the gap between instance-level fairness and distribution-level fairness in multimodal generation? The paper shows that optimizing fairness at individual prompt level does not translate to fair statistical distributions across generated images. A training algorithm yielding strong correlation between improvements in Individual Unfairness ($I$) and Group Unfairness ($G$) would resolve this.

- **DPO for multimodal fairness**: Can Direct Preference Optimization (DPO) effectively mitigate social biases in text-to-image models despite lack of automatic reward feedback in image space? While DPO is established for text, applying it to image generation for fairness is hypothesized as promising but remains unverified. Experimental results showing text-to-image models fine-tuned with DPO achieving significantly lower Group Unfairness scores would resolve this.

## Limitations

- Benchmark construction relies heavily on surrogate models for filtering, which may not perfectly transfer to target models
- Evaluation focuses on English-language prompts and Western-centric datasets (LAION, COCO), limiting generalizability to other cultural contexts
- Privacy evaluation methodology using CLIP similarity only captures surface-level visual similarity, not semantic content memorization

## Confidence

- **High confidence**: Safety evaluation findings showing decoupling between input-level and output-level resilience; fairness results demonstrating over-refusal and historical inaccuracy issues; privacy findings about CLIP embedding similarity limitations
- **Medium confidence**: Hallucination scenario performance rankings; adversarial robustness transfer rates; OOD robustness results showing distribution shift vulnerabilities
- **Low confidence**: Privacy evaluation methodology using CLIP similarity (only captures surface-level visual similarity, not semantic content memorization); fairness metric interpretations (O metric captures "overkill" but may miss subtle demographic biases)

## Next Checks

1. **Temporal robustness validation**: Re-run the safety benchmark on model versions released 6 months apart (e.g., GPT-4V vs GPT-4o) to verify whether alignment improvements actually reduce harmful generations rather than just increasing refusals

2. **Cross-cultural generalization**: Extend the fairness and safety evaluations to non-Western datasets and multilingual prompts to assess whether reported vulnerabilities persist across cultural contexts

3. **Adversarial transferability validation**: Systematically test whether adversarial examples optimized against open-source surrogates actually transfer to commercial models across all six trustworthiness dimensions, not just safety