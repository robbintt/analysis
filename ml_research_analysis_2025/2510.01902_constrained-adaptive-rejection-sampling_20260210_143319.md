---
ver: rpa2
title: Constrained Adaptive Rejection Sampling
arxiv_id: '2510.01902'
source_url: https://arxiv.org/abs/2510.01902
tags:
- cars
- sampling
- samples
- constrained
- grammar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CARS improves exact constrained sampling efficiency by adaptively
  pruning invalid prefixes during generation. The method maintains a trie of invalid
  continuations and subtracts their probability mass from future draws, ensuring monotonic
  improvement in acceptance rate while preserving exact distributional fidelity.
---

# Constrained Adaptive Rejection Sampling

## Quick Facts
- arXiv ID: 2510.01902
- Source URL: https://arxiv.org/abs/2510.01902
- Reference count: 35
- Primary result: CARS achieves 10% higher branch coverage in fuzzing while requiring 215 generations for 100 valid samples vs 1000+ for rejection sampling

## Executive Summary
CARS addresses the fundamental inefficiency of exact constrained sampling from language models by adaptively pruning invalid prefixes during generation. The method maintains a trie of invalid continuations and subtracts their probability mass from future draws, ensuring monotonic improvement in acceptance rate while preserving exact distributional fidelity. In practical benchmarks, CARS transforms intractable exact sampling problems into practical ones by exploiting constraint structure to avoid wasted computation.

## Method Summary
CARS is an exact constrained sampling method that maintains a trie of invalid prefixes discovered during generation. When sampling, it computes for each prefix the probability of extending to a valid sequence (avoiding invalid prefixes), then reweights the distribution to sample from sequences avoiding the trie. On rejection, CARS adds not just the rejected prefix but all invalid continuations of all shorter prefixes, aggressively pruning the search space. This ensures monotonic improvement in acceptance rate while maintaining exact fidelity to the constrained LM distribution.

## Key Results
- CARS achieves 10% higher branch coverage than baselines in fuzzing benchmarks
- Requires 215 generations for 100 valid samples versus 1000+ for rejection sampling
- In molecular synthesis, produces 100 valid molecules using 183 generations versus 793 for standard rejection sampling
- Better validity (0.87 vs 0.85) and diversity metrics compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Maintaining a trie of invalid prefixes allows CARS to subtract probability mass from future draws, monotonically improving acceptance rates.
- Mechanism: As each sample is generated, CARS identifies invalid prefixes and records them in a trie. The probability mass of these prefixes is subtracted from future sampling distributions, reshaping R_W to avoid known-invalid paths. This ensures prefixes proven invalid are never revisited.
- Core assumption: Constraints are prefix-checkable, allowing efficient identification of invalid continuations during generation.
- Evidence anchors:
  - [abstract] "adaptively rules out constraint-violating continuations by recording them in a trie and subtracting their probability mass"
  - [Section 3] "CARS maintains a finite set W... called invalid prefixes... The quantity p_ε determines the total probability of all sequences avoiding W"
  - [corpus] Neighbor work "Constrained Sampling for Language Models Should Be Easy" notes constrained decoding often distorts distributions; CARS addresses this via exact pruning.

### Mechanism 2
- Claim: Adaptive reweighting of the sampling distribution preserves exact fidelity to the constrained LM distribution.
- Mechanism: CARS samples from R_W, a reweighted distribution proportional to the original LM distribution but renormalized over sequences not extending any invalid prefix in W. Since R_W maintains proportionality to P(w) for valid sequences, yielded samples exactly follow P_L.
- Core assumption: The LM distribution P and the constraint L are fixed; the trie accurately captures all discovered invalid prefixes without false positives.
- Evidence anchors:
  - [Section 3] "The CARS algorithm samples an element of L according to the target distribution P_L" (Theorem 1)
  - [Section 3] "In R_W the probability of each sequence w∈L is proportional to P(w)"
  - [corpus] Neighbor "Fast Controlled Generation... with Adaptive Weighted Rejection Sampling" similarly uses rejection for exactness but differs in weighting strategy; CARS uses trie-based pruning for efficiency.

### Mechanism 3
- Claim: Aggressive update strategy accelerates convergence compared to naive adaptive rejection.
- Mechanism: Unlike ARS (which only adds the rejected prefix), CARS also adds, for each prefix of the invalid sequence, all tokens that would immediately violate the constraint. This "broader" pruning removes more probability mass per rejection, reducing future invalid samples faster.
- Core assumption: The constraint checker can efficiently enumerate invalid next tokens for any prefix (i.e., vocabulary-wide prefix check).
- Evidence anchors:
  - [Section 3, Figure 1] Illustrates how CARS prunes more branches than ARS for the arithmetic grammar example.
  - [Section 4.1] "CARS lowers rejection to rates in the 70-95% range after just 100 samples" vs. >99% for ARS.
  - [corpus] No direct corpus comparison for this specific update strategy; efficacy is claimed based on in-paper benchmarks.

## Foundational Learning

- **Concept: Rejection Sampling (RS)**
  - Why needed here: RS is the baseline exact method; CARS is an adaptive extension. Understanding RS's inefficiency (discarding invalid outputs) motivates CARS.
  - Quick check question: If an LM generates outputs where 5% satisfy a constraint, how many RS draws are needed on average for 10 valid samples? (Answer: ~200)

- **Concept: Trie Data Structure**
  - Why needed here: CARS uses a trie to store invalid prefixes with probabilities, enabling efficient prefix lookup and probability propagation during updates.
  - Quick check question: How does a trie support finding all entries with a given prefix? (Answer: Traverse from root following prefix characters; all descendants are matches)

- **Concept: Prefix-Checkable Constraints**
  - Why needed here: CARS assumes constraints can be evaluated incrementally on prefixes (e.g., partial parse tree validity). This allows early rejection and trie updates.
  - Quick check question: For a grammar `E ::= 'a' | 'b' E`, is the prefix `'b' 'a'` prefix-checkable? (Answer: Yes, it can be completed to `'b' 'a'` but not to `'b' 'b'...` if no further `'b'`s are allowed.)

## Architecture Onboarding

- **Component map:**
  - Language Model (LM) -> Constraint Checker -> Invalid Prefix Trie -> Adaptive Sampler

- **Critical path:**
  1. Sample sequence w from current R_W (using trie to guide/resample)
  2. Check if w ∈ L (constraint checker). If valid, yield w
  3. Identify invalid prefix(es) in w (shortest and continuations of shorter prefixes)
  4. Update trie: add new invalid prefixes, recompute/propagate probabilities upward
  5. Repeat from step 1

- **Design tradeoffs:**
  - **Trie depth/size vs. pruning efficiency:** More aggressive updates (deeper trie) improve acceptance but increase memory and update cost. Tune based on constraint complexity.
  - **Exactness vs. speed:** CARS is exact; inexact methods (GCD) are faster but biased. Choose based on need for fidelity.
  - **Update strategy:** CARS's broad updates are most efficient but require full vocabulary prefix checks. Simpler strategies (e.g., RSFT) are easier to implement but less effective.

- **Failure signatures:**
  - **Timeouts/low acceptance:** Trie not pruning effectively; check constraint checker correctness or if constraints are too sparse/misaligned with LM.
  - **Distributional bias:** Trie may be incorrectly rejecting valid prefixes; audit update logic and constraint checker.
  - **Memory blowup:** Trie growing unboundedly; consider pruning very low-probability branches or capping size.

- **First 3 experiments:**
  1. **Toy grammar validation:** Implement CARS for a simple grammar (e.g., balanced parentheses) and verify exactness (compare sample distribution to rejection sampling) and efficiency (generations per valid sample).
  2. **Ablate update strategy:** Compare CARS's full update vs. ARS (single-prefix) vs. RSFT (first-token only) on a fuzzing benchmark; measure acceptance rate improvement and final branch coverage.
  3. **Constraint alignment test:** Evaluate CARS on a task where LM is poorly aligned with constraints (e.g., requiring specific rare tokens); observe if timeouts occur as noted in Appendix E.5, and compare to GCD's inexact but always-valid output.

## Open Questions the Paper Calls Out

- **Question:** What are the theoretical or empirical upper bounds on CARS efficiency when facing adversarially constructed constraints designed to maximize trie depth and minimize pruning effectiveness?
- **Basis in paper:** [explicit] "Although, in theory, CARS could still require many rejections for adversarial constraints, we argue—and demonstrate empirically—that real-world constrained LM tasks fit the CARS setting well"
- **Why unresolved:** The paper acknowledges this theoretical limitation but does not characterize its severity or construct adversarial examples to test robustness.
- **What evidence would resolve it:** Systematic experiments with constructed adversarial grammars or constraints, plus theoretical analysis of worst-case trie growth and acceptance rates.

## Limitations

- CARS depends critically on prefix-checkable constraints; efficiency gains disappear for constraints requiring full-sequence validation
- Computational overhead of maintaining and updating the trie is not fully characterized, particularly for large vocabularies
- Timeout results reveal CARS can fail when LM distribution is misaligned with constraints, a realistic scenario not fully addressed

## Confidence

- **High confidence**: CARS maintains exact distributional fidelity to P_L (Theorem 1 is proven and method design supports this claim)
- **Medium confidence**: CARS improves sample efficiency compared to rejection sampling for prefix-checkable constraints (supported by empirical results but limited to specific benchmarks)
- **Low confidence**: CARS is broadly practical for real-world constrained generation tasks (timeout results and computational overhead concerns suggest limitations)

## Next Checks

1. Characterize the constraint class where CARS provides meaningful efficiency gains versus exact-prefix-checkable constraints versus general constraints. Implement a benchmark with constraints requiring full-sequence validation to test the method's limits.

2. Measure and report the computational overhead of trie maintenance, particularly the INVALID() function's enumeration of invalid continuations. Compare runtime per sample for CARS versus rejection sampling across different vocabulary sizes and constraint complexities.

3. Evaluate CARS's behavior when the LM distribution is poorly aligned with constraints (as in Table 4). Test whether the method can detect when it's unlikely to succeed and gracefully fall back to inexact methods or alternative approaches.