---
ver: rpa2
title: Interpretability and Generalization Bounds for Learning Spatial Physics
arxiv_id: '2506.15199'
source_url: https://arxiv.org/abs/2506.15199
tags:
- data
- training
- error
- function
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work rigorously analyzes the accuracy, convergence, and generalization
  of ML models applied to linear differential equations, specifically the 1D Poisson
  equation. Using numerical analysis techniques, it proves that generalization depends
  critically on the function space of the training data, not just the amount or discretization
  of data.
---

# Interpretability and Generalization Bounds for Learning Spatial Physics

## Quick Facts
- **arXiv ID:** 2506.15199
- **Source URL:** https://arxiv.org/abs/2506.15199
- **Reference count:** 40
- **Key outcome:** This work rigorously analyzes the accuracy, convergence, and generalization of ML models applied to linear differential equations, specifically the 1D Poisson equation. Using numerical analysis techniques, it proves that generalization depends critically on the function space of the training data, not just the amount or discretization of data. For parameter learning with finite differences, the error increases with both grid spacing and polynomial order of the training data. For linear models, the learned weights converge to a projection of the true operator onto the training data subspace, meaning generalization outside this subspace is impossible regardless of data quantity. Empirically, this behavior is replicated across various model types, including white-box methods, black-box models, and physics-specific techniques like DeepONets and Neural Operators. Surprisingly, different models exhibit opposing generalization behaviors. The work introduces a new interpretability lens by extracting Green's function representations from black-box model weights and proposes a cross-validation technique for measuring generalization in physical systems.

## Executive Summary
This paper establishes rigorous generalization bounds for machine learning models applied to linear differential equations, demonstrating that model performance depends critically on the function space of training data rather than just data quantity or discretization. Through theoretical analysis and extensive empirical validation, the authors show that linear models converge to projections of the true physical operator onto the training data subspace, making generalization outside this subspace impossible regardless of dataset size. The work introduces novel interpretability techniques by extracting Green's function representations from black-box models and provides a cross-validation framework for assessing generalization in physical systems.

## Method Summary
The authors analyze the 1D Poisson equation −k·d²u/dx² = f(x) with boundary conditions u(0) = u(1) = 0. They generate 25 datasets from different function spaces (polynomials, sines, cosines, piecewise linear) with varying degrees and use 10,000 examples per dataset. Eight model types are evaluated: finite difference SINDy, PINN inverse, linear models, Deep Linear, MLP, DeepONet, Fourier Neural Operator, and PI-DeepONet. The primary evaluation metric is MSE loss, with cross-validation heatmaps comparing performance across different function subspaces. Theoretical analysis focuses on proving convergence properties and error bounds for parameter learning and operator approximation.

## Key Results
- For linear models, learned weights converge to the projection of the true operator onto the training data subspace, making generalization outside this subspace impossible regardless of data quantity
- Parameter learning error with finite differences increases with both grid spacing and polynomial order of training data due to truncation artifacts
- Black-box neural operators can be interpreted as Green's functions by extracting impulse responses from one-hot encoded inputs
- Different model architectures exhibit opposing generalization behaviors when trained on identical function subspaces
- Physics-informed losses do not solve the subspace limitation problem and suffer similar generalization failures to black-box models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** In linear operator learning, gradient descent converges to the projection of the true physical operator onto the subspace spanned by the training data, implying that generalization outside this subspace is impossible regardless of data quantity.
- **Mechanism:** The optimization dynamics dictate that the weight matrix $W$ updates based on the covariance of the training inputs. If the training data $f$ lies in a restricted subspace (low rank), the gradient update direction $(A-W)BB^T$ is orthogonal to the nullspace of the data. Consequently, the components of the true operator $A$ that lie outside the data subspace remain at their initialization values and are never learned.
- **Core assumption:** The training data is sampled from a function space with a rank lower than the full dimensionality of the discretized grid.
- **Evidence anchors:**
  - [abstract]: "For linear models, the learned weights converge to a projection of the true operator onto the training data subspace, meaning generalization outside this subspace is impossible regardless of data quantity."
  - [Section 3.3, Theorem 3.2]: Proves convergence to $W^* = AUU^T + W_0(I - UU^T)$, showing the nullspace is preserved from initialization.
  - [corpus]: Neighbors confirm generalization is a major open problem, though this specific subspace projection proof is unique to the paper.

### Mechanism 2
- **Claim:** For physics-informed parameter discovery (inverse problems) using finite differences, the estimation error increases with the complexity (polynomial degree) of the training data due to truncation artifacts.
- **Mechanism:** A finite difference stencil of order $q$ introduces local truncation errors proportional to $\Delta x^q$. When fitting a physical parameter $w$, the loss function integrates these errors over the domain. Higher-order polynomial forcing functions activate higher-order derivative terms that the stencil cannot resolve, systematically biasing the recovered parameter away from the true value.
- **Core assumption:** The spatial discretization order $q$ is fixed and lower than the spectral content of the training data.
- **Evidence anchors:**
  - [abstract]: "For parameter learning... the error increases with both grid spacing and polynomial order of the training data."
  - [Section 3.2, Theorem 3.1]: Derives the error bound $|w-k|/|k| \approx \mu_q \Delta x^q$ dependent on data order $p$.
  - [corpus]: General consensus on PINN sensitivity to discretization, but this specific polynomial-order error mechanism is paper-specific.

### Mechanism 3
- **Claim:** Black-box neural operators can be mechanistically interpreted as Green's functions by probing the model with one-hot encoded impulses.
- **Mechanism:** The Green's function represents the system's response to a point source (Dirac delta). In a discretized setting, passing a one-hot vector $e_j$ through a learned model $Model(f)$ ideally produces the $j$-th column of the discrete Green's function matrix. If the model has generalized correctly, this extracted structure resembles the analytical kernel; if overfitted to a subspace, the structure is uninterpretable.
- **Core assumption:** The model has effectively learned a linear superposition principle or is operating in a regime where impulse responses are diagnostic of the global operator.
- **Evidence anchors:**
  - [abstract]: "...demonstrate a new mechanistic interpretability lens... whereby Green's function representations can be extracted from the weights of black-box models."
  - [Section 4.4, Eq 14]: Defines the extraction method $A_{ij} \leftrightarrow Model(f=e_j)_i$.
  - [corpus]: Weak connection; corpus focuses on operator learning accuracy rather than this specific interpretability extraction technique.

## Foundational Learning

- **Concept:** **Green's Functions**
  - **Why needed here:** This is the target structure the ML models are trying to approximate. Understanding that a linear PDE solution is an integral convolution against a kernel is required to interpret the "weights" and the proposed extraction method.
  - **Quick check question:** Can you explain why evaluating a model on a one-hot vector reveals a column of the Green's function operator?

- **Concept:** **Numerical Truncation Error (Finite Differences)**
  - **Why needed here:** Essential to understand Mechanism 2. One must grasp that discrete derivatives are approximations with error terms dependent on grid spacing $\Delta x$ and stencil width.
  - **Quick check question:** If you use a 3-point stencil (2nd order accurate) to fit data generated from a 4th-order polynomial, why does the error not vanish even if you have infinite data points?

- **Concept:** **Linear Subspaces and Rank**
  - **Why needed here:** Core to Mechanism 1. The paper frames generalization failure as a linear algebra problem where the "information content" (rank) of the dataset limits the learnable subspace.
  - **Quick check question:** If a dataset of size $N=10,000$ lies entirely on a line in 100-dimensional space, what is the effective rank of the data matrix?

## Architecture Onboarding

- **Component map:** Data Generator -> Solver (Ground Truth) -> Model (Variable Architecture) -> Evaluator (Cross-Validation Heatmap)

- **Critical path:** The "Cross-Validation Technique" is the central evaluation loop.
  1. Train model on dataset $D_{train}$ (e.g., Sine functions).
  2. Evaluate MSE on $D_{train}$ (In-distribution).
  3. Evaluate MSE on $D_{test}$ from a different subspace (OOD).
  4. Visualize the Cross-Error Heatmap (Fig 4) to diagnose if the model learned the true operator or just the subspace projection.

- **Design tradeoffs:**
  - **Physics-Informed vs. Black-Box:** The paper suggests physics-informed losses (PINNs/PI-DeepONet) do *not* solve the subspace limitation; they suffer similar generalization failures to black-box models if the data diversity is insufficient.
  - **Data Complexity:** Simpler, broader data (Piecewise Linear/FEM) often yields better generalization than "smoother" but lower-rank data (low-order polynomials).

- **Failure signatures:**
  - **The "Diagonal" Heatmap:** Low error only when Train set == Test set.
  - **The "Block Lower-Triangular" Pattern:** Indicates subspace generalization (e.g., training on Poly-5 generalizes to Poly-3, but not vice versa).
  - **Low Training Error, High OOD Error:** The primary indicator of subspace projection (Mechanism 1).

- **First 3 experiments:**
  1. **Reproduce Theorem 3.2:** Train a simple linear model $u=Wf$ on Polynomial data of degree $p=3$. Verify that the learned $W$ fails to generalize to Sine data, and visually confirm $W$ does not match the analytical Green's function matrix.
  2. **Green's Function Extraction:** Take the trained model from Exp 1. Input a one-hot vector. Visualize the output. Then, retrain on "FEM" (full rank) data and observe how the one-hot response transforms into the correct triangular Green's function shape.
  3. **Noise Sensitivity:** Add Gaussian noise to the training data and observe if the sharp generalization boundaries in the cross-validation heatmap "smear" or if the floor of the error simply rises (Paper suggests the latter).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the derived generalization bounds and subspace limitations persist for nonlinear differential equations, or does the lack of superposition fundamentally alter the failure modes?
- **Basis in paper:** [explicit] The analysis is restricted to "linear differential equations" and relies on Green's functions, though the authors note that "even linear DEs can pose a challenge" for future benchmarks.
- **Why unresolved:** The theoretical proofs (specifically Theorem 3.2) rely heavily on linearity and superposition to define the projection of the true operator onto the training subspace.
- **Evidence:** Extending Theorem 3.2 to a standard nonlinear benchmark (e.g., Navier-Stokes) to see if the model still converges to a projected subspace operator.

### Open Question 2
- **Question:** What specific architectural inductive biases cause DeepONets and Neural Operators to exhibit "opposing generalization behaviors" when trained on identical function subspaces?
- **Basis in paper:** [explicit] The abstract and conclusion state: "Surprisingly, different classes of models can exhibit opposing generalization behaviors."
- **Why unresolved:** The paper empirically observes this divergence across model types but focuses its theoretical derivations on the convergence of the linear and finite difference models rather than the architectural specificities of neural operators.
- **Evidence:** Ablation studies isolating architectural components (e.g., Fourier integral operators vs. trunk-net decompositions) to identify which feature causes the reversal in generalization direction.

### Open Question 3
- **Question:** Can active learning strategies be formulated to automatically identify and compensate for the "incomplete operator" problem by discovering the underlying subspace of the data?
- **Basis in paper:** [explicit] The conclusion suggests "extracting generalized scientific knowledge may require new methods capable of compensating for these limitations by discovering the underlying subspace of the data."
- **Why unresolved:** The paper rigorously defines the problem (convergence to $W^* = AU U^T$) but does not propose a procedural method for ensuring the training data spans the required subspace during data collection.
- **Evidence:** An algorithm that iteratively queries for training samples that maximize the rank of the learned operator or minimize the orthogonality defect of the weight matrix.

## Limitations
- The theoretical analysis is limited to linear differential equations and may not extend to nonlinear problems
- The empirical validation focuses on 1D problems, with unclear scalability to higher dimensions
- The Green's function extraction method assumes the learned model approximates a linear operator, limiting its applicability to highly nonlinear models

## Confidence
- **High Confidence:** The subspace projection mechanism for linear models (Mechanism 1) - supported by rigorous mathematical proof and clear empirical validation
- **Medium Confidence:** The finite difference error mechanism (Mechanism 2) - theoretical bounds exist but depend on specific assumptions about data and discretization
- **Medium Confidence:** Black-box Green's function extraction - empirical results are promising but the method's limitations for non-linear models are not fully characterized

## Next Checks
1. **Theoretical Extension:** Prove that the subspace projection phenomenon extends to non-linear operator learning architectures under appropriate linearization assumptions around the learned solution
2. **Dimensionality Scaling:** Test whether the observed generalization patterns persist when moving from 1D to 2D or 3D Poisson equations, particularly examining how data rank scales with dimensionality
3. **Regularization Impact:** Systematically study how different regularization strategies (weight decay, dropout, early stopping) affect the sharpness of generalization boundaries in the cross-validation heatmaps