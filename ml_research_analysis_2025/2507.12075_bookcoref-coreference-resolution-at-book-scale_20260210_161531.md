---
ver: rpa2
title: 'BOOKCOREF: Coreference Resolution at Book Scale'
arxiv_id: '2507.12075'
source_url: https://arxiv.org/abs/2507.12075
tags:
- book
- coreference
- coref
- character
- gold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BOOKCOREF, the first book-scale coreference
  resolution benchmark, addressing the gap in evaluation resources for long narrative
  texts. The authors propose an automatic pipeline that combines entity linking, LLM
  filtering, and windowed coreference expansion to produce high-quality silver annotations.
---

# BOOKCOREF: Coreference Resolution at Book Scale

## Quick Facts
- arXiv ID: 2507.12075
- Source URL: https://arxiv.org/abs/2507.12075
- Reference count: 27
- Primary result: First book-scale coreference resolution benchmark with 50 silver-annotated books and 3 gold test books, showing current models improve +20 CoNLL-F1 when trained on silver data but still lag behind shorter text performance

## Executive Summary
This paper introduces BOOKCOREF, the first book-scale coreference resolution benchmark, addressing the gap in evaluation resources for long narrative texts. The authors propose an automatic pipeline that combines entity linking, LLM filtering, and windowed coreference expansion to produce high-quality silver annotations. BOOKCOREF contains 50 silver-annotated books (over 10.8 million tokens) and a manually annotated test set of 3 books (229k tokens). The pipeline achieves 80.5 CoNLL-F1 on the test set, demonstrating high annotation quality. Experiments show that while long-document systems like Longdoc improve by +20 CoNLL-F1 when trained on BOOKCOREF, their performance still lags behind results on shorter texts, highlighting open challenges in book-scale coreference.

## Method Summary
The BOOKCOREF pipeline uses a three-stage approach: Cluster Initialization via fine-tuned ReLiK for Character Linking, Cluster Refinement via Qwen2 7B LLM filtering to ensure precision, and Cluster Expansion using Maverick on 1500-word windows followed by Maverick-xl on grouped windows (G=10) for full-book expansion. The dataset includes 50 silver-annotated books generated automatically and 3 gold-standard books manually annotated for evaluation. Long-document models like Longdoc and Maverick-xl are trained on the silver data using 1500-token windows and evaluated on the gold test set.

## Key Results
- BOOKCOREF pipeline achieves 80.5 CoNLL-F1 on the gold test set
- Longdoc improves by +20 CoNLL-F1 (from 46.6 to 67.0) when trained on BOOKCOREFsilver
- Performance gap persists between full-book evaluation and windowed evaluation, indicating fundamental challenges

## Why This Works (Mechanism)

### Mechanism 1
The BookCoref Pipeline generates high-quality silver coreference annotations by cascading from high-precision initialization to high-recall expansion. The pipeline first uses Character Linking (specialized Entity Linking) to tag explicit character mentions. A critical LLM-based verification step then filters these links to maximize precision, operating on the assumption that false positives are more damaging than false negatives. Finally, a coreference model (Maverick) expands these seed clusters on text windows to include pronouns and other mentions, with a second pass on grouped windows to recover mentions missed in local contexts.

### Mechanism 2
Training on the book-scale BOOKCOREFsilver dataset significantly improves the performance of long-document coreference models. Existing long-document systems (e.g., Longdoc) are trained on shorter benchmarks and struggle with full-book coherence. Fine-tuning on BOOKCOREFsilver exposes these models to realistic, long-range coreference patterns and entity distributions found in full narratives. This in-domain training data bridges the gap, teaching models to better maintain consistency over hundreds of thousands of tokens.

### Mechanism 3
A persistent performance gap exists between evaluating on full books versus shorter text windows, indicating a fundamental challenge for current models and metrics. This gap is not solved by training data alone. It points to architectural limitations in current models (e.g., incremental caching, windowed processing) which struggle to maintain global consistency across an entire book. Furthermore, standard coreference metrics (like CEAF_ϕ4 and B³) may be inherently biased or exhibit low agreement when evaluating predictions at the book scale, penalizing models disproportionately.

## Foundational Learning

- **Concept: Coreference Resolution**
  - Why needed here: This is the central task of the paper. It is the process of identifying all expressions (mentions) in a text that refer to the same real-world entity.
  - Quick check question: Given the sentences "Elizabeth read the letter. She sighed," does a coreference system link "She" to "Elizabeth"?

- **Concept: Entity Linking vs. Coreference Resolution**
  - Why needed here: The paper's pipeline separates these. Entity Linking connects explicit mentions (like "Mr. Darcy") to a knowledge base (here, the character list), while Coreference Resolution groups all mentions (including pronouns like "he") together. The pipeline uses EL for high-precision initialization and CR for high-recall expansion.
  - Quick check question: Would the task of connecting "Pride and Prejudice" to its Wikipedia article be considered entity linking or coreference resolution?

- **Concept: Silver vs. Gold Annotations**
  - Why needed here: The paper creates a large "silver" dataset automatically and a small "gold" dataset manually. Understanding that silver data is heuristically produced and contains noise, while gold data is human-curated and serves as the ground truth for evaluation, is critical for interpreting the results.
  - Quick check question: Why might training a model on a silver-standard dataset be riskier than training on a gold-standard dataset?

## Architecture Onboarding

- **Component map:**
  - Input: A book's full text + a list of character names
  - 1. Cluster Initialization: A fine-tuned ReLiK model performs Character Linking to create initial, high-recall clusters
  - 2. Cluster Refinement: A Qwen2 7B LLM verifies each link, filtering out low-confidence assignments to ensure high precision
  - 3. Cluster Expansion: A Maverick model expands seed clusters on 1500-word windows. A second pass on grouped windows (merging 10 consecutive windows) recovers mentions missed in isolation. Final output is merged, book-level clusters

- **Critical path:** The pipeline's success depends entirely on the Cluster Initialization and Refinement steps. If these fail to produce high-precision seed clusters for key characters, the expansion phase will propagate errors, rendering the final silver dataset ineffective for training.

- **Design tradeoffs:**
  - Precision vs. Recall in Pipeline: The design prioritizes precision early (LLM filtering), assuming recall is easier to regain during expansion. This risks low final recall if the expansion model is weak.
  - Window Size: Using small (1500-word) windows for the first expansion is efficient but creates a "edge case" where characters absent from a window cannot be resolved. The second, grouped-window pass mitigates this at the cost of increased computation.

- **Failure signatures:**
  - Pipeline Failure: Low scores on the BOOKCOREFgold test set for models trained on BOOKCOREFsilver, indicating the silver data is too noisy.
  - Model Failure (Memory/Context): Out-of-memory errors or prohibitive inference times when running standard coreference models on full books without adaptation.

- **First 3 experiments:**
  1. Validate Pipeline Quality: Run the full BookCoref Pipeline on the three gold-standard books and compare the output against the human annotations to verify the reported ~80.5 CoNLL-F1 score.
  2. Benchmark System Improvement: Train a long-document model like Longdoc on the BOOKCOREFsilver dataset and evaluate its performance on the BOOKCOREFgold test set. Compare this to its off-the-shelf performance to measure the improvement.
  3. Ablate Pipeline Components: Systematically disable a pipeline step (e.g., remove LLM filtering or skip the grouped-window expansion) and measure the performance drop on the gold test set to quantify each component's contribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can coreference evaluation metrics be refined to resolve the discrepancy between link-based scores (e.g., MUC) and entity-based scores (e.g., CEAF_ϕ4) when evaluating full-book predictions?
- Basis in paper: The authors note in Section 5 and Appendix G that while MUC scores remain high for full books, CEAF_ϕ4 drops significantly compared to windowed evaluations, highlighting the need to "increase the robustness of CR metrics in the book setting."

### Open Question 2
- Question: How can high-performance generative and encoder-only coreference models be adapted for book-scale processing without incurring prohibitive computational costs?
- Basis in paper: Section 9 lists "exploring efficient solutions for adapting current generative and encoder-only models for book-scale processing without incurring exponential computational costs" as a key open challenge.

### Open Question 3
- Question: Do current book-scale coreference resolution capabilities generalize to non-canonical or contemporary texts?
- Basis in paper: Section 10 states, "Testing with less renowned books would benefit the evaluation capabilities of BOOKCOREF gold, a direction that should be prioritized by future work."

## Limitations

- The pipeline's performance is highly dependent on manual intervention in LitBank annotations, with unspecified guidelines for filtering non-explicit mentions
- Memory requirements for full-book processing remain prohibitive, with Maverick requiring approximately 1200GB of VRAM
- The dataset is limited to English literature, raising questions about generalizability to other languages and domains

## Confidence

**High Confidence**: The reported pipeline performance (~80.5 CoNLL-F1 on gold test set) and the +20 CoNLL-F1 improvement from training on BOOKCOREFsilver are well-supported by the experimental results presented.

**Medium Confidence**: The claim that book-scale coreference remains fundamentally challenging despite training data improvements is supported but not definitively proven, as architectural limitations and metric biases may contribute to the performance gap.

**Low Confidence**: The long-term utility of BOOKCOREFsilver for training robust book-scale models depends on factors not fully explored, including the dataset's coverage of diverse narrative structures and potential systematic biases in the automatic annotation pipeline.

## Next Checks

1. **Component Ablation Study**: Systematically disable individual pipeline components (LLM filtering, grouped-window expansion) and measure their impact on gold test set performance to quantify each component's contribution to the final 80.5 CoNLL-F1 score.

2. **Metric Calibration Analysis**: Conduct a systematic study comparing CEAF_ϕ4 and B³ scores between book-scale and window-level evaluations on the same texts to quantify potential metric bias.

3. **Cross-Domain Generalization Test**: Evaluate models trained on BOOKCOREFsilver on coreference resolution tasks from different domains (e.g., legal documents, scientific texts) to assess the dataset's utility beyond literary narratives.