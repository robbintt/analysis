---
ver: rpa2
title: 'Design Probes for AI-Driven AAC: Addressing Complex Communication Needs in
  Aphasia'
arxiv_id: '2504.09435'
source_url: https://arxiv.org/abs/2504.09435
tags:
- system
- pwas
- design
- communication
- aphasia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored how AI can support individuals with aphasia
  in communication through a two-phase research through design approach. The researchers
  developed four AI-enhanced AAC prototypes addressing real-time communication and
  future conversation preparation needs, then evaluated them with 11 participants.
---

# Design Probes for AI-Driven AAC: Addressing Complex Communication Needs in Aphasia

## Quick Facts
- arXiv ID: 2504.09435
- Source URL: https://arxiv.org/abs/2504.09435
- Reference count: 40
- Primary result: AI-enhanced AAC prototypes show promise for supporting communication needs in aphasia, with visual verification and keyword-based sentence generation being particularly beneficial features.

## Executive Summary
This study explored AI-driven augmentative and alternative communication (AAC) solutions for individuals with aphasia through a two-phase research through design approach. The researchers developed four prototype systems addressing both real-time communication and future conversation preparation needs. Testing with 11 participants revealed that visual verification features, sentence generation from keywords, and error correction capabilities were particularly valuable. The study identified key design insights for future AAC systems, including the importance of multimodal feedback, personalized conversation summaries, and multiple sentence options to support user autonomy. However, challenges were noted around timing issues in group conversations and varying effectiveness based on aphasia severity levels.

## Method Summary
The research employed a two-phase approach combining user-centered design with empirical evaluation. In Phase 1, researchers conducted in-depth interviews with four individuals with aphasia to identify communication needs and challenges, then developed four AI-enhanced AAC prototypes. Phase 2 involved testing these prototypes with 11 participants through structured tasks including 1:1 and 2:1 conversations. The evaluation used Likert-scale questionnaires to measure perceived benefits and identify areas for improvement. Participants engaged with different prototype combinations to assess various features like visual verification, sentence generation, and error correction capabilities.

## Key Results
- Visual verification features and keyword-based sentence generation were consistently valued across participants
- AI-generated images helped validate expressions and reduce misunderstandings in communication
- Participants valued multiple sentence options for autonomy and multimodal feedback for clarity
- Error correction features showed varying effectiveness based on aphasia severity levels
- Timing issues in group conversations emerged as a significant challenge

## Why This Works (Mechanism)
The AI-driven AAC system works by providing real-time support for both production and comprehension aspects of communication. The visual verification mechanism allows users to confirm AI-generated content matches their intent before sharing, reducing errors and building confidence. Keyword-to-sentence generation bridges the gap between limited vocabulary and complex expression needs, while multimodal feedback (combining text, audio, and visual elements) accommodates the diverse sensory processing challenges common in aphasia. The system's flexibility in offering multiple response options and personalized conversation summaries addresses the individual variability in aphasia symptoms and communication preferences.

## Foundational Learning
1. **Aphasia heterogeneity**: Different types of aphasia affect communication in varied ways (why needed: to understand diverse user needs; quick check: compare feature effectiveness across severity levels)
2. **Multimodal feedback importance**: Combining text, audio, and visual elements improves comprehension (why needed: aphasia often affects multiple communication modalities; quick check: measure comprehension accuracy with different feedback combinations)
3. **Timing sensitivity**: Real-time conversation support must account for processing delays (why needed: aphasia affects processing speed; quick check: measure response time impacts on conversation flow)
4. **Visual verification benefits**: Allowing users to confirm AI outputs reduces anxiety and errors (why needed: builds trust in system reliability; quick check: compare error rates with/without verification)
5. **Personalization requirements**: Individual preferences and capabilities vary significantly (why needed: supports user autonomy and adoption; quick check: track usage patterns across different personalization settings)
6. **Error correction mechanisms**: Systems must accommodate communication breakdowns (why needed: aphasia often involves word-finding difficulties; quick check: measure repair success rates)

## Architecture Onboarding
**Component Map**: User Input -> Keyword Extraction -> AI Processing -> Multiple Sentence Generation -> Visual Verification -> Output Selection -> Multimodal Feedback -> Conversation Summary

**Critical Path**: User Input → Keyword Extraction → AI Processing → Visual Verification → Output Selection

**Design Tradeoffs**: 
- Real-time responsiveness vs. accuracy of AI-generated content
- Simplicity of interface vs. flexibility of features
- Personalization options vs. system complexity
- Visual emphasis vs. text/audio balance

**Failure Signatures**:
- AI outputs misaligned with user intent
- Timing delays disrupting conversation flow
- Overwhelming interface complexity
- Insufficient error correction capabilities

**First Experiments**:
1. Test visual verification accuracy by comparing error rates with/without verification step
2. Evaluate keyword extraction accuracy across different aphasia severity levels
3. Measure conversation flow impact when using multiple vs. single sentence options

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Small sample size (N=11) limits generalizability across aphasia severity levels
- Single-session design may not capture long-term adoption patterns
- Artificial experimental setting may not reflect natural conversation complexity
- Limited exploration of cultural and linguistic factors in diverse populations

## Confidence
High: Visual verification features and sentence generation from keywords
Medium: Error correction and AI-generated images effectiveness
Low: Timing-related observations in group conversations

## Next Checks
1. Conduct longitudinal field studies with 30+ participants across multiple severity levels to assess sustained effectiveness and adoption patterns
2. Implement A/B testing comparing visual verification versus audio feedback in real conversation scenarios with 20 participants
3. Evaluate cross-cultural adaptability by testing prototypes with bilingual participants and in multilingual conversation contexts