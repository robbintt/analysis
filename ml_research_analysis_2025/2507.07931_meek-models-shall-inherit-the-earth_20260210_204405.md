---
ver: rpa2
title: Meek Models Shall Inherit the Earth
arxiv_id: '2507.07931'
source_url: https://arxiv.org/abs/2507.07931
tags:
- loss
- compute
- inference
- performance
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a model showing that the performance gap between
  large AI models and smaller models with limited compute budgets shrinks over time
  due to diminishing returns in compute scaling. Under current training practices,
  even exponentially growing compute investments by big organizations yield only modest
  capability advantages over much smaller models.
---

# Meek Models Shall Inherit the Earth

## Quick Facts
- arXiv ID: 2507.07931
- Source URL: https://arxiv.org/abs/2507.07931
- Authors: Hans Gundlach; Jayson Lynch; Neil Thompson
- Reference count: 19
- Models show convergence in capabilities over time due to diminishing returns on compute scaling

## Executive Summary
This paper develops a mathematical model demonstrating that the performance gap between large AI models and smaller models with limited compute budgets shrinks over time. The key driver is diminishing marginal returns to compute scaling - as frontier labs invest exponentially more compute, the resulting capability improvements follow a power law that compresses the gap. The authors show this convergence occurs under current training practices, where shared algorithmic and hardware progress benefits all actors while investment growth is isolated to frontier labs. This implies greater democratization of AI as high-performance models become accessible to a wider range of actors, though the authors caution that regulation should evolve beyond simply limiting compute to target data, algorithms, and research breakthroughs.

## Method Summary
The authors use analytical modeling based on Chinchilla scaling laws to predict loss convergence between SOTA and meek models. They derive equations for loss difference over time incorporating hardware, algorithmic, and investment growth rates. The model translates loss differences to benchmark performance using sigmoid fitting to MMLU data. Empirical validation uses Artificial Analysis LLM Leaderboard data comparing frontier APIs to open-source models. The analysis includes both training and inference modes, with inference showing faster convergence due to rapidly falling costs.

## Key Results
- Loss difference between large and small models peaks around 2026-2027 then converges
- Performance gap on MMLU benchmarks narrows from ~20% to near parity over time
- Inference cost convergence occurs much faster than training convergence (9x vs 3.6x annual improvement)
- SOTA models maintain advantages in reasoning tasks, but these gaps also narrow over time

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The performance gap between high-compute and low-compute models shrinks because marginal returns on compute diminish following a power law.
- **Mechanism:** Training loss scales as $L(C) \propto C^{-\alpha} + L_0$. Because this curve flattens as compute ($C$) increases, exponentially higher investments by frontier labs yield only linear or sub-linear improvements in loss reduction, compressing the capability gap.
- **Core assumption:** The Chinchilla scaling law holds ($\alpha \approx 0.155$) and irreducible loss $L_0$ is approached asymptotically.
- **Evidence anchors:**
  - [abstract] "...diminishing returns to compute scaling will lead to a convergence of AI model capabilities."
  - [section] Equation (1) & (2) define the power-law relationship; Figure 1 visualizes the narrowing loss gap.
  - [corpus] "The Race to Efficiency" supports the shift away from raw compute scaling toward efficiency, while "The Illusion of Diminishing Returns" warns that short-task benchmarks may mask compounding long-horizon capabilities, challenging the "diminishing returns" premise.
- **Break condition:** If scaling laws shift (e.g., new architectures break the $C^{-\alpha}$ curve) or if "breakthrough" capabilities discontinuously jump (Section 6), convergence fails.

### Mechanism 2
- **Claim:** "Meek" models improve faster relative to their baseline because algorithmic and hardware progress are shared factors, whereas investment growth is isolated to frontier actors.
- **Mechanism:** Both SOTA and meek models benefit from hardware ($g_h \approx 1.4$x) and algorithmic efficiency ($g_{alg} \approx 2.8$x). Loss is determined by effective compute. As these "shared" multipliers grow, the relative contribution of the "unshared" investment multiplier ($g_i$) to total capability diminishes, closing the gap.
- **Core assumption:** Algorithmic progress and hardware improvements diffuse widely and are accessible to resource-constrained actors.
- **Evidence anchors:**
  - [abstract] "...marginal capability returns to raw compute shrink substantially."
  - [section] Section 2 derivation of Eq (3), showing shared terms $(g_{alg}g_h)^t$ dominate the investment term $g_i^t$ over time.
  - [corpus] "Understanding Scaling Laws..." suggests scaling laws describe success but not necessarily the mechanism, implying reliance on these rates is an assumption.
- **Break condition:** If frontier labs monopolize algorithmic breakthroughs (private data/methods) or if hardware export controls sever the $g_h$ link for meek actors.

### Mechanism 3
- **Claim:** The utility gap closes faster than the loss gap because benchmark performance maps to loss via a sigmoid function, saturating at ceilings.
- **Mechanism:** Capabilities (benchmarks) are modeled as a sigmoid of loss (Eq 6). As models approach the benchmark ceiling (e.g., 80% MMLU), large differences in loss translate to vanishingly small differences in score, creating an illusion of parity.
- **Core assumption:** Benchmark saturation is a valid proxy for general capability saturation.
- **Evidence anchors:**
  - [abstract] "...proxies like training loss differences capture important capability measures..."
  - [section] Section 3.1, Figure 2 (Sigmoid fit), and Figure 3 (Performance difference).
  - [corpus] Corpus evidence for specific sigmoid mapping is weak; no direct papers on sigmoid-benchmark saturation were provided.
- **Break condition:** If new benchmarks with higher ceilings (e.g., ARC-AGI) are introduced, or if "spiky" capabilities emerge that don't follow the sigmoid smoothness (Section 6 adversarial settings).

## Foundational Learning

- **Concept:** Power Law Scaling (Chinchilla/Kaplan)
  - **Why needed here:** The entire convergence thesis rests on the mathematical property that $Loss \propto Compute^{-\alpha}$. Without this, the "diminishing returns" argument fails.
  - **Quick check question:** If compute increases by 100x, and $\alpha=0.5$, does loss halve or drop by an order of magnitude?

- **Concept:** Cross-Entropy Loss & Perplexity
  - **Why needed here:** The paper uses "loss difference" ($\Delta L$) as the primary metric of inequality. Understanding that $\Delta L$ represents "bits of surprise" per token is crucial for the hypothesis testing view (Section 3.2).
  - **Quick check question:** Does a lower loss indicate the model is more or less "surprised" by the test data?

- **Concept:** Ergodicity & Stationary Distributions
  - **Why needed here:** The paper uses these assumptions (Appendix A) to prove that two models become indistinguishable (SPRT) as loss converges.
  - **Quick check question:** If the data distribution changes over time (non-stationary), does the "indistinguishability" proof still hold?

## Architecture Onboarding

- **Component map:** Chinchilla scaling law parameters (A, α, L₀) -> Growth rates (g_h, g_alg, g_i) -> Loss difference equation (Eq 3) -> Sigmoid mapping (Eq 6) -> Performance gap over time

- **Critical path:** Estimating the **Inflection Time** (Eq 4). This requires validating the investment growth rate ($g_i$) and the scaling exponent ($\alpha$). If the exponent is lower (slower returns), the convergence takes longer.

- **Design tradeoffs:**
  - **Training vs. Inference:** The paper models two modes. The "Inference Model" (Eq 12) converges much faster ($g_{inf} \approx 9x$) than the "Training Model" ($g_i \approx 3.6x$).
  - **Assumption:** Relying on the Inference model assumes users can access the *results* of large training runs (e.g., via APIs or distillation) at falling prices.

- **Failure signatures:**
  - **Divergence:** If the gap in Figure 5 (MMLU-Pro) widens instead of narrows, the model is broken.
  - **Paradigm Shift:** If "reasoning" (Section 6) changes the distribution from "human text" to "synthetic chain-of-thought," the fixed-distribution assumption invalidates the SPRT indistinguishability proof.

- **First 3 experiments:**
  1. **Re-calibrate $g_i$:** Update the compute investment growth rate using 2024-2025 training runs (e.g., Llama 3, GPT-5 rumors) to see if the inflection point moves.
  2. **Validate Inference Convergence:** Plot the price/performance curve of open-source models vs. frontier APIs (Section 5) using Artificial Analysis data to check if the 9x inference cost reduction trend persists.
  3. **Stress Test the Sigmoid:** Fit the sigmoid function (Eq 6) to a non-saturated benchmark (e.g., coding challenges) to see if the "saturation" effect is an artifact of MMLU's ceiling.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the convergence of "meek" models apply to training paradigms using adaptively chosen data or synthetic data rather than fixed distributions?
- **Basis in paper:** [explicit] Section 6 states, "We are uncertain about our model in these new cases where AIs are trained on adaptively chosen data or their own synthetic data."
- **Why unresolved:** The current model relies on fixed-distribution next-token prediction assumptions, which may break if RL or synthetic techniques fundamentally alter the learning distribution.
- **What evidence would resolve it:** Empirical scaling analysis of model performance gaps specifically in reinforcement learning or synthetic data regimes.

### Open Question 2
- **Question:** Does the convergence of capabilities hold in adversarial settings where small performance differences result in large win-rate differentials?
- **Basis in paper:** [explicit] Section 6 notes that in competitive games, "adversaries with exponentially increasing compute might continuously diverge from their competitor."
- **Why unresolved:** While overall loss converges, "winner-take-all" dynamics in adversarial tasks imply that tiny loss advantages might still yield total dominance, preventing democratization.
- **What evidence would resolve it:** Longitudinal analysis of win-rates between large and small models in competitive environments (e.g., cyber-security or strategy games).

### Open Question 3
- **Question:** Does scaling inference-time compute yield the same diminishing returns as pre-training compute?
- **Basis in paper:** [explicit] Section 2.1 asks, "Will this new paradigm eventually yield the same diminishing returns as pretraining compute?"
- **Why unresolved:** While the authors argue one model suggests similar dynamics, they acknowledge inference compute might be "qualitatively different" and yield new types of pretraining.
- **What evidence would resolve it:** Studies mapping inference compute scaling to benchmark performance to verify if it follows the power-law decay seen in training.

## Limitations

- The model assumes Chinchilla scaling laws hold indefinitely, but models approaching irreducible loss L₀ may see slower convergence
- The assumption that algorithmic progress and hardware improvements diffuse equally to all actors may not hold if frontier labs develop proprietary architectures
- The sigmoid mapping from loss to benchmark performance assumes benchmarks capture general capability, but "spiky" capabilities may not follow this smooth relationship
- The ergodic and stationary distribution assumptions underlying the SPRT indistinguishability proof may break down with non-stationary data distributions

## Confidence

- **High Confidence:** The mathematical framework for loss convergence (Equations 1-3) is well-specified and the diminishing returns mechanism is sound given the scaling law assumptions
- **Medium Confidence:** The empirical validation using MMLU and MMLU-Pro benchmarks shows the expected trend, but the sigmoid mapping and its generalizability across different benchmark types requires further testing
- **Low Confidence:** The extrapolation to governance implications assumes current trends continue and that the identified chokepoints (data, algorithms, research) will remain the primary regulatory targets as AI evolves

## Next Checks

1. **Re-calibrate Investment Growth Rate:** Update the compute investment growth rate gᵢ using 2024-2025 training run data (Llama 3, GPT-5 rumors) to verify the predicted inflection point in 2026-2027. Track whether the 3.57x annual growth rate persists or slows.

2. **Validate Inference Model Assumptions:** Test the inference convergence prediction (g_inf≈9x) by analyzing Artificial Analysis LLM Leaderboard data to verify whether the price/performance gap between frontier models and open-source alternatives continues narrowing at the projected rate.

3. **Stress Test Sigmoid Generalizability:** Fit the sigmoid function mapping loss to performance on multiple benchmarks with different ceilings (MMLU, HumanEval, ARC-AGI) to determine whether the "saturation" effect is universal or benchmark-specific, particularly for tasks requiring reasoning or long-horizon planning.