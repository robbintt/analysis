---
ver: rpa2
title: A novel Neural-ODE model for the state of health estimation of lithium-ion
  battery using charging curve
arxiv_id: '2505.05803'
source_url: https://arxiv.org/abs/2505.05803
tags:
- battery
- attention
- batteries
- https
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accurately estimating the
  state of health (SOH) of lithium-ion batteries, a critical factor for ensuring the
  safe and reliable operation of electric vehicles. The authors propose a novel data-driven
  approach called ACLA, which integrates an attention mechanism, convolutional neural
  network (CNN), and long short-term memory network (LSTM) within an augmented neural
  ordinary differential equation (ANODE) framework.
---

# A novel Neural-ODE model for the state of health estimation of lithium-ion battery using charging curve

## Quick Facts
- arXiv ID: 2505.05803
- Source URL: https://arxiv.org/abs/2505.05803
- Reference count: 0
- Primary result: ACLA achieves RMSE as low as 1.01% and 2.24% on TJU and HUST datasets for SOH estimation

## Executive Summary
This paper addresses the challenge of accurately estimating the state of health (SOH) of lithium-ion batteries, a critical factor for ensuring the safe and reliable operation of electric vehicles. The authors propose a novel data-driven approach called ACLA, which integrates an attention mechanism, convolutional neural network (CNN), and long short-term memory network (LSTM) within an augmented neural ordinary differential equation (ANODE) framework. The model uses normalized charging time corresponding to specific voltages in the constant current charging phase as input to predict SOH and remaining useful life. ACLA is trained on NASA and Oxford datasets and validated on TJU and HUST datasets. The results show that ACLA outperforms benchmark models NODE and ANODE, achieving root mean square errors (RMSE) for SOH estimation as low as 1.01% and 2.24% on the TJU and HUST datasets, respectively. This demonstrates ACLA's superior accuracy and generalizability across diverse battery datasets.

## Method Summary
The ACLA model treats battery SOH decay as a continuous-time dynamic system using an augmented neural ordinary differential equation (ANODE) framework. The model extracts features from the constant current (CC) charging curve, specifically normalized charging time corresponding to specific voltages during the CC phase. These features are processed through an attention mechanism that weights specific intervals of the charging curve, followed by 1D CNN layers (64 and 32 filters) for local feature extraction and LSTM layers (64 hidden units) to capture degradation trends across cycles. The augmented ODE solver (Dopri5) integrates the hidden states to predict SOH. The model is trained using AdamW optimizer with Lookahead, employing a warm-up schedule and custom loss function combining MSE of SOH and mean error of time features.

## Key Results
- ACLA achieves RMSE of 1.01% on TJU dataset and 2.24% on HUST dataset for SOH estimation
- The model outperforms benchmark models NODE and ANODE in both accuracy and generalizability
- Attention mechanism focusing on the start of charging curve (Att_start) reduces training time by ~15-20% and improves accuracy on HUST dataset
- ACLA maintains performance when trained on NASA (LCO/NCA) and tested on HUST (LFP) without fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Modeling State of Health (SOH) decay as a continuous-time dynamic system allows for more robust generalization across battery chemistries than discrete-step models.
- **Mechanism:** The Augmented Neural Ordinary Differential Equation (ANODE) framework treats SOH evolution as an initial value problem ($dSOH/d\tau = f(SOH(\tau), \theta)$). By using an ODE solver (Dopri5) to compute the state trajectory, the model learns the *rate* of degradation rather than just fitting to discrete capacity points. This continuous representation appears to better capture the underlying physical degradation process (e.g., SEI layer growth) which is inherently continuous.
- **Core assumption:** Battery degradation follows a deterministic path describable by ordinary differential equations, and the hidden states learned by the network sufficiently represent physical aging factors.
- **Evidence anchors:**
  - [abstract]: "...integrates... into the augmented neural ordinary differential equation (ANODE) framework."
  - [section 3.1.3]: "SOH decay is treated as a dynamic evolution system. Thus, the goal of ANODE is to solve the hidden states to predict the SOH."
  - [corpus]: Related work in the corpus supports the shift toward physics-informed modeling, specifically "Learning the P2D Model for Lithium-Ion Batteries," which emphasizes deriving parameters from physical models to ensure stability.
- **Break condition:** Performance degrades if the degradation trajectory is highly non-deterministic or stochastic (e.g., rapid random thermal events) which cannot be easily modeled as a smooth ODE trajectory.

### Mechanism 2
- **Claim:** Applying attention mechanisms to specific segments of the charging curve reduces noise and focuses computational capacity on the most chemically active phases.
- **Mechanism:** The attention layer learns to assign non-uniform weights to the input feature vector (normalized charging times). The paper finds that weighting the *start* of the charging curve (voltage rise phase) is most effective for LFP batteries (HUST), likely because internal resistance changes are most pronounced here.
- **Core assumption:** Not all voltage-time points in a charging curve are equally informative for predicting SOH; specific regions correlate more strongly with degradation modes like capacity fade or impedance rise.
- **Evidence anchors:**
  - [abstract]: "...integrates the attention mechanism... within an augmented neural ordinary differential equation..."
  - [section 4.3.1]: "On the HUST dataset, when attention layers focus on the first three features [Att_start], the model achieves optimal performance... reflecting an accuracy improvement of up to 18%."
  - [corpus]: Corpus evidence is weak regarding specific attention placement on charging curves; however, "SOH-KLSTM" and "Degradation Self-Supervised Learning" generally support hybrid attention models for feature selection in battery diagnostics.
- **Break condition:** If the charging protocol varies significantly (e.g., different C-rates) without normalization, the attention weights learned for a specific curve shape may fail to generalize.

### Mechanism 3
- **Claim:** Normalized charging time acts as a chemistry-agnostic health indicator, enabling cross-dataset generalization.
- **Mechanism:** Instead of using raw voltage or current, the model uses *time* to reach specific voltage setpoints during the Constant Current (CC) phase. As a battery ages, internal resistance increases, causing voltage to rise faster (less time to reach cutoff) or capacity to decrease. This time-based feature is normalized, making it transferable between different battery types (e.g., NCA vs LFP).
- **Core assumption:** The relationship between charging time and capacity fade is monotonic and consistent across the specific battery chemistries included in the training set.
- **Evidence anchors:**
  - [abstract]: "This model employs normalized charging time corresponding to specific voltages in the constant current charging phase as input..."
  - [section 2.2]: "Extracting features from the constant current (CC) charging curve is regarded as an effective approach... It directly reflects aging phenomena such as increased internal resistance."
  - [corpus]: "Adapting Amidst Degradation" explicitly discusses the challenge of data scarcity and the need for cross-domain estimation, validating the problem this mechanism attempts to solve, though it proposes test-time training as an alternative solution.
- **Break condition:** The mechanism may fail if the testing battery has a drastically different voltage window (e.g., different cut-off voltages) than the training set, making the "specific voltages" inputs incompatible.

## Foundational Learning

- **Concept:** Neural Ordinary Differential Equations (NODE/ANODE)
  - **Why needed here:** The core of the ACLA model is an ODE solver replacing standard network layers. You cannot debug or tune this model without understanding how the adjoint method works for backpropagation through the ODE solver.
  - **Quick check question:** Can you explain why an ANODE (Augmented NODE) might learn faster than a standard NODE? (Answer: Augmented dimensions provide extra "information highways" for gradients, preventing training instability).

- **Concept:** Partial Charging Curve Features
  - **Why needed here:** The model does not use full discharge cycles. It relies specifically on the CC (Constant Current) charging phase. Understanding electrochemical impedance spectroscopy (EIS) equivalent circuit models helps explain *why* voltage rise time correlates with SOH.
  - **Quick check question:** Why is the CC phase preferred over the CV (Constant Voltage) phase for feature extraction in this context? (Answer: The CC phase voltage curve is highly sensitive to internal resistance changes, a key aging indicator).

- **Concept:** Attention Mechanisms in Time Series
  - **Why needed here:** The paper explicitly optimizes "where" to look in the charging curve.
  - **Quick check question:** In the context of this paper, does the attention mechanism look across *time* (cycles) or across *features* (voltage points)? (Answer: Across features/voltage points within a single cycle, specifically weighting specific voltage intervals).

## Architecture Onboarding

- **Component map:** Data Prep (Time extraction) -> Attention Weighting -> CNN/LSTM Encoding -> ANODE Integration
- **Critical path:** The most fragile component is the Data Prep; if the voltage sampling points ($N_V$) do not align with the test battery's voltage range, the model fails immediately.
- **Design tradeoffs:**
  - **Attention Scope:** The paper shows `Att_start` (first 3 points) reduces training time by ~15-20% and improves accuracy on HUST, whereas `Att_all` is computationally heavier and prone to noise.
  - **Solver Step Size:** Using the adjoint method allows variable step sizes (Dopri5), trading off precise integration for memory efficiency during training.
- **Failure signatures:**
  - **Under-estimation:** Observed in the NODE baseline (Fig 6), likely due to "information loss" in deep sequences that the LSTM/Attention in ACLA prevents.
  - **Divergence:** If the training dataset is too small (<50%), the ODE trajectory can diverge from the physical reality, though ACLA shows robustness down to 50% splits.
- **First 3 experiments:**
  1. **Baseline Validation (Section 4.2):** Train on NASA/Oxford (Batteries A1, A3, B1, B3, B7) and validate on the remaining batteries. Verify RMSE < 1.5% to ensure the attention mechanism is functioning.
  2. **Ablation on Attention (Section 4.3.1):** Implement `Att_start` vs. `Att_all` on the HUST dataset. You should observe that `Att_start` yields lower RMSE (~2.24%) than `Att_all` (~2.43%) and significantly lower training time.
  3. **Cross-Chemistry Generalization (Section 4.3):** Train on NASA (LCO/NCA) and Test on HUST (LFP) without fine-tuning. Success is defined as RMSE < 2.5%. If this fails, check the normalization of the charging time features.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a dynamic, learnable attention window improve performance compared to the fixed attention regions (start, mid, end) used in this study?
- Basis in paper: [explicit] Table 4 shows that the optimal attention configuration varies by dataset (TJU performs best with "Att-end" while HUST performs best with "Att-start"), yet the authors adopt a fixed "Att_start" configuration for the final comparative analysis in Table 5.
- Why unresolved: The paper does not propose a mechanism to automatically adapt the attention region to the specific degradation characteristics of the battery being tested, leaving a potential performance gap for datasets that favor middle or end-region features.
- What evidence would resolve it: A modified ACLA architecture where the attention weights and their target regions are dynamically selected via a secondary network or meta-learning approach, validated against the static configurations.

### Open Question 2
- Question: How does the ACLA model perform under variable ambient temperature conditions, which significantly alter charging curves?
- Basis in paper: [inferred] Section 2.2 claims the selected charging profile features are "robust across different operating conditions," but the experimental validation (Table 1) utilizes datasets with distinct chemistries without explicitly isolating or discussing the impact of temperature fluctuations on the normalized charging time inputs.
- Why unresolved: Charging time is highly sensitive to temperature; without explicit validation on temperature-varied datasets, it is unclear if the model is learning degradation features or merely fitting temperature-dependent charging kinetics.
- What evidence would resolve it: Validation results from a controlled experiment where the model, trained on data from one temperature (e.g., 25°C), is tested on degradation data collected at significantly different ambient temperatures (e.g., 0°C or 45°C).

### Open Question 3
- Question: Is the ACLA model applicable to fast-charging protocols where the Constant Current (CC) phase is shortened or eliminated?
- Basis in paper: [explicit] Section 2.2 states the method relies on "the constant current charging phase" and notes that "the charging protocol is typically well controlled," implicitly excluding modern fast-charging strategies (e.g., constant power or boost charging) from the current scope.
- Why unresolved: The reliance on equidistant voltage sampling during the CC phase (Table 2) suggests the input structure may fail or become undefined if the charging profile deviates significantly from the standard CC-CV (Constant Current-Constant Voltage) paradigm.
- What evidence would resolve it: A feasibility study applying the normalized time feature extraction to batteries cycled with multi-stage constant current or pulse charging protocols.

## Limitations
- The model's performance on batteries with significantly different voltage windows or charging protocols remains unverified
- The assumption that battery degradation follows a smooth, deterministic ODE trajectory may not hold for batteries experiencing rapid, stochastic degradation events
- Specific architectural details of CNN/LSTM and ANODE components (kernel sizes, padding, internal network structure) were not fully specified

## Confidence
- **High:** Cross-dataset validation results demonstrating superior accuracy and generalizability
- **Medium:** Exact reproducibility limited by unspecified architectural details (kernel sizes, padding, internal network structure)
- **Low:** Model's applicability to fast-charging protocols and variable temperature conditions remains unverified

## Next Checks
1. **Architecture Verification**: Implement the exact CNN/LSTM architecture with specific kernel sizes, strides, and padding as used in the original implementation, then compare training curves and final RMSE on a subset of the NASA dataset.
2. **Voltage Window Sensitivity**: Test the model's performance when trained on batteries with one voltage range (e.g., 3.6-4.2V) and evaluated on batteries with different ranges (e.g., 3.0-4.2V), measuring degradation in accuracy.
3. **Stochastic Degradation Test**: Introduce artificial noise or non-smooth degradation patterns into the training data to assess whether the ODE-based model's performance degrades compared to discrete-step alternatives.