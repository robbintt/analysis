---
ver: rpa2
title: Agentic Entropy-Balanced Policy Optimization
arxiv_id: '2510.14545'
source_url: https://arxiv.org/abs/2510.14545
tags:
- arxiv
- zhang
- aepo
- wang
- entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of high-entropy issues in agentic
  reinforcement learning for web agents, specifically high-entropy rollout collapse
  and high-entropy token gradient clipping. The proposed Agentic Entropy-Balanced
  Policy Optimization (AEPO) algorithm tackles these issues by balancing entropy in
  both the rollout and policy update phases.
---

# Agentic Entropy-Balanced Policy Optimization

## Quick Facts
- **arXiv ID**: 2510.14545
- **Source URL**: https://arxiv.org/abs/2510.14545
- **Reference count**: 40
- **Primary result**: AEPO achieves 47.6% Pass@1 on GAIA with 1K RL samples using Qwen3-14B

## Executive Summary
Agentic Entropy-Balanced Policy Optimization (AEPO) addresses high-entropy issues in web agent reinforcement learning by balancing entropy in both rollout and policy update phases. The algorithm tackles rollout collapse through dynamic sampling budget allocation and consecutive branch penalties, while preserving exploratory gradients via stop-gradient clipping operations. Experiments demonstrate consistent performance improvements across 14 benchmarks, with AEPO maintaining stable policy entropy while improving rollout diversity.

## Method Summary
AEPO operates within the VERL framework using PPO with entropy-aware modifications. During rollout, it employs entropy pre-monitoring to allocate global versus branch sampling budgets based on question entropy (H_root) and average tool-call entropy (H_avg_tool). The algorithm penalizes consecutive high-entropy branch sampling to prevent collapse into few trajectories. For policy updates, AEPO modifies standard PPO clipping by applying stop-gradient operations on high-entropy tokens, preserving gradients when tokens have positive advantage. The method uses 1K open-source web search samples, evaluates on 14 benchmarks, and achieves results with Qwen3-14B using 16×H800 GPUs.

## Key Results
- Achieves 47.6% Pass@1 and 65.0% Pass@5 on GAIA benchmark
- Obtains 11.2% Pass@1 and 26.0% Pass@5 on Humanity's Last Exam
- Demonstrates 43.0% Pass@1 and 70.0% Pass@5 on WebWalkerQA
- Shows stable entropy curves versus entropy collapse in baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dynamically allocating rollout sampling budgets based on entropy pre-monitoring improves exploration coverage.
- **Mechanism:** AEPO first generates one complete trajectory to estimate question entropy (H_root) and average tool-call entropy (H_avg_tool). When H_root > H_avg_tool, the algorithm allocates more budget to global sampling (diverse starting points); when H_root < H_avg_tool, it prioritizes branch sampling (deepening exploration along promising paths).
- **Core assumption:** Information gain correlates positively with entropy, and the initial trajectory's entropy distribution generalizes to subsequent rollouts.
- **Evidence anchors:**
  - [abstract]: "adaptively allocate global and branch sampling budget through entropy pre-monitoring"
  - [Section 3.1.1]: Equation (5) shows m = k · σ(β · (H_root - H_avg_tool)), with theoretical justification from information bottleneck theory
  - [corpus]: Related work on entropy-gated optimization exists but focuses on token-level, not trajectory-level allocation
- **Break condition:** If question entropy is consistently miscalibrated (e.g., misleading initial trajectory), budget allocation may systematically favor wrong strategy.

### Mechanism 2
- **Claim:** Penalizing consecutive high-entropy branch sampling prevents rollout collapse and increases trajectory diversity.
- **Mechanism:** AEPO tracks consecutive branching count l per trajectory and applies penalty factor (1 - P̂(l)) to branch probability. As l increases, branching probability decreases, forcing the algorithm to explore other trajectories.
- **Core assumption:** High-entropy consecutive steps indicate over-commitment to a single reasoning path rather than genuine uncertainty requiring deeper exploration.
- **Evidence anchors:**
  - [Section 2.2.1]: Pilot experiments show 56.5% of high-entropy turns are consecutive, and 93.4% of branches concentrate on 1-3 trajectories with ARPO
  - [Figure 6]: AEPO shows more distinct cluster centers (62 vs 54) and tighter intra-cluster distances
  - [corpus]: ECHO and related work address tree-structured rollouts but do not explicitly penalize consecutive branching
- **Break condition:** If genuine multi-step uncertainty exists (correct exploration requires consecutive branching), penalty may prematurely terminate useful paths.

### Mechanism 3
- **Claim:** Stop-gradient operation on high-entropy clipping preserves gradients on exploratory tokens while maintaining stable forward pass.
- **Mechanism:** In Equation (8), the clipping term uses sg(δ) in forward pass (preserving original computation) but allows gradient flow to 1 + ε_h when δ > 1 + ε_h and advantage is positive. This prevents gradient zeroing on high-entropy tokens that vanilla PPO would clip.
- **Core assumption:** High-entropy tokens with positive advantage represent valuable exploration that should be reinforced, not suppressed.
- **Evidence anchors:**
  - [Section 3.2.1]: Gradient update rule in Equation (9) explicitly shows F_j,t(θ) = 1 + ε_h for high-entropy positive-advantage tokens
  - [Figure 8]: AEPO maintains more stable entropy curves compared to clipping-optimized RL methods that show sharp fluctuations
  - [corpus]: "Arbitrary Entropy Policy Optimization" paper confirms GRPO suffers from entropy collapse
- **Break condition:** If high-entropy tokens are actually noise rather than signal, preserving their gradients may amplify incorrect patterns.

## Foundational Learning

- **Concept: Importance Sampling Ratio (δ = π_θ / π_ref)**
  - Why needed here: Central to understanding why clipping affects gradients and how stop-gradient modifies this relationship
  - Quick check question: If δ > 1 + ε_h, what happens to the gradient in vanilla PPO vs AEPO?

- **Concept: Token Entropy Calculation (H_t = -Σ p_t,j log p_t,j)**
  - Why needed here: Quantifies model uncertainty at each step, driving both rollout and policy decisions
  - Quick check question: How would you interpret H_t = 0 vs H_t = log(V) where V is vocabulary size?

- **Concept: Advantage Estimation (Ā = (r - mean) / std)**
  - Why needed here: Determines which tokens receive reinforcement; AEPO modifies this with entropy-awareness
  - Quick check question: Why does standard outcome-based RL assign the same advantage to all tokens in a trajectory?

## Architecture Onboarding

- **Component map:** Entropy Pre-Monitoring Module -> Dynamic Rollout Engine -> Entropy-Balanced Policy Optimizer -> Tool Environment Interface
- **Critical path:** 1. Pre-monitoring → determines sampling budget allocation (m global, k-m branch) 2. Rollout phase → generates trajectory pool with adaptive branching 3. Reward computation → outcome-based rewards from tool results 4. Advantage estimation → combines accuracy-based and entropy-based advantages 5. Policy update → stop-gradient clipping applied per Equation (8-9)
- **Design tradeoffs:** Pre-monitoring adds one trajectory generation overhead per batch but reduces wasted sampling; consecutive penalty may miss genuinely deep exploration paths in exchange for diversity; stop-gradient requires careful tuning of ε_h (default 0.28) to balance stability vs exploration
- **Failure signatures:** Entropy collapse: Training curves show sharp entropy drops → check clipping thresholds, may need higher ε_h; Rollout collapse: Branch distribution shows < 3 unique paths per batch → increase base sampling probability α; Gradient explosion: Loss spikes after early updates → verify stop-gradient implementation, check KL coefficient
- **First 3 experiments:** 1. Baseline comparison on single dataset (e.g., WebWalkerQA): Implement AEPO vs GRPO vs ARPO with identical backbone (Qwen3-8B), tracking entropy curves and branching diversity. Expected: AEPO shows more stable entropy and diverse rollouts. 2. Ablation study: Remove pre-monitoring (fix global/branch split at 50/50) and remove consecutive penalty separately. Expected: Both components contribute ~3-5% performance gain. 3. Clipping threshold sweep: Test ε_h ∈ {0.2, 0.28, 0.35} on GAIA benchmark. Expected: 0.28 matches paper findings; too low causes collapse, too high reduces stability.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the "Entropy Pre-Monitoring" mechanism's heuristic linking entropy to information gain effectively distinguish between informative uncertainty and model confusion?
- **Basis in paper:** [inferred] Section 3.1.1 states the authors "simply model the sampling information gain" as proportional to entropy (Eq. 4) without theoretical proof.
- **Why unresolved:** High entropy can indicate noise or hallucination rather than useful exploration potential; allocating global budget based purely on H_root might waste resources on ill-posed queries.
- **What evidence would resolve it:** A correlation analysis between the entropy-derived budget allocation (m) and the actual information gain (reward improvement) of the sampled trajectories.

### Open Question 2
- **Question:** Is the "Consecutive Branch Penalty" (linear slope γ) robust across different tool environments, or does it require per-tool tuning?
- **Basis in paper:** [inferred] Section 4.4 fixes the penalty slope (γ=0.2) and sensitivity (β=0.2) without ablation, despite the branching statistics varying by tool type (Section 2.2).
- **Why unresolved:** A fixed linear penalty might over-suppress exploration in tools with naturally high variance (e.g., broad web searches) while failing to curb collapse in deterministic tools (e.g., calculators).
- **What evidence would resolve it:** Ablation studies showing performance sensitivity to γ across heterogeneous tool sets (e.g., comparing search engines vs. code interpreters).

### Open Question 3
- **Question:** Does the stop-gradient operation on high-entropy tokens introduce gradient bias that hampers convergence in large-scale training regimes (>1K samples)?
- **Basis in paper:** [inferred] The authors highlight results with "1K RL samples" and modify the gradient flow (Eq. 9), but do not verify if the induced bias (discussed in Appendix B) degrades stability over long horizons.
- **Why unresolved:** While useful for preserving gradients in early exploration, enforcing a fixed gradient scale (1+ε_h) prevents the model from naturally reducing the gradient magnitude as it masters certain skills, potentially leading to oscillation in later training.
- **What evidence would resolve it:** Training convergence curves on larger datasets (e.g., 50k+ samples) comparing AEPO's stability against unbiased baseline algorithms like GRPO.

## Limitations

- The entropy pre-monitoring mechanism assumes question entropy reliably predicts subsequent rollout structure without validation across diverse web tasks
- The consecutive branch penalty (0.2×l) is introduced without systematic ablation to quantify its marginal contribution versus simpler diversity mechanisms
- The stop-gradient clipping approach fundamentally changes the optimization landscape, but theoretical analysis of its convergence properties is absent

## Confidence

- **High confidence**: The identification of rollout collapse and gradient clipping as fundamental problems in agentic RL is well-supported by pilot experiments (Section 2.2.1). The empirical improvements on benchmark datasets (47.6% GAIA Pass@1, 11.2% HLE Pass@1 with 1K samples) are statistically significant and robust across 14 benchmarks.
- **Medium confidence**: The three proposed mechanisms work synergistically, but their individual contributions are not fully decoupled. The entropy-aware advantage formulation (Equation 11) lacks clear interpretation of the weight parameter 'a', and its relationship to established advantage estimation methods is not established.
- **Low confidence**: The generalizability of results beyond the specific Qwen3-14B architecture and VERL framework remains untested. The paper doesn't address how AEPO would perform with different backbone sizes or alternative tool environments.

## Next Checks

1. **Mechanism isolation experiment**: Implement AEPO with pre-monitoring disabled (fixed 50/50 global/branch split) and consecutive penalty disabled separately, then combined. Measure performance drop and entropy stability to quantify each mechanism's marginal contribution.

2. **Threshold sensitivity analysis**: Systematically vary the consecutive branch penalty coefficient (0.1×l, 0.2×l, 0.3×l) and entropy clipping bounds (ε_h = 0.2, 0.28, 0.35) on GAIA benchmark. Plot performance curves to identify optimal operating regions and robustness margins.

3. **Architecture generalization test**: Implement AEPO on a different backbone (e.g., Llama-3-8B or Qwen2.5-14B) using identical training protocol. Compare performance retention rate to establish whether improvements are architecture-specific or more broadly applicable.