---
ver: rpa2
title: 'Breaking Free from MMI: A New Frontier in Rationalization by Probing Input
  Utilization'
arxiv_id: '2503.06202'
source_url: https://arxiv.org/abs/2503.06202
tags:
- rationale
- conference
- information
- input
- rationales
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the diminishing marginal returns problem in
  maximum mutual information (MMI)-based rationalization, where identifying additional
  rationale components provides minimal benefit once a substantial portion has been
  found. The authors propose an alternative objective based on the norm of intermediate
  representations, inspired by observations from out-of-distribution detection.
---

# Breaking Free from MMI: A New Frontier in Rationalization by Probing Input Utilization

## Quick Facts
- **arXiv ID**: 2503.06202
- **Source URL**: https://arxiv.org/abs/2503.06202
- **Reference count**: 40
- **Primary result**: N2R (norm to rationale) method outperforms MMI-based rationalization with 5.1-13.5% F1 score improvements across 4 text and 1 graph classification datasets

## Executive Summary
This paper addresses the diminishing marginal returns problem in maximum mutual information (MMI)-based rationalization, where identifying additional rationale components provides minimal benefit once a substantial portion has been found. The authors propose an alternative objective based on the norm of intermediate representations, inspired by observations from out-of-distribution detection. Their method, N2R (norm to rationale), outperforms vanilla MMI and several improved variants on four text classification datasets and one graph classification dataset using three different network architectures (GRUs, BERT, and GCN). The norm-based approach achieves better rationale quality and faster convergence while being more efficient than LLM-based alternatives.

## Method Summary
The paper introduces N2R (norm to rationale), a novel rationalization method that replaces the MMI objective with a norm-based approach using intermediate representation norms. The method is inspired by the observation that models with better input utilization tend to perform better in out-of-distribution detection tasks. N2R is tested across four text classification datasets (SST-2, Rotten Tomatoes, AGNews, IMDB) and one graph classification dataset (Reddit), using three different architectures: GRUs, BERT, and GCN. The approach is evaluated against vanilla MMI, several MMI variants, and a representative LLM (llama-3.1-8b-instruct), showing consistent improvements in rationale quality measured by F1 scores.

## Key Results
- N2R achieves 5.1% to 13.5% F1 score improvements over MMI on benchmark datasets
- The method converges faster than MMI-based approaches
- N2R outperforms vanilla MMI and several improved variants across all tested architectures
- The approach is more efficient than LLM-based rationalization while achieving comparable results
- Combining N2R with MMI yields further improvements

## Why This Works (Mechanism)
The paper proposes that intermediate representation norms serve as a proxy for input utilization, with larger norms indicating better use of input information. This mechanism is inspired by the observation that models with better input utilization tend to perform better in out-of-distribution detection tasks. By optimizing for representation norms rather than prediction reproduction, the method can identify more informative rationale components without suffering from diminishing marginal returns.

## Foundational Learning
- **Maximum Mutual Information (MMI)**: A common rationalization objective that aims to maximize the mutual information between input and output; needed to understand the baseline being improved upon
- **Rationalization**: The process of identifying which input components (words, tokens) are most relevant for model predictions; fundamental concept for interpretability
- **Out-of-distribution (OOD) detection**: The task of identifying inputs that differ from the training distribution; provides theoretical motivation for the norm-based approach
- **Intermediate representation norms**: The magnitude of hidden layer activations used as a proxy for input utilization; central to the proposed method
- **Diminishing marginal returns**: The phenomenon where additional rationale components provide minimal benefit once a substantial portion has been identified; the core problem being addressed

## Architecture Onboarding
- **Component map**: Input text -> Encoder (GRU/BERT/GCN) -> Intermediate representations -> Norm calculation -> Rationale selection -> Output prediction
- **Critical path**: The norm-based objective flows through the intermediate representations to identify rationale tokens, then these tokens are used for final prediction
- **Design tradeoffs**: Norm-based objectives may be less interpretable than MMI's mutual information maximization but avoid diminishing returns; simpler than LLM-based approaches while maintaining quality
- **Failure signatures**: Poor OOD detection performance may indicate insufficient input utilization; diminishing returns in rationale identification suggests MMI-like behavior
- **3 first experiments**: 1) Compare N2R vs MMI F1 scores on SST-2 dataset, 2) Test convergence speed differences between methods, 3) Evaluate OOD detection performance as a proxy for input utilization

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- The theoretical connection between input utilization and OOD detection success remains speculative rather than rigorously established
- Generalization to more diverse text classification tasks (multi-label, hierarchical classification) and non-classification scenarios remains untested
- The practical significance of 5.1-13.5% F1 improvements depends on specific application context
- Comparison with llama-3.1-8b-instruct uses a single LLM without exploring other configurations

## Confidence
- **High confidence**: N2R outperforms MMI on benchmark datasets; norm-based objectives improve rationale quality and convergence speed
- **Medium confidence**: The theoretical motivation linking input utilization to OOD detection success is plausible but not rigorously validated
- **Medium confidence**: Combining N2R with MMI yields further improvements, though the practical benefit over standalone N2R needs clarification

## Next Checks
1. Conduct ablation studies isolating the contribution of norm-based objectives from architectural differences, testing N2R across a broader range of network architectures including transformers with different attention mechanisms
2. Validate the method on more challenging text classification tasks such as multi-label classification, hierarchical classification, and datasets with significant class imbalance to assess robustness
3. Systematically compare N2R with multiple LLM-based rationalization approaches (varying sizes and prompting strategies) to establish the precise conditions under which N2R provides efficiency advantages while maintaining quality