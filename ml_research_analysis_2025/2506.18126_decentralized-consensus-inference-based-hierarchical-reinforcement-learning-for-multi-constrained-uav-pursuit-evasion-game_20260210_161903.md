---
ver: rpa2
title: Decentralized Consensus Inference-based Hierarchical Reinforcement Learning
  for Multi-Constrained UAV Pursuit-Evasion Game
arxiv_id: '2506.18126'
source_url: https://arxiv.org/abs/2506.18126
tags:
- policy
- uni00000013
- formation
- agents
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of Cooperative Evasion and Formation
  Coverage (CEFC) in multi-constrained pursuit-evasion games (MC-PEG) for quadrotor
  UAV swarms under communication limitations. The proposed solution, Consensus Inference-based
  Hierarchical Reinforcement Learning (CI-HRL), features a two-level framework: a
  high-level policy using Consensus-oriented Multi-Agent Communication (ConsMAC) for
  target localization and a low-level policy for obstacle avoidance, navigation, and
  formation using alternative training and policy distillation.'
---

# Decentralized Consensus Inference-based Hierarchical Reinforcement Learning for Multi-Constrained UAV Pursuit-Evasion Game

## Quick Facts
- arXiv ID: 2506.18126
- Source URL: https://arxiv.org/abs/2506.18126
- Reference count: 40
- Multi-constrained UAV swarm achieves superior formation stability and collision avoidance in pursuit-evasion games under communication limitations

## Executive Summary
This paper addresses the Cooperative Evasion and Formation Coverage (CEFC) challenge in multi-constrained pursuit-evasion games for quadrotor UAV swarms operating under communication limitations. The proposed Consensus Inference-based Hierarchical Reinforcement Learning (CI-HRL) framework decomposes the complex evasion-coverage task into strategic hierarchy with a high-level policy for target localization using Consensus-oriented Multi-Agent Communication (ConsMAC), and a low-level policy for obstacle avoidance, navigation, and formation maintenance. CI-HRL enables decentralized consensus inference from local observations, improving collaborative evasion and task completion. Experimental results from MPE and SITL simulations demonstrate CI-HRL outperforms baselines in formation stability, navigation efficiency, and collision avoidance, with SITL validation confirming robustness against real-world factors like wind and sensor noise.

## Method Summary
CI-HRL implements a two-level hierarchical framework where the high-level policy selects discrete anchor points (target destinations) every 10 steps using ConsMAC for consensus-based global state estimation from local observations, while the low-level policy outputs continuous acceleration commands at every step to maintain formation and avoid obstacles. The framework employs alternative training (AT-M) to pre-train specialized formation and obstacle avoidance modules in isolation before merging, and uses policy distillation to compress multiple fixed-size policies into one adaptive policy for dynamic group sizes. The system operates under decentralized execution with centralized training, using Proximal Policy Optimization (PPO) for learning.

## Key Results
- CI-HRL achieves 13.9% higher formation stability (F) and 9.2% better navigation efficiency (NE) compared to PPO baseline in MPE simulations
- Collision rate reduced by 42.1% compared to baselines while maintaining superior formation quality
- SITL validation shows CI-HRL maintains performance under real-world factors including wind and sensor noise, with 18.3% better coverage rate than PPO

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing the coupled evasion-coverage task into a strategic hierarchy separates temporal scales, allowing a slow "manager" to handle consensus while a fast "worker" handles reactive dynamics.
- **Mechanism:** The framework splits the control loop. The **High-Level Policy** operates at a lower frequency (every 10 steps), selecting discrete anchor points (target destinations) based on long-term goals and adversarial positioning. The **Low-Level Policy** operates at every step, receiving the anchor point as a sub-goal and outputting continuous acceleration commands to maintain formation and avoid obstacles. This prevents the high-level planner from micromanaging collision avoidance, which typically destabilizes training.
- **Core assumption:** The environment dynamics allow for a distinct separation of time-scales; specifically, that a target selected every 10 steps remains valid long enough for the low-level controller to react to immediate physical constraints.
- **Evidence anchors:**
  - [abstract] "...delegates target localization to a high-level policy, while adopting a low-level policy to manage obstacle avoidance..."
  - [Section IV-A] "CI-HRL automatically determines the anchor point... as the temporary target location over a specified duration... while the low-level policy solely accounts for formation with neighbors..."
  - [corpus] Similar hierarchical separations are used in "Hierarchical Policy-Gradient Reinforcement Learning for Multi-Agent Shepherding Control" to manage complex target dynamics.
- **Break condition:** If the adversary moves significantly faster than the high-level decision frequency (10 steps), the anchor points may become stale, forcing the low-level policy into unrecoverable "dead-ends."

### Mechanism 2
- **Claim:** Supervised alignment of local communication embeddings to the global state forces heterogeneous local agents to converge on a shared representation ("consensus") without a central broadcaster.
- **Mechanism:** The **ConsMAC** module uses a memory-augmented attention network to process neighbor messages. Crucially, it trains a "Global Estimator" to predict the true global state ($g_t$) from these local messages using MSE loss ($L_{ConsMAC}$). By forcing the latent message vector ($m$) to contain enough information to reconstruct the global state, the mechanism ensures that if two agents successfully reconstruct the global state, their internal representations (and thus their high-level decisions) are implicitly aligned.
- **Core assumption:** The local observations from neighbors within the limited communication range ($\delta_{obs}$) contain sufficient information to reconstruct the relevant parts of the global state.
- **Evidence anchors:**
  - [abstract] "...Consensus-oriented Multi-Agent Communication (ConsMAC), to enable agents to perceive global information and establish consensus from local states..."
  - [Section IV-C-2] "...minimize (11) ensures $F(m) \to g$... interpreted as the establishment of consensus among neighbors."
  - [corpus] "Consensus-based Decentralized Multi-agent Reinforcement Learning" similarly utilizes consensus protocols to align agents in decentralized settings.
- **Break condition:** In sparse communication scenarios (e.g., isolated agents), the Global Estimator loss cannot converge, leading to hallucinated global states and inconsistent anchor point selection.

### Mechanism 3
- **Claim:** Alternative training (AT-M) resolves the "collision-formation" conflict by pre-training specialized modules in isolation before integrating them.
- **Mechanism:** Standard MAPPO struggles with the trade-off between tight formation (requiring proximity) and collision avoidance (requiring separation). AT-M splits the policy network into a Formation/Navigation module and an Obstacle Avoidance module. It trains the former in an empty environment and the latter in a dense obstacle environment. It then fuses the weights (taking the Formation module from step 1 and the Obstacle module from step 2) and fine-tunes. This prevents the "catastrophic interference" where learning to avoid obstacles degrades formation-keeping capabilities.
- **Core assumption:** The skills required for navigation/formation and obstacle avoidance are largely independent and can be composed additively in the network's feature space.
- **Evidence anchors:**
  - [Section IV-B-2] "...AT-M aims to obtain a policy network... consists of three MLP-based parts... AT-M first independently trains DNNs... and obtains two sets of corresponding parameters..."
  - [Table III] Shows that "AT-M-Step 3" (the merged/fine-tuned model) outperforms single-stage training ("Step 1" or "Step 2" alone) in balancing formation ($F$) and collision ($C$).
- **Break condition:** If obstacles require complex maneuvers that fundamentally break the formation topology (e.g., narrow corridors requiring single-file line), the pre-trained formation module may fight the obstacle module during the fine-tuning phase.

## Foundational Learning

- **Concept:** **Dec-POMDP (Decentralized Partially Observable Markov Decision Process)**
  - **Why needed here:** The entire system relies on agents acting on local observations ($z_i$) rather than global states. Understanding this distinction is critical to grasping why ConsMAC is necessary for "global" consensus.
  - **Quick check question:** Can an agent directly observe the position of an adversary blocked by an obstacle without communication? (Answer: No, it relies on $M_{nei,i}$).

- **Concept:** **Policy Distillation**
  - **Why needed here:** The low-level policy must handle dynamic group sizes (e.g., 3 agents splitting into 2 and 1). The paper uses distillation to compress multiple fixed-size policies (teacher models) into one adaptive policy (student model) to save memory and enable runtime flexibility.
  - **Quick check question:** How does the system handle a transition from a 5-agent formation to a 3-agent formation without crashing? (Answer: The student policy $\pi_L$ learns to imitate the specific formation policies for $c=3,4,5...$ via MSE loss on actions).

- **Concept:** **CTDE (Centralized Training with Decentralized Execution)**
  - **Why needed here:** This explains how the Global Estimator is trained (it needs the true global state $g_t$ as a label, available only during training) but runs decentralized later.
  - **Quick check question:** Does the Global Estimator have access to the true global state $g_t$ during the execution phase in the field? (Answer: No, that is only used for the supervised loss $L_{ConsMAC}$ during training).

## Architecture Onboarding

- **Component map:** Local LiDAR ($d$) -> High-Level (ConsMAC) -> Anchor Point ($p_{a,i}$) -> Low-Level (AT-M) -> Acceleration ($u_i$)
- **Critical path:** The **ConsMAC Global Estimator** is the linchpin. If the supervised loss ($L_{ConsMAC}$) does not converge, the messages $m$ do not encode global consensus, and the Anchor Point Selector makes inconsistent decisions (e.g., two agents splitting to different targets when they should stick together).
- **Design tradeoffs:**
  - **Discrete vs. Continuous Anchor Points:** The paper uses a discrete set $\{-8, 0, 8\}$ for efficiency. This limits precision but drastically reduces the high-level action space complexity.
  - **AT-M vs. Curriculum Learning:** AT-M is faster but requires manual module definition; Curriculum Learning (CL-M) is more automated but showed performance degradation in the paper (Table V) due to "irreversible loss of formation ability."
- **Failure signatures:**
  - **Oscillation:** If the high-level decision frequency (every 10 steps) is too slow for a fast-moving adversary, agents may oscillate between targets.
  - **Consensus Collapse:** If similarity matrices (Fig. 11) show low cosine similarity during critical turns, the communication range $\delta_{obs}$ may be too small, or the message dimension (64) is insufficient.
  - **Deadlocks:** AT-M may fail if the "Formation" and "Obstacle" MLPs output opposing acceleration vectors that cancel out.
- **First 3 experiments:**
  1.  **Sanity Check (MPE):** Train the Low-Level Policy (AT-M) *only* on a fixed formation (e.g., $c=5$) in a static obstacle field. Verify that the "merged" Step 3 model actually performs better than the "obstacle-only" Step 2 model (as per Table III).
  2.  **Consensus Ablation:** Run the High-Level policy in MPE without the ConsMAC communication module (ConsMAC-wo-Com). Verify performance drops significantly (Fig. 6) to prove the mechanism isn't just memorizing a fixed route.
  3.  **SITL Transfer:** Deploy the trained policy to the Gazebo SITL environment with wind enabled. Check the "Formation Stability" metric ($F$) to ensure the low-level controller compensates for physical disturbances (Table VIII).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the CI-HRL framework be adapted to ensure convergence and stability in super-large scale scenarios where centralized critics typically struggle?
- Basis in paper: [explicit] The conclusion identifies "policy convergence challenges in super-large scale scenarios" stemming from the exponential growth of complexity in the centralized critic and topological constraints.
- Why unresolved: Current experiments validated performance up to 15 agents, but the centralized training component of the CTDE architecture faces theoretical and computational limits as agent count increases significantly.
- What evidence would resolve it: Demonstrated policy convergence and maintained performance metrics in simulations with significantly larger swarm sizes (e.g., >50 agents), potentially utilizing fully decentralized training methods.

### Open Question 2
- Question: Can an altitude planner be effectively integrated into the framework to enable 3D maneuverability without degrading the learned 2D formation and navigation stability?
- Basis in paper: [explicit] Section VI explicitly proposes future work on "the incorporation of an altitude planner for altitude adjustments while maintaining swarmâ€™s fundamental 2D behaviors."
- Why unresolved: The current system model constrains UAVs to a fixed altitude ($u_z \equiv 0$) to simplify the problem, leaving the interaction between dynamic altitude adjustments and horizontal formation control unexplored.
- What evidence would resolve it: Successful validation in SITL environments where agents dynamically adjust altitude to avoid obstacles or optimize paths while preserving horizontal formation error metrics.

### Open Question 3
- Question: Does training the swarm against adaptive adversaries that dynamically adjust their strategies improve robustness compared to the fixed or rule-based adversaries used in current evaluations?
- Basis in paper: [explicit] The authors suggest that "advances in adversarial RL" provide a promising direction for designing "adaptive adversaries that can dynamically adjust their strategies."
- Why unresolved: The current study primarily utilizes a default PPO policy or simple rule-based strategies (e.g., chase nearest) for the predator, which may not reflect sophisticated real-world adversarial tactics.
- What evidence would resolve it: Comparative analysis showing higher evasion success rates and task completion when CI-HRL agents are trained against self-evolving adversarial policies versus the static baselines.

## Limitations

- Communication Range Dependency: The ConsMAC mechanism relies heavily on the observation range $\delta_{obs}$ for message exchange, with no analysis of performance degradation as communication range varies.
- Sim-to-Real Transfer Gaps: SITL validation uses simplified physics compared to real-world conditions, not addressing GPS drift, actuator delays, or complex wind patterns.
- Scalability Constraints: The hierarchical approach shows effectiveness with up to 6 agents, but does not establish theoretical or empirical bounds on scalability or analyze communication overhead growth.

## Confidence

**High Confidence:** The hierarchical decomposition mechanism (Mechanism 1) is well-supported by ablation studies showing degradation when high-level decisions are made at every step rather than every 10 steps.

**Medium Confidence:** The ConsMAC consensus mechanism (Mechanism 2) is theoretically sound, but confidence is reduced by the lack of analysis showing how message dimensionality affects consensus quality across different scenarios.

**Medium Confidence:** The alternative training approach (Mechanism 3) shows strong empirical results, but the manual decomposition of policy modules requires domain expertise that may not generalize to other task combinations.

## Next Checks

1. **Communication Range Sensitivity Analysis:** Systematically vary $\delta_{obs}$ from 2 m to 20 m in MPE environments to quantify the threshold at which consensus breaks down, and determine if adaptive communication ranges could improve robustness.

2. **Formation Topology Stress Test:** Evaluate CI-HRL on narrow passage scenarios requiring single-file navigation to determine if the formation and obstacle avoidance modules conflict irreconcilably, and test whether the policy can learn to temporarily dissolve formations.

3. **Extended Scalability Test:** Scale experiments to 12+ agents in the same MPE environment to identify at what point the discrete anchor point space becomes insufficient and whether communication latency causes consensus divergence.