---
ver: rpa2
title: 'AggTruth: Contextual Hallucination Detection using Aggregated Attention Scores
  in LLMs'
arxiv_id: '2506.18628'
source_url: https://arxiv.org/abs/2506.18628
tags:
- attention
- heads
- aggtruth
- detection
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of detecting contextual hallucinations
  in Large Language Models (LLMs) during Retrieval-Augmented Generation (RAG) tasks,
  which remains a significant challenge for deploying LLMs in real-world applications.
  The proposed method, AggTruth, analyzes the distribution of internal attention scores
  focused on provided context passages to detect hallucinations.
---

# AggTruth: Contextual Hallucination Detection using Aggregated Attention Scores in LLMs

## Quick Facts
- arXiv ID: 2506.18628
- Source URL: https://arxiv.org/abs/2506.18628
- Reference count: 22
- Proposed method AggTruth achieves up to 0.886 AUROC for hallucination detection across multiple LLMs and tasks

## Executive Summary
This paper introduces AggTruth, a novel method for detecting contextual hallucinations in Large Language Models during Retrieval-Augmented Generation tasks. The approach analyzes the distribution of internal attention scores focused on provided context passages to identify when models generate information not supported by the retrieved context. Four attention score aggregation techniques (Sum, Cosine Similarity, Entropy, and Jensen-Shannon Divergence) are proposed and evaluated across multiple LLMs including Llama-2, Llama-3.1, Phi-3.5, and Gemma-2 on QA and summarization tasks. The method demonstrates superior performance compared to existing state-of-the-art approaches, with the Sum aggregation technique achieving the highest AUROC values ranging from 0.617 to 0.886 across different datasets and tasks.

## Method Summary
AggTruth works by analyzing attention patterns within LLMs during context processing to detect hallucinations. The method computes attention scores between the [CLS] token and all tokens in retrieved context passages, then aggregates these scores using one of four techniques: Sum (simple addition of attention scores), Cosine Similarity (measuring similarity to a uniform distribution), Entropy (measuring uncertainty in attention distribution), and Jensen-Shannon Divergence (measuring deviation from uniform distribution). These aggregated scores serve as features for hallucination detection. The approach is evaluated across multiple models and tasks, demonstrating that careful selection of attention heads (using Spearman correlation or above-random performance) can achieve comparable performance while reducing computational complexity. The Sum aggregation consistently performs best, though other methods show competitive results in specific scenarios.

## Key Results
- AggTruth outperforms state-of-the-art hallucination detection methods across multiple LLMs and tasks
- The Sum aggregation technique achieves the highest AUROC values (0.617-0.886) across most datasets
- Using only 10-20% of attention heads selected by Spearman correlation or above-random performance achieves comparable or better results than using all heads
- Performance is stable in both same-task and cross-task settings, indicating good generalization capability

## Why This Works (Mechanism)
AggTruth leverages the observation that when LLMs generate hallucinated content, their attention patterns diverge from the expected distribution focused on relevant context. By analyzing how attention weights are distributed across context tokens, the method can detect when models are relying less on provided information and potentially generating unsupported content. The aggregation techniques transform raw attention patterns into meaningful features that capture different aspects of attention distribution, allowing for robust hallucination detection across various scenarios.

## Foundational Learning
- Attention mechanisms: How LLMs process and weigh different input tokens; needed to understand how AggTruth analyzes internal model behavior; quick check: verify understanding of self-attention and cross-attention concepts
- Retrieval-Augmented Generation: The RAG framework that provides context for generation; needed to understand the problem space; quick check: explain how retrieved context is integrated into the generation process
- Information retrieval metrics: AUROC, precision, recall; needed to evaluate detection performance; quick check: calculate AUROC from a confusion matrix
- Distribution comparison measures: Entropy, Jensen-Shannon Divergence; needed to understand aggregation techniques; quick check: compute entropy for a simple probability distribution
- Statistical correlation: Spearman correlation; needed to understand head selection methodology; quick check: calculate Spearman correlation between two ranked lists

## Architecture Onboarding

**Component Map:**
[CLS token] -> [Attention heads] -> [Context tokens] -> [Attention scores] -> [Aggregation] -> [Hallucination score]

**Critical Path:**
The critical path involves computing attention scores between the [CLS] token and context tokens, applying the chosen aggregation method, and using the resulting score for hallucination detection. This process must be efficient enough to run in real-time or near real-time for practical deployment.

**Design Tradeoffs:**
The method trades off computational complexity against detection accuracy. Using all attention heads provides comprehensive information but is computationally expensive, while selecting a subset of heads reduces complexity but may miss important patterns. The four aggregation techniques represent different tradeoffs between sensitivity to different types of attention distribution changes.

**Failure Signatures:**
The method may fail when attention patterns don't clearly distinguish between hallucinated and non-hallucinated generations, particularly in cases where models correctly generate content with minimal attention to context or when context itself is ambiguous. Performance may also degrade when attention distributions are inherently noisy or when the relationship between attention patterns and hallucination is not strong.

**Three First Experiments:**
1. Compute attention scores for a simple QA example with clear context and verify the aggregation output
2. Compare aggregation results between a clearly hallucinated and non-hallucinated generation
3. Test the effect of using different percentages of attention heads on detection performance

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The study focuses only on QA and summarization tasks, limiting generalizability to other LLM applications
- Performance variation across aggregation methods suggests the approach may not be universally optimal
- Reliance on synthetic datasets raises questions about real-world applicability
- The computational efficiency gains from using fewer attention heads need further validation

## Confidence
- High confidence in the core methodology and experimental design
- Medium confidence in the generalizability of results across diverse real-world scenarios
- Medium confidence in the computational efficiency claims due to limited ablation studies

## Next Checks
1. Test AggTruth on additional task types beyond QA and summarization, such as code generation, creative writing, or multi-modal tasks, to assess broader applicability.
2. Validate the computational efficiency claims by conducting a comprehensive ablation study comparing performance using different percentages of attention heads (e.g., 5%, 10%, 20%, 50%, 100%) across multiple models and tasks.
3. Conduct experiments using real-world datasets from production LLM deployments to assess performance in practical scenarios with noisy, incomplete, or adversarial inputs.