---
ver: rpa2
title: 'FedAgentBench: Towards Automating Real-world Federated Medical Image Analysis
  with Server-Client LLM Agents'
arxiv_id: '2509.23803'
source_url: https://arxiv.org/abs/2509.23803
tags:
- data
- agents
- client
- across
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces FedAgentBench, the first benchmark for evaluating
  LLM agents in automating real-world federated learning workflows for medical image
  analysis. The benchmark includes 201 medical datasets across 6 modalities, 40 FL
  algorithms, and evaluates 24 LLMs across 4 key FL phases: client selection, data
  preprocessing, label harmonization, and federated training.'
---

# FedAgentBench: Towards Automating Real-world Federated Medical Image Analysis with Server-Client LLM Agents

## Quick Facts
- **arXiv ID:** 2509.23803
- **Source URL:** https://arxiv.org/abs/2509.23803
- **Reference count:** 5
- **Primary result:** First benchmark evaluating LLM agents on automating federated learning workflows for medical image analysis across 201 datasets and 24 models

## Executive Summary
This paper introduces FedAgentBench, the first benchmark for evaluating LLM agents in automating real-world federated learning workflows for medical image analysis. The benchmark includes 201 medical datasets across 6 modalities, 40 FL algorithms, and evaluates 24 LLMs across 4 key FL phases: client selection, data preprocessing, label harmonization, and federated training. Proprietary models like GPT-4.1 achieve near-perfect scores (94-100%), while open-source models like DeepSeek-V3 and Qwen QwQ 32B show competitive performance (80-91%). Label harmonization remains the most challenging task. Fine-grained guidance consistently outperforms goal-oriented prompting, and larger models don't always perform better. The framework is modular, privacy-preserving, and extensible to other domains.

## Method Summary
The method employs a role-specialized multi-agent system with 7 LLM agents (S1-S4 server-side, C1-C3 client-side) coordinating across 4 FL phases using 16 tools. Agents orchestrate FL workflows without accessing raw data, preserving privacy while automating client selection, preprocessing, label harmonization, and training. The system uses two prompting styles: fine-grained (step-by-step) and goal-oriented (high-level). Evaluation runs on 201 curated medical datasets across 6 modalities with injected heterogeneity, testing 24 LLMs across 5 runs per task with both guidance types. Performance metrics include success rates, task-specific metrics (precision/recall/F1 for selection, schema compliance for preprocessing, exact-match accuracy for harmonization), and token/time usage.

## Key Results
- GPT-4.1 achieves near-perfect scores (94-100%) across all phases with fine-grained guidance
- DeepSeek-V3 and Qwen QwQ 32B show competitive performance (80-91%) despite being open-source
- Label harmonization consistently shows lowest success rates across all models (exact-match accuracy 0.61 for GPT-4.1)
- Fine-grained guidance outperforms goal-oriented prompting for weaker models, with performance gap inversely correlated to model capability
- Larger models don't always perform better; some mid-tier models match or exceed larger counterparts in specific tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Role-specialized multi-agent coordination enables effective orchestration of complex federated learning pipelines by decomposing the workflow into discrete phases with well-defined agent responsibilities.
- Mechanism: Seven distinct agents (S1, S2, S3, S4, C1, C2, C3) handle specific functions—client orchestration (S1, C1, S2), data preprocessing (C2), label harmonization (C3), and federated training (S3, S4)—allowing each agent to operate within a bounded problem space with tailored toolsets.
- Core assumption: The FL workflow can be cleanly decomposed into independent phases with well-defined interfaces, and phase-level decisions can be made without cross-phase reasoning.
- Evidence anchors:
  - [abstract] "intelligent agents at each hospital client and the central server agent collaboratively manage FL setup and model training with minimal human intervention"
  - [section 2.1] "the server-client agent system A={S1, S2, S3, S4, C1, C2, C3} comprises 7 role-specialized LLM agents responsible for: (1) client selection and server-client communication or orchestration (S1, S2, C1), (2) data preprocessing and cleaning (C2), (3) label harmonization (C3), and (4) federated model selection and training (S3, S4)"
  - [corpus] Related work on ontology-based harmonization (arxiv 2505.20020) supports multi-component approaches to FL complexity, though doesn't directly validate agent specialization.
- Break condition: When tasks become highly interdependent requiring cross-phase reasoning; label harmonization's consistently lowest success rates (noted in Section 3.2 as "most challenging task") suggest inter-task dependencies can break down specialization benefits when agents lack shared context.

### Mechanism 2
- Claim: Confining agents to an orchestration layer that exchanges only configuration signals, file paths, and status messages preserves FL privacy guarantees while enabling automation.
- Mechanism: Agents trigger local operations via tool wrappers but never access or transmit raw images, model weights, or sensitive metadata; preprocessing and harmonization execute entirely on client machines with agents providing only logic/configuration.
- Core assumption: Orchestration decisions can be made from metadata alone without direct data inspection, and semantic conflicts can be resolved through structured descriptions.
- Evidence anchors:
  - [abstract] Framework described as "privacy-preserving" and operating "without sharing sensitive patient data"
  - [section 2.4] "We explicitly prevent agents from ever accessing or transmitting raw data, model weights, or sensitive metadata. The server receives approvals/configuration signals only, not images, so the agent layer never handles patient data... In label harmonization, the agent creates mapping logic, but the mapping execution and label replacement are performed entirely on the local client side"
  - [corpus] MedHE (arxiv 2511.09043) similarly separates privacy-preserving computation from orchestration, supporting this design pattern.
- Break condition: When metadata proves insufficient for accurate decisions—label harmonization's low exact-match accuracy (0.61 even for GPT-4.1) suggests agents may need visibility into actual label distributions to resolve semantic conflicts effectively.

### Mechanism 3
- Claim: Fine-grained (step-by-step) prompting outperforms goal-oriented (high-level) prompting by reducing multi-step planning burden, with the performance gap inversely correlated to model planning capability.
- Mechanism: Explicit instructions constrain solution space and reduce planning errors; stronger models (GPT-4.1, DeepSeek-V3) narrow the gap through better autonomous planning, while weaker models show dramatic degradation under goal-oriented guidance.
- Core assumption: The performance gap between prompt styles reflects planning capability differences rather than just instruction-following ability.
- Evidence anchors:
  - [section 3.2] "Fine-grained guidance is seen to outperform goal-oriented guidance across almost every model, especially for weaker agents. More capable models like GPT-4.1 and DeepSeek-V3 close this gap, showing their capability to plan even based on implicit prompts"
  - [section 3.2] For GPT-3.5-Turbo under fine-grained vs goal-oriented: overall scores drop from ~28% to ~22%, demonstrating planning fragility in weaker models
  - [corpus] Limited direct corpus evidence on prompt granularity effects in FL specifically; this appears to be a novel contribution.
- Break condition: When tasks require creative problem-solving beyond scripted steps—goal-oriented prompts may enable better solutions if models are capable, but this remains untested in the current benchmark design.

## Foundational Learning

- **Concept: Federated Learning workflow phases and their interdependencies**
  - Why needed here: The entire benchmark evaluates agent performance across four sequential FL phases (client selection → data preprocessing → label harmonization → federated training); understanding phase inputs/outputs and failure propagation is prerequisite to interpreting agent coordination results.
  - Quick check question: Why does label harmonization failure cascade to training failure even if client selection succeeded?

- **Concept: LLM agent tool invocation patterns and planning modes**
  - Why needed here: All 24 evaluated LLMs operate through a 16-tool suite with three autonomy tiers (guided tool invocation, autonomous planning, independent script generation); distinguishing these modes is essential for understanding performance differences.
  - Quick check question: What distinguishes an agent using "guided tool invocation" from one performing "autonomous planning" when both complete the same preprocessing task?

- **Concept: Medical imaging data heterogeneity dimensions**
  - Why needed here: The benchmark deliberately introduces realistic heterogeneity (resolution variations, format changes, noisy labels, irrelevant files, modality-inconsistent images) to simulate real clinical PACS environments; understanding these dimensions explains why preprocessing and harmonization are failure bottlenecks.
  - Quick check question: Why would two hospitals' dermatology datasets for the same disease task have incompatible label schemas, and what agent capability is required to reconcile them?

## Architecture Onboarding

- **Component map:**
  - Server workspace (Ws): FL algorithm registry (40 algorithms), orchestrator agents S1-S4, code templates
  - Client workspace (Wc): Dataset metadata (data cards), preprocessing/harmonization tools, agents C1-C3
  - Multi-agent coordination layer: 7 role-specialized agents distributed across 4 workflow phases
  - Tool suite: 16 tools assigned based on agent role (see Appendix B.1 per paper)
  - Evaluation harness: 201 datasets across 6 modalities with injected heterogeneity artifacts

- **Critical path:**
  1. User provides task specification T (natural language)
  2. S1 interprets T, broadcasts modality/task requirements to all client agents
  3. Each C1 reads local data card, evaluates dataset relevance, returns matching datasets (if any)
  4. S2 aggregates responses, selects eligible client subset C_sel
  5. C2 at each selected client performs: subfolder organization → file cleaning → data cleaning → normalization
  6. C3 at each client performs: class inspection → label mapping → data reorganization
  7. S3 queries FL algorithm registry, selects method based on T and cross-client characteristics
  8. S4 distributes configuration to clients, initiates training, logs metrics

- **Design tradeoffs:**
  - Agent specialization vs. coordination overhead: More agents = clearer responsibility boundaries but increased communication complexity
  - Privacy strictness vs. decision quality: Metadata-only orchestration preserves privacy but may miss semantic nuances requiring data inspection
  - Prompt specificity vs. model autonomy: Fine-grained prompts improve weaker model performance but reduce generalization and increase prompt engineering burden

- **Failure signatures:**
  - Domain reasoning gaps: Agents miss subtle label semantic mismatches (e.g., folder names vs. coarse class definitions)
  - Multi-step planning failures: Agents skip sequential operations in single execution cycles (common in preprocessing)
  - Overconfidence/shortcutting: Agents assign plausible but incorrect mappings (e.g., grouping "melanoma metastasis" as benign)
  - Structured task hallucination: Models output irrelevant content (philosophical monologues, tutorials) instead of structured responses
  - Task/modality mismatch: Keyword matching overrides hierarchical task understanding (recommending segmentation datasets for classification tasks)
  - Procedural overthinking: Reasoning agents speculate without grounding in available information (debating selection without reading data cards)

- **First 3 experiments:**
  1. **Baseline validation**: Run GPT-4.1 on Dermatology environment with fine-grained guidance across all 4 phases—establishes upper bound (94-100% per paper) and validates tool/evaluation pipeline integration.
  2. **Prompt granularity ablation**: Compare Qwen QwQ 32B performance under fine-grained vs. goal-oriented guidance on label harmonization task—quantifies planning capability gap for mid-tier models.
  3. **Failure mode characterization**: Run DeepSeek-R1 and Gemma-27B on histopathology label harmonization—captures hallucination and overthinking failure modes for analysis; log token counts and decision traces for debugging.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural modifications or training approaches could substantially improve LLM agent performance on label harmonization, which showed the lowest success rates across all models?
- Basis in paper: [explicit] The authors state: "Across almost all agents, label harmonization shows lowest success rates, regardless of guidance type. This suggests that aligning semantic labels across clients remains one of the hardest challenges."
- Why unresolved: Current models struggle with semantic reasoning across heterogeneous medical taxonomies. Even top performers like GPT-4.1 only achieve 61-87% exact-match accuracy depending on guidance type.
- What evidence would resolve it: Demonstrated improvement in label harmonization accuracy through new methods (e.g., specialized prompting, fine-tuning, retrieval augmentation) evaluated on the FedAgentBench label harmonization tasks.

### Open Question 2
- Question: How does agent-driven FL performance translate to real-world deployments with actual network latency, bandwidth fluctuations, and client dropouts?
- Basis in paper: [explicit] The authors acknowledge: "We currently assume stable network conditions and do not model dynamic communication bandwidth."
- Why unresolved: The benchmark simulates data and task complexity but abstracts away infrastructure challenges that critically affect real FL deployments.
- What evidence would resolve it: Evaluation of agent performance in FedAgentBench environments augmented with simulated network perturbations, or deployment studies in actual hospital network conditions.

### Open Question 3
- Question: What mechanisms could enable real-time monitoring, safe interruption, and regulatory compliance verification within agent-driven FL systems?
- Basis in paper: [explicit] The authors state: "We do not incorporate real-time monitoring or interruption mechanisms. We do not simulate safety check or regulatory compliance assessment but it can be seamlessly integrated into the system."
- Why unresolved: Autonomous FL systems in healthcare require safety guardrails and auditability for clinical deployment, which the current framework does not address.
- What evidence would resolve it: Integration and evaluation of monitoring/compliance modules into the FedAgentBench framework with measurable safety and auditability metrics.

### Open Question 4
- Question: Does FedAgentBench performance in medical imaging generalize to other FL domains (finance, IoT, etc.) with different data modalities and regulatory constraints?
- Basis in paper: [explicit] The authors note: "It is worth noting that while we simulate healthcare environments in this work, the framework can be readily extended to other FL settings such as finance, IoT, etc."
- Why unresolved: No empirical validation outside healthcare; domain-specific challenges (e.g., tabular financial data, IoT sensor heterogeneity) may present different agent failure modes.
- What evidence would resolve it: Cross-domain evaluation results showing comparable agent performance when FedAgentBench is adapted to non-medical FL environments.

## Limitations
- Label harmonization task shows consistently lowest success rates across all models, indicating fundamental limitations in semantic reasoning for heterogeneous medical taxonomies
- Privacy-preserving metadata-only orchestration may sacrifice decision quality when semantic conflicts require data inspection
- Tool specifications, prompt templates, and detailed perturbation methodologies remain unspecified, preventing faithful reproduction
- No evaluation of real-world infrastructure challenges including network latency, bandwidth fluctuations, or client dropouts
- Missing safety monitoring, interruption mechanisms, and regulatory compliance verification for clinical deployment

## Confidence
- **High confidence**: Role-specialized agent coordination effectiveness, privacy-preserving orchestration mechanism, fine-grained prompting benefits for weaker models
- **Medium confidence**: Claims about larger models not always performing better, competitive performance of open-source models, modular extensibility to other domains
- **Low confidence**: Claims about privacy guarantees enabling "minimal human intervention," assertion that label harmonization remains most challenging task across all model tiers

## Next Checks
1. Characterize semantic reasoning failures in label harmonization by comparing agent-generated mappings against ground truth on histopathology datasets, measuring semantic distance and identifying common error patterns
2. Validate privacy-preserving orchestration claims by attempting to reconstruct sensitive information from metadata-only workflows in controlled experiments
3. Test cross-domain extensibility by adapting the framework to non-medical federated tasks (e.g., financial or IoT sensor data) and measuring performance degradation