---
ver: rpa2
title: 'Bridging the Arithmetic Gap: The Cognitive Complexity Benchmark and Financial-PoT
  for Robust Financial Reasoning'
arxiv_id: '2601.21157'
source_url: https://arxiv.org/abs/2601.21157
tags:
- direct
- financial
- reasoning
- cognitive
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces the Cognitive Complexity Benchmark (CCB)
  and the Iterative Dual-Phase Financial-PoT framework to address the problem of "Cognitive
  Collapse" in financial quantitative reasoning by Large Language Models. The CCB
  dataset comprises 95 real-world Chinese A-share annual reports and stratifies tasks
  into three dimensions: Data Source, Mapping Difficulty, and Result Unit.'
---

# Bridging the Arithmetic Gap: The Cognitive Complexity Benchmark and Financial-PoT for Robust Financial Reasoning

## Quick Facts
- **arXiv ID**: 2601.21157
- **Source URL**: https://arxiv.org/abs/2601.21157
- **Reference count**: 25
- **Primary result**: Financial-PoT elevates Qwen3-235B model's average accuracy from 59.7% to 67.3%, achieving up to 10-fold gains on high-complexity reasoning tasks.

## Executive Summary
This study addresses the "Cognitive Collapse" problem in financial quantitative reasoning, where Large Language Models exhibit severe accuracy degradation on complex multi-step financial calculations. The authors introduce the Cognitive Complexity Benchmark (CCB), a three-dimensional taxonomy (Data Source, Mapping Difficulty, Result Unit) to diagnose reasoning failures, and the Iterative Dual-Phase Financial-PoT framework that decouples semantic extraction from symbolic computation. Evaluations show the framework significantly improves accuracy on high-complexity financial reasoning tasks, particularly for cross-table synthesis and ambiguous mappings.

## Method Summary
The research constructs the Cognitive Complexity Benchmark using 95 real-world Chinese A-share annual reports from 19 Baijiu industry companies. The Financial-PoT framework operates in two phases: Phase 1 uses LLMs to extract semantic variables and formulate logic into a structured schema, while Phase 2 iteratively generates and executes Python code in a sandbox environment, with self-correction loops for up to 3 iterations. The method is evaluated across four model sizes (8B, 32B, 235B parameters) on 14 financial indicators, comparing against Direct and Chain-of-Thought baselines.

## Key Results
- Financial-PoT framework elevates Qwen3-235B model's average accuracy from 59.7% to 67.3%
- Up to 10-fold gains on high-complexity reasoning tasks (e.g., Inventory Turnover Days from 0.0% to 25.3%)
- Outperforms larger models on parameter efficiency, with Qwen3-32B + PoT matching GPT-oss-120B + CoT
- Demonstrates "Cognitive Collapse" recovery, particularly for cross-table synthesis and ambiguous mappings

## Why This Works (Mechanism)

### Mechanism 1: Decoupling Semantic Extraction from Symbolic Computation
Separating variable extraction/logic formulation from arithmetic execution improves accuracy by preventing error propagation from semantic misinterpretation into numerical errors. The two-phase architecture isolates semantic understanding in Phase 1 and delegates arithmetic to deterministic symbolic execution in Phase 2.

### Mechanism 2: Iterative Self-Correction Loop
An iterative execution loop with exception feedback enables autonomous debugging of generated code. Runtime failures become learning signals that the model uses to generate corrected code versions, effectively allowing the model to "debug" its own logic dynamically.

### Mechanism 3: Cognitive Complexity Stratification for Diagnosis
Stratifying tasks along three dimensions (Data Source, Mapping Difficulty, Result Unit) enables precise diagnosis of where reasoning degrades. This reveals non-linear "Cognitive Collapse" at high complexity, showing that performance degradation is not uniform across task types.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Prompting**
  - Why needed here: The paper positions Financial-PoT against CoT as a baseline; understanding CoT's limitations (probabilistic token prediction causing "arithmetic hallucinations") is essential to grasp why decoupling helps.
  - Quick check question: On a multi-step financial calculation (e.g., ROE = Net Income / Average Equity), what specific failure mode would CoT exhibit that PoT would avoid?

- **Concept: Neuro-Symbolic Architecture**
  - Why needed here: Financial-PoT is a neuro-symbolic system combining LLMs (neural, semantic) with Python execution (symbolic, deterministic). Understanding this hybrid paradigm is key to implementing the framework.
  - Quick check question: In a neuro-symbolic system, which component handles "Free Cash Flow = OCF - CAPEX" logic vs. the actual subtraction operation?

- **Concept: Working Memory Load in Long-Context Reasoning**
  - Why needed here: The Data Source dimension measures "search distance" and context retention; cross-table synthesis fails partly because of memory/attention limits. This connects to why decoupling helps.
  - Quick check question: Why would extracting variables from a Balance Sheet and Income Statement simultaneously cause more errors than from a single table?

## Architecture Onboarding

- **Component map**: Document context (Draw) + Target indicators (Itarget) -> Semantic Logic Formulation (M_logic) -> Calculation Schema (S_schema) -> Code Generation (M_code) -> Python code (C(t)) -> Executor (E) -> Results (R(t)) or Exceptions (E(t)) -> Validator (V) -> Feedback loop (up to T=3)

- **Critical path**: Document ingestion (OCR → parsed statements) → Semantic parsing (LLM → S_schema with extracted variables) → Code synthesis (M_code → C(0)) → Execution loop (E → R or E, with up to T=3 corrections) → Result validation and output

- **Design tradeoffs**: PoT vs. CoT adds code generation overhead; for low-complexity tasks, CoT may be faster and equally accurate. Paper suggests dynamic routing as optimal. Iteration depth (T=3) balances robustness with latency. Smaller models (8B) benefit less from PoT due to weaker semantic understanding.

- **Failure signatures**: Direct/COT shows "Cognitive Collapse" with non-linear accuracy drop on cross-table, ambiguous, or days-based tasks. PoT on small models may regress on simple tasks due to code generation overhead. Persistent failures like Inventory Turnover Days remain near-zero for all models.

- **First 3 experiments**: 1) Replicate a cross-table calculation comparing Direct, CoT, and PoT on one model to observe "Cognitive Collapse" recovery. 2) Test iterative correction loop by injecting deliberate error and verifying correction within T=3 iterations. 3) Stratify results by Data Source dimension to confirm cross-table tasks show largest PoT vs. CoT improvement.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the Financial-PoT framework maintain its robust performance when applied to a diverse, large-scale corpus outside the specific domain of Chinese A-share annual reports? The authors state future work will expand CCB from 95 reports to a diverse large-scale corpus to test generalization.

- **Open Question 2**: Can a dynamic routing mechanism effectively select between Direct, CoT, and PoT paradigms to optimize efficiency without sacrificing accuracy? The authors propose integrating dynamic routing to selectively apply PoT for optimal efficiency as a specific future direction.

- **Open Question 3**: What is the minimum model capacity required for the "Semantic Logic Formulation" phase to successfully drive the Financial-PoT framework? The authors note that for the 8B model, parameter scaling yields greater gains than PoT, identifying a "dual bottleneck where symbolic execution depends on a minimum threshold of semantic understanding."

## Limitations
- The exact prompt templates for Semantic Logic Formulation and Code Generation phases are not specified, making exact replication challenging.
- The evaluation dataset (95 annual reports with ground-truth labels) is not publicly available, preventing independent verification.
- The "Cognitive Collapse" phenomenon interpretation may be multifaceted, including context window limitations, attention pattern degradation, or fundamental LLM arithmetic weaknesses.

## Confidence
- **High confidence**: The decoupling mechanism between semantic extraction and symbolic computation is well-supported by the evidence, with architectural improvement from 59.7% to 67.3% average accuracy directly reported.
- **Medium confidence**: The Cognitive Complexity Benchmark's three-dimensional stratification effectively diagnoses performance bottlenecks, though independent validation with different datasets would strengthen this claim.
- **Medium confidence**: The superiority of Financial-PoT over CoT and Direct paradigms is well-demonstrated on the CCB dataset, but the extent of improvement may vary across different financial domains or languages.

## Next Checks
1. **Replication with alternative financial datasets**: Test Financial-PoT on financial reports from a different industry or geographic region (e.g., US SEC filings or European annual reports) to verify the architecture's generalizability beyond Chinese A-share reports.

2. **Ablation study on iteration depth**: Systematically vary the maximum iteration count (T=1, 2, 3, 5, 10) to determine the optimal tradeoff between accuracy improvement and computational cost, and identify whether T=3 is truly optimal or merely sufficient.

3. **Error analysis on persistent failures**: Conduct a detailed analysis of why Inventory Turnover Days and AR Turnover Days remain near-zero across all models, distinguishing between OCR extraction errors, semantic misunderstanding, and arithmetic complexity to identify whether these represent fundamental limitations or solvable technical challenges.