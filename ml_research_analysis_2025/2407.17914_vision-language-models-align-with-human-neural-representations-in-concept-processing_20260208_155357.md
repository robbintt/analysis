---
ver: rpa2
title: Vision-Language Models Align with Human Neural Representations in Concept Processing
arxiv_id: '2407.17914'
source_url: https://arxiv.org/abs/2407.17914
tags:
- brain
- vlms
- language
- lxmert
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VLMs show higher brain alignment than language-only models for
  concept processing, especially when visual context is provided. Controlled ablation
  studies reveal that for VL encoders (VisualBERT, LXMERT), brain alignment stems
  from genuinely learning more human-like concepts, while for some VLMs it is highly
  sensitive to visual context at inference.
---

# Vision-Language Models Align with Human Neural Representations in Concept Processing

## Quick Facts
- arXiv ID: 2407.17914
- Source URL: https://arxiv.org/abs/2407.17914
- Reference count: 40
- VLMs show higher brain alignment than language-only models for concept processing, especially when visual context is provided.

## Executive Summary
This paper investigates how vision-language models (VLMs) process concepts compared to human brain activity. Using Representational Similarity Analysis on fMRI data, the authors find that vision-language encoders (like VisualBERT and LXMERT) show the highest alignment with human neural representations, outperforming both language-only models and generative VLMs. The study reveals that some VLMs rely heavily on visual context at inference time rather than having truly learned multimodal concepts, while encoders demonstrate more robust semantic representations that persist even without images.

## Method Summary
The study uses Representational Similarity Analysis (RSA) to compare model embeddings with human fMRI responses to 180 concrete concepts. Ten different VLMs (contrastive, encoder, and generative architectures) are evaluated in two conditions: sentence-only and picture (concept + image). Hidden states are extracted from specific layers, averaged across six contexts per concept, and compared to brain representational dissimilarity matrices via Spearman correlation. LXMERT requires random noise images in the sentence condition, while contrastive models sum image and text embeddings for the picture condition.

## Key Results
- Vision-language encoders (VisualBERT, LXMERT) achieve the highest brain alignment in both language and visual brain networks
- Generative VLMs underperform compared to encoders despite excelling at downstream tasks
- Some VLMs show context-sensitive alignment (CLIP, VisualBERT) while encoders demonstrate learned concepts robust to ablation
- Brain alignment differences cannot be attributed to semantic information already present in language-only modules

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Integration Depth
Vision-language encoders achieve higher brain alignment because their architectures enforce deeper fusion of visual and linguistic streams through cross-modal attention modules. This mirrors the tight coupling in human semantic processing, unlike generative VLMs that use shallow adaptors to map visual tokens into frozen LLM space.

### Mechanism 2: Inference Context Sensitivity (Ablation)
High brain alignment in some VLMs is driven by visual context availability at inference rather than internalized multimodal concepts during pretraining. Models showing significant alignment drops without images rely on external visual grounding rather than internal concept representation.

### Mechanism 3: Semantic Information Partitioning
Multimodal pretraining fundamentally reshapes the semantic geometry of language representations, creating embedding spaces distinct from language-only baselines. The brain-relevant information in multimodal models is not shared with their language-only counterparts.

## Foundational Learning

- **Representational Similarity Analysis (RSA)**: The evaluation metric comparing distances between concepts rather than predicting raw voxel values. Why needed: Understanding this is critical for interpreting results. Quick check: Does RSA require a linear mapping between model dimensions and brain voxels? (Answer: No)

- **VLM Architectural Families**: Different model types (Contrastive, Encoder, Generative) use distinct approaches to combine visual and linguistic information. Why needed: The paper's conclusions are heavily stratified by architecture. Quick check: Which architecture uses a "cross-modal module" to learn relations between visual features and text? (Answer: Vision-Language Encoders)

- **Ablation in Cognitive Modeling**: The control logic distinguishing "seeing" from "understanding." Why needed: The paper's strongest causal claims come from ablation studies. Quick check: If a VLM maintains brain alignment with random noise images, what does that imply? (Answer: Alignment comes from internalized text representations, not visual context)

## Architecture Onboarding

- **Component map**: Input (Text + Visuals) -> Backbone (Contrastive: Separate encoders + Similarity matching; Encoder: Object Detector + Transformer + Cross-Attention; Generative: Vision Encoder + Adaptor + Frozen/Finetuned LLM) -> Output (Layer-wise hidden states for RDMs)

- **Critical path**: Stimulus Preparation (Pair concept words with contexts) -> Extraction (Extract hidden states from specific layers) -> Aggregation (Average representations over 6 contexts) -> Comparison (Compute Spearman correlation between Model RDM and Brain RDM)

- **Design tradeoffs**: Generative vs. Encoder (Generative models excel at tasks but lack brain alignment; Encoders align better with biology but are less capable of open-ended generation). Inference Context (Adding images boosts alignment but masks whether model has learned concepts)

- **Failure signatures**: High Task Performance, Low Alignment (Generative VLMs); Context Dependency (Significant performance drop in ablation)

- **First 3 experiments**: 1) Baseline RSA with vanilla BERT against Pereira dataset in Sentence Condition. 2) Architecture Sweep comparing VL Encoder vs. Generative VLM in Picture Condition. 3) Ablation Replication on LXMERT (using noise images) vs. CLIP

## Open Questions the Paper Calls Out

- Does autoregressive pretraining specifically hinder formation of human-like semantic representations compared to encoder-based training?
- To what extent are multimodal integration mechanisms in VLMs represented in brain regions outside standard language and visual networks?
- Is the superior brain alignment of vision-language encoders attributable to their cross-modal attention mechanisms or their specific pretraining data?

## Limitations

- The study relies on a single fMRI dataset with 180 concept words and 16 participants, which may not generalize across cognitive tasks or brain regions
- Ablation methodology differs across model types, potentially introducing confounds
- The paper doesn't explore how model scale or training data diversity affects brain alignment

## Confidence

- **High Confidence**: VL encoders achieve highest brain alignment; multimodality benefits brain alignment
- **Medium Confidence**: Generative VLMs underperform despite strong downstream task performance; alignment differences reflect architectural depth
- **Medium Confidence**: Some VLMs show context-sensitive alignment while others show learned concepts robust to ablation
- **Low Confidence**: Semantic geometry of multimodal representations is fundamentally reshaped

## Next Checks

1. Replicate findings across multiple fMRI datasets with different cognitive tasks and concept categories
2. Test additional VLM architectures including newer multimodal models and varying model scales
3. Conduct controlled experiments varying training data diversity and modality balance