---
ver: rpa2
title: 'Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source Retrieval-Augmented
  Generation'
arxiv_id: '2510.02388'
source_url: https://arxiv.org/abs/2510.02388
tags:
- routing
- agent
- accuracy
- rule
- path
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a rule-driven routing framework for hybrid-source
  retrieval-augmented generation (RAG) that dynamically selects between relational
  databases and unstructured documents based on query characteristics. The framework
  addresses the limitation of existing RAG systems that either rely solely on unstructured
  documents or naively combine both sources, which can lead to noise and inefficiency.
---

# Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2510.02388
- Source URL: https://arxiv.org/abs/2510.02388
- Authors: Haoyue Bai; Haoyu Wang; Shengyu Chen; Zhengzhang Chen; Lu-An Tang; Wei Cheng; Haifeng Chen; Yanjie Fu
- Reference count: 40
- Primary result: Rule-driven routing framework dynamically selects between relational databases and unstructured documents for hybrid-source RAG, achieving higher accuracy than static strategies while maintaining moderate computational cost.

## Executive Summary
This paper introduces a rule-driven routing framework for hybrid-source retrieval-augmented generation (RAG) that dynamically selects between relational databases and unstructured documents based on query characteristics. The framework addresses the limitation of existing RAG systems that either rely solely on unstructured documents or naively combine both sources, which can lead to noise and inefficiency. The proposed method includes a rule-driven routing agent that scores candidate paths using interpretable rules, a rule-making expert agent that refines rules based on feedback, and a path-level meta-cache for reusing routing decisions.

## Method Summary
The framework employs a rule-driven routing agent that evaluates queries against a rule set to assign scores to candidate augmentation paths (Doc, DB, Hybrid, LLM). The path with maximum score is selected via argmax. Rules encode observable patterns—for example, numerical queries receive +3 for DB path, "how/why" queries receive +3 for Doc path. After processing query batches, the system generates diagnostics containing path-level and rule-level statistics. A rule-making expert agent interprets these as natural language feedback and proposes rule modifications offline. The path-level meta-cache stores routing decisions by query embedding similarity to reduce latency while preserving answer freshness for dynamic data.

## Key Results
- Experiments on three QA benchmarks demonstrate consistent improvements over static strategies and learned routing baselines
- Achieves higher accuracy while maintaining moderate computational cost compared to baseline approaches
- Rule refinement improves accuracy across datasets, with TATQA F1 increasing from ~0.080 to >0.096 with batch size 50
- Path-level meta-cache variant shows near-zero routing time while maintaining highest accuracy versus 2+ seconds for learned routing approaches

## Why This Works (Mechanism)

### Mechanism 1: Rule-Driven Path Scoring
The routing agent evaluates queries against a rule set R, assigning additive scores to candidate paths (Doc, DB, Hybrid, LLM). The path with maximum score is selected via argmax. Rules encode observable patterns—for example, numerical queries receive +3 for DB path, "how/why" queries receive +3 for Doc path. Core assumption: Query types exhibit consistent regularities in their alignment with retrieval paths; these patterns are capturable via explicit rules rather than learned classifiers.

### Mechanism 2: Feedback-Driven Rule Refinement
After processing a query batch, the system generates diagnostics M^(t) containing path-level and rule-level statistics. The rule-making expert agent interprets these as natural language feedback and proposes rule modifications: R^(t+1) = A_RULE(R^(t), M^(t)). This operates offline before deployment. Core assumption: LLMs can meaningfully interpret performance diagnostics and propose effective rule modifications in natural language.

### Mechanism 3: Path-Level Meta-Cache
For each query, store (z_q, S_Doc, S_DB, S_Hybrid, S_LLM) where z_q = φ(q) is the embedding. New queries check cache via sim(z_cached, z_new) ≥ τ. If hit, reuse scores; if miss, run full routing and update cache. Core assumption: Semantically similar queries should receive similar routing decisions; embedding similarity is a sufficient proxy.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG) basics
  - Why needed here: The entire framework operates within RAG paradigm; understanding how retrieval augments generation is prerequisite.
  - Quick check question: Can you explain why LLMs alone struggle with domain-specific, time-sensitive queries?

- Concept: Query embedding and semantic similarity
  - Why needed here: Path-level meta-cache relies on embedding-based similarity matching; must understand vector representations.
  - Quick check question: How would you choose embedding model and threshold τ for semantic cache matching?

- Concept: Rule-based expert systems
  - Why needed here: The core routing mechanism uses interpretable if-then rules; understanding rule conflict resolution and scoring is essential.
  - Quick check question: What happens when multiple rules fire for the same query path?

## Architecture Onboarding

- Component map:
  Rule-Driven Routing Agent -> Path-Level Meta-Cache -> Augmentation Paths (Basic, Doc, DB, Hybrid) -> Summarize Agent

- Critical path:
  1. Query arrives → check meta-cache (embedding similarity)
  2. If cache miss → routing agent scores all paths using rules
  3. Select argmax path → retrieve evidence (Doc/DB/Hybrid) or skip (Basic)
  4. Feed evidence + query to summarize agent → return answer
  5. Periodically: collect QA outcomes → diagnostics → expert agent refines rules

- Design tradeoffs:
  - Rule complexity vs interpretability: More nuanced rules capture subtler patterns but reduce transparency
  - Cache threshold τ: Higher τ reduces false cache hits but lowers hit rate
  - Update batch size: Smaller batches adapt faster but may overfit; paper finds 25-50 optimal
  - Tie-breaking priority: Paper uses predefined order but doesn't specify; critical to document

- Failure signatures:
  - **Routing always selects same path**: Rules may be dominated by one high-scoring condition; check rule trigger frequencies
  - **Cache degrades accuracy**: τ too low, semantically different queries incorrectly matched
  - **No improvement from rule updates**: Diagnostics insufficient or expert agent failing to extract signal
  - **Hybrid path rarely selected**: Rules may penalize Hybrid; verify if this matches oracle distribution

- First 3 experiments:
  1. **Path coverage analysis**: Run each static path (Basic, Doc, DB, Hybrid) on held-out queries; measure which queries each path answers correctly to verify complementarity assumption per Figure 1a.
  2. **Rule sensitivity test**: Ablate individual rules and measure accuracy delta; identify which rules drive performance vs noise.
  3. **Cache threshold sweep**: Vary τ from 0.85 to 0.99; plot hit rate vs accuracy degradation to find operating point for your query distribution.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance depends heavily on quality and coverage of initial rule set; baseline rules capture only part of query-path alignment patterns
- Meta-cache trades answer freshness for latency reduction, potentially struggling with rapidly changing knowledge bases
- Rule-making expert agent operates offline using batch processing, limiting real-time adaptability

## Confidence
**High confidence**: The core routing mechanism using interpretable rules for query-to-path mapping is well-established and experimental results consistently show improvements over static baselines across all three datasets.

**Medium confidence**: The feedback-driven rule refinement mechanism shows measurable improvements, but magnitude varies significantly across datasets. The claim that "textual gradient" is superior to other refinement methods needs more direct comparison.

**Medium confidence**: The path-level meta-cache provides latency benefits, but accuracy-cost tradeoff curves suggest diminishing returns at higher cache thresholds. General applicability to diverse query distributions requires further validation.

## Next Checks
1. **Rule robustness test**: Systematically remove individual rules and measure accuracy degradation across all datasets to quantify each rule's contribution and identify potential redundancy or overfitting.

2. **Distribution shift evaluation**: Test the framework on queries from domains significantly different from training data (e.g., medical vs. general knowledge) to assess rule generalization and identify failure modes.

3. **Dynamic data scenario**: Implement a version where underlying data changes over time (e.g., stock prices, news events) and measure how quickly rule-based routing degrades versus learned routing approaches, quantifying the freshness-latency tradeoff.