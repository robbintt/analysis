---
ver: rpa2
title: Structured Document Translation via Format Reinforcement Learning
arxiv_id: '2512.05100'
source_url: https://arxiv.org/abs/2512.05100
tags:
- translation
- node-chrf
- document
- data
- markup
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of translating structured documents\
  \ while preserving XML/HTML markup. The proposed FormatRL method uses Group Relative\
  \ Policy Optimization to optimize novel structure-aware rewards\u2014TreeSim for\
  \ structural similarity and Node-chrF for node-level translation quality\u2014moving\
  \ beyond token-level likelihood optimization."
---

# Structured Document Translation via Format Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.05100
- Source URL: https://arxiv.org/abs/2512.05100
- Reference count: 40
- This paper addresses the challenge of translating structured documents while preserving XML/HTML markup, showing significant improvements across six metrics on the SAP software-documentation benchmark.

## Executive Summary
This paper introduces FormatRL, a method that combines supervised fine-tuning with reinforcement learning to translate structured XML documents while preserving their markup structure. The approach uses Group Relative Policy Optimization (GRPO) with novel structure-aware rewards - TreeSim for structural similarity and Node-chrF for node-level translation quality. Unlike traditional token-level likelihood optimization, FormatRL directly optimizes metrics that capture both translation quality and structural fidelity, achieving significant improvements on the SAP software-documentation benchmark across six evaluation metrics while maintaining translation quality.

## Method Summary
FormatRL operates in two phases: first, a supervised fine-tuning (SFT) phase where a pre-trained LLM is fine-tuned on real and synthetic structured document pairs; second, a reinforcement learning phase using GRPO where the model generates multiple candidates per document and updates its policy based on structure-aware rewards. The method introduces TreeSim, which measures structural similarity using tree edit distance, and Node-chrF, which evaluates translation quality at the XML node level through parallel traversal. The RL phase uses group-relative advantages computed within sets of 8 candidates to stabilize training without requiring a separate critic network, while KL regularization prevents drift from the SFT initialization.

## Key Results
- Average gains of 3.69 XML-Match, 2.16 XML-BLEU, 0.22 Content-BLEU, and 0.93 StrucAUC scores compared to SFT baseline
- Structure-aware rewards (TreeSim, Node-chrF) directly optimize document-level metrics rather than token-level likelihoods
- 1:1 ratio of real to synthetic data prevents structural collapse while enabling effective RL training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: If structural similarity is explicitly rewarded via tree edit distance, the policy model may learn to generate XML with higher structural fidelity than token-level likelihood optimization alone.
- Mechanism: TreeSim computes the normalized Zhang-Shasha tree edit distance between predicted and reference XML trees. The reward directly penalizes node insertions, deletions, and relabelings, providing a dense training signal that correlates with document structure correctness.
- Core assumption: Structural errors in generated documents can be meaningfully captured by tree edit distance, and optimizing this metric transfers to downstream utility.
- Evidence anchors:
  - [abstract]: "TreeSim, which measures structural similarity between predicted and reference XML trees"
  - [section 3.3.1]: "TreeSim measures structural similarity between the predicted and reference XML trees... using the Zhang-Shasha tree edit distance"
  - [corpus]: Weak direct corpus support; neighbor papers address structured output correctness but not tree edit distance specifically for translation.
- Break condition: If documents contain semantically equivalent but structurally different valid representations, TreeSim may penalize correct outputs. Invalid XML receives a fixed penalty (-0.1), which may not provide sufficient gradient signal.

### Mechanism 2
- Claim: If translation quality is measured at the XML node level rather than document level, the reward may better align with both structural and translation objectives.
- Mechanism: Node-chrF performs parallel depth-first traversal of predicted and reference trees, pairing corresponding nodes. When tags match, it computes chrF on text content; mismatched or unpaired nodes score 0. This couples structural alignment with translation quality.
- Core assumption: Node-level alignment via traversal order approximates semantic correspondence between source and target document structures.
- Evidence anchors:
  - [abstract]: "Node-chrF, which measures translation quality at the level of XML nodes"
  - [section 3.3.1]: "If the translation contains structural mistakes, however, nodes become misaligned, and the reward degrades substantially"
  - [corpus]: No direct corpus evidence for node-level chrF in structured translation.
- Break condition: If correct translations reorder nodes (e.g., language-specific word order changes affecting tag placement), Node-chrF may penalize valid outputs.

### Mechanism 3
- Claim: If relative advantages are computed within groups of candidate outputs rather than against an absolute value function, optimization may stabilize without requiring a separate critic network.
- Mechanism: GRPO generates K candidates per input, computes rewards for each, then calculates advantages as (reward - group_mean) / group_std. The policy is updated to increase likelihood of above-average outputs. KL divergence regularization prevents drift from the SFT initialization.
- Core assumption: Within-group variance provides sufficient signal for policy improvement without an explicit value function approximator.
- Evidence anchors:
  - [section 3.3.2]: "GRPO computes advantages by comparing each generation's reward against the group mean, effectively learning which translations are better than average within the same context"
  - [section 3.3.2]: "The second term (1b) is a Kullback-Leibler divergence regularizer that prevents the optimized policy from deviating too far from the supervised fine-tuned model"
  - [corpus]: Neighbor papers reference GRPO in DeepSeek-R1 but not for translation tasks.
- Break condition: If all K candidates are poor quality, group-relative advantages may amplify noise rather than signal. Small group sizes (K=8 in this work) may limit advantage estimation quality.

## Foundational Learning

- Concept: **Proximal Policy Optimization (PPO) and policy gradient methods**
  - Why needed here: GRPO is a simplification of PPO; understanding the base algorithm clarifies why the value network was removed and what tradeoffs this introduces.
  - Quick check question: Can you explain why removing the value network reduces memory but may increase variance in gradient estimates?

- Concept: **chrF (character n-gram F-score)**
  - Why needed here: Node-chrF builds directly on chrF; understanding the base metric clarifies what node-level chrF preserves and what it sacrifices.
  - Quick check question: Why might chrF be preferred over BLEU for node-level evaluation in this context?

- Concept: **Zhang-Shasha tree edit distance**
  - Why needed here: TreeSim is a normalized inverse of this distance; understanding the algorithm clarifies its computational complexity and what edit operations it considers.
  - Quick check question: What is the time complexity of Zhang-Shasha, and how does document tree size affect reward computation cost?

## Architecture Onboarding

- Component map:
  - Pre-trained LLM -> SFT on real + synthetic pairs -> SFT checkpoint
  - SFT checkpoint -> GRPO sampling (K=8) -> Reward computation (TreeSim, Node-chrF) -> Group advantages -> Policy update with KL regularization -> Final model

- Critical path:
  1. Synthetic data quality determines SFT baseline; poor markup injection propagates to GRPO phase
  2. Reward function selection directly shapes optimization trajectory (Table 3 shows reward-metric alignment)
  3. KL penalty coefficient (β=0.01) controls the tradeoff between structural improvements and translation quality retention

- Design tradeoffs:
  - **TreeSim vs. Node-chrF reward**: TreeSim optimizes for structure (higher XML-Match); Node-chrF balances structure and translation (higher combined scores per Table 3)
  - **Group size K**: Larger K improves advantage estimation but increases compute; K=8 was chosen empirically
  - **Real vs. synthetic data ratio**: Pure synthetic causes structural collapse (XML-Match <20%); 1:1 ratio recommended

- Failure signatures:
  - XML-Match drops while Content-BLEU stays stable → reward overfitting to translation quality; increase structural reward weight
  - Both metrics degrade → KL penalty too low (catastrophic forgetting) or learning rate too high
  - Training diverges → check reward scaling (should be |r| ∈ [0,10])

- First 3 experiments:
  1. **Baseline replication**: Train SFT model on 100 real document pairs, verify XML-Match ≈85% (Table 1 baseline) before proceeding to GRPO
  2. **Ablation on reward choice**: Run GRPO with TreeSim only, Node-chrF only, and combined rewards; compare XML-Match and Content-BLEU to isolate reward effects
  3. **Synthetic data sensitivity**: Train SFT with 0%, 50%, and 100% synthetic data mixed with real data; measure structural degradation to calibrate data augmentation strategy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can FormatRL extrapolate to structured documents containing XML tags not seen during training?
- Basis in paper: [explicit] The authors acknowledge they restricted the tag set during synthesis and "did not evaluate this extrapolation capability" due to budget constraints.
- Why unresolved: Current experiments only cover tags present in the development set, leaving robustness to novel vocabulary unknown.
- What evidence would resolve it: Evaluation on a test set containing Out-Of-Vocabulary (OOV) tags to measure structural failure rates.

### Open Question 2
- Question: How does FormatRL perform under rigorous human evaluation frameworks tailored for structured errors?
- Basis in paper: [explicit] The paper states the current human evaluation was "simple" and lacked "error taxonomies tailored to structured documents (e.g., MQM... or ESA)."
- Why unresolved: Existing automatic metrics (BLEU/COMET) have known shortcomings at the document level, and the small-scale human eval may not capture nuanced structural failures.
- What evidence would resolve it: A large-scale human assessment using specific error taxonomies for tag nesting and structural fidelity.

### Open Question 3
- Question: Does tag abstraction via placeholders improve generalization compared to semantic tags?
- Basis in paper: [explicit] The authors "did not explore tag abstraction using placeholder tags" and suggest it might help generalization but introduces pre-/post-editing complexity.
- Why unresolved: It is unclear if the model relies on semantic tag meaning or just structural patterns; abstraction could decouple this.
- What evidence would resolve it: Ablation studies comparing model performance when trained on semantic tags versus generic placeholders.

## Limitations

- Evaluation relies on a single proprietary benchmark (SAP software-documentation) with limited training data (100 real pairs), raising questions about generalizability to other structured document domains.
- TreeSim metric assumes a single valid structural representation, potentially penalizing semantically equivalent but structurally different XML outputs.
- RL optimization uses small group sizes (K=8) which may limit the quality of advantage estimation for complex document structures.

## Confidence

**High Confidence**: The experimental results showing significant improvements over SFT baseline on the SAP benchmark are well-supported by the reported metrics (XML-Match, XML-BLEU, Content-BLEU, StrucAUC).

**Medium Confidence**: The claim that Node-chrF better aligns with both structure and translation objectives is supported by Table 3 showing improved combined scores, but the tradeoff curves between different reward combinations are not fully explored.

**Low Confidence**: The generalizability claim to other structured document types is weakly supported, relying only on the SAP dataset without testing HTML, JSON, or other markup languages.

## Next Checks

1. **Cross-Domain Transfer**: Test FormatRL on HTML document translation using a publicly available benchmark like WMT's HTML datasets. Compare performance against the SAP results to validate generalizability beyond the training domain.

2. **Reward Ablation with Larger Groups**: Re-run the GRPO experiments with K=16 and K=32 to assess whether larger candidate pools improve stability and performance, particularly for documents with complex nested structures.

3. **Structural Equivalence Handling**: Create a test set where semantically equivalent documents have different valid XML structures (reordered tags, alternative nesting). Measure whether TreeSim unfairly penalizes correct translations and whether Node-chrF is more robust to these variations.