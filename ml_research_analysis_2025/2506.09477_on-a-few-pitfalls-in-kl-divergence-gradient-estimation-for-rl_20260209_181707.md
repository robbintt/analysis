---
ver: rpa2
title: On a few pitfalls in KL divergence gradient estimation for RL
arxiv_id: '2506.09477'
source_url: https://arxiv.org/abs/2506.09477
tags:
- gradient
- estimate
- divergence
- estimates
- vanilla
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies and corrects common pitfalls in implementing
  KL divergence gradient estimation for reinforcement learning in large language models.
  It shows that naively differentiating KL divergence estimates as loss functions
  produces incorrect gradients, with variance-reduced implementations inadvertently
  minimizing reverse KL divergence instead of the intended forward KL.
---

# On a few pitfalls in KL divergence gradient estimation for RL

## Quick Facts
- **arXiv ID**: 2506.09477
- **Source URL**: https://arxiv.org/abs/2506.09477
- **Reference count**: 12
- **Primary result**: Naive KL gradient estimation in RL fails; proper sequence-level estimates with variance reduction significantly improve KL minimization

## Executive Summary
This paper identifies critical implementation pitfalls in KL divergence gradient estimation for reinforcement learning with large language models. The authors demonstrate that common approaches to minimizing KL divergence as a regularization term often compute incorrect gradients, leading to suboptimal learning behavior. Through theoretical analysis and empirical validation, they show that properly implemented sequence-level KL gradient estimates with appropriate variance reduction techniques substantially improve the efficacy of KL regularization, particularly in high-KL regimes like on-policy distillation.

## Method Summary
The paper analyzes three distinct pitfalls in KL divergence gradient estimation for RL in LLMs. First, naive auto-differentiation through KL divergence estimates as loss functions produces biased gradients that fail to minimize the target KL divergence. Second, variance-reduced gradient estimates inadvertently minimize reverse KL divergence instead of the intended forward KL. Third, token-level KL loss implementations only capture partial derivatives of the full sequence-level KL gradient, missing the impact of earlier tokens on later ones. The authors propose correct gradient estimators including vanilla gradient with stop-gradient, sequence-level cumulative estimates, and variance reduction via leave-one-out baselines. Experiments validate these findings in both tabular and LLM settings, comparing implementations against analytic gradients and measuring KL minimization efficacy.

## Key Results
- Naive KL gradient estimation produces zero-mean gradients that fail to minimize KL divergence
- Variance-reduced KL estimates minimize reverse KL divergence instead of forward KL, with regularization strength dependent on arbitrary control variate multipliers
- Token-level KL losses miss temporal dependencies, only capturing partial gradients of sequence-level KL
- Sequence-level KL gradient with variance reduction improves KL minimization efficacy by approximately 2x in high-KL distillation regimes
- Proper implementation reduces sequence-level KL from 7.7 to 3.0 in on-policy distillation experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Differentiating through Monte Carlo KL estimates as loss functions produces biased gradient estimates that fail to minimize the target KL divergence.
- Mechanism: The KL divergence KL(π, πref) = E_y∼π[log(π(y)/πref(y))] depends on π through both the sampling distribution and the integrand. The full gradient comprises a score function derivative and a path-wise derivative. Naive auto-differentiation through the KL estimate only captures the path-wise component, systematically missing the contribution from the sampling distribution's dependence on parameters.
- Core assumption: Parameters influence both the probability density and the sampling process, which holds for autoregressive language models where token probabilities affect sequence generation.
- Evidence anchors:
  - [abstract] "...naively differentiating KL divergence estimates as loss functions produces incorrect gradients..."
  - [section 3] "E[dKL] = KL ≠⇒ E[∇dKL] = ∇KL... simply differentiating the KL estimate ∇KL only accounts for the path-wise part"
  - [corpus] Related work on gradient estimation (Schulman et al., 2015a) confirms the distinction between score function and path-wise derivatives in stochastic computation graphs.
- Break condition: If the sampling distribution were fixed (off-policy with frozen behavior policy), path-wise derivatives alone would suffice.

### Mechanism 2
- Claim: Variance-reduced KL gradient estimates inadvertently minimize reverse KL divergence KL(πref, π) rather than the intended forward KL KL(π, πref).
- Mechanism: The variance-reduced estimate dKL_var-reduced = log(π(y)/πref(y)) + πref(y)/π(y) - 1 adds a control variate that is zero-mean. When differentiated, the control variate term contributes a non-zero expected gradient equal to ∇KL(πref, π). The regularization effect scales with the arbitrary multiplier λ on the control variate, making the intended KL weight unreliable.
- Core assumption: Practitioners treat the control variate as optional variance reduction without recognizing its gradient contribution.
- Evidence anchors:
  - [abstract] "...variance-reduced implementations inadvertently minimizing reverse KL divergence instead of the intended forward KL."
  - [section 3] "E[∇KL_var-reduced] = ∇KL(πref, π) ≠ ∇KL(π, πref)... the positions of the two distributions are reversed"
  - [section 6] Zhang et al. (2025) concurrently noted variance-reduced estimates enforce reverse KL; this paper identifies it as a bug rather than a design choice.
- Break condition: If λ=0, the control variate is removed and regularization vanishes entirely, confirming the effect is incidental.

### Mechanism 3
- Claim: Token-level KL losses produce partial gradients that miss the causal impact of earlier tokens on later token-level KL divergences.
- Mechanism: The sequence-level KL gradient requires accounting for how token yt influences future log-ratios ρs for s>t. Token-level losses compute Σ_t ∇dKL(π(·|y_1:t-1), πref(·|y_1:t-1)), which only captures immediate per-step contributions. The full gradient includes cross-temporal terms where early tokens affect subsequent conditional distributions.
- Core assumption: Autoregressive generation creates temporal dependencies where earlier token choices constrain later token distributions.
- Evidence anchors:
  - [abstract] "...token-level loss implementations only capture partial derivatives of the full sequence-level KL gradient, missing the impact of earlier tokens on later ones."
  - [section 4.1] "the above gradient does not account for the impact that token yt has on the future tokens ys, s > t which impacts future token-level KL divergences"
  - [section 5.3] "properly implemented sequence-level KL gradient improves upon the efficacy of minimizing the sequence-level KL, especially in high-KL regime"
- Break condition: If tokens were independent (non-autoregressive generation), token-level gradients would be complete.

## Foundational Learning

- Concept: **Score function vs. path-wise gradient estimators**
  - Why needed here: Understanding why auto-diff through KL estimates fails requires distinguishing between gradients through the log-probability (path-wise) and gradients through the sampling distribution (score function/REINFORCE).
  - Quick check question: Given L(θ) = E_y∼p_θ[f(y)], write the two terms in ∇_θ L and identify which auto-diff captures.

- Concept: **Forward vs. reverse KL divergence**
  - Why needed here: The variance-reduced estimate accidentally minimizes reverse KL, which has different optima when combined with reward maximization. Forward KL is mode-covering; reverse KL is mode-seeking.
  - Quick check question: For a mixture of Gaussians reference and unimodal policy, sketch which policy each KL direction would produce.

- Concept: **Temporal credit assignment in sequence models**
  - Why needed here: Sequence-level KL gradients require propagating credit across timesteps, analogous to how rewards in RL depend on earlier actions.
  - Quick check question: In a 3-token sequence, write the contribution of token 1's parameters to the token-3 KL term.

## Architecture Onboarding

- Component map:
  - KL loss layer: Computes per-token or sequence-level KL estimate. Incorrect implementations use `loss = -log(π/πref)` with auto-diff; correct implementations use `loss = sg(log(π/πref)) * log(π)` for the vanilla gradient.
  - Gradient estimator: Vanilla gradient: `log(π(y)/πref(y)) · ∇logπ(y)`. Variance-reduced variants add leave-one-out baselines or cumulative log-ratios.
  - Tokenizer/detokenizer: Required for computing per-token log-probabilities; ensure consistent handling of special tokens and padding.

- Critical path:
  1. Sample sequences y_1:T from π (on-policy).
  2. Compute token log-ratios ρ_t = log(π(y_t|y_1:t-1) / πref(y_t|y_1:t-1)).
  3. For sequence-level gradient, accumulate cumulative log-ratios: `Σ_{s≥t} ρ_s`.
  4. Multiply by `∇logπ(y_t|y_1:t-1)` and sum over timesteps.
  5. Apply leave-one-out baseline if batch size n>1: `v_i = mean(Σρ_j for j≠i)`.

- Design tradeoffs:
  - Token-level vs. sequence-level: Token-level is simpler but incomplete; sequence-level is correct but higher variance. Use sequence-level with variance reduction for high-KL regimes (distillation).
  - Variance reduction: Leave-one-out requires batch size >1. Cumulative estimate reduces variance by ~half but cannot easily add further baselines.
  - Analytic gradient: Only feasible for small vocabularies (tabular); intractable for LLMs due to |Y| = V^T_max.

- Failure signatures:
  - KL divergence increases during "minimization": Check if using `∇(vanilla KL estimate)` instead of `(vanilla KL estimate) · ∇logπ`.
  - Policy converges to wrong optimum under reward+KL: Verify forward vs. reverse KL by checking if `KL(π,πref)` matches `KL(πref,π)` at convergence.
  - No regularization effect despite λ>0: Confirm control variate multiplier is applied to the loss, not just the estimate.

- First 3 experiments:
  1. **Gradient bias validation**: In tabular setting, compute MSE between each gradient estimate and analytic gradient over 100 random policies. Confirm vanilla (incorrect) has zero expected gradient and variance-reduced converges to reverse KL gradient.
  2. **KL minimization sanity check**: Initialize π≠πref and run gradient descent using each estimate. Plot KL(π,πref) over steps. Vanilla (incorrect) should increase KL; variance-reduced should decrease but slower; correct vanilla should match analytic.
  3. **On-policy distillation A/B test**: Distill 70B→8B with token-level vs. sequence-level KL gradient. Measure sequence-level KL(π,πref) over training. Expect sequence-level with variance reduction to minimize KL faster, with larger gap in high-KL early training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the distinction between enforcing forward KL (via correct gradients) versus reverse KL (via incorrect variance-reduced gradients) significantly impact the final performance or convergence properties in long-run RL training where the policy deviates substantially from the reference?
- Basis in paper: [explicit] The authors state on page 6, "We speculate that for long-run RL training... where $\pi$ is allowed to deviate significantly from $\pi_{ref}$, different divergences might make a difference in the limit. We leave this for further experiments."
- Why unresolved: The paper's LLM experiments focus on either low-KL regimes (reward maximization), where the choice of divergence makes little practical difference, or high-KL distillation, which may not reflect the dynamics of long-run policy optimization.
- What evidence would resolve it: A comparison of forward vs. reverse KL gradient estimators in a long-horizon RL setting (e.g., extended RLHF or reasoning training) where the policy drifts far from the initialization, measuring both task performance and KL divergence over time.

### Open Question 2
- Question: What are the specific trade-offs between computational cost and statistical accuracy when using exact analytic gradients versus the proposed sequence-level Monte Carlo estimates for KL regularization?
- Basis in paper: [explicit] On page 4, regarding the analytic gradient, the authors note: "In practice, this involves additional trade-off between computation and statistical accuracy, which we do not explore further in this work."
- Why unresolved: While the paper proves analytic gradients offer strictly lower variance, they require summing over the sample space (intractable for full sequences). The practical trade-off regarding wall-clock time vs. variance reduction for feasible analytic approximations (e.g., per-token) was not quantified.
- What evidence would resolve it: Empirical benchmarks comparing the training step duration and convergence speed of models using the proposed Monte Carlo estimates against models using tractable analytic approximations of the gradient.

### Open Question 3
- Question: Can effective control variates be constructed at the token level to further reduce the variance of the "cumulative estimate" for sequence-level KL gradients?
- Basis in paper: [inferred] On page 6, discussing the cumulative estimate, the paper states: "because it is difficult to compute useful control variate at the token level, it is hard to improve upon the cumulative estimate with other variance reduction methods."
- Why unresolved: The paper introduces a cumulative estimate to account for temporal dependencies but notes a limitation in applying standard variance reduction techniques (like leave-one-out) effectively at the token level within this formulation.
- What evidence would resolve it: A derivation of a token-level control variate that correlates with future log-ratios (preserving the temporal dependency) or an empirical demonstration of a baseline that reduces variance for the cumulative estimate without introducing bias.

## Limitations

- Analysis restricted to autoregressive models where temporal dependencies create missing gradient terms
- Finite-sample effects in variance reduction depend on batch size, optimal sizes not characterized
- Assumes differentiable reference policies, may not hold in practical RLHF scenarios
- Limited exploration of alternative variance reduction techniques beyond leave-one-out baselines

## Confidence

- **High confidence**: Core mathematical analysis identifying why naive KL gradient estimation fails and why variance-reduced estimates minimize reverse KL
- **Medium confidence**: Claim that token-level losses miss temporal dependencies in sequence-level KL gradients
- **Medium confidence**: Experimental results showing sequence-level gradient improvements in LLM distillation

## Next Checks

1. **Gradient variance analysis across KL regimes**: Systematically measure the variance of each gradient estimator (vanilla, variance-reduced, sequence-level) as a function of KL divergence magnitude to quantify when variance reduction becomes critical.

2. **Alternative variance reduction comparison**: Implement and compare against other variance reduction techniques like antithetic sampling, delta methods, or policy gradient-specific baselines to determine if leave-one-out is optimal.

3. **Non-autoregressive model validation**: Test the gradient estimation mechanisms in non-autoregressive generation tasks (e.g., masked language modeling or parallel decoding) to validate whether temporal credit assignment concerns are specific to autoregressive models.