---
ver: rpa2
title: 'A Baseline for Self-state Identification and Classification in Mental Health
  Data: CLPsych 2025 Task'
arxiv_id: '2504.14066'
source_url: https://arxiv.org/abs/2504.14066
tags:
- adaptive
- maladaptive
- recall
- sentence
- baseline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper describes a baseline approach for the CLPsych 2025 A.1
  task of classifying self-states in mental health data from Reddit. The method uses
  few-shot learning with a 4-bit quantized Gemma 2 9B model and a preprocessing step
  that first identifies relevant sentences indicating self-state evidence, then performs
  binary classification to determine if the sentence shows an adaptive or maladaptive
  self-state.
---

# A Baseline for Self-state Identification and Classification in Mental Health Data: CLPsych 2025 Task
## Quick Facts
- arXiv ID: 2504.14066
- Source URL: https://arxiv.org/abs/2504.14066
- Reference count: 5
- Third place out of fourteen systems with test-time recall of 0.579

## Executive Summary
This paper presents a baseline approach for the CLPsych 2025 A.1 task of classifying self-states in mental health data from Reddit. The method employs few-shot learning with a 4-bit quantized Gemma 2 9B model, using a two-stage process that first identifies relevant sentences containing self-state evidence, then performs binary classification to determine if the self-state is adaptive or maladaptive. By simplifying the task to sentence-level classification, the approach achieves third place among fourteen submissions, demonstrating that this simplification better aligns with human annotation granularity than more complex span identification methods.

## Method Summary
The approach uses few-shot learning with a 4-bit quantized Gemma 2 9B model in a two-stage pipeline. First, a preprocessing step identifies sentences that contain evidence of self-states using LLM-based sentence selection. Then, these selected sentences undergo binary classification to determine whether they represent adaptive or maladaptive self-states. This sentence-level classification approach was specifically designed to match the granularity of human annotations, contrasting with alternative span identification methods that proved more complex and less aligned with how annotators actually marked self-state evidence.

## Key Results
- Achieved third place out of fourteen systems in the CLPsych 2025 A.1 task
- Test-time recall of 0.579 on the classification task
- Outperformed alternative LLM span identification methods by simplifying to sentence-level classification

## Why This Works (Mechanism)
The approach succeeds by aligning computational processing with human annotation patterns. Sentence-level classification matches how annotators naturally identify self-state evidence, avoiding the complexity and misalignment of span-based methods. The 4-bit quantization provides computational efficiency while maintaining sufficient model capacity for nuanced mental health language understanding. Few-shot learning enables effective adaptation to the specific task with limited training data, while the two-stage pipeline first narrows the scope to relevant sentences before classification, reducing noise and improving focus.

## Foundational Learning
- **Few-shot learning** - enables model adaptation with minimal examples; needed because mental health data requires domain-specific understanding; quick check: test with varying shot counts to find optimal balance
- **4-bit quantization** - reduces model size and computational requirements; needed for practical deployment and faster inference; quick check: compare performance against full-precision model
- **Sentence-level vs span-level classification** - determines granularity of analysis; needed to match human annotation patterns; quick check: evaluate both approaches on same dataset
- **Binary classification of adaptive/maladaptive states** - simplifies complex psychological assessment; needed to make the task computationally tractable; quick check: verify classification aligns with clinical definitions

## Architecture Onboarding
- **Component map**: Data Preprocessing -> Sentence Selection -> Binary Classification
- **Critical path**: Raw Reddit text → Sentence identification → Adaptive/maladaptive classification
- **Design tradeoffs**: Simplified sentence-level approach vs. more complex span identification; computational efficiency (4-bit quantization) vs. potential precision loss; few-shot learning vs. full fine-tuning
- **Failure signatures**: Missing multi-sentence self-state expressions, overlooking implicit language, misclassifying nuanced adaptive/maladaptive boundaries, performance degradation with non-standard mental health terminology
- **First experiments**: 1) Test model on multi-sentence self-state expressions to assess context capture limitations; 2) Compare 4-bit quantized vs full-precision model performance on same task; 3) Validate few-shot prompting approach across different mental health conditions

## Open Questions the Paper Calls Out
None

## Limitations
- Sentence-level classification may miss nuanced self-state expressions spanning multiple sentences or appearing in non-propositional forms
- 4-bit quantization could impact the model's ability to capture subtle linguistic markers of adaptive versus maladaptive states
- Reliance on few-shot prompts raises reproducibility questions across different mental health contexts and languages

## Confidence
- Sentence-level classification approach effectiveness: **Medium** - performs well for this specific task but may not generalize to all self-state expressions
- 4-bit quantization impact: **Medium** - computational benefits clear, but potential performance trade-offs uncertain
- Task simplification strategy: **Medium** - successful here but may not be optimal for all mental health classification tasks

## Next Checks
1. Evaluate model performance on multi-sentence self-state expressions and implicit language patterns to assess whether sentence-level classification misses important context
2. Compare performance between full-precision and 4-bit quantized versions of the same model to quantify the computational efficiency trade-off
3. Test the few-shot prompting approach across different mental health conditions and languages to establish generalizability limits