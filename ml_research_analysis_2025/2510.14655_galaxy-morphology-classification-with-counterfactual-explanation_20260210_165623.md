---
ver: rpa2
title: Galaxy Morphology Classification with Counterfactual Explanation
arxiv_id: '2510.14655'
source_url: https://arxiv.org/abs/2510.14655
tags:
- galaxy
- spiral
- counterfactual
- latent
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for galaxy morphology classification
  that combines a classical encoder-decoder architecture with an invertible flow to
  enable counterfactual explanations. The model is trained end-to-end on Galaxy10
  DECaLS dataset and achieves an accuracy of ~80%, with SSIM of 0.96 and MSE of 0.006
  for counterfactual similarity.
---

# Galaxy Morphology Classification with Counterfactual Explanation

## Quick Facts
- arXiv ID: 2510.14655
- Source URL: https://arxiv.org/abs/2510.14655
- Reference count: 40
- Accuracy: ~80% on Galaxy10 DECaLS dataset

## Executive Summary
This paper proposes a galaxy morphology classification model that generates counterfactual explanations through an encoder-decoder architecture combined with an invertible flow. The model achieves ~80% accuracy on the Galaxy10 DECaLS dataset while producing realistic counterfactual images that highlight decision-relevant features. By splitting the latent space into class-dependent and class-independent components, the approach maintains background invariance while enabling meaningful edits to galaxy morphology. The invertible flow maps latent vectors to a Gaussian mixture model in hidden space, allowing classification via nearest cluster means and counterfactual generation through boundary crossing.

## Method Summary
The model combines a classical encoder-decoder architecture with an invertible flow to enable counterfactual explanations. An encoder produces a 32-dimensional latent vector split into class-dependent (z1, 24 dims) and class-independent (z2, 8 dims) components. Only z1 passes through the invertible flow for classification, while both z1 and z2 are required for image reconstruction. The invertible flow maps z1 to hidden space h where classes form Gaussian clusters, enabling classification via nearest cluster means. Training uses reconstruction loss (VGG16 perceptual), Maximum Mean Discrepancy (MMD) regularization for Lipschitz continuity, and information bottleneck loss. Counterfactual images are generated by crossing decision boundaries in hidden space and inverting back through the flow.

## Key Results
- Achieves ~80% classification accuracy on Galaxy10 DECaLS dataset
- Counterfactual images show SSIM of 0.96 and MSE of 0.006 for reconstruction quality
- Successfully highlights decision-relevant features like galaxy shape and central bulge while maintaining background invariance
- Analysis reveals fine structures (spiral arms) are lost during compression, making some spiral classes difficult to distinguish

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Invertible flow enables counterfactual generation by preserving bijective mappings between latent and hidden spaces.
- **Mechanism:** The invertible flow transforms latent vectors z into hidden space h where classes form Gaussian clusters. Since the mapping is bijective, crossing decision boundaries in h and inverting back produces valid, in-distribution counterfactual images.
- **Core assumption:** Assumes the coupling layer structure preserves sufficient information for meaningful reconstruction.
- **Evidence anchors:**
  - [abstract]: "combines a classical encoder-decoder architecture with an invertible flow to enable counterfactual explanations"
  - [section 2]: "INNs achieve this by using specific structures that ensure bijective (one-to-one) mappings between the input and output spaces, called coupling layers"
  - [corpus]: Weak direct corpus evidence for INN-based counterfactuals; neighbor papers focus on diffusion models and transformers for galaxy tasks.

### Mechanism 2
- **Claim:** Splitting latent space into class-dependent (z1) and class-independent (z2) components localizes edits to decision-relevant features.
- **Mechanism:** Only z1 passes through the invertible flow for classification; z2 preserves background. Counterfactual intervention modifies z1 while z2 stays fixed, ensuring background invariance.
- **Core assumption:** Assumes decoder requires both z1 and z2 for reconstruction.
- **Evidence anchors:**
  - [section 2]: "splitting the latent vector into class-dependent z1 and class-independent z2 components, where only z1 is used by the invertible flow F for classification"
  - [section 3]: "there is no change in background during reconstruction"
  - [corpus]: Not directly addressed in corpus neighbors; latent disentanglement is a common theme but not specifically validated.

### Mechanism 3
- **Claim:** MMD regularization enforces Lipschitz continuity, ensuring smooth latent space for realistic interpolation.
- **Mechanism:** By constraining latent vectors to approximate a Gaussian distribution, MMD loss ensures small changes in latent space correspond to small perceptual changes in image space, preserving counterfactual similarity.
- **Core assumption:** Assumes Gaussian prior is appropriate for the latent distribution.
- **Evidence anchors:**
  - [section 2]: "The latent space is regularized with Maximum Mean Discrepancy (MMD) to keep the encoding function E Lipschitz continuous"
  - [section 2]: "LMMD encourages the latent vector (z) to approximate a Gaussian distribution. This ensures the interpolability of the latent space"
  - [corpus]: Corpus neighbors use various regularization strategies but do not specifically validate MMD for this application.

## Foundational Learning

- **Concept:** Invertible Neural Networks (INNs) / Normalizing Flows
  - **Why needed here:** Core to bijective mapping between z and h; enables exact inversion for counterfactual generation.
  - **Quick check question:** Can you explain why a coupling layer preserves invertibility while still allowing non-linear transformations?

- **Concept:** Gaussian Mixture Models (GMMs) for classification
  - **Why needed here:** Hidden space h is structured as class-conditional Gaussians; classification uses nearest cluster mean.
  - **Quick check question:** How does a GMM simplify decision boundaries compared to a standard softmax classifier?

- **Concept:** Maximum Mean Discrepancy (MMD)
  - **Why needed here:** Regularizes latent space to be smooth and Gaussian-like, ensuring interpolability.
  - **Quick check question:** What kernel is used in this paper's MMD loss, and why might an RBF kernel be appropriate for image latent spaces?

## Architecture Onboarding

- **Component map:** Image → Encoder → z (split to z1, z2) → z1 → Invertible Flow → h (classification via cluster proximity) → invert to z_cf → decode to image

- **Critical path:**
  1. Image → Encoder → z (split to z1, z2)
  2. z1 → Invertible Flow → h (classification via cluster proximity)
  3. Training: Reconstruction loss + MMD + Information Bottleneck
  4. Inference (counterfactual): Shift h across boundary → invert to z_cf → decode to image

- **Design tradeoffs:**
  - Latent dimension (32 total) balances reconstruction quality vs. compression; too small loses fine structures (spiral arms).
  - β=3 in information bottleneck trades classification accuracy vs. uncertainty calibration.
  - Split ratio (24:8) between z1 and z2 affects how much class info vs. background is preserved.

- **Failure signatures:**
  - Spiral classes (barred/unbarred, tight/loose) produce nearly identical counterfactuals → fine structure lost in compression.
  - Disturbed galaxies scatter in latent space (41% accuracy) → class inherently multi-modal.
  - If z2 duplicates z1 info, counterfactuals ignore z1 edits → check with MMD ablation.

- **First 3 experiments:**
  1. **Latent dimension sweep:** Test z dimensions [16, 32, 64] to see if spiral arm details improve without harming SSIM.
  2. **Ablate MMD loss:** Remove LMMD and observe if counterfactuals still respect background invariance and stay in-distribution.
  3. **Class-wise latent analysis:** For disturbed galaxies, visualize h space to confirm whether GMM assumption fails; consider mixture components > 1 per class.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the encoder-decoder architecture be modified to prevent the loss of fine structures, such as spiral arms, during the latent space compression?
- **Basis in paper:** [explicit] The Conclusion states, "In future work, we aim to address the current limitation related to capturing fine details within the images."
- **Why unresolved:** The current compression process removes these details before the invertible flow can utilize them, causing distinct classes (e.g., unbarred tight vs. loose spirals) to become indistinguishable in the latent space.
- **What evidence would resolve it:** A revised architecture that preserves high-frequency details, demonstrated by improved separation of spiral classes in the t-SNE plots and higher classification F1-scores for those classes.

### Open Question 2
- **Question:** What specific insights regarding class relationships and data quality can be derived from analyzing the distributions within the invertible flow ($F$)?
- **Basis in paper:** [explicit] The Conclusion notes an interest in "exploring the interpretability that the distributions inside F hold... to reveal any limitations of the classifier or the data."
- **Why unresolved:** While the model identifies potential labeling issues (e.g., in the "Disturbed" class), a formal method or metric for extracting these insights from the hidden space distributions has not been established.
- **What evidence would resolve it:** A defined heuristic or visualization technique applied to $F$ that successfully isolates mislabeled samples or quantifies the ambiguity between overlapping classes.

### Open Question 3
- **Question:** Is the Gaussian Mixture Model (GMM) prior in the hidden space sufficient to model the complex boundaries of galaxy morphology, or does it contribute to class confusion?
- **Basis in paper:** [inferred] The Methodology assumes the invertible flow maps latent vectors to a GMM, but the Results show that similar spiral classes form inseparable clusters in the hidden space ($h_1$).
- **Why unresolved:** The rigid structure of a GMM may force distinct but visually similar galaxy types into overlapping Gaussian clusters, failing to capture the nuances of hierarchical morphology.
- **What evidence would resolve it:** Comparative experiments using non-parametric or multi-modal distribution priors in the invertible flow, resulting in distinct clusters for currently confused classes.

## Limitations

- Fine structures like spiral arms are lost during latent space compression, making distinct spiral classes indistinguishable in counterfactuals
- Disturbed galaxy class shows only 41% accuracy due to inherent multi-modality that doesn't fit the GMM assumption
- Exact counterfactual generation algorithm is underspecified, particularly how to determine direction for crossing decision boundaries

## Confidence

- **High confidence:** Basic architecture works as described (encoder-decoder + invertible flow), achieves reasonable classification accuracy (~80%) and reconstruction quality (SSIM 0.96, MSE 0.006). Background invariance claim (z2 preserves background) is well-supported.
- **Medium confidence:** Counterfactual generation mechanism (bijective mapping enabling boundary crossing) is theoretically sound but practical effectiveness depends on latent space dimensionality and distribution assumptions. MMD regularization's role in maintaining smoothness is plausible but not directly validated.
- **Low confidence:** Spiral class distinctions and disturbed galaxy performance suggest fundamental limitations. The claim that counterfactuals are "in-distribution" relies on the Gaussian assumption without rigorous validation.

## Next Checks

1. **Latent dimension sensitivity:** Systematically vary latent dimensions (16, 32, 64) and measure counterfactual quality for spiral classes specifically—can we recover arm structure?

2. **MMD ablation study:** Remove L_MMD and verify whether counterfactuals remain in-distribution and background-invariant, directly testing the regularization's necessity.

3. **Class-wise latent distribution analysis:** For disturbed galaxies, visualize h-space clustering to confirm whether GMM assumption fails, then test alternative distributions (e.g., mixture components >1 per class).