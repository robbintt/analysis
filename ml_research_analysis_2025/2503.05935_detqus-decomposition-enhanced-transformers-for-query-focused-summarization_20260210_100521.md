---
ver: rpa2
title: 'DETQUS: Decomposition-Enhanced Transformers for QUery-focused Summarization'
arxiv_id: '2503.05935'
source_url: https://arxiv.org/abs/2503.05935
tags:
- table
- data
- summarization
- tabular
- tables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents DETQUS, a novel system for query-focused tabular
  summarization that leverages table decomposition to improve summarization accuracy.
  DETQUS uses a large language model to selectively reduce table size by retaining
  only query-relevant columns while preserving essential information, enabling more
  efficient processing of large tables.
---

# DETQUS: Decomposition-Enhanced Transformers for QUery-focused Summarization

## Quick Facts
- arXiv ID: 2503.05935
- Source URL: https://arxiv.org/abs/2503.05935
- Reference count: 10
- Key result: Achieved ROUGE-L score of 0.4437 on QTSUMM dataset, outperforming REFACTOR (0.422)

## Executive Summary
DETQUS addresses the challenge of query-focused tabular summarization by introducing a table decomposition strategy that selectively reduces table size while preserving query-relevant information. The system leverages large language models to identify and retain only essential columns needed for answering specific queries, enabling more efficient processing of large tables that would otherwise exceed model token limits. By fine-tuning the Omnitab encoder-decoder model and implementing intelligent column reduction, DETQUS achieves state-of-the-art performance on the QTSUMM dataset while maintaining scalability for larger tables.

## Method Summary
DETQUS operates through a multi-stage approach where large tables are first decomposed into query-relevant subsets using LLM-based column selection. The system employs a fine-tuned Omnitab model for generating query-focused summaries from these reduced tables. The decomposition process intelligently identifies and retains only columns that are likely to contribute to answering the query while preserving essential information, effectively addressing the token limitation constraints of modern language models when processing large tabular data.

## Key Results
- Achieved ROUGE-L score of 0.4437 on QTSUMM dataset
- Outperformed previous state-of-the-art REFACTOR model (ROUGE-L: 0.422)
- Demonstrated effective handling of large tables through selective column reduction strategy

## Why This Works (Mechanism)
DETQUS works by addressing the fundamental challenge of token limitations when processing large tables for summarization. The system uses LLMs to intelligently decompose tables by identifying and retaining only columns relevant to specific queries, significantly reducing input size while preserving essential information. This selective reduction enables the fine-tuned Omnitab model to focus computational resources on query-relevant content rather than processing entire tables, resulting in more efficient and accurate summarization performance.

## Foundational Learning

**Large Language Models (LLMs)**: Why needed - provide the reasoning capability to identify query-relevant columns from large tables; Quick check - evaluate column selection accuracy on diverse query types

**Table Decomposition**: Why needed - addresses token limitations when processing large tables in a single pass; Quick check - measure information retention versus reduction ratio

**Query-Focused Summarization**: Why needed - ensures generated summaries address specific user queries rather than generic overviews; Quick check - assess query-relevance through human evaluation

**Encoder-Decoder Architecture**: Why needed - enables end-to-end generation of coherent summaries from tabular input; Quick check - compare performance with alternative architectures on summarization quality

**ROUGE Metrics**: Why needed - provides standardized evaluation of summarization quality through n-gram overlap; Quick check - validate results using human evaluation for factual consistency

## Architecture Onboarding

Component Map: LLM Column Selector -> Table Decomposition -> Omnitab Fine-tuning -> Summary Generation

Critical Path: The core workflow involves receiving table and query, using LLM to identify relevant columns, decomposing table, feeding reduced table to fine-tuned Omnitab, and generating final summary.

Design Tradeoffs: The primary tradeoff involves information loss during column reduction versus processing efficiency. The system prioritizes query-relevance over completeness, which may omit potentially useful information but enables handling of larger tables within token constraints.

Failure Signatures: Potential failures include over-aggressive column reduction leading to incomplete summaries, LLM misidentification of relevant columns, and fine-tuned Omnitab generating hallucinated content when insufficient information is provided.

First Experiments:
1. Baseline comparison without decomposition to quantify the impact of the column reduction strategy
2. Ablation study testing Omnitab with different fine-tuning datasets to optimize performance
3. Stress test with progressively larger tables to determine practical token limit boundaries

## Open Questions the Paper Calls Out
The paper acknowledges that the decomposition strategy may inadvertently remove query-relevant information during column reduction, though it does not provide systematic analysis of this potential information loss. The evaluation is limited to a single dataset (QTSUMM), raising questions about generalizability to other tabular structures and domains. The modest improvement over baseline methods (2.17 percentage points in ROUGE-L) suggests room for further optimization of the decomposition approach.

## Limitations
- Single-dataset evaluation (QTSUMM) limits generalizability claims
- Potential information loss during column reduction not systematically evaluated
- Modest performance improvement (2.17 percentage points) may not justify complexity for all use cases
- ROUGE-L metric limitations in capturing factual accuracy and semantic equivalence

## Confidence
High confidence in the technical implementation and methodology description
Medium confidence in the claimed performance improvement due to single-dataset evaluation
Medium confidence in the decomposition strategy's generalizability across different table structures and query types

## Next Checks
1. Evaluate DETQUS on additional tabular summarization datasets with varying table structures and domain characteristics to assess generalizability
2. Conduct human evaluation studies comparing summaries generated by DETQUS versus baseline methods, focusing on factual accuracy, relevance, and coherence
3. Perform ablation studies to quantify the impact of each component (column reduction, Omnitab fine-tuning, table decomposition) on overall performance and identify potential information loss during the decomposition process