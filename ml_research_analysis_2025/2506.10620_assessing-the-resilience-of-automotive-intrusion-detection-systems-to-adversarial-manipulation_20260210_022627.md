---
ver: rpa2
title: Assessing the Resilience of Automotive Intrusion Detection Systems to Adversarial
  Manipulation
arxiv_id: '2506.10620'
source_url: https://arxiv.org/abs/2506.10620
tags:
- attacks
- attack
- adversarial
- detection
- decay
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the resilience of automotive intrusion
  detection systems (IDSs) to adversarial manipulation, focusing on evasion attacks
  that aim to bypass detection while maintaining malicious intent. The authors extend
  their prior work by evaluating gradient-based adversarial attacks under white-box,
  grey-box, and black-box scenarios against state-of-the-art IDSs on two real-world
  datasets (ReCAN and CarHacking).
---

# Assessing the Resilience of Automotive Intrusion Detection Systems to Adversarial Manipulation

## Quick Facts
- arXiv ID: 2506.10620
- Source URL: https://arxiv.org/abs/2506.10620
- Reference count: 40
- This paper investigates gradient-based adversarial attacks against automotive IDSs across white-box, grey-box, and black-box scenarios, showing evasion attacks can degrade detection performance by up to 60%.

## Executive Summary
This study evaluates the resilience of automotive intrusion detection systems to adversarial manipulation, extending prior work by testing gradient-based evasion attacks (FGSM, BIM, DeepFool) against state-of-the-art IDSs on two real-world datasets. The research adapts computer vision adversarial techniques to the automotive domain with domain-specific constraints ensuring physical plausibility. Results demonstrate that evasion attacks are feasible but highly dependent on dataset quality, target IDS architecture, and attacker knowledge, with white-box attacks achieving up to 60% degradation in detection performance while grey- and black-box attacks show variable but still impactful results.

## Method Summary
The paper evaluates three gradient-based adversarial attack algorithms (FGSM, BIM, DeepFool) against six IDS architectures (FFNN, CANdito, LSTM/GRU predictors) on ReCAN and CarHacking datasets. Attacks are tested across three knowledge scenarios: white-box (full model access), grey-box (oracle model access), and black-box (transferability). Domain-specific adaptations include signal range clipping to valid CAN bus values, rounding to bit-level representations, and maintaining attack semantic integrity. The evaluation measures true positive rate degradation, aggregate perturbation magnitude, and attack preservation (AP) scores.

## Key Results
- White-box attacks degrade IDS performance by up to 60%, with DeepFool consistently outperforming other algorithms
- Grey-box attacks achieve up to 39% degradation while black-box attacks reach 39% effectiveness
- Autoencoder-based IDSs (particularly CANdito) show higher resilience to adversarial manipulation than predictor-based models
- Predictor-based oracles generate more transferable adversarial examples than autoencoders in grey- and black-box scenarios
- Paradoxically, some grey- and black-box attacks increased detection rates, suggesting limited-knowledge attackers may inadvertently make attacks more conspicuous

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Gradient-based evasion attacks can degrade automotive IDS detection performance by manipulating payload features within physically plausible constraints.
- **Mechanism:** The attack algorithms compute perturbations by exploiting gradient ascent of the loss function, pushing inputs toward lower-confidence regions near the decision boundary. Domain-specific adaptations (clipping to [0,1] normalized signal ranges, rounding to valid bit representations) ensure perturbed payloads remain physically realizable on the CAN bus.
- **Core assumption:** The IDS model's decision boundary is differentiable and can be approximated via gradient computation.
- **Evidence anchors:** [abstract] "Results show that evasion attacks can degrade IDS detection performance by up to 60% in white-box scenarios"; [section 6] "The algorithms we propose are derived from gradient-based techniques... the common rationale... is to leverage the backpropagation algorithm against the model under attack"; [corpus] Limited direct corpus validation for this specific automotive domain adaptation; related work (paper 4872) confirms ML-based IDS vulnerability to black-box adversarial attacks in network security contexts
- **Break condition:** If the target IDS uses non-differentiable components or quantization that destroys gradient information, gradient-based methods may fail to converge.

### Mechanism 2
- **Claim:** Adversarial example transferability enables effective attacks even in black-box scenarios where attackers lack direct model access.
- **Mechanism:** Surrogate models (oracles) trained on similar data can generate adversarial perturbations that transfer to the target IDS. The paper demonstrates that predictor-based architectures (LSTM/GRU) produce more transferable examples than autoencoder-based models, potentially due to similar learned representations across recurrent architectures.
- **Core assumption:** Surrogate and target models learn sufficiently similar decision boundaries despite different training data or architectures.
- **Evidence anchors:** [abstract] "predictor-based oracles generate more transferable adversarial examples"; [section 7.4.4] "In the black-box scenario, the predictive models remain the overall better oracles, consistently achieving better evasion performances than the autoencoders"; [corpus] Paper 4872 addresses black-box adversarial attacks against ML-based IDS, supporting transferability concerns in network security domains
- **Break condition:** If surrogate and target models have fundamentally different architectures or training distributions, transferability degrades significantly (evidenced by CANdito's resistance to transfer-based attacks).

### Mechanism 3
- **Claim:** Autoencoder-based IDS architectures exhibit higher resilience to gradient-based adversarial manipulation compared to predictor-based models.
- **Mechanism:** CANdito's symmetric architecture with bidirectional LSTM layers and sequence reconstruction may create smoother, less gradient-sensitive decision boundaries. The initial fully connected layer and non-overlapping window approach prevent over-weighting recent packets, potentially making the model less sensitive to targeted perturbations.
- **Core assumption:** The architectural properties of sequence-to-sequence autoencoders inherently produce more robust representations than sequence-to-point predictors.
- **Evidence anchors:** [abstract] "Autoencoder-based models like CANdito show higher resilience"; [section 7.4.5] "CANdito remains the most effective intrusion detection algorithm across all scenarios, it is also the worst-performing oracle"; [corpus] No direct corpus evidence for this specific architectural comparison in automotive IDS
- **Break condition:** If the autoencoder's reconstruction threshold is set too low or training data lacks diversity, the model may become vulnerable to specific perturbation patterns.

## Foundational Learning

- **Concept: CAN bus protocol and signal semantics**
  - Why needed here: Adversarial perturbations must remain within physically valid signal ranges; understanding CAN frame structure (data fields, IDs, signal types) is essential for generating realistic attack payloads.
  - Quick check question: Can you explain why perturbing a CAN payload bit outside the identified "physval" range might cause a denial-of-service rather than an evasion attack?

- **Concept: Anomaly detection thresholds and false positive trade-offs**
  - Why needed here: The paper notes that adversarial perturbations can sometimes increase detection rates; understanding how thresholds affect detection sensitivity helps interpret these paradoxical results.
  - Quick check question: Why might a grey-box attack using an autoencoder oracle increase the target predictor IDS's detection rate?

- **Concept: Transferability in adversarial machine learning**
  - Why needed here: Black-box attacks rely on surrogate models; understanding what architectural properties enhance or inhibit transferability is critical for realistic threat assessment.
  - Quick check question: Based on the results, would you expect adversarial examples generated by a GRU predictor to transfer more effectively to an LSTM or an autoencoder IDS? Why?

## Architecture Onboarding

- **Component map:** Preprocessing module -> Evasion algorithm core -> Oracle models -> Attack injection module
- **Critical path:**
  1. Identify target CAN ID and attack type (fuzzy, continuous change, masquerade replay, injection replay)
  2. Train oracle model on available data (varies by attacker knowledge scenario)
  3. Generate adversarial perturbations using selected algorithm with domain constraints
  4. Validate perturbation preserves attack semantics (signal-level inspection)
  5. Precompute injection sequence or identify real-time insertion points

- **Design tradeoffs:**
  - **Perturbation magnitude vs. evasion success:** DeepFool achieves highest AP scores (up to 0.79) and largest TPR degradation but may nullify attack intent
  - **Oracle architecture selection:** Predictors generate better transferable examples but autoencoders more resilient as targets
  - **Attack type complexity:** Fuzzy attacks easier to detect but also easier to evade; masquerade replay harder to detect and evade
  - **Real-time vs. precomputed:** Real-time gradient computation incompatible with CAN bus timing; precomputation requires traffic pattern matching

- **Failure signatures:**
  - **Semantic loss:** Perturbation pushes signal values toward legitimate baseline, nullifying attack intent (see Figure 2e)
  - **Algorithm non-convergence:** DeepFool fails to find evasive point within iteration limit (see Figure 2f)
  - **Transfer failure:** Grey/black-box attacks sometimes increase detection rate (negative delta TPR in tables)
  - **Injection point scarcity:** For rapidly varying signals, precomputed sequences lack matching traffic patterns

- **First 3 experiments:**
  1. **Baseline IDS performance:** Train all six architectures on ReCAN/CarHacking datasets, measure recall/precision on non-evasive attacks to establish detection capabilities
  2. **White-box evasion validation:** For each oracle-IDS pair (diagonal entries in result tables), apply all three evasion algorithms, measure TPR degradation and aggregate perturbation; verify DeepFool achieves highest AP and largest TPR drop
  3. **Transferability assessment:** Train oracles and IDSs on disjoint dataset splits, test adversarial examples generated by each oracle against all target IDSs; confirm predictor-based oracles achieve higher cross-model evasion rates than autoencoders

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well do adversarial perturbations preserve the original malicious intent and physical impact of an attack when executed on a live vehicle?
- Basis in paper: [explicit] The Conclusion states, "Future work will aim to evaluate how well perturbed attack sequences preserve the original attack intent," and the Limitations section notes the lack of a real test vehicle prevented evaluating influence on vehicle behavior.
- Why unresolved: The study utilized pre-collected traffic logs, making it impossible to determine if the mathematically successful evasion actually achieved the attacker's physical goals (e.g., specific sensor manipulation).
- What evidence would resolve it: Experiments on a hardware-in-the-loop (HIL) simulator or real vehicle measuring the physical output of perturbed commands to verify if the semantic attack goal is maintained.

### Open Question 2
- Question: Are score-based adversarial attack methods more effective or transferable than gradient-based methods in the automotive CAN domain?
- Basis in paper: [explicit] The Conclusion explicitly calls for "further research [to] investigate the applicability and effectiveness of alternative adversarial approaches, such as score-based methods, in tabular and temporal domains."
- Why unresolved: The paper focused exclusively on gradient-based techniques (FGSM, BIM, DeepFool), leaving the efficacy of other attack strategies in this specific domain unknown.
- What evidence would resolve it: A comparative study benchmarking score-based attacks against the gradient-based methods analyzed in the paper, using the same datasets (ReCAN, CarHacking).

### Open Question 3
- Question: Can adversarial training be adapted for automotive IDSs to mitigate evasion attacks without causing unacceptable false positive rates?
- Basis in paper: [inferred] The Discussion on Defenses section suggests adversarial training is a potential mitigation but warns it presents "unique challenges" and may increase false positives, potentially rendering the system "practically unusable."
- Why unresolved: While adversarial training is standard in computer vision, the strict safety requirements and data characteristics of automotive systems make the trade-off between robustness and false alarms unverified.
- What evidence would resolve it: Evaluation of false positive rates and detection accuracy on models trained with perturbed examples to see if a viable balance exists for safety-critical deployment.

## Limitations

- **Dataset representativeness**: Evaluation relies on two specific datasets with limited attack diversity and realistic traffic patterns, making generalizability to broader automotive contexts uncertain
- **Physical feasibility validation**: Limited validation that generated adversarial CAN frames would successfully propagate through actual vehicle ECUs without causing errors or triggering hardware-level detection
- **Oracle effectiveness gap**: Performance gap between surrogate and target models in black-box scenarios limits practical relevance of theoretical transferability advantages

## Confidence

- **High confidence**: Gradient-based evasion attacks can degrade IDS performance by up to 60% in white-box scenarios
- **Medium confidence**: Autoencoder-based architectures exhibit higher resilience to adversarial manipulation
- **Low confidence**: Grey-box and black-box attacks achieve significant but variable effectiveness

## Next Checks

1. **Cross-dataset validation**: Evaluate the same attack scenarios across additional automotive datasets with different vehicle models and attack types to assess generalizability of observed evasion effectiveness patterns

2. **Hardware-in-the-loop testing**: Implement successful adversarial examples on actual CAN bus hardware to verify that perturbed frames propagate correctly and evade detection without causing communication errors or triggering ECUs' built-in error handling

3. **Adaptive defense evaluation**: Test whether incorporating adversarial training using the proposed evasion techniques improves IDS resilience across all attack scenarios, particularly focusing on grey-box and black-box cases where transferability proved challenging