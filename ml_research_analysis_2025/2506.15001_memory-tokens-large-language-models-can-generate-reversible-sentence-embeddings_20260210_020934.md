---
ver: rpa2
title: 'Memory Tokens: Large Language Models Can Generate Reversible Sentence Embeddings'
arxiv_id: '2506.15001'
source_url: https://arxiv.org/abs/2506.15001
tags:
- text
- embedding
- sequence
- llama
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces memory tokens, a method to generate reversible\
  \ sentence embeddings by training a special token\u2019s embedding to reconstruct\
  \ a fixed text sequence. Using a frozen large language model (LLM), the memory token\
  \ embedding is optimized via autoregressive training on a single sequence."
---

# Memory Tokens: Large Language Models Can Generate Reversible Sentence Embeddings

## Quick Facts
- arXiv ID: 2506.15001
- Source URL: https://arxiv.org/abs/2506.15001
- Reference count: 11
- Primary result: Memory tokens enable exact text reconstruction using greedy decoding with large language models

## Executive Summary
This work introduces memory tokens, a method to generate reversible sentence embeddings by training a special tokenâ€™s embedding to reconstruct a fixed text sequence. Using a frozen large language model (LLM), the memory token embedding is optimized via autoregressive training on a single sequence. When used as input, the model can exactly reconstruct the original text via greedy decoding. Evaluated on English and Spanish datasets with sequences up to ~240 tokens and models from 100M to 8B parameters, Llama 3.1 8B successfully reconstructed all sequences. Smaller models struggled with longer sequences, showing accuracy degraded with sequence length. Memory tokens offer potential applications in memory-based retrieval, text compression, and controlled generation, while also raising concerns about misuse in adversarial attacks.

## Method Summary
The memory tokens approach involves training a special token's embedding to reconstruct a fixed text sequence through autoregressive training. A frozen LLM is used where only the memory token embedding is optimized to predict the target sequence. During inference, the optimized memory token embedding is used as input, and the model generates the original text through greedy decoding. The method was evaluated across different sequence lengths and model sizes, demonstrating exact reconstruction capabilities with larger models while showing degradation with smaller models and longer sequences.

## Key Results
- Llama 3.1 8B successfully reconstructed all sequences up to ~240 tokens using greedy decoding
- Smaller models showed degraded accuracy with longer sequences
- Exact reconstruction achieved on English and Spanish datasets
- Memory tokens enable potential applications in retrieval, compression, and controlled generation

## Why This Works (Mechanism)
Memory tokens work by creating a learned embedding that encodes the complete target text sequence. During training, the embedding is optimized to maximize the likelihood of the target sequence under the frozen LLM's autoregressive decoding. This creates a compact representation where the embedding alone, when processed by the model, can regenerate the exact original text. The method exploits the LLM's capacity to decode from learned embeddings, effectively turning the embedding space into a reversible codebook for text sequences.

## Foundational Learning
- **Autoregressive Language Modeling**: Why needed - to generate text token by token from an embedding input. Quick check - model predicts next token given previous tokens.
- **Embedding Optimization**: Why needed - to find the optimal embedding that reconstructs the target text. Quick check - embedding gradients computed via backpropagation through frozen model.
- **Greedy Decoding**: Why needed - to ensure deterministic, exact reconstruction. Quick check - always select highest probability token at each step.
- **Sequence Length Scaling**: Why needed - to understand model capacity limits. Quick check - accuracy tracked as sequence length increases.
- **Cross-Lingual Generalization**: Why needed - to validate method across languages. Quick check - tested on both English and Spanish datasets.

## Architecture Onboarding

**Component Map**: Memory Token Embedding -> Frozen LLM -> Autoregressive Decoder -> Target Sequence Reconstruction

**Critical Path**: The core process flows from the optimized memory token embedding through the frozen LLM's transformer layers to generate the target sequence token by token. The embedding must capture sufficient information to drive the entire generation process.

**Design Tradeoffs**: Using a frozen LLM limits computational overhead during training but constrains adaptability. Greedy decoding ensures exact reconstruction but may miss alternative valid decodings. The method trades model flexibility for training efficiency and reversibility guarantees.

**Failure Signatures**: Smaller models fail on longer sequences with increasing reconstruction errors. Performance degrades when greedy decoding produces tokens that cannot be continued to form the target sequence. Cross-lingual performance may vary based on language model pretraining data.

**3 First Experiments**:
1. Test memory token reconstruction with different decoding strategies (beam search, sampling) to assess robustness.
2. Evaluate performance on progressively longer sequences to identify scalability limits.
3. Test adversarial robustness by applying paraphrasing or noise to input text and measuring reconstruction accuracy.

## Open Questions the Paper Calls Out
The paper raises concerns about potential misuse of memory tokens in adversarial attacks, though specific attack vectors are not detailed. The scalability of the method to longer sequences and larger models beyond the tested 240-token limit and 8B parameters remains unexplored. Additionally, the robustness of memory tokens to real-world variations like paraphrasing, noise, or domain shifts is not investigated.

## Limitations
- Exact reconstruction relies on greedy decoding, which may not hold under alternative decoding strategies
- Performance degrades significantly for smaller models with longer sequences
- Method's robustness to noise, paraphrasing, or adversarial inputs is unexplored
- Scalability beyond 240 tokens and 8B parameters remains untested

## Confidence
- **High**: Core claim that memory tokens enable exact text reconstruction with Llama 3.1 8B
- **Medium**: Scalability and robustness claims, as results degrade for smaller models and longer sequences but broader testing is needed
- **Low**: Potential applications (retrieval, compression) as these are speculative and not empirically validated

## Next Checks
1. Test memory tokens with beam search or sampling-based decoding to assess robustness to decoding strategy changes.
2. Evaluate performance on longer sequences (>240 tokens) and larger models (>8B parameters) to determine scalability limits.
3. Assess robustness to adversarial inputs, such as paraphrased or noisy text, to understand real-world applicability.