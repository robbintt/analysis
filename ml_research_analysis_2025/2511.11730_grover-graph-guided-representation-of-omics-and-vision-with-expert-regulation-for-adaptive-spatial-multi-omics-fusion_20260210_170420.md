---
ver: rpa2
title: 'GROVER: Graph-guided Representation of Omics and Vision with Expert Regulation
  for Adaptive Spatial Multi-omics Fusion'
arxiv_id: '2511.11730'
source_url: https://arxiv.org/abs/2511.11730
tags:
- spatial
- grover
- omics
- modalities
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GROVER introduces a graph-guided spatial multi-omics fusion framework
  that integrates transcriptomics, proteomics, and histology images at single-spot
  resolution. It uses a KAN-GCN encoder to model spatial and feature graphs, followed
  by spot-feature-pair contrastive learning to align heterogeneous modalities.
---

# GROVER: Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion

## Quick Facts
- **arXiv ID:** 2511.11730
- **Source URL:** https://arxiv.org/abs/2511.11730
- **Reference count:** 9
- **Primary result:** A graph-guided spatial multi-omics fusion framework integrating transcriptomics, proteomics, and histology images, achieving significant clustering performance gains (e.g., +4.4% ARI on Human Tonsil) through adaptive expert routing and contrastive learning.

## Executive Summary
GROVER introduces a graph-guided framework for adaptive spatial multi-omics fusion that integrates transcriptomics, proteomics, and histology images at single-spot resolution. The method uses a KAN-GCN encoder to model spatial and feature graphs, followed by spot-feature-pair contrastive learning to align heterogeneous modalities. A self-adaptive mixture-of-experts model with dynamic gating selectively weights modalities per spot based on quality, suppressing noise and enhancing robustness. Experiments on four spatial omics datasets show GROVER outperforms state-of-the-art methods across multiple clustering metrics.

## Method Summary
GROVER integrates spatial coordinates, RNA expression, protein abundance, and H&E image patches through a two-stage graph-based pipeline. First, spatial and feature graphs are constructed using KNN, and embeddings are extracted via a KAN-GCN encoder. Second, intra-modal attention fuses spatial and feature representations, followed by contrastive learning to align RNA, ADT, and image embeddings. Finally, a self-adaptive mixture-of-experts (MoE) with dynamic gating selectively weights the three modalities per spot based on quality scores, producing the final fused representation used for downstream clustering.

## Key Results
- GROVER outperforms state-of-the-art methods on four spatial omics datasets, achieving significant gains in clustering metrics (e.g., +4.4% ARI on Human Tonsil).
- The self-adaptive MoE module with dynamic gating is critical, with ablation showing ARI dropping from 46.5% to 42.5% when removed.
- Spot-feature-pair contrastive learning significantly improves spatial coherence, with Silhouette Coefficient dropping from 38.2 to 21.6 when removed.
- The model demonstrates stable performance across hyperparameter ranges and handles modality noise effectively.

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Expert Routing
- **Claim:** Adaptive expert routing improves clustering by selectively suppressing low-quality modalities at the single-spot level.
- **Mechanism:** A MoE layer with dynamic gating assigns confidence scores to RNA, Protein, and Image embeddings per spot, setting weights for unreliable modalities to zero using a threshold (γ=0.3).
- **Core assumption:** Noise and data quality are heterogeneously distributed across tissue, requiring modality selection to vary by location.
- **Evidence anchors:** MoE ablation shows ARI dropping from 46.5% to 42.5%; MoRE and MOIRA papers support handling incomplete/noisy modalities.
- **Break condition:** If modality quality is uniform across tissue, gating may add unnecessary complexity.

### Mechanism 2: Spot-Feature-Pair Contrastive Learning
- **Claim:** Contrastive learning bridges the semantic gap between omics and images by enforcing consistency despite resolution mismatches.
- **Mechanism:** Masked InfoNCE loss maximizes similarity between aligned embeddings for the same spot while pushing apart non-similar spots, using a similarity mask to prevent over-penalizing biologically similar neighbors.
- **Core assumption:** Corresponding spots in different modalities share biological semantics that can be aligned in a shared latent space.
- **Evidence anchors:** Contrastive loss ablation shows SC dropping significantly (38.2 → 21.6); TF-DWGNet uses tensor fusion but contrastive alignment is distinct.
- **Break condition:** Severe many-to-many mapping between image patches and sequencing spots may break the positive pair assumption.

### Mechanism 3: KAN-GCN for Nonlinear Dependencies
- **Claim:** KANs enhance modeling of nonlinear dependencies in spatial graphs compared to standard GCN linear weights.
- **Mechanism:** KAN-GCN replaces fixed linear transformations with learnable univariate functions on edges, capturing complex non-linear relationships between spots.
- **Core assumption:** Spatial neighbor-feature relationships are highly non-linear and not well-approximated by linear graph convolutions.
- **Evidence anchors:** KAN ablation shows supervised metrics dropping (ARI 46.5 → 42.7); novel architectural choice with no direct spatial omics evidence.
- **Break condition:** If spatial dependencies are primarily linear, KAN may overfit or add computational overhead.

## Foundational Learning

- **Concept: Graph Convolutional Networks (GCN)**
  - **Why needed here:** GROVER relies on message passing between spots defined by spatial coordinates and feature similarity.
  - **Quick check question:** Can you explain how a GCN updates a node's embedding based on its neighbors?

- **Concept: Contrastive Learning (InfoNCE)**
  - **Why needed here:** Essential for aligning heterogeneous modalities (e.g., RNA vs. Image) without explicit paired labels.
  - **Quick check question:** How does the temperature parameter τ affect the gradient in contrastive loss?

- **Concept: Mixture of Experts (MoE)**
  - **Why needed here:** Used to dynamically route information, selecting the best modality per spot rather than averaging all inputs.
  - **Quick check question:** In an MoE layer, what is the role of the "gating network" versus the "expert networks"?

## Architecture Onboarding

- **Component map:** Input (spatial coords S, Omics features F^(m), Image patches) → Graphs (Spatial Graph G_S + Feature Graphs G_F^(m)) → Encoder (KAN-GCN) → Embeddings (e_S, e_F) → Fusion 1 (Intra-modal Attention) → Alignment (Contrastive Loss) → Fusion 2 (Inter-modal Adaptive MoE) → Decoder (Reconstruction)

- **Critical path:** The Self-adaptive MoE (Eq 17-23) is the critical differentiator. Ensure the gating threshold γ and renormalization step (Eq 20) are implemented correctly, as this handles the "quality heterogeneity."

- **Design tradeoffs:** KAN-GCN offers better supervised performance (ARI) but showed mixed results in unsupervised metrics (SC) in ablations. Standard GCN might be a fallback if training time is a bottleneck.

- **Failure signatures:**
  - **Collapse:** If all modality weights fall below threshold γ, check the default fallback logic (Eq 23).
  - **Over-smoothing:** If clustering blurs boundaries, reduce KAN-GCN depth or check contrastive loss convergence.

- **First 3 experiments:**
  1. **MoE Validation:** Run the "w/o MoE" ablation (simple summation vs. gated fusion) to verify the adaptive routing is actually filtering noise.
  2. **Threshold Scan:** Sweep the confidence threshold γ (e.g., 0.1 to 0.5) to find the sweet spot for filtering modalities without losing signal.
  3. **Visual Alignment:** Visualize gating weights on the tissue image to confirm the model "turns off" modalities in low-quality regions (e.g., artifacts).

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions beyond stating the framework is "highly modular and flexible" for incorporating SOTA pathology foundation models, which remains experimentally unsubstantiated.

## Limitations
- KAN contribution is weakest link - while showing supervised gains (ARI), mixed unsupervised results (SC improved without KAN) suggest dataset-specific rather than architectural benefits.
- Performance relies on unproven assumptions about modality quality heterogeneity and contrastive alignment overcoming resolution mismatches.
- Computational efficiency and scalability to single-cell resolution datasets remain unvalidated.

## Confidence
- **High Confidence:** MoE gating mechanism improves clustering performance and handles modality noise (ARI 46.5 → 42.5 without MoE).
- **Medium Confidence:** Contrastive learning effectively aligns heterogeneous modalities despite resolution mismatches (SC 38.2 → 21.6 without contrastive loss).
- **Low Confidence:** KAN-GCN provides meaningful benefits over standard GCN for spatial omics (ARI improves but SC shows mixed results).

## Next Checks
1. **Dataset Generalization Test:** Apply GROVER to a new spatial omics dataset (e.g., 10x Multiome or MERFISH) to verify adaptive gating and contrastive alignment work beyond the original four datasets.
2. **KAN vs GCN Head-to-Head:** Run a controlled ablation comparing KAN-GCN to standard GCN with identical hyperparameters on the same datasets to isolate whether KANs provide real benefits or just overfit.
3. **Modality Quality Heterogeneity Validation:** Generate synthetic spatial omics data with controlled noise patterns to test whether MoE gating correctly identifies and suppresses low-quality modalities based on spatial location.