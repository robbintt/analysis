---
ver: rpa2
title: Synthetic CT Generation from Time-of-Flight Non-Attenutaion-Corrected PET for
  Whole-Body PET Attenuation Correction
arxiv_id: '2504.07450'
source_url: https://arxiv.org/abs/2504.07450
tags:
- images
- imaging
- attenuation
- image
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a deep learning approach to generate synthetic
  CT (sCT) images directly from Time-of-Flight non-attenuation corrected PET images
  for PET/MR attenuation correction. The method leverages a pre-trained Vector Quantization
  model fine-tuned on institutional TOF NAC PET and CT volume pairs.
---

# Synthetic CT Generation from Time-of-Flight Non-Attenutaion-Corrected PET for Whole-Body PET Attenuation Correction

## Quick Facts
- arXiv ID: 2504.07450
- Source URL: https://arxiv.org/abs/2504.07450
- Reference count: 0
- Primary result: Deep learning model generates synthetic CT from TOF NAC PET with MAE 74.49 HU and PSNR 28.66 dB for PET/MR attenuation correction.

## Executive Summary
This study presents a deep learning approach to generate synthetic CT (sCT) images directly from Time-of-Flight non-attenuation corrected PET images for PET/MR attenuation correction. The method leverages a pre-trained Vector Quantization model fine-tuned on institutional TOF NAC PET and CT volume pairs. The best model achieved a mean absolute error of 74.49 Hounsfield Units and peak signal-to-noise ratio of 28.66 dB within the body contour region, demonstrating improved reconstruction of bone and soft tissue structures. This approach highlights the effectiveness of using pre-trained deep learning models for medical image translation tasks, with potential to enhance PET attenuation correction workflows in PET/MR systems.

## Method Summary
The method uses a Vector Quantization model pre-trained on large-scale natural image datasets, fine-tuned on institutional 35 TOF NAC PET-CT pairs. PET volumes are resampled to 1.5mm³ resolution and normalized to [-1,1], then sliced into 2D axial, coronal, and sagittal planes. A pre-trained Latent Diffusion Model (LDM-f4) is selected based on lowest MSE on a CT-to-CT reconstruction task. The model is fine-tuned in three configurations: Scratch, No-frozen (full fine-tuning), and Enc-frozen (encoder frozen). 5-fold cross-validation is used with L1 loss and AdamW optimizer (lr=1e-5). Final 3D sCT volumes are reconstructed by median fusion of the three plane reconstructions within the body contour mask.

## Key Results
- MAE of 74.49 Hounsfield Units and PSNR of 28.66 dB achieved within body contour region
- Enc-frozen fine-tuning configuration outperformed Scratch and No-frozen approaches
- Visual results demonstrate improved synthesis of spine and soft tissue organs from PET images
- Method successfully leverages pre-trained models for medical image translation tasks

## Why This Works (Mechanism)

### Mechanism 1
Pre-training on large-scale natural image datasets provides superior initialization for medical image translation compared to training on medical data alone. Natural images (9M samples from OpenImages) teach the model universal visual features—edges, textures, spatial hierarchies—that transfer to medical image reconstruction. The encoder learns robust feature extraction; the decoder learns faithful reconstruction from compressed representations. Core assumption: Low-level and mid-level visual primitives learned from natural images generalize to CT/PET intensity distributions.

### Mechanism 2
Vector quantization with a learned codebook enables discrete latent representations that are robust to noisy PET inputs. The encoder maps inputs to continuous features; VQ discretizes them by matching to nearest codebook entry; the decoder reconstructs from discrete codes. This bottleneck forces compression of essential patterns while filtering input noise. Core assumption: The codebook capacity (size) is sufficient to represent anatomical diversity in whole-body CT.

### Mechanism 3
Time-of-Flight NAC PET contains latent anatomical structure information sufficient for CT synthesis. TOF timing narrows annihilation localization, reducing noise and improving spatial resolution in NAC PET. Even without attenuation correction, radiotracer distribution correlates with tissue boundaries (e.g., bone vs. soft tissue uptake patterns). Core assumption: The mapping from TOF NAC PET intensity patterns to CT Hounsfield Units is learnable and consistent across patients.

## Foundational Learning

- **Attenuation Correction in PET**: Why needed here: The entire motivation is generating sCT for attenuation correction in PET/MR where CT is unavailable. Without understanding that photon attenuation causes quantification errors, the problem statement is opaque. Quick check question: Why can't raw PET counts be used directly for quantification?

- **Vector Quantization (VQ-VAE family)**: Why needed here: The core architecture uses VQ to discretize encoder outputs into a codebook before decoding. Quick check question: What happens to reconstruction quality if the codebook size is too small?

- **Transfer Learning / Fine-tuning Strategies**: Why needed here: The paper compares Scratch, No-frozen, and Enc-frozen configurations. Understanding which layers to freeze is critical for reproduction. Quick check question: Why might freezing the encoder while training the decoder preserve useful features?

## Architecture Onboarding

- **Component map**: Input TOF NAC PET volume → resampled to 1.5mm³ → sliced into 2D axial/coronal/sagittal planes → encoder maps to latent features → codebook discretizes features → decoder reconstructs 2D sCT → median fusion across 3 planes → 3D sCT volume

- **Critical path**: Obtain pre-trained LDM-f4 checkpoint → preprocess 35 PET-CT pairs (resample, normalize, slice) → fine-tune with L1 loss, AdamW (lr=1e-5), 5-fold CV → evaluate with MAE, PSNR, SSIM, DSC within body mask → fuse 2D outputs via median to reconstruct 3D sCT

- **Design tradeoffs**: Scratch vs. fine-tuning: Scratch requires more data; fine-tuning leverages pre-training but risks negative transfer. Enc-frozen vs. No-frozen: Freezing encoder preserves learned features; full fine-tuning adapts all layers but may overfit on small data (n=35). 2D slicing vs. 3D native: 2D leverages pre-trained 2D models but introduces fusion artifacts ("jagged boundaries" noted in paper).

- **Failure signatures**: Codebook collapse: Many unused codes → check codebook utilization stats. Jagged slice boundaries: Visible in fusion → consider 3D-native architectures or improved fusion. Bone reconstruction errors: High MAE in bone region → may need bone-specific loss weighting.

- **First 3 experiments**: 1) Reproduce CT-to-CT reconstruction baseline to validate LDM-f4 checkpoint quality (target: MAE comparable to Table 1). 2) Ablate fine-tuning configurations (Scratch, No-frozen, Enc-frozen) on held-out fold; confirm Enc-frozen achieves best MAE/PSNR. 3) Visual inspection of sCT for bone vs. soft tissue regions; compare difference maps to ground truth CT.

## Open Questions the Paper Calls Out

- **What is the quantitative impact of the generated sCT on downstream PET attenuation correction accuracy?**: The authors state, "Future work will assess the impact of sCT on PET attenuation correction." The current study evaluates sCT quality via image similarity metrics but does not measure resulting error in PET standardized uptake values or clinical diagnostic accuracy.

- **Can an adapter module mapping PET embeddings directly to CT embeddings improve sCT quality compared to fine-tuning the decoder?**: The discussion suggests "developing an adapter to map embeddings from TOF NAC PET directly to CT... might further improve synthetic CT quality." The current method fine-tunes the decoder, which the authors suspect may not fully leverage the pre-trained decoder's potential stability.

- **Do Vision Transformer (ViT) architectures outperform the Latent Diffusion Model (LDM) for this translation task when scaled to larger datasets?**: The authors note, "Vision Transformers (ViT)... could be beneficial, as pre-trained ViT models for image generation may enhance performance." The study was limited to one pre-trained framework (LDM); the scalability and feature extraction capabilities of ViTs remain untested.

## Limitations

- Small dataset size (n=35 PET-CT pairs) raises concerns about generalizability and potential overfitting
- No external validation on independent institutional datasets to verify broad applicability
- 2D slicing approach introduces potential artifacts at slice boundaries during median fusion
- Code and pretrained model checkpoints not publicly available, creating significant reproduction barriers

## Confidence

- **High confidence**: TOF NAC PET contains latent anatomical structure information; vector quantization provides robustness to noisy inputs
- **Medium confidence**: Pre-training on natural images provides superior initialization compared to medical data alone
- **Low confidence**: Generalizability of approach to other PET/MR systems and patient populations without external validation

## Next Checks

1. Test the trained model on an independent PET-CT dataset from a different institution to assess generalizability and identify potential dataset-specific biases
2. Examine codebook utilization statistics (number of used vs. unused codes) during PET→CT translation to confirm that the codebook capacity is appropriate and that collapse is not occurring
3. Compare median fusion with alternative approaches (e.g., multi-plane attention, 3D native architectures) to quantify and reduce the jagged boundary artifacts noted in the visual results