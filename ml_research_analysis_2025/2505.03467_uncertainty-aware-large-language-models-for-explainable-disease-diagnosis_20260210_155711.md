---
ver: rpa2
title: Uncertainty-Aware Large Language Models for Explainable Disease Diagnosis
arxiv_id: '2505.03467'
source_url: https://arxiv.org/abs/2505.03467
tags:
- diagnostic
- uncertainty
- diagnosis
- data
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed ConfiDx, a fine-tuned large language model
  that recognizes and explains diagnostic uncertainty in disease diagnosis. ConfiDx
  was trained on real-world clinical notes annotated with diagnostic criteria to improve
  uncertainty awareness and explanation capabilities.
---

# Uncertainty-Aware Large Language Models for Explainable Disease Diagnosis

## Quick Facts
- arXiv ID: 2505.03467
- Source URL: https://arxiv.org/abs/2505.03467
- Reference count: 40
- Primary result: ConfiDx fine-tuned LLM achieves F1 score of 0.709 for uncertainty recognition, dramatically outperforming off-the-shelf models (0.102) while providing clinically interpretable explanations

## Executive Summary
This study addresses the critical gap in clinical LLM deployment: recognizing and explaining diagnostic uncertainty when clinical evidence is insufficient. ConfiDx, a fine-tuned large language model, demonstrates superior performance in diagnosing diseases while explicitly identifying when clinical notes lack sufficient evidence per diagnostic criteria. The approach combines task decomposition, diagnostic criteria alignment, and multi-task learning to achieve robust uncertainty awareness and explanation capabilities that exceed both open-source and commercial models.

## Method Summary
ConfiDx fine-tunes open-source 70B LLMs using LoRA adaptation on clinical notes annotated with diagnostic criteria from authoritative guidelines. The approach employs a multi-task learning framework that decomposes disease diagnosis into four subtasks: diagnosis, diagnostic explanation, uncertainty recognition, and uncertainty explanation. Training data balances evidence-complete and evidence-incomplete cases (1:1 ratio), with the latter generated by masking portions of complete evidence. The model uses cross-entropy loss for classification tasks and soft F1 loss for explanation generation, evaluated on MIMIC-IV and external datasets.

## Key Results
- Uncertainty recognition F1 score improved from 0.102 (off-the-shelf) to 0.709 (ConfiDx)
- Diagnostic accuracy maintained at 0.869 while adding uncertainty awareness
- Strong generalization to unseen diseases (accuracy 0.260-0.289) and external institutions
- Explanation quality metrics improved significantly across all evaluation dimensions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Diagnostic criteria integration during fine-tuning aligns model behavior with clinical guideline adherence, enabling systematic evidence sufficiency assessment.
- **Mechanism:** The model learns to explicitly map diagnostic criteria (e.g., "polyuria, polydipsia, and unexplained weight loss") to varied patient descriptions in clinical notes, creating a verifiable evidence-checking capability rather than relying on implicit pattern matching.
- **Core assumption:** Diagnostic criteria serve as reliable proxies for determining whether sufficient evidence exists for confident diagnosis (Assumption: this binary framing may oversimplify clinical reality).
- **Evidence anchors:**
  - [abstract] "fine-tuning open-source LLMs with diagnostic criteria" and "formalized the task and assembled richly annotated datasets that capture varying degrees of diagnostic ambiguity"
  - [section] "By explicitly mapping diagnostic criteria to varied symptom descriptions, the annotated data enabled ConfiDx to understand the meaning of the diagnostic criteria thoroughly"
  - [corpus] Limited direct corpus support—corpus papers focus on LLM misalignment (e.g., "Right Prediction, Wrong Reasoning") rather than criteria-based alignment mechanisms
- **Break condition:** If diagnostic criteria are ambiguous, contradictory across sources, or fail to capture condition heterogeneity, the alignment mechanism degrades.

### Mechanism 2
- **Claim:** Task decomposition into four subtasks (diagnosis, diagnostic explanation, uncertainty recognition, uncertainty explanation) with multi-task learning improves complex instruction following over end-to-end approaches.
- **Mechanism:** Decomposition reduces the cognitive load on the model by treating each component as a distinct learning objective with separate instruction sets, while shared representations transfer knowledge across subtasks (ablation shows each task contributes meaningfully).
- **Core assumption:** The four subtasks are genuinely separable and learning them jointly does not create harmful interference (Assumption: diagnostic accuracy and uncertainty recognition may have conflicting optimization pressures).
- **Evidence anchors:**
  - [abstract] "jointly address diagnostic uncertainty recognition and explanation"
  - [section] "This decomposition enabled more effective processing through a multi-task learning framework, with separate instruction sets for each component"
  - [corpus] Weak corpus evidence—no neighboring papers specifically validate multi-task decomposition for diagnostic uncertainty
- **Break condition:** If subtasks require contradictory representations (e.g., confident diagnosis vs. uncertainty awareness), performance plateaus or degrades on individual metrics.

### Mechanism 3
- **Claim:** Fine-tuning on explicitly annotated evidence-incomplete cases (generated by masking portions of evidence) teaches the model to recognize insufficiency rather than defaulting to overconfent predictions.
- **Mechanism:** Training data contains 1:1 ratio of evidence-complete to evidence-incomplete cases, creating balanced exposure that counters the off-the-shelf tendency to "overestimate the sufficiency of clinical note information."
- **Core assumption:** Masking evidence from complete notes creates realistic uncertainty scenarios that transfer to real-world incomplete documentation (Assumption: artificial masking may not capture the nuance of naturally ambiguous presentations).
- **Evidence anchors:**
  - [abstract] "annotated with diagnostic criteria to improve uncertainty awareness and explanation capabilities"
  - [section] "Evidence-incomplete notes were generated by masking portions of the evidence, simulating incomplete clinical information"
  - [corpus] "Modeling Clinical Uncertainty in Radiology Reports" discusses explicit vs. implicit uncertainty but in radiology context, not general clinical notes
- **Break condition:** If masked cases differ systematically from naturally incomplete cases (distribution shift), robustness on real-world uncertainty will be overestimated.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** ConfiDx uses LoRA for parameter-efficient fine-tuning of 70B models; understanding rank/alpha/dropout tradeoffs is essential for reproduction.
  - **Quick check question:** Can you explain why LoRA enables fine-tuning large models on limited GPU memory, and what the rank=8, alpha=16 configuration implies about adaptation capacity?

- **Concept: Soft F1 Loss for Generation Tasks**
  - **Why needed here:** Explanation generation uses soft F1 loss rather than cross-entropy to handle variable-length outputs; understanding this distinction is critical for debugging explanation quality.
  - **Quick check question:** Why would standard cross-entropy loss be problematic when comparing generated explanations to ground-truth explanations of potentially different lengths?

- **Concept: Diagnostic Criteria as Structured Knowledge**
  - **Why needed here:** The entire approach depends on curating authoritative diagnostic criteria (e.g., ADA diabetes criteria, AHA myocardial infarction criteria); without this, annotation and training collapse.
  - **Quick check question:** For a disease you're familiar with, can you identify where its diagnostic criteria would come from, and what ambiguities might arise when mapping those criteria to unstructured clinical text?

## Architecture Onboarding

- **Component map:** Clinical notes + task instruction -> Open-source 70B LLMs (LLaMA-3.1-70B, DeepSeek-R1-70B, BioLLaMA-70B, Med42-70B) -> LoRA adaptation (rank=8, alpha=16, dropout=0.05) -> Multi-task heads (4 subtasks) -> Cross-entropy + soft F1 losses -> Diagnosis + uncertainty flag + structured explanations

- **Critical path:**
  1. Diagnostic criteria curation from authoritative guidelines (manual, requires clinical expertise)
  2. Multi-agent annotation framework + expert verification (produces training data)
  3. Evidence masking to generate balanced complete/incomplete cases
  4. Multi-task LoRA fine-tuning (10 epochs, batch size 4, lr 1e-4)
  5. Evaluation across 4 subtasks + cross-site generalization

- **Design tradeoffs:**
  - 70B models chosen for reliability vs. computational cost (smaller models excluded due to limited context length and clinical knowledge)
  - LoRA efficiency vs. full fine-tuning expressiveness (no hyperparameter tuning performed beyond defaults)
  - Multi-task learning vs. task-specific models (ablation shows interdependence, but separate models might optimize individual metrics better)
  - Artificial evidence masking vs. naturally incomplete cases (scalability vs. ecological validity)

- **Failure signatures:**
  - Off-the-shelf F1 for uncertainty recognition: 0.102 → if your baseline is higher, check data leakage
  - Interpretation accuracy for uncertainty explanation: 0.017 baseline → near-random indicates model is not attempting the task
  - If cross-site performance (UMN-CDR) drops >30% from MIMIC test, model has overfit to documentation style
  - If ablation shows removing one task improves others, subtasks may have conflicting gradients

- **First 3 experiments:**
  1. **Reproduce baseline gap:** Run off-the-shelf LLaMA-3.1-70B on 100 MIMIC test samples; verify F1 uncertainty ~0.10 and interpretation accuracy ~0.02. This confirms the problem exists before investing in fine-tuning.
  2. **Single-disease pilot:** Fine-tune on one disease type (e.g., Type 1 Diabetes) with 500 notes; evaluate whether diagnostic criteria alignment emerges. This validates the annotation and training pipeline before scaling.
  3. **Ablation checkpoint:** Train with 3 subtasks (excluding uncertainty explanation) and compare to full 4-task model on interpretation accuracy. This isolates whether explanation capability transfers from other subtasks or requires explicit training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would fine-tuning large-scale commercial LLMs (e.g., GPT-4o) on private clinical notes yield superior diagnostic uncertainty recognition compared to the open-source ConfiDx?
- Basis in paper: [explicit] The authors state, "we were unable to validate the proposed approach on these large-scale commercial models," because they "generally cannot process privacy-sensitive clinical notes."
- Why unresolved: Privacy restrictions prevented the authors from applying the ConfiDx fine-tuning framework to proprietary state-of-the-art models using the rich MIMIC-IV and UMN-CDR datasets.
- What evidence would resolve it: Performance metrics (specifically F1 scores for uncertainty recognition) derived from fine-tuning models like GPT-4o on the MIMIC-IV dataset within a secure enclave, compared against ConfiDx baselines.

### Open Question 2
- Question: Can smaller, resource-efficient LLMs (e.g., 7B or 13B parameters) achieve comparable uncertainty recognition capabilities to ConfiDx through specific optimization?
- Basis in paper: [explicit] The authors explicitly note they "did not evaluate smaller LLMs" because they "typically have limited in-context lengths" and often "lack sufficient clinical knowledge."
- Why unresolved: The study prioritized 70B parameter models to ensure reliability and context handling, leaving the potential performance of smaller, more deployable models for this specific task undetermined.
- What evidence would resolve it: An ablation study applying the ConfiDx fine-tuning methodology to smaller models (e.g., LLaMA-3-8B) using context extension techniques, followed by evaluation on the MIMIC test set.

### Open Question 3
- Question: How does the reliance on strict diagnostic criteria affect ConfiDx's performance in clinical domains with ambiguous or highly subjective diagnostic standards?
- Basis in paper: [inferred] The methods section notes the authors excluded "diseases without clear diagnostic criteria" and focused on specific specialties with established guidelines (cardiology, hepatology, endocrinology).
- Why unresolved: The model's architecture and annotation process rely on mapping evidence to definitive criteria; it is unclear if this approach translates to specialties where "insufficient evidence" is subjective or criteria are less binary (e.g., psychiatry).
- What evidence would resolve it: Evaluating the model's uncertainty recognition F1 scores on datasets from specialties with less standardized diagnostic criteria, such as complex chronic pain or psychiatric disorders.

## Limitations

- **Data Representation Bias:** The 1:1 ratio of evidence-complete to evidence-incomplete cases may not reflect real-world clinical documentation patterns, potentially overestimating uncertainty recognition performance in deployment.
- **Criteria Generality:** The approach assumes diagnostic criteria from authoritative guidelines serve as reliable proxies for evidence sufficiency, but clinical practice often involves nuanced interpretation beyond strict guideline adherence.
- **Cross-Specialty Generalization:** While the model shows robustness across 12 diseases, performance on more heterogeneous disease categories (neurological, psychiatric, rare diseases) remains untested.

## Confidence

- **High Confidence (4/5):** The core finding that fine-tuning on diagnostic criteria significantly improves uncertainty recognition and explanation quality is well-supported by the 0.102→0.709 F1 improvement and comprehensive ablation studies.
- **Medium Confidence (3/5):** The robustness claims on unseen diseases and cross-institutional generalization are demonstrated but rely on relatively small test sets (409 and 908 notes respectively).
- **Low Confidence (2/5):** The superiority over commercial models in uncertainty recognition is claimed but tested only on MIMIC-U (6 diseases), which may not represent their full capabilities.

## Next Checks

1. **Ecological Validity Test:** Evaluate ConfiDx on a naturally occurring dataset of clinical notes where incomplete evidence is the dominant pattern (rather than the balanced 1:1 ratio used in training).
2. **Clinical Expert Review:** Conduct systematic evaluation of 100 randomly selected uncertainty explanations by board-certified physicians to assess whether the model's explanations would be clinically actionable.
3. **Cross-Specialty Robustness:** Test ConfiDx on clinical notes from specialties not represented in training (e.g., neurology, psychiatry, dermatology) to evaluate generalization to disease categories with less standardized diagnostic criteria.