---
ver: rpa2
title: 'Everywhere Attack: Attacking Locally and Globally to Boost Targeted Transferability'
arxiv_id: '2501.00707'
source_url: https://arxiv.org/abs/2501.00707
tags:
- attack
- targeted
- attacks
- everywhere
- transferability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving targeted adversarial
  transferability, where attacks crafted against one model fail to consistently fool
  other models due to attention mismatches between surrogate and victim models. The
  proposed Everywhere Attack method splits victim images into non-overlapping blocks
  and jointly attacks each block with the same target label, effectively planting
  multiple target objects across the image to increase overlap with victim model attention
  regions.
---

# Everywhere Attack: Attacking Locally and Globally to Boost Targeted Transferability

## Quick Facts
- arXiv ID: 2501.00707
- Source URL: https://arxiv.org/abs/2501.00707
- Reference count: 16
- Primary result: Improves targeted adversarial transferability by attacking multiple local blocks alongside global image, increasing success rates by 28.8%-300% on ImageNet

## Executive Summary
This paper addresses the challenge of improving targeted adversarial transferability, where attacks crafted against one model fail to consistently fool other models due to attention mismatches between surrogate and victim models. The proposed Everywhere Attack method splits victim images into non-overlapping blocks and jointly attacks each block with the same target label, effectively planting multiple target objects across the image to increase overlap with victim model attention regions. Extensive experiments on ImageNet demonstrate that this approach universally improves state-of-the-art targeted attacks, with the Logit attack's transferability increasing by 28.8%-300%. The method also shows strong performance against transformer-based models and real-world systems like Google Cloud Vision, with improvements of up to 175% in some cases.

## Method Summary
The Everywhere Attack improves targeted adversarial transferability by simultaneously attacking both the global image and multiple local image blocks. The method divides the image into an M×M grid (M=4), samples N blocks (N=9), and pads non-block areas with dataset mean values. During each iteration, the attack computes gradients for the global image and all sampled local blocks simultaneously, then updates the perturbation using iterative methods like MI-FGSM, DI-FGSM, or TI-FGSM. The core insight is that by forcing the adversarial perturbation to create the target object in multiple spatial locations, the attack increases the probability of matching the attention regions of different victim models, thereby improving transferability.

## Key Results
- Logit attack transferability increases by 28.8%-300% compared to baseline TMDI-FGSM
- Universal improvement across all tested targeted attacks (MI, DI, TI variants)
- Strong performance against real-world systems: +51.6% on Google Cloud Vision API, +175.4% on Roboflow
- Effective against transformer models, though with lower absolute success rates than CNNs

## Why This Works (Mechanism)
The Everywhere Attack succeeds by addressing a fundamental limitation of targeted transfer attacks: attention mismatch between surrogate and victim models. When a surrogate model focuses on different image regions than the victim model, the adversarial perturbation may not align with the victim's attention regions, causing the attack to fail. By attacking multiple local blocks simultaneously with the same target label, the method creates an "army of targets" across the image, dramatically increasing the probability that at least one perturbed region will overlap with the victim model's attention focus. This spatial coverage strategy ensures that regardless of where the victim model looks, it encounters evidence of the target class, improving the likelihood of successful misclassification.

## Foundational Learning
- **Adversarial Transferability**: The phenomenon where adversarial examples crafted for one model can fool other models. Why needed: This is the core problem being addressed - improving the success rate when transferring targeted attacks between models. Quick check: Review baseline transfer rates for targeted vs untargeted attacks.
- **Attention Mechanisms in Vision Models**: How convolutional and transformer-based models focus on different image regions. Why needed: The attack exploits differences in spatial attention patterns between models. Quick check: Visualize attention maps for different model architectures on sample images.
- **Iterative FGSM Variants**: Methods like MI-FGSM, DI-FGSM, and TI-FGSM that build adversarial examples through iterative gradient updates. Why needed: These form the base attack framework that Everywhere Attack extends. Quick check: Understand how momentum, diverse inputs, and translation invariance improve attack effectiveness.

## Architecture Onboarding

**Component Map:**
Global Image -> Block Division (M×M) -> Local Block Sampling (N blocks) -> Padding with Dataset Mean -> Joint Attack (Global + Local) -> Gradient Computation -> Perturbation Update

**Critical Path:**
1. Perturb global image → 2. Divide into M×M blocks → 3. Sample N blocks → 4. Pad non-block areas → 5. Concatenate global + local images → 6. Compute joint gradients → 7. Update perturbation

**Design Tradeoffs:**
- Computational cost vs. transferability: Processing 10 images per iteration (1 global + 9 local) significantly increases GPU memory usage but improves success rates
- Block size vs. spatial coverage: Larger M provides finer spatial coverage but increases computational overhead
- Sampling strategy: Random sampling of N blocks balances computational efficiency with attack effectiveness

**Failure Signatures:**
- Out-of-memory errors when processing large batches of global + local images
- Low transferability on transformer models despite high success on CNNs
- Degradation in visual quality due to increased perturbation area across multiple blocks

**First 3 Experiments to Run:**
1. Baseline TMDI-FGSM attack on ImageNet to establish reference transfer rates
2. Everywhere Attack with M=4, N=9 to verify reported improvements
3. Transferability test against different model architectures (CNNs vs. transformers) to observe attention-based performance differences

## Open Questions the Paper Calls Out
- **Question:** Why do standard Vision Transformers (ViT, PiT) exhibit significantly higher resilience to the Everywhere Attack compared to Visformer?
- **Basis in paper:** [explicit] The supplementary material explicitly states: "An interesting observation is that vit_b_16 and pit_b_24 are much more resilient under attack than visformer, which deserves future study."
- **Why unresolved:** The paper notes the discrepancy in success rates but does not investigate the architectural specificities (e.g., attention mechanisms or patch merging) that cause this divergence.
- **What evidence would resolve it:** A comparative analysis of the attention maps and feature robustness of ViT/PiT versus Visformer when subjected to the block-wise perturbations.

## Limitations
- Significant computational overhead due to processing 10 images per iteration (1 global + 9 local blocks)
- Lower absolute transferability rates on transformer models compared to CNNs
- Reliance on specific hyperparameters (M=4 blocks, N=9 sampled blocks) that may require tuning for different scenarios

## Confidence
- **High Confidence:** The core methodology of splitting images into blocks and attacking multiple regions simultaneously is clearly specified and reproducible
- **Medium Confidence:** The transferability improvements against real-world systems (Google Cloud Vision) are demonstrated but may depend on specific API implementations
- **Medium Confidence:** The Logit attack's dramatic improvements (28.8%-300%) are reported, but these may be influenced by specific implementation details of the base TMDI-FGSM attack

## Next Checks
1. **Reproduce the computational overhead:** Measure actual GPU memory usage and runtime when processing the 10-image batch (1 global + 9 local) to verify the practical feasibility of the approach on standard hardware configurations
2. **Test sensitivity to block sampling:** Validate whether the reported improvements hold when varying the number of sampled blocks (N) or block grid size (M), as the paper only reports results for M=4 and N=9
3. **Verify padding implementation:** Confirm that the "mean value of the dataset" padding uses the correct ImageNet normalization statistics ([0.485, 0.456, 0.406] normalized to 0), as this detail is critical for faithful reproduction