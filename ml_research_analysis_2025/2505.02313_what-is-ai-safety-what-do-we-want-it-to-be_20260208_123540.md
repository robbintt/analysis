---
ver: rpa2
title: What Is AI Safety? What Do We Want It to Be?
arxiv_id: '2505.02313'
source_url: https://arxiv.org/abs/2505.02313
tags:
- safety
- systems
- research
- conception
- harms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the conceptual ambiguity in the field of\
  \ AI safety, arguing that a simple yet powerful definition\u2014AI safety as the\
  \ prevention or mitigation of harms from AI systems\u2014is the most effective framework.\
  \ The authors critique two competing conceptions: the \u201CCatastrophic Conception,\u201D\
  \ which focuses solely on existential risks from future systems, and the \u201C\
  Engineering Conception,\u201D which limits AI safety to technical safety engineering\
  \ methodologies."
---

# What Is AI Safety? What Do We Want It to Be?

## Quick Facts
- arXiv ID: 2505.02313
- Source URL: https://arxiv.org/abs/2505.02313
- Authors: Jacqueline Harding; Cameron Domenico Kirk-Giannini
- Reference count: 18
- One-line primary result: The paper argues for defining AI safety as the prevention or mitigation of harms from AI systems, rejecting narrower conceptions focused only on existential risks or technical engineering approaches.

## Executive Summary
This paper addresses conceptual ambiguity in AI safety by arguing that a broad definition—AI safety as harm prevention—is most effective. The authors critique two competing conceptions: the "Catastrophic Conception" focusing only on existential risks, and the "Engineering Conception" limiting safety to technical methodologies. Through conceptual engineering, they argue that the Safety Conception better unifies the field by encompassing both catastrophic and non-catastrophic harms and promotes more comprehensive harm reduction strategies.

The paper concludes that adopting the Safety Conception fosters a more inclusive, impactful approach to AI safety research and governance, encouraging integration across disciplines and avoiding arbitrary exclusions that limit safety efforts.

## Method Summary
The authors employ conceptual engineering/ameliorative inquiry to analyze and evaluate competing definitions of AI safety. They compile definitions from research papers, organizations, and mission statements, classifying them into three conceptions: Safety (harm prevention broadly), Catastrophic (existential/future risks), and Engineering (safety engineering methodology). The analysis evaluates each conception against two purposes: providing unifying explanation of paradigmatic research programs and being conducive to reducing harms from AI systems.

## Key Results
- The Safety Conception (harm prevention broadly) best unifies AI safety research and promotes effective harm reduction
- The Catastrophic Conception artificially restricts solutions to existential risks, potentially deprioritizing high-impact present harm interventions
- The Engineering Conception excludes necessary non-engineering interventions (governance, philosophy, sociology) required for effective harm mitigation

## Why This Works (Mechanism)

### Mechanism 1: Causal Continuity of Harms
Current "social" harms (e.g., bias, toxicity) share underlying technical causal mechanisms with potential future "catastrophic" harms. The paper argues that harms like hate speech and dangerous instructions (e.g., bomb-making) both stem from properties of pre-training corpora and objective function behavior, rather than being distinct problem classes requiring entirely different theories of change.

### Mechanism 2: Merit-Based Allocation Efficiency
Broadening the definition of "AI Safety" to include all harms optimizes resource allocation by prioritizing interventions based on marginal impact rather than categorical exclusion. The "Catastrophic Conception" artificially restricts the pool of admissible solutions to those addressing future/existential risks, potentially deprioritizing high-impact interventions on present harms that degrade societal resilience.

### Mechanism 3: Methodological Completeness
Restricting AI Safety to "Safety Engineering" methodologies excludes necessary non-engineering interventions (governance, philosophy, sociology) required for effective harm mitigation. The "Engineering Conception" fails to account for harms that arise from the interaction of the system with social structures (sociotechnical harms), which require normative or political analysis rather than reliability engineering.

## Foundational Learning

- **Concept: Conceptual Engineering / Ameliorative Inquiry**
  - Why needed here: The paper does not just describe the field; it seeks to *change* the operative definition to serve practical goals (harm reduction). Understanding this methodology distinguishes the paper's normative argument from a descriptive survey.
  - Quick check question: Are you analyzing what "AI Safety" *is* currently, or what it *should be* to maximize harm reduction?

- **Concept: Operative vs. Manifest Concepts**
  - Why needed here: The paper relies on the gap between what researchers *say* they are doing (Manifest: "preventing harm") and how they *act* (Operative: "funding only existential risk research"). Identifying this gap is key to diagnosing the field's fragmentation.
  - Quick check question: Does a funding body claim to support "AI Safety" broadly (Manifest) but only review proposals on "AGI alignment" (Operative)?

- **Concept: The Catastrophic Conception**
  - Why needed here: This is the primary counter-argument the paper engages with. It posits that safety is defined by the *magnitude* and *temporal distance* of the risk. You must understand this to evaluate the tradeoffs.
  - Quick check question: Does the proposed research scope exclude "bias" and "misinformation" because they are not immediately existential threats?

## Architecture Onboarding

- **Component map:** The "Safety Conception" architecture consists of three integrated research pillars:
  1. **Non-empirical/Theoretical:** Harm taxonomies, alignment theory, governance proposals
  2. **Empirical Understanding:** Model evaluation, interpretability, red-teaming
  3. **Systemic Mitigation:** Training interventions (RLHF), deployment filters, sociotechnical audits

- **Critical path:**
  1. **Harm Identification:** Define a harm (e.g., representational bias OR autonomous misuse)
  2. **Scope Check:** Does preventing this harm fall under "Safety" (Yes, per Safety Conception) or "Ethics/Other"?
  3. **Methodology Matching:** Select tool from engineering (robustness) OR social science (auditing) based on the harm's nature, not field boundaries

- **Design tradeoffs:**
  - **Breadth vs. Focus:** Adopting the Safety Conception risks diluting focus on existential risks by including "mundane" harms (e.g., algorithmic bias)
  - **Mitigation:** The paper argues prioritization should be based on *merit* (expected harm reduction), not category

- **Failure signatures:**
  - **"Safety-washing":** Using the vocabulary of safety to resist regulation or ignore non-catastrophic harms
  - **Epistemological Silos:** Maintaining separate "Ethics" and "Safety" teams, preventing bias researchers from informing alignment research

- **First 3 experiments:**
  1. **Audit Project Portfolio:** Review your organization's safety grants. Calculate the percentage addressing present/social harms vs. future/catastrophic harms to identify operative bias.
  2. **Cross-Disciplinary Pilot:** Pair a "bias/toxicity" researcher with an "alignment/scheming" researcher on a shared threat model (e.g., how cultural biases might influence an agent's goal generalization).
  3. **Definition Stress Test:** Apply the "Safety Conception" to a borderline case (e.g., environmental impact of training). If the project aims to mitigate harm, it counts as safety; assess if it would have been excluded under a "Catastrophic" or "Engineering" framework.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does structuring the AI safety research community around The Safety Conception lead to measurably better harm reduction outcomes compared to The Catastrophic or Engineering Conceptions?
- Basis in paper: [inferred] The authors argue The Safety Conception "is likely to result in more effective harm reduction than alternatives" and compare three scenarios, but acknowledge this is contested—some worry it could deprioritize catastrophic risk work. This empirical claim is not tested.
- Why unresolved: The paper provides conceptual arguments but no empirical evidence comparing actual harm reduction outcomes under different disciplinary structures.
- What evidence would resolve it: Longitudinal studies comparing research communities or organizations that adopt different conceptions, measuring harm reduction effectiveness across both catastrophic and non-catastrophic risks.

### Open Question 2
- Question: To what extent do harm mitigation strategies developed for non-catastrophic AI risks (e.g., bias, misinformation) generalize to catastrophic risks?
- Basis in paper: [explicit] The authors claim "there are significant continuities between catastrophic and non-catastrophic risks from AI" and that "many of the concrete catastrophic risks... are simply 'scaled up' versions of risks already well-studied by researchers focusing on social harms," but do not provide systematic evidence for this generalization claim.
- Why unresolved: The paper makes the continuity claim argumentatively; the actual transferability of mitigation strategies across risk types remains empirically unverified.
- What evidence would resolve it: Studies testing whether interventions effective for social harms (e.g., content moderation, adversarial robustness techniques) successfully transfer to catastrophic risk scenarios.

### Open Question 3
- Question: What institutional mechanisms most effectively promote productive integration between researchers working on social harms and those working on catastrophic harms?
- Basis in paper: [explicit] The authors conclude that "there should be greater disciplinary integration" and suggest concrete steps (shared venues, unified teams, cross-domain engagement), but do not investigate which mechanisms actually work.
- Why unresolved: The recommendations are prescriptive; the paper does not study existing integration efforts or test different approaches.
- What evidence would resolve it: Comparative case studies of conferences, labs, or funding bodies that have attempted integration, measuring collaboration outcomes, cross-citation rates, and joint research productivity.

## Limitations

- The continuity thesis between present "social" harms and future catastrophic risks remains speculative without empirical validation
- The definition's boundary conditions remain unclear - environmental impacts of training and economic disruption from automation may not clearly qualify as "AI safety" issues
- The argument for methodological pluralism assumes non-engineering approaches are necessary for effective harm reduction but lacks empirical evidence showing these are more effective than technical solutions alone

## Confidence

- **High**: The observation that definitional ambiguity exists across the field and affects research prioritization
- **Medium**: The claim that a unified definition could improve research efficiency and integration
- **Low**: The assertion that present harm research provides direct transferable insights to future catastrophic risk scenarios

## Next Checks

1. **Portfolio Analysis**: Audit funding organizations' safety grant distributions to quantify how the Safety Conception would expand vs. current operational definitions
2. **Cross-Method Validation**: Test whether bias/toxicity research methods (e.g., dataset auditing) successfully identify failure modes in agent alignment scenarios
3. **Stakeholder Survey**: Measure researcher agreement with the Safety Conception across different AI safety subcommunities to identify potential resistance points