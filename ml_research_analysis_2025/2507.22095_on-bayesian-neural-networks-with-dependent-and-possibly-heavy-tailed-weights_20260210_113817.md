---
ver: rpa2
title: On Bayesian Neural Networks with Dependent and Possibly Heavy-Tailed Weights
arxiv_id: '2507.22095'
source_url: https://arxiv.org/abs/2507.22095
tags:
- random
- neural
- prior
- gaussian
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies Bayesian deep neural networks with dependent\
  \ and possibly heavy-tailed weights, addressing limitations of standard Gaussian\
  \ priors such as independent coordinates and unrealistic weight assumptions. The\
  \ authors prove that, under mild conditions on activation functions and L\xE9vy\
  \ measures, the sequential infinite-width limit of the network output is independent\
  \ of the order of node growth, extending prior results."
---

# On Bayesian Neural Networks with Dependent and Possibly Heavy-Tailed Weights

## Quick Facts
- arXiv ID: 2507.22095
- Source URL: https://arxiv.org/abs/2507.22095
- Reference count: 39
- This paper studies Bayesian deep neural networks with dependent and possibly heavy-tailed weights, proving order-independent infinite-width limits and characterizing posterior distributions as Gaussian mixtures with dependent coordinates.

## Executive Summary
This work addresses limitations of standard Gaussian priors in Bayesian neural networks by developing a theoretical framework for dependent and heavy-tailed weight distributions. The authors prove that under mild conditions on activation functions and Lévy measures, the sequential infinite-width limit of network outputs is independent of node growth order. For Gaussian likelihood, they characterize the posterior distribution in the infinite-width limit as a Gaussian mixture with dependent, non-identically distributed coordinates. Crucially, they establish sufficient conditions for the invertibility of the random covariance matrix under the prior, enabling identification of the posterior limit.

## Method Summary
The authors extend the neural network-Gaussian process correspondence to settings with dependent and heavy-tailed weights by leveraging Lévy processes and their properties. They analyze sequential infinite-width limits where neurons are added one at a time, proving that the limiting distribution is independent of the order of node addition. For Gaussian likelihood functions, they derive the posterior distribution in the infinite-width limit, showing it follows a Gaussian mixture model with dependent coordinates. The framework accommodates various weight distributions beyond the standard Gaussian case, including those with heavy tails, by carefully controlling the growth of Lévy measures.

## Key Results
- Sequential infinite-width limit is independent of node growth order under mild conditions on activation functions and Lévy measures
- For Gaussian likelihood, posterior distribution in infinite-width limit is a Gaussian mixture with dependent, non-identically distributed coordinates
- Sufficient conditions provided for invertibility of the random covariance matrix under the prior, enabling posterior identification

## Why This Works (Mechanism)
The order-independence of the infinite-width limit stems from the martingale convergence properties of the neural network outputs as width increases. The dependent coordinate structure in the posterior arises naturally from the correlation structure induced by the dependent weight prior. The Gaussian mixture characterization emerges from the conjugate prior-posterior relationship in the infinite-width regime, while the invertibility conditions ensure the posterior remains well-defined.

## Foundational Learning

1. **Lévy Processes** (why needed: Generalize Gaussian priors to heavy-tailed distributions; quick check: Verify the Lévy measure satisfies the integrability conditions)
2. **Neural Network-Gaussian Process Correspondence** (why needed: Bridge finite and infinite-width regimes; quick check: Confirm the kernel function converges to the expected limiting form)
3. **Martingale Convergence** (why needed: Establish order-independent limits; quick check: Check the variance of network outputs converges appropriately)
4. **Gaussian Mixture Posteriors** (why needed: Characterize infinite-width posterior structure; quick check: Validate the mixture components match theoretical predictions)
5. **Covariance Matrix Invertibility** (why needed: Ensure posterior identification; quick check: Test the sufficient conditions hold in simulations)

## Architecture Onboarding

Component map: Input -> Activation Functions -> Weight Priors (Lévy measures) -> Sequential Width Expansion -> Infinite-Width Limit -> Posterior Mixture

Critical path: The key computational path runs through the sequential width expansion, where each new neuron's contribution must be properly integrated into the existing network output distribution. The activation function properties and weight prior structure critically determine whether the infinite-width limit exists and is order-independent.

Design tradeoffs: Heavy-tailed priors offer more realistic weight uncertainty modeling but require stronger technical conditions for theoretical guarantees. Dependent weights provide richer correlation structures but complicate the posterior analysis. Sequential width expansion enables rigorous infinite-width analysis but may be computationally less efficient than parallel approaches.

Failure signatures: Non-convergence of the network output distribution indicates violated assumptions on activation functions or Lévy measures. Non-invertible covariance matrices suggest insufficient conditions for posterior identification. Order-dependence of limits signals breakdown of the martingale convergence properties.

First experiments:
1. Test order-independence by growing networks with different neuron addition sequences
2. Verify posterior mixture structure by comparing empirical and theoretical covariance matrices
3. Evaluate heavy-tailed behavior by measuring tail decay rates in network outputs

## Open Questions the Paper Calls Out
None

## Limitations
- The proof relies on technical assumptions about measurability that are not fully verified
- Posterior covariance invertibility established through sufficient rather than necessary conditions
- Numerical validation limited to two network architectures, lacking heavy-tailed case exploration

## Confidence
- Order-independent infinite-width limit: High (conditional on stated assumptions)
- Posterior limit characterization: Medium (well-defined under given conditions, but assumptions may be restrictive)
- Prior covariance invertibility: Medium (sufficient conditions provided, but completeness unknown)

## Next Checks
1. Test the order-independent limit property empirically across multiple activation functions and network architectures beyond those presented
2. Conduct extensive simulations with heavy-tailed priors to verify theoretical predictions about tail behavior
3. Develop and benchmark practical sampling algorithms for the infinite-width posterior mixture, comparing computational efficiency and accuracy against finite-width approximations