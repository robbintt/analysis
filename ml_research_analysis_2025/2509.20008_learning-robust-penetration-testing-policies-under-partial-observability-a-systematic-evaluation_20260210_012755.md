---
ver: rpa2
title: 'Learning Robust Penetration-Testing Policies under Partial Observability:
  A systematic evaluation'
arxiv_id: '2509.20008'
source_url: https://arxiv.org/abs/2509.20008
tags:
- learning
- network
- testing
- host
- penetration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically evaluates methods for handling partial
  observability in reinforcement learning for penetration testing. The authors extend
  the Network Attack Simulator (NASim) to create StochNASim, introducing variable
  network sizes and stochastic topology generation to better reflect real-world complexity.
---

# Learning Robust Penetration-Testing Policies under Partial Observability: A systematic evaluation

## Quick Facts
- **arXiv ID:** 2509.20008
- **Source URL:** https://arxiv.org/abs/2509.20008
- **Reference count:** 40
- **Primary result:** Simple observation augmentation outperforms complex memory architectures in penetration testing POMDPs

## Executive Summary
This paper systematically evaluates methods for handling partial observability in reinforcement learning for penetration testing. The authors extend the Network Attack Simulator (NASim) to create StochNASim, introducing variable network sizes and stochastic topology generation to better reflect real-world complexity. They compare PPO variants with different approaches to partial observability: frame-stacking, observation augmentation, LSTM, and transformer architectures. Through comprehensive hyperparameter tuning and evaluation across five random seeds, they find that simple observation augmentation (PPO-AO) significantly outperforms complex architectures, converging three times faster while achieving higher cumulative rewards. Manual inspection of learned policies reveals that PPO-AO develops efficient, human-like strategies, while transformer-based approaches learn suboptimal brute-force policies. The work demonstrates that simple history aggregation is sufficient for this task and provides valuable insights into policy behavior beyond quantitative metrics.

## Method Summary
The paper extends NASim to create StochNASim, a stochastic penetration testing environment with variable network sizes (5-8 hosts) and dynamically generated topologies. The authors evaluate PPO variants with different partial observability handling mechanisms: frame-stacking, observation augmentation via element-wise max pooling, LSTM, and transformer architectures. They conduct hyperparameter optimization using Optuna across 250 trials with median pruner, then evaluate the best configurations across five random seeds. The observation augmentation method maintains an aggregated history matrix that combines with current observations via element-wise maximum, creating a monotonically improving belief state.

## Key Results
- PPO-AO converges three times faster than complex architectures while achieving higher cumulative rewards
- PPO-AO achieves a median score of 220.6 over 100 episodes on variable network configurations
- Manual policy inspection reveals PPO-AO learns efficient scanning-then-exploitation strategies while transformers learn suboptimal brute-force approaches
- Training on stochastic topologies significantly improves generalization compared to fixed configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Observation augmentation via element-wise max pooling outperforms recurrent and transformer-based memory for partial observability in penetration testing tasks.
- Mechanism: The environment maintains a state matrix where only four host values change (compromised, reachable, discovered, access level). By applying element-wise maximum aggregation $O^{aug}_t = max(O^{aug}_{t-1}, O_t)$ across observations, discovered information persists permanently within an episode. This creates an explicit, monotonically improving belief state without learned compression.
- Core assumption: The state features are binary or bounded such that element-wise max preserves semantics (e.g., "discovered" stays true once observed).
- Evidence anchors:
  - [abstract] "simple observation augmentation (PPO-AO) significantly outperforms complex architectures, converging three times faster while achieving higher cumulative rewards"
  - [section 4.2] Describes the aggregation wrapper that "stacks an aggregated matrix of observations below the latest observation"
  - [corpus] No direct corroboration; corpus neighbors focus on LLM-based penetration testing, not RL memory mechanisms
- Break condition: If environment dynamics allow state features to revert (e.g., hosts becoming undiscoverable, services changing), static aggregation would encode stale information incorrectly.

### Mechanism 2
- Claim: Training on stochastically regenerated network topologies produces policies that generalize to unseen configurations.
- Mechanism: StochNASim generates new host properties (services, processes, OS assignments) and network sizes (5-8 hosts) per episode. This prevents memorization of fixed action sequences and forces the agent to learn conditional policies based on observed state rather than positional heuristics.
- Core assumption: The action semantics remain consistent across regenerations (same exploit targets same OS-service pair), enabling transferable policy learning.
- Evidence anchors:
  - [section 5.4] "policies trained on static configurations show poor generalization to novel scenarios, while those trained in our variable environment maintain consistent performance"
  - [section 4.1] Describes network regeneration procedure with variable host counts
  - [corpus] Weak support; neighboring papers don't address stochastic training for RL generalization
- Break condition: If action space changes fundamentally between episodes (different exploit types available), learned action embeddings would fail to transfer.

### Mechanism 3
- Claim: The penetration testing task structure makes simple history aggregation sufficient because information gathered is never invalidated within an episode.
- Mechanism: The task has monotonic information gain—once a host's OS or services are scanned, this knowledge remains valid for the episode duration. This eliminates the need for selective memory or belief revision that recurrent architectures provide.
- Core assumption: Network state is static during each episode (no defensive responses, no service restarts).
- Evidence anchors:
  - [section 6] "given the information-retrieval nature of the environment, simply encoding observation history as augmented inputs proves highly effective"
  - [section 6] Authors explicitly note this approach "might become brittle" in dynamic environments where "previously collected observations invalid"
  - [corpus] No direct evidence; related work doesn't compare memory mechanisms systematically
- Break condition: Any environment modification where hosts can change state (firewall updates, service shutdowns, detection-triggered responses) would require forgetting mechanisms that PPO-AO lacks.

## Foundational Learning

- Concept: **Partially Observable MDPs (POMDPs)**
  - Why needed here: The paper's core problem formulation—in agents only observe action outcomes, not full network state, breaking the Markov property.
  - Quick check question: Can you explain why frame-stacking partially addresses partial observability but doesn't achieve the same performance as observation augmentation?

- Concept: **Proximal Policy Optimization (PPO)**
  - Why needed here: All evaluated variants build on PPO; understanding its clipped surrogate objective and actor-critic structure is prerequisite.
  - Quick check question: Why does PPO's stability make it suitable as a baseline for comparing memory mechanisms?

- Concept: **Belief State Estimation**
  - Why needed here: The theoretical justification for history aggregation—agents must maintain belief distributions over hidden states to act optimally.
  - Quick check question: How does PPO-AO's aggregation matrix approximate belief state maintenance?

## Architecture Onboarding

- Component map:
  StochNASim Environment -> Observation Augmentation Wrapper -> PPO Base -> Discrete Action Space

- Critical path:
  1. Environment reset → new network generated (5-8 hosts, new service/process assignments)
  2. Agent receives partial observation → wrapper aggregates with history
  3. PPO policy selects action from 96-dimensional discrete space
  4. Environment returns reward (negative action cost + discovery/host values) and new observation
  5. Wrapper updates aggregation matrix via element-wise max

- Design tradeoffs:
  - **PPO-AO vs. PPO-LSTM**: AO is 3x faster to converge but hardcoded to monotonic state features; LSTM could theoretically handle non-monotonic dynamics but underperforms empirically
  - **Variable vs. fixed network size**: Variable enforces generalization but requires observation padding and action space padding
  - **Scan cost vs. exploit cost**: Scans (cost=1) are cheap but provide information; exploits (cost=3) are expensive but necessary for reward—optimal policy balances this tradeoff

- Failure signatures:
  - **PPO-TrXL learns brute-force**: No scanning behavior, sequentially attempts all exploits—indicates transformer failed to learn information-gathering value
  - **PPO-LSTM high step count**: Uses scans but requires ~2x steps, suggesting poor history compression
  - **Vanilla PPO high entropy**: Even action distribution without strategic focus, indicating inability to maintain state estimates
  - **Training on fixed NASim**: Fast convergence but catastrophic transfer failure to new configurations (Figure 7b)

- First 3 experiments:
  1. **Reproduce PPO-AO vs. PPO baseline on single network size** (5 hosts only, fixed configuration): Establish performance gap in controlled setting; expect PPO-AO to achieve ~180 reward vs. PPO's ~100
  2. **Ablate aggregation method**: Replace element-wise max with (a) element-wise min, (b) most recent only, (c) concatenation without aggregation—to validate max is critical for monotonic feature preservation
  3. **Test transfer to out-of-distribution network sizes** (9-12 hosts): Evaluate whether policies trained on 5-8 hosts generalize beyond training distribution; PPO-AO should maintain performance better than PPO-FS or LSTM based on paper's generalization claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do observation augmentation strategies compare to recurrent architectures in dynamic network environments where host states change during an episode?
- Basis in paper: [explicit] The authors note that in future work involving dynamic environments, "hand-crafted observation augmentations might become brittle and infeasible to maintain."
- Why unresolved: The current StochNASim environment assumes static host states during episodes, making simple aggregation sufficient for retaining history.
- What evidence would resolve it: Evaluating PPO-AO against PPO-LSTM in a modified environment with mid-episode firewall rule changes or host shutdowns.

### Open Question 2
- Question: Can history-aware models with stronger inductive biases or explicit memory mechanisms outperform simple observation aggregation?
- Basis in paper: [explicit] The authors propose developing models with "stronger inductive biases or explicit memory mechanisms" to better handle evolving, partially observed domains.
- Why unresolved: Standard LSTMs and transformers struggled to capture the specific reasoning required, often learning suboptimal brute-force policies.
- What evidence would resolve it: Designing a specialized architecture that matches PPO-AO's sample efficiency while learning adaptive rather than static history representations.

### Open Question 3
- Question: Does the superiority of observation augmentation persist as the action space complexity and number of network properties increase?
- Basis in paper: [inferred] The authors limit the environment to 2 OSs and services to avoid "combinatorial explosion," acknowledging this simplified setup facilitates the current analysis.
- Why unresolved: It is unclear if the efficiency of PPO-AO degrades relative to recurrent methods when the dimensionality of the observation space scales up.
- What evidence would resolve it: Re-running the systematic comparison using networks with significantly more services, processes, and operating systems.

## Limitations

- The study focuses on static network environments where information never becomes invalid, limiting generalizability to dynamic scenarios with changing host states
- Manual policy inspection provides qualitative insights but lacks systematic behavioral metrics for comparing policy strategies
- The ablation study doesn't test simpler aggregation alternatives (element-wise min, most recent only) to isolate the contribution of max pooling

## Confidence

- **High Confidence:** PPO-AO outperforms PPO baseline in convergence speed and final reward
- **Medium Confidence:** PPO-AO's superiority over LSTM and Transformer architectures
- **Medium Confidence:** Generalization benefits from stochastic training
- **Low Confidence:** Claims about PPO-TrXL learning "brute-force" policies without scanning

## Next Checks

1. **Ablation of Aggregation Method:** Test PPO-AO variants using element-wise minimum, most recent observation only, and concatenation without aggregation to isolate the contribution of max pooling to performance gains.

2. **Architectural Transfer Study:** Train PPO-TrXL and PPO-LSTM on variable topologies (not just PPO-AO) and compare their generalization to fixed networks, isolating architectural effects from training distribution effects.

3. **Behavioral Analysis Protocol:** Implement automated policy analysis measuring scan ratio, step efficiency, and action entropy across episodes to quantitatively verify claims about "brute-force" vs. "efficient" policy structures.