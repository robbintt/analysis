---
ver: rpa2
title: 'VoyagerVision: Investigating the Role of Multi-modal Information for Open-ended
  Learning Systems'
arxiv_id: '2507.00079'
source_url: https://arxiv.org/abs/2507.00079
tags:
- task
- agent
- tasks
- should
- voyager
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "VoyagerVision is a multimodal extension of the open-ended Minecraft\
  \ agent Voyager that incorporates visual inputs via GPT-4o. The system integrates\
  \ screenshots from the agent\u2019s point of view into the curriculum, action, and\
  \ critic agents, enabling the pursuit of building-focused tasks in addition to resource\
  \ gathering."
---

# VoyagerVision: Investigating the Role of Multi-modal Information for Open-ended Learning Systems

## Quick Facts
- arXiv ID: 2507.00079
- Source URL: https://arxiv.org/abs/2507.00079
- Authors: Ethan Smyth; Alessandro Suglia
- Reference count: 40
- One-line primary result: VoyagerVision achieved an average of 2.75 unique structures built across 50 iterations, while the original Voyager could not build at all.

## Executive Summary
VoyagerVision is a multimodal extension of the open-ended Minecraft agent Voyager that incorporates visual inputs via GPT-4o. The system integrates screenshots from the agent’s point of view into the curriculum, action, and critic agents, enabling the pursuit of building-focused tasks in addition to resource gathering. In experiments, VoyagerVision achieved an average of 2.75 unique structures built across 50 iterations, while the original Voyager could not build at all. In building unit tests, the agent succeeded in 39% of attempts overall, with higher success (60%) in flat worlds compared to regular worlds (35%). These results demonstrate that visual feedback improves spatial task performance and extends the agent’s open-ended capabilities.

## Method Summary
VoyagerVision extends the original Voyager framework by integrating GPT-4o for multimodal reasoning, capturing screenshots from the agent’s point of view using Prismarine-Viewer, and modifying prompts to encourage building tasks. The three-agent loop (curriculum→action→critic) remains, but now all agents receive both textual and visual input. The system stores successful code solutions in a skill library for reuse, and the critic agent provides feedback for up to 3 retries. Unit tests evaluated five building tasks (pole, wall, stairs, portal, pyramid) in both flat and regular worlds, with additional open-ended trials measuring unique structures built.

## Key Results
- VoyagerVision built an average of 2.75 unique structures across 50 iterations, while the original Voyager built none.
- In building unit tests, the agent succeeded in 39% of attempts overall, with 60% success in flat worlds versus 35% in regular worlds.
- The agent achieved resource-gathering milestones (wooden/stone/iron pickaxe) in 25, 21, and 21 iterations respectively.

## Why This Works (Mechanism)

### Mechanism 1: Visual Context for Spatial Reasoning
Providing screenshot inputs from the agent's point-of-view enables spatial reasoning for building tasks that are impossible for text-only systems. The multimodal model (GPT-4o) receives pixel data alongside textual environment information, allowing the critic agent to verify building completion by "seeing" the structure, and the curriculum/action agents to reason about spatial relationships when planning construction. This assumes vision-language models can extract actionable spatial information from 2D screenshots of 3D environments.

### Mechanism 2: Iterative Task Decomposition with Skill Library
Storing successful code solutions in a skill library allows the agent to decompose complex tasks and reuse learned sub-skills. The action agent generates JavaScript code, which is stored when successful. When a new task is proposed, the agent can retrieve and compose existing skills (e.g., "create a pole") to solve more complex tasks (e.g., "create stairs" described as "a sequence of poles"). This assumes task descriptions can be effectively related to previously solved tasks by the LLM, and the code for sub-tasks is modular and composable.

### Mechanism 3: Automated Self-Critique and Feedback Loop
A critic agent, provided with multimodal input, can self-verify task completion and generate actionable feedback for the action agent to re-attempt failed tasks. After code execution, the critic agent receives the task, context, textual info, and a screenshot, outputting JSON with `success` (boolean) and `critique` (string). If failed, the critique is fed back to the action agent for up to 3 retries. This assumes the critique generated is specific and actionable enough to guide the action agent toward a corrected solution.

## Foundational Learning

- **Open-Endedness**: The system's goal isn't to solve one task, but to autonomously explore an environment and define its own tasks of increasing complexity. *Quick check*: Can you explain the difference between an agent solving a predefined benchmark and an open-ended agent pursuing tasks "of its own choosing"?

- **Vision-Language Models (VLMs)**: The key innovation is replacing a text-only LLM with a VLM (GPT-4o) to process pixel data. Understanding VLM capabilities and limitations is essential. *Quick check*: How does a VLM differ from a standard LLM, and what new type of input can it process?

- **Code Generation for Embodied Agents**: The agent doesn't output low-level actions but generates executable JavaScript code (via the Mineflayer API) that the game engine runs. *Quick check*: Instead of predicting 'move forward,' what does the action agent in this system produce, and what format must it follow?

## Architecture Onboarding

- **Component map**: Environment State -> Screenshot & Text -> Curriculum Agent (Proposes Task) -> Action Agent (Generates Code) -> Code Execution -> Updated State -> Critic Agent (Verifies with Screenshot) -> If Fail: Critique to Action Agent for Retry (max 3); If Success: Store Code & Loop.

- **Critical path**: The agent captures a screenshot, receives a task from the curriculum agent, generates code via the action agent, executes it in the environment, and the critic agent verifies completion using the screenshot and provides feedback if needed.

- **Design tradeoffs**: Substantial prompt changes were needed for GPT-4o and building focus, resulting in a drop in resource-gathering performance compared to original GPT-4 prompts. The system requires task completion before evaluation, preventing mid-task correction. The static screenshot position sometimes leads to poor verification when the agent doesn't reposition optimally.

- **Failure signatures**: Terrain navigation failure (agent fails to find flat terrain in regular worlds), repetitive failure loops (action agent fails to adapt after repeated failures), object placement errors (placing crafting tables on invalid or obstructed blocks), hallucinated success (critic reports success for incomplete structures), and complex multi-step failure (tasks requiring multiple steps and specific item combinations have very low success rates).

- **First 3 experiments**:
  1. Establish a baseline by running original Voyager (text-only) on predefined building prompts to generate failure logs, comparing with VoyagerVision's first attempt logs.
  2. Component ablation by running VoyagerVision on unit tests in flat worlds with no screenshot provided to the Critic Agent, comparing success rate and critique specificity.
  3. Terrain sensitivity analysis by running unit tests in "regular" worlds with measured terrain complexity, plotting success rate vs. complexity to identify the break point where spatial reasoning fails.

## Open Questions the Paper Calls Out

### Open Question 1
Can prompt engineering specifically optimized for GPT-4o restore or improve resource-gathering efficiency to match the original text-only GPT-4 Voyager baseline? The authors note that the performance drop in resource gathering "could potentially be due to the fact that the prompts written for the original Voyager maximised performance for GPT-4" and suggest an optimized prompt is needed to isolate model capability. This remains unresolved as current experiments utilized only minimally functional prompt tweaks.

### Open Question 2
Does incorporating intermediate visual feedback during task execution (within-task evaluation) improve the agent's ability to adapt to spatial errors? Section 5 identifies the inability to provide mid-task feedback as a limitation, proposing that "Future iterations should support within-task evaluation, such as incorporating intermediate screenshots to guide progress." The current framework prevents real-time error correction by requiring tasks to be fully completed before verification.

### Open Question 3
To what extent does the lack of explicit task decomposition mechanisms contribute to failures in constructing spatially complex structures? Section 5 states the system "struggled with constructing spatially and geometrically complex structures, largely due to the absence of a mechanism for breaking down tasks into manageable steps." The agent currently generates code in a single pass without planning sequential actions before code generation.

## Limitations
- The system's reliance on a fixed screenshot perspective and lack of mid-task feedback are significant architectural limitations that are acknowledged but not quantified in impact.
- The critic agent's feedback loop occasionally hallucinates success, reporting incomplete structures as complete (e.g., 3/5 pyramids reported as complete, but only 2/5 truly so).
- Performance drops significantly in regular worlds (35% success) compared to flat worlds (60% success) due to terrain navigation challenges.

## Confidence

- **High**: Visual feedback via screenshots is necessary and improves spatial task success (2.75 vs 0 unique structures, 60% vs 35% flat/regular world success).
- **Medium**: The skill library effectively decomposes and reuses code for complex tasks (staircase described as "sequence of poles").
- **Medium**: The critic agent's feedback loop is effective at guiding the action agent to success (within 3 retries).
- **Low**: The specific impact of the skill library and critique mechanism without ablation studies.

## Next Checks

1. **Component Ablation Study**: Run VoyagerVision on unit tests in flat worlds but systematically disable the skill library and the critique feedback loop (single attempt only). Compare success rates to the full system to isolate the contribution of each component.

2. **Terrain Complexity Analysis**: Systematically vary terrain complexity (e.g., measured by slope, obstruction density) in regular worlds. Run unit tests and plot success rate vs. complexity to identify the exact environmental break point where spatial reasoning fails.

3. **Critic Reliability Audit**: For all unit test runs, log every critic JSON output. Manually classify critiques as "actionable" or "vague/novel." Correlate this classification with the subsequent success rate of the 3 allowed retries to quantify the effectiveness of the feedback.