---
ver: rpa2
title: 'Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models via
  Plot Hole Detection'
arxiv_id: '2504.11900'
source_url: https://arxiv.org/abs/2504.11900
tags:
- story
- error
- continuity
- stories
- plot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces plot hole detection as a novel task for evaluating
  deep narrative understanding in large language models (LLMs). The authors develop
  FLAWED FICTIONS MAKER, an algorithm that introduces controlled plot holes into human-written
  stories by negating established facts in later acts.
---

# Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models via Plot Hole Detection

## Quick Facts
- arXiv ID: 2504.11900
- Source URL: https://arxiv.org/abs/2504.11900
- Reference count: 40
- Key outcome: Plot hole detection benchmark reveals LLMs struggle with narrative consistency, especially in long stories, highlighting gaps in deep language understanding.

## Executive Summary
This paper introduces plot hole detection as a novel task for evaluating deep narrative understanding in large language models (LLMs). The authors develop FLAWED FICTIONS MAKER, an algorithm that introduces controlled plot holes into human-written stories by negating established facts in later acts. Using this method, they construct FLAWED FICTIONS, a high-quality benchmark for plot hole detection, consisting of 414 short stories (average 731 words) with human-verified annotations. Experiments show that even state-of-the-art LLMs like GPT-4o and Claude 3.5 Sonnet struggle with plot hole detection, with performance significantly degrading as story length increases. The study also reveals that LLM-generated stories (e.g., from summarization and adaptation tasks) contain substantially more plot holes than human-written originals. The findings highlight substantial gaps in LLMs' narrative comprehension capabilities and demonstrate the value of plot hole detection as a test bed for evaluating deep language understanding.

## Method Summary
The paper introduces plot hole detection as a benchmark for evaluating deep narrative understanding in LLMs. The FLAWED FICTIONS MAKER algorithm introduces controlled plot holes by negating established facts in later acts of human-written stories. The process involves: (1) partitioning stories into three acts, (2) extracting propositions from the setup, (3) generating counterfactual stories where propositions are negated, (4) "patching" the original setup with counterfactual later acts to create continuity errors, and (5) human verification. The resulting FLAWED FICTIONS benchmark contains 414 stories with human-verified annotations. The paper evaluates various LLMs on binary classification (continuity error exists or not) and localization tasks (identifying specific sentences with errors), revealing significant performance degradation as story length increases.

## Key Results
- LLMs like GPT-4o and Claude 3.5 Sonnet struggle with plot hole detection, especially as story length increases.
- Performance on plot hole detection significantly degrades with longer stories, with models achieving near-random performance on stories over 2000 words.
- LLM-generated stories contain substantially more plot holes than human-written originals, suggesting current generation models struggle with narrative consistency.
- Inference-time scaling (reasoning tokens) does not inherently resolve continuity errors unless the search process is guided toward specific contradiction hypotheses.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Plot hole detection can be operationalized as a "patching" task where an original story context is mismatched with a counterfactual continuation.
- Mechanism: The system extracts facts from Act 1 ($\phi$), generates a counterfactual story where $\neg\phi$ is true ($f_{\neg\phi}$), and constructs a "patched" story using the original Act 1 and the counterfactual Acts 2 and 3 ($f_{patch} := A_1 \cdot A^{\neg\phi}_2 \cdot A^{\neg\phi}_3$). This forces a continuity error between the established context and the subsequent narrative events.
- Core assumption: Readers (or models) rely on the initial context (Act 1) to build a "belief world" ($W_b$) and will detect violations when later text implies an impossible world ($W_f = \{\}$).
- Evidence anchors:
  - [abstract] "FlawedFictionsMaker... introduces controlled plot holes... by negating established facts in later acts."
  - [section 3] "Step 4. Re-building Story ('Patching')... given the original story... and its counterfactual... we create a story with a potential continuity error by concatenating $A_1$ from the original story and the subsequent acts from the counterfactual."
  - [corpus] Weak direct link; related work focuses on story generation rather than consistency detection algorithms.
- Break condition: If the counterfactual generation ($f_{\neg\phi}$) fails to negate $\phi$ plausibly (e.g., by ignoring the premise), the resulting story may lack a sharp contradiction or be too incoherent to evaluate.

### Mechanism 2
- Claim: Model performance on narrative consistency is inversely correlated with context length and the distance between the establishing fact and its contradiction.
- Mechanism: As the number of tokens between $A_1$ (context) and $A_3$ (error) increases, the model's ability to maintain the "belief world" state degrades, leading to near-random performance on the classification task.
- Core assumption: Transformers suffer from "lost in the middle" phenomena or effective context window limitations that impair state tracking over long narratives.
- Evidence anchors:
  - [abstract] "Experiments show that... performance significantly degrading as story length increases."
  - [section 5.1] "FLAWED FICTIONS LONG... proves particularly difficult, with almost all models obtaining close to random level performance on the classification task."
  - [corpus] "Can LLMs Generate Good Stories? Insights and Challenges from a Narrative Planning Perspective" discusses similar challenges in maintaining long-form coherence, indirectly supporting the difficulty.
- Break condition: If a model employs a robust external memory or state-tracking mechanism (outside standard Transformer attention), this length-dependent degradation might be mitigated.

### Mechanism 3
- Claim: Inference-time scaling (reasoning tokens) does not inherently resolve continuity errors unless the search process is guided toward specific contradiction hypotheses.
- Mechanism: Increasing "reasoning effort" (tokens generated before the final answer) allows models to explore hypotheses. However, for plot holes, the hypothesis space is quadratic relative to sentence pairs ($O(N^2)$). Without guided search, models may persist on wrong hypotheses, causing performance to plateau or drop.
- Core assumption: Current "reasoning" models (like o1) use free-form thought chains that are inefficient for exhaustive pairwise comparison required by the continuity error definition.
- Evidence anchors:
  - [abstract] "Performance significantly degrading as story length increases... regardless of the reasoning effort allowed."
  - [section 5.1] "o3-mini... represents an increase from less than 1000 reasoning tokens... to over 5000... yet results in degraded performance."
  - [corpus] Not explicitly covered in neighbors; this is a specific finding of the paper regarding the limitations of inference scaling for this task structure.
- Break condition: If a reasoning model is specifically fine-tuned to generate "audit trails" that explicitly check entity states against earlier paragraphs, this mechanism might fail (i.e., the model would improve).

## Foundational Learning

- Concept: **Three-Act Structure**
  - Why needed here: The algorithm relies on partitioning stories into Setup ($A_1$), Confrontation ($A_2$), and Resolution ($A_3$) to ensure the contradiction occurs across a meaningful narrative gap.
  - Quick check question: Can you identify the specific sentence in the "Setup" of a short story that establishes a character's key trait, and a sentence in the "Resolution" that violates it?

- Concept: **Counterfactual Reasoning**
  - Why needed here: The core generation step requires predicting how the story would change if a specific fact $\phi$ were flipped to $\neg\phi$, while keeping the rest of the narrative coherent enough to be readable.
  - Quick check question: If a character is established as "blind," how would a scene where they "catch a ball" need to be rewritten to maintain logic, and what new inconsistency would that create?

- Concept: **Continuity Error vs. Factual Error**
  - Why needed here: The benchmark focuses on internal consistency (fictional world logic) rather than external truth (real-world facts), which requires tracking the specific rules established by the text $f$.
  - Quick check question: If a story set in a magical world states "dragons are invisible," is it a continuity error if a character later "sees a dragon"? (Answer: Yes, internal logic violation).

## Architecture Onboarding

- Component map: ThreeActExtract -> PropExtract -> Counterfact -> Patching -> Filtering
- Critical path: The **Patching** logic ($A_1 \cdot A^{\neg\phi}_2$). If the LLM in Step 3 is too creative and rewrites $A_1$ (which is discarded) or fails to propagate $\neg\phi$ to $A_2/A_3$, the resulting "patched" story will not contain the intended continuity error.
- Design tradeoffs: The system uses LLMs to *generate* errors (Step 3) and *filter* them (Step 5).
    - *Pro*: Creates natural, semantically complex errors that rule-based systems miss.
    - *Con*: LLMs are prone to "hallucinating" coherence, potentially creating "unbridgeable" plot holes that are too obvious, or subtle errors that are actually just reader misinterpretation. The human verification step is a required "tax" on this automation.
- Failure signatures:
    - **False Positives in Generation**: The filtering LLM flags an error, but humans reject it because the "error" is actually a genre convention (e.g., magic) or a misinterpretation of character motivation.
    - **Length Collapse**: Evaluation models defaulting to "No Error" on stories > 2000 words due to context limitations (Table 6 results).
    - **Semantic Drift**: The counterfactual story changes the plot so radically that the "patched" version becomes nonsensical rather than just inconsistent.
- First 3 experiments:
    1.  **Sanity Check**: Run the `ThreeActExtract` prompt on 10 diverse short stories to verify the LLM (GPT-4o recommended) can reliably identify $A_1$. If $A_1$ is misidentified, the whole pipeline fails.
    2.  **Single-Step Validation**: Run the `PropExtract` and `Counterfact` steps on a story where you know the ground truth fact (e.g., "The hero is mortal"). Check if the rewrite is minimal (only changes necessary for $\neg\phi$) or excessive.
    3.  **Baseline Eval**: Evaluate a cheap baseline (e.g., Llama-3.1-8B) on the *classification* task only (ignore localization). Compare the "Always No Error" rate vs. actual accuracy to establish if the model is just guessing.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the failure of inference-time scaling on plot hole detection stem from the absence of specific training data or an inherent inadequacy of current scaling methods?
- Basis in paper: [explicit] Section 5.1 explicitly asks "whether the absence of datasets similar to FLAWED FICTIONS while training reasoning models explains the limited improvements observed, or whether inference time scaling is not adequate for solving problems like plot hole detection?"
- Why unresolved: The authors observe that increased reasoning tokens degrade performance in reasoning models like o3-mini but defer the causal investigation.
- What evidence would resolve it: Training a reasoning model specifically on the FLAWED FICTIONS dataset and observing if inference-time scaling then improves performance.

### Open Question 2
- Question: Can large amounts of synthetic training data generated by FLAWED FICTIONS MAKER enhance LLMs' reasoning capabilities in broader domains?
- Basis in paper: [explicit] The Conclusion identifies it as a "promising direction" to "investigate whether using FLAWED FICTIONS MAKER to generate large amounts of synthetic training data could enhance LLMsâ€™ reasoning capabilities more broadly."
- Why unresolved: The paper utilizes the algorithm solely for evaluation and benchmark construction rather than for model training.
- What evidence would resolve it: Fine-tuning models on generated plot-hole data and evaluating transfer performance on external reasoning benchmarks (e.g., MMLU).

### Open Question 3
- Question: Can the methodology for detecting continuity errors in fiction be adapted to verify consistency in non-fictional contexts?
- Basis in paper: [explicit] The Conclusion suggests future work "apply similar approaches to non-fictional contexts like fact-checking, misinformation detection, and education."
- Why unresolved: The current work is restricted to fictional narratives and does not test the algorithm's efficacy on real-world factual inconsistencies.
- What evidence would resolve it: An adaptation of the continuity error detection pipeline applied to news articles or expository texts to measure contradiction detection accuracy.

## Limitations
- Dataset Accessibility: The FLAWED FICTIONS and FLAWED FICTIONS LONG datasets were not available at the time of writing, with exact release dates unclear.
- Model Configuration Ambiguity: Reasoning models use qualitative "low/medium/high" reasoning effort settings without specific token budgets, limiting precise reproducibility.
- Counterfactual Generation Variability: The pipeline relies on GPT-4-turbo for counterfactual generation, but the specific version and temperature settings are not specified, potentially affecting consistency.

## Confidence

- **High Confidence**: The core finding that LLMs struggle with plot hole detection across both classification and localization tasks, particularly as story length increases. The correlation between story length and performance degradation is consistently demonstrated.
- **Medium Confidence**: The claim that inference-time scaling (reasoning tokens) does not inherently resolve continuity errors. While supported by results showing o3-mini's degraded performance with increased reasoning tokens, the qualitative effort settings limit precision.
- **Medium Confidence**: The assertion that LLM-generated stories contain substantially more plot holes than human-written originals. This finding is based on comparative analysis but depends on the specific generation and filtering pipeline.

## Next Checks

1. **Sanity Check**: Run the `ThreeActExtract` prompt on 10 diverse short stories to verify the LLM (GPT-4o recommended) can reliably identify $A_1$. If $A_1$ is misidentified, the whole pipeline fails.
2. **Performance Gap Analysis**: Compare model performance on FLAWED FICTIONS vs FLAWED FICTIONS LONG to quantify the impact of story length on CEEval-Full accuracy, expecting a 30-40% drop on longer stories.
3. **Genre Convention Validation**: Test model performance on stories with fantastical elements (e.g., fairy tales) to verify it does not incorrectly flag genre conventions as continuity errors.