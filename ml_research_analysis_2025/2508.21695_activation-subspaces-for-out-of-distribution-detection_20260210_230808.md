---
ver: rpa2
title: Activation Subspaces for Out-of-Distribution Detection
arxiv_id: '2508.21695'
source_url: https://arxiv.org/abs/2508.21695
tags:
- detection
- activation
- insignificant
- directions
- decisive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of detecting out-of-distribution
  (OOD) data in deep learning models. The authors propose a novel method called ActSub
  that leverages the weight matrix of the classification head to decompose activations
  into decisive and insignificant subspaces.
---

# Activation Subspaces for Out-of-Distribution Detection

## Quick Facts
- arXiv ID: 2508.21695
- Source URL: https://arxiv.org/abs/2508.21695
- Authors: Barış Zöngür; Robin Hesse; Stefan Roth
- Reference count: 40
- Primary result: Novel ActSub method achieves SOTA OOD detection performance using subspace decomposition of activations

## Executive Summary
This paper addresses the challenge of detecting out-of-distribution (OOD) data in deep learning models. The authors propose a novel method called ActSub that leverages the weight matrix of the classification head to decompose activations into decisive and insignificant subspaces. The decisive subspace contributes significantly to the final classifier output, while the insignificant subspace has minimal effect on the model's output but encodes input statistics useful for OOD detection. The method combines two score functions: one based on the cosine similarity of the insignificant component and another based on the energy of the shaped decisive component.

## Method Summary
ActSub decomposes the classification head's weight matrix using SVD into two orthogonal subspaces. The decisive subspace (containing the top singular vectors) contributes most to the classification output, while the insignificant subspace (containing the bottom singular vectors) has minimal effect on classification but captures input statistics useful for OOD detection. The method computes two scores: (1) a cosine similarity-based score using the insignificant component against stored ID activations, and (2) an energy-based score using the decisive component after applying activation shaping (specifically SCALE). These scores are combined multiplicatively with a tunable parameter λ. The optimal subspace split is determined by balancing the norms of the two components on ID data.

## Key Results
- Achieves state-of-the-art OOD detection performance on ImageNet-1k, CIFAR, and OpenOOD benchmarks
- Outperforms established baselines including MSP, Energy, and Gram matrices in both near-OOD and far-OOD detection tasks
- Demonstrates consistent improvement across various pre-trained architectures (ResNet-50, MobileNetV2, DenseNet-101)

## Why This Works (Mechanism)
The method works by exploiting the observation that while the classification head weight matrix determines the decisive subspace for classification, the complementary insignificant subspace captures input-specific features that differ between ID and OOD data. By decomposing activations using SVD of the classification weights, ActSub isolates components that are crucial for classification from those that encode subtle input statistics. The cosine similarity in the insignificant subspace detects OOD samples by their deviation from ID activation patterns, while the energy in the shaped decisive component captures confidence in classification decisions.

## Foundational Learning
- **SVD decomposition of weight matrices**: Needed to identify orthogonal subspaces with different functional roles. Quick check: Verify that W = UΣV^T reconstructs the original weight matrix.
- **Activation subspace projection**: Required to separate decisive from insignificant components. Quick check: Confirm that projecting onto top k singular vectors yields higher classification scores than bottom k vectors.
- **Cosine similarity for OOD detection**: Uses angular distance in activation space to measure distributional differences. Quick check: Verify that ID samples cluster more tightly than OOD samples in the insignificant subspace.
- **Energy-based scoring**: Measures the magnitude of decisive components after activation shaping. Quick check: Confirm that high-energy activations correlate with in-distribution confidence.
- **Activation shaping (SCALE)**: Modifies decisive components to enhance OOD detection signal. Quick check: Verify that pruning and scaling improves separation between ID and OOD distributions.

## Architecture Onboarding

Component Map: Pre-trained model -> SVD decomposition -> Subspace projection -> Dual scoring (cosine similarity + energy) -> Score combination

Critical Path: Load pre-trained model → Extract classification weights → Perform SVD → Determine optimal k → Project activations → Compute dual scores → Combine scores

Design Tradeoffs:
- Memory vs performance: Storing activation bank enables cosine similarity but requires additional memory
- Sensitivity to k: Optimal subspace dimension varies with dataset and architecture
- Hyperparameter tuning: λ and pruning percentage p require validation

Failure Signatures:
- Suboptimal k selection leads to poor separation between ID and OOD distributions
- Incorrect SCALE parameters reduce the effectiveness of the decisive component
- Memory constraints may prevent storing sufficient activation bank for accurate cosine similarity

First Experiments:
1. Verify SVD decomposition correctly splits weight matrix into orthogonal subspaces
2. Test cosine similarity score alone on a small dataset to confirm OOD detection capability
3. Evaluate energy score with different SCALE parameters to find optimal configuration

## Open Questions the Paper Calls Out
None

## Limitations
- Requires storing activations from a subset of training data, potentially limiting scalability for very large datasets
- Performance depends on careful hyperparameter tuning (k, λ, pruning percentage p)
- Method assumes access to pre-trained models with well-defined classification heads

## Confidence
- **High Confidence**: Core mathematical framework for subspace decomposition via SVD and dual-scoring methodology are well-defined and reproducible
- **Medium Confidence**: Experimental results show strong performance across multiple benchmarks, though some variance in hyperparameter choices may affect reproducibility
- **Medium Confidence**: Claim of "state-of-the-art" performance is supported by comparisons to established baselines, though direct comparison methodology details are somewhat limited

## Next Checks
1. **Hyperparameter Sensitivity**: Systematically test different pruning percentages p (e.g., 0.1, 0.3, 0.5) and neighbor counts N (e.g., 1, 10, 100) on a validation split to verify robustness of performance claims
2. **Memory Scaling Analysis**: Evaluate the memory requirements of storing activation banks across different ID dataset sizes (e.g., CIFAR-10 vs ImageNet-1k) to assess practical scalability
3. **Cross-Dataset Generalization**: Apply the trained ActSub detector from one ID dataset (e.g., CIFAR-10) to OOD detection on a different ID dataset (e.g., ImageNet-1k) to test the method's generalization beyond the training distribution