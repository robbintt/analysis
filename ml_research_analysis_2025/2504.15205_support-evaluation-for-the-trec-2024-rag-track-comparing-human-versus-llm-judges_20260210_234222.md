---
ver: rpa2
title: 'Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus LLM
  Judges'
arxiv_id: '2504.15205'
source_url: https://arxiv.org/abs/2504.15205
tags:
- support
- human
- judge
- gpt-4o
- manual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates whether large language models (LLMs) can
  reliably replace human judges for assessing the "support" of RAG-generated answers,
  i.e., whether cited documents factually back the answer sentences. The authors compare
  GPT-4o judgments against human annotators under two conditions: manual assessments
  from scratch and manual assessments with post-editing of GPT-4o predictions.'
---

# Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus LLM Judges

## Quick Facts
- arXiv ID: 2504.15205
- Source URL: https://arxiv.org/abs/2504.15205
- Reference count: 39
- Human vs. LLM support assessment for RAG answers shows 56% agreement from scratch, 72% with post-editing, with run-level Kendall's τ > 0.79

## Executive Summary
This paper evaluates whether large language models (LLMs) can reliably replace human judges for assessing the "support" of RAG-generated answers—whether cited documents factually back the answer sentences. The authors compare GPT-4o judgments against human annotators under two conditions: manual assessments from scratch and manual assessments with post-editing of GPT-4o predictions. Across 45 participant submissions on 36 topics, GPT-4o and human judges perfectly agreed 56% of the time in the from-scratch condition, increasing to 72% with post-editing. Run-level weighted precision and recall scores were highly correlated (Kendall's τ > 0.79) between human and GPT-4o judgments. A further disagreement study with an independent human judge showed stronger alignment with GPT-4o than with the original human judge, indicating that LLMs can be a reliable alternative for support assessment. Qualitative analysis identified specific error patterns in both human and LLM judgments, offering insights for improving future support evaluation.

## Method Summary
The study compares GPT-4o LLM judgments against human annotators for evaluating whether cited passages support RAG-generated answer sentences. The evaluation uses a three-level support scale: Full Support (FS), Partial Support (PS), and No Support (NS), with weighted precision and recall metrics. The research employs sparse annotations, evaluating only the first cited passage per sentence due to budget constraints. Human annotations were collected under two conditions: from-scratch assessment (6,742 annotations on 22 topics) and post-editing where GPT-4o predictions were visible (4,165 annotations on 14 topics). GPT-4o was prompted via Azure API to classify each sentence-passage pair into one of the three support levels.

## Key Results
- GPT-4o and human judges perfectly agreed 56% of the time in the from-scratch condition, increasing to 72% with post-editing
- Run-level weighted precision and recall scores showed high correlation (Kendall's τ > 0.79) between human and GPT-4o judgments
- An independent human judge aligned more closely with GPT-4o (κ = 0.27) than with the original human judge (κ = 0.07)

## Why This Works (Mechanism)

### Mechanism 1: Run-Level Agreement Despite Instance-Level Disagreement
- LLM judges can reliably rank RAG systems even when instance-level judgments frequently disagree with humans
- Aggregation at the run-level smooths out individual annotation differences
- Systematic errors that affect all runs similarly cancel out, preserving relative ordering
- The paper shows Kendall's τ of 0.884–0.892 for run-level precision/recall in from-scratch conditions

### Mechanism 2: Post-Editing Anchors Human Judgment to LLM Predictions
- Showing LLM predictions to human annotators increases agreement from 56% to 72%
- The post-editing condition provides a default label that humans accept unless they spot obvious errors
- This reduces cognitive load and partially calibrates human judges toward LLM tendencies

### Mechanism 3: Label Distribution Divergence Reflects Different Decision Boundaries
- LLMs systematically prefer "partial support" while humans prefer "no support" for ambiguous cases
- GPT-4o assigns partial support when thematic similarity exists between passage and sentence
- This creates asymmetric disagreements concentrated in the middle of the support spectrum

## Foundational Learning

### Concept 1: Three-Level Support Grading Scheme
- Why needed: Understanding Full Support (FS), Partial Support (PS), and No Support (NS) with their definitions is essential to interpreting all results and failure modes
- Quick check: Given an answer sentence "Taylor Swift dated John Mayer in 2010" and a passage that only mentions "Swift had a high-profile romance in 2010" without naming Mayer, what support level applies?

### Concept 2: Weighted Precision vs. Recall for Support
- Why needed: The paper's metric design differently penalizes overcitation (precision) vs. undercitation (recall)
- Quick check: If a RAG system cites 5 passages per sentence but only 1 provides any support, which metric suffers more—precision or recall?

### Concept 3: Agreement Metrics (Cohen's κ vs. Kendall's τ)
- Why needed: The paper uses κ for inter-annotator agreement on individual labels and τ for rank correlation at run-level aggregation
- Quick check: Why would the independent judge achieve κ of 0.27 with GPT-4o but only 0.07 with the original human judge, yet both show similar overall patterns?

## Architecture Onboarding

### Component Map:
- Input Processing: Answer sentences + first cited passage per sentence
- LLM Judge (GPT-4o): Prompt-based classification into FS/PS/NS
- Human Judge Interface: Two modes—(1) from-scratch assessment, (2) post-editing with GPT-4o label visible
- Metric Computation: Weighted precision = sum(weights)/citation_count; weighted recall = sum(weights)/sentence_count

### Critical Path:
1. Receive RAG output: segmented answer sentences with citations to MS MARCO V2.1 passages
2. For each sentence–first-citation pair, apply judge (human or LLM) → FS/PS/NS
3. Map to weights: FS=1.0, PS=0.5, NS=0
4. Compute sentence-level metrics, aggregate to run-level by averaging across topics

### Design Tradeoffs:
- **Sparse vs. Dense**: Paper chose sparse (first citation only) for topic diversity (36 topics) over comprehensive per-sentence evaluation
- **Single vs. Multi-passage Prompting**: Single-passage prompting "anecdotally" performs better
- **From-scratch vs. Post-editing**: Post-editing saves annotation time but risks anchoring bias

### Failure Signatures:
- **GPT-4o errors**: Confuses similar words ("police" vs. "security specialists"), misses end-of-sentence information, over-assigns PS for thematic similarity without textual support
- **Human errors**: Miss information in middle/end of passages, rely on background knowledge instead of passage text, occasionally mark unsupported as FS

### First 3 Experiments:
1. **Replicate label distribution analysis**: Run GPT-4o judge on 100+ sentence-passage pairs from your RAG outputs. Plot FS/PS/NS distribution—expect PS dominance (~50%) per Figure 5.
2. **Inter-annotator baseline**: Have two humans label 50–100 pairs from scratch. Measure Cohen's κ between humans and against GPT-4o to establish your baseline agreement before relying on LLM-only evaluation.
3. **Correlation preservation test**: If comparing multiple RAG configurations, compute run-level scores with both judges on the same subset. Verify Kendall's τ > 0.7 before scaling to LLM-only evaluation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can evaluation frameworks be adapted to robustly assess support for answer sentences containing multiple citations without requiring exhaustive manual annotation?
- Basis: Section 3.4 states this requires "a much larger annotation budget"
- Why unresolved: Current study used sparse annotations evaluating only first citation due to high cost
- What evidence would resolve it: A methodology for aggregating support scores across multiple citations that demonstrates high correlation with human judgments

### Open Question 2
- Question: Why do LLM judges exhibit a strong bias against assigning "No Support" labels compared to human judges?
- Basis: Section 5 notes this label distribution divergence but doesn't determine if due to model calibration or semantic interpretation
- Why unresolved: Study revealed fundamental divergence but not underlying causes
- What evidence would resolve it: A calibration analysis of LLM confidence scores on "No Support" instances or an ablation study varying instructions

### Open Question 3
- Question: To what extent do specific LLM error patterns degrade the reliability of automatic support assessment?
- Basis: Section 5 identifies qualitative error patterns but doesn't quantify frequency or propose mitigation
- Why unresolved: Authors qualitatively identify failures but don't quantify or propose fixes
- What evidence would resolve it: A targeted evaluation set showing improved performance via modified prompting strategies

## Limitations

- Sparse annotation strategy evaluates only first cited passage per sentence, potentially missing critical support relationships in secondary citations
- Single-passage prompting approach lacks systematic comparison against multi-passage prompting strategies
- The study doesn't address potential model-version drift in GPT-4o performance over time

## Confidence

- **High Confidence**: Run-level correlation results (Kendall's τ > 0.79) demonstrating that LLM judges reliably preserve system rankings
- **Medium Confidence**: Post-editing mechanism's effectiveness in improving agreement rates from 56% to 72%
- **Medium Confidence**: Label distribution asymmetry as a systematic rather than random phenomenon

## Next Checks

1. **Multi-passage prompting comparison**: Systematically evaluate whether including all cited passages in a single prompt improves GPT-4o's support assessment accuracy
2. **Temporal stability test**: Repeat the GPT-4o evaluation on a fixed subset using different versions of the model to quantify performance drift
3. **Cross-architecture validation**: Apply the same LLM judge protocol to RAG systems with different architectures to verify run-level correlation preservation holds across diverse system designs