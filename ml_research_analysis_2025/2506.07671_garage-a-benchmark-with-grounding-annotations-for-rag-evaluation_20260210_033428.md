---
ver: rpa2
title: 'GaRAGe: A Benchmark with Grounding Annotations for RAG Evaluation'
arxiv_id: '2506.07671'
source_url: https://arxiv.org/abs/2506.07671
tags:
- grounding
- question
- information
- questions
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces GaRAGe, a large-scale RAG benchmark with\
  \ 2,366 questions and over 35,000 manually annotated passages, designed to evaluate\
  \ LLMs\u2019 ability to identify and ground responses in relevant information from\
  \ both private documents and the Web. Each question includes annotations for passage\
  \ relevance, question type (time-sensitivity, complexity, popularity, domain), and\
  \ human-curated long-form answers with citations."
---

# GaRAGe: A Benchmark with Grounding Annotations for RAG Evaluation

## Quick Facts
- arXiv ID: 2506.07671
- Source URL: https://arxiv.org/abs/2506.07671
- Reference count: 40
- Primary result: Introduces GaRAGe, a large-scale RAG benchmark with 2,366 questions and 35,000+ manually annotated passages to evaluate grounding, factuality, deflection, and attribution capabilities of LLMs.

## Executive Summary
GaRAGe is a large-scale RAG benchmark with 2,366 questions and over 35,000 manually annotated passages, designed to evaluate LLMs' ability to identify and ground responses in relevant information from both private documents and the Web. Each question includes annotations for passage relevance, question type (time-sensitivity, complexity, popularity, domain), and human-curated long-form answers with citations. The authors propose new metrics including Relevance-Aware Factuality (RAF) to measure whether responses are grounded strictly in relevant passages and deflection ability when grounding is insufficient. Evaluation of state-of-the-art models shows poor performance: top models achieve at most 60% RAF, with significant drops on time-sensitive and private-domain questions. Deflection performance is also weak, with true positive rates up to 31%. The results highlight that current models tend to over-summarize and struggle with relevancy detection, especially with noisy or sparse private grounding sources.

## Method Summary
GaRAGe evaluates RAG systems through zero-shot inference on 2,366 questions using a specific prompt requiring answers be grounded only in provided search results or a deflection if insufficient. The benchmark uses Web and private knowledge bases (Enron, SEC, Arxiv, AWS DevOps) with query decomposition, cross-encoder reranking, and noise injection for grounding collection. Human annotators validate questions and label each passage as ANSWER-THE-QUESTION, RELATED-INFORMATION, OUTDATED, UNKNOWN, or IRRELEVANT. GPT-4o serves as judge for eligibility, factuality, RAF, deflection, and attribution metrics. RAF specifically measures factuality restricted to relevant passages only, while deflection is evaluated via true positive and false positive rates on 427 questions with insufficient grounding.

## Key Results
- Top models achieve at most 60% Relevance-Aware Factuality (RAF), indicating significant over-summarization beyond relevant passages
- Deflection performance remains poor with maximum true positive rates of 31%, showing models default to generation even with insufficient grounding
- Performance degrades significantly on time-sensitive questions (~10% drop) and private-domain questions (47.8% relevant grounding vs 85.6% for Web)
- RAF scores drop sharply when moving from high to medium relevance density conditions, revealing sensitivity to retrieval noise

## Why This Works (Mechanism)

### Mechanism 1
Fine-grained passage-level relevancy annotations enable detection of models that summarize indiscriminately rather than ground in verified sources. By annotating each grounding passage with labels (ANSWER-THE-QUESTION, RELATED-INFORMATION, OUTDATED, UNKNOWN, IRRELEVANT), the benchmark separates factuality (supported by context) from relevance-aware factuality (supported only by relevant context). This exposes models that achieve high unadjusted factuality by including irrelevant or outdated passages. The core assumption is that models scoring well on standard factuality but poorly on relevance-aware factuality are over-summarizing rather than filtering for relevance.

### Mechanism 2
Deflection performance reveals models' inability to recognize insufficient grounding, a critical failure mode for real-world RAG reliability. The benchmark includes 427 questions with grounding annotated as insufficient for answering. True positive deflection (correctly refusing to answer) requires models to assess collective passage adequacy, not just individual relevance. Low true positive rates (max 31.1%) indicate models default to generation even when evidence is missing. The core assumption is that deflection is an explicit capability requiring calibration, not an emergent behavior from standard instruction-following training.

### Mechanism 3
Performance degradation on time-sensitive and private-domain questions stems from compounding retrieval noise with domain sparsity. Fast-changing questions have noisier grounding (more OUTDATED passages). Private-domain questions (Enron, SEC filings) have lower relevant-passage density (47.8% vs 85.6% for Web). Models trained primarily on general Web data lack robustness to sparse, domain-specific signal distributions. The core assumption is that the performance gap reflects both retrieval quality and models' limited capacity to reason over temporal metadata and domain-specific patterns.

## Foundational Learning

- Concept: **Factuality vs. Relevance-Aware Factuality**
  - Why needed here: Standard factuality measures whether claims are supported by any context; RAF measures support only from relevant passages. This distinction is central to GaRAGe's contribution.
  - Quick check question: If a model cites an outdated passage that technically supports its claim but is no longer accurate, does it pass factuality? Does it pass RAF?

- Concept: **Grounding Annotations at Passage Level**
  - Why needed here: GaRAGe's key innovation is annotating individual passages for relevancy, not just document-level labels. This enables fine-grained attribution evaluation.
  - Quick check question: What is the difference between annotating an entire retrieved set as "contains answer" vs. labeling each passage as ANSWER-THE-QUESTION, RELATED-INFORMATION, OUTDATED, or IRRELEVANT?

- Concept: **Deflection as a Measured Capability**
  - Why needed here: Real-world RAG systems must know when to refuse. GaRAGe treats deflection as a first-class metric with true positive (correct refusal) and false positive (unnecessary refusal) rates.
  - Quick check question: Why is a low false positive deflection rate (correctly answering when possible) insufficient if the true positive rate is also low?

## Architecture Onboarding

- Component map: Question Construction -> Grounding Collection -> Human Annotation -> Evaluation Metrics
- Critical path: Question generation must pass validity/ambiguity filters before grounding collection. Retrieval results feed into human annotation; annotators label each passage for relevancy. Annotated passages determine gold-standard answers (including deflection labels). Model outputs are evaluated against all metrics using GPT-4o as judge.
- Design tradeoffs: Human annotation cost ($9.65/sample) vs. synthetic annotation reliability; long-form answers (avg 192 words) vs. easier-to-evaluate short answers; mixed Web + private grounding reflects real-world RAG but complicates retrieval consistency; GPT-4o as sole judge enables scalable evaluation but introduces model-specific bias.
- Failure signatures: High unadjusted factuality + low RAF → model summarizes indiscriminately without relevance filtering; low deflection TP + low deflection FP → model rarely refuses even when appropriate (hallucination risk); sharp performance drop from "high" to "medium" grounding quality → model lacks robustness to noise; larger performance gap on private vs. Web questions → model over-relies on parametric knowledge from Web-pretraining distributions.
- First 3 experiments: Baseline RAF comparison - Run your RAG pipeline on GaRAGe with standard prompt; compare unadjusted factuality vs. RAF to quantify over-summarization gap. Deflection calibration - Evaluate true positive deflection rate on the 427 insufficient-grounding questions; if <20%, add explicit refusal training examples and re-measure. Grounding quality sensitivity - Stratify evaluation by passage relevance density (low/medium/high per Figure 6); identify the noise threshold where your model's RAF drops >15% from high to medium conditions.

## Open Questions the Paper Calls Out
- How does RAG performance vary in multilingual settings compared to the English-only results observed in GaRAGe? The authors acknowledge the benchmark contains only English datapoints and plan to explore multilingual settings in the future.
- To what extent does using GPT-4o as the sole evaluator bias the performance metrics for other models? The authors note in limitations that results might be biased towards GPT models.
- What specific training mechanisms can effectively bridge the performance gap in deflection tasks (currently ~31% TPR) without increasing false positives? The paper highlights a significant gap where models struggle to deflect when grounding is insufficient.

## Limitations
- Benchmark relies on GPT-4o as sole evaluation judge, introducing potential model-specific bias toward GPT models
- Dataset focuses exclusively on English-language queries from specific domains (Enron, SEC, Arxiv, AWS DevOps), limiting generalizability
- Manual annotation cost of $9.65 per sample makes dataset expansion expensive, potentially limiting benchmark growth and diversity

## Confidence
- **High confidence**: The RAF metric design and its distinction from standard factuality; the core finding that models achieve at most 60% RAF; the identification of over-summarization as a key failure mode.
- **Medium confidence**: The attribution of performance degradation specifically to retrieval noise vs. model limitations; the claim that deflection is an "explicit capability" rather than emergent behavior.
- **Low confidence**: The generalizability of findings beyond the specific domains tested; the extent to which GPT-4o judge bias affects metric reliability.

## Next Checks
1. Judge bias validation: Run the same evaluation pipeline using a different LLM judge (e.g., Claude 3.5 or Gemini 1.5) on a random 10% sample of GaRAGe questions and compare RAF and deflection scores across judges.

2. Domain generalization test: Apply the benchmark to a non-Web dataset (e.g., medical literature or legal documents) and measure whether the RAF gap between high and low relevance density conditions persists, isolating retrieval vs. model effects.

3. Deflection capability isolation: Train a baseline model with explicit refusal examples matched to insufficient grounding scenarios and re-evaluate deflection TPR to determine whether the 31% ceiling reflects architectural limitations or training gaps.