---
ver: rpa2
title: How can representation dimension dominate structurally pruned LLMs?
arxiv_id: '2503.04377'
source_url: https://arxiv.org/abs/2503.04377
tags:
- dimension
- attention
- representation
- pruning
- sparsity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how representation dimension influences
  the performance of structurally pruned large language models (LLMs). The authors
  focus on the linear transformations within LLM transformer blocks and analyze how
  pruning approaches, specifically SliceGPT, that reduce representation dimensions
  affect model functionality.
---

# How can representation dimension dominate structurally pruned LLMs?

## Quick Facts
- arXiv ID: 2503.04377
- Source URL: https://arxiv.org/abs/2503.04377
- Authors: Mingxue Xu; Lisa Alazraki; Danilo P. Mandic
- Reference count: 7
- Primary result: Representation dimension reduction through SliceGPT pruning predicts perplexity and accuracy changes via analytical relations

## Executive Summary
This paper investigates how representation dimension influences the performance of structurally pruned large language models (LLMs). The authors focus on the linear transformations within LLM transformer blocks and analyze how pruning approaches, specifically SliceGPT, that reduce representation dimensions affect model functionality. Through mechanistic analysis of activation flow during model forward passes, they demonstrate that representation dimension dominates both linear transformations and model predictions, ultimately determining model performance.

The authors derive explicit analytical relations to estimate pruned model performance metrics—perplexity and multiple-choice accuracy—based solely on representation dimension sparsity. These relations are validated empirically using Llama-3-8B-Instruct and Phi-3-mini-4k-Instruct models. For perplexity, they show that ln PPL0(D)/ln PPL(D) = 1-s, where s is the sparsity. For accuracy, they demonstrate that ln acc(D)/acc0(D) ∝ 1-s. The consistency between these relations for different evaluation metrics provides empirical support for their theoretical framework.

## Method Summary
The authors use SliceGPT, a structured pruning method that reduces representation dimensions by removing entire rows and columns from weight matrices, to prune Llama-3-8B-Instruct and Phi-3-mini-4k-Instruct models at varying sparsity levels. They evaluate the pruned models on WikiText2 for perplexity and multiple-choice benchmarks (ARC-e, ARC-c, WinoGrande, PIQA) for accuracy. The method involves applying SliceGPT to create pruned models, then measuring performance metrics to validate the derived analytical relations between sparsity and performance. The theoretical framework connects entropy scaling with dimension reduction to explain how representation dimension affects both perplexity and accuracy.

## Key Results
- Representation dimension d dominates information capacity in all linear transformations throughout LLM transformer blocks
- Perplexity follows the relation ln PPL0(D)/ln PPL(D) = 1-s, where s is sparsity
- Multiple-choice accuracy follows ln acc(D)/acc0(D) ∝ 1-s, consistent with the inverse perplexity-accuracy relationship
- The analytical relations are empirically validated on Llama-3-8B-Instruct and Phi-3-mini-4k-Instruct models

## Why This Works (Mechanism)

### Mechanism 1: Representation Dimension Dominance in Linear Transformations
- **Claim:** The representation dimension d determines the information capacity flowing through all linear transformations in transformer blocks.
- **Mechanism:** In LLM transformers, most computations are linear transformations defined by weight matrices (Wnorm, Wq, Wk, Wv, Wo, Wgate, Wup, Wdown). These matrices either map from dimension d or to dimension d. The final hidden state output has shape R^(h×l×d), making d the bottleneck dimension that constrains all transformations. When pruning reduces d to (1-s)d, it uniformly constrains the capacity of every linear operation.
- **Core assumption:** Linear transformations are the primary information carriers; non-linear operations (softmax, RMSNorm, σ) are dimension-preserving or secondary in their impact on model capacity.
- **Evidence anchors:** [abstract], [section 2, page 2], [corpus]
- **Break condition:** Mechanism may not hold if non-linear transformations become dominant or if architecture uses fundamentally different information routing.

### Mechanism 2: Entropy-Perplexity Scaling Under Dimension Reduction
- **Claim:** Perplexity changes predictably with sparsity s according to ln PPL0(D)/ln PPL(D) = 1-s.
- **Mechanism:** The entropy of output embeddings H(Eout) = ld·κ(t) scales linearly with dimension d. After pruning, entropy shifts to (1-s)ld·κ(t), yielding H(E'out)/H(Eout) = 1-s. Since PPL(E) = 2^H(E), the logarithmic relationship follows directly.
- **Core assumption:** The unit entropy κ(t) remains approximately constant before and after pruning; information capacity scales linearly with dimension.
- **Evidence anchors:** [abstract], [section 3.1, page 3], [corpus]
- **Break condition:** Breaks if pruning is non-uniform across dimensions, if κ(t) shifts due to data distribution changes, or if calibration and evaluation datasets differ significantly.

### Mechanism 3: Accuracy-Dimension Proportionality
- **Claim:** Multiple-choice accuracy follows ln acc(D)/acc0(D) ∝ 1-s.
- **Mechanism:** Accuracy and perplexity are inversely related for well-generalized models (high accuracy correlates with low perplexity). Since both metrics share nearly identical ranges (1/PPL ∈ (0,1] and acc ∈ [0,1]), the logarithmic form mirrors the perplexity relationship with inverted scaling.
- **Core assumption:** Multiple-choice tasks provide unambiguous exact-match accuracy; the inverse perplexity-accuracy correlation holds consistently across sparsity levels.
- **Evidence anchors:** [abstract], [section 3.2, page 3-4], [corpus]
- **Break condition:** May not hold for generative tasks without clear exact-match criteria, or when pruning differentially affects specific token types or reasoning paths.

## Foundational Learning

- **Concept: Representation dimension (embedding/model dimension)**
  - Why needed: This is the central variable in the paper's analysis. Understanding that d controls information capacity in every weight matrix is essential for grasping why dimension reduction systematically affects performance.
  - Quick check question: In a transformer, why does the representation dimension appear in every weight matrix shape?

- **Concept: Entropy-perplexity relationship (PPL = 2^H)**
  - Why needed: The paper's perplexity-sparsity formula derives directly from how entropy scales with dimension. Without understanding PPL(E) = 2^H(E), the derivation remains opaque.
  - Quick check question: If entropy doubles, what happens to perplexity?

- **Concept: Structured vs. unstructured pruning**
  - Why needed: SliceGPT performs structured pruning by removing entire rows/columns to reduce dimension d uniformly. This differs from unstructured weight-level pruning and explains why effects are predictable and systematic.
  - Quick check question: Why might structured pruning produce more predictable performance changes than unstructured pruning?

## Architecture Onboarding

- **Component map:**
  Input embedding: Ein ∈ R^(l×d) where l = sequence length
  RMSNorm layers: Wnorm ∈ R^d (learnable normalization weights)
  Attention block: Wq, Wk, Wv ∈ R^(d×h_attn×h_dim), Wo ∈ R^(h_attn×h_dim×d)
  MLP block: Wgate, Wup ∈ R^(d×m), Wdown ∈ R^(m×d) where m = intermediate size
  Output: ⊕h_i=1 Ai ∈ R^(h×l×d) with h_attn×h_dim = d in default setting
  SliceGPT: Removes rows/columns to reduce d → (1-s)d across all matrices

- **Critical path:**
  1. Ein → RMSNorm → attention (Q,K,V projections → softmax → Wo) → RMSNorm → MLP (gate × up → down) → residual addition
  2. All weight matrices either project from d or to d
  3. SliceGPT applies uniform dimension reduction at each layer
  4. Reduced d constrains information flow throughout entire forward pass

- **Design tradeoffs:**
  - Higher sparsity s: Faster inference, lower memory, but exponentially rising perplexity and linearly declining accuracy
  - The analytical formulas enable prediction without running full evaluations
  - Uniform dimension reduction (SliceGPT) vs. layer-specific reduction (unexplored in this paper)
  - The paper notes potential safety issues: model collapse and unknown backdoor features at high sparsity

- **Failure signatures:**
  - Perplexity rising faster than predicted: Check for data distribution mismatch between calibration and evaluation
  - Accuracy dropping more sharply than predicted: May indicate task-specific sensitivity to certain dimensions
  - Model collapse at extreme sparsity (s > 0.5): Consistent with paper's warning about functionality shifting

- **First 3 experiments:**
  1. **Baseline validation:** Apply SliceGPT to Llama-3-8B-Instruct at sparsities [0.1, 0.2, 0.3, 0.4], measure WikiText2 perplexity, and verify ln PPL0/ln PPL ≈ 1-s with slope ~-1 and intercept ~1
  2. **Cross-architecture generalization:** Apply identical sparsity levels to a different architecture (e.g., Mistral-7B) and test whether the fitted coefficients (a, b in y = as + b) differ significantly from Table 3 values
  3. **Task transfer test:** Evaluate the accuracy formula on a multiple-choice benchmark not used in the paper (e.g., MMLU subsets) to validate whether ln acc/acc0 ∝ 1-s generalizes across task types

## Open Questions the Paper Calls Out
- Do the analytical relations between sparsity and performance hold for structured pruning methods other than SliceGPT?
- Does the representation dimension dominance persist in open-ended generative tasks rather than just multiple-choice accuracy?
- How does the reduction in representation dimension specifically affect safety features, such as backdoor persistence or robustness to model collapse?

## Limitations
- Analytical relations are validated only for SliceGPT pruning on Llama-3-8B-Instruct and Phi-3-mini-4k-Instruct models
- Performance evaluation limited to WikiText2 and short multiple-choice tasks, not generative tasks
- Assumption that unit entropy κ(t) remains constant across pruning levels may not hold for extreme sparsity or domain-shifted data

## Confidence
- Representation dimension dominance in linear transformations: Medium - Mechanistically sound but relies on uniform SliceGPT application
- Entropy-perplexity scaling relation: High - Direct mathematical derivation with empirical validation
- Accuracy-dimension proportionality: Medium - Consistent with inverse perplexity-accuracy relationship but limited task scope

## Next Checks
1. Test whether the analytical relations hold when pruning is applied to different model families (e.g., Mistral, Qwen) with varying architectural choices
2. Evaluate performance on generative tasks beyond multiple-choice to assess accuracy formula generalization
3. Investigate the safety claims by systematically probing pruned models for model collapse and backdoor features at different sparsity levels