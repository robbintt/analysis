---
ver: rpa2
title: Does Less Hallucination Mean Less Creativity? An Empirical Investigation in
  LLMs
arxiv_id: '2512.11509'
source_url: https://arxiv.org/abs/2512.11509
tags:
- creativity
- divergent
- layers
- dola
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how hallucination-reduction techniques\
  \ affect creative generation in large language models (LLMs). The authors evaluate\
  \ three methods\u2014Chain of Verification (CoVe), Decoding by Contrasting Layers\
  \ (DoLa), and Retrieval-Augmented Generation (RAG)\u2014on two creativity benchmarks\
  \ (NeoCoder and CS4) across multiple model families (LLaMA, Qwen, Mistral) and scales\
  \ (1B\u201370B parameters)."
---

# Does Less Hallucination Mean Less Creativity? An Empirical Investigation in LLMs

## Quick Facts
- arXiv ID: 2512.11509
- Source URL: https://arxiv.org/abs/2512.11509
- Reference count: 7
- Primary result: Hallucination-reduction methods have opposing effects on divergent creativity while preserving convergent creativity

## Executive Summary
This paper investigates how three hallucination-reduction techniques (Chain of Verification, Decoding by Contrasting Layers, and Retrieval-Augmented Generation) affect creative generation in large language models. The authors evaluate these methods across multiple model families and scales on two creativity benchmarks, finding that CoVe enhances divergent creativity while DoLa suppresses it, with RAG showing minimal impact. They further investigate why DoLa reduces creativity using linear probes to identify layer correlations with creative output, demonstrating that amplifying creativity-correlated layers can boost divergent creativity without harming convergent performance.

## Method Summary
The authors evaluate three hallucination-reduction methods (CoVe, DoLa, RAG) on LLaMA, Qwen, and Mistral models across 1B-70B parameter scales. They measure convergent and divergent creativity using NeoCoder and CS4 benchmarks. To understand why DoLa suppresses creativity, they train linear probes on model activations to identify layer-level correlations with creative output. They then develop a method to amplify creativity-correlated layers while suppressing anti-correlated ones to boost divergent creativity.

## Key Results
- CoVe enhances divergent creativity across most models and datasets, with improvements up to 12.5% on NeoCoder
- DoLa suppresses divergent creativity across all models, reducing performance by up to 8% on CS4
- RAG shows minimal impact on creativity, likely due to retrieval quality issues and semantic mismatch
- Early transformer layers show stronger correlations with creativity than later layers
- Amplifying creativity-correlated layers while suppressing anti-correlated ones can boost divergent creativity without harming convergent performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Chain of Verification (CoVe) enhances divergent creativity.
- **Mechanism:** The iterative verification process functions analogously to human brainstorming, encouraging broader exploration of the solution space and avoiding premature commitment to a single response path.
- **Core assumption:** The act of generating and answering verification questions prompts the model to explore alternative reasoning paths and solutions.
- **Evidence anchors:** [abstract]: "CoVe enhances divergent thinking... DoLa suppresses it." [section 3, "Results" subsection "CoVe increases divergent creativity"]: "Our observation supports the hypothesis that questioning improves creativity... by encouraging a broader exploration of the solution space in the model."
- **Break condition:** If verification questions do not encourage exploration (e.g., they are too narrow or force premature closure), the effect may diminish.

### Mechanism 2
- **Claim:** Decoding by Contrasting Layers (DoLa) suppresses divergent creativity by contrasting against creativity-correlated early layers.
- **Mechanism:** DoLa improves factuality by contrasting the output of a later layer against an earlier layer. Early layers show stronger correlations with creativity, so the contrastive operation inadvertently suppresses the features necessary for creative generation.
- **Core assumption:** Creative representations are more strongly encoded in early transformer layers than in later ones.
- **Evidence anchors:** [abstract]: "DoLa suppresses divergent creativity across all models." [section 3, "Further Studies on DoLa"]: "This finding supports our hypothesis that DoLa's contrastive decoding mechanism... inadvertently suppresses divergent creativity by removing the very layer activations responsible for creative generation."
- **Break condition:** This mechanism holds if the layer selected for contrast is one of the creativity-correlated early layers, which the paper's finding suggests is generally the case.

### Mechanism 3
- **Claim:** Retrieval-Augmented Generation (RAG) has minimal impact on creativity due to retrieval quality issues (semantic mismatch).
- **Mechanism:** RAG's effect is neutral because retrieved documents (coding tutorials, documentation) lack the specific algorithmic insights needed for narrative-style competitive programming problems, introducing noise but not providing genuinely new and relevant information.
- **Core assumption:** The neutral effect is primarily due to the mismatch between query format (narrative) and retrieval corpus (technical documentation), rather than RAG being inherently neutral for creativity.
- **Evidence anchors:** [section 3, "Results" subsection "RAG has no effect on divergent creativity"]: "This neutral effect could stem from retrieval quality issues... semantic mismatch between anecdotal problem descriptions and tutorial-style documentation results in retrieved documents that lack the specific algorithmic insights needed..."
- **Break condition:** This finding may not generalize to tasks where the retrieval corpus is well-aligned with the query type.

## Foundational Learning

- **Concept:** Divergent vs. Convergent Creativity
  - **Why needed here:** This is the core framework used to evaluate the models' creative performance, with the key finding being opposing effects on divergent creativity while preserving convergent creativity.
  - **Quick check question:** According to the paper's adopted definition from human psychology, which type of creativity involves generating different ideas, and which involves solving a problem correctly and within means?

- **Concept:** DoLa (Decoding by Contrasting Layers)
  - **Why needed here:** Understanding DoLa's standard operation is essential to grasp why the authors hypothesized it might affect creativity, as it works by subtracting the logits of a dynamically chosen earlier layer from a later layer to improve factuality.
  - **Quick check question:** What is the core mechanism DoLa uses to make a large language model more factual?

- **Concept:** Linear Probes
  - **Why needed here:** The authors use linear probes to provide evidence for their hypothesis about why DoLa reduces creativity, making it key to following their causal argument.
  - **Quick check question:** In this paper, what were the linear probes trained to predict, and what layer-level pattern did they discover that supports the explanation for DoLa's effect on creativity?

## Architecture Onboarding

- **Component map:** Base LLM -> Hallucination-Reduction Layer (CoVe, DoLa, RAG) -> Creativity Benchmarks (NeoCoder, CS4) -> Evaluation Metrics (Convergent/Divergent creativity scores)
- **Critical path:** A new engineer should focus on understanding the standard implementation of the three hallucination-reduction methods. The most critical path for extending this work is the "Further Studies on DoLa" pipeline: Training linear probes on model activations -> Identifying creativity-correlated layers -> Modifying the decoding process.
- **Design tradeoffs:** The paper reveals a tradeoff between factual accuracy (hallucination reduction) and creative exploration (divergent creativity). The choice of method (e.g., CoVe for more creativity, DoLa for more factuality) depends on the task's priorities. There is also a tradeoff in the probe-based creativity enhancement method: it is model-specific, meaning probes trained on one model's activations do not transfer well to others.
- **Failure signatures:**
    - **RAG for NeoCoder**: Expect minimal creative impact due to the noted semantic mismatch between narrative problems and technical documentation.
    - **DoLa for creative tasks**: Expect a consistent decrease in divergent creativity metrics.
    - **Cross-model probe application**: Expect poor results if linear probes trained on one model are applied to another.
- **First 3 experiments:**
  1. **Reproduce Baseline Creativity**: Run the NeoCoder and CS4 benchmarks on a base LLaMA or Mistral model without any hallucination-reduction techniques to establish baseline convergent and divergent creativity scores.
  2. **Implement and Test CoVe and DoLa**: Implement the standard CoVe and DoLa methods. Run the same benchmarks and confirm the paper's findings: CoVe should increase divergent creativity, and DoLa should decrease it, while both leave convergent creativity relatively unchanged.
  3. **Train and Apply Creativity Probes**: Implement the linear probe training pipeline described in the "Further Studies on DoLa" section. Train probes on the base model's activations from NeoCoder, identify the creativity-correlated/anti-correlated layers, and then modify the decoding process to amplify/suppress these layers. Verify if this boosts divergent creativity without harming convergent performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific mechanisms by which Chain of Verification (CoVe) enhances divergent creativity, and which stages of the verification process are responsible for this improvement?
- Basis in paper: [explicit] The authors state in the Limitations section that they "did not conduct ablation studies to isolate the mechanism responsible for this improvement" and that "systematic investigation of the specific mechanism on why questioning increases divergent creativity remains an important direction for future work."
- Why unresolved: While the paper demonstrates that CoVe improves divergent creativity, it does not determine if the benefit stems from the generation of verification questions, the multi-stage reasoning structure, or the revision process itself.
- What evidence would resolve it: Ablation studies that isolate individual components of the CoVe pipeline (e.g., removing the verification questioning step vs. the final revision step) to observe the impact on divergent creativity scores.

### Open Question 2
- Question: Can Retrieval-Augmented Generation (RAG) be optimized to enhance divergent creativity, or does external grounding inherently limit creative exploration?
- Basis in paper: [explicit] The authors report that RAG showed "minimal impact," which they attribute to "potential retrieval quality issues" and "semantic mismatch" between the problems and the corpus. They explicitly state they "did not systematically measure retrieval quality or explore alternative retrieval strategies."
- Why unresolved: It is unclear if the neutral effect is due to poor retrieval (noise from irrelevant tutorials) or if the act of grounding inherently constrains the model's ability to make the "unconventional connections" required for creativity.
- What evidence would resolve it: Experiments utilizing high-quality, domain-specific retrieval corpora aligned with the creative task, along with quantitative measurements of retrieval relevance, to test if improved grounding boosts or stifles divergent thinking.

### Open Question 3
- Question: Do the observed trade-offs between hallucination reduction and creativity persist in authentic scientific hypothesis generation contexts?
- Basis in paper: [explicit] The authors note in the Limitations that their benchmarks (programming and story generation) are "only a proxy of actual scientific hypothesis generation" and call for "future work" to determine if these relationships hold in "authentic scientific discovery contexts."
- Why unresolved: Scientific creativity involves a distinct balance of factual constraint and novel ideation that may differ from the logic-heavy constraints of NeoCoder or the stylistic constraints of CS4.
- What evidence would resolve it: The development and application of new benchmarks specifically designed to measure scientific hypothesis generation, evaluating whether CoVe, DoLa, and RAG exhibit the same divergent/convergent creativity dynamics found in this study.

## Limitations

- The paper does not conduct ablation studies to isolate whether the creativity effects are specifically due to the verification questions in CoVe or other components of the method.
- The retrieval quality assessment for RAG is qualitative rather than quantitative, making it difficult to verify whether semantic mismatch is the true cause of the neutral effect.
- The probe-based creativity enhancement method is model-specific, with probes trained on one model showing poor transfer to others.

## Confidence

- **High**: The core finding that hallucination-reduction methods have opposing effects on divergent creativity while preserving convergent creativity; the observation that early transformer layers correlate more strongly with creativity than later layers.
- **Medium**: The specific mechanisms explaining why CoVe enhances and DoLa suppresses divergent creativity; the claim that retrieval quality issues cause RAG's neutral effect.
- **Low**: The generalizability of findings across different creative domains beyond programming/coding tasks.

## Next Checks

1. **Cross-Domain Creativity Testing**: Evaluate the three hallucination-reduction methods on creativity benchmarks from non-technical domains (e.g., storytelling, poetry, or visual description generation) to test generalizability.
2. **Quantitative Retrieval Quality Analysis**: Implement metrics to measure semantic similarity between queries and retrieved documents in RAG, and correlate these with creativity performance to verify the retrieval quality hypothesis.
3. **Probe Transferability Study**: Systematically test how well linear probes trained on one model transfer to other models of different sizes and architectures, and develop methods to improve cross-model probe applicability.