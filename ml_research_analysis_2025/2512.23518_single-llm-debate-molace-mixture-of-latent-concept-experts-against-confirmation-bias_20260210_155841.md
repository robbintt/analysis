---
ver: rpa2
title: 'Single LLM Debate, MoLaCE: Mixture of Latent Concept Experts Against Confirmation
  Bias'
arxiv_id: '2512.23518'
source_url: https://arxiv.org/abs/2512.23518
tags:
- bias
- prompt
- debate
- base
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles confirmation bias in large language models,
  where prompts with implied preferred answers lead models to reinforce that bias
  rather than explore alternatives. The authors introduce Mixture of Latent Concept
  Experts (MoLaCE), a lightweight inference-time framework that mitigates this by
  mixing experts instantiated as different activation strengths over latent concepts
  shaping model responses.
---

# Single LLM Debate, MoLaCE: Mixture of Latent Concept Experts Against Confirmation Bias

## Quick Facts
- arXiv ID: 2512.23518
- Source URL: https://arxiv.org/abs/2512.23518
- Reference count: 23
- Primary result: Mixture of Latent Concept Experts (MoLaCE) reduces confirmation bias in LLMs by mixing experts instantiated as different activation strengths over latent concepts shaping model responses.

## Executive Summary
This paper tackles confirmation bias in large language models, where prompts with implied preferred answers lead models to reinforce that bias rather than explore alternatives. The authors introduce Mixture of Latent Concept Experts (MoLaCE), a lightweight inference-time framework that mitigates this by mixing experts instantiated as different activation strengths over latent concepts shaping model responses. The core insight is that differently phrased prompts reweight latent concepts in prompt-specific ways, so no single fixed intervention can be applied universally. MoLaCE enables a single LLM to emulate debate's benefits internally, while remaining efficient and scalable, and can be integrated into multi-agent debate frameworks to diversify perspectives and reduce correlated errors. Empirically, MoLaCE consistently reduces confirmation bias, improves robustness, and matches or surpasses multi-agent debate while requiring only a fraction of the computation.

## Method Summary
MoLaCE extracts a steering vector using Contrastive Activation Addition (CAA) from pairs of prompts with opposing stances. It then creates a mixture of experts by applying different intervention strengths (Î± values) to this steering vector at a specific network layer. An adaptive gate uses cosine similarity between the input prompt and steering vector to weight these experts, producing a final output that balances debiasing with coherence.

## Key Results
- MoLaCE consistently reduces confirmation bias across multiple datasets compared to baseline LLMs
- The framework matches or surpasses multi-agent debate performance while requiring significantly less computation
- No single fixed intervention strength works universally across all prompts, validating the need for the mixture approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: If confirmation bias manifests as a linearly separable direction in the activation space, then bias can be mitigated by counter-steering the model's hidden states.
- Mechanism: The paper extracts a steering vector $v$ using Contrastive Activation Addition (CAA). By computing the difference in residual stream activations between prompt pairs with opposing stances (e.g., "support" vs. "challenge"), they isolate a direction associated with confirmation bias. During inference, they add this vector scaled by a factor $\alpha$ to shift the posterior probability away from biased concepts.
- Core assumption: Confirmation bias is primarily localized in a specific activation direction that generalizes across prompts, rather than being entangled inextricably with semantic content.
- Evidence anchors:
  - [abstract] "...mixing experts instantiated as different activation strengths over latent concepts..."
  - [section 2.2] "...extract a confirmation-bias based steering direction $v$ using Contrastive Activation Addition (CAA)... we steer the model by applying an additive intervention $h^{(L)}_t \leftarrow h^{(L)}_t + \alpha v^{(L)}$."
  - [corpus] The paper "Latent Debate" (arxiv: 2512.01909) supports the premise of interpreting internal thinking via latent frameworks, though MoLaCE specifically focuses on activation engineering.
- Break condition: This mechanism likely fails if the bias is non-linear or if the contrastive prompt pairs used to extract $v$ fail to isolate the "bias" concept from the "topic" concept, resulting in corrupted outputs.

### Mechanism 2
- Claim: Since the optimal intervention strength varies unpredictably by prompt, marginalizing over multiple intervention strengths (experts) yields more robust debiasing than a fixed hyperparameter.
- Mechanism: Instead of selecting a single $\alpha$, MoLaCE treats $\alpha$ as a latent variable. It defines "experts" as the model distributions resulting from different steering strengths (e.g., $\alpha \in \{-3, \dots, 3\}$). A gating network computes mixture weights based on the cosine similarity between the input prompt and the steering vector, effectively soft-selecting the most relevant counter-bias strength.
- Core assumption: The cosine similarity between the input prompt encoding and the steering vector provides a reliable signal for how much intervention is required (the "gate").
- Evidence anchors:
  - [abstract] "...differently phrased prompts reweight latent concepts in prompt-specific ways, so no single fixed intervention can be applied universally..."
  - [section 3.2] "MoLaCE combines the outputs of $\alpha$-experts at each decoding step... The gate assigns mixture weights... measuring how the input prompt aligns with the latent concept direction."
  - [corpus] Corpus papers like "Understanding Bias Reinforcement..." (arxiv: 2503.16814) highlight that standard self-correction fails because it lacks adaptive diversity, which this mixture mechanism attempts to solve.
- Break condition: This fails if the gating function (cosine similarity) is a poor proxy for the required intervention magnitude, potentially amplifying noise or selecting sub-optimal experts.

### Mechanism 3
- Claim: If a single model simulates internal debate via latent concept diversity, it recovers the benefits of multi-agent debate (error correction) without the computational cost and correlated errors of multi-agent systems.
- Mechanism: The framework allows a single LLM to emulate debate by generating outputs from a mixture of perspectives (the steered experts). This creates "intra-agent" diversity. When integrated into multi-agent debate, it reduces "echo chambers" because the agents are no longer sampling from identical, biased distributions but from diversified mixtures.
- Core assumption: Diversity in the "activation space" translates to diversity in "reasoning logic" sufficient to correct factual errors.
- Evidence anchors:
  - [abstract] "...enables a single LLM to emulate the benefits of debate internally... can be integrated into multi-agent debate frameworks to diversify perspectives..."
  - [section 4.4] "When all agents share the same biased representations, collaborative reasoning tends to reinforce rather than counteract the skew... MoLaCE recovers accuracy."
  - [corpus] WISE (arxiv: 2512.02405) and related debate papers emphasize diversity as a key factor for robustness, validating the theoretical goal, though MoLaCE implements this via weights rather than distinct agents.
- Break condition: Breaks if the "experts" in the mixture are not sufficiently independent (i.e., if the steering magnitude changes style but not the underlying reasoning path), resulting in false diversity.

## Foundational Learning

- Concept: **Bayesian Mixture of Latent Concepts**
  - Why needed here: The paper frames LLM generation not as a single deterministic path but as a mixture of latent concepts $P(z|x) = \int P(z|x, \theta) P(\theta|x) d\theta$. Understanding this is required to grasp how "steering" merely shifts the probability mass over these concepts rather than hard-coding outputs.
  - Quick check question: If a steering vector successfully shifts the posterior $P(\theta|x)$, what does that imply about the geometry of the "truth-aligned" vs. "bias-aligned" concepts in the hidden state space?

- Concept: **Contrastive Activation Addition (CAA)**
  - Why needed here: This is the specific technique used to extract the "bias direction." One must understand that the vector $v$ is derived from the *difference* in activations between paired prompts, not from the prompts themselves.
  - Quick check question: Why is it necessary to use *pairs* of prompts (e.g., "Is MSG harmful?" vs. "Is MSG safe?") rather than just a set of biased prompts to calculate the steering vector?

- Concept: **Mixture of Experts (MoE) Gating**
  - Why needed here: MoLaCE adapts the MoE architecture not for scaling parameters, but for scaling *interventions*. The "gate" here is a lightweight mechanism (cosine similarity) rather than a learned neural network, which dictates how the system dynamically selects steering strengths.
  - Quick check question: In a standard MoE, experts are parameter sets (weights). In MoLaCE, what constitutes an "expert" and how does the gate weight them?

## Architecture Onboarding

- Component map: Contrastive Dataset -> Vector Extractor -> Expert Instantiation -> Adaptive Gate -> Mixture Decoder
- Critical path: The dependency flows from the **quality of contrastive pairs** $\to$ **extraction of clean steering vectors** $\to$ **accuracy of the Gate's similarity score**. If the pairs are confounded (e.g., differing in length rather than stance), the vector $v$ will capture noise, and the Gate will apply useless interventions.
- Design tradeoffs:
  - **Fixed vs. Adaptive Alpha**: The paper argues fixed $\alpha$ is brittle (Remark 1). The tradeoff is complexity: MoLaCE requires computing a mixture (inference overhead) vs. single-pass generation.
  - **Layer Selection**: Interventions are applied at a specific layer (e.g., layer 16). Earlier layers may be too syntactic; later layers too committed to a specific token. The paper finds middle layers optimal (Section 4.2).
- Failure signatures:
  - **Vector Entanglement**: The steering vector $v$ changes the topic of the response rather than just the stance (e.g., changing "MSG" to "Salt" instead of "Safe MSG").
  - **Gate Collapse**: The gate assigns near-uniform weights because the prompt is ambiguous, resulting in "average" or washed-out answers.
  - **Over-Steering**: High $\alpha$ values produce incoherent text (hallucinations or grammar collapse).
- First 3 experiments:
  1. **Vector Isolation Validation**: Run the Linear Probe experiment (Figure 3) to confirm that the extracted vector $v$ actually correlates with "Truth Alignment" (high probe accuracy) rather than random noise.
  2. **Alpha Sensitivity Sweep**: On a small validation set, test single-alpha performance across the range $\{-3, \dots, 3\}$ to verify the claim that no single $\alpha$ is universally optimal (Remark 1).
  3. **Gate Ablation**: Compare MoLaCE's adaptive gate against a simple Uniform or Majority-Vote baseline (Figure 6) to quantify the value added by the cosine-similarity gating logic.

## Open Questions the Paper Calls Out
None

## Limitations
- The gating mechanism's reliance on cosine similarity may not reliably predict when and how much intervention is needed for ambiguous prompts
- The steering vector extraction depends on perfectly aligned contrastive pairs, which may not generalize well across different model architectures
- There's an inherent tension between generating diverse viewpoints and maintaining coherent, non-contradictory outputs

## Confidence
- **High confidence**: The core claim that MoLaCE reduces confirmation bias compared to baseline LLMs is well-supported by empirical results across multiple datasets
- **Medium confidence**: The assertion that MoLaCE matches or surpasses multi-agent debate while requiring less computation is supported but could benefit from more systematic comparison
- **Medium confidence**: The claim that no single fixed intervention strength works universally is demonstrated through sensitivity analysis

## Next Checks
1. **Prompt ambiguity stress test:** Create a benchmark of ambiguous prompts (e.g., "This product is good for X") and measure how MoLaCE's gate performs versus human judgment on appropriate intervention strength.
2. **Layer sensitivity analysis:** Systematically test MoLaCE with steering applied at different network layers (e.g., layers 6, 12, 18, 24) across multiple model architectures to determine if middle layers are universally optimal or architecture-dependent.
3. **Expert independence quantification:** Develop metrics to measure the semantic and stylistic independence of the mixture components (e.g., pairwise BLEU scores, embedding distances, or factual consistency checks) to ensure the diversity is substantive rather than superficial.