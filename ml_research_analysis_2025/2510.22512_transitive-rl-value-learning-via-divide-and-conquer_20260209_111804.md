---
ver: rpa2
title: 'Transitive RL: Value Learning via Divide and Conquer'
arxiv_id: '2510.22512'
source_url: https://arxiv.org/abs/2510.22512
tags:
- learning
- value
- offline
- park
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Transitive RL (TRL) is a new value learning algorithm for offline
  goal-conditioned RL that uses a divide-and-conquer approach based on the triangle
  inequality. It converts the triangle inequality structure in GCRL into a practical
  value update rule, reducing Bellman recursions from O(T) to O(log T) compared to
  temporal difference methods while avoiding the high variance of Monte Carlo methods.
---

# Transitive RL: Value Learning via Divide and Conquer

## Quick Facts
- arXiv ID: 2510.22512
- Source URL: https://arxiv.org/abs/2510.22512
- Reference count: 27
- Key outcome: TRL achieves O(log T) Bellman recursions via triangle inequality, outperforming TD and MC methods on long-horizon offline GCRL tasks without horizon tuning.

## Executive Summary
Transitive RL (TRL) is a new value learning algorithm for offline goal-conditioned RL that uses a divide-and-conquer approach based on the triangle inequality. It converts the triangle inequality structure in GCRL into a practical value update rule, reducing Bellman recursions from O(T) to O(log T) compared to temporal difference methods while avoiding the high variance of Monte Carlo methods. TRL addresses the overestimation problem in max operations by using in-sample subgoals and soft expectile regression. Experiments on challenging long-horizon tasks show TRL outperforms or matches existing TD- and MC-based approaches, achieving strong performance without needing to tune horizon-dependent parameters. It also achieves the best overall performance on the standard OGBench benchmark.

## Method Summary
TRL trains a goal-conditioned action-value function Q(s,a,g) using transitive Bellman updates based on the triangle inequality. The method samples trajectory chunks and recursively combines shorter trajectory segments via intermediate subgoals, effectively halving the horizon at each recursion level. Key innovations include using in-trajectory behavioral subgoals with expectile regression (κ > 0.5) to prevent overestimation, and distance-based re-weighting to prioritize accurate short-range values. The algorithm uses BCE loss with polyak-averaged targets and can be combined with DDPG+BC for policy extraction. TRL operates entirely offline using only unlabeled trajectory data without environment interaction.

## Key Results
- Achieves O(log T) Bellman recursions vs O(T) for TD learning and high variance for MC methods
- Outperforms or matches existing TD- and MC-based approaches on challenging long-horizon tasks
- Achieves best overall performance on standard OGBench benchmark (10 environments, 50 tasks)
- Demonstrates strong performance without needing to tune horizon-dependent parameters

## Why This Works (Mechanism)

### Mechanism 1: Logarithmic Complexity via Triangle Inequality
The triangle inequality in GCRL enables divide-and-conquer value updates that require O(log T) Bellman recursions instead of O(T). GCRL's temporal distance d*(s,g) satisfies d*(s,g) ≤ d*(s,w) + d*(w,g). When w lies on a shortest path, equality holds. TRL recursively combines shorter trajectory segments via intermediate subgoals, halving the effective horizon at each recursion level rather than stepping one action at a time. Core assumption: Deterministic environments where the triangle inequality holds exactly. Evidence: Abstract states O(log T) vs O(T) recursions; Appendix A proves B(n) ≤ log n / log(4/3) for random sampling variant.

### Mechanism 2: In-Sample Subgoals Prevent Overestimation
Restricting subgoals to behavioral (in-trajectory) states with expectile regression prevents catastrophic value overestimation from the max operator. The max_w∈S operator would require evaluating millions of states and amplify any positive bias. TRL: (1) samples k only from within the same trajectory segment, and (2) replaces hard max with expectile regression (κ > 0.5) over these in-sample candidates. Core assumption: Behavioral subgoals are sufficient even from random/unlabeled data. Evidence: Abstract mentions addressing overestimation with in-sample subgoals and expectile regression; ablation study shows random subgoals substantially degrade performance vs. in-trajectory subgoals.

### Mechanism 3: Distance-Based Re-weighting for Stability
Distance-based re-weighting prioritizes accurate short-range values before propagating to longer ranges. Weight w(s_i, s_j) = 1/(1 + log_γ Q(s_i, a_i, s_j))^λ focuses loss on shorter chunks. Since TRL composes shorter segments into longer ones, accurate base cases stabilize longer-horizon estimates—mirroring classical dynamic programming. Core assumption: Transitive dependency—longer-horizon value accuracy depends on shorter-horizon accuracy. Evidence: Section 4.3.1 explains transitive dependency; ablation shows λ > 0 improves performance on several tasks.

## Foundational Learning

- **Goal-Conditioned RL (GCRL)**: Learning a policy π(a|s,g) that reaches any goal state from any start state in minimum steps. Why needed: TRL is specialized for offline GCRL where policies must reach arbitrary goals from arbitrary states. Quick check: Why does hindsight relabeling allow learning from trajectories that didn't reach their original goal?

- **TD vs. Monte Carlo Tradeoffs**: TD learning has O(T) bias accumulation while MC methods have high variance. Why needed: TRL is motivated by these failure modes. Quick check: Why does 1-step TD struggle specifically on long-horizon tasks?

- **Expectile Regression**: Replaces max over in-sample subgoals; κ controls softness (κ = 0.5 is mean/behavioral, κ → 1.0 approaches hard max). Why needed: Prevents overestimation from max operator. Quick check: What does κ = 0.5 vs. κ = 0.9 compute in the expectile loss?

## Architecture Onboarding

- **Component map**: Q(s,a,g) -> Target Q̄ -> Policy π(a|s,g) -> Oracle Q_φ (when needed)
- **Critical path**: 1) Sample trajectory τ ~ D, 2) Sample i < j uniformly, then k ∈ [i,j-1], 3) Compute target Q̄(s_i, a_i, s_k) × Q̄(s_k, a_k, s_j), 4) Apply weighted expectile BCE loss, 5) Polyak update targets, periodically extract policy
- **Design tradeoffs**: κ: 0.7 for long-horizon; 0.5 can work on shorter tasks. λ: 0 for long-horizon maze; 1-2 for some manipulation tasks. Policy: Reparameterized gradients (default) vs. rejection sampling (multi-modal cases). Oracle φ: Distillation required when goal space ≠ state space
- **Failure signatures**: Value explosion: Using random instead of in-trajectory subgoals. Poor long-horizon performance: κ too low or λ misconfigured. Zero success on robotics: Previous triangle-inequality methods fail without in-sample maximization
- **First 3 experiments**: 1) TRL vs. TD-{1,5,10,25,50,100} on humanoidmaze-giant with κ=0.7, λ=0. Verify TRL matches best TD-n without horizon tuning. 2) Ablate subgoal sampling: in-trajectory vs. random on puzzle-4x4. Expect ~2x performance drop. 3) Test oracle distillation on humanoidmaze-large where φ(g) = (x,y) only. Verify 2D-conditioned policy achieves 5D goals.

## Open Questions the Paper Calls Out

### Open Question 1
Can a divide-and-conquer value learning technique be applied to learn an unbiased value function in stochastic environments? Basis: Section 6 states it remains an open question whether similar techniques could be applied to stochastic environments, which is a limitation of TRL. Why unresolved: TRL relies on vanilla triangle inequality structure that holds for deterministic settings but is biased in stochastic environments. What evidence would resolve it: Theoretical derivation of unbiased update rule for stochastic dynamics or empirical results showing TRL variants matching TD performance in stochastic benchmarks.

### Open Question 2
Can TRL be extended to general reward-based RL tasks beyond goal-conditioned RL? Basis: Section 6 explicitly asks whether TRL can be extended to general reward-based RL tasks. Why unresolved: The transitive Bellman update relies on shortest-path structure inherent to goal-reaching distances, which doesn't naturally map to arbitrary reward functions. What evidence would resolve it: Formulation of transitive backup operator for general rewards that reduces recursive depth while maintaining convergence.

### Open Question 3
Can the reliance on in-trajectory behavioral subgoals be relaxed to allow arbitrary or learned subgoals without destabilizing training? Basis: Section 4.2 and Figure 5b show random subgoals cause performance collapse, forcing use of in-trajectory subgoals. Why unresolved: Restricting subgoals to dataset trajectories stabilizes training but limits ability to compose value estimates across distinct data trajectories or synthesize novel paths. What evidence would resolve it: Modified subgoal sampling strategy or regularization technique allowing maximization over arbitrary dataset states without value overestimation.

## Limitations
- Deterministic assumption: Relies on triangle inequality holding exactly, which may not hold in stochastic environments
- Subgoal restriction: Requires in-trajectory behavioral subgoals, limiting ability to compose across distinct trajectories
- Theoretical gaps: No formal proof of convergence for the expectile regression approach in stochastic settings

## Confidence
- Mechanism 1 (log T complexity): Medium-High - Proven theoretically for deterministic settings, but stochastic extension remains open
- Mechanism 2 (in-sample subgoals): High - Ablation studies clearly demonstrate degradation with random subgoals
- Mechanism 3 (distance weighting): Medium - Shows performance improvements but theoretical justification is limited

## Next Checks
1. **Stochastic environment validation**: Test TRL on benchmark stochastic GCRL tasks (e.g., Mujoco-based goal-reaching) to verify triangle inequality approximation holds and measure performance degradation compared to deterministic settings.

2. **Dataset coverage analysis**: Systematically vary trajectory dataset coverage (from dense to sparse) and measure how performance scales with in-trajectory subgoal availability, identifying minimum coverage threshold for effective TRL operation.

3. **Cross-task hyperparameter transfer**: Train TRL with fixed κ and λ parameters across heterogeneous task families (maze navigation, manipulation, puzzle-solving) to measure robustness and identify whether task-specific tuning remains necessary.