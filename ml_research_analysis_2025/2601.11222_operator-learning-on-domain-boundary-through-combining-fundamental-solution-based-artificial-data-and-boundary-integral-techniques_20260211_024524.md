---
ver: rpa2
title: Operator learning on domain boundary through combining fundamental solution-based
  artificial data and boundary integral techniques
arxiv_id: '2601.11222'
source_url: https://arxiv.org/abs/2601.11222
tags:
- boundary
- mad-bno
- training
- data
- equation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MAD-BNO, a novel operator learning framework
  that uses exclusively boundary data to learn solutions for linear elliptic PDEs
  with known fundamental solutions. The key innovation is training neural operators
  on synthetic Dirichlet-Neumann boundary data pairs generated from fundamental solutions,
  eliminating the need for full-domain sampling or external data.
---

# Operator learning on domain boundary through combining fundamental solution-based artificial data and boundary integral techniques

## Quick Facts
- **arXiv ID:** 2601.11222
- **Source URL:** https://arxiv.org/abs/2601.11222
- **Authors:** Haochen Wu; Heng Wu; Benzhuo Lu
- **Reference count:** 36
- **Primary result:** Novel operator learning framework using exclusively boundary data to learn solutions for linear elliptic PDEs with known fundamental solutions

## Executive Summary
This paper introduces MAD-BNO, a novel operator learning framework that uses exclusively boundary data to learn solutions for linear elliptic PDEs with known fundamental solutions. The key innovation is training neural operators on synthetic Dirichlet-Neumann boundary data pairs generated from fundamental solutions, eliminating the need for full-domain sampling or external data. The framework reformulates PDEs using boundary integral formulations, reducing the learning task to boundary-to-boundary mappings. MAD-BNO is validated on Laplace, Poisson, and Helmholtz equations in 2D and 3D, achieving accuracy comparable to or better than existing methods while reducing training time by 85%-95%.

## Method Summary
MAD-BNO is a boundary integral-based operator learning framework that trains neural networks exclusively on boundary data pairs. The method generates synthetic training data by combining fundamental solutions (e.g., logarithmic kernels for Laplace) from randomly sampled source points outside the domain. A single linear layer (400→400) learns the Dirichlet-to-Neumann map, which is then used with boundary integral formulas to reconstruct interior solutions. The approach eliminates the need for full-domain sampling and achieves significant computational efficiency gains over traditional methods.

## Key Results
- MAD-BNO achieves accuracy comparable to or better than existing methods (PI-DeepONet) for Laplace, Poisson, and Helmholtz equations
- Training time reduced by 85%-95% compared to baseline methods
- Superior performance in high-frequency scenarios (up to k=100 for Helmholtz) and excellent generalization to out-of-distribution cases
- Demonstrates robustness on complex geometries including star-shaped and kidney-shaped domains

## Why This Works (Mechanism)

### Mechanism 1: Dimensionality Reduction via Boundary Integral Formulation
The method leverages Green's representation formula to reconstruct interior solutions using only boundary data, reducing the learning task from volume-to-volume to boundary-to-boundary mappings. This significantly reduces training complexity while preserving interior solution accuracy.

### Mechanism 2: Physics-Consistency via Fundamental Solution Synthesis
Training data is synthesized directly from fundamental solutions, ensuring analytical error-free data that enforces physical consistency. This eliminates the noise inherent in numerical PDE solvers and provides a dense sampling of the solution space through linear combinations.

### Mechanism 3: Alignment of Network Capacity with Operator Linearity
The Dirichlet-to-Neumann map for linear elliptic PDEs is mathematically a linear operator. The framework exploits this by using a single bias-free linear layer, which is theoretically optimal for this task and outperforms deeper nonlinear networks.

## Foundational Learning

- **Concept: Fundamental Solution (Green's Function)**
  - **Why needed:** This is the "seed" for the entire framework. The method relies on knowing the closed-form solution for a point source to generate synthetic training data.
  - **Quick check:** Can you write down the fundamental solution for the 2D Laplace equation used in the paper? (Answer: $-\frac{1}{2\pi} \ln|x-y|$)

- **Concept: Boundary Integral Equations**
  - **Why needed:** Understanding how the interior of a domain can be reconstructed purely from surface data is the theoretical justification for why the network doesn't need to see the inside of the box.
  - **Quick check:** If you know $u$ everywhere on the boundary, what additional information does Eq. (3) say you need to compute $u$ inside the domain? (Answer: The normal derivative $\partial u / \partial n$).

- **Concept: Operator Linearity**
  - **Why needed:** The paper deliberately chooses a "trivial" linear network architecture. Without understanding that the *physics* (the mapping) is linear, this architectural choice looks like a bug rather than a feature.
  - **Quick check:** If you double the input Dirichlet boundary values for a linear PDE, what happens to the output Neumann values? (Answer: They double).

## Architecture Onboarding

- **Component map:** Geometry Definition -> Data Generation -> Training -> Reconstruction
- **Critical path:**
  1. Define boundary $\partial \Omega$ and collocation points
  2. Generate synthetic data: sample source points → compute Dirichlet/Neumann pairs
  3. Train bias-free linear network (400→400) minimizing MSE loss
  4. Reconstruct interior using predicted boundary data in boundary integral formulas

- **Design tradeoffs:**
  - **Efficiency vs. Generality:** Extremely fast (85-95% time reduction) but restricted to linear PDEs with known fundamental solutions
  - **Mesh-free Training vs. Meshed Inference:** Training is mesh-free, but interior reconstruction requires volume integration (mesh) for full field

- **Failure signatures:**
  - Source points too close to boundary causing singularity errors
  - High-frequency wavenumber exceeding discretization capacity
  - Applying to non-linear PDEs where linear architecture assumption fails

- **First 3 experiments:**
  1. Replicate Table 1 to verify single linear layer outperforms deeper MLPs on Laplace problem
  2. Train on "logarithmic" synthetic data and test on "polynomial" or "exponential" functions to confirm generalization
  3. Compare wall-clock time against PINN/PI-DeepONet for same 50k epochs to validate 10x speedup claim

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the MAD-BNO framework be extended to nonlinear PDEs?
- **Basis:** The methodology explicitly exploits linearity by utilizing a single bias-free linear layer and relies on the superposition principle for data generation
- **Why unresolved:** Current architectural choice and data generation strategy are mathematically restricted to linear operators
- **What evidence would resolve it:** Formulation using nonlinear activation functions or modified training pipeline for nonlinear operator behavior

### Open Question 2
- **Question:** What performance gains can be achieved by integrating MAD-BNO with modern advanced network architectures?
- **Basis:** Authors state "Potential synergies with modern data-driven techniques... remain to be investigated"
- **Why unresolved:** Current implementation uses simple linear network to demonstrate mathematical concept
- **What evidence would resolve it:** Comparative benchmarks integrating boundary-centric loss with architectures like Transformers or FNO

### Open Question 3
- **Question:** How does the method perform on complex 3D geometries and large-scale problems involving volume integrals?
- **Basis:** Authors note "more complex scenarios in 3D are reserved for future investigation" and current inference relies on $O(N^2)$ direct evaluation
- **Why unresolved:** 3D validation limited to surface error metrics, computational cost of full reconstruction not benchmarked
- **What evidence would resolve it:** Numerical results on intricate 3D meshes demonstrating accuracy and wall-clock time

## Limitations

- Framework restricted to linear PDEs with known fundamental solutions, limiting broader scientific computing applications
- Claims about training data density in solution space lack rigorous mathematical proof
- Computational advantage measurements lack ablation studies showing relative contributions of different design choices

## Confidence

- **High Confidence:** Dimensionality reduction via boundary integral formulation is well-established and correctly applied; empirical validation of single-layer networks is convincing
- **Medium Confidence:** Synthetic data generation is methodologically sound, but physical consistency claims need more rigorous comparison against solver errors
- **Low Confidence:** Generalization claims to out-of-distribution cases based on limited testing without systematic exploration of failure modes

## Next Checks

1. Test framework on nonlinear PDE (e.g., nonlinear Poisson) to quantify breakdown of linear architecture assumption and measure performance degradation
2. Conduct systematic ablation studies varying: (a) number of boundary collocation points, (b) source point distribution density, and (c) linear vs. nonlinear network architectures
3. Compare against high-precision numerical solver (not just PINN/PI-DeepONet) to quantify actual error introduced by synthetic data approach versus solver discretization errors