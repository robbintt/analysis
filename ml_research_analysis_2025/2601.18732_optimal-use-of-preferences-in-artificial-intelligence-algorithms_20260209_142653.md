---
ver: rpa2
title: Optimal Use of Preferences in Artificial Intelligence Algorithms
arxiv_id: '2601.18732'
source_url: https://arxiv.org/abs/2601.18732
tags:
- training
- when
- learning
- decision
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a formal theory for the architectural choice
  in AI systems between embedding preferences in training objectives versus applying
  them post-processing calibrated predictions. The core method models training as
  information acquisition, where predictors induce distributions of posterior beliefs.
---

# Optimal Use of Preferences in Artificial Intelligence Algorithms

## Quick Facts
- arXiv ID: 2601.18732
- Source URL: https://arxiv.org/abs/2601.18732
- Reference count: 17
- Primary result: Embedding preferences in training objectives induces a mean-preserving contraction of posterior beliefs, making preference-free training plus ex post application of preferences weakly dominant for expected-utility decision problems.

## Executive Summary
This paper develops a formal theory for when to embed preferences in AI training objectives versus applying them post-processing calibrated predictions. The core insight is that preference embedding reduces the value of information at the margin, inducing a mean-preserving contraction of learned posterior beliefs. Combined with convexity of expected-utility decision problems in beliefs, this yields a separation principle: training with preference-free strictly proper scoring rules and implementing preferences ex post weakly dominates preference-embedded training uniformly across decision problems. The paper also identifies conditions where preference embedding can dominate (high decision-stage cognitive costs) and extends the framework to RLHF in LLMs.

## Method Summary
The paper models training as information acquisition where predictors induce distributions of posterior beliefs. Using convex-order machinery, it establishes a diminishing-value-of-information condition: preference embedding makes informativeness less valuable at the margin, inducing mean-preserving contraction of learned posteriors. Combined with convexity of expected-utility decision problems in beliefs, this yields a robust separation principle. The framework extends to RLHF by modeling the generator as inducing a distribution of posteriors, with preference embedding reducing capability on alternative reward functions.

## Key Results
- Preference embedding induces mean-preserving contraction of posterior beliefs under diminishing-value-of-information condition
- Separation principle: preference-free training plus ex post preference application weakly dominates preference-embedded training for any expected utility decision problem
- Cognitive constraints can reverse the dominance: when decision-stage cognitive costs are high, preference embedding can dominate by automating threshold computation
- RLHF preference embedding reduces robustness to objective uncertainty and reward misspecification

## Why This Works (Mechanism)

### Mechanism 1: Mean-Preserving Contraction Under Preference Embedding
Embedding preferences in training objectives reduces learned informativeness by making marginal information less valuable. With convex-order monotone learning frictions, this induces a mean-preserving contraction of posterior beliefs. The diminishing-value-of-information condition means that when preferences are embedded, the marginal benefit of acquiring more informative posteriors falls, causing the optimizer to shift toward less dispersed posteriors.

### Mechanism 2: Separation Principle via Convexity of Indirect Value
For any expected-utility decision problem, training with preference-free strictly proper scoring rules and applying preferences ex post weakly dominates preference-embedded training. The indirect value V(q) = sup_a E[u(a,Y)|q] is convex in beliefs. Convex-order contraction implies E[V(Q_embedded)] ≤ E[V(Q_separated)], and the post-processing stage recovers the Bayes-optimal action rule without sacrificing learned informativeness.

### Mechanism 3: Cognitive Constraints Reverse the Dominance
When decision-stage cognitive costs scale with information processed, preference embedding can dominate by compressing the signal. Rational inattention models attention cost as λ·I(Y;Q). Preference embedding reduces mutual information at the cost of decision value. Embedding dominates when λcog > ∆I/∆MI—the attention savings exceed the information loss.

## Foundational Learning

- **Concept: Strictly proper scoring rules and Bayes risk**
  - Why needed: The paper defines "preference-free" via strictly proper losses that elicit calibrated probabilities. Bayes risk curvature governs learning incentives.
  - Quick check: Given a loss L(p,y), can you compute its Bayes risk HL(q) and verify whether L is strictly proper?

- **Concept: Convex order and mean-preserving spreads**
  - Why needed: The contraction result and welfare comparisons rely on convex order; Q ⪰cx Q' means Q is more dispersed (more informative) with the same mean.
  - Quick check: If E[Q] = E[Q'] and Var(Q) > Var(Q'), does Q ⪰cx Q' necessarily hold? (No—variance alone doesn't determine convex order.)

- **Concept: Information design / Bayesian persuasion**
  - Why needed: The paper models training as choosing an information structure, drawing on Kamenica-Gentzkow and Strack-Yang machinery.
  - Quick check: What does Bayes plausibility require of the distribution of posterior beliefs? (E[Q] = prior µ.)

## Architecture Onboarding

- **Component map:** Training objective (Bayes risk Ht) -> Learning friction C(Q) -> Learned posterior distribution Qt -> Decision stage (post-processing rule or embedded threshold) -> Cognitive cost (optional)

- **Critical path:**
  1. Verify the training loss is strictly proper (if preference-free) or characterize its transformation (if embedded)
  2. Check diminishing-value condition: is H1 − H0 convex? (Second derivative test in binary case)
  3. Assess posterior comparability (nested signal families satisfy this; arbitrary frictions may not)
  4. Evaluate decision-stage friction: can users implement optimal a*(q)?
  5. If cognitive costs bind, compute λ* = ∆I/∆MI to determine reversal threshold

- **Design tradeoffs:**
  - Separation: preserves option value across objectives; robust to objective uncertainty; requires capable downstream implementation
  - Embedding: reduces cognitive burden; locks in a single objective; sacrifices flexibility; can amplify misspecification
  - KL strength: larger λ preserves generator diversity; smaller λ concentrates on training objective

- **Failure signatures:**
  - Posterior contraction despite sufficient data: check if loss weighting unintentionally satisfies diminishing-value condition
  - Goodhart amplification in RLHF: reward proxy correlates with true quality on baseline but diverges under optimization
  - Users ignore calibrated outputs: cognitive costs may exceed decision-value gains; consider interface compression

- **First 3 experiments:**
  1. Calibrate both preference-free and preference-embedded predictors; compare posterior variance (E[Q²] or full distribution). Expect mean-preserving contraction under embedding if mechanism holds.
  2. Deploy to human subjects with controlled cognitive load; measure uptake and decision quality. Test Theorem 3's prediction that embedding dominates when λcog > λ*.
  3. In RLHF setting, vary KL penalty λ and evaluate under alternative scalarizations w' ≠ w. Test Theorem 5's capability-tax prediction: small λ should hurt performance on w' if maximizers differ.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Which classes of algorithmic fairness policies can be implemented as ex post decision rules without sacrificing predictive information?
- Basis: The conclusion states a direction to "connect the framework more directly to fairness constraints by treating fairness objectives as a form of objective uncertainty."
- Why unresolved: The current theory is developed for scalar binary beliefs, whereas fairness often involves multi-dimensional or complex constraints that may not translate simply to post-processing thresholds.
- What evidence would resolve it: A theoretical characterization of fairness constraints that are compatible with ex post implementation, or empirical results showing that specific fairness interventions applied post-training do not induce a mean-preserving contraction of posteriors.

### Open Question 2
- Question: How does the separation principle generalize to multi-class or multi-criteria environments where convex order is only a partial order?
- Basis: The paper notes that "multi-class and multi-criteria settings require richer information orders and raise new selection issues" compared to the binary model.
- Why unresolved: In dimensions greater than one, posterior distributions are often incomparable in convex order, making the single-index "informativeness frontier" assumed in the main results inapplicable.
- What evidence would resolve it: A derivation of separation results for multivariate beliefs that addresses the selection issues inherent in partial orders, or identification of restrictions on learning frictions that restore comparability.

### Open Question 3
- Question: How does the optimal architectural choice change when objectives evolve dynamically over time?
- Basis: The author lists a third direction: "formalising how modular designs preserve option value under such evolution" regarding dynamic objectives and model updates.
- Why unresolved: The current model is static; it does not account for the temporal evolution of governance interventions or the costs associated with retraining versus updating post-processing rules.
- What evidence would resolve it: A dynamic extension of the model showing that the welfare gain from separation increases with the volatility of objectives or the speed of regulatory change.

### Open Question 4
- Question: What is the optimal division of labour between base models, modular post-processing layers, and generator-level fine-tuning (like RLHF) in AI alignment?
- Basis: The conclusion identifies the "promising research direction" of "characterising the optimal division of labour" between base capabilities, post-processing, and fine-tuning.
- Why unresolved: While the paper analyzes RLHF as preference embedding, it does not fully characterize the trade-off frontier balancing the "capability tax" of embedding against the inference costs of post-processing.
- What evidence would resolve it: A formal model or empirical benchmark that minimizes total system cost (training, capability loss, and inference friction) to find the optimal mix of generator alignment versus downstream routing.

## Limitations
- Convex-order monotonicity of learning frictions (Assumption 2) holds for idealized information design problems but is not explicitly verified for practical optimizers like SGD + weight decay
- Comparability assumption (Assumption 4) is satisfied for nested signal families but may fail for arbitrary neural architectures, limiting the robustness claim
- Mapping theoretical conditions to real-world training regimes remains challenging due to complex interactions between optimization dynamics and preference embedding

## Confidence

- Separation Principle (Theorem 2): High confidence in mathematical derivation; medium confidence in practical dominance given real-world learning friction complexity
- Cognitive Constraint Reversal (Theorem 3): High confidence in theoretical conditions; medium confidence in λ* calibration without empirical human-AI interaction data
- RLHF Extension (Theorem 5): High confidence in proof structure; medium confidence in generalization from binary to high-dimensional generative models

## Next Checks

1. Implement and test the curvature condition (Assumption 3) for cross-entropy with asymmetric class weights across multiple network architectures and optimizers to verify diminishing-value-of-information empirically.

2. Design a human-subject experiment comparing uptake and decision quality for separated (calibrated) vs. embedded preference outputs under varying cognitive load conditions, directly testing Theorem 3's reversal threshold.

3. In a controlled RLHF setup with LLMs, systematically vary KL penalty λ and evaluate performance under held-out reward scalarizations w' ≠ w to quantify capability-tax predictions from Theorem 5.