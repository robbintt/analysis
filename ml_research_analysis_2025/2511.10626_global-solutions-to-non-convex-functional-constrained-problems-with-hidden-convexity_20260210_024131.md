---
ver: rpa2
title: Global Solutions to Non-Convex Functional Constrained Problems with Hidden
  Convexity
arxiv_id: '2511.10626'
source_url: https://arxiv.org/abs/2511.10626
tags:
- convex
- optimization
- hidden
- constrained
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops the first provable algorithms to find global
  solutions for non-convex constrained optimization problems with hidden convexity,
  where the problem admits a convex reformulation under an unknown variable transformation.
  The key challenge is that the transformation is implicit or unknown, making direct
  use of the convex reformulation impossible, while standard gradient methods applied
  to the original non-convex formulation may fail due to constraints.
---

# Global Solutions to Non-Convex Functional Constrained Problems with Hidden Convexity

## Quick Facts
- **arXiv ID:** 2511.10626
- **Source URL:** https://arxiv.org/abs/2511.10626
- **Reference count:** 40
- **Primary result:** First provable algorithms to find global solutions for non-convex constrained problems with hidden convexity, achieving $\widetilde{\mathcal{O}}(\varepsilon^{-3})$ non-smooth and $\widetilde{\mathcal{O}}(\varepsilon^{-1})$ smooth oracle complexities.

## Executive Summary
This paper develops the first provable algorithms to find global solutions for non-convex constrained optimization problems where the problem admits a convex reformulation under an unknown variable transformation. The key challenge is that the transformation is implicit or unknown, making direct use of the convex reformulation impossible, while standard gradient methods applied to the original non-convex formulation may fail due to constraints. The authors propose two distinct algorithmic frameworks that handle equality constraints and make no constraint qualification assumptions.

## Method Summary
The paper introduces two algorithmic frameworks for non-convex constrained optimization with hidden convexity. The first is a modified inexact proximal point method with shifted constraints that preserves feasibility at each iteration by introducing a violation budget τ. This ensures each subproblem satisfies Slater's condition, allowing use of efficient convex optimization methods as inner solvers. The second approach is a bundle-level method with shifted constraints that achieves better oracle complexity in the smooth setting. Both methods avoid constraint qualification assumptions and can handle equality constraints. The algorithms are validated on non-smooth and smooth geometric programming problems.

## Key Results
- Modified inexact proximal point method achieves $\widetilde{\mathcal{O}}(\varepsilon^{-3})$ complexity for non-smooth problems using switching subgradient methods
- Shifted bundle-level method achieves $\widetilde{\mathcal{O}}(\varepsilon^{-1})$ complexity in smooth setting when optimal value is known
- Adaptive line-search procedure achieves $\widetilde{\mathcal{O}}(\lambda^*\varepsilon^{-1})$ complexity when optimal value is unknown under strong duality
- Both methods handle equality constraints without requiring constraint qualifications

## Why This Works (Mechanism)

### Mechanism 1: Constraint Shifting to Enforce Slater's Condition in Subproblems
By shifting constraints outward by a budget τ, the algorithm ensures that each proximal subproblem satisfies Slater's condition, even if the original non-convex problem does not. The method introduces a "violation budget" τ, and while the original problem might have equality constraints (where Slater's fails), the subproblems are relaxed via φ₂⁽ᵏ⁾(x) - τ ≤ 0. This shift guarantees the existence of an interior point for the subproblem, allowing standard feasible inner solvers to converge.

### Mechanism 2: Proximal Regularization for Strong Convexity
Proximal point regularization transforms the weakly convex (non-convex) problem into a sequence of strongly convex subproblems. The algorithm adds a quadratic penalty ̂ρ/2 ||x - x⁽ᵏ⁾||² to both the objective and constraints. This lifting ensures the subproblems are strongly convex, permitting the use of efficient convex solvers for the inner loop.

### Mechanism 3: Linear Minorant Shifting (Bundle-Level)
Standard linear approximations fail in non-convex settings due to negative curvature; shifting these cuts allows the Bundle-Level method to converge globally. Instead of using standard lower bounds F(x) ≥ F(y) + ⟨∇F(y), x-y⟩, the method relaxes the constraint of the subproblem using a shift parameter τ. This prevents the algorithm from projecting iterates to extreme points of the domain.

## Foundational Learning

**Hidden Convexity & Convex Reformulation:** The entire paper rests on the premise that the non-convex problem can be mapped to a convex one via an unknown transformation u = c(x). Understanding this explains why global solutions are theoretically tractable despite non-convexity.

**Inexact Proximal Point Method (IPPM):** This is the algorithmic backbone for the non-smooth setting. It treats optimization as a sequence of regularized subproblems, where "inexact" means we tolerate errors in solving these subproblems.

**Constraint Qualifications (CQ) / Slater's Condition:** Standard constrained optimization often fails without CQ. This paper's main novelty is bypassing this requirement via shifting.

## Architecture Onboarding

**Component map:** Driver (Outer Loop) -> Inner Solver -> Oracle

**Critical path:**
1. Initialize x⁽⁰⁾ (must be τ-feasible)
2. Construct subproblems with regularization and shifted constraints
3. Execute inner solver until target accuracy ε_in is reached
4. Update iterate and check global convergence

**Design tradeoffs:** IPPM vs. Bundle-Level: IPPM is robust (no CQ needed) but has slower oracle complexity. Bundle-Level is faster but requires strong duality or knowledge of optimal value F* for best performance.

**Failure signatures:**
- Oscillation: If using Bundle-Level without proper shifts, iterates may bounce between domain boundaries
- Infeasibility: If τ is too small for the problem's inherent "distance" from feasibility, the inner solver may fail to find a feasible point for the subproblem

**First 3 experiments:**
1. 2D Non-Smooth CNLS: Visualize the trajectory of IPPM+SwSG to verify global convergence on a non-convex landscape
2. Smooth Geometric Programming: Compare IPPM+ACGD vs. Shifted Bundle-Level to validate the speedup claimed in Section 5.1
3. High-Dimensional Stress Test: Run S-BL+AdaLS on a synthetic instance where F* is unknown to test the adaptive line-search robustness

## Open Questions the Paper Calls Out

**Can we develop simple, single-loop algorithms for constrained hidden convex optimization that avoid solving quadratic linearly constrained subproblems?** The authors state their algorithms are "fairly complicated" and suggest developing single-loop methods without additional quadratic linearly constrained subproblems would be interesting.

**What are the minimax optimal gradient complexities for hidden convex optimization problems?** While the proposed methods match the best-known complexity for the unconstrained setting, the authors note they "still do not know what are the minimax optimal gradient complexities."

**Can regularization be effectively performed in the convex reformulation space (U-space) rather than the original variable space (X-space) to improve convergence?** The authors propose exploring other natural choices for regularization, specifically suggesting it is "conceptually possible to regularize the iterates in the U-space."

## Limitations

- Reliance on the existence of hidden convex reformulation may limit applicability to problems without such structures
- Practical estimation of weak convexity and hidden convexity constants for real-world problems is not addressed
- Computational efficiency of ACGD QP subproblems in high dimensions requires further investigation

## Confidence

**High Confidence:** Core theoretical results showing shifted subproblems satisfy Slater's condition and oracle complexity bounds for both IPPM and Bundle-Level methods.

**Medium Confidence:** Practical implementation details and hyperparameter selection strategies, particularly for choosing τ relative to ε and estimating weak convexity constants.

**Low Confidence:** Generalizability to problems without explicit hidden convexity structures and computational efficiency of QP subproblems in high dimensions.

## Next Checks

1. Develop and validate a practical procedure to detect whether a given non-convex constrained problem admits a hidden convex reformulation, and estimate the weak convexity and hidden convexity constants automatically.

2. Implement and test an adaptive scheme for selecting the constraint shift parameter τ during optimization, rather than using theoretical bounds that may be overly conservative.

3. Evaluate the computational scaling of the Bundle-Level method with AdaLS on problems with d > 100 variables, particularly focusing on the cost of solving the QP subproblems and the effectiveness of the line-search procedure.