---
ver: rpa2
title: Evaluating Legal Reasoning Traces with Legal Issue Tree Rubrics
arxiv_id: '2512.01020'
source_url: https://arxiv.org/abs/2512.01020
tags:
- issue
- legit
- reasoning
- legal
- issues
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LEGIT is a large-scale legal judgment prediction dataset with structured
  legal issue trees serving as rubrics for evaluating reasoning traces. The dataset
  includes 24,000+ Korean court cases with automatically extracted facts and issue
  trees, enabling assessment of both issue coverage and correctness.
---

# Evaluating Legal Reasoning Traces with Legal Issue Tree Rubrics

## Quick Facts
- arXiv ID: 2512.01020
- Source URL: https://arxiv.org/abs/2512.01020
- Reference count: 40
- Primary result: LEGIT enables reliable rubric-based evaluation of legal reasoning, revealing that RAG and RL provide complementary improvements

## Executive Summary
LEGIT introduces a large-scale legal judgment prediction dataset with structured legal issue trees serving as rubrics for evaluating reasoning traces. The dataset includes 24,000+ Korean court cases with automatically extracted facts and issue trees, enabling assessment of both issue coverage and correctness. Expert evaluations show strong agreement between human lawyers and LLMs using LEGIT rubrics, validating their reliability. Evaluation of 12 frontier LLMs reveals significant gaps in legal reasoning ability, with errors categorized as either decomposition (missing relevant issues) or deduction (incorrect reasoning about identified issues).

## Method Summary
LEGIT uses a two-stage LLM extraction process to convert raw Korean court judgments into atomic facts and hierarchical legal issue trees. The evaluation framework employs LLM-as-a-judge with per-issue rubrics, scoring each issue on coverage (binary) and correctness (binary), with weighted aggregation into a final LEGIT score (/10). RAG experiments use BM25 or mContriever for citation retrieval, while RL experiments employ GRPO with LEGIT scores as rewards. The dataset is split into 24,106 training cases and 300 test cases, with difficulty levels based on issue count.

## Key Results
- LEGIT rubrics achieve strong inter-rater agreement between human lawyers (α=0.87) and LLMs (α=0.62-0.74)
- Errors propagate hierarchically through issue trees, with incorrect reasoning causing greater parent issue degradation than omission
- RAG improves both issue coverage and correctness, while RL prioritizes correctness at the expense of coverage
- Combining RAG and RL could substantially enhance LLM legal reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modular issue-level rubrics enable more reliable LLM-as-a-judge evaluation than coarse holistic scoring.
- Mechanism: Breaking evaluation into atomic decisions per issue reduces cognitive load on the evaluator and limits compounding errors, similar to how chain-of-thought improves reasoning by decomposition.
- Core assumption: LLM evaluators can reliably assess binary coverage/correctness per issue even if they struggle with holistic quality judgments.
- Evidence anchors:
  - Expert evaluations show strong agreement between human lawyers and LLMs using LEGIT rubrics, validating their reliability.
  - Modular LEGIT rubrics show higher evaluation consistency than the coarse Likert scale.
  - PLawBench similarly uses rubric-based evaluation for legal practice.

### Mechanism 2
- Claim: Errors propagate hierarchically through legal issue trees—child issue failures degrade parent correctness more severely than simple omission.
- Mechanism: Legal reasoning follows backward chaining; if a sub-issue is incorrectly resolved, any conclusion depending on it inherits the error. The penalty for incorrect reasoning exceeds the penalty for omission because incorrect reasoning actively misleads downstream inference.
- Core assumption: Issue trees accurately reflect the logical dependencies in legal reasoning.
- Evidence anchors:
  - Errors categorized as either decomposition (missing relevant issues) or deduction (incorrect reasoning about identified issues).
  - Parent issue correctness drops from ~90% when all children correct to ~45-58% when children are covered but incorrect.
  - Limited direct corpus evidence on hierarchical error propagation in legal reasoning specifically.

### Mechanism 3
- Claim: RAG and RL with rubrics provide complementary improvements—RAG expands issue coverage while RL sharpens correctness through conservative issue selection.
- Mechanism: RAG provides external legal knowledge that surfaces additional relevant issues and improves reasoning accuracy across all components. RL with LEGIT rewards directly optimizes for score, which penalizes incorrect reasoning more heavily than missed issues (3 points for correctness vs. 2 for coverage), leading models to avoid uncertain issues.
- Core assumption: The LEGIT scoring scheme correctly prioritizes correctness over coverage.
- Evidence anchors:
  - RAG improves both issue coverage and correctness, while RL with LEGIT rubrics prioritizes correctness at the expense of coverage.
  - RL prioritizes correctness at the cost of coverage.
  - This trade-off is consistent with the analysis showing that the penalty for incorrect reasoning is more severe than omitting the issue altogether.

## Foundational Learning

- Concept: **Legal Issue Trees (Hierarchical Argument Structures)**
  - Why needed here: LEGIT represents cases as trees where each node contains party arguments and court conclusions, enabling structured evaluation of reasoning depth.
  - Quick check question: Given a contract dispute about payment, can you decompose it into sub-issues about contract validity, breach, and damages?

- Concept: **LLM-as-a-Judge with Instance-Specific Rubrics**
  - Why needed here: Generic evaluation criteria fail for expert domains; LEGIT uses per-case rubrics extracted from actual judgments to ground evaluation.
  - Quick check question: Why might an LLM rate a legal answer highly on a Likert scale while missing key legal issues entirely?

- Concept: **GRPO (Group Relative Policy Optimization) for Rubric Rewards**
  - Why needed here: RL training uses LEGIT scores as rewards, requiring an objective that handles non-binary, multi-component scoring signals.
  - Quick check question: How does using LEGIT score as reward differ from using only final-order correctness as reward?

## Architecture Onboarding

- Component map:
  Raw judgments -> Fact extraction (Gemini-2.0-Flash) -> Issue tree extraction (2-pass LLM) -> Rubric conversion -> Per-issue LLM judge -> Weighted aggregation

- Critical path:
  1. Quality of extracted issue trees determines rubric reliability
  2. Evaluator model capability limits evaluation reliability
  3. RL training reward design shapes coverage-correctness tradeoff

- Design tradeoffs:
  - Modular vs. holistic evaluation: Modular has higher compute cost but better inter-rater consistency
  - Training/test evaluator separation: Reduces reward hacking but requires two model deployments
  - Scoring weights (5/2/3): Final order weighted highest; correctness weighted above coverage based on error propagation analysis

- Failure signatures:
  - LLM judges tend to mark issues as covered/correct more liberally than humans
  - RL training reduces issue coverage while improving correctness
  - Low retrieval recall still provides gains via "persona effect" but limits RAG ceiling

- First 3 experiments:
  1. Validate rubric extraction quality: Sample 50 training cases, manually check fact extraction and issue tree accuracy
  2. Calibrate evaluator selection: Compare 2-3 LLM judges against human expert annotations to select evaluator with acceptable α threshold
  3. Baseline RAG+RL combination: Train RL model starting from RAG-augmented inputs to test whether combined approach achieves both high coverage and high correctness

## Open Questions the Paper Calls Out

- **Question:** Does combining RAG with RL resolve the trade-off where RL improves correctness at the expense of issue coverage?
- **Basis in paper:** The authors state in the Abstract and Conclusion that "The complementary benefits of RAG and RL suggest combining these approaches could substantially enhance LLM legal reasoning capabilities."
- **Why unresolved:** The paper evaluates RAG and RL independently. RL training prioritized correctness by pruning uncertain issues, whereas RAG improved both. The interaction between providing external context (RAG) and optimizing the rubric reward (RL) remains untested.
- **What evidence would resolve it:** Train a model using RAG inputs within the GRPO RL framework and measure if it achieves higher LEGIT scores than the RL-only baseline.

- **Question:** Can the LEGIT framework generalize to Common Law jurisdictions where legal reasoning may rely more on case precedent than Civil Code structures?
- **Basis in paper:** Section 8 notes: "Extending LEGIT to diverse jurisdictions and languages is a promising direction, and we leave it as future work."
- **Why unresolved:** The dataset is currently restricted to Korean civil and administrative cases. Legal issue trees in Common Law systems might exhibit different hierarchical structures or reasoning patterns.
- **What evidence would resolve it:** Construct a LEGIT-style dataset using US/UK court judgments and validate the inter-rater agreement between human experts and LLM judges.

- **Question:** Can the LEGIT evaluation framework be extended to verify citation accuracy in jurisdictions with open legal data?
- **Basis in paper:** Section 8 states: "LEGIT does not directly address citation accuracy... However, as Korean court judgments are not freely disclosed to the public, we find this approach infeasible at the time of writing."
- **Why unresolved:** The current rubrics evaluate issue coverage and logical conclusion correctness but ignore whether the LLM cites valid sources.
- **What evidence would resolve it:** Apply the LEGIT framework to a jurisdiction with open legal data and add a "Citation Validity" score to the rubric.

## Limitations

- The evaluation relies on Korean-language models, limiting generalizability to English-language legal reasoning
- Dataset construction depends heavily on automated extraction that may miss nuanced legal arguments
- The "persona effect" observed in retrieval suggests benefits from stylistic matching rather than substantive legal knowledge

## Confidence

- **High confidence:** The modular evaluation framework demonstrably improves inter-rater consistency compared to holistic scoring
- **Medium confidence:** The hierarchical error propagation mechanism and RAG/RL complementarity effects are well-supported but may depend on specific LEGIT scoring weights
- **Low confidence:** Generalization of these results to other legal domains or languages remains unproven

## Next Checks

1. Replicate the error propagation analysis with alternative legal reasoning datasets to test if the hierarchical penalty structure holds across domains
2. Conduct ablation studies varying the 5/2/3 scoring weights to determine if the RAG/RL complementarity is robust to scoring scheme changes
3. Test whether the evaluation framework maintains reliability when applied to English-language legal cases using the same LLM-as-judge methodology