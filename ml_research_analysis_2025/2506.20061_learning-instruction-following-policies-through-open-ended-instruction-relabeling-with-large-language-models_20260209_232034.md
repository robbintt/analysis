---
ver: rpa2
title: Learning Instruction-Following Policies through Open-Ended Instruction Relabeling
  with Large Language Models
arxiv_id: '2506.20061'
source_url: https://arxiv.org/abs/2506.20061
tags:
- instructions
- instruction
- learning
- reward
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OIR, a novel approach that uses large language
  models (LLMs) to automatically generate open-ended instructions from agent trajectories
  in reinforcement learning. By retrospectively relabeling unsuccessful trajectories
  with meaningful subtasks, OIR enriches training data without requiring human annotations.
---

# Learning Instruction-Following Policies through Open-Ended Instruction Relabeling with Large Language Models

## Quick Facts
- **arXiv ID**: 2506.20061
- **Source URL**: https://arxiv.org/abs/2506.20061
- **Reference count**: 35
- **Primary result**: OIR improves sample efficiency, instruction coverage, and policy performance in instruction-following RL by using LLMs to relabel unsuccessful trajectories with meaningful subtasks.

## Executive Summary
This paper introduces OIR, a novel approach that uses large language models (LLMs) to automatically generate open-ended instructions from agent trajectories in reinforcement learning. By retrospectively relabeling unsuccessful trajectories with meaningful subtasks, OIR enriches training data without requiring human annotations. The method addresses challenges in instruction-following RL, such as sparse rewards and reliance on human-labeled datasets. Evaluated on the Craftax environment, OIR significantly improves sample efficiency, instruction coverage, and policy performance compared to baselines. It achieves higher success rates and completes more tasks, demonstrating superior generalization to unseen instructions. The approach leverages LLM-generated semantic rewards and a prioritized instruction buffer to focus learning on challenging tasks. Overall, OIR effectively transforms sparse, unsuccessful trajectories into valuable learning experiences, advancing instruction-following RL capabilities.

## Method Summary
OIR employs LLMs to relabel unsuccessful trajectories by identifying semantically relevant subtasks the agent implicitly accomplished. The method converts failed trajectories into informative training samples by generating candidate instructions via LLM prompting, computing cosine-similarity rewards between state transitions and instructions using shared embedding spaces, and maintaining a prioritized instruction replay buffer that focuses on learning-boundary tasks. The policy learns through off-policy RL (PQN backbone) using these relabeled instructions as goals, enabling training without environment-provided rewards or human annotations.

## Key Results
- OIR achieves higher success rates and completes more tasks compared to baselines in the Craftax environment
- The method demonstrates superior generalization to unseen instructions through effective open-ended instruction relabeling
- Prioritized instruction buffer with learning-boundary focus significantly improves sample efficiency and policy performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based hindsight instruction relabeling converts failed trajectories into informative training samples by identifying semantically relevant subtasks the agent implicitly accomplished.
- Mechanism: The LLM receives a temporally-structured prompt describing state-action sequences, then generates K candidate instructions that explain observed behavior. These instructions serve as alternative labels for the same trajectory, enabling learning from episodes that failed under their original instructions.
- Core assumption: The LLM can accurately infer meaningful subtasks from trajectory text descriptions without access to environment dynamics or ground-truth task structure.
- Evidence anchors:
  - [abstract] "Our core idea is to employ LLMs to relabel unsuccessful trajectories by identifying meaningful subtasks the agent has implicitly accomplished, thereby enriching the agent's training data."
  - [Section 4.1] Equation 4: "{i_e,k}^K_k=1 ∼ L(prompt(τ_e))" formalizes instruction generation; Section 4.1 notes "each trajectory yields a set of K candidate instructions suitable for the corresponding behavior."
  - [corpus] Limited direct corpus support; related work SAC-GLAM (arXiv:2410.12481) combines hindsight relabeling with RL for LLM agents, suggesting cross-validation of the relabeling concept in agent settings.
- Break condition: If LLM generates instructions unrelated to actual trajectory content (hallucination), or if trajectory text representations fail to capture task-relevant state changes, relabeled instructions will provide misleading reward signals.

### Mechanism 2
- Claim: Embedding-based cosine similarity provides a semantic reward signal that enables learning without environment-provided rewards.
- Mechanism: At each timestep, the reward r_t(o_t, i) = cosim(f_state(o_t, a_t, o_{t+1}), f_instr(i)) measures semantic alignment between state transitions and instructions. Episodes terminate when r_t exceeds threshold δ, indicating instruction completion.
- Core assumption: The joint embedding space of f_state and f_instr meaningfully captures semantic correspondence between physical state changes and natural language instructions.
- Evidence anchors:
  - [Section 4.2] Equation 8 defines the reward function; "An episode is deemed successful the first time the reward exceeds a predefined threshold δ."
  - [Figure 4] Ablation shows threshold δ significantly impacts learning: lower thresholds yield faster early learning but potentially misleading guidance; higher thresholds ensure semantically clearer signals.
  - [corpus] CAST (arXiv:2508.13446) explores counterfactual labels for vision-language-action models, suggesting embedding alignment challenges persist across instruction-following architectures.
- Break condition: If the embedding space fails to align semantically meaningful state changes with corresponding instructions (e.g., "defeat zombie" embeds similarly to "approach zombie"), rewards become uninformative or misleading.

### Mechanism 3
- Claim: Prioritized instruction replay buffer maintains curriculum focus on learning-boundary tasks while preserving diversity.
- Mechanism: Instructions are categorized by empirical return into Learning-boundary (τ_low < R̄ ≤ τ_high), Failing (R̄ ≤ τ_low), and Mastered (R̄ > τ_high). The buffer uses round-robin eviction prioritizing learning-boundary instructions, with uniform sampling during environment resets.
- Core assumption: Tasks at the learning boundary provide the highest information gain for policy improvement.
- Evidence anchors:
  - [Section 4.3] Equation 9-10 formalize priority ordering; "Instructions in the learning–boundary category are considered most valuable because they reveal the agent's current frontier of competence."
  - [Section 4.3] "This eviction-based replay mechanism balances exploration... and exploitation..., leading to faster and more robust policy improvement."
  - [corpus] No direct corpus validation of this specific prioritization scheme; assumes transfer from Prioritized Level Replay (PLR) literature.
- Break condition: If buffer capacity is too small relative to instruction diversity, infrequently sampled but important tasks may be evicted, causing forgetting. The paper acknowledges this limitation for instructions like "Wake Up."

## Foundational Learning

- **Concept: Hindsight Experience Replay (HER)**
  - Why needed here: OIR extends HER from state-based goals to open-ended natural language instructions. Without understanding HER's core insight—learning from achieved states rather than intended goals—the rationale for LLM relabeling is unclear.
  - Quick check question: Given a failed trajectory intended for goal G, what alternative goal would HER assign, and how does OIR's approach differ?

- **Concept: Instruction-Conditioned MDPs**
  - Why needed here: The paper formalizes instruction-following as an MDP extended with instruction space I. The policy π_θ(a_t | s_t, f_instr(i)) conditions on both state and instruction embeddings.
  - Quick check question: How does the instruction enter the policy's decision process, and what happens when the instruction embedding is uninformative?

- **Concept: Semantic Embedding Spaces (e.g., SBERT)**
  - Why needed here: The reward function relies on cosine similarity between f_state and f_instr embeddings. Understanding that these encode semantic meaning into shared vector space is essential.
  - Quick check question: If f_instr("collect wood") and f_instr("chop tree") have high cosine similarity but f_instr("defeat zombie") is distant, what does this imply for reward signal quality across tasks?

## Architecture Onboarding

- **Component map:**
  Environment → Policy π_θ → Trajectories τ_e
                              ↓
  Text Conversion h(τ_e) → LLM Prompt → K Candidate Instructions
                              ↓
                    Cosine Similarity Reward (Eq. 8)
                              ↓
                    Prioritized Instruction Buffer B
                              ↓
                    Uniform Sampling → Environment Reset

- **Critical path:**
  1. Trajectory collection with current policy
  2. LLM prompt construction from trajectory text
  3. Candidate instruction generation (K per trajectory)
  4. Reward computation via embedding similarity
  5. Buffer update with priority eviction
  6. Policy update using off-policy RL (PQN backbone)

- **Design tradeoffs:**
  - **Threshold δ:** Lower = denser rewards, faster early learning, risk of rewarding partial completion. Higher = sparser rewards, slower convergence, clearer semantic signals. Paper uses δ=0.9.
  - **Buffer capacity vs. forgetting:** Small buffer (B=10 in paper) focuses curriculum but risks evicting infrequent tasks. Larger buffer preserves diversity but dilutes learning-boundary focus.
  - **K candidates:** More candidates increase instruction diversity but raise LLM costs and may introduce noise.

- **Failure signatures:**
  - Policy plateaus on specific tasks while improving others → buffer eviction removing those task instructions.
  - High reward but low ground-truth success → threshold δ too low, rewarding partial/unrelated behaviors.
  - LLM generates generic/exploration instructions → prompt constraints not enforced; verify prompt excludes "move," "explore," etc.
  - Embedding similarity uniform across instructions → encoder f_state or f_instr not discriminative; check encoder quality.

- **First 3 experiments:**
  1. **Sanity check:** Run OIR with ground-truth reward (PQN baseline) to establish performance upper bound. Compare against OIR with embedding reward to quantify reward-function gap.
  2. **Threshold sweep:** Test δ ∈ {0.3, 0.5, 0.7, 0.9} on a subset of instructions. Plot learning curves to identify the point where sparse rewards begin to hurt convergence vs. where dense rewards become misleading.
  3. **Buffer ablation:** Replace prioritized eviction with random eviction and measure: (a) instruction diversity over training, (b) performance on learning-boundary vs. mastered tasks, (c) forgetting rate on infrequent instructions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adaptive or curriculum-based strategies for the instruction replay buffer mitigate the forgetting of infrequent tasks while maintaining sample efficiency?
- Basis in paper: [explicit] The Conclusion states that the "limited capacity of our instruction buffer can lead to the forgetting of infrequently sampled tasks" and suggests future work on "improved buffer management."
- Why unresolved: The current implementation uses a static, fixed-size buffer with round-robin eviction, which empirically causes performance drops on rare instructions (e.g., "Wake Up") as training progresses.
- What evidence would resolve it: Experiments comparing the current fixed-capacity eviction against dynamic buffer sizing or frequency-aware retention strategies, showing stable performance on rare tasks over long training runs.

### Open Question 2
- Question: Is it possible to dynamically adjust the cosine-similarity reward threshold ($\delta$) to optimize the trade-off between early exploration and final policy precision?
- Basis in paper: [inferred] The ablation study (Figure 4) shows that lower $\delta$ values accelerate early learning but may misguide the policy, while higher values are too sparse initially. The authors note that effectiveness "varies considerably" with this hyperparameter.
- Why unresolved: The paper relies on a fixed threshold ($\delta=0.9$), which forces a manual trade-off between dense, potentially noisy signals and sparse, precise signals.
- What evidence would resolve it: Results from an implementation where $\delta$ is annealed or adjusted based on learning progress, demonstrating superior convergence speed without the loss of semantic precision seen in low-threshold fixed baselines.

### Open Question 3
- Question: How robust is the policy learning when the LLM generates hallucinated or semantically irrelevant instructions, and can filtering mechanisms automatically correct this?
- Basis in paper: [explicit] The Limitations section notes the method "depends on the quality of instructions generated by pretrained LLMs, potentially inheriting their biases or inaccuracies," and suggests future work on "instruction filtering."
- Why unresolved: While the authors use cosine similarity for rewards, they do not analyze the negative impact of low-quality or hallucinated LLM outputs on the instruction buffer or policy convergence.
- What evidence would resolve it: An analysis quantifying the rate of LLM hallucinations in the instruction buffer and experiments showing that automated quality filtering (e.g., self-consistency checks) improves final policy performance.

## Limitations

- Data efficiency concerns: OIR requires running Qwen3-8B for each trajectory, introducing computational overhead that scales with trajectory volume
- Buffer capacity constraints: The prioritized instruction buffer with capacity B=10 creates a fundamental tradeoff between curriculum concentration and instruction diversity, risking forgetting of rarely-used but important tasks
- Generalization boundary: The method's effectiveness is evaluated on Craftax-Classic, and performance may degrade in environments with more complex dynamics, partial observability, or continuous state spaces

## Confidence

- **High confidence**: The core mechanism of using LLMs for hindsight instruction relabeling is technically sound and well-grounded in existing HER literature. The reward function formulation using cosine similarity between state and instruction embeddings is clearly specified and implementable.
- **Medium confidence**: The prioritized buffer eviction strategy shows theoretical merit but lacks extensive empirical validation across diverse environments. The specific threshold values (δ=0.9, τ_low, τ_high) appear tuned to Craftax and may not generalize.
- **Low confidence**: The claim that OIR enables learning "from scratch" without human-labeled data is somewhat overstated, as the approach still requires pretrained SBERT encoders and LLM capabilities that were trained on human-curated data.

## Next Checks

1. **Environment transfer test**: Evaluate OIR on a continuous-control instruction-following task (e.g., Meta-World or RoboCup) to assess performance in environments with more complex dynamics and state representations.

2. **Memory pressure analysis**: Systematically vary the instruction buffer capacity B from 5 to 50 and measure: (a) learning speed on boundary tasks, (b) success rate on infrequent instructions, (c) computational overhead from increased buffer operations.

3. **Reward function ablation**: Replace the cosine-similarity reward with ground-truth environment rewards (when available) on a subset of instructions, then compare learning curves to quantify the performance gap introduced by the semantic embedding-based reward.