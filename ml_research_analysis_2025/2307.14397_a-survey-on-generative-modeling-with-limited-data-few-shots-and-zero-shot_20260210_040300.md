---
ver: rpa2
title: A Survey on Generative Modeling with Limited Data, Few Shots, and Zero Shot
arxiv_id: '2307.14397'
source_url: https://arxiv.org/abs/2307.14397
tags:
- learning
- image
- data
- generation
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews Generative Modeling under Data
  Constraint (GM-DC), focusing on limited-data, few-shot, and zero-shot settings.
  It introduces novel taxonomies for GM-DC tasks (e.g., unconditional/conditional
  generation, cross-domain adaptation, subject-driven modeling) and methodological
  approaches (e.g., transfer learning, data augmentation, meta-learning, frequency-aware
  modeling).
---

# A Survey on Generative Modeling with Limited Data, Few Shots, and Zero Shot

## Quick Facts
- **arXiv ID:** 2307.14397
- **Source URL:** https://arxiv.org/abs/2307.14397
- **Reference count:** 40
- **Primary result:** Comprehensive survey of generative modeling under data constraints, introducing novel taxonomies and analyzing over 230 papers across limited-data, few-shot, and zero-shot settings.

## Executive Summary
This survey systematically reviews Generative Modeling under Data Constraint (GM-DC), categorizing tasks (unconditional/conditional generation, cross-domain adaptation, subject-driven modeling) and methodological approaches (transfer learning, data augmentation, meta-learning, frequency-aware modeling). The authors analyze 230+ papers to identify key challenges including overfitting, frequency bias, and incompatible knowledge transfer. They propose that adaptation-aware kernel modulation, semantic offset alignment in multimodal space, and internal patch statistics learning are promising mechanisms for addressing data scarcity. The study serves as a practical roadmap for advancing generative modeling under limited data constraints while highlighting critical open questions in cross-domain transfer, data selection strategies, and zero-shot adaptation for evolving concepts.

## Method Summary
The survey conducts a comprehensive analysis of GM-DC approaches through systematic categorization and extensive literature review. It establishes evaluation benchmarks using StyleGAN-2 pretrained on FFHQ (70k images) as source, with 10-shot subsets from FFHQ-Baby (2.5k total) and AFHQ-Cat (5k total) as target domains. Performance is measured via FID (using clean-FID library) and Intra-LPIPS (diversity), with 5000 generated samples evaluated against entire target datasets. Key adaptation methods analyzed include AdAM (adaptation-aware kernel modulation), OKM (offset-aware kernel modulation), and RICK. The survey emphasizes that method effectiveness depends critically on domain proximity and sample selection, with FID variance ranging from 49.9 to 90.0 based on which 10 images are chosen.

## Key Results
- The survey identifies **kernel modulation** and **semantic offset alignment** as two primary mechanisms for effective few-shot and zero-shot adaptation respectively
- **Domain proximity** is established as a critical constraint, with distant domain adaptation (e.g., faces to flowers) frequently resulting in "incompatible knowledge transfer" artifacts
- **Frequency-aware modeling** and **adaptive data augmentation** (like ADA) are highlighted as essential for preventing overfitting and preserving high-frequency details in limited data regimes
- The analysis reveals that **data selection strategy** has as much impact on performance as algorithmic improvements, with different 10-shot subsets producing FID variance up to 40 points

## Why This Works (Mechanism)

### Mechanism 1: Adaptation-Aware Kernel Modulation
Selectively tuning model parameters based on their estimated importance to the target domain may prevent destruction of useful source knowledge during few-shot transfer. A probing step first estimates the "importance" of convolutional kernels (e.g., using Fisher Information) relative to the few target samples. High-importance kernels are preserved or modulated with lightweight weights, while low-importance ones are fine-tuned. This theoretically balances plasticity (learning new concepts) and stability (retaining general features). The paper assumes that parameter importance can be reliably estimated from very few samples (10-shot) and that "unimportant" parameters in the source domain are the correct substrate for target adaptation. Evidence anchors include AdAM and OKM methods using probing and kernel modulation, with analysis suggesting successful methods should employ "adaptive or data-aware regularization schemes". Break condition: If the source and target domains are too distant (low proximity), the estimated "importance" may be meaningless, leading to negative transfer or "incompatible knowledge" (e.g., transferring "glasses" to "flowers").

### Mechanism 2: Semantic Offset Alignment in Multimodal Space
Leveraging the geometric alignment between text and image embeddings in pre-trained models (like CLIP) may enable zero-shot domain adaptation without target samples. The method calculates a "direction vector" in the embedding space between a source text prompt (e.g., "Photo") and a target text prompt (e.g., "Sketch"). It then forces the generator's output distribution to shift in a parallel direction in image space, aligning the visual shift with the textual semantic shift. Assumes the joint embedding space has sufficient linearity and alignment so that the vector difference between "Photo" and "Sketch" text corresponds to the required visual transformation. Evidence anchors include StyleGAN-NADA using directional loss to align $\Delta T$ and $\Delta I$, with discussion of "Incompatible Knowledge Transfer" in remote domains. Break condition: Fails if the target concept is semantically distant or complex, causing "offset misalignment" where the text direction does not map to the required visual transformation.

### Mechanism 3: Internal Patch Statistics for Generation
Learning the distribution of internal patches within a single image may allow for generation when no external training data is available (One-shot/IGM). Instead of learning global semantics from a dataset, the model treats the single image as a collection of patches (texture/structure). It learns the statistics of these local patches and uses a pyramid/coarse-to-fine approach to generate new images that share the same internal statistics but differ in global layout. Assumes that the single image contains sufficient internal recurrence and diversity (self-similarity) to define a valid generative distribution. Evidence anchors include SinGAN and SinDDM training on patch statistics, with discussion of IGM (Internal patch distribution Generative Modeling). Break condition: Fails if the single image lacks diverse textures or contains a complex, unique global structure that cannot be reconstructed by recombining local patches.

## Foundational Learning

- **Concept: Domain Proximity**
  - Why needed here: The success of Transfer Learning (the dominant approach in the survey) is strictly conditioned on the semantic "distance" between the Source Domain ($D_s$) and Target Domain ($D_t$).
  - Quick check question: If you adapt a generator trained on "Human Faces" ($D_s$) to "Flowers" ($D_t$), will standard fine-tuning work, or will it result in "incompatible knowledge" (e.g., flowers with faces)?

- **Concept: Spectral Bias**
  - Why needed here: Generative models naturally prioritize low-frequency components (smooth structures) over high-frequency details. Understanding this bias is necessary to diagnose why limited-data models produce blurry outputs or fail to capture fine textures.
  - Quick check question: Why does a standard GAN trained on limited data struggle to reproduce the edges of a petal, and which approach (e.g., frequency-aware discriminator) directly targets this?

- **Concept: The Overfitting/Mode-Collapse Trade-off**
  - Why needed here: In limited data regimes (10-100 samples), the discriminator often "memorizes" the training set, leading to mode collapse where the generator only reproduces exact training samples.
  - Quick check question: How does "Adaptive Discriminator Augmentation" (ADA) dynamically prevent the discriminator from becoming overconfident during training?

## Architecture Onboarding

- **Component map:** Source Backbone -> Modulator/Adapter -> Probe/Estimator -> Discriminator/Critic -> Prior Preservation Loss
- **Critical path:**
  1. **Probing:** Analyze the pre-trained source generator to identify which layers/kernels are relevant to the target concept.
  2. **Modulation Design:** Insert trainable parameters (modulation layers) or select a subset of weights for fine-tuning based on the probe.
  3. **Sample Selection:** Carefully curate the few-shot target samples, as their choice significantly impacts FID.
  4. **Training:** Run the adaptation loop with constraints (e.g., directional loss, regularization) to guide the shift from $D_s$ to $D_t$.
- **Design tradeoffs:**
  - **Full Fine-tuning vs. Modulation:** Full fine-tuning offers higher plasticity but risks catastrophic forgetting; Modulation preserves source knowledge but may underfit remote target domains.
  - **Data Augmentation Strength:** Too much augmentation leaks into generation (e.g., generating rotated flowers); too little leads to overfitting. Adaptive strategies (like ADA) are preferred.
- **Failure signatures:**
  - **Incompatible Artifacts:** Generated images show features from the source domain (e.g., human ears on a cat) due to poor source-target alignment.
  - **Frequency Loss:** Outputs look smooth or "painterly" in the wrong way, lacking high-frequency texture details.
  - **Mode Collapse:** Generated samples are nearly identical to the few training samples, indicating the discriminator has memorized the data.
- **First 3 experiments:**
  1. **Baseline Adaptation:** Fine-tune a standard StyleGAN2 on a 10-shot dataset (e.g., FFHQ-Baby) to establish a baseline for mode collapse and FID.
  2. **Augmented Baseline:** Apply an adaptive augmentation technique (e.g., ADA) to the same 10-shot setup to measure the improvement in generalization.
  3. **Cross-Domain Test:** Attempt to adapt a "Face" generator to a "Church" dataset (distant domain) using a modulation method (e.g., AdAM) to test robustness against domain gap.

## Open Questions the Paper Calls Out

### Open Question 1
How can knowledge transfer mechanisms be improved to effectively model target domains that are semantically distant or remote from the source domain (e.g., adapting from Human Faces to Flowers)?
- Basis in paper: [Explicit] The authors identify this as a critical gap in Section 7.3.3, noting that current methods often fail or produce "incompatible knowledge transfer" when domain gaps are large.
- Why unresolved: Current adaptation algorithms prioritize knowledge preservation from the source, often resulting in artifacts (e.g., transferring hats to flowers) and low synthesis quality when domains diverge significantly.
- What evidence would resolve it: Development of adaptation methods that achieve competitive FID scores and high semantic fidelity on cross-domain benchmarks with significant domain gaps.

### Open Question 2
What constitutes an optimal data selection or curation strategy for GM-DC, and how does sample choice quantitatively affect model performance?
- Basis in paper: [Explicit] Section 7.3.5 highlights that data-centric approaches are "relatively overlooked," while Figure 7 demonstrates that different random sets of 10-shot data yield drastically different FID results.
- Why unresolved: Current research focuses almost exclusively on algorithmic improvements (training procedures) rather than the quality or characteristics of the limited input data.
- What evidence would resolve it: A systematic study or framework that defines "informative" samples for GM-DC and demonstrates improved stability and generation quality through curated data selection.

### Open Question 3
Can zero-shot generative models be grounded to synthesize realistic images for evolving semantic concepts or events that post-date their pre-training data?
- Basis in paper: [Explicit] In Section 7.3.2, the authors describe the challenge of generating images for new concepts (e.g., "Mass for the Beginning of the Petrine Ministry") as a "relatively unexplored and challenging area."
- Why unresolved: Models typically cannot generate concepts not present in their massive pre-training datasets without explicit fine-tuning or retraining.
- What evidence would resolve it: The successful implementation of a mechanism (e.g., continual learning or semantic editing) that allows a frozen or lightly-modified model to generate high-fidelity images of novel, evolving concepts.

## Limitations
- The survey's taxonomy may not fully capture emerging hybrid approaches that combine multiple methodological strategies
- Effectiveness of many proposed mechanisms remains primarily validated through their original papers rather than comprehensive cross-method comparisons
- Specific quantitative comparisons between methods are affected by high variance in few-shot settings due to sample selection sensitivity

## Confidence
- **High Confidence:** The categorization framework (task taxonomy, approach taxonomy, method taxonomy) and identification of key challenges are well-supported by the extensive corpus analysis of 230+ papers
- **Medium Confidence:** The analysis of task-approach-method interactions and identification of promising future directions are reasonable extrapolations but may evolve as new methodologies emerge
- **Low Confidence:** Specific quantitative comparisons between methods (particularly in Table 6) are affected by high variance in few-shot settings due to sample selection sensitivity

## Next Checks
1. **Sample Selection Sensitivity:** Reproduce the 10-shot adaptation experiments with multiple random subsets to quantify the variance reported in Figure 7 and validate whether performance differences between methods remain consistent across different data selections.
2. **Cross-Domain Transfer Robustness:** Test kernel modulation approaches (AdAM, OKM) on increasingly distant domain pairs (e.g., faces→animals→abstract art) to empirically validate the "domain proximity" hypothesis and identify the breaking point for incompatible knowledge transfer.
3. **Frequency-Aware Augmentation Impact:** Implement and compare frequency-aware augmentation strategies against standard augmentation methods in the 10-100 shot regime to measure their specific impact on high-frequency detail preservation and overall FID scores.