---
ver: rpa2
title: 'MAGIC: Near-Optimal Data Attribution for Deep Learning'
arxiv_id: '2504.16430'
source_url: https://arxiv.org/abs/2504.16430
tags:
- data
- training
- learning
- attribution
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MAGIC, a new method for data attribution
  in deep learning that overcomes limitations of existing approaches. While prior
  work could only weakly predict how training data affects model predictions, MAGIC
  achieves near-perfect predictions by exactly calculating the influence function
  using recent advances in metagradient computation.
---

# MAGIC: Near-Optimal Data Attribution for Deep Learning

## Quick Facts
- arXiv ID: 2504.16430
- Source URL: https://arxiv.org/abs/2504.16430
- Authors: Andrew Ilyas; Logan Engstrom
- Reference count: 18
- Primary result: Achieves Spearman correlations up to 0.97 for data attribution vs 0.25 for existing methods

## Executive Summary
This paper introduces MAGIC, a method for data attribution that achieves near-perfect predictions of how training data affects model predictions. Unlike prior work that could only weakly predict data influence, MAGIC computes the exact influence function using metagradient computation via the REPLAY algorithm. The key innovation is focusing on "single-model" attribution where predictions are made for a specific trained model rather than averaged across variations, enabling perfect attribution in principle when all randomness sources are fixed.

## Method Summary
MAGIC computes exact influence functions by leveraging the REPLAY algorithm to backpropagate through the entire training trajectory. The method requires an iterative and smooth learning algorithm (modified with ε_root in Adam), deterministic training (fixed seeds, data order), and state checkpointing during training. For each test sample, REPLAY unrolls the training computation graph backward to accumulate contributions at each step, yielding the exact gradient of model output with respect to data weights. Predictions are made using a first-order Taylor approximation around the full training set, which is optimal locally but degrades at larger drop fractions due to curvature.

## Key Results
- Achieves Spearman correlations of 0.97 vs 0.25 for existing methods (TRAK, EK-FAC) on CIFAR-10
- Near-perfect linear data modeling score (LDS) at 1-5% drop fractions, declining predictably at larger drops
- Successfully scales to larger models including GPT-2 and Gemma-2B with comparable performance
- Cost scales linearly with test samples (O(N·n)), winning when n ≪ N but losing for large n

## Why This Works (Mechanism)

### Mechanism 1: Exact Influence Function via Metagradient Computation
- Claim: MAGIC computes the exact influence function (gradient of model output with respect to data weights) rather than approximating it, enabling near-perfect prediction of data effects.
- Mechanism: The method applies the REPLAY algorithm to unroll the training computation graph and apply chain rule backward through all T training steps, accumulating contributions at each step.
- Core assumption: The learning algorithm is iterative (defined by step-wise updates) and smooth (small perturbations to data weights cause small changes to the influence function gradient).
- Break condition: If the learning algorithm is not smooth (e.g., has discrete non-differentiable operations), the influence function may not be well-defined or may change unpredictably with ε perturbations.

### Mechanism 2: Single-Model Deterministic Attribution Setting
- Claim: By fixing all sources of randomness (initialization, data ordering), the irreducible error term from training variance becomes zero, enabling perfect attribution in principle.
- Mechanism: Standard data attribution error decomposes as E[(f̂(w) - f(w))²] = (f̂(w) - E[f(w)])² + E[(f(w) - E[f(w)])²]. The single-model setting enforces deterministic A and φ, so Var[φ(A(w))] = 0, eliminating the second term.
- Core assumption: The specific questions users want to answer concern individual trained models rather than algorithm-averaged behavior; or averaging single-model predictions across seeds suffices for standard attribution.
- Break condition: If downstream applications require understanding algorithm-level behavior without access to multiple seeds, single-model predictions may not generalize.

### Mechanism 3: First-Order Taylor Approximation for Local Prediction
- Claim: Near the full training set (small drop fractions), a linear Taylor approximation around w = 1_n accurately predicts model output changes.
- Mechanism: The predictor f̂(w) = f(1_n) + (∂f(w)/∂w|_{w=1_n})^T(w - 1_n) uses the exact influence function as the linear coefficient. This is optimal among linear predictors in a local ball around the full dataset.
- Core assumption: The model output function f(w) is locally smooth in data weight space; curvature becomes significant only at larger drop fractions.
- Break condition: Performance degrades predictably as drop fraction increases (Figure 4 shows LDS declining from ~1.0 at 1% drop to ~0.75-0.85 at 20% drop).

## Foundational Learning

- **Influence Functions (Classical)**: Why needed here: MAGIC's core innovation is computing the exact influence function for deep networks; understanding the classical definition (sensitivity of estimates to infinitesimal data perturbations) is prerequisite to appreciating why approximations in prior work fail. Quick check question: Given a convex loss minimizer θ* = argmin Σ w_i · ℓ(z_i; θ), what does ∂θ*/∂w_i represent?

- **Metagradients / Differentiation Through Optimization**: Why needed here: MAGIC relies on backpropagating through the entire training trajectory to compute exact gradients with respect to pre-training design choices (data weights). Quick check question: If training involves T SGD steps, how many intermediate states must be stored or recomputed to compute ∇_w θ_T?

- **Taylor Approximation Error Analysis**: Why needed here: Understanding why predictions degrade at higher drop fractions requires grasping how curvature (second-order terms) causes linear approximations to diverge from true function values. Quick check question: For a function f with bounded second derivative M, what is the worst-case error of a first-order Taylor approximation at distance δ from the expansion point?

## Architecture Onboarding

- **Component map**: Training State Checkpointing -> REPLAY Backward Pass -> Influence Function Aggregation
- **Critical path**: 1. Ensure learning algorithm is smooth (may require modifications like ε_root in Adam, warmup pretraining) 2. Fix all randomness sources (seed, data order) 3. Run training with state checkpointing 4. For each test sample of interest, run REPLAY backward pass (cost: ~3-5× forward training per sample)
- **Design tradeoffs**: Memory vs. Compute: REPLAY requires log(T) memory with checkpointing but T + T·log(T) total training steps in compute; full trajectory storage avoids recomputation but requires O(T) memory. Test Sample Scalability: Cost scales linearly with number of test samples n (O(N·n) total), whereas TRAK/EK-FAC have fixed upfront cost; MAGIC wins for n ≪ N but loses for large n. Smoothness Constraints: Standard training may not be smooth; modifications (ε_root > 0 in Adam, pretraining warmup) may be needed but change the learning algorithm.
- **Failure signatures**: Non-smooth training: Gradients explode or become undefined; check if loss/accuracy curves show discrete jumps. Large drop fractions: Predictions correlate but have large absolute error; Taylor approximation is hitting curvature. Metagradient blowup: Backward pass produces NaN/Inf; likely numerical instability in optimizer backward (try larger ε_root).
- **First 3 experiments**: 1. Reproduce CIFAR-10 baseline: Train ResNet-9 with fixed seed and deterministic ordering; compute MAGIC for 5-10 test samples with 1% drop fraction; verify Spearman ρ > 0.9 against ground truth retraining 2. Ablate smoothness: Train same model without ε_root modification; compare metagradient magnitude and prediction quality to assess smoothness impact 3. Scale test: Measure wall-clock time for n = 1, 10, 50 test samples; identify crossover point where MAGIC becomes more expensive than TRAK for your infrastructure

## Open Questions the Paper Calls Out
- Can metagradient computation be efficiently shared or batched across multiple test samples to reduce MAGIC's linear scaling cost?
- Can higher-order Taylor approximations extend MAGIC's accuracy to larger training data modifications?
- How can smooth counterparts be systematically identified or constructed for arbitrary non-smooth learning algorithms?
- What downstream applications become tractable with near-perfect single-model data attribution?

## Limitations
- Requires learning algorithms to be both iterative and smooth, which may not hold for standard training routines without modifications
- Computational cost scales linearly with the number of test samples, making large-scale attribution expensive
- REPLAY implementation details are delegated to external work [EIC+25] and not fully specified in this paper

## Confidence
- **High**: Mathematical formulation of MAGIC, REPLAY algorithm, and Taylor approximation framework
- **Medium**: Experimental results showing near-perfect correlations on small-scale tasks
- **Low**: Claims about practical scalability and efficiency relative to existing methods

## Next Checks
1. **Ablate Smoothness Modifications**: Compare MAGIC predictions using standard Adam (ε = 10⁻⁸) versus ε_root > 0 across different pretraining durations to quantify the smoothness requirement empirically.
2. **Scale-Up Cost Analysis**: Measure wall-clock time and memory usage for 100+ test samples on CIFAR-10, and compare directly with TRAK/EK-FAC implementations under identical hardware constraints.
3. **Generalization Across Tasks**: Apply MAGIC to a non-vision task (e.g., text classification with BERT) to verify that the method generalizes beyond the specific models and datasets presented.