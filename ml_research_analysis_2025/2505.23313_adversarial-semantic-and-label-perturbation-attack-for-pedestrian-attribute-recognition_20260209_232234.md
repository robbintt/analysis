---
ver: rpa2
title: Adversarial Semantic and Label Perturbation Attack for Pedestrian Attribute
  Recognition
arxiv_id: '2505.23313'
source_url: https://arxiv.org/abs/2505.23313
tags:
- adversarial
- attack
- attribute
- pedestrian
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper pioneers adversarial attack and defense research for
  pedestrian attribute recognition (PAR), addressing the task's vulnerability to perturbations.
  The proposed method, termed ASL-PAR, employs semantic and label perturbation strategies
  based on the PromptPAR framework.
---

# Adversarial Semantic and Label Perturbation Attack for Pedestrian Attribute Recognition

## Quick Facts
- arXiv ID: 2505.23313
- Source URL: https://arxiv.org/abs/2505.23313
- Reference count: 40
- One-line primary result: Adversarial attack (ASL-PAR) degrades PromptPAR performance from 88.76 to 48.25 mA on PETA while maintaining invisibility

## Executive Summary
This paper introduces ASL-PAR, the first adversarial attack specifically targeting pedestrian attribute recognition (PAR) models. The attack combines label perturbation (shifting attributes within semantically-grouped body parts) and semantic perturbation (misaligning visual-textual features in CLIP space) to generate universal adversarial noise. A defense strategy using noise filtering and prompt fine-tuning partially restores performance. Experiments demonstrate strong effectiveness on digital datasets (PETA, PA100K, MSP60K, RAPv2) and physical-world settings, though cross-model transferability remains limited.

## Method Summary
ASL-PAR generates universal adversarial noise by optimizing against perturbed labels and aligning visual-textual features in CLIP embedding space. The attack freezes a pre-trained PromptPAR model and updates a shared noise pattern across all samples using gradient descent on a combined loss (L_cse + 0.5·L_GL) with L∞ constraint (≤10/255). Label perturbation shifts attributes within pedestrian part groups (gender, head, upper/lower body, foot, hand), while semantic perturbation maximizes misalignment between corrupted visual features and semantically-shifted text features. A defense strategy combines convolutional noise filtering with learnable text prompts to partially restore performance.

## Key Results
- Attack reduces PETA mA from 88.76 to 48.25 using combined semantic and label perturbation
- Cross-dataset transferability achieved (PETA→PA100K: 52.10 mA)
- Physical-world patch-level attacks validated on real images
- Defense partially restores performance to 85.82 mA on attacked models
- Global attacks more effective than patch-level in digital settings

## Why This Works (Mechanism)

### Mechanism 1: Label Perturbation via Part-Based Semantic Shifting
Shifting attribute labels within semantically-grouped body parts generates adversarial noise that misleads PAR models more effectively than random perturbation. Attributes are partitioned into groups (gender, head, upper body, lower body, foot, hand), and true labels are shifted to other attributes in the same group. The adversarial noise η is optimized to make the model predict these perturbed labels via gradient descent on L_cse. Intra-part attribute swaps remain within plausible prediction error ranges, making attacks harder to detect while reducing the optimization burden on noise learning.

### Mechanism 2: Semantic Perturbation via CLIP Feature Space Misalignment
Optimizing adversarial noise to align visual-textual features with perturbed labels in CLIP embedding space amplifies attack potency. After label perturbation, the Global-Local similarity aggregator computes P_GL between image and text features. The noise is optimized to maximize alignment between corrupted image features and semantically-shifted text features via L_GL loss, progressively disrupting the multimodal fusion pipeline. CLIP's joint vision-language space can be systematically corrupted by gradient-based noise optimization that forces visual embeddings toward semantically-shifted text representations.

### Mechanism 3: Defense via Noise Filtering and Prompt Re-alignment
Combining input-level noise filtering with learnable text prompts partially restores performance under attack. A convolutional filter learns to denoise adversarial inputs. Since filtering is imperfect, learnable text prompts are fine-tuned to re-align textual semantic features with corrupted visual embeddings, compensating for residual noise. Adversarial noise has learnable spatial patterns, and text prompts can be adjusted to counteract shifted visual-textual alignment.

## Foundational Learning

- **Multi-label classification with attribute correlations**: PAR is a multi-label task where attributes co-occur (e.g., "male" correlates with certain clothing). Label perturbation exploits these intra-part correlations to create plausible but incorrect predictions. Quick check: Why would perturbing "black shirt" to "white shirt" be more effective than perturbing it to "female"?

- **CLIP vision-language joint embeddings**: The semantic perturbation attack exploits CLIP's shared embedding space. Understanding how images and text align as vectors is essential to grasp how noise corrupts this alignment. Quick check: What happens to the cosine similarity between an image embedding and its correct text embedding when noise is optimized to align with a perturbed text embedding?

- **Universal adversarial perturbations**: ASL-PAR generates a single noise pattern shared across all images in a dataset, trading per-image potency for black-box deployability. Quick check: What advantage does a universal perturbation offer over instance-specific attacks in real-world deployment?

## Architecture Onboarding

- **Component map**: CLIP Visual Encoder (ViT-L/14) -> CLIP Text Encoder -> Multi-modal Transformer -> Global-Local Similarity Aggregator -> FFN Classification Head -> Attack Module (optimizes universal noise η) -> Defense Filter (conv filter + learnable text prompts)

- **Critical path**: I_noisy = I + η → CLIP Visual Encoder → F_img → [F_img, F_text] → Multi-modal Transformer → F^img_fusion, F^text_fusion → FFN → Prob (sigmoid outputs) → Attack: Compute L_cse + αL_GL, backprop to update η (not model weights) → Defense: Filter(I_noisy) + fine-tune text prompts on noisy data

- **Design tradeoffs**: Global vs. patch-level noise: Global achieves ~48.25 mA but impractical physically; patch-level (3×30×30) works in real-world but less effective (52.16 vs. 48.25 mA). L∞ constraint (10/255): Ensures invisibility; unconstrained achieves 47.27 mA vs. constrained 48.25 mA—small potency loss for stealth. Universal vs. per-image: Single noise enables black-box deployment; cross-dataset transfer works (PETA→PA100K: 52.10 mA) but cross-model transfer fails.

- **Failure signatures**: Cross-model transfer collapse: PromptPAR-trained noise barely affects MambaPAR (56.75 → 56.03 mA). Abstract attribute interference: RAPv2's abstract attributes reduce attack effectiveness due to semantic overlap. Domain shift degradation: MSP60K (indoor) → PETA (mixed) attacks show weakened transfer.

- **First 3 experiments**: 1) Baseline validation: Load pre-trained PromptPAR weights, run inference on PETA test set, verify mA ≈ 88.76. 2) Attack sanity check: Initialize random noise η, optimize for 40 epochs with L = L_cse + 0.5·L_GL under L∞ ≤ 10/255, confirm mA drops to 48-52 range. 3) Component ablation: Test (label perturbation only) vs. (semantic perturbation only) vs. (both) on PETA to verify each contributes (Table III shows both required for optimal 48.25 mA).

## Open Questions the Paper Calls Out

### Open Question 1
How can defense strategies for pedestrian attribute recognition be adapted to require minimal noisy training data while maintaining high robustness? The current defense approach "relies heavily on a substantial amount of noisy data for training" and identifies developing a method that can be "efficiently applied in practice" as an important future direction. Resolution would require a defense framework achieving comparable restoration of model performance using few-shot or zero-shot learning techniques without extensive retraining on large noise datasets.

### Open Question 2
Can an adversarial perturbation strategy be developed to achieve simultaneous effectiveness across different model architectures (e.g., Transformer vs. Mamba) and datasets? The paper explicitly notes that "the ultimate attack goal, that the added adversarial noise be able to cross datasets and models simultaneously, is not achieved," specifically citing failure to transfer from PromptPAR to MambaPAR. Resolution would require demonstration of a universal adversarial perturbation generated on a Transformer-based PAR model that significantly degrades the performance of a Mamba-based or CNN-based PAR model without architecture-specific tuning.

### Open Question 3
How can semantic perturbation strategies be refined to handle datasets with complex, abstract attributes that overlap with localized body-part features? The Results section on RAPv2 discusses a performance gap attributed to the dataset containing "attributes of more abstract concepts" (e.g., occupation) that cause "unintended interference" during the semantic perturbation process. Resolution would require an enhanced attack method that explicitly models or disentangles abstract semantic attributes from spatial part attributes, resulting in improved attack performance on heterogeneous datasets like RAPv2.

## Limitations

- Cross-model transferability remains limited, with attacks trained on PromptPAR showing minimal effect on MambaPAR (56.75 → 56.03 mA)
- The defense mechanism requires substantial additional training data with adversarial examples, limiting practical deployment
- Performance degrades on datasets with predominantly abstract attributes (RAPv2) due to semantic overlap issues

## Confidence

**High Confidence**: Attack mechanism effectively degrades PromptPAR performance on same-dataset digital evaluation (PETA mA: 88.76 → 48.25); Global noise attacks achieve stronger results than patch-level attacks in digital settings; Cross-dataset transferability works between similar datasets (PETA→PA100K).

**Medium Confidence**: Physical-world patch-level attacks maintain effectiveness under real-world conditions; The defense strategy provides meaningful but incomplete protection (restores performance to ~85.82 mA); Label perturbation within semantic groups is more effective than random perturbation.

**Low Confidence**: Cross-model transferability will generalize to other PAR architectures; Performance on datasets with predominantly abstract attributes (RAPv2) represents typical behavior.

## Next Checks

1. Test the ASL-PAR attack against three additional PAR models (e.g., ViT-based, CNN-based, and Mamba-based) to quantify the generalization gap across architectural paradigms.

2. Evaluate attack effectiveness on datasets where >50% of attributes are abstract concepts (not body-part-specific) to determine if the semantic perturbation mechanism breaks down.

3. Measure defense performance as a function of adversarial training data volume to establish the data efficiency tradeoff and identify minimum requirements for practical deployment.