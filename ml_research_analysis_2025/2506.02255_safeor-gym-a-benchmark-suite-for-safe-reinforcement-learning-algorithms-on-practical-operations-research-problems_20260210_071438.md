---
ver: rpa2
title: 'SafeOR-Gym: A Benchmark Suite for Safe Reinforcement Learning Algorithms on
  Practical Operations Research Problems'
arxiv_id: '2506.02255'
source_url: https://arxiv.org/abs/2506.02255
tags:
- violations
- cost
- demand
- time
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SafeOR-Gym, a benchmark suite of nine operations
  research environments tailored for safe reinforcement learning. The environments
  capture realistic planning, scheduling, and control problems with structured constraints,
  hybrid discrete-continuous action spaces, and cost-based constraint violations.
---

# SafeOR-Gym: A Benchmark Suite for Safe Reinforcement Learning Algorithms on Practical Operations Research Problems

## Quick Facts
- arXiv ID: 2506.02255
- Source URL: https://arxiv.org/abs/2506.02255
- Reference count: 40
- Safe RL benchmark suite for OR problems with CMDP interface and mathematical optimization baselines

## Executive Summary
SafeOR-Gym introduces nine operations research environments designed to benchmark safe reinforcement learning algorithms on realistic planning, scheduling, and control problems. The environments feature structured constraints, hybrid discrete-continuous action spaces, and cost-based constraint violations integrated through the CMDP interface. Experiments reveal that while some tasks are tractable, others expose fundamental limitations in current approaches, particularly for problems involving mixed-integer decisions and nonlinear constraints. The benchmark aims to catalyze future research in safe RL for real-world decision-making problems.

## Method Summary
The benchmark evaluates five safe RL algorithms (CPO, TRPOLag, P3O, OnCRPO, DDPGLag) across nine OR environments using the Constrained Markov Decision Process interface. Environments are parameterized with deterministic case study data, and algorithms are trained using OmniSafe's Experimental Grid framework on AWS g4dn.xlarge instances. Performance is measured against Gurobi-computed optimal solutions, with "reasonable optimality" defined as <35% gap from optimal reward. Action sanitization strategies (prop, disable, none) handle infeasible actions, and evaluation uses 10-episode averages of reward and cost metrics.

## Key Results
- Current safe RL algorithms achieve reasonable optimality on simple continuous problems (InvMgmtEnv) but struggle with non-convex and mixed-integer constraints (BlendingEnv, RTNEnv, STNEnv)
- On-policy methods (TRPOLag, OnCRPO) generally outperform off-policy DDPGLag in constraint compliance, though at higher sample complexity
- Action sanitization strategies significantly impact learning dynamics, with prop preserving exploration gradients while disable ensures strict feasibility
- Large train-evaluation performance gaps (up to 30%) indicate overfitting issues, particularly for P3O across multiple environments

## Why This Works (Mechanism)

### Mechanism 1: CMDP Formulation Separates Safety from Optimization
SafeOR-Gym models OR problems as Constrained Markov Decision Processes with explicit cost signals rather than implicit reward shaping. Each environment maintains dual signal channels—rewards capture economic objectives while costs independently track constraint violations. Safe RL algorithms use Lagrangian methods to balance these objectives, solving trust-region subproblems that maximize reward subject to cumulative cost constraints.

### Mechanism 2: Action Sanitization Strategies Enable Exploration-Feasibility Tradeoffs
Configurable action correction strategies (prop, disable, none) allow researchers to control how aggressively infeasible actions are modified. "Prop" proportionally scales actions to satisfy constraints while preserving direction; "disable" nullifies violating actions entirely; "none" allows violations to propagate with cost penalties. This creates different learning dynamics—prop maintains gradient information, disable prevents infeasible region exploration, none provides dense feedback but risks instability.

### Mechanism 3: Ground Truth Comparison via Mathematical Optimization
Each environment provides optimal solutions from mathematical programming (Gurobi) as ground truth, enabling quantitative gap analysis between learned policies and theoretical optima. Since all environments are based on deterministic OR formulations, the authors solve equivalent mixed-integer nonlinear programs to obtain optimal objective values. Safe RL performance is measured as percentage gap from optimality, with "reasonable optimality" defined as <35% gap with low costs.

## Foundational Learning

- **Concept: Constrained Markov Decision Processes (CMDPs)**
  - Why needed: Core formalism distinguishing safe RL from standard RL—treating safety as negative rewards fails because agents may still violate constraints if reward is high enough, while explicit cost constraints enable formal safety guarantees.
  - Quick check: If an agent receives reward +100 for reaching a goal but cost 50 for entering a hazard zone with cost limit 10, what should it do? (Answer: Find a path avoiding hazards entirely, since cumulative cost 50 > limit 10 makes the policy infeasible regardless of reward.)

- **Concept: Mixed-Integer Programming Fundamentals**
  - Why needed: Many SafeOR-Gym environments combine discrete decisions with continuous variables—understanding why this creates non-convex feasible regions helps interpret algorithm failures.
  - Quick check: Why can't gradient-based methods easily solve problems with binary variables? (Answer: Gradients don't exist for discrete jumps; the feasible set is non-convex with multiple local optima separated by infeasible regions.)

- **Concept: Trust-Region and Lagrangian Optimization**
  - Why needed: Understanding how CPO, TRPOLag, and OnCRPO work—trust regions prevent large policy updates that could violate safety, while Lagrange multipliers dynamically adjust penalty strength based on constraint satisfaction.
  - Quick check: If a Lagrange multiplier for a cost constraint is increasing during training, what does that indicate? (Answer: The agent is consistently violating the constraint, so the penalty is being strengthened to force compliance.)

## Architecture Onboarding

- **Component map:**
  SafeOR-Gym Environment Layer -> OmniSafe CMDP Wrapper -> Algorithm Layer (CPO, TRPOLag, P3O, OnCRPO, DDPGLag) -> Evaluation Layer

- **Critical path:**
  1. Select environment → analyze problem structure (InvMgmtEnv for continuous decisions, UCEnv for binary+continuous, BlendingEnv for non-convex constraints)
  2. Configure sanitization → choose action correction strategy based on constraint complexity (prop for exploration, disable for safety-critical testing)
  3. Initialize algorithm → start with TRPOLag or OnCRPO (best performers across most environments)
  4. Tune cost parameters → adjust penalty weights if agent learns conservative no-action policies
  5. Validate against optimal → compare to Gurobi solution; >35% gap indicates fundamental algorithm limitations

- **Design tradeoffs:**
  - On-policy vs Off-policy: On-policy methods show better constraint compliance but require more environment interactions; DDPGLag is sample-efficient but exhibits highest residual violations
  - Prop vs Disable sanitization: Prop preserves gradient direction for learning but may allow near-violations; disable ensures strict feasibility but can block exploration entirely
  - Cost penalty magnitude: High penalties ensure quick constraint satisfaction but may create sparse reward signals; low penalties allow exploration but risk deployment of unsafe policies

- **Failure signatures:**
  - Near-zero reward + high cost: Agent learned no-action policy (common in BlendingEnv)—indicates cost penalties dominate reward signal
  - Training-evaluation gap >30%: Overfitting to training episodes (P3O shows this)—algorithm memorizes trajectories rather than learning generalizable policy
  - Oscillating costs without convergence: Non-convex landscape preventing stable gradients—typical in RTNEnv/STNEnv with integer constraints
  - Slow cost reduction (flat curves): Temporal dependencies and indirect constraints make credit assignment difficult

- **First 3 experiments:**
  1. InvMgmtEnv with TRPOLag: Start with best-performing environment to validate pipeline—should achieve ~65% of optimal reward with near-zero cost within 3000 epochs
  2. BlendingEnv comparing prop vs disable vs none: Run all three sanitization strategies to understand action correction effects—expect all strategies to struggle (reward gap >99% from optimal)
  3. UCEnv comparing on-policy (TRPOLag) vs off-policy (DDPGLag): Direct comparison of safety-performance tradeoffs—TRPOLag should achieve lower costs but DDPGLag may show faster initial learning

## Open Questions the Paper Calls Out

### Open Question 1
How can safe reinforcement learning algorithms be adapted to effectively handle mixed-integer variables and nonlinear, non-convex constraints typical in operations research? The paper shows current algorithms perform poorly on problems involving mixed-integer variables or nonlinear, nonconvex constraints, with RTNEnv and STNEnv showing sluggish progress due to non-convexities introduced by integer-based constraints.

### Open Question 2
Can safety constraints be directly encoded into neural network policies via architectural innovations to eliminate the need for soft penalties or post-hoc projections? The paper suggests new approaches to directly encode safety constraints into neural network policies, possibly through architectural innovations or differentiable constraint satisfaction layers.

### Open Question 3
How can automated parameter tuning methods be developed to improve the consistency and reduce the manual overhead of safe RL algorithms across diverse environments? The paper calls for developing automated parameter tuning methods to reduce manual overhead, noting that performance gaps may stem from suboptimal tuning of environment cost parameters.

### Open Question 4
Can action-constrained RL methods be advanced to enforce hard constraints during action selection to better ensure safety in industrial control tasks? The paper proposes advancing action-constrained RL methods that enforce hard constraints during action selection rather than relying on soft penalties or post hoc projection.

## Limitations

- Exact algorithm hyperparameters and training seeds are unspecified, preventing exact reproduction of results
- Limited ablation studies isolating the effects of action sanitization strategies and cost penalty parameters
- Most environments use deterministic parameters despite emphasis on practical applicability to stochastic real-world settings

## Confidence

- **High confidence**: Current safe RL algorithms struggle with non-convex OR problems (BlendingEnv, RTNEnv, STNEnv), aligning with established theoretical limitations
- **Medium confidence**: Algorithm comparison claims, as exact hyperparameters and training seeds are unspecified
- **Low confidence**: Generalization to real-world stochastic settings, as most environments use deterministic parameters

## Next Checks

1. Reproduce InvMgmtEnv results with TRPOLag using paper-specified parameters (100 episodes/epoch, 3000 epochs) to verify the claimed 65% optimality gap
2. Test action sanitization impact by running BlendingEnv with all three strategies (prop, disable, none) under identical conditions to quantify exploration-feasibility tradeoffs
3. Validate off-policy limitations by comparing DDPGLag performance across environments with and without cost penalty tuning to isolate whether high violations stem from algorithm choice or hyperparameter mismatch