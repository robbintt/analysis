---
ver: rpa2
title: 'The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple, Self-Contained
  Directives'
arxiv_id: '2510.17388'
source_url: https://arxiv.org/abs/2510.17388
tags:
- labels
- label
- instruction
- shot
- instructions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper reveals that instruction-tuned large language models
  (IT-LLMs) exhibit significant fragility to semantically neutral variations in option
  label formats (alphabetic, numeric, Roman) in multiple-choice settings, despite
  explicit instructions. Performance differences can exceed 30 percentage points between
  formats, with models often failing to execute simple atomic instructions.
---

# The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple, Self-Contained Directives

## Quick Facts
- arXiv ID: 2510.17388
- Source URL: https://arxiv.org/abs/2510.17388
- Authors: Henry Lim; Kwan Hui Lim
- Reference count: 6
- Key outcome: Instruction-tuned LLMs show significant fragility to semantically neutral variations in option label formats, with performance differences exceeding 30 percentage points despite explicit formatting instructions.

## Executive Summary
Instruction-tuned large language models (IT-LLMs) exhibit a fundamental weakness in following simple, atomic instructions, particularly when it comes to adhering to formatting directives. Despite explicit instructions, models show substantial performance variance when option labels are presented in different formats (alphabetic, numeric, Roman), with numeric formats consistently yielding the best results. This "atomic instruction gap" persists even with in-context learning and larger model sizes, suggesting that instruction tuning improves task performance but fails to address underlying symbolic biases. The study reveals that format sensitivity originates from pre-training and is not mitigated by standard instruction-tuning approaches, highlighting the need for targeted optimization of atomic instruction-following capabilities.

## Method Summary
The study evaluates 20 IT-LLMs (0.5B-72B parameters) on MMLU and MMLU-Pro datasets with modified MCQ prompts containing explicit formatting instructions. Four experimental paradigms are tested: (1) explicit instructions with three label formats (alphabetic, numeric, Roman), (2) instructions removed, (3) option content removed, and (4) 3-shot in-context examples. Models are evaluated using strict exact-match accuracy and label fidelity metrics, with 4-bit quantization on NVIDIA RTX 5090 GPU (70B models use CPU offloading with 5% subsampling). Performance ranges and statistical significance are analyzed across model families.

## Key Results
- Models show >30 percentage point accuracy differences between label formats, with numeric formats outperforming alphabetic and Roman consistently
- Three-shot in-context learning yields no statistically significant improvements in format robustness or instruction adherence
- Larger models achieve higher overall accuracy but remain inconsistent in following formatting instructions
- When explicit instructions are removed, performance degrades and format sensitivity ranges expand by 5-10 percentage points
- Models frequently output numerals despite explicit instructions to use alphabetic or Roman labels, indicating strong pre-trained format biases

## Why This Works (Mechanism)

### Mechanism 1
Symbolic format biases originate from pre-training and persist through instruction tuning. Base models develop strong priors for numeric formats during pre-training on large corpora, and instruction tuning optimizes for task completion metrics without explicitly targeting format invariance.

### Mechanism 2
Numerical label priors dominate even under explicit contradictory instructions. When models encounter conflicting signals (numeric priors vs. non-numeric instructions), pre-trained numeric preferences override explicit directives, with models frequently generating numeral responses despite formatting instructions.

### Mechanism 3
In-context learning fails to override format biases for atomic instructions. Few-shot demonstrations provide task-pattern guidance but insufficient signal strength to overcome deeply embedded format preferences, as the exemplars are interpreted through existing biased representations.

## Foundational Learning

- Concept: **Instruction Tuning vs. Task Performance**
  - Why needed here: Critical to understand that improving accuracy ≠ improving instruction adherence; the paper shows these are orthogonal capabilities
  - Quick check question: Can you explain why a model with higher MMLU accuracy might still fail to follow a simple formatting directive?

- Concept: **Format/Tokenization Bias**
  - Why needed here: Models develop preferences for certain symbolic representations (numeric > alphabetic > Roman) based on pre-training exposure
  - Quick check question: Why would semantically identical labels (A/1/I) produce 30+ percentage point accuracy differences?

- Concept: **Statistical Significance Testing (Wilcoxon Signed-Rank)**
  - Why needed here: The paper relies on Bonferroni-corrected Wilcoxon tests to distinguish real improvements from noise
  - Quick check question: If 3-shot shows 15/20 models improving but statistical tests show p > 0.05, what does this mean for the intervention's effectiveness?

## Architecture Onboarding

- Component map: Prompt Engineering Layer -> Evaluation Pipeline -> Model Zoo
- Critical path: 1. Select label format → 2. Construct instruction+question+options → 3. Generate response via greedy decoding → 4. Exact-match evaluation → 5. Compute accuracy + label fidelity
- Design tradeoffs: Strict exact-match vs. flexible parsing (paper chose strict to measure true instruction-following); 4-bit quantization enabled 20-model comparison but may affect small models disproportionately; MCQ-only evaluation controlled isolation but limits generalizability
- Failure signatures: Format collapse (outputting "1" when instructed to use "A"), below-baseline performance (<25% on 4-option questions with non-numeric labels), range explosion (>50pp spread between formats)
- First 3 experiments: 1. Run your model on identical MCQ content with A/B/C/D vs. 1/2/3/4 vs. I/II/III/IV labels; compute accuracy delta; 2. Remove explicit formatting instructions; measure accuracy drop and range expansion; 3. Present only labels with instructions (no option content); verify if model achieves random baseline or collapses

## Open Questions the Paper Calls Out

### Open Question 1
What specific training strategies or fine-tuning objectives can effectively mitigate symbolic label bias and improve atomic instruction-following in IT-LLMs? The paper demonstrates the problem exists but does not propose or test any training interventions.

### Open Question 2
Does the observed fragility to atomic instructions generalize beyond multiple-choice tasks to open-ended generation and multi-turn dialogue settings? The study focused exclusively on MCQ tasks for controlled evaluation.

### Open Question 3
To what extent do proprietary commercial models (e.g., GPT-4, Claude) exhibit atomic instruction-following failures compared to open-weight models? The study's cost limitations restricted evaluation to publicly available open-weight models.

### Open Question 4
What are the relative contributions of pretraining data composition versus instruction-tuning strategies to the observed symbolic label bias? The paper shows instruction tuning does not mitigate label sensitivity but the root cause in pretraining remains unisolated.

## Limitations
- The study's focus on 4-bit quantized models raises questions about whether results would differ with full-precision inference
- The strict exact-match evaluation protocol may overestimate instruction-following failures in real-world applications where flexible parsing is acceptable
- The findings may not generalize beyond multiple-choice contexts to open-ended tasks or multi-turn dialogue settings

## Confidence

- **Format Sensitivity Claims**: High - robust empirical evidence with consistent patterns across model families and datasets
- **Mechanism Attribution Claims**: Medium - compelling evidence for persistence but attribution between pre-training and IT sources remains somewhat speculative
- **In-Context Learning Claims**: Low - well-supported negative result but alternative strategies not explored

## Next Checks

1. **Cross-Domain Transfer Test**: Evaluate the same format sensitivity pattern on open-ended tasks (e.g., summarization with formatting instructions) to assess generalization beyond multiple-choice contexts.

2. **Instruction Tuning Ablation**: Train a model with explicit format-invariance objectives or diverse label format exposure during instruction tuning to determine if targeted training can mitigate the observed biases.

3. **Full-Precision Comparison**: Replicate key experiments with full-precision inference on a subset of models to quantify the impact of quantization on format sensitivity and instruction-following performance.