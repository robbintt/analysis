---
ver: rpa2
title: Evaluating Contrastive Feedback for Effective User Simulations
arxiv_id: '2505.02560'
source_url: https://arxiv.org/abs/2505.02560
tags:
- user
- information
- relevance
- documents
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the use of contrastive feedback in prompting
  strategies for LLM-based user simulations in interactive information retrieval.
  The authors test whether providing summaries of relevant, irrelevant, or both types
  of documents during a simulated search session improves the effectiveness of LLM
  agents compared to baseline approaches that use only topic descriptions.
---

# Evaluating Contrastive Feedback for Effective User Simulations

## Quick Facts
- arXiv ID: 2505.02560
- Source URL: https://arxiv.org/abs/2505.02560
- Reference count: 28
- Primary result: Contrastive feedback (summaries of relevant and irrelevant documents) improves LLM-based user simulation effectiveness compared to baselines

## Executive Summary
This paper evaluates contrastive feedback in prompting strategies for LLM-based user simulations in interactive information retrieval. The authors test whether providing summaries of relevant, irrelevant, or both types of documents during simulated search sessions improves LLM agent effectiveness compared to baseline approaches using only topic descriptions. They implement eight different user configurations in the SimIIR 3 framework and evaluate them on two TREC test collections using information gain and sDCG metrics. Results show that all LLM-based users outperform random baselines, with contrastive approaches (CRF and PRF) achieving the highest effectiveness, especially after several queries. The study demonstrates that contrastive feedback enhances simulated user behavior and highlights the potential of LLMs for more realistic user simulations, while also revealing limitations in existing test collections.

## Method Summary
The study uses the SimIIR 3 framework with PyTerrier integration, employing Llama3.3-70B (Q4_K_M quantization) for both query generation and relevance judgment. Eight user configurations are tested: Full Topic with Contrastive Feedback (FTTC), Title with Contrastive Feedback (CRF), Positive Relevance Feedback (PRF), Negative Relevance Feedback (NRF), Random (RND), and variants. The simulation uses BM25 ranking, temperature 1.0 for query generation, and temperature 0.0 for deterministic relevance judgments. Document summaries from judged documents are added to the prompt for subsequent queries, creating an iterative knowledge state. Sessions run for 1200 seconds with a journalist persona across TREC Core17 and Core18 collections.

## Key Results
- All LLM-based user configurations (FTTC, TTT, RND, RND*, PRF, NRF, CRF, CRF') significantly outperform random baselines
- Contrastive approaches (CRF and PRF) achieve the highest effectiveness, particularly after several queries
- An "uptake phase" is observed where differences between configurations become more pronounced over time
- Unjudged documents retrieved by LLM queries pose a significant evaluation challenge due to incomplete relevance judgments

## Why This Works (Mechanism)

### Mechanism 1: Contrastive In-Context Learning
Providing the LLM with summaries of both relevant and irrelevant documents within the prompt improves search effectiveness through contrastive learning principles. By exposing the model to positive and negative examples simultaneously, it can better internalize the specific boundaries and distinctions of the information need, refining its "knowledge state." The model can utilize the juxtaposition of "what to look for" and "what to avoid" to improve subsequent query formulation and relevance judgment.

### Mechanism 2: Iterative Knowledge State Accumulation
Simulated user effectiveness increases over the course of a search session as the agent accumulates contextual summaries of seen documents. This dynamic updating of the prompt with summaries of judged documents allows the agent to refine its understanding of the topic based on the specific corpus distribution, leading to better performance in later queries.

### Mechanism 3: Guided Relevance Decision Making
LLMs provided with explicit topic narratives and feedback summaries outperform random and simple heuristic-based baselines in relevance judging tasks. The LLM acts as a classifier where the "knowledge state" serves as the decision boundary, with temperature set to 0 for deterministic, context-driven decisions that reduce randomness.

## Foundational Learning

- **Concept: Interactive Information Retrieval (IIR) & SimIIR 3**
  - Why needed here: This paper evaluates simulations, not live users. IIR treats search as a multi-step session rather than a single-turn request, requiring the SimIIR 3 framework to manage session state.
  - Quick check question: How does the evaluation metric (sDCG) differ from standard search metrics like NDCG? (Hint: It accounts for the effort/cost of multiple queries in a session).

- **Concept: Contrastive Learning vs. Contrastive Prompting**
  - Why needed here: The authors transfer a concept from model training (contrastive loss functions) to inference (prompt engineering). Understanding this distinction is vital to interpreting results as "prompting strategy effectiveness" rather than "model fine-tuning."
  - Quick check question: In this paper, are the model weights updated based on the relevant/irrelevant documents? (Answer: No, they are fed as context).

- **Concept: Relevance Feedback (RF) & Pseudo-Relevance Feedback (PRF)**
  - Why needed here: The user configurations (PRF, NRF, CRF) are variants of classic RF techniques. You need to know that PRF typically assumes top-k results are relevant to expand queries, whereas this study uses explicit LLM judgments to drive the feedback loop.
  - Quick check question: Why might "Negative Relevance Feedback" (using irrelevant docs) be harder to utilize than Positive Relevance Feedback? (Hint: Irrelevant docs may share very few features, making the "negative" signal noisy).

## Architecture Onboarding

- **Component map:** Controller (SimIIR 3 Framework) -> Agent (Llama3.3-70B) -> Retriever (BM25 via PyTerrier) -> Context Manager (Python logic for summaries)
- **Critical path:**
  1. Init: Load TREC Topic -> Construct Initial Prompt
  2. Query Loop: LLM generates query → BM25 retrieves SERP → LLM examines summaries → LLM outputs relevance decision → Update State with summaries → Update Prompt with new feedback → Repeat until timeout
- **Design tradeoffs:**
  - BM25 vs. Neural Retrieval: Chosen for efficiency, isolating LLM's query reformulation capability
  - Summarization vs. Full Text: Trades context fidelity for token efficiency
  - FTTC vs. TTT: FTTC provides higher guidance but is less realistic; TTT is more realistic but risks drift
- **Failure signatures:**
  - Unjudged Documents: LLMs retrieve documents lacking relevance labels in qrels, causing false negatives
  - Topical Drift: Observed in CRF' configuration when only title + feedback is used
- **First 3 experiments:**
  1. Sanity Check: Run RND and FTTC on single TREC topic to verify baseline functionality
  2. Feedback Ablation: Compare PRF vs. NRF on Core17 to determine negative example utility
  3. Contrastive Validation: Run CRF vs. FTTC on uptake phase (queries 4-8) to verify performance gains

## Open Questions the Paper Calls Out

- **Open Question 1:** Under what specific conditions does Contrastive Relevance Feedback (CRF) outperform Positive Relevance Feedback (PRF), particularly regarding the distribution of relevance judgments? The authors note inconsistent results across Core17 and Core18, requiring analysis across collections with varying relevance densities.

- **Open Question 2:** Can LLM-based relevance judgments be reliably used to evaluate user simulations involving unjudged documents retrieved by synthetic queries? Current resources lack complete judgments to validate such simulations automatically, and using LLMs for relevance judgments remains controversial.

- **Open Question 3:** To what extent does excluding full topic descriptions (narratives) in contrastive prompts induce topic drift during extended search sessions? The CRF' configuration's underperformance suggests potential drift, but the paper did not qualitatively analyze query content to confirm semantic deviation.

## Limitations

- **Prompt Template Specificity:** Exact prompt structures for query generation, relevance judgment, and document summarization are not provided in the paper, creating significant reproduction barriers.
- **Evaluation Completeness:** Substantial portions of documents retrieved by LLM-generated queries lack relevance judgments in TREC qrels files, creating systematic underestimation of performance.
- **Summarization Process:** The methodology for generating document summaries used in feedback mechanism is unclear, including what LLM/method is used, summary length, and granularity.

## Confidence

- **High Confidence:** LLM-based users (FTTC, TTT, PRF, NRF, CRF) significantly outperform random baselines
- **Medium Confidence:** Contrastive approaches (CRF and PRF) achieve highest effectiveness, especially after several queries
- **Medium Confidence:** Iterative accumulation of knowledge state via document summaries improves performance
- **Low Confidence:** Broad claim that contrastive feedback "enhances simulated user behavior" for more realistic user simulations

## Next Checks

1. **Prompt Template Extraction and Replication:** Obtain and replicate exact prompt templates from GitHub repository; run minimal test to verify prompts generate expected queries and judgments.

2. **Negative Feedback Ablation Study:** Conduct focused experiment comparing PRF vs. NRF on subset of Core17 topics to isolate contribution of negative examples.

3. **Unjudged Document Impact Analysis:** Log proportion of retrieved documents without qrels judgments for each configuration; calculate adjusted sDCG excluding unjudged documents to quantify impact on conclusions.