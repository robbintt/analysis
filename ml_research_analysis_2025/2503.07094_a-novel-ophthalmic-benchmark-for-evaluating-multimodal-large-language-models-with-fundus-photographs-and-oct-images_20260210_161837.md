---
ver: rpa2
title: A Novel Ophthalmic Benchmark for Evaluating Multimodal Large Language Models
  with Fundus Photographs and OCT Images
arxiv_id: '2503.07094'
source_url: https://arxiv.org/abs/2503.07094
tags:
- images
- large
- language
- multimodal
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a novel ophthalmic benchmark for evaluating
  multimodal large language models (MLLMs) using fundus photographs and OCT images.
  The benchmark addresses limitations of existing medical MLLM evaluations by focusing
  on real-world clinical complexities rather than theoretical knowledge.
---

# A Novel Ophthalmic Benchmark for Evaluating Multimodal Large Language Models with Fundus Photographs and OCT Images

## Quick Facts
- arXiv ID: 2503.07094
- Source URL: https://arxiv.org/abs/2503.07094
- Reference count: 40
- Key outcome: Novel benchmark evaluates MLLMs on real ophthalmic images, showing significant variability in diagnostic accuracy across diseases with models performing well on diabetic retinopathy but struggling with complex conditions like choroidal neovascularization.

## Executive Summary
This study introduces a novel ophthalmic benchmark designed to evaluate multimodal large language models (MLLMs) using real-world fundus photographs and OCT images rather than theoretical medical knowledge. The benchmark addresses limitations of existing medical MLLM evaluations by focusing on clinical complexities through expert-annotated images. Seven MLLMs were evaluated using a standardized API framework, revealing significant variability in diagnostic accuracy across different ophthalmic conditions. The study highlights that while models perform well on straightforward conditions like diabetic retinopathy, they struggle with complex cases requiring clinical context or involving secondary manifestations of diseases.

## Method Summary
The benchmark evaluates seven MLLMs using fundus photographs (439 images across 17 disease categories) and OCT images (75 images across 8 disease categories) sourced from clinical databases. Images underwent expert curation through a triphasic quality control process involving five ophthalmologists with 10+ years experience and three senior reviewers with 15+ years experience. Models were evaluated via API using standardized prompts with restricted-domain candidate diagnosis lists to minimize output variance. Accuracy was calculated using standard classification metrics (TP+TN)/(TP+TN+FP+FN) per disease and aggregated across conditions. The evaluation framework used base64-encoded images submitted through model APIs with fixed prompt templates.

## Key Results
- Significant variability in diagnostic accuracy across diseases, with models performing well on diabetic retinopathy but struggling with choroidal neovascularization and other complex conditions
- Gemini-2.0-Flash achieved the highest accuracy on fundus photograph diagnosis (FPD) while GPT-4o-mini showed limited capability on OCT diagnosis (OCTD)
- All models demonstrated near-zero accuracy on conditions requiring clinical context like choroidal neovascularization, laser scars, and myopia-related pathologies
- Restricted-domain multiple-choice prompting reduced output variance and improved comparability across models compared to open-ended generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clinical-image-based benchmarks provide better discriminative power than medical exam question banks for evaluating MLLM diagnostic capability.
- Mechanism: Real clinical images avoid training data exposure risk and require models to perform visual pattern recognition rather than recall theoretical knowledge. Expert-validated annotations with triphasic quality control ensure ground-truth reliability.
- Core assumption: Literature-sourced images have lower likelihood of appearing in model pre-training corpora compared to publicly available exam question banks.
- Evidence anchors:
  - [abstract]: "benchmark addresses limitations of existing medical MLLM evaluations by focusing on real-world clinical complexities rather than theoretical knowledge"
  - [section 5.1]: "These literature-derived images are more clinically relevant and carry a lower risk of data leakage compared to open medical exam question banks"
  - [corpus]: GROK paper similarly notes MLLMs "often fail to fully exploit the synergy between color fundus photography (CFP) and optical coherence tomography (OCT)"

### Mechanism 2
- Claim: Disease-specific diagnostic accuracy varies systematically based on diagnostic complexity factors—overlapping disease relationships, requirement for clinical context, and data imbalance.
- Mechanism: CNV shows near-zero accuracy across all models because it is a secondary manifestation of multiple underlying conditions; laser scars require surgical history not present in images; rare conditions have fewer training examples.
- Core assumption: MLLMs lack implicit reasoning about diagnostic hierarchies and exclusion-based diagnosis that clinicians perform using patient history.
- Evidence anchors:
  - [abstract]: "models performing well on conditions like diabetic retinopathy but struggling with others like choroidal neovascularization"
  - [section 5.1]: "CNV is a form of retinal neovascularization that can be caused by multiple fundus diseases...the model may incorrectly identify other underlying diseases"
  - [corpus]: Weak direct evidence for this mechanism in corpus; neighboring papers focus on technical improvements rather than diagnostic complexity analysis

### Mechanism 3
- Claim: Restricted-domain multiple-choice prompting reduces output variance and improves comparability across models compared to open-ended generation.
- Mechanism: Providing a fixed candidate diagnosis list constrains the output space, reducing random errors from free-form text and enabling standardized accuracy calculation via exact string matching.
- Core assumption: The candidate list is comprehensive enough that the correct diagnosis is always present as an option.
- Evidence anchors:
  - [section 3.4]: "restricted-domain answers...reduces the randomness associated with single-choice questions and minimizes the likelihood of highly deviant answers"
  - [section 3.5]: Accuracy formula using TP/TN/FP/FN requires discrete classification outputs
  - [corpus]: EH-Benchmark paper addresses hallucinations from limited ophthalmic knowledge, suggesting constrained outputs may help

## Foundational Learning

- Concept: Multimodal Large Language Model (MLLM) architecture
  - Why needed here: Understanding that MLLMs combine a vision encoder (processing fundus/OCT images) with a language model (generating diagnostic output) explains why models can interpret medical images at all.
  - Quick check question: Can you explain why an MLLM can process both an OCT scan and a text prompt asking for a diagnosis?

- Concept: Fundus photography vs. Optical Coherence Tomography (OCT)
  - Why needed here: The benchmark evaluates both modalities separately; understanding that fundus provides 2D surface retinal views while OCT provides cross-sectional retinal layer analysis is essential for interpreting why models might perform differently on each.
  - Quick check question: Would you expect the same retinal disease to be equally visible on both fundus photography and OCT? Why or why not?

- Concept: Benchmark data leakage and contamination
  - Why needed here: The paper explicitly addresses this limitation in existing benchmarks; understanding why medical exam questions pose "high risk of exposure" helps evaluate the validity of benchmark results.
  - Quick check question: If an MLLM was trained on USMLE practice questions, why would evaluating it on those same questions give misleading performance estimates?

## Architecture Onboarding

- Component map:
  - Base64-encoded fundus photographs/OCT images → API-based MLLM inference → Restricted-domain candidate list output → Accuracy calculation using TP/TN/FP/FN

- Critical path:
  1. Source images from MuReD (fundus) and Dataset C8 of Retinal OCT Image Classification database (OCT)
  2. Triphasic quality control: technical validation → comorbidity exclusion → expert consensus
  3. Convert images to base64, submit via API with fixed prompt template
  4. Parse model outputs against candidate list, compute accuracy by disease

- Design tradeoffs:
  - Small OCT sample (75 images): Enables rigorous expert validation but limits statistical power for OCTD task
  - Restricted-domain choices: Improves comparability but may not reflect clinical reality where differential diagnosis is open-ended
  - API-based evaluation: Ensures fair comparison but prevents analysis of internal representations
  - Exclusion of advanced comorbidities: Simplifies ground truth but reduces clinical realism

- Failure signatures:
  - Near-zero accuracy on CNV, HTR, LS, ODP, ODC across all models (likely indicates fundamental limitation, not model-specific bug)
  - GPT-4o-mini correctly identifying only CSR in OCTD (suggests mode collapse or task misunderstanding)
  - QVQ-72B-Preview excluded from OCT analysis for "hardly correctly identified OCT images"

- First 3 experiments:
  1. Baseline replication: Run Gemini-2.0-Flash and GPT-4o on the FPD task using provided prompts; verify accuracy rankings match paper (Gemini > GPT-4o for fundus)
  2. Error analysis on CNV: Manually inspect all CNV images and model predictions; test hypothesis that models predict underlying diseases (ARMD, myopia) rather than CNV itself
  3. Cross-modal consistency check: For any patients with both fundus and OCT available, compare whether models give consistent diagnoses across modalities

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does simultaneous input of fundus photographs and OCT images for the same patient yield superior diagnostic accuracy compared to evaluating either modality independently?
- **Basis in paper:** [explicit] The Future Work section states, "referencing both fundus photographs and OCT images simultaneously could potentially yield superior diagnostic accuracy compared to using a single type of image."
- **Why unresolved:** The current study evaluated the modalities (FPD and OCTD) as separate tasks and did not test models on paired inputs from the same patient.
- **What evidence would resolve it:** A comparative evaluation where MLLMs are prompted with both image types concurrently versus single-modality inputs.

### Open Question 2
- **Question:** Can the inclusion of comprehensive clinical metadata (e.g., medical and surgical history) resolve the diagnostic ambiguity for conditions where models currently fail, such as choroidal neovascularization (CNV) and myopia?
- **Basis in paper:** [explicit] The Discussion attributes low accuracy in CNV and myopia to the lack of "additional clinical context" and "complete clinical information," noting that images alone can be ambiguous.
- **Why unresolved:** The benchmark dataset relied solely on images to protect patient privacy and lacked the associated clinical history required for these specific diagnoses.
- **What evidence would resolve it:** An evaluation using a dataset that links imaging with relevant patient history to determine if performance on context-dependent diseases improves.

### Open Question 3
- **Question:** Are current MLLMs capable of accurate disease staging (e.g., distinguishing stages of diabetic retinopathy) based solely on imaging data?
- **Basis in paper:** [explicit] The Limitations section notes that "accurate disease staging is crucial" and states that "current diagnostic accuracy of large models is insufficient for such nuanced tasks."
- **Why unresolved:** The benchmark was designed primarily for conditional classification rather than the granular staging required for clinical management decisions.
- **What evidence would resolve it:** A specialized benchmark designed to evaluate model performance on stage-specific classification criteria.

## Limitations

- Limited dataset size (439 fundus images, 75 OCT images) may constrain statistical power for rare disease detection and generalizability across diverse clinical settings
- API-based evaluation prevents analysis of internal model representations and may introduce variability based on undocumented model configurations
- Restricted-domain candidate lists may not reflect real-world clinical practice where differential diagnoses are open-ended
- Potential data leakage remains a concern despite efforts to use literature-sourced images, as pre-training corpora are opaque
- Benchmark focuses on diagnostic classification accuracy without evaluating clinical reasoning processes or integration of patient history

## Confidence

- **High confidence**: Clinical relevance of using real ophthalmic images rather than theoretical exam questions; effectiveness of restricted-domain prompting in reducing output variance; superior performance of Gemini-2.0-Flash on fundus diagnosis compared to other models
- **Medium confidence**: Disease-specific accuracy patterns (CNV, HTR, LS showing near-zero accuracy) reflect fundamental model limitations rather than benchmark design issues; generalizability of findings to other ophthalmic conditions and clinical contexts
- **Low confidence**: Claims about specific mechanisms for disease-specific failures without direct experimental validation; assumptions about lower data leakage risk from literature-sourced images without verification of pre-training data sources

## Next Checks

1. **Replication study with independent data**: Curate a separate dataset of fundus photographs and OCT images from different sources to verify benchmark findings and assess generalizability
2. **Error analysis for CNV and rare diseases**: Manually examine all cases where models failed on CNV, HTR, LS, ODP, and ODC to distinguish between annotation errors, model limitations, and benchmark design constraints
3. **Open-ended prompting comparison**: Evaluate a subset of models using free-response prompts without candidate lists to assess whether constrained outputs artificially inflate accuracy for simpler diagnoses while masking capabilities for complex cases