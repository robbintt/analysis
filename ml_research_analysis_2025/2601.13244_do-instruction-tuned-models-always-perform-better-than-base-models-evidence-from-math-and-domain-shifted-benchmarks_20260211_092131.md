---
ver: rpa2
title: Do Instruction-Tuned Models Always Perform Better Than Base Models? Evidence
  from Math and Domain-Shifted Benchmarks
arxiv_id: '2601.13244'
source_url: https://arxiv.org/abs/2601.13244
tags:
- base
- instruction-tuned
- arxiv
- performance
- math
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the widely held assumption that instruction-tuned
  models always outperform base models. The authors conduct a systematic comparison
  using Pass@20 evaluation across standard math benchmarks (GSM8K, Math-500), perturbed
  variants designed to disrupt solution patterns, and a domain-specific medical benchmark
  (MedCalc).
---

# Do Instruction-Tuned Models Always Perform Better Than Base Models? Evidence from Math and Domain-Shifted Benchmarks

## Quick Facts
- arXiv ID: 2601.13244
- Source URL: https://arxiv.org/abs/2601.13244
- Authors: Prateek Munjal; Clement Christophe; Ronnie Rajan; Praveenkumar Kanithi
- Reference count: 28
- Base models outperform instruction-tuned variants on GSM8K (up to 32.67% higher Pass@20) and MedCalc in zero-shot settings

## Executive Summary
This paper challenges the widely held assumption that instruction-tuned models always outperform base models. The authors conduct a systematic comparison using Pass@20 evaluation across standard math benchmarks (GSM8K, Math-500), perturbed variants designed to disrupt solution patterns, and a domain-specific medical benchmark (MedCalc). Their results show that under zero-shot chain-of-thought evaluation, base models consistently outperform instruction-tuned variants on GSM8K, with drops as high as 32.67% for Llama3-70B. Base models also surpass instruction-tuned models on MedCalc in zero-shot settings. The performance advantage of instruction tuning is found to be highly evaluator-dependent, particularly for LaTeX-heavy outputs, and primarily benefits small language models rather than large ones. These findings suggest that instruction tuning does not universally improve reasoning capabilities and highlight the need for more robust evaluation benchmarks.

## Method Summary
The authors compare base models and instruction-tuned variants across four benchmarks using Pass@20 evaluation. Base models use chain-of-thought decoding with K=20 samples selected at the first token, followed by greedy decoding. Instruction-tuned models use repeated stochastic sampling (temperature=0.05, max_tokens=8192). Answer extraction for base models employs an auxiliary LLM (Qwen3-0.6B) using a JSON prompt, while instruction-tuned models enforce JSON output format with json_repair library. Two evaluators are used: a standard regex-based grader and MathVerify. The evaluation covers GSM8K (1,319 test), Math-500 (500 test), Math-Perturb Hard (279 test), and MedCalc (1,000 test across 55 calculation types). Inference runs on 8x NVIDIA H200 GPUs via vLLM.

## Key Results
- Base models outperform instruction-tuned variants on GSM8K with drops up to 32.67% for Llama3-70B
- Base models surpass instruction-tuned models on MedCalc in zero-shot settings
- Performance advantage of instruction tuning is highly evaluator-dependent, especially for LaTeX-heavy outputs
- Instruction tuning primarily benefits small language models rather than large ones

## Why This Works (Mechanism)
The mechanism behind these findings relates to how instruction tuning affects model behavior. Base models maintain stronger chain-of-thought reasoning capabilities when given freedom to explore multiple solution paths, while instruction tuning may constrain reasoning to more rigid, template-like responses. The evaluator dependency suggests that instruction-tuned models may produce mathematically correct but formatting-inconsistent answers that fail regex-based matching. Small models benefit more from instruction tuning because they lack the inherent reasoning capabilities that larger base models possess, making explicit instruction-following more valuable at smaller scales.

## Foundational Learning
- **Chain-of-thought reasoning**: Why needed - enables complex problem-solving by breaking down tasks into intermediate steps. Quick check - verify models generate coherent intermediate reasoning steps.
- **Pass@K evaluation**: Why needed - measures whether at least one of K generated answers is correct, capturing model uncertainty. Quick check - ensure K is sufficiently large to capture model performance distribution.
- **Instruction tuning**: Why needed - adapts models to follow explicit instructions but may constrain reasoning flexibility. Quick check - compare instruction-following accuracy against raw reasoning ability.
- **LaTeX parsing in graders**: Why needed - mathematical expressions require specialized parsing beyond standard text. Quick check - test grader on known correct/incorrect LaTeX variations.
- **Zero-shot vs few-shot learning**: Why needed - different evaluation paradigms reveal different model capabilities. Quick check - verify prompt formatting consistency across conditions.
- **Model scaling effects**: Why needed - instruction tuning benefits may vary dramatically with model size. Quick check - test across at least three distinct model sizes.

## Architecture Onboarding
**Component map:** Input benchmark → Base model (CoT decoding) -> Auxiliary LLM extraction -> Evaluator OR Input benchmark → Instruction-tuned model (stochastic sampling) -> JSON parser -> Evaluator

**Critical path:** Data preparation → Model loading → Sampling strategy execution → Answer extraction → Evaluation scoring

**Design tradeoffs:** The choice between chain-of-thought freedom (base models) versus instruction-following rigidity (instruction-tuned) creates a fundamental tension between exploration and exploitation in reasoning strategies. The evaluation method (regex vs semantic verification) introduces significant variance in reported performance.

**Failure signatures:** Evaluator discrepancies of up to 10.8% indicate that base models may produce correct but formatting-inconsistent answers. Answer extraction failures occur when base models lack clear output structure. Instruction-tuned models may fail to generate valid JSON despite formatting requirements.

**First experiments:** 1) Run both evaluators on identical outputs to quantify scoring variance. 2) Test answer extraction pipeline on held-out examples before full evaluation. 3) Compare Pass@20 vs Pass@5 to understand the impact of K on relative performance.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluator dependency creates significant uncertainty in absolute performance measurements, particularly for LaTeX-heavy outputs where formatting variations can cause up to 10.8% scoring differences.
- Limited scope to zero-shot and few-shot settings without exploring in-context learning variations that might favor instruction-tuned models differently.
- Focus on specific model families and sizes may miss performance patterns in other architectures or parameter regimes.

## Confidence
- High confidence in relative performance comparison between base and instruction-tuned models on GSM8K and MedCalc
- Medium confidence in broader claim that instruction tuning does not universally improve reasoning
- Low confidence in extrapolating findings to all mathematical or reasoning tasks

## Next Checks
1. Re-run GSM8K evaluation using a more robust answer extraction method to verify base model advantage persists across different graders.
2. Test a wider range of model sizes to map the transition point where instruction tuning benefits diminish.
3. Apply the same base-vs-instruction-tuned comparison to additional reasoning-heavy benchmarks to assess generalizability.