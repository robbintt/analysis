---
ver: rpa2
title: Class-wise Balancing Data Replay for Federated Class-Incremental Learning
arxiv_id: '2507.07712'
source_url: https://arxiv.org/abs/2507.07712
tags:
- data
- tasks
- learning
- replay
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FedCBDR, a federated class-incremental learning
  method that addresses class imbalance through a global-perspective data replay module
  and a task-aware temperature scaling module. The GDR module reconstructs privacy-preserving
  global representations using SVD to enable balanced, class-aware sampling across
  clients.
---

# Class-wise Balancing Data Replay for Federated Class-Incremental Learning

## Quick Facts
- arXiv ID: 2507.07712
- Source URL: https://arxiv.org/abs/2507.07712
- Authors: Zhuang Qi; Ying-Peng Tang; Lei Meng; Han Yu; Xiaoxiao Li; Xiangxu Meng
- Reference count: 40
- Primary result: FedCBDR achieves 2%-15% Top-1 accuracy improvement over six state-of-the-art methods across varying heterogeneity levels and task splits.

## Executive Summary
This paper addresses class imbalance in federated class-incremental learning (FCIL) through a dual-module approach. The Global-perspective Data Replay (GDR) module reconstructs privacy-preserving global representations using SVD to enable balanced, class-aware sampling across clients. The Task-aware Temperature Scaling (TTS) module dynamically adjusts logits' temperature at both class and instance levels to mitigate imbalance between historical and current task samples. The method achieves significant accuracy improvements (2%-15%) over six state-of-the-art methods on CIFAR-10, CIFAR-100, and TinyImageNet while maintaining balanced class-wise sampling distributions.

## Method Summary
FedCBDR implements class-incremental learning in federated settings by addressing two key challenges: (1) class imbalance within replay buffers due to local non-IID data distributions, and (2) imbalance between replayed and new task classes. The GDR module encrypts local features using random orthogonal matrices and uploads pseudo-features to the server, which performs SVD to compute leverage scores for representative sampling. The TTS module applies different temperature values and loss weights to historical versus current task samples during training. The approach uses ResNet-18 backbone with dynamic classifier expansion, operating over 100 communication rounds per task with 2 local epochs per round.

## Key Results
- FedCBDR achieves 2%-15% Top-1 accuracy improvement over six state-of-the-art methods including Finetune, FedEWC, FedLwF, TARGET, LANDER, and Re-Fed.
- The method maintains balanced class-wise sampling distributions across varying heterogeneity levels (β ∈ {0.1, 0.5, 1.0}) and task splits.
- Experimental validation conducted on CIFAR-10, CIFAR-100, and TinyImageNet with different client configurations (K=5 or K=10) and task structures.

## Why This Works (Mechanism)

### Mechanism 1: Global-Perspective Coordinate Sampling via SVD
The paper proposes reconstructing a global pseudo-feature space using Singular Value Decomposition (SVD) to enable representative sampling that overcomes local non-IID data distributions. Clients encrypt local features using random orthogonal matrices and upload "pseudo-features" to the server, which aggregates these and performs SVD to compute leverage scores for every sample. High-scoring samples are selected for the global replay buffer, ensuring that important data points in the global latent space are retained even if they are rare locally. This mechanism assumes leverage scores calculated on encrypted pseudo-features are valid proxies for sample importance in the original raw feature space.

### Mechanism 2: Task-Aware Temperature Scaling (TTS)
The method dynamically adjusts the softmax temperature to sharpen the decision boundary for historical (minority) classes while smoothing it for current (majority) classes, mitigating overwriting. A lower temperature (τ_old < 1) is applied to logits of replayed samples to sharpen the distribution and enhance sensitivity to minority classes, while a higher temperature (τ_new > 1) is applied to new samples to smooth the distribution. This prevents the gradient from the dominant new classes from overwhelming the gradient signal of the scarce old classes.

### Mechanism 3: Loss Reweighting via Sample Frequency
The TTS module combines temperature scaling with explicit weights to amplify the optimization effect of tail-class samples during training. By assigning higher scalar weights to the loss of replayed samples (ω_old > ω_new), the optimizer is forced to prioritize retaining historical knowledge over fitting new data patterns rapidly. This approach balances the optimization landscape without requiring generative upsampling.

## Foundational Learning

- **Concept: Federated Class-Incremental Learning (FCIL) & Catastrophic Forgetting**
  - Why needed: The entire architecture is designed to solve the specific forgetting problem in FCIL where updating a model on new tasks shifts weights and destroys performance on old tasks.
  - Quick check: If a client has only "Dog" images in Task 2 and no replay buffer, what happens to its ability to classify "Cat" from Task 1?

- **Concept: Singular Value Decomposition (SVD) & Leverage Scores**
  - Why needed: This is the "brain" of the GDR module. SVD decomposes a matrix into singular values representing variance magnitude, and leverage scores use left-singular vectors to identify samples that heavily influence the data structure.
  - Quick check: If a sample has a leverage score near 0, is it likely a redundant point or a unique outlier in the latent space? (Hint: The paper argues it is redundant/unimportant).

- **Concept: Softmax Temperature Scaling**
  - Why needed: This powers the TTS module by controlling the "sharpness" of the probability distribution. Temperature T controls how confident the model appears - T → 0 creates a "hard" one-hot vector, while T → ∞ creates a uniform distribution.
  - Quick check: To make a model "more confident" (i.e., increase gradients for the correct class), should you increase or decrease the temperature?

## Architecture Onboarding

- **Component map**: Client Node: Local Trainer → ISVD Encoder (generates X'_k using random P_k, Q) → Transmitter. Server Node: Aggregator → Global SVD Analyzer (Computes U, Σ, V) → Leverage Score Calculator → Index Broadcaster. Local Buffer: Storage for selected indices + raw data.

- **Critical path**:
  1. Client training round finishes
  2. ISVD Encryption: Clients generate pseudo-features (Crucial: Privacy constraint)
  3. Global Selection: Server computes leverage scores and picks top k indices per class
  4. Synchronization: Server sends indices back; clients load specific samples into B_pre
  5. TTS Training: Next round uses L_TTS with dual temperatures

- **Design tradeoffs**:
  - Privacy vs. Utility: Random orthogonal matrices guarantee privacy but add computational overhead and may obscure fine-grained feature details
  - Buffer Size (M): Smaller buffers require higher precision in leverage score selection; larger buffers increase local storage costs

- **Failure signatures**:
  - "Class Collapse": If GDR fails to select samples for a specific minority class, the TTS module cannot recover that class
  - "Gradient Explosion": If τ_old is too small (e.g., 0.1), logits become too sharp, causing numerical instability in cross-entropy loss

- **First 3 experiments**:
  1. Sanity Check (Privacy Bypass): Run GDR without random orthogonal encryption to establish "upper bound" of SVD-based sampling utility
  2. TTS Ablation (Temp Sensitivity): Fix replay buffer and vary τ_old from 0.5 to 1.0, plotting accuracy on Task 1 vs. Task Current
  3. Heterogeneity Stress Test: Simulate worst-case non-IID scenario (Dirichlet β=0.1) to verify if global leverage scores correct local imbalance

## Open Questions the Paper Calls Out
1. How can the Global-perspective Data Replay (GDR) module be modified to reduce feature transmission costs without compromising global coordination benefits? The authors plan to investigate lightweight sampling strategies to reduce feature transmission costs in FedCBDR.

2. Can the Task-aware Temperature Scaling (TTS) module be made adaptive to remove the need for manual hyperparameter tuning for different tasks? The paper identifies the need to develop more robust post-sampling balancing methods that mitigate class imbalance with less sensitivity to hyperparameters.

3. What are the theoretical privacy guarantees of the Inverse Singular Value Decomposition (ISVD) mechanism against feature reconstruction attacks? The paper claims the GDR module is privacy-preserving but provides no formal privacy analysis or robustness testing against specific reconstruction attacks.

## Limitations
- The SVD-based privacy mechanism has limited empirical validation in related works, which typically use generative or gradient-based replay instead
- Exact implementation details for random orthogonal matrix generation and buffer management are underspecified, creating potential reproducibility gaps
- Temperature scaling ranges are narrow and may not generalize across datasets with different difficulty levels

## Confidence
- **High Confidence**: The core mechanism of task-aware temperature scaling and its effectiveness in mitigating imbalance between historical and current task samples is well-supported by experimental results showing 2%-15% accuracy improvements
- **Medium Confidence**: The leverage score-based global sampling approach is theoretically justified, but practical utility loss from privacy-preserving encryption is not quantified against non-encrypted baselines
- **Low Confidence**: Exact implementation details of SVD encryption and buffer management are insufficient for perfect reproducibility

## Next Checks
1. **Privacy vs Utility Benchmark**: Run GDR module without random orthogonal encryption to establish upper bound performance, then compare against encrypted version to quantify privacy-induced utility loss

2. **Temperature Scaling Sensitivity**: Fix replay buffer and systematically vary τ_old from 0.5 to 1.0, plotting per-task accuracy to identify optimal temperature ranges for different datasets

3. **Non-IID Stress Test**: Simulate worst-case heterogeneity (Dirichlet β=0.1) to verify whether global leverage scores actually correct local imbalance or if the global space becomes too noisy for effective sampling