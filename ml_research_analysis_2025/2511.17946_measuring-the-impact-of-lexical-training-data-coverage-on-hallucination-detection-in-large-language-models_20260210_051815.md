---
ver: rpa2
title: Measuring the Impact of Lexical Training Data Coverage on Hallucination Detection
  in Large Language Models
arxiv_id: '2511.17946'
source_url: https://arxiv.org/abs/2511.17946
tags:
- n-gram
- features
- training
- tree
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether lexical training data coverage
  can serve as a signal for detecting hallucinations in large language models. The
  authors construct a scalable suffix array over the RedPajama corpus to retrieve
  n-gram statistics for both prompts and model generations, and evaluate their effectiveness
  for hallucination detection across three QA benchmarks.
---

# Measuring the Impact of Lexical Training Data Coverage on Hallucination Detection in Large Language Models

## Quick Facts
- arXiv ID: 2511.17946
- Source URL: https://arxiv.org/abs/2511.17946
- Authors: Shuo Zhang; Fabrizio Gotti; Fengran Mo; Jian-Yun Nie
- Reference count: 40
- This paper investigates whether lexical training data coverage can serve as a signal for detecting hallucinations in large language models.

## Executive Summary
This paper investigates whether lexical training data coverage can serve as a signal for detecting hallucinations in large language models. The authors construct a scalable suffix array over the RedPajama corpus to retrieve n-gram statistics for both prompts and model generations, and evaluate their effectiveness for hallucination detection across three QA benchmarks. They find that while occurrence-based features are weak predictors when used alone, they yield modest gains when combined with log-probabilities, particularly on datasets with higher intrinsic model uncertainty. These findings suggest that lexical coverage features provide a complementary signal for hallucination detection.

## Method Summary
The authors build a suffix array index over the RedPajama corpus (1.3T tokens) to enable efficient n-gram frequency queries. For each prompt-generation pair, they extract: (1) generation log-probabilities, (2) prompt log-probabilities, (3) raw n-gram counts (1-5 grams for prompts, 1-2 grams for generations), (4) n-gram language model scores, and (5) key phrase frequencies via GPT-4o. They evaluate standalone feature predictive value using AUROC and classifier performance using decision trees and MLPs on balanced 50/50 splits across three QA benchmarks (TriviaQA, CoQA, NQ-Open).

## Key Results
- Occurrence-based features alone have low AUROC (0.50-0.55) but improve classifier accuracy when combined with log-probabilities, especially on datasets with higher model uncertainty
- In extractive QA, generation-side occurrence frequency strongly correlates with truthfulness, while prompt-side features show modest predictive value
- Decision trees using combined features achieve 10-20 point accuracy improvements over log-probability only models on certain datasets
- Tree analysis reveals that format-driven hallucinations (e.g., "Q:" outputs) cluster in specific n-gram occurrence ranges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lexical training-data coverage provides a complementary signal to model-internal confidence measures for hallucination detection.
- Mechanism: N-gram occurrence statistics capture direct lexical overlap with pretraining data, whereas learned embeddings encode distributed co-occurrence patterns. Raw counts reflect whether specific token sequences were explicitly seen during training, potentially revealing out-of-distribution behavior that log-probabilities miss.
- Core assumption: Surface-level statistics offer different information than compressed model representations.
- Evidence anchors:
  - [abstract] "occurrence-based features are weak predictors when used alone, they yield modest gains when combined with log-probabilities, particularly on datasets with higher intrinsic model uncertainty"
  - [Section 1] "raw n-gram occurrence statistics reflect the direct lexical overlap of specific token sequences. As such, they may better capture novelty or out-of-distribution behavior"
  - [corpus] Related work on manifold-based sampling (arXiv:2601.06196) and predictive coding approaches (arXiv:2601.15652) similarly combine multiple signals, suggesting complementarity is a robust pattern, though direct corpus evidence for lexical coverage specifically is limited.
- Break condition: If model representations already encode exact memorization patterns without compression loss, lexical overlap would provide no additional information.

### Mechanism 2
- Claim: In extractive QA, generation-side occurrence frequency strongly correlates with truthfulness.
- Mechanism: When answers are extracted from provided passages, the generated span's frequency in pretraining data indicates whether the model is reproducing a well-attested fact versus fabricating. High-frequency spans are more likely correct because they reflect genuine knowledge.
- Core assumption: Extractive answers map to entities/facts with consistent pretraining exposure.
- Evidence anchors:
  - [Section 5.1] "we observe a strong correlation between the corpus occurrence of the extracted answer span and faithfulness under the EM evaluation setting in extractive QA"
  - [Section 5.1] "For generation-side signals, we typically observe an S-shaped trend: highly frequent generations are often hallucinated... whereas less frequent ones below a certain threshold tend to represent genuine entities"
  - [corpus] No direct corpus corroboration for this specific extractive-QA finding.
- Break condition: If extractive tasks require reasoning beyond retrieval (e.g., multi-hop), frequency may not indicate correctness.

### Mechanism 3
- Claim: Occurrence features stabilize decision boundaries by catching format-driven hallucinations that log-probabilities miss.
- Mechanism: Certain failure modes (e.g., outputting "Q:" instead of answers) cluster in narrow n-gram occurrence ranges. These artifacts have high log-probability (common patterns) but anomalous lexical profiles under the training distribution, enabling occurrence-based classifiers to flag them.
- Core assumption: Format artifacts exhibit consistent lexical signatures distinct from genuine answers.
- Evidence anchors:
  - [Section 6.1] "the tree with occurrence features cleanly separates 57 hallucinated instances along the path gen_occ_2 > -1.73 → gen_occ_2 ≤ -1.39"
  - [Section 6.1] Table 3 shows examples where model outputs "Q:" and these fall within a specific 2-gram occurrence range
  - [corpus] No direct corpus evidence for this artifact-detection mechanism.
- Break condition: If model generates fluent but factually wrong content that mimics training distribution (adversarial hallucinations), occurrence features won't help.

## Foundational Learning

- Concept: **Suffix arrays**
  - Why needed here: Core infrastructure enabling O(log m) n-gram lookup over 1.3T-token corpus without exhaustive search.
  - Quick check question: Given a tokenized corpus, can you explain how a suffix array enables efficient substring frequency queries?

- Concept: **N-gram language modeling**
  - Why needed here: The paper uses both raw frequency (Eq. 3) and conditional likelihood scoring (Eq. 4); understanding the difference is essential for feature engineering.
  - Quick check question: Why might a conditional n-gram score (Eq. 4) behave differently than raw frequency (Eq. 3) for rare vs. common prompts?

- Concept: **AUROC for binary classification**
  - Why needed here: Primary metric for evaluating standalone feature predictive value; AUROC > 0.5 indicates signal above random.
  - Quick check question: If a feature has AUROC = 0.55, is it practically useful alone? What if combined with another feature?

## Architecture Onboarding

- Component map:
  - **Suffix array index**: Rust-based SA-IS implementation with PyO3 bridge; built over flattened, tokenized RedPajama corpus (1.3T tokens)
  - **Feature extractors**: (1) Log-probability extractor (model forward pass), (2) N-gram occurrence counter (suffix array queries), (3) Key phrase extractor (GPT-4o for answer-seeking spans)
  - **Classifier**: Decision tree (depth 3–20) or MLP (32–64–32); inputs are standardized feature vectors
  - **Evaluation**: AUROC for feature analysis, accuracy for classifier comparison

- Critical path:
  1. Tokenize prompt and generation with model tokenizer
  2. Extract n-grams (1≤n≤5 for prompts, 1–2 for generations)
  3. Query suffix array for counts → compute S_raw and/or S_ng
  4. Extract log-probabilities from model forward pass
  5. Train classifier on balanced 50/50 hallucinated/non-hallucinated splits

- Design tradeoffs:
  - **N-gram length**: 3–4 grams perform best; 5-grams suffer sparsity (44.4% zero-occurrence in NQ-Open)
  - **Stopword filtering**: Modest AUROC gains for short n-grams, but limited overall impact
  - **Raw frequency vs. conditional scoring**: Raw frequency more interpretable; conditional scoring better captures lexical familiarity but requires prefix counts

- Failure signatures:
  - High training accuracy but low test accuracy with deep trees → overfitting to dataset-specific patterns
  - AUROC ≈ 0.5 for occurrence features alone → confirm this is expected; value is in combination
  - Zero prediction consistency across bootstrap runs → model is unstable on that dataset (log-only showed 0.00 on NQ-Open)

- First 3 experiments:
  1. **Baseline replication**: Reproduce AUROC curves (Figure 1) for log-probability vs. occurrence features on TriviaQA using the provided suffix array code; confirm log-probability outperforms standalone.
  2. **Feature ablation**: Train depth-3 decision trees with log-probability only vs. full feature set; measure test accuracy gap on NQ-Open (expected: ~14-point improvement with full features per Table 2).
  3. **Artifact detection test**: Filter CoQA generations for cases where output contains "Q:" or similar prompt artifacts; verify these cluster in low 2-gram occurrence ranges as claimed in Section 6.1.

## Open Questions the Paper Calls Out

- **Question**: Would occurrence-based detection features generalize to larger models (e.g., 70B+ parameters) with instruction-tuning and RLHF alignment, where post-training may alter the relationship between pretraining coverage and hallucination?
  - **Basis in paper**: [explicit] Limitations section states findings are based on RedPajama-INCITE models and "may not fully generalize to larger models with more sophisticated pretraining and post-training alignments."
  - **Why unresolved**: The study only evaluates 3B and 7B base models without instruction-tuning or alignment, leaving the effect of post-training on coverage-hallucination relationships untested.
  - **What evidence would resolve it**: Replicating the n-gram coverage experiments on instruction-tuned models (e.g., LLaMA-2-Chat, GPT-4 class models) and comparing AUROC and classifier performance.

- **Question**: Could incorporating semantic similarity (paraphrasing, synonymy) substantially improve the predictive power of coverage-based features beyond exact n-gram matching?
  - **Basis in paper**: [explicit] Limitations section notes "our analysis is limited to exact n-gram frequency counts and does not account for paraphrasing, synonymy, or semantic similarity."
  - **Why unresolved**: The study operationalizes coverage via surface-level token overlap, but conceptually similar content may receive low coverage scores if phrased differently, potentially losing signal.
  - **What evidence would resolve it**: Augmenting occurrence features with semantic matching (e.g., embedding-based retrieval, paraphrase detection) and measuring performance gains against the exact-match baseline.

- **Question**: Is the observed relationship between training data coverage and hallucination causal, or does coverage merely correlate with other factors (e.g., question difficulty, topic familiarity)?
  - **Basis in paper**: [explicit] Limitations section states "our results are correlational and do not establish a causal relationship between training data exposure and hallucination behavior."
  - **Why unresolved**: High n-gram coverage may coincide with well-known topics where models perform well for reasons beyond memorization (e.g., better representations from diverse contexts), making causation difficult to isolate.
  - **What evidence would resolve it**: Controlled experiments manipulating training data exposure (e.g., training models on data with artificially modified n-gram frequencies) and measuring resulting hallucination rate changes.

## Limitations
- Findings based on 3B and 7B base models may not generalize to larger models with instruction-tuning or alignment
- Analysis limited to exact n-gram frequency counts, not accounting for paraphrasing or semantic similarity
- Results are correlational and do not establish causal relationship between training data exposure and hallucination behavior

## Confidence
- **Suffix array construction**: High - Rust implementation with PyO3 bridge specified
- **Feature extraction**: Medium - Key phrase extraction prompt unspecified
- **Classifier training**: High - Standard hyperparameters and procedures provided
- **Evaluation methodology**: High - Clear metrics and validation procedures

## Next Checks
1. Build suffix array on RedPajama subset and verify n-gram frequency queries return expected counts
2. Confirm AUROC curves match Figure 1 for log-probability vs. occurrence features on TriviaQA
3. Test that format artifacts (e.g., "Q:" outputs) cluster in specific n-gram occurrence ranges as claimed