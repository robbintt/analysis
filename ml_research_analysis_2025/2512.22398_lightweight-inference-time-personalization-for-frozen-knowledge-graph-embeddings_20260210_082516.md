---
ver: rpa2
title: Lightweight Inference-Time Personalization for Frozen Knowledge Graph Embeddings
arxiv_id: '2512.22398'
source_url: https://arxiv.org/abs/2512.22398
tags:
- personalization
- features
- alignment
- rank
- frozen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes GatedBias, a lightweight inference-time personalization
  framework that adapts frozen KG embeddings to individual user contexts without retraining
  or compromising global accuracy. The key idea is gate personalization through graph
  structure: entity attributes extracted from the training KG serve as binary gates,
  while profile features act as conditioning signals over these gates.'
---

# Lightweight Inference-Time Personalization for Frozen Knowledge Graph Embeddings

## Quick Facts
- arXiv ID: 2512.22398
- Source URL: https://arxiv.org/abs/2512.22398
- Authors: Ozan Oguztuzun; Cerag Oguztuzun
- Reference count: 3
- Primary result: GatedBias achieves 6--30× greater rank improvements when preference signals are boosted

## Executive Summary
This paper introduces GatedBias, a lightweight inference-time framework that personalizes frozen knowledge graph (KG) embeddings for individual users without retraining. The key insight is separating entity attribute gates from user profile conditioning signals, allowing personalized candidate ranking through a small number of trainable parameters while keeping the backbone model frozen. Evaluation on Amazon-Book and Last-FM datasets shows statistically significant improvements in alignment metrics while preserving cohort performance.

## Method Summary
GatedBias operates by extracting binary gates from entity attributes in the training KG, then using user profile features to condition these gates during inference. This architecture enables personalization without gradient flow through the frozen backbone embeddings. The framework maintains parameter efficiency while achieving strong personalization performance, validated through counterfactual perturbation experiments that demonstrate causal responsiveness to preference signals.

## Key Results
- Statistically significant improvements in alignment metrics on Amazon-Book and Last-FM datasets
- 6--30× greater rank improvements when preference signals are boosted
- Preserves cohort performance while enabling individual personalization
- Requires only a few trainable parameters compared to full retraining approaches

## Why This Works (Mechanism)
The mechanism works by leveraging the inherent structure of knowledge graphs to create a gating mechanism that can be conditioned on user-specific features. By extracting binary attributes from entities and using them as gates, the framework creates a sparse, interpretable personalization layer. The user profile features then act as conditioning signals that modulate these gates, allowing for personalized ranking without modifying the frozen embeddings themselves.

## Foundational Learning
- **Knowledge Graph Embeddings**: Dense vector representations of entities and relations; needed to understand the frozen model being personalized
  - Quick check: Can represent triples as vector operations
- **Graph Structure Attributes**: Binary properties derived from KG topology; needed to create gating mechanism
  - Quick check: Can identify entity types and relational patterns
- **Inference-time Adaptation**: Techniques for modifying model behavior without retraining; needed to understand personalization approach
  - Quick check: Can distinguish from fine-tuning paradigms
- **Personalization in Recommender Systems**: Methods for adapting recommendations to individual users; needed to contextualize the problem
  - Quick check: Can explain cold-start vs warm-start personalization
- **Causal Inference in ML**: Methods for establishing cause-effect relationships; needed to validate personalization effectiveness
  - Quick check: Can design counterfactual experiments

## Architecture Onboarding

**Component Map**: User Profile Features -> Gate Conditioning Layer -> Binary Entity Attribute Gates -> Frozen KG Embeddings -> Personalized Rankings

**Critical Path**: User profile features modulate binary gates derived from entity attributes, which then weight interactions with frozen KG embeddings to produce personalized rankings.

**Design Tradeoffs**: 
- Freezes backbone model to avoid computational overhead and catastrophic forgetting
- Uses binary gates for interpretability and parameter efficiency
- Separates conditioning from gating to maintain modularity

**Failure Signatures**:
- Poor KG quality leading to uninformative gates
- Profile features poorly aligned with entity attributes
- Overly sparse gating reducing personalization effectiveness

**Three First Experiments**:
1. Ablation study removing profile conditioning to measure baseline gate effectiveness
2. Varying gate sparsity to find optimal balance between personalization and generalization
3. Testing on entities with no associated attributes to evaluate fallback behavior

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on binary attribute gates may limit expressiveness for nuanced preferences
- Scalability to massive KGs with millions of entities remains unclear
- Performance may degrade significantly with lower KG quality
- Computational overhead of maintaining user-specific gate weights at scale

## Confidence

**High Confidence**: Core technical contribution is well-defined and mathematically sound; separation of gradient flow is clearly implemented and validated

**Medium Confidence**: Empirical improvements are statistically significant but practical significance unclear; counterfactual experiments compelling but perturbation magnitude may not reflect realistic shifts

**Medium Confidence**: Claims of preserving cohort performance are supported but trade-off analysis could be more comprehensive

## Next Checks

1. **Cross-domain generalization**: Test GatedBias on non-recommendation KG tasks (e.g., question answering or entity linking) to evaluate generalization beyond collaborative filtering

2. **Dynamic preference adaptation**: Implement online learning variant with incremental gate parameter updates, measuring personalization accuracy and computational overhead over extended periods

3. **KG quality sensitivity analysis**: Systematically degrade KG completeness and correctness to quantify performance degradation under realistic data quality variations, comparing against baseline personalization approaches