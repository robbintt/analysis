---
ver: rpa2
title: 'Steering LLMs toward Korean Local Speech: Iterative Refinement Framework for
  Faithful Dialect Translation'
arxiv_id: '2511.06680'
source_url: https://arxiv.org/abs/2511.06680
tags:
- dialect
- translation
- dia-refine
- metrics
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of translating standard Korean
  into regional dialects (Jeolla, Gyeongsang, Jeju), where large language models often
  default to standard Korean rather than producing authentic dialectal outputs. The
  authors propose the DIA-REFINE framework, which iteratively refines translations
  using feedback from an ensemble of dialect classifiers to guide models toward the
  target dialect.
---

# Steering LLMs toward Korean Local Speech: Iterative Refinement Framework for Faithful Dialect Translation

## Quick Facts
- arXiv ID: 2511.06680
- Source URL: https://arxiv.org/abs/2511.06680
- Reference count: 0
- Large language models often default to standard Korean instead of producing authentic dialectal outputs when translating to regional dialects (Jeolla, Gyeongsang, Jeju).

## Executive Summary
This paper addresses the challenge of translating standard Korean into regional dialects, where large language models often default to standard Korean rather than producing authentic dialectal outputs. The authors propose the DIA-REFINE framework, which iteratively refines translations using feedback from an ensemble of dialect classifiers to guide models toward the target dialect. To address evaluation limitations of n-gram metrics, which favor surface similarity over dialectal fidelity, the paper introduces two new metrics: the dialect fidelity score (DFS) and target dialect ratio (TDR). Experiments show that DIA-REFINE consistently improves dialect fidelity, particularly the multi-candidate variant, and that traditional n-gram metrics often mask failures by rewarding source copying.

## Method Summary
The DIA-REFINE framework implements an iterative loop of translation, verification, and feedback using external dialect classifiers. The process generates an initial translation, verifies it with an ensemble dialect classifier, and if misclassified, requests retranslation with explicit feedback about the wrong label. Two variants are tested: single-candidate (DIA-REFINE(S)) and multi-candidate (DIA-REFINE(M)), where k candidates are generated and selected via classifier posterior probability. The framework is evaluated using novel metrics—dialect fidelity score (DFS) measuring cosine similarity in dialect-aware embedding space, and target dialect ratio (TDR) measuring classification success—to address limitations of traditional n-gram metrics that reward source copying over genuine dialect translation.

## Key Results
- DIA-REFINE improves dialect fidelity across all tested models, with the multi-candidate variant showing the most pronounced corrective effect (TDR reaching 0.28 under ICL vs. 0.06 for single-candidate)
- Traditional n-gram metrics (BLEU/chrF++) often mask failures by rewarding source copying, while DFS and TDR successfully distinguish "False Success" cases (high n-gram, low dialect fidelity) from genuine dialect attempts
- Model responsiveness varies significantly; some models show minimal gains even with ICL + DIA-REFINE(M), suggesting the framework amplifies existing dialect capability rather than creating it

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Iterative feedback from external dialect classifiers guides LLMs toward higher-fidelity dialect outputs.
- **Mechanism:** The framework generates an initial translation → an ensemble dialect classifier verifies whether the output matches the target dialect → if misclassified, explicit feedback (including the wrong label) is injected into the retry prompt → the model re-translates with corrective guidance, up to two retries.
- **Core assumption:** LLMs can incorporate explicit, task-specific feedback about classification errors to adjust subsequent outputs.
- **Evidence anchors:** [abstract]: "iterative loop of translation, verification, and feedback using external dialect classifiers"; [section 3.3]: "If it was classified as a non-targeted dialect (i.e., C(y) ≠ d_tgt), the framework requests retranslation using a prompt that contains explicit feedback"; [corpus: Speech Translation Refinement using Large Language Models]: Reports that LLM-based iterative refinement improves translation quality.

### Mechanism 2
- **Claim:** Multi-candidate generation with classifier-guided selection outperforms single-candidate refinement for recovering from initial failures.
- **Mechanism:** Generate k candidates → rank by dialect classifier's posterior probability → select top candidate for verification → retry if needed, incrementally expanding k on each retry.
- **Core assumption:** Increasing the search space raises the probability that at least one candidate satisfies the dialect condition.
- **Evidence anchors:** [section 6.3]: "DIA-REFINE(M) demonstrates the most pronounced corrective effect in converting initial failures into successful dialect translations" with TDR reaching 0.28 under ICL; [table 5]: Multi-candidate variant achieves model-avg TDR of 0.62 vs. 0.42 for single-candidate variant.

### Mechanism 3
- **Claim:** Dialect-aware embedding metrics (DFS) and classification-based metrics (TDR) unmask "False Success" cases that n-gram metrics reward.
- **Mechanism:** DFS computes log-ratio of cosine similarities between hypothesis–dialect reference and hypothesis–standard source in a fine-tuned embedding space; TDR measures proportion of outputs classified as the target dialect. Together, they penalize source-copying and reward genuine dialectal shifts.
- **Core assumption:** Embeddings from a dialect-aware encoder capture linguistically meaningful variation that surface n-gram overlap cannot.
- **Evidence anchors:** [abstract]: "distinguish between False Success cases, where high n-gram scores obscure failures in dialectal translation, and True Attempt cases"; [table 1]: Hypothesis 2 (False Success) scores BLEU 52.54 / DFS -0.672; Hypothesis 1 (True Attempt) scores BLEU 27.78 / DFS 0.672.

## Foundational Learning

- **Concept: Ensemble Classification for Robustness**
  - Why needed here: DIA-REFINE depends on reliable dialect verification. A single classifier may have blind spots; ensemble aggregation reduces variance and improves trust in the feedback signal.
  - Quick check question: Given three classifiers with accuracies 0.93, 0.91, and 0.94, what ensemble strategy would you use to maximize robustness?

- **Concept: Iterative Prompting with Explicit Feedback**
  - Why needed here: The core loop requires understanding how to structure retry prompts that convey error information (wrong label, oscillation detection) without confusing the model.
  - Quick check question: If a model's first output is classified as "Standard" instead of "Jeju," what feedback element must the retry prompt include for DIA-REFINE?

- **Concept: Embedding Space Geometry for Evaluation**
  - Why needed here: DFS relies on cosine similarity in a dialect-aware embedding space. Understanding why fine-tuned embeddings outperform vanilla embeddings is critical for interpreting results.
  - Quick check question: Why does DFS use the log-ratio of similarities rather than raw cosine similarity?

## Architecture Onboarding

- **Component map:** LLM Translator → Ensemble Dialect Classifier → Feedback Generator → Retry Prompt → LLM Translator
- **Critical path:**
  1. Input standard Korean sentence → LLM generates candidate(s)
  2. Classifier predicts dialect label and posterior probability
  3. If label ≠ target dialect: construct feedback prompt → retry (max 2 retries)
  4. If label = target dialect or retries exhausted: return output
  5. Evaluate final output with DFS, TDR, and n-gram metrics

- **Design tradeoffs:**
  - Single-candidate (S) vs. Multi-candidate (M): M achieves higher TDR but increases inference cost (k× generation)
  - k value: Starting at 3 and incrementing on each retry improves exploration but compounds cost
  - Retry limit: Two retries balance correction opportunity vs. latency; more retries show diminishing returns

- **Failure signatures:**
  - Oscillation: Model alternates between two wrong dialect labels on consecutive retries (detected and flagged with special prompt)
  - Persistent False Success: High BLEU/chrF++ with negative DFS and near-zero TDR—model defaults to standard Korean
  - Classifier overconfidence: If ensemble confidence is miscalibrated, selection may favor incorrect candidates

- **First 3 experiments:**
  1. Establish baseline False Success rate: Run ZS translation across all target dialects, compute BLEU vs. TDR to quantify the evaluation gap.
  2. Ablate feedback specificity: Compare DIA-REFINE with full feedback (wrong label + target dialect) vs. minimal feedback (only "incorrect") to validate feedback design.
  3. Sweep k values for multi-candidate: Test k ∈ {2, 3, 5} under ICL + DIA-REFINE(M) to find cost-quality trade-off point beyond which TDR gains plateau.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the iterative refinement process in DIA-REFINE preserve the semantic equivalence of the source text, or does it inadvertently induce hallucinations or meaning shifts while optimizing for dialectal fidelity?
- Basis in paper: Section 9 (Limitations) explicitly states that "Semantic preservation was not evaluated" and that "qualitative aspects such as fluency and naturalness require verification."
- Why unresolved: The proposed metrics (DFS, TDR) measure stylistic shift and target dialect classification success, but they do not verify if the meaning remains constant during the re-translation loop.
- What evidence would resolve it: A human evaluation by native speakers to assess meaning preservation, or the integration of a semantic evaluation metric (e.g., COMET) to correlate dialect fidelity with semantic accuracy.

### Open Question 2
- Question: Can the DIA-REFINE framework remain effective in data-scarce scenarios where training a high-performance ensemble classifier (requiring thousands of labeled samples) is not feasible?
- Basis in paper: Section 9 notes that the framework's efficacy relies on an external classifier requiring "substantial labeled data," leaving its "performance under data-scarce conditions undetermined."
- Why unresolved: The current implementation relies on a robust 5-model ensemble trained on 10,000 samples per class; it is unknown if a weaker verifier (e.g., few-shot classifier) would provide sufficient signal to steer the LLM.
- What evidence would resolve it: Experiments comparing the framework's performance using full ensemble verification versus weak supervision (e.g., single small model or zero-shot LLM-as-a-judge) on low-resource dialects.

### Open Question 3
- Question: Is the DIA-REFINE framework effective for "standard-to-dialect" translation where the linguistic divergence between the standard and dialect is subtle, rather than salient?
- Basis in paper: Section 9 states that additional methods are required to "handle dialects lacking distinctive characteristics" (like Gangwon or Chungcheong), as this study focused only on dialects with "pronounced features."
- Why unresolved: The classifier and feedback loop may struggle to converge if the linguistic boundary between the standard language and the target dialect is ambiguous or overlapping.
- What evidence would resolve it: Application of the framework to dialects with low lexical/phonetic distance from standard Korean, measuring the TDR and convergence rate compared to the high-salience dialects tested in the paper.

## Limitations

- The framework's dependence on high-quality dialect classifiers introduces a critical bottleneck, with ensemble accuracy still leaving ~5% of cases where feedback may be incorrect or noisy.
- The evaluation assumes that dialect fidelity, as measured by DFS and TDR, correlates with human perceptions of naturalness—a relationship not directly validated with human raters.
- Some models (HyperCLOVAX, Llama-3.1, EEVE) show minimal gains even with ICL + DIA-REFINE(M), suggesting the framework amplifies existing model capability rather than creating it.

## Confidence

- **High Confidence:** The observation that n-gram metrics (BLEU, chrF++) systematically fail to detect "False Success" cases where models copy the source rather than translate into dialect.
- **Medium Confidence:** The claim that DIA-REFINE consistently improves dialect fidelity across models, though improvements are model-dependent.
- **Low Confidence:** The assertion that the multi-candidate variant is universally superior, as the paper does not report on the cost-benefit trade-off or demonstrate statistical significance.

## Next Checks

1. **Human Evaluation Validation:** Conduct a human evaluation study comparing model outputs before and after DIA-REFINE refinement, focusing on whether improved TDR and DFS correlate with perceived naturalness and authenticity of dialect usage.

2. **Classifier Robustness Stress Test:** Evaluate the ensemble classifier's performance on adversarial inputs—dialectal sentences with high standard Korean overlap—to quantify the risk of incorrect feedback triggering counterproductive refinements.

3. **Cost-Quality Pareto Analysis:** Systematically measure inference time and token usage for DIA-REFINE(S) vs. DIA-REFINE(M) across varying k values, mapping TDR improvement against computational cost to identify optimal operating points.