---
ver: rpa2
title: On Reasoning Strength Planning in Large Reasoning Models
arxiv_id: '2506.08390'
source_url: https://arxiv.org/abs/2506.08390
tags:
- reasoning
- token
- strength
- vectors
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how large reasoning models (LRMs) pre-plan
  and control their reasoning strength (measured by the number of reasoning tokens)
  before generating answers. Using linear probing, the authors show that reasoning
  token counts can be predicted from question activations with correlations over 0.8,
  indicating LRMs plan reasoning strength in advance.
---

# On Reasoning Strength Planning in Large Reasoning Models

## Quick Facts
- arXiv ID: 2506.08390
- Source URL: https://arxiv.org/abs/2506.08390
- Reference count: 40
- Key outcome: Large reasoning models encode reasoning token count in early activations, enabling prediction and steering via activation modification.

## Executive Summary
This paper investigates how large reasoning models (LRMs) pre-plan and control their reasoning strength before generating answers. Using linear probing, the authors show that reasoning token counts can be predicted from question activations with correlations over 0.8, indicating LRMs plan reasoning strength in advance. They discover that LRMs encode this strength through pre-allocated direction vectors in their activation space, where the vector's magnitude controls reasoning token allocation. Manipulating these vectors via activation steering causally affects reasoning length and performance. The study also reveals these vectors modify logits of the end-of-reasoning token to control reasoning termination. Two applications are demonstrated: detecting overthinking behavior and enabling efficient reasoning on simple problems by reducing unnecessary reasoning tokens. The findings provide insights into LRM reasoning mechanisms and offer practical tools for controlling reasoning behavior.

## Method Summary
The study extracts d-dimensional residual stream activations at the ` thread` token position from R1-Distill-Qwen-7B model. A Lasso regression (α=10) is trained to predict reasoning token count from these activations. The difference-in-means vector is computed between difficulty levels to identify the reasoning strength direction. Activation steering is applied during inference by adding λ-scaled versions of this vector to the residual stream before the first reasoning token generation. The approach is validated on MATH dataset (5 difficulty levels, 9:1 train/test split) and evaluated on MATH500, AIME2024, OlympiadBench, and MMLU benchmarks.

## Key Results
- Linear probes achieve R > 0.8 correlation between question activations and reasoning token counts
- Steering vectors exhibit cosine similarity ~0.99 across difficulty levels, indicating a shared direction
- Activation steering with λ ∈ [-0.2, 0.2] can reduce or increase reasoning tokens by >20% without significant accuracy loss
- The steering vector primarily affects logits of the end-of-reasoning token rather than other tokens

## Why This Works (Mechanism)

### Mechanism 1: Pre-computation of Reasoning Allocation
Large Reasoning Models determine the computational budget (reasoning token count) for a query before generating the reasoning trace. The model encodes a "reasoning strength" signal into residual stream activations at the start-of-reasoning token, which is linearly decodable. This indicates the model has "planned" the effort required based on the input prompt's embeddings. Evidence shows prediction accuracy improves in deeper layers, suggesting planning capability is refined as processing deepens.

### Mechanism 2: Magnitude-Controlled Directional Steering
The model allocates reasoning strength via a specific direction vector in activation space, where the vector's magnitude modulates the intensity of reasoning. A "pre-allocated" direction vector r^(l) in the residual stream is pushed further along for harder problems. Intervening by adding or subtracting a vector parallel to this direction causally increases or decreases reasoning length. The difference-in-means vector extracted from the dataset accurately approximates the model's intrinsic reasoning control direction.

### Mechanism 3: Termination Logit Suppression
Reasoning length is controlled by modulating the probability of generating the end-of-reasoning token. The directional vector influences the unembedding layer's output logits, specifically suppressing the logit for the termination token. A larger vector magnitude (indicating higher required reasoning) lowers the probability of stopping, thereby extending the chain-of-thought. The steering vector primarily impacts the end-of-reasoning token logits rather than random tokens.

## Foundational Learning

- **Residual Stream & Activation Patching**: The paper locates the "reasoning plan" in the residual stream. Understanding that activations are vectors that can be modified algebraically during the forward pass is essential for comprehending the intervention mechanism. Quick check: If you add vector v to the residual stream at layer 10, does it affect layer 11? (Answer: Yes, residual connections propagate it forward).

- **Linear Probing**: The evidence for "planning" rests entirely on the ability of a linear classifier to predict token counts from activations. Quick check: Does a high probing accuracy prove the model uses that information causally? (Answer: No, it proves the information is encoded; causality requires intervention).

- **Difference-in-Means (Diff-in-Means)**: The method for finding the steering vector relies on averaging activation differences between "short reasoning" and "long reasoning" inputs. Quick check: Why average over many samples rather than using one contrast pair? (Answer: To denoise the vector and isolate the specific feature of interest from random variance).

## Architecture Onboarding

- **Component map**: Input -> Prompt + ` thread` token -> Residual stream extraction at ` thread` token position -> Control Vector calculation -> Intervention: activation = activation + (lambda * control_vector)

- **Critical path**: 1) Run inference on calibration dataset to collect activations; 2) Train Linear Probe to verify predictability (R > 0.8); 3) Compute Diff-in-Means vector; 4) Apply vector during inference before first reasoning token generation

- **Design tradeoffs**: Lambda Selection (small lambda reduces tokens slightly; large lambda breaks coherence); Layer Selection (early layers disrupt semantic understanding; late layers too close to output); Vector normalization affects steering magnitude

- **Failure signatures**: Collapsed Reasoning (model jumps immediately to `` with empty reasoning - Lambda too negative); Infinite Loop (model never generates `` - Lambda too positive); Semantic Drift (model reasons about wrong topic - Vector contaminated)

- **First 3 experiments**: 1) Validation of Encoding: Train linear regression on activations to predict token counts, verify R > 0.8; 2) Causal Steering Test: Apply negative steering (λ = -0.2) on medium difficulty problem, verify reasoning length decreases >20% without accuracy drop; 3) Logit Inspection: Extract logits for `` token before/after steering, confirm negative steering increases logit (making stopping more likely)

## Open Questions the Paper Calls Out

- **Open Question 1**: Would non-linear probes (e.g., MLPs) significantly improve the prediction performance of reasoning token counts compared to the linear probes utilized in this study? The authors restricted their methodology to linear regression to prove the existence of a planning mechanism, leaving the potential for higher accuracy via non-linear relationships untested.

- **Open Question 2**: Do the pre-allocated reasoning strength planning mechanisms generalize to large reasoning models based on diverse backbones beyond the Qwen-series and Llama variants? While the phenomenon was observed in Qwen and briefly in Llama-8B, the universality of this specific "pre-allocated vector" mechanism across different architectural paradigms is unconfirmed.

- **Open Question 3**: Can the identified overthink detection and efficient reasoning potentials be transitioned into robust, real-world practice for model serving? The paper demonstrates the concept of reducing tokens via steering but does not validate if this method maintains robustness in production environments or complex, noisy pipelines.

## Limitations

- The study focuses on Qwen-series models, raising questions about generalizability to other LRM architectures
- The steering vector's purity is uncertain - it may encode multiple correlated features beyond reasoning length
- Correlation between activations and token counts does not establish deliberate planning mechanisms
- OOD robustness remains untested beyond controlled difficulty levels in mathematical domains

## Confidence

- **High Confidence (7/10)**: Existence of linearly decodable reasoning token count information; causal effect of activation steering; differential impact on end-of-reasoning token logits
- **Medium Confidence (5/10)**: Interpretation as deliberate computational budget planning; single shared directional vector claim; effectiveness for practical applications
- **Low Confidence (3/10)**: Generalization to non-mathematical domains; stability across temperature settings; long-term impact of repeated steering

## Next Checks

- **Check 1**: Cross-Architecture Validation - Apply methodology to at least two additional LRM architectures (e.g., QwQ-7B and DeepSeek-R1-Distill-Qwen-14B) to verify whether the directional vector mechanism is architecture-specific or a general LRM phenomenon.

- **Check 2**: Feature Isolation Experiment - Conduct ablation study applying steering vector to subsets of reasoning tokens (first half vs. second half) to determine whether it controls reasoning initiation, progression, or termination. Test steering on non-mathematical domains to assess domain specificity.

- **Check 3**: Dynamic Steering Protocol - Implement adaptive steering where lambda parameter adjusts based on intermediate reasoning quality signals (e.g., self-consistency among early reasoning steps) to test whether reasoning process is truly open-loop or can be dynamically modulated.