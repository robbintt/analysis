---
ver: rpa2
title: Is More Context Always Better? Examining LLM Reasoning Capability for Time
  Interval Prediction
arxiv_id: '2601.10132'
source_url: https://arxiv.org/abs/2601.10132
tags:
- llms
- reasoning
- temporal
- walmart
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically examines large language models (LLMs)
  for predicting time intervals between recurring user actions, such as repeated purchases.
  The authors benchmark GPT-4o, Gemini-2.5, and Claude 3.5 against both statistical
  and machine learning baselines across proprietary and Instacart datasets.
---

# Is More Context Always Better? Examining LLM Reasoning Capability for Time Interval Prediction

## Quick Facts
- arXiv ID: 2601.10132
- Source URL: https://arxiv.org/abs/2601.10132
- Reference count: 15
- Key outcome: LLMs underperform dedicated ML models on quantitative accuracy metrics for time interval prediction, with moderate context improving performance but excessive context degrading it.

## Executive Summary
This study systematically evaluates large language models (LLMs) for predicting time intervals between recurring user actions in e-commerce settings. Benchmarking GPT-4o, Gemini-2.5, and Claude 3.5 against statistical and machine learning baselines across proprietary and Instacart datasets, the authors find that while LLMs outperform simple statistical estimators, they consistently underperform dedicated ML models on quantitative accuracy metrics like MAPE and RMSE. The research reveals that moderate contextual information can slightly improve LLM accuracy, but excessive high-level context often degrades performance by introducing noise. These results highlight fundamental limitations in LLM reasoning for structured temporal tasks and suggest that richer context does not always lead to better reasoning.

## Method Summary
The study evaluates LLMs (GPT-4o, Gemini-2.5 Pro, Claude 3.5 Sonnet) in zero-shot settings for predicting inter-purchase time intervals using three context levels: Zero (historical intervals only), Medium (plus product metadata and summary stats), and High (plus recency features and user lifecycle details). ML baselines include RandomForest, XGBoost, and DNN with quantile loss, while statistical baselines use mean, median, and EMA per user-category. The evaluation uses two datasets: a proprietary grocery dataset (5,780 users, 12 product types, 110K+ records) and Instacart (2,661 users, 10 categories, ~194K orders), filtered to user-category pairs with ≥5 purchases and intervals ≤20 days. Performance is measured using RMSE, MAE, MAPE, and business-oriented tolerance accuracy (TA@0, TA@1, TA@2).

## Key Results
- LLMs outperform statistical baselines on tolerance accuracy metrics but underperform ML models by 50%+ on MAPE and RMSE
- Moderate context (Medium level) slightly improves LLM accuracy over zero-context, but high-context prompts degrade performance
- The performance gap is largest for error metrics, while LLMs show competitive TA@k performance, suggesting better approximate classification than exact regression
- Claude 3.5 Sonnet performs best among LLMs but still lags ML baselines significantly

## Why This Works (Mechanism)

### Mechanism 1
Moderate context improves temporal interval prediction, but excessive context degrades accuracy through a "context-as-noise" effect. Focused, decision-relevant information (basic product attributes, summary statistics) aids temporal reasoning by highlighting stable quantitative cues. Semantically rich but temporally irrelevant details divert model attention from underlying numerical patterns. Core assumption: LLMs weight narrative features similarly regardless of temporal relevance, causing attention dilution. Evidence anchors: [abstract] "although moderate context can improve LLM accuracy, adding further user-level detail degrades performance"; [section 3.1/RQ2] "high-level narrative context often degrades performance, sometimes matching the zero-context baseline... green highlights show that medium-context prompts encourage the model to rely on stable quantitative cues... while red highlights show that high-context prompts surface narrative details that models overweight".

### Mechanism 2
LLMs exhibit an "impedance mismatch" when converting qualitative linguistic descriptions into quantitative regression outputs. Traditional ML models directly operate on structured numerical features, while LLMs must first translate textual representations into internal representations before performing regression, introducing translation error at each step. Core assumption: The tokenization-text representation-numeric output pipeline introduces cumulative error not present in direct numerical processing. Evidence anchors: [section 3.1/RQ1] "This performance gap is likely due to a fundamental 'impedance mismatch' for the LLM. The ML model directly operates on structured numerical features, whereas the LLM must first translate qualitative linguistic descriptions into an internal representation before performing regression, introducing significant error."; [section 3.1] ML MAPE of 29.92% vs. best LLM at 57.45%—92% relative improvement for ML.

### Mechanism 3
LLMs perform better on approximate temporal classification than exact-day precision, aligning with business-relevant tolerance thresholds. LLMs capture gestalt temporal patterns (e.g., "next 48 hours") more effectively than precise numerical regression, as their training emphasizes semantic understanding over quantitative precision. Core assumption: LLM architecture and training objectives favor categorical/ordinal reasoning over continuous numerical output. Evidence anchors: [section 3.1] "the disparity in performance is significantly larger for error metrics than for accuracy metrics, as LLMs' performance on TA@k is competitive. This suggests that while LLMs fail at quantitative precision (i.e. pinpointing the exact day), they are better at approximate temporal classification."; [section 3.1] LLMs outperform statistical baselines on TA@k metrics despite inferior RMSE/MAPE.

## Foundational Learning

- **Zero-shot evaluation methodology**: Used to isolate inherent temporal reasoning capabilities without task-specific guidance. Why needed: Few-shot examples could mask fundamental limitations by providing task-specific formatting guidance. Quick check: Can you explain why few-shot prompting would conflate reasoning assessment with in-context learning ability?

- **Tolerance Accuracy (TA@k) metrics**: Business-oriented metrics measuring proportion of predictions within k days of actual. Why needed: Captures real-world utility where exact precision is unnecessary. Quick check: If a model achieves 22.98% TA@1 and 33.93% TA@2, what proportion of predictions fall between 1-2 days of error?

- **Quantile loss optimization**: Used in ML baselines to accommodate discrete interval distributions. Why needed: Aligns with business metrics and handles skewed temporal data. Quick check: Why would median quantile loss outperform MSE for predicting purchase intervals that may have long-tail outliers?

## Architecture Onboarding

- **Component map**: Zero/Medium/High context prompts -> GPT-4o/Gemini-2.5/Claude 3.5 -> Numeric output parsing -> RMSE/MAE/MAPE + TA@{0,1,2} evaluation
- **Critical path**: Filter user-category pairs (≥5 purchases, intervals ≤20 days) -> Format historical intervals as text prompt at chosen context level -> Parse LLM numeric output -> Compare against ground truth using tolerance accuracy and error metrics
- **Design tradeoffs**: Context richness vs. noise (Medium optimal); LLM vs. ML deployment (LLMs cost ~$0.003-0.01/call with 1.2-19s latency vs. 50%+ accuracy gains for ML); Approximation vs. precision (LLMs viable for "next 48 hours" predictions; use ML when day-precision matters)
- **Failure signatures**: High-context prompts causing worse-than-zero-context performance; LLM outputs clustering near historical mean/median without incorporating contextual signals; MAPE exceeding 70% with reasonable TA@2
- **First 3 experiments**: 1) Establish baseline gap: Run all three LLMs at zero-context on held-out test set; confirm ML outperforms by >50% on MAPE as paper reports; 2) Context sweep: For Claude-3.5 (best performer), systematically test intermediate context levels between Medium and High to find optimal information threshold before degradation begins; 3) Classification reframing: Convert regression task to 3-5 bucket classification and measure whether LLM-ML gap narrows on classification metrics

## Open Questions the Paper Calls Out

- **Hybrid architectures**: Can combining statistical models with LLMs outperform both pure ML and pure LLM approaches on temporal interval prediction? Basis: [explicit] "offer guidance for designing future context-aware hybrid models that integrate statistical precision with linguistic flexibility." Unresolved: Paper benchmarks pure approaches but does not test integrated hybrid architecture.

- **Few-shot prompting**: Would few-shot prompting or task-specific fine-tuning close the performance gap between LLMs and dedicated ML models? Basis: [explicit] "focused on zero-shot evaluation, leaving advanced prompt tuning strategies to future work." Unresolved: All experiments use zero-shot settings; role of in-context examples remains untested.

- **Context degradation mechanism**: What mechanism causes high-context prompts to degrade LLM temporal reasoning, and can prompt engineering mitigate this effect? Basis: [inferred] Hypothesizes "context-as-noise effect" but does not empirically validate mechanism or test prompt engineering solutions. Unresolved: Observation documented but whether stems from attention dilution, token budget constraints, or semantic interference is untested.

## Limitations

- Context-as-noise effect observed but not mechanistically explained; the paper does not identify whether degradation stems from attention dilution, semantic interference, or increased computational burden
- Translation-error mechanism for LLMs converting linguistic to numerical representations lacks direct empirical validation; impedance mismatch claim inferred from performance gaps rather than measured at each processing stage
- Zero-shot methodology may underestimate LLM potential since few-shot prompting could significantly improve performance by providing task-specific formatting guidance

## Confidence

- **High confidence**: LLMs underperform dedicated ML models on quantitative accuracy metrics; moderate context improves accuracy over zero context; high context degrades performance relative to medium context; LLMs show competitive performance on tolerance accuracy metrics despite quantitative errors
- **Medium confidence**: Context-as-noise effect is primarily driven by attention dilution from narrative features; impedance mismatch between linguistic representation and numerical regression is primary driver of LLM performance gaps; LLMs are inherently better suited for approximate classification than exact regression
- **Low confidence**: Exact threshold where context transitions from beneficial to detrimental varies systematically across domains; classification reframing would eliminate most of the LLM-ML performance gap; observed effects generalize beyond e-commerce purchase prediction to other temporal reasoning domains

## Next Checks

1. **Mechanistic validation of context degradation**: Conduct ablation studies where narrative elements are systematically removed from high-context prompts while preserving quantitative information. If performance improves when narrative is removed, this confirms attention dilution; if not, alternative mechanisms require investigation.

2. **Direct measurement of translation error**: Implement controlled experiment comparing LLM performance when numerical values are presented in lexical form versus structured numerical format. Test whether alignment layers that map numbers directly to lexical space reduce performance gap, providing direct evidence for translation-error mechanism.

3. **Cross-domain generalization test**: Apply same experimental framework to different temporal prediction domain (e.g., server maintenance intervals, social media posting patterns) with different interval distributions and context features. Validate whether context-as-noise effect and impedance mismatch are domain-specific or fundamental limitations of LLM temporal reasoning architecture.