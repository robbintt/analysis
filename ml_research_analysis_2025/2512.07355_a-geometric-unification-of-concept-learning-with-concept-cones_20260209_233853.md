---
ver: rpa2
title: A Geometric Unification of Concept Learning with Concept Cones
arxiv_id: '2512.07355'
source_url: https://arxiv.org/abs/2512.07355
tags:
- concept
- concepts
- learning
- cone
- dictionary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a geometric unification between Concept
  Bottleneck Models (CBMs) and Sparse Autoencoders (SAEs) by showing they both learn
  concept cones - sets of linear directions in activation space whose nonnegative
  combinations form convex cones. CBMs provide human-annotated concept directions
  through supervised learning, while SAEs discover emergent concept directions through
  sparse coding.
---

# A Geometric Unification of Concept Learning with Concept Cones

## Quick Facts
- arXiv ID: 2512.07355
- Source URL: https://arxiv.org/abs/2512.07355
- Authors: Alexandre Rocchi--Henry; Thomas Fel; Gianni Franchi
- Reference count: 40
- Primary result: Geometric unification between CBMs and SAEs through concept cones with optimal sparsity range of 0.01-0.05%

## Executive Summary
This paper establishes a mathematical framework that unifies Concept Bottleneck Models (CBMs) and Sparse Autoencoders (SAEs) through the concept of "concept cones" - convex cones formed by nonnegative combinations of concept directions in activation space. The authors prove that both CBMs (with human-annotated concepts) and SAEs (with emergent concepts) learn representations that can be described as convex cones. They introduce the "containment hypothesis" that CBM concepts should lie within or be well-approximated by SAE concept cones, and propose metrics to quantify geometric and semantic alignment between these approaches. Extensive experiments across multiple backbones and datasets validate the framework and identify optimal sparsity levels and architectural choices.

## Method Summary
The authors develop a geometric framework where both CBMs and SAEs are represented as convex cones in activation space. CBMs learn concept directions through supervised learning with human annotations, while SAEs discover emergent concept directions through sparse coding. The containment hypothesis states that CBM concepts should lie within or be well-approximated by SAE concept cones. To measure alignment, they introduce the Scaled Cosine Similarity (SCS) metric for geometric alignment and semantic alignment through CLIP-based comparison. Experiments evaluate multiple SAE variants (BatchTopK, Archetypal, etc.) across different sparsity levels, expansion factors, and layers of ResNet-50 on image classification tasks.

## Key Results
- Geometric unification established: both CBMs and SAEs learn concept cones in activation space
- Optimal "sweet spot" identified at intermediate sparsity levels (0.01-0.05%) and expansion factors (3×)
- Deeper layers of ResNet-50 show stronger semantic alignment with CBM concepts
- BatchTopK and Archetypal SAE variants demonstrate superior coverage and alignment compared to other variants
- Proposed metrics successfully distinguish trained from untrained SAEs, validating their ability to measure meaningful semantic alignment

## Why This Works (Mechanism)
The geometric unification works because both CBMs and SAEs, despite their different learning objectives, produce representations that can be described as convex cones - sets of linear directions in activation space whose nonnegative combinations form convex cones. CBMs explicitly learn directions aligned with human-annotated concepts through supervised learning, while SAEs discover emergent directions through sparse coding that naturally form convex cone structures. The containment hypothesis captures the intuition that human-annotated concepts (CBM) should be expressible as combinations of the emergent concepts discovered by SAEs, reflecting the hierarchical nature of semantic representations in neural networks.

## Foundational Learning

**Concept Cones** - Convex cones formed by nonnegative combinations of concept directions in activation space. Needed to provide a unified geometric representation for both CBMs and SAEs. Quick check: Verify that nonnegative combinations of concept vectors produce valid convex cones in activation space.

**Scaled Cosine Similarity (SCS)** - Metric measuring geometric alignment between concept directions from different models. Needed to quantify how well SAE-discovered concepts align with human-annotated CBM concepts. Quick check: SCS values should be bounded between -1 and 1, with higher values indicating better alignment.

**Containment Hypothesis** - Assertion that CBM concept directions should lie within or be well-approximated by SAE concept cones. Needed to establish the theoretical relationship between supervised and emergent concept learning. Quick check: Test containment by projecting CBM concepts onto SAE cone spaces and measuring reconstruction error.

## Architecture Onboarding

**Component Map:** CBM/SAE concepts -> Concept Cones -> Alignment Metrics (SCS/semantic) -> Validation Experiments

**Critical Path:** Concept learning (CBM/SAE) → Cone construction → Geometric alignment (SCS) → Semantic alignment (CLIP-based) → Performance evaluation

**Design Tradeoffs:** The framework balances mathematical rigor (convex cone representation) with practical applicability (CLIP-based semantic alignment). Tradeoff between sparsity level and coverage: higher sparsity yields more interpretable concepts but may miss important directions.

**Failure Signatures:** Poor alignment metrics may indicate insufficient sparsity, inappropriate expansion factors, or architectural mismatch between CBM and SAE approaches. Geometric alignment without semantic alignment suggests mathematical coincidence rather than meaningful concept discovery.

**First Experiments:**
1. Verify convex cone construction by testing nonnegative combinations of learned concept directions
2. Measure SCS alignment between random vs. trained SAE concepts to establish baseline performance
3. Compare semantic alignment across different SAE variants at varying sparsity levels

## Open Questions the Paper Calls Out
None

## Limitations
- Geometric framework assumes linear separability and may not capture true nonlinear nature of learned representations
- Optimal sparsity range (0.01-0.05%) may not generalize across all architectures and domains
- Focus on quantitative metrics may not fully capture functional utility for downstream tasks and explainability

## Confidence

**High:** Mathematical framework establishing concept cone representation is rigorous and well-supported. Experimental methodology for measuring alignment is sound and reproducible. Finding that deeper layers show stronger semantic alignment is consistent across experiments.

**Medium:** "Sweet spot" identification for sparsity and expansion factors is well-supported but may require re-validation for different architectures. Relative performance rankings of SAE variants may depend on implementation details.

**Low:** Assumption that geometric alignment directly translates to functional utility lacks extensive validation. Generalizability to non-vision domains remains untested.

## Next Checks

1. Test containment hypothesis and sweet spot identification across diverse model architectures (Transformers, Vision Transformers) and non-vision domains to assess generalizability.

2. Conduct ablation studies to isolate contributions of individual components in proposed metrics, particularly examining robustness to noise in concept annotations.

3. Design functional validation experiments measuring impact of geometric alignment on downstream task performance, concept-based explanations, and human interpretability.