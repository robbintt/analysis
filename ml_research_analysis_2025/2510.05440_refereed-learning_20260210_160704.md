---
ver: rpa2
title: Refereed Learning
arxiv_id: '2510.05440'
source_url: https://arxiv.org/abs/2510.05440
tags:
- protocol
- verifier
- query
- refereed
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work initiates the study of refereed learning, where a weak
  learner-verifier interacts with two competing provers (one honest) to assess the
  quality of black-box models. The main challenge is choosing between two opaque models
  that approximate a ground-truth function, with minimal access to both the models
  and ground truth.
---

# Refereed Learning

## Quick Facts
- arXiv ID: 2510.05440
- Source URL: https://arxiv.org/abs/2510.05440
- Reference count: 13
- Key outcome: Protocol allows weak learner to verify black-box models using two competing provers, making only one ground truth query while achieving (1+ε) multiplicative error for zero-one loss

## Executive Summary
This work introduces refereed learning, where a weak verifier interacts with two competing provers (one honest) to assess black-box model quality. The key challenge is comparing opaque models that approximate a ground-truth function with minimal access to both models and ground truth. The main technical contribution is a protocol that enables the learner to sample from distributions it cannot efficiently sample from directly, using competing provers through "certifiable sum" and "certifiable index" sub-protocols.

For zero-one loss functions, the protocol makes only one query to ground truth, communicates O((1+1/ε²)poly(d)) bits with provers, and outputs a model whose loss is within a multiplicative factor (1+ε) of the best model. This significantly improves over single-prover methods requiring queries at nearly all domain points. The work also presents lower bounds justifying the protocol's parameters and extends to general metric loss functions with 3+ε multiplicative error, demonstrating efficiency when models are juntas.

## Method Summary
The method employs two competing provers to enable verification of black-box models with minimal ground truth access. The core approach uses recursive bisection through certifiable sum and certifiable index protocols to verify claims about exponentially large sets with only O(d) communication and 2 queries. The verifier delegates queries to both provers and makes a single decisive ground truth query when answers disagree. This refereed query delegation reduces verifier queries to O(1) while preserving correctness. The protocol works by having provers compute sums over sub-regions and identifying which half contains a lie, recursively narrowing down to a single verifiable point.

## Key Results
- Achieves (1+ε) multiplicative error for zero-one loss with only one ground truth query
- Communication complexity of O((1+1/ε²)poly(d)) bits
- Proves exponential prover time is necessary for pure multiplicative error guarantees
- Extends to general metric loss functions with 3+ε multiplicative error
- Demonstrates efficiency for juntas (models depending on polylog(d) input bits)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Two competing provers enable verification even when the learner cannot directly perform the computation.
- **Mechanism:** The protocol pits prover P0 and prover P1 against each other. When one prover makes a false claim, the honest prover can identify which sub-region contains the error. The verifier recursively narrows the domain until reaching a single point that can be checked with one query.
- **Core assumption:** At least one prover is honest (follows the protocol); provers have competing incentives (each promotes a different model).
- **Evidence anchors:**
  - [abstract] "a weak learner-verifier interacts with two competing provers (one honest)"
  - [Section 1.1.3, Page 6-7] "if a malicious prover misreports the value of the sum, then it must incorrectly report the value of the sum on at least one half of the domain"
  - [corpus] Weak relevance: related theorem prover literature focuses on formal verification rather than adversarial multi-prover models
- **Break condition:** If both provers collude (both dishonest), the protocol fails; if the honest prover lacks sufficient computational power to identify errors, verification may be incomplete.

### Mechanism 2
- **Claim:** Certifiable sum allows verifying claims about exponentially large sums with only O(d) communication and 2 queries.
- **Mechanism:** For function t: {0,1}^d → Q, the protocol recursively bisects the domain. Each prover claims values for the total sum and sums over each half. The honest prover identifies which half contains a lie. After d rounds, only one point remains, verifiable with direct query.
- **Core assumption:** The function t is queryable; the domain has structure amenable to recursive bisection; provers can compute sums over sub-regions.
- **Evidence anchors:**
  - [abstract] "evaluating sums over exponentially large sets with membership query access only"
  - [Lemma 3.2, Page 12-17] Complete protocol with O(λ·poly(d)) complexity, 2 queries to t
  - [corpus] No directly comparable certifiable sum mechanisms in neighboring papers
- **Break condition:** If t cannot be queried efficiently, or if numerical precision (λ) is unbounded, communication costs grow without bound.

### Mechanism 3
- **Claim:** Refereed query delegation reduces verifier queries to O(1) while preserving correctness.
- **Mechanism:** The verifier sends queries to both provers. If answers agree, accept. If they disagree, make ONE ground truth query to identify the liar, then trust only that prover for all subsequent queries.
- **Core assumption:** The ground truth oracle is deterministic and queryable at least once; the honest prover always returns correct answers.
- **Evidence anchors:**
  - [abstract] "makes only one query to the ground truth"
  - [Lemma 3.8, Page 18-19] Formal statement: protocol with verifier query complexity ≤ 1
  - [corpus] No comparable delegation mechanisms in neighboring papers
- **Break condition:** If the first disagreement point is adversarially chosen by malicious provers to maximize information leakage, or if the ground truth query fails, the protocol cannot proceed.

## Foundational Learning

- **Concept: Zero-one loss and empirical risk minimization**
  - **Why needed here:** The protocol's core task is comparing L_D(h_ρ, f) = Pr[h_ρ(x) ≠ f(x)] between two models. Understanding how sampling from the disagreement set S = {x : h_0(x) ≠ h_1(x)} amplifies the signal is essential.
  - **Quick check question:** Given two classifiers with losses L(h_0, f) = 0.01 and L(h_1, f) = 0.03 on a uniform distribution over {0,1}^d, how many samples from the disagreement set are needed to distinguish them with confidence 1-β?

- **Concept: Total variation distance and distribution approximation**
  - **Why needed here:** The certifiable sample protocol only approximately samples from the target distribution (TV distance δ). Understanding how this approximation affects downstream estimation is critical for setting δ correctly.
  - **Quick check question:** If samples come from distribution D' with d_TV(D', D) = δ, how much bias does this introduce in estimating E_{x~D}[t(x)] for bounded t?

- **Concept: Interactive proof systems and competitive provers**
  - **Why needed here:** This is the foundational model—understanding that the verifier's power comes from exploiting the assumption that at least one prover is honest, and that dishonesty creates detectable inconsistencies.
  - **Quick check question:** Why does the certifiable sum protocol fail if both provers can collude on a consistent false claim? What structural property of the protocol prevents this?

## Architecture Onboarding

- **Component map:**
  - Oracle interfaces: Query access to h_0, h_1 (black-box models); query access to f (ground truth); query access to Q_D (distribution PMF)
  - Prover modules: Implement certifiable sum, certifiable index, and model evaluation; each prover has full query access
  - Verifier core: Orchestrate protocols, make the single decisive ground truth query, aggregate results
  - Communication layer: O((1 + 1/ε²)poly(d)) bits exchanged over O(d) rounds

- **Critical path:**
  1. Compute |S| (disagreement set size) via certifiable sum → requires prover claims on 2^d values
  2. Sample m = O((1+1/ε²)log(1/β)) points from S via certifiable index
  3. Query f on sampled points → this is the SINGLE ground truth query (or delegate via query delegation protocol)
  4. Return model with lower empirical loss

- **Design tradeoffs:**
  - **Pure multiplicative error (η=0):** Requires exponential prover time in general (Theorem 5.4); achieved only for special cases like juntas
  - **Mixed error (α>1, η>0):** Allows polynomial-time provers with O(1/ε²η) queries to f
  - **Zero-one vs. general loss:** Zero-one achieves (1+ε) factor; general metrics only achieve (3+ε) due to loss rescaling (Definition 4.5)
  - **Assumption:** Provers need white-box access to h_0, h_1 for efficiency; black-box access requires Ω(2^d) queries (lower bound, Page 27)

- **Failure signatures:**
  - Prover timeout or excessive communication → likely missing structural knowledge (e.g., junta structure)
  - Verifier cannot determine winning model → ground truth query may have been malformed, or both provers are dishonest
  - Certifiable sum returns inconsistent values across rounds → one prover is cheating; verify the single-point query at round d

- **First 3 experiments:**
  1. **Junta validation:** Test on d=20, j=5 (juntas with 5 relevant bits). Verify provers run in poly(d) time and verifier makes exactly 1 query. Compare against brute-force baseline.
  2. **Adversarial prover test:** Inject a malicious prover that consistently lies. Verify the protocol correctly identifies the honest prover and returns the better model.
  3. **Precision sweep:** Vary ε ∈ {0.1, 0.01, 0.001} and measure communication complexity growth. Confirm O(1/ε²) scaling empirically.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the multiplicative error guarantee be improved from $(3+\epsilon)$ to $(1+\epsilon)$ for general metric loss functions?
- Basis in paper: [explicit] Section 4.2 contrasts the $(3+\epsilon)$ result for general metrics with the $(1+\epsilon)$ result for zero-one loss, noting the worse slack parameter.
- Why unresolved: The current technique relies on the triangle inequality via a rescaled distribution, which introduces the factor of 3.
- What evidence would resolve it: A new protocol achieving $(1+\epsilon)$ for general metrics, or a lower bound proving the factor of 3 is inherent.

### Open Question 2
- Question: Are there broad hypothesis classes other than juntas for which provers can be implemented efficiently (in polynomial time)?
- Basis in paper: [explicit] Section 6.1 highlights juntas as a specific case for efficient prover implementation, contrasting with the general case requiring exponential time.
- Why unresolved: The exponential overhead for general models is linked to the hardness of computing disagreement sets, and it is unknown which other structures mitigate this.
- What evidence would resolve it: Identification of another function class with an efficient refereed learning protocol, or a reduction showing efficiency is impossible for other specific classes.

### Open Question 3
- Question: Do the proposed protocols preserve their guarantees when both provers are strategic with opposing goals, rather than assuming at least one is honest?
- Basis in paper: [explicit] Section 1.1.1 notes that while the definition assumes an honest prover, protocols "appear to preserve their guarantees" under strategic behavior, citing supporting evidence but not a formal proof for this setting.
- Why unresolved: The main analysis relies on the honest prover assumption; rigorous equilibrium analysis for the specific learning protocols is not provided.
- What evidence would resolve it: A formal game-theoretic proof showing the protocols function correctly with two strategic provers, or a counter-example where the honest-prover guarantee fails.

### Open Question 4
- Question: Is the $O(1/\epsilon^2)$ dependence in the communication complexity of the zero-one loss protocol tight?
- Basis in paper: [inferred] Section 4.1 establishes a communication complexity of $(1+1/\epsilon^2)poly(d)$, while Section 5 provides lower bounds on samples and prover time but does not address communication bits.
- Why unresolved: The standard sampling bounds suggest $1/\epsilon^2$, but the interactive refereed setting might theoretically allow for more communication-efficient verification.
- What evidence would resolve it: A lower bound proving $\Omega(1/\epsilon^2)$ communication is necessary, or a new protocol with $o(1/\epsilon^2)$ dependence.

## Limitations
- Reliance on at least one honest prover creates a critical single point of failure if both provers collude
- Exponential prover time requirement for pure multiplicative error guarantees significantly limits practical applicability
- Approximate sampling (TV distance δ) introduces approximation error that compounds with estimation variance
- Degradation from (1+ε) to (3+ε) multiplicative error for general metric loss functions represents a substantial theoretical gap

## Confidence
- **High confidence:** The core refereed learning framework and certifiable sum/index protocols (based on formal proofs in Sections 3.1-3.2 and Lemma 3.2)
- **Medium confidence:** Lower bounds justifying exponential prover time for η=0 (Theorem 5.4 relies on adversarial constructions)
- **Medium confidence:** Junta-specific efficiency claims (practical performance depends on exact junta structure)
- **Low confidence:** Extension to general metric loss functions achieving 3+ε error (limited empirical validation)

## Next Checks
1. **Honest prover robustness test:** Implement a controlled experiment where both provers occasionally collude to lie on specific queries. Measure detection rate and verify whether the verifier correctly identifies and isolates dishonest behavior.
2. **Approximate sampling validation:** For the certifiable sample protocol, systematically vary the TV distance parameter δ and measure its impact on the final model selection accuracy. Establish quantitative bounds on δ that preserve (1+ε) guarantees.
3. **Scaling experiment:** Implement the protocol for d=50 with sparse juntas (j=5-10 relevant bits) and measure actual communication complexity, prover computation time, and ground truth query savings compared to single-prover baselines.