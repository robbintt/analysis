---
ver: rpa2
title: Log-Normal Multiplicative Dynamics for Stable Low-Precision Training of Large
  Networks
arxiv_id: '2506.17768'
source_url: https://arxiv.org/abs/2506.17768
tags:
- multiplicative
- weight
- learning
- training
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Log-Normal Multiplicative Dynamics (LMD),
  a new optimizer that mimics biological synaptic dynamics by using multiplicative
  updates and log-normal noise injection. LMD is derived from a Bayesian learning
  rule with log-normal posterior distributions over weights, enabling stable training
  under low-precision forward operations.
---

# Log-Normal Multiplicative Dynamics for Stable Low-Precision Training of Large Networks

## Quick Facts
- arXiv ID: 2506.17768
- Source URL: https://arxiv.org/abs/2506.17768
- Reference count: 27
- Primary result: LMD achieves stable training of large networks from scratch under low-precision forward passes, outperforming AdamW and Madam in both ViT and GPT-2 experiments

## Executive Summary
This paper introduces Log-Normal Multiplicative Dynamics (LMD), a new optimizer designed for stable training of large networks under low-precision operations. LMD is derived from a Bayesian learning rule with log-normal posterior distributions over weights, enabling stable training when forward passes are performed in low-precision formats. The method is easy to implement, requires only one additional vector compared to AdamW, and effectively suppresses the exponential weight growth issue seen in standard multiplicative updates.

## Method Summary
LMD is a variational Bayesian optimizer that models weights as samples from log-normal distributions, using multiplicative updates with log-normal noise injection. The method incorporates a multiplicative weight decay derived from a log-normal prior, which acts as soft clipping in logarithmic space to prevent weight explosion. LMD uses the EG± trick to represent signed weights with multiplicative dynamics, and decouples gradient and regularization updates to prevent training instability. The optimizer requires minimal implementation changes beyond AdamW, adding only one additional vector and sampling weights from the log-normal distribution during training.

## Key Results
- LMD achieves stable training-from-scratch of Vision Transformer and GPT-2, even with low-precision (MXFP6) forward operations
- In ViT training, LMD significantly outperforms AdamW and Madam, with weight norms stabilizing around 55 (vs 260 for AdamW and 577 for Madam)
- For GPT-2, LMD achieves better perplexity and more stable learning dynamics under low-precision forward passes compared to AdamW and Madam
- Multiplicative noise injection is demonstrated as crucial for low-precision training stability through ablation studies

## Why This Works (Mechanism)

### Mechanism 1: Multiplicative Weight Decay
- **Claim:** Multiplicative weight decay stabilizes training by suppressing the exponential weight growth inherent to multiplicative updates.
- **Mechanism:** LMD derives its regularizer from a log-normal prior over weights, resulting in a penalty term that operates in logarithmic space, pulling the weight distribution's median toward a fixed reference value to counteract weight explosion.
- **Core assumption:** Excessive weight growth is the primary failure mode preventing multiplicative weight updates from training large networks from scratch.
- **Evidence anchors:** Abstract states LMD "effectively suppresses the exponential weight growth issue seen in standard multiplicative updates"; Figure 3 shows multiplicative weight decay maintains stable weight ℓ2 norm while additive decay fails in GPT-2; derivation shows update acts as decay in log-space.
- **Break condition:** If hard clipping (as in Madam) achieves comparable performance without training instability.

### Mechanism 2: Multiplicative Noise Injection
- **Claim:** Multiplicative noise injection is critical for training stability when forward passes are quantized to low-precision (MX) data formats.
- **Mechanism:** LMD samples weights from a log-normal distribution, injecting noise proportional to weight magnitude that survives the rounding process in low-precision quantization, preventing training dynamics from being destabilized by systematic quantization errors.
- **Core assumption:** Standard training dynamics are disrupted by large rounding errors in low-precision formats, and this disruption can be countered by noise.
- **Evidence anchors:** Abstract states "multiplicative noise injection is crucial for low-precision training stability"; Figure 4 demonstrates sampled training maintains stable dynamics in MXFP6 while mean training leads to instability; related work identifies low-precision instability as a key open problem.
- **Break condition:** If future low-precision hardware has negligible rounding error, or if additive noise proves equally effective.

### Mechanism 3: Decoupled Gradient and Regularization Updates
- **Claim:** Decoupling gradient and regularization updates prevents training instability caused by conflicting signals.
- **Mechanism:** LMD computes gradient and regularization terms independently, combining a signed gradient momentum with the raw regularization penalty to ensure both can influence the weight without one dominating the accumulator.
- **Core assumption:** Merging gradient and regularization into a single momentum term causes instability when their optimal directions differ.
- **Evidence anchors:** Section 3 states LMD addresses instability from incorporating gradient and weight penalties with momentum accumulation by decoupling them; Algorithm 1 explicitly shows separate computation of gradient and regularizer.
- **Break condition:** If a single, properly tuned momentum term is shown to be equally stable.

## Foundational Learning

- **Concept: Multiplicative Weight Update (MWU)**
  - **Why needed here:** This is the fundamental operation LMD improves upon, scaling weights by a factor rather than using additive gradient descent.
  - **Quick check question:** How does a multiplicative update differ from an additive gradient step?

- **Concept: Log-Normal Distribution**
  - **Why needed here:** LMD models weights as samples from this distribution, creating proportional noise when sampling `mε`.
  - **Quick check question:** Why is the log-normal distribution more natural for multiplicative dynamics than a normal distribution?

- **Concept: Variational Inference / Bayesian Learning Rule**
  - **Why needed here:** LMD is derived from this framework, minimizing a loss plus a KL-divergence term that naturally gives rise to the regularizer and update rule.
  - **Quick check question:** What two terms does the variational learning objective balance?

## Architecture Onboarding

- **Component Map:** LMD state includes weight median vector `m` (split into positive `m+` and negative `m-` via EG± trick) and momentum vector `ν`. Core operations: sample noise `ε`, compute sampled weight `θ = m ⊙ ε`, calculate decoupled gradient `g` and regularizer `r`, update momentum `ν`, apply multiplicative update to `m`.

- **Critical Path:** The forward pass during training is most critical, where multiplicative noise injection (`θ = m ⊙ ε`) must be correctly implemented and weights quantized to low-precision MX format. Errors here directly break stability mechanisms.

- **Design Tradeoffs:**
  - Memory vs. Stability: EG± trick doubles weight parameters, trading memory for signed weight representation capability
  - Noise vs. Convergence: Noise magnitude (`σ`) and reference value (`mr`) must be tuned; larger `σ` aids stability but may slow convergence

- **Failure Signatures:**
  - Runaway weight norm: Indicates multiplicative decay (`mr`, `τ`) is insufficient
  - Training divergence in MXFP6: Suggests missing or insufficient noise injection
  - Poor final accuracy: May result from incorrect initialization or improperly tuned learning rate relative to noise scale

- **First 3 Experiments:**
  1. Sanity Check: Implement core LMD update on small MLP for MNIST, verify bounded weight norm and reasonable accuracy
  2. Ablation on Noise: Run same MLP in simulated low-precision with/without multiplicative noise, confirm training fails without noise
  3. Scale Test: Train small Transformer (nanoGPT) on simple dataset, compare LMD against AdamW and Madam, monitor performance and weight norm dynamics

## Open Questions the Paper Calls Out
None

## Limitations
- The exact form of the regularizer `R(θ)` and its relationship to `mr` and `τ` is not fully specified for general cases
- Implementation details of multi-MC sampling across gradient accumulation steps are not explicitly stated
- The dependency of noise injection effectiveness on specific noise scale and precision format requires further exploration

## Confidence

### Major Uncertainties
The core claims about multiplicative weight decay and noise injection are well-supported, but several aspects require further investigation regarding the exact regularizer form, initialization schemes, and multi-MC sampling implementation details.

### Confidence Labels
- **High Confidence:** Multiplicative weight decay stabilizes training by suppressing exponential weight growth (well-demonstrated experimentally)
- **Medium Confidence:** Multiplicative noise injection is crucial for low-precision training stability (strongly supported by ablation but requires further exploration)
- **Medium Confidence:** Decoupling gradient and regularization updates prevents training instability (presented as solution but specific failure modes not thoroughly investigated)

## Next Checks

1. Implement and validate the core LMD update rule on a small MLP for MNIST, verifying bounded weight norm and reasonable accuracy to confirm basic stability mechanism

2. Perform ablation study on noise injection by training the same MLP in simulated low-precision with and without multiplicative noise (`ε`), confirming training fails without noise to directly test low-precision stability claim

3. Test the EG± trick's impact on memory and performance by running ViT experiment with and without sign-constrained weight representation, measuring tradeoff between memory usage and final accuracy