---
ver: rpa2
title: 'SlideItRight: Using AI to Find Relevant Slides and Provide Feedback for Open-Ended
  Questions'
arxiv_id: '2505.04584'
source_url: https://arxiv.org/abs/2505.04584
tags:
- feedback
- learning
- slide
- student
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates SlideItRight, an AI system that combines
  text-based feedback generated by large language models (LLMs) with retrieved relevant
  lecture slides to support learning from open-ended questions. A 2x2 crowdsourcing
  experiment (N=91) compared learning gains and student perceptions across four conditions:
  human feedback, AI feedback, slide feedback, and combined AI + slide feedback.'
---

# SlideItRight: Using AI to Find Relevant Slides and Provide Feedback for Open-Ended Questions

## Quick Facts
- arXiv ID: 2505.04584
- Source URL: https://arxiv.org/abs/2505.04584
- Authors: Chloe Qianhui Zhao; Jie Cao; Eason Chen; Kenneth R. Koedinger; Jionghao Lin
- Reference count: 37
- Key outcome: AI-facilitated multimodal feedback (text + slides) supports learning comparably to human feedback, with high student satisfaction but cognitive load concerns.

## Executive Summary
This study evaluates SlideItRight, an AI system that combines text-based feedback generated by large language models (LLMs) with retrieved relevant lecture slides to support learning from open-ended questions. A 2x2 crowdsourcing experiment (N=91) compared learning gains and student perceptions across four conditions: human feedback, AI feedback, slide feedback, and combined AI + slide feedback. Learning gains improved significantly in all conditions, but differences between conditions were not statistically significant. Students reported high satisfaction and perceived learning gains across all modalities, but found slide feedback difficult to understand. AI feedback was rated highly for personalization and actionability but lower in trust compared to human feedback. The combined approach yielded the highest average learning gain (14.8%) but raised concerns about cognitive load. The findings suggest that AI-facilitated multimodal feedback can support learning comparably to human feedback while offering scalability advantages, though careful attention to cognitive load and clarity is needed for optimal implementation.

## Method Summary
SlideItRight uses GPT-4 Vision to convert lecture slides into multimodal vector representations, then retrieves the top-3 most relevant slides via semantic similarity matching against question embeddings. GPT-4o generates personalized text feedback using RAG context (retrieved slides + student response). A 2x2 experiment with 91 participants compared learning gains and perceptions across human/AI × slide/no-slide conditions, measuring pre/post-test differences and survey ratings.

## Key Results
- All feedback conditions produced statistically significant learning gains (p < 0.001), with the combined AI+slide condition achieving the highest average gain (14.8%).
- No statistically significant differences between conditions, despite trends favoring human feedback and combined approaches.
- Students rated AI feedback highly for personalization (79.17%) and actionability (87.50%) but lower in trust compared to human feedback.
- Slide feedback was perceived as helpful but difficult to understand, with combined modalities raising cognitive load concerns.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieving relevant lecture slides grounds AI-generated feedback in verified course materials, potentially improving accuracy and relevance.
- Mechanism: A two-stage retrieval process converts slides into vector representations (capturing textual, visual, and layout features via GPT-4 Vision), then matches question vectors against slide vectors via semantic similarity to retrieve the top-3 most relevant slides. Retrieved content is integrated into the LLM prompt as an external knowledge base.
- Core assumption: Grounding LLM outputs in instructor-created materials reduces hallucinations and improves pedagogical relevance.
- Evidence anchors:
  - [abstract]: "relevant lecture slide retrieved from the slides hub"
  - [section]: "RAG enhances knowledge-intensive language processing tasks by incorporating external knowledge databases, producing more specific and accurate responses"
  - [corpus]: Limited direct evidence in neighboring papers on RAG effectiveness specifically for feedback generation; corpus focus is on multimodal systems broadly.
- Break condition: When vector similarity fails to capture pedagogical relevance (e.g., a slide is semantically similar but contextually inappropriate), or when retrieved slides conflict with AI-generated feedback, causing student confusion.

### Mechanism 2
- Claim: Presenting feedback through both visual (slides) and verbal (text) channels leverages dual-channel processing, which may support learning under appropriate load conditions.
- Mechanism: The system delivers textual AI feedback alongside visual slide content, relying on Mayer's multimedia learning principle that learners process information through separate visual and verbal channels with limited capacity.
- Core assumption: Learners have distinct processing channels and can integrate multimodal information without excessive cognitive load.
- Evidence anchors:
  - [abstract]: "learning with more modalities could help utilize more separate channels, reduce the cognitive load and facilitate students' learning"
  - [section]: "enabling parallel processing of information via separate channels (visual instructional material and textual guidance)"
  - [corpus]: "LLM-based Multimodal Feedback Produces Equivalent Learning and Better Student Perceptions than Educator Feedback" supports comparable or better outcomes with multimodal AI feedback.
- Break condition: When combined modalities present conflicting or overly dense information, cognitive load increases and learning benefits diminish. Participants reported: "Too much information at once made it difficult to process everything in a timed setting."

### Mechanism 3
- Claim: Analyzing individual student responses enables generation of personalized, actionable feedback that learners perceive as relevant to their specific errors.
- Mechanism: GPT-4o processes free-response answers using learner-centered prompting strategies grounded in learning science, producing tailored explanations and corrective guidance.
- Core assumption: Personalized feedback addressing specific response content outperforms generic feedback for learning and engagement.
- Evidence anchors:
  - [abstract]: "Students reported high satisfaction and perceived learning gains across all modalities"
  - [section]: "AI feedback was rated highly for personalization and actionability" (79.17% personalization rating, 87.50% actionable)
  - [corpus]: Weak corpus evidence specifically quantifying personalization effects in LLM feedback systems.
- Break condition: When student responses are too brief to analyze meaningfully, or when the LLM generates overgeneralized, verbose, or insufficiently calibrated feedback (e.g., "take answer, re-phrase answer back and add 'but you could say more'").

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Core mechanism for grounding AI feedback in external course materials to reduce hallucination risk.
  - Quick check question: Can you explain how embedding-based semantic search retrieves contextually relevant documents for an LLM prompt?

- Concept: Cognitive Load Theory / Multimedia Learning Principles (Mayer)
  - Why needed here: The system's multimodal design explicitly references dual-channel processing assumptions; understanding load tradeoffs is critical for interpreting results.
  - Quick check question: What are the two core assumptions of cognitive load theory as applied to multimedia learning?

- Concept: Learner-Centered Feedback Framework
  - Why needed here: The system's prompting strategy is grounded in this framework; understanding its three components helps evaluate feedback quality.
  - Quick check question: What three elements does learner-centered feedback emphasize (per the Ryan et al. framework cited)?

## Architecture Onboarding

- Component map:
  Input layer (system config, student responses, questions, slides) -> Vision processing (GPT-4 Vision for slide understanding) -> Retrieval engine (semantic similarity matching for top-3 slides) -> Feedback generation (GPT-4o with RAG context) -> Output layer (multimodal feedback panel)

- Critical path:
  1. Student submits open-ended response
  2. System queries vector database for top-3 relevant slides based on question embedding
  3. Retrieved slide content + pre-generated vision understanding injected into LLM prompt
  4. LLM generates personalized feedback conditioned on student response and slide context
  5. Interface renders AI feedback text, slide image (zoomable), and vision explanation

- Design tradeoffs:
  - Pre-caching vs. real-time generation: Vision understanding is pre-computed for speed, but cannot adapt to dynamic content changes
  - Relevance threshold: Retrieving more slides increases context but risks cognitive overload; system limits to 3
  - Personalization depth: Detailed feedback aids advanced learners but may overwhelm beginners ("assumes user is advanced")

- Failure signatures:
  - Cognitive overload: Participant reports "too much information at once" under time constraints
  - Modal conflict: "Having AI feedback and retrieved slides was helpful, but sometimes they contradicted each other"
  - Calibration drift: "I wasn't entirely sure if the feedback was being nice to me or if my answers were actually good"
  - Overgeneralization: Feedback rephrases answer without substantive correction

- First 3 experiments:
  1. Retrieval accuracy validation: Manually annotate 50 question–slide pairs and measure precision@3 for semantic retrieval against expert judgments.
  2. Cognitive load profiling: Use a secondary task measure or self-report scale to quantify load differences between text-only, slide-only, and combined feedback conditions.
  3. Feedback calibration audit: Compare AI feedback assessments (correct/incorrect/partial) against human expert rubric scores for 100 student responses to identify systematic over- or under-praise patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why did the combined AI and slide feedback approach fail to yield significantly superior learning gains despite integrating multiple modalities?
- Basis in paper: [explicit] The authors explicitly ask, "if the combined approach integrates the best of both human-like and content-grounded feedback, why didn’t it yield superior results?"
- Why unresolved: The study observed a trend where combined feedback produced the highest average gain (14.8%), but statistical significance was not reached, potentially due to increased cognitive load or sample size limitations.
- What evidence would resolve it: A study with a larger sample size that specifically measures cognitive load (e.g., using a validated scale) to determine if information density masks the pedagogical benefits of combined modalities.

### Open Question 2
- Question: How can retrieved slide feedback be redesigned to improve clarity and reduce the difficulty students face in understanding it?
- Basis in paper: [explicit] The results show students found slide feedback helpful but reported "difficulty in understanding it," and the discussion notes that slide-based approaches "lacked specific guidance for improvement."
- Why unresolved: The current system retrieves raw slides based on semantic similarity, but the paper did not test methods to simplify or annotate the visual content for better comprehension.
- What evidence would resolve it: An A/B test comparing raw slide retrieval against AI-summarized or visually annotated slide feedback to measure changes in comprehension scores and perceived clarity.

### Open Question 3
- Question: Does the effectiveness of SlideItRight generalize to authentic classroom populations compared to the crowdsourced sample used in this study?
- Basis in paper: [explicit] The limitations section states that the "modest sample size and single-domain context constrain its broader applicability" and suggests future research "explore system effectiveness across more diverse, classroom-based populations."
- Why unresolved: The study relied on compensated crowdsourced workers who may have different motivation levels and engagement patterns compared to students in a voluntary or graded educational setting.
- What evidence would resolve it: A field deployment in a university course analyzing learning gains and system log data (e.g., time on task) to compare against the crowdsourced baseline.

## Limitations

- The 2x2 experimental design with N=91 may be underpowered for detecting meaningful differences between feedback modalities, as no statistically significant differences were found despite observable trends.
- The study relies on self-reported survey measures without objective behavioral or performance validation, limiting the ability to distinguish perceived from actual learning gains.
- Cognitive load assessment relies on qualitative participant feedback rather than standardized cognitive load scales, making it difficult to quantify the tradeoff between multimodal benefits and processing demands.

## Confidence

- Learning gains improvement: High - All conditions showed statistically significant learning gains (p < 0.001), with the combined AI+slide condition achieving the highest average gain (14.8%).
- Equivalent effectiveness of AI vs human feedback: Medium - While learning gains were similar across conditions, the lack of statistical significance could reflect either true equivalence or insufficient power. The trend favoring human feedback suggests potential differences not captured by the study design.
- Cognitive load concerns with multimodal feedback: High - Multiple participants explicitly reported processing difficulties with combined AI and slide feedback, and this concern is supported by cognitive load theory predictions about modality integration limits.

## Next Checks

1. Conduct a formal cognitive load assessment using validated scales (e.g., Paas scale) to quantify processing demands across feedback conditions and identify optimal modality combinations.
2. Perform a power analysis to determine minimum sample size needed to detect meaningful differences between feedback conditions, then replicate with adequate statistical power.
3. Implement an independent expert coding scheme to evaluate feedback accuracy and appropriateness, comparing AI-generated feedback against human expert assessments to validate perceived personalization and actionability ratings.