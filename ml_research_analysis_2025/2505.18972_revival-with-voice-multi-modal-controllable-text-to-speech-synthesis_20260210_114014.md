---
ver: rpa2
title: 'Revival with Voice: Multi-modal Controllable Text-to-Speech Synthesis'
arxiv_id: '2505.18972'
source_url: https://arxiv.org/abs/2505.18972
tags:
- speech
- voice
- face
- image
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses challenges in face-driven text-to-speech synthesis
  by proposing a multi-modal controllable system. The key innovations include: (1)
  augmenting face images with style transfer to bridge the gap between real faces
  and artistic portraits, (2) using high-quality audio-only speech corpora alongside
  audio-visual datasets via alternating voice embeddings trained through contrastive
  learning, and (3) employing sampling-based decoding with prompting to ensure consistent
  voice generation across multiple inferences.'
---

# Revival with Voice: Multi-modal Controllable Text-to-Speech Synthesis

## Quick Facts
- arXiv ID: 2505.18972
- Source URL: https://arxiv.org/abs/2505.18972
- Reference count: 0
- Primary result: State-of-the-art face-driven TTS with MOS 4.14 and FMS 3.86 on LRS3

## Executive Summary
This paper presents RV-TTS, a multi-modal controllable text-to-speech system that generates natural speech from face images, including artistic portraits. The key innovations include augmenting training faces with style transfer to bridge the photorealistic-to-artistic gap, using alternating voice embeddings trained via contrastive learning to leverage high-quality audio-only corpora, and employing sampling-based decoding with prompting to ensure voice consistency across multiple inferences. The system achieves state-of-the-art performance on LRS3 dataset with MOS 4.14 and FMS 3.86, demonstrating strong speaker identification accuracy of 76% on LRS3 and 73% on artistic portraits.

## Method Summary
RV-TTS uses a face encoder (ResNet50) and audio encoder (ECAPA-TDNN) pre-trained via contrastive learning to align face-driven and audio-driven voice embeddings in a shared space. During training, the model alternates between these embeddings depending on whether audio-visual or audio-only data is used, allowing it to learn from high-quality speech corpora while maintaining face-voice associations. Input faces are randomly augmented with style transfer, grayscale, or blur to improve generalization to artistic portraits. The speech language model (24-layer Transformer) conditions on both voice embeddings and descriptive text via cross-attention, predicting 9-level RVQ codes that are decoded to waveforms. Voice consistency across inferences is achieved through sampling-based decoding followed by prompting.

## Key Results
- Achieves MOS score of 4.14 and FMS score of 3.86 on LRS3 dataset
- Outperforms previous face-driven TTS methods by significant margins (2.30 MOS improvement over FaceTTS)
- Maintains 76% speaker identification accuracy on LRS3 and 73% on artistic portraits
- Successfully generates speech from both real faces and artistic portraits with controllable characteristics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Alternating between face-driven and audio-driven voice embeddings during training improves speech quality while preserving face-voice association.
- Mechanism: The face encoder and audio encoder are pre-trained with contrastive learning (InfoNCE loss) to share a common embedding space. During TTS training, when audio-visual data is sampled, the face-driven voice embedding conditions the model; when audio-only data is sampled, the audio-driven embedding is used instead. This allows the model to learn high-quality synthesis from clean audio corpora (LibriTTS-R) while maintaining the face-to-voice mapping learned from audio-visual datasets.
- Core assumption: Face-driven and audio-driven voice embeddings encode comparable speaker identity information that can be used interchangeably as conditioning signals.
- Evidence anchors: [abstract] "using high-quality audio-only speech corpora alongside audio-visual datasets via alternating voice embeddings trained through contrastive learning"; [section 3.4.1] "we observe an absolute drop of 0.5 and 0.29 in both MOS and FMS scores, respectively, when High-Quality (HQ) audio data are removed"; [corpus] Related work FaceSpeak similarly addresses portrait-to-speech but does not employ this alternating embedding strategy.
- Break condition: If face and audio embeddings do not align in the shared space (low cosine similarity for same speakers), the alternation strategy may fail to transfer quality gains.

### Mechanism 2
- Claim: Style augmentation during training enables voice generation from non-photorealistic faces (paintings, old photos).
- Mechanism: Input face images are randomly augmented with 50% probability using neural style transfer (CAST), gray-scaling, or blurring. This simulates the distribution shift encountered at inference when processing artistic portraits, reducing the train-test discrepancy.
- Core assumption: The visual features relevant for voice prediction are preserved across artistic stylization; style transfer does not destroy identity-related facial attributes.
- Evidence anchors: [abstract] "augmenting face images with style transfer to bridge the gap between real faces and artistic portraits"; [section 3.4.1] "training with style augmentation can provide not only better face matching but also improved naturalness in the generated speech"; [corpus] Weak corpus evidence; FaceSpeak mentions "portraits of different styles" but does not isolate style augmentation as a mechanism.
- Break condition: If style transfer corrupts facial features critical for voice inference (e.g., distorting mouth region geometry), the model may learn spurious correlations.

### Mechanism 3
- Claim: Sampling-based decoding followed by prompting ensures diverse yet consistent voice generation across multiple inferences.
- Mechanism: The speech language model uses sampling-based decoding (top-30 with repetition penalty 1.2) to generate multiple voice candidates. The user selects a preferred sample, which is then used as a prompt for subsequent generations. The model's in-context learning ability ensures the prompted voice remains consistent.
- Core assumption: The speech code LM can perform voice consistency via prompting; generated samples contain sufficient voice identity information to serve as effective prompts.
- Evidence anchors: [abstract] "employing sampling-based decoding with prompting to ensure consistent voice generation across multiple inferences"; [section 2.4] "we can also generate consistent voices across multiple inferences by providing the previously generated sample as a prompt, which is also known as in-context learning ability"; [corpus] Corpus does not report comparable prompting strategies for face-driven TTS systems.
- Break condition: If prompt length is insufficient or the model lacks strong in-context learning, voice consistency across utterances may degrade.

## Foundational Learning

- Concept: Contrastive Learning (InfoNCE Loss)
  - Why needed here: Required to align face-driven and audio-driven embeddings in a shared latent space, enabling alternation between modalities during training.
  - Quick check question: Can you explain why maximizing similarity for same-speaker pairs while minimizing it for different-speaker pairs creates a useful shared embedding space?

- Concept: Residual Vector Quantization (RVQ) Codes
  - Why needed here: The speech language model autoregressively predicts 9-level RVQ codes, which are decoded to waveforms. Understanding the delayed pattern across codebook levels is essential.
  - Quick check question: Why does RVQ use multiple codebook levels with a delay pattern rather than a single codebook?

- Concept: Cross-Attention Conditioning
  - Why needed here: The model conditions on concatenated voice embeddings and descriptive text embeddings via cross-attention layers, following the MusicGen architecture.
  - Quick check question: How does cross-attention differ from concatenation-based conditioning in terms of how the model accesses conditioning information?

## Architecture Onboarding

- Component map: Face image → Face Encoder → voice embedding → concatenate with T5 text embedding → cross-attention → Transformer predicts RVQ codes → decoder → waveform
- Critical path: Face image → Face Encoder → voice embedding → concatenate with T5 text embedding → cross-attention → Transformer predicts RVQ codes → decoder → waveform
- Design tradeoffs: Using audio-only data improves quality but requires contrastive pre-training to align embedding spaces; style augmentation improves artistic portrait generalization but may introduce noise into face-voice associations; sampling-based decoding enables diversity but requires prompting step for consistency.
- Failure signatures: Low speaker identification accuracy on artistic portraits: style augmentation may be insufficient or too aggressive; inconsistent voices across utterances: prompt may not be properly incorporated or sampling temperature too high; noisy output despite high-quality training data: voice embedding alignment may be poor; check contrastive loss convergence.
- First 3 experiments: 1) Validate contrastive alignment: Measure cosine similarity between face-driven and audio-driven embeddings for same-speaker vs. different-speaker pairs on a held-out set; 2) Ablate style augmentation intensity: Test 0%, 25%, 50%, 75% augmentation rates and measure MOS/FMS on both photorealistic and artistic portrait test sets; 3) Test prompt effectiveness: Generate multiple utterances with and without prompting; measure voice consistency via speaker embedding cosine similarity across utterances.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the alternating voice embedding approach generalize to multilingual face-driven TTS while preserving cross-lingual face-voice associations?
- Basis in paper: [inferred] All training datasets (LRS3, VoxCeleb2, LibriTTS-R) are English-only, and no multilingual evaluation is conducted.
- Why unresolved: Face-to-voice associations may be culturally biased, and voice characteristics vary across languages.
- What evidence would resolve it: Training and evaluating on multilingual audio-visual datasets with cross-lingual speaker identification tests.

### Open Question 2
- Question: Does the shared embedding space assumption obscure fundamental differences between face-driven and audio-driven voice representations?
- Basis in paper: [inferred] Table 1 shows removing contrastive learning improves MOS (3.73→3.61) while reducing FMS and SIM, suggesting a trade-off between audio quality and face-voice association.
- Why unresolved: The contrastive loss enforces alignment but may distort distinct modality-specific information.
- What evidence would resolve it: Probing experiments comparing what acoustic and facial information each embedding captures.

### Open Question 3
- Question: How sensitive is style augmentation to the diversity and domain of style images used during training?
- Basis in paper: [explicit] The paper uses "copyright-free paintings, photos, and artworks" but does not analyze whether specific style categories affect generalization.
- Why unresolved: The random sampling approach may not optimally bridge the gap between photorealistic faces and specific artistic mediums (oil paintings, sketches, sculptures).
- What evidence would resolve it: Controlled experiments with curated style sets targeting specific artistic domains.

### Open Question 4
- Question: Can the prompting-based voice consistency approach scale to longer-form speech generation without degradation?
- Basis in paper: [inferred] The VCS evaluation uses only 3 samples per speaker; consistency over extended narratives remains untested.
- Why unresolved: In-context learning may have limits as prompt context grows or drifts over long sequences.
- What evidence would resolve it: Long-form synthesis evaluation measuring voice consistency across paragraphs or minutes of generated speech.

## Limitations

- The alternating embedding strategy assumes perfect alignment between face-driven and audio-driven embeddings, but the paper does not report actual cosine similarity distributions for validation.
- Style augmentation may have limited generalization beyond the 20 artistic portraits tested, with unclear performance on extremely degraded portrait images.
- The effectiveness of sampling-based decoding with prompting for voice consistency is demonstrated qualitatively but lacks quantitative validation of in-context learning performance.

## Confidence

- **High confidence**: MOS and FMS score improvements over baseline methods are well-supported by experimental results on LRS3 dataset; speaker identification accuracy numbers (76% on LRS3, 73% on artistic portraits) are directly measurable and reported.
- **Medium confidence**: The claimed mechanism of alternating embeddings improving quality while preserving face-voice association is supported by ablation studies, but the underlying assumption of perfect embedding alignment remains unverified; style augmentation mechanism shows promising results but lacks rigorous analysis of style transfer preserving identity-relevant facial features.
- **Low confidence**: The effectiveness of sampling-based decoding with prompting for voice consistency is demonstrated qualitatively but lacks quantitative validation of in-context learning performance; generalizability to arbitrary portrait styles beyond the 20 tested examples is not established.

## Next Checks

1. **Validate embedding alignment**: Measure and report the distribution of cosine similarities between face-driven and audio-driven voice embeddings for same-speaker pairs versus different-speaker pairs on a held-out validation set. This will confirm whether the alternating embedding strategy is operating on well-aligned representations.

2. **Test style augmentation limits**: Systematically vary the style augmentation intensity (0%, 25%, 50%, 75%, 100%) and evaluate performance on a broader set of portrait types including highly degraded images, testing the hypothesis that style augmentation improves generalization across artistic styles.

3. **Quantify prompting effectiveness**: Design a controlled experiment comparing voice consistency (via speaker embedding similarity) across multiple utterances generated with and without prompting, measuring the actual improvement in consistency rather than relying on qualitative demonstrations.