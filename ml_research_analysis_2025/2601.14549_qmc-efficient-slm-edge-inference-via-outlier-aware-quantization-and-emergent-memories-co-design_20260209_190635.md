---
ver: rpa2
title: 'QMC: Efficient SLM Edge Inference via Outlier-Aware Quantization and Emergent
  Memories Co-Design'
arxiv_id: '2601.14549'
source_url: https://arxiv.org/abs/2601.14549
tags:
- quantization
- memory
- reram
- arxiv
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QMC addresses the challenge of deploying small language models
  (SLMs) on edge devices by proposing an outlier-aware quantization framework co-designed
  with heterogeneous non-volatile memories. The core method partitions weights into
  outliers and inliers, preserving outliers in high-precision on-chip MRAM and aggressively
  quantizing inliers into multi-level ReRAM, with noise-aware scale optimization for
  robustness.
---

# QMC: Efficient SLM Edge Inference via Outlier-Aware Quantization and Emergent Memories Co-Design

## Quick Facts
- arXiv ID: 2601.14549
- Source URL: https://arxiv.org/abs/2601.14549
- Reference count: 40
- 6.27×–7.27× memory reduction, 7.62× lower data movement, and up to 10.98× energy and 12.48× latency improvements over FP16, with accuracy on par with or better than state-of-the-art quantization methods

## Executive Summary
QMC introduces a retraining-free, framework-agnostic quantization method for efficient edge deployment of small language models (SLMs). It partitions weights into magnitude-based outliers and inliers, storing outliers in high-precision on-chip MRAM and aggressively quantizing inliers into multi-level ReRAM. A noise-aware scale optimizer accounts for ReRAM bit-error rates to maintain accuracy. QMC achieves up to 12.48× latency and 10.98× energy reduction while preserving or improving accuracy over FP16 and competitive quantization methods.

## Method Summary
QMC is a post-training quantization (PTQ) framework that partitions SLM weights by magnitude into outliers (top ρ%, default 0.3) and inliers. Outliers are stored in 5-bit precision on MRAM; inliers are quantized to 2–3 bits with noise-aware scale optimization for ReRAM storage. The noise model uses measured MLC ReRAM bit-error rates to guide scale selection, minimizing expected distortion under device noise. A unified Model Weight Controller orchestrates parallel MRAM and ReRAM fetches, with minimal synchronization overhead. The approach is data-free and framework-agnostic, requiring only pre-trained model weights.

## Key Results
- 6.27×–7.27× memory reduction versus FP16 baselines
- Up to 10.98× energy and 12.48× latency improvements over FP16
- Outperforms INT4, MXINT4, AWQ, GPTQ, and emerging memory co-design baselines under realistic ReRAM noise
- Accuracy (perplexity and reasoning tasks) matches or exceeds state-of-the-art PTQ methods

## Why This Works (Mechanism)

### Mechanism 1: Magnitude-Based Outlier Partitioning
Partitioning weights by magnitude into outliers (high-precision) and inliers (aggressively quantized) preserves accuracy while enabling compression, without requiring calibration data. For each weight tensor W, apply a global threshold τ to separate the top ρ% weights by magnitude (W_out) from remaining inliers (W_in). Outliers receive 5-bit quantization (Q_high); inliers receive 2–3 bit quantization (Q_low). Core assumption: Weight magnitude correlates with importance to model accuracy—large-magnitude weights contribute disproportionately to output quality.

### Mechanism 2: Heterogeneous Memory Co-Design with NVMs
Mapping reliability-critical outliers to MRAM and densely compressed inliers to MLC ReRAM improves energy and latency versus homogeneous DRAM-based systems. On-chip MRAM (UCIe 3.0, high reliability, low read latency) stores 5-bit outliers. Off-chip MLC ReRAM (3.3GHz bus, high density, 2–3 bit/cell) stores inliers. LPDDR5 handles KV caches and activations. A unified Model Weight Controller coordinates parallel fetches and synchronization.

### Mechanism 3: Noise-Aware Scale Optimization for ReRAM
Explicitly modeling ReRAM bit-error rate (BER) during quantization scale optimization preserves accuracy under realistic device noise. For inlier weights, optimize scale s by minimizing expected distortion L(s) = E[||W_in - (Q(W_in; s) + e)||²], where e ∈ {−Δ(s), 0, +Δ(s)} is device noise derived from measured MLC ReRAM BER.

## Foundational Learning

- **Post-Training Quantization (PTQ) vs. Quantization-Aware Training (QAT)**
  - Why needed here: QMC is explicitly PTQ-based and retraining-free; understanding this distinction clarifies why calibration data independence matters.
  - Quick check question: Does the method require access to training data or backpropagation?

- **Multi-Level Cell (MLC) ReRAM and Bit-Error Rate (BER)**
  - Why needed here: MLC ReRAM enables density (2–3 bits/cell) but introduces state confusion; BER modeling is central to the noise-aware optimizer.
  - Quick check question: Why does storing more bits per cell increase read uncertainty?

- **Outlier Weights in LLMs/SLMs**
  - Why needed here: Heavy-tailed weight distributions with a small fraction of high-magnitude weights are well-documented; QMC's partitioning relies on this structure.
  - Quick check question: What fraction of weights typically account for most quantization error in transformers?

## Architecture Onboarding

- Component map: Weight partitioner (magnitude thresholding) -> Dual-precision quantizer (5-bit MRAM, 2–3 bit ReRAM) -> Memory system (MRAM, MLC ReRAM, LPDDR5) -> Model Weight Controller (parallel fetch coordination) -> Noise modeler (BER-driven scale optimization)

- Critical path: 1. Partition weights by magnitude (select ρ) 2. Optimize inlier scale s*_ReRAM under noise model 3. Optimize outlier scale s*_MRAM via standard MSE 4. Reconstruct full tensor (merge step) 5. Runtime: parallel MRAM+ReRAM fetch → clock-domain sync → compute

- Design tradeoffs: Outlier ratio ρ: ↑ accuracy but ↑ MRAM usage and potential latency bottleneck (optimal ~0.3 per Figure 3). ReRAM mode: 3-bit MLC → higher density, higher BER; 2-bit MLC → lower density, better noise tolerance. Memory bandwidth split: imbalance creates bottlenecks; constrained by power budget (Eq. 4)

- Failure signatures: Accuracy collapse: ρ too low; ReRAM noise model mismatch; outlier bit-width insufficient. Latency plateau/regression: MRAM access becomes bottleneck (ρ > 0.3); sync overhead dominates. Area overflow: MRAM lower density than expected; ReRAM cell area underestimated

- First 3 experiments: 1. Sweep outlier ratio ρ ∈ {0.1, 0.2, 0.3, 0.4, 0.5} on WikiText; plot perplexity vs. latency/energy to validate sweet spot. 2. Compare 2-bit vs. 3-bit MLC ReRAM under noise injection (using Figure 2 BER profiles) to verify robustness claims. 3. End-to-end benchmark vs. FP16, RTN INT4, AWQ, GPTQ on Hymba/LLaMA/Qwen models using extended NVMain simulation to confirm system-level energy/latency gains

## Open Questions the Paper Calls Out

### Open Question 1
Can QMC's outlier-aware quantization effectively scale to larger language models (>3B parameters) where weight outlier distributions and sensitivity patterns may differ significantly from SLMs? Basis: The paper explicitly scopes evaluation to "SLMs and hybrid architectures under 3B parameters" and states "we therefore focus on these class of models as our primary target," leaving larger models unexplored. Why unresolved: Larger models exhibit different weight distribution characteristics and quantization sensitivity patterns; the fixed global outlier ratio heuristic may not transfer. What evidence would resolve it: Empirical evaluation of QMC on 7B+ parameter models, comparing accuracy, compression, and efficiency against SOTA methods.

### Open Question 2
Can combining QMC with advanced PTQ methods (e.g., AWQ, GPTQ) yield additive improvements in accuracy-efficiency trade-offs? Basis: Section 3.5 states "QMC is orthogonal to existing PTQ frameworks" and "enables QMC to be combined with methods to deliver state-of-the-art accuracy," but no experimental combination is demonstrated. Why unresolved: The claimed orthogonality is theoretical; practical integration may introduce conflicts in scale optimization or outlier identification. What evidence would resolve it: Experiments applying QMC on top of AWQ/GPTQ-quantized models, measuring perplexity and system metrics.

### Open Question 3
How does the optimal outlier ratio vary across different model architectures (transformer vs. SSM vs. hybrid), and would adaptive layer-wise ratios provide further gains? Basis: The paper applies a uniform global outlier ratio across all layers and architectures, acknowledging "more complex layer-wise strategies unnecessary" without systematic ablation across heterogeneous architectures. Why unresolved: Different architectures have distinct weight distribution patterns; SSMs exhibit different recurrence properties that may benefit from architecture-specific tuning. What evidence would resolve it: Layer-wise sensitivity analysis across transformer, Mamba, and Hymba models; comparison of fixed vs. adaptive ratio strategies.

## Limitations
- Claims about heterogeneous NVM co-design benefits rely on NVSim/NVMain simulations and UCIe projections rather than measured silicon
- Real-world memory controller synchronization, thermal variation, and process-induced BER drift are not fully characterized
- Weight magnitude correlation with importance may degrade for models with explicit regularization or post-training normalization

## Confidence
- **High**: Magnitude-based outlier partitioning effectiveness, dual-precision storage strategy, noise-aware scale optimization logic, benchmark accuracy and latency trends
- **Medium**: NVM model fidelity, synchronization overhead assumptions, BER stability under stress, end-to-end system energy claims
- **Low**: Direct comparison to homogeneous NVM alternatives (due to lack of published SLM co-design baselines), actual 2.5D/UCIe integration overhead

## Next Checks
1. Sweep outlier ratio ρ ∈ {0.1, 0.2, 0.3, 0.4, 0.5} on WikiText; plot perplexity vs. latency/energy to confirm the ~0.3 sweet spot and rule out overcompression artifacts.
2. Implement noise injection using Figure 2 BER profiles for 2-bit vs. 3-bit MLC ReRAM; verify accuracy resilience claims under realistic error rates.
3. Extend NVMain simulations to include detailed memory controller timing and area models; rerun end-to-end benchmarks on Hymba/LLaMA/Qwen to confirm 10.98× energy and 12.48× latency improvements hold under more conservative assumptions.