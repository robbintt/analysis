---
ver: rpa2
title: 'RECAP: Reproducing Copyrighted Data from LLMs Training with an Agentic Pipeline'
arxiv_id: '2510.25941'
source_url: https://arxiv.org/abs/2510.25941
tags:
- recap
- data
- copyrighted
- feedback
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes RECAP, an agentic pipeline for extracting memorized
  training data from large language models (LLMs). The core idea is to iteratively
  refine model outputs using a feedback loop where a secondary model identifies discrepancies
  between generated text and target passages, then provides minimal correction hints
  to guide subsequent generations.
---

# RECAP: Reproducing Copyrighted Data from LLMs Training with an Agentic Pipeline

## Quick Facts
- arXiv ID: 2510.25941
- Source URL: https://arxiv.org/abs/2510.25941
- Reference count: 40
- RECAP achieves 78% improvement over prior methods with average ROUGE-L score of 0.46 for extracting copyrighted content

## Executive Summary
This paper introduces RECAP, an agentic pipeline that extracts memorized training data from large language models through iterative refinement. The method uses a feedback loop where a secondary model identifies discrepancies between generated text and target passages, providing minimal correction hints to guide subsequent generations. To overcome alignment-induced refusals, RECAP includes a jailbreaking module that rephrases blocked prompts. Tested on EchoTrace, a new benchmark of over 70,000 passages from 20 arXiv papers and 35 books, the approach demonstrates reliable extraction of thousands of verbatim passages from copyrighted works while maintaining low false positive rates on non-training data.

## Method Summary
RECAP employs an iterative refinement approach that treats LLM memorization extraction as a guided generation problem. The pipeline uses a secondary model to analyze discrepancies between current outputs and target passages, then provides minimal correction hints for the next generation attempt. When models refuse to generate copyrighted content due to alignment training, RECAP's jailbreaking module rephrases prompts to bypass restrictions. This agentic approach contrasts with brute-force extraction methods by focusing on strategic refinement rather than random sampling, enabling efficient discovery of memorized verbatim passages across multiple model families.

## Key Results
- Achieves average ROUGE-L score of 0.46 for extracting copyrighted content across four model families
- Demonstrates 78% improvement over best prior method (EchoTrace)
- Successfully extracts thousands of verbatim passages from copyrighted books like Harry Potter
- Shows negligible false positives on non-training data, indicating reliable memorization detection

## Why This Works (Mechanism)
RECAP works by treating memorization extraction as an iterative optimization problem rather than a one-shot generation task. The discrepancy-based feedback mechanism identifies specific gaps between generated text and target passages, allowing the model to focus its refinement efforts on missing content rather than regenerating entire passages. This targeted approach is particularly effective because LLMs tend to retain verbatim information in contiguous blocks, making discrepancy detection a powerful guide for extraction. The jailbreaking component addresses the fundamental tension between memorization (which persists despite alignment) and refusal mechanisms (which block access to memorized content), enabling extraction of information that would otherwise be inaccessible.

## Foundational Learning
- Iterative refinement algorithms: Why needed - Enables progressive improvement of outputs through multiple generations; Quick check - Verify convergence behavior across different model families
- Discrepancy detection in text: Why needed - Identifies specific gaps between generated and target content for targeted refinement; Quick check - Test detection accuracy on known memorized vs non-memorized passages
- Jailbreaking techniques: Why needed - Bypasses alignment-induced refusals to access memorized copyrighted content; Quick check - Measure success rate against various alignment mechanisms
- ROUGE-L metric for text matching: Why needed - Quantifies similarity between generated and target passages for evaluation; Quick check - Compare ROUGE-L scores with human judgment on memorization quality
- Agentic pipeline design: Why needed - Coordinates multiple specialized models for comprehensive extraction strategy; Quick check - Validate component coordination efficiency

## Architecture Onboarding

Component map: Input prompt -> Jailbreak module (if needed) -> LLM generation -> Discrepancy detector -> Feedback generator -> Refined prompt -> Repeat until convergence

Critical path: The core extraction loop follows: (1) Initial prompt generation, (2) LLM response generation, (3) Discrepancy analysis, (4) Hint generation, (5) Refined prompt creation, (6) Repeat. This path determines extraction success rate and efficiency.

Design tradeoffs: RECAP prioritizes extraction accuracy over speed by using iterative refinement rather than one-shot generation. The jailbreaking component adds complexity but enables access to content blocked by alignment. The choice of discrepancy-based feedback over alternative guidance methods (like semantic similarity) favors verbatim extraction over paraphrasing detection.

Failure signatures: Common failure modes include: (1) Convergence to incorrect passages due to ambiguous hints, (2) Persistent refusals even after jailbreaking attempts, (3) Local optima where the model settles on partial matches, (4) Excessive iteration counts without improvement indicating poor discrepancy detection.

3 first experiments:
1. Test RECAP on known memorized content from publicly available model training data to establish baseline extraction accuracy
2. Compare iterative refinement vs. one-shot generation approaches on identical prompts across multiple model families
3. Evaluate jailbreaking effectiveness by measuring extraction success rates with and without the jailbreaking module on alignment-trained models

## Open Questions the Paper Calls Out
None

## Limitations
- Text matching metrics may not fully capture semantic equivalence or paraphrasing in memorized content
- Jailbreaking effectiveness against evolving alignment techniques remains untested beyond current model versions
- Benchmark focuses on verbatim extraction rather than nuanced understanding of what constitutes "memorization" versus learned patterns

## Confidence
High: Core extraction methodology and iterative refinement approach
Medium: Broader implications for copyright compliance and practical deployment scenarios
Medium: Effectiveness against adaptive model defenses and evolving alignment techniques

## Next Checks
1. Test RECAP's effectiveness against newer model versions with enhanced alignment training and dynamic refusal mechanisms
2. Expand EchoTrace benchmark to include more diverse paraphrasing scenarios and semantic equivalence tests beyond exact string matching
3. Conduct adversarial testing with deliberately corrupted or modified training passages to assess false positive/negative rates under realistic conditions