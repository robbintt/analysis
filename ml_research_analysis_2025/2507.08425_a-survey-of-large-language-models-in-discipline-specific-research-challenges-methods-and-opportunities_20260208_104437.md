---
ver: rpa2
title: 'A Survey of Large Language Models in Discipline-specific Research: Challenges,
  Methods and Opportunities'
arxiv_id: '2507.08425'
source_url: https://arxiv.org/abs/2507.08425
tags:
- llms
- arxiv
- language
- large
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically analyses the integration of large language
  models (LLMs) into discipline-specific research across mathematics, physics, chemistry,
  biology, and the humanities/social sciences. The study categorises techniques into
  internal knowledge optimisation (CPT, SFT, RLHF) and external interaction methods
  (prompt engineering, RAG, agent-based methods, tool-use integration), examining
  their advantages, limitations, and applications.
---

# A Survey of Large Language Models in Discipline-specific Research: Challenges, Methods and Opportunities

## Quick Facts
- arXiv ID: 2507.08425
- Source URL: https://arxiv.org/abs/2507.08425
- Reference count: 40
- This survey systematically analyses LLM integration into discipline-specific research across mathematics, physics, chemistry, biology, and humanities/social sciences, categorising techniques and identifying key challenges.

## Executive Summary
This comprehensive survey examines how large language models are being integrated into discipline-specific research across mathematics, physics, chemistry, biology, and the humanities/social sciences. The study categorises techniques into internal knowledge optimisation methods (CPT, SFT, RLHF) and external interaction approaches (prompt engineering, RAG, agent-based methods, tool-use integration), providing a systematic analysis of their applications and limitations. The research highlights that while LLMs have demonstrated transformative potential, including exceptional performance in mathematical problem-solving, their effective deployment faces significant barriers including insufficient discipline-specific datasets, technical complexity for non-AI specialists, lack of standardised evaluation metrics, and high computational costs.

## Method Summary
The survey employs a comprehensive methodology examining LLM applications across five major disciplines, systematically categorising techniques into internal optimisation and external interaction methods. The analysis covers the advantages, limitations, and practical applications of each approach, with particular attention to discipline-specific challenges and opportunities. The study draws on multiple sources to provide a balanced view of current capabilities and barriers to wider adoption in academic research contexts.

## Key Results
- LLMs demonstrate transformative potential across disciplines, with some achieving top 500 US Math Olympiad rankings in problem-solving
- Major barriers include insufficient discipline-specific datasets, technical complexity for non-AI specialists, and lack of standardised evaluation benchmarks
- Effective deployment requires addressing data quality issues, developing user-friendly interdisciplinary collaboration tools, and reducing computational resource requirements

## Why This Works (Mechanism)
The effectiveness of LLMs in discipline-specific research stems from their ability to process and generate complex domain-specific content through both internal optimisation and external interaction methods. Internal optimisation techniques like CPT, SFT, and RLHF enhance the model's inherent understanding of discipline-specific concepts, while external methods such as prompt engineering, RAG, and agent-based approaches enable dynamic interaction with external knowledge sources and tools. This dual approach allows LLMs to bridge the gap between general language understanding and specialised domain expertise, though success depends heavily on addressing data quality, computational efficiency, and accessibility challenges.

## Foundational Learning
- **Discipline-specific dataset curation**: Essential for training models on domain-relevant content; quick check: verify dataset size, diversity, and annotation quality
- **Fine-tuning methodologies**: Critical for adapting general LLMs to specific disciplines; quick check: assess parameter efficiency and retention of general capabilities
- **Evaluation metric standardisation**: Necessary for comparing model performance across different implementations; quick check: validate metric relevance to domain-specific tasks
- **Computational resource optimisation**: Key to making LLM deployment accessible to researchers; quick check: benchmark resource requirements against typical institutional capabilities
- **Interdisciplinary collaboration frameworks**: Important for bridging AI expertise and domain knowledge; quick check: evaluate tool usability for non-AI specialists

## Architecture Onboarding
**Component map**: Data Sources -> Preprocessing -> Model Training -> Fine-tuning -> External Integration -> Evaluation
**Critical path**: Data curation and quality control -> Appropriate fine-tuning methodology selection -> Integration of external tools and knowledge sources -> Standardised evaluation framework
**Design tradeoffs**: Balance between model size and computational efficiency vs. domain-specific performance accuracy
**Failure signatures**: Poor performance due to insufficient training data, inappropriate fine-tuning approaches, or lack of integration with domain-specific tools
**First experiments**:
1. Conduct small-scale fine-tuning on discipline-specific datasets to evaluate initial performance gains
2. Implement basic prompt engineering techniques to test external interaction capabilities
3. Develop prototype evaluation metrics specific to discipline-relevant tasks

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions beyond those already addressed in the main analysis.

## Limitations
- Publication bias may affect analysis, as the survey may overrepresent successful implementations while underrepresenting failed attempts
- Specific quantitative thresholds for computational resource requirements across different disciplines are not provided
- The categorisation of techniques (internal vs external methods) may not be universally applicable across all disciplines

## Confidence
- **High confidence**: Major challenges identified (insufficient discipline-specific datasets, technical barriers for non-AI specialists, lack of standardised evaluation benchmarks, high computational costs) are well-supported and align with broader literature
- **Medium confidence**: Effectiveness of proposed solutions and specific ranking of challenges may vary based on implementation details and field-specific requirements
- **Low confidence**: Math Olympiad performance claims require independent verification of methodology and results

## Next Checks
1. Verify the Math Olympiad performance claims through independent sources and examine the specific problem sets and evaluation methodology used
2. Conduct a systematic review of failed or problematic LLM deployments in discipline-specific research to balance the publication bias identified in the limitations section
3. Develop and test a pilot framework for standardised evaluation metrics across multiple disciplines to address the benchmark limitation identified in the survey