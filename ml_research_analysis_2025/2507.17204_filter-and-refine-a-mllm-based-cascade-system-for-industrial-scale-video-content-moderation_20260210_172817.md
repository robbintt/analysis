---
ver: rpa2
title: 'Filter-And-Refine: A MLLM Based Cascade System for Industrial-Scale Video
  Content Moderation'
arxiv_id: '2507.17204'
source_url: https://arxiv.org/abs/2507.17204
tags:
- content
- moderation
- classification
- mllm
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multimodal large language model (MLLM)-based
  cascade system for industrial-scale video content moderation. The system addresses
  the computational inefficiency and poor discriminative capability of MLLMs in classification
  tasks.
---

# Filter-And-Refine: A MLLM Based Cascade System for Industrial-Scale Video Content Moderation

## Quick Facts
- arXiv ID: 2507.17204
- Source URL: https://arxiv.org/abs/2507.17204
- Reference count: 5
- Primary result: MLLM cascade system achieves 66.50% F1 improvement over traditional models while using only 2% of fine-tuning data

## Executive Summary
This paper presents a multimodal large language model (MLLM)-based cascade system for industrial-scale video content moderation. The system addresses the computational inefficiency and poor discriminative capability of MLLMs in classification tasks. It employs a two-stage router-ranking architecture: a lightweight router filters out low-risk videos using embedding-based retrieval, and an MLLM ranker performs fine-grained classification on the high-risk subset. To adapt MLLMs for classification, the authors propose a supervised fine-tuning approach using minimal data, including VQA, captioning, and classification datasets. Offline experiments show the MLLM approach improves F1 score by 66.50% over traditional models while using only 2% of the fine-tuning data required by traditional classifiers. Online A/B testing demonstrates a 41% increase in automatic moderation volume and 19.16% precision gain. The cascading design reduces computational cost to 1.5% compared to full-scale MLLM deployment.

## Method Summary
The system uses a two-stage cascade architecture: a lightweight router filters low-risk videos via embedding-based retrieval against a seed bank, and an MLLM ranker performs fine-grained classification on the high-risk subset. The MLLM (LLaVA with Mistral-7B backbone) is adapted for classification through supervised fine-tuning on a 1:1:1 mixture of VQA, captioning, and classification data (300K samples total). The ranker outputs single-token Yes/No responses, with classification probabilities extracted via softmax normalization of logits. The router uses embedding similarity scoring to eliminate 97.5% of traffic while maintaining low latency. The system is evaluated on 50K samples from router output, showing 68.73 PR-AUC and 66.50% F1 improvement over traditional X-VLM classifiers.

## Key Results
- MLLM approach improves F1 score by 66.50% over traditional classifiers while using only 2% of fine-tuning data
- Cascading deployment reduces computational cost to 1.5% of direct full-scale MLLM deployment
- Online A/B testing shows 41% increase in automatic moderation volume and 19.16% precision gain
- Multi-task learning achieves 68.73 PR-AUC, outperforming sequential training (66.96) and zero-shot LLaVA (23.17)

## Why This Works (Mechanism)

### Mechanism 1: Cascade Filtering Reduces MLLM Compute to 1.5%
The two-stage router-ranking architecture enables industrial-scale MLLM deployment by filtering ~97.5% of traffic before expensive MLLM inference. The router uses embedding-based retrieval against a seed bank of high-risk videos to compute semantic similarity. Only videos exceeding a similarity threshold proceed to the MLLM ranker, which performs fine-grained classification via single-token prediction. The core assumption is that the router maintains sufficiently high recall that harmful content is rarely filtered out at stage one. Evidence shows the cascading deployment reduces computational cost to only 1.5% of direct full-scale deployment. Break condition: If router recall drops (e.g., novel harm types absent from seed bank), harmful content bypasses the ranker entirely.

### Mechanism 2: Single-Token Probability Extraction Enables Generative-to-Discriminative Conversion
Constraining MLLM output to a single token (Yes/No) and extracting softmax-normalized logits yields calibrated classification scores without architectural changes. During inference, the model generates a single token; logits for "Y" and "N" tokens are extracted and softmaxed to produce probability scores S = [p_Y, p_N]. This allows threshold tuning and downstream ensembling while preserving the generative backbone. The core assumption is that the MLLM's internal representations are sufficiently discriminative after SFT; single-token constraint does not degrade reasoning quality. Evidence shows Algorithm 1 explicitly implements softmax normalization over binary token logits. Break condition: If the model produces out-of-vocabulary tokens or refuses to answer, probability extraction fails.

### Mechanism 3: Mixed Multi-Task SFT with Minimal Classification Data
Supervised fine-tuning on a 1:1:1 mixture of VQA, captioning, and classification data (totaling 300K samples) yields strong moderation performance using only 2% of the annotation data required by traditional classifiers. VQA and caption data provide visual comprehension and contextual grounding; classification data injects task-specific discriminative signals. Multi-task mixing outperforms sequential phased learning across all prompt templates. The core assumption is that VQA and caption tasks transfer positively to moderation judgment. Evidence shows multi-task learning achieves 68.73 PR-AUC vs. 66.96 for sequential training. Break condition: If classification data is too small or imbalanced, model may overfit to VQA/caption patterns without learning moderation boundaries.

## Foundational Learning

- **Embedding-based retrieval (nearest-neighbor search)**: Why needed here - The router relies on embedding similarity between candidate videos and a curated seed bank; understanding approximate nearest neighbor (ANN) indexing is critical for low-latency retrieval. Quick check - Can you explain why cosine similarity in embedding space approximates semantic relatedness, and why ANN trades off recall for speed?

- **Next-token prediction and logit extraction**: Why needed here - The ranker operates via next-token prediction; classification probabilities come from raw logits, not from sampling. Understanding autoregressive generation is essential for debugging output issues. Quick check - Given logits [2.1, -0.5] for tokens ["Yes", "No"], what is the softmax probability for "Yes"?

- **Prompt engineering for constrained generation**: Why needed here - The system uses four prompt templates to steer model behavior; prompt structure directly affects classification performance (P2 outperforms P4). Quick check - Why might asking two questions separately (P2) outperform combining them into one prompt (P4)?

## Architecture Onboarding

- **Component map**: Video upload -> Router embedding extraction -> Similarity scoring vs. seed bank -> (if above threshold) -> MLLM inference -> Single-token probability extraction -> Threshold decision -> Action/No-action

- **Critical path**: Video upload → Router embedding extraction → Similarity scoring vs. seed bank → (if above threshold) → MLLM inference → Single-token probability extraction → Threshold decision → Action/No-action

- **Design tradeoffs**:
  - Router threshold: Higher threshold → more MLLM invocations → higher recall but higher compute. Lower threshold → lower compute but higher miss rate.
  - Multi-task vs. sequential training: Multi-task yields slightly better performance; sequential is more flexible for incremental updates to classification data.
  - Prompt template: P2 (separate questions) > P4 (combined); definitions can introduce noise.

- **Failure signatures**:
  - Router miss spike: Novel harm type not represented in seed bank → traffic bypasses ranker → undetected violations.
  - Ranker token mismatch: Model outputs token outside {"Y", "N"} → probability extraction fails → fallback required.
  - Distribution shift: Online traffic drifts from SFT distribution → calibrated scores become unreliable.

- **First 3 experiments**:
  1. Router ablation: Vary the number of seed videos and selection strategy (Centroid-Proximity vs. Manual); measure recall@K on held-out violation set.
  2. Prompt sweep: Run all four prompt templates (P1–P4) on a fixed validation set; compare PR-AUC and Max-F1 to confirm P2 superiority in your data regime.
  3. Threshold calibration: Sweep decision thresholds on router similarity and ranker probability; plot precision-recall curves to identify operating points that meet latency and accuracy constraints.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the cascade architecture be modified to mitigate the risk of missed detection (false negatives) caused by the lightweight router filtering out non-compliant videos?
- Basis in paper: The authors state in the Limitations section that "due to the limitations of the router component, the system still carries a risk of missed detection."
- Why unresolved: The router acts as a hard filter; any violating video incorrectly classified as low-risk by the router is permanently excluded from the MLLM ranker, and the paper does not propose a mechanism to recover these specific cases.
- What evidence would resolve it: A proposed architectural adjustment (e.g., a secondary low-cost sampler or uncertainty-based routing) demonstrating improved recall on edge cases without compromising the 98.5% computational savings.

### Open Question 2
- Question: Can the reliance on human-annotated data for Supervised Fine-Tuning (SFT) be further reduced or eliminated to prevent the introduction of label noise?
- Basis in paper: The Limitations section notes, "The current model still relies on a small amount of human-annotated data, which may introduce additional noise."
- Why unresolved: While the system uses minimal data (2%), it still requires a labeled classification dataset ($D_3$) for the ranker, making it vulnerable to human annotation errors.
- What evidence would resolve it: Experiments demonstrating that a purely synthetic or weakly-supervised fine-tuning approach matches the performance of the current human-annotated baseline.

### Open Question 3
- Question: Can the single-token output constraint be relaxed to provide interpretable reasoning for moderation decisions without violating the strict latency requirements of industrial deployment?
- Basis in paper: The authors restrict the MLLM to a single-token output (Algorithm 1) to optimize for speed and classification probability, effectively discarding the generative explanatory capabilities of the model.
- Why unresolved: The paper highlights the superior reasoning of MLLMs as a motivation, but the proposed solution transforms the model into a "black box" classifier similar to the traditional models it replaces.
- What evidence would resolve it: A latency-analysis of a "chain-of-thought" approach that successfully balances the generation of explanatory text with the online serving constraints.

## Limitations

- The system still carries a risk of missed detection due to limitations of the router component filtering out non-compliant videos
- The current model still relies on a small amount of human-annotated data, which may introduce additional noise
- The specific prompt templates (P1-P4) and router embedding model architecture are not fully specified, limiting reproducibility

## Confidence

- **High Confidence**: The fundamental feasibility of using MLLMs for content moderation through supervised fine-tuning, and the general architecture of a cascade system (router + ranker) to reduce computational costs
- **Medium Confidence**: The specific 66.50% F1 improvement over traditional models and the 41% increase in automatic moderation volume, which depend heavily on implementation details that are underspecified
- **Low Confidence**: The claim that multi-task learning with only 300K samples (2% of traditional data) achieves superior performance without knowing the baseline data requirements

## Next Checks

1. **Router recall validation**: Conduct controlled experiments varying the seed bank size and selection strategy (Centroid-Proximity vs Manual) on a held-out violation set. Measure recall@K and false negative rates across different harm categories to characterize when the router misses content.

2. **Prompt template ablation study**: Implement and test all four prompt templates (P1-P4) on a fixed validation set with known ground truth. Compare PR-AUC, Max-F1, and calibration quality across templates to confirm which format works best for your specific moderation task and model configuration.

3. **Threshold calibration and operating point analysis**: Systematically sweep decision thresholds on both router similarity scores and ranker probability outputs. Plot precision-recall curves to identify operating points that meet your specific latency and accuracy constraints.