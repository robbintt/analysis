---
ver: rpa2
title: Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards
arxiv_id: '2509.24981'
source_url: https://arxiv.org/abs/2509.24981
tags:
- rover
- policy
- arxiv
- reasoning
- diversity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ROVER (Random Policy Valuation for Diverse
  Reasoning), a minimalist reinforcement learning method for improving LLM reasoning
  with verifiable rewards. The key insight is that in deterministic, tree-structured
  MDPs with binary terminal rewards, the optimal action can be recovered from Q-values
  of a fixed uniform random policy, eliminating the need for iterative policy evaluation-improvement
  loops.
---

# Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards

## Quick Facts
- arXiv ID: 2509.24981
- Source URL: https://arxiv.org/abs/2509.24981
- Authors: Haoran He; Yuxiao Ye; Qingpeng Cai; Chen Hu; Binxing Jiao; Daxin Jiang; Ling Pan
- Reference count: 34
- One-line primary result: Achieves +8.2 pass@1 and +16.8 pass@256 improvements over baselines on competition math reasoning tasks while improving diversity by +17.6%

## Executive Summary
ROVER (Random Policy Valuation for Diverse Reasoning) introduces a minimalist reinforcement learning method for improving LLM reasoning with verifiable rewards. The key insight is that in deterministic, tree-structured MDPs with binary terminal rewards, the optimal action can be recovered from Q-values of a fixed uniform random policy, eliminating the need for iterative policy evaluation-improvement loops. By evaluating a uniformly random policy and sampling actions via softmax over its Q-values, ROVER achieves optimality while preserving diversity. Extensive experiments demonstrate superiority on competition-level math reasoning tasks, maintaining higher entropy throughout training and discovering novel reasoning strategies absent from base models.

## Method Summary
ROVER evaluates a uniformly random policy and samples actions via softmax over its Q-values, achieving optimality while preserving diversity. The method bypasses standard policy iteration by parameterizing Q intrinsically through the LLM's log-probabilities with a fixed baseline policy. Low-variance rewards are computed using group reward centering. Training uses intrinsic Q-parameterization Q(s,a) = ρ(log π_θ - log π_old), Bellman targets with vocabulary-averaged next Q, and MSE loss. The approach avoids the instability and diversity collapse common in PPO/GRPO methods while maintaining theoretical optimality guarantees under deterministic tree-structured MDP assumptions.

## Key Results
- Achieves +8.2 improvement on pass@1 and +16.8 on pass@256 over baselines on competition-level math reasoning tasks
- Improves diversity by +17.6% while maintaining quality guarantees
- Maintains higher entropy throughout training compared to PPO/GRPO methods
- Discovers novel reasoning strategies absent from base models and shows enhanced reflection behaviors with more frequent "forking tokens"
- Results are consistent across multiple model sizes (4B-8B parameters) and benchmarks including AIME, HMMT, and OlympiadBench

## Why This Works (Mechanism)

### Mechanism 1: Optimal Policy Recovery via Uniform Policy Q-Values
In deterministic tree-structured MDPs with binary terminal rewards, optimal actions can be derived from Q-values of a fixed uniform random policy without iterative policy improvement. The uniform policy Q-value Q^π^u(s,a) equals the probability that taking action a at state s, then acting uniformly randomly until termination, yields a correct outcome. Since each action leads to a disjoint subtree (tree structure), Q-values naturally rank action quality—zero for branches with no valid solutions, positive for promising paths. The method relies on finite-horizon, deterministic transitions, tree-structured dynamics, and binary terminal rewards.

### Mechanism 2: Diversity Preservation via Softmax Q-Sampling
Sampling actions proportional to exp(Q/ρ) rather than greedy selection maintains quality guarantees while preserving reasoning diversity. Higher Q-values indicate denser successful continuations. Softmax sampling explores multiple pathways proportionally to their estimated success probability rather than collapsing to a single path. Temperature ρ controls exploration-exploitation; as ρ→0, the policy converges to optimal. The approach assumes multiple valid reasoning paths exist for a given problem and Q-values meaningfully discriminate between optimal and suboptimal actions at "key states."

### Mechanism 3: Practical Training Stability via Relative Q-Parameterization and Group Centering
Intrinsically parameterizing Q through LLM log-probabilities with a fixed baseline policy and group reward centering enables stable training at scale. Q(s,a) = ρ(log π_θ - log π_old) measures improvement over the behavior policy, reducing target drift. Group reward centering (r̃ = r - mean(r_group)) reduces variance from sparse binary rewards by providing comparative signal within sampled groups. The method assumes the LLM's intrinsic policy provides a reasonable prior for navigation and mean-centered rewards adequately capture relative quality within groups.

## Foundational Learning

### Concept: Generalized Policy Iteration (GPI)
Why needed here: ROVER's core contribution is bypassing the GPI loop that standard RL methods use. Understanding GPI clarifies what ROVER eliminates and why this simplification is theoretically grounded. Quick check: Can you explain why iterative evaluation-improvement cycles cause non-stationary targets in PPO/GRPO?

### Concept: MDP Structure Taxonomy
Why needed here: ROVER's theoretical guarantee depends on specific MDP properties (deterministic, tree-structured, binary terminal rewards). Recognizing when these hold vs. break is critical for knowing when to apply ROVER. Quick check: Does an LLM reasoning task with tool calls (intermediate API feedback) still satisfy ROVER's assumptions? Why or why not?

### Concept: Softmax Operator vs. Max Operator in Value Learning
Why needed here: ROVER uses mean operator (uniform policy evaluation) combined with softmax action selection. Understanding why this differs from standard Q-learning's max operator clarifies the theoretical contribution. Quick check: In the tabular example (Figure 5), why does Q-learning converge to a single mode while ROVER's softmax sampling covers all four?

## Architecture Onboarding

### Component Map:
Rollout Buffer -> Reward Centering Module -> Q-Estimator -> Bellman Target Constructor -> Loss Computer

### Critical Path:
Prompt → Rollout n responses → Verify rewards → Center rewards → For each position: compute current Q, compute target Q', compute loss → Backprop → Update θ

### Design Tradeoffs:
- Group size n: Larger n reduces variance but increases compute per step. Paper uses n=5-8.
- Temperature ρ: Default ρ=1 works robustly; lower improves pass@1 at diversity cost; higher preserves diversity but slows convergence.
- β (Q' scaling): β∈[0.2, 1.0] is robust; β=0 breaks entropy; β>1 diminishes reward signal.

### Failure Signatures:
- Entropy collapse to near-zero: Check if β is too low or ρ is too small (<0.1)
- No learning progress: Check if all rewards in groups are identical (centering yields zeros)
- Pass@1 improves but pass@k degrades: Temperature may be too low, causing mode collapse
- Training instability/spikes: Check if response length varies dramatically; may need length filtering

### First 3 Experiments:
1. Validate on tabular MDP: Implement the toy example from Figure 5(a) with 4 correct terminal states. Confirm ROVER discovers all 4 modes while Q-learning with ε-greedy converges to one.
2. Countdown task ablation: Train on TinyZero countdown dataset with ρ∈{0.01, 0.1, 1.0, 3.0}. Plot entropy curves and distinct solutions found. Validate Figure 8 results.
3. Q' term ablation: Train with β∈{0.0, 0.2, 1.0, 5.0}. Measure entropy, response length, and pass@k. Confirm β=0 causes collapse per Figure 14.

## Open Questions the Paper Calls Out

### Open Question 1
Can ROVER maintain its theoretical guarantees and performance in MDPs that violate the assumptions of deterministic transitions or binary terminal rewards? The paper notes that while autoregressive LLM generation aligns with these properties, they "may not strictly hold in all extended RLVR applications (e.g., with tool calls or with intermediate feedback)." The optimality proof relies specifically on tree structure and binary rewards; it is unknown if the method degrades gracefully if the MDP becomes cyclic or has dense rewards.

### Open Question 2
To what extent does the intrinsic Q-parameterization approximate the true Q-values of a uniform random policy versus acting as a regularizer? The method implements Q as ρ(log π_θ - log π_old) for practical scalability, acknowledging that this "introduces approximation" to the theoretical framework. The paper proves optimality using exact Q-values, but the practical algorithm relies on an intrinsic parameterization that might introduce biases not explored in the theory.

### Open Question 3
Does ROVER scale efficiently to models significantly larger than 8B parameters? The paper states, "Our experiments are limited to math reasoning tasks with models up to 8B parameters due to restricted computational resources." While the method avoids diversity collapse in smaller models, the dynamics of intrinsic Q-estimation and the stability of the Bellman target are untested in larger parameter spaces.

## Limitations
- Theoretical guarantees rely on strict MDP assumptions (deterministic, tree-structured, binary terminal rewards) that rarely hold in practical LLM applications
- Intrinsic Q-parameterization via log-probability differences lacks theoretical grounding in deep RL literature and may not generalize beyond math reasoning
- Diversity preservation claims depend heavily on specific reward centering scheme and temperature tuning, with limited ablation on alternative diversity metrics

## Confidence

| Claim | Confidence |
|-------|------------|
| Empirical performance improvements on established benchmarks | High |
| Theoretical correctness of uniform policy Q-value derivation under stated assumptions | High |
| Stability advantages over PPO/GRPO variants | High |
| Claims about discovering novel reasoning strategies | Medium |
| Robustness to temperature settings | Medium |
| Generalization across model scales (4B-8B parameters) | Medium |
| Intrinsic Q-parameterization mechanism | Low |
| Transfer to non-mathematical reasoning tasks with intermediate rewards | Low |
| Specific role of Q' term in preventing entropy collapse | Low |

## Next Checks

1. Test ROVER on a modified countdown task with stochastic transitions or cycles to verify theoretical assumptions break as expected.
2. Implement an ablation study removing the Q' term (β=0) on competition math problems to quantify the exact entropy collapse effect.
3. Apply ROVER to a non-mathematical reasoning task with intermediate rewards (e.g., tool-use reasoning) and measure performance degradation relative to stated assumptions.