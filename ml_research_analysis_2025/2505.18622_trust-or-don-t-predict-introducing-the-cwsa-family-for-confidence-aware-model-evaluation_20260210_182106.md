---
ver: rpa2
title: 'Trust, or Don''t Predict: Introducing the CWSA Family for Confidence-Aware
  Model Evaluation'
arxiv_id: '2505.18622'
source_url: https://arxiv.org/abs/2505.18622
tags:
- cwsa
- confidence
- accuracy
- selective
- predictions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CWSA and CWSA+, two new metrics for evaluating
  machine learning models that use confidence scores in selective prediction. Traditional
  metrics fail to capture the reliability of predictions when models abstain from
  making uncertain predictions.
---

# Trust, or Don't Predict: Introducing the CWSA Family for Confidence-Aware Model Evaluation

## Quick Facts
- arXiv ID: 2505.18622
- Source URL: https://arxiv.org/abs/2505.18622
- Reference count: 23
- This paper introduces CWSA and CWSA+, two new metrics for evaluating machine learning models that use confidence scores in selective prediction.

## Executive Summary
Traditional evaluation metrics fail to capture the reliability of machine learning models when they abstain from making uncertain predictions. This paper introduces CWSA (Confidence-Weighted Selective Accuracy) and CWSA+ as new metrics that explicitly reward confident correct predictions while penalizing overconfident mistakes. The metrics address a critical gap in selective prediction evaluation by providing threshold-local, interpretable scores that better reflect trust and reliability in safety-critical applications.

## Method Summary
CWSA and CWSA+ evaluate selective prediction models by incorporating confidence scores into metric computation. The metrics filter predictions by confidence threshold τ, apply linear confidence weighting φ(c) = (c-τ)/(1-τ), and compute either signed scores (CWSA) or unsigned correctness-weighted scores (CWSA+). Experiments on MNIST, CIFAR-10, and synthetic models demonstrate that these metrics effectively distinguish between calibrated and overconfident models, outperforming classical metrics like selective accuracy and AURC in trust-sensitive settings.

## Key Results
- CWSA and CWSA+ satisfy desirable axioms including monotonicity, overconfidence penalty, and calibration responsiveness
- Experiments show the metrics effectively distinguish between calibrated and overconfident models
- The metrics are computationally efficient, threshold-local, and interpretable for safety-critical AI applications

## Why This Works (Mechanism)

### Mechanism 1: Asymmetric Reward-Penalty Weighting
CWSA produces a signed metric that explicitly penalizes overconfident misclassifications more heavily than low-confidence errors. The formula CWSA(τ) = (1/|Sτ|) Σ φ(ci) · (2·I{ŷᵢ=yᵢ} − 1) multiplies a confidence-weighted score by +1 for correct predictions and −1 for incorrect ones. Since φ(c) = (c − τ)/(1 − τ) increases with confidence, incorrect predictions at high confidence contribute larger negative values.

### Mechanism 2: Threshold-Local Decomposability
CWSA/CWSA+ enable per-instance attribution and threshold-specific evaluation, unlike integrated metrics like AURC. The metric is computed as a simple average over the retained set Sτ = {i | ci ≥ τ}. Each instance contributes φ(ci) · (2δi − 1) independently, making the evaluation both local to the threshold and interpretable.

### Mechanism 3: Confidence-Weighted Normalization (CWSA+)
CWSA+ produces a bounded [0, 1] score that distinguishes marginally-confident correctness from highly-confident correctness. By removing the penalty term and computing CWSA+(τ) = (1/|Sτ|) Σ φ(ci) · I{ŷᵢ=yᵢ}, the metric rewards correct predictions proportionally to how far above threshold their confidence lies.

## Foundational Learning

- **Selective Prediction (Abstention)**: Why needed here: The entire CWSA framework assumes models can decline to predict when confidence < τ. Understanding the coverage-accuracy trade-off is prerequisite.
  - Quick check question: Given a model with 90% accuracy and 70% coverage at τ = 0.8, what happens to coverage if τ rises to 0.95?

- **Calibration vs. Trustworthiness**: Why needed here: The paper explicitly distinguishes ECE (calibration) from CWSA (trustworthiness). A perfectly calibrated model can still be dangerous if it makes high-confidence errors.
  - Quick check question: A model predicts "benign" with 99% confidence on a malignant tumor. If ECE is low, is the model trustworthy for clinical deployment?

- **Risk-Coverage Curve (RCC) and AURC**: Why needed here: CWSA is positioned as an alternative to AURC that preserves threshold-locality. Understanding AURC's limitations motivates the new metrics.
  - Quick check question: Why does AURC fail to distinguish two models with identical area but different error distributions across confidence levels?

## Architecture Onboarding

- Component map: Input -> Filter -> Weight -> Score
- Critical path: Threshold selection τ → coverage determination → confidence weighting → metric computation. The threshold τ must be chosen before evaluation; it cannot be reverse-engineered from the metric alone.
- Design tradeoffs: CWSA vs CWSA+: Use CWSA when overconfident errors are safety-critical (penalty matters). Use CWSA+ for ranking/comparison where interpretability of [0,1] scale is preferred.
- Failure signatures: Negative CWSA at high coverage → systematic overconfident errors; CWSA+ near 0.5 at high τ → correct predictions barely exceed threshold.
- First 3 experiments:
  1. Compute CWSA/CWSA+ at τ ∈ {0.5, 0.7, 0.9, 0.95} on your trained model to verify Selective Accuracy increases with τ while CWSA may decrease if overconfident errors exist.
  2. Synthesize an overconfident variant of your model by scaling logits before softmax and confirm CWSA drops more than Selective Accuracy.
  3. Plot coverage vs CWSA+ curve to identify the τ value where CWSA+ begins to degrade—this is the operational ceiling for trustworthy deployment.

## Open Questions the Paper Calls Out

### Open Question 1
Can CWSA be adapted into a differentiable loss function to enable end-to-end optimization of selective prediction models?
- Basis in paper: The paper states, "Future work may investigate smoothed or relaxed variants allowing for end-to-end optimization as the loss function" because current formulations lack inherent differentiability due to hard thresholding.
- Why unresolved: The reliance on a discrete selection threshold (τ) prevents the backpropagation of gradients through the metric's reward-penalty mechanism.
- What evidence would resolve it: The derivation of a continuous relaxation of the thresholding function or weighting mechanism that preserves the metric's axiomatic properties while remaining differentiable.

### Open Question 2
How can the CWSA framework be generalized to regression, structured prediction, and multi-label classification?
- Basis in paper: The authors identify the limitation that "this work has focused on multi-class classification" and explicitly list extending the metrics to "regression settings, structured prediction, and multi-label classification" as a main challenge.
- Why unresolved: The current metric definitions rely on discrete correctness indicators (I[yᵢ = ŷᵢ]) and scalar confidence scores derived from softmax outputs, which do not translate directly to continuous errors or complex output spaces.
- What evidence would resolve it: New formulations of CWSA that incorporate distance-based error metrics for regression or structural compatibility functions for sequences, validated on non-classification benchmarks.

### Open Question 3
What are the theoretical connections between CWSA and formal uncertainty frameworks like conformal prediction or Bayesian inference?
- Basis in paper: The paper notes, "There remains considerable theoretical work in connecting CWSA to formal risk bounds, Bayesian uncertainty and conformal prediction."
- Why unresolved: While CWSA is empirically validated against calibration metrics, it lacks a formal theoretical link to the statistical guarantees provided by conformal prediction sets or Bayesian posterior distributions.
- What evidence would resolve it: Proofs demonstrating how CWSA scores correlate with or bound the coverage probabilities of conformal predictors or the fidelity of Bayesian uncertainty estimates.

## Limitations
- Confidence calibration dependency: CWSA assumes softmax confidences meaningfully reflect prediction uncertainty
- Threshold selection sensitivity: The operational meaning of CWSA/CWSA+ depends critically on appropriate τ selection
- Synthetic data representativeness: Controlled synthetic confidence distributions may not capture real-world complexity

## Confidence
- **High confidence**: The mathematical formulation of CWSA/CWSA+ is sound and the theoretical axioms (monotonicity, overconfidence penalty) are correctly proven.
- **Medium confidence**: Experimental results showing CWSA/CWSA+ distinguishing between model types are compelling but rely on synthetic data with controlled properties.
- **Medium confidence**: The claim that CWSA/CWSA+ are superior to AURC for threshold-specific evaluation is supported but not conclusively proven across diverse scenarios.

## Next Checks
1. Apply CWSA/CWSA+ to a safety-critical application (e.g., medical diagnosis or autonomous driving perception) and compare metric behavior against actual deployment outcomes and failure rates.
2. Test whether CWSA/CWSA+ remain meaningful when models face adversarial examples or distribution shifts that systematically degrade confidence calibration.
3. Experiment with non-linear confidence weighting (e.g., quadratic or sigmoid-based φ(c)) to determine if linear scaling is optimal for capturing trust-relevant behaviors.