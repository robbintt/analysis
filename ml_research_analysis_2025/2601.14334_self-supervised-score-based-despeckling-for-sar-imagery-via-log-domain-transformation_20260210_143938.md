---
ver: rpa2
title: Self-Supervised Score-Based Despeckling for SAR Imagery via Log-Domain Transformation
arxiv_id: '2601.14334'
source_url: https://arxiv.org/abs/2601.14334
tags:
- image
- noise
- despeckling
- self-supervised
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces S4DM, a self-supervised framework for despeckling
  SAR imagery using score-based generative models in the log domain. The method transforms
  SAR data using logarithmic and Log-Yeo-Johnson (LYJ) transformations to convert
  multiplicative Gamma-distributed speckle into approximately Gaussian noise, enabling
  score-based modeling.
---

# Self-Supervised Score-Based Despeckling for SAR Imagery via Log-Domain Transformation

## Quick Facts
- arXiv ID: 2601.14334
- Source URL: https://arxiv.org/abs/2601.14334
- Authors: Junhyuk Heo
- Reference count: 17
- Key outcome: S4DM achieves higher ENL scores than FANS, SAR-BM3D, DIP, and S3DIP on Sentinel-1 data while offering faster inference times

## Executive Summary
This paper introduces S4DM, a self-supervised framework for despeckling SAR imagery using score-based generative models in the log domain. The method transforms SAR data using logarithmic and Log-Yeo-Johnson (LYJ) transformations to convert multiplicative Gamma-distributed speckle into approximately Gaussian noise, enabling score-based modeling. A self-supervised objective inspired by Corruption2Self trains a neural network to predict clean images from corrupted versions of the input, eliminating the need for clean ground truth. Experiments on Sentinel-1 data show S4DM achieves higher ENL (Equivalent Number of Looks) scores compared to baselines like FANS, SAR-BM3D, DIP, and S3DIP, indicating superior speckle suppression in homogeneous regions. Additionally, S4DM offers faster inference times, making it a practical and efficient solution for SAR image restoration.

## Method Summary
S4DM addresses SAR despeckling by first applying a logarithmic transform to convert multiplicative speckle into additive noise, then applying a Log-Yeo-Johnson (LYJ) transformation to approximate Gaussian noise distribution. The method trains a score-based neural network using a self-supervised objective that corrupts the input with Gaussian noise and trains the network to reconstruct the original noisy image. During inference, the trained network processes the log+LYJ transformed SAR image to predict the clean signal, which is then inverted back to the original domain. The approach eliminates the need for clean ground truth images and achieves both high despeckling performance and fast inference speeds.

## Key Results
- S4DM achieves higher ENL scores on Sentinel-1 test images compared to FANS, SAR-BM3D, DIP, and S3DIP
- Inference time of S4DM is 0.05 seconds versus ~81 seconds for S3DIP
- Method successfully handles both VV and VH polarization data from Sentinel-1
- Self-supervised approach eliminates need for clean reference images

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Logarithmic and Log-Yeo-Johnson (LYJ) transformations convert multiplicative Gamma-distributed speckle into approximately additive Gaussian noise.
- **Mechanism:** SAR speckle is fundamentally multiplicative ($X_{data} = X_{target} \odot \nu$). A logarithmic transform converts this multiplication into addition ($Y_{data} = \log X_{target} + \log \nu$). However, log-Gamma noise is non-Gaussian. The LYJ transformation ($T_{\lambda}$) is then applied to reshape the noise probability distribution, minimizing skewness and kurtosis to approximate a Gaussian distribution ($Z_{data} = T_{\lambda^\dagger}(\log X_{target}) + \xi$).
- **Core assumption:** The parameter $\lambda^\dagger$ can be successfully optimized such that the residual noise $\xi$ behaves statistically as Gaussian noise $\mathcal{N}(0, \sigma^2_{data}I)$, satisfying the preconditions for Tweedie's formula.
- **Evidence anchors:**
  - [abstract] "convert the speckle noise residuals into an approximately additive Gaussian distribution."
  - [section 3.2] "This step is crucial as it allows the principled application of score-matching techniques and Tweedie's formula in the transformed domain."
  - [corpus] Limited direct corpus validation for the specific LYJ+SAR combination; related work "Speckle2Self" addresses speckle in ultrasound but uses different self-supervision.
- **Break condition:** If the LYJ transformation fails to achieve approximate Gaussianity (e.g., heavy tails persist), the score-based denoiser will mismatch the true noise model, potentially leaving artifacts or over-smoothing.

### Mechanism 2
- **Claim:** A self-supervised objective based on "Corruption2Self" allows the model to learn the clean signal by relating the expectation of the noisy input to the expectation of the target using a shared score function.
- **Mechanism:** The method creates a corrupted version $Z_t$ of the input $Z_{data}$ by adding Gaussian noise. It leverages the fact that $E[Z_{data}|Z_t]$ and $E[Z_{target}|Z_t]$ share the same score term ($S_t$) but differ in variance scaling. By formulating a loss that reconstructs the known noisy input $Z_{data}$ from $Z_t$ using a blending factor $\delta$ (Eq. 11), the network $f_\theta$ is implicitly forced to predict $E[Z_{target}|Z_t]$ (the clean signal) without ever seeing it.
- **Core assumption:** The noise variances $\sigma^2_{data}$ and $\sigma^2_{target} \approx 0$ are known or estimated accurately; the relationship $\delta = (\sigma^2_t - \sigma^2_{data})/(\sigma^2_t - \sigma^2_{target})$ holds valid.
- **Evidence anchors:**
  - [section 3.3] "Leveraging the common score term in Eqs. (9) and (10), we can relate the conditional expectation..."
  - [section 3.4] "By minimizing this loss, $f_\theta(Z_t, t)$... is trained such that the term $\delta \hat{Z} + (1-\delta)Z_t$ accurately approximates $E[Z_{data}|Z_t]$."
  - [corpus] The method cites Corruption2Self (Tu et al., 2025); corpus neighbors do not contradict this approach but offer alternatives like "SARMAE" for representation learning.
- **Break condition:** Inaccurate estimation of the input noise level $\sigma^2_{data}$ will distort the blending factor $\delta$, causing the network to predict a weighted average of the wrong signal/noise mix.

### Mechanism 3
- **Claim:** Amortized inference via a pre-trained network provides significantly faster despeckling compared to per-image optimization methods.
- **Mechanism:** Unlike Deep Image Prior (DIP) or S3DIP, which optimize network weights iteratively for every single image (taking ~80s), S4DM trains a generic network $f_\theta$ once on a dataset. Inference is a single forward pass plus fast transform/invert operations (~0.05s).
- **Core assumption:** The training dataset is sufficiently representative of the test images so that the pre-trained weights generalize effectively without per-image fine-tuning.
- **Evidence anchors:**
  - [abstract] "exhibits significantly shorter inference times compared to many existing self-supervised techniques"
  - [table 1] Shows S4DM at 0.05s vs S3DIP at ~81s.
  - [corpus] Consistent with general deep learning trends vs. optimization-based methods.
- **Break condition:** If test images have significantly different noise statistics (e.g., different number of looks $L$) or scattering mechanisms than the training set, the fixed weights may fail to despeckle effectively.

## Foundational Learning

**Concept:** Tweedie's Formula
- **Why needed here:** This is the mathematical bridge allowing the network to estimate a clean image from a noisy one using the "score" (gradient of log-density). It justifies why making noise Gaussian is necessary.
- **Quick check question:** Given $X_t = X_0 + \sigma \epsilon$, how does Tweedie's formula relate $E[X_0|X_t]$ to the score function $\nabla \log p(X_t)$?

**Concept:** Multiplicative vs. Additive Noise Models
- **Why needed here:** Standard denoisers assume additive noise ($y = x + n$). SAR noise is multiplicative ($y = x \cdot n$). You must understand why the Log transform is a non-negotiable first step.
- **Quick check question:** Why does applying a log-transform to SAR intensity data convert multiplicative speckle into additive noise?

**Concept:** Score-Based Generative Models (SGM)
- **Why needed here:** The paper frames despeckling as a generative problem (reversing noise corruption). Understanding that the network learns the "gradient of the data distribution" helps interpret the training objective.
- **Quick check question:** In the context of this paper, does the network predict the clean image directly, or the "score" which is then used to derive the clean image? (Trick: The paper uses the score relationship to define the target for the network).

## Architecture Onboarding

**Component map:**
Noisy SAR $X_{data}$ -> Log Transform -> LYJ Transform (λ†) -> $Z_{data}$ -> (Training only: Add noise ξ -> $Z_t$) -> Network $f_\theta(Z_t, t)$ -> $\hat{Z}$ -> Inverse LYJ -> Exponential -> Despeckled $X$

**Critical path:** The estimation of the LYJ parameter $\lambda^\dagger$ (minimizing kurtosis/skewness) and the input noise variance $\sigma^2_{data}$. If these are wrong, the "Gaussian" assumption fails and the loss weighting $\delta$ is incorrect.

**Design tradeoffs:**
- **Generalization vs. Specificity:** The model trades the potentially higher performance of per-image optimization (S3DIP) for massive speed gains (S4DM).
- **Gaussian Approximation:** Using LYJ forces a Gaussian approximation which may not perfectly hold, potentially introducing minor bias compared to modeling the exact Gamma distribution directly (if that were tractable).

**Failure signatures:**
- **Residual "Spikiness":** If LYJ fails to normalize the noise, the model may smooth out high-frequency details thinking they are noise, or leave speckle intact.
- **Bias in Homogeneous Areas:** If $\sigma_{data}$ is overestimated, the model may over-smooth, reducing texture in agricultural fields.

**First 3 experiments:**
1. **Noise Distribution Validation:** Take a patch of homogeneous SAR data, apply Log + LYJ with the calculated $\lambda$, and plot the histogram. Confirm it fits a Gaussian curve (Q-Q plot).
2. **Ablation on Transform:** Train the same network using only Log-transformed data (no LYJ) vs. Log+LYJ. Compare ENL scores to quantify the impact of the Gaussianization.
3. **Inference Speed Benchmark:** Replicate Table 1 timing. Measure the breakdown: Time for Preprocessing vs. Network Forward Pass vs. Postprocessing.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does S4DM quantitatively balance speckle suppression with the preservation of fine textural details compared to modern self-supervised CNN baselines?
- **Basis:** [inferred] The paper relies exclusively on Equivalent Number of Looks (ENL) for quantitative evaluation and omits comparisons against self-supervised CNNs like SAR2SAR and MERLIN, which are mentioned in the introduction but excluded from the experimental benchmarks.
- **Why unresolved:** High ENL scores can result from excessive over-smoothing; without metrics for detail preservation or texture, the trade-off between the score-based model and standard SSL CNNs remains undefined.
- **What evidence would resolve it:** Quantitative results using no-reference edge preservation indices or tests on simulated data with ground truth to calculate PSNR/SSIM against SAR2SAR and MERLIN.

### Open Question 2
- **Question:** To what extent does the accuracy of the Gaussian approximation in the Log-Yeo-Johnson domain impact the stability of the score-matching objective?
- **Basis:** [inferred] The method theoretically depends on the LYJ transformation converting noise residuals into an "approximately Gaussian" distribution to validate the use of Tweedie's formula, but the paper provides no analysis on the model's sensitivity to deviations from this Gaussian ideal.
- **Why unresolved:** If the transformed noise retains significant skewness or kurtosis, the assumptions underlying the self-supervised objective (Eq. 12) may be violated, potentially leading to biased reconstruction.
- **What evidence would resolve it:** Ablation studies measuring performance degradation when the input noise is intentionally forced to have higher non-Gaussian statistics after transformation.

### Open Question 3
- **Question:** Does the assumption of a global LYJ transformation parameter ($\lambda$) limit performance in spatially heterogeneous scenes?
- **Basis:** [inferred] The method selects $\lambda$ to minimize sample kurtosis, likely operating on global image statistics. This assumes stationary noise characteristics which may not hold for complex, heterogeneous SAR scenes containing both urban and natural terrain.
- **Why unresolved:** A single transformation parameter may fail to optimally normalize speckle across different regions of the same image (e.g., bright urban targets vs. dark water), potentially leaving residual non-Gaussian noise in local patches.
- **What evidence would resolve it:** Evaluation of local noise statistics and despeckling quality in spatially distinct regions (urban vs. agricultural) within the same large-scale image.

## Limitations
- The LYJ transformation parameter estimation and noise variance calculation are critical steps not fully specified, creating potential reproducibility issues
- The paper does not address how well the method preserves fine textural details versus over-smoothing for high ENL scores
- No comparison is made against modern self-supervised CNN methods like SAR2SAR and MERLIN

## Confidence

**Major Uncertainties:**
- **Parameter Tuning:** The critical steps of estimating the Log-Yeo-Johnson (LYJ) parameter $\lambda^\dagger$ and the input noise variance $\sigma^2_{data}$ are not fully specified in the paper. Poor estimation will break the Gaussian approximation and loss weighting, respectively. **Confidence: Medium** on the practical robustness of these estimation steps.
- **Architecture Details:** The specific neural network architecture (number of layers, channels, attention) is not provided, only described as "U-Net-like." This limits exact reproducibility. **Confidence: Low** in replicating the reported inference speed without this detail.

**Confidence Labels:**
- **High Confidence:** The core mechanism of using log transforms to convert multiplicative noise to additive noise is well-established in SAR literature. The self-supervised objective derivation from Corruption2Self is mathematically sound given the assumptions.
- **Medium Confidence:** The experimental results (ENL scores, inference times) are internally consistent and compared against relevant baselines. However, the confidence in the superiority of LYJ over simpler transforms is lower due to lack of direct ablation.
- **Low Confidence:** The paper's claims about the "principled application" of score-matching rely on the successful Gaussianization of SAR speckle, a step that is more complex than for standard additive Gaussian noise and lacks extensive corpus validation.

## Next Checks

1. **Transform Ablation:** Train and evaluate the model with only log transform (no LYJ) on the same Sentinel-1 dataset to quantify the exact benefit of the Gaussianization step.
2. **Noise Level Sensitivity:** Systematically vary the estimated $\sigma^2_{data}$ by ±20% and measure the impact on ENL scores and visual quality to understand the robustness of the self-supervised loss.
3. **Architecture Simplification:** Replace the implied complex U-Net with a simpler MLP or shallow CNN and measure the change in ENL and inference time to establish the minimum effective architecture.