---
ver: rpa2
title: Discrete Prompt Tuning via Recursive Utilization of Black-box Multimodal Large
  Language Model for Personalized Visual Emotion Recognition
arxiv_id: '2509.04480'
source_url: https://arxiv.org/abs/2509.04480
tags:
- prompt
- prompts
- proposed
- recognition
- emotion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of recognizing individual-specific
  emotional responses in visual content, which is difficult for general-purpose Multimodal
  Large Language Models (MLLMs) due to their training on broad datasets. To tackle
  this, the authors propose a discrete prompt tuning method that recursively refines
  natural language prompts to better adapt MLLM responses to each user's emotional
  profile.
---

# Discrete Prompt Tuning via Recursive Utilization of Black-box Multimodal Large Language Model for Personalized Visual Emotion Recognition

## Quick Facts
- arXiv ID: 2509.04480
- Source URL: https://arxiv.org/abs/2509.04480
- Reference count: 40
- Primary result: Recursive discrete prompt tuning improves personalized visual emotion recognition accuracy up to 44.9% on the Affection dataset.

## Executive Summary
This paper addresses the challenge of recognizing individual-specific emotional responses in visual content using Multimodal Large Language Models (MLLMs). General-purpose MLLMs trained on broad datasets struggle to capture personalized emotional responses that vary across individuals. The authors propose a discrete prompt tuning method that recursively refines natural language prompts to adapt MLLM responses to each user's emotional profile. The approach generates multiple prompts, evaluates their performance on user-specific training data, and iteratively improves them using feedback from a large language model, without modifying model parameters. Experiments on the Affection dataset demonstrate that this method outperforms conventional deep learning approaches and other MLLM-based baselines in personalized visual emotion recognition accuracy.

## Method Summary
The proposed method uses a two-stage process with GPT-4o as the prompt-generating LLM and LLaVA-v1.6-Mistral-7B as the target MLLM. It starts with N=6 initial prompts generated by the LLM, then evaluates each prompt on L training images from a specific user. The top-3 and worst-3 performing prompts are selected as feedback examples, and the LLM generates T=5 modified prompts. This process repeats for I1=20 modification steps, I2=2 restarts, and I3=3 initializations, creating hundreds of candidate prompts. The final emotion prediction uses majority voting across H=5 top-performing prompts. The method works entirely through API calls without access to model parameters, making it applicable to black-box MLLMs.

## Key Results
- Proposed method achieves 44.9% accuracy on personalized visual emotion recognition
- Outperforms conventional deep learning (38.7%) and MLLM baselines (40.6%)
- User-specific prompts (44.9%) significantly outperform cross-user prompts (40.4%)
- Majority voting improves stability from 43.1% to 44.9% compared to single prompts

## Why This Works (Mechanism)

### Mechanism 1
Iterative discrete prompt refinement with performance-based feedback improves personalized VER accuracy over fixed prompts. The LLM generates modified prompts by analyzing characteristics of high-performing (top-k) and low-performing (worst-k) prompts, enabling directed search in prompt space toward user-specific optimal prompts. Core assumption: the LLM's self-correction capability can meaningfully analyze why certain prompts perform better for specific users and generate improved variants. Evidence: [abstract] "recursively refines natural language prompts...iteratively improves them using feedback from the MLLM", [Section III-C] "The LLM refines prompt representations by increasing and decreasing their similarities to appropriate and inappropriate examples", [Table 3] Proposed method (44.9%) vs CM4 initial prompt (40.6%) vs CM5 single iteration (41.6%).

### Mechanism 2
LLM-generated cross-user prompts underperform user-specific prompts, validating personalization. Prompts optimized for one user encode their emotional response patterns; applying to different users yields lower accuracy due to individual variability in emotional elicitation. Core assumption: individual emotional responses differ systematically enough that user-specific prompt features don't transfer well. Evidence: [Section I] "emotional responses elicited by visual stimuli vary across individuals...roller coaster may invoke 'excitement' in some viewers but 'fear' in others", [Table 3] CM6 (cross-user prompt) achieves 40.4% vs proposed method's 44.9%, [Section IV-B] "prompts generated by the proposed method are personalized, rather than generic for VER users in general".

### Mechanism 3
Majority voting across multiple top-performing prompts stabilizes predictions and reduces outlier errors. Aggregating predictions from H=5 independently optimized prompts averages out prompt-specific biases and random prediction errors. Core assumption: optimized prompts, while individually high-performing, make partially uncorrelated errors that cancel through voting. Evidence: [abstract] "demonstrates the importance of prompt personalization and iterative refinement", [Section III-D] "emotion label obtained through the majority voting...becomes the final recognition result", [Table 3] CM7 (single prompt, no voting) 43.1% vs proposed method (majority voting) 44.9%.

## Foundational Learning

- **Discrete vs. Soft Prompts**
  - Why needed here: The method uses natural language prompts (discrete) rather than continuous vectors (soft), enabling black-box deployment and human interpretability.
  - Quick check question: Can you explain why soft prompts require gradient access while discrete prompts work with API-only models?

- **Black-box MLLM Constraints**
  - Why needed here: State-of-the-art models (GPT-4o, Gemini) restrict parameter/gradient access; method must work without internal model modification.
  - Quick check question: What information can and cannot be obtained from a black-box model API response?

- **Emotion Label Taxonomy (Mikel's Wheel)**
  - Why needed here: Understanding the 8 emotion categories and their distances enables proper evaluation (ECC, EMC metrics account for emotional similarity).
  - Quick check question: Why might confusing "excitement" with "amusement" be less severe than confusing "excitement" with "sadness"?

## Architecture Onboarding

- **Component map**: GPT-4o (prompt generation) -> LLaVA-v1.6-Mistral-7B (emotion classification) -> Performance evaluation -> Prompt refinement loop

- **Critical path**: 
  1. Generate N=6 initial prompts via GPT-4o
  2. Evaluate each prompt on L training images using LLaVA
  3. Select top-3 and worst-3 prompts as feedback
  4. GPT-4o generates T=5 modified prompts
  5. Repeat I1=20 modifications × I2=2 restarts × I3=3 initializations
  6. Select H=5 best prompts, run on test image, majority vote

- **Design tradeoffs**: 
  - High iteration counts (I1=20, I2=2, I3=3) increase accuracy but multiply API calls and latency
  - N=6 initial prompts balance exploration vs. cost; too few limits diversity
  - H=5 for voting: more prompts increase stability but with diminishing returns
  - Discrete prompts: interpretable and black-box compatible, but may underperform gradient-based soft prompts if available

- **Failure signatures**: 
  - Accuracy plateaus or declines across iterations → LLM not generating meaningful improvements
  - Specific emotions consistently misclassified → class imbalance or insufficient training coverage (anger in this paper)
  - Top prompts produce identical predictions → insufficient prompt diversity, voting ineffective
  - LLM generates malformed prompts → check t_init/t_mod template formatting

- **First 3 experiments**:
  1. **Sanity check**: Run I1=1, I2=1, I3=1 with N=3 initial prompts on single user's data; verify accuracy improves from initial to modified prompts (expect ~2-3% gain per Table 4 progression).
  2. **Ablation on iteration depth**: Compare I1=5 vs I1=10 vs I1=20 with I2=1, I3=1; plot training accuracy curve to identify plateau point before full computational investment.
  3. **Cross-user validation**: For 3 users, test (a) user-specific prompts vs (b) cross-user prompts; confirm personalization gap (~4% per Table 3: 40.4% vs 44.9%) reproduces on your data.

## Open Questions the Paper Calls Out

### Open Question 1
How can the discrete prompt tuning framework be modified to achieve robust recognition performance for under-represented emotion categories (e.g., anger) where class imbalance causes widely scattered predictions? Basis: The authors explicitly note in Section V that performance for anger "does not exhibit a noticeable improvement" and predictions are "extensively scattered across different categories" due to class imbalance. They state: "In future studies, it is necessary to design a model that can better leverage the knowledge of MLLMs to address class imbalance and achieve robust performance even with limited training samples."

### Open Question 2
What is the minimum number of user-specific image–emotion label pairs required for effective personalization, and how does performance degrade as training data decreases? Basis: The paper uses 30% of each user's images as training data (approximately 259 images per user on average), noting that "it is challenging to collect sufficient data specific to a target user's VER" but no experiments investigate the lower bound of training data requirements.

### Open Question 3
How does the method's computational cost and latency scale with the number of iterations (I1, I2, I3), and what is the cost–accuracy tradeoff for practical deployment? Basis: The experimental settings specify I1=20, I2=2, I3=3, generating hundreds of prompts across iterations, with each prompt requiring MLLM evaluation on all training images. This implies substantial API calls and computational overhead for black-box MLLMs like GPT-4o, but no analysis of efficiency or cost is provided.

### Open Question 4
Does the personalized prompt tuning approach generalize across different MLLM architectures (e.g., GPT-4o, Gemini, Claude) without re-optimization, or are prompts model-specific? Basis: The experiments use only LLaVA-v1.6-Mistral-7B as the target MLLM and GPT-4o as the prompt-generating LLM. The authors claim the method works for "both white-box and black-box MLLMs" but provide no empirical validation of cross-model transfer or compatibility.

## Limitations

- Heavy reliance on sufficient training data per user (864 images on average) limits real-world applicability
- Computationally expensive recursive tuning process requiring thousands of API calls
- Performance degradation for minority emotion classes (anger) due to class imbalance
- No systematic evaluation of minimum viable training data or cost-benefit tradeoffs

## Confidence

**High confidence** (supported by direct experimental evidence):
- Iterative discrete prompt tuning improves accuracy over initial prompts
- User-specific prompts outperform cross-user prompts
- Majority voting improves stability over single prompts

**Medium confidence** (supported by single experiment or theoretical argument):
- LLM can meaningfully analyze why prompts perform differently and generate improvements
- Recursive refinement will continue to improve beyond tested iteration limits
- Approach generalizes to other personalization tasks beyond visual emotion recognition

**Low confidence** (extrapolated or minimally supported):
- Method maintains effectiveness with significantly fewer training images per user
- Computational cost is justified by performance gains in practical applications
- Approach works equivalently across different MLLM architectures or emotional taxonomies

## Next Checks

1. **Ablation study on training data quantity**: Systematically vary the number of training images per user (e.g., 100, 500, 1000, 2000) and measure accuracy degradation. This will quantify the minimum viable data requirement and help understand the method's practical limitations.

2. **Cross-dataset generalization test**: Apply the same personalized tuning approach to a different visual emotion dataset (e.g., Flickr-Faces-HQ with emotion annotations) using the same 8 emotion categories. This will validate whether the prompt optimization strategy transfers across different visual domains and data distributions.

3. **Cost-benefit analysis with iteration limit**: Compare accuracy versus API call count by varying I1 (modification iterations) from 5 to 30 while keeping other parameters fixed. Plot the accuracy improvement curve to identify the point of diminishing returns and quantify the practical cost-effectiveness threshold.