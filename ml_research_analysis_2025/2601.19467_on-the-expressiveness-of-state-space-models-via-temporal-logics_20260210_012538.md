---
ver: rpa2
title: On the Expressiveness of State Space Models via Temporal Logics
arxiv_id: '2601.19467'
source_url: https://arxiv.org/abs/2601.19467
tags:
- diagonal
- languages
- which
- time-invariant
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes a formal hierarchy of expressive power\
  \ for State Space Models (SSMs) by mapping different SSM variants to fragments of\
  \ linear temporal logic over finite traces (LTLf). The authors prove that diagonal\
  \ SSMs with fixed precision capture exactly the star-free languages (PLTLf), while\
  \ diagonal SSMs with logarithmic precision can recognize counting languages (PLTLf[\u2190\
  \u2212]) including non-regular languages."
---

# On the Expressiveness of State Space Models via Temporal Logics

## Quick Facts
- **arXiv ID:** 2601.19467
- **Source URL:** https://arxiv.org/abs/2601.19467
- **Reference count:** 39
- **Primary result:** SSM variants map to LTLf fragments; diagonal fixed-precision = star-free, diagonal logarithmic = counting languages, time-invariant = AC⁰ regular languages

## Executive Summary
This paper establishes a formal hierarchy of expressive power for State Space Models (SSMs) by mapping different SSM variants to fragments of linear temporal logic over finite traces (LTLf). The authors prove that diagonal SSMs with fixed precision capture exactly the star-free languages (PLTLf), while diagonal SSMs with logarithmic precision can recognize counting languages (PLTLf[←−#]) including non-regular languages. Time-invariant SSMs are shown to compute modular predicates, recognizing languages in UN-PLTLf[MOD] (regular languages in AC0), and mixed SSMs combine these capabilities. The paper provides systematic translations between logical formulas and SSM architectures, demonstrates tight non-expressibility bounds (e.g., diagonal fixed-precision SSMs cannot recognize (aa)*), and situates SSMs within the broader transformer expressiveness landscape, showing structural alignment with unique hard-attention transformers.

## Method Summary
The paper establishes expressiveness bounds by constructing systematic translations from temporal logic formulas to SSM architectures and vice versa. The core method involves analyzing recurrence dynamics h_t = gate(x_t)·h_{t-1} + inc(x_t) with output z_t = ϕ(h_t, x_t), where different SSM variants (diagonal vs time-invariant, fixed vs logarithmic precision) are evaluated against PLTLf fragments with temporal operators (Y, P, S) and extensions (counting ←−#, modular predicates MOD^r_m). The authors prove non-expressibility results through monotonicity arguments and spectral properties, while constructive proofs show which languages each SSM variant can recognize. The approach leverages the finite-trace setting to enable precise characterizations that distinguish between regular and non-regular language recognition capabilities.

## Key Results
- Diagonal fixed-precision SSMs recognize exactly star-free languages (PLTLf), cannot recognize (aa)* or any counting languages
- Diagonal logarithmic-precision SSMs recognize counting languages (PLTLf[←−#]), including non-regular languages like {a^n | n ≡ 0 mod k}
- Time-invariant SSMs compute modular predicates, recognizing UN-PLTLf[MOD] languages (regular languages in AC⁰)
- Mixed SSMs combine capabilities, recognizing union of star-free and modular predicate languages
- SSMs align structurally with unique hard-attention transformers, providing theoretical justification for expressiveness differences

## Why This Works (Mechanism)
The paper works by exploiting the precise mathematical structure of SSM recurrence dynamics and their relationship to temporal logic operators. The mechanism relies on three key insights: (1) the gate-inc structure enables controlled state evolution that can encode temporal dependencies through diagonal matrices with specific spectral properties, (2) precision levels (fixed vs logarithmic) directly determine the ability to distinguish between symbol counts, enabling counting language recognition at the cost of exponential precision requirements, and (3) the connection to temporal logic provides a formal framework where non-expressibility can be proven through monotonicity arguments and expressibility through constructive formula-to-SSM translations. The finite-trace setting is crucial as it allows complete characterization of what can be computed in bounded time, distinguishing SSMs from models that require infinite traces.

## Foundational Learning
- **Temporal logic over finite traces (LTLf)**: Why needed - provides formal language framework for characterizing SSM expressiveness; Quick check - verify formulas can be evaluated on finite sequences with well-defined semantics
- **State Space Model recurrence dynamics**: Why needed - core computational mechanism that determines what patterns can be recognized; Quick check - trace state evolution h_t for simple sequences to verify expected behavior
- **Counting languages and non-regularity**: Why needed - establishes boundary between what SSMs can and cannot compute; Quick check - test if proposed SSM can distinguish between different numbers of symbol repetitions
- **Modular arithmetic in automata**: Why needed - characterizes the computational power of time-invariant SSMs; Quick check - verify cyclic permutation matrices correctly implement modular counting
- **Monotonicity arguments for non-expressibility**: Why needed - provides rigorous method to prove SSM limitations; Quick check - demonstrate that increasing symbol repetitions preserves or changes recognition behavior
- **Diagonal vs time-invariant SSMs**: Why needed - architectural distinction that determines expressive hierarchy; Quick check - compare recognition capabilities on star-free vs counting languages

## Architecture Onboarding
- **Component map:** Input sequence → SSM layers (diagonal/time-invariant) → FNN output → language recognition; ϕ(h_t, x_t) combines state and input
- **Critical path:** Recurrence h_t = gate(x_t)·h_{t-1} + inc(x_t) → state evolution → output function ϕ → final classification
- **Design tradeoffs:** Fixed precision (efficient but limited to star-free) vs logarithmic precision (expressive but requires exponential precision) vs time-invariant (modular arithmetic only)
- **Failure signatures:** Diagonal fixed-precision fails on counting languages (aa)*; logarithmic precision requires exponential precision for large counting moduli; time-invariant limited to AC⁰ regular languages
- **First experiments:** (1) Implement modular predicate layer (Lemma 6) with cyclic permutation matrix; (2) Test yesterday operator encoding (Lemma 11) on binary sequences; (3) Verify (aa)* non-recognition for diagonal fixed-precision SSMs

## Open Questions the Paper Calls Out
None

## Limitations
- Non-expressibility results rely on infinite-precision assumptions that may not hold in practical implementations
- Logarithmic precision SSMs require exponential precision for counting languages, creating a practical-expressiveness gap
- Time-invariant SSMs are severely limited to AC⁰ regular languages, restricting their practical utility
- Numerical stability of recurrence dynamics may affect the theoretical bounds in finite-precision implementations
- The abstract constructions don't provide concrete architectural guidelines for practical SSM design

## Confidence
- **High confidence:** The logical framework mapping SSMs to LTLf fragments is mathematically rigorous and the constructions for representing basic temporal operators (yesterday, since) are explicit and verifiable
- **Medium confidence:** The non-expressibility bounds for fixed-precision SSMs rely on theoretical arguments about monotonicity and recurrence that hold in the infinite-precision setting but may require careful numerical analysis for practical implementations
- **Medium confidence:** The logarithmic precision SSM results for counting languages are theoretically sound but the exponential precision requirements create a practical-expressibility gap that limits direct applicability

## Next Checks
1. **Numerical stability verification:** Implement the yesterday operator encoding from Lemma 11 and test on sequences of varying lengths (10, 100, 1000 symbols) to verify that the state dynamics remain within the theoretical bounds and the output correctly identifies previous symbols.

2. **Monotonicity testing:** For diagonal fixed-precision SSMs, systematically test recognition of (aa)* and (ab)* on sequences where symbol repetitions are increased incrementally to empirically verify the monotonicity-based non-expressibility result from Theorem 3.

3. **Precision scaling experiment:** Implement the logarithmic precision SSM for counting languages and measure the actual precision requirements needed to correctly recognize languages like L_k = {a^i | i ≡ 0 (mod k)} for increasing values of k (2, 4, 8, 16) to quantify the practical-expressiveness gap.