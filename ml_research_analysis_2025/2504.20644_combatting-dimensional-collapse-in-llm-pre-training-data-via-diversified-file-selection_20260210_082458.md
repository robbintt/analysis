---
ver: rpa2
title: Combatting Dimensional Collapse in LLM Pre-Training Data via Diversified File
  Selection
arxiv_id: '2504.20644'
source_url: https://arxiv.org/abs/2504.20644
tags:
- selection
- training
- performance
- data
- disf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of dimensional collapse in large
  language model (LLM) pre-training data selection, where methods focusing on specific
  domains improve task performance in those areas but cause overall degradation across
  diverse tasks. The authors propose a Diversified File Selection (DiSF) method that
  selects the most decorrelated text files by minimizing the Frobenius norm of the
  feature covariance matrix, achieving more uniform eigenvalues and enhanced diversity.
---

# Combatting Dimensional Collapse in LLM Pre-Training Data via Diversified File Selection

## Quick Facts
- arXiv ID: 2504.20644
- Source URL: https://arxiv.org/abs/2504.20644
- Reference count: 40
- Primary result: 1.5x training efficiency and 5x data efficiency with 1.5% file selection on TinyLlama models

## Executive Summary
This paper addresses dimensional collapse in LLM pre-training data selection, where domain-specific selection methods improve performance in targeted areas but degrade overall task performance. The authors propose Diversified File Selection (DiSF), a method that selects the most decorrelated text files by minimizing the Frobenius norm of feature covariance matrices. Using a greedy algorithm with theoretical approximation guarantees under γ-weakly submodular optimization, DiSF achieves more uniform eigenvalues and enhanced diversity in training data. The method is evaluated on TinyLlama architectures (120M to 1.1B parameters) across nine tasks from the Harness framework.

## Method Summary
The Diversified File Selection method operates by selecting text files that minimize the Frobenius norm of the feature covariance matrix, which corresponds to maximizing diversity and decorrelation among selected files. The approach uses a classical greedy algorithm that iteratively selects files based on their contribution to reducing covariance, with theoretical approximation guarantees under γ-weakly submodular optimization. The method is designed to combat dimensional collapse that occurs when pre-training data selection focuses too narrowly on specific domains, leading to degraded performance across diverse tasks.

## Key Results
- DiSF selects only 1.5% of 590M training files in SlimPajama corpus while maintaining strong performance
- Achieves approximately 1.5x training efficiency compared to full-data pre-training
- Demonstrates 5x data efficiency under a 50B token budget compared to baseline methods
- Outperforms existing baselines across nine diverse tasks from the Harness framework

## Why This Works (Mechanism)
The method works by explicitly addressing dimensional collapse through diversified selection. When data selection focuses on specific domains, the model's representation space becomes skewed toward those domains, causing poor generalization to others. By minimizing the Frobenius norm of feature covariance matrices, DiSF ensures that selected files span diverse directions in the feature space, maintaining uniform eigenvalues that correspond to balanced representation across all dimensions. This prevents the model from overfitting to narrow data distributions while preserving the essential diversity needed for robust generalization across multiple tasks.

## Foundational Learning
**Frobenius Norm Minimization**: Measures the total covariance across all feature pairs, ensuring balanced representation. *Why needed*: Provides a mathematical framework for quantifying and maximizing diversity in data selection. *Quick check*: Verify that selected files show uniform eigenvalue distribution in covariance matrix.

**γ-Weakly Submodular Optimization**: Provides theoretical guarantees for greedy approximation algorithms. *Why needed*: Ensures that the greedy selection process provides near-optimal solutions with provable bounds. *Quick check*: Confirm approximation ratio holds for your specific data distribution.

**Feature Covariance Analysis**: Captures relationships between different aspects of text data. *Why needed*: Enables quantitative measurement of diversity and redundancy in selected files. *Quick check*: Compute correlation matrices before and after selection to verify decorrelation.

**Greedy Algorithm Selection**: Iteratively builds optimal file subsets through sequential selection. *Why needed*: Provides computationally efficient solution to NP-hard optimization problem. *Quick check*: Compare greedy solution quality against exhaustive search on small subsets.

## Architecture Onboarding
**Component Map**: Data Files -> Feature Extraction -> Covariance Matrix Computation -> Greedy Selection -> Selected Subset
**Critical Path**: Feature extraction and covariance computation represent the computational bottleneck, requiring O(n²) operations for n features across m files.
**Design Tradeoffs**: The method trades computational complexity during selection for improved training efficiency and data efficiency during model training. Higher feature dimensionality improves selection quality but increases computational cost quadratically.
**Failure Signatures**: Dimensional collapse may persist if feature extraction fails to capture domain-specific characteristics, or if the greedy algorithm converges prematurely due to poor initialization or inadequate iteration count.
**First Experiments**: 1) Test covariance matrix computation on small file subsets to verify feature extraction quality, 2) Run greedy selection on reduced dataset to validate approximation guarantees, 3) Compare eigenvalue distributions before and after selection to confirm diversity improvement.

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Evaluation limited to TinyLlama architectures (120M-1.1B parameters), may not scale to larger models
- Assumes file-level decorrelation translates to effective model generalization, which may not hold uniformly
- Computational resources required for feature covariance analysis may be substantial for large-scale datasets
- Long-term stability and performance across extended inference tasks remains unexplored

## Confidence
**High Confidence**: Theoretical framework for addressing dimensional collapse is sound with established greedy algorithm guarantees
**Medium Confidence**: Empirical efficiency gains (1.5x training, 5x data) are compelling but may not scale linearly to larger models
**Low Confidence**: Assumption that 1.5% file selection maintains comprehensive knowledge space coverage is not fully validated

## Next Checks
1. Evaluate DiSF performance on larger LLM architectures (7B+ parameters) to verify scalability of efficiency gains
2. Conduct ablation studies varying selection percentage thresholds to determine optimal trade-offs
3. Test method on datasets with different domain characteristics to assess robustness across distributions