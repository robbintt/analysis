---
ver: rpa2
title: Practical Machine Learning for Aphasic Discourse Analysis
arxiv_id: '2511.17553'
source_url: https://arxiv.org/abs/2511.17553
tags:
- discourse
- aphasia
- word
- context
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addressed the labor-intensive manual coding of Correct
  Information Units (CIUs) in aphasia discourse analysis by training five supervised
  ML models to automate CIU identification. The core method involved using token-level
  features from human-coded transcripts to train SVM, k-NN, Random Forest, and other
  models for classifying words and CIUs.
---

# Practical Machine Learning for Aphasic Discourse Analysis

## Quick Facts
- arXiv ID: 2511.17553
- Source URL: https://arxiv.org/abs/2511.17553
- Reference count: 0
- Primary result: k-NN and SVM-rbf classifiers achieved CIU F1 scores of 0.889 and 0.886 on aphasia discourse transcripts

## Executive Summary
This study addresses the labor-intensive manual coding of Correct Information Units (CIUs) in aphasia discourse analysis by training supervised ML models to automate CIU identification. Using token-level features from human-coded transcripts, the authors trained five supervised models (SVM, k-NN, Random Forest, Decision Tree, and an ensemble) to classify words and CIUs. Results showed near-perfect accuracy for distinguishing words from non-words across all models, but lower accuracy and F1 scores for CIU classification. Ablation tests revealed token-level and linguistic features were critical for CIU detection, while broader context had minimal impact. The findings demonstrate feasibility for automating lexical segmentation but highlight the need for richer linguistic representations to accurately identify discourse informativeness.

## Method Summary
The study used 130 Cat Rescue picture description transcripts from AphasiaBank, preprocessing CHAT format files to isolate participant utterances and remove fillers. Tokenization was performed with spaCy, followed by manual labeling for WORD/NOT-WORD and CIU/NOT-CIU classifications. Features included character n-grams, local context windows (±1-2 tokens), and handcrafted linguistic markers. Five classifiers (SVM with linear and RBF kernels, k-NN, Random Forest, Decision Tree) were trained and evaluated using accuracy, precision, recall, F1, and AUC metrics. Ablation studies systematically removed feature groups to assess their contribution to CIU detection performance.

## Key Results
- WORD vs NON-WORD classification achieved near-perfect accuracy (0.995) across all five models
- k-NN achieved best CIU classification performance with F1 score of 0.889 and accuracy of 0.824
- Token-level and linguistic features were critical for CIU detection, while broader context windows showed minimal impact
- Ablation tests confirmed that removing token-level features substantially degraded CIU F1 scores

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Lexical discrimination (word vs. non-word) operates as a linearly separable classification requiring minimal contextual features.
- **Mechanism:** Character n-grams and lightweight linguistic markers create distinct feature clusters that allow all tested architectures to draw clean decision boundaries between valid words and non-lexical artifacts.
- **Core assumption:** The surface lexical properties captured by character-level features are sufficient for word identification in aphasic transcripts.
- **Evidence anchors:**
  - [abstract]: "baseline model training produced a high accuracy across transcripts for word vs non-word, with all models achieving near perfect performance (0.995)"
  - [section]: "distinguishing lexical tokens from non-lexical artifacts is a linearly separable, low-complexity problem requiring minimal contextual or engineered features"
  - [corpus]: Neighbor paper on AS-ASR (arxiv 2506.06566) addresses related lexical recognition challenges for aphasic speech on edge devices

### Mechanism 2
- **Claim:** CIU detection depends primarily on token-level and handcrafted linguistic features, not broader discourse context.
- **Mechanism:** Informativeness judgments rely on fine-grained lexical and morphological cues that are locally encoded; expanding context windows beyond ±1-2 tokens provides negligible additional signal.
- **Core assumption:** Communicative informativeness in picture-description tasks is captured through localized lexical precision rather than distributed discourse structure.
- **Evidence anchors:**
  - [abstract]: "Ablation tests revealed token-level and linguistic features were critical for CIU detection, while broader context had minimal impact"
  - [section]: "context-related ablations (-context_char, -context_window) yielded smaller changes, suggesting that fine-grained lexical representation contributes more to CIU detection than additional context"
  - [corpus]: Weak direct corpus support for this specific mechanism; neighbor papers focus on LLM-based approaches rather than feature ablation analysis

### Mechanism 3
- **Claim:** Non-linear decision boundaries provide measurable advantage for context-sensitive CIU classification over linear or single-tree models.
- **Mechanism:** k-NN captures local similarity neighborhoods while SVM-rbf kernels allow curved boundaries, enabling both to better model the nuanced, multi-level nature of informativeness compared to rigid linear separators.
- **Core assumption:** The CIU construct involves non-linearly separable feature interactions that single decision boundaries cannot capture.
- **Evidence anchors:**
  - [abstract]: "lower accuracy (0.824 for k-NN) and F1 scores (0.889 for k-NN) for CIU classification"
  - [section]: "k-NN showed a modest improvement with a two-token window (+Ctx 2), indicating that limited local context can enhance similarity-based methods"
  - [corpus]: The Text Aphasia Battery paper (arxiv 2511.20507) suggests LLMs may offer alternative pathways for capturing context-sensitive deficits

## Foundational Learning

- **Concept:** Supervised binary classification with handcrafted features
  - **Why needed here:** All five models operate on the same token-level feature set to classify WORD/CIU labels derived from human coders.
  - **Quick check question:** Can you explain why the same feature inputs produce different CIU F1 scores across model architectures?

- **Concept:** Ablation study methodology
  - **Why needed here:** The paper systematically removes feature groups to isolate which inputs drive CIU performance.
  - **Quick check question:** What would you conclude if removing a feature group improved rather than degraded F1?

- **Concept:** Precision-recall tradeoffs in imbalanced classification
  - **Why needed here:** CIU vs. non-CIU classification shows modest precision-recall gaps, and class imbalance from labeling subjectivity could affect AUC interpretation.
  - **Quick check question:** Why might accuracy be misleading if non-CIU tokens vastly outnumber CIU tokens in the training data?

## Architecture Onboarding

- **Component map:** Transcript preprocessing (CHAT parsing → participant isolation → filler removal) → Tokenization (spaCy) → Feature extraction (character n-grams, linguistic markers, context windows) → Classification (5 model variants) → Evaluation (Accuracy, F1, AUC, ablation tests)
- **Critical path:** Token-level character features and handcrafted linguistic markers → k-NN or SVM-rbf classifier → CIU prediction with confidence scoring
- **Design tradeoffs:** Wider context windows (±2 tokens) provide marginal gains for k-NN but negligible benefits for other models; simpler models (DT) offer interpretability at cost of ~10% F1 loss; ceiling on WORD classification suggests feature investment should focus on CIU-specific linguistic representations.
- **Failure signatures:**
  - CIU F1 dropping below 0.80 typically indicates token-level features were removed
  - High WORD accuracy but low CIU accuracy indicates missing linguistic/semantic features rather than preprocessing issues
  - Large gaps between AUC and F1 suggest threshold calibration needed
- **First 3 experiments:**
  1. Retrain k-NN with expanded context window (±2 tokens) on held-out Cat Rescue transcripts not in original split to test generalization
  2. Enforce CIU=0 when WORD=0 as hard constraint and measure precision improvement without retraining
  3. Add calibrated confidence scores and route low-confidence predictions (0.4-0.6 range) to human review; measure clinician verification time reduction versus manual coding baseline

## Open Questions the Paper Calls Out

- Can the trained k-NN and SVM-rbf models generalize to discourse tasks beyond picture description, such as narrative or conversational speech?
- Can large language models (LLMs) outperform traditional classifiers on CIU identification when provided with task instructions or training examples?
- What discourse-level or multimodal features (e.g., topic structure, prosody, visual grounding) would meaningfully improve CIU detection accuracy?
- How does modeling annotator disagreement through soft labels affect CIU classifier performance and reliability?

## Limitations

- Feature engineering dependency creates fundamental barrier to exact reproduction and generalization
- Data access and labeling constraints require specialized training and may limit independent validation
- Generalizability concerns arise from single-task evaluation and assumptions about context irrelevance

## Confidence

- **High confidence**: WORD vs NON-WORD classification results (0.995 accuracy) are mechanistically sound and reproducible
- **Medium confidence**: CIU classification F1 scores (0.824-0.889) and ablation findings are credible but implementation-dependent
- **Low confidence**: Claims about context irrelevance and sufficiency of token-level features require more extensive validation across multiple discourse tasks

## Next Checks

1. Request complete feature engineering documentation from authors and replicate ablation study with identical features to verify context-insensitivity finding
2. Apply trained models to CIU classification on a different picture description task (e.g., Cookie Theft) without retraining to assess generalizability
3. Calculate Cohen's kappa between model predictions and blinded human raters on held-out transcripts to determine if ML system achieves human-level consistency for CIU judgments