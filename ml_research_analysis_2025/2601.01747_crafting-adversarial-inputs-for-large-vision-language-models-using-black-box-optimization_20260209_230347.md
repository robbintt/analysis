---
ver: rpa2
title: Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box
  Optimization
arxiv_id: '2601.01747'
source_url: https://arxiv.org/abs/2601.01747
tags:
- minigpt-4
- adversarial
- attack
- llav
- instructblip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ZO-SPSA, a black-box adversarial attack method
  for jailbreaking large vision-language models (LVLMs). Unlike white-box approaches,
  ZO-SPSA uses zeroth-order optimization via simultaneous perturbation stochastic
  approximation to estimate gradients without requiring model access, enabling attacks
  purely through input-output interactions.
---

# Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization

## Quick Facts
- arXiv ID: 2601.01747
- Source URL: https://arxiv.org/abs/2601.01747
- Reference count: 10
- Key outcome: Introduces ZO-SPSA, a black-box adversarial attack method for large vision-language models achieving 83.0% success rate on InstructBLIP and up to 50% memory efficiency gains over white-box methods.

## Executive Summary
This paper presents ZO-SPSA, a novel black-box adversarial attack method designed to jailbreak large vision-language models (LVLMs) without requiring access to model internals. By leveraging zeroth-order optimization via simultaneous perturbation stochastic approximation, ZO-SPSA estimates gradients solely through input-output interactions, making it applicable in real-world settings where model access is restricted. The method is evaluated on three prominent LVLMs—InstructBLIP, LLaVA, and MiniGPT-4—demonstrating high attack success rates and notable memory efficiency improvements. The results highlight significant vulnerabilities in current LVLMs' safety mechanisms and underscore the need for robust defenses against black-box adversarial attacks.

## Method Summary
The paper introduces ZO-SPSA, a black-box adversarial attack framework for large vision-language models. Unlike traditional white-box methods that require full model access, ZO-SPSA employs zeroth-order optimization using simultaneous perturbation stochastic approximation (SPSA) to estimate gradients. This allows the attacker to craft adversarial inputs by observing only the model's outputs, without needing gradients or internal states. The approach is particularly suited for scenarios where the target model is a black box, such as commercial or proprietary LVLMs. The method is evaluated on three LVLMs—InstructBLIP, LLaVA, and MiniGPT-4—using image-text inputs, with success measured by the model's failure to refuse harmful prompts.

## Key Results
- Achieved 83.0% attack success rate on InstructBLIP, demonstrating high effectiveness in black-box jailbreak scenarios.
- Demonstrated strong adversarial transferability, with a 64.18% success rate on MiniGPT-4, highlighting cross-model vulnerabilities.
- Reduced GPU memory consumption by up to 50% compared to white-box attack methods, offering a memory-efficient alternative for adversarial testing.

## Why This Works (Mechanism)
The paper leverages zeroth-order optimization via simultaneous perturbation stochastic approximation (SPSA) to estimate gradients without requiring access to the model's internal parameters or gradients. This approach allows the attacker to craft adversarial inputs by iteratively perturbing the input and observing the output, making it effective in black-box settings where model internals are inaccessible.

## Foundational Learning
- **Zeroth-Order Optimization (ZOO)**: A derivative-free optimization method used when gradients are unavailable; needed because the target LVLM is a black box.
- **Simultaneous Perturbation Stochastic Approximation (SPSA)**: A technique for gradient estimation using random perturbations; enables efficient gradient approximation in high-dimensional spaces.
- **Adversarial Transferability**: The phenomenon where adversarial examples crafted for one model also fool others; highlights shared vulnerabilities across LVLMs.
- **Black-Box Attack**: An attack strategy that does not require access to model internals, only input-output pairs; critical for real-world applicability.
- **Jailbreak Attack**: A method to bypass safety filters in language models, prompting them to generate harmful or restricted content; central to the paper's threat model.

## Architecture Onboarding
- **Component Map**: Input Image/Text -> ZO-SPSA Optimizer -> Perturbed Input -> LVLM -> Output (Safety Violation or Not)
- **Critical Path**: Input perturbation via ZO-SPSA → LVLM inference → Safety violation check → Success/Failure
- **Design Tradeoffs**: Black-box approach sacrifices some attack precision for broader applicability and reduced resource usage.
- **Failure Signatures**: High variance in transferability rates across models; inconsistent safety filter robustness.
- **First Experiments**: (1) Validate attack success on InstructBLIP under varying perturbation budgets. (2) Test transferability to MiniGPT-4 with different input modalities. (3) Benchmark memory usage against a white-box baseline.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to three LVLMs, restricting generalizability across the broader landscape of vision-language models.
- Transferability results show substantial variability (64.18% vs. 30.8% on MiniGPT-4), suggesting performance may be highly model-dependent.
- Memory efficiency claims are relative to unspecified white-box baselines, making absolute resource comparisons unclear.

## Confidence
- **High**: The 83.0% attack success rate on InstructBLIP is well-supported by the methodology and results presented.
- **Medium**: Transferability claims are credible but show notable variability across target models, warranting cautious interpretation.
- **Medium**: Memory efficiency improvements are plausible but lack detailed benchmarking against specific white-box methods for full validation.

## Next Checks
1. Replicate the attack across a broader set of LVLMs to assess generalizability and model-specific vulnerabilities.
2. Conduct controlled experiments comparing memory usage with specific white-box baselines to validate the 50% reduction claim.
3. Test the robustness of defenses (e.g., input sanitization or hidden state monitoring) against ZO-SPSA attacks to evaluate practical mitigation strategies.