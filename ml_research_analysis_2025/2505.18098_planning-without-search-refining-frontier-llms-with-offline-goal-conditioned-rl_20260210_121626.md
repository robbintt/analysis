---
ver: rpa2
title: 'Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned
  RL'
arxiv_id: '2505.18098'
source_url: https://arxiv.org/abs/2505.18098
tags:
- agent
- language
- which
- value
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Planning with a Natural Language Critic (PNLC),
  a method that enables large language models (LLMs) to plan and reason in multi-turn
  interactive tasks without requiring direct fine-tuning of the LLM or inference-time
  search. PNLC trains a lightweight, goal-conditioned value function offline to predict
  the likelihood of achieving various future outcomes given a proposed action.
---

# Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL

## Quick Facts
- arXiv ID: 2505.18098
- Source URL: https://arxiv.org/abs/2505.18098
- Reference count: 40
- One-line primary result: PNLC achieves state-of-the-art performance on interactive benchmarks while requiring 8-10x less inference time than search-based methods.

## Executive Summary
This paper introduces Planning with a Natural Language Critic (PNLC), a method enabling large language models to plan and reason in multi-turn interactive tasks without direct fine-tuning or inference-time search. PNLC trains a lightweight, goal-conditioned value function offline to predict outcome likelihoods, then uses this as a natural language critic during inference to help the LLM refine its strategies. Experiments across web shopping, social deduction games, and persuasion tasks show PNLC consistently outperforms state-of-the-art methods including RL fine-tuning, prompting-based reasoning, and search-based approaches, while requiring significantly less inference time.

## Method Summary
PNLC combines offline goal-conditioned reinforcement learning with natural language feedback to enable planning without search. The method trains a lightweight MLP value function on trajectory data to predict the likelihood of achieving various future outcomes given a proposed action. During inference, this value function generates natural language descriptions of positive and negative possible futures, which the LLM uses to refine its high-level strategies. The approach operates on abstract "thoughts" rather than low-level actions, reducing decision complexity while preserving planning-relevant information. PNLC requires no direct LLM fine-tuning and achieves planning capability through offline training of the value function.

## Key Results
- On the persuasion task, PNLC achieved an average donation of $0.87 compared to $0.47-$0.78 for best baselines
- In web shopping, PNLC achieved a score of 78.2 versus 53.9-77.1 for competitors
- PNLC required 5-6 seconds of inference time versus 46-62 seconds for search methods

## Why This Works (Mechanism)

### Mechanism 1: Goal-Conditioned Value Function as Outcome Predictor
Training a lightweight goal-conditioned Q-function on offline trajectory data enables accurate prediction of outcome likelihoods without requiring direct LLM fine-tuning. The Q-function Q(s, a^tht, g) outputs the probability of reaching goal g given state s and proposed thought a^tht, trained using a modified IQL loss with goal-conditioning.

### Mechanism 2: Natural Language Value as Rich Feedback Signal
Converting scalar Q-values into natural language descriptions of positive/negative futures enables LLMs to reason about tradeoffs more effectively than scalar rewards. For n=4 goals (2 positive, 2 negative), the critic outputs paired goals and likelihoods, providing interpretable outcome descriptions the LLM can weigh.

### Mechanism 3: Thought-Level Abstraction Enables Efficient Planning
Operating on high-level thoughts rather than low-level utterances reduces decision complexity while preserving planning-relevant information. Agent actions are composite: thought a^tht followed by environment action a^env, with the value function evaluating only thoughts.

## Foundational Learning

- **Markov Decision Processes and Q-Learning**
  - Why needed here: PNLC frames interactive tasks as MDPs and uses IQL for offline training
  - Quick check question: Can you explain why IQL uses expectile loss L_τ₂ rather than standard MSE for value training?

- **Goal-Conditioned Reinforcement Learning**
  - Why needed here: The core innovation is learning Q(s, a, g)—values conditioned on reaching specific goal states
  - Quick check question: How does goal-conditioning change the training objective compared to standard Q-learning?

- **Chain-of-Thought Prompting and ReAct Agents**
  - Why needed here: PNLC builds on the ReAct paradigm where agents generate thoughts before actions
  - Quick check question: In ReAct, what is the relationship between the "thought" component and the environment action?

## Architecture Onboarding

- **Component map:**
  Offline phase: Trajectory dataset → Summarization prompt (cheap LLM) → Embedding model (GPT-3) → MLP value function (2 hidden layers, ~128 units)
  Inference phase: State → LLM proposes thought → Goal proposal prompt → Value function scores goals → Natural language value → Refinement prompt → Final thought → Action generation

- **Critical path:**
  1. Data collection: Generate 2-12k trajectories using naive prompting
  2. Preprocessing: Summarize trajectories and extract (s, a^tht, s', g) tuples
  3. Training: IQL training for ~100 iterations, batch size 32, learning rate 4e-4 to 1e-4
  4. Inference: n=4 goals, m=2 refinement iterations

- **Design tradeoffs:**
  - Summarization quality vs. cost: Cheaper LLMs reduce cost but may lose relevant state information
  - Goal count (n): More goals provide richer signal but increase latency; paper finds n=4 sufficient
  - Refinement iterations (m): More iterations help but with diminishing returns; m=2 used in experiments

- **Failure signatures:**
  - Removing goal-conditioning drops performance to baseline prompting levels
  - Removing refinement also collapses performance
  - "ReAct+Replan" ablation (LLM scores goals without training) achieves only 59.1 vs 78.2

- **First 3 experiments:**
  1. Reproduce WebShop ablation: Compare full PNLC against PNLC-without-goals on held-out instructions
  2. Test data quality sensitivity: Train PNLC on progressively smaller trajectory subsets
  3. Cross-task transfer check: Train value function on one task and evaluate on another

## Open Questions the Paper Calls Out
1. Can PNLC be adapted to support generalist reasoning across diverse tasks without training task-specific value functions?
2. What are the specific quality and diversity requirements of the offline data necessary for PNLC to function effectively?
3. Does the reliance on LLM "intuition" limit PNLC's applicability to complex domains requiring specialized, expert-level knowledge?

## Limitations
- The paper does not fully specify summarization prompt templates, goal proposal formats, or refinement instructions
- The embedding model variant significantly affects MLP input size and performance
- The reward computation method for partial trajectories remains underspecified

## Confidence
- **High confidence** in the core contribution: combining goal-conditioned value learning with natural language feedback provides a novel approach
- **Medium confidence** in the claimed mechanism: ablation studies support importance but lack detailed analysis of why natural language values outperform scalar values
- **Medium confidence** in generalization: strong performance across diverse task domains but small offline datasets and task-specific prompt engineering raise scalability questions

## Next Checks
1. Implement a controlled ablation comparing natural language value against scalar Q-values on a held-out test set
2. Test the method's sensitivity to summarization quality by training on two versions of the same data with different embedding qualities
3. Conduct cross-task transfer experiments where the value function is trained on one task and evaluated on another