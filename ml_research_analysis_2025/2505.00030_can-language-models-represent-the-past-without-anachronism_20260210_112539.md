---
ver: rpa2
title: Can Language Models Represent the Past without Anachronism?
arxiv_id: '2505.00030'
source_url: https://arxiv.org/abs/2505.00030
tags:
- language
- human
- period
- historical
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Researchers attempted to use language models to simulate early
  twentieth-century perspectives by prompting or fine-tuning contemporary models with
  period text. In-context prompting failed to produce convincing period style, as
  evidenced by a RoBERTa date-prediction model estimating continuations to be from
  the 21st century.
---

# Can Language Models Represent the Past without Anachronism?

## Quick Facts
- arXiv ID: 2505.00030
- Source URL: https://arxiv.org/abs/2505.00030
- Authors: Ted Underwood; Laura K. Nelson; Matthew Wilkens
- Reference count: 32
- Primary result: Simple prompting strategies fail to produce convincing early twentieth-century prose, while fine-tuning improves stylistic authenticity but remains imperfect

## Executive Summary
This study investigates whether contemporary language models can authentically represent early twentieth-century English prose without anachronism. The researchers found that in-context prompting with period text consistently fails to produce convincing period style, with automated date-prediction models estimating continuations as modern rather than historical. Fine-tuning a model on approximately 200 period passages produced more stylistically convincing output that fooled automated date-prediction judges, though human evaluators could still distinguish fine-tuned outputs from authentic period text. The study concludes that while fine-tuning shows promise, period-pretrained models remain the only guaranteed anachronism-free approach despite their fluency limitations.

## Method Summary
The researchers evaluated language model performance on representing early twentieth-century English through three approaches: in-context prompting, fine-tuning on period text, and comparison with period-pretrained models. They used a RoBERTa date-prediction model to assess stylistic authenticity and conducted human evaluations to compare fine-tuned outputs with authentic period text. The fine-tuning corpus consisted of approximately 200 passages from early twentieth-century literature, and evaluation focused on both automated metrics (Jensen-Shannon divergence) and human judgments of stylistic authenticity.

## Key Results
- In-context prompting with period text failed to produce convincing early twentieth-century style, with date-prediction models estimating continuations as 21st century
- Fine-tuning on ~200 period passages achieved Jensen-Shannon divergence of 0.002 with ground truth, close to period-pretrained models at 0.006
- Human evaluators could distinguish fine-tuned outputs from authentic period text despite automated judges being fooled
- Period-pretrained models remained the only approach guaranteed to avoid anachronism, though with fluency limitations

## Why This Works (Mechanism)
The study demonstrates that contemporary language models' training on predominantly modern text creates inherent biases that simple prompting cannot overcome. Period-specific linguistic patterns require more substantial retraining to capture authentically, as the models' internal representations are fundamentally shaped by their training data distribution. Fine-tuning provides a pathway to modifying these representations, but the limited corpus size and the depth of modern linguistic patterns embedded in the models constrain the effectiveness of this approach.

## Foundational Learning
- **Date-prediction models**: Used to assess whether generated text appears period-appropriate by estimating its temporal origin; needed to provide automated evaluation beyond human judgment, quick check by testing on known period vs. modern text
- **Jensen-Shannon divergence**: Statistical measure comparing probability distributions between generated and authentic period text; needed to quantify stylistic similarity, quick check by comparing against baseline distributions
- **Fine-tuning**: Process of adapting a pre-trained model to new domain data; needed to modify model representations for period-specific language, quick check by monitoring validation loss during training
- **In-context prompting**: Strategy of providing examples within the prompt to guide generation style; needed as initial approach for period simulation, quick check by evaluating prompt effectiveness across multiple examples
- **Language model evaluation**: Framework for assessing generated text quality and authenticity; needed to compare different approaches systematically, quick check by establishing baseline performance metrics

## Architecture Onboarding
Component map: Contemporary pre-trained LM -> Fine-tuning on period text -> Evaluation (RoBERTa date-prediction + human judgment) -> Period-pretrained LM comparison

Critical path: Fine-tuning corpus → Model adaptation → Automated date-prediction evaluation → Human evaluation → Authenticity assessment

Design tradeoffs: The study balanced between the fluency of contemporary models and the authenticity of period-pretrained models, ultimately finding that fine-tuning provides a middle ground but with limitations in both directions.

Failure signatures: In-context prompting produces clearly modern text, fine-tuned models retain subtle anachronistic features detectable by human experts, and period-pretrained models sacrifice fluency for authenticity.

Three first experiments: 1) Test in-context prompting with varying numbers of period examples to find effectiveness threshold; 2) Compare fine-tuning on different-sized period corpora to assess diminishing returns; 3) Evaluate whether combining period-pretrained models with contemporary fine-tuning improves results.

## Open Questions the Paper Calls Out
None

## Limitations
- Fine-tuning corpus size (~200 passages) may be insufficient to capture full linguistic diversity of early twentieth-century English
- Automated date-prediction judge may not capture all dimensions of period-appropriate style
- Human evaluation was limited to distinguishing fine-tuned from authentic text rather than comprehensive quality assessment
- Study focused on literary prose, limiting generalizability to other text types or periods

## Confidence
- High confidence in the failure of in-context prompting strategies
- Medium confidence in fine-tuning effectiveness given limited corpus size and evaluation scope
- Medium confidence in automated evaluation methodology, though validation against broader human judgments would strengthen claims

## Next Checks
1. Evaluate fine-tuned models on diverse period writing tasks beyond the current prompt types to assess generalizability
2. Conduct larger-scale human evaluations with domain experts to identify specific anachronistic features the models retain
3. Test whether increasing the fine-tuning corpus size beyond 200 passages produces diminishing returns or continued improvement in period authenticity