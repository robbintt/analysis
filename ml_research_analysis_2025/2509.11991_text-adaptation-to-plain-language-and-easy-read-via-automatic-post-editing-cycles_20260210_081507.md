---
ver: rpa2
title: Text Adaptation to Plain Language and Easy Read via Automatic Post-Editing
  Cycles
arxiv_id: '2509.11991'
source_url: https://arxiv.org/abs/2509.11991
tags:
- adaptation
- text
- similarity
- language
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study addresses automatic text adaptation to Plain Language\
  \ (PL) and Easy Read (ER) in Spanish, targeting accessibility for diverse audiences.\
  \ The proposed approach uses Automatic Post-Editing Cycles (APEC), where initial\
  \ LLM adaptations\u2014via BM25 retrieval or Direct Preference Optimization\u2014\
  are iteratively refined to improve readability while preserving semantic content."
---

# Text Adaptation to Plain Language and Easy Read via Automatic Post-Editing Cycles

## Quick Facts
- arXiv ID: 2509.11991
- Source URL: https://arxiv.org/abs/2509.11991
- Reference count: 40
- One-line primary result: APEC method significantly improved readability over initial adaptations and achieved first place in PL and second in ER in the CLEARS challenge

## Executive Summary
This paper addresses automatic text adaptation to Plain Language (PL) and Easy Read (ER) in Spanish using Automatic Post-Editing Cycles (APEC). The approach iteratively refines initial LLM adaptations through self-evaluation cycles, optimizing for readability while preserving semantic content. APEC outperformed direct end-to-end methods, achieving state-of-the-art results in the CLEARS challenge for both PL and ER tasks.

## Method Summary
The method uses LLaMA 3.1 8B Instruct for initial adaptations via BM25 retrieval or Direct Preference Optimization (DPO), followed by APEC refinement cycles. Initial adaptations are generated with either 5-shot BM25 RAG demonstrations or DPO-tuned models. APEC performs up to 5 iterations where an LLM analyzes the current adaptation against guidelines, generates a revised version, and accepts changes only if the average of Fernández Huerta readability index and embedding cosine similarity improves. The final ensemble selects the highest-scoring variant per sample.

## Key Results
- APEC significantly improved readability metrics over initial adaptations (PL: +16.54 FH points; ER: +18.25 FH points)
- Achieved first place in PL and second in ER in the CLEARS challenge
- Lexical similarity decreased during APEC cycles, indicating prioritization of readability over matching reference wording
- BM25 retrieval consistently outperformed embedding-based demonstration retrieval for this task
- DPO-initialized adaptations achieved higher readability than supervised fine-tuning while maintaining semantic fidelity

## Why This Works (Mechanism)

### Mechanism 1
Iterative LLM self-evaluation cycles can progressively improve text adaptation quality along measurable dimensions. The APEC loop provides guideline-based critique, targeted correction, and metric-gated acceptance, exploiting LLMs' strength at refinement tasks. The combined metric (FH + embedding similarity) may correlate with genuine adaptation quality without introducing systematic distortions. Break condition: semantic drift accumulates despite embedding checks, or FH gains come from superficial changes that don't improve actual comprehensibility.

### Mechanism 2
Lexical retrieval (BM25) outperforms semantic embedding retrieval for selecting adaptation demonstrations in this domain. BM25 matches on surface-level lexical overlap, which may better capture structural/functional similarity in adaptation tasks than dense embeddings that conflate semantic relatedness. Break condition: for very short inputs or highly paraphrased pairs, semantic retrieval may recover relevant demonstrations that lexical matching misses.

### Mechanism 3
Direct Preference Optimization produces initial adaptations with higher readability than supervised fine-tuning while maintaining competitive semantic fidelity. DPO trains the model to prefer human reference adaptations over zero-shot outputs, implicitly learning the transformation style without forcing exact lexical reproduction. This yields more aggressive simplification than SFT, which optimizes for token-level match. Break condition: if zero-shot outputs are too divergent from references, the preference signal becomes noisy.

## Foundational Learning

- Concept: **Fernández Huerta Readability Index**
  - Why needed here: The primary optimization target for APEC cycles; a Spanish adaptation of Flesch-Kincaid based on sentence length and syllable count
  - Quick check question: Can you explain why optimizing FH alone might not guarantee accessibility for the target ER/PL audiences?

- Concept: **Reference-less Evaluation for Text Simplification**
  - Why needed here: APEC operates without access to human references during iteration, relying only on input-adaptation similarity and readability metrics
  - Quick check question: What are two failure modes where embedding similarity + FH could both improve while actual adaptation quality degrades?

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: A core method for generating initial adaptations; understanding how contrastive pairs are constructed (reference vs. zero-shot) is essential for reproduction
  - Quick check question: How would you construct DPO training pairs if you had multiple human references per source with varying quality levels?

## Architecture Onboarding

- Component map: Input Processing -> Initial Adaptation Module (BM25 or DPO) -> APEC Loop (5 cycles) -> Metrics Layer -> Ensemble Selection
- Critical path: 1. Dataset preparation -> 2. Initial adaptation (BM25 or DPO) -> 3. APEC refinement cycles -> 4. Ensemble selection (for test: pick higher-scoring variant per sample)
- Design tradeoffs: APEC iterations (5) vs. computational cost (5x latency); Temperature (0.8) for diversity vs. stability; BM25 vs. DPO initialization (simplicity vs. higher initial FH)
- Failure signatures: Semantic drift (embedding similarity drops >0.05 while FH rises); Stagnation (no improvement across 3+ consecutive cycles); Hallucination accumulation (analysis identifies non-existent issues)
- First 3 experiments: 1) Baseline replication: Implement BM25 + APEC on development set to validate pipeline and reproduce FH gains; 2) Ablation on cycle count: Test 1, 3, 5, 7 cycles to characterize diminishing returns; 3) Metric sensitivity analysis: Replace FH with alternative readability metric and compare final quality via manual inspection of 20 samples

## Open Questions the Paper Calls Out

- Question: How do the adaptations generated by APEC correlate with human evaluations of readability, meaning preservation, and usability conducted by the actual target audiences (e.g., people with cognitive disabilities)?
  - Basis in paper: The authors state in the Discussion that "it might nonetheless be worth exploring the correlation between the different degrees of adaptation featured in our approach and human evaluations."
  - Why unresolved: The study relied exclusively on automated metrics due to the high operational cost and complexity of organizing human evaluation campaigns
  - What evidence would resolve it: A user study involving target demographics comparing APEC outputs to human references using task-based success rates and subjective readability ratings

- Question: Can alternative metrics, specifically pretrained metrics tuned for adaptation tasks, improve the APEC refinement process compared to the current use of readability indices and general embedding similarity?
  - Basis in paper: The Conclusion explicitly lists "explor[ing] alternative metrics to both gear adaptation refinements and evaluate the results, in particular pretrained metrics tuned to adaptation tasks" as future work
  - Why unresolved: The current implementation uses general-purpose metrics which the authors acknowledge may be limited in assessing the specific nuances of Plain Language and Easy Read validity
  - What evidence would resolve it: Ablation studies substituting the current FH index/embedding scoring mechanism with metrics like SLE to measure changes in final adaptation quality

- Question: Is it possible to optimize the iterative APEC approach for online, real-time usage, or is the method fundamentally limited to offline processing due to computational costs?
  - Basis in paper: The Discussion notes that APEC "comes with a higher computational cost than direct end-to-end methods" and "might thus be more suited for offline text adaptation"
  - Why unresolved: The paper evaluates quality but does not analyze inference time or computational efficiency trade-offs required for real-time applications
  - What evidence would resolve it: Latency benchmarks of the APEC cycles compared to single-pass DPO or BM25 baselines, potentially testing early-stopping criteria to balance speed and quality

## Limitations

- Metric correlation uncertainty: The APEC optimization target combines readability and embedding similarity, but there's no direct evidence this correlates with human judgments of adaptation quality for PL/ER audiences
- DPO training details unspecified: Critical hyperparameters like training duration, batch size, and learning rate schedule remain unknown, making exact reproduction uncertain
- Long input handling unclear: The method filtered out examples exceeding 3,000 tokens without reporting exclusion statistics or analyzing APEC behavior for typical document lengths

## Confidence

**High Confidence**: The APEC framework is clearly specified and implemented; the iterative refinement mechanism is straightforward and reproducible. The reported improvements in readability and embedding similarity over initial adaptations are directly measurable and internally consistent.

**Medium Confidence**: The relative performance of BM25 vs. embedding retrieval for demonstrations, and DPO vs. supervised fine-tuning for initial adaptation, are supported by the reported metrics but rely on specific dataset characteristics. These findings may not generalize to other languages or text domains.

**Low Confidence**: The claim that APEC achieves optimal trade-offs between readability and semantic preservation is weakly supported. The method optimizes a constructed metric rather than validated accessibility outcomes, and the significant lexical divergence from references suggests potential quality issues not captured by the evaluation.

## Next Checks

1. **Human Evaluation Validation**: Conduct a small-scale human study (n=30) where participants rate 20 randomly selected APEC adaptations on actual readability and comprehension for target PL/ER audiences. Compare these ratings against the automated FH+embedding metric to validate whether the optimization target correlates with genuine accessibility improvements.

2. **Cycle Sensitivity Analysis**: Systematically vary the number of APEC cycles (1, 3, 5, 7) on a held-out validation set and plot the trajectory of FH, embedding similarity, and lexical similarity. This will reveal whether the claimed 5-cycle optimum is robust or whether benefits plateau earlier, helping optimize computational efficiency.

3. **Cross-Domain Transfer Test**: Apply the APEC method to a different Spanish text simplification task (e.g., biomedical abstracts or legal documents) without additional fine-tuning. Measure whether the same pattern of readability gains with semantic preservation holds, or whether the method's effectiveness is domain-specific to the administrative texts in ClearSim.