---
ver: rpa2
title: 'Latte: Collaborative Test-Time Adaptation of Vision-Language Models in Federated
  Learning'
arxiv_id: '2507.21494'
source_url: https://arxiv.org/abs/2507.21494
tags:
- memory
- latte
- clients
- client
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Latte addresses the challenge of test-time adaptation of vision-language
  models in decentralized federated learning settings, where each client has limited
  data and unique distributions. The method introduces a hybrid memory system where
  each client maintains both a local memory of its own high-confidence embeddings
  and an external memory of class prototypes from relevant clients.
---

# Latte: Collaborative Test-Time Adaptation of Vision-Language Models in Federated Learning

## Quick Facts
- arXiv ID: 2507.21494
- Source URL: https://arxiv.org/abs/2507.21494
- Reference count: 40
- Key outcome: Latte significantly outperforms baselines on domain adaptation and corruption benchmarks, achieving accuracy improvements up to 9.11% on VLCS and 2.69% on CIFAR-10-C while maintaining negligible computational and communication costs

## Executive Summary
Latte addresses the challenge of test-time adaptation of vision-language models in decentralized federated learning settings where each client has limited data and unique distributions. The method introduces a hybrid memory system where each client maintains both a local memory of its own high-confidence embeddings and an external memory of class prototypes from relevant clients. During communication, clients upload prototypes to a server and download similar prototypes from other clients, enabling collaborative adaptation while preserving personalization. The adaptation process combines embedding similarity and uncertainty to robustly handle both in-distribution and out-of-distribution samples.

## Method Summary
Latte is a memory-based test-time adaptation method for vision-language models in federated learning. Each client maintains a local memory storing high-confidence embeddings from its own test data and an external memory containing class prototypes from other relevant clients. The server collects prototypes from all clients, then uses each client's prototype as a query vector to retrieve the top-k most similar prototypes from other clients. During inference, Latte combines local and external memories with entropy-based filtering and dual weighting by similarity and uncertainty to produce robust class prototypes. The method decouples local adaptation from global communication, allowing flexible communication periods while maintaining personalization.

## Key Results
- Outperforms state-of-the-art baselines by up to 9.11% accuracy on VLCS domain adaptation benchmark
- Maintains accuracy gains with communication periods up to 200 samples while reducing communication cost
- Shows robustness to out-of-distribution clients, with theoretical bounds proving error depends only on in-distribution samples
- Achieves 2.69% accuracy improvement on CIFAR-10-C corruption benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid local-external memory enables collaboration while preserving personalization.
- Mechanism: Each client maintains a local memory Li storing its own high-confidence embeddings, and an external memory Ei storing prototypes retrieved from similar clients. During prediction, both memories are merged with entropy-based filtering, allowing in-distribution prototypes to replace high-uncertainty local entries while discarding irrelevant OOD prototypes.
- Core assumption: Clients with similar distributions produce mutually beneficial prototypes; OOD clients can be filtered via similarity thresholds.
- Evidence anchors:
  - [abstract] "each client maintains a local memory to store embeddings from its own historical test data and an external memory to store class prototypes from other relevant clients"
  - [section 3.3] "for a given client i, only a small subset of clients' prototypes may be in-distribution (ID)... while the majority may be out-of-distribution (OOD)"
  - [corpus] Related work FedCTTA addresses continual TTA in FL but uses different aggregation; no direct evidence for hybrid memory specifically.
- Break condition: If all clients have completely disjoint distributions (no ID pairs), external memory provides no benefit and may introduce noise.

### Mechanism 2
- Claim: Prototype similarity-based retrieval filters OOD clients before they harm local adaptation.
- Mechanism: Clients compute class prototypes as entropy-weighted averages of local embeddings (Equation 1). The server uses each client's uploaded prototype as a query vector to retrieve the top-ke most similar prototypes from the global memory (Equation 2), excluding the client's own prototype. This coarse filtering reduces both communication cost and OOD contamination.
- Core assumption: Prototype cosine similarity correlates with distribution similarity; similar distributions yield similar class prototypes.
- Evidence anchors:
  - [section 3.3] "this step serves only as a coarse filtering of prototypes. During inference, an adaptive aggregation will be performed"
  - [section 5, Figure 4] Visualization shows clients primarily obtain prototypes from clients with similar distributions
  - [corpus] TOFA addresses training-free FL adaptation for VLMs but uses different retrieval; no direct comparison available.
- Break condition: If embedding space is poorly calibrated (e.g., class prototypes from different distributions have high cosine similarity), retrieval will fail to distinguish ID from OOD.

### Mechanism 3
- Claim: Dual weighting by similarity and uncertainty provides robustness to misclassified and OOD samples.
- Mechanism: During inference (Equation 4), aggregation weights combine embedding similarity (f⊤m) and entropy H(m): w = exp(β·similarity) × exp(-γ·entropy). High-similarity, low-uncertainty samples dominate the class prototype c_y used for memory logits. Theorem 4.3 proves error bounds depend only on n_ID, not n_OOD.
- Core assumption: Correctly classified samples have lower entropy on average; OOD samples have lower similarity to ID test samples.
- Evidence anchors:
  - [section 3.4] "By assigning higher weights to samples with high similarity and low uncertainty, ci_y becomes more robust to both OOD prototypes from other clients and uncertain embeddings"
  - [section 4, Theorem 4.3] "the error bound decreases as nID increases but remains unaffected by the growth of nOOD"
  - [corpus] Weak corpus evidence; pFedBBN addresses class imbalance in FL-TTA but uses balanced batch normalization, not dual weighting.
- Break condition: If misclassified samples systematically have low entropy (confident wrong predictions), the weighting scheme may amplify errors rather than suppress them.

## Foundational Learning

- Concept: Memory-based Test-Time Adaptation (TTA) for VLMs
  - Why needed here: Latte extends memory-based TTA (e.g., TDA, DMN-ZS) to federated settings. Without understanding how memory stores high-confidence embeddings and uses similarity for prediction, the collaborative extension is opaque.
  - Quick check question: Can you explain why memory-based TTA methods improve over zero-shot CLIP as test samples accumulate?

- Concept: Federated Learning with Heterogeneous Clients
  - Why needed here: Latte's core problem is balancing personalization (adapting to each client's unique Di) with collaboration (sharing knowledge across clients). Understanding FL heterogeneity is prerequisite.
  - Quick check question: Why does sharing a single global memory across heterogeneous FL clients harm personalization?

- Concept: k-Nearest Neighbor Classification in Embedding Space
  - Why needed here: Theoretical analysis (Lemma C.10) proves Latte's classifier asymptotically approaches a 1-NN classifier. Understanding k-NN generalization bounds clarifies why error decreases with N.
  - Quick check question: Why does the error bound in Theorem 4.2 scale as O((k/N)^(1/(d+1))) rather than O(1/√N)?

## Architecture Onboarding

- Component map:
  - Client-side: Local memory Li (c priority queues) -> External memory Ei -> Adaptation module (similarity+entropy weighting)
  - Server-side: Global memory G (c×n×d tensor storing one prototype per client per class) -> Retrieval coordinator
  - Communication channel: Upload prototypes (c×d per client) -> Download retrieved prototypes (c×ke×d per client)

- Critical path:
  1. Encode test image → embedding f, initial CLIP prediction
  2. Update local memory Li_ŷ (insert or replace highest-entropy entry)
  3. Merge local+external memories with entropy threshold τ(kl)
  4. Compute dual-weighted class prototypes ci_y → memory logits z_mem
  5. Final prediction: z_post = z_pre + α·z_mem
  6. (Asynchronous) Upload prototypes to server, download retrieved external memory

- Design tradeoffs:
  - **Local memory size kl**: Larger kl captures more diverse samples but may include more misclassified entries. Paper uses kl∈[2,15] across datasets.
  - **External memory size ke**: Larger ke increases collaboration potential but risks OOD contamination. Paper uses ke∈[5,20].
  - **Communication period T**: Decoupling allows T up to 200 samples with minimal degradation (Figure 3), but larger T delays knowledge transfer.

- Failure signatures:
  - Accuracy degrades below baseline CLIP: Likely external memory dominated by OOD prototypes; reduce ke or increase similarity threshold τ(k_e).
  - High variance across seeds: Local memory initialization sensitive; check if priority queues fill before external memory updates arrive.
  - Communication cost exceeds budget: Server-side retrieval computing all pairwise similarities; implement approximate nearest neighbor search.

- First 3 experiments:
  1. **Single-client ablation**: Run Latte with ke=0 (local only) vs. kl=0 (external only) on VLCS. Verify both contribute per Figure 6 (left).
  2. **OOD robustness test**: Simulate n_OOD≫n_ID by assigning different domains to most clients. Confirm accuracy remains stable vs. TDA(global) baseline.
  3. **Communication efficiency sweep**: Vary communication period T∈{1,10,50,200} on TerraIncognita. Reproduce Figure 3 curve; if performance drops at T=50, check prototype staleness.

## Open Questions the Paper Calls Out
None

## Limitations
- Hybrid memory effectiveness assumes sufficient overlap between client distributions; in highly heterogeneous settings with disjoint domains, external memory contribution may be negligible or harmful
- Prototype retrieval relies on cosine similarity as proxy for distribution similarity, but no empirical validation that this correlation holds across all tested datasets
- Dual weighting assumes entropy correlates with prediction correctness, yet systematic evaluation of entropy calibration across misclassified vs. correct samples is absent

## Confidence
- **High**: Accuracy improvements on domain adaptation benchmarks (VLCS, OfficeHome, TerraIncognita), communication efficiency claims
- **Medium**: OOD robustness theoretical bounds, negligible computational overhead claims
- **Low**: Generalizability to completely disjoint distributions, prototype retrieval effectiveness across arbitrary VLM architectures

## Next Checks
1. Stress test on completely disjoint distributions: Run Latte on clients with non-overlapping label spaces (e.g., CIFAR-10 vs. CIFAR-100) to verify accuracy doesn't degrade below baseline
2. Entropy calibration analysis: Measure average entropy for correctly vs. incorrectly classified samples in local memory; if misclassified samples have systematically lower entropy, weighting scheme amplifies errors
3. Retrieval similarity validation: For each VLCS client pair, compute actual distribution similarity (e.g., KL divergence) vs. prototype cosine similarity to quantify retrieval accuracy