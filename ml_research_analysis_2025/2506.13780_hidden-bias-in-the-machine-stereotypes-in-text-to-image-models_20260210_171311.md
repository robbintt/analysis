---
ver: rpa2
title: 'Hidden Bias in the Machine: Stereotypes in Text-to-Image Models'
arxiv_id: '2506.13780'
source_url: https://arxiv.org/abs/2506.13780
tags:
- bias
- images
- prompts
- negative
- actions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines stereotypes and biases in text-to-image (T2I)
  models through a systematic study of over 16,000 images generated from 160 diverse
  prompts spanning occupations, traits, actions, ideologies, emotions, family roles,
  place descriptions, spirituality, and life events. Using Stable Diffusion 1.5 and
  Flux-1 models, the authors find significant disparities in gender, race, age, and
  body type representations across generated images, often reinforcing societal stereotypes.
---

# Hidden Bias in the Machine: Stereotypes in Text-to-Image Models

## Quick Facts
- arXiv ID: 2506.13780
- Source URL: https://arxiv.org/abs/2506.13780
- Authors: Sedat Porikli; Vedat Porikli
- Reference count: 30
- Primary result: Systematic study finds significant demographic disparities in T2I model outputs across 16,000+ images, with Western and white representations dominating positive attributes while negative associations skew toward males and certain ethnicities.

## Executive Summary
This paper examines stereotypes and biases in text-to-image (T2I) models through a systematic study of over 16,000 images generated from 160 diverse prompts spanning occupations, traits, actions, ideologies, emotions, family roles, place descriptions, spirituality, and life events. Using Stable Diffusion 1.5 and Flux-1 models, the authors find significant disparities in gender, race, age, and body type representations across generated images, often reinforcing societal stereotypes. For example, high-income occupations were predominantly white, while low-income roles showed higher representation of Latino and Black individuals; negative attributes and actions were associated more with males; and certain ideologies were linked to overweight body types. The study highlights the need for more inclusive datasets and development practices to mitigate these biases in generative visual systems.

## Method Summary
The study generated 16,000+ images using 160 unique prompts across 9 thematic categories, with 50+ images per topic using random seeds. Two models were evaluated: Stable Diffusion 1.5 (UNet-based) and Flux-1 (DiT-based), both using original HuggingFace checkpoints at 512×512 resolution with 20 sampling steps. Multiple human annotators labeled outputs for gender, race, age, and somatotype categories, comparing positive vs. negative prompt groups. The authors also compared T2I outputs to 8,000 images from Google Image Search as a baseline.

## Key Results
- High-income occupations showed 70% white representation (SD1.5) and 96% white (Flux-1)
- Negative attributes were associated with 91% male representation (SD1.5) and 88% male (Flux-1)
- Positive place descriptions skewed 99% toward Western representations in both models
- "Terrorist" prompts generated 98% Middle Eastern representations (SD1.5)
- Life events like weddings and retirements reinforced gendered and racial biases with up to 100% Western settings

## Why This Works (Mechanism)

### Mechanism 1: Training Data Distribution Replication
T2I models encode and reproduce demographic patterns present in their web-scraped training corpora. Models trained on large-scale datasets like LAION learn statistical associations between text concepts and visual features; when prompts lack explicit demographic markers, the model samples from the learned conditional distribution, which reflects historical representation patterns in the training data. The observed biases originate primarily from training data composition rather than architectural choices or sampling procedures.

### Mechanism 2: Implicit Concept-Demographic Association
Models learn implicit associations between abstract concepts (emotions, ideologies, traits) and specific demographic groups. Text encoders create joint embeddings where semantically related concepts cluster together; co-occurrence patterns in training captions create statistical links between concept words and demographic descriptors that persist even when prompts are neutral. The text encoder's representation space preserves co-occurrence statistics that influence generation.

### Mechanism 3: Western-Centric Default Sampling
When prompts lack geographical or cultural markers, models default to Western visual conventions and settings. Western content dominates English-language web datasets; the model's prior distribution over visual styles, settings, and cultural practices reflects this imbalance, causing neutral prompts to generate Western-associated imagery. The sampling process from the learned distribution selects the mode, which corresponds to the most frequent training pattern.

## Foundational Learning

- Concept: Diffusion Model Denoising Process
  - Why needed here: Understanding how iterative denoising transforms random noise into structured images clarifies where bias can enter—during conditioning, during sampling, or in the learned prior.
  - Quick check question: Can you explain why a diffusion model's output for a neutral prompt might reflect the most common training pattern rather than an equal distribution?

- Concept: Text-Image Contrastive Learning (CLIP-style)
  - Why needed here: T2I models use text encoders trained via contrastive learning; understanding how text and image embeddings are aligned helps explain how word associations transfer to visual outputs.
  - Quick check question: How would co-occurrence of "CEO" with "male" in training captions affect the text embedding space?

- Concept: Intersectionality in Bias Measurement
  - Why needed here: The paper explicitly notes that "bias categories such as gender and race often intersect, leading to amplified stereotypes"; measuring single dimensions may miss compounding effects.
  - Quick check question: If a model generates 80% male for "doctor" and 80% white for "doctor," what percentage would you expect to be white male if biases were independent?

## Architecture Onboarding

- Component map: Prompt → Text Encoder → Conditioning Vectors → Cross-Attention in Denoiser → Iterative Denoising (20 steps) → VAE Decoder → Output Image
- Critical path: Text Encoder converts prompts to conditioning vectors (likely CLIP-based for both models), UNet (SD1.5) or DiT (Flux-1) performs iterative denoising with cross-attention layers, VAE decodes from latent space to output image
- Design tradeoffs: SD1.5 vs Flux-1 comparison reveals architectural influence—Flux-1 showed stronger White bias (96-100% across categories) while SD1.5 showed more variation. Both architectures exhibited similar pattern directions but different magnitudes, suggesting architecture modulates but doesn't eliminate data-driven biases.
- Failure signatures: Race: High-income roles → 70% White (SD1.5), 96% White (Flux-1); Gender: Negative attributes → 91% Male (SD1.5), 88% Male (Flux-1); Geography: Positive place descriptions → 99% Western (both models); Ideology: "Terrorist" prompts → 98% Middle Eastern (SD1.5), 67% (Flux-1)
- First 3 experiments: 1) Baseline replication: Generate 50 images per topic using SD1.5 and Flux-1 with identical seeds; verify bias distributions match reported percentages. 2) Counterfactual prompting: For high-bias categories, test if explicit demographic modifiers ("a female CEO," "a Black doctor") shift distributions or if model resists counter-stereotypical generation. 3) Ablation by category: Isolate which prompt categories (occupations vs. emotions vs. places) show the highest bias magnitude to prioritize mitigation efforts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Will T2I models trained on datasets containing AI-generated images exhibit progressively amplified biases compared to models trained only on human-created content?
- Basis in paper: The paper identifies this feedback loop risk but does not empirically test it; measuring bias amplification across training generations would require controlled longitudinal experiments.
- What evidence would resolve it: Train T2I models iteratively on synthetic datasets and measure whether bias metrics increase across generations compared to baseline models trained on original web-scraped data.

### Open Question 2
- Question: Can a unified evaluation framework comprehensively measure biases across all identified categories (gender, race, age, somatotype, religion, geography) with cross-model comparability?
- Basis in paper: Existing studies use inconsistent definitions and metrics; this paper introduces its own labeling scheme but does not propose a standardized benchmark applicable beyond its 160-prompt set.
- What evidence would resolve it: Develop and validate a benchmark suite with standardized prompts, annotation protocols, and quantitative metrics that produce comparable bias scores across different T2I architectures and training regimes.

### Open Question 3
- Question: Do bias patterns in T2I model outputs vary significantly across different languages and cultural contexts in prompts?
- Basis in paper: The paper uses English-language prompts and Western-centric bias definitions, while citing prior work on multilingual data and culturally aware interfaces without testing cross-cultural variation.
- What evidence would resolve it: Generate images from translated prompts across multiple languages and culturally-adapted prompt variants, then compare bias label distributions to identify culturally-dependent versus universal bias patterns.

## Limitations
- Prompt Representation Gap: The 160-prompt benchmark may not capture the complete bias landscape present in real-world usage, as it omits critical categories like disability and religion.
- Dataset Annotation Quality: Human labeling introduces subjectivity, particularly for race, somatotype, and age categories where cultural context and visual interpretation vary.
- Temporal Stability: The study doesn't examine how bias patterns evolve across model versions or with fine-tuning approaches that could mitigate identified disparities.

## Confidence
- High Confidence: The systematic finding that both SD1.5 and Flux-1 models reproduce demographic stereotypes in occupation, emotion, and place descriptions.
- Medium Confidence: The claim that "biases stem from training data composition rather than architectural choices" is supported by comparing two different architectures but doesn't rule out other contributing factors.
- Low Confidence: The assertion that "certain ideologies were linked to overweight body types" requires careful interpretation due to limited methodology specification.

## Next Checks
1. **Prompt Robustness Testing**: Systematically vary prompt wording to determine if observed bias patterns are robust to semantic equivalence or sensitive to specific word choices in the text encoder.
2. **Cross-Cultural Validation**: Replicate the study using non-English prompts and models trained on multilingual datasets to test whether Western-centric defaults persist across languages and cultural contexts.
3. **Mitigation Efficacy Testing**: Apply common bias mitigation techniques (counter-stereotypical fine-tuning, balanced dataset curation, prompt engineering with demographic modifiers) to measure which approaches most effectively reduce the identified disparities without degrading overall model performance.