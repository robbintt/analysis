---
ver: rpa2
title: How Well Does First-Token Entropy Approximate Word Entropy as a Psycholinguistic
  Predictor?
arxiv_id: '2507.22209'
source_url: https://arxiv.org/abs/2507.22209
tags:
- entropy
- word
- first-token
- surprisal
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Monte Carlo sampling provides unbiased estimates of word entropy\
  \ in language models, outperforming first-token approximations in psycholinguistic\
  \ regression experiments. Using 512 samples per word, MC entropy better predicts\
  \ self-paced reading times than first-token entropy, especially for R\xE9nyi entropy\
  \ (\u03B1=1/2), which improved log-likelihood scores on 5 of 6 eye-tracking corpora."
---

# How Well Does First-Token Entropy Approximate Word Entropy as a Psycholinguistic Predictor?

## Quick Facts
- **arXiv ID**: 2507.22209
- **Source URL**: https://arxiv.org/abs/2507.22209
- **Reference count**: 21
- **Primary result**: Monte Carlo sampling estimates of word entropy outperform first-token approximations as psycholinguistic predictors in regression experiments on reading times.

## Executive Summary
This paper evaluates whether first-token entropy from language models (LMs) adequately approximates true word entropy for predicting human reading behavior. Using GPT2-small, the authors compare first-token entropy against Monte Carlo (MC) sampling of entire word probabilities. MC sampling treats words as variable-length sequences of subword tokens, sampling until a word boundary is reached. The MC approach provides unbiased estimates of word-level uncertainty, whereas first-token entropy systematically underestimates entropy—especially for multi-token words. In regression models predicting self-paced and eye-tracking reading times, MC entropy (particularly Rényi entropy with α=1/2) yields better log-likelihood improvements than first-token approximations.

## Method Summary
The study uses GPT2-small to estimate word probabilities via MC sampling (|S|=512 samples per word). For each context, tokens are partitioned into whitespace-initial (TB) and non-initial (TI) sets. Sampling proceeds by drawing an initial token from TB, then continuing with TI∪{EOW} until a word boundary is reached. Word surprisal is the sum of token surprisals plus EOW. Both Shannon and Rényi (α=1/2) entropy are computed. These predictors are evaluated in linear mixed-effects (LME) regression models predicting reading times from six corpora: Natural Stories and Brown (self-paced), Dundee, Provo, and GECO (eye-tracking). Models include word length, unigram surprisal, LM surprisal, and previous-word fixation as covariates, with random intercepts and slopes per subject.

## Key Results
- MC sampling produces unbiased estimates of word entropy; first-token approximations systematically underestimate, especially for multi-token words.
- MC entropy improves log-likelihood scores over first-token entropy in 5 of 6 eye-tracking corpora, with strongest gains for Rényi entropy (α=1/2).
- For self-paced reading, MC Shannon entropy shows consistent improvements, while first-token entropy sometimes hurts prediction.
- The distortion from first-token approximation correlates with the proportion of multi-token words in a corpus.

## Why This Works (Mechanism)
Monte Carlo sampling accurately estimates word-level probability by accounting for all possible token sequences that form a word. First-token entropy only considers the distribution over initial tokens, ignoring continuation uncertainty. By sampling full word completions, MC captures the true uncertainty a reader faces when anticipating a word, making it a more accurate psycholinguistic predictor.

## Foundational Learning
- **Monte Carlo sampling**: Estimating probabilities by repeated random sampling. *Why needed*: To approximate full word probabilities given subword tokenization. *Quick check*: Verify that sampled word probabilities sum to approximately 1 across many samples.
- **Rényi entropy**: A generalization of Shannon entropy parameterized by α; α=1/2 emphasizes rare events more than Shannon. *Why needed*: To test if alternative entropy measures better capture anticipatory uncertainty. *Quick check*: Compare α=1/2 vs. α=1 (Shannon) performance in regression.
- **Linear mixed-effects models**: Regression models with both fixed and random effects to account for subject-level variability. *Why needed*: To model individual differences in reading behavior across subjects. *Quick check*: Ensure random effects structure is justified by comparing model fits with/without specific slopes.
- **Subword tokenization**: Breaking words into smaller units (e.g., WordPiece, BPE) for LM input. *Why needed*: Modern LMs use subword vocabularies, complicating word-level probability estimation. *Quick check*: Inspect tokenization of multi-token words to confirm sampling logic.
- **Surprisal**: Negative log-probability of a word given context. *Why needed*: A standard psycholinguistic predictor of processing difficulty. *Quick check*: Correlate surprisal with reading times to verify expected negative relationship.
- **Permutation tests**: Non-parametric significance tests by randomly shuffling labels. *Why needed*: To assess whether entropy predictors improve models beyond chance. *Quick check*: Run test on permuted data to confirm false positive rate.

## Architecture Onboarding
**Component map**: Context -> GPT2-small tokenization -> MC sampling (TB/TI sets, EOW) -> Word probability estimates -> Entropy computation -> LME regression (reading time ~ entropy + controls + random effects) -> Evaluation (ΔLL, permutation test)

**Critical path**: Context → MC sampling → Entropy → LME model → Log-likelihood improvement. Sampling must be unbiased and efficient; entropy must be correctly computed; LME must converge and properly assess predictor contribution.

**Design tradeoffs**: Fixed |S|=512 balances accuracy and computation; separate TB/TI sets handle word boundary ambiguity; capping at 20 tokens prevents infinite loops; using GPT2-small prioritizes reproducibility over state-of-the-art performance.

**Failure signatures**: High coefficient of variation in entropy estimates indicates insufficient sampling; negative ΔLL suggests entropy is a worse predictor than baseline; LME convergence warnings indicate over-specified random effects; inconsistent performance across corpora suggests sampling or entropy computation errors.

**First experiments**: 1) Verify MC sampling by checking that estimated word probabilities sum to 1 over many samples. 2) Compare first-token vs. MC entropy estimates on a small, manually inspectable corpus. 3) Fit a simple LME with MC entropy on a single corpus to confirm expected sign and significance.

## Open Questions the Paper Calls Out
- **Vocabulary size and first-token distortion**: The paper notes that the degree to which first-token entropy distorts true word entropy may depend on the size of an LM’s subword vocabulary. This was not quantified across different models or vocabulary sizes.
- **Differential predictive power for reading measures**: MC Shannon entropy improves self-paced reading but often fails to improve eye-tracking measures. The paper does not explain why anticipatory uncertainty would correlate differently with these two behavioral measures.
- **Cross-linguistic robustness of Rényi α=1/2**: The study relies on English corpora; in morphologically rich languages, the optimal α parameter or utility of MC estimation might differ due to more complex word structures.

## Limitations
- Results are based on GPT2-small and may not generalize to larger or differently architected LMs.
- Fixed sampling budget (|S|=512) and token cap (20) were not explored for sensitivity.
- LME model complexity and convergence handling could influence relative performance comparisons.

## Confidence
- **High confidence**: MC sampling produces unbiased word entropy estimates; MC entropy outperforms first-token entropy as a psycholinguistic predictor; systematic underestimation by first-token approximations is demonstrated.
- **Medium confidence**: Rényi entropy (α=1/2) provides the best log-likelihood improvements; specific improvement magnitudes; LME convergence behavior and random effect specifications.

## Next Checks
1. **Sampling stability analysis**: Vary the number of Monte Carlo samples (e.g., 128, 256, 512, 1024) and measure coefficient of variation across words to confirm stability thresholds and assess sensitivity to the |S|=512 choice.
2. **Cross-LM validation**: Apply the same MC sampling procedure to GPT2-medium or GPT2-large and compare relative performance of entropy measures to assess whether results generalize beyond GPT2-small.
3. **Entropy measure sensitivity**: Systematically vary the Rényi α parameter (e.g., α=1/4, 1/2, 3/4, 1) and evaluate log-likelihood improvements to determine if α=1/2 is optimal or if performance varies by corpus/measure type.