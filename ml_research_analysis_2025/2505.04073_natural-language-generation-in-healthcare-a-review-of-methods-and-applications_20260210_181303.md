---
ver: rpa2
title: 'Natural Language Generation in Healthcare: A Review of Methods and Applications'
arxiv_id: '2505.04073'
source_url: https://arxiv.org/abs/2505.04073
tags:
- generation
- medical
- text
- clinical
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic review analyzed 113 studies to examine natural
  language generation (NLG) methods and applications in healthcare. The review found
  that transformer-based models, particularly encoder-decoder and decoder-only architectures,
  dominate recent NLG research, with 61.64% of text-to-text generation studies using
  encoder-decoder transformers and 27.40% using decoder-only transformers.
---

# Natural Language Generation in Healthcare: A Review of Methods and Applications

## Quick Facts
- arXiv ID: 2505.04073
- Source URL: https://arxiv.org/abs/2505.04073
- Reference count: 0
- Primary result: Systematic review of 113 studies reveals transformer-based architectures dominate NLG in healthcare, with clinical summarization and automated documentation being primary applications.

## Executive Summary
This systematic review analyzed 113 studies to examine natural language generation (NLG) methods and applications in healthcare. The review found that transformer-based models, particularly encoder-decoder and decoder-only architectures, dominate recent NLG research, with 61.64% of text-to-text generation studies using encoder-decoder transformers and 27.40% using decoder-only transformers. The most common evaluation metrics were ROUGE (used in 74 studies) and BLEU (used in 60 studies), with human evaluation using Likert scales being the most prevalent human assessment method (36 studies). NLG applications were categorized into four primary domains: clinical summarization, automated documentation, medical dialogue generation, and data augmentation. The review highlights how NLG is transforming clinical workflows by reducing documentation burden, improving clinical decision support, and enabling privacy-preserving synthetic data generation for research.

## Method Summary
The review followed PRISMA guidelines, conducting systematic literature searches across seven databases (PubMed, ACM Digital Library, Web of Science, Science Direct, Scopus, Embase, ACL Anthology) from January 2018 to December 2024. The search used Boolean combinations of NLG terms ("Text Generation" OR "Natural Language Generation" OR "Sequence to Sequence") with healthcare terms ("Clinical" OR "Medicine" OR "Healthcare" OR "Health" OR "EHR" OR "EMR"). Two-stage screening via Covidence was performed by six reviewers, with pairwise assignment and third-reviewer resolution for disputes. Data extraction included 10 predefined elements covering NLG category, model architecture, data sources, clinical applications, and evaluation methods.

## Key Results
- Transformer-based models dominate healthcare NLG research, with encoder-decoder models used in 61.64% of text-to-text studies and decoder-only models in 27.40%
- ROUGE and BLEU remain the most prevalent automatic evaluation metrics despite acknowledged limitations in capturing semantic nuances
- Clinical summarization and automated documentation are the most common NLG applications, addressing physician documentation burden
- Human evaluation using Likert scales is the primary method for assessing generated text quality, used in 36 studies
- The review identifies critical gaps in evaluation methodology and calls for frameworks that capture clinical relevance and factual accuracy

## Why This Works (Mechanism)

### Mechanism 1: Transformer Architecture for NLG
- **Claim:** Transformer-based architectures dominate text-to-text NLG in healthcare due to their ability to capture long-distance dependencies and support large-scale pretraining.
- **Mechanism:** Encoder modules transform input sequences into latent vector representations capturing semantic context, while decoder modules generate output sequences token by token, conditioned on these representations. Encoder-decoder models (e.g., T5) use bidirectional encoders for full context understanding, while decoder-only models (e.g., GPT) use self-attention over entire sequences to predict next tokens.
- **Core assumption:** The pretrain-finetune paradigm effectively transfers general language knowledge to specialized medical contexts through domain-specific adaptation.
- **Evidence anchors:** 61.64% of text-to-text generation studies use encoder-decoder transformers and 27.40% use decoder-only transformers; transformer models demonstrated superior performance in capturing long-distance dependencies.
- **Break condition:** Performance may degrade significantly on low-resource medical sub-domains without sufficient domain-specific pretraining data or with novel medical terminology not represented in pretraining corpus.

### Mechanism 2: N-gram Evaluation Metrics
- **Claim:** N-gram overlap metrics (ROUGE, BLEU) remain the dominant automatic evaluation method in healthcare NLG due to computational efficiency, despite inability to capture semantic nuances.
- **Mechanism:** ROUGE calculates recall by measuring overlap of n-grams, longest common subsequences, or skip-bigrams between generated and reference text. BLEU focuses on precision by measuring n-gram generation accuracy. These surface-level metrics provide fast, scalable proxy for text quality by quantifying lexical similarity.
- **Core assumption:** Higher n-gram overlap correlates positively with human judgments of quality for generated medical text (fluency, relevance).
- **Evidence anchors:** ROUGE is most widely used metric (74/113 studies, 65.49%); BLEU focuses on N-gram generation accuracy.
- **Break condition:** Assumption fails when semantically equivalent texts use different phrasing (paraphrasing), leading to low scores despite high clinical accuracy.

### Mechanism 3: Clinical Summarization for Documentation Burden
- **Claim:** Clinical summarization NLG systems reduce documentation burden by condensing lengthy medical text into concise summaries that retain key clinical information.
- **Mechanism:** Models are trained on paired datasets of long-form medical text and corresponding reference summaries. During inference, encoder processes full dialogue or report, and decoder generates abstractive summary. Attention mechanisms focus model on clinically salient content, and PEFT allows efficient adaptation to specific summarization tasks.
- **Core assumption:** Generated summaries are factually accurate, complete with respect to critical clinical details, and trusted by clinicians for use in workflows.
- **Evidence anchors:** Clinical summarization transforms original long text into concise summaries while capturing key information; primary applications include radiology report summarization.
- **Break condition:** Systems may "hallucinate" or omit critical findings, leading to potential patient safety issues.

## Foundational Learning

- **Concept: Transformer Architecture (Encoder-Decoder, Decoder-only)**
  - **Why needed here:** This is the foundational architecture for all modern NLG systems discussed. Understanding the difference between encoder-decoder (T5, BART) and decoder-only (GPT) models is critical for selecting the right architecture for a given task.
  - **Quick check question:** For a task requiring a concise summary of a long patient history, which transformer variant's bidirectional encoder would be more advantageous for understanding the full input context?

- **Concept: N-gram vs. Embedding-based Evaluation Metrics**
  - **Why needed here:** The paper reveals a gap between widespread use of surface-level metrics (ROUGE, BLEU) and the need for semantic understanding. An engineer must know when to use BERTScore over BLEU.
  - **Quick check question:** A generated summary is clinically accurate but uses different medical synonyms than the reference text. Would you expect ROUGE or BERTScore to better reflect its quality?

- **Concept: Multimodal Fusion in NLG**
  - **Why needed here:** A significant portion of healthcare NLG involves non-text inputs like medical images (image-to-text) or structured EHR data. Understanding how to fuse these modalities is key to building systems described in the paper.
  - **Quick check question:** In an image-to-text generation system for radiology reports, what component is typically responsible for converting the X-ray image into a format the text-generation transformer can process?

## Architecture Onboarding

- **Component map:** Data preparation → Encoder processing (contextualizing input) → Decoder generation (producing text) → Dual evaluation (automatic + human) → Application-specific output
- **Critical path:** Data preparation (pairing inputs/outputs) → Encoder processing (contextualizing input) → Decoder generation (producing text) → Dual evaluation (automatic + human)
- **Design tradeoffs:**
  - Encoder-Decoder vs. Decoder-only: Encoder-decoder excels at tasks with distinct input and output (summarization), while decoder-only is more flexible for open-ended generation but harder to control for factual grounding.
  - Automatic vs. Human Evaluation: Automatic metrics are cheap and fast but miss semantic nuance. Human evaluation is expensive and slow but captures clinical relevance. A hybrid approach is recommended.
  - Pretrained vs. Custom Model: Pretrained models (GPT, T5) offer a strong starting point but require domain-specific fine-tuning. Building from scratch offers control but is resource-prohibitive.
- **Failure signatures:**
  - Hallucination: Generated text contains factually incorrect or non-existent clinical details
  - Omission: Critical findings from input are missing from output
  - Bias Amplification: Model outputs reflect or amplify biases in historical training data
  - Metric Gaming: High ROUGE/BLEU scores but low human-rated quality due to gaming lexical overlap
- **First 3 experiments:**
  1. Fine-tune a standard encoder-decoder transformer (e.g., BART or T5) on a public medical summarization dataset (e.g., MEDIQA-Chat). Evaluate using ROUGE, BLEU, and small-scale human evaluation (Likert scale) to establish baseline and understand the gap between automatic and human metrics.
  2. Compare the baseline encoder-decoder model against a decoder-only model (e.g., GPT-2 or Llama) on the same task. Hypothesis: Encoder-decoder will perform better on ROUGE due to bidirectional understanding, but decoder-only may generate more fluent text.
  3. Evaluate outputs from Experiment 1 using embedding-based metrics (BERTScore) alongside n-gram metrics. Have clinicians rate outputs on clinical accuracy. Analyze which automatic metric correlates more strongly with human judgments to inform a better evaluation protocol.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can evaluation metrics be developed to capture clinical relevance and factual accuracy beyond surface-level text overlap?
  - **Basis in paper:** The authors note that current metrics like ROUGE and BLEU "cannot account for the language variations and nuanced quality" and call for metrics that assess "clinical relevance, factual accuracy, and potential impact on patient outcomes."
  - **Why unresolved:** Current automatic metrics focus on n-gram overlap rather than clinical correctness, while robust human evaluation is too resource-intensive to scale.
  - **What evidence would resolve it:** The creation and validation of automated evaluation frameworks that strongly correlate with expert human judgment on clinical safety and coherence.

- **Open Question 2:** What frameworks are required to effectively integrate NLG systems into real-world clinical workflows?
  - **Basis in paper:** The review states that future work "should also explore the efficient integration of NLG systems into real-world clinical workflows and test the efficacy of AI implementation."
  - **Why unresolved:** Most reviewed studies focus on model architecture and automatic metrics rather than usability or impact on actual clinical delivery.
  - **What evidence would resolve it:** Results from longitudinal deployment studies measuring clinician burnout reduction, time saved, and error rates in live healthcare settings.

- **Open Question 3:** How can NLG systems be optimized to process multimodal data such as voice commands and omics data for hands-free clinical operation?
  - **Basis in paper:** The authors highlight the need to "explore the development of voice-activated NLG systems to enable hands-free operation" and to utilize "multimodalities of medical data, such as... omics data."
  - **Why unresolved:** The paper notes that while image-to-text is common, the integration of other modalities like voice for surgical settings remains less explored.
  - **What evidence would resolve it:** Successful development and benchmarking of transformer-based models that accept non-textual inputs (e.g., genomic sequences, audio) to generate accurate clinical narratives.

## Limitations

- The review's reliance on automatic evaluation metrics (ROUGE, BLEU) despite acknowledged limitations represents a significant gap in validating clinical utility.
- The systematic review methodology depends on accurate literature search strings across multiple databases, with exact query formulations not fully specified.
- The review lacks quantitative analysis of model performance across different clinical applications, focusing instead on architectural prevalence and evaluation metric usage patterns.

## Confidence

- **High Confidence:** Transformer architecture dominance (encoder-decoder and decoder-only models) and evaluation metric prevalence (ROUGE, BLEU) - based on direct quantitative analysis of 113 studies.
- **Medium Confidence:** Clinical application categorization and burden reduction claims - well-supported by literature review but involve more subjective interpretation of study outcomes.
- **Low Confidence:** Claims about hallucination rates and safety concerns - the review identifies these as challenges but does not provide systematic quantification of error rates across studies.

## Next Checks

1. **Corpus Verification:** Conduct a focused search using the exact keyword combinations specified in the methods section across the listed databases to verify the 113-study count and assess whether search string variations might have missed relevant papers.
2. **Performance Gap Analysis:** Identify the subset of studies that report quantitative performance metrics and analyze the correlation between automatic metric scores and reported human evaluation scores to quantify the evaluation gap identified in the review.
3. **Error Rate Quantification:** Systematically extract and synthesize data on hallucination rates, omission rates, and bias incidents from the reviewed studies to provide empirical estimates of the safety concerns raised in the discussion.