---
ver: rpa2
title: 'PHDME: Physics-Informed Diffusion Models without Explicit Governing Equations'
arxiv_id: '2601.21234'
source_url: https://arxiv.org/abs/2601.21234
tags:
- diffusion
- phdme
- hamiltonian
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PHDME is a physics-informed diffusion framework for forecasting
  dynamical systems with scarce data and unknown governing equations. The core idea
  is to learn a Gaussian process distributed Port-Hamiltonian system (GP-dPHS) as
  a reusable physics prior, which captures energy-based dynamics without requiring
  explicit PDEs.
---

# PHDME: Physics-Informed Diffusion Models without Explicit Governing Equations

## Quick Facts
- **arXiv ID:** 2601.21234
- **Source URL:** https://arxiv.org/abs/2601.21234
- **Reference count:** 40
- **Primary result:** Achieves 28% lower MSE than standard diffusion models while maintaining calibrated uncertainty bounds for dynamical systems with unknown governing equations

## Executive Summary
PHDME introduces a novel physics-informed diffusion framework for forecasting dynamical systems when governing equations are unknown and training data is scarce. The core innovation is learning a Gaussian process distributed Port-Hamiltonian system (GP-dPHS) as a reusable physics prior, capturing energy-based dynamics without requiring explicit PDEs. This prior is distilled into a diffusion model through physics-consistent data generation and an uncertainty-weighted residual loss, enabling fast, amortized trajectory sampling. Experiments on string wave equations, shallow water systems, and real-world spring systems demonstrate superior generalization to unseen initial conditions compared to standard diffusion models and NeuralODE baselines.

## Method Summary
PHDME operates in two stages: first, a GP-dPHS is trained on sparse observational data to learn energy-based dynamics without explicit governing equations; second, this physics prior is distilled into a diffusion model through synthetic trajectory generation and uncertainty-weighted physics constraints. The GP-dPHS captures Hamiltonian structure via learned energy gradients, while the diffusion model provides fast sampling capabilities. The framework handles 4-channel state representations (momentum and strain fields) and uses conformal calibration to ensure reliable uncertainty quantification. The approach combines flexible generative modeling with structure-aware representations, offering reliable predictions under data scarcity.

## Key Results
- 28% lower MSE than standard diffusion models on spatiotemporal forecasting tasks
- Maintains calibrated uncertainty bounds (95% coverage vs 90% target) through conformal prediction
- NeuralODE baselines fail to generalize under unseen initial conditions, often predicting constant fields
- Energy conservation preserved in predictions (non-increasing Hamiltonian over time)

## Why This Works (Mechanism)
PHDME works by combining the generalization power of physics-based priors with the sampling efficiency of diffusion models. The GP-dPHS provides a reusable representation of system dynamics that captures energy conservation without requiring explicit PDEs, while the diffusion model enables fast, amortized inference. The uncertainty-weighted physics residual loss ensures that the learned model respects physical constraints, and conformal calibration provides reliable uncertainty quantification. This hybrid approach overcomes the data scarcity problem while maintaining physical consistency.

## Foundational Learning
- **Port-Hamiltonian Systems:** Energy-based dynamical system formulation with structure-preserving properties; needed for modeling conservative systems without explicit PDEs; quick check: verify ∂tH ≤ 0 for learned dynamics
- **Gaussian Process Priors:** Non-parametric Bayesian models for learning system dynamics from sparse data; needed when explicit governing equations are unknown; quick check: monitor GP posterior variance for stability
- **Diffusion Models:** Generative models that denoise latents through a reverse process; needed for fast, amortized trajectory sampling; quick check: ensure denoising steps preserve physical constraints
- **Conformal Prediction:** Distribution-free uncertainty quantification method; needed for reliable uncertainty bounds without distributional assumptions; quick check: verify calibration coverage matches target confidence
- **Random Fourier Features:** Approximation method for scaling GP inference; needed to generate synthetic trajectories from GP posterior; quick check: ensure RFF samples preserve learned dynamics

## Architecture Onboarding
- **Component map:** GP-dPHS (learn physics) -> Trajectory Generator (RFF sampling) -> Diffusion Model (fast inference) -> Conformal Calibration (uncertainty)
- **Critical path:** Data → GP-dPHS → RFF samples → Diffusion training → Inference → Calibration
- **Design tradeoffs:** Physics accuracy vs. computational efficiency; explicit vs. implicit physical constraints; sample complexity vs. generalization
- **Failure signatures:** NeuralODE collapse (constant predictions), GP flattening (high variance regions), energy drift (growing Hamiltonian)
- **First experiments:** 1) Train GP-dPHS on simple sinusoidal initial condition and verify stable dynamics; 2) Vary λ_phys from 0 to 10 and measure physics constraint satisfaction; 3) Implement conformal calibration and verify coverage matches target confidence

## Open Questions the Paper Calls Out
None

## Limitations
- Requires careful hyperparameter tuning (λ_phys, λ_bc, learning rate, diffusion schedule) for optimal performance
- Computational cost of generating synthetic trajectories via Random Fourier Features scales with desired sample quality
- Performance depends on quality of initial sparse trajectory data and may struggle with highly nonlinear dynamics
- Exact U-Net architecture and boundary condition enforcement details are underspecified

## Confidence
- **High confidence:** Core theoretical framework combining GP-dPHS with diffusion models is sound and addresses a clear research gap
- **Medium confidence:** Quantitative results (28% MSE improvement) are plausible but depend on resolving implementation details
- **Low confidence:** Computational feasibility claims and exact performance metrics require complete specification of architecture and hyperparameters

## Next Checks
1. Implement GP-dPHS with assumed operator templates (J=[[0,∂x],[−∂x,0]], R=0) and verify learned energy gradients produce stable dynamics on sinusoidal initial conditions
2. Systematically vary λ_phys from 0 to 10 after training diffusion model, measuring MSE, energy conservation (∂tH ≤ 0), and prediction smoothness to quantify physics loss trade-off
3. Implement split conformal calibration with 100 samples per trajectory on held-out validation set, reporting actual coverage vs target 90% and analyzing miscalibration patterns