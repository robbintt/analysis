---
ver: rpa2
title: Explanation format does not matter; but explanations do -- An Eggsbert study
  on explaining Bayesian Optimisation tasks
arxiv_id: '2504.20567'
source_url: https://arxiv.org/abs/2504.20567
tags:
- explanation
- explanations
- participants
- task
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the impact of explanation formats on user
  performance in Bayesian Optimization (BO) tasks. Researchers created an accessible
  egg cooking scenario where participants tuned six parameters to achieve a perfect
  soft-boiled egg.
---

# Explanation format does not matter; but explanations do -- An Eggsbert study on explaining Bayesian Optimisation tasks

## Quick Facts
- **arXiv ID**: 2504.20567
- **Source URL**: https://arxiv.org/abs/2504.20567
- **Reference count**: 40
- **Primary result**: Any explanation format significantly improves Bayesian Optimization task success (29%→54%) compared to no explanations, with no significant differences between formats.

## Executive Summary
This study investigates how different explanation formats affect user performance in Bayesian Optimization tasks. Researchers created an accessible egg cooking scenario where participants tuned six parameters to achieve a perfect soft-boiled egg. Three explanation formats—visual bar charts, rule-based lists, and natural language descriptions—were compared against a no-explanation baseline in a between-subjects online study with 213 participants. Results showed that any explanation format significantly increased task success rate (from 29% to 54%), reduced trials needed, and improved understanding and trust without increasing task load. No significant differences were found between the three explanation formats, suggesting that explanation format choice can be based on user preference rather than effectiveness.

## Method Summary
The study used an accessible egg cooking scenario where participants had to tune six parameters (egg mass, fridge temperature, etc.) to achieve a perfect soft-boiled egg. A Gaussian Process surrogate model was pre-trained on the egg cooking function, and the TNTRules explainer method generated explanations by sampling the search space, clustering data, pruning by variance, and producing "Tune/No Tune" rules. Participants were randomly assigned to one of four conditions: Visual explanation (bar charts), Rule-based explanation (text lists), Natural Language explanation, or a No-explanation baseline. The study measured success rate (achieving the perfect egg), number of trials needed, task load (NASA-TLX), and user understanding/trust through surveys.

## Key Results
- Explanations significantly improved task success rate from 29% to 54%
- Any explanation format reduced the number of trials needed to succeed
- No significant differences in performance or task load between Visual, Rule-based, and Natural Language formats
- Explanations improved user understanding and trust without increasing perceived task load

## Why This Works (Mechanism)

### Mechanism 1: Search Space Reduction via Actionable Segmentation
Providing explanations improves task success primarily by constraining the user's search space to actionable parameter regions rather than improving their global understanding of the optimization function. The explanation methods binarize parameters into "Tune" and "No Tune" categories, filtering out low-sensitivity parameters and reducing the effective dimensionality of the problem for the human operator. This directs cognitive effort toward modifying high-impact parameters rather than searching the entire space.

### Mechanism 2: Format-Agnostic Cognitive Offloading
The cognitive benefit of explanations is derived from the semantic content (what to do), not the syntactic format (how it looks), provided the format meets a baseline threshold of clarity. The study found no statistical difference in performance or task load between Visual, Rule-based, and Natural Language formats. This suggests the bottleneck in human-in-the-loop BO is decision-uncertainty ("what do I change?"), not information-decoding speed. Once the "Tune/No Tune" signal is received, the format becomes irrelevant to the tuning outcome.

### Mechanism 3: Trust-Driven Adherence
Explanations serve as a trust signal that validates the system's competence, increasing the likelihood that users will adhere to recommendations rather than random exploration. Transparency acts as a trust heuristic. When users see the system's "reasoning" (even if simplified into rules), their "Propensity to Trust" increases. This reduces the noise injected by user hesitation or second-guessing.

## Foundational Learning

- **Concept: Bayesian Optimization (BO) Basics**
  - **Why needed here**: To understand that BO is a sequential, black-box optimization method used when the underlying function is unknown or expensive to evaluate.
  - **Quick check question**: Can you explain why BO uses a surrogate model (like a Gaussian Process) instead of directly optimizing the target function?

- **Concept: Surrogate Model Explainability (XBO)**
  - **Why needed here**: The paper explains the BO search space, not the physical egg itself. You must distinguish between explaining the physics and explaining the optimizer's belief state.
  - **Quick check question**: Does the TNTRules method explain why the egg cooks a certain way, or does it explain which parameters the optimizer believes are most promising to change?

- **Concept: Human-in-the-Loop Tuning Dynamics**
  - **Why needed here**: The study measures "trials to success." Understanding that the human is an active agent attempting to minimize a loss function (overcooked/undercooked egg) is central to the experimental design.
  - **Quick check question**: In this study, does the human select the next point to evaluate, or does the algorithm? (Answer: The human selects based on algorithmic suggestions).

## Architecture Onboarding

- **Component map**: Gaussian Process surrogate model -> TNTRules explainer -> Frontend interface with sliders and recommendation table
- **Critical path**: 1. Initialization: GP is pre-trained with scenario data. 2. Explanation Generation: TNTRules samples the GP to define valid "Tune" ranges. 3. User Action: User sees ranges, adjusts sliders, and submits. 4. Evaluation: Ground truth function calculates `t_cooked`. 5. Feedback: UI displays egg state.
- **Design tradeoffs**: Format flexibility vs. standardization suggests prioritizing implementation simplicity over complex visualizations for basic tuning tasks. The study used coarse "Tune/No Tune" buckets; highly granular explanations might increase cognitive load without proportional performance gains.
- **Failure signatures**: Gaming the midpoint (users might blindly select the midpoint of the "Tune" range), explanation-reality mismatch (if GP uncertainty is high, "Tune" ranges will be wide and unhelpful).
- **First 3 experiments**:
  1. Format Stress Test: Replicate with >10 parameters to see if Visual formats regain an advantage over Text due to scanning speed.
  2. Fidelity Stress Test: Deliberately degrade the explainer's accuracy to measure the break point of "Trust" and user performance.
  3. Feedback Latency: Test if the "instant feedback" loop is critical to the explanation's value by introducing a 5-minute delay.

## Open Questions the Paper Calls Out
The authors note that format equivalence may not hold in higher-dimensional spaces (>6 parameters) where visual scanning advantages could emerge. They also acknowledge that the explanation formats tested represent a narrow subset of possible approaches, and more sophisticated formats might show differential effects.

## Limitations
- The egg cooking scenario may not fully represent real-world BO complexity where evaluation costs are higher and parameter interactions are more intricate
- Format equivalence may not hold in higher-dimensional spaces (>6 parameters) where visual scanning advantages could emerge
- The explanation formats tested represent a narrow subset of possible approaches; more sophisticated formats might show differential effects

## Confidence
- **High confidence**: Explanations improve BO task success rates (29%→54%) and reduce trials. Large sample size (N=213) and pre-registered design strengthen this finding.
- **Medium confidence**: No format differences in performance. While statistically supported, this may be task-dependent and could vary with domain complexity.
- **Medium confidence**: Trust improvements from explanations. Self-reported measures are subject to response bias, though consistency across multiple scales supports the finding.

## Next Checks
1. **Dimensionality stress test**: Replicate the study with 15+ parameters to determine if visual formats regain performance advantages due to scanning efficiency.
2. **Fidelity impact study**: Systematically vary explanation accuracy (100%→50%→0%) to identify the break point where trust turns to mistrust and performance degrades.
3. **Domain transferability**: Test the format-agnostic finding in a real-world BO application (e.g., hyperparameter tuning) where evaluation costs are higher and stakes matter more than in a cooking simulation.