---
ver: rpa2
title: 'A Statistical Analysis for Per-Instance Evaluation of Stochastic Optimizers:
  How Many Repeats Are Enough?'
arxiv_id: '2503.16589'
source_url: https://arxiv.org/abs/2503.16589
tags:
- repeats
- number
- estimate
- error
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the critical question of how many experimental
  repeats are needed to reliably evaluate the performance of stochastic optimizers,
  particularly in the context of emerging hardware platforms where computational resources
  are limited. The authors present a statistical analysis of common performance metrics,
  developing guidelines for experiment design to measure optimizer performance with
  high confidence and accuracy.
---

# A Statistical Analysis for Per-Instance Evaluation of Stochastic Optimizers: How Many Repeats Are Enough?

## Quick Facts
- arXiv ID: 2503.16589
- Source URL: https://arxiv.org/abs/2503.16589
- Reference count: 0
- Primary result: Statistical framework for determining minimum experimental repeats needed to reliably evaluate stochastic optimizer performance

## Executive Summary
This paper addresses the fundamental question of how many experimental repeats are needed to reliably evaluate stochastic optimizers, particularly for emerging hardware platforms with limited computational resources. The authors develop a statistical framework that derives confidence intervals for success probability estimates and their impact on performance metrics like Computational Effort to Solution (CETS). They establish a lower bound on required repeats and propose an adaptive algorithm that dynamically adjusts repeat counts during experiments to achieve user-defined accuracy thresholds while minimizing computational waste.

## Method Summary
The methodology centers on estimating the success probability p_s of a stochastic optimizer through binomial proportion estimation (BPE), then propagating uncertainty to derived performance metrics like CETS. The core approach uses Agresti-Coull confidence intervals for success probability estimation, derives a lower bound L(p̂_s) on required sample size for a given relative error threshold e_T, and implements an adaptive algorithm that iteratively increases repeat counts until the bound is satisfied. The framework specifically addresses the non-linear error propagation that occurs when symmetric confidence intervals in success probability translate to highly asymmetric intervals in metrics involving logarithmic transformations.

## Key Results
- Using insufficient repeats can lead to up to 50% underestimation of CETS and incorrect conclusions about optimal hyperparameters
- Confidence intervals for optimized metrics can span multiple orders of magnitude with insufficient repeats
- The adaptive algorithm maintains relative estimation error below 10% thresholds across a wide range of success probabilities
- For WalkSAT on SAT instances, fixed repeat counts (100 vs 10,000) can flip the ranking of hyperparameters compared to well-sampled results

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Confidence intervals for success probability estimates shrink proportionally to $1/\sqrt{n}$, where $n$ is the number of independent experimental repeats.
- **Mechanism**: The true success probability $p_s(i)$ is estimated via binomial proportion estimation. As $n$ increases, the sampling distribution of $\hat{p}_s(i)$ tightens around the true value, reducing the margin of error.
- **Core Assumption**: Optimizer runs are independent and identically distributed Bernoulli trials for fixed problem instance and hyperparameters.
- **Evidence Anchors**: [abstract] and [section III A] provide theoretical foundation; corpus lacks direct validation of $1/\sqrt{n}$ relationship.
- **Break Condition**: Fails if runs are not independent or if underlying success probability drifts during experiment.

### Mechanism 2
- **Claim**: Uncertainty in success probability estimate propagates non-linearly to derived metrics like CETS, causing highly asymmetric confidence intervals.
- **Mechanism**: CETS calculation involves logarithmic transformation: $R_c(i) = \frac{\ln(1-c)}{\ln(1-p_s(i))}$. This non-linear mapping transforms symmetric CIs for $p_s(i)$ into highly asymmetric CIs for $R_c(i)$ and CETS.
- **Core Assumption**: The $R_c(i)$ formula accurately reflects computational effort required.
- **Evidence Anchors**: [section III B] demonstrates asymmetric CIs; corpus lacks analysis of non-linear error propagation.
- **Break Condition**: Invalid if underlying model for performance metric is incorrect.

### Mechanism 3
- **Claim**: An adaptive algorithm can dynamically determine required experimental repeats to achieve user-defined relative error threshold.
- **Mechanism**: Algorithm 1 starts with initial repeats, calculates lower bound $L(\hat{p}_s(i))$, and iteratively adds repeats until $n \ge L(\hat{p}_s(i))$, balancing reliability and computational cost.
- **Core Assumption**: Lower bound formula provides sufficiently tight estimate and success probability remains stable during adaptive process.
- **Evidence Anchors**: [section IV, Alg. 1] provides explicit pseudo-code; corpus lacks adaptive algorithm analysis.
- **Break Condition**: Will over/under-sample if lower bound is systematically loose/tight for given optimizer/problem combination.

## Foundational Learning

- **Concept**: **Binomial Proportion Estimation (BPE)**
  - **Why needed here**: Core statistical tool for estimating success probability $p_s(i)$ from limited optimizer runs, foundation for all derived metrics.
  - **Quick check question**: If optimizer finds solution in 7 out of 10 runs, what is point estimate for success probability? (Answer: 0.7)

- **Concept**: **Confidence Interval (CI)**
  - **Why needed here**: Single point estimate is noisy; CI provides range likely to contain true probability, giving reliability measure. Entire analysis rests on understanding CI width vs $n$.
  - **Quick check question**: 95% CI [4.5, 5.5] means what? (Answer: Colloquially, 95% chance true value in range; formally, 95% of calculated intervals would contain true value if experiment repeated.)

- **Concept**: **Propagation of Uncertainty**
  - **Why needed here**: Demonstrates symmetric CI for $p_s$ leads to highly asymmetric CI for derived $R_c$. Understanding non-linear transformation crucial for interpreting error bars on reported metrics.
  - **Quick check question**: If $y = 1/x$ and $x$ has symmetric CI, will $y$'s CI also be symmetric? (Answer: No. Non-linear relationship means small $x$ decrease causes larger $y$ increase than small $x$ increase.)

## Architecture Onboarding

- **Component map**: Experiment Runner -> Statistics Module -> Adaptive Controller
- **Critical path**: 1) Initialize with error threshold $e_T$, confidence level, initial repeats $n_{init}$; 2) Run optimizer for $n_{init}$ repeats; 3) Calculate initial $\hat{p}_s(i)$ and $L(\hat{p}_s(i))$; 4) Loop while $n_{current} < L(\hat{p}_s(i))$: run additional repeats, update estimates, recalculate bound; 5) Report final metric and CI.
- **Design tradeoffs**:
  - Agresti-Coull vs other CI methods: Paper uses Agresti-Coull for good boundary coverage and simplicity
  - Initial repeats ($n_{init}$): Larger $n_{init}$ stabilizes initial estimate but risks oversampling easy problems
  - Error Threshold ($e_T$): Smaller threshold yields higher precision but requires significantly more repeats
- **Failure signatures**:
  1. Oversampling for easy problems with high $p_s$
  2. Non-convergence for very low $p_s$ (< 0.01) where $L(\hat{p}_s(i))$ approaches infinity
  3. Incorrect hyperparameter selection when using too few repeats
- **First 3 experiments**:
  1. **Baseline Validation**: Implement Agresti-Coull CI and $L(\hat{p}_s(i))$ function; validate against synthetic data plots
  2. **Adaptive Loop Dry Run**: Run Algorithm 1 on synthetic problem with controlled true $p_s$ (0.1, 0.5, 0.9) to observe convergence
  3. **Real Optimizer Benchmark**: Apply full system to benchmark real stochastic optimizer (WalkSAT) on SAT instances; compare fixed vs adaptive approaches

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can analytical derivation of scaling function $s(\hat{p}_s(i))$ be developed to tighten lower bound on repeats for high success probabilities?
- **Basis in paper**: Authors note empirical methods can find $s(\hat{p}_s(i))$ value but rely on empirically determined lookup table.
- **Why unresolved**: Existing scaling factors (1.5, 2.5) derived from simulation rather than mathematical proof, potentially leaving bound looser than necessary.
- **What evidence would resolve it**: Closed-form expression for $s(\hat{p}_s(i))$ that provably maintains relative error threshold $e_T$ across all probabilities.

### Open Question 2
- **Question**: Does sequential stopping criterion in Algorithm 1 distort coverage probability of Agresti-Coull confidence interval?
- **Basis in paper**: Algorithm adaptively updates sample size $n$ and re-evaluates stopping condition, whereas Agresti-Coull interval derived for fixed-sample experiments.
- **Why unresolved**: Sequential testing often inflates Type I error rates or shrinks coverage, phenomenon not addressed in paper's derivation.
- **What evidence would resolve it**: Simulation analysis comparing nominal confidence level ($1-\alpha$) against empirical coverage rate with adaptive stopping rule.

### Open Question 3
- **Question**: How can adaptive framework be extended to efficiently identify optimal iteration count $i^*$ that minimizes CETS without exhaustive sampling?
- **Basis in paper**: Paper defines $CETS^{opt}$ as minimization over $i$, but Algorithm 1 only determines repeats for fixed pre-selected iteration count.
- **Why unresolved**: Finding global minimum requires evaluating curve of $CETS(i)$ values; running adaptive algorithm separately for each $i$ is computationally inefficient.
- **What evidence would resolve it**: Algorithm that shares information across different iteration cutoffs to simultaneously converge on $i^*$ and required $n$.

## Limitations

- Statistical assumptions of independent and identically distributed Bernoulli trials may not hold for real-world stochastic optimizers with state dependencies or non-stationarity
- Theoretical framework developed specifically for binary success/failure metrics and CETS-like performance metrics, with limited validation for other metric types
- Adaptive algorithm's computational overhead and convergence behavior for very low success probabilities remains incompletely characterized

## Confidence

- **High Confidence**: 1/√n relationship for binomial proportion estimation accuracy is well-established statistical theory
- **Medium Confidence**: Non-linear error propagation from symmetric to asymmetric CIs is theoretically sound but lacks comprehensive empirical validation
- **Low Confidence**: Adaptive algorithm's effectiveness across all optimizer/problem combinations, particularly edge cases, remains incompletely characterized

## Next Checks

1. **Synthetic Stress Test**: Systematically evaluate Algorithm 1 across synthetic success probabilities (0.01 to 0.99) and error thresholds (0.05 to 0.2) to characterize convergence behavior and identify failure modes, particularly edge cases where p_s approaches 0 or 1.

2. **Real-World Generalization**: Apply complete methodology to at least three diverse stochastic optimization domains (combinatorial optimization, continuous parameter tuning, reinforcement learning) with different performance metrics to validate general applicability of non-linear error propagation framework.

3. **Assumption Violation Impact**: Design experiments that deliberately violate IID assumption (stateful optimizers, non-stationarity) to quantify how much derived bounds degrade and identify breakdown scenarios.