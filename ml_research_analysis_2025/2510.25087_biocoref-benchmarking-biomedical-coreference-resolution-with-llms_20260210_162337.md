---
ver: rpa2
title: 'BioCoref: Benchmarking Biomedical Coreference Resolution with LLMs'
arxiv_id: '2510.25087'
source_url: https://arxiv.org/abs/2510.25087
tags:
- coreference
- biomedical
- resolution
- llama
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper benchmarks generative large language models (LLMs)
  on biomedical coreference resolution using the CRAFT corpus. Four prompting strategies
  were tested: local-only, reference context, abbreviation-aware, and entity-aware,
  against a SpanBERT baseline.'
---

# BioCoref: Benchmarking Biomedical Coreference Resolution with LLMs

## Quick Facts
- arXiv ID: 2510.25087
- Source URL: https://arxiv.org/abs/2510.25087
- Authors: Nourah M Salem; Elizabeth White; Michael Bada; Lawrence Hunter
- Reference count: 0
- Primary result: LLaMA 8B/17B with entity-aware prompting achieved highest F1 scores (0.740/0.678) on biomedical coreference resolution

## Executive Summary
This paper benchmarks generative large language models on biomedical coreference resolution using the CRAFT corpus. Four prompting strategies were tested: local-only, reference context, abbreviation-aware, and entity-aware, against a SpanBERT baseline. LLaMA 8B and 17B models achieved the highest F1 scores under entity-aware prompting (0.740 and 0.678, respectively), outperforming the larger 70B model. Pronoun coreference showed the strongest performance (F1 up to 0.975), while structured auxiliary inputs like abbreviation and entity dictionaries significantly improved recall. The study highlights that model size alone is insufficient; lightweight, domain-specific prompt engineering can substantially enhance LLM performance in biomedical coreference tasks.

## Method Summary
The study used the CRAFT corpus with 67 full-text biomedical articles. Four LLaMA models (8B, 17B, 70B, 405B) were evaluated using four prompting strategies: local-only (no auxiliary input), reference context (first paragraph included), abbreviation-aware (dictionary of abbreviations), and entity-aware (both abbreviation and entity dictionaries). Documents were chunked into 200-word paragraphs. GPT-4o extracted auxiliary dictionaries from the first 750 words of each document, validated against CRAFT annotations. The SpanBERT baseline used a discriminative approach with span-ranking. Evaluation measured precision, recall, and F1 scores with partial character overlap (â‰¥2 characters) as the matching criterion.

## Key Results
- LLaMA 8B and 17B models achieved highest F1 scores (0.740 and 0.678) under entity-aware prompting
- Larger 70B model underperformed smaller 8B/17B models on this task
- Pronoun coreference showed strongest performance (F1 up to 0.975)
- Entity-aware and abbreviation-aware prompting significantly improved recall over local-only approach
- SpanBERT baseline achieved only 0.132 F1, substantially lower than generative LLM approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured auxiliary inputs (abbreviation and entity dictionaries) improve recall in biomedical coreference resolution.
- Mechanism: Explicit domain grounding via dictionary injection reduces search space for antecedent resolution. When models receive validated abbreviation-definition pairs or entity lists, they bypass ambiguous surface-form matching and leverage curated semantic mappings instead of relying solely on implicit pretraining knowledge.
- Core assumption: The quality of dictionary extraction (GPT-4o on first 750 words, validated against CRAFT) accurately represents document-level entities.
- Evidence anchors:
  - [abstract] "structured auxiliary inputs like abbreviation and entity dictionaries significantly improved recall"
  - [section 6] "Both the abbreviation-aware and entity-aware settings demonstrated measurable gains in recall and F1, especially for the 8B model"
  - [corpus] Related work on coreference in RAG systems (arXiv:2507.07847) supports that disambiguation aids retrieval accuracy, though direct comparison to biomedical contexts is limited.
- Break condition: If dictionaries contain extraction errors or fail to cover novel entities introduced mid-document, grounding degrades and recall gains diminish.

### Mechanism 2
- Claim: Smaller LLMs (8B, 17B parameters) can outperform larger models (70B) on domain-specific coreference when combined with targeted prompting.
- Mechanism: Smaller models may generalize more conservatively, making fewer overconfident semantic leaps. Larger models, despite greater capacity, can overextend contextual associations and misresolve ambiguous mentions. Domain-specific prompts constrain the task sufficiently that lightweight models achieve better precision-F1 tradeoffs.
- Core assumption: Performance differences stem from model behavior under prompting constraints rather than infrastructure or inference variability.
- Evidence anchors:
  - [abstract] "LLaMA 8B and 17B models achieved the highest F1 scores... outperforming the larger 70B model"
  - [section 6] "smaller models generalize more conservatively and make fewer overconfident errors, whereas larger models may be more susceptible to prompt misalignment"
  - [corpus] No direct corpus evidence on inverse scaling in biomedical NLP; related coreference papers focus on accuracy improvements rather than model size effects.
- Break condition: If task complexity increases (e.g., multi-document coreference or no auxiliary dictionaries), larger models may regain advantage.

### Mechanism 3
- Claim: Pronoun coreference is substantially easier for LLMs than definite noun phrase or abbreviation resolution in biomedical texts.
- Mechanism: Pronouns rely on short-range syntactic cues and are well-represented in general-domain pretraining corpora. Biomedical abbreviations and definite NPs require domain-specific knowledge and longer context integration, which current prompting strategies only partially address.
- Core assumption: The lexical heuristics used to classify coreference types accurately reflect linguistic categories.
- Evidence anchors:
  - [abstract] "Pronoun coreference showed the strongest performance (F1 up to 0.975)"
  - [section 6] "pronoun coreference consistently achieved the highest F1 scores across all LLaMA models... likely due to the frequent occurrence of pronouns in pretraining corpora"
  - [corpus] Corpus evidence on pronoun vs. NP resolution is weak; related papers emphasize ambiguity broadly without type-specific breakdowns.
- Break condition: If pronouns reference entities introduced many paragraphs prior, performance would likely drop significantly.

## Foundational Learning

- Concept: Coreference Resolution
  - Why needed here: The task requires identifying all expressions referring to the same underlying entity across a document. Biomedical texts add complexity via identical surface forms (gene/protein names), heavy abbreviation use, and long-distance dependencies (23% of links exceed 500 words).
  - Quick check question: Given "The mice were treated with the compound. It showed rapid absorption," can you identify the two coreference chains and their likely antecedents?

- Concept: Prompt Engineering for Structured Tasks
  - Why needed here: The paper demonstrates that prompting strategy (local-only vs. entity-aware) affects F1 scores by up to 0.15 absolute. Understanding how to encode auxiliary information (dictionaries, context windows) into prompts is essential for replicating results.
  - Quick check question: How would you structure a prompt that asks an LLM to resolve abbreviations using a provided dictionary while excluding pronoun resolution?

- Concept: Span-Based vs. Generative Coreference Approaches
  - Why needed here: SpanBERT (discriminative, span-ranking) achieves only 0.132 F1, while generative LLMs reach 0.740+. Understanding the architectural difference (clustering embeddings vs. text generation with explicit resolution) explains when each approach is appropriate.
  - Quick check question: Why might a generative model outperform a span-based encoder on long-range coreference despite having no task-specific fine-tuning?

## Architecture Onboarding

- Component map: Document preprocessing -> Auxiliary extraction -> Prompt construction -> LLM inference -> Evaluation
- Critical path: 1. Chunk document into 200-word segments (critical for context window alignment) 2. Extract and validate auxiliary dictionaries from first 750 words (critical for entity-aware and abbreviation-aware experiments) 3. Run inference with experiment-specific prompt (determines which auxiliary inputs are injected) 4. Parse JSON output or fallback regex (affects evaluation accuracy)
- Design tradeoffs:
  - Local-only (Exp 1) vs. reference context (Exp 2): Adding first paragraph as reference reduces recall for 8B/70B, suggesting unstructured context can introduce noise.
  - Abbreviation vs. entity dictionaries: Entity dictionaries provide broader semantic grounding but may include irrelevant terms; abbreviation dictionaries are narrower but highly precise.
  - Model size vs. task specificity: 8B with entity-aware prompting outperforms 70B, indicating prompt quality can outweigh raw capacity for constrained tasks.
- Failure signatures:
  - Recall drops when reference paragraph is added (Exp 2) -> model attends to irrelevant context
  - F1 declines with 500-word chunks vs. 200-word chunks -> proximity alone insufficient, context integration fails
  - SpanBERT baseline at 0.132 F1 -> general-domain fine-tuning mismatches biomedical discourse
- First 3 experiments:
  1. Replicate local-only resolution (Exp 1) with LLaMA 8B on 5 CRAFT articles to establish baseline and validate chunking pipeline.
  2. Compare abbreviation-aware (Exp 3) vs. entity-aware (Exp 4) prompting on same articles, measuring recall delta to isolate dictionary contribution.
  3. Test distance-aware chunking (500-word segments per Table 4) to probe context window sensitivity and confirm performance decline pattern.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would a hybrid generative-extractive architecture perform compared to the standalone approaches benchmarked in this study?
- Basis in paper: [explicit] The Conclusion states future work should "explore fine-tuning strategies... and hybrid generative extractive systems to further enhance recall and robustness."
- Why unresolved: This paper strictly compares a discriminative baseline (SpanBERT) against generative LLMs (LLaMA) in isolation, without testing combined architectures.
- What evidence would resolve it: A system that uses SpanBERT for span extraction followed by an LLM for clustering/resolution, evaluated on the same CRAFT subset.

### Open Question 2
- Question: Why do smaller LLaMA models (8B/17B) consistently outperform the larger 70B model on biomedical coreference tasks?
- Basis in paper: [inferred] The Results section notes that smaller models outperform larger ones, hypothesizing that larger models may be susceptible to "prompt misalignment and semantic overreach," but this mechanism is not proven.
- Why unresolved: The paper establishes the counter-intuitive performance gap but does not conduct analysis (e.g., attention probing) to confirm if the issue is specific to domain data or prompt sensitivity.
- What evidence would resolve it: An ablation study analyzing attention patterns or confidence calibration across model scales when processing domain-specific terminology.

### Open Question 3
- Question: Can retrieval-augmented context effectively resolve the performance drop observed in long-distance coreference links?
- Basis in paper: [inferred] The paper notes performance sensitivity to "long-range context" and Figure 2 shows over 23% of coreference links span distances greater than the model's effective context window.
- Why unresolved: The methodology relies on fixed paragraph chunking (200-500 words), which physically separates antecedents from their mentions in the input.
- What evidence would resolve it: An experiment utilizing the full 10M-token context window of LLaMA 4 Scout or a RAG setup to retrieve distant antecedents for resolution.

## Limitations
- Dictionary extraction reliability: Auxiliary dictionaries generated from only first 750 words may miss entities introduced later in documents
- Prompt structure complexity: Significant variation across experiments introduces confounding factors beyond stated experimental variables
- Evaluation granularity: Partial character overlap metric may inflate performance scores for semantically incorrect resolutions

## Confidence
- High confidence: Pronoun coreference performance (F1 up to 0.975) - well-established in literature and consistently observed across experiments
- Medium confidence: Smaller models outperforming larger models with entity-aware prompting - observed pattern but may be sensitive to specific prompt construction and domain characteristics
- Low confidence: Dictionary extraction as the primary driver of recall improvements - limited validation beyond initial extraction step

## Next Checks
1. Extended dictionary validation: Re-run entity-aware experiments with dictionaries extracted from full documents rather than just first 750 words, measuring recall improvement for entities introduced mid-document.
2. Prompt ablation study: Systematically test each component of the entity-aware prompt (reference context, abbreviation list, entity list) independently to quantify individual contributions to performance gains.
3. Cross-corpus evaluation: Validate results on a biomedical corpus with different annotation conventions (e.g., PubMed abstracts with coreference annotations) to assess generalizability beyond CRAFT's specific domain and annotation style.