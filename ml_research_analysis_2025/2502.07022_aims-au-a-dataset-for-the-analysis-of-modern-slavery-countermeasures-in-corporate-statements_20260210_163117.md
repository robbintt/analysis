---
ver: rpa2
title: 'AIMS.au: A Dataset for the Analysis of Modern Slavery Countermeasures in Corporate
  Statements'
arxiv_id: '2502.07022'
source_url: https://arxiv.org/abs/2502.07022
tags:
- slavery
- statements
- modern
- reporting
- relevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AIMS.au, a dataset of 5,731 annotated modern
  slavery statements from the Australian Modern Slavery Register. It includes sentence-level
  annotations for 11 questions aligned with mandatory reporting criteria.
---

# AIMS.au: A Dataset for the Analysis of Modern Slavery Countermeasures in Corporate Statements

## Quick Facts
- arXiv ID: 2502.07022
- Source URL: https://arxiv.org/abs/2502.07022
- Reference count: 40
- Sentence-level annotations for 11 questions aligned with mandatory reporting criteria

## Executive Summary
This paper introduces AIMS.au, a dataset of 5,731 annotated modern slavery statements from the Australian Modern Slavery Register. The dataset includes sentence-level annotations for 11 questions aligned with mandatory reporting criteria, enabling machine learning models to detect relevant disclosures in corporate statements. Experiments show fine-tuned models significantly outperform zero-shot approaches, with Llama3.2 (3B) achieving the highest F1 scores when context is included. The dataset provides a valuable resource for automated compliance analysis in supply chain transparency.

## Method Summary
The paper develops a dataset for analyzing modern slavery disclosures in corporate statements. Researchers translated seven mandatory reporting criteria into eleven sentence-level classification questions. They annotated 5,731 statements from the Australian Modern Slavery Register, with sentences labeled as relevant or irrelevant for each criterion. The dataset includes training data (5,670 statements with basic criteria, 4,657 with advanced criteria), a validation set, and a test set. Models were fine-tuned on this data using BERT, DistilBERT, and Llama architectures, with context windows of up to 100 words. Performance was evaluated using Macro F1 scores across the 11 questions.

## Key Results
- Fine-tuned models significantly outperform zero-shot approaches for modern slavery disclosure detection
- Llama3.2 (3B) achieves highest F1 scores (0.721 Macro F1 with context) when context is included
- Context windows improve performance by providing semantic cues for disambiguation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuned smaller models can outperform larger models in zero-shot settings for specialized compliance detection tasks.
- Mechanism: The dataset provides high-quality, domain-specific sentence-level annotations that enable supervised fine-tuning. This specialized training signal allows smaller models (like Llama3.2 3B) to learn task-specific patterns more effectively than larger general-purpose models relying solely on prompt engineering.
- Core assumption: The annotation quality is sufficient to transfer domain knowledge without introducing excessive noise that would degrade model performance.
- Evidence anchors:
  - [abstract] "Experiments show fine-tuned models significantly outperform zero-shot approaches, with Llama3.2 (3B) achieving the highest F1 scores when context is included."
  - [section] Table 3 shows fine-tuned Llama3.2 (3B) achieves 0.721 Macro F1 with context, compared to 0.439 for GPT4o zero-shot (Table 2).
  - [corpus] Weak/no corpus evidence for this specific comparative performance claim on this dataset.

### Mechanism 2
- Claim: Contextual information improves the detection of relevant disclosures by reducing ambiguity in sentence-level classification.
- Mechanism: Providing surrounding text (up to 100 words balanced before/after the target sentence) gives the model additional semantic cues to disambiguate whether a sentence constitutes a substantive disclosure versus vague or distracting corporate language.
- Core assumption: The relevant context lies within the immediate ±50-word window; critical context is not located further away.
- Evidence anchors:
  - [section] Section 4 describes the "With context" setup: "we provide additional context by including up to 100 words balanced before and after the target sentence."
  - [section] Table 3 shows context improves Llama3.2 (3B) performance from 0.694 to 0.721 Macro F1.
  - [corpus] The ESGBench paper (arXiv:2511.16438) similarly uses supporting evidence/context for ESG QA, but doesn't directly validate this context window size.

### Mechanism 3
- Claim: Sentence-level annotation granularity enables scalable automated compliance analysis that statement-level approaches cannot achieve.
- Mechanism: Breaking down the Act's mandatory criteria into 11 granular questions answered via sentence-level extraction creates a precise supervision signal. This allows models to pinpoint specific relevant disclosures rather than making coarse statement-level judgments that may miss or misattribute information.
- Core assumption: The mandatory criteria can be decomposed into atomic sentence-level judgments without losing critical cross-sentence semantic dependencies.
- Evidence anchors:
  - [section] "We translated the seven mandatory content criteria into eleven questions designed to be answered by extracting relevant sentences within the context of the entire statement."
  - [section] Figure 4 shows the relevant sentence ratio per question, demonstrating the sparse nature of relevant information.
  - [corpus] The AIMSCheck paper (arXiv:2506.01671) extends this approach across jurisdictions, supporting the generalizability of the sentence-level detection paradigm.

## Foundational Learning

- Concept: **Transfer Learning / Fine-tuning for Domain Adaptation**
  - Why needed here: The task requires adapting general-purpose language models to the specialized domain of legal compliance in modern slavery reporting. The success of fine-tuned smaller models over zero-shot larger ones demonstrates that domain-specific training data is critical.
  - Quick check question: Can you explain why a fine-tuned 3B parameter model might outperform a zero-shot multi-hundred-billion parameter model on a specialized task?

- Concept: **Class Imbalance and F1 Score**
  - Why needed here: The dataset is highly imbalanced (Figure 4 shows most sentences are irrelevant). Accuracy would be a misleading metric; F1 score is chosen to properly evaluate performance on the minority positive class.
  - Quick check question: Why is accuracy a poor metric for this dataset, and what does a high F1 score tell you that accuracy does not?

- Concept: **Inter-Annotator Agreement (IAA) and Annotation Noise**
  - Why needed here: The dataset creation involved complex, subjective annotations. The paper explicitly discusses IAA scores and annotator omissions, which directly impacts model performance and the reliability of evaluations.
  - Quick check question: What does a high Inter-Annotator Agreement score indicate about the dataset's quality, and what are its limitations as discussed in the paper?

## Architecture Onboarding

- Component map:
  - PDF statements → PyMuPDF/ABBYY extraction → Sentence segmentation → Annotation specs → Human annotations → "Gold" validation/test sets
  - Pre-trained checkpoints (BERT, DistilBERT, Llama) → LoRA adapters (for Llama) / Full fine-tuning (BERT) → Binary classification head
  - Target sentence ± context (up to 100 words) → Tokenization → Batch assembly with fixed-size buffer
  - Sentence-level binary classification → F1 score per criterion → Macro F1 for overall performance on "gold" test sets

- Critical path:
  1. Sentence extraction and matching from PDFs (most fragile step due to OCR/formatting issues)
  2. Annotation quality assurance (IAA thresholds, "gold" set creation)
  3. Fine-tuning on noisy annotations (using union strategy for double-annotated data)
  4. Evaluation on high-quality "gold" test sets to ensure reliability

- Design tradeoffs:
  - Context window size (100 words) vs. memory/computation cost and potential for introducing noise. The paper selected 100 words; a larger window might capture more but cost more.
  - Sentence-level vs. Statement-level annotation: Chose sentence-level for precision and ML utility, but this increases annotation cost and complexity.
  - Union vs. Intersection for double annotations: Chose union to minimize omissions, accepting potential noise.
  - Excluding C7 ("any other relevant info"): Removed due to subjectivity, trading completeness for annotation consistency.

- Failure signatures:
  - High recall, low precision: Model predicts most sentences as relevant (observed in zero-shot Llama3.2)
  - Low recall on specific criteria: Annotator omissions in training data lead to missed relevant sentences (e.g., for "remediation")
  - Context window too small: Misclassification of sentences requiring distant context to resolve ambiguity

- First 3 experiments:
  1. Reproduce the "No context" baseline: Fine-tune BERT-base on the training split and evaluate on the provided "gold" test set. Verify you can achieve comparable F1 scores.
  2. Ablate context window size: Compare F1 scores with context windows of 0, 50, 100, and 200 words to find the optimal trade-off.
  3. Analyze annotator disagreement: Train models on the intersection vs. union of double-annotated data to quantify the impact of annotation noise on model performance.

## Open Questions the Paper Calls Out
None

## Limitations

- Dataset Availability and Reproducibility: The dataset is currently unavailable pending acceptance, creating a fundamental barrier to verification. The exact sentence matching algorithm between PDF extraction and annotations is not fully specified (distance threshold unknown), and LoRA configuration details for Llama models are omitted.
- Annotation Quality and Domain Expertise: The dataset contains known omissions where relevant sentences were missed during annotation, particularly for complex criteria like remediation. This annotation noise directly impacts model training quality and evaluation reliability.
- Context Window Design Choice: The 100-word context window selection lacks empirical justification through ablation studies. Complex corporate statements often use section headers, footnotes, or cross-references that may require document-level understanding beyond the chosen window.

## Confidence

- Fine-tuned models significantly outperform zero-shot approaches: High confidence
- Llama3.2 (3B) achieves highest F1 scores with context: High confidence
- Sentence-level annotation granularity enables scalable automated compliance analysis: Medium confidence

## Next Checks

1. Context Window Ablation Study: Systematically evaluate model performance across multiple context window sizes (0, 50, 100, 200 words) to empirically determine the optimal trade-off between performance gains and computational cost.

2. Annotation Noise Impact Analysis: Train separate models using intersection vs. union strategies for double-annotated data to quantify the performance impact of annotation omissions. Compare these results against the reported union-based training to measure how annotation noise propagates through the pipeline.

3. Cross-Jurisdictional Generalization Test: Apply the trained models to modern slavery statements from other jurisdictions (UK, US, EU) that may have different reporting requirements and disclosure patterns to validate whether the Australian-specific training generalizes to broader modern slavery compliance detection.