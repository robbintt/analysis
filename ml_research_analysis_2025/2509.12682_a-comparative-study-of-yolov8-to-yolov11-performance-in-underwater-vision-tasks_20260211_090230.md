---
ver: rpa2
title: A Comparative Study of YOLOv8 to YOLOv11 Performance in Underwater Vision Tasks
arxiv_id: '2509.12682'
source_url: https://arxiv.org/abs/2509.12682
tags:
- yolo
- yolov8
- underwater
- each
- yolov11
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compares the performance of four YOLO models (YOLOv8
  to YOLOv11) on underwater object detection tasks using two marine datasets: Coral
  Disease (4,480 images, 18 classes) and Fish Species (7,500 images, 20 classes).
  Models were trained with four different data splits (25%, 50%, 75%, 100%) and evaluated
  on precision, recall, mAP50, mAP50-95, inference time, and FPS.'
---

# A Comparative Study of YOLOv8 to YOLOv11 Performance in Underwater Vision Tasks

## Quick Facts
- arXiv ID: 2509.12682
- Source URL: https://arxiv.org/abs/2509.12682
- Reference count: 39
- Models tested: YOLOv8-s to YOLOv11-s on Coral Disease (4,480 images, 18 classes) and Fish Species (7,500 images, 20 classes) datasets

## Executive Summary
This study provides the first controlled comparison of recent YOLO versions (v8-v11) on underwater object detection tasks. Using two marine datasets, the authors evaluate model performance across different training data splits and metrics. Results show that while inference speed improves markedly with newer YOLO versions, accuracy gains saturate after YOLOv9 in underwater settings. YOLOv10 emerges as offering the best speed-accuracy tradeoff for embedded AUV deployment, while Grad-CAM analysis reveals limitations in explainability for complex multi-task detection models.

## Method Summary
The study trains four YOLO model variants (v8-s through v11-s) on two underwater datasets: Coral Disease (4,480 images, 18 classes) and Fish Species (7,500 images, 20 classes). Models are trained from scratch with 100 epochs, 640px input resolution, and four different training data splits (25%, 50%, 75%, 100%). Evaluation uses standard detection metrics including precision, recall, mAP@50, mAP@50-95, inference time, and FPS. Grad-CAM explainability is applied post-hoc to analyze model attention patterns.

## Key Results
- Accuracy improvements across YOLO versions are minimal in underwater settings, with mAP50-95 differences of <0.03 between YOLOv8 and YOLOv11 at 100% training data
- YOLOv10 achieves the best speed-accuracy tradeoff with 43.9 FPS and lowest GFLOPs (25.1), outperforming YOLOv11 despite being optimized for efficiency
- Inference speed improves markedly across versions (21→44 FPS from v8→v11), but GFLOPs reduction (28.8→21.7) primarily drives efficiency gains
- Grad-CAM analysis reveals models remain susceptible to noise and background elements, highlighting limitations in explainability for multi-task detection models

## Why This Works (Mechanism)

### Mechanism 1
Architectural innovations in YOLOv8–v11 primarily improve inference efficiency, not accuracy, when applied to underwater imagery. Newer versions reduce computational overhead through anchor-free heads, NAS-optimized backbones, and lightweight transformers. These optimizations decrease GFLOPs and improve FPS, but feature extractors remain tuned to terrestrial patterns that do not generalize to underwater degradation modes like turbidity and color shift.

### Mechanism 2
YOLOv10 provides the optimal speed-accuracy tradeoff for AUV deployment due to NAS-optimized edge efficiency. YOLOv10 uses Neural Architecture Search to prune redundant layers and optimize the neck, achieving the lowest GFLOPs and highest FPS in this study. YOLOv11 adds attention mechanisms that increase parameter count without proportional accuracy gains in underwater settings.

### Mechanism 3
Grad-CAM explainability degrades for multi-task detection models because gradient aggregation conflates localization and classification signals. In YOLO, each grid cell predicts bounding boxes and class probabilities simultaneously, causing gradient signals to disperse across spatial and classification pathways. This produces heatmaps that highlight background noise rather than target objects.

## Foundational Learning

- **One-stage vs. two-stage object detection**: YOLO is a one-stage detector chosen for AUVs due to latency constraints versus accuracy of two-stage detectors like Faster R-CNN. Quick check: Why does a one-stage detector sacrifice some accuracy for speed compared to a two-stage detector?

- **IoU thresholds and mAP metrics**: The paper reports mAP@50 and mAP@50-95; understanding these metrics is essential to interpret minimal differences between YOLO versions. Quick check: What does mAP@50-95 tell you about localization precision that mAP@50 does not?

- **Grad-CAM for explainability**: The paper uses Grad-CAM to probe model attention; understanding its limitations for multi-task models explains contradictory results. Quick check: Why does Grad-CAM, designed for classification, struggle with object detection models that predict both boxes and classes?

## Architecture Onboarding

- **Component map**: Input image (640×640) → Backbone feature extraction → Neck feature fusion → Head (box regression + classification) → NMS → Detections
- **Critical path**: Input image (640×640) → Backbone feature extraction → Neck feature fusion → Head (box regression + classification) → NMS → Detections
- **Design tradeoffs**:
  - Anchor-free (v8/v10/v11) vs. Hybrid (v9): Anchor-free simplifies architecture but may struggle with extreme aspect ratios
  - NAS-optimized (v10) vs. Attention-based (v11): NAS reduces GFLOPs; attention captures global context but adds overhead
  - GFLOPs vs. Accuracy: Lower GFLOPs (v10: 25.1) improve FPS; higher parameters (v11: 9.4M) do not guarantee mAP gains in underwater domains
- **Failure signatures**:
  - Accuracy saturation across versions: Indicates architectural innovations are not domain-adapted; consider underwater-specific pretraining
  - Grad-CAM highlighting background: Suggests model relies on spurious features; may indicate overfitting to noise or insufficient domain augmentation
  - FPS drop from v10→v11: Attention mechanisms add latency without accuracy benefit; avoid v11 for strict edge constraints
- **First 3 experiments**:
  1. Baseline replication: Train YOLOv8–v11 on Coral and Fish datasets with 100% data; verify mAP@50-95 and FPS match reported values (~0.70–0.80 mAP, 21–44 FPS)
  2. Data ablation: Train with 25%/50%/75% splits to confirm accuracy scales with data but version-to-version differences remain minimal
  3. Domain augmentation test: Apply underwater-specific augmentations (color shift, turbidity simulation, haze reduction) to YOLOv10; measure mAP improvement to test if accuracy saturation is data- or architecture-limited

## Open Questions the Paper Calls Out

### Open Question 1
How can explainability metrics be adapted to accurately assess complex, multi-task object detectors like YOLO, given current contradictions between Grad-CAM heatmaps and standard performance metrics? The study found Grad-CAM often highlights background noise and spurious features, failing to align with quantitative improvements in precision and recall.

### Open Question 2
Why does YOLOv11-s underperform YOLOv10-s in inference speed (FPS) on underwater tasks despite architectural optimizations designed for efficiency? Tables show YOLOv10-s consistently achieving higher FPS than YOLOv11-s, contradicting expectations of improvement.

### Open Question 3
Does the observed accuracy saturation between YOLO versions persist when utilizing larger model variants (e.g., 'medium' or 'large') or different input resolutions? The study was restricted to 's' (small) versions and 640px input size, leaving scaling behavior in underwater domains untested.

## Limitations
- Relatively small validation/test sets (20 samples per class) may not provide robust statistical significance for model comparisons
- Focus on small YOLO variants ("s" models) limits generalizability to larger model families
- Grad-CAM analysis for YOLO models remains exploratory due to technique's design for single-task classification rather than multi-task detection

## Confidence
- **High Confidence**: Efficiency improvements across YOLO versions (mAP saturation after v9, FPS improvements from v8 to v11) are well-supported by quantitative metrics
- **Medium Confidence**: YOLOv10 provides optimal speed-accuracy tradeoff for AUV deployment is supported by FPS measurements but requires validation across diverse underwater conditions
- **Medium Confidence**: Grad-CAM limitations for multi-task models are theoretically sound but lack direct corpus evidence specific to YOLO architectures

## Next Checks
1. Replicate the study with expanded validation sets (minimum 50 samples per class) to verify statistical robustness of mAP differences
2. Test YOLOv10's speed-accuracy tradeoff across additional marine datasets with varying turbidity and lighting conditions to confirm AUV deployment suitability
3. Develop and validate task-specific explainability methods for YOLO models that separate box regression and classification gradients to improve Grad-CAM fidelity