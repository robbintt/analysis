---
ver: rpa2
title: Time-series surrogates from energy consumers generated by machine learning
  approaches for long-term forecasting scenarios
arxiv_id: '2506.20253'
source_url: https://arxiv.org/abs/2506.20253
tags:
- data
- power
- consumption
- figure
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a comparative evaluation of generative AI methods
  for creating synthetic long-term electricity consumption time series at the household
  level. The goal is to produce realistic, anonymized surrogates that can replace
  scarce real data for applications like state estimation and grid planning.
---

# Time-series surrogates from energy consumers generated by machine learning approaches for long-term forecasting scenarios

## Quick Facts
- arXiv ID: 2506.20253
- Source URL: https://arxiv.org/abs/2506.20253
- Reference count: 40
- Primary result: WGAN and MABF outperform DDPM and HMM in generating realistic synthetic household electricity consumption profiles with better consumer-type preservation

## Executive Summary
This study evaluates four generative AI methods for creating synthetic long-term electricity consumption time series at the household level, targeting applications like state estimation and grid planning where real data may be scarce or privacy-sensitive. Using a German household dataset with 15-minute resolution, the authors compare a hybrid Wasserstein GAN (WGAN), Denoising Diffusion Probabilistic Model (DDPM), Hidden Markov Model (HMM), and Masked Autoregressive Bernstein polynomial normalizing Flows (MABF) against a simple persistence baseline. Results show that WGAN and MABF perform best across multiple metrics including MAE, RMSE, MAPE, MMD, Pearson correlation, and SSIM, while also best preserving daily patterns, seasonal variations, and consumer-type distributions. The synthetic data maintains high fidelity while ensuring privacy, making it suitable for long-term energy forecasting and system modeling.

## Method Summary
The study uses the OpenMeter dataset of ~550 German households with 15-minute resolution, filtering for consumers with ≥2.5 years of data and ≤5% missing values. Data is split into 1.5-year training and 1-year test sets, with min-max normalization applied. Consumer types are clustered using PCA on typical weeks followed by k-means++ (k=15). Four generative models are implemented: WGAN with LSTM generator and feed-forward discriminator using conditional inputs (time embeddings, cluster ID, temperature); DDPM with 1D U-Net noise predictor; HMM with 50 states per weekday-season combination; and MABF with autoregressive normalizing flows using Bernstein polynomials and MADE architecture. Synthetic profiles are generated for 1-year periods and evaluated against real test data using error metrics (MAE, RMSE, MAPE), distributional metrics (MMD), structural metrics (SSIM, Pearson), UMAP visualization, and consumer-type distribution preservation via Hungarian matching.

## Key Results
- WGAN and MABF outperform DDPM and HMM across multiple metrics including lower MAE, RMSE, MAPE, and MMD, as well as higher Pearson correlation and SSIM
- These models best preserve daily patterns, seasonal variations, and consumer-type distributions in synthetic profiles
- DDPM and HMM generate more diverse profiles but show bias toward dominant consumer types and overestimate consumption
- The simple persistence baseline performs worst, underscoring the need for advanced generative models

## Why This Works (Mechanism)

### Mechanism 1
Conditional adversarial training with LSTM generators preserves temporal dynamics and consumer-type distributions better than unconditional generation. The hybrid WGAN uses Wasserstein distance with gradient penalty as a stable training objective. The LSTM generator captures sequential dependencies in 15-minute resolution consumption data, while the feed-forward discriminator receives conditional inputs (time embeddings, consumer cluster ID, temperature) via learned embedding layers. The adversarial game forces the generator to produce profiles that are indistinguishable from real household patterns across all conditional contexts.

Core assumption: Energy consumption exhibits learnable conditional dependencies on temporal, seasonal, and consumer-type features that can be captured by the embedding and LSTM architecture.

Evidence anchors:
- [abstract] "hybrid Wasserstein GAN (WGAN)... MABF perform best across multiple metrics, including lower MAE, RMSE, MAPE, and MMD, as well as higher Pearson correlation and SSIM"
- [section 3.1] "The generator is implemented by utilizing a Long Short-Term Memory (LSTM) network to capture temporal dependencies... The discriminator is designed as a feed-forward neural network."
- [corpus] Weak direct corpus support; neighbor papers focus on forecasting rather than synthetic generation. "Leveraging Hypernetworks and Learnable Kernels for Consumer Energy Forecasting Across Diverse Consumer Types" addresses consumer-type differentiation but not GAN-based synthesis.

Break condition: Mode collapse where generator produces limited variety of profiles; discriminator saturation providing no useful gradients; training instability visible in oscillating loss curves.

### Mechanism 2
Denoising Diffusion Probabilistic Models capture full data distributions without explicit temporal factorization, generating diverse samples but with bias toward dominant consumer types. DDPMs progressively add Gaussian noise to real consumption trajectories (forward process), then learn to reverse this degradation via a 1D U-Net noise predictor. The iterative denoising process (typically hundreds of steps) reconstructs samples from pure noise. Unlike autoregressive models, DDPMs model the joint distribution directly without sequential dependency assumptions, which allows coherent long-range synthesis but can overgenerate samples from high-density regions of training data.

Core assumption: The Markovian noise degradation process is reversible and the learned denoising function generalizes to generate novel samples beyond training distribution.

Evidence anchors:
- [abstract] "DDPM and HMM generate more diverse profiles but show bias toward dominant consumer types and overestimate consumption"
- [section 3.2] "DDPMs provide a principled way of modeling the full data distribution without relying on strict temporal factorization. This allows them to generate diverse and coherent samples over extended horizons"
- [corpus] No direct corpus evidence for DDPMs in energy consumption synthesis; this appears to be a novel application in this domain.

Break condition: Overestimation of consumption values; generation of implausible negative power values; creation of sub-clusters with no correspondence to real daily patterns (visible in UMAP analysis).

### Mechanism 3
Autoregressive normalizing flows with Bernstein polynomial transformations provide stable density estimation while enforcing positive-domain constraints critical for power consumption data. MABF factorizes the joint distribution via chain rule, modeling each time step conditionally on all previous values. Bernstein polynomials provide flexible, monotonic transformations with guaranteed numerical stability. The MADE (Masked Autoencoder for Distribution Estimation) architecture enforces autoregressive dependencies. A chained transformation (scale → Bernstein polynomial → log) ensures outputs remain in the positive domain, directly addressing the physical constraint that power consumption cannot be negative.

Core assumption: The conditional distribution at each time step can be adequately modeled by a parametric transformation from a base Gaussian distribution.

Evidence anchors:
- [abstract] "WGAN and MABF perform best across multiple metrics"
- [section 3.4] "only MABF is correctly modeling the truncated strictly positive distribution of the data"
- [section 8.6.4] "We chain three transformations h = log ◦ h2 ◦ h1: (1) a scale term... (2) flexible Bernstein polynomial... (3) logarithmic transform, to ensure the resulting distribution is fixed to the positive domain"
- [corpus] No corpus evidence for Bernstein polynomial normalizing flows in energy applications; represents a novel contribution.

Break condition: Overestimation of peak loads; underestimation of morning consumption; numerical instability if Bernstein polynomial order is insufficient.

## Foundational Learning

- Concept: Wasserstein distance and gradient penalty in GANs
  - Why needed here: Standard GAN loss (Jensen-Shannon divergence) provides vanishing gradients when discriminator saturates. Wasserstein distance provides meaningful gradients even in saturated regimes, while gradient penalty enforces the 1-Lipschitz constraint more effectively than weight clipping.
  - Quick check question: Can you explain why enforcing the 1-Lipschitz constraint on the discriminator/critic is necessary for tractable Wasserstein distance computation?

- Concept: Autoregressive factorization and MADE architecture
  - Why needed here: MABF and similar models must ensure each time step depends only on preceding values. The MADE architecture uses masked connections to enforce this autoregressive property efficiently in a single forward pass, rather than requiring sequential computation.
  - Quick check question: How does masking in MADE differ from standard autoregressive RNN computation in terms of training efficiency?

- Concept: Diffusion models: forward/reverse processes
  - Why needed here: DDPMs require understanding the trade-off between sample quality and computational cost. More denoising steps improve quality but increase inference time. The noise schedule (how much noise is added per step) critically affects both training stability and sample fidelity.
  - Quick check question: Why does the iterative denoising process in DDPMs typically require hundreds of steps at inference time, and what computational burden does this create?

## Architecture Onboarding

- Component map:
  Data pipeline: OpenMeter dataset → min-max scaling → train/test split (1.5yr/1yr) → 96-timestep daily samples (15-min resolution) → time/temperature embeddings
  Consumer typing module: PCA on typical weeks → k-means++ clustering (k=15) on PC1-5 → cluster assignments used as conditional inputs
  Model implementations: WGAN (LSTM generator + FFN discriminator + condition blocks), DDPM (1D U-Net noise predictor), HMM (50 states per weekday-season combination), MABF (MADE + Bernstein polynomials + covariate conditioners)
  Evaluation pipeline: Error metrics (MAE, RMSE, MAPE) + distributional metrics (MMD) + structural metrics (SSIM, Pearson) + UMAP visualization + consumer-type preservation analysis

- Critical path:
  1. Data preprocessing with strict quality filters (≥2.5 years, ≤5% NaN, implausible value removal)
  2. Consumer clustering on training data only to prevent leakage
  3. Embedding layer training for categorical variables (cluster-id, sensor-id)
  4. Model-specific training with conditional inputs
  5. Generation of year-long synthetic profiles
  6. Multi-metric evaluation comparing synthetic profiles to held-out test data

- Design tradeoffs:
  - WGAN: Best consumer-type preservation and distributional fidelity, but adversarial training instability risk; requires careful discriminator-generator balance (5 critic iterations per generator iteration used)
  - DDPM: Best seasonality capture and sample diversity, but overestimates consumption and generates negative values; high inference cost due to iterative denoising
  - MABF: Only model respecting positive-domain constraint; likelihood-based training is stable; but peak-load overestimation persists
  - HMM: Simplest and most interpretable; fast inference; but poor at capturing daily fluctuations and consumer heterogeneity

- Failure signatures:
  - Negative power values (DDPM, HMM, WGAN): indicates distribution boundary not enforced
  - Consumer-type collapse (DDPM, HMM): model generates only dominant cluster types; check cluster assignment frequencies
  - Seasonality loss: daily patterns preserved but winter/summer magnitude differences flattened
  - Mode collapse in GAN: low sample diversity visible in UMAP as tight clusters rather than covering the real data manifold
  - Peak-load overestimation: evening consumption systematically higher than real data

- First 3 experiments:
  1. Reproduce consumer clustering pipeline: Apply PCA to typical weeks from a subset of the OpenMeter data, cluster with k-means++ (k=15), and verify that cluster centroids show interpretable consumption patterns (e.g., morning-peakers vs. evening-peakers).
  2. Train a baseline MABF on single-day samples (96 timesteps): Start with D=48 (30-minute intervals) to reduce computational cost. Verify that the chained transformation (scale→Bernstein→log) produces only positive values and that the MADE masking correctly enforces autoregressive dependencies.
  3. Ablate conditional inputs in WGAN: Train one model with only time embeddings, another with time + temperature, and a third with full conditioning (time + temperature + cluster-id). Compare consumer-type preservation rates to isolate which conditions most improve fidelity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Hybrid-Transformer architectures or federated learning approaches improve synthetic load profile generation compared to the evaluated WGAN, DDPM, HMM, and MABF models?
- Basis in paper: [explicit] "Future research should implement the latest advances in machine learning like Hybrid-Transformers or federated learning"
- Why unresolved: The study limited comparison to four generative architectures; newer approaches were not evaluated despite rapid advances in sequence modeling.
- What evidence would resolve it: Comparative benchmarking of Hybrid-Transformers and federated learning models on the same OpenMeter dataset using the proposed evaluation metrics (MAE, RMSE, MMD, consumer-type preservation).

### Open Question 2
- Question: How will emerging "New Power Consumers" (EVs, heat pumps, battery storage, electric heating) affect the ability of generative models to produce realistic household load profiles?
- Basis in paper: [explicit] "Particularly with the rise of 'New Power Consumers' (NPCs), such as electric heating or heat pumps, electric vehicles (EVs), or battery storage will provide even more heterogeneity among prosumers and might cause new challenges for ML models in the future."
- Why unresolved: The current dataset and models were developed without substantial EV or heat pump penetration; the impact of these high-variance loads on synthetic profile fidelity is unknown.
- What evidence would resolve it: Re-training and evaluating models on household data with known EV/heat pump adoption, measuring whether synthetic profiles capture their distinct consumption patterns.

### Open Question 3
- Question: How can the systematic bias toward dominant consumer types in DDPM and HMM models be mitigated to ensure adequate representation of minority consumption patterns?
- Basis in paper: [inferred] Results show "HMM- and DDPM-model exhibited a bias as effectively these ML-models predominantly generated surrogates for the most populated clusters," while WGAN and MABF preserved diversity better. No solution is proposed.
- Why unresolved: The paper identifies but does not address this mode-collapse-like behavior in diffusion and Markov-based approaches.
- What evidence would resolve it: Testing class-balanced sampling, conditional generation with explicit minority-type conditioning, or loss modifications that penalize cluster imbalance.

### Open Question 4
- Question: What is the formal privacy guarantee of the generated synthetic profiles against re-identification or membership inference attacks?
- Basis in paper: [inferred] The paper claims synthetic data "fulfills criteria like anonymisation - preserving privacy concerns mitigating risks of specific profiling of single customers" but provides no quantitative privacy analysis. The correlation analysis showed some synthetic profiles correlated with multiple real consumers of the same type.
- Why unresolved: No differential privacy mechanisms, membership inference tests, or re-identification experiments were conducted.
- What evidence would resolve it: Formal privacy audits using membership inference attacks, k-anonymity analysis, or integration of differential privacy with measurement of privacy-utility tradeoffs.

## Limitations

- The OpenMeter dataset covers only German households with 15-minute resolution, limiting applicability to other regions or time granularities
- Model architectures and hyperparameters are partially unspecified, requiring assumptions for reproduction
- The evaluation focuses on statistical fidelity and consumer-type preservation but does not assess downstream task performance (e.g., actual state estimation accuracy using synthetic data)

## Confidence

- High confidence: Comparative model performance rankings (WGAN and MABF outperform DDPM and HMM across multiple metrics), consumer-type preservation analysis, and the core finding that synthetic data maintains high fidelity while ensuring privacy
- Medium confidence: Specific hyperparameter choices and architectural details, as these are partially unspecified in the paper and require assumptions for reproduction
- Low confidence: Generalization to other datasets, time resolutions, or geographic regions, and the absence of downstream task validation (e.g., state estimation performance using synthetic data)

## Next Checks

1. Validate consumer clustering pipeline on a held-out subset of OpenMeter data to ensure clusters represent meaningful consumption patterns rather than artifacts
2. Conduct ablation studies removing conditional inputs (cluster-id, temperature) from WGAN to quantify their contribution to consumer-type preservation
3. Evaluate synthetic data performance on an actual state estimation task to verify practical utility beyond statistical fidelity metrics