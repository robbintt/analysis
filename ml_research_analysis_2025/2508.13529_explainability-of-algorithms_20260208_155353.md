---
ver: rpa2
title: Explainability of Algorithms
arxiv_id: '2508.13529'
source_url: https://arxiv.org/abs/2508.13529
tags:
- trust
- methods
- understanding
- they
- opacity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper identifies two distinct forms of algorithmic opacity:
  legal opacity arising from proprietary restrictions on accessing model details,
  and epistemic opacity resulting from the inherent complexity of deep neural networks
  that makes their inner workings incomprehensible to humans. The author argues that
  local post-hoc interpretability methods like LIME and SHAP have significant limitations
  including fragility to small perturbations, lack of precision, and inability to
  provide genuine counterfactual understanding without structural information.'
---

# Explainability of Algorithms

## Quick Facts
- arXiv ID: 2508.13529
- Source URL: https://arxiv.org/abs/2508.13529
- Authors: Andrés Páez
- Reference count: 0
- Key outcome: Current XAI methods face fundamental limitations, and the relationship between explainability and trust is more complex than commonly assumed

## Executive Summary
This paper provides a conceptual analysis of algorithmic opacity and explainability, distinguishing between legal opacity (access restrictions) and epistemic opacity (inherent complexity). The author argues that local post-hoc interpretability methods like LIME and SHAP have significant limitations including fragility to small perturbations, lack of precision, and inability to provide genuine counterfactual understanding without structural information. While global methods using surrogate models offer some improvement, they face trade-offs between accuracy and interpretability. The analysis shows that explainability does not reliably build trust in AI systems, with empirical evidence both supporting and contradicting this common assumption.

## Method Summary
The paper is a conceptual/philosophical analysis with literature review methodology. No original data or experiments were conducted. The author synthesizes findings from cited XAI literature to support analytical claims about the limitations of explanation methods. The approach involves examining technical properties of local and global XAI methods, analyzing empirical studies on trust-explainability relationships, and providing conceptual frameworks for understanding different forms of opacity.

## Key Results
- Local post-hoc interpretability methods (LIME, SHAP) are fragile to small perturbations and lack causal grounding
- Surrogate models provide objectual understanding but face accuracy-interpretability trade-offs
- Explainability does not reliably build trust in AI systems; the relationship is empirically mixed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local post-hoc interpretability methods provide limited explanatory power due to inherent fragility and lack of causal grounding.
- Mechanism: These methods work by systematically perturbing inputs to determine which features most influence a given output. However, small input transformations or sampling variations can generate substantially different explanations, and the methods identify correlations rather than causal relationships.
- Core assumption: Feature importance rankings from perturbation-based methods reflect meaningful model behavior rather than artifacts of the explanation method itself.
- Evidence anchors:
  - [Section 2, Page 6-7]: "LIME and SHAP are vulnerable to adversarial attacks, such as perturbations that produce perceptively indistinguishable inputs that are assigned the same predicted label yet result in very different interpretations."
  - [Section 3, Page 10]: "By tinkering with the input, users can only establish piecemeal correlations that cannot be generalised in any obvious way."
  - [corpus]: The meta-analysis "Is Trust Correlated With Explainability in AI?" examines the assumption that explainability inherently boosts user trust, suggesting the relationship requires more rigorous examination.
- Break condition: When explanation methods produce inconsistent outputs for nearly identical inputs, or when explanations cannot support actionable counterfactual reasoning about model behavior.

### Mechanism 2
- Claim: Surrogate models enable objectual understanding by providing interpretable functional structure, but face accuracy-interpretability trade-offs.
- Mechanism: Simple models (decision trees, linear models) approximate the opaque model's behavior by extracting general rules or visible decision paths. This provides the "functional scaffolding" required for counterfactual reasoning that local methods lack.
- Core assumption: A simplified model can capture sufficient functional relationships of the complex model to be practically useful without introducing misleading simplifications.
- Evidence anchors:
  - [Section 2, Page 8]: "Simple decision trees, rule lists, example-based methods, and even dialogical explanations will perform much better in this kind of task."
  - [Section 3, Page 10]: "Surrogate models provide general rules that have been mined from the data or extracted directly from the model, thus providing the underlying functional scaffolding required to reason counterfactually."
  - [corpus]: Related work on LLM explainability benchmarks (BELL) notes that understanding decision-making processes is critical for trust, bias detection, and performance evaluation.
- Break condition: When surrogate models either overfit and deteriorate accuracy compared to the teacher model, or when high-fidelity explanations use entirely different features than the black box (misleading users into trusting problematic models).

### Mechanism 3
- Claim: Trust in AI systems depends primarily on demonstrated reliability and performance, not on explainability, with empirical evidence showing mixed or negative effects of explanations on trust.
- Mechanism: Users judge machines by outcomes rather than intentions. Cognitive trust in AI is based on reliability evidence; explanations do not consistently increase trust and may decrease it when explanations are low-quality or when they accentuate confirmation bias.
- Core assumption: Users process explanations rationally and update their trust calibrated to explanation quality and model accuracy.
- Evidence anchors:
  - [Section 4, Page 12]: "Papenmeier et al. (2019) found that high-fidelity explanations actually decreased user's trust in high accuracy classification algorithms."
  - [Section 4, Page 11]: "In general, 'humans are judged by their intentions, while machines are judged by their outcomes'."
  - [corpus]: The meta-analysis on trust and explainability directly examines this relationship, confirming the contested nature of this assumption.
- Break condition: When explanations are unreliable, superficial, or optimize only for fidelity without matching the black box's actual reasoning—these can generate false trust (automation bias) or decrease trust even in accurate systems.

## Foundational Learning

- Concept: **Epistemic vs. Legal Opacity**
  - Why needed here: Understanding the source of opacity determines appropriate remediation strategies—legal opacity requires policy/regulatory solutions; epistemic opacity requires technical XAI approaches.
  - Quick check question: Can you distinguish whether access restrictions or model complexity is the primary barrier to understanding a given system?

- Concept: **Model Identifiability Problem**
  - Why needed here: Deep models have many parameter settings with equivalent predictive accuracy, making it impossible to determine which specific model is being used—this fundamentally limits what explanations can reveal.
  - Quick check question: If two neural networks produce identical outputs on all test inputs but have different internal weights, can an explanation method distinguish which is "correct"?

- Concept: **Understanding-Why vs. Objectual Understanding**
  - Why needed here: Local explanations provide understanding-why (for specific predictions) but lack the structural knowledge needed for genuine counterfactual reasoning; objectual understanding (via surrogate models) provides the functional scaffolding that makes counterfactual reasoning possible.
  - Quick check question: Can a user generate actionable recourse (steps to change an unfavorable decision) from a local explanation alone, or do they need structural knowledge of the model?

## Architecture Onboarding

- Component map: Opaque Model Layer (DNNs) -> Local Explanation Layer (LIME/SHAP) -> Global Explanation Layer (Surrogate models) -> Trust Calibration Layer (User interfaces)
- Critical path: For actionable understanding, users need both local explanations (for specific predictions) AND global surrogate models (for structural/functional understanding)—local methods alone cannot support genuine counterfactual reasoning or algorithmic recourse.
- Design tradeoffs:
  - Accuracy vs. Interpretability: More interpretable surrogate models sacrifice fidelity to the original model
  - Fidelity vs. Faithfulness: High-fidelity explanations may use different features than the black box, misleading users
  - Completeness vs. Usability: Complete explanations (full model details) are unusable; usable explanations are incomplete
  - Transparency vs. Trust: More explanation does not reliably produce more trust; may decrease trust or create automation bias
- Failure signatures:
  - Explanation inconsistency: Similar inputs produce radically different explanations (robustness failure)
  - Explanation manipulation: Adversarial perturbations produce desired explanations without changing predictions
  - Misleading fidelity: Surrogate model matches predictions but uses spurious features
  - Trust miscalibration: Users over-trust systems with explanations regardless of actual model quality (automation bias)
- First 3 experiments:
  1. **Robustness stress test**: Apply small perturbations to inputs and measure explanation stability—if LIME/SHAP outputs vary significantly for nearly identical inputs, the explanation method is unreliable for that model class.
  2. **Fidelity-faithfulness audit**: Compare features used by surrogate model vs. features actually important in the black box (via controlled interventions) to detect whether high-fidelity explanations are misleading.
  3. **Trust calibration measurement**: A/B test user trust and task performance with vs. without explanations across explanation quality levels—determine if explanations improve calibrated trust or merely increase over-reliance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions, if any, does providing explainability for AI systems reliably increase user trust?
- Basis in paper: [explicit] The author states "there is a highly contentious debate around the effectiveness of explainability in building trust," citing studies where explanations both increased and decreased trust, and where improved self-reports of trust did not translate to improved task performance.
- Why unresolved: Empirical evidence is mixed and context-dependent; some studies show transparency positively impacts trust while others show it can have negative effects or no effect.
- What evidence would resolve it: Systematic studies identifying the specific contextual factors (user expertise, task type, explanation format, domain) that determine whether explainability increases or decreases trust, combined with behavioral measures rather than self-reports.

### Open Question 2
- Question: Can XAI methods be developed that satisfy GDPR-style legal requirements for making the logic of automated decision systems available to end-users?
- Basis in paper: [explicit] The author states: "It is still an open question whether the technological sector will be able to comply with this legal requirement."
- Why unresolved: Current XAI methods face fundamental limitations including fragility to perturbations, lack of precision, inability to provide genuine counterfactual understanding, and the model identifiability problem in deep networks.
- What evidence would resolve it: Demonstration of XAI methods that pass legal scrutiny for providing meaningful explanations to non-expert end-users in high-stakes domains, or regulatory clarification on what constitutes adequate compliance.

### Open Question 3
- Question: How can counterfactual explanation methods be augmented with causal and structural information to provide actionable recourse for affected individuals?
- Basis in paper: [explicit] The author cites Karimi et al. (2021) showing that current counterfactual explanations "do not translate to an optimal or feasible set of actions" due to "lack of consideration of causal relations governing the world."
- Why unresolved: Local post-hoc interpretability methods lack the structural and functional information about model workings required for genuine counterfactual reasoning; surrogate models provide functional correlations but not causal knowledge.
- What evidence would resolve it: Development and validation of explanation methods that integrate causal models of the domain, enabling users to identify feasible interventions that reliably change outcomes.

## Limitations
- The paper relies entirely on conceptual analysis and literature synthesis without original empirical validation, limiting confidence in its claims about XAI limitations
- Key empirical claims about trust-explainability relationships depend on cited studies with varying methodologies and contexts that are not fully examined
- No specific datasets, model architectures, or hyperparameters are provided for replicating technical analyses of explanation robustness or fidelity

## Confidence
- High confidence in the conceptual distinction between legal and epistemic opacity as fundamental barriers to understanding
- Medium confidence in claims about local explanation fragility, supported by cited literature but requiring replication
- Medium confidence in the trust-explainability relationship findings, given mixed empirical evidence across studies
- Low confidence in specific trade-off boundaries between accuracy and interpretability without empirical calibration

## Next Checks
1. **Replication of explanation robustness**: Apply controlled perturbations to LIME/SHAP explanations across multiple model types to verify fragility claims
2. **Fidelity-faithfulness audit**: Compare feature importance rankings between black-box models and their surrogate explanations using interventional testing
3. **Trust calibration experiment**: Conduct A/B testing with controlled explanation quality levels to measure calibrated vs. over-trust effects on user decision-making