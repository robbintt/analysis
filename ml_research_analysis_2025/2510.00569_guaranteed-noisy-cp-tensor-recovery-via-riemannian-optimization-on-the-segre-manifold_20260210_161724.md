---
ver: rpa2
title: Guaranteed Noisy CP Tensor Recovery via Riemannian Optimization on the Segre
  Manifold
arxiv_id: '2510.00569'
source_url: https://arxiv.org/abs/2510.00569
tags:
- tensor
- decomposition
- convergence
- manifold
- regression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of recovering a low-CP-rank tensor
  from noisy linear measurements, which is central to tensor PCA, regression, and
  related high-dimensional data analysis tasks. The authors address this challenge
  by formulating the recovery as an optimization problem over the Segre manifold,
  the smooth Riemannian manifold of rank-one tensors, which allows for inherent feasibility
  of the CP decomposition throughout the iterations.
---

# Guaranteed Noisy CP Tensor Recovery via Riemannian Optimization on the Segre Manifold

## Quick Facts
- **arXiv ID**: 2510.00569
- **Source URL**: https://arxiv.org/abs/2510.00569
- **Reference count**: 40
- **Primary result**: Riemannian Gradient Descent and Riemannian Gauss-Newton algorithms on the Segre manifold achieve local linear and quadratic-then-linear convergence for CP tensor recovery from noisy measurements, outperforming CP-ALS with relaxed incoherence requirements.

## Executive Summary
This paper addresses the fundamental problem of recovering low-CP-rank tensors from noisy linear measurements, which is central to tensor PCA and regression tasks. The authors develop two novel Riemannian optimization algorithms—RGD and RGN—that operate directly on the Segre manifold of rank-one tensors. By leveraging the intrinsic geometry of this manifold, the methods guarantee feasibility of the CP decomposition throughout optimization iterations, avoiding the infeasibility issues that plague Euclidean approaches. The theoretical analysis establishes local convergence guarantees with relaxed incoherence requirements, while extensive experiments demonstrate superior performance compared to classical CP-ALS baselines across various noise levels and coherence conditions.

## Method Summary
The method formulates CP tensor recovery as optimization over the Segre manifold, the smooth Riemannian manifold of rank-one tensors. Two algorithms are proposed: Riemannian Gradient Descent (RGD) and Riemannian Gauss-Newton (RGN). Both methods use tangent-space projections followed by retractions (specifically T-HOSVD) to update rank-one components while preserving CP structure. The algorithms operate on either the full tensor (decomposition) or through linear measurements (regression), with RGN achieving faster initial convergence through second-order information. The methods require initialization via spectral methods (CPCA or HOSVD) and operate under bounded incoherence assumptions on the tensor factors.

## Key Results
- RGD achieves local linear convergence for CP tensor recovery from noisy measurements
- RGN exhibits an initial local quadratic convergence phase that transitions to linear convergence near the noise floor
- Both methods outperform classical CP-ALS in reconstruction accuracy, particularly in high-coherence regimes
- Convergence guarantees require only bounded incoherence rather than vanishing incoherence as tensor dimensions grow
- Synthetic experiments validate theoretical predictions across various noise scales and coherence conditions

## Why This Works (Mechanism)

### Mechanism 1: Feasibility Preservation via Segre Manifold
Optimizing directly on the Segre manifold guarantees CP structure is maintained at every iteration, avoiding infeasibility issues in Euclidean approaches. Each rank-one component is constrained to the Segre manifold throughout optimization. Tangent-space projections followed by retractions (T-HOSVD) map updates back onto the manifold, ensuring feasibility without explicit constraints. The core assumption is that T-HOSVD retraction sufficiently approximates the exponential map for local convergence. Break condition: If retraction quality degrades (e.g., high-rank updates far from rank-one), convergence guarantees may fail.

### Mechanism 2: Two-Phase Convergence of RGN
RGN exhibits local quadratic convergence when error $\varepsilon^{(t)} \gg \eta^{d-1}$, transitioning to linear convergence as iterates approach the noise floor. The error recursion combines first- and second-order terms, with quadratic dominance when $\varepsilon^{(t)} \gg \eta^{d-1}$ and linear contraction once $\varepsilon^{(t)} \lesssim \eta^{d-1}$. The core assumption is that initialization error and noise satisfy specific bounds (Corollary 4.2). Break condition: If $\eta$ is too large or initialization is poor, quadratic phase may be skipped entirely.

### Mechanism 3: Relaxed Incoherence Requirements
Convergence requires only bounded incoherence $\eta$, not vanishing $\eta \to 0$ as in prior ALS analyses. The error bounds depend on $\eta$ but do not require it to scale with dimension, allowing non-orthogonal factors. The core assumption is that factors are sufficiently distinguishable with $\eta$ bounded by a small constant. Break condition: If factors are nearly collinear ($\eta \to 1$), convergence slows to near-stalling.

## Foundational Learning

- **Concept: Riemannian Manifolds and Tangent Spaces**
  - Why needed here: Understanding how gradient descent works on constrained surfaces (Segre manifold) requires knowing that tangent vectors represent allowed directions, and retractions map back to the manifold.
  - Quick check question: Explain why projecting a Euclidean gradient onto the tangent space before retraction preserves the manifold constraint.

- **Concept: CP Tensor Decomposition**
  - Why needed here: The paper targets CP-rank-$r$ tensors expressed as sums of rank-one outer products. Knowing the identifiability issues (Kruskal's criterion) clarifies why geometric structure matters.
  - Quick check question: Given $T = \sum_{i=1}^r \lambda_i u_{1,i} \otimes \cdots \otimes u_{d,i}$, what is the dimension of the Segre manifold for a single rank-one component?

- **Concept: Gauss-Newton Optimization**
  - Why needed here: RGN approximates the Hessian using $J^\top J$ structure from the least-squares loss, yielding faster convergence than first-order methods.
  - Quick check question: Why does Gauss-Newton achieve quadratic convergence only when the residual is small (near noise floor)?

## Architecture Onboarding

- **Component map**: Segre Manifold -> Tangent Space Projection -> Retraction (T-HOSVD) -> Updated Tensor

- **Critical path**: 
  1. Initialize $r$ rank-one tensors via spectral method (CPCA or HOSVD)
  2. Compute residual $\sum_{i=1}^r A(T_i) - Y$
  3. For each component $i$: project gradient onto tangent space (Eq. 3), apply update (RGD or RGN), retract via T-HOSVD
  4. Iterate until $\|T^{(t)} - T^{(t-1)}\|_F / \|T^{(t-1)}\|_F < \text{tol}$ or error plateaus

- **Design tradeoffs**:
  - RGD: $O(rdp^*)$ per iteration, simpler, linear convergence; step-size $\alpha_t$ selection matters
  - RGN: $O(rnp^* d\bar{p})$ for regression, faster early convergence, but inverts $d_f \times d_f$ system each iteration; higher memory
  - Retraction choice: T-HOSVD is efficient; geodesic retractions (Swijsen et al. 2022) are more accurate but costlier

- **Failure signatures**:
  - Error stalls above noise floor → likely initialization too far or $\eta$ too large
  - Exploding gradients → step size too large for RGD
  - Numerical instability in RGN → tangent-space basis poorly conditioned; use QR orthogonalization

- **First 3 experiments**:
  1. **Noiseless CP decomposition with known rank**: Verify RGD achieves linear convergence ($\varepsilon^{(t)} \approx 2^{-t}\varepsilon^{(0)}$) and RGN achieves quadratic ($\varepsilon^{(t)} \approx 2^{-2^t}\varepsilon^{(0)}$) on a $30 \times 30 \times 30$ tensor with $r=3$.
  2. **Sweep coherence $\eta \in \{0, 0.5, 0.75\}$**: Confirm convergence rate slows as $\eta$ increases but remains bounded; check if RGN quadratic phase shortens.
  3. **Tensor regression with varying sample size $n$**: Compare RGD/RGN vs. CP-ALS and RRR on $n = 2\bar{p}^{3/2}r$ observations; measure final relative Frobenius error after 30 iterations.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can the theoretical analysis be extended to handle unknown CP rank, including automatic rank selection and provable guarantees under rank mis-specification?
- **Open Question 2**: Can alternative retraction mappings (beyond T-HOSVD) improve convergence rates or computational efficiency on the Segre manifold?
- **Open Question 3**: Can the local convergence guarantees be strengthened to global convergence under the same or weakened incoherence assumptions?

## Limitations
- Analysis assumes exact knowledge of CP rank and does not address rank selection or mis-specification
- Each iteration involves retractions and tangent-space projections, which may become computationally intensive in ultra-high dimensions
- The paper adopts T-HOSVD retraction, leaving exploration of alternative mappings to future work

## Confidence
- **High Confidence**: Feasibility preservation via Segre manifold; two-phase RGN convergence
- **Medium Confidence**: Relaxed incoherence requirements; CP-ALS baseline comparison
- **Low Confidence**: Exact retraction error bounds; CPCA initialization robustness

## Next Checks
1. **Retraction Error Analysis**: Implement both T-HOSVD and exact geodesic retraction (Swijsen et al. 2022) on synthetic tensors; measure maximum retraction error $\|R_X(\xi) - \exp_X(\xi)\|_F$ across random tangent vectors to quantify approximation quality.

2. **Incoherence Breakpoint Test**: Systematically vary factor coherence from $\eta=0$ (orthogonal) to $\eta=0.99$ (nearly collinear) on fixed tensor size; record iteration count to reach 1% relative error threshold for both RGD and RGN to identify practical incoherence limits.

3. **Initialization Sensitivity Study**: Compare random initialization vs. CPCA vs. HOSVD initialization on tensors with identical $\eta$ and noise levels; measure final relative error after 30 iterations and success rate (error below 10% threshold) across 50 random seeds.