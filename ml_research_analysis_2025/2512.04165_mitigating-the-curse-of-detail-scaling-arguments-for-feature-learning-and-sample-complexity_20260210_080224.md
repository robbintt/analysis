---
ver: rpa2
title: 'Mitigating the Curse of Detail: Scaling Arguments for Feature Learning and
  Sample Complexity'
arxiv_id: '2512.04165'
source_url: https://arxiv.org/abs/2512.04165
tags:
- learning
- feature
- layer
- kernel
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of predicting when deep neural
  networks exhibit feature learning and determining the sample complexity required
  for learning in the "rich regime" where the network width is finite and feature
  learning is possible. The authors develop a theoretical framework that combines
  Large Deviation Theory (LDT) with variational approximations to predict the data
  and width scales at which various feature learning patterns emerge.
---

# Mitigating the Curse of Detail: Scaling Arguments for Feature Learning and Sample Complexity

## Quick Facts
- arXiv ID: 2512.04165
- Source URL: https://arxiv.org/abs/2512.04165
- Authors: Noa Rubin; Orit Davidovich; Zohar Ringel
- Reference count: 40
- Primary result: The paper predicts when deep neural networks exhibit feature learning and determines sample complexity scaling for learning in the "rich regime" using a combination of Large Deviation Theory and variational approximations.

## Executive Summary
This paper addresses the fundamental challenge of predicting when deep neural networks exhibit feature learning and determining the sample complexity required for learning in the "rich regime" where the network width is finite and feature learning is possible. The authors develop a theoretical framework that combines Large Deviation Theory (LDT) with variational approximations to predict the data and width scales at which various feature learning patterns emerge. The approach successfully reproduces known results for two-layer networks while making novel predictions for complex architectures including three-layer non-linear networks and softmax attention heads. The framework provides a tractable alternative to exact theories of feature learning while maintaining first-principles predictive power, successfully extending theoretical understanding beyond analytically tractable toy models.

## Method Summary
The core method involves deriving lower bounds on sample complexity using Large Deviation Theory, then approximating these bounds using a variational approach that compares different feature learning patterns. The approach bounds posterior alignment probability using Chernoff inequality, yielding sample complexity scaling with the negative log-probability of achieving high alignment in the prior distribution. The intractable LDT energy is approximated by minimizing a variational energy over candidate feature learning patterns using Feynman-Bogoliubov inequality, transforming infinite-dimensional integration into finite optimization. The framework proposes three main patterns—Gaussian Process (GP), Gaussian Feature Learning (GFL), and Specialization—and develops heuristics for how these patterns propagate between layers. Layer-wise feature learning effects propagate via predictable kernel modifications, allowing the theory to predict both sample complexity and which features emerge in different architectures.

## Key Results
1. Successfully reproducing known results for two-layer networks, including sample complexity scaling exponents (e.g., P* ∝ d for two-layer FCNs on cubic targets)
2. Novel predictions for complex architectures, including three-layer non-linear networks where they predict P* ∝ d scaling and the transition between specialization-magnetization and GP-specialization patterns
3. Predictions for softmax attention heads with P* ∝ √Ld³ scaling and accurate prediction of the scaling of specializing neurons with network width

## Why This Works (Mechanism)

### Mechanism 1: Large Deviation Theory Bounds Sample Complexity via Prior Alignment Probability
The minimal sample size P* required for learning scales with the negative log-probability of achieving high alignment in the prior distribution. The approach bounds posterior alignment probability using Chernoff inequality, yielding P* ∝ -2κ·log(Pr_p0[Af ≥ α])/k. Networks achieving strong alignment in the prior are statistical outliers whose rarity determines learning difficulty. This works when k = P⁻¹Σ E_p0[(f(x_ν) - y(x_ν))²] is O(1) and the bound tightens when overfitting is small (k/κ ∼ O(1)).

### Mechanism 2: Variational Approximation Makes Intractable Bounds Computable
The intractable LDT energy E(α) can be approximated by minimizing a variational energy over candidate feature learning patterns. Using Feynman-Bogoliubov inequality, E(α) ≈ min_q Ẽ_q(α), where Ẽ_q decomposes into layer-wise excess weight terms (Δ_{l,i}) plus a target-kernel alignment term (a_y). This transforms infinite-dimensional integration into finite optimization. The approach assumes kernel fluctuations are weak enough that ˜K_l can be replaced by its expectation under q (kernel-adaptation approximation).

### Mechanism 3: Three Pattern Taxonomy Captures Dominant Feature Learning Modes
Comparing just three interpretable patterns—GP (lazy), GFL (covariance modification), Specialization (mean shift)—correctly predicts sample complexity scaling. Each pattern has interpretable energy cost: GP has Δ=0 but large a_y (bad for high-degree targets); Specialization pays Δ ∝ d·M but reduces a_y ∝ N/M; GFL interpolates. Minimizing Ẽ_q selects the dominant pattern. These three patterns span the relevant feature learning modes for FCNs; more complex circuits reduce to combinations.

### Mechanism 4: Feature Propagation Rules Predict Cross-Layer Effects
Layer-wise feature learning effects propagate via predictable kernel modifications. (i) Specialization creates spectral spike with RKHS norm ∝ N_l/M; (ii) Amplified features create amplified higher-order features (D^m scaling); (iii) Lazy layers preserve relative spectral scales. Features are sufficiently low-rank that they can be treated as spectral spikes in kernel space.

## Foundational Learning

- Concept: **Large Deviation Theory (LDT)**
  - Why needed here: Provides rigorous mathematical framework for bounding probabilities of rare events (high alignment in prior), which directly yields sample complexity bounds.
  - Quick check question: Can you explain why Chernoff bounds give tighter estimates than Chebyshev for rare event probabilities?

- Concept: **Reproducing Kernel Hilbert Space (RKHS) Norm**
  - Why needed here: The alignment term a_y = ⟨y, K⁻¹, y⟩ measures target complexity relative to network kernel; understanding RKHS geometry is essential for interpreting sample complexity.
  - Quick check question: Why does a high RKHS norm indicate a "hard" function to learn with a given kernel?

- Concept: **Variational Methods / Feynman-Bogoliubov Inequality**
  - Why needed here: Enables tractable approximation of intractable integrals by optimizing over a family of simpler distributions.
  - Quick check question: How does the choice of variational family q affect the tightness of the upper bound on E(α)?

## Architecture Onboarding

- Component map:
  - Prior distribution -> Gaussian weights with layer-dependent variance (σ²_l/N_{l-1})
  - Posterior bound -> Eq. 4 connects posterior alignment to prior via data term
  - Energy functional -> Ẽ_q (Eq. 10) with Δ_{l,i} (layer costs) and a_y (target alignment)
  - Pattern library -> GP, GFL, Specialization (Fig. 4)
  - Propagation rules -> Claims (i)-(iii) for kernel evolution

- Critical path:
  1. Define architecture, activation, data measure, target function
  2. Compute GP kernel K_0 for input layer
  3. Enumerate candidate patterns per layer
  4. Compute Ẽ_q for each pattern combination using propagation rules
  5. Select minimum-energy pattern → predicts sample complexity AND which features emerge

- Design tradeoffs:
  - Standard vs. mean-field scaling: Mean-field gives N²/M specialization cost vs. N/M for standard (Appendix E.1.1)
  - Width vs. depth: For 3-layer networks, Sp.-Mag. gives P* ∝ (N₁N₂d)^{1/3} while GP-Sp. gives P* ∝ √(N₂d); pattern selection depends on N₁/d ratio
  - Ridge κ: Larger κ tightens bounds but may underestimate true sample complexity if overfitting is benign

- Failure signatures:
  - Bound becomes vacuous: κ → 0 or strong overfitting regime
  - Pattern prediction wrong: Superposition effects (Elhage et al.), multi-feature interactions
  - Scaling mismatch: Check if assumed data measure matches actual distribution; layer normalization may be needed for non-i.i.d. data (Fig. 6)

- First 3 experiments:
  1. Two-layer FCN validation: Train erf network on Hermite-3 target; verify P* ∝ d and count specializing neurons (∝ √(N/d)). Compare to exact LDT solution (Fig. 2).
  2. Three-layer pattern transition: Fix N₂, d, P; vary N₁. Verify transition from Sp.-Mag. to GP-Sp. pattern as N₁ increases, per Table 1 prediction (Fig. 3c, Fig. 8).
  3. Softmax attention scaling: Train single attention head on cubic target; verify P* ∝ √(Ld³) collapse when plotting alignment vs. P/√(Ld³) (Fig. 3b).

## Open Questions the Paper Calls Out

### Open Question 1
Can the variational framework be extended to predict feature learning dynamics during early training stages, rather than only at equilibrium? The current framework assumes networks trained to equilibrium (Bayesian posterior). Bayesian convergence times can be slow, and real SGD training may exhibit different feature learning patterns at early stages. Establishing this would require theoretical derivations connecting equilibrium predictions to dynamical phenomena, or empirical validation showing alignment between predicted patterns and early-training feature emergence.

### Open Question 2
How can the framework handle multi-feature interactions, particularly in settings involving superposition where multiple features are learned simultaneously? The current variational ansatz focuses on single-feature patterns (GP, GFL, Specialization) and may not capture complex interactions when networks learn multiple correlated features. This requires extensions of the heuristic rules to multi-feature scenarios, validated against experiments on tasks requiring simultaneous learning of multiple features.

### Open Question 3
Can the effective ridge conjecture—that κ should remain O(1) even in vanishing ridge regimes—be formally established? When κ→0, overfitting effects can emerge, potentially making the lower bound vacuous. The relationship between explicit ridge and effective ridge in rich regimes remains unclear. This would require formal proof connecting effective ridge theory to the LDT bounds, or empirical studies showing the bound remains tight with appropriate effective ridge values.

### Open Question 4
How can feature propagation rules be rigorously quantified for general CNNs and transformers beyond the simple architectures analyzed? The paper validates rules on FCNs, simple CNNs with non-overlapping patches, and basic attention heads, but modern architectures involve more complex connectivity patterns. This requires systematic derivation and empirical validation of propagation rules for architectures with skip connections, patch overlap, and multi-head attention mechanisms.

## Limitations

- Pattern Scope Limitation: The theory relies on only three canonical feature learning patterns and may miss complex phenomena like feature superposition or multi-feature interactions observed in practice.
- Gaussian Assumption: The prior and data distributions are assumed Gaussian, which may not hold for real-world datasets and could significantly alter kernel evolution and feature learning dynamics.
- Kernel-Adaptation Approximation: The replacement of ˜K_l with its expectation under q assumes weak kernel fluctuations, which strong feature learning could invalidate.

## Confidence

- High Confidence: Sample complexity predictions for two-layer FCNs (P* ∝ d scaling for cubic targets), Specialization pattern emergence and neuron scaling (∝ √(N/d)), transition between specialization-magnetization and GP-specialization patterns in three-layer networks, softmax attention head scaling (P* ∝ √Ld³)
- Medium Confidence: Three-layer network pattern predictions (Table 1) - validated on limited parameter sweeps, feature propagation rules (Claims i-iii) - numerical validation but theoretical justification could be strengthened, general architecture predictions - framework is extensible but untested on diverse architectures
- Low Confidence: Performance on non-Gaussian data distributions, handling of feature superposition and multi-feature interactions, behavior in the strong overfitting regime (κ → 0)

## Next Checks

1. **Multi-Layer Network Pattern Transitions**: Systematically sweep all three widths (N₁, N₂, N₃) in a three-layer network to map the full phase diagram of pattern transitions. Validate predictions from Table 1 and identify any additional patterns that emerge in under-explored regions of the parameter space.

2. **Real-World Dataset Validation**: Apply the framework to a standard computer vision task (e.g., CIFAR-10 classification) with a simple CNN. Measure whether predicted sample complexity and feature learning patterns align with empirical observations, particularly focusing on how the theory handles complex feature interactions present in natural images.

3. **Feature Superposition Test**: Design a synthetic target function that requires multi-feature superposition (e.g., sum of multiple high-degree Hermite polynomials with different weight vectors). Train networks and measure whether the observed sample complexity and feature learning patterns match theoretical predictions, or whether new patterns emerge that violate the current framework's assumptions.