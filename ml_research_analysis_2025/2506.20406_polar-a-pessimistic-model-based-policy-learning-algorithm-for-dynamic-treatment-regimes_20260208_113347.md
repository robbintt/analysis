---
ver: rpa2
title: 'POLAR: A Pessimistic Model-based Policy Learning Algorithm for Dynamic Treatment
  Regimes'
arxiv_id: '2506.20406'
source_url: https://arxiv.org/abs/2506.20406
tags:
- policy
- page
- transition
- function
- polar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces POLAR, a pessimistic model-based algorithm
  for optimizing dynamic treatment regimes (DTRs) from offline data. The key innovation
  is incorporating uncertainty quantification into a modified reward function that
  penalizes actions in poorly represented regions of the history-action space.
---

# POLAR: A Pessimistic Model-based Policy Learning Algorithm for Dynamic Treatment Regimes

## Quick Facts
- arXiv ID: 2506.20406
- Source URL: https://arxiv.org/abs/2506.20406
- Reference count: 40
- Primary result: Introduces a pessimistic model-based algorithm for offline DTR optimization that incorporates uncertainty quantification to avoid extrapolation errors in poorly represented regions.

## Executive Summary
POLAR is a novel pessimistic model-based algorithm designed for optimizing dynamic treatment regimes from offline data. The key innovation lies in incorporating uncertainty quantification into the reward function, penalizing actions in regions of the history-action space with poor data coverage. This approach addresses the critical challenge of partial data coverage in observational healthcare data, where certain patient history-treatment combinations are underrepresented. The algorithm demonstrates superior performance compared to state-of-the-art methods on both synthetic data and real-world MIMIC-III datasets, particularly excelling in low-data regimes and when the behavior policy has limited exploration.

## Method Summary
POLAR operates by first estimating transition dynamics from offline data using maximum likelihood estimation, then quantifying uncertainty for each history-action pair using methods like Gaussian Process posterior variance. A pessimistic penalty is incorporated into the reward function to discourage actions with high uncertainty, effectively creating a safety shield against distribution shift. The algorithm uses an actor-critic framework where the critic estimates Q-values under the modified (pessimistic) reward model, and the actor updates policy parameters via gradient descent. Policy optimization is performed without computationally intensive minimax or constrained optimization procedures, making it more scalable than some competing approaches.

## Key Results
- POLAR outperforms state-of-the-art methods including DTR-Q, DDQN, MOPO, and MILO on both synthetic data and MIMIC-III dataset
- The algorithm achieves near-optimal, history-aware treatment strategies while being particularly effective in low-data regimes
- POLAR demonstrates robust performance even under model misspecification, maintaining stability when the assumed model class differs from the true data-generating process
- Theoretical analysis establishes finite-sample bounds on policy suboptimality as functions of sample size and decision stages

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If the uncertainty of the model is penalized in the reward function, the policy avoids actions in poorly represented regions of the history-action space, reducing distribution shift errors.
- **Mechanism:** The algorithm constructs a modified reward $\tilde{r}_k(h_k, a_k) = \hat{r}_k(h_k, a_k) - \epsilon_k \Gamma_k(h_k, a_k)$, where $\Gamma_k$ is an uncertainty quantifier (e.g., derived from the posterior variance in a Gaussian Process). By subtracting this penalty, the value of actions with high epistemic uncertainty is artificially lowered, discouraging the actor from selecting them.
- **Core assumption:** The uncertainty quantifier $\Gamma_k$ provides a valid high-probability upper bound on the $L_1$ error between the estimated and true transition dynamics (Assumption 1).
- **Evidence anchors:**
  - [abstract]: "...quantifies uncertainty for each history-action pair. A pessimistic penalty is then incorporated into the reward function to discourage actions with high uncertainty."
  - [section 3.2]: "We define a modified expected reward function... $\tilde{r}_k(h_k, a_k) = \hat{r}_k(h_k, a_k) - \epsilon_k \Gamma_k(h_k, a_k)$."
  - [corpus]: Paper 35168 ("Learning a Pessimistic Reward Model in RLHF") supports the general efficacy of pessimistic reward modeling for robustness.
- **Break condition:** If the uncertainty quantifier $\Gamma_k$ fails to bound the model error (e.g., due to extreme model misspecification not covered by the prior), the penalty may be insufficient, leading to erroneous overestimation of value in out-of-distribution regions.

### Mechanism 2
- **Claim:** If transition dynamics are explicitly estimated from data, the policy can be optimized using simulated rollouts without requiring computationally intensive minimax optimization used in some model-free methods.
- **Mechanism:** POLAR uses Maximum Likelihood Estimation (MLE) to learn $\hat{P}$ (e.g., Linear or GP models). It then performs policy updates on this "imagined" model. Theoretical analysis (Theorem 1) decomposes suboptimality into model shift error (from $\hat{P}$ vs $P^*$) and policy optimization error, allowing for finite-sample guarantees.
- **Core assumption:** The transition dynamics $P^*$ lie within or can be approximated by the chosen model class $\mathcal{P}$.
- **Evidence anchors:**
  - [abstract]: "...estimates transition dynamics from offline data... without relying on computationally intensive minimax or constrained optimization procedures."
  - [section 3.1]: "POLAR first estimates transition dynamics... via maximum likelihood estimation (MLE)."
  - [corpus]: Paper 41420 ("Optimal Single-Policy Sample Complexity...") emphasizes the difficulty of distribution shift in offline RL, which model-based approaches attempt to mitigate via explicit modeling.
- **Break condition:** If the model class is misspecified and the model shift error term $V^{\pi^\dagger}_{P^*, r}$ dominates, the policy will converge to a suboptimal solution regardless of the optimization steps taken.

### Mechanism 3
- **Claim:** If the policy conditions on the full history using flexible basis functions (e.g., B-splines), it can effectively learn non-Markovian dependencies common in clinical trajectories.
- **Mechanism:** The policy $\pi_k$ maps history $H_k$ to actions. To handle the continuous, growing history space, POLAR projects history features using linear sieves (B-splines) $\Upsilon_{L_k}(\bar{s}_k)$. This allows the critic to approximate the Q-function $Q^{\pi(t)}_{k, \tilde{M}}$ without assuming the Markov property holds.
- **Core assumption:** The Q-function is smooth enough to be approximated by the selected basis functions (Assumption S3).
- **Evidence anchors:**
  - [abstract]: "...yields near-optimal, history-aware treatment strategies."
  - [section 3.3]: "We represent each function $f_k$... [using] B-splines... to approximate... over the joint state space."
  - [corpus]: N/A (Corpus focus is general RL; specific history handling is unique to this DTR formulation).
- **Break condition:** If the history space dimensionality grows such that the "curse of dimensionality" renders the B-spline approximation poor (low $L_k$ relative to data), the function approximation error $G_k$ will cause policy instability.

## Foundational Learning

- **Concept:** **Dynamic Treatment Regimes (DTRs) vs. MDPs**
  - **Why needed here:** Unlike standard RL which often assumes Markov states ($S_t$) or infinite horizons, DTRs operate on finite horizons ($K$ stages) and rely on full patient history ($H_k$), which is critical for medical data where past treatments influence current state.
  - **Quick check question:** Does the decision at step $k$ depend only on the current vital signs, or does the sequence of past medications matter?

- **Concept:** **Pessimism / Conservative Offline RL**
  - **Why needed here:** In offline learning, the algorithm cannot explore; it must infer from a fixed dataset. Standard "optimistic" approaches fail when they extrapolate value to unobserved actions. Pessimism acts as a safety guard.
  - **Quick check question:** How does the algorithm behave when encountering a patient history that has never been seen in the training data?

- **Concept:** **Actor-Critic Architecture**
  - **Why needed here:** This is the optimization loop. The *Critic* estimates the value of actions (using the pessimistic reward), and the *Actor* updates the policy parameters ($\theta$) to maximize this estimated value via gradient descent.
  - **Quick check question:** What specific target does the Critic regress onto during the update step?

## Architecture Onboarding

- **Component map:** Data Module -> Model Estimator -> Uncertainty Quantifier -> Reward Modifier -> Critic -> Actor
- **Critical path:** The estimation of the uncertainty quantifier $\Gamma$ is the single point of failure; if $\Gamma$ is underestimated in out-of-distribution regions, the pessimistic shield fails.
- **Design tradeoffs:**
  - **Linear vs. GP Model:** Linear is computationally cheaper and allows for tighter theoretical bounds (Theorem 2), but GP captures complex non-linear dynamics at a higher computational cost (Theorem 4).
  - **Penalty Coefficient $\epsilon$:** High $\epsilon$ increases safety (conservatism) but may reduce the policy value by avoiding potentially beneficial but under-sampled actions.
- **Failure signatures:**
  - **Paralyzed Policy:** Policy collapses to a single "safe" action (usually the most common one in dataset); suggests $\epsilon$ is too high.
  - **OOM Crash:** GP fitting on large datasets; requires sparse variational approximation (Supplement A.2).
  - **Diverging Q-values:** Standard offline RL failure mode; implies the pessimistic penalty $\Gamma$ is insufficient to control distribution shift.
- **First 3 experiments:**
  1. **Synthetic Validation:** Implement the linear simulation (Section 5.1) to verify Theorem 2 bounds. Plot Subopt($\pi^{(T)}$) vs. sample size $n$.
  2. **Hyperparameter Sensitivity:** Sweep $\epsilon$ (penalty) on the MIMIC-III subset. Identify the "Goldilocks" zone where policy value is maximized (Figure 3 analysis).
  3. **Robustness Check (Model Mismatch):** Train a GP-based POLAR on Linear synthetic data (or vice versa) to validate the robustness claims in Supplement F.3.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can POLAR be modified to handle asynchronous decision points where treatment intervals are irregular or patient-specific?
- Basis in paper: [explicit] The authors state that the current framework assumes aligned decision points, which often fails in real-world clinical settings, and suggest extending the method to continuous-time models like point processes as future research.
- Why unresolved: The current algorithm assumes discrete, aligned stages ($t=1, \dots, K$).
- What evidence would resolve it: A formulation of POLAR using continuous-time point processes that maintains finite-sample suboptimality bounds under irregular intervals.

### Open Question 2
- Question: Can the computational efficiency of POLAR be improved to handle long-horizon or high-frequency decision-making regimes?
- Basis in paper: [explicit] The authors identify that the computational cost increases with the number of decision stages ($K$) due to evaluating Q-functions over sampled pairs, posing challenges for long horizons.
- Why unresolved: The current actor-critic implementation involves complexity that scales with $K$, making it potentially prohibitive for long trajectories.
- What evidence would resolve it: An empirical or theoretical demonstration of an optimization routine that reduces the per-iteration complexity relative to $K$.

### Open Question 3
- Question: Can the pessimism hyperparameters be determined adaptively without relying on computationally expensive grid search?
- Basis in paper: [inferred] While Theorem 1 provides theoretical values for $\bar{c}_k$ involving unknown constants, the authors state in the real-data analysis that they "recommend running POLAR with a candidate set of $c$ values... via cross-validated grid search."
- Why unresolved: There is a disconnect between the theoretical values (which depend on unknown norms) and the practical need for grid search, suggesting a lack of an adaptive estimation procedure.
- What evidence would resolve it: An adaptive tuning mechanism that estimates the coverage/uncertainty constants online or via data-driven heuristics that matches the performance of the best fixed grid-search value.

## Limitations

- **Model specification uncertainty**: The paper assumes a specific structure for the uncertainty quantifier Γ that may not hold for all datasets or model classes, particularly in extreme model misspecification scenarios.
- **Approximation quality**: The effectiveness of B-spline basis functions depends critically on appropriate knot placement and basis selection, which is not fully specified in the methodology.
- **Off-policy evaluation bias**: While the paper demonstrates superior performance empirically, the reliance on off-policy evaluation for performance assessment introduces potential bias, particularly for rare patient histories.

## Confidence

- **High confidence**: The core pessimism mechanism (incorporating uncertainty penalties) is well-established in the offline RL literature and the theoretical bounds under linear models appear sound.
- **Medium confidence**: The empirical results show POLAR outperforming baselines, but the relative improvements depend on specific dataset characteristics and hyperparameter tuning.
- **Low confidence**: Claims about robustness to model misspecification are supported by limited ablation studies (Supplement F.3) and would benefit from more extensive validation across diverse model mismatches.

## Next Checks

1. **Uncertainty quantification validation**: Systematically test whether Γ_k provides valid high-probability bounds on L1 error across different model classes (linear, GP, neural networks) using synthetic data with known ground truth.
2. **Hyperparameter sensitivity analysis**: Conduct a comprehensive grid search over the pessimism parameter ε across multiple dataset regimes (varying sample sizes, behavior policy coverage) to identify optimal selection strategies.
3. **Model misspecification stress test**: Design experiments where the true transition dynamics are deliberately drawn from a model class different from the estimation method (e.g., GP truth with linear estimation) to quantify robustness limits.