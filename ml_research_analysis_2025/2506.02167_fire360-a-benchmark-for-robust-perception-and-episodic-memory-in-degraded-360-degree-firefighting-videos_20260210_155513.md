---
ver: rpa2
title: 'Fire360: A Benchmark for Robust Perception and Episodic Memory in Degraded
  360-Degree Firefighting Videos'
arxiv_id: '2506.02167'
source_url: https://arxiv.org/abs/2506.02167
tags:
- fire360
- object
- dataset
- reasoning
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Fire360 introduces a large-scale benchmark for evaluating AI perception
  under safety-critical degradation. The dataset includes 228 360-degree videos from
  firefighter training under conditions like smoke, low light, and thermal distortion,
  annotated for actions, objects, and degradation.
---

# Fire360: A Benchmark for Robust Perception and Episodic Memory in Degraded 360-Degree Firefighting Videos

## Quick Facts
- **arXiv ID:** 2506.02167
- **Source URL:** https://arxiv.org/abs/2506.02167
- **Reference count:** 40
- **Primary result:** Introduces Fire360 benchmark with 228 360° firefighting videos to evaluate AI perception under safety-critical degradation.

## Executive Summary
Fire360 introduces a large-scale benchmark for evaluating AI perception under safety-critical degradation. The dataset includes 228 360-degree videos from firefighter training under conditions like smoke, low light, and thermal distortion, annotated for actions, objects, and degradation. It supports five tasks: Visual Question Answering, Temporal Action Captioning, Object Localization, Safety-Critical Reasoning, and Transformed Object Retrieval (TOR). TOR tests matching pristine objects to their fire-damaged counterparts in unpaired scenes. Human experts achieve 83.5% on TOR, while GPT-4o scores 39.8%, highlighting model failures in reasoning under transformation and occlusion. Fire360 exposes significant performance gaps between humans and models in degraded, real-world scenarios, providing tools to advance robust, memory-augmented AI systems for emergency response.

## Method Summary
Fire360 evaluates zero-shot perception and reasoning on five tasks using 228 360° videos (3840×1920, 60fps) with JSON annotations. Tasks include Visual Question Answering, Temporal Action Captioning, Object Localization, Safety-Critical Reasoning, and Transformed Object Retrieval (TOR). The dataset is split 60/20/20 (Train/Val/Test). Models use CLIP, BLIP-2, GPT-4o, and Grounding DINO for inference. Equirectangular frames are optionally converted to rectilinear projections (90° FOV) to reduce panoramic distortion. TOR uses Grounding DINO for proposals, VLM encoding, and cosine similarity for retrieval. Evaluation uses task-specific metrics including Exact Match, BLEU-4, IoU, and Checklist Accuracy.

## Key Results
- Human experts achieve 83.5% on TOR, while GPT-4o scores 39.8%, highlighting significant model failure under transformation.
- VQA accuracy drops to 9.8% under high degradation, correlating with TOR errors (r = 0.72).
- Rectilinear projection improves VQA accuracy by 8–9% across models by reducing panoramic distortion.
- Models frequently misidentify structural elements (pipes, ladders) as transformed objects (30% of GPT-4o TOR errors).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rectilinear projection improves model performance by reducing panoramic distortion.
- Mechanism: Converting equirectangular frames to 90° FOV rectilinear views reduces the ~70% spatial distortion at polar regions, allowing models trained on standard 2D imagery to localize and reason more effectively.
- Core assumption: Models have been trained predominantly on rectilinear imagery and lack inherent understanding of equirectangular geometry.
- Evidence anchors:
  - [section]: "Performance improves to 62.4% when GPT-4o receives rectilinear input, indicating high sensitivity to panoramic distortion (Figure 4)."
  - [section]: "Projecting the scene into a rectilinear format improves detection accuracy by mitigating geometric distortion, yielding an IoU of 47.1%."
  - [corpus]: Weak direct evidence; related work on panoramic distortion (PanoVPR, [39]) addresses place recognition, not object retrieval.
- Break condition: If models are specifically trained on equirectangular data with augmentation, rectilinear conversion may not yield gains.

### Mechanism 2
- Claim: TOR evaluates transformation-invariant recognition by requiring cross-scene object matching without temporal continuity.
- Mechanism: Grounding DINO proposes candidate regions; vision-language models encode both pristine and degraded instances; cosine similarity selects the best match. Success requires material inference (e.g., soot vs. melting) and shape reasoning under occlusion.
- Core assumption: Models can generalize object identity across irreversible physical degradation using learned visual priors.
- Evidence anchors:
  - [abstract]: "TOR tests whether models can match pristine exemplars to fire-damaged counterparts in unpaired scenes, evaluating transformation-invariant recognition."
  - [section]: "Given a pristine reference image, the model must retrieve its transformed counterpart from a degraded 360° frame in a different scene."
  - [corpus]: GCAgent (episodic memory for long-video understanding) shares conceptual grounding but does not address physical degradation.
- Break condition: If retrieval relies on texture-based embeddings (e.g., standard CLIP), performance collapses when surface features are destroyed.

### Mechanism 3
- Claim: Degradation severity strongly predicts cross-task failure correlation.
- Mechanism: Under high smoke/low light, VQA accuracy drops to <10% and correlates with TOR errors (r = 0.72), suggesting shared vulnerability in visual grounding rather than task-specific incompetence.
- Core assumption: Failure modes are systemic (grounding, occlusion handling) rather than isolated to individual task architectures.
- Evidence anchors:
  - [section]: "In high-degradation scenes, VQA accuracy drops to 9.8%, and correlates with TOR error rates (Pearson r = 0.72)."
  - [section]: "GPT-4o accuracy falls below 10%, with captions becoming generic or hallucinated."
  - [corpus]: Descriptor (DTPQA benchmark) also links perception robustness to downstream reasoning but in traffic contexts.
- Break condition: If tasks are decoupled via task-specific adapters or fine-tuning, correlation may weaken.

## Foundational Learning

- Concept: **Equirectangular projection and rectilinear conversion**
  - Why needed here: All Fire360 frames are stored in equirectangular format; models must either handle distortion natively or use preprocessing.
  - Quick check question: Can you explain why a 90° FOV rectilinear crop reduces distortion compared to the raw panoramic view?

- Concept: **Vision-language embeddings and cosine similarity retrieval**
  - Why needed here: TOR uses CLIP/BLIP-2/GPT-4o embeddings to match objects across scenes; understanding embedding space is critical.
  - Quick check question: Given two normalized embeddings with cosine similarity of 0.85, what does this imply about their semantic relationship?

- Concept: **IoU-based evaluation for detection and retrieval**
  - Why needed here: Object Localization and TOR both use IoU ≥ 0.5 as the success threshold; proposal-based pipelines depend on this metric.
  - Quick check question: If a predicted box has coordinates [100, 100, 50, 50] and ground truth is [110, 110, 60, 60], is IoU ≥ 0.5?

## Architecture Onboarding

- Component map:
  - Input: 228 × 360° videos (3840×1920, 60fps) → OpenCV preprocessing → rectilinear frames (90° FOV)
  - Detection: Grounding DINO (threshold 0.4) → ~36 proposals/frame → NMS (IoU ≥ 0.3)
  - Retrieval: CLIP/BLIP-2/GPT-4o encoding → cosine similarity → top-1 selection
  - Evaluation: Task-specific scorers (exact match, BLEU-4, checklist accuracy, IoU)

- Critical path:
  1. Load video → extract keyframes
  2. Convert equirectangular → rectilinear (optional but recommended)
  3. Run Grounding DINO for proposals
  4. Encode proposals + reference with VLM
  5. Rank by similarity; evaluate IoU against ground truth

- Design tradeoffs:
  - Equirectangular vs. rectilinear: Rectilinear improves accuracy (+8–9%) but loses global context; may need multi-crop for full coverage.
  - Proposal threshold: Lower threshold (0.3) increases recall but adds compute; higher threshold (0.5) speeds up but misses degraded objects.
  - VLM choice: GPT-4o (39.8% TOR) outperforms CLIP (32.5%) and BLIP-2 (35.1%) but requires API access and cost.

- Failure signatures:
  - Visual distractors: Pipes/ladders mistaken for helmets (30% of GPT-4o TOR errors)
  - Material confusion: Plastic vs. metal under charring (20% of BLIP-2 errors)
  - Occlusion sensitivity: Smoke-obscured objects missed entirely (25% of CLIP errors)
  - Hallucinated captions: Generic descriptions under heavy smoke (GLaMM BLEU-4 = 0.341 vs. human 0.85)

- First 3 experiments:
  1. Reproduce TOR baseline with rectilinear projections; confirm GPT-4o ~39.8% and CLIP ~32.5% on the 154-target test set.
  2. Ablate proposal threshold (0.3 vs. 0.4 vs. 0.5) to measure precision-recall tradeoff under high degradation (smoke level 4–5).
  3. Test multi-crop rectilinear strategy (e.g., 4 × 90° views) vs. single front view to quantify global context benefit for VQA and localization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating temporal modeling into the Transformed Object Retrieval (TOR) task improve robustness under severe distortion and occlusion?
- Basis in paper: [explicit] Section 6 states: "Fire360 currently lacks temporal modeling in TOR; extending to multi-frame tracking could improve robustness under severe distortion and occlusion."
- Why unresolved: The current TOR formulation operates on single keyframes, discarding temporal continuity that could aid in tracking objects through intermittent occlusion and smoke.
- What evidence would resolve it: A comparative study evaluating TOR performance when models have access to multi-frame sequences versus single frames, showing measurable gains in retrieval accuracy under high-degradation conditions.

### Open Question 2
- Question: To what extent do models trained on Fire360 generalize to real-world emergency response scenarios across different geographic regions and institutional protocols?
- Basis in paper: [explicit] Section 3 notes: "Future dataset extensions will include international recordings to address institutional and geographic diversity" and "full generalization remains an open direction."
- Why unresolved: Fire360 is collected at a single U.S. training institute following NFPA standards; variability in equipment, building construction, and procedures across regions remains untested.
- What evidence would resolve it: Zero-shot evaluation of Fire360-trained models on independently collected firefighting footage from international or structurally distinct environments, with performance gap analysis.

### Open Question 3
- Question: Does fine-tuning on degraded fire imagery cause overfitting to noise patterns (e.g., smoke textures) rather than learning robust degradation-invariant features?
- Basis in paper: [explicit] Section 6 cautions: "future fine-tuning may risk overfitting to noise patterns (e.g., smoke) and requires significant compute."
- Why unresolved: Current evaluations use zero-shot prompting; the trade-off between adaptation and robustness under domain-specific fine-tuning is unexplored.
- What evidence would resolve it: Experiments comparing zero-shot versus fine-tuned model performance on held-out degradation types (e.g., thermal distortion vs. smoke), measuring whether accuracy gains transfer or collapse on unseen degradation patterns.

### Open Question 4
- Question: What architectural modifications enable vision-language models to handle equirectangular 360° distortion as effectively as rectilinear projections?
- Basis in paper: [inferred] Figure 4 shows consistent performance improvements (8–10% VQA accuracy gains) when using rectilinear projections versus equirectangular views across all models, indicating current architectures are not designed for panoramic geometry.
- Why unresolved: Models like GPT-4o, Qwen-VL, and LLaVA are trained predominantly on rectilinear imagery; the 70% higher spatial distortion near equirectangular poles remains an unaddressed architectural blind spot.
- What evidence would resolve it: Development and evaluation of 360°-native model variants (e.g., spherical convolution backbones or distortion-aware positional encodings) that close the rectilinear-to-equirectangular performance gap on Fire360 benchmarks.

## Limitations

- Prompt templates and safety-critical reasoning checklists are referenced but not fully specified, potentially affecting reproducibility.
- Human expert performance on TOR (83.5%) is cited but methodological details are sparse, making comparison with models less transparent.
- Generalization beyond fire scenarios is untested; performance on other degradation types (e.g., underwater, dust) is unknown.

## Confidence

- **High:** TOR task design, baseline performance metrics, correlation between degradation and task failure.
- **Medium:** Human vs. model performance gap on TOR, effectiveness of rectilinear projection.
- **Low:** Prompt design impact on task performance, robustness to non-fire degradation types.

## Next Checks

1. Reproduce TOR baseline on rectilinear projections to confirm model performance differences (GPT-4o ~39.8%, CLIP ~32.5%) and error patterns (distractor confusion, material inference failures).
2. Conduct ablation studies on proposal thresholds (0.3 vs. 0.4 vs. 0.5) under high-degradation scenes to quantify precision-recall tradeoffs.
3. Test multi-crop rectilinear strategy (e.g., 4 × 90° views) vs. single front view to measure impact on global context for VQA and localization tasks.