---
ver: rpa2
title: Mamba Drafters for Speculative Decoding
arxiv_id: '2506.01206'
source_url: https://arxiv.org/abs/2506.01206
tags:
- mamba
- drafter
- drafting
- target
- drafters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mamba Drafters for Speculative Decoding Daewon Choi et al. (2025)
  This paper addresses the challenge of accelerating large language model inference
  using speculative decoding, where a fast drafter generates candidate tokens that
  are verified by a target model.
---

# Mamba Drafters for Speculative Decoding

## Quick Facts
- **arXiv ID:** 2506.01206
- **Source URL:** https://arxiv.org/abs/2506.01206
- **Reference count:** 37
- **Key outcome:** Mamba-130M drafters achieve 2x higher throughput than Transformer external drafters in speculative decoding while using 20GB less memory than self-speculation methods like EAGLE.

## Executive Summary
This paper introduces Mamba as an external drafter for speculative decoding to accelerate large language model inference. Traditional external drafters are slow, while self-speculation methods require expensive retraining per target model. Mamba's state-space model architecture with linear recurrence enables faster drafting with constant memory usage, avoiding the quadratic complexity of Transformers. The authors further enhance efficiency with a tree search algorithm formulated as a multi-armed bandit problem that dynamically optimizes draft tree structure. Experiments show Mamba drafters outperform traditional Transformer external drafters, achieve comparable performance to EAGLE with significantly less memory, and maintain stable acceptance length on long contexts (1k-8k tokens).

## Method Summary
The authors propose using Mamba, a state-space model (SSM), as an external drafter for speculative decoding. Mamba's linear recurrence structure enables faster drafting with constant memory usage, avoiding the quadratic complexity of Transformers. They implement tree-structured drafting with a batch generation approach that copies states without KV-cache overhead. To optimize the draft tree structure, they formulate drafting as a multi-armed bandit problem using UCB-based tree search with configurable candidate structures. The approach is evaluated on multiple benchmarks including XSum, CNN-DM, GSM-8K, MT-Bench, Alpaca, and HumanEval, demonstrating significant improvements in throughput and memory efficiency compared to both Transformer external drafters and self-speculation methods.

## Key Results
- Mamba-based drafters achieve 2x higher throughput than traditional Transformer external drafters across multiple benchmarks
- Comparable performance to EAGLE while using 20GB less memory and maintaining cross-model adaptability without retraining
- Maintains stable acceptance length on long contexts (1k-8k tokens) where EAGLE shows degradation
- Outperforms Transformer external drafters in math reasoning, code generation, and instruction following tasks

## Why This Works (Mechanism)
Mamba's state-space model architecture with selective state spaces and hardware-aware algorithms enables efficient long-sequence processing through linear recurrence, avoiding the quadratic complexity of attention mechanisms. The tree-structured drafting formulation as a multi-armed bandit problem allows dynamic optimization of draft tree structures based on acceptance patterns, maximizing throughput by balancing exploration of new tree configurations with exploitation of known good structures. The batch-wise state caching and graph caching mechanisms enable efficient state management during tree decoding without the memory overhead of traditional KV-caches.

## Foundational Learning

**State-Space Models (SSM)**
- *Why needed:* Understanding Mamba's core architecture and why it can draft faster than Transformers
- *Quick check:* Can explain the difference between Mamba's HiPPO matrix and attention mechanisms in handling long sequences

**Speculative Decoding**
- *Why needed:* Core inference optimization framework where fast drafter generates candidates verified by target model
- *Quick check:* Can describe the acceptance criterion and how throughput is calculated in speculative decoding

**Multi-Armed Bandit (MAB)**
- *Why needed:* Tree search optimization algorithm used to dynamically configure draft trees
- *Quick check:* Can explain UCB policy and how it balances exploration vs exploitation in tree configuration selection

**Batch-wise State Caching**
- *Why needed:* Critical optimization for Mamba tree drafting that avoids KV-cache overhead
- *Quick check:* Can describe how state copying differs from attention-based caching and why it's more memory efficient

## Architecture Onboarding

**Component Map:** FineWeb-Edu/ShareGPT -> Mamba-130M drafter -> Target model (Pythia-6.9B/Mistral-7B) -> Verification/Throughput calculation

**Critical Path:** Data preprocessing -> Mamba pretraining/SFT -> Tree-structured drafting implementation -> Speculative decoding integration -> Evaluation on benchmarks

**Design Tradeoffs:** Mamba drafters trade some drafting accuracy for significantly higher speed and lower memory usage compared to Transformer external drafters; tree search adds computational overhead but optimizes draft structure dynamically

**Failure Signatures:** Lower than expected throughput indicates inefficient Mamba tree drafting implementation; acceptance length drops on longer contexts suggest context extrapolation failure in Mamba state handling

**First Experiments:** 1) Profile drafting latency separately to verify Mamba speed advantage, 2) Plot acceptance length vs input length to confirm stability on long contexts, 3) Compare memory usage during inference to validate 20GB savings over EAGLE

## Open Questions the Paper Calls Out

**Open Question 1:** Can a single Mamba model leverage its efficient architecture to function as both the drafter and the verifier (target model) in a self-speculation framework? The authors note this represents an intriguing direction for future research.

**Open Question 2:** How can the tree-based search method be improved to be less dependent on manual hyperparameter settings? The authors suggest developing adaptive optimization strategies for the MAB formulation.

**Open Question 3:** What specific structural properties allow a small Mamba drafter to align better with large Transformer targets than small Transformer drafters? The paper observes better alignment but doesn't fully explain the theoretical cause.

**Open Question 4:** Can the memory overhead required for Mamba's hidden state maintenance be further optimized for tree-structured drafting? The authors mention potential optimization through efficient memory management techniques.

## Limitations
- Mamba drafter performance heavily depends on specific SSM implementation and scaling details not fully specified in the paper
- Tree-structured drafting algorithm requires precise implementation of batch-wise state caching that could significantly impact performance
- Multi-armed bandit-based tree search introduces unspecified hyperparameters (λUCB and λγ) affecting reproducibility
- Limited evaluation on long-context datasets (only ~80 samples per dataset) may not generalize to all scenarios

## Confidence

**High Confidence:** The core claim that Mamba-based drafters achieve 2x higher throughput than Transformer external drafters is well-supported by experimental results across multiple benchmarks.

**Medium Confidence:** The claim about Mamba maintaining stable acceptance length on long contexts is supported but based on limited dataset sampling, requiring further validation.

**Low Confidence:** The practical implementation details in the drafting algorithm could introduce bottlenecks not captured in the analysis, and the specific impact of MAB-based tree search optimization remains partially speculative.

## Next Checks

1. **Parameter Sensitivity Analysis:** Systematically vary λUCB and λγ in the MAB-based tree search across different target models to determine optimal configurations and assess robustness.

2. **Long-Context Generalization Test:** Evaluate Mamba drafters on additional long-context datasets beyond LongBench subtasks with diverse input lengths up to 16k tokens to verify claimed stability.

3. **Architectural Ablation Study:** Compare Mamba drafter performance against a Transformer external drafter with equivalent parameter count and training procedure, controlling for differences in drafting speed.