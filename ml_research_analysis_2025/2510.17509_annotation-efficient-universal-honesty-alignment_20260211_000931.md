---
ver: rpa2
title: Annotation-Efficient Universal Honesty Alignment
arxiv_id: '2510.17509'
source_url: https://arxiv.org/abs/2510.17509
tags:
- confidence
- arxiv
- elical
- data
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of making large language models
  (LLMs) honest by accurately expressing their confidence levels. The proposed method,
  Elicitation-Then-Calibration (EliCal), is a two-stage training framework that first
  elicits the model's internal confidence using self-consistency signals from unlabeled
  data, and then calibrates this confidence with a small set of correctness annotations.
---

# Annotation-Efficient Universal Honesty Alignment

## Quick Facts
- arXiv ID: 2510.17509
- Source URL: https://arxiv.org/abs/2510.17509
- Reference count: 40
- Primary result: Achieves near-optimal honesty alignment with only 1,000 labeled samples (0.18% of full supervision)

## Executive Summary
This paper addresses the challenge of making large language models (LLMs) honest by accurately expressing their confidence levels. The proposed method, Elicitation-Then-Calibration (EliCal), is a two-stage training framework that first elicits the model's internal confidence using self-consistency signals from unlabeled data, and then calibrates this confidence with a small set of correctness annotations. Experiments on the newly introduced HonestyBench benchmark demonstrate that EliCal achieves near-optimal honesty alignment with only 1,000 labeled samples (0.18% of full supervision), outperforming calibration-only baselines and showing strong generalization to unseen tasks like MMLU. This approach offers a scalable, annotation-efficient solution for improving LLM trustworthiness.

## Method Summary
EliCal is a two-stage training framework for honesty alignment. Stage 1 (Elicitation) uses self-consistency signals from unlabeled data to train a linear head that maps model hidden states to confidence scores. The model generates 20 sampled responses and 1 greedy response per question, then computes self-consistency confidence as the proportion of sampled responses semantically matching the greedy answer. Stage 2 (Calibration) fine-tunes from the Stage 1 checkpoint using a small set of correctness annotations. The framework uses LoRA modules (rank=8, α=16) on all linear layers while freezing the LLM backbone, enabling confidence prediction without degrading QA capabilities.

## Key Results
- EliCal achieves 84.36 AUROC on in-domain HonestyBench-Eval with only 1k annotations vs 73.41 for calibration-only baseline
- Spearman correlation between self-consistency confidence and accuracy reaches 0.789 on TQ dataset
- OOD generalization to MMLU shows 7.1 AUROC improvement over calibration-only with same annotation budget
- QA accuracy preserved during calibration (Qwen-7B maintains ~35.7% average across tasks)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Self-consistency confidence provides a learnable proxy for correctness that correlates with actual model capability.
- **Mechanism:** The model's internal confidence is reflected by the proportion of semantically consistent responses across multiple generations. When this signal is used as supervision (MSE loss), the model learns to externalize its internal confidence state through a linear head without requiring correctness labels.
- **Core assumption:** The hidden states at the final layer encode sufficient information about the model's uncertainty/confidence to be linearly decodable.
- **Evidence anchors:** Spearman correlation = 0.789 between self-consistency confidence and accuracy on TQ dataset; Eli-Only performs on par with Consis-Sem baseline.
- **Break condition:** If hidden states do not linearly encode uncertainty signals, or if self-consistency fails to correlate with correctness in a new domain, Stage 1 elicitation will not transfer.

### Mechanism 2
- **Claim:** Decomposing honesty alignment into elicitation + calibration reduces annotation requirements from 560k to ~1k correctness labels.
- **Mechanism:** Stage 1 provides a pretrained confidence representation using abundant self-consistency signals (annotation-free). Stage 2 only needs to learn a residual mapping from this elicited confidence to actual correctness, requiring fewer correctness annotations because the model has already learned to express its internal state.
- **Core assumption:** The mapping from elicited confidence to correctness is approximately linear or low-complexity after elicitation.
- **Evidence anchors:** EliCal achieves near-optimal alignment with only 1k correctness annotations (~0.18% of full supervision); EliCal (1k) achieves 84.36 AUROC vs Cal-Only (1k) at 73.41 on in-domain.
- **Break condition:** If Stage 1 fails to provide useful representations, Stage 2 will not converge efficiently and may require more annotations.

### Mechanism 3
- **Claim:** LoRA-based adaptation preserves original QA capabilities while enabling confidence prediction.
- **Mechanism:** By freezing backbone parameters and only training LoRA modules plus a linear head, the model's generative capabilities remain intact while additional capacity learns to map hidden states to confidence scores.
- **Core assumption:** LoRA rank-8, α=16 provides sufficient expressive power for confidence prediction without interfering with frozen representations.
- **Evidence anchors:** Only LoRA modules and linear head are updated during training; frozen backbone preserves original QA capabilities.
- **Break condition:** If LoRA rank is insufficient, or if confidence prediction gradients interfere with frozen representations, QA performance may degrade or confidence predictions may be noisy.

## Foundational Learning

- **Concept:** Self-consistency as uncertainty estimation
  - **Why needed here:** The entire Stage 1 supervision comes from self-consistency signals; understanding how to compute and interpret these signals is prerequisite to implementing the elicitation stage.
  - **Quick check question:** Given 20 sampled responses, how would you compute self-consistency confidence against a greedy response?

- **Concept:** AUROC and calibration metrics (ECE)
  - **Why needed here:** The paper evaluates honesty alignment using AUROC (discrimination) and ECE (calibration); interpreting results requires understanding what these metrics measure.
  - **Quick check question:** If a model has AUROC=0.5 on a dataset, what does that imply about its confidence predictions?

- **Concept:** LoRA low-rank adaptation
  - **Why needed here:** The architecture uses LoRA modules on all linear layers; implementing this correctly requires understanding rank, alpha scaling, and where LoRA is applied.
  - **Quick check question:** For a linear layer y=Wx with d=4096, how many parameters does LoRA add with rank=8?

## Architecture Onboarding

- **Component map:** Frozen LLM backbone → LoRA modules on all linear layers (rank=8, α=16) → Final layer hidden state → Linear head → Confidence score
- **Critical path:** Generate 20 sampled responses + 1 greedy response per question → Compute self-consistency labels using semantic similarity → Train Stage 1 on 560k questions with MSE loss → Sample 1k questions with correctness labels → Train Stage 2 starting from Stage 1 checkpoint
- **Design tradeoffs:** Linear head vs deeper MLP (linear head is cheaper but may limit expressiveness); LoRA rank selection (rank=8 chosen); annotation budget (1k annotations suffice for in-domain)
- **Failure signatures:** AUROC near 0.5 (confidence not discriminating correct from incorrect); QA accuracy drops (LoRA interfering); OOD generalization fails (Stage 1 data may not cover target domain)
- **First 3 experiments:** 1) Implement Consis-Sem baseline on HonestyBench-Eval to establish target performance (~73 AUROC). 2) Train EliCal with {1k, 10k, 100k} Stage 2 annotations and plot AUROC scaling curves. 3) Evaluate trained EliCal on MMLU and compare Cal-Only vs EliCal generalization gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is the EliCal framework when the model's self-consistency signals diverge significantly from actual correctness (i.e., consistent hallucinations)?
- Basis in paper: Stage 1 Elicitation relies entirely on self-consistency as supervision. Figure 2 shows a strong correlation (Spearman = 0.789) between self-consistency and accuracy, but it is not perfect, implying a subset of data where the model is consistently wrong.
- Why unresolved: If the model is consistently hallucinating (high self-consistency, zero accuracy), Stage 1 may teach the model to be confidently wrong. It is unclear if the small calibration set (1k samples) is sufficient to correct for systematic bias.
- What evidence would resolve it: An evaluation of EliCal specifically on a filtered subset where the model exhibits high Consis-Sem scores but zero Accuracy, measuring the AUROC before and after calibration.

### Open Question 2
- Question: Can this method for "universal honesty" generalize to tasks that lack discrete ground-truth answers, such as creative writing or open-ended reasoning?
- Basis in paper: Section 5 and Section 3.1 define the task using "factual QA datasets" and a binary correctness indicator. The entire framework assumes the existence of a verifiable "correct" answer for both consistency checking and calibration.
- Why unresolved: The definition of honesty used (confidence ≈ probability of correctness) relies on the existence of ground truth. Applying this to subjective tasks would require redefining the target for the calibration stage.
- What evidence would resolve it: Experiments applying EliCal to non-factual benchmarks where "correctness" is replaced by metrics like human preference alignment or logical validity scores.

### Open Question 3
- Question: Does the annotation efficiency of EliCal scale effectively to models with significantly larger parameter counts (e.g., 70B+) or proprietary architectures?
- Basis in paper: Section 5 limits the study to three models: Qwen2.5-7B, Qwen2.5-14B, and Llama3-8B. The authors claim a "scalable solution," but the specific data efficiency ratio (~0.18% of data) is only empirically validated on mid-sized models.
- Why unresolved: Larger models may exhibit different internal representations of uncertainty or different overconfidence patterns. It is unverified if the 1k annotation sample size is sufficient to calibrate the LoRA adapters of models with significantly higher capacity.
- What evidence would resolve it: Extending the results to include larger open-source models (e.g., Llama-3-70B) to see if the performance gap between EliCal (1k) and the Upper Bound remains consistent.

## Limitations

- Annotation efficiency gains are primarily demonstrated in-domain; OOD generalization to MMLU shows weaker improvements, suggesting the 1k annotation claim may not extend to truly unseen domains without additional Stage 1 data.
- The linear head architecture may not capture all uncertainty information, potentially limiting calibration performance for complex uncertainty representations.
- The framework assumes discrete ground-truth answers exist, limiting applicability to subjective or open-ended tasks where "correctness" is not well-defined.

## Confidence

- **High confidence:** Self-consistency confidence provides a learnable proxy for correctness (Spearman correlation = 0.789; ablation studies support elicitation capability)
- **Medium confidence:** EliCal achieves near-optimal honesty alignment with only 1,000 labeled samples (primarily in-domain; OOD generalization weaker)
- **Medium confidence:** LoRA-based preservation of QA capabilities (architecture clear but lacks detailed ablations on rank selection and relationship to calibration performance)

## Next Checks

1. **Cross-domain elicitation validation:** Test EliCal on a completely disjoint dataset (e.g., coding tasks or medical QA) with minimal Stage 2 annotations to verify if Stage 1 self-consistency signals generalize beyond the training domains.

2. **Linear head capacity analysis:** Replace the linear head with a small MLP (2-3 layers) in both stages to determine if the linear assumption is limiting calibration performance, particularly for complex uncertainty representations.

3. **Annotation efficiency scaling:** Conduct a more granular analysis of Stage 2 annotation requirements across {100, 500, 1k, 5k} samples to establish the precise annotation threshold where EliCal's benefits plateau compared to calibration-only approaches.