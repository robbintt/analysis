---
ver: rpa2
title: 'Cracking the Code of Action: a Generative Approach to Affordances for Reinforcement
  Learning'
arxiv_id: '2504.17282'
source_url: https://arxiv.org/abs/2504.17282
tags:
- template
- image
- actions
- code
- affordable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoGA (Code as Generative Affordances), a
  method that leverages pre-trained vision-language models to automatically generate
  code that determines affordable actions for reinforcement learning agents in GUI-based
  web navigation tasks. By reducing the action space to only relevant actions, CoGA
  significantly improves sample efficiency compared to standard RL agents, achieving
  over 10x faster learning on the MiniWob++ benchmark.
---

# Cracking the Code of Action: a Generative Approach to Affordances for Reinforcement Learning

## Quick Facts
- arXiv ID: 2504.17282
- Source URL: https://arxiv.org/abs/2504.17282
- Authors: Lynn Cherif; Flemming Kondrup; David Venuto; Ankit Anand; Doina Precup; Khimya Khetarpal
- Reference count: 14
- One-line primary result: CoGA reduces action space via intent-based affordances, achieving over 10x faster learning on MiniWob++ web navigation tasks.

## Executive Summary
This paper introduces CoGA (Code as Generative Affordances), a method that leverages pre-trained vision-language models to automatically generate code that determines affordable actions for reinforcement learning agents in GUI-based web navigation tasks. By reducing the action space to only relevant actions, CoGA significantly improves sample efficiency compared to standard RL agents, achieving over 10x faster learning on the MiniWob++ benchmark. The method uses VLMs to extract intents and object templates from task descriptions and observations, then generates and verifies code that returns affordable actions (both action types and pixel coordinates) given a visual observation.

## Method Summary
CoGA generates affordance scripts through a VLM prompting pipeline that identifies task intents and extracts object templates from observations. A critique VLM verifies and refines the generated code through iterative feedback. The resulting scripts use template matching to identify affordable actions at inference time. These scripts are then integrated into a Double DQN agent with action masking, constraining the agent to only sample from relevant actions during both action selection and bootstrapping.

## Key Results
- CoGA achieves over 10x faster learning compared to standard RL agents on MiniWob++ benchmark
- Affordance scripts show high precision and recall (F1-scores near 1.0 on most tasks)
- CoGA matches or exceeds behavior cloning performance with limited expert demonstrations
- The method generalizes within task families, using the same affordance script across related tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraining the action space via intent-based affordances improves sample efficiency by reducing exploration burden.
- Mechanism: CoGA generates code that returns a subset of relevant actions (both action types and pixel coordinates) given an observation. This hard mask prevents the RL agent from sampling irrelevant actions, converting a 4×1024 action space into a much smaller affordable subset per state.
- Core assumption: The VLM can correctly identify intents and detect affordable objects via template matching with sufficient recall; missing affordances (low recall) would block learning.
- Evidence anchors:
  - [abstract] "By greatly reducing the number of actions that an agent must consider... CoGA is orders of magnitude more sample efficient than its RL agent"
  - [Section 3.3] "If the predicted affordances have low recall, CoGA would fail. In such a case, using soft masking during training where unaffordable actions are assigned low probability would allow CoGA to slowly catch up..."
  - [corpus] Weak direct corpus support for this specific mechanism; related work on VLA fine-tuning (π_RL) addresses sample efficiency but via different approaches.
- Break condition: Low-recall affordance scripts prevent policy convergence; tasks requiring fine-grained spatial reasoning (e.g., bisect-angle) where template matching cannot express affordances.

### Mechanism 2
- Claim: VLM-generated code captures affordances more efficiently than querying VLMs at inference time.
- Mechanism: The VLM is prompted once per task (before training) to: (1) identify intents, (2) extract object template images via coordinate-based prompting, (3) write template-matching code. This distills VLM knowledge into executable scripts, avoiding per-step VLM inference costs.
- Core assumption: Pre-trained VLMs possess sufficient grounding between visual elements and coordinate spaces when given gridded reference images.
- Evidence anchors:
  - [Section 3.1] "This pipeline alleviates the need for expensive VLM inference calls during the RL stage."
  - [Section 6.1] "VLMs struggle to parse an image into pixel coordinates... we superimposed a granular coordinate system onto an observation when showing it to the VLM for template image extraction."
  - [corpus] No direct corpus evidence on code-as-affordance distillation; VoxPoser (cited in paper) uses similar code-generation for value maps but in robotics context.
- Break condition: VLM coordinate errors propagate to template extraction; tasks with high visual variability exhaust template coverage.

### Mechanism 3
- Claim: Critique-verified code generation improves affordance script reliability.
- Mechanism: A second VLM reviews generated code against execution errors and sample observations, providing feedback for regeneration. Scripts are scored on precision/recall against 5 manually annotated ground-truth test cases; best-performing scripts are retained.
- Core assumption: Critique VLM can identify meaningful issues without access to ground-truth affordances; manual test cases are representative.
- Evidence anchors:
  - [Section 3.2] "The feedback is reused by the critique VLM to improve and regenerate the code. This process repeats up to 3 times... We retain the best performing scripts across pipeline runs and critique iterations."
  - [Figure 2] F1-scores show high precision/recall on most tasks (e.g., click-test, click-tab near 1.0), with failures on use-slider/use-spinner.
  - [corpus] No corpus evidence on VLM-based code verification for affordances.
- Break condition: Critique VLM mislabels code quality; token limits truncate iterative improvement for complex tasks (e.g., click-shades, use-slider-2 noted as affected).

## Foundational Learning

- Concept: **Affordances in RL (Khetarpal et al., 2020)**
  - Why needed here: CoGA builds on the formal definition of affordances as state-action pairs that complete intents—understanding this distinguishes affordances (task-type-level) from goals (instance-specific).
  - Quick check question: In click-test-2, why is clicking *either* button an affordance, not the goal?

- Concept: **Template Matching (OpenCV)**
  - Why needed here: The generated code uses grayscale normalized cross-correlation (TM_CCOEFF_NORMED) with threshold-based detection; understanding parameters (threshold, grayscale conversion) is essential for debugging failure cases.
  - Quick check question: Why does grayscale template matching improve generalization over color-based detection?

- Concept: **DQN with Action Masking**
  - Why needed here: CoGA applies affordances as hard masks on the action space during both action selection and bootstrapping; understanding how masking affects Q-value updates is critical for debugging learning dynamics.
  - Quick check question: What happens to Q-values of masked actions during training—do they ever get updated?

## Architecture Onboarding

- Component map: Task description + gridded observation -> VLM -> Intents & Object Templates -> Affordance Script -> Critique VLM (iterative refinement) -> Best Script (F1-score) -> DQN with Action Masking -> RL Agent

- Critical path:
  1. Template image quality (VLM coordinate accuracy) ->
  2. Template matching recall (threshold tuning) ->
  3. Affordance script F1-score (determines learning feasibility) ->
  4. RL sample efficiency gain

- Design tradeoffs:
  - **Hard vs. soft masking**: Hard masking provides faster convergence but fails catastrophically with low recall; soft masking is more robust but reduces efficiency gains
  - **Template matching vs. OCR**: Template matching works for visual objects but fails on varying text; OCR is inconsistent on low-resolution MiniWob++ screenshots
  - **Number of template images**: 5 sample observations balance generalization vs. coverage; too few misses variants, too many increases false positives

- Failure signatures:
  - **Low F1-score on affordance script**: Agent cannot learn task (e.g., use-slider, use-spinner with F1 < 0.6)
  - **High precision, low recall**: Agent may converge but slowly; some valid actions never explored
  - **High recall, low precision**: Efficiency gains diminish; approaches baseline RL performance
  - **Token limit errors**: Complex tasks with many affordance elements fail code generation

- First 3 experiments:
  1. **Reproduce F1-score evaluation**: Manually annotate 5 test observations per task; run generated scripts and compute precision/recall to validate template extraction quality before RL training
  2. **Ablate masking strategy**: Compare hard masking vs. soft masking (assign ε-probability to non-affordable actions) on a task with known imperfect recall
  3. **Test cross-task generalization**: Apply click-test-2's affordance script to click-button-sequence (same GUI, different policy) to verify claimed within-family generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does implementing soft masking for unaffordable actions improve robustness and final performance in scenarios where the generated affordance scripts suffer from low recall?
- Basis in paper: [explicit] Section 3.3 states that if predicted affordances have low recall, CoGA fails. It suggests that "using soft masking during training where unaffordable actions are assigned low probability would allow CoGA to slowly catch up," but this is not implemented or tested in the experiments.
- Why unresolved: The current implementation uses hard masking (probability 0), which permanently excludes valid actions if the script misses them, causing the agent to fail. The trade-off between sample efficiency (hard masking) and safety (soft masking) remains unquantified.
- What evidence would resolve it: A comparative study on tasks with imperfect affordance scripts, measuring success rates and sample efficiency curves for hard masking vs. soft masking with various probability penalties.

### Open Question 2
- Question: Can the reliance on manually annotated ground truth test cases for code verification be replaced by a fully automated VLM-generated test suite without compromising reliability?
- Basis in paper: [explicit] Section 6.1 identifies the manual labeling of test cases as a limitation, noting, "one could investigate the VLM's ability to create its own test cases." The authors also acknowledge that the critique VLM is not always correct in its assessments.
- Why unresolved: The current pipeline depends on human verification to select the best script. Automating this step is necessary for scaling to diverse tasks but risks reinforcing hallucinations if the VLM grades its own work inaccurately.
- What evidence would resolve it: A demonstration of a "self-supervised" verification pipeline where VLM-generated test cases select scripts that achieve comparable F1-scores and RL performance to those selected via human-annotated ground truth.

### Open Question 3
- Question: To what extent can replacing template matching with robust Optical Character Recognition (OCR) or grounding models improve CoGA's performance on tasks involving dynamic text-based objects?
- Basis in paper: [explicit] Section 6.1 notes that template matching is a limitation for "varying text-based objects" and explicitly states, "In future work, one might explore using more robust OCR tools." The authors excluded text-entry tasks due to inconsistent OCR results.
- Why unresolved: The paper relies on template matching (OpenCV), which fails when object appearances (like text labels) vary. Determining if modern OCR or foundation models for grounding can reliably extract these affordances is critical for broader applicability.
- What evidence would resolve it: An ablation study on previously excluded or low-performing text-based MiniWob++ tasks (e.g., `email-inbox`) comparing the precision/recall of template matching against a VLM-based grounding or OCR approach.

### Open Question 4
- Question: How does CoGA's performance scale when integrated with more sophisticated RL agents (e.g., Transformer-based or policy gradient methods) in complex, partially observable environments?
- Basis in paper: [explicit] Section 6.2 states: "A promising direction for future work is to augment more competent RL agents with CoGA, as our method is complimentary to any RL algorithm." The authors note their results were limited by the DQN backbone in complex tasks.
- Why unresolved: The experiments utilized a Double DQN agent. It is unclear if the sample efficiency gains from affordance masking persist or diminish when used with state-of-the-art architectures that may have different exploration properties.
- What evidence would resolve it: Experiments applying CoGA masks to a Transformer-based agent or PPO agent on multi-step tasks (e.g., `click-checkboxes-large`), comparing the learning curve against the DQN baseline.

## Limitations
- Template matching limits generalization to tasks with highly variable visual elements or text-based objects
- Method performance depends critically on achieving high recall in affordance scripts, with no clear recovery mechanism when recall is low
- Evaluation confined to MiniWob++ benchmark, leaving generalization to real-world GUI tasks unverified

## Confidence
- **High confidence**: Core mechanism of using code generation to distill VLM affordances and measured efficiency gains on MiniWob++ tasks where scripts achieve F1 > 0.8
- **Medium confidence**: Generalizability claims to other GUI tasks and robustness of critique-verified code generation process, given limited ablation studies and no real-world testing
- **Low confidence**: Scalability to complex tasks with many affordances due to token limit constraints and manual effort required for ground-truth affordance annotation

## Next Checks
1. **Reproduce F1-score evaluation**: Manually annotate 5 test observations per task; run generated scripts and compute precision/recall to validate template extraction quality before RL training
2. **Ablate masking strategy**: Compare hard masking vs. soft masking (assign ε-probability to non-affordable actions) on a task with known imperfect recall
3. **Test cross-task generalization**: Apply click-test-2's affordance script to click-button-sequence (same GUI, different policy) to verify claimed within-family generalization