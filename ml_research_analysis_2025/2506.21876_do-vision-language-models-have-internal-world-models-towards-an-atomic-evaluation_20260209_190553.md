---
ver: rpa2
title: Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation
arxiv_id: '2506.21876'
source_url: https://arxiv.org/abs/2506.21876
tags:
- world
- perception
- object
- arxiv
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a systematic benchmark to evaluate the fundamental
  world modeling capabilities of vision-language models. The authors develop a two-stage
  framework that assesses perception (visual, spatial, temporal, quantitative, and
  motion) and prediction (mechanistic simulation, transitive inference, compositional
  inference) to provide atomic evaluation.
---

# Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation

## Quick Facts
- **arXiv ID:** 2506.21876
- **Source URL:** https://arxiv.org/abs/2506.21876
- **Reference count:** 40
- **Key outcome:** VLMs show near-random accuracy on motion trajectory tasks and significant attribute entanglement, with prediction accuracy only 40-85% versus human performance near 100%

## Executive Summary
This paper systematically evaluates whether vision-language models (VLMs) possess internal world models by decomposing world modeling into atomic perception and prediction tasks. The authors develop a two-stage framework that isolates 5 perception dimensions (visual, spatial, temporal, quantitative, motion) and 3 prediction types (mechanistic simulation, transitive inference, compositional inference). Using 6 simulated environments and 660 experiments across 15 models, they find VLMs perform well on single-frame perception but fail dramatically on tasks requiring temporal reasoning, physical simulation, and disentangled representations. The benchmark reveals that current VLMs lack the coherent, mechanistic understanding of physical dynamics that humans possess, with performance gaps particularly severe in motion trajectory prediction and compositional reasoning tasks.

## Method Summary
The authors create a systematic benchmark evaluating VLMs' world modeling capabilities through controlled counterfactual experiments across 6 simulated environments. The framework tests 23 fine-grained dimensions: 15 perception tasks (spatial, temporal, quantitative, motion, visual) and 15 prediction tasks (mechanistic simulation, transitive inference, compositional inference). Each task presents models with multiple-choice questions requiring identification of correct physical states or predictions. Counterfactual states are generated by perturbing either actions or previous states while holding other variables constant, creating visually similar distractors that test genuine mechanistic understanding. Performance is measured through accuracy metrics and standardized Relative Entanglement (s-RE) analysis to quantify attribute independence. The evaluation uses standardized prompts with greedy decoding across all models.

## Key Results
- VLMs achieve near-random accuracy (40-60%) on motion trajectory prediction despite 80-95% accuracy on single-frame motion detection
- Significant attribute entanglement observed (s-RE >10%) with spurious correlations like color-speed associations
- Prediction accuracy remains low (40-85%) even after filtering out perceptual errors, indicating mechanistic knowledge gaps
- Human performance approaches 100% accuracy on most tasks, highlighting substantial VLM limitations
- Stimulus differentiability scaling shows VLMs succeed only with large physical differences, struggling with fine-grained distinctions

## Why This Works (Mechanism)

### Mechanism 1: Controlled Counterfactual Disentanglement
Systematic manipulation of single dimensions while holding others constant enables causal attribution of model failures to specific perceptual or reasoning deficits. The benchmark generates counterfactual states by perturbing either actions or previous states, creating visually similar distractors that require genuine mechanistic understanding to distinguish from ground truth. Models that rely on spurious correlations will fail on counterfactuals, while models with true world understanding will succeed.

### Mechanism 2: Dual-Stage Failure Attribution
World modeling failures decompose into two distinct sources: perceptual encoding errors and mechanistic prediction errors, which can be isolated through filtered evaluation. By filtering instances where models correctly answer perception questions, residual prediction errors reveal whether failure stems from representation quality or transition knowledge.

### Mechanism 3: Stimulus Differentiability Scaling
VLM performance scales with physical magnitude of differences rather than exhibiting uniform perceptual resolution, revealing a coarse-grained "myopia" in world representation. Models succeed when stimulus differences are large but fail at fine-grained distinctions, suggesting statistical pattern matching rather than precise physical grounding.

## Foundational Learning

- **World Model (as distinct from predictive model):** Why needed: The paper specifically evaluates mechanistic world models—not just statistical predictors. A regression model can predict loan default without simulating causal dynamics. Quick check: Can you explain why a high-accuracy XGBoost model for collision prediction might still fail the paper's mechanistic simulation tests?

- **Disentangled Representation:** Why needed: The entanglement analysis (s-RE metric) assumes orthogonal perceptual dimensions should be independently processable. Without this concept, color-speed confusions seem arbitrary. Quick check: If a model associates blue with "fast," what test would distinguish spurious correlation from valid physical knowledge (e.g., blue cars being sports cars)?

- **Transitive vs. Compositional Inference:** Why needed: The prediction stage distinguishes chaining predictions over time (transitive) from merging concurrent mechanisms (compositional). These require different reasoning architectures. Quick check: Why is predicting "ball C moves up after simultaneous hits from A and B" compositional rather than just transitive?

## Architecture Onboarding

- **Component map:** Perception layer (5 dimensions → 15 subtasks) → Prediction layer (3 inference types → 15 subtasks) → Simulator infrastructure (6 environments → 100K+ instances) → Evaluation pipeline (Counterfactual generation → Model inference → Filtered vs. unfiltered analysis)

- **Critical path:** Perception tasks → Mechanistic simulation → Transitive inference → Compositional inference (each stage builds on the previous)

- **Design tradeoffs:** Simulator diversity vs. controlled comparison (multiple simulators increase generalization but introduce domain confounds); Synthetic vs. real data (controlled counterfactuals require simulation; real-world validation shows similar patterns but with smaller scale); Frontier model subset evaluation (computational constraints forced 100-instance sampling, reducing statistical power)

- **Failure signatures:** Near-random on MT (Motion Trajectory) despite high MD (Motion Detection) → temporal coherence deficit; s-RE >10% on Color × Quantity tasks → attribute entanglement; Prediction accuracy unchanged after perceptual filtering → mechanistic knowledge gap

- **First 3 experiments:** 1) Run baseline evaluation on single simulator (e.g., ManiSkill spatial tasks) to establish evaluation pipeline and parsing reliability; 2) Implement entanglement analysis: For Discrete Quantity task, vary only object color while holding count constant; compute s-RE to quantify color-number confounds; 3) Execute filtered prediction analysis: On Physion collision tasks, first verify model correctly identifies moving object/color/shape, then evaluate prediction accuracy on filtered subset vs. full set

## Open Questions the Paper Calls Out

### Open Question 1
Can VLMs be trained to develop truly disentangled representations of world attributes, eliminating spurious correlations like color-speed associations? The authors find VLMs lack disentangled understanding and demonstrate significant entanglement between perceptual dimensions, but do not propose interventions to create independent, robust world representations.

### Open Question 2
What architectural or training modifications would enable VLMs to form coherent dynamic scene representations for motion trajectory and temporal extension tasks? Current VLMs perform well on single-frame tasks but fail on tasks requiring integration across consecutive frames; the paper identifies the gap but not the solution.

### Open Question 3
Do the world modeling limitations identified in simulated environments generalize to real-world visual scenarios with comparable severity? The authors acknowledge simulators may evaluate VLMs on somewhat out-of-distribution data and include limited real-world validation showing consistent alignment, but this was only on ~50 data points per dimension.

## Limitations
- Evaluation relies entirely on simulated environments, which may not fully capture real-world physical dynamics and visual complexity
- Prompt engineering approach uses standardized template across all tasks, which may systematically underestimate VLM capabilities
- Filtering methodology assumes perceptual and prediction capabilities are modular, but this assumption requires further validation

## Confidence
- **High Confidence:** Near-random performance on motion trajectory tasks (40-60% accuracy) is well-supported across multiple environments and model families; s-RE entanglement metric findings are statistically robust
- **Medium Confidence:** Mechanistic simulation failure attribution is methodologically sound but depends on filtering assumption validity; stimulus differentiability scaling claims are supported but could be influenced by task design confounds
- **Low Confidence:** Claims about VLM performance relative to human baselines should be interpreted cautiously given different testing conditions; generalizability of findings across diverse real-world applications remains uncertain

## Next Checks
1. **Interactive Prompt Testing:** Implement dynamic, task-specific prompt engineering that adapts based on model responses rather than using a standardized template. Compare results against the baseline to quantify potential underestimation of VLM capabilities.

2. **Physical Validation Study:** Conduct controlled experiments with robotic systems in physical environments to validate simulation findings. Test whether VLMs can predict outcomes of simple physical interactions (ball rolling down ramps, object collisions) with the same accuracy patterns observed in simulation.

3. **Cross-Domain Transfer Evaluation:** Assess VLM performance on the same atomic tasks across multiple real-world domains (robotics manipulation, autonomous driving scenarios, household environments) to determine if the identified limitations persist outside controlled simulations.