---
ver: rpa2
title: Describe What You See with Multimodal Large Language Models to Enhance Video
  Recommendations
arxiv_id: '2508.09789'
source_url: https://arxiv.org/abs/2508.09789
tags:
- video
- recommendation
- audio
- mllms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a framework that enhances video recommendation
  by using off-the-shelf Multimodal Large Language Models (MLLMs) to generate rich
  natural-language summaries of video clips. These summaries, which capture high-level
  semantics, intent, and context, are integrated into standard recommendation models
  via text embeddings.
---

# Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations

## Quick Facts
- **arXiv ID:** 2508.09789
- **Source URL:** https://arxiv.org/abs/2508.09789
- **Reference count:** 40
- **Primary result:** MLLM-generated captions boost video recommendation metrics by up to 60% relative improvement over traditional features.

## Executive Summary
This work proposes using off-the-shelf Multimodal Large Language Models (MLLMs) to generate rich natural-language summaries of video clips for enhanced video recommendation. The framework extracts text captions from video and audio using Qwen-VL, Whisper, and Qwen-Audio, then encodes them via BGE-large to produce content representations. Experiments on the MicroLens-100K dataset show these MLLM-derived features significantly outperform traditional visual, audio, and metadata features, achieving up to 60% relative improvement in HR@10 and nDCG@10. The approach is model-agnostic, zero-finetuning, and demonstrates that leveraging MLLMs as on-the-fly knowledge extractors can yield more intent-aware and effective video recommendations.

## Method Summary
The method processes raw video and audio through a two-stage pipeline: first, Qwen-VL generates video captions from frames (extracted from the first 30 seconds), and Whisper plus Qwen-Audio generates audio captions from transcriptions and sound classification. These captions are concatenated and encoded into fixed-length embeddings using BGE-large. The resulting item embeddings are then used in standard recommendation architectures (Two-Towers or SASRec) trained to predict user interactions. The entire framework operates in a zero-finetuning manner, treating the MLLMs as frozen feature extractors.

## Key Results
- MLLM-generated video features improve HR@10 by ~24% over raw VideoMAE features in Two-Towers.
- MLLM-generated audio features achieve ~60% relative gain in HR@10 over raw audio features.
- The framework is model-agnostic, working with both Two-Towers and SASRec architectures.
- Scaling MLLM backbone size (to 7B) offers diminishing returns, suggesting the baseline model already captures relevant information.

## Why This Works (Mechanism)

### Mechanism 1
Replacing low-level visual/audio features with high-level MLLM-generated text descriptions improves recommendation ranking metrics. MLLMs convert raw pixels and waveforms into natural language that captures intent, cultural references, and humor ("world knowledge"), bridging the "semantic gap" where traditional features fail to distinguish between a serious event and a parody. The text encoder (BGE-large) maps these rich descriptions into a vector space that aligns better with user intent than the vector space of raw embeddings. Break condition: If the MLLM hallucinates frequently or produces generic descriptions, the resulting embeddings may fail to discriminate between distinct items, degrading performance.

### Mechanism 2
A dedicated two-stage audio pipeline (transcription + knowledge fusion) captures thematic and tonal cues ("mood") that raw spectrogram-based features miss. The pipeline first extracts text/sound classes via Whisper, then feeds them to Qwen-Audio to generate intent-aware descriptions (e.g., "upbeat soundtrack"). This recovers lost semantic information from the audio modality. Break condition: If the video relies heavily on non-speech audio where Whisper fails to transcribe meaningful text, the audio description may be too sparse to help.

### Mechanism 3
Using a frozen, off-the-shelf MLLM as a feature extractor is sufficient for recommendation tasks, avoiding the instability or cost of fine-tuning. By freezing the MLLM, the system treats the model as a static "knowledge extractor," and the downstream recommender learns to align with the MLLM's output distribution without updating the MLLM's weights. Break condition: If the downstream task requires identifying very specific, subtle details, a frozen model's generalized features might lack the necessary granularity compared to a fine-tuned encoder.

## Foundational Learning

- **Concept: Two-Tower vs. Generative Architectures**
  - *Why needed:* The paper tests the framework on two distinct architectures to prove it is "model-agnostic." You must understand that Two-Towers typically retrieve via embedding similarity (dot product), while SASRec (Generative) predicts next items via sequential transformers.
  - *Quick check:* Does the MLLM feature injection happen in the user encoder or the item encoder? (Answer: Item encoder).

- **Concept: Semantic Gap in Computer Vision**
  - *Why needed:* The core problem statement is that raw features (pixels/spectrograms) lack "semantics" (meaning/intent). Understanding this gap explains *why* converting video-to-text is a logical step for retrieval.
  - *Quick check:* Why does an optical flow feature fail to identify a "parody"? (Answer: It detects motion, not cultural context).

- **Concept: Zero-Shot Prompting**
  - *Why needed:* The method relies on prompting an MLLM without training examples. The quality of the output depends entirely on the prompt design and the model's pre-existing knowledge.
  - *Quick check:* What is the primary risk of zero-shot prompting in this architecture? (Answer: Hallucination or inconsistent formatting).

## Architecture Onboarding

- **Component map:** Raw Video + Raw Audio -> Qwen-VL (frames -> Video Caption) + Whisper + Qwen-Audio (transcription/sound class -> Audio Caption) -> Concatenate Captions -> BGE-large (encode -> Item Embedding) -> Two-Towers or SASRec (User/Item Encoders)
- **Critical path:** The Audio Knowledge Fusion stage is the most brittle. If Whisper fails to transcribe (e.g., music-only clips), Qwen-Audio must rely solely on sound classification, potentially reducing the semantic richness required for the reported 60% gain.
- **Design tradeoffs:**
  - *Latency vs. Semantics:* Generating captions via MLLMs adds significant inference latency compared to extracting video embeddings (VideoMAE). This is likely an offline process (batch), not real-time.
  - *Fixed vs. Adaptive:* Using frozen models lowers cost but limits the system's ability to adapt to new slang or trends not present in the MLLM's training data.
- **Failure signatures:**
  - **Hallucination Cascade:** If Qwen-VL invents entities not in the video (e.g., "Eiffel Tower" visible in background when it's not), the recommender will surface irrelevant items.
  - **Modality Mismatch:** If the text encoder (BGE) fails to align the caption "upbeat soundtrack" with user interaction patterns (e.g., users actually liking the video for the *visuals*, not the audio), the signal adds noise.
- **First 3 experiments:**
  1. **Baseline Sanity Check:** Train Two-Towers on raw VideoMAE vs. MLLM-Text features on a 10K subset. Verify if HR@10 delta mirrors the paper's ~24% gain before scaling.
  2. **Audio Ablation:** Run the pipeline with MLLM-Video only vs. MLLM-Video+Audio. Confirm if audio adds the specific "intent" boost (checking for the 60% lift in audio-heavy subsets).
  3. **Qualitative Inspection:** Sample 50 videos where the recommender failed using raw features but succeeded with MLLM features. Manually check if the MLLM captions contain "world knowledge" (e.g., named entities, parodies) absent in raw metadata.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the proposed framework maintain its performance advantage when applied to long-form video content (e.g., movies or long YouTube videos) beyond the tested 4-minute limit?
  - *Basis:* The authors restrict the evaluation to a maximum of 4 minutes for "computational efficiency," acknowledging that MLLMs can theoretically process longer videos but leaving this specific application untested.
  - *Why unresolved:* The MicroLens-100K dataset consists of short-form "TikTok-style" clips; the scalability of the single-summary approach for much longer, complex temporal dependencies remains undemonstrated.
  - *What evidence would resolve it:* Evaluation results on a long-form video benchmark (e.g., MovieLens) showing that a single MLLM caption can effectively represent full-length content without losing critical narrative context.

- **Open Question 2:** Can unified "omni-MLLMs" that natively process audio and video jointly outperform the proposed modular two-stage pipeline?
  - *Basis:* The conclusion explicitly identifies "emerging omni-MLLMs" (e.g., Qwen Omni) as a promising future direction, contrasting them with the paper's decoupled approach of using separate Whisper, Qwen-Audio, and Qwen-VL models.
  - *Why unresolved:* The current modular design may fail to capture subtle cross-modal synergies (e.g., tone matching visual irony) that a natively integrated attention mechanism might preserve.
  - *What evidence would resolve it:* A comparative study benchmarking the modular pipeline against a unified omni-model on the same MicroLens-100K dataset.

- **Open Question 3:** Why do larger MLLM backbones fail to improve recommendation metrics despite generating qualitatively richer captions?
  - *Basis:* Table 2 shows that swapping the baseline MLLM for a larger 7B model yielded no performance gain. The authors note that while the larger model produces "richer captions," it offers "diminishing returns."
  - *Why unresolved:* It is unclear if the bottleneck lies in the fixed size of the downstream text encoder, the recommender model's capacity, or if the additional semantic details in the 7B output act as noise rather than signal.
  - *What evidence would resolve it:* Ablation studies pairing the 7B MLLM with larger text encoders or varying the prompt specificity to determine if the extra information is simply being compressed away or ignored.

## Limitations

- The framework relies on frozen MLLMs as feature extractors without finetuning, sacrificing adaptability to domain-specific trends or emerging cultural references.
- The reported gains depend heavily on the specific prompt design and the quality of the Qwen-VL/Qwen-Audio models, which are not open-sourced for verification.
- The complex 7-day rolling window evaluation introduces significant risk of data leakage if not implemented precisely.

## Confidence

- **High confidence:** The core mechanism of converting multimodal signals to text descriptions works as described, given the reported quantitative improvements over traditional features.
- **Medium confidence:** The specific claim that MLLM features outperform VideoMAE by ~24% in Two-Towers and achieve 60% relative gains in audio-heavy cases is credible but requires exact hyperparameter replication to verify.
- **Low confidence:** The claim about model-agnosticism across Two-Towers and SASRec is weakly supported, as SASRec results are not presented in the main paper body (only referenced in supplement).

## Next Checks

1. **Data Leakage Audit:** Implement the 7-day rolling window split with timestamp validation to ensure no interactions from day k appear in training sets.
2. **Prompt Quality Assessment:** Conduct a manual review of 50 MLLM-generated captions to quantify hallucination rates and verify semantic richness claims (presence of named entities, cultural references).
3. **Audio-Heavy Subset Analysis:** Filter the dataset to videos where audio content dominates (music, speech) and measure if the claimed 60% audio feature gains replicate in this controlled subset.