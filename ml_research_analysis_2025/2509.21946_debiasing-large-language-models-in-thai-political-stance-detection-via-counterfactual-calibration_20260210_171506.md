---
ver: rpa2
title: Debiasing Large Language Models in Thai Political Stance Detection via Counterfactual
  Calibration
arxiv_id: '2509.21946'
source_url: https://arxiv.org/abs/2509.21946
tags:
- stance
- political
- bias
- sentiment
- thai
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ThaiFACTUAL, a lightweight, model-agnostic
  framework for debiasing large language models in Thai political stance detection.
  The authors address the problem of systematic biases in LLMs, particularly sentiment-stance
  entanglement and entity favoritism, which compromise fairness and reliability in
  politically sensitive contexts.
---

# Debiasing Large Language Models in Thai Political Stance Detection via Counterfactual Calibration

## Quick Facts
- **arXiv ID:** 2509.21946
- **Source URL:** https://arxiv.org/abs/2509.21946
- **Reference count:** 10
- **Key outcome:** Introduces ThaiFACTUAL, a lightweight, model-agnostic framework for debiasing LLMs in Thai political stance detection by reducing sentiment-stance entanglement and entity favoritism without requiring model fine-tuning.

## Executive Summary
This paper addresses the problem of systematic biases in large language models when performing political stance detection, specifically the entanglement of sentiment and stance, and favoritism toward certain political entities. ThaiFACTUAL introduces a novel framework that uses counterfactual data augmentation and rationale-based supervision to debias LLMs without requiring model fine-tuning. The authors release the first high-quality Thai political stance dataset with annotations for stance, sentiment, rationales, and bias markers. Experimental results show that ThaiFACTUAL significantly reduces spurious correlations, improves zero-shot generalization, and enhances fairness across multiple LLMs, achieving the lowest bias-SSC of 9.8 and highest OOD generalization of 65.2.

## Method Summary
ThaiFACTUAL is a model-agnostic debiasing framework that operates post-hoc without requiring fine-tuning of the base LLM. The method uses counterfactual data augmentation to generate entity-swapped variants of political tweets, preserving sentiment while changing political targets. A lightweight calibration module re-scores LLM predictions by comparing outputs across counterfactual pairs and incorporating rationale alignment. The framework leverages human-annotated rationales that explicitly link stance to entity-specific reasoning rather than sentiment, enabling the calibration module to downweight sentiment-driven predictions. The approach is evaluated on Thai political stance detection using a novel dataset of 270 tweets about three political figures, measuring bias reduction through Bias-SSC and RStd metrics while maximizing Macro-F1 and OOD generalization.

## Key Results
- ThaiFACTUAL achieves Bias-SSC of 9.8, significantly lower than baseline (21.7) and other debiasing methods
- OOD generalization improves from 56.4 (baseline) to 65.2 with ThaiFACTUAL
- The framework maintains high Macro-F1 of 73.5 while reducing spurious sentiment-to-stance correlations
- Zero-shot generalization is enhanced across multiple LLMs including GPT-4 and LLaMA-3

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Counterfactual entity substitution breaks spurious sentiment-to-stance mappings by forcing target-dependent reasoning.
- **Mechanism:** The framework generates paired inputs where political entities are swapped (e.g., "Pita" → "Thaksin") while preserving lexical sentiment cues. A calibration module compares predictions across counterfactual pairs to identify and downweight predictions driven by sentiment rather than entity-specific reasoning.
- **Core assumption:** Sentiment cues are non-causal features for stance; the true causal feature is the relationship between text content and the specific political target.
- **Evidence anchors:**
  - [abstract] "ThaiFACTUAL uses counterfactual data augmentation and rationale-based supervision to disentangle sentiment from stance."
  - [Page 4, Table 1] "Counterfactual calibration breaks spurious sentiment-to-stance mapping."
  - [Page 6, Appendix B] "This substitution forces the model to focus on the political target rather than reusing learned sentiment-to-stance correlations."
  - [corpus] Related work (arXiv:2509.15361) shows counterfactual inference effectively debiases multimodal LLMs by isolating causal features.
- **Break condition:** If sentiment actually carries causal information for stance in some contexts (e.g., sarcastic praise), counterfactual swaps may remove legitimate signal. The paper acknowledges this limitation for "abstract ideologies" beyond named entities.

### Mechanism 2
- **Claim:** Rationale-based supervision provides an intermediate reasoning scaffold that separates affective tone from political position.
- **Mechanism:** Human-annotated rationales explicitly link stance to entity-specific reasoning rather than sentiment. These rationales condition the calibration module to upweight predictions aligning with target-focused explanations and downweight sentiment-driven predictions.
- **Core assumption:** Rationales encode the true causal reasoning process humans use for stance attribution, which differs from LLMs' sentiment-correlation heuristics.
- **Evidence anchors:**
  - [Page 3] "We introduce the concept of a neutral rationale to disentangle sentiment from stance... highlighting the necessity of reasoning about political alignment independently of emotional language."
  - [Page 7, Appendix E.2] "Rationale Text: A short explanation explicitly linking stance and sentiment, often used to guide model training."
  - [corpus] Weak direct evidence; related stance detection work (arXiv:2502.19276) uses disentangled representations but not rationale supervision specifically.
- **Break condition:** If rationales are inconsistent across annotators or fail to capture implicit cultural reasoning (e.g., Thai sarcasm), supervision signal degrades. Paper reports Fleiss' κ = 0.84, suggesting adequate but not perfect agreement.

### Mechanism 3
- **Claim:** Post-hoc calibration enables debiasing for black-box LLMs without parameter access.
- **Mechanism:** A lightweight auxiliary model learns to rescore LLM outputs using features from counterfactual pairs and rationale alignment. The base LLM remains frozen; calibration happens via output adjustment rather than weight modification.
- **Core assumption:** Bias manifests primarily in output distributions that can be corrected post-hoc, rather than in internal representations requiring fine-tuning.
- **Evidence anchors:**
  - [Page 2] "Instead of altering the base LLM, we construct auxiliary calibration models that learn to adjust the output stance label."
  - [Page 7, Appendix D] "For ThaiFACTUAL, counterfactual data was injected as an auxiliary correction layer—LLMs predict, then a small calibration module re-scores."
  - [corpus] arXiv:2507.02595 (MPF) similarly uses post-deployment alignment, supporting feasibility of post-hoc approaches.
- **Break condition:** If bias originates from deep representational entanglement that cannot be surfaced through output analysis alone, calibration will underperform fine-tuning approaches. Paper does not compare against parameter-modifying baselines.

## Foundational Learning

- **Concept: Counterfactual causal inference**
  - Why needed here: Understanding what "would have happened" if the entity changed while sentiment stayed constant is the core reasoning pattern ThaiFACTUAL exploits.
  - Quick check question: Given "Thaksin is corrupt" → Against, what stance should "Paetongtarn is corrupt" predict if the model uses entity-independent reasoning?

- **Concept: Spurious correlation vs causal feature**
  - Why needed here: The paper's central claim is that sentiment is spuriously correlated with stance in LLM training data, while entity-content relationship is causal.
  - Quick check question: If training data always shows positive sentiment with "support" labels, how can you distinguish whether sentiment causes stance or merely correlates with it?

- **Concept: Distribution shift in political discourse**
  - Why needed here: OOD generalization (65.2 vs 56.4 baseline) matters because political entities and discourse patterns change rapidly; models must generalize to unseen figures.
  - Quick check question: Why would a model trained on 2023 Thai politics fail on a new political figure emerging in 2025?

## Architecture Onboarding

- **Component map:**
  Dataset Layer -> Counterfactual Generator -> Base LLM -> Calibration Module -> Evaluation Suite

- **Critical path:**
  1. Curate balanced entity/stance/sentiment tuples (avoid lexical leakage)
  2. Generate counterfactual pairs for each input
  3. Extract LLM predictions for original + counterfactual
  4. Train calibration module to minimize prediction variance across counterfactuals while maximizing rationale alignment
  5. Evaluate on held-out entity (OOD test)

- **Design tradeoffs:**
  - **Entity-substitution only vs. broader counterfactuals:** Current approach handles named figures well but not abstract ideologies; paper notes automation of broader counterfactuals is unsolved.
  - **Post-hoc vs. fine-tuning:** Enables black-box deployment but limits access to internal representations that could improve debiasing.
  - **Dataset size (270 samples) vs. coverage:** Enables controlled experiments but limits generalization; paper acknowledges need for larger dynamic corpora.

- **Failure signatures:**
  - **High Bias-SSC (>15):** Counterfactuals not breaking sentiment-stance mapping; check entity swap quality
  - **Low OOD (<55):** Calibration overfitting to seen entities; increase rationale diversity
  - **High RStd (>10):** Prediction instability across stance classes; rebalance training data
  - **Rationale-annotation disagreement:** Cultural nuances missed; review annotator guidelines for sarcasm/indirect speech

- **First 3 experiments:**
  1. **Baseline bias audit:** Run GPT-4 raw predictions on all 270 samples; compute Bias-SSC, RStd. Expect ~21.7 SSC per Table 1.
  2. **Counterfactual-only ablation:** Apply entity-swapping calibration without rationale supervision. If SSC drops <15, rationales provide significant additional signal.
  3. **OOD holdout test:** Train calibration on Paetongtarn + Pita, evaluate on Thaksin. Target >60 OOD to confirm generalization beyond memorized entities.

## Open Questions the Paper Calls Out

- **Question:** Can counterfactual augmentation be automated to capture broader political events and abstract ideologies beyond entity substitution?
  - **Basis in paper:** [explicit] The authors state that "counterfactual augmentation is currently restricted to entity substitutions and does not yet capture broader political events or abstract ideologies, with automated generation still an open challenge."
  - **Why unresolved:** Current counterfactual construction relies on manual entity swapping; generating semantically coherent counterfactuals for complex political events or ideological concepts requires nuanced semantic rewriting capabilities not yet developed.
  - **What evidence would resolve it:** An automated system that generates counterfactuals for protest scenarios, policy debates, or ideological positions with human-evaluated semantic preservation and effectiveness in reducing bias comparable to entity-based counterfactuals.

- **Question:** How do debiased stance predictions propagate into downstream applications such as political event forecasting, misinformation detection, or ideological clustering?
  - **Basis in paper:** [explicit] The authors acknowledge they "do not explicitly measure downstream impacts on tasks such as political event forecasting, misinformation detection, or ideological clustering. Future research should examine how debiased stance predictions propagate into these broader applications."
  - **Why unresolved:** The evaluation focuses narrowly on stance classification metrics rather than measuring whether fairness improvements transfer to downstream tasks.
  - **What evidence would resolve it:** Benchmarks showing performance changes on misinformation detection, event forecasting, and ideological clustering when using ThaiFACTUAL-debiased versus raw LLM stance predictions.

- **Question:** Does integrating counterfactual signals during instruction-tuning or fine-tuning achieve stronger debiasing than post-hoc calibration?
  - **Basis in paper:** [explicit] The authors note that "ThaiFACTUAL operates in a post-hoc black-box setting, limiting deeper integration of counterfactual signals" and suggest "future work may explore integrating counterfactual signals earlier in the training pipeline, such as during instruction-tuning or fine-tuning, to achieve stronger debiasing."
  - **Why unresolved:** The model-agnostic design prioritizes deployment flexibility over accessing internal representations, making training-stage integration unexplored.
  - **What evidence would resolve it:** Comparative experiments showing bias reduction when counterfactual supervision is applied during fine-tuning versus post-hoc calibration, with metrics for bias-SSC, RStd, and OOD generalization.

## Limitations
- The calibration module architecture remains underspecified, creating ambiguity about implementation details
- The 270-sample dataset, though carefully balanced, may not capture the full diversity of Thai political discourse
- The approach handles named political figures well but not abstract ideologies or broader political events
- The model-agnostic design limits access to internal representations that could improve debiasing

## Confidence

- **High confidence:** Sentiment-stance entanglement exists in LLMs (supported by baseline Bias-SSC 21.7)
- **Medium confidence:** ThaiFACTUAL's post-hoc calibration approach works without fine-tuning (mechanism described but architecture details missing)
- **Medium confidence:** Rationale-based supervision provides measurable benefit (improvement from counterfactual-only to full ThaiFACTUAL noted but mechanism unclear)

## Next Checks
1. **Architecture Verification:** Implement multiple calibration architectures (linear classifier, MLP, prompt-based) to determine which configuration reproduces the reported Bias-SSC of 9.8
2. **Ablation Testing:** Compare ThaiFACTUAL against a counterfactual-only baseline and a rationale-only baseline to quantify individual contribution of each component
3. **Cross-Domain Transfer:** Evaluate on non-Thai political data or abstract political positions (beyond named entities) to test generalizability beyond the 3 Thai political figures