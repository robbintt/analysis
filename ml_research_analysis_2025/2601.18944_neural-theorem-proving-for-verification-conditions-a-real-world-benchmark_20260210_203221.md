---
ver: rpa2
title: 'Neural Theorem Proving for Verification Conditions: A Real-World Benchmark'
arxiv_id: '2601.18944'
source_url: https://arxiv.org/abs/2601.18944
tags:
- verification
- benchmark
- program
- why3
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first real-world benchmark for neural theorem
  proving in program verification. The authors extract verification conditions (VCs)
  from industrial verification projects using Why3 and Frama-C, then translate them
  into Isabelle, Lean, and Rocq using expert-written rules.
---

# Neural Theorem Proving for Verification Conditions: A Real-World Benchmark

## Quick Facts
- arXiv ID: 2601.18944
- Source URL: https://arxiv.org/abs/2601.18944
- Reference count: 40
- Primary result: Best neural model achieves only 2.08% pass rate on real-world verification conditions versus 18% for classical hammers

## Executive Summary
This paper presents the first benchmark for neural theorem proving on real-world program verification conditions. The authors extract VCs from industrial verification projects using Why3 and Frama-C, then translate them into Isabelle, Lean, and Rocq using expert-written rules. They evaluate both fine-tuned and general-purpose language models on this benchmark, finding that neural approaches lag significantly behind classical hammers. Error analysis reveals models struggle with syntactic complexity, semantic confusion, and hallucination. The benchmark and extraction method provide a foundation for advancing neural approaches to automated program verification.

## Method Summary
The benchmark pipeline extracts verification conditions from real-world C programs using Why3 and Frama-C, then translates them to Isabelle, Lean, and Rocq using ~2,400 expert-written rules. The "complication process" erases annotations (assert, lemma, lemma application) to restore VCs to their fully-automated ideal form. Models generate proofs zero-shot with context, and outputs are verified by ITPs with 10-minute timeouts. The benchmark includes 600 VCs from industrial projects like the Linux kernel scheduler and Contiki-OS memory allocator.

## Key Results
- Best neural theorem prover achieves only 2.08% pass@1 versus 18% for Sledgehammer
- At least 24% of Isabelle proofs failed solely due to syntax errors in nested expressions
- >64% of Lean proofs generated by Goedel-Prover-V2-32B exhibited semantic degeneration patterns
- 9%+ of Isabelle failures involved hallucinated tactics or undefined constants

## Why This Works (Mechanism)

### Mechanism 1
Expert-written translation rules preserve semantic equivalence when converting verification conditions across proof assistant languages. The pipeline dumps Why3 VCs to XML ASTs, then applies ~2,400 human-curated mapping and rewriting rules (~800 per target). Rules handle syntax sugar, idiomatic rewrites, and constant mappings. Core assumption: Simple Typed Theory (Why3's logic) is entailed by the logics of mainstream ITPs, ensuring translation feasibility without semantic loss.

### Mechanism 2
Removing auxiliary annotations increases VC difficulty while preserving provability. The "complication process" erases three annotation types—assert (subgoal lemmas), lemma (global lemmas), and lemma application instantiations—that human developers added to help ATPs. This restores VCs to their fully-automated ideal form. Core assumption: Annotations are syntactically identifiable and their erasure does not alter the underlying proof obligation's truth.

### Mechanism 3
Hybrid evaluation using both fine-tuned theorem provers and general-purpose LLMs reveals distinct failure modes specific to VC structure. Models attempt proofs zero-shot with context; outputs are extracted and verified by ITPs with 10-minute timeouts. Hammers (Sledgehammer, CoqHammer) provide classical baselines. Core assumption: Pass@n metrics meaningfully capture proof capability; syntax/type errors in generated proofs represent genuine model limitations.

## Foundational Learning

- **Verification Conditions (VCs)**: The core artifact—logical propositions encoding program correctness that must be proven. Unlike competition math problems, VCs are machine-generated, deeply nested, and semantically dense.
  - Why needed: VCs are what neural models must prove; understanding their structure is essential for interpreting results.
  - Quick check: Given a binary search implementation with loop invariants, can you sketch the structure of the VC that proves functional correctness?

- **VCG vs. ATP vs. ITP**: The benchmark sits at the intersection: VCGs (Why3, Frama-C) generate VCs; ATPs (Z3, CVC5) attempt automated proof; ITPs (Isabelle, Lean, Rocq) provide the formal language for neural proof generation.
  - Why needed: Understanding this distinction clarifies why the benchmark requires all three components.
  - Quick check: Why might a VC be provable in Isabelle but not by Z3 alone?

- **Hammer Tools**: Sledgehammer and CoqHammer represent the state-of-the-art classical baseline (18% pass@1). Understanding their SMT/ATP integration clarifies why they outperform neural approaches on industrial VCs.
  - Why needed: Hammers provide the baseline that neural approaches must exceed.
  - Quick check: What types of reasoning (arithmetic, quantifier instantiation, theory combination) do hammers excel at that current NTP models struggle with?

## Architecture Onboarding

- **Component map**: Industrial C projects (Linux kernel, Contiki OS) → Frama-C → Why3 intermediate → Why3 VCG → complication (annotation erasure) → translation (Why3 AST → XML → Python S-expression → Target ITP via YAML rules + Python adapters) → Evaluation (Model generates proof → ITP checker with 10-min timeout → Pass/fail)

- **Critical path**: Why3 source → VCG → complication → translation → ITP theory file. Breaks at any stage (e.g., unsupported datatype, failed termination check) halt pipeline.

- **Design tradeoffs**: Rule-based vs. LLM translation (rules chosen for reliability); benchmark size (600 VCs from 7.5k for diversity vs. cost); annotation erasure (increases difficulty but may make some VCs practically unsolvable).

- **Failure signatures**: Syntactic errors (mismatched parentheses in nested expressions >24% Isabelle failures); Semantic degeneration (repetitive meaningless tactic sequences 64% in Lean); Hallucination (invoking non-existent tactics or undefined constants 9%+ Isabelle failures).

- **First 3 experiments**:
  1. Run Sledgehammer and Minilang on Isabelle subset; verify reported 18% and 11.46% pass@8 rates.
  2. Classify 50 random Lean failures into syntactic/semantic/hallucination categories using paper's taxonomy.
  3. Manually trace 5 VCs through Why3→XML→S-expression→Isabelle; verify semantic preservation at each stage.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can a hybrid system combining NTP with annotation synthesis achieve higher automation rates than current ATPs? The paper states NTP4VC and annotation synthesis are "complementary" and "can be applied orthogonally," but their integration remains unexplored.

- **Open Question 2**: Does domain-specific fine-tuning on VCs correct the "semantic confusion" where models apply imperative logic to declarative proofs? General-purpose pre-training data likely biases models toward imperative code generation rather than declarative proof tactics.

- **Open Question 3**: How can models be improved to handle the extreme syntactic complexity (deeply nested formulas) of real-world VCs? Standard transformers struggle with the specific deep nesting and parenthesis balancing found in VCs.

## Limitations

- The benchmark's reliance on expert-written translation rules (2,400+ rules) introduces potential semantic drift risks not exhaustively validated.
- Annotation erasure, while theoretically preserving provability, may have removed domain-specific guidance making some VCs practically unsolvable.
- Generalizability is limited by focus on Why3-generated VCs; real-world verification projects may use diverse VCGs with different VC structures.

## Confidence

- **High Confidence**: The 2.08% pass@1 for best NTP models versus 18% for Sledgehammer represents a robust empirical finding. The error taxonomy is well-supported by concrete failure statistics.
- **Medium Confidence**: The claim that annotation erasure restores VCs to their "fully-automated ideal form" assumes annotations don't encode essential domain knowledge.
- **Low Confidence**: The benchmark's generalizability beyond Why3's output patterns remains uncertain.

## Next Checks

1. **Translation Fidelity Audit**: Manually verify semantic preservation for 20 random VCs by comparing Why3 source, translated Isabelle/Lean/Rocq, and checking whether equivalent proof obligations hold across all three target languages.

2. **Annotation Impact Study**: Re-run the 600-VC benchmark with partial annotation restoration (e.g., restore only assert annotations) to quantify how much practical difficulty stems from complete erasure versus other factors.

3. **Generalization Experiment**: Apply the same neural models to VCs generated by alternative VCGs (Dafny, Boogie) to test whether the benchmark's difficulty characterization extends beyond Why3's output patterns.