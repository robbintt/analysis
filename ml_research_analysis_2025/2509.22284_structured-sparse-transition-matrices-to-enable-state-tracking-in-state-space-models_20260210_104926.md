---
ver: rpa2
title: Structured Sparse Transition Matrices to Enable State Tracking in State-Space
  Models
arxiv_id: '2509.22284'
source_url: https://arxiv.org/abs/2509.22284
tags:
- state
- group
- diagonal
- matrices
- transition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently enabling state-space
  models (SSMs) to track states in finite-state automata (FSA) while maintaining computational
  efficiency. The authors propose PD-SSM, a novel structured sparse parametrization
  of transition matrices in SSMs.
---

# Structured Sparse Transition Matrices to Enable State-Space Models

## Quick Facts
- arXiv ID: 2509.22284
- Source URL: https://arxiv.org/abs/2509.22284
- Authors: Aleksandar Terzić; Nicolas Menet; Michael Hersche; Thomas Hofmann; Abbas Rahimi
- Reference count: 40
- Primary result: PD-SSM achieves universal FSA emulation with optimal state dimension and outperforms other SSM variants on state tracking tasks

## Executive Summary
This paper introduces PD-SSM, a novel structured sparse parametrization of transition matrices in state-space models (SSMs) that enables efficient finite-state automaton (FSA) state tracking while maintaining computational efficiency. The key innovation is parametrizing the transition matrix as the product of a column one-hot matrix P and a complex-valued diagonal matrix D, which preserves the computational efficiency of diagonal SSMs while enabling universal FSA emulation. The method achieves state-of-the-art performance on various FSA state tracking tasks and multivariate time-series classification, demonstrating both theoretical guarantees (BIBO stability, optimal state dimension) and practical effectiveness.

## Method Summary
PD-SSM parametrizes the transition matrix A(ut) = P(ut)D(ut) where P is column one-hot (binary, exactly one nonzero per column) and D is complex diagonal. This structure is closed under multiplication, allowing parallel scan computation in Θ(LN) operations rather than Θ(LN³) for dense matrices. The diagonal matrix D(ut) has entries constrained to (0,1) via sigmoid-based magnitude generation, guaranteeing BIBO stability. The P matrix encodes arbitrary FSA transition functions while maintaining Θ(N) parallel scan efficiency. The method achieves universal FSA emulation with optimal (minimal) state dimension N using a single layer and linear readout of size N×N.

## Key Results
- PD-SSM achieves universal FSA emulation with optimal state dimension N using single-layer architecture
- Significantly outperforms other SSM variants on various FSA state tracking tasks
- Achieves state-of-the-art performance on multivariate time-series classification, surpassing neural controlled differential equations
- Successfully integrated into hybrid Transformer-SSM architecture for complex FSA state tracking with variable-length English sentences

## Why This Works (Mechanism)

### Mechanism 1
PD parametrization enables efficient parallel scans with linear complexity while maintaining FSA emulation capability. The transition matrix A(ut) = P(ut)D(ut) where P is column one-hot and D is complex diagonal. This structure is closed under multiplication—products of PD matrices remain PD—allowing parallel scan computation in Θ(LN) operations rather than Θ(LN³) for dense matrices.

### Mechanism 2
BIBO stability is guaranteed through bounded diagonal entries, enabling reliable training without unbounded state growth. The diagonal matrix D(ut) has entries constrained to (0,1) via sigmoid-based magnitude generation: |D(ut)| = σ(MLP(ut)). Combined with ||At||∞ ≤ 1-ε, this yields the bound ||xt||₂ ≤ √N·B/ε for bounded inputs.

### Mechanism 3
Any N-state FSA can be emulated with optimal (minimal) state dimension N using single-layer PD-SSM with linear readout. The P matrix can encode arbitrary state-to-state mappings (column one-hot structure naturally represents permutations/transition functions). Complex diagonal D provides compact encoding for cyclic substructures (e.g., modular counters).

## Foundational Learning

- **Finite-State Automata and Transformation Groups**: Understanding which automata are "solvable" vs "non-solvable" determines theoretical limits of diagonal SSMs and motivates PD-SSM's design. *Quick check: Can you explain why the A5 group (60 permutations, non-solvable) cannot be emulated by bounded-depth diagonal SSMs per Merrill et al. 2024?*

- **Parallel Scan (Associative Scan) for Recurrent Computation**: PD-SSM's efficiency claim rests on computing chained matrix products A₁·A₂·...·AL in Θ(log L) parallel steps with Θ(LN) total work. *Quick check: Given the binary associative operator (A_{t+1}, b_{t+1})•(A_t, b_t) = (A_{t+1}A_t, A_{t+1}b_t + b_{t+1}), trace how parallel scan computes all prefix states.*

- **Straight-Through Estimators for Discrete Operations**: The hardmax in P generation is non-differentiable; surrogate gradients (softmax backward) enable learning through discrete selection. *Quick check: Why must hardmax be preserved in the forward pass (not relaxed to softmax) to maintain Θ(N) parallel scan efficiency?*

## Architecture Onboarding

- **Component map**: Input embedding u_t → MLP magnitude generator (sigmoid) → MLP phase generator (sigmoid) → Dictionary selection MLP → Hardmax for P matrix → Parallel scan computation → Linear readout from real/imaginary state components
- **Critical path**: Input u_t through three parallel MLPs generating |D|, φ(D), and dictionary weights s(ut) → Construct P via hardmax → Parallel scan over sequence with Θ(LN) matrix multiplies → Linear readout from concatenated real/imaginary state components
- **Design tradeoffs**: Dictionary size K: Larger K increases expressivity but adds parameters; State dimension N vs embedding D: Paper uses N=D=128 for FSA tasks; Linear vs nonlinear readout: Linear readout critical for state tracking; Real vs complex diagonal: Complex enables compact cyclic encoding but adds complexity
- **Failure signatures**: Poor length generalization despite good training accuracy → Check if model learned shortcuts vs true FSA emulation; Degrading accuracy with more generators → Increase dictionary size K; Training instability → Verify |D| bounded in (0,1) via sigmoid
- **First 3 experiments**: Reproduce FSA emulation on Parity and Cycle tasks (Table 2): Single layer, N=128, K=6, train 100K steps on sequences length 3-40, validate to length 256; Ablate P generation strategy: Compare deterministic hardmax vs stochastic Gumbel sampling on Arithmetic task; Measure runtime scaling: Implement parallel scan in JAX, measure PD-SSM vs diagonal vs dense SD-SSM at D=512, L=64,256,512

## Open Questions the Paper Calls Out

1. Can more efficient generation strategies for the column one-hot matrix P(u_t) eliminate the runtime overhead of PD-SSM relative to diagonal SSMs?

2. How does PD-SSM perform when utilized in full large-scale pretraining scenarios compared to standard Transformer or Mamba architectures?

3. Does integrating PD-SSM layers into earlier blocks of a hybrid architecture improve state tracking compared to placing them at the output?

## Limitations

- Requires linear readouts for optimal FSA tracking performance, limiting applicability to tasks requiring nonlinear decision boundaries
- Performance claims on multivariate time-series classification are based on a relatively small subset of 6 datasets with no ablation studies
- Computational efficiency claim is not fully realized in practice due to P generation overhead, achieving only ~7× slowdown vs diagonal SSMs

## Confidence

- **High confidence**: BIBO stability properties, Θ(N) parallel scan complexity for PD matrices, universal FSA emulation with optimal state dimension
- **Medium confidence**: Computational efficiency comparisons (implementation details may affect real-world performance), performance claims on multivariate time-series classification (limited dataset coverage)
- **Low confidence**: Claims about outperforming neural controlled differential equations, as comparison methodology is not fully specified

## Next Checks

1. Benchmark runtime scaling: Measure PD-SSM vs diagonal SSM vs dense SD-SSM at state dimensions D=512, sequence lengths L=64,256,512 to verify claimed complexity and efficiency improvements

2. Validate FSA tracking with nonlinear readouts: Test whether nonlinear readout MLP layers consistently degrade performance across multiple FSA tasks as claimed

3. Test non-solvable group expressivity limits: Systematically vary dictionary size K on A5 and S5 tasks to determine the relationship between expressivity and K, confirming that K≥32 is required for optimal performance