---
ver: rpa2
title: 'ABLEIST: Intersectional Disability Bias in LLM-Generated Hiring Scenarios'
arxiv_id: '2510.10998'
source_url: https://arxiv.org/abs/2510.10998
tags:
- disability
- ableism
- harm
- across
- conversations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study audits six large language models across 2,820 hiring
  conversations for intersectional disability bias. The authors introduce ABLEIST,
  an eight-metric framework grounded in disability and intersectionality literature
  to detect subtle ableist and intersectional harms.
---

# ABLEIST: Intersectional Disability Bias in LLM-Generated Hiring Scenarios

## Quick Facts
- arXiv ID: 2510.10998
- Source URL: https://arxiv.org/abs/2510.10998
- Reference count: 40
- Large language models amplify ableist harms 1.15x-58x when disability identities are present, with 99.7% of disability conversations containing at least one harm

## Executive Summary
This study introduces ABLEIST, an eight-metric framework for detecting intersectional disability bias in LLM-generated hiring conversations. The authors evaluate six large language models across 2,820 conversations and find that adding disability identities increases ableist harms by 1.15x to 58x compared to baseline, with 99.7% of disability-related conversations containing at least one harm. Mainstream safety tools failed to detect any harm, prompting the authors to fine-tune Llama-3.1-8B-Instruct, achieving macro F1-scores of 0.75-0.94 across ABLEIST metrics.

## Method Summary
The authors generated 2,820 hiring conversations using 47 intersectional candidate profiles (disability × gender × caste × nationality) across six LLMs. They developed ABLEIST, an eight-metric framework grounded in disability studies literature, and used GPT-5-chat-latest as a teacher to generate synthetic labels. A fine-tuned Llama-3.1-8B-Instruct model with LoRA adapters was trained to detect these harms, achieving macro F1-scores of 0.75-0.94. Gold-standard validation was performed on 165 conversations with Krippendorff's α = 0.71.

## Key Results
- Adding disability identity increased ABLEIST harm by 1.15x to 58x compared to baseline
- 99.7% of disability-related conversations contained at least one harm
- Intersectional harms grew by 10-51% when disability intersected with marginalized gender and caste identities
- Mainstream safety tools failed to detect any harm in the generated conversations

## Why This Works (Mechanism)

### Mechanism 1: Harm Amplification Through Identity Salience
When disability markers are made salient in prompts, LLMs generate content reflecting encoded societal stereotypes about disability. The framing triggers stereotypical associations present in training data.

### Mechanism 2: Intersectional Compounding of Harms
Multiple marginalization axes create qualitatively distinct discrimination forms beyond additive effects. Intersectional identities trigger specific harmful tropes simultaneously (e.g., tokenism and inspiration porn).

### Mechanism 3: Safety Tool Detection Failure
Mainstream toxicity detection tools target overt toxicity and fail to identify subtle, covert ableist harms because they appear benevolent (e.g., "they can be just as productive with assistive technology").

## Foundational Learning

- **Intersectionality Theory (Crenshaw, 2017)**: Understanding how overlapping marginalized identities produce compounded harms rather than additive ones. Quick check: Why did the paper find 10–51% increases in intersectional harms for multiply-marginalized candidates versus only ~6% for dominant identity combinations?

- **Disability Studies Frameworks (Ableism, Inspiration Porn, Technoableism)**: The 8 ABLEIST metrics derive from disability studies literature with specific theoretical meanings. Quick check: Why is "they can be just as productive with the right tools" classified as Technoableism rather than supportive accommodation language?

- **LLM-as-Judge / Knowledge Distillation**: Using GPT-5 as a teacher to generate synthetic labels, then distilling to Llama-3.1-8B-Instruct. Quick check: Why did raising reasoning effort in GPT-5 reduce labeling performance for this task?

## Architecture Onboarding

**Component map**: Conversation Generation (47 profiles × 6 LLMs × 5 convs × 2 occupations) → ABLEIST Annotation Layer (GPT-5 teacher → Llama-3.1-8B-Instruct student) → Gold-Standard Validation (165 convs × 8 metrics) → Detection Model Output (YAML labels)

**Critical path**: 1) Define intersectional profiles per Table 8 2) Generate conversations using recruiter-persona prompts 3) Apply ABLEIST metrics via validated LLM annotator 4) Validate on gold-standard; deploy fine-tuned Llama model

**Design tradeoffs**: Geographic scope addresses Global South gap but tests Western LLMs on Indian caste context; 8 metrics grounded in disability studies but may overlap; distillation reduces cost but inherits teacher errors

**Failure signatures**: Safety tools output near-zero scores across all ABLEIST harms; higher reasoning effort in GPT-5 decreased annotation performance; open-weight models showed higher ableism scores but lower intersectional harm detection

**First 3 experiments**: 1) Run fine-tuned Llama model on gold-standard test split to confirm macro F1 of 0.75–0.94 2) Apply Perspective API and other safety tools to verify near-zero detection 3) Isolate conversations by identity combination to quantify compounding effect (10–51% increase for marginalized intersections vs. ~6% for dominant)

## Open Questions the Paper Calls Out
- How do ABLEIST harms manifest in high-stakes domains outside of hiring, such as healthcare or education?
- How does disability bias compound with religious and socioeconomic identities in LLM outputs?
- Can model abstention mechanisms effectively mitigate the generation of intersectional ableist harms in real-time?

## Limitations
- Framework generalizability limited to evaluated context (Indian caste, gender, nationality) and hiring domain
- Teacher-student error propagation may compound systematic biases from GPT-5 labeling
- Safety tool comparison doesn't explore whether existing tools can be adapted versus requiring specialized frameworks

## Confidence
**High confidence**: Harm amplification (1.15x-58x increase) with gold-standard validation; safety tool failure demonstrated through systematic testing
**Medium confidence**: Intersectional compounding mechanism and detection model performance require additional validation datasets
**Medium confidence**: Caste dimension's contribution to intersectional harm needs cultural context for full interpretation

## Next Checks
1. Apply the fine-tuned Llama-3.1-8B-Instruct model to an independent dataset from a different cultural context to measure performance degradation
2. Fine-tune Perspective API or OpenAI Moderation on ABLEIST-specific data to quantify whether existing tools can be adapted
3. Compute inter-metric correlations on gold-standard dataset to identify overlapping harm categories and retrain with merged metrics or hierarchical classification