---
ver: rpa2
title: 'Evaluation Under Imperfect Benchmarks and Ratings: A Case Study in Text Simplification'
arxiv_id: '2504.09394'
source_url: https://arxiv.org/abs/2504.09394
tags:
- sentence
- simplification
- text
- evaluation
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of reliably evaluating text
  simplification, a task crucial for improving information accessibility. Existing
  benchmarks suffer from two major issues: low-quality data with disfluent or simplistic
  examples, and inconsistent human ratings due to low inter-annotator agreement.'
---

# Evaluation Under Imperfect Benchmarks and Ratings: A Case Study in Text Simplification

## Quick Facts
- arXiv ID: 2504.09394
- Source URL: https://arxiv.org/abs/2504.09394
- Authors: Joseph Liu; Yoonsoo Nam; Xinyue Cui; Swabha Swayamdipta
- Reference count: 33
- Key outcome: Proposes SynthSimpliEval benchmark and LLM-as-a-jury approach, achieving high correlation between model size and simplification quality

## Executive Summary
This paper addresses the challenge of reliably evaluating text simplification systems, which suffer from low-quality benchmarks and inconsistent human ratings. The authors propose three key contributions: a synthetic benchmark with high inter-annotator agreement, an LLM jury approach for scalable evaluation, and a learnable metric trained on synthetic data that outperforms existing metrics. Their approach demonstrates that high-quality data can be effectively obtained through synthetic benchmarks and LLM-based ratings, providing a practical framework for evaluating text simplification and potentially other NLP tasks with limited high-quality annotations.

## Method Summary
The authors create SynthSimpliEval by generating 260 complex sentences (60 from SimpEval2022, 200 synthetic) and simplifying them using Llama 3 models of varying sizes (1B, 3B, 8B, 70B). They employ a diverse LLM jury (7 models including Gemma, Qwen, Mixtral, Deepseek, Claude, GPT-4o) using few-shot prompting with rationale generation to score simplifications. The jury scores are aggregated via arithmetic mean and validated against model size. Finally, they train a small neural network (64 neurons) on sentence embeddings to predict jury scores, achieving significantly better correlation with model quality than traditional metrics.

## Key Results
- Human pilot study on SynthSimpliEval achieves high inter-annotator agreement (ICC of 0.671) compared to 0.153-0.228 on existing datasets
- LLM jury with few-shot rationale prompting correlates strongly with model size (Spearman 0.822)
- Learnable metric trained on LLM-rated synthetic data achieves 0.22 correlation with model size, significantly outperforming previous metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Within a single model family, larger models produce higher-quality text simplifications that humans can consistently distinguish, allowing model size to serve as a proxy for quality.
- Mechanism: Scaling laws predict improved linguistic capabilities with increased parameters. This creates a detectable quality gradient across model sizes (1B, 3B, 8B, 70B). When quality differences are sufficiently distinct, human annotators achieve high agreement, validating the proxy.
- Core assumption: Quality improvements scale predictably with parameter count within a model family.
- Evidence anchors:
  - [abstract]: "larger models produce higher-quality simplifications... human ratings on our benchmark exhibit high inter-annotator agreement"
  - [section 3.2]: "human ratings strongly correlate with model size... Spearman rank correlation... of 0.759"
  - [corpus]: Related work on "Progressive Document-level Text Simplification via Large Language Models" confirms LLM scaling benefits for simplification tasks.
- Break condition: Task difficulty at extremes (too simple or beyond all models' capabilities) eliminates meaningful quality differentiation.

### Mechanism 2
- Claim: A diverse jury of LLM evaluators using few-shot prompting with rationale generation provides a scalable, reliable proxy for human evaluation.
- Mechanism: Aggregating judgments from multiple independent models reduces individual biases through variance reduction. Rationale generation (chain-of-thought) decomposes evaluation into explicit criteria before scoring, improving judgment quality.
- Core assumption: Individual LLM biases are uncorrelated, so averaging cancels noise while preserving signal.
- Evidence anchors:
  - [abstract]: "auto-evaluation with a panel of LLM judges (LLMs-as-a-jury) often suffices to obtain consistent ratings"
  - [section 4.2]: "few-shot prompting with rationale generation, combined with score averaging, yields simplification scores that best correlate with model sizes [Spearman 0.822]"
  - [corpus]: "Doubly-Robust LLM-as-a-Judge" paper validates jury approaches while noting estimation complexities with imperfect personas.
- Break condition: Jury diversity collapse (correlated biases), task outside judge competence, or self-evaluation bias (mitigated by excluding judges from simplifier family).

### Mechanism 3
- Claim: Learnable metrics trained on synthetic, LLM-jury-rated data outperform traditional metrics because the training signal better reflects modern simplification quality.
- Mechanism: Traditional metrics trained on noisy human labels learn inconsistent patterns. LLM-jury-labeled synthetic data provides a cleaner, more consistent training signal that reflects contemporary model capabilities. A small neural network learns to approximate jury judgments from sentence embeddings.
- Core assumption: LLM-jury ratings provide superior ground truth compared to noisy human-labeled datasets.
- Evidence anchors:
  - [abstract]: "existing learnable metrics benefit from training on our synthetic, LLM-rated data. A small neural network... achieves a 0.22 correlation with model size, significantly outperforming previous metrics"
  - [section 5]: "resulting model has a correlation of 0.22 with the model size score, substantially higher than previous metrics"
  - [corpus]: Related work (JEBS, JUDGEBERT) confirms ongoing research into better evaluation datasets and models for specialized domains.
- Break condition: Learned metric performance bounded by jury quality; cannot capture nuances visible to jury but not encoded in embeddings. Lower accuracy than full jury (0.22 vs 0.822 Spearman).

## Foundational Learning

- Concept: **Spearman Rank Correlation**
  - Why needed here: Primary metric for validating whether evaluation scores (human or LLM) monotonically increase with model size (quality proxy).
  - Quick check question: If LLM-jury scores achieve Spearman 0.822 with model size, what does this imply about the jury's ability to rank simplification quality?

- Concept: **Intraclass Correlation Coefficient (ICC)**
  - Why needed here: Quantifies consistency across multiple raters. High ICC validates that the evaluation protocol produces reliable, reproducible scores.
  - Quick check question: Why is achieving ICC of 0.671 on the new benchmark significant compared to 0.153-0.228 on existing datasets?

- Concept: **Few-shot Prompting with Rationale**
  - Why needed here: The specific prompting strategy that maximizes LLM-judge performance by providing examples and requiring explicit reasoning before scoring.
  - Quick check question: According to ablation studies, why does including rationale generation improve correlation with model size?

## Architecture Onboarding

- Component map:
  - **Simplifiers**: Llama 3 models (1B, 3B, 8B, 70B Instruct) generate simplifications from complex sentences
  - **Benchmark (SynthSimpliEval)**: 260 complex sentences (60 human-written + 200 synthetic) × 4 simplifier outputs
  - **Quality Oracle (LLM-as-a-Jury)**: 7 diverse LLM judges (Gemma, Qwen, Mixtral, Deepseek, Claude, GPT-4o) → scores aggregated via arithmetic mean
  - **Learnable Metric**: Sentence embeddings (all-mpnet-base-v2) → feedforward network (64 neurons) trained to predict jury scores

- Critical path:
  1. Generate complex-simple pairs using simplifiers of varying sizes
  2. Score pairs using diverse LLM jury with few-shot rationale prompts
  3. Validate jury scores correlate with model size (Spearman > 0.8)
  4. Train lightweight metric on jury-labeled data for fast inference

- Design tradeoffs:
  - **Jury vs. Learned Metric**: Jury (Spearman ~0.82) is accurate but expensive; learned metric (Spearman 0.22) is fast but approximate
  - **Synthetic vs. Human Data**: Synthetic is scalable/consistent but may inherit model biases; human data is authentic but noisy with low agreement (ICC < 0.23)
  - **Unified vs. Multi-dimensional Scoring**: Unified scoring achieves higher inter-judge agreement (ICC 0.619 vs. dimensional breakdown)

- Failure signatures:
  - **Ceiling/floor effects**: All simplifications too similar in quality for meaningful ranking
  - **Self-preference bias**: Judges favor outputs from their own model family (mitigated by cross-family evaluation)
  - **Proxy overfitting**: Metric optimizes for model size prediction rather than actual simplification quality

- First 3 experiments:
  1. **Validate size-quality proxy**: Replicate human pilot—have experts rate 80 simplification pairs from 4 model sizes; compute ICC and Spearman correlation with size
  2. **Optimize jury configuration**: Ablate prompt design (few-shot vs. 1-shot, rationale vs. no-rationale, temperature) on 1040 pairs; select configuration maximizing size correlation
  3. **Train and benchmark learned metric**: Generate 400 additional synthetic pairs with jury labels; train feedforward network on embeddings; compare Spearman correlation (target: >0.20) against FKGL, LENS, BERTScore, SLE

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the proposed synthetic data and LLM-as-a-jury framework be effectively generalized to other subjective generation tasks beyond text simplification?
- **Basis in paper:** [Explicit] The conclusion states the approach offers a "practical recipe for evaluation in other tasks where high-quality annotated data may be limited."
- **Why unresolved:** The paper is restricted to a case study on text simplification; it does not test the framework on tasks with different constraints (e.g., summarization length constraints).
- **What evidence would resolve it:** Successful application of the SynthSimpliEval methodology to distinct tasks like style transfer or summarization, showing high correlation between the LLM jury and human ground truth.

### Open Question 2
- **Question:** What architectural or data modifications are necessary to close the performance gap between small learnable metrics and large LLM juries?
- **Basis in paper:** [Inferred] In Section 5, the small neural network trained on synthetic data achieved a correlation of 0.22, which significantly outperformed previous metrics but remained far lower than the LLM jury’s correlation of 0.822.
- **Why unresolved:** While the authors demonstrate that better training data helps, the learnable metric still fails to capture the nuances that the full LLM jury captures.
- **What evidence would resolve it:** A learnable metric that achieves a Spearman correlation comparable to the LLM jury (>0.80) on the SynthSimpliEval benchmark.

### Open Question 3
- **Question:** Is model size a robust proxy for simplification quality across diverse model families and architectures, rather than just the Llama 3 family?
- **Basis in paper:** [Inferred] The authors operationalize simplification quality using four Llama 3 models of increasing size (1B–70B) (Section 3.1), validated by a small pilot study (Section 3.2).
- **Why unresolved:** The correlation between size and quality is assumed based on scaling laws, but may not hold uniformly across different architectures or specialized fine-tunes.
- **What evidence would resolve it:** A study showing that model rankings based on size (within families) consistently align with human ratings for non-Llama models (e.g., Gemma, Mistral).

## Limitations

- **Data Quality Concerns**: The synthetic benchmark may inherit biases from generation models, potentially introducing systematic errors through model-family correlations or domain-specific artifacts.
- **Jury Configuration Sensitivity**: Optimal LLM-jury performance depends heavily on specific prompting configurations, with limited exploration of the full hyperparameter space.
- **Proxy Validity**: Using model size as a proxy for simplification quality assumes monotonic scaling within model families, which may break down at extremes or across different architectures.

## Confidence

- **High Confidence**: Empirical findings that larger models produce consistently better simplifications distinguishable by humans (ICC 0.671), LLM jury evaluations correlate strongly with model size (Spearman 0.822), and learned metrics benefit from LLM-labeled synthetic data (correlation 0.22).
- **Medium Confidence**: Generalizability of these methods beyond text simplification to other NLP tasks, and long-term stability of LLM jury evaluations as model capabilities evolve.
- **Low Confidence**: Exact mechanisms by which rationale generation improves jury performance, and whether the learned metric captures true simplification quality or merely optimizes for model size prediction.

## Next Checks

1. **Cross-Domain Generalization**: Apply the LLM-jury approach to evaluate simplifications across domains not represented in the original 20 (e.g., legal, technical, or domain-specific scientific text) to test robustness against domain shift.

2. **Temporal Validation**: Re-run the jury evaluation pipeline after 6-12 months with updated judge models to assess whether the strong correlation (Spearman 0.822) persists as model capabilities evolve.

3. **Human-in-the-Loop Calibration**: Conduct a targeted human validation study where experts compare jury scores against human judgments on a stratified sample of simplifications, focusing on cases where the learned metric (correlation 0.22) diverges from jury predictions.