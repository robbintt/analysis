---
ver: rpa2
title: 'MRCEval: A Comprehensive, Challenging and Accessible Machine Reading Comprehension
  Benchmark'
arxiv_id: '2503.07144'
source_url: https://arxiv.org/abs/2503.07144
tags:
- reasoning
- knowledge
- comprehension
- question
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MRCEval introduces a novel taxonomy for machine reading comprehension,
  categorizing the key capabilities required into three levels: context comprehension,
  external knowledge comprehension, and reasoning. Based on this taxonomy, MRCEval
  constructs a comprehensive benchmark with 13 distinct tasks and 2.1K multi-choice
  questions, generated using LLMs as both sample generators and selection judges.'
---

# MRCEval: A Comprehensive, Challenging and Accessible Machine Reading Comprehension Benchmark

## Quick Facts
- arXiv ID: 2503.07144
- Source URL: https://arxiv.org/abs/2503.07144
- Reference count: 32
- Primary result: Even state-of-the-art LLMs struggle on relation/event facts understanding and context-faithful tasks, with specific weaknesses revealed through 13-task taxonomy evaluation

## Executive Summary
MRCEval introduces a novel taxonomy for machine reading comprehension, categorizing the key capabilities required into three levels: context comprehension, external knowledge comprehension, and reasoning. Based on this taxonomy, MRCEval constructs a comprehensive benchmark with 13 distinct tasks and 2.1K multi-choice questions, generated using LLMs as both sample generators and selection judges. Extensive evaluation of 28 models, including 14 open-source and 14 closed-source models, reveals that MRC remains highly challenging, with even the most competitive models like Gemini-2.0-flash and o1-mini struggling on specific tasks. The benchmark highlights weaknesses in relation or event facts understanding and context-faithful tasks, emphasizing the need for further advancements in natural language understanding for LLMs.

## Method Summary
MRCEval constructs a benchmark by extracting samples from 11 source datasets and converting them to 4-option multi-choice questions using GPT-4o. A three-judge voting mechanism (LLama-3-8B-Instruct, Qwen-2.5-7B-Instruct, GPT-4o-mini) filters samples, retaining those where at least one judge fails. The benchmark covers 13 sub-tasks across 3 main categories, with ~200 samples per sub-task. Models are evaluated using a greedy decoding strategy or temperature 0.0, with answers extracted via regex for option labels (A/B/C/D). The evaluation targets 28 models total, with accuracy reported per sub-task and overall.

## Key Results
- MRCEval reveals that all evaluated models struggle with counterfactual faithfulness, with 36.8% of samples mispredicted by ALL models
- Large commercial models excel at inconsistent tasks but perform poorly on counterfactual tasks, while smaller open-source models show the opposite pattern
- Models demonstrate strong performance on entity facts extraction (only 6.3% all-model error) but significant weaknesses in relation facts (25.9%) and event facts (37.3%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-model voting with lightweight LLMs effectively identifies challenging samples for benchmark construction.
- Mechanism: Three diverse judge models evaluate each sample; instances where at least one judge fails are retained, filtering out trivially easy questions.
- Core assumption: Different model architectures have complementary failure modes, so their union captures diverse challenge types.
- Evidence anchors: [abstract]: "leveraging advanced Large Language Models (LLMs) as both sample generators and selection judges"; [section 3.2]: "For each sample, if at least one of the judges answers incorrectly, we put the sample as the candidate"
- Break condition: If judge models are highly correlated in their errors, selection diversity degrades and the benchmark may overfit to specific failure modes.

### Mechanism 2
- Claim: Larger models exhibit a trade-off between reasoning capability and context-faithful reading due to parametric knowledge interference.
- Mechanism: Models with more parameters encode more factual knowledge, improving reasoning tasks but causing failures when context contradicts memorized facts (counterfactual scenarios).
- Core assumption: Parametric knowledge scales with model size and actively interferes with context-faithful reading.
- Evidence anchors: [section 5.1]: "Large commercial models are better at inconsistent tasks but worse at counterfactual tasks, smaller open-source models do the opposite. This is because models with more parameters remember more facts"
- Break condition: If reasoning and memorization capabilities can be disentangled through architecture or training, the trade-off may weaken.

### Mechanism 3
- Claim: Hierarchical taxonomy decomposition (3 levels, 13 sub-tasks) reveals capability gaps that aggregate metrics obscure.
- Mechanism: The taxonomy separates context comprehension (facts + faithfulness), external knowledge (commonsense/world/domain), and reasoning (logical/arithmetic/multi-hop/temporal), exposing task-specific weaknesses.
- Core assumption: The 13 sub-tasks represent measurably separable cognitive capabilities.
- Evidence anchors: [abstract]: "categorizing the key capabilities required into three levels: context comprehension, external knowledge comprehension, and reasoning"
- Break condition: If sub-task performances are highly correlated, the taxonomy provides limited diagnostic value beyond aggregate scores.

## Foundational Learning

- Concept: **Context Faithfulness vs. Parametric Knowledge**
  - Why needed here: The counterfactual sub-task reveals that models struggle to override memorized facts with passage content—a core limitation in real-world MRC.
  - Quick check question: If a passage states "Paris is the capital of Germany," will your model answer "Germany" when asked about Paris's country, or default to "France"?

- Concept: **Multi-hop Reasoning Chains**
  - Why needed here: Several sub-tasks require chaining inferences across multiple document spans or knowledge sources.
  - Quick check question: To answer "Which company acquired the firm that created TypeScript?", what two facts must be retrieved and composed?

- Concept: **LLM-as-Judge Selection Bias**
  - Why needed here: The benchmark selects samples where judge models fail; understanding this helps interpret whether results reflect general LLM limitations or judge-specific gaps.
  - Quick check question: If all three judge models share a common architectural weakness (e.g., poor temporal reasoning), which sub-tasks might be over-represented in the final benchmark?

## Architecture Onboarding

- Component map:
  - Taxonomy: 3 main tasks → 13 sub-tasks (see Table 1)
  - Source datasets: 11 (SQuAD 1.0, DocRED, MAVEN-Arg, FaithEval, COSMOSQA, KoRC, PubMedQA, LogicBench, DROP, MoreHopQA, MenatQA)
  - Generator: GPT-4o (question/option generation for datasets lacking multi-choice format)
  - Judges: LLama-3-8B-Instruct, Qwen-2.5-7B-Instruct, GPT-4o-mini (voting filter)
  - Evaluation targets: 28 models (14 open-source, 14 closed-source)

- Critical path:
  1. Extract samples from source dataset development sets
  2. Convert to 4-option multi-choice (GPT-4o generates questions for DocRED/MAVEN-Arg, incorrect options for others)
  3. Apply 3-judge voting filter (retain samples where ≥1 judge fails)
  4. Denoise via sampling analysis (targeted cleanup for relation/event/world-knowledge tasks)
  5. Randomly sample ~200 instances per sub-task

- Design tradeoffs:
  - Automated generation vs. noise: GPT-4o generation introduces errors (incorrect answers, duplicate options)—partially mitigated by denoising step
  - Judge selection: Lightweight judges are cost-efficient but may miss challenges unique to larger models
  - Sample size: ~200/sub-task balances coverage against construction cost; statistical power for fine-grained comparisons is limited

- Failure signatures:
  - Counterfactual faithfulness: 36.8% of samples mispredicted by ALL evaluated models (highest common error rate—see Figure 3)
  - Relation facts: 25.9% all-model error rate
  - Event facts: 37.3% all-model error rate
  - Entity facts: Only 6.3% all-model error (relative strength)

- First 3 experiments:
  1. **Reproduce baseline pipeline**: Run the 2.1K benchmark evaluation on 2–3 representative models (e.g., GPT-4o, Llama-3.3-70B, Qwen-2.5-14B) to validate infrastructure and prompt formatting.
  2. **Ablate judge voting threshold**: Compare benchmarks constructed with 1-judge vs. 3-judge (≥1 failure) vs. 3-judge (≥2 failures) selection to quantify judge contribution to difficulty.
  3. **Counterfactual error analysis**: Manually inspect 20 counterfactual samples where all 6 top models fail to characterize parametric interference patterns and identify potential mitigation strategies.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLM architectures or training paradigms be modified to improve performance on relation/event extraction and counterfactual tasks without degrading their parametric knowledge utilization?
- Basis in paper: [explicit] The Conclusion highlights that models "struggle" specifically with "relation or event facts understanding and context-faithful" tasks.
- Why unresolved: The error analysis reveals models prioritize memorized parameters over context integration (hallucination), yet larger models (which memorize more) perform better on domain knowledge, suggesting a trade-off.
- What evidence would resolve it: A training method that improves context-grounded reasoning while retaining world knowledge, validated by higher scores on the "Context Faithful" sub-tasks.

### Open Question 2
- Question: Does the use of lightweight LLMs (8B/7B parameters) as "judges" for sample selection introduce a bias that makes the benchmark specifically challenging for small models rather than a general test of comprehension?
- Basis in paper: [inferred] Section 3.2 utilizes Llama-3-8B-Instruct and Qwen-2.5-7B-Instruct to select samples where "at least one judge answers incorrectly."
- Why unresolved: Samples difficult for small models might be trivially easy for large models, while samples difficult for large models might be filtered out if small models solve them.
- What evidence would resolve it: An ablation study using a high-capacity model (e.g., GPT-4o) as the sole judge to compare the resulting dataset's difficulty curve.

### Open Question 3
- Question: To what extent does the automated generation of incorrect options (distractors) by GPT-4o introduce noise, such as ambiguous or implausible alternatives, that artificially lowers model accuracy?
- Basis in paper: [inferred] Appendix D (Error Study) explicitly identifies "Samples generation noise" where GPT-4o generates incorrect answers or fails to follow instructions.
- Why unresolved: The paper acknowledges the lack of "refined manual de-noising" in the Limitations section, leaving the impact of this noise on the final rankings unquantified.
- What evidence would resolve it: A human evaluation of the distractors in the "most common prediction error" subset to verify they are logically sound and unambiguous.

## Limitations

- The automated construction pipeline using GPT-4o for question generation introduces sampling noise that may inflate error rates for specific sub-tasks
- The three-judge voting mechanism for sample selection may overrepresent failure modes specific to lightweight judge architectures rather than general LLM capabilities
- The ~200 samples per sub-task provides adequate coverage for high-level capability assessment but lacks statistical power for fine-grained performance comparisons

## Confidence

- **High Confidence**: The overall finding that even state-of-the-art models struggle with counterfactual faithfulness and reasoning tasks is well-supported by the multi-model evaluation and consistent error patterns across architectures.
- **Medium Confidence**: The taxonomy-based decomposition revealing capability gaps is methodologically sound, though the separability of the 13 sub-tasks and whether they represent truly distinct cognitive capabilities remains partially unverified.
- **Low Confidence**: The specific mechanism explaining the scale-dependent trade-off between reasoning and memorization capabilities is speculative, with limited empirical validation beyond correlational observations across model families.

## Next Checks

1. **Judge Diversity Analysis**: Systematically evaluate whether the three judge models have correlated failure modes by computing pairwise error correlations across the 2.1K samples, and test whether replacing judges with larger models produces meaningfully different sample selections.

2. **Counterfactual Faithfulness Ablation**: Design a controlled experiment varying the semantic distance between passage content and memorized facts (e.g., plausible vs. implausible contradictions) to determine whether models can learn to prioritize context over parametric knowledge when the conflict is less severe.

3. **Sub-task Independence Validation**: Conduct factor analysis on the 13 sub-task accuracies across multiple model evaluations to empirically verify whether the taxonomy captures separable cognitive capabilities or whether performance patterns are dominated by a few underlying factors.