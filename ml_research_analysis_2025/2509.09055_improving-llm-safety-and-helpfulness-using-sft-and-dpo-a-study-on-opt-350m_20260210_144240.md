---
ver: rpa2
title: 'Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M'
arxiv_id: '2509.09055'
source_url: https://arxiv.org/abs/2509.09055
tags:
- helpfulness
- alignment
- responses
- reward
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates alignment methods for smaller language\
  \ models, specifically evaluating Supervised Fine-Tuning (SFT), Direct Preference\
  \ Optimization (DPO), and a combined SFT+DPO approach on the OPT-350M model using\
  \ the Anthropic Helpful-Harmless RLHF dataset. The study introduces three metrics\u2014\
  Harmlessness Rate (HmR), Helpfulness Rate (HpR), and Combined Alignment Score (CAS)\u2014\
  derived from reward model scores to quantify alignment performance."
---

# Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M

## Quick Facts
- arXiv ID: 2509.09055
- Source URL: https://arxiv.org/abs/2509.09055
- Authors: Piyush Pant
- Reference count: 11
- Primary result: SFT+DPO pipeline outperforms both SFT and DPO alone on safety and helpfulness metrics for OPT-350M

## Executive Summary
This paper investigates alignment methods for smaller language models, specifically evaluating Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and a combined SFT+DPO approach on the OPT-350M model using the Anthropic Helpful-Harmless RLHF dataset. The study introduces three metrics—Harmlessness Rate (HmR), Helpfulness Rate (HpR), and Combined Alignment Score (CAS)—derived from reward model scores to quantify alignment performance. Results show that while SFT outperforms DPO in overall alignment, the SFT+DPO combination achieves the highest scores across all metrics, indicating complementary strengths of the two methods. DPO's weaker performance is attributed to noisy training data, short training duration (1 epoch vs. 2 for SFT), and limited GPU resources. The work demonstrates that hybrid alignment pipelines can significantly improve both safety and helpfulness in compact LLMs, with implications for resource-constrained deployments.

## Method Summary
The study evaluates three alignment approaches on OPT-350M: SFT (2 epochs), DPO (1 epoch), and a sequential SFT+DPO pipeline. SFT trains on chosen responses from the Anthropic HH-RLHF dataset using cross-entropy loss, while DPO uses preference triplets (chosen/rejected pairs) with LoRA/PEFT. The combined approach applies SFT first, then refines with DPO. Models are evaluated on 100 prompts (50 harmlessness, 50 helpfulness) using a DeBERTa-v3 reward model, with metrics defined as HmR (≥ -3 threshold), HpR (> -2 threshold), and CAS (mean of HmR and HpR).

## Key Results
- SFT achieves higher alignment scores than DPO across all metrics (HmR: 62% vs 57%, HpR: 63% vs 57%, CAS: 62% vs 57%)
- SFT+DPO pipeline achieves the highest scores: HmR 65%, HpR 66%, CAS 55%
- DPO's weaker performance attributed to noisy training data and resource constraints (1 epoch vs 2 for SFT)
- Sequential fine-tuning demonstrates complementary strengths of SFT and DPO approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised Fine-Tuning (SFT) establishes a foundational behavioral prior by directly teaching the model to imitate human-curated safe and helpful responses.
- Mechanism: SFT minimizes cross-entropy loss on a dataset of preferred (chosen) responses. This adjusts the model's weights to increase the probability of generating tokens that align with the human-selected outputs, effectively encoding safe and helpful behaviors as the default generation pattern.
- Core assumption: The "chosen" responses in the training data represent a sufficiently high-quality and consistent distribution of desired behavior that can be learned through direct imitation.
- Evidence anchors:
  - [abstract] "The results show that while SFT outperforms DPO, The combined SFT+DPO model outperforms all others..."
  - [section 3.3] "The objective during training was to minimize the cross-entropy loss between the model's predicted output and the ground truth response, thereby encouraging the model to imitate safe and helpful behavior."
  - [corpus] Corpus signals consistently identify SFT as a core baseline alignment technique.
- Break condition: SFT's effectiveness degrades significantly if the training data is noisy or contains poor examples. Its rigidity can also limit performance on nuanced preference trade-offs.

### Mechanism 2
- Claim: Direct Preference Optimization (DPO) refines a model's behavior by optimizing for human preference between response pairs, without an explicit reward model.
- Mechanism: DPO uses a contrastive loss function on pairs of "chosen" (y+) and "rejected" (y-) responses. It directly adjusts the policy (π) to increase the probability of generating y+ while decreasing the probability of y-, based on a mathematical re-parameterization of the reward function.
- Core assumption: The preference signal in the data (that y+ is better than y-) is accurate and consistent, allowing the model to learn from the contrast between responses.
- Evidence anchors:
  - [abstract] "...Direct Preference Optimization (DPO), and a combined SFT+DPO approach... demonstrating the complementary nature of these techniques."
  - [section 2] "Given a prompt x, preferred response y+, and non-preferred response y-, the DPO objective is: LDPO = −log( σ( β * (log π(y+|x) − log π(y−|x) ) ) )."
  - [corpus] Paper 22912 notes DPO as a simpler alternative to complex RLHF for steering models.
- Break condition: The DPO mechanism is highly sensitive to label noise in the preference pairs. The paper explicitly states that mislabeled or ambiguous examples "can mislead the DPO objective, which relies heavily on accurate comparisons" (Section 5.4).

### Mechanism 3
- Claim: A sequential combination of SFT followed by DPO leverages the complementary strengths of both methods, yielding superior alignment.
- Mechanism: SFT first stabilizes the model and provides a strong baseline of coherent, safe behavior. DPO then operates on this more stable foundation, using its contrastive learning to fine-tune the model's responses based on nuanced human preferences that simple imitation might miss.
- Core assumption: The benefits of DPO are best realized when applied to a model that already possesses a reasonable generation capability, rather than a raw, potentially unstable base model.
- Evidence anchors:
  - [abstract] "The results show that while SFT outperforms DPO, The combined SFT+DPO model outperforms all others across all metrics, demonstrating the complementary nature of these techniques."
  - [section 5.1] "The SFT+DPO model achieves the highest HpR (66%) and CAS (55%), showing the benefit of sequential fine-tuning followed by DPO."
  - [corpus] Corpus neighbors (e.g., Paper 50755) explore similar hybrid pipelines.
- Break condition: The pipeline's success is contingent on the quality of the SFT phase. If the initial SFT model is poor, DPO cannot effectively refine it. The paper also notes DPO's performance was hampered by resource constraints, indicating the refinement step itself may have been suboptimal.

## Foundational Learning

- Concept: **Cross-Entropy Loss**
  - Why needed here: This is the core objective function for the SFT phase. Understanding it is essential to grasp how the model is taught to imitate specific, pre-defined responses.
  - Quick check question: How does minimizing cross-entropy loss on a dataset of "chosen" responses directly influence the model's generation behavior?

- Concept: **Contrastive Loss / Preference Optimization**
  - Why needed here: This is the principle behind DPO. The model learns not from a single correct answer, but by comparing two options and learning to increase the probability of the better one.
  - Quick check question: How does the DPO loss function adjust model probabilities based on a (prompt, chosen_response, rejected_response) triplet?

- Concept: **Reward Model Evaluation**
  - Why needed here: The study uses a proxy reward model to score outputs. Understanding that this is a learned model with its own biases and thresholds (e.g., < -3 = harmful) is critical for interpreting the reported metrics.
  - Quick check question: Why might a reward model-based evaluation be more scalable but potentially less accurate than human evaluation?

## Architecture Onboarding

- Component map:
  - OPT-350M (base) -> SFT fine-tuning -> SFT+DPO refinement -> Reward model evaluation

- Critical path:
  1.  Data Prep: From the HH-RLHF dataset, extract (prompt, chosen_response) pairs for SFT. Use the full (prompt, chosen, rejected) triplets for DPO.
  2.  SFT Training: Train the base model on the SFT pairs for 2 epochs. This is the foundation for the best-performing pipeline.
  3.  DPO Refinement: Initialize from the SFT model. Fine-tune using the DPO loss on the preference triplets for at least 1 epoch.
  4.  Evaluation: Generate responses to the 100-prompt test set, score them with the reward model, and calculate HmR, HpR, and CAS.

- Design tradeoffs:
  - SFT vs. DPO: SFT provides robust foundational behavior but may be rigid. DPO adds nuanced preference alignment but is unstable and sensitive to noise.
  - Full vs. PEFT: The paper used LoRA/PEFT for DPO due to compute constraints. This trades off some model capacity and alignment performance for feasibility in resource-limited environments.
  - Evaluation via Reward Model: Using a proxy model is scalable and reproducible but is an imperfect proxy for actual human judgment.

- Failure signatures:
  - Sensitivity to Noisy Data: DPO performance degrades sharply with mislabeled preference pairs (identified as a key issue in the paper).
  - Weak Base Model: If the base model produces incoherent or empty outputs (another noted issue), the preference signal for DPO becomes meaningless.
  - Insufficient Refinement: The paper suggests training DPO for only 1 epoch limited its potential, indicating the need for adequate compute.

- First 3 experiments:
  1.  Baseline Evaluation: Evaluate the raw OPT-350M model on the 100-prompt test set to establish benchmark HmR, HpR, and CAS scores.
  2.  SFT-Only Ablation: Train a model using only Supervised Fine-Tuning on the "chosen" responses and evaluate. This isolates the contribution of the SFT phase.
  3.  Hybrid Pipeline Implementation: Take the SFT-trained model from experiment 2 and further fine-tune it using DPO on the preference triplets. Compare its performance to both the baseline and SFT-only models to validate the hybrid approach.

## Open Questions the Paper Calls Out

- **Question:** Does the superior performance of the SFT+DPO pipeline hold when applied to LLMs significantly larger than OPT-350M?
  - **Basis in paper:** [explicit] The conclusion states that "future research should explore its scalability to larger LLMs."
  - **Why unresolved:** The study was computationally constrained to the 350M parameter model; it is unknown if the complementary benefits of DPO persist at larger scales where base capabilities differ.
  - **What evidence would resolve it:** Replicating the exact SFT, DPO, and SFT+DPO training regimen on models exceeding 7B parameters and comparing the resulting alignment scores.

- **Question:** Can DPO outperform SFT in isolation if trained for an equal duration (epochs) on de-noised data?
  - **Basis in paper:** [inferred] The authors note DPO underperformed likely due to "noisy data" and "training constraints" (1 epoch for DPO vs. 2 for SFT), hypothesizing it has untapped potential.
  - **Why unresolved:** The experimental setup was uneven due to GPU limitations, preventing a fair "apples-to-apples" comparison of the theoretical maxima of SFT versus DPO.
  - **What evidence would resolve it:** A controlled experiment where DPO and SFT are trained for the same number of epochs on a dataset filtered for labeling consistency.

- **Question:** How well do the automated reward model scores (HmR, HpR) correlate with human evaluations of safety and helpfulness?
  - **Basis in paper:** [explicit] The conclusion suggests "Evaluating the models using human annotators... could also provide more nuanced insights."
  - **Why unresolved:** The study relied exclusively on the OpenAssistant reward model for evaluation to save costs and avoid API rate limits.
  - **What evidence would resolve it:** Correlating the automated scores against a human-annotated test set to validate the reliability of the proposed metrics.

## Limitations
- The study relies on a proxy reward model for evaluation, which may not capture nuanced human preferences
- DPO's performance was limited by noisy training data and resource constraints (1 epoch vs. 2 for SFT)
- The study only tests on a single model size (OPT-350M), limiting generalizability
- Specific hyperparameters for training are not reported, hindering reproducibility

## Confidence

- **High Confidence:** The core finding that a sequential SFT+DPO pipeline can outperform either method alone is well-supported by the results and aligns with established principles of alignment fine-tuning.
- **Medium Confidence:** The attribution of DPO's weaker performance to noisy data and resource constraints is plausible but would benefit from more direct evidence, such as an analysis of the preference dataset quality.
- **Low Confidence:** Specific claims about the exact magnitude of improvement (e.g., HmR and HpR percentages) are difficult to verify without the reported hyperparameters and full reproducibility details.

## Next Checks
1. **Hyperparameter Sensitivity Analysis:** Conduct a controlled experiment varying DPO's training duration (e.g., 1 vs. 2 vs. 3 epochs) and LoRA rank to isolate the effect of these factors on performance, confirming if the SFT+DPO combination's advantage persists under optimal DPO conditions.
2. **Human Evaluation Study:** Replace or supplement the reward model evaluation with a small-scale human judgment study on a subset of the test prompts to validate the proxy metrics and assess the real-world quality of the aligned outputs.
3. **Dataset Quality Audit:** Perform a statistical analysis of the Anthropic HH-RLHF dataset to quantify the prevalence of ambiguous or contradictory preference pairs, directly testing the hypothesis that noisy data significantly degraded DPO's performance.