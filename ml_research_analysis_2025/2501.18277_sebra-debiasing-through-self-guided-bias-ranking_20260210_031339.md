---
ver: rpa2
title: 'Sebra: Debiasing Through Self-Guided Bias Ranking'
arxiv_id: '2501.18277'
source_url: https://arxiv.org/abs/2501.18277
tags:
- bias
- spuriosity
- ranking
- sebra
- spurious
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents Sebra, a self-guided bias ranking framework
  that mitigates spurious correlations without human supervision. The core idea leverages
  the hardness-spuriosity symmetry: harder-to-learn samples tend to have fewer spurious
  features.'
---

# Sebra: Debiasing Through Self-Guided Bias Ranking

## Quick Facts
- arXiv ID: 2501.18277
- Source URL: https://arxiv.org/abs/2501.18277
- Reference count: 34
- Primary result: Self-supervised bias ranking framework achieving 10% average improvement on UrbanCars and CelebA benchmarks

## Executive Summary
Sebra is a self-guided debiasing framework that addresses spurious correlations without requiring human supervision or prior knowledge of bias attributes. The method leverages the hardness-spuriosity symmetry principle, which states that samples with fewer spurious features are harder to learn via standard ERM. By dynamically steering the training process to follow this principle, Sebra generates fine-grained spuriosity rankings for each class. These rankings are then used in a contrastive learning framework to effectively mitigate multiple biases simultaneously, outperforming state-of-the-art unsupervised debiasing methods.

## Method Summary
Sebra operates in two phases: ranking and mitigation. In the ranking phase, it modifies standard ERM training with a self-guided selection mechanism that sequentially learns samples based on their spuriosity, generating a rank list for each class. The mitigation phase employs supervised contrastive learning where samples with the same rank are treated as negative pairs (likely sharing spurious features) and pushed apart in feature space, while samples with different ranks are pulled together. This process effectively disentangles core attributes from spurious correlations without requiring any human annotation of bias attributes.

## Key Results
- Achieves 10% average improvement on UrbanCars and CelebA datasets
- Outperforms state-of-the-art unsupervised debiasing methods by 6% on BAR benchmark
- Demonstrates capability for outlier detection and discovery of previously unknown biases
- Shows consistent improvements across multiple benchmarks including ImageNet-1K

## Why This Works (Mechanism)

### Mechanism 1: Hardness-Spuriosity Symmetry
The core principle is that samples with higher spuriosity (more spurious features) are easier to learn and converge faster than samples relying on core attributes. ERM preferentially fits simple, non-causal correlations early in training. By monitoring convergence speed through loss trajectories, one can infer relative spuriosity of samples. This mechanism relies on the assumption that ease of learning is inversely correlated with core feature presence.

### Mechanism 2: Self-Guided Selection and Conservation
Sebra modifies ERM with dynamic sample selection ($v_i$) to remove samples once they exceed a confidence threshold, ensuring the model focuses on remaining hard samples. It applies weighting ($u_i \propto p_y^{1/\beta}$) to enforce a "Hardness-Spuriosity Conservation Law," preventing gradient interference from overwhelming the ranking signal. This maintains a sequential learning order from high spuriosity to low spuriosity.

### Mechanism 3: Rank-Based Contrastive Separation
The temporal order of learning (rank) is mapped to spatial separation in feature space. Samples with the same rank (likely sharing the same spurious feature) are pushed apart as negative pairs, while samples with different ranks are pulled together as positive pairs. This forces the encoder to ignore features shared by same-ranked samples, effectively disentangling core and spurious attributes.

## Foundational Learning

**Empirical Risk Minimization (ERM)**
- Why needed: Sebra is a modification of standard ERM; understanding the baseline "simplicity bias" of SGD is required to understand why Sebra needs to intervene in the training loop.
- Quick check: Does standard ERM prioritize easy or hard features during the initial epochs?

**Spurious Correlations (Shortcuts)**
- Why needed: The entire ranking logic depends on distinguishing "core" (causal) features from "spurious" (non-causal but correlated) features.
- Quick check: In a "cows on grass" dataset, is the shape of the cow the core or spurious feature?

**Supervised Contrastive Learning**
- Why needed: The second stage of Sebra uses the generated rankings to construct contrastive pairs ($L_{sup}_{con}$).
- Quick check: In contrastive learning, do we minimize or maximize the distance between negative pairs?

## Architecture Onboarding

**Component map:**
Input -> ERM Backbone -> Sebra Head (Calculates $u_i$ weighting, $v_i$ selection) -> Sorted Rank List.
Input -> Encoder ($f_{enc}$) -> Contrastive Head -> Loss ($L_{sup}_{con} + \gamma L_{CE}$).

**Critical path:** The calculation of the selection variable $v_i$ (Eq. 2) using the threshold $p_{critical}$. If this threshold is incorrect, the ranking logic breaks immediately.

**Design tradeoffs:**
- Ranking Resolution vs. Compute: Fine-grained ranking requires strictly sequential learning, which slows down training compared to parallel ERM.
- Sensitivity: The method is sensitive to label noise; noisy labels are often treated as "hard" (low spuriosity) samples, corrupting the ranking.

**Failure signatures:**
- Stall: $v_i$ never switches to 0; no samples are ranked (Learning rate or $\lambda$ may be too low).
- Collapse: All samples ranked at Epoch 1 (Threshold $p_{critical}$ may be too low).

**First 3 experiments:**
1. **Ranking Validation:** Run Sebra on UrbanCars/CelebA validation set with ground-truth bias labels; report Kendall's $\tau$ (Target > 0.65).
2. **Ablation ($u$ and $v$):** Train using only selection ($v$) vs. selection + weighting ($u+v$) to confirm the contribution of the "Conservation Law" (Table 3).
3. **Mitigation Stress Test:** Train the contrastive phase on the full dataset and evaluate worst-group accuracy (WG Acc) against ERM and JTT baselines.

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Relies on hardness-spuriosity symmetry assumption, which may not hold universally across all datasets and model architectures
- Contrastive separation assumes samples learned at the same time share the same spurious feature, which may not always be true in complex datasets with multiple interacting biases
- Performance degrades when spurious features are coincidentally easier to learn than core attributes

## Confidence

**High Confidence:** The mechanism of hardness-spuriosity symmetry and its application in the ranking phase (based on consistent experimental results across multiple benchmarks).

**Medium Confidence:** The self-guided selection and conservation mechanism, as it relies on specific hyperparameter tuning that may not generalize perfectly.

**Medium Confidence:** The rank-based contrastive separation approach, given that the assumption about same-ranked samples sharing spurious features requires further validation.

## Next Checks

1. **Ranking Robustness Test:** Apply Sebra to datasets where spurious features are deliberately made easier to learn than core attributes (e.g., simple texture vs. complex shape) and measure ranking accuracy.

2. **Multiple Bias Validation:** Test Sebra on datasets with three or more distinct spurious biases simultaneously to verify the claim of handling multiple biases.

3. **Architectural Transferability:** Evaluate Sebra's performance when transferred from CNNs to Vision Transformers to assess architectural dependency of the hardness-spuriosity symmetry.