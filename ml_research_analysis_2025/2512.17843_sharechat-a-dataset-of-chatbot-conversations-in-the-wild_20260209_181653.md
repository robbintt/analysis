---
ver: rpa2
title: 'ShareChat: A Dataset of Chatbot Conversations in the Wild'
arxiv_id: '2512.17843'
source_url: https://arxiv.org/abs/2512.17843
tags:
- user
- grok
- conversation
- platforms
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SHARECHAT is a large-scale dataset of 142,808 real-world chatbot
  conversations from five platforms (ChatGPT, Perplexity, Grok, Gemini, Claude), preserving
  native features like citations and thinking traces. It addresses the lack of authentic,
  multi-platform, long-context interaction data in existing corpora.
---

# ShareChat: A Dataset of Chatbot Conversations in the Wild

## Quick Facts
- **arXiv ID**: 2512.17843
- **Source URL**: https://arxiv.org/abs/2512.17843
- **Reference count**: 40
- **Primary result**: 142,808 real-world chatbot conversations preserving native platform features (citations, thinking traces, timestamps)

## Executive Summary
SHARECHAT is a large-scale dataset of 142,808 real-world chatbot conversations from five platforms (ChatGPT, Perplexity, Grok, Gemini, Claude), preserving native features like citations and thinking traces. It addresses the lack of authentic, multi-platform, long-context interaction data in existing corpora. The dataset covers 101 languages, spans 660,293 turns, and demonstrates longer conversations (avg. 4.62 turns, 1,115 tokens) and lower toxicity than prior datasets. Three case studies showcase its utility: conversation completeness analysis reveals platform differences in satisfying user intent; source grounding analysis contrasts retrieval strategies (Grok: X-heavy; Perplexity: diversified); temporal analysis exposes contrasting response latency dynamics. SHARECHAT enables studying extended interaction dynamics, platform-specific behaviors, and evidence-based generation.

## Method Summary
The dataset was collected by querying Internet Archive for publicly shared conversation URLs from five platforms (ChatGPT, Perplexity, Grok, Gemini, Claude) between April 2023 and October 2025. Platform-specific Selenium scrapers extracted structured JSON containing turns, metadata (timestamps, model versions, citations, thinking blocks), and applied Microsoft Presidio PII removal with GPT-OSS-120B validation. The data was analyzed using Llama-3.1-8B for topic classification, Qwen3-8B for completeness evaluation, and Detoxify + OpenAI Moderation for toxicity detection.

## Key Results
- Longer conversations than prior datasets (avg. 4.62 turns vs. 2.02 in LMSYS) with lower toxicity (4.1% user toxicity vs. 6.05% in WildChat)
- Platform-specific behaviors: ChatGPT/Claude show 82-87% complete intentions vs. Perplexity at 67%
- Source grounding reveals Grok relies heavily on X sources while Perplexity uses more diversified sources

## Why This Works (Mechanism)

### Mechanism 1: Platform-Native Metadata Preservation
- Claim: Preserving platform-specific affordances (citations, thinking traces, timestamps) enables analysis of how interface design shapes user behavior.
- Mechanism: By capturing non-textual context through platform-specific extraction pipelines (e.g., clicking thinking bars in Claude, parsing source panes in Grok), the dataset retains signals about retrieval strategies, reasoning visibility, and interaction pacing that text-only corpora strip away.
- Core assumption: Users adapt prompting and engagement to available interface features, and these adaptations are informative for understanding real-world usage.
- Evidence anchors:
  - [abstract]: "ShareChat distinguishes itself by preserving native platform affordances, such as citations and thinking traces"
  - [section 2]: Platform-specific extraction pipelines with Selenium; Table 2 shows feature availability (thinking blocks in Claude/Grok, citations in Perplexity/Grok)
  - [corpus]: Weak direct corpus support; related work on in-the-wild conversations (RICoTA) focuses on red-teaming, not metadata preservation
- Break condition: If interface features are removed or homogenized by platforms (e.g., citation format changes), historical data becomes less representative of current usage.

### Mechanism 2: Self-Selection Bias Toward Constructive Interactions
- Claim: Public URL sharing acts as a natural filter, producing conversations with lower toxicity and higher task completion than gateway-collected datasets.
- Mechanism: Users who opt to share conversations publicly self-select for interactions they deem valuable, successful, or novel, reducing observer bias while introducing selection bias toward positive outcomes.
- Core assumption: Shared conversations represent a meaningful subset of authentic usage, even if they underrepresent mundane or failed interactions.
- Evidence anchors:
  - [abstract]: "demonstrates longer conversations (avg. 4.62 turns, 1,115 tokens) and lower toxicity than prior datasets"
  - [section 3.2]: SHARECHAT user toxicity 4.1%/2.9% (Detoxify/OpenAI) vs. WildChat 6.05% and LMSYS 3.08%; LLM toxicity 1.6%/3.2% vs. WildChat 5.18%
  - [corpus]: Neighbor paper "Mass-Scale Analysis of In-the-Wild Conversations" confirms in-the-wild data reveals different patterns than controlled collection
- Break condition: If platforms incentivize sharing failed or adversarial interactions (e.g., for viral content), toxicity rates could increase significantly.

### Mechanism 3: Extended Multi-Turn Context for Interaction Dynamics
- Claim: Longer conversation depth (avg. 4.62 turns vs. 2.02 in LMSYS) enables studying intention evolution, partial completions, and context-dependent degradation.
- Mechanism: Multi-turn conversations capture how users refine prompts, shift goals, and co-construct solutions—dynamics invisible in single-turn benchmarks.
- Core assumption: Extended conversations contain signals about user satisfaction, model coherence, and failure modes that accumulate across turns.
- Evidence anchors:
  - [abstract]: "substantially longer context windows and greater interaction depth than prior datasets"
  - [section 4.1]: Completeness analysis shows ChatGPT/Claude at 82%/87% complete verdicts vs. Perplexity at 67%; partial verdicts reveal unsatisfied intermediate goals
  - [section 5]: "Prior work has shown that model reliability can degrade as instructions are refined across turns"
  - [corpus]: Limited direct support; HumanLLM paper discusses cognitive patterns in extended interactions
- Break condition: If multi-turn coherence improves substantially in future models, historical degradation patterns may become obsolete.

## Foundational Learning

- Concept: Self-Selection vs. Observer Bias
  - Why needed here: SHARECHAT trades observer bias (Hawthorne effect in monitored studies) for self-selection bias (users share only certain conversations); understanding this tradeoff is critical for interpreting results.
  - Quick check question: Would a dataset of all private conversations likely show higher or lower toxicity than SHARECHAT? Why?

- Concept: Platform Affordances
  - Why needed here: Features like citations, thinking traces, and timestamps are not universal; knowing which platforms expose what metadata determines what analyses are possible.
  - Quick check question: Which two platforms provide turn-level timestamps, and what temporal analysis does this enable?

- Concept: Proxy Metrics for User Satisfaction
  - Why needed here: Completeness scores and toxicity detection are automated approximations, not ground truth; interpreting them requires understanding their limitations.
  - Quick check question: If an LLM judge labels an intention "partial," what are three possible underlying causes?

## Architecture Onboarding

- Component map: Internet Archive URL discovery -> Selenium scraping -> JSON parsing -> Presidio PII removal -> LLM auditor -> Storage with platform-specific metadata fields -> Classification/evaluation pipelines

- Critical path: 1. Identify share URLs via Internet Archive search 2. Render pages with Selenium, extract structured JSON 3. Apply PII redaction, validate with LLM auditor 4. Store in unified schema with platform-specific metadata fields 5. Run classification/evaluation pipelines for downstream analysis

- Design tradeoffs:
  - Ecological validity vs. selection bias: Public URLs capture authentic usage but skew toward share-worthy interactions
  - Multi-platform coverage vs. imbalance: ChatGPT is 72% of data; Claude is <1%
  - Metadata richness vs. consistency: Only ChatGPT/Grok have timestamps; only Perplexity/Grok have citations

- Failure signatures:
  - Platform UI changes breaking Selenium selectors (e.g., thinking bar element moves)
  - PII leakage in unsupported languages (Presidio gaps)
  - Imbalanced cross-platform comparisons without stratified sampling
  - Timestamp interpolation errors for missing ChatGPT turns

- First 3 experiments:
  1. Reproduce the completeness analysis pipeline: Extract intentions from 100 conversations per platform, classify verdicts, compare aggregate scores to Figure 5
  2. Validate toxicity detection: Sample 200 conversations, compare Detoxify vs. OpenAI Moderation flags, manually review discrepancies
  3. Extend source grounding analysis: For Perplexity conversations with 50+ citations, correlate source diversity with completeness verdicts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do automated completeness and topic classification metrics correlate with human judgments of satisfaction and intent?
- Basis in paper: [explicit] "Future research is needed to validate these automated metrics against human judgment..."
- Why unresolved: The study relied on LLM-based evaluation (Qwen3-8B) as a proxy for user satisfaction, with only 100 conversations human-validated for completeness labeling.
- What evidence would resolve it: Large-scale human annotation comparing automated verdicts (complete/partial/incomplete) against user-reported satisfaction scores.

### Open Question 2
- Question: How do platform-specific usage patterns evolve as commercial systems undergo continuous updates and feature changes?
- Basis in paper: [explicit] "Future research is needed to fully explore these dimensions, particularly how these patterns evolve longitudinally as platform features change."
- Why unresolved: The dataset spans 30 months but analyses focus on cross-sectional comparisons rather than longitudinal trajectory modeling.
- What evidence would resolve it: Time-series analysis tracking topic distributions, completeness scores, and engagement patterns across version updates documented in SHARECHAT metadata.

### Open Question 3
- Question: What specific interface features or affordances drive platform-dependent reliability differences for particular user intents?
- Basis in paper: [inferred] Discussion states "determine which features drive reliability for specific user intents" as enabled research direction after observing platform-specific completeness variability.
- Why unresolved: Correlational analysis shows platforms perform differently (ChatGPT/Claude: 82-87% complete vs. Perplexity: 67%), but causal attribution to specific features remains untested.
- What evidence would resolve it: Controlled experiments manipulating individual platform features (citations, thinking traces) while holding model constant, or regression analysis isolating feature effects in SHARECHAT data.

### Open Question 4
- Question: What accounts for the "mirroring effect" where platforms receiving more toxic inputs generate more toxic outputs?
- Basis in paper: [inferred] Toxicity analysis notes significant correlation (ρ=0.90) between user and LLM toxicity rates, but mechanism remains unexplained.
- Why unresolved: The paper documents the correlation but does not investigate whether this stems from model behavior, user self-selection, or conversational dynamics.
- What evidence would resolve it: Turn-level analysis examining whether toxicity escalates within conversations, or cross-platform comparison controlling for user toxicity levels.

## Limitations

- Self-selection bias: Public URL sharing overrepresents positive, share-worthy interactions while underrepresenting mundane or failed conversations
- Platform imbalance: ChatGPT comprises 72% of conversations, creating significant coverage gaps for other models
- Temporal coverage: Data spans April 2023-October 2025, potentially missing recent platform changes and model improvements

## Confidence

**High Confidence:**
- Platform-specific metadata preservation (citations, thinking traces, timestamps) is successfully captured and structured
- Lower toxicity rates compared to existing datasets are consistently observed across multiple detection methods
- Longer conversation depth (4.62 turns average) is verified through token and turn counting
- Multilingual coverage spanning 101 languages is confirmed through language detection

**Medium Confidence:**
- Completeness analysis verdicts (complete, partial, incomplete) accurately reflect user satisfaction and intention fulfillment
- Source grounding patterns (Grok's X-heavy vs. Perplexity's diversified retrieval) represent genuine platform differences
- Temporal analysis of response latency captures meaningful platform-specific dynamics
- The dataset enables meaningful study of extended interaction dynamics and platform-specific behaviors

**Low Confidence:**
- SHARECHAT conversations are fully representative of "authentic" chatbot usage patterns
- Observed platform differences in completeness and source grounding generalize beyond the specific time period captured
- Toxicity detection results remain stable across different detection thresholds and cultural contexts
- The dataset's ecological validity sufficiently compensates for its selection bias limitations

## Next Checks

1. **Temporal Stability Analysis**: Collect a fresh sample of publicly shared conversations from the same five platforms over a 30-day period (e.g., January 2025) and compare completeness rates, toxicity levels, and source grounding patterns to the SHARECHAT baseline. This will test whether observed platform differences persist across time and whether any significant shifts have occurred in user behavior or model performance.

2. **Representativeness Audit**: Deploy a parallel collection method using the LMSYS-Chatbot-arena approach (real-time monitoring of user interactions) to gather a smaller control dataset of non-shared conversations. Compare conversation length distributions, toxicity rates, and topic frequencies between the two datasets to quantify the selection bias introduced by public sharing.

3. **Cross-Cultural Toxicity Validation**: Manually review and annotate a stratified sample of 500 conversations across 10 languages (including both supported and unsupported languages for PII removal) to validate automated toxicity detection results. This will identify false positive/negative rates and assess whether toxicity patterns observed in English generalize to other linguistic and cultural contexts.