---
ver: rpa2
title: Analyzing decision tree bias towards the minority class
arxiv_id: '2501.04903'
source_url: https://arxiv.org/abs/2501.04903
tags:
- decision
- trees
- class
- tree
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors challenge the common belief that decision trees are
  biased towards the majority class in imbalanced data settings. They show theoretically
  that decision trees fit to purity can actually be biased towards the minority class,
  depending on the data generating process and tree structure.
---

# Analyzing decision tree bias towards the minority class

## Quick Facts
- arXiv ID: 2501.04903
- Source URL: https://arxiv.org/abs/2501.04903
- Reference count: 32
- Primary result: Decision trees fit to purity can be biased towards the minority class, not just the majority class as commonly believed

## Executive Summary
This paper challenges the widely held belief that decision trees inherently favor the majority class in imbalanced datasets. Through theoretical proofs and simulations, the authors demonstrate that when decision trees are optimized for purity (impurity reduction at each split), they can actually exhibit a bias toward the minority class. This occurs because minority class observations tend to appear at extreme values of predictors, leading trees to preferentially split on these regions. The bias is particularly pronounced in stochastic settings with limited predictors and has important implications for random forests and other tree-based ensemble methods.

## Method Summary
The authors use Python 3.12.7 with scikit-learn 1.5.1 to implement their simulations. They employ RandomForestClassifier with n_estimators=1 (single tree), bootstrap=False, and max_features=2 to ensure both predictors are considered at each split. Trees are fit to purity without early stopping parameters. The key metric is the ratio of expected prevalence estimate (E[P_E]) to true prevalence (k/n), where values >1 indicate minority-class bias. Simulations use 500,000 iterations for Theorem 1 verification and 10,000 iterations for Appendix 3, with data generated from uniform distributions and specific stochastic/deterministic scenarios.

## Key Results
- Decision trees fit to purity show bias toward the minority class in stochastic settings, with E[P_E]/(k/n) ratios of ~1.176 for n=10 and ~1.075 for n=50
- This bias occurs because minority class observations tend to occur at extreme predictor values, leading to preferential splitting
- Regularization techniques like limiting tree depth (max_depth=2) can reduce this bias, with prevalence ratios dropping from 1.1019 to 0.9917 in Appendix 3 simulations
- The findings challenge the assumption that imbalance-handling techniques (undersampling, cost-sensitive learning) are always beneficial for tree-based models

## Why This Works (Mechanism)
The mechanism stems from how decision trees optimize for purity at each split. When minority class observations are clustered at extreme values of predictors, the purity gain from splitting at these boundaries is maximized. This creates a feedback loop where trees preferentially create nodes that capture minority instances, leading to over-prediction. The effect is amplified in stochastic settings where the relationship between predictors and outcome is weak or random, making purity-based splits more likely to capture noise rather than signal.

## Foundational Learning
- **Purity optimization in decision trees**: Why needed - understanding the core mechanism that drives the bias; Quick check - verify that splitting criterion maximizes impurity reduction
- **Stochastic vs deterministic data generating processes**: Why needed - the paper distinguishes between these settings and their different bias behaviors; Quick check - confirm that random label assignment vs threshold-based assignment creates different bias patterns
- **Prevalence estimation in classification**: Why needed - the key metric for measuring bias is the ratio of estimated to true prevalence; Quick check - verify that prevalence estimates are computed correctly from tree predictions
- **RandomForestClassifier with n_estimators=1**: Why needed - this configuration creates single trees for analysis while maintaining consistency with the paper's methodology; Quick check - confirm that single-tree configuration produces the expected splitting behavior

## Architecture Onboarding
- **Component map**: Data generation -> Tree fitting (RandomForestClassifier) -> Prevalence estimation -> Bias ratio calculation
- **Critical path**: The purity optimization step is critical - any deviation from perfect purity (via regularization parameters) will alter the bias magnitude
- **Design tradeoffs**: The choice between Gini impurity and entropy for splitting, and whether to use separate test sets vs out-of-bag samples for prevalence estimation
- **Failure signatures**: If prevalence ratios don't match theoretical predictions, check for incomplete splits (impurity not reaching zero) or incorrect test set methodology
- **First experiments**: 1) Verify Theorem 1 baseline case (n=10, m=2) produces ratio ~1.176, 2) Test Appendix 3 simulation with max_depth=2 regularization, 3) Compare Gini vs entropy splitting criteria on the same dataset

## Open Questions the Paper Calls Out
- **Direction of bias**: How specific relationships between predictors and outcomes dictate whether bias favors majority or minority class, requiring a theoretical framework linking P(Y|X) distributions to bias direction
- **Standard imbalance techniques**: Whether undersampling and cost-sensitive learning are detrimental when applied to models that may already be biased toward the minority class, needing empirical studies on calibration error
- **Generalizability**: Whether other machine learning models exhibit similar minority-class bias under the identified conditions, requiring analysis of models like neural networks and SVMs on the same stochastic datasets

## Limitations
- The exact impurity criterion (Gini vs entropy) used in simulations is not explicitly specified, affecting splitting decisions
- Test set methodology for Appendix 3 simulations is unclear, particularly whether separate validation sets or out-of-bag samples were used
- The paper focuses on binary classification with specific data distributions, limiting generalizability to multi-class or real-world imbalanced datasets

## Confidence
- **High confidence**: Theoretical proofs and general direction of findings regarding minority-class bias potential
- **Medium confidence**: Exact numerical values for Theorem 1 simulations, dependent on implementation details like impurity criterion
- **Medium confidence**: Appendix 3 results, pending clarification on test set construction methodology

## Next Checks
1. Verify the exact impurity criterion (Gini vs entropy) by testing if reported ratios change meaningfully under different criteria
2. Confirm test set methodology for Appendix 3 - whether separate test set was drawn or out-of-bag samples were used
3. Reproduce the baseline Theorem 1 case (n=10, m=2) to verify the ~1.176 ratio, as this is the foundational result