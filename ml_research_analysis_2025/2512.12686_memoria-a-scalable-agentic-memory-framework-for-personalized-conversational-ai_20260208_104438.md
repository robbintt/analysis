---
ver: rpa2
title: 'Memoria: A Scalable Agentic Memory Framework for Personalized Conversational
  AI'
arxiv_id: '2512.12686'
source_url: https://arxiv.org/abs/2512.12686
tags:
- memory
- user
- memoria
- session
- triplets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Memoria addresses the challenge of memoryless interactions in LLM-based
  conversational systems by introducing a scalable, modular memory framework. It combines
  session-level summarization with a weighted knowledge graph to maintain both short-term
  dialogue coherence and long-term personalization.
---

# Memoria: A Scalable Agentic Memory Framework for Personalized Conversational AI

## Quick Facts
- arXiv ID: 2512.12686
- Source URL: https://arxiv.org/abs/2512.12686
- Authors: Samarth Sarin; Lovepreet Singh; Bhaskarjit Sarmah; Dhagash Mehta
- Reference count: 14
- Key outcome: Achieved 87.1% accuracy on single-session-user questions and 80.8% on knowledge-updates using a modular memory framework with weighted knowledge graph

## Executive Summary
Memoria introduces a scalable, modular memory framework for LLM-based conversational agents that addresses the challenge of memoryless interactions. The system combines session-level summarization with a weighted knowledge graph to maintain both short-term dialogue coherence and long-term personalization. By incrementally capturing user traits and preferences as structured entities and relationships, Memoria applies recency-based weighting for prioritization while significantly reducing inference latency by up to 38.7% and average token length to under 400 tokens compared to full-context prompting.

## Method Summary
The framework employs a hybrid memory architecture that processes conversational data through incremental session summarization and knowledge graph construction. User traits and preferences are extracted as structured entities with relationships, weighted by recency to maintain prioritization. The system uses a modular design allowing independent scaling of session management and knowledge graph components. Evaluation was conducted on the LongMemEvals dataset, comparing performance against baseline full-context prompting approaches across accuracy, latency, and token efficiency metrics.

## Key Results
- Achieved 87.1% accuracy on single-session-user questions
- Achieved 80.8% accuracy on knowledge-updates
- Reduced inference latency by up to 38.7% and average token length to under 400 tokens compared to full-context prompting

## Why This Works (Mechanism)
The framework's effectiveness stems from its dual-memory approach that separates immediate session context from persistent user knowledge. Session-level summarization provides real-time dialogue coherence while the weighted knowledge graph maintains long-term personalization without overwhelming the LLM with full conversation history. Recency-based weighting ensures that the most relevant and recent information receives higher priority during inference, optimizing both accuracy and efficiency. The modular architecture allows independent scaling of components, enabling better resource allocation and system responsiveness.

## Foundational Learning

**Knowledge Graph Construction**: Needed for organizing user traits as structured entities with relationships; quick check: verify entity extraction accuracy on test conversations.

**Recency-Based Weighting**: Required to prioritize recent information while maintaining historical context; quick check: test weight decay rates on different conversation lengths.

**Session Summarization**: Essential for maintaining short-term dialogue coherence without full context; quick check: evaluate summary completeness on multi-turn conversations.

**Modular Architecture Design**: Enables independent scaling of memory components; quick check: measure performance impact when scaling individual modules.

## Architecture Onboarding

**Component Map**: User Input -> Session Summarizer -> Entity Extractor -> Knowledge Graph Manager -> Weighted Retriever -> LLM Interface

**Critical Path**: User query → Knowledge graph retrieval with weighted entities → Session context injection → LLM response generation

**Design Tradeoffs**: The framework trades complete historical context for improved latency and token efficiency, accepting potential minor accuracy losses for significant performance gains in production environments.

**Failure Signatures**: Memory degradation over extended sessions, entity extraction failures on ambiguous inputs, and knowledge graph bloat from excessive relationships.

**3 First Experiments**:
1. Test entity extraction accuracy on diverse conversational styles and noise levels
2. Measure latency scaling with concurrent user count (10, 100, 1000 users)
3. Evaluate knowledge graph pruning effectiveness on conversation history over 1000+ turns

## Open Questions the Paper Calls Out
None

## Limitations

- Evaluation limited to single benchmark dataset (LongMemEvals), constraining generalizability
- Does not address scalability bottlenecks with thousands of concurrent users or extremely long conversation histories
- Handling of conflicting or contradictory information over extended periods remains unexplored

## Confidence

- **High Confidence**: Modular architecture design, latency reduction claims, and basic functionality of session-level summarization
- **Medium Confidence**: Knowledge graph prioritization mechanism and trait extraction accuracy depend heavily on specific dataset
- **Low Confidence**: Long-term scalability claims, conflict resolution capabilities, and multilingual support are not empirically validated

## Next Checks

1. Conduct cross-dataset validation using diverse conversational domains (customer service, healthcare, education) to assess generalizability beyond LongMemEvals.

2. Stress-test the knowledge graph's performance with concurrent users (100+) and extended conversation histories (1000+ turns) to identify potential bottlenecks in indexing and retrieval.

3. Evaluate the system's handling of contradictory information by introducing conflicting statements over time and measuring how effectively it resolves or flags inconsistencies.