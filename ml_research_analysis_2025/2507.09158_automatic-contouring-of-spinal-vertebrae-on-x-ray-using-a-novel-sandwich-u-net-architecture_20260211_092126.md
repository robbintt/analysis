---
ver: rpa2
title: Automatic Contouring of Spinal Vertebrae on X-Ray using a Novel Sandwich U-Net
  Architecture
arxiv_id: '2507.09158'
source_url: https://arxiv.org/abs/2507.09158
tags:
- segmentation
- u-net
- activation
- image
- novel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of accurately segmenting thoracic
  vertebrae on X-ray images, a task critical for diagnosing spinal disorders but traditionally
  manual and error-prone. The authors propose a novel U-Net architecture that uses
  ReLU activation functions during the down-sampling phase and Attention-based ReLU
  (AReLU) during the up-sampling phase, forming a "sandwich" structure.
---

# Automatic Contouring of Spinal Vertebrae on X-Ray using a Novel Sandwich U-Net Architecture

## Quick Facts
- arXiv ID: 2507.09158
- Source URL: https://arxiv.org/abs/2507.09158
- Reference count: 40
- Primary result: Novel U-Net architecture with dual activation functions (ReLU encoder, AReLU decoder) achieves 83.58% Dice score on thoracic AP X-ray vertebrae segmentation, improving 4.1% over baseline

## Executive Summary
This paper addresses the challenge of accurately segmenting thoracic vertebrae on X-ray images, a task critical for diagnosing spinal disorders but traditionally manual and error-prone. The authors propose a novel U-Net architecture that uses ReLU activation functions during the down-sampling phase and Attention-based ReLU (AReLU) during the up-sampling phase, forming a "sandwich" structure. This dual activation approach allows the encoder to focus on robust feature extraction while enabling the decoder to reconstruct features adaptively, emphasizing important regions. The model was trained on a dataset of 300 AP view X-ray images of the thoracic spine, with 240 images for training/validation and 60 for testing. The proposed architecture achieved a Dice score of 83.58%, representing a 4.1% improvement over the baseline U-Net model (80.13%). This enhancement demonstrates the effectiveness of the sandwich U-Net in improving segmentation accuracy, particularly for vertebral borders and partial vertebrae, offering a reliable tool for assisting radiologists and surgeons in spinal assessments.

## Method Summary
The paper introduces a "Sandwich U-Net" architecture that modifies the standard U-Net by using ReLU activation in the encoder and Attention-based ReLU (AReLU) in the decoder. The model uses 6 encoder blocks with ReLU followed by max-pooling, a bottleneck, and 5 decoder blocks with AReLU (α=0.9, β=0.9) for adaptive feature reconstruction. The architecture includes skip connections between corresponding encoder and decoder layers. The model was trained on 192 AP view thoracic X-ray images from the BUU dataset, with 48 for validation and 60 for testing. Dice Loss and ADAM optimizer were used with early stopping on validation loss. The augmentation pipeline included random crops, brightness/contrast adjustments, scaling, shifting, rotation, noise addition, flips, and blur. The proposed model achieved 83.58% Dice score compared to 80.13% for the baseline U-Net.

## Key Results
- Sandwich U-Net achieved 83.58% Dice score on 60-image test set
- 4.1% improvement over baseline U-Net (80.13% Dice)
- Incremental AReLU layer addition showed monotonic improvement: 0 layers (80.13%) to 5 layers (83.58%)
- Qualitative improvements in vertebral border accuracy and partial vertebrae delineation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating activation functions by network phase improves segmentation accuracy on boundary regions.
- Mechanism: ReLU in the encoder provides sparse activations (setting negative values to zero), which suppresses irrelevant features during compression. AReLU in the decoder introduces learnable attention parameters (α, β) that amplify positive elements and suppress negative elements adaptively during reconstruction. This asymmetry allows the encoder to focus on robust feature extraction while the decoder emphasizes salient regions.
- Core assumption: The optimal activation strategy for feature compression differs from that for spatial reconstruction; learnable attention in the decoder specifically addresses boundary ambiguity in X-ray images.
- Evidence anchors:
  - [abstract] "This dual activation approach allows the encoder to focus on robust feature extraction while enabling the decoder to reconstruct features adaptively, emphasizing important regions."
  - [Section 4.1] "The different activation functions during down-sampling and up-sampling allows the encoder to focus on robust feature extraction while enabling the decoder to reconstruct features adaptively."
  - [corpus] Related work (Panoptic Segmentation and Labelling of Lumbar Spine Vertebrae using Modified Attention Unet) demonstrates attention mechanisms improve vertebrae segmentation, but does not isolate activation function effects.

### Mechanism 2
- Claim: Incremental application of AReLU across decoder layers produces progressive accuracy gains, with maximum benefit when all up-sampling layers use AReLU.
- Mechanism: Table 1 shows a monotonic increase in Dice score from 80.13% (0 AReLU layers) to 83.58% (5 AReLU layers). Each additional AReLU layer introduces learnable α/β parameters that refine feature reconstruction at progressively higher resolutions, cumulatively improving boundary delineation.
- Core assumption: The cumulative effect is linear and independent per layer; there are no diminishing returns or interference between AReLU layers in this configuration.
- Evidence anchors:
  - [Section 4.3, Table 1] Shows incremental Dice improvements: 0→80.13%, 1→80.22%, 2→81.02%, 3→81.48%, 4→82.98%, 5→83.58%.
  - [Section 5.4] "The integration of dual activation functions (ReLU and AReLU) resulted in a Dice score of 83.58%, representing a 4.1% improvement over the standard U-Net."
  - [corpus] No direct corpus evidence for incremental AReLU layer ablation; this appears novel to this paper.

### Mechanism 3
- Claim: Adding a sixth encoder block (deeper than standard U-Net) captures more abstract and globally relevant features, improving fine-grained detail recognition.
- Mechanism: The additional down-sampling layer reduces spatial resolution at the bottleneck, enabling efficient capture of global contextual information. This deeper representation improves the network's ability to understand vertebral structures holistically before reconstruction.
- Core assumption: The dataset and model capacity are sufficient to train the deeper encoder without overfitting; global context meaningfully informs local boundary decisions.
- Evidence anchors:
  - [Section 4.1] "With the inclusion of an additional down-sampling layer, the network allocates greater capacity to learning a richer and more comprehensive representation of the input."
  - [Section 4.1] "Furthermore, the added down-sampling layer reduces spatial resolution in the bottleneck, facilitating the efficient capture of global contextual information."
  - [corpus] R2MF-Net (multi-directional spine X-ray segmentation) uses recurrent residual multi-path fusion, suggesting architectural depth helps, but does not isolate encoder depth effects.

## Foundational Learning

- Concept: **U-Net encoder-decoder architecture with skip connections**
  - Why needed here: The Sandwich U-Net modifies the standard U-Net; understanding skip connections is essential to see why feature extraction and reconstruction can be asymmetrically optimized.
  - Quick check question: Can you explain why skip connections prevent information loss during up-sampling?

- Concept: **ReLU and sparse activation**
  - Why needed here: The encoder relies on ReLU's property of deactivating negative neurons to achieve sparse, efficient feature encoding.
  - Quick check question: How does ReLU mitigate the vanishing gradient problem compared to sigmoid or tanh?

- Concept: **Learnable activation functions and attention mechanisms**
  - Why needed here: AReLU introduces trainable α and β parameters; understanding how attention modulates feature importance is key to grasping the decoder's adaptive reconstruction.
  - Quick check question: What is the role of the sigmoid function in AReLU's positive branch (Equation 4)?

## Architecture Onboarding

- Component map: Input (512×512 AP X-ray) -> 6 encoder blocks (conv+ReLU+maxpool) -> Bottleneck -> 5 decoder blocks (upsample+conv+AReLU) -> Output (segmentation mask)
- Critical path: Encoder depth (6 blocks) -> Bottleneck -> Decoder with AReLU (5 blocks) -> Dice loss optimization -> Early stopping on validation loss
- Design tradeoffs:
  - Deeper encoder (6 vs. standard 4-5) improves global context but increases parameter count and risk of overfitting on small datasets
  - AReLU introduces 2 learnable parameters per decoder layer; improves accuracy but adds complexity vs. fixed ReLU
  - Early stopping at 120 epochs (vs. 200) prevents overfitting but may underutilize model capacity with more data
- Failure signatures:
  - Smooth, imprecise contours on vertebral borders (baseline U-Net behavior per Fig. 7)
  - Over-segmentation or under-segmentation on lower vertebrae where anatomical variation is high
  - Training divergence if α/β initialized poorly (e.g., α=0.99, β=1.5 yields lower Dice per Table 2)
- First 3 experiments:
  1. Replicate baseline U-Net on BUU thoracic AP subset (240 train/60 test) to confirm 80.13% Dice; establish ablation baseline
  2. Add AReLU to only the final decoder layer; measure incremental Dice gain (expect ~80.22% per Table 1)
  3. Full Sandwich U-Net (all 5 decoder layers with AReLU, α=0.9, β=0.9); target ≥83% Dice and visually inspect border accuracy on partial vertebrae

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Sandwich U-Net architecture generalize effectively to cervical, lumbar, and sacral spine regions as well as lateral view X-rays?
- Basis in paper: [explicit] The authors state in the Limitations section: "this study focuses exclusively on the thoracic region and the AP view of spine X-rays, limiting its generalisability to other anatomical regions and imaging perspectives."
- Why unresolved: The current experiments were confined to thoracic AP view images only; no experiments were conducted on other spinal regions or imaging perspectives.
- What evidence would resolve it: Training and evaluating the same architecture on cervical, lumbar, and sacral datasets, and on lateral view X-rays, reporting Dice scores and edge accuracy comparable to or better than the 83.58% achieved on thoracic AP views.

### Open Question 2
- Question: To what extent can segmentation accuracy be improved by training with larger batch sizes, higher image resolutions, and more advanced hardware configurations?
- Basis in paper: [explicit] The authors note: "hardware constraints restricted both the batch size and image resolution during training, which may have impacted model performance. The Dice score could potentially be improved by increasing the batch size with a more advanced hardware configuration."
- Why unresolved: The reported experiments were conducted under constrained computational settings (Google Colab with 12.6GB RAM), leaving the upper bound of model performance unexplored.
- What evidence would resolve it: Re-running experiments with larger batch sizes (e.g., 16, 32) and higher input resolutions (e.g., 1024×1024) on GPU hardware with greater memory, comparing resulting Dice scores against the baseline 83.58%.

### Open Question 3
- Question: Does the proposed Sandwich U-Net maintain its performance advantage when evaluated on publicly available spine X-ray datasets with different annotation protocols?
- Basis in paper: [explicit] The authors state: "we plan to validate our approach on publicly available medical image datasets to evaluate its robustness, applicability, and generalisability across diverse clinical scenarios."
- Why unresolved: The model was trained and tested on a single custom-annotated dataset (BUU), and the authors could not directly compare against recent state-of-the-art methods due to annotation differences.
- What evidence would resolve it: Training the model on one or more public spine X-ray datasets (e.g., from open challenges or repositories) and reporting performance metrics, alongside comparisons with published state-of-the-art methods on the same benchmarks.

## Limitations
- Small dataset size (300 images) may limit generalizability and increase overfitting risk
- Only tested on AP view thoracic vertebrae, no evaluation on lateral views or lumbar spine regions
- AReLU parameters (α=0.9, β=0.9) fixed without systematic hyperparameter optimization
- No training curves or standard deviation across multiple runs reported

## Confidence
- **High Confidence:** The mechanism by which ReLU provides sparse activations and AReLU introduces learnable attention parameters is well-established and directly supported by the paper's architecture description and ablation results (Dice scores 80.13%→83.58% with full AReLU integration).
- **Medium Confidence:** The claim that incremental AReLU layer addition produces progressive accuracy gains assumes linear, independent effects per layer. While Table 1 shows monotonic improvement, the absence of statistical significance testing or variance reporting weakens confidence in this cumulative benefit.
- **Medium Confidence:** The deeper encoder's ability to capture global context is theoretically sound, but the paper provides limited empirical evidence isolating depth effects from the activation function changes, and no comparison to standard 4-5 block U-Net depths.

## Next Checks
1. Replicate the ablation study with statistical significance testing (e.g., paired t-tests) across 5+ training runs to establish confidence intervals for Dice score improvements.
2. Evaluate the Sandwich U-Net on a larger, multi-view spine dataset (including lateral views and lumbar regions) to assess generalizability beyond the AP thoracic subset.
3. Conduct a controlled experiment isolating encoder depth (4 vs. 5 vs. 6 blocks) while keeping activation functions constant to quantify the marginal benefit of architectural depth independent of the dual-activation design.