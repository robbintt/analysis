---
ver: rpa2
title: 'OCT Data is All You Need: How Vision Transformers with and without Pre-training
  Benefit Imaging'
arxiv_id: '2502.12379'
source_url: https://arxiv.org/abs/2502.12379
tags:
- data
- scratch
- pre-trained
- medical
- epochs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether pre-training Vision Transformers
  (ViTs) on ImageNet is necessary for Optical Coherence Tomography (OCT) image classification.
  Experiments compare ViT (Pre-trained) and ViT (Scratch) models on a four-category
  retinal pathology dataset across different data scales.
---

# OCT Data is All You Need: How Vision Transformers with and without Pre-training Benefit Imaging

## Quick Facts
- arXiv ID: 2502.12379
- Source URL: https://arxiv.org/abs/2502.12379
- Authors: Zihao Han; Philippe De Wilde
- Reference count: 19
- Key outcome: ViT models trained from scratch achieve comparable or slightly better accuracy (91% vs 90%) than pre-trained models on OCT image classification when sufficient data is available, challenging the necessity of ImageNet pre-training for medical imaging tasks.

## Executive Summary
This study investigates whether pre-training Vision Transformers (ViTs) on ImageNet is necessary for Optical Coherence Tomography (OCT) image classification. Through controlled experiments comparing ViT (Pre-trained) and ViT (Scratch) models on a four-category retinal pathology dataset across different data scales, the authors demonstrate that while pre-training accelerates convergence in small datasets, training from scratch achieves comparable or slightly better accuracy when sufficient OCT data is available. Large-scale training yields high performance (AUC > 0.9) for both approaches, with minimal differences in confusion matrices and ROC curves. The findings suggest that domain-specific characteristics and data volume play critical roles, reducing the necessity of ImageNet pre-training for OCT classification.

## Method Summary
The study uses the Kermany et al. OCT dataset from Kaggle, comparing two ViT variants: ViT (Pre-trained) initialized with ImageNet21K weights and ViT (Scratch) with random initialization. Both models process 224×224 grayscale OCT images using 16×16 patches. Training employs Adam optimizer with learning rate 1×10⁻⁴, batch size 32, and 400 epochs for large-scale (2000+ training images) and 200 epochs for small-scale (~400 training images) experiments. The models are evaluated on accuracy, confusion matrices, and per-class ROC/AUC metrics. Basic augmentation includes random flips and small rotations.

## Key Results
- Large-scale: ViT (Scratch) achieves 90.86–91.00% accuracy vs. ViT (Pre-trained) at 90.07–90.86%
- Both models achieve AUC > 0.9 on most classes with large data
- Small-scale: ViT (Scratch) shows slightly better final accuracy (60.35%) than ViT (Pre-trained) (57.36%)
- Pre-trained model shows faster early convergence but doesn't outperform Scratch by final evaluation

## Why This Works (Mechanism)

### Mechanism 1: Domain Gap Attenuates Transfer Benefits
ImageNet pre-training provides diminishing returns when source and target domains differ substantially in imaging physics and visual statistics. OCT scans exhibit distinct contrast, texture, and noise distributions compared to natural images. Features learned from RGB photographs have limited overlap with grayscale retinal layer structures and pathology patterns. When domain gap is large, pre-trained weights require significant adaptation, reducing their initialization advantage.

### Mechanism 2: Data Volume Enables Direct Feature Learning
Sufficient in-domain training data allows scratch models to learn domain-specific features that match or exceed transferred representations. ViT's self-attention mechanism can capture long-range dependencies and global structures directly from OCT data. When training samples exceed a threshold (~2000 images), the model has sufficient signal to learn retinal layer patterns, lesion morphology, and pathological variations without relying on external priors.

### Mechanism 3: Pre-training Accelerates Early Convergence
Pre-trained weights provide a better optimization starting point, reducing epochs needed to reach stable performance. ImageNet pre-training initializes attention patterns and patch embeddings with sensible defaults. Even if domain-specific features require adaptation, these initialized weights reduce the random exploration phase of optimization.

## Foundational Learning

- **Concept: Vision Transformer (ViT) Patch Embedding**
  - Why needed here: ViT divides images into 16×16 patches treated as sequence tokens. Understanding this is essential for debugging why certain OCT features are/aren't captured.
  - Quick check question: Can you explain why a 224×224 OCT image produces 196 patch tokens?

- **Concept: Self-Attention for Global Context**
  - Why needed here: The paper claims ViT's global attention benefits medical imaging by capturing long-range dependencies across retinal layers. This distinguishes it from CNNs' local receptive fields.
  - Quick check question: How does self-attention differ from convolution in capturing relationships between distant image regions?

- **Concept: Transfer Learning Domain Gap**
  - Why needed here: The central finding hinges on quantifying mismatch between ImageNet and OCT characteristics. Without this, results appear contradictory.
  - Quick check question: Name two visual features that transfer well from ImageNet to medical imaging, and two that do not.

## Architecture Onboarding

**Component map:**
Input (224×224 grayscale OCT) → Patch embedding (16×16 patches → 196 tokens + position encoding) → Transformer encoder × L layers (self-attention + MLP blocks) → [CLS] token → Classification head (4 classes: CNV, DME, Drusen, Normal)

**Critical path:**
1. Data preprocessing: Resize/crop to 224×224, retain grayscale (single-channel)
2. Patch embedding initialization: Random (Scratch) OR ImageNet21K weights (Pre-trained)
3. Fine-tuning: Adam optimizer, lr=1e-4, batch=32, 200–400 epochs
4. Evaluation: Accuracy, confusion matrices, per-class ROC/AUC

**Design tradeoffs:**

| Decision | Option A | Option B | Consideration |
|----------|----------|----------|---------------|
| Initialization | ImageNet21K pre-trained | Random scratch | Pre-trained: faster convergence; Scratch: slightly better final accuracy with large data |
| Dataset size | >2000 images | ~400 images | Large data reduces pre-training necessity; small data benefits from early convergence aid |
| Training duration | 400 epochs | 200 epochs | Scratch models may require more epochs to plateau |

**Failure signatures:**
- Scratch model on small data: Slow convergence, high variance, potential overfitting (accuracy ~60%)
- Pre-trained model on large data: May plateau slightly below scratch (observed 90% vs 91%)
- Class confusion: Normal ↔ Drusen misclassification common across both approaches
- Convergence stall: Validation accuracy saturates early—trigger learning rate decay

**First 3 experiments:**
1. **Baseline replication:** Train ViT-Base from scratch on large-scale OCT (2000+ images, 400 epochs). Target: >90% accuracy. Confirms data sufficiency.
2. **Pre-trained comparison:** Fine-tune ImageNet21K-pretrained ViT on identical data. Compare convergence speed (epochs to 80% accuracy) and final performance.
3. **Data ablation:** Train both variants on progressively smaller subsets (2000 → 500 → 200 images). Identify crossover point where pre-training provides final-accuracy advantage, not just speed.

## Open Questions the Paper Calls Out
None

## Limitations
- Exact ViT model variant (Base, Large, etc.) and parameter count are not specified
- Learning rate schedule, data augmentation ranges, and precise train/validation/test splits are unspecified
- Study focuses solely on 4-class OCT classification without examining broader pathology coverage or multimodal integration
- Domain gap quantification relies on qualitative discussion rather than empirical measurement of feature space distances

## Confidence
- **High confidence:** The core empirical finding that ViT (Scratch) matches or slightly exceeds ViT (Pre-trained) accuracy on large-scale OCT data (>90% accuracy, AUC > 0.9)
- **Medium confidence:** The claim that pre-training primarily accelerates convergence rather than improving final accuracy, given limited ablation on convergence speed metrics
- **Low confidence:** The generalization that domain gap and data volume universally reduce ImageNet pre-training necessity across all medical imaging tasks, as this extends beyond the OCT-specific results

## Next Checks
1. **Cross-dataset generalization:** Train the best-performing model (ViT-Scratch) on OCT dataset A and evaluate on OCT dataset B from different medical centers to assess domain shift robustness
2. **Feature space analysis:** Use linear probing or representation similarity metrics (CCA, CKA) to quantify actual domain gap between ImageNet-pretrained and OCT-specific features
3. **Self-supervised pretraining:** Replace ImageNet pre-training with OCT-specific self-supervised methods (e.g., SimCLR, MAE) to test whether domain-relevant pretraining outperforms both ImageNet and random initialization