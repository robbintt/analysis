---
ver: rpa2
title: 'Mind the Motions: Benchmarking Theory-of-Mind in Everyday Body Language'
arxiv_id: '2511.15887'
source_url: https://arxiv.org/abs/2511.15887
tags:
- arxiv
- nonverbal
- cues
- explanation
- mind
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Motion2Mind, a framework for evaluating AI
  models' ability to interpret nonverbal cues (NVCs) in video contexts. It uses an
  expert-curated body-language dictionary as a proxy knowledge base to create a dataset
  of 1,022 video clips with fine-grained NVC annotations and psychological interpretations
  across 222 cue types and 397 mind states.
---

# Mind the Motions: Benchmarking Theory-of-Mind in Everyday Body Language

## Quick Facts
- arXiv ID: 2511.15887
- Source URL: https://arxiv.org/abs/2511.15887
- Reference count: 34
- Current AI models significantly underperform humans in interpreting body language, with accuracy gaps of 20-40 points

## Executive Summary
This paper introduces Motion2Mind, a novel framework for evaluating AI models' ability to interpret nonverbal cues (NVCs) in video contexts. The framework uses an expert-curated body-language dictionary as ground truth to create a dataset of 1,022 video clips with fine-grained annotations across 222 cue types and 397 mind states. The study reveals significant performance gaps between current AI models (including GPT-4o and Qwen2.5-VL) and human annotators, particularly in contextual explanation and prediction tasks, with models showing tendencies toward over-interpretation.

## Method Summary
Motion2Mind evaluates AI theory-of-mind capabilities through three distinct components: Detection (identifying NVCs in videos), Knowledge (mapping cues to meanings using a body-language dictionary), and Explanation (reasoning about contextual cues). The framework uses an expert-curated body-language dictionary as ground truth, containing 222 NVC types mapped to 397 mind states across 19 everyday scenarios. The dataset consists of 1,022 video clips annotated for fine-grained NVC presence and psychological interpretations, enabling systematic benchmarking of AI models' performance in interpreting body language.

## Key Results
- Current AI models significantly underperform human annotators across all tasks, with accuracy gaps ranging from 20-40 percentage points
- Models exhibit systematic over-interpretation tendencies, particularly on "invalid" cues lacking dictionary-supported meanings
- Performance gaps are most pronounced in contextual explanation and prediction tasks compared to basic detection and knowledge retrieval

## Why This Works (Mechanism)
Motion2Mind works by disentangling the complex task of body language interpretation into three hierarchical components: detection of nonverbal cues, knowledge mapping using a structured dictionary, and contextual explanation. The framework's effectiveness stems from using expert-curated ground truth that provides fine-grained annotations across multiple scenarios, enabling systematic evaluation of where AI models fail in theory-of-mind tasks. The multi-component structure allows identification of specific failure modes, revealing that models struggle most with contextual reasoning rather than basic cue detection.

## Foundational Learning

1. **Theory-of-Mind (ToM)** - Understanding that others have mental states different from one's own. *Why needed:* Forms the psychological basis for interpreting body language. *Quick check:* Can the model predict behavior based on inferred mental states?

2. **Nonverbal Communication (NVC)** - Cues like gestures, facial expressions, and posture that convey meaning without words. *Why needed:* The primary input modality for the framework. *Quick check:* Can the model identify and categorize distinct NVC types?

3. **Contextual Reasoning** - Ability to integrate multiple cues and situational factors into coherent interpretations. *Why needed:* Essential for accurate mind state inference beyond isolated cue detection. *Quick check:* Does the model maintain consistent interpretations across changing contexts?

## Architecture Onboarding

**Component Map:** Video Input -> Detection Module -> Knowledge Base Lookup -> Contextual Reasoning -> Mind State Output

**Critical Path:** The most error-prone sequence is Video Input -> Detection Module -> Contextual Reasoning, as this combines perception challenges with reasoning difficulties that current models struggle to integrate.

**Design Tradeoffs:** The framework trades breadth of scenarios for depth of annotation, prioritizing fine-grained ground truth over extensive scenario coverage. This enables precise performance measurement but may limit generalizability.

**Failure Signatures:** Models consistently over-interpret invalid cues, showing false positive rates 2-3x higher than humans. They also struggle with multi-cue integration, often providing inconsistent interpretations when multiple NVCs appear simultaneously.

**3 First Experiments:**
1. Test model performance on isolated vs. combined NVC presentations to measure integration capabilities
2. Evaluate cross-scenario generalization by testing models on scenarios not seen during training
3. Measure sensitivity to temporal ordering of NVCs to assess temporal reasoning abilities

## Open Questions the Paper Calls Out
None

## Limitations
- The expert-curated body-language dictionary may reflect cultural biases inherent in Western psychological frameworks
- The 1,022 video clips represent a relatively constrained sample of everyday interactions, limiting generalizability
- Claims about AI "over-interpretation" tendencies require additional validation using alternative coding schemes or cultural contexts

## Confidence

**High confidence** in the methodology's validity for detecting systematic performance gaps between AI models and human annotators

**Medium confidence** in the generalizability of findings to real-world applications, given the controlled experimental conditions

**Low confidence** in claims about AI "over-interpretation" tendencies without additional validation using alternative coding schemes or cultural contexts

## Next Checks
1. Test the framework's robustness across diverse cultural contexts by replicating the study with body-language dictionaries and video samples from non-Western cultures to assess cultural bias in both the knowledge base and model performance
2. Expand the video dataset size and scenario diversity by an order of magnitude to evaluate whether current performance gaps persist across broader population samples and interaction types
3. Conduct longitudinal validation with multiple expert annotator groups to establish inter-rater reliability for the fine-grained NVC annotations and assess potential subjectivity in the ground truth labels