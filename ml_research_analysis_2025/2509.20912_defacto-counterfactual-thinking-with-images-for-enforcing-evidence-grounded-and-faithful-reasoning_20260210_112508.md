---
ver: rpa2
title: 'DeFacto: Counterfactual Thinking with Images for Enforcing Evidence-Grounded
  and Faithful Reasoning'
arxiv_id: '2509.20912'
source_url: https://arxiv.org/abs/2509.20912
tags:
- reasoning
- answer
- visual
- evidence
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "DeFacto introduces a counterfactual thinking framework to improve\
  \ multimodal reasoning by enforcing evidence\u2013answer consistency. The approach\
  \ trains models with three complementary paradigms\u2014positive supervision, counterfactual\
  \ abstention, and random-masking\u2014ensuring predictions are grounded in the correct\
  \ visual evidence."
---

# DeFacto: Counterfactual Thinking with Images for Enforcing Evidence-Grounded and Faithful Reasoning

## Quick Facts
- arXiv ID: 2509.20912
- Source URL: https://arxiv.org/abs/2509.20912
- Reference count: 15
- Key outcome: DeFacto improves multimodal reasoning accuracy and faithfulness with mAP 35.5, AP50 49.8, AP75 35.0, IoU 49.2, and accuracy 60.8 on faithfulness benchmarks.

## Executive Summary
DeFacto introduces a counterfactual thinking framework to improve multimodal reasoning by enforcing evidence-answer consistency. The approach trains models with three complementary paradigms—positive supervision, counterfactual abstention, and random-masking—ensuring predictions are grounded in the correct visual evidence. Evidence regions are automatically localized using a language-guided pipeline with open-vocabulary detection, enabling the construction of a counterfactual dataset (DeFacto-100K) and a human-annotated benchmark (DeFacto-1.5K). Models are optimized via GRPO-based reinforcement learning with tailored rewards for answer correctness, format consistency, and region selection coherence. Experiments show substantial gains in both accuracy and faithfulness, with mAP 35.5, AP50 49.8, AP75 35.0, IoU 49.2, and accuracy 60.8 on the faithfulness benchmark, surpassing strong baselines.

## Method Summary
DeFacto trains multimodal models using three paradigms: positive supervision (matching correct answers to evidence), counterfactual abstention (avoiding predictions when evidence is perturbed), and random-masking (encouraging attention to relevant regions). Evidence regions are automatically localized through a language-guided pipeline with open-vocabulary detection. A counterfactual dataset (DeFacto-100K) is constructed by perturbing evidence masks, and a human-annotated benchmark (DeFacto-1.5K) is used for evaluation. Models are optimized via GRPO-based reinforcement learning with rewards for answer correctness, format consistency, and region selection coherence.

## Key Results
- Substantial improvements in accuracy and faithfulness metrics over strong baselines
- mAP 35.5, AP50 49.8, AP75 35.0, IoU 49.2, and accuracy 60.8 on faithfulness benchmarks
- DeFacto-100K dataset and DeFacto-1.5K benchmark enable systematic evaluation of evidence-grounded reasoning

## Why This Works (Mechanism)
The counterfactual thinking framework enforces evidence-grounded reasoning by training models to align their predictions with correct visual evidence. By introducing counterfactual scenarios where evidence is perturbed, the model learns to abstain from answering when evidence is insufficient or inconsistent. The three training paradigms ensure that the model not only predicts correctly but also grounds its reasoning in the appropriate visual regions, improving both accuracy and faithfulness.

## Foundational Learning
- **Counterfactual reasoning**: Generating alternative scenarios to test model robustness; needed to ensure predictions are grounded in correct evidence
- **Evidence localization**: Automatically identifying relevant image regions; needed to provide training signals for evidence-grounded reasoning
- **GRPO-based reinforcement learning**: Optimizing models with tailored rewards; needed to balance accuracy and faithfulness objectives
- **Open-vocabulary detection**: Detecting objects without predefined categories; needed for flexible evidence localization across diverse images
- **Mask perturbation**: Altering evidence regions to create counterfactual examples; needed to train models to abstain when evidence is insufficient

## Architecture Onboarding

### Component Map
Image Encoder -> Evidence Localization Pipeline -> Mask Perturbation Module -> Training Paradigms (Positive Supervision, Counterfactual Abstention, Random-Masking) -> GRPO-based RL Optimizer -> Multimodal Reasoning Model

### Critical Path
1. Image and question input to evidence localization pipeline
2. Automatic region detection and mask generation
3. Perturbation of evidence masks for counterfactual training
4. Model training with three paradigms and GRPO optimization
5. Evaluation on faithfulness benchmarks

### Design Tradeoffs
- **Automatic vs. human annotation**: Automatic region localization enables large-scale dataset construction but may introduce noise
- **Three training paradigms**: Comprehensive but computationally intensive
- **GRPO vs. supervised learning**: Better faithfulness but requires careful reward design and may have optimization instability

### Failure Signatures
- Model may correlate with evidence regions superficially without genuine reasoning
- Automatic localization may miss subtle evidence or include irrelevant regions
- Reinforcement learning may converge slowly or get stuck in local optima

### Exactly 3 First Experiments
1. **Ablation of training paradigms**: Remove each of the three paradigms to quantify their individual contributions to faithfulness
2. **Human evaluation of reasoning**: Assess whether model reasoning genuinely incorporates evidence beyond what mAP/IoU metrics capture
3. **Cross-dataset generalization**: Evaluate on an external dataset with different image distributions to test true generalization

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- Automatic region localization may introduce noise or bias, affecting training quality
- Faithfulness metrics (mAP, IoU) do not directly measure genuine reasoning incorporation
- Computational overhead from counterfactual dataset generation and GRPO optimization

## Confidence
- **High**: Experimental results show statistically significant improvements in accuracy and faithfulness
- **Medium**: Evidence-grounded reasoning is supported by faithfulness metrics but lacks direct causal reasoning validation
- **Low**: Generalization claims are not empirically validated on external datasets

## Next Checks
1. Conduct systematic ablation study removing each training paradigm to quantify individual contributions
2. Implement human evaluation protocol to assess genuine reasoning incorporation beyond faithfulness metrics
3. Evaluate trained models on external, unseen dataset to assess true generalization capability