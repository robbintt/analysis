---
ver: rpa2
title: The German Commons - 154 Billion Tokens of Openly Licensed Text for German
  Language Models
arxiv_id: '2510.13996'
source_url: https://arxiv.org/abs/2510.13996
tags:
- text
- german
- data
- language
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the German Commons, the largest collection
  of openly licensed German text for language model training. The dataset addresses
  the scarcity of large-scale, legally compliant German text by aggregating 154.56
  billion tokens from 35.78 million documents across 41 sources spanning seven domains
  including legal, scientific, cultural, political, news, economic, and web content.
---

# The German Commons - 154 Billion Tokens of Openly Licensed Text for German Language Models

## Quick Facts
- arXiv ID: 2510.13996
- Source URL: https://arxiv.org/abs/2510.13996
- Reference count: 40
- Key outcome: 154.56 billion tokens of openly licensed German text across 7 domains

## Executive Summary
This work introduces the German Commons, the largest collection of openly licensed German text for language model training. The dataset addresses the scarcity of large-scale, legally compliant German text by aggregating 154.56 billion tokens from 35.78 million documents across 41 sources spanning seven domains including legal, scientific, cultural, political, news, economic, and web content. Each document carries explicit open licenses (CC-BY-SA 4.0 or equivalent), enabling truly open model development without the licensing uncertainties of web-scraped alternatives. A comprehensive processing pipeline implements quality filtering, deduplication, text formatting, and PII removal, retaining 50.73% of input while maintaining consistent quality. Corpus statistics demonstrate suitability for language model pretraining, with balanced text complexity distribution across domains and minimal toxic content. The dataset and its open-source preprocessing pipeline are fully reproducible and extensible.

## Method Summary
The corpus construction follows a systematic approach: (1) sourcing exclusively from institutional providers with verifiable open licenses, (2) extracting text from diverse formats using domain-specific parsers, (3) applying German-specific quality filtering with language identification and 27 heuristics, (4) implementing paragraph-level deduplication using LSH bloom filters, and (5) removing PII through regex and Presidio. The pipeline processes 305B initial tokens, retaining 50.73% after filtering. Documents are partitioned by domain and source, with explicit SPDX license metadata. The final corpus contains 154.56B tokens across 35.78M documents, with 82% from historical news and cultural domains.

## Key Results
- 154.56 billion tokens of German text from 35.78 million documents
- 50.73% retention rate after comprehensive quality filtering and deduplication
- 41 source datasets spanning 7 domains with verifiable open licenses
- 82% of tokens from news and cultural domains (predominantly historical content)
- Scientific domain represents only 0.54% of tokens despite including arXiv and OpenAlex

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sourcing exclusively from institutional providers with verifiable licensing enables legally compliant model training.
- Mechanism: By aggregating from national libraries, government agencies, and academic institutions rather than web scrapes, each document carries explicit SPDX-canonical license URLs. This creates chain-of-custody from source to training data, reducing legal uncertainty for downstream model redistribution.
- Core assumption: Institutional providers accurately label and have authority to license their holdings; no independent license audit is performed.
- Evidence anchors:
  - [abstract] "systematic sourcing from established data providers with verifiable licensing"
  - [Section 3.2] "We follow Kandpal et al. in adopting the Open Knowledge Foundation's Open Definition 2.1"
  - [corpus] Related work (Common Pile v0.1) shows similar approach yielding 8TB of compliant English text, suggesting transferability of institutional sourcing strategies.
- Break condition: If downstream jurisdictions reject the interpretation that openly licensed text permits model training, or if institutional sources contain mislicensed content, legal exposure remains.

### Mechanism 2
- Claim: Multi-stage quality filtering with German-specific heuristics retains ~51% of input while eliminating low-quality artifacts.
- Mechanism: The pipeline applies sequential filters—language identification (FastText with 0.65 German threshold), length filtering (≥32 GPT-2 tokens), quality heuristics (repetition, word length distribution, OCR-specific anomalies), paragraph-level deduplication (20-gram MinHash, 0.8 collision), and PII removal. Percentile-based thresholds (5th/95th) adapt to the corpus distribution rather than imported English parameters.
- Core assumption: Quality heuristics designed for web text generalize to German OCR-sourced institutional documents; manual spot-checking validated parameter choices.
- Evidence anchors:
  - [abstract] "processing pipeline implements comprehensive quality filtering, deduplication, and text formatting fixes"
  - [Section 5] "Overall retention reached 50.73% of input"
  - [corpus] Weak corpus signal—no comparative evidence that this specific filtering regime improves downstream model quality versus alternatives.
- Break condition: If filtering removes linguistically valuable content (e.g., historical orthography flagged as OCR errors), domain-specific performance may degrade.

### Mechanism 3
- Claim: Domain-balanced composition across seven thematic areas supports learning diverse linguistic registers.
- Mechanism: The corpus spans web (43% of docs), news (37%), cultural (17%), legal, political, scientific, and economic domains with varying text complexity. Analysis shows scientific content at 63.8% "special language" versus web at 81.4% "everyday," enabling models to acquire both formal and colloquial German.
- Core assumption: Statistical domain balance at the corpus level translates to balanced model competence; no training-time mixing ratios are specified.
- Evidence anchors:
  - [Section 5] "balanced complexity distribution across domains enables learning across linguistic registers"
  - [Table 1] Shows token distribution across all seven domains
  - [corpus] No direct evidence from trained models; related datasets (Dolma, FineWeb) suggest domain diversity improves generalization but results are language-dependent.
- Break condition: If a target application requires domain expertise not represented proportionally (e.g., scientific/technical German at only 0.54% of tokens), supplementary data would be needed.

## Foundational Learning

- Concept: **Open license taxonomy (CC0, CC-BY, CC-BY-SA)**
  - Why needed here: The corpus explicitly excludes non-commercial and research-only licenses; practitioners must understand attribution vs. share-alike obligations for derivative models.
  - Quick check question: Can you redistribute a model trained on CC-BY-SA data under a proprietary license?

- Concept: **Tokenization and sequence length distribution**
  - Why needed here: The corpus reports GPT-2 token counts and segments data by context length (≤2048, ≤8192, ≤32768, >32768 tokens), which informs batching strategy and architecture choices.
  - Quick check question: If training a model with 4096 context window, which domain provides the most tokens per document on average?

- Concept: **OCR error modes in historical German text**
  - Why needed here: ~35% of tokens come from cultural/news domains with significant OCR content; diacritic errors (ä→a, ö→o) and Fraktur-to-Latin misrecognition persist despite filtering.
  - Quick check question: What percentage of documents were removed by OCR-specific quality filters, and might this systematically exclude certain historical periods?

## Architecture Onboarding

- Component map:
  - Sourcing layer: 41 source datasets → domain classifiers → license validators
  - Extraction layer: PDF/TEI parsers (Grobid, OlmOCR, mwparserfromhell) → plain text
  - Quality layer: FastText language ID → length filter → 25+ quality heuristics → LSH deduplication
  - Privacy layer: Regex + Presidio PII detection → replacement/redaction
  - Output layer: Parquet files partitioned by domain/source with SPDX metadata

- Critical path: Language filtering (46% removed) → quality heuristics → deduplication (2.7% additional removal). The language filter is the dominant loss point; multilingual source corpora like The Stack retain only 2.22% of documents.

- Design tradeoffs:
  - **Trust vs. audit**: Accepting institutional license labels without independent verification enables scale but retains misattribution risk.
  - **Retention vs. precision**: 50.73% retention is aggressive; conservative filtering would preserve more content at potential quality cost.
  - **Paragraph vs. document deduplication**: Paragraph-level preserves more long documents but may leave semantic near-duplicates intact.

- Failure signatures:
  - **Temporal bias**: 82% of tokens from news/cultural domains, predominantly historical; contemporary German underrepresented.
  - **Dialect gap**: Standard German dominates; Swiss, Austrian, Low German dialects minimally represented.
  - **Scientific scarcity**: Only 0.54% of tokens from scientific domain despite inclusion of arXiv, OpenAlex.

- First 3 experiments:
  1. **Domain ablation**: Train separate models on each domain subset to quantify domain-specific perplexity and downstream task performance.
  2. **Temporal slicing**: Partition news/cultural by publication date to measure nostalgia bias in generated text.
  3. **Filtering sensitivity**: Re-process a source corpus (e.g., Wikipedia) with relaxed thresholds to measure quality-filter impact on retention and model perplexity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can specialized LLM-based correction models effectively reduce OCR errors in the historical segments of the corpus without introducing hallucinations or misinterpreting historical orthography?
- Basis in paper: [explicit] The authors state in Section 6 that they did not apply LLM-based correction due to "hallucination risks and misinterpretation of historical texts," but suggest it "would be a future improvement if specialized correction models become available."
- Why unresolved: The authors intentionally excluded LLM-based cleaning to avoid altering the historical integrity of the text, leaving the trade-off between OCR noise reduction and text fidelity unresolved.
- What evidence would resolve it: A comparative study measuring hallucination rates and semantic drift in historical German texts processed by specialized LLM correction models versus the current regex-based pipeline.

### Open Question 2
- Question: To what extent does the corpus's temporal bias toward historical content induce "nostalgia bias" in resulting models, and does this degrade performance on contemporary language tasks?
- Basis in paper: [explicit] Section 6 notes that the heavy representation of 18th–20th-century texts "induces nostalgia bias" and that "Adding contemporary German text to rebalance the temporal distribution is paramount for future extensions."
- Why unresolved: The authors highlight the skew but do not quantify the impact of this bias on downstream model behavior or benchmark performance.
- What evidence would resolve it: Evaluation of models trained on this corpus against contemporary German benchmarks compared to models trained on temporally balanced datasets.

### Open Question 3
- Question: How can the underrepresentation of non-standard German varieties (e.g., Swiss, Austrian, Low German) be effectively mitigated given the scarcity of openly licensed dialectal resources?
- Basis in paper: [explicit] The authors acknowledge in Section 6 that "Standard German dominates content" and that "Targeted inclusion of dialectal and minority language varieties can improve this situation, if they become available under open licenses."
- Why unresolved: The paper identifies the scarcity of source material as the primary barrier, leaving the methodology for sourcing or synthesizing such data an open problem.
- What evidence would resolve it: Identification of new open-licensed dialectal archives or the successful integration/validation of synthetic data generation techniques to augment the corpus.

### Open Question 4
- Question: Do current regex- and Presidio-based PII removal methods fail to capture contextually sensitive information within the historical legal and parliamentary records?
- Basis in paper: [explicit] Section 6 states that the applied PII removal "provides limited security" and constitutes "surface-level modifications," implying deeper or contextual PII may persist.
- Why unresolved: The authors rely on pattern matching which may miss non-standard or historical formats of personal data present in the institutional records.
- What evidence would resolve it: A manual audit or advanced named entity recognition analysis of the "Legal" and "Political" subsets to detect residual PII missed by the current surface-level filters.

## Limitations

- **Legal Compliance Risk**: No independent audit confirms license accuracy; share-alike obligations may impose redistribution requirements not fully analyzed.
- **Temporal Bias**: 82% of tokens from historical news/cultural domains; contemporary German underrepresented.
- **Scientific Scarcity**: Despite including arXiv/OpenAlex, scientific content comprises only 0.54% of tokens.

## Confidence

- **High Confidence**: Token counts and domain distributions, processing pipeline steps and retention rates, license classification methodology
- **Medium Confidence**: Claims about balanced linguistic register learning, effectiveness of quality filtering heuristics, downstream model utility without empirical validation
- **Low Confidence**: Long-term legal compliance for model training, generalization of filtering parameters, real-world impact of temporal bias

## Next Checks

1. **Legal Compliance Audit**: Engage intellectual property counsel to review CC-BY-SA share-alike obligations and model training interpretation under German/EU law.

2. **Temporal Distribution Analysis**: Partition news/cultural domains by publication date to quantify contemporary versus historical content, then train small models on different temporal slices.

3. **Scientific Domain Augmentation**: Supplement corpus with additional scientific German text, re-process with relaxed filters, and measure changes in scientific domain perplexity and task performance.