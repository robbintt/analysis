---
ver: rpa2
title: 'DecoKAN: Interpretable Decomposition for Forecasting Cryptocurrency Market
  Dynamics'
arxiv_id: '2512.20028'
source_url: https://arxiv.org/abs/2512.20028
tags:
- series
- forecasting
- time
- decokan
- cryptocurrency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DecoKAN addresses the challenge of accurate and interpretable forecasting
  in cryptocurrency markets, where traditional deep learning models struggle to decouple
  long-term trends from high-frequency volatility and lack transparency. The proposed
  framework integrates multi-level Discrete Wavelet Transform (DWT) for hierarchical
  signal decomposition with Kolmogorov-Arnold Networks (KANs) for interpretable nonlinear
  modeling, enabling frequency-specific analysis while preserving intrinsic model
  interpretability.
---

# DecoKAN: Interpretable Decomposition for Forecasting Cryptocurrency Market Dynamics

## Quick Facts
- arXiv ID: 2512.20028
- Source URL: https://arxiv.org/abs/2512.20028
- Reference count: 40
- Primary result: DecoKAN achieves lowest average MSE on cryptocurrency datasets (BTC, ETH, XMR), outperforming state-of-the-art baselines

## Executive Summary
DecoKAN addresses the challenge of accurate and interpretable forecasting in cryptocurrency markets by integrating multi-level Discrete Wavelet Transform (DWT) with Kolmogorov-Arnold Networks (KANs). The framework decomposes complex time series into frequency-specific components, processes each with dedicated KAN branches, then reconstructs predictions. Extensive experiments demonstrate DecoKAN's superior performance on real-world cryptocurrency datasets, achieving approximately 15-29% MSE reduction compared to competitive baselines while providing intrinsic interpretability through symbolic formula extraction.

## Method Summary
DecoKAN is a forecasting framework that decomposes multivariate time series using multi-level DWT, processes each frequency component with specialized KAN Resolution Branches, and reconstructs predictions via IDWT. The architecture uses RevIN for normalization, parallel KAN mixers (Temporal + Feature) within each branch, and applies L1 + entropy regularization during training. B-spline activations with grid size G=5 and order k=3 provide the learnable nonlinear transformations, while pruning reveals functional specialization between approximation (dense) and detail (sparse) branches.

## Key Results
- Reduces average MSE by approximately 15.0% on BTC, 29.1% on ETH, and 15.1% on XMR compared to WPMixer
- Achieves top-10 symbolic formulas with R² > 0.98 for extracted economic relationships
- Demonstrates asymmetric sparsity between branches: Approximation Branch retains 95.21% connections, Detail Branch only 23.72%

## Why This Works (Mechanism)

### Mechanism 1: Frequency-Specific Decomposition
DWT isolates low-frequency structural trends from high-frequency volatility by iteratively applying Daubechies 4 filters. Each coefficient series is processed independently, minimizing spectral interference and enabling specialized modeling of distinct market dynamics.

### Mechanism 2: KAN Interpretable Nonlinear Modeling
KAN mixers replace fixed activations with parameterized B-spline functions and sparsity regularization. The Kolmogorov-Arnold theorem justifies this approach, enabling extraction of symbolic formulas that capture meaningful economic relationships.

### Mechanism 3: Functional Branch Specialization
Post-training pruning reveals asymmetric connectivity patterns: Approximation Branch remains dense (95.21% retention) while Detail Branch becomes sparse (23.72% retention). This suggests low-frequency trends require distributed representation while high-frequency patterns are captured by sparse critical pathways.

## Foundational Learning

- **Discrete Wavelet Transform (DWT)**
  - Why needed: Core decomposition mechanism; understanding approximation vs. detail coefficients is essential for interpreting branch-level outputs
  - Quick check: Given a 96-step lookback window with db4 wavelet and m=2 decomposition levels, what are the lengths of the resulting coefficient series (A2, D2, D1)?

- **B-spline basis functions**
  - Why needed: KAN's learnable activations are linear combinations of B-splines; grid size and spline order control expressiveness and smoothness
  - Quick check: With grid size G=5 and spline order k=3, how many B-spline basis functions Bj(x) contribute to each activation φ(x)?

- **Kolmogorov-Arnold Representation Theorem**
  - Why needed: Theoretical foundation justifying KAN architecture—any multivariate continuous function can be represented as compositions of univariate functions
  - Quick check: Why does this theorem suggest that edge-level learnable activations (KAN) might be sufficient, versus node-level fixed activations (MLP)?

## Architecture Onboarding

- **Component map:** Input (L×C) -> RevIN.norm -> Transpose (C×L) -> DWT -> [Branch processing in parallel] -> IDWT -> Transpose -> RevIN.denorm -> Output (T×C)

- **Critical path:** Input (L×C) → RevIN.norm → Transpose (C×L) → DWT → [Branch processing in parallel] → IDWT → Transpose → RevIN.denorm → Output (T×C)

- **Design tradeoffs:**
  - Wavelet choice (db4 vs db2): db4 offers better smoothness for trend capture; db2 is more localized for sharp transitions
  - Decomposition level (m): Higher m provides finer frequency separation but increases branch count and parameters
  - Grid size G and spline order k: Larger G increases expressiveness but also parameters and regularization complexity

- **Failure signatures:**
  - High training loss with low validation loss: Over-regularization (γ too high) forcing excessive sparsity
  - Nan outputs during training: RevIN variance near zero—check epsilon/clamp implementation
  - Inference latency spike: Check for unnecessary gradient tracking
  - Symbolic formulas with R² < 0.8: Candidate function library may be insufficient or pruning threshold τ too aggressive

- **First 3 experiments:**
  1. Baseline sanity check: Run DecoKAN (m=1, G=5) vs. MLP-only variant on ETH dataset to verify KAN contribution
  2. Decomposition ablation: Compare m=0 vs. m=1 vs. m=2 on BTC dataset, monitoring MSE and pruning ratios
  3. Interpretability validation: Extract top-5 symbolic formulas from Detail Branch after ETH training, verifying R² > 0.95 and consistency across random seeds

## Open Questions the Paper Calls Out
- Can efficient KAN implementations (CUDA-accelerated grids, quantization) reduce training overhead without compromising interpretability?
- Can KAN modules be integrated into other forecasting backbones (e.g., Transformers) to create new interpretable models?
- Can DecoKAN's transparency be leveraged for financial tasks like market manipulation detection and DeFi risk assessment?

## Limitations
- Interpretability claims partially validated; practical economic significance of symbolic formulas requires external validation
- Functional specialization evidence sensitive to hyperparameter choices rather than intrinsic data structure
- Cross-frequency coupling effects not fully evaluated—independent branch processing may lose critical interaction information

## Confidence
- Core Decomposition Mechanism: High
- KAN Interpretability Claims: Medium  
- Functional Specialization Evidence: Low-Medium

## Next Checks
1. Design experiment with artificially generated cryptocurrency data containing strong cross-frequency dependencies to test whether DecoKAN loses critical information through independent branch processing
2. Train DecoKAN on ETH dataset with 5 different random seeds, extracting top-5 symbolic formulas from Detail Branch each time to assess R² consistency, functional form stability, and economic interpretability
3. Implement KAN-only baseline (no DWT) with matched parameter count to DecoKAN, comparing performance on BTC dataset across horizons T=24, 48, 96 to isolate decomposition benefits beyond parameter scaling