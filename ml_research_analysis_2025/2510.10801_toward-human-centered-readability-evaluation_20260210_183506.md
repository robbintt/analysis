---
ver: rpa2
title: Toward Human-Centered Readability Evaluation
arxiv_id: '2510.10801'
source_url: https://arxiv.org/abs/2510.10801
tags:
- health
- evaluation
- human
- text
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the Human-Centered Readability Score (HCRS),\
  \ a five-dimensional evaluation framework for assessing simplified health texts.\
  \ HCRS addresses limitations in existing NLP metrics (BLEU, SARI, FKGL) by incorporating\
  \ clarity, trustworthiness, tone appropriateness, cultural relevance, and actionability\u2014\
  dimensions critical for effective health communication."
---

# Toward Human-Centered Readability Evaluation

## Quick Facts
- arXiv ID: 2510.10801
- Source URL: https://arxiv.org/abs/2510.10801
- Authors: Bahar İlgen; Georges Hattab
- Reference count: 9
- Introduces HCRS: A five-dimensional framework evaluating simplified health texts beyond surface-level simplicity

## Executive Summary
This paper proposes the Human-Centered Readability Score (HCRS), a framework that evaluates simplified health texts across five dimensions: clarity, trustworthiness, tone appropriateness, cultural relevance, and actionability. Unlike traditional metrics like BLEU or FKGL, HCRS integrates automatic measures with structured human feedback to capture relational and contextual aspects critical for effective health communication. The framework aims to advance readability evaluation by making it more responsive to diverse users' needs in high-stakes health contexts.

## Method Summary
HCRS combines automatic readability indices (FKGL, SMOG), pragmatic feature extraction (sentiment, politeness, empathy classifiers, imperative/procedural language), and validated survey instruments with structured human feedback. Each of the five dimensions is operationalized through specific automatic features and human Likert-scale ratings, combined using empirically calibrated weights. A hybrid protocol collects micro-ratings from target users, aggregates results, and integrates feedback through participatory workshops to iteratively refine the evaluation framework.

## Key Results
- Identifies five dimensions missing from existing NLP readability metrics: trustworthiness, actionability, and cultural relevance
- Proposes hybrid automatic-human scoring formulas for each dimension (ToneHCRS, CultureHCRS, ActionHCRS)
- Warns that RLAIF may amplify biases in high-stakes health communication contexts
- Demonstrates need for participatory feedback loops to ensure alignment with real-world user needs

## Why This Works (Mechanism)

### Mechanism 1: Multi-dimensional decomposition of readability
Decomposing readability into five distinct dimensions captures aspects invisible to surface-level metrics. Each dimension is operationalized through specific automatic features combined with structured human ratings. Core assumption: These five dimensions are sufficiently comprehensive and can be measured reliably across populations. Evidence: Existing metrics fail to address trustworthiness, actionability, and cultural relevance (Table 1). Break condition: If dimension weights vary drastically across populations, the composite score may not generalize.

### Mechanism 2: Hybrid automatic-human integration
Combining automatic measures with structured human feedback captures relational and contextual aspects neither approach captures alone. Automatic features are computed alongside human ratings on validated survey instruments, with per-dimension hybrid scores using empirically calibrated weights. Core assumption: Human raters can reliably assess abstract qualities on Likert scales. Evidence: Automatic metrics correlate only weakly-to-moderately with human judgments (Alva-Manchego et al., 2021). Break condition: If human annotation is inconsistent or expensive at scale, the hybrid approach becomes impractical.

### Mechanism 3: Participatory feedback loop for iterative refinement
Embedding HCI techniques into the evaluation pipeline makes metric optimization responsive to real-world needs. Lightweight annotation interfaces collect micro-ratings → stakeholder workshops review aggregated outputs → iterative updates recalibrate HCRS dimension weights and criteria. Core assumption: Target users will engage with evaluation interfaces and provide meaningful, consistent feedback. Evidence: Simplified health texts improved comprehension accuracy from 33% to 59% (Leroy et al., 2022). Break condition: If target populations do not participate due to accessibility barriers or trust issues, the feedback loop fails.

## Foundational Learning

- **Concept: Surface-level vs. human-centered metrics**
  - Why needed here: BLEU, SARI, and FKGL measure n-gram overlap, edit operations, and sentence length—but cannot assess whether a text feels trustworthy, respectful, or actionable to real users.
  - Quick check question: Can you explain why a text with a low FKGL score might still alienate readers?

- **Concept: Validated survey instruments in health communication**
  - Why needed here: HCRS relies on instruments like the Trust in Health Information Questionnaire and cognitive load indices to operationalize subjective dimensions.
  - Quick check question: What are two risks of using ad-hoc Likert items instead of validated instruments?

- **Concept: RLAIF limitations in high-stakes domains**
  - Why needed here: The paper warns that AI-generated feedback loops risk misalignment with human values when optimization targets are generic rather than domain-specific.
  - Quick check question: Why might RLAIF amplify biases in public health communication contexts?

## Architecture Onboarding

- **Component map:**
  - Input layer: Raw simplified health texts
  - Automatic evaluation module: Readability indices (FKGL, SMOG), sentiment/emotion classifiers, politeness classifiers, empathy detectors, imperative/procedural language analyzers, cultural entity matching
  - Human evaluation module: Micro-survey interfaces with Likert-scale items for each of five dimensions
  - Scoring layer: Weighted hybrid formulas per dimension (Tone_HCRS, Culture_HCRS, Action_HCRS, etc.)
  - Feedback integration layer: Aggregation pipeline feeding into participatory workshops and model retraining

- **Critical path:**
  1. Collect simplified texts and compute automatic features
  2. Deploy micro-surveys to target users (5–10 min per session)
  3. Aggregate human ratings and calibrate dimension weights on validation data
  4. Compute composite HCRS scores
  5. Feed results into stakeholder workshops for criteria refinement
  6. Update model training objectives (e.g., multi-objective learning, auxiliary classifiers)

- **Design tradeoffs:**
  - Automation vs. coverage: Clarity and actionability can be partially automated; trustworthiness and cultural relevance require more human input
  - Scalability vs. fidelity: Lightweight micro-surveys scale better but may miss nuanced feedback; in-depth participatory workshops are richer but resource-intensive
  - RLAIF vs. RLHF: RLAIF scales feedback collection but risks misalignment; RLHF maintains human grounding but is costly

- **Failure signatures:**
  - Low inter-rater reliability on human Likert ratings (suggests ambiguous dimension definitions)
  - HCRS scores that contradict observed comprehension outcomes (suggests miscalibrated weights)
  - Participatory feedback dominated by one stakeholder group (suggests inclusion gaps)
  - Overfitting to automatic features when human feedback is sparse

- **First 3 experiments:**
  1. Dimension alignment study: Correlate each automatic feature with human ratings across 50+ simplified health texts to identify predictive validity
  2. Weight calibration pilot: Use validation set (n=30–50 texts) to empirically tune α, β, γ, λ, δ, μ coefficients via regression against ground-truth user evaluations
  3. Participatory feedback loop prototype: Deploy micro-survey interface to 20–30 target users, aggregate results, conduct one stakeholder workshop to assess refinement effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Which existing automatic readability metrics correlate most strongly with human-centered dimensions in simplified health texts?
- Basis in paper: Listed as Q1 in introduction; authors state they "investigate" this alignment
- Why unresolved: Paper proposes framework but presents no empirical correlation data between existing metrics and five human-centered dimensions
- What evidence would resolve it: Correlation coefficients from user studies comparing SARI, BLEU, FKGL, BERTScore, SALSA against human ratings on each HCRS dimension

### Open Question 2
- Question: Can a composite HCRS integrating automatic features with structured human feedback achieve measurably stronger alignment with user evaluations than the best standalone automatic metric?
- Basis in paper: Listed as Q2; framed as core empirical question the framework is designed to address
- Why unresolved: Hybrid scoring formulas are proposed conceptually but no validation study demonstrates superior alignment over baseline metrics
- What evidence would resolve it: Controlled comparison showing HCRS composite scores predict user-perceived quality better than SARI, BLEU, or FKGL alone

### Open Question 3
- Question: What are the optimal weights for combining automatic measures and human feedback across HCRS dimensions, and do these weights generalize across populations and health domains?
- Basis in paper: Limitations state "weighting of dimensions is currently conceptual and requires calibration against real-world user judgments"
- Why unresolved: Formulas specify coefficients but these are placeholders; no calibration procedure or population-specific values provided
- What evidence would resolve it: Empirical calibration studies determining weights that maximize alignment with user judgments, tested for stability across literacy levels, cultural groups, and health subdomains

### Open Question 4
- Question: Can HCRS dimensions and evaluation protocols generalize to non-health domains where clarity, trust, and usability are critical?
- Basis in paper: Conclusion notes framework "extends to any domain" but "core dimensions may require adaptation" and "significant work required to ensure generalizability, validity, and relevance outside of health contexts"
- Why unresolved: No empirical evidence tests whether five-dimension structure and measurement approaches transfer effectively to domains like legal, financial, or technical communication
- What evidence would resolve it: Cross-domain validation studies applying HCRS to non-health texts with dimension redefinition and recalibration as needed

## Limitations
- Framework's effectiveness depends critically on unknown dimension weights and reliability of human Likert ratings for abstract constructs
- No empirical validation or calibration dataset provided, making it impossible to assess whether automatic features predict intended dimensions
- Claims about participatory feedback loops improving model alignment are speculative without evidence of stakeholder engagement effectiveness

## Confidence

- **High confidence**: Identification of five critical dimensions missing from current NLP metrics is well-supported by literature on health communication barriers
- **Medium confidence**: Hybrid automatic-human integration approach is theoretically sound but lacks empirical validation of weight calibration or inter-rater reliability
- **Low confidence**: Claims about participatory feedback loops improving model alignment are speculative without evidence of stakeholder engagement effectiveness or long-term impact on model outputs

## Next Checks

1. **Dimensional alignment study**: Correlate each automatic feature (FKGL, sentiment, politeness, empathy, imperative detection) with human ratings across 50+ simplified health texts to verify predictive validity for each dimension

2. **Weight calibration experiment**: Using a validation set (n=30-50 texts), empirically tune α, β, γ, λ, δ, μ coefficients via regression against ground-truth user evaluations to establish baseline scoring parameters

3. **Human annotation reliability test**: Conduct inter-rater reliability analysis on Likert-scale ratings for all five dimensions across diverse annotator pools to assess consistency and identify problematic constructs