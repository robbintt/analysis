---
ver: rpa2
title: A Comprehensive Dataset for Human vs. AI Generated Text Detection
arxiv_id: '2510.22874'
source_url: https://arxiv.org/abs/2510.22874
tags:
- text
- arxiv
- dataset
- detection
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a dataset for detecting AI-generated text.
  The dataset contains over 58,000 samples combining authentic New York Times articles
  with synthetic versions generated by multiple state-of-the-art LLMs including Gemma-2-9b,
  Mistral-7B, Qwen-2-72B, LLaMA-8B, Yi-Large, and GPT-4-o.
---

# A Comprehensive Dataset for Human vs. AI Generated Text Detection

## Quick Facts
- arXiv ID: 2510.22874
- Source URL: https://arxiv.org/abs/2510.22874
- Reference count: 40
- Dataset contains over 58,000 samples combining authentic New York Times articles with synthetic versions generated by multiple state-of-the-art LLMs

## Executive Summary
This paper introduces a comprehensive dataset for detecting AI-generated text, containing over 58,000 samples pairing authentic New York Times articles with synthetic versions generated by six different LLMs including Gemma-2-9b, Mistral-7B, Qwen-2-72B, LLaMA-8B, Yi-Large, and GPT-4-o. The dataset supports two key tasks: distinguishing human-written from AI-generated text (achieving 58.35% accuracy) and attributing AI texts to their generating models (achieving 8.92% accuracy). The authors establish baseline results using a rewriting-based approach that leverages the observation that LLMs make fewer edits when rewriting their own outputs compared to human text, providing a valuable resource for advancing research on AI content detection and attribution methods.

## Method Summary
The dataset combines authentic New York Times articles with AI-generated text created using the same abstracts as prompts for six different LLMs. The detection approach uses a rewriting-based method where a rewriter model (e.g., GPT-3.5-Turbo) processes input text and calculates the Levenshtein edit distance between the original and rewritten versions. For binary classification, human text is identified when all edit distances exceed a threshold (median of maximum distances). For model attribution, the system predicts the specific LLM by finding which model produces the minimum edit distance when rewriting the text. This zero-shot, unsupervised approach trades accuracy for generalizability and lower data labeling costs.

## Key Results
- Task A (Human vs. AI classification): 58.35% accuracy using rewrite resistance detection
- Task B (Model attribution): 8.92% accuracy in identifying which of 6 LLMs generated the text
- Dataset contains 58,502 rows with prompts, human stories, and generated text from 6 different models
- Rewriting-based baseline achieves only marginally better than random guessing for binary classification

## Why This Works (Mechanism)

### Mechanism 1: Reconstruction Error via Stylistic Rewriting
The system calculates Levenshtein edit distance between original text and a rewrite by an LLM. Low edit distance implies the rewriter found the original text structurally acceptable, characteristic of AI generative patterns. Core assumption: LLMs share implicit structural preferences and will modify human prose (high variance/creativity) more aggressively than standardized AI prose. Evidence shows human-written inputs typically result in larger modifications while AI-generated inputs lead to much smaller changes. This fails if human text is formulaic or AI text is prompted to be creative.

### Mechanism 2: Homology-Based Model Attribution
The system creates a distance profile by having multiple candidate LLMs attempt a rewrite of the same text. The source model is predicted to be the one producing the minimum edit distance. Core assumption: specific model architectures possess distinct "stylistic fingerprints" that are self-reinforcing during regeneration. Evidence shows this hypothesis is weak or the signal is noisy, with only 8.92% accuracy achieved. This fails when the signal is indistinguishable from noise, as the specific models may have converged in style.

### Mechanism 3: Threshold-Based Human Filtering
Human authorship is detected by the absence of low edit distances across all AI models. The system aggregates maximum edit distances and classifies as Human if all distances exceed a predefined threshold. Core assumption: Human writing exhibits high perplexity or novelty that violates the compression/rewriting heuristics of standard LLMs uniformly. Evidence shows this achieves 58.35% accuracy on binary classification. This fails if the rewriter model is sufficiently advanced to mimic human-level creativity in rephrasing.

## Foundational Learning

- **Concept: Levenshtein Distance (Edit Distance)**
  - Why needed: This is the core metric used to quantify "change" during the rewriting process. You cannot interpret the baseline results without understanding that this metric counts the minimum number of single-character edits to change one string into another.
  - Quick check: If an AI output changes "I am going to the store" to "I'm heading to the shop," is the Levenshtein distance high or low relative to the string length?

- **Concept: Perplexity vs. "Acceptability"**
  - Why needed: The paper implies AI text is "more acceptable" to other AIs. This relates to how LLMs calculate loss. Text generated by an LLM sits in a low-loss region of the latent space, making it statistically easier for another LLM to reconstruct or rewrite with minor changes.
  - Quick check: Why would a prompt "Rewrite this concisely" produce fewer changes in an AI-generated essay compared to a human essay?

- **Concept: Mode Collapse / Stylistic Convergence**
  - Why needed: The 8.92% attribution accuracy suggests the models might sound too similar. Understanding mode collapse helps explain why distinguishing "Gemma" from "Qwen" is harder than distinguishing "Human" from "Robot."
  - Quick check: If Model A and Model B were both trained on similar internet corpora, how might that impact the baseline's ability to distinguish between their outputs?

## Architecture Onboarding

- **Component map**: NYT Abstracts (Prompts) + URLs (Human Stories) -> 6 Generators (LLMs) -> Rewriter Model + Levenshtein Calculator -> Threshold comparator (Task A) / Argmin selector (Task B)
- **Critical path**: Data Hygiene (extracting full text from URLs) -> Prompt Engineering (ensuring comparable lengths/styles) -> Threshold Tuning (using median of maximum edit distance)
- **Design tradeoffs**: Rewriting vs. Classification (zero-shot logic trades accuracy for generalizability), Abstract-as-Prompt (trades narrative depth for controlled input constraint)
- **Failure signatures**: Task B Failure (<10% accuracy indicates the "fingerprint" hypothesis appears invalid), Task A Weak Signal (~58% accuracy suggests faint signal or too capable rewriter)
- **First 3 experiments**: 1) Threshold Sensitivity Analysis (plot accuracy against varying thresholds), 2) Rewriter Ablation (swap rewriter model to test stylistic homophily), 3) Supervised Baseline (train BERT-classifier to quantify performance gap)

## Open Questions the Paper Calls Out

### Open Question 1
Can neural classification methods significantly outperform the rewriting-based baseline in model attribution? The authors note their baseline achieves only 8.92% accuracy on the attribution task and call for more sophisticated methods. Evidence: The paper only establishes a baseline using edit distance via rewriting; it does not evaluate supervised neural networks on the attribution task. Resolution: Benchmark results from training transformer-based classifiers on the dataset to determine if they yield higher accuracy than the 8.92% baseline.

### Open Question 2
How does detection performance degrade when text is generated using agentic systems or fine-tuning rather than standard prompting? The "Future Work" section states that LLM-generated news can be made more challenging and human-like via "in-context learning, finetuning or agentic systems." Evidence: The current dataset utilizes standard generation methods and does not cover these advanced, harder-to-detect generation techniques. Resolution: Extending the dataset with samples generated via agentic workflows or fine-tuned models and evaluating the drop in detection accuracy.

### Open Question 3
Can detection models trained on this dataset generalize to unseen language model architectures? Section 3.5 lists "Cross-Model Generalization" as a key research direction. Evidence: While the dataset includes multiple models, the paper does not present experiments testing the transferability of a detector trained on this specific set of LLMs to a completely new, unseen model. Resolution: A zero-shot evaluation where a detector trained on this dataset is tested against text generated by a newly released LLM not present in the training data.

## Limitations
- The model attribution hypothesis (Task B) is not well-supported, achieving only 8.92% accuracy which is below random chance for 6-class classification
- The binary classification accuracy (58.35%) is only marginally better than random guessing (50%), suggesting the rewrite resistance signal is weak
- The exact rewriter model specification is ambiguous, with the paper not clearly stating whether a single rewriter or ensemble approach was used

## Confidence

- **High Confidence**: Dataset compilation methodology and binary classification task setup are well-specified and reproducible. Use of Levenshtein distance as a rewrite resistance metric is clearly documented.
- **Medium Confidence**: Rewriting-based detection mechanism is plausible and supported by observed patterns, but modest accuracy suggests weak signal. Exact rewriter model and prompt are not definitively specified.
- **Low Confidence**: Model attribution hypothesis is not well-supported by results. 8.92% accuracy indicates the proposed mechanism fails in this context, likely due to stylistic convergence among models or insufficient discriminative signal.

## Next Checks

1. **Rewriter Model Ablation**: Reproduce the baseline using different rewriter models (e.g., GPT-3.5-Turbo, LLaMA-3-8B) to determine if weak signal in Task B is specific to rewriter choice or fundamental limitation of rewrite resistance approach.

2. **Prompt Sensitivity Analysis**: Systematically vary the rewriting prompt (e.g., "Rewrite concisely," "Improve clarity," "Fix grammar") to test if prompt engineering can strengthen the discriminative signal for either task, particularly for model attribution.

3. **Supervised vs. Zero-Shot Comparison**: Train a simple supervised classifier (e.g., fine-tuned BERT model) on the same dataset and compare its performance to the zero-shot rewriting baseline to quantify the performance gap and determine if lack of signal is inherent to rewrite resistance concept or limitation of unsupervised approach.