---
ver: rpa2
title: Assessing Semantic Annotation Activities with Formal Concept Analysis
arxiv_id: '2501.11123'
source_url: https://arxiv.org/abs/2501.11123
tags:
- annotation
- ontology
- concept
- concepts
- ontologies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes using Formal Concept Analysis (FCA) to assess
  how domain experts' ontologies are used during semantic annotation of digital resources.
  Annotators tag resources using concepts from provided taxonomies, and FCA generates
  concept lattices from these annotations.
---

# Assessing Semantic Annotation Activities with Formal Concept Analysis

## Quick Facts
- arXiv ID: 2501.11123
- Source URL: https://arxiv.org/abs/2501.11123
- Reference count: 0
- Primary result: FCA-based lattice visualization enables domain experts to guide annotators and refine ontologies through pattern detection in semantic annotations.

## Executive Summary
This paper proposes using Formal Concept Analysis (FCA) to assess semantic annotation activities by analyzing how annotators use domain expert-provided ontologies. The approach transforms annotation data into concept lattices that visually reveal patterns such as frequently combined concepts, unused concepts, and unexpected combinations. Domain experts can then use these insights to guide annotators and refine the underlying ontologies. The method was implemented in @note, a tool for annotating literary texts, and evaluated on annotating Borges' "The Library of Babel."

## Method Summary
The method builds a formal context where annotated resources are objects and ontology concepts (including inferred super-concepts via taxonomy) are attributes. FCA is applied to generate a concept lattice, which is visualized as a Hasse diagram. Domain experts inspect this lattice to identify patterns like unused concepts (appearing at the bottom of the lattice) and frequently co-occurring concepts (merged nodes), using these insights to refine the ontology or update annotation guidelines. The approach deliberately restricts ontologies to is-a hierarchies to ensure domain expert accessibility.

## Key Results
- FCA-generated concept lattices effectively reveal patterns in concept usage that domain experts can interpret
- Domain experts successfully used lattice inspection to identify and refine unused or poorly organized ontology concepts
- The method proved feasible for domain experts with limited technical background while maintaining effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Formal Concept Analysis transforms sparse annotation data into a navigable lattice that reveals hidden co-occurrence patterns.
- Mechanism: FCA constructs a formal context where annotated resources are objects and ontology concepts are attributes. The resulting concept lattice groups resources by shared attribute sets, surfacing which concepts are used together, in isolation, or never used.
- Core assumption: Annotators' concept selections reflect meaningful semantic judgments rather than random choices.
- Evidence anchors:
  - [abstract] "FCA generates concept lattices from these annotations. These lattices visually reveal patterns of concept usage, such as frequently combined concepts or unused concepts."
  - [section 2.4] "The upper part of the lattice is formed by the attribute concepts associated with each ontology concept... The bottom part of the lattice is associated with the formal concepts that are a combination of the upper attribute concepts."
- Break condition: If annotators apply concepts randomly or without semantic intent, the lattice reveals noise rather than structure.

### Mechanism 2
- Claim: Taxonomical inheritance propagates annotations upward, preserving ontology structure within the lattice.
- Mechanism: When an annotator tags a resource with concept OC, the formal context also records all super-concepts OC' where OC ⊆ OC'. This ensures the lattice reflects both specific annotations and the hierarchical relationships from the source ontology.
- Core assumption: The ontology uses only is-a (taxonomic) relationships; other relations are intentionally excluded for domain expert accessibility.
- Evidence anchors:
  - [section 2.3] "We constrain the shape of the ontologies to single concepts arranged in a multiple inheritance hierarchy."
  - [section 2.4] "If an annotation is tagged with an ontology concept OC, it will also be associated with any other ontology concept OC' such that OC ⊆ OC'."
- Break condition: If the ontology includes non-taxonomic relations (e.g., part-of, causality), the inheritance mechanism produces misleading attribute sets.

### Mechanism 3
- Claim: Visual lattice inspection by domain experts triggers iterative ontology refinement without requiring technical expertise.
- Mechanism: Hasse diagrams display formal concepts with quantitative annotations (extent size, percentage). Domain experts identify unused concepts (bottom of lattice), concepts that always co-occur (merged nodes), and unexpected combinations, then refine the ontology or update annotation guidelines.
- Core assumption: Domain experts can interpret Hasse diagrams and translate visual patterns into corrective actions.
- Evidence anchors:
  - [section 4.3] "After completing the assessment, the domain experts performed the following corrective actions... They also refined the annotation ontology... erasing useless concepts, rethinking concept sub-hierarchies, choosing better names for concepts."
  - [section 5] "Does the method allow domain experts to enhance their ontologies after the annotation activity? The response to this question was unanimously affirmative."
- Break condition: If lattice complexity grows beyond ~50 concepts, visual inspection becomes impractical without additional tooling.

## Foundational Learning

- Concept: **Formal Context and Formal Concept**
  - Why needed here: The entire FCA approach hinges on understanding objects × attributes matrices and how formal concepts (extent, intent pairs) are derived.
  - Quick check question: Given objects {A, B} with attributes {x, y} and {x, z} respectively, what is the extent of the concept with intent {x}?

- Concept: **Taxonomical (Is-A) Hierarchy**
  - Why needed here: The method deliberately restricts ontologies to is-a hierarchies; understanding inheritance propagation is essential for interpreting lattices correctly.
  - Quick check question: If "Mammal ⊆ Animal" and an annotator tags a resource with "Mammal," which concepts appear in the formal context for that resource?

- Concept: **Hasse Diagram Reading**
  - Why needed here: Domain experts and engineers must navigate concept lattices visually; misreading node relationships leads to incorrect ontology refinements.
  - Quick check question: In a Hasse diagram, if node C1 has a line to C2 above it, what does that mean about their extent/intent relationship?

## Architecture Onboarding

- Component map:
  - Ontology Editor -> Annotation Interface -> FCA Engine -> Lattice Viewer

- Critical path:
  1. Domain expert defines taxonomy via Ontology Editor
  2. Annotators tag resources via Annotation Interface
  3. FCA Engine builds formal context (resources as objects, ontology concepts + ancestors as attributes)
  4. Lattice Viewer renders Hasse diagram; expert inspects and decides on corrective actions
  5. Loop back to step 1 with refined ontology or updated guidelines

- Design tradeoffs:
  - **Simplicity vs. expressiveness**: Restricting ontologies to is-a hierarchies lowers barrier for domain experts but limits semantic richness (no property relations)
  - **Automation vs. human judgment**: FCA automates pattern detection; corrective actions remain manual to preserve expert oversight
  - **Scalability vs. interpretability**: Larger ontologies produce denser lattices; visual inspection degrades beyond ~30-50 concepts without filtering or abstraction

- Failure signatures:
  - **Empty or single-node lattice**: No annotations or all resources share identical concept sets
  - **Bottom concept contains many ontology concepts**: Concepts marked unused; may indicate unclear guidelines or ontology mismatch
  - **Experts unable to interpret lattice**: Diagram complexity too high; needs subgroup projection or filtering

- First 3 experiments:
  1. **Baseline annotation test**: Provide a simple 10-concept taxonomy; have 3 annotators tag 20 resources; verify lattice matches expected co-occurrence patterns
  2. **Inheritance validation**: Confirm that tagging a leaf concept correctly propagates to all ancestor concepts in the formal context
  3. **Expert feedback loop**: Run the full workflow with a domain expert (even on synthetic data); observe whether they can propose at least one ontology refinement from lattice inspection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the ontological assumption be expanded beyond simple taxonomical hierarchies (is-a relationships) to include more complex relationships without compromising usability for non-technical domain experts?
- Basis in paper: [explicit] Section 7 identifies the confinement to taxonomical arrangements as a weakness and explicitly states, "Whether this assumption can be relaxed without compromising usability may be the object of future inquiries."
- Why unresolved: The current method intentionally restricts ontologies to simple hierarchies to ensure domain experts can author them without advanced knowledge representation skills.
- What evidence would resolve it: A user study demonstrating that domain experts can successfully use an extended tool to manage ontologies with non-taxonomic relationships.

### Open Question 2
- Question: Do sophisticated visualization techniques for concept lattices scale effectively for larger ontologies while remaining intelligible to domain experts?
- Basis in paper: [explicit] Section 7 highlights the visual representation of the lattice as a potential weakness, asking whether sophisticated visualization techniques "will be well received by domain experts" for larger structures.
- Why unresolved: The case study utilized a relatively small ontology; it is uncertain if the Hasse diagram approach remains feasible as the number of formal concepts grows significantly.
- What evidence would resolve it: Eye-tracking or usability studies involving domain experts analyzing lattices generated from large-scale, dense annotation datasets.

### Open Question 3
- Question: Can semi-automatic assessment support, such as predefined rules, effectively detect good and bad uses of the ontology within the concept lattice?
- Basis in paper: [explicit] Section 5 notes expert suggestions for "semi-automatic assessment support (e.g., by adding rules)," and Section 7 lists "support for automating some assessment aspects" as an ongoing/future effort.
- Why unresolved: The current approach relies entirely on the manual visual inspection of the lattice by the domain expert, which is labor-intensive.
- What evidence would resolve it: The development and validation of an automated rule-based system that flags anomalies with high precision and recall compared to human expert assessment.

### Open Question 4
- Question: Does the implementation of dynamic or continuous assessment improve annotation quality compared to post-hoc analysis?
- Basis in paper: [explicit] Section 5 reports that experts suggested "dynamic/continuous assessment, so that intermediate assessment results can impact the annotation activity," which is listed as a future work direction in Section 7.
- Why unresolved: The current workflow assesses the annotation activity only after it is complete, preventing annotators from receiving corrective feedback during the process.
- What evidence would resolve it: A comparative experiment measuring the accuracy and completeness of annotations performed with real-time feedback versus those performed with only post-hoc review.

## Limitations

- Scalability concerns: Visual lattice inspection becomes impractical with large ontologies or dense annotations, with no concrete thresholds or tooling recommendations provided
- Evaluation depth: Assessment was informal and qualitative rather than quantitative, with no comparison to alternative annotation assessment methods
- Implementation opacity: The @note tool and FCA engine specifics are not publicly available, limiting reproducibility

## Confidence

- **High confidence**: FCA's ability to reveal co-occurrence patterns and unused concepts (Mechanism 1) is well-supported by formal theory and demonstrated in case study
- **Medium confidence**: Effectiveness of domain expert-driven ontology refinement via lattice inspection (Mechanism 3) is plausible but relies on subjective expert judgment without quantitative validation
- **Low confidence**: Scalability of approach for large ontologies or high-volume annotations is speculative; paper acknowledges this as weakness without mitigation strategies

## Next Checks

1. **Scalability test**: Apply method to ontology with 50+ concepts and 100+ annotated resources; measure expert time to identify patterns and assess lattice readability
2. **Controlled comparison**: Compare FCA-based assessment with baseline (frequency analysis) on synthetic dataset to quantify added value
3. **Expert usability study**: Conduct formal study with domain experts of varying technical backgrounds to evaluate their ability to interpret lattices and propose refinements, measuring time and accuracy