---
ver: rpa2
title: Benchmarking Document Parsers on Mathematical Formula Extraction from PDFs
arxiv_id: '2512.09874'
source_url: https://arxiv.org/abs/2512.09874
tags:
- formulas
- formula
- document
- evaluation
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a benchmark for evaluating PDF parsers on
  mathematical formula extraction, addressing the lack of semantically-aware evaluation
  metrics in existing benchmarks. The authors propose a novel framework using synthetic
  PDFs with precise LaTeX ground truth, combined with an LLM-based matching pipeline
  and LLM-as-a-judge for semantic formula assessment.
---

# Benchmarking Document Parsers on Mathematical Formula Extraction from PDFs

## Quick Facts
- arXiv ID: 2512.09874
- Source URL: https://arxiv.org/abs/2512.09874
- Reference count: 40
- Introduces novel benchmark for PDF parsers using synthetic PDFs with precise LaTeX ground truth and LLM-based evaluation

## Executive Summary
This paper addresses the critical gap in evaluating PDF parsers for mathematical formula extraction by introducing a comprehensive benchmark framework. Traditional evaluation methods lack semantic awareness when assessing formula extraction quality, leading to unreliable performance assessments. The authors develop a novel approach using synthetic PDF generation with precise LaTeX ground truth, combined with an LLM-based matching pipeline and semantic evaluation through LLM-as-a-judge methodology. Through extensive human validation involving 30 evaluators rating 750 formula pairs, the study establishes that LLM-based evaluation achieves strong correlation with human judgment (Pearson r=0.78), significantly outperforming traditional metrics. The benchmark evaluates over 20 contemporary PDF parsers across 100 synthetic documents containing more than 2,000 formulas, revealing substantial performance variations that provide actionable insights for practitioners.

## Method Summary
The methodology centers on synthetic PDF generation with precise LaTeX ground truth to enable semantically-aware evaluation of mathematical formula extraction. The authors employ an LLM-based matching pipeline to align extracted formulas with ground truth, followed by LLM-as-a-judge assessment for semantic evaluation. Human validation was conducted on 250 formula pairs, with 30 evaluators providing 750 ratings total, establishing ground truth correlation benchmarks. The framework evaluates 20+ contemporary PDF parsers across 100 synthetic documents containing 2,000+ formulas, enabling systematic performance comparison. This approach addresses the fundamental limitation of existing benchmarks that rely on text-based metrics inappropriate for mathematical notation, where semantic equivalence often transcends surface-level textual similarity.

## Key Results
- LLM-based evaluation achieves Pearson correlation of 0.78 with human judgment on formula semantic quality
- Traditional metrics perform poorly: CDM correlation r=0.34, text similarity correlation r≈0
- Benchmark evaluates 20+ PDF parsers across 100 synthetic documents with 2,000+ formulas
- Significant performance disparities revealed among evaluated parsers, providing actionable guidance for selection

## Why This Works (Mechanism)
The benchmark succeeds by addressing the fundamental mismatch between traditional text-based evaluation metrics and the semantic nature of mathematical formulas. Mathematical expressions often maintain semantic equivalence despite textual differences (e.g., equivalent expressions written in different notation styles), making surface-level similarity metrics inadequate. The synthetic PDF generation approach provides precise LaTeX ground truth, enabling exact semantic comparison impossible with real-world documents where ground truth is ambiguous. The LLM-based matching pipeline leverages advanced pattern recognition to align extracted formulas with ground truth despite formatting variations, while the LLM-as-a-judge methodology captures nuanced semantic understanding that traditional metrics miss. Human validation provides the essential calibration that grounds the automated evaluation in actual human perception of formula quality and equivalence.

## Foundational Learning
- **Synthetic PDF generation**: Creates controlled test documents with precise ground truth; needed because real documents lack exact formula ground truth; quick check: verify LaTeX precision matches PDF rendering
- **LLM-based matching pipeline**: Aligns extracted formulas with ground truth using semantic understanding; needed because simple string matching fails on mathematical notation variations; quick check: test on equivalent formulas with different syntax
- **LLM-as-a-judge methodology**: Evaluates semantic quality of extracted formulas; needed because traditional metrics cannot capture mathematical equivalence; quick check: compare LLM scores with human ratings on edge cases
- **Human validation methodology**: Establishes ground truth correlation for automated metrics; needed because automated systems require human calibration; quick check: calculate inter-rater reliability across evaluator pairs
- **Semantic formula evaluation**: Assesses mathematical meaning rather than textual similarity; needed because mathematical equivalence transcends surface representation; quick check: test on mathematically equivalent but textually different expressions
- **PDF parser benchmarking framework**: Systematizes evaluation across multiple tools; needed because inconsistent evaluation prevents meaningful comparisons; quick check: verify all parsers tested under identical conditions

## Architecture Onboarding

**Component Map:**
Synthetic PDF Generator -> PDF Parsers (20+) -> Formula Extractor -> LLM Matching Pipeline -> LLM-as-a-judge -> Human Validation -> Correlation Analysis

**Critical Path:**
PDF Generation → Formula Extraction → LLM Matching → Semantic Evaluation → Human Validation → Performance Benchmarking

**Design Tradeoffs:**
The synthetic approach sacrifices real-world document complexity for precise ground truth control, enabling semantically-aware evaluation impossible with real documents. This creates a controlled environment where exact LaTeX ground truth is available, but may not fully capture the noise and variability of actual scientific PDFs. The LLM-based evaluation trades computational cost and potential model bias for semantic understanding that traditional metrics cannot achieve. Human validation provides crucial calibration but introduces scalability limitations compared to fully automated approaches.

**Failure Signatures:**
Poor LLM matching performance indicates parser extraction quality issues or LLM prompt engineering problems. Low correlation with human validation suggests either inadequate human study design or LLM model limitations in understanding mathematical semantics. Parser performance variance across documents may indicate sensitivity to specific formula types or document structures. Traditional metric failures on semantically equivalent formulas reveal their inadequacy for mathematical content evaluation.

**3 First Experiments:**
1. Generate synthetic PDFs with known LaTeX ground truth and verify exact correspondence between PDF rendering and source LaTeX
2. Test LLM matching pipeline on pairs of mathematically equivalent but textually different formulas to validate semantic recognition
3. Conduct pilot human validation on 10 formula pairs with 5 evaluators to establish baseline inter-rater reliability

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic PDF generation may not fully capture real-world document complexity and variability
- Human validation sample size (30 evaluators, 750 ratings) may be insufficient for definitive correlation establishment
- LLM-as-a-judge methodology introduces potential bias through model choice and prompt engineering
- Benchmark focuses exclusively on formula extraction, potentially overlooking broader document understanding capabilities

## Confidence

**High Confidence:**
- LLM-based evaluation correlation with human judgment (r=0.78) is robust and well-validated
- Superiority of LLM-based metrics over traditional methods is well-demonstrated

**Medium Confidence:**
- Performance disparities among 20+ PDF parsers are reliable within synthetic context but may not fully translate to real documents
- Correlation findings and semantic evaluation reliability could benefit from larger-scale validation

## Next Checks
1. Apply benchmark methodology to diverse corpus of actual scientific PDFs to assess real-world performance translation
2. Conduct larger-scale human validation study (minimum 100 evaluators) to strengthen correlation findings and explore inter-rater reliability
3. Implement same evaluation pipeline using multiple LLM models (GPT-4, Claude, Gemini) to assess consistency and model-specific biases