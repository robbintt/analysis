---
ver: rpa2
title: General Post-Processing Framework for Fairness Adjustment of Machine Learning
  Models
arxiv_id: '2504.16238'
source_url: https://arxiv.org/abs/2504.16238
tags:
- fairness
- adjuster
- adversarial
- loss
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a post-processing framework for fairness
  adjustment of machine learning models that adapts in-processing techniques to be
  applied after model training. The approach separates accuracy optimization from
  fairness adjustment, treating the baseline model as a black box and applying an
  "adjuster" model to modify predictions based on fairness constraints.
---

# General Post-Processing Framework for Fairness Adjustment of Machine Learning Models

## Quick Facts
- arXiv ID: 2504.16238
- Source URL: https://arxiv.org/abs/2504.16238
- Authors: Léandre Eberhard; Nirek Sharma; Filipp Shelobolin; Aalok Ganesh Shanbhag
- Reference count: 7
- This paper introduces a post-processing framework for fairness adjustment that adapts in-processing techniques to be applied after model training, achieving comparable fairness-accuracy tradeoffs to adversarial debiasing.

## Executive Summary
This paper introduces a post-processing framework for fairness adjustment of machine learning models that adapts in-processing techniques to be applied after model training. The approach separates accuracy optimization from fairness adjustment, treating the baseline model as a black box and applying an "adjuster" model to modify predictions based on fairness constraints. The method is demonstrated to achieve comparable fairness-accuracy tradeoffs to adversarial debiasing on real-world datasets including Adult, German credit, and COMPAS, while offering advantages such as decoupling fairness tuning from model implementation, flexibility in training data requirements, compatibility with complex or proprietary models, and enhanced interpretability.

## Method Summary
The framework treats a trained baseline model as a black-box and applies a secondary "adjuster" model that modifies predictions based on fairness constraints. The adjuster is trained using the baseline's predictions (not true labels) as pseudo-labels during adjustment training, optimizing a loss function that combines prediction accuracy and fairness constraints. The method is demonstrated using XGBoost classifiers with adversarial debiasing as the comparison baseline, evaluated on Adult, German credit, and COMPAS datasets for binary classification with Equalized Odds fairness constraints.

## Key Results
- Achieves comparable fairness-accuracy tradeoffs to adversarial debiasing on Adult, German credit, and COMPAS datasets
- Demonstrates near-identical performance to in-processing methods across multiple fairness metrics
- Provides theoretical analysis showing bounded accuracy loss when approximating adversarial debiasing solutions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating accuracy optimization from fairness adjustment can produce comparable fairness-accuracy tradeoffs to joint optimization methods.
- Mechanism: The framework treats a trained baseline model as a black-box and applies a secondary "adjuster" model that modifies predictions based on fairness constraints, using the baseline's predictions (not true labels) as pseudo-labels during adjustment training.
- Core assumption: The residuals of the baseline model and the fairness adjustment are approximately orthogonal, meaning fairness corrections can be made with minimal accuracy degradation.
- Evidence anchors:
  - [abstract] "adapts in-processing techniques for use as a post-processing step... decoupling fairness adjustments from the model training process"
  - [Section 3] "Notice here that we do not use the true label Y in the loss function in the adjustment step!"
  - [Section 4.1] "In our results, we observe the third case where the residuals and difference and adjustments are near-orthogonal"
- Break condition: If baseline model predictions are highly inaccurate (large residuals correlated with fairness-critical subgroups), the accuracy-fairness tradeoff may degrade significantly.

### Mechanism 2
- Claim: The adjustment approach can theoretically reproduce similar solutions to jointly-trained adversarial debiasing models with bounded accuracy loss.
- Mechanism: By treating the adversarial debiasing solution as a decomposition of baseline predictions plus an adjustment, mathematical analysis shows the accuracy difference is bounded by the dot product of baseline residuals and the difference between adjuster and adversarial debiasing corrections.
- Core assumption: Assumption: The adjuster model has sufficient complexity to represent the necessary corrections, and both models converge to solutions within their respective model classes.
- Evidence anchors:
  - [Section 4.1, Proposition 4.2] "∆MSE = L(ŷ + g*(X)) - L(h*(X)) ≤ Σ 2(ŷi - yi)(gi* - (hi* - ŷi))"
  - [Section 4.2, Proposition 4.5] "BCE(f + g, Y) ≤ BCE(h, Y) + (σ(f) - Y)⊤[g - (h - f)]"
  - [Section 4.1, Proposition 4.3] Shows exact equivalence for linear regression under convexity assumptions.
- Break condition: If the adjuster model class is significantly less expressive than needed, or if convergence assumptions fail, bounds may not hold.

### Mechanism 3
- Claim: Post-hoc fairness adjustment provides practical advantages including black-box compatibility, separate training data for fairness tuning, and interpretability of individual adjustments.
- Mechanism: By requiring only baseline predictions (not model internals or training labels), the adjuster can be trained on different datasets, applied to proprietary models, and produces interpretable adjustment values for each prediction.
- Core assumption: Assumption: Access to baseline predictions and protected attribute information at adjustment time; sufficient data with protected attributes for adjuster training.
- Evidence anchors:
  - [Section 6] "eliminates the need to modify the loss function of the underlying model"
  - [Section 6] "training labels for the adjuster can be obtained from a different dataset than the baseline model"

## Foundational Learning

### Post-processing fairness adjustment
- Why needed: Enables fairness tuning without retraining baseline models or accessing training data
- Quick check: Framework works with any black-box model providing predictions

### Adversarial debiasing comparison
- Why needed: Provides theoretical foundation and empirical baseline for comparison
- Quick check: Adjuster should reproduce similar solutions with bounded accuracy loss

### Orthogonal residual assumption
- Why needed: Justifies minimal accuracy degradation during fairness adjustment
- Quick check: Correlation between baseline residuals and fairness adjustments should be near-zero

## Architecture Onboarding

### Component map
Baseline Model -> Prediction Generation -> Adjuster Model -> Fairness-Adjusted Predictions

### Critical path
1. Train baseline model on full dataset
2. Generate baseline predictions on fairness adjustment dataset
3. Train adjuster model using baseline predictions and protected attributes
4. Apply adjuster to new predictions

### Design tradeoffs
- Black-box compatibility vs. potential accuracy loss
- Separate training data requirements vs. data availability
- Interpretability of adjustments vs. model complexity

### Failure signatures
- High correlation between baseline residuals and fairness adjustments indicates orthogonality assumption violation
- Accuracy degradation exceeding theoretical bounds suggests model class mismatch
- Inability to access baseline predictions or protected attributes at adjustment time

### 3 first experiments
1. Test framework with deliberately biased baseline models to assess orthogonality assumption limits
2. Evaluate adjuster performance with restricted model classes (e.g., linear adjusters)
3. Compare fairness adjustment on models with varying accuracy levels

## Open Questions the Paper Calls Out
None

## Limitations
- Core orthogonality assumption is validated through observation rather than formal proof
- Theoretical bounds rely on assumptions including convexity, model convergence, and bounded complexity
- Framework assumes access to baseline predictions and protected attribute information at adjustment time

## Confidence

### Major Uncertainties and Limitations

**Confidence: Low-Medium** for the orthogonality assumption underlying the adjuster framework. While the paper shows theoretical bounds on accuracy loss and demonstrates empirical performance comparable to in-processing methods, the core assumption that residuals and fairness adjustments are orthogonal is validated primarily through observation rather than formal proof.

**Confidence: Medium** for the theoretical bounds. The mathematical analysis provides interesting insights but relies on several assumptions including convexity (for linear regression results), model convergence, and bounded complexity.

**Confidence: High** for the practical advantages claim. The benefits of black-box compatibility, separate training data requirements, and interpretability are well-established and clearly demonstrated through the experimental setup.

## Next Checks

1. **Formal Orthogonality Analysis**: Conduct systematic empirical analysis across diverse model classes (not just XGBoost) to quantify the actual correlation between baseline residuals and fairness adjustments. Test with deliberately biased baselines to understand when orthogonality breaks down.

2. **Robustness to Model Complexity**: Evaluate the framework's performance when the adjuster model class is significantly less expressive than the baseline (e.g., using linear adjusters with complex neural network baselines) to validate the bounded accuracy loss claims.

3. **Cross-Dataset Generalization**: Test the framework on datasets with different characteristics (multiclass classification, regression tasks, different protected attribute distributions) to assess the generalizability of both the empirical results and theoretical bounds beyond the binary classification case.