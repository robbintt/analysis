---
ver: rpa2
title: 'CoBEVMoE: Heterogeneity-aware Feature Fusion with Dynamic Mixture-of-Experts
  for Collaborative Perception'
arxiv_id: '2509.17107'
source_url: https://arxiv.org/abs/2509.17107
tags:
- perception
- fusion
- expert
- features
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of fusing heterogeneous sensor
  data from multiple agents in collaborative perception systems, where agents have
  different viewpoints and spatial positions. The proposed method, CoBEVMoE, introduces
  a Dynamic Mixture-of-Experts (DMoE) module that generates agent-specific expert
  kernels conditioned on local features, allowing the model to explicitly capture
  both feature similarity and heterogeneity across agents.
---

# CoBEVMoE: Heterogeneity-aware Feature Fusion with Dynamic Mixture-of-Experts for Collaborative Perception

## Quick Facts
- arXiv ID: 2509.17107
- Source URL: https://arxiv.org/abs/2509.17107
- Reference count: 40
- Primary result: Improves IoU for camera-based BEV segmentation by +1.5% on OPV2V and AP@50 for LiDAR-based 3D object detection by +3.0% on DAIR-V2X-C

## Executive Summary
This paper addresses the challenge of fusing heterogeneous sensor data from multiple agents in collaborative perception systems, where agents have different viewpoints and spatial positions. The proposed method, CoBEVMoE, introduces a Dynamic Mixture-of-Experts (DMoE) module that generates agent-specific expert kernels conditioned on local features, allowing the model to explicitly capture both feature similarity and heterogeneity across agents. To further enhance inter-expert diversity and discriminability, a Dynamic Expert Metric Loss (DEML) is introduced. Experiments on the OPV2V and DAIR-V2X-C datasets demonstrate that CoBEVMoE achieves state-of-the-art performance, validating the effectiveness of the expert-based heterogeneous feature modeling approach in multi-agent collaborative perception.

## Method Summary
CoBEVMoE introduces a Dynamic Mixture-of-Experts (DMoE) module that generates agent-specific expert kernels conditioned on local BEV features. Each agent's features pass through global average pooling and an MLP to produce an embedding, which is decoded via a deconvolution block to generate a unique 3x3 convolutional kernel for that agent. This kernel is applied to a preliminary fused feature map produced by a shared self-attention layer. A gating mechanism based on the preliminary fused feature adaptively weighs each expert's contribution. To enhance inter-expert diversity, a Dynamic Expert Metric Loss (DEML) is introduced, which uses triplet loss to pull each expert toward the fused feature while pushing it away from the most similar other expert. The model is trained end-to-end with a combined task loss and DEML, achieving state-of-the-art results on OPV2V and DAIR-V2X-C datasets.

## Key Results
- Improves IoU for camera-based BEV segmentation by +1.5% on OPV2V dataset
- Improves AP@50 for LiDAR-based 3D object detection by +3.0% on DAIR-V2X-C dataset
- DEML with optimal λ=0.4 further enhances performance by encouraging expert diversity
- Ablation studies confirm both DMoE and DEML contribute to performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamically generating expert kernels conditioned on each agent's local BEV features allows the model to capture agent-specific perceptual cues that would be suppressed by standard attention-based fusion.
- Mechanism: Each agent's features first pass through global average pooling and an MLP to produce a compact embedding. This embedding is then decoded via a deconvolution block to generate a unique 3x3 convolutional kernel ($W_k$) for that agent. This kernel is applied to a preliminary fused feature map ($F_c$) produced by a shared self-attention layer. The result is an agent-specific expert output ($E_k$) that has been filtered through a lens unique to that agent's viewpoint.
- Core assumption: The information necessary to distinguish an agent's unique contribution from shared, redundant information is encoded within its local BEV features and can be decoded into a useful convolutional operator.
- Evidence anchors:
  - [abstract] "In DMoE, each expert is dynamically generated based on the input features of a specific agent, enabling it to extract distinctive and reliable cues..."
  - [section III.C] "For each agent k, the BEV feature x_k is first subjected to spatial global average pooling... then decoded via a deconvolution block to generate a convolutional kernel..."
  - [corpus] Related work (e.g., MoE-Fusion, MetaBEV) applies MoE for modality fusion, but CoBEVMoE's key distinction is using agent features to *generate* the expert parameters themselves, rather than selecting from a static pool.
- Break condition: If agent features are so corrupted by noise (e.g., extreme pose errors, sensor failure) that the generated kernels are meaningless, the expert output will degrade the final fusion instead of enhancing it. The system's performance is thus tied to the reliability of local feature extraction.

### Mechanism 2
- Claim: A learned gating mechanism based on a preliminary fused feature can adaptively weigh the contributions of each expert, balancing between reinforcing shared semantics and incorporating diverse, agent-specific cues.
- Mechanism: A gating network takes the preliminary fused feature $F_c$ (from the self-attention layer) and produces a scalar weight ($\tilde{\alpha}_k$) for each expert. The final output of the DMoE module is a weighted sum of all expert outputs ($E_k$) added back to $F_c$ via a residual connection ($F_{res} = F_c + \sum \tilde{\alpha}_k \cdot E_k$). This allows the model to selectively amplify an expert's output when it provides complementary information (high gate weight) or suppress it if it's redundant (low gate weight).
- Core assumption: The preliminary fused feature ($F_c$) contains sufficient context to judge the utility and reliability of each agent's specific expert output.
- Evidence anchors:
  - [abstract] "All expert outputs are then adaptively fused through a gated aggregation mechanism that accounts for reliability and complementarity."
  - [section III.C] "we apply global average pooling along the channel dimension of the input collaborative features... and then use a softmax function to generate the weights... combined with a residual connection..."
  - [corpus] Standard practice in MoE. Break condition applies universally: if the gating network fails to learn a meaningful signal, it may collapse to uniform weighting, negating the benefit of specialized experts.

### Mechanism 3
- Claim: A Dynamic Expert Metric Loss (DEML) provides explicit training supervision to encourage the expert representations to be diverse yet consistent with the overall task, preventing representational collapse.
- Mechanism: For each expert output $E_k$, a triplet is formed: Anchor = preliminary fused feature $F_c$, Positive = $E_k$, Negative = the output of the *most similar* other expert. The loss ($L_{DEML}$) pulls $E_k$ toward $F_c$ (task alignment) while pushing it away from its closest neighbor (diversity). This combats the tendency of experts to converge to similar functions during training, ensuring each learns a truly unique perspective.
- Core assumption: Forcing expert representations apart in feature space leads to a more comprehensive fusion that captures complementary information rather than just redundant features.
- Evidence anchors:
  - [abstract] "To further enhance inter-expert diversity and discriminability, a Dynamic Expert Metric Loss (DEML) is introduced."
  - [section III.D] "Based on the constructed triplets... This encourages the positive sample to remain close to the anchor and the negative sample to stay farther away... alleviating feature redundancy..."
  - [corpus] This is a novel, task-specific contribution. Corpus signals on MoE for perception do not emphasize this type of explicit metric learning for expert regularization, making it a key differentiator of the proposed method.

## Foundational Learning

- **Concept: Bird's-Eye View (BEV) Representation**
  - Why needed here: The entire collaborative perception pipeline operates in BEV space. You must understand how multi-view camera images or LiDAR points are transformed into a unified top-down spatial grid ($H \times W$) to understand how features from different agents are spatially aligned and fused.
  - Quick check question: Explain how a feature map of size $C \times H \times W$ in BEV space corresponds to the physical world and why this space is used for collaboration between agents with different poses.

- **Concept: Mixture-of-Experts (MoE) with Gating Networks**
  - Why needed here: This is the core architectural component. Understanding that an MoE system consists of several "expert" networks and a "gating" network that decides which expert to use (or how to weight them) is fundamental to grasping the DMoE module.
  - Quick check question: Describe the role of the gating network in a standard Mixture-of-Experts model. How does its output influence the final prediction?

- **Concept: Metric Learning with Triplet Loss**
  - Why needed here: The proposed DEML is a variant of triplet loss used for regularization. Understanding the anchor-positive-negative framework is required to grasp how the loss function enforces diversity among experts.
  - Quick check question: In a triplet loss formulation for expert $k$, what do the anchor, positive, and negative samples represent, and what is the optimization objective for their relative distances?

## Architecture Onboarding

- **Component map:**
  1. **Local Encoders:** Image/BEV Encoder (e.g., ResNet34 + SinBEV) on each agent.
  2. **Collaboration Module:** The core DMoE block.
      - **Input:** Aligned BEV features $X$ from N agents.
      - **Path A (Shared Semantics):** Self-Attention Fusion (SwapFusion) → preliminary fused feature $F_c$.
      - **Path B (Agent-Specific):** For each agent: AvgPool → MLP → Deconv → Kernel $W_k$.
      - **Expert Application:** Convolution with $W_k$ on $F_c$ → Expert output $E_k$.
      - **Gating & Fusion:** Gate($F_c$) → weights $\tilde{\alpha}_k$. Final output: $F_{res} = F_c + \sum \tilde{\alpha}_k \cdot E_k$.
  3. **Task Heads:** Segmentation / Detection heads on the fused features $F_{res}$.
  4. **Loss:** Task Loss ($L_{task}$) + DEML ($L_{DEML}$).

- **Critical path:** The generation of agent-specific kernels ($W_k$) from local features and their application to the shared preliminary feature ($F_c$). If this dynamic generation process fails to produce meaningful kernels, the expert outputs will be noise, and the gated fusion will fail to provide benefit.

- **Design tradeoffs:**
  - **Dynamic vs. Static Experts:** Dynamic kernels add computational overhead (MLP + Deconv per agent) but provide tailored fusion. Static experts would be faster but less adaptive to viewpoint heterogeneity.
  - **Loss Weight ($\lambda$):** A high $\lambda$ forces expert diversity but may push them away from the task-relevant features in $F_c$. A low $\lambda$ risks expert collapse. The paper finds a sweet spot around $\lambda=0.4$.

- **Failure signatures:**
  - **Representational Collapse:** Gate weights become uniform ($\approx 1/N$) and expert outputs become highly similar. The model performs no better than the baseline self-attention fusion. Indicates DEML is too weak or learning dynamics are poor.
  - **Dominant Expert:** A single gate weight dominates ($\approx 1.0$). The model effectively ignores other agents, reducing to a form of single-agent or "best-view" perception. Indicates gating network instability.
  - **Pose Sensitivity:** Performance drops sharply with simulated pose noise, as misalignment corrupts both the generation of expert kernels and the self-attention fusion.

- **First 3 experiments:**
  1. **Baseline Comparison:** Reproduce the results of CoBEVT on OPV2V. This establishes your baseline.
  2. **Module Ablation:** Add the DMoE module (without DEML, using $\lambda=0$) to the baseline. Observe the IoU gain. This tests the core contribution.
  3. **Loss Ablation:** Introduce the DEML with the optimal $\lambda=0.4$ found in the paper. Measure the additional IoU improvement over DMoE-only to validate the regularization effect.

## Open Questions the Paper Calls Out
- **Question:** How can the CoBEVMoE framework be adapted to maintain performance under strict communication bandwidth constraints?
  - **Basis in paper:** [explicit] The authors explicitly identify extending CoBEVMoE to "bandwidth-constrained settings" as a primary direction for future work.
  - **Why unresolved:** The current intermediate fusion approach transmits dense BEV features to balance performance and bandwidth, but a specific mechanism for compressing features or selecting critical agents for transmission has not been implemented or evaluated.
  - **What evidence would resolve it:** Experiments reporting performance (IoU/AP) versus transmission cost (KB/frame) on standard collaborative datasets.

- **Question:** Does incorporating temporal dependencies into the Dynamic Mixture-of-Experts module improve multi-agent prediction and planning?
  - **Basis in paper:** [explicit] The conclusion states the intent to "explore its integration with temporal modeling for multi-agent prediction and planning."
  - **Why unresolved:** The current architecture processes single frames, and the Dynamic Expert Metric Loss (DEML) is optimized for spatial heterogeneity rather than temporal consistency or motion forecasting.
  - **What evidence would resolve it:** Evaluation on sequence-based benchmarks showing improvements in tracking or motion forecasting accuracy when temporal layers are integrated.

## Limitations
- **Expert Kernel Generation:** The exact MLP hidden dimension, deconvolution architecture, and SinBEV configuration remain unspecified, creating a gap between described mechanism and implementable code.
- **DEML Hyperparameters:** The triplet margin (m) and loss balancing factor (β) are not disclosed, making it difficult to reproduce the exact regularization effect observed in the experiments.
- **Code Availability:** The absence of released code means reproducing the precise architecture, especially the interaction between SwapFusion and DMoE, relies on inference from the paper text.

## Confidence
- **High:** The core claim that dynamic, agent-conditioned expert kernels improve collaborative perception performance is supported by strong experimental results (+1.5% IoU, +3.0% AP@50). The paper provides clear architectural diagrams and ablation studies demonstrating the contribution of both DMoE and DEML.
- **Medium:** The DEML regularization mechanism is novel and well-motivated, but the lack of specific hyperparameter values and the reliance on the hardest negative sample selection introduce some uncertainty about the precise mechanism of action.
- **Medium:** The claim that the model can handle heterogeneous viewpoints is well-supported, but the experiments use simulated pose noise only for DAIR-V2X-C. Real-world testing across a wider range of viewpoint variations would strengthen this claim.

## Next Checks
1. **Ablation Study:** Implement the DMoE module without DEML (λ=0) and compare IoU/AP to the baseline (CoBEVT). This directly tests the contribution of dynamic expert kernels.
2. **DEML Impact:** Introduce DEML with varying λ values (0.1, 0.4, 0.8). Measure the impact on both expert diversity (MSE between experts) and task performance (IoU/AP). This validates the regularization effect.
3. **Robustness to Pose Noise:** Replicate the pose noise experiment on OPV2V (if applicable) or apply it to DAIR-V2X-C. Measure the degradation in performance with increasing noise levels to assess the model's robustness to viewpoint heterogeneity.