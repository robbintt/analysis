---
ver: rpa2
title: Exploring Effective Strategies for Building a Customised GPT Agent for Coding
  Classroom Dialogues
arxiv_id: '2506.07194'
source_url: https://arxiv.org/abs/2506.07194
tags:
- coding
- dialogue
- agent
- human
- classroom
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored practical strategies for developing a MyGPT
  agent to automate coding of classroom dialogue using the CDAS framework. While providing
  only a codebook led to poor accuracy, performance improved significantly with structured
  prompts, decision-tree logic, and iterative refinement based on error analysis.
---

# Exploring Effective Strategies for Building a Customised GPT Agent for Coding Classroom Dialogues

## Quick Facts
- arXiv ID: 2506.07194
- Source URL: https://arxiv.org/abs/2506.07194
- Authors: Luwei Bai; Dongkeun Han; Sara Hennessy
- Reference count: 0
- Primary result: Achieved 64.9% turn-level precision on CDAS dialogue coding using structured prompts and ~120 contextualized examples

## Executive Summary
This study explored practical strategies for developing a MyGPT agent to automate coding of classroom dialogue using the CDAS framework. While providing only a codebook led to poor accuracy, performance improved significantly with structured prompts, decision-tree logic, and iterative refinement based on error analysis. Optimal results were achieved with around 120 carefully selected examples, contextualised dialogue samples, and segmented instructions prioritising clarity and reducing cognitive load. The final agent reached 64.9% turn-level precision, approaching human inter-coder reliability. The findings suggest that effective automation requires more than examples—it demands tailored, cognitively optimised prompt design. This approach enables researchers and educators to build lightweight, context-sensitive coding assistants without large datasets or specialised infrastructure.

## Method Summary
The study used OpenAI's MyGPT agent (GPT-4) to develop a dialogue coding assistant for the CDAS framework. The method involved 13 design strategies applied iteratively: starting with a baseline codebook-only configuration, adding contextualized examples (12, 120, and 500 variants), implementing segmented instructions with decision-tree logic, and conducting error analysis-driven refinements. The agent was tested on 1,386 dialogue turns from 3 UK primary classroom lessons, with performance measured via precision, recall, and F1 scores. Human inter-coder reliability (~70-75%) served as the benchmark for acceptable performance.

## Key Results
- Performance improved from 16-67% precision (codebook-only) to 64.9% turn-level precision with structured prompts and ~120 contextualized examples
- Decision-tree logic and segmented instructions reduced cognitive load and improved classification accuracy
- Iterative error analysis targeting frequent confusions (e.g., EL vs. RE) produced significant gains
- Some codes (IC, SC, RC) remained at 0% precision despite all strategies
- Contextualized dialogue samples preserved inter-turn dependencies better than isolated single-turn examples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured, modular prompts with decision-tree logic improve classification accuracy on complex multi-code tasks by reducing cognitive load on the model.
- Mechanism: Hierarchical IF-THEN conditions decompose complex reasoning into sequential decisions, allowing the model to process one decision node at a time rather than holding multiple overlapping rules simultaneously.
- Core assumption: LLMs exhibit behavior analogous to human cognitive load constraints; when overloaded, they drop or conflate rules.
- Evidence anchors:
  - [abstract] "performance improved significantly with structured prompts, decision-tree logic, and iterative refinement based on error analysis"
  - [section 2.3-2.4] Cites Yue et al. (2023) on cognitive load in LLMs; notes that when multiple rules are added together, the agent often ignores certain rules
  - [corpus] Related work on decomposed prompting (Khot et al., 2022) supports modular task decomposition
- Break condition: If the coding scheme has few categories (≤3-4) with minimal overlap, cognitive load may not be the limiting factor; simpler prompts may suffice.

### Mechanism 2
- Claim: Carefully selected contextualized examples (≈120) outperform both minimal examples and large-scale example dumps for dialogue coding tasks.
- Mechanism: Contextualized dialogue excerpts preserve inter-turn dependencies and discourse flow cues that isolated single-turn examples lose; however, token limits cause the model to compress or ignore information beyond a threshold.
- Core assumption: The model's context window and attention mechanism selectively weight earlier and more structured input; excessive examples trigger abstraction rather than item-level learning.
- Evidence anchors:
  - [abstract] "Optimal results were achieved with around 120 carefully selected examples"
  - [section 5, Table 3] Precision improved from 12 to 120 examples but plateaued or declined at 500 for most codes
  - [corpus] Limited direct corpus evidence on optimal example counts for custom GPT agents; related work focuses on fine-tuning, not in-context configuration
- Break condition: If the target coding scheme relies primarily on single-utterance semantics (e.g., sentiment) rather than cross-turn dialogic function, contextualized examples may add noise without benefit.

### Mechanism 3
- Claim: Iterative human-in-the-loop refinement targeting frequent error patterns produces greater gains than static prompt design.
- Mechanism: Error analysis identifies systematic confusion (e.g., EL vs. RE, A vs. EL in multi-utterance turns); targeted rule clarifications and anchor examples close specific decision boundaries.
- Core assumption: The model's errors are patterned and addressable through rule clarification rather than architectural change.
- Evidence anchors:
  - [abstract] "iterative refinement based on error analysis" cited as key to performance gains
  - [section 5, Strategy 8] "Frequent error categories were addressed through repeated feedback cycles"
  - [corpus] Long et al. (2024) similarly report iterative prompt refinement for classroom dialogue coding
- Break condition: If error patterns are diffuse or if the scheme itself has low human inter-coder reliability, iterative refinement may converge on noise rather than signal.

## Foundational Learning

- Concept: Deductive coding with theory-derived categories
  - Why needed here: The agent must apply a predefined scheme (CDAS) rather than induce categories; understanding the difference prevents conflating classification with thematic analysis.
  - Quick check question: Can you explain why CDAS codes are defined by speaker intention rather than utterance content?

- Concept: Token limits and context window compression
  - Why needed here: Strategy 2 (token limit awareness) and the 500-example performance plateau make sense only if you understand that models compress long inputs.
  - Quick check question: What behavior would you expect if you exceed the model's effective context length—more examples or fewer?

- Concept: Decision-tree vs. flat rule application
  - Why needed here: The core intervention is reorganizing flat codebook rules into hierarchical IF-THEN logic.
  - Quick check question: Given codes ELI and OI, how would a decision tree encode "if no link to prior utterance, code as OI"?

## Architecture Onboarding

- Component map:
  Instruction Configuration (role + 4 modules) -> Example Bank (≈120) -> Feedback Loop

- Critical path:
  1. Baseline test with codebook-only configuration -> expect poor accuracy (16-67% precision)
  2. Add ~120 contextualized examples with segmented instructions
  3. Run confusion matrix analysis; identify top 3 error confusions
  4. Add targeted clarification rules and anchor examples; re-test
  5. Repeat until precision approaches human inter-coder reliability (~70%)

- Design tradeoffs:
  - Contextualized vs. single-turn examples: Context helps inter-dependent codes (ELI, EL, RE, RB); single-turn helps formulaic codes (A, Q)
  - Example quantity vs. instruction clarity: Clear instructions outperform raw example volume beyond ~120
  - Thread length: Long threads (>20 coding turns) risk drift; reset sessions periodically

- Failure signatures:
  - Precision on one code improves while others drop -> likely cognitive overload; simplify or reorder rules
  - Model defaults to conversational ChatGPT behavior -> add "dialogue coding-only" reminder in role definition
  - Consistent confusion between two codes (e.g., A vs. EL) -> missing disambiguating anchor examples

- First 3 experiments:
  1. Baseline: Configure agent with CDAS codebook only; measure per-code precision on held-out test set (expect <40% on most codes).
  2. Ablation: Add 120 contextualized examples without decision-tree structure; compare to 120 examples with decision-tree prompt—measure delta.
  3. Error-targeted iteration: Identify top 2 confusion pairs from experiment 2; add 5 anchor examples per pair and one clarification rule; measure precision change.

## Open Questions the Paper Calls Out

- To what extent do the identified prompt engineering strategies transfer to other LLM platforms with different architectures and token limits?
  - Basis in paper: [explicit] The authors state in the Limitations: "The work was conducted using a MyGPT agent, which has specific technical constraints...these may not directly transfer to other LLM platforms with different architectures or token limits."
  - Why unresolved: Only one platform was tested; no comparative analysis across LLMs was conducted.
  - What evidence would resolve it: Comparative experiments applying the same strategies to multiple LLM platforms (e.g., Claude, LLaMA, Gemini) with performance metrics.

- How can the agent be improved for low-performing code categories (IC, SC, RC) that achieved 0% precision?
  - Basis in paper: [inferred] Table 4 shows IC, SC, and RC codes achieved 0% precision and recall, indicating these remain unaddressed despite all strategies employed.
  - Why unresolved: The paper does not provide targeted strategies for these codes, which may require fundamentally different approaches.
  - What evidence would resolve it: Error analysis on IC, SC, RC codes with targeted prompt modifications tested against human-coded examples of these categories.

- Can the strategies generalize to classroom dialogue coding in different educational stages, subjects, and cultural settings?
  - Basis in paper: [explicit] "It used transcripts from UK primary classrooms in core subjects, where dialogic practices and curricular norms may not be typical of other educational stages, subjects, or cultural settings."
  - Why unresolved: The study used a single dataset; no cross-context validation was performed.
  - What evidence would resolve it: Replication studies applying the strategies to secondary, higher education, and non-UK classroom dialogue data.

## Limitations

- Only tested on UK primary classroom dialogue with a specific coding scheme (CDAS), limiting generalizability
- Optimal example count of ~120 is based on limited experimental range without intermediate points
- Some code categories (IC, SC, RC) achieved 0% precision despite all strategies applied
- Exact instruction text and examples not fully disclosed, preventing exact replication

## Confidence

- High Confidence: Basic premise that MyGPT agents can automate coding with reasonable accuracy; performance improves with contextualized examples
- Medium Confidence: Structured prompts with decision-tree logic improve accuracy by reducing cognitive load; ~120 examples is optimal
- Low Confidence: Generalizability of specific prompt design strategies to other coding schemes or educational contexts

## Next Checks

1. **Ablation Testing of Prompt Structure**: Conduct controlled experiments comparing flat versus hierarchical prompt structures on the same example set to isolate the effect of decision-tree logic on performance.

2. **Example Quantity Optimization**: Test additional example counts (e.g., 30, 60, 180, 240) to determine if the optimal range is truly around 120 or if performance plateaus earlier or later.

3. **Cross-Scheme Generalization**: Apply the refined prompt design and example selection strategies to a different dialogue coding scheme (e.g., from a different educational context or coding framework) to assess generalizability.