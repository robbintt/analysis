---
ver: rpa2
title: A Simple Method to Enhance Pre-trained Language Models with Speech Tokens for
  Classification
arxiv_id: '2512.07571'
source_url: https://arxiv.org/abs/2512.07571
tags:
- audio
- tokens
- speech
- language
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to enhance pre-trained language models
  with speech tokens for classification tasks. The method addresses the challenge
  of integrating speech information by applying lasso-based feature selection to multimodal
  Bag-of-Words representations, retaining only the most important audio tokens for
  the task.
---

# A Simple Method to Enhance Pre-trained Language Models with Speech Tokens for Classification

## Quick Facts
- arXiv ID: 2512.07571
- Source URL: https://arxiv.org/abs/2512.07571
- Authors: Nicolas Calbucura; Valentin Barriere
- Reference count: 28
- Primary result: Achieves state-of-the-art results on argumentative fallacy detection by integrating speech tokens into language models through lasso-based selection

## Executive Summary
This paper introduces a method to enhance pre-trained language models with speech tokens for classification tasks. The approach addresses the challenge of integrating speech information by applying lasso-based feature selection to multimodal Bag-of-Words representations, retaining only the most important audio tokens for the task. The language model is then adapted to these tokens using self-supervised language modeling before fine-tuning on the downstream task. The method improves performance compared to unimodal models, larger SpeechLM models, and methods integrating audio via learned representations, demonstrating effectiveness even when audio was previously considered counterproductive.

## Method Summary
The method follows a three-step pipeline: First, SpeechTokenizer extracts discrete tokens from audio (using an 8-layer RVQ architecture) and builds multimodal Bag-of-Words representations with text tokens. Second, lasso-based (ℓ1) logistic regression selects task-relevant audio tokens, filtering the vocabulary from 8,196 to approximately 73-80 tokens. Third, new audio tokens are added to the LLM vocabulary and pre-trained using self-supervised language modeling with a frozen backbone, then fine-tuned with LoRA adapters for the downstream classification task. The approach specifically uses acoustic-only tokens (layers 2-8) rather than semantic tokens (layer 1) to avoid redundancy with text semantics.

## Key Results
- Achieves state-of-the-art performance on Argumentative Fallacy Detection and Classification tasks
- Demonstrates that even random audio token selection improves unimodal model performance
- Shows that acoustic-only tokens outperform semantic+acoustic token integration
- Outperforms both unimodal text models and larger pre-trained multimodal models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lasso-based (ℓ1) feature selection on Bag-of-Words representations identifies task-relevant audio tokens while eliminating redundant or noisy ones.
- Mechanism: The ℓ1 regularization forces sparse coefficients in logistic regression, zeroing out irrelevant tokens. This reduces the audio vocabulary from 8,196 to ~73-80 tokens (<1%), addressing the "audio overflow" problem where high-frequency audio tokens (50Hz) overwhelm text tokens (~2Hz).
- Core assumption: Task-relevant acoustic information concentrates in a small subset of tokens; the rest is noise or redundancy.
- Evidence anchors:
  - [abstract] "By applying a simple lasso-based feature selection on multimodal Bag-of-Words representation, we retain only the most important audio tokens for the task"
  - [section 2.2] "The size of the final vocabulary V'a is less then 1% of Va"
  - [corpus] Fun-Audio-Chat Technical Report (FMR=0.65) confirms temporal resolution mismatch between speech tokens (25Hz) and text tokens (~3Hz) as a critical challenge in speech-text integration.
- Break condition: If downstream task has no acoustic correlate (purely semantic), selection may yield near-empty vocabulary with no benefit.

### Mechanism 2
- Claim: Restricting audio tokens to paralinguistic (acoustic) layers rather than semantic layers improves multimodal fusion with text-based LLMs.
- Mechanism: SpeechTokenizer's 8-layer RVQ architecture separates semantic content (layer 1) from paralinguistic content (layers 2-8). Since the LLM already processes text semantics, adding semantic audio tokens creates redundancy; acoustic-only tokens provide complementary prosodic/paralinguistic signals.
- Core assumption: Text modality already captures semantic content adequately; audio's value lies in non-semantic cues (intonation, emphasis, affect).
- Evidence anchors:
  - [section 2.1] "This aims to integrate only paralinguistics information from the speech while leaving the semantic content in the text"
  - [section 4, Table 2] "the model trained with acoustic-only tokens reach higher performances than the ones using also semantic tokens"
  - [corpus] Weak/no direct corpus evidence on semantic vs. acoustic token separation for LLM integration.
- Break condition: If text transcripts are noisy/incomplete (ASR errors), semantic audio tokens may become necessary.

### Mechanism 3
- Claim: Self-supervised pre-training of audio token embeddings with frozen LLM backbone enables integration without catastrophic forgetting of linguistic knowledge.
- Mechanism: New audio tokens are added to the LLM vocabulary; only their embeddings are trained via causal language modeling loss LCLM. This places audio tokens in the LLM's representation space before task fine-tuning, avoiding early-stage audio dominance that causes "semantic and attention sparsity."
- Core assumption: Audio tokens can be mapped into the LLM's existing embedding space without disrupting text representations.
- Evidence anchors:
  - [section 2.3] "All the weights of the LLM stay frozen except for the embeddings of the audio tokens in order to not mess with the model's initial representations"
  - [section 4] "The model trained without pre-training benefits from the lasso-based AT filtering, as without it, it reaches low F1 of .487 with a very standard deviation 5 times higher"
  - [corpus] Baichuan-Audio (FMR=0.67) uses similar text-guided alignment for speech integration, supporting the general approach.
- Break condition: If audio and text modalities have fundamentally incompatible structure, embedding adaptation may fail to align them meaningfully.

## Foundational Learning

- **Residual Vector Quantization (RVQ)**
  - Why needed here: SpeechTokenizer uses RVQ to produce hierarchical discrete tokens; layer 1 = semantic, layers 2-8 = acoustic. Understanding this decomposition is essential for selecting appropriate token layers.
  - Quick check question: Given an 8-layer RVQ tokenizer, which layers would you use for a task where transcripts are already clean but prosody matters?

- **ℓ1 Regularization (Lasso) for Feature Selection**
  - Why needed here: The method's token selection relies on ℓ1 forcing sparsity in logistic regression coefficients. Without understanding this, you cannot tune regularization strength or interpret selected tokens.
  - Quick check question: Why would ℓ1 produce a sparse solution while ℓ2 would not, given identical data?

- **Parameter-Efficient Fine-Tuning (LoRA)**
  - Why needed here: Fine-tuning freezes the base LLM and only updates LoRA adapters + classification head. This preserves pre-trained knowledge while adapting to the downstream task.
  - Quick check question: If you observe overfitting during fine-tuning, what LoRA hyperparameters would you adjust first?

## Architecture Onboarding

- **Component map:**
  1. SpeechTokenizer → Converts raw audio → 8-layer discrete tokens (1024 vocab per layer)
  2. Feature Selector → ℓ1 logistic regression on BoW → Selected token set (~73-80 tokens)
  3. Embedding Pre-training → Frozen LLaMA-3B + trainable audio embeddings → Causal LM loss
  4. Task Fine-tuning → LoRA adapters + linear classification head → Cross-entropy on task labels

- **Critical path:**
  1. Extract audio tokens from all samples using SpeechTokenizer
  2. Build multimodal BoW (text + audio), train ℓ1 classifier to select relevant audio tokens
  3. Filter audio sequences to retain only selected tokens; add to LLM vocabulary
  4. Pre-train audio embeddings with frozen backbone (MM-USED dataset used in paper)
  5. Fine-tune with LoRA on downstream task (MM-USED-Fallacy)

- **Design tradeoffs:**
  - **Vocabulary size vs. information:** Stronger ℓ1 regularization → fewer tokens → less noise but risk of missing signal. Paper selected ~73-80 tokens (matching text sequence length ~31-35).
  - **Acoustic-only vs. semantic+acoustic:** Acoustic-only (7 layers) outperformed full (8 layers) in experiments, but this is task-dependent.
  - **Pre-training necessity:** Pre-training stabilizes training; without it, high variance (5× higher std) observed. However, random token selection still improves over unimodal.

- **Failure signatures:**
  - High training variance without pre-training (std 5× higher per paper)
  - Unimodal text outperforming multimodal → likely token selection too aggressive or wrong layer selection
  - Larger multimodal model (Qwen2-Audio-7B) underperforming small LLaMA-3B → suggests small datasets benefit from targeted token selection over generic pre-training

- **First 3 experiments:**
  1. **Baseline comparison:** Run unimodal text-only LLaMA-3B on your task to establish baseline; compare with random audio token selection to verify the "even random helps" finding.
  2. **Ablation on token layers:** Compare 7-layer (acoustic-only) vs. 8-layer (semantic+acoustic) selection to determine if your task benefits from semantic audio information.
  3. **Pre-training vs. no pre-training:** With identical token selection, compare pre-trained embeddings vs. direct fine-tuning to measure variance reduction and performance impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can this discrete token fusion method be effectively extended to language generation tasks?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section that "experiments are restricted to a classification setting" and suggest extending the approach to generation tasks.
- Why unresolved: The method relies on a classification head; it is unclear if the token integration strategy supports the token-by-token requirements of text generation.
- What evidence would resolve it: Successful implementation and benchmarking of the method on speech-to-text generation tasks (e.g., summarization).

### Open Question 2
- Question: Do the pre-trained audio embeddings generalize to downstream tasks without specific fine-tuning?
- Basis in paper: [explicit] The authors note that they currently "fine-tune the audio embeddings jointly with the downstream task" and suggest evaluating generalization without this step.
- Why unresolved: It is unknown if the embeddings learn transferable representations or if their utility is dependent on joint optimization with the target task.
- What evidence would resolve it: Performance metrics on downstream tasks where the audio token embeddings remain frozen during training.

### Open Question 3
- Question: How do the selected speech tokens correlate with expert-defined acoustic features?
- Basis in paper: [explicit] The authors propose a "correlational study between the learned speech tokens and expert acoustic features" to improve interpretability.
- Why unresolved: While the method selects "important" tokens, the physical acoustic properties (pitch, intensity, etc.) these tokens represent remain opaque.
- What evidence would resolve it: A statistical analysis mapping high-importance tokens to specific acoustic signal characteristics.

### Open Question 4
- Question: What is the optimal vocabulary size for the selected speech tokens?
- Basis in paper: [explicit] The authors mention it "would be interesting to experiment with a lower lasso regularization to get a larger number of speech tokens and empirically find the optimal size."
- Why unresolved: The current method selects fewer than 1% of the initial vocabulary, but the trade-off between vocabulary size and information density has not been fully explored.
- What evidence would resolve it: Ablation studies varying the lasso penalization to identify the performance curve relative to token count.

## Limitations

- Method relies on specific dataset (MM-USED-Fallacy) for validation, limiting generalization claims without broader task coverage
- ℓ1 regularization strength (C parameter) not specified, requiring empirical tuning that may limit robustness across datasets
- Acoustic-only token selection (layers 2-8) outperforms semantic+acoustic, but this may be dataset-specific since transcripts are clean and complete
- Computational cost of 3-step pipeline (tokenization, selection, pre-training) versus simpler alternatives not evaluated

## Confidence

**High Confidence:**
- Lasso-based feature selection effectively reduces audio vocabulary while improving performance (empirical evidence from Table 2, ablation studies)
- Pre-training audio embeddings is necessary for stable training (variance reduction from 5× higher std without pre-training)
- Random token selection still improves unimodal baseline (Table 2, last row)

**Medium Confidence:**
- Acoustic-only tokens outperform semantic+acoustic integration (results depend on transcript quality and task nature)
- Small LLaMA-3B outperforms larger Qwen2-Audio-7B (may reflect dataset size rather than architectural superiority)
- Method outperforms both unimodal and other multimodal approaches (generalization across tasks not established)

**Low Confidence:**
- Claims about why semantic tokens hurt performance (mechanism assumes text already captures semantics well)
- Optimal token count of ~73-80 tokens is universally applicable (likely task-specific tuning)
- Pre-training on MM-USED dataset is optimal (alternative pre-training strategies not explored)

## Next Checks

1. **Cross-dataset generalization test**: Apply the exact pipeline to a different multimodal classification dataset (e.g., sentiment analysis from audio+text) to verify whether acoustic-only token selection and ~73-80 token count remain optimal, or if results are specific to argumentative fallacy detection.

2. **Semantic token ablation under noisy transcripts**: Repeat the layer selection experiment (acoustic-only vs. semantic+acoustic) on the same dataset but with artificially corrupted/noisy transcripts to test whether semantic audio tokens become beneficial when text quality degrades.

3. **Token selection sensitivity analysis**: Systematically vary the ℓ1 regularization strength (C parameter) across multiple orders of magnitude to quantify the stability of the ~73-80 token selection threshold and determine whether performance degrades gracefully as token count deviates from this value.