---
ver: rpa2
title: A Field Guide to Deploying AI Agents in Clinical Practice
arxiv_id: '2509.26153'
source_url: https://arxiv.org/abs/2509.26153
tags:
- clinical
- data
- governance
- project
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A key outcome of this field guide is the demonstration that successful
  deployment of generative AI agents in clinical practice requires shifting focus
  from algorithmic development to the sociotechnical work of implementation. Through
  deployment of an immune-related adverse event detection agent at Mass General Brigham,
  the authors found that less than 20% of effort was spent on model development, while
  over 80% went to data integration, validation, economic value, drift management,
  and governance.
---

# A Field Guide to Deploying AI Agents in Clinical Practice

## Quick Facts
- arXiv ID: 2509.26153
- Source URL: https://arxiv.org/abs/2509.26153
- Reference count: 40
- Primary result: Successful clinical AI deployment requires shifting from model optimization to sociotechnical implementation work

## Executive Summary
This field guide presents lessons from deploying a generative AI agent for immune-related adverse event (irAE) detection at Mass General Brigham, revealing that successful clinical AI implementation requires dramatically more effort on sociotechnical infrastructure than on model development. The authors demonstrate that less than 20% of effort goes to algorithmic work while over 80% is consumed by data integration, validation, governance, drift management, and economic value creation. Through their "crawl-walk-run" validation framework and concrete gate criteria (F1 > 0.75, precision > 0.80), the guide provides actionable frameworks for moving AI pilots into routine clinical use. The central insight is that bridging the "valley of death" between AI promise and real-world impact requires treating implementation as the primary engineering challenge rather than an afterthought.

## Method Summary
The authors deployed a multi-agent LLM system for automated irAE detection from clinical notes, using a daily batch processing approach through Azure private endpoints to a HIPAA-compliant OpenAI API. The pipeline extracted notes from Epic EHR via Snowflake research enclave, processed them through extractor and predictor agents, and produced CTCAE-graded outputs with evidence links for human verification. Validation followed a four-phase approach: retrospective gold standard creation, silent mode monitoring, limited field study, and production rollout. The system achieved F1=0.88, sensitivity=0.84, and precision=0.95, with time-to-alert under 2 days and override rates below 10%.

## Key Results
- Deployment revealed an 80/20 inversion: <20% effort on model development, >80% on sociotechnical implementation
- Achieved gate criteria with F1=0.88, sensitivity=0.84, precision=0.95 for irAE detection
- Batch processing (daily) reduced integration complexity compared to real-time approaches for latency-tolerant use cases
- "Crawl-walk-run" validation framework successfully managed safety risks of stochastic LLM behavior

## Why This Works (Mechanism)

### Mechanism 1: The 80/20 Sociotechnical Inversion
If deployment teams shift focus from model optimization to sociotechnical "heavy lifts," they are more likely to cross the "valley of death" from pilot to production. The paper observes that <20% of effort determines algorithmic performance, while >80% determines real-world viability. By treating data integration, validation, economics, drift, and governance as the primary engineering constraints rather than afterthoughts, the system accumulates the structural trust required for clinical adoption. The bottleneck to clinical impact is implementation infrastructure, not model intelligence. If the institution lacks resources to fund the "heavy lift" infrastructure (e.g., bridge engineers, data concierges), model improvements will yield diminishing returns and fail to deploy.

### Mechanism 2: Right-Sizing Latency via Batch Processing
If a clinical use case tolerates latency (e.g., 24–96 hours), shifting from real-time to daily batch processing reduces integration complexity and accelerates proof-of-value. Real-time HL7/FHIR feeds require significant engineering overhead before value is proven. By aligning the inference schedule (daily batches) with the clinical tolerance for the event (irAE biobank registration within 96 hours), the team leverages existing research enclaves (Snowflake) rather than building custom streaming pipelines. The specific clinical workflow (surveillance) does not require immediate intervention, allowing the team to trade latency for lower engineering costs. If the clinical need shifts to emergent detection (e.g., sepsis), the batch architecture fails to deliver timely alerts.

### Mechanism 3: Continuous Validation via Shadow Mode & HCI
If validation is treated as a perpetual service rather than a one-off test, the system maintains safety despite stochastic LLM behavior. LLMs introduce failure modes like hallucination and context loss. The paper uses a "crawl-walk-run" approach: Retrospective Gold Standard → Silent Mode (logged but hidden) → Limited Field Study. This continuously exposes the model to real data while containing risk via Human-in-the-Loop (HITL) verification. Ground-truth labels and human auditor capacity are available to monitor output quality continuously. If the volume of data overwhelms human verification capacity, the validation pipeline becomes a bottleneck or fails to catch drift.

## Foundational Learning

- **Concept: OMOP/Common Data Models (CDM)**
  - Why needed here: The paper assumes data is unified in a research enclave (Snowflake) mapped to standards like OMOP/i2b2. Without understanding how structured and unstructured data are mapped to a CDM, the "Data Integration" heavy lift is impossible.
  - Quick check question: Can you query a patient's problem list and unstructured notes simultaneously using a standardized vocabulary in your data warehouse?

- **Concept: RACI Matrix (Responsible, Accountable, Consulted, Informed)**
  - Why needed here: Governance is identified as a "heavy lift." The paper uses a RACI matrix to define accountability (e.g., who decides the threshold for go-live). Understanding this is prerequisite to navigating the organizational structure.
  - Quick check question: For your current project, is the "Accountable" party for model safety the data scientist or the Chief Medical Informatics Officer (CMIO)?

- **Concept: F1 Score vs. Precision Trade-offs**
  - Why needed here: The paper defines specific "gate metrics" (F1 > 0.75, Precision > 0.80) to progress between phases. Engineers must understand why high precision is prioritized to avoid "alert fatigue" in clinical workflows.
  - Quick check question: If your model detects 100% of events but has 50% precision (high false positives), will clinicians stop trusting the alerts?

## Architecture Onboarding

- **Component map:** Epic EHR -> Snowflake Research Enclave (Bronze/Silver/Gold) -> Azure Function (5AM trigger) -> Azure Container Apps (Extractor & Predictor) -> Azure OpenAI (Private Endpoint) -> Azure Blob Storage (Predictions + Reports)
- **Critical path:** The path from Snowflake Enclave to Azure OpenAI via Private Endpoints. If the private link isn't established, PHI cannot traverse to the inference engine securely, stalling the entire project.
- **Design tradeoffs:** Batch vs. Real-time: Paper chose batch (daily) to save engineering effort; traded immediate alerts for lower cost. Hybrid Architecture: Suggests using LLMs for extraction + Classical ML for classification to improve interpretability vs. using an LLM for the entire pipeline.
- **Failure signatures:** Hallucination: Model attributes events to immunotherapy when none was mentioned. Context Loss: Model misses evidence in long notes. Drift: Silent vendor API updates cause performance erosion without code changes.
- **First 3 experiments:**
  1. Sandbox Feasibility: Query the research enclave to extract a cohort of notes for a specific patient population to verify data latency and completeness.
  2. Retrospective Silent Run: Run the LLM pipeline on the extracted notes without showing clinicians; log accuracy against a manually chart-reviewed "gold standard" of 50 cases.
  3. Friends-and-Family Preview: Deploy the interface to a small group of friendly users (e.g., research coordinators) to validate UI/UX and hallucination rates before governance review.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What validated, scalable approaches can detect model drift and data drift in clinical LLM agents, and what thresholds should trigger remediation?
- Basis in paper: [explicit] The authors state that "unlike traditional machine learning, there are currently no widely accepted or validated approaches for detecting model or data drift in these systems," and that their proposed monitoring methods are "currently being implemented, with evaluation planned as sufficient data accumulates."
- Why unresolved: LLM drift manifests as hallucinations or context loss rather than metric degradation, and no consensus protocols exist for clinical settings.
- What evidence would resolve it: Prospective studies comparing drift detection methods (embedding divergence, gold-set rescoring, LLM-as-judge audits) with clinically meaningful outcomes across multiple deployments.

### Open Question 2
- Question: How effective is "LLM-as-judge" monitoring for detecting hallucinations, factual inconsistencies, and unsafe outputs in live clinical agent deployments?
- Basis in paper: [explicit] The authors write they are "evaluating the efficacy of direct LLM-as-judge monitoring of the LLM's live output behavior, including automated tracking of failure modes such as factual consistency, tone, and hallucination rates on production inferences."
- Why unresolved: This approach is proposed but not yet validated; automated judges may themselves exhibit drift or bias.
- What evidence would resolve it: Head-to-head comparison of LLM-as-judge against human clinician audits on large production datasets, with sensitivity/specificity for failure mode detection.

### Open Question 3
- Question: How should governance frameworks differentiate privacy and regulatory requirements for prompt engineering on clinical data versus fine-tuning LLMs on clinical data?
- Basis in paper: [explicit] The authors note that "prompt engineering, where real clinical data are used to optimize prompting strategies, adds a new privacy consideration for governance and regulatory bodies, and can create confusion with regard to the presence or absence of PHI privacy risk." Table 7 calls for research to "refine guidance around privacy risk for clinical data, fine-tuned models, and prompt optimisation."
- Why unresolved: Current IRB and compliance frameworks were designed for training, not for optimizing prompts on PHI, creating ambiguity.
- What evidence would resolve it: Empirical analysis of PHI exposure risks across prompt engineering vs. fine-tuning workflows, resulting in tiered governance protocols.

### Open Question 4
- Question: How generalizable is the 80/20 effort distribution between model development and sociotechnical implementation across different clinical use cases, healthcare settings, and resource levels?
- Basis in paper: [inferred] The 80/20 finding is derived from a single irAE detection deployment at one academic medical center with 21 stakeholders; the paper provides no evidence of external validity across diverse institutions or use cases.
- Why unresolved: Community hospitals, resource-limited settings, and different clinical workflows may face different implementation challenges and effort distributions.
- What evidence would resolve it: Multi-site deployment studies tracking effort allocation across the five heavy lifts in varied healthcare environments and use cases.

## Limitations
- Single-site study at large academic medical center limits generalizability to smaller institutions
- 80/20 sociotechnical inversion may reflect institutional capacity rather than universal property
- Gate criteria (F1 > 0.75, precision > 0.80) derived from one use case may not apply across clinical domains
- Human-in-the-loop verification approach doesn't scale to large populations

## Confidence
- **High confidence**: Deployment complexity exceeds model development (supported by concrete time allocation data)
- **Medium confidence**: Specific gate criteria as universal thresholds (derived from one clinical use case)
- **Medium confidence**: Proposed frameworks as broadly applicable (effectiveness across diverse structures untested)
- **Low confidence**: Batch processing always preferable to real-time (depends heavily on specific workflows)

## Next Checks
1. Cross-institutional replication: Deploy same irAE detection agent at 2-3 different health systems to test 80/20 principle validity
2. Threshold sensitivity analysis: Systematically vary gate criteria across 5-10 clinical use cases to test generalizability
3. Scale validation feasibility study: Measure human verification capacity limits by deploying to progressively larger populations and testing automated alternatives