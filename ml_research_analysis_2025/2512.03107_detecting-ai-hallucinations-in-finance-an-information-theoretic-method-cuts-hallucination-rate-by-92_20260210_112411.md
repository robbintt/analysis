---
ver: rpa2
title: 'Detecting AI Hallucinations in Finance: An Information-Theoretic Method Cuts
  Hallucination Rate by 92%'
arxiv_id: '2512.03107'
source_url: https://arxiv.org/abs/2512.03107
tags:
- entropy
- evidence
- eclipse
- hallucination
- capacity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ECLIPSE detects AI hallucinations in finance by measuring mismatch
  between semantic entropy and evidence capacity. The method combines entropy estimation
  via multi-sample clustering with perplexity decomposition to quantify how models
  use evidence.
---

# Detecting AI Hallucinations in Finance: An Information-Theoretic Method Cuts Hallucination Rate by 92%

## Quick Facts
- arXiv ID: 2512.03107
- Source URL: https://arxiv.org/abs/2512.03107
- Reference count: 40
- Primary result: ECLIPSE achieves ROC AUC 0.89 and average precision 0.90 on financial QA hallucinations

## Executive Summary
ECLIPSE is an information-theoretic method for detecting AI hallucinations in finance that measures mismatch between semantic entropy and evidence capacity. The method combines entropy estimation via multi-sample clustering with perplexity decomposition to quantify how models use evidence. On a controlled financial QA dataset (200 samples, balanced hallucinated/clean), ECLIPSE achieves ROC AUC of 0.89 and average precision of 0.90, substantially outperforming entropy-only baseline (AUC 0.50). A controlled ablation with Claude-3-Haiku, which lacks token-level log probabilities, shows AUC dropping to 0.59 with coefficient magnitudes decreasing by 95%, demonstrating that ECLIPSE is logprob-native and requires structured token-level uncertainty.

## Method Summary
ECLIPSE detects hallucinations by computing the gap between semantic entropy (uncertainty over answer meanings) and evidence capacity (how much evidence constrains the answer). The method generates K=10 samples at temperature 0.7, clusters them by extracted fact triples using spaCy NER and regex, then computes entropy over cluster probabilities. Perplexity decomposition features are extracted via API queries with/without evidence to compute log-likelihoods (LQ, LQE), capacity lift (∆L), and other logprob-based features. A logistic regression classifier combines these features to output calibrated hallucination probabilities.

## Key Results
- ECLIPSE achieves ROC AUC 0.89 and average precision 0.90 on 200-sample financial QA dataset
- Perplexity decomposition features (LQE, ∆L, ratio) exhibit largest learned coefficients (+1.728, -1.604, -1.756)
- Claude-3-Haiku ablation (no logprobs) shows AUC drops to 0.59 with 95% coefficient magnitude collapse
- At 30% coverage threshold, ECLIPSE reduces hallucination rate by 92% in financial QA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perplexity decomposition features detect evidence-ignoring behavior, which is the primary signal for hallucination.
- Mechanism: Compute ∆L = LQE − LQ (log-likelihood of answer with evidence minus without). Near-zero ∆L indicates evidence is being ignored; positive values indicate grounded generation.
- Core assumption: Hallucinations arise when models generate from prior knowledge rather than conditioning on retrieved evidence.
- Evidence anchors: [abstract] "perplexity decomposition features exhibit the largest learned coefficients"; [Table 2] LQE coefficient +1.728, ∆L coefficient −1.604, ratio coefficient −1.756 are the three largest magnitudes.

### Mechanism 2
- Claim: ECLIPSE is logprob-native—it requires calibrated token-level uncertainties and degrades to near-random when logprobs are unavailable.
- Mechanism: All features (entropy, capacity, perplexity decomposition) depend on log probabilities. When replaced with heuristic estimates, coefficient magnitudes collapse 90–96%.
- Core assumption: Token-level log probabilities carry structured information about model uncertainty that cannot be approximated from output text alone.
- Evidence anchors: [abstract] "Claude-3-Haiku, which lacks token-level log probabilities, shows AUC dropping to 0.59 with coefficient magnitudes decreasing by 95%"; [Table 5] Feature retention ratios: LQE 4%, ∆L flips sign, pmax 0%.

### Mechanism 3
- Claim: Entropy alone is insufficient; capacity features provide necessary signal for distinguishing honest uncertainty from hallucination.
- Mechanism: Semantic entropy H measures uncertainty over answer clusters. Evidence capacity C measures how much evidence constrains the answer. The joint signal (H − Hpref) identifies misalignment.
- Core assumption: Honest uncertainty (weak evidence, high entropy) differs from evidence-ignoring behavior (strong evidence, high entropy).
- Evidence anchors: [abstract] "entropy-only baseline (AUC 0.50)"; [Table 3] H only = 0.50 AUC; +Ceff = 0.68 (+0.18).

## Foundational Learning

- Concept: **Semantic Entropy via Clustering**
  - Why needed here: Requires generating K samples, clustering by meaning (not surface form), and computing entropy over cluster probabilities.
  - Quick check question: Can you explain why semantic entropy differs from token-level entropy, and why the paper uses fact-triple extraction for clustering?

- Concept: **Perplexity and Log-Likelihood Decomposition**
  - Why needed here: Core features (LQ, LQE, ∆L) are log-likelihoods from the model. Understanding perplexity = exp(−avg log prob) is essential.
  - Quick check question: If LQE >> LQ, what does this imply about how the model used the evidence?

- Concept: **Calibration and Selective Prediction**
  - Why needed here: ECLIPSE outputs calibrated hallucination probabilities via logistic regression; deployment requires setting coverage thresholds.
  - Quick check question: At 30% coverage, ECLIPSE reduces hallucination rate by 92%. What trade-off does this represent?

## Architecture Onboarding

- Component map: Multi-sample generator → Semantic clusterer → Entropy estimator → Perplexity extractor → Feature vector assembler → Logistic regression classifier
- Critical path: API log probability access → perplexity features → classifier. If logprobs unavailable, method fails (AUC 0.59).
- Design tradeoffs:
  - K=10 samples balances cost vs. entropy estimate quality; higher K improves clustering
  - Temperature 0.7 encourages diversity for semantic entropy; lower temperature reduces spread
  - 1% numeric tolerance for clustering is domain-specific; looser for qualitative domains
- Failure signatures:
  - Coefficient magnitudes <0.2 → likely logprob signal degraded
  - ∆L coefficient positive → feature engineering error or sign flip
  - Entropy-only AUC >0.7 → dataset may have artifacts (check for memorization)
- First 3 experiments:
  1. Validate logprob dependency: Run ECLIPSE on a model with/without logprob access; confirm coefficient collapse pattern
  2. Ablate feature groups: Replicate Table 3 (H only → +Ceff → +perplexity) on your domain data
  3. Coverage calibration: Plot hallucination rate vs. coverage on validation set to select deployment threshold

## Open Questions the Paper Calls Out

- **Question 1**: Does ECLIPSE generalize to naturally occurring hallucinations, or does performance degrade compared to the synthetic perturbations used in the controlled study?
  - Basis in paper: [explicit] "The dataset does not include naturally occurring model hallucinations at scale, which may exhibit different characteristics."
  - Why unresolved: The 200-sample evaluation uses four constructed perturbation types (wrong numbers, entity swaps, contradictions, fabrications). Natural hallucinations may follow different patterns that the perplexity decomposition features fail to capture.
  - What evidence would resolve it: Evaluation on datasets containing unprompted, naturally generated hallucinations (e.g., model outputs collected without deliberate perturbation), comparing detection AUC against the synthetic benchmark.

- **Question 2**: Can ECLIPSE maintain strong performance (AUC ~0.89) on standardized hallucination benchmarks such as TruthfulQA and HaluEval?
  - Basis in paper: [explicit] "Establishing performance on common benchmarks (e.g., TruthfulQA, HaluEval) remains important future work."
  - Why unresolved: The current 0.89 AUC is on a custom financial QA dataset. Cross-paper comparisons in Table 6 come from different evaluations on different datasets, preventing direct performance claims.
  - What evidence would resolve it: Direct evaluation of ECLIPSE on TruthfulQA, HaluEval, and other established benchmarks with identical protocols to baseline methods.

- **Question 3**: To what extent does the Claude ablation performance drop stem from logprob unavailability versus model architecture differences?
  - Basis in paper: [inferred] "Claude ablation confounds: Our Claude-3-Haiku ablation changes both the model family and the availability of log probabilities."
  - Why unresolved: The 0.89→0.59 AUC drop and 90–96% coefficient collapse could partially reflect Claude's different training, rather than purely the lack of token-level log probabilities.
  - What evidence would resolve it: Within-model-family experiment where real logprobs are artificially replaced with estimated proxies (e.g., on GPT-3.5-turbo), isolating the logprob-native effect from architectural confounds.

## Limitations

- **Dataset Scope and Generality**: The evaluation uses a carefully constructed financial QA dataset with 200 examples, which provides strong control but limited generalizability. Domain transfer to legal, medical, or general QA remains untested.
- **Log Probability Dependency**: ECLIPSE requires token-level log probabilities for all key features. The method fails when logprobs are unavailable, creating a critical dependency on API providers supporting logprob=True.
- **Feature Engineering Sensitivity**: The semantic entropy clustering relies on spaCy NER and regex for fact extraction with specific thresholds (1% numeric tolerance, 50% Jaccard entity overlap) that may not generalize across domains.

## Confidence

**High Confidence Claims**:
- ECLIPSE achieves ROC AUC 0.89 and AP 0.90 on the controlled financial dataset
- Perplexity decomposition features (LQE, ∆L, ratio) have the largest learned coefficients
- Claude-3-Haiku ablation confirms logprob dependency (AUC 0.59, coefficient collapse)

**Medium Confidence Claims**:
- Semantic entropy + capacity features improve over entropy-only baseline (0.50 → 0.68)
- 92% hallucination rate reduction at 30% coverage is achievable in financial QA
- The 4 perturbation types comprehensively cover financial hallucination patterns

**Low Confidence Claims**:
- ECLIPSE will perform similarly in non-financial domains without additional validation
- The 1% numeric tolerance and 50% Jaccard overlap thresholds are optimal across domains
- No alternative clustering or feature extraction methods could match current performance

## Next Checks

1. **Domain Transfer Validation**: Apply ECLIPSE to a non-financial QA dataset (legal, medical, or general) with ground-truth hallucination labels. Compare ROC AUC and feature coefficient patterns to the financial results. Test whether perplexity decomposition features remain dominant when evidence is non-numerical.

2. **Log Probability Availability Analysis**: Systematically test ECLIPSE across models with varying logprob support (GPT-4, Claude, Gemini, open-weight models). Document the exact relationship between logprob access, feature coefficient magnitudes, and detection performance. Identify whether any heuristic approximations can partially recover performance when logprobs are unavailable.

3. **Threshold Sensitivity and Calibration**: Conduct comprehensive coverage calibration across the full range (0-100%). Plot hallucination rate vs. coverage for multiple deployment thresholds. Calculate precision-recall breakeven points and cost-benefit analysis for different operational scenarios. Validate calibration using proper scoring rules (Brier score, expected calibration error).