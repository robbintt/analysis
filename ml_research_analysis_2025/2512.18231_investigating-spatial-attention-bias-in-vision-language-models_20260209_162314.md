---
ver: rpa2
title: Investigating Spatial Attention Bias in Vision-Language Models
arxiv_id: '2512.18231'
source_url: https://arxiv.org/abs/2512.18231
tags:
- bias
- spatial
- image
- visual
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a systematic spatial attention bias in Vision-Language
  Models (VLMs), where they consistently prioritize describing left-positioned content
  over right-positioned content in horizontally concatenated images. The bias was
  observed across seven diverse VLM architectures, with models describing left-positioned
  content first in approximately 97% of cases under neutral prompting conditions.
---

# Investigating Spatial Attention Bias in Vision-Language Models

## Quick Facts
- **arXiv ID:** 2512.18231
- **Source URL:** https://arxiv.org/abs/2512.18231
- **Authors:** Aryan Chaudhary; Sanchit Goyal; Pratik Narang; Dhruv Kumar
- **Reference count:** 14
- **Primary result:** VLMs consistently describe left-positioned content first in ~97% of cases, persisting despite right-to-left language training and directional instructions

## Executive Summary
This paper identifies a systematic spatial attention bias in Vision-Language Models (VLMs) where they consistently prioritize describing left-positioned content over right-positioned content in horizontally concatenated images. The bias was observed across seven diverse VLM architectures, with models describing left-positioned content first in approximately 97% of cases under neutral prompting conditions. Testing on an Arabic-finetuned model ruled out language reading direction as the primary cause, and analysis of training dataset annotation guidelines found no explicit left-first ordering instructions. The findings suggest architectural factors are responsible for this bias, which persists even when models are given explicit instructions to describe from right to left.

## Method Summary
The study evaluated seven diverse VLM architectures on horizontally concatenated image pairs from Caltech-101 and Desktop UI datasets. Caltech-101 used 50 unique image pairs plus their horizontal flips (100 total test images), while Desktop UI used 10 screenshots forming 45 pairwise combinations. Models were tested under neutral prompting ("Describe the image"), simple directional prompting ("Describe from left to right/right to left"), and structured JSON prompting. The primary metric measured whether left or right content was described first, with character length differences used for directional bias analysis.

## Key Results
- VLMs show 97% left-first description preference under neutral prompting across seven architectures
- Bias persists in Arabic-finetuned models, ruling out language reading direction
- Explicit directional instructions often fail, with models ignoring right-to-left prompts
- Dense UI contexts can reverse bias polarity in proprietary models (GPT-5 Nano: -179.7 on Desktop UI vs +3.3 on Caltech)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Positional embedding asymmetry creates preferential attention weights for left-positioned image tokens.
- **Mechanism:** Rotary Position Embeddings (RoPE) and similar position encodings assign different positional codes to tokens based on sequence position. When image patches are flattened row-major (left-to-right, top-to-bottom), the resulting token positions may create systematically stronger attention patterns for earlier (left-side) positions during autoregressive generation.
- **Core assumption:** Positional encodings transfer asymmetrically from 1D text sequences to 2D spatial token arrangements.
- **Evidence anchors:**
  - [abstract] "suggesting the bias is consistent with architectural factors rather than explicit training data instructions"
  - [section 4.5.3] "If positional embeddings (such as RoPE) create asymmetric attention patterns, this could manifest as the observed bias"
  - [corpus] Paper 109729 (arXiv:2509.21984) directly investigates spatial bias in LVLMs and identifies position embedding design as a contributing factor

### Mechanism 2
- **Claim:** Vision encoder patch ordering establishes left-to-right processing priors before language model involvement.
- **Mechanism:** Vision Transformers divide images into patches and process them sequentially. Standard row-major flattening orders patches left-to-right per row. This ordering may become encoded in the visual representations themselves, independent of downstream positional embeddings.
- **Core assumption:** Patch sequence order during visual encoding creates persistent spatial priors that survive projection to the language model.
- **Evidence anchors:**
  - [section 4.5.3] "If patch ordering or spatial relationship encoding favors left-to-right patterns, this architectural choice could produce positional bias independent of language or training data"
  - [abstract] Bias persists "across seven diverse VLM architectures" with different visual encoders
  - [corpus] No corpus papers directly test patch ordering interventions

### Mechanism 3
- **Claim:** The bias is architecture-intrinsic and resistant to instruction-level interventions.
- **Mechanism:** Cross-modal alignment projectors learn mappings that may encode spatial processing preferences. Fine-tuning on RTL languages or explicit directional instructions cannot override these learned spatial priors because they operate at the representation level, not the generation policy level.
- **Core assumption:** Spatial attention patterns are baked into learned weights during pretraining and cannot be steered by prompting alone.
- **Evidence anchors:**
  - [abstract] "bias persists despite right-to-left language training, ruling out language reading direction as the primary cause"
  - [section 4.1] AIN model fine-tuned on Arabic (RTL) shows identical 97% left-first preference
  - [section 4.3] Qwen2.5-VL "systematically failed to follow the directional constraint" when instructed to describe right-to-left

## Foundational Learning

- **Concept:** Transformer positional encodings (RoPE, sinusoidal, learned)
  - Why needed here: The hypothesized mechanism involves asymmetric attention patterns emerging from how position information is encoded for image tokens.
  - Quick check question: Can you explain why RoPE might treat position 0 differently from position 100 during attention computation?

- **Concept:** Vision Transformer (ViT) patch embedding and tokenization
  - Why needed here: Understanding how 2D images become 1D token sequences is essential to trace where spatial ordering is introduced.
  - Quick check question: Given a 224×224 image with 16×16 patches, how many tokens are produced and what determines their order?

- **Concept:** Cross-modal alignment in VLMs (projector/adapter layers)
  - Why needed here: The projector bridges visual and language spaces and may encode or amplify spatial biases during transfer.
  - Quick check question: What is the role of a Q-Former or linear projector in BLIP-2/LLaVA-style architectures?

## Architecture Onboarding

- **Component map:** Visual encoder -> Positional encoder -> Projector/Adapter -> LLM backbone
- **Critical path:**
  1. Image → patch extraction (defines spatial ordering)
  2. Patches + position embeddings → visual encoder output
  3. Visual tokens → projector → LLM embedding space
  4. LLM attention over visual tokens → generation order

- **Design tradeoffs:**
  - 1D vs 2D positional encodings: 1D is simpler but may introduce spatial bias; 2D preserves spatial structure but requires architecture changes
  - Freezing vs fine-tuning visual encoder: Frozen encoders fix any spatial biases from pretraining
  - Simple vs structured prompting: Structured outputs (JSON) reduce but don't eliminate bias (see Table 3)

- **Failure signatures:**
  - Model describes left image first despite "describe right to left" instruction
  - Model generates longer descriptions for left-positioned content under neutral prompting
  - Bias magnitude increases with simpler prompts and open-source models (InternVL2: +472.8 char average difference)
  - Dense UI contexts may reverse bias polarity (GPT-5 Nano: -179.7 on Desktop UI vs +3.3 on Caltech)

- **First 3 experiments:**
  1. **Patch order ablation:** Randomize or reverse patch ordering in the visual encoder and measure left-first percentage on the Caltech-101 pairs.
  2. **Position encoding swap:** Test with 2D-aware positional embeddings vs standard 1D RoPE on the same model backbone.
  3. **Attention visualization:** Extract and plot attention weights from early LLM layers on horizontally concatenated images to confirm stronger attention to left-positioned tokens.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Which specific architectural component—positional embeddings (e.g., RoPE) or vision encoder patch ordering—is causally responsible for the systematic spatial attention bias?
- **Basis in paper:** [explicit] Section 4.5.3 "Architectural Factors" hypothesizes these two mechanisms, and Section 5 explicitly calls for mechanistic interpretability to identify responsible components.
- **Why unresolved:** The paper identifies architectural factors as the likely cause through elimination of other hypotheses but does not perform internal mechanism analysis to pinpoint the exact source.
- **What evidence would resolve it:** Causal intervention studies, such as modifying positional embedding schemes or altering patch processing order in the vision encoder, to observe changes in spatial preference.

### Open Question 2
- **Question:** Do implicit human annotation patterns in training data contribute to spatial bias despite the absence of explicit guidelines?
- **Basis in paper:** [explicit] Section 4.5.2 notes that "implicit human tendencies could still influence training data," and Section 5 lists quantitative analysis of annotations as necessary future work.
- **Why unresolved:** The study analyzed annotation *guidelines* (which were neutral) but did not analyze the *actual* sequential descriptions produced by human annotators in the training sets.
- **What evidence would resolve it:** A large-scale quantitative analysis of description order in datasets like Visual Genome or PixMo to measure the frequency of left-to-right sequencing by annotators.

### Open Question 3
- **Question:** Can the spatial attention bias be eliminated or reversed by pre-training a VLM entirely from scratch on right-to-left (RTL) languages?
- **Basis in paper:** [inferred] from Section 4.5.1, where the authors explicitly note that the Arabic-finetuned model (AIN) was not pre-trained from scratch, leaving the dominance of architectural priors over language pre-training uncertain.
- **Why unresolved:** The tested Arabic model was only fine-tuned on RTL data, meaning the underlying "left-biased" architecture was already fixed; the effect of full RTL pre-training remains unknown.
- **What evidence would resolve it:** Training a VLM from initialization solely on RTL language data and evaluating its spatial preference on concatenated images using neutral prompts.

### Open Question 4
- **Question:** Why does high information density in complex UIs trigger a polarity shift from left-bias to right-bias in proprietary models like GPT-5 Nano?
- **Basis in paper:** [explicit] Section 4.3 documents a "Shift in Polarity" where models favored the right side on the Desktop UI dataset, a phenomenon the paper describes but does not explain.
- **Why unresolved:** The paper quantifies the shift but acknowledges it only as a qualitative observation, noting that dense contexts alter scanning behaviors without proposing a mechanism.
- **What evidence would resolve it:** Attention analysis comparing object-centric versus text-dense inputs to determine if "recency bias" in the context window overrides visual spatial priors in dense layouts.

## Limitations
- Reliance on manual analysis for detecting which image content appears first prevents systematic reproduction and scaling
- Only tests horizontal concatenation, not investigating vertical arrangements (top-bottom) or diagonal relationships
- Mechanism investigations remain correlational without experimental manipulation to establish causality

## Confidence

- **High Confidence:** The existence of systematic left-first bias across multiple architectures and prompting conditions (97% observed in neutral prompting)
- **Medium Confidence:** The architectural origin hypothesis, particularly regarding positional encodings and patch ordering
- **Low Confidence:** The generalizability to other spatial arrangements and the specific contribution of each architectural component

## Next Checks
1. **Patch Order Intervention:** Modify the vision encoder to process patches in reverse or random order and measure whether the left-first bias persists or reverses
2. **Positional Encoding Comparison:** Implement and test the same VLM architecture with 2D-aware positional encodings versus standard 1D RoPE to quantify impact on spatial bias
3. **Attention Pattern Analysis:** Extract and visualize attention weights from early LLM layers when processing horizontally concatenated images to directly observe attention patterns