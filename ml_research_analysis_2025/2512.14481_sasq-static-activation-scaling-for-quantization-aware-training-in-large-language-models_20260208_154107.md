---
ver: rpa2
title: 'SASQ: Static Activation Scaling for Quantization-Aware Training in Large Language
  Models'
arxiv_id: '2512.14481'
source_url: https://arxiv.org/abs/2512.14481
tags:
- quantization
- sasq
- training
- factors
- static
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes SASQ (Static Activation Scaling Quantization-aware
  training), a method for efficiently quantizing large language models (LLMs) without
  retraining weights. SASQ optimizes only quantization factors during training, enabling
  static inference with high accuracy and reduced computational overhead compared
  to dynamic quantization methods.
---

# SASQ: Static Activation Scaling for Quantization-Aware Training in Large Language Models

## Quick Facts
- arXiv ID: 2512.14481
- Source URL: https://arxiv.org/abs/2512.14481
- Authors: Shizhuo Mao; Song Chen; Yi Kang
- Reference count: 5
- Primary result: Achieves 5.2% lower perplexity than QuaRot and 4.7% lower than FP16 on WikiText2 for W8A8 quantization of LLaMA2-7B

## Executive Summary
This paper introduces SASQ (Static Activation Scaling for Quantization-aware training), a method for efficiently quantizing large language models without retraining weights. SASQ optimizes only quantization factors during training, enabling static inference with high accuracy and reduced computational overhead compared to dynamic quantization methods. The approach adaptively truncates outliers in activation distributions, improving quantization performance while preserving model capabilities. On LLaMA2-7B, SASQ achieves state-of-the-art results on WikiText2 and demonstrates effectiveness on zero-shot classification tasks while maintaining 87.5% of FP16 accuracy on mathematical reasoning benchmarks through a novel phased quantization mechanism.

## Method Summary
SASQ employs quantization-aware training (QAT) that exclusively optimizes activation quantization factors while keeping pretrained weights frozen. The method uses fake quantization operators with straight-through estimation (STE) to enable gradient flow through discrete operations. Initial quantization factors are computed via post-training quantization (PTQ) from calibration data, then refined through training on WikiText103 using perplexity minimization as the objective. The approach includes adaptive outlier truncation via learned clamp thresholds and a phased quantization mechanism that switches between per-channel static quantization in the prefill phase and per-tensor dynamic quantization during autoregressive generation.

## Key Results
- On LLaMA2-7B, achieves 5.2% lower perplexity than QuaRot and 4.7% lower than FP16 on WikiText2
- Maintains 87.5% of FP16 accuracy on mathematical reasoning benchmarks (AIME2024, MATH-500)
- Outperforms baseline methods on zero-shot classification tasks
- Ablation shows clamp function is critical for performance (perplexity degradation from 5.214→5.467 when removed)

## Why This Works (Mechanism)

### Mechanism 1: Quantization Factor Optimization via Gradient Descent
- Claim: Optimizing only activation quantization factors through QAT yields better static quantization than PTQ-derived factors.
- Mechanism: Differentiable fake quantization operators are inserted into the forward pass. Straight-through estimation (STE) propagates gradients through non-differentiable operations (rounding gradients = 1, clamped value gradients = 0 outside range). Perplexity on validation samples serves as the loss. Only scaling factors $S_x$ are updated; weights remain frozen.
- Core assumption: Pre-trained weights already encode optimal functional relationships; suboptimality arises from misaligned quantization parameters rather than weight distribution.
- Evidence anchors:
  - [abstract] "SASQ exclusively optimizes only the quantization factors (without changing pre-trained weights)"
  - [section 4.2] "SASQ directly incorporates only the numerical values of the quantization factors into the training process, utilizing perplexity minimization as the optimization objective"
  - [corpus] Related work (RoSTE, QWHA) confirms parameter-efficient QAT approaches are actively studied, but SASQ's factor-only approach is distinct.

### Mechanism 2: Adaptive Outlier Truncation via Learned Clamp Thresholds
- Claim: Non-linear transformation of activations through learned scaling + clamp improves quantization by selectively truncating harmful outliers while preserving distributional structure.
- Mechanism: Quantization factors determine the effective clamp threshold: $X_{int} = \text{clamp}(\text{round}(X_{fp}/S), -2^{N-1}, 2^{N-1}-1)$. Smaller $S$ values tighten the clamp range, truncating more outliers. Training discovers factors that balance outlier suppression against information loss.
- Core assumption: Some activation outliers are quantization artifacts rather than semantically essential; selective truncation removes noise while preserving signal.
- Evidence anchors:
  - [abstract] "SASQ adaptively truncates some outliers, thereby reducing the difficulty of quantization while preserving the distributional characteristics"
  - [section 4.3, Table 3] Ablation shows removing clamp ("w/o clamp") degrades PPL from 5.214→5.467 on Wiki2 (LLaMA2-7B), while removing rounding has minimal effect (5.214→5.241)

### Mechanism 3: Phased Quantization for Autoregressive Generation
- Claim: Different generation phases require different quantization granularities to balance accuracy and efficiency.
- Mechanism: Prefill phase uses per-channel static quantization (factors trained on multi-token matrices match large input scale). Generation phase switches to per-tensor dynamic quantization (single-token processing makes per-channel factors suboptimal and computationally expensive). The transition occurs at the EOS token boundary.
- Core assumption: Quantization factors learned on large token batches are ill-suited for single-token autoregressive steps due to distribution shift.
- Evidence anchors:
  - [section 4.4] "continuing per-channel quantization during the generation phase would significantly increase floating-point operations"
  - [section 5.2.2, Table 2] Naive static quantization achieves near-zero accuracy on AIME2024; phased SASQ preserves 87.5%

## Foundational Learning

- **Concept: Straight-Through Estimation (STE)**
  - Why needed here: Enables gradient flow through discrete quantization operations. Understanding why gradients bypass rounding (treated as identity) but not out-of-range clamps is essential for debugging training instability.
  - Quick check question: If all activation values fall outside the quantization range after several epochs, what happens to gradients and how would you detect this?

- **Concept: Quantization Granularity (per-tensor vs. per-channel vs. per-token)**
  - Why needed here: SASQ uses per-channel for weights/activations in prefill and per-tensor dynamic in generation. Understanding the accuracy-overhead tradeoff determines when each is appropriate.
  - Quick check question: Why does per-token dynamic quantization preserve accuracy better than per-tensor static, and what is its computational cost during inference?

- **Concept: Activation Outliers in Transformers**
  - Why needed here: LLaMA-family models exhibit extreme activation outliers that dominate quantization scaling, compressing the majority of values into fewer bits. Understanding this explains why naive static quantization fails catastrophically.
  - Quick check question: If you plot the activation distribution of layer 9 in LLaMA2-7B, what shape would you expect and how does SASQ modify it?

## Architecture Onboarding

- **Component map:** Calibration data -> PTQ initialization -> Fake quantization layer -> Factor optimizer -> Trained scaling factors
- **Critical path:**
  1. Load FP16 model → freeze all weight parameters
  2. Run calibration data through model → collect activation statistics → compute initial $S_x$ via PTQ formula
  3. Wrap linear layers with fake quantization → add $S_x$ to optimizer
  4. Train on WikiText103 (6 epochs, lr=2e-4) → minimize perplexity
  5. Export trained $S_x$ → deploy with static quantization for prefill, dynamic for generation

- **Design tradeoffs:**
  - **Symmetric vs. asymmetric quantization:** Paper uses symmetric (simpler, one fewer parameter) but sacrifices 1-bit range for asymmetric distributions
  - **Global vs. block-wise training:** Global joint optimization guarantees theoretical optimality but requires full-model memory; block-wise is memory-efficient but may yield locally optimal factors that combine poorly
  - **Phased vs. uniform quantization:** Phased preserves autoregressive quality but adds inference complexity and requires EOS detection

- **Failure signatures:**
  - **Training divergence:** Learning rate too high → $S_x$ collapses to near-zero → all activations clamped → zero gradients. Monitor $S_x$ magnitude histograms
  - **No improvement over FP16:** Likely weights were accidentally unfrozen; verify gradient computation excludes weight tensors
  - **Catastrophic generation degradation:** Static quantization used in generation phase for long sequences. Switch to phased mode

- **First 3 experiments:**
  1. **Sanity check:** Run PTQ-only static quantization on LLaMA2-7B with calibration data. Expect PPL >100. This confirms the baseline problem SASQ solves
  2. **Factor-only training ablation:** Train SASQ on WikiText103, then evaluate on held-out WikiText2. Compare full SASQ vs. "w/o clamp" variant to validate the truncation mechanism
  3. **Phased vs. static on generation task:** Apply both to AIME2024 benchmark. Expect near-zero for pure static, ~87% preservation for phased. This validates the autoregressive mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the phased quantization mechanism, which successfully preserved mathematical reasoning capabilities, be generalized to other complex generative tasks like code generation or long-context dialogue without significant accuracy degradation?
- Basis in paper: [explicit] The paper states in Section 4.4: "As an exploratory study, this section aims to provide a preliminary assessment... we propose a phased quantization mechanism... and evaluate it using mathematical reasoning as a case study."
- Why unresolved: The authors validated the phased approach (switching from per-channel static to per-tensor dynamic quantization) specifically on mathematical benchmarks (AIME2024, MATH-500), but did not test it on other autoregressive tasks mentioned as challenges, such as code generation.
- What evidence would resolve it: Experimental results applying the phased SASQ framework to code generation benchmarks (e.g., HumanEval, MBPP) and open-ended dialogue datasets, demonstrating similar retention rates of the FP16 model's capabilities.

### Open Question 2
- Question: What is the theoretical mechanism by which the clamp function in SASQ improves perplexity compared to the FP16 baseline, given that truncation typically implies information loss?
- Basis in paper: [inferred] In Section 5.4, the authors note that "quantized models achieve lower perplexity than their floating-point counterparts" and identify the clamp function as the decisive factor. However, they only "posit that applying low-cost transformations... can potentially enhance model performance" without providing a definitive theoretical proof.
- Why unresolved: While the empirical ablation confirms the clamp function's role, the paper lacks a rigorous theoretical explanation for why adaptive truncation (a non-linear transformation) results in better generative performance than the full-precision distribution.
- What evidence would resolve it: A theoretical analysis or detailed distributional study showing how the learned clamping thresholds regularize the activation space or reduce noise that exists in the FP16 model, accompanied by layer-wise analysis of information flow.

### Open Question 3
- Question: Does the SASQ framework's reliance on training activation scaling factors remain effective for ultra-low-bit quantization (e.g., 4-bit or 2-bit) where outlier sensitivity is significantly higher?
- Basis in paper: [inferred] The paper evaluates SASQ exclusively on W8A8 (8-bit) quantization (Section 5.1). However, the "Related Work" section (Page 2) discusses "ultra-low-bits quantization" as a distinct and challenging area, implying the current SASQ validation is limited to 8-bit precision.
- Why unresolved: It is unclear if optimizing scaling factors alone is sufficient to maintain accuracy when the quantization step size increases drastically (as in 4-bit), or if weight retraining would become necessary at lower bit-widths.
- What evidence would resolve it: Application of the SASQ method to W4A4 or W4A8 configurations on the standard LLaMA benchmarks to observe if the perplexity and zero-shot accuracy remain competitive with state-of-the-art low-bit methods.

## Limitations

- The adaptive truncation mechanism's generalization beyond WikiText103 is unclear, as it was only validated on autoregressive tasks.
- Phased quantization's computational overhead during generation is not quantified, with no FLOPs comparison or latency measurements provided.
- The calibration subset size and sampling strategy from Pile validation set are unspecified, which could affect reproducibility of the initial PTQ factors.

## Confidence

- **High confidence** in the core claim that optimizing only quantization factors through QAT improves static quantization accuracy over PTQ-derived factors, supported by perplexity improvements (5.2% lower than QuaRot, 4.7% lower than FP16 on WikiText2).
- **Medium confidence** in the adaptive outlier truncation mechanism's contribution, as the ablation shows clamp removal degrades performance, but the mechanism's broader applicability beyond autoregressive tasks is uncertain.
- **Medium confidence** in phased quantization's effectiveness for autoregressive tasks, demonstrated by 87.5% accuracy preservation on AIME2024, though the added inference complexity and computational cost are not fully characterized.

## Next Checks

1. **Distribution Shift Validation:** Apply SASQ-trained factors to out-of-domain text (e.g., code, legal documents) and measure perplexity degradation compared to domain-specific retraining. This tests whether learned factors generalize beyond WikiText103.
2. **Computational Overhead Measurement:** Implement phased quantization inference and measure FLOPs and latency during generation phase compared to uniform static quantization. This quantifies the claimed "significant" overhead.
3. **Outlier Sensitivity Analysis:** Systematically vary the clamp threshold (via manual scaling of $S_x$) and measure impact on both perplexity and downstream task accuracy. This reveals whether the learned truncation is optimal or overly aggressive for certain tasks.