---
ver: rpa2
title: Tractable Sharpness-Aware Learning of Probabilistic Circuits
arxiv_id: '2508.05537'
source_url: https://arxiv.org/abs/2508.05537
tags:
- sharpness
- epoch
- data
- regularized
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the first tractable curvature-based regularizer
  for training Probabilistic Circuits (PCs), addressing overfitting in expressive,
  deep architectures. By leveraging the structured DAG of PCs, the authors derive
  exact and efficient expressions for the trace of the Hessian of the log-likelihood,
  a scalar measure of surface curvature.
---

# Tractable Sharpness-Aware Learning of Probabilistic Circuits

## Quick Facts
- arXiv ID: 2508.05537
- Source URL: https://arxiv.org/abs/2508.05537
- Authors: Hrithik Suresh; Sahil Sidheekh; Vishnu Shreeram M. P; Sriraam Natarajan; Narayanan C. Krishnan
- Reference count: 40
- Primary result: Introduces the first tractable curvature-based regularizer for training Probabilistic Circuits, achieving up to 65% reduction in overfitting and 49% improvement in test log-likelihood.

## Executive Summary
This work introduces the first tractable curvature-based regularizer for training Probabilistic Circuits (PCs), addressing overfitting in expressive, deep architectures. By leveraging the structured DAG of PCs, the authors derive exact and efficient expressions for the trace of the Hessian of the log-likelihood, a scalar measure of surface curvature. For tree-structured PCs, they compute the full Hessian in closed form, while for general DAGs, the trace remains efficiently computable. They propose minimizing this trace as a sharpness-aware regularization objective, which translates into a gradient-norm penalty yielding simple quadratic updates for EM and seamless integration with gradient-based methods. Experiments on synthetic and real-world datasets show the regularizer consistently guides PCs toward flatter minima, reducing overfitting—especially in low-data regimes—by up to 65% and improving test log-likelihood by up to 49%.

## Method Summary
The method introduces a sharpness-aware regularization framework for Probabilistic Circuits by computing the trace of the log-likelihood Hessian, which serves as an efficient proxy for surface curvature. The key innovation is showing that for PCs, this trace simplifies to a sum of squared partial derivatives that can be computed exactly using a single forward-backward pass with edge flows. For EM training, the authors reformulate the optimization to avoid cubic equations, yielding a quadratic update rule that's computationally efficient. The regularizer integrates seamlessly with both gradient-based and EM-based training methods, requiring only first-order derivatives while achieving second-order curvature awareness.

## Key Results
- Reduces overfitting by up to 65% in low-data regimes through guidance toward flatter minima
- Improves test log-likelihood by up to 49% compared to standard training
- Achieves exact Hessian trace computation in linear time relative to model parameters and dataset size
- Provides closed-form quadratic updates for EM training, avoiding computational complexity of cubic solutions

## Why This Works (Mechanism)

### Mechanism 1: Exact Hessian Trace Computation
- Claim: Minimizing the trace of the log-likelihood Hessian reduces overfitting by steering optimization toward flatter minima that generalize better.
- Mechanism: The paper shows that the trace of the Hessian of the log-likelihood simplifies to the sum of squared partial derivatives (sum-squared-gradients). This can be computed exactly and efficiently (linear in model parameters and dataset size) using edge flows from a single forward-backward pass. This tractability allows for precise sharpness-aware regularization, unlike in deep neural networks where exact Hessian computation is intractable and approximations are required.
- Core assumption: The trace of the Hessian is a valid proxy for loss-surface sharpness, and flatter minima correlate with better generalization performance.
- Evidence anchors:
  - [abstract] "...we show that the trace of the Hessian of the log-likelihood-a sharpness proxy that is typically intractable in deep neural networks-can be computed efficiently for PCs."
  - [Section "A Tractable Sharpness Regularizer for PCs"] "Thus, the absolute trace... simplifies to the sum of squared partial derivatives... This enables sharpness-aware regularization using only first-order derivatives..."
  - [corpus] The corpus contains work on scaling and representation learning with PCs (e.g., "Scaling Probabilistic Circuits via Data Partitioning", "Tractable Representation Learning with Probabilistic Circuits") and sharpness-aware minimization in other domains (e.g., "LSAM: Asynchronous Distributed Training with Landscape-Smoothed Sharpness-Aware Minimization"), supporting the broader relevance of these concepts. However, no single neighbor paper replicates the exact Hessian trace tractability claim for PCs.
- Break condition: The theoretical derivation is conditional on the specific structure of Probabilistic Circuits (smoothness, decomposability). The correlation between flat minima and generalization is an empirical observation, not a universal guarantee, and is demonstrated in the experiments.

### Mechanism 2: Sharpness-Aware Expectation-Maximization (SA-EM)
- Claim: Integrating the Hessian-trace regularizer into the EM M-step yields a simple, closed-form quadratic update rule for sum-node parameters, avoiding the computational complexity of cubic solutions.
- Mechanism: Directly minimizing the Hessian trace in EM leads to a cubic equation (Proposition 5). However, the authors exploit a key property: for PC parameters, the squared gradient is a monotonic function of the gradient itself. Therefore, minimizing the gradient is a valid surrogate for minimizing its square (the Hessian trace contribution). This reformulation yields a simpler quadratic equation (Theorem 2) with a unique, non-negative closed-form solution that can be easily integrated into the M-step.
- Core assumption: The surrogate objective (gradient-norm minimization) is sufficiently equivalent to the original Hessian-trace minimization for effective regularization.
- Evidence anchors:
  - [Section "Sharpness-Aware EM"] "...we can reformulate this objective into an equivalent gradient norm minimization problem, resulting in a quadratic equation with closed-form parameter updates."
  - [Proof of Theorem 2 in Appendix] "Forming the Lagrangian and applying KKT conditions leads to the quadratic equation... which is nonnegative... Thus, complex roots cannot occur."
  - [corpus] Weak or missing. The corpus does not contain papers on EM for PCs with sharpness regularization. This is a novel contribution of the paper.
- Break condition: The equivalence is formally proven in the paper (Theorem 2). The practical effectiveness of the quadratic update is validated experimentally against standard EM (Table 2, Appendix Table 4).

### Mechanism 3: Regularization Strength and Overfitting Reduction
- Claim: A small regularization strength (μ) is sufficient to capture most generalization gains in low-data regimes, reducing overfitting significantly without causing severe underfitting.
- Mechanism: The regularizer penalizes the Hessian trace, which is higher at sharp minima. In low-data settings, standard training tends to converge to these sharp minima, leading to overfitting. The ablation study on μ (Figure 4) shows that even a small penalty pushes parameters away from these sharp regions toward flatter, more robust solutions. As data increases, overfitting naturally decreases, and the benefits of the regularizer plateau.
- Core assumption: The optimal regularization strength (μ) can be determined via validation performance, and the relationship between μ, sharpness, and generalization follows the expected trend.
- Evidence anchors:
  - [abstract] "Experiments on synthetic and real-world datasets show that the regularizer consistently guides PCs toward flatter minima, reducing overfitting—especially in low-data regimes..."
  - [Section "Experiments and Results", Q4] "We observe that even a small μ ∈ (0, 0.1] is sufficient to capture most of the gains-reducing overfitting and curvature."
  - [corpus] Weak or missing. The specific interaction of μ with overfitting in PCs is a key empirical finding of this work.
- Break condition: The claim is conditional on the experimental results (Figures 3, 4, 6-9, 14-17). The mechanism's effectiveness is empirically demonstrated but not mathematically guaranteed for all datasets or PC architectures.

## Foundational Learning

- Concept: **Sharpness-Aware Minimization (SAM)**.
  - Why needed here: To understand the paper's core motivation. The authors adapt the SAM principle from deep learning (which uses approximations) to PCs, where they can compute the sharpness proxy (Hessian trace) exactly.
  - Quick check question: What is the primary goal of SAM? (Answer: To improve generalization by finding parameter regions with low curvature, not just low loss.)

- Concept: **Hessian Matrix and its Trace**.
  - Why needed here: The Hessian trace is the central technical tool. You must grasp that it quantifies local curvature and that its tractability in PCs is the key insight enabling the method.
  - Quick check question: Why is the Hessian trace a suitable scalar measure of sharpness? (Answer: It sums the curvature along all principal axes; a larger value indicates a sharper, narrower minimum.)

- Concept: **Probabilistic Circuits (PCs) Structure**.
  - Why needed here: The method's efficiency relies entirely on the specific DAG structure of PCs (sum and product nodes, circuit flow). You need to understand this structure to follow the forward-backward pass and flow-based computations.
  - Quick check question: What are the two main internal node types in a PC, and what mathematical operations do they represent? (Answer: Sum nodes compute weighted mixtures; product nodes compute factorizations.)

## Architecture Onboarding

- Component map: Forward Pass -> Backward Pass (Flow Computation) -> Hessian Trace Calculator -> Gradient-Based Trainer/SA-EM Trainer
- Critical path:
    1. Implement standard PC forward/backward passes. This is the foundation.
    2. Implement flow-based Hessian trace computation (Algorithm 1). Validate against autograd on a small network.
    3. Implement the gradient-norm-based M-step (Theorem 2) for EM. This is a self-contained, per-node algebraic update.
    4. Integrate the trace term as a regularizer into your existing gradient-based loss function.
- Design tradeoffs:
    - Tree-structured vs. DAG PCs: The full Hessian is tractable for trees, but only the trace is tractable for general DAGs. The paper focuses on the trace for broad applicability. (Section "Hessian for General (DAG-Structured) PCs").
    - Cubic vs. Quadratic M-step: Solving the cubic from directly minimizing the trace (Proposition 5) is unstable and costly. The quadratic update from gradient-norm minimization (Theorem 2) is the preferred, efficient implementation.
    - Fixed vs. Adaptive μ: A fixed μ is simpler, but an adaptive schedule based on the degree of overfitting can dynamically balance the regularization strength.
- Failure signatures:
    - Exploding NaNs in M-step: Likely due to taking the square root of a negative number in the quadratic solution (the discriminant). This should be mathematically impossible given non-negative flows, so check for numerical issues in flow computation.
    - No generalization improvement: The regularization weight μ may be too low. Consult the ablation (Figure 4) and tune it up on a validation set.
    - Excessive underfitting: The regularization weight μ may be too high, pushing the model away from the optimal likelihood. Reduce μ or use an adaptive schedule.
- First 3 experiments:
  1. Validate Trace Computation: On a small PC, compare the Hessian trace from your flow-based implementation against PyTorch's `autograd` Hessian trace. They should match exactly. (Figure 3[left]).
  2. Low-Data Generalization Test: Train a PC (e.g., EinsumNet) on a small fraction (1-5%) of a synthetic dataset (e.g., Spiral) with and without the regularizer (λ > 0). Plot train vs. validation NLL and sharpness (Hessian trace) over epochs. Expect the regularized model to have a smaller generalization gap and lower sharpness. (Figure 6).
  3. SA-EM vs. EM on Real Data: Train an HCLT using PyJuice with standard EM and your SA-EM (quadratic M-step) on a binary density estimation benchmark (e.g., DNA) with limited data. Compare final test NLL and degree of overfitting. Expect SA-EM to show improvement, particularly in low-data regimes. (Table 2).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do asymmetric valleys analogous to those in deep neural networks exist in the log-likelihood landscape of Probabilistic Circuits?
- Basis in paper: [explicit] The Conclusion explicitly proposes "investigating the log-likelihood landscape to identify the presence of asymmetric valleys analogous to those observed in DNNs."
- Why unresolved: The current work characterizes sharpness using the trace of the Hessian (a scalar average of curvature), which quantifies flatness but does not capture the directional asymmetry of the loss surface.
- What evidence would resolve it: Empirical visualizations or theoretical derivations of the loss landscape that reveal non-uniform steepness on opposing sides of the optima in the PC parameter space.

### Open Question 2
- Question: Can a formal theoretical framework be developed to guarantee convergence in over-parameterized PCs when using sharpness-aware objectives?
- Basis in paper: [explicit] The Conclusion lists "developing a theoretical framework for understanding convergence in over-parameterized PCs" as a primary direction for future work.
- Why unresolved: The paper demonstrates empirical convergence to flat minima and improved generalization but lacks formal bounds or proofs regarding optimization dynamics in high-capacity regimes.
- What evidence would resolve it: A theoretical proof establishing conditions under which the proposed gradient-norm penalty ensures convergence to optimal or near-optimal minima for expressive PC structures.

### Open Question 3
- Question: Is it possible to efficiently compute or approximate off-diagonal Hessian entries for general DAG-structured PCs to enable full second-order optimization?
- Basis in paper: [inferred] The authors note on Page 5 that "computing the off-diagonal entries of the Hessian is intractable" for general DAGs due to the exponential growth of dependency paths.
- Why unresolved: The method currently utilizes only the Hessian trace; deriving the full Hessian is restricted to tree-structured PCs, limiting the complexity of possible optimization strategies for general architectures.
- What evidence would resolve it: A polynomial-time algorithm for approximating off-diagonal elements in dense DAGs, or a demonstration that trace-only information is sufficient for tasks typically requiring the full Hessian, such as Natural Gradient Descent.

## Limitations
- The method's efficacy depends on PC-specific properties (smoothness, decomposability) that may not generalize to other tractable probabilistic models.
- The optimal regularization strength μ determination currently requires validation-based tuning, and the adaptive scheduling approach is proposed but not extensively validated across diverse scenarios.
- The quadratic M-step reformulation relies on a property specific to PC parameters that may not extend to all models.

## Confidence

**High Confidence**: The tractability claims (exact Hessian trace computation, quadratic M-step updates) are mathematically proven and experimentally validated on synthetic and real datasets. The computational complexity analysis (linear in parameters and data) is sound given the flow-based implementation.

**Medium Confidence**: The generalization claims rely on empirical correlation between flat minima and better performance, which holds in the tested domains but may vary with dataset characteristics or PC architectures beyond those evaluated.

**Low Confidence**: The optimal regularization strength μ determination currently requires validation-based tuning, and the adaptive scheduling approach is proposed but not extensively validated across diverse scenarios.

## Next Checks

1. Test the method on PC architectures with different structural constraints (e.g., structured decomposability) to verify the trace computation remains tractable and effective.

2. Conduct systematic ablation studies across a broader range of regularization strengths μ and dataset sizes to map the boundary conditions where benefits plateau or reverse.

3. Evaluate whether the gradient-norm surrogate in SA-EM maintains its equivalence when extended to more complex PC variants (e.g., those with additional constraint types or parameter sharing).