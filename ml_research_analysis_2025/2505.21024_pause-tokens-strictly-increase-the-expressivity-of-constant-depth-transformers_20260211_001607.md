---
ver: rpa2
title: Pause Tokens Strictly Increase the Expressivity of Constant-Depth Transformers
arxiv_id: '2505.21024'
source_url: https://arxiv.org/abs/2505.21024
tags:
- tokens
- transformer
- pause
- circuit
- precision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides the first formal theoretical analysis of how
  pause tokens affect Transformer expressivity. The authors prove that constant-depth
  Transformers with pause tokens achieve strict computational separation: those without
  pause tokens can only compute a strict subset of AC0 functions, while those with
  polynomial pause tokens can compute all of AC0; logarithmic-precision Transformers
  with pause tokens achieve TC0 expressivity.'
---

# Pause Tokens Strictly Increase the Expressivity of Constant-Depth Transformers

## Quick Facts
- **arXiv ID:** 2505.21024
- **Source URL:** https://arxiv.org/abs/2505.21024
- **Authors:** Charles London; Varun Kanade
- **Reference count:** 40
- **Primary result:** Constant-depth Transformers with polynomial pause tokens achieve AC⁰ expressivity; with logarithmic precision, they achieve TC⁰ expressivity.

## Executive Summary
This paper provides the first formal theoretical analysis of how pause tokens affect Transformer expressivity. The authors prove that constant-depth Transformers with pause tokens achieve strict computational separation: those without pause tokens can only compute a strict subset of AC⁰ functions, while those with polynomial pause tokens can compute all of AC⁰; logarithmic-precision Transformers with pause tokens achieve TC⁰ expressivity. Empirically, they show that two-layer causally masked Transformers cannot learn parity without pause tokens but can learn it when pause tokens are added, demonstrating practical benefits. These results explain prior empirical findings about pause token effectiveness and position them as a distinct mechanism for enhancing Transformer reasoning, complementary to chain-of-thought prompting.

## Method Summary
The paper analyzes Transformers augmented with pause tokens—extra tokens that can be attended to by all subsequent tokens. The theoretical framework proves that bounded-precision Transformers without pause tokens compute only a strict subset of AC⁰ functions, while adding polynomial pause tokens enables full AC⁰ expressivity. With logarithmic precision, adding pause tokens achieves TC⁰ expressivity. Empirically, they train 2-layer causally masked Transformers on parity tasks with 0, n, and 2n pause tokens, using auxiliary supervision (threshold hints) to guide pause token usage. The architecture uses learned positional encodings, 4 attention heads, and 32-dimensional hidden states.

## Key Results
- Constant-precision Transformers without pause tokens compute only a strict subset of AC⁰ functions
- Adding polynomial pause tokens enables full AC⁰ expressivity via circuit simulation
- Logarithmic-precision Transformers with pause tokens achieve TC⁰ expressivity
- Empirically, 2-layer causal Transformers learn parity with pause tokens but fail without them

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pause tokens serve as intermediate computational units that expand the parallel computational width available to fixed-depth Transformers.
- **Mechanism:** Each pause token encodes a circuit element (gate, edge, or argument). Positional encodings specify connectivity patterns. Two Transformer layers simulate one circuit layer: the first attention layer copies previously computed values to argument positions; the second aggregates inputs and computes gate outputs via the feedforward network. With polynomial pause tokens, the Transformer can represent arbitrary AC⁰ circuits.
- **Core assumption:** Saturation arithmetic (clipping to representable range) allows attention over polynomial-length sequences without overflow, which is a reasonable approximation of quantized neural networks but not universal.
- **Evidence anchors:**
  - [abstract] "With bounded-precision activations, Transformers without pause tokens compute only a strict subset of AC⁰ functions, while adding a polynomial number of pause tokens allows them to express the entire class."
  - [Section 4.1] Figure 1 and proof construction showing how tokens represent circuit vertices and edges.
  - [corpus] "The Counting Power of Transformers" discusses related counting properties but does not directly validate pause tokens; corpus evidence for this mechanism is weak.
- **Break condition:** If modular arithmetic replaces saturation arithmetic, the equivalence breaks because modular addition is not in AC⁰ (Transformer would belong to ACC⁰ instead).

### Mechanism 2
- **Claim:** Causal masking creates a bottleneck for global information aggregation that pause tokens partially alleviate by providing dedicated workspace positions.
- **Mechanism:** In causally masked Transformers, only the final token can attend to the entire sequence. Pause tokens, positioned after the input, can each attend globally and serve as parallel aggregators. This enables simulation of circuit structures requiring n threshold gates in the first layer (e.g., parity circuits).
- **Core assumption:** The task benefits from parallelizable computation patterns rather than strictly serial reasoning; pause tokens cannot compensate for problems requiring sequential depth.
- **Evidence anchors:**
  - [abstract] "Empirically, we demonstrate that two-layer causally masked Transformers can learn parity when supplied with pause tokens, a function that they appear unable to learn without them."
  - [Section 5] Figure 2 shows causal+no-pause drops to chance at ~100 bits, while pause tokens sustain accuracy.
  - [corpus] "Change of Thought: Adaptive Test-Time Computation" notes Transformers in fixed-depth pass are limited to TC⁰, consistent with these bounds.
- **Break condition:** Removing causal masking also improves performance (paper shows ~200-bit threshold), indicating pause tokens are not the only solution to the bottleneck.

### Mechanism 3
- **Claim:** Logarithmic (vs. constant) numeric precision enables threshold-like computations, upgrading expressivity from AC⁰ to TC⁰.
- **Mechanism:** With logarithmic precision, attention layers can average over polynomial-length sequences without saturation clipping, and feedforward networks can implement exact threshold comparisons. This allows simulation of TC⁰ circuits (threshold gates) rather than just AC⁰ (AND/OR/NOT).
- **Core assumption:** Feedforward networks retain sufficient precision to implement thresholding even if weights are quantized.
- **Evidence anchors:**
  - [abstract] "For logarithmic-precision Transformers, we show that adding pause tokens achieves expressivity equivalent to TC⁰."
  - [Section 4.2] Theorem 4.5 and proof construction leveraging logarithmic-precision summation.
  - [corpus] "Probability Distributions Computed by Hard-Attention Transformers" characterizes distributional power but does not directly address precision thresholds.
- **Break condition:** If activations are quantized to constant precision while only weights retain logarithmic precision, expressivity may degrade unless feedforward thresholding is preserved.

## Foundational Learning

- **Concept: Circuit complexity classes (AC⁰, TC⁰)**
  - **Why needed here:** The paper's entire theoretical framework maps Transformer expressivity to these classes. AC⁰ = constant-depth AND/OR/NOT circuits; TC⁰ adds threshold gates. Known that AC⁰ ⊊ TC⁰ (parity separates them).
  - **Quick check question:** Can an AC⁰ circuit compute parity of n bits?

- **Concept: Fixed-point arithmetic with saturation**
  - **Why needed here:** The precision regime (constant vs. logarithmic) determines which circuit class the Transformer can simulate. Saturation arithmetic (clipping overflow) is assumed throughout proofs.
  - **Quick check question:** In 8-bit fixed-point with saturation, what happens when you sum 1000 values of 1.0?

- **Concept: Logspace uniformity**
  - **Why needed here:** Ensures circuit/Transformer families are systematically constructible rather than hard-coded per input size. Required for meaningful uniform separations.
  - **Quick check question:** Why can non-uniform circuit families recognize undecidable languages?

## Architecture Onboarding

- **Component map:**
  - Input tokens: n task tokens + b(n) pause tokens (pause tokens initialized to 0 or fixed value)
  - Positional encodings: logspace-computable, encode circuit structure or task-relative positions
  - Attention: Standard multi-head self-attention with optional causal mask (M matrix)
  - Feedforward: Position-wise network implementing threshold/logical operations
  - Output: Read from final token position

- **Critical path:**
  1. Design positional encoding scheme that encodes desired computation structure
  2. Allocate pause tokens proportional to circuit complexity (polynomial for AC⁰, quasi-polynomial for fixed-depth uniform separations)
  3. Train with auxiliary supervision on pause token positions (paper uses threshold hints for parity)

- **Design tradeoffs:**
  - More pause tokens → more expressivity but longer sequences and compute
  - Constant precision → AC⁰, requires careful saturation handling; Logarithmic precision → TC⁰, more practical
  - Causal masking → realistic for LLMs but restricts global aggregation; removing it helps but may not match pause token gains

- **Failure signatures:**
  - Training without hints on pause tokens: models fail to learn to use them (gradient-based learning struggles)
  - Insufficient pause tokens for task complexity: no expressivity gain
  - Using pause tokens for serial-reasoning tasks: no benefit (pause tokens increase parallel width, not depth)

- **First 3 experiments:**
  1. **Parity task with controlled pause tokens:** Train 2-layer causal Transformer on parity with 0, n, 2n pause tokens; measure length generalization. Expect collapse without pause tokens, sustained accuracy with them.
  2. **Ablate auxiliary supervision:** Compare training with vs. without threshold hints on pause tokens. Expect failure without hints.
  3. **Precision sweep:** Compare constant (4-bit) vs. logarithmic (16-bit) precision on a TC⁰ task (e.g., majority). Expect constant precision to fail regardless of pause tokens.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can TF[L, L, 0] (logarithmic-precision Transformers without pause tokens) be strictly separated from TF[L, L, P] (with polynomial pause tokens)?
- **Basis in paper:** [explicit] Section 7 states: "While we prove that TF[L, L, P] = TC0, we are unable to show that TF[L, L,0] ⊊ TF[L, L, P]. This is primarily due to the lack of known superpolynomial lower bounds for TC0 circuits."
- **Why unresolved:** Proving separation requires superpolynomial TC0 lower bounds, a long-standing open problem in circuit complexity.
- **What evidence would resolve it:** Establishing superpolynomial lower bounds for TC0 circuits, or alternatively, deriving Transformer-specific arguments that bypass circuit complexity barriers.

### Open Question 2
- **Question:** Can gradient-based training access the full expressivity benefits of pause tokens, or is explicit parallelizable supervision necessary?
- **Basis in paper:** [explicit] Section 7: "Our work focuses entirely on what Transformers with pause tokens can compute, but provides no theoretical results on whether they can learn to use this additional expressivity effectively."
- **Why unresolved:** The paper proves computational equivalence but does not address learnability; experiments required hint labels for parity learning.
- **What evidence would resolve it:** Theoretical analysis of whether gradient descent can find parameters utilizing pause tokens, or empirical demonstrations of learning without task-specific supervision.

### Open Question 3
- **Question:** How does Transformer expressivity with pause tokens change under different numerical assumptions, such as modular arithmetic instead of saturation arithmetic?
- **Basis in paper:** [explicit] Section 7 notes that with modular arithmetic, "this breaks the equivalence as modular addition is not in AC0 (in this case the Transformer belongs to ACC0)."
- **Why unresolved:** The choice of arithmetic model affects expressivity, but only saturation arithmetic is analyzed in depth.
- **What evidence would resolve it:** Formal analysis proving which circuit class Transformers with modular arithmetic and pause tokens can simulate.

### Open Question 4
- **Question:** Why do pause tokens provide empirical benefits for parity learning beyond simply removing causal masking?
- **Basis in paper:** [inferred] Figure 2 shows pause tokens outperform non-causal Transformers without pause tokens, suggesting benefits beyond providing tokens for global information aggregation.
- **Why unresolved:** The paper hypothesizes pause tokens act as "workspace" positions simplifying attention routing but does not formally characterize this advantage.
- **What evidence would resolve it:** Analysis of learned attention patterns with versus without pause tokens, or theoretical bounds on attention complexity for routing computations.

## Limitations

- The AC⁰→AC⁰+pause separation depends critically on saturation arithmetic assumptions that may not hold in modern Transformers with normalization layers
- The empirical results are demonstrated only on parity learning with a specific 2-layer architecture and small hidden dimensions
- The necessity of auxiliary supervision for pause token training is a significant practical limitation
- The paper does not address learnability—whether gradient-based training can access the full expressivity benefits

## Confidence

**High confidence** in the theoretical separation proofs: The AC⁰ and TC⁰ expressivity results follow established circuit complexity techniques and are mathematically rigorous. The gap between constant-precision (AC⁰) and logarithmic-precision (TC⁰) Transformers with pause tokens is well-founded.

**Medium confidence** in practical applicability: The empirical parity learning results are clear, but the necessity of auxiliary supervision for pause token training is a significant practical limitation. The 2-layer, 32-dim architecture is quite small compared to modern LLMs, raising questions about scaling.

**Medium confidence** in the causal masking bottleneck claim: While the empirical results support the claim, the observation that removing causal masking also improves performance suggests pause tokens are one of multiple solutions rather than a unique mechanism.

## Next Checks

1. **Ablation of auxiliary supervision:** Systematically compare training with and without threshold hints on pause tokens across multiple random seeds and task lengths. Quantify the exact degradation in learning curves and test whether alternative training schemes (reinforcement learning, auxiliary losses without explicit thresholds) can recover performance.

2. **Cross-task generalization:** Test pause token effectiveness on other TC⁰-complete problems beyond parity, such as majority function, sorting networks, or graph connectivity problems. This would validate whether the expressivity gains translate to practical problem-solving beyond the parity example.

3. **Precision-robustness evaluation:** Implement experiments with mixed precision (logarithmic weights but constant activation precision) and evaluate whether the TC⁰ expressivity is preserved. Additionally, test whether normalization layers that prevent pure saturation behavior degrade the AC⁰→AC⁰+pause separation.