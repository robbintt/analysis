---
ver: rpa2
title: 'Beyond Random Sampling: Efficient Language Model Pretraining via Curriculum
  Learning'
arxiv_id: '2506.11300'
source_url: https://arxiv.org/abs/2506.11300
tags:
- training
- learning
- tokens
- data
- curriculum
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work presents the first systematic study of curriculum learning\
  \ for large language model pretraining. It evaluates three curriculum strategies\u2014\
  vanilla ordering, pacing-based sampling, and interleaved curricula\u2014across six\
  \ difficulty metrics computed on a fixed-quality dataset."
---

# Beyond Random Sampling: Efficient Language Model Pretraining via Curriculum Learning

## Quick Facts
- arXiv ID: 2506.11300
- Source URL: https://arxiv.org/abs/2506.11300
- Reference count: 40
- Primary result: Curriculum learning accelerates early-to-mid-stage convergence by 18–45% and yields up to 3.5% final performance gains when used as warmup

## Executive Summary
This work presents the first systematic study of curriculum learning for large language model pretraining. It evaluates three curriculum strategies—vanilla ordering, pacing-based sampling, and interleaved curricula—across six difficulty metrics computed on a fixed-quality dataset. Training up to 100B tokens, the experiments show that curriculum learning accelerates early-to-mid-stage convergence by 18–45% and, when used as warmup, yields sustained gains of up to 3.5% in final performance. The most effective metrics are compression ratio, MTLD, and Flesch Reading Ease, which reflect linguistic richness and readability. The approach incurs negligible computational overhead, as metrics are lightweight and model-independent, and operates orthogonally to data selection or pruning methods. Results hold across model scales (0.5B to 3B parameters) and suggest curriculum learning is a practical, low-cost mechanism to improve pretraining efficiency and generalization.

## Method Summary
The paper investigates curriculum learning for LLM pretraining by ordering data from easy to hard based on six difficulty metrics: Compression Ratio, Fertility, Flesch Reading Ease, MTLD, Number of Tokens, and Perplexity. The CulturaX English dataset is split into 10 difficulty groups. Three strategies are evaluated: (1) vanilla—strict easy-to-hard ordering, (2) pacing—linear/quadratic/inverse-quadratic across 10 groups, and (3) interleaved—10 segments each with linear pacing within. A warmup variant trains with CL for an initial phase then switches to random sampling. Models use LLaMA3.2-style decoder-only architecture with AdamW optimization and fixed LR=2e-4, trained up to 100B tokens.

## Key Results
- Curriculum learning consistently improves convergence in early and mid-training phases, reducing training steps by 18-45%
- Compression Ratio, Fertility, and MTLD metrics tied to linguistic richness and redundancy benefit most from linear pacing
- Flesch Reading Ease benefits most from quadratic pacing, yielding 1.6% above baseline at final checkpoint
- CL-based warmup yields lasting gains when used as a warmup strategy, with up to 3.5% improvement in final performance
- The approach incurs negligible computational overhead and operates orthogonally to data selection or pruning methods

## Why This Works (Mechanism)

### Mechanism 1: Progressive Complexity Buildup
Early-stage training on lower-difficulty samples accelerates initial convergence by providing lower-variance gradients and simpler pattern regularities, allowing the model to form stable intermediate representations before encountering noisier, high-complexity data.

### Mechanism 2: Pacing-Induced Generalization
Gradual difficulty transitions with intra-group diversity improve mid-training generalization over strict ordering. Pacing functions (linear, quadratic) allow smooth distribution shifts rather than abrupt jumps, while intra-group random sampling preserves diversity.

### Mechanism 3: Warmup-Induced Representation Persistence
Initial CL training positions the model in a flatter, more generalizable region of the loss landscape. When random training continues, the model retains this initialization advantage, converging to a higher final performance rather than regressing to the baseline.

## Foundational Learning

- **Curriculum Learning (CL)**: The paradigm of ordering training data from easy to hard to mimic structured learning. Why needed: The entire method builds on CL to accelerate pretraining efficiency. Quick check: Can you explain why CL might help early-stage convergence but not necessarily change the final convergence point if the full dataset is eventually seen?

- **Difficulty Metrics for Text**: Compression Ratio, MTLD, and Flesch Reading Ease measure text complexity through information density, lexical diversity, and readability. Why needed: The choice of difficulty metric determines what "easy" vs. "hard" means. Quick check: Why might compression ratio be a better difficulty signal than perplexity?

- **Pacing Functions**: Linear, quadratic, and inverse quadratic functions govern the rate at which harder data is introduced. Why needed: These modulate the curriculum's intensity and smoothness of transitions. Quick check: When would inverse quadratic pacing be preferred over quadratic pacing?

## Architecture Onboarding

- **Component map**: Data preprocessing pipeline -> Curriculum scheduler -> Training loop -> Evaluation harness
- **Critical path**: 1) Select difficulty metric (compression ratio, MTLD, or Flesch Reading Ease), 2) Compute metrics and partition data into 10 difficulty groups, 3) Choose strategy (vanilla, pacing, or interleaved), 4) Train with CL for initial phase then switch to random sampling for warmup strategy, 5) Evaluate at checkpoints
- **Design tradeoffs**: Vanilla CL is simpler but may cause abrupt jumps; pacing smooths transitions but requires tuning; fewer groups → coarser transitions, more groups → diminishing returns
- **Failure signatures**: Perplexity-based ordering shows late-stage degradation (noisy samples), no improvement over baseline (metric-group mismatch), performance drops after CL-to-random transition (warmup too short)
- **First 3 experiments**: 1) Baseline comparison: Train 0.5B model on 10B tokens with random shuffling, 2) Vanilla CL with MTLD: Order data by MTLD and compare convergence, 3) CL warmup + random continuation: Train with MTLD-ordered data for 10B tokens then random for another 10B

## Open Questions the Paper Calls Out

### Open Question 1
Can adaptive or model-aware curricula that respond to a model's evolving capabilities yield further efficiency gains over static, precomputed difficulty scores? The authors note they "do not explore adaptive or dynamic curricula" and suggest future work should explore adaptive and model-aware curricula.

### Open Question 2
How does curriculum learning during pretraining influence performance and sample efficiency in subsequent downstream fine-tuning regimes? The limitations section states the work "does not examine the impact of curriculum learning on downstream fine-tuning" and identifies this as an important direction.

### Open Question 3
Do the observed benefits of curriculum learning generalize to encoder-based architectures and multilingual pretraining contexts? The authors list the restriction to "decoder-only architectures and English-language data" as a primary limitation regarding generalizability.

## Limitations
- Conclusions based on a single curated English dataset (CulturaX subset), limiting generalizability to other domains or languages
- Analysis focuses on 0.5B, 1.5B, and 3B parameter models, leaving scaling behavior to trillion-parameter models untested
- Several key design choices—such as optimal number of difficulty groups and minimal effective warmup duration—are not systematically explored

## Confidence

- **High confidence**: Early-to-mid-stage convergence acceleration (18-45%) is well-supported by consistent results across multiple metrics and model scales
- **Medium confidence**: Sustained performance gains from CL-based warmup (up to 3.5%) are demonstrated, but minimal effective warmup duration and long-term stability beyond 100B tokens are not established
- **Low confidence**: Claims about generality of pacing function choices (linear vs. quadratic) are based on limited experimentation and may not transfer to other datasets or model architectures

## Next Checks

1. **Dataset Generalization Test**: Repeat curriculum learning experiments on a non-English corpus to verify identified difficulty metrics and pacing strategies remain effective
2. **Scaling Behavior Analysis**: Train a 7B or 13B parameter model with same curriculum strategies to assess whether convergence acceleration scales proportionally with model size
3. **Dynamic Difficulty Adjustment**: Implement periodic recomputation of difficulty metrics during training and compare performance to static metric baseline