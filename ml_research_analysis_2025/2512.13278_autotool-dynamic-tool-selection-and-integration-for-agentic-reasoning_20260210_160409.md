---
ver: rpa2
title: 'AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning'
arxiv_id: '2512.13278'
source_url: https://arxiv.org/abs/2512.13278
tags:
- tool
- arxiv
- reasoning
- tools
- tool-selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AutoTool, a framework that enables large
  language models to dynamically select and integrate external tools during reasoning
  trajectories. Unlike prior approaches that rely on fixed, predefined toolsets, AutoTool
  trains agents to adaptively choose from a diverse and evolving set of over 1,000
  tools.
---

# AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning

## Quick Facts
- **arXiv ID:** 2512.13278
- **Source URL:** https://arxiv.org/abs/2512.13278
- **Reference count:** 32
- **Primary result:** Dynamic tool selection improves performance by up to 7.7% on code generation and 6.9% on multimodal understanding

## Executive Summary
AutoTool introduces a framework enabling large language models to dynamically select and integrate external tools during reasoning trajectories. Unlike prior approaches that rely on fixed, predefined toolsets, AutoTool trains agents to adaptively choose from a diverse and evolving set of over 1,000 tools. The method uses a dual-phase optimization pipeline: first, supervised fine-tuning and reinforcement learning to stabilize long chain-of-thought reasoning, then KL-regularized Plackett-Luce ranking to refine multi-step tool selection. Experiments across ten benchmarks show AutoTool improves performance by up to 7.7% on code generation and 6.9% on multimodal understanding compared to state-of-the-art baselines, while generalizing effectively to unseen tools during inference.

## Method Summary
AutoTool employs a dual-phase optimization pipeline to enable dynamic tool selection during agentic reasoning. Phase I applies supervised fine-tuning followed by reinforcement learning to stabilize long chain-of-thought generation with tool integration. Phase II then refines multi-step tool selection using KL-regularized Plackett-Luce ranking, optimizing only the selection segments while keeping reasoning and tool-execution steps frozen. The framework uses embedding-anchored selection, where tools are chosen based on distance distributions in a shared embedding space, enabling generalization to unseen tools without retraining.

## Key Results
- AutoTool achieves 6.4% gains on math and science benchmarks (AIME24/25, GPQA-Diamond)
- 4.5% improvement on search QA tasks (HotpotQA, 2Wiki, Bamboogle)
- 7.7% boost on code generation and 6.9% on multimodal understanding tasks

## Why This Works (Mechanism)

### Mechanism 1: Embedding-Anchored Tool Selection for Unseen Tool Generalization
Embedding-anchored selection enables generalization to unseen tools by operating in a representation space rather than memorizing tool identifiers. During trajectory generation, the LLM produces a selection rationale followed by an anchor token with a predicted embedding vector. Tools are selected by computing softmax-normalized distances between the predicted embedding and tool embeddings. New tools can be added without retraining the selection policy.

### Mechanism 2: Dual-Phase Optimization Separating Trajectory Stabilization from Selection Refinement
A two-phase pipeline prevents collapse between reasoning and tool-selection learning objectives. Phase I applies SFT followed by RL to stabilize long CoT generation with tool integration. Phase II freezes reasoning and tool-execution steps, optimizing only tool-selection segments via KL-regularized PL ranking. Non-selection components are masked during Phase II loss computation.

### Mechanism 3: Plackett-Luce Ranking as Policy Optimization Surrogate
PL ranking converts trajectory-level rewards into a differentiable distribution optimizable via cross-entropy loss instead of direct policy gradient. For N sampled trajectories per query, rewards induce a PL distribution. Proposition 3.1 proves the equivalence between optimal policy and matching PL distributions, permitting tractable training via CE loss.

## Foundational Learning

- **Concept:** Chain-of-Thought (CoT) Interleaved with Tool Use
  - **Why needed here:** AutoTool's trajectories alternate between reasoning, tool selection, and tool execution; understanding this interleaving pattern is prerequisite to grasping the training pipeline.
  - **Quick check question:** Given a math problem requiring web lookup followed by computation, can you sketch a trajectory with τ_reason, τ_select, τ_integrate segments?

- **Concept:** KL-Regularized Reinforcement Learning
  - **Why needed here:** Phase II uses KL divergence to constrain policy updates, preventing excessive deviation from Phase I.
  - **Quick check question:** Why would unconstrained RL over tool selection risk degrading previously learned reasoning capabilities?

- **Concept:** Ranking Losses (Plackett-Luce)
  - **Why needed here:** Understanding how ranking-based objectives differ from policy gradient methods explains Phase II training stability.
  - **Quick check question:** In PL ranking, if trajectory A has reward 0.9 and B has reward 0.1, what is the probability A is ranked first in a two-item permutation?

## Architecture Onboarding

- **Component map:** Data Curation Pipeline → Phase I Training (SFT → GRPO) → Phase II Training (PL ranking) → Inference Engine (embedding-anchored selection)
- **Critical path:** Phase I stability → Phase II selection refinement → Embedding-anchored inference. Failure at Phase I cascades to Phase II; embedding quality at inference determines unseen-tool generalization.
- **Design tradeoffs:**
  - Fixed training toolset (460) vs. evolving inference toolset (1,346): Ensures stable embedding learning but requires generalization at test time.
  - Masked Phase II loss: Focuses optimization on selection but assumes reasoning quality is already sufficient.
  - Rollout size N=8: Balances ranking resolution against computational cost.
- **Failure signatures:**
  - Selection collapse: Model always picks the same tool → check Phase II reward variance.
  - Unseen-tool failure: Sharp performance drop with 886 unseen tools → embedding alignment issue.
  - Reasoning degradation after Phase II: Phase I benchmarks regress → KL weight β may be too low.
- **First 3 experiments:**
  1. Ablate Phase II: Train only Phase I (SFT + GRPO) and compare against full AutoTool on AIME24 and HotpotQA.
  2. Unseen-tool stress test: Run