---
ver: rpa2
title: Comparison of Scoring Rationales Between Large Language Models and Human Raters
arxiv_id: '2509.23412'
source_url: https://arxiv.org/abs/2509.23412
tags:
- scoring
- human
- llms
- similarity
- rationales
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares scoring rationales between human raters and
  large language models (LLMs) for Chinese language essay assessments. Using cosine
  similarity, quadratic weighted kappa, and normalized mutual information, it finds
  that GPT-4o and Claude 3.5 Sonnet show the highest agreement with human scoring,
  with GPT-4o achieving a QWK of 0.8646 and 0.8976 for two human raters.
---

# Comparison of Scoring Rationales Between Large Language Models and Human Raters

## Quick Facts
- arXiv ID: 2509.23412
- Source URL: https://arxiv.org/abs/2509.23412
- Authors: Haowei Hua; Hong Jiao; Dan Song
- Reference count: 6
- Key outcome: GPT-4o and Claude 3.5 Sonnet achieve highest QWK (0.8646, 0.8976) with human raters, with rationale similarity highest when scores match and tighter PCA clustering for better models.

## Executive Summary
This study compares scoring rationales between human raters and large language models for Chinese language essay assessments. Using cosine similarity, quadratic weighted kappa, and normalized mutual information, it finds that GPT-4o and Claude 3.5 Sonnet show the highest agreement with human scoring, with GPT-4o achieving a QWK of 0.8646 and 0.8976 for two human raters. Cosine similarity between LLM and human rationales is highest when scores match, and PCA visualizations show tighter clustering of rationales for better-performing models. The findings demonstrate that top LLMs not only score accurately but also produce rationales semantically aligned with human reasoning, supporting their potential for automated scoring when both accuracy and interpretability are strong.

## Method Summary
The study evaluates 7 LLMs (GPT-3.5, GPT-4, GPT-4o, Gemini 1.5/2.0, Claude 3.5 Sonnet, OpenAI o1) on 30 AP Chinese essay-response (ER2) prompts. Using few-shot prompting with College Board examples, models generate scores (0-6) and rationales. Scoring agreement is measured via QWK and NMI against two human raters, while rationale alignment is assessed through cosine similarity of SBERT embeddings and visualized with PCA.

## Key Results
- GPT-4o achieves highest QWK of 0.8646 and 0.8976 with two human raters
- Cosine similarity between LLM and human rationales peaks when absolute score difference is 0
- Top models (GPT-4o, Claude 3.5) show tighter PCA clustering in rationale embeddings, especially for high scores (5-6)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** High scoring accuracy between LLMs and human raters is predictive of semantic alignment in the underlying reasoning rationales.
- **Mechanism:** When an LLM assigns a score identical to a human rater, the latent reasoning path (expressed as text) tends to share higher vector similarity with the human's rationale. This suggests the model is not merely guessing the correct ordinal value but potentially activating similar evaluative concepts.
- **Core assumption:** Textual rationales faithfully represent the actual "reasoning" process of the LLM rather than post-hoc rationalizations.
- **Evidence anchors:**
  - [abstract] "Cosine similarity between LLM and human rationales is highest when scores match."
  - [results] Tables 2â€“4 show that mean cosine similarity is highest when the absolute score difference is 0, compared to differences of 1 or 2.
  - [corpus] General corpus support exists for LLM scoring accuracy (neighbor: "Comparing Human and AI Rater Effects"), but specific correlation mechanisms between score match and rationale similarity are unique to this paper.
- **Break condition:** If an LLM achieves high QWK scores via superficial heuristics (e.g., essay length) without generating semantically relevant justifications, this correlation would weaken or vanish.

### Mechanism 2
- **Claim:** Improved model architecture and scale lead to tighter clustering in the semantic vector space of rationales.
- **Mechanism:** Advanced models (e.g., GPT-4o, Claude 3.5) generate rationales that are less semantically noisy. When reduced to principal components, these rationales form compact clusters, particularly for high-scoring essays, indicating consistent internal representations of quality.
- **Core assumption:** Tighter PCA clustering implies greater stability and "correctness" of the reasoning logic, rather than simply repetitive phrasing.
- **Evidence anchors:**
  - [abstract] "PCA visualizations show tighter clustering of rationales for better-performing models."
  - [results] "In score 5 (Figure 5) and score 6 (Figure 6), embeddings from GPT-4o, Claude 3.5 Sonnet... form compact clusters."
  - [corpus] Weak direct evidence in corpus; related work focuses on scoring reliability rather than rationale clustering geometry.
- **Break condition:** If low-dimensional PCA visualization misrepresents high-dimensional distances (e.g., projecting distant points close together), the visual clustering argument fails.

### Mechanism 3
- **Claim:** Rationale similarity is robust even under minor score disagreements for high-performing models.
- **Mechanism:** Top-tier models maintain semantic overlap with human raters even when the final ordinal score diverges by one point, whereas lower-performing models show significant semantic drift.
- **Core assumption:** Assumption: A one-point score delta is often a minor calibration difference rather than a fundamental disagreement on quality.
- **Evidence anchors:**
  - [results] "When the absolute score difference increased to 1 (Table 3), cosine similarity values generally increased [for top models like Claude 3.5]."
  - [results] "Claude 3.5 Sonnet maintained the highest mean similarity of 0.6224" even with a one-score difference.
  - [corpus] Not explicitly supported by provided corpus neighbors.
- **Break condition:** If score discrepancies stem from fundamentally different interpretations of the rubric (e.g., strictness on grammar vs. content), semantic similarity would likely drop.

## Foundational Learning

- **Concept:** Quadratic Weighted Kappa (QWK)
  - **Why needed here:** This is the primary metric for evaluating scoring consistency. Unlike raw accuracy, QWK accounts for the *ordinal* nature of essay scores (i.e., predicting a 6 when the score is 5 is better than predicting a 1).
  - **Quick check question:** If Human Rater gives a 6 and the LLM gives a 5, does QWK penalize this more or less than if the LLM gives a 3?

- **Concept:** Cosine Similarity & Vector Embeddings
  - **Why needed here:** To compare "rationales," text must be converted to numbers. Cosine similarity measures the angle between these vectors, quantifying how semantically similar the LLM's justification is to the human's justification regardless of text length.
  - **Quick check question:** Does a cosine similarity of 1.0 mean the texts are identical word-for-word, or just identical in meaning?

- **Concept:** Principal Component Analysis (PCA)
  - **Why needed here:** The paper uses PCA to visualize high-dimensional rationale embeddings in 2D. This allows for the visual inspection of "clustering" to gauge how consistent a model's reasoning is across different essays.
  - **Quick check question:** If points are "tighter" in a PCA plot for Score 6 than Score 1, what does that imply about the model's consistency for high-quality essays?

## Architecture Onboarding

- **Component map:** Input (AP Chinese Essay + Few-Shot Prompt) -> Engine (7 LLMs) -> Parser (Extracts Score + Rationale) -> Vectorizer (SBERT Embeddings) -> Evaluator (QWK, Cosine Similarity, PCA)

- **Critical path:** The robustness of the **Few-Shot Prompt** is critical. If the prompt does not explicitly force the model to output a rationale *and* a score in a parsable format, the entire evaluation pipeline fails. The prompt must include the scoring guidelines (Task Completion, Delivery, Language Use).

- **Design tradeoffs:**
  - **SBERT vs. Raw BERT:** The study uses SBERT because it is optimized for semantic similarity (Siamese network structure), whereas standard BERT is not.
  - **Cosine vs. Jaccard:** Cosine is chosen to handle rationales of varying lengths (capturing semantic direction), while Jaccard would only look at token overlap.
  - **PCA vs. t-SNE:** PCA is used for linear dimensionality reduction to observe global clustering structures; t-SNE might preserve local structures better but is harder to interpret for global variance.

- **Failure signatures:**
  - **Semantic Hallucination:** High cosine similarity but low QWK (the model explains its reasoning well using human-like terms but applies the rubric incorrectly).
  - **Format Drift:** The LLM fails to separate the score from the rationale, breaking the parser.
  - **Cluster Dispersion:** In PCA, if high scores (5-6) are dispersed rather than clustered, the model lacks a consistent definition of "high quality."

- **First 3 experiments:**
  1. **Prompt Validation:** Run the exact prompt from Appendix A on a sample essay with GPT-3.5 vs. GPT-4o to verify that the output format (Score + Rationale) is stable and parsable.
  2. **Metric Correlation:** Calculate the correlation coefficient between "Absolute Score Difference" and "Cosine Similarity" across all 30 essays to statistically verify if score agreement truly predicts rationale alignment.
  3. **Cluster Reproduction:** Generate SBERT embeddings for the "Score 1" vs. "Score 6" rationales and plot them. Check if "Score 1" points are visibly more dispersed (noisier) than "Score 6" points as claimed.

## Open Questions the Paper Calls Out
None

## Limitations
- Data Availability and Representativeness: The study uses 30 Chinese essays from U.S. college students taking AP Chinese courses. This relatively small sample may not capture the full diversity of essay quality and topics encountered in actual AP Chinese exams. Additionally, the absence of original essays, human scores, and human rationales in the public domain prevents independent verification of the findings.
- Prompt Design and Execution: The study relies on few-shot prompting with a template that includes 3-5 College Board scoring examples. The exact prompt structure and the number of examples for each model are not fully specified. Variations in prompt design can significantly impact both scoring accuracy and rationale quality, making it difficult to assess whether observed differences between models are due to inherent capabilities or prompt sensitivity.
- Chinese Language Processing: The study uses SBERT for embedding generation, but the specific model variant suitable for Chinese text is not explicitly stated. Using an English-centric SBERT model could degrade the quality of semantic similarity comparisons for Chinese rationales, potentially biasing the results.

## Confidence
- **High Confidence:** The finding that GPT-4o and Claude 3.5 Sonnet achieve the highest QWK scores (0.8646 and 0.8976) with human raters is strongly supported by the reported numerical results and is consistent with the general performance of these models in other NLP benchmarks.
- **Medium Confidence:** The claim that cosine similarity between LLM and human rationales is highest when scores match is supported by the data showing increased similarity for zero-score differences. However, this correlation could be influenced by prompt design and the inherent verbosity of rationales for different score ranges.
- **Low Confidence:** The assertion that tighter PCA clustering for high-scoring essays implies greater consistency in the model's definition of "high quality" is the most speculative. PCA is a linear dimensionality reduction technique, and the visual interpretation of cluster tightness may not accurately reflect the true semantic relationships in the high-dimensional embedding space.

## Next Checks
1. **Corpus Expansion and Diversity:** Replicate the study with a larger and more diverse corpus of Chinese essays, including samples from different exam years, topics, and student backgrounds. This will test the generalizability of the findings and identify potential biases in the original dataset.
2. **Prompt Ablation Study:** Conduct a systematic study varying the number of few-shot examples, the specific examples chosen, and the prompt phrasing. Compare the resulting QWK and cosine similarity scores to isolate the impact of prompt design on model performance and rationale quality.
3. **Alternative Embedding and Visualization Methods:** Repeat the analysis using different embedding techniques optimized for Chinese text (e.g., multilingual SBERT variants, Chinese-specific sentence transformers) and alternative dimensionality reduction methods (e.g., t-SNE, UMAP) for visualization. This will help validate the robustness of the semantic similarity and clustering findings.