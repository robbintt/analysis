---
ver: rpa2
title: A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring
  Text and Image internal Correlations
arxiv_id: '2508.11141'
source_url: https://arxiv.org/abs/2508.11141
tags:
- image
- detection
- rumor
- text
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of multimodal rumor detection,
  where existing methods often overlook the content within images and the inherent
  relationships between text and images across different visual scales, resulting
  in loss of critical information. To tackle this, the authors propose a novel cross-modal
  rumor detection scheme based on contrastive learning called the Multi-scale Image
  and Context Correlation exploration algorithm (MICC).
---

# A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text and Image internal Correlations

## Quick Facts
- arXiv ID: 2508.11141
- Source URL: https://arxiv.org/abs/2508.11141
- Reference count: 0
- Achieves 0.938 accuracy and 0.917 F1 on Weibo; 0.926 accuracy and 0.932 F1 on PHEME

## Executive Summary
This paper introduces MICC, a novel cross-modal rumor detection scheme that leverages contrastive learning to explore text-image internal correlations at multiple visual scales. The core innovation is a Scale-aware Contrastive Language-Image Projection (SCLIP) encoder that generates unified semantic embeddings for text and multi-scale image patches, enabling fine-grained alignment via dot-product similarity. Building on this, a Cross-Modal Multi-Scale Alignment module identifies the most relevant image regions per scale using a Top-K selection strategy, guided by mutual information maximization and the information bottleneck principle. A scale-aware fusion network then integrates these highly correlated multi-scale image features with global text features, assigning adaptive weights based on semantic importance and cross-modal relevance. Extensive experiments on Weibo and PHEME datasets demonstrate MICC's superiority over state-of-the-art approaches, achieving substantial performance gains in rumor detection accuracy and F1-score.

## Method Summary
MICC operates in two stages: (1) Pretraining, where SCLIP encoders for text and multi-scale images are frozen and projection heads are trained via InfoNCE loss on Flickr30K/COCO-CN; (2) Fine-tuning, where all modules are trained end-to-end on Weibo/PHEME using BCE loss. SCLIP employs multi-scale CNNs (32×32, 64×64 kernels) to extract patch embeddings at different spatial granularities, followed by Transformers and MLP projections into a unified space. Cross-modal alignment constructs a relevance matrix via dot-products between text and image patches, selecting Top-K regions per scale to maximize mutual information while compressing redundancy. Scale-aware fusion combines semantic importance scores (from FFN) and relevance scores via weighted sum, then aggregates via softmax for weighted fusion with global text features. The fused representation is classified by an MLP to predict rumor likelihood.

## Key Results
- MICC achieves 0.938 accuracy and 0.917 F1 on Weibo, outperforming all comparison models.
- MICC achieves 0.926 accuracy and 0.932 F1 on PHEME, demonstrating strong cross-dataset generalization.
- Ablation studies confirm alignment module is critical (accuracy drops 8-20 points when removed), while scale-aware fusion provides additional gains over direct concatenation.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Multi-scale convolutional receptive fields in SCLIP enable fine-grained visual semantic extraction without explicit segmentation, improving cross-modal alignment for rumor detection.
- **Mechanism**: Parallel convolutions with varying kernel sizes (e.g., [32, 64]) produce patch embeddings at different spatial granularities. These are encoded through shared Transformer blocks and projected into a unified semantic space via contrastive pretraining (InfoNCE loss), allowing dot-product similarity to measure text-image relevance.
- **Core assumption**: Rumor-relevant visual cues exist at multiple scales within an image, and noise can be filtered through learned semantic alignment rather than explicit region segmentation.
- **Evidence anchors**:
  - [abstract] "we design an SCLIP encoder to generate unified semantic embeddings for text and multi-scale image patches through contrastive pretraining, enabling their relevance to be measured via dot-product similarity"
  - [section 3.2.2] Describes Multi-Scale CNN encoding with varying receptive fields producing feature maps that are flattened into patch sequences
  - [corpus] Limited direct validation of multi-scale benefit in rumor detection specifically; neighboring papers focus on contrastive learning for propagation structure rather than multi-scale visual encoding
- **Break condition**: If image patches at all scales encode redundant or noisy features with no discriminative text correlation, the multi-scale design adds computational cost without alignment gains.

### Mechanism 2
- **Claim**: Top-K selection of image regions based on dot-product relevance approximates mutual information maximization and information bottleneck compression, suppressing task-irrelevant noise.
- **Mechanism**: After constructing a cross-modal relevance matrix D via dot-products between text vector T and multi-scale image patches, the model selects the top-K patches per scale. This is theoretically motivated by InfoNCE, where higher dot-products correlate with higher conditional posterior probability p(v|T), approximating mutual information maximization while compressing redundant information.
- **Core assumption**: Dot-product similarity in the contrastive projection space reliably reflects semantic relevance for rumor detection, and the K highest-scoring regions contain the most task-relevant information.
- **Evidence anchors**:
  - [section 3.3] "selecting the Top-K dot product regions is equivalent to identifying the subset with the highest semantic relevance, which approximates mutual information maximization through a hard selection mechanism"
  - [figure 6] K=2 yields optimal performance on both datasets; higher K degrades results, suggesting noise increases with more regions
  - [corpus] "Propagation Tree Is Not Deep" paper also uses contrastive learning but for graph structure, not image-text alignment—cross-domain validation of MI-based selection is weak
- **Break condition**: If dot-product similarity captures superficial correlation (e.g., color matches) rather than semantic inconsistency signals specific to rumors, the alignment module will amplify irrelevant features.

### Mechanism 3
- **Claim**: Scale-Aware Fusion Network balances semantic importance (learned via FFN) and cross-modal relevance (from alignment) to prevent text feature suppression during fusion.
- **Mechanism**: A feedforward network computes semantic importance scores for each selected patch. These are combined with relevance scores via weighted sum (controlled by λ), then normalized via softmax for weighted aggregation. The fused visual representation is concatenated with global text features for classification.
- **Core assumption**: Text features carry primary discriminative signal for rumor detection, and improper weighting can suppress this during fusion; adaptive weighting preserves modality balance.
- **Evidence anchors**:
  - [section 3.4] Equations 23-25 describe the score fusion and weighted aggregation; λ=0.65-0.70 optimizes performance
  - [table 5] Ablation: removing alignment (MICC-A/γ, MICC-A/δ) causes larger performance drops than removing fusion (MICC-Ms/α, MICC-Ms/β), confirming alignment's critical role
  - [corpus] No corpus papers specifically validate weighted fusion strategies for rumor detection
- **Break condition**: If the FFN learns spurious importance patterns or λ is poorly tuned, fusion may over-emphasize irrelevant visual regions or under-utilize text semantics.

## Foundational Learning

- **Concept: Contrastive Learning (InfoNCE Loss)**
  - **Why needed here**: Enables unified embedding space where semantically aligned text-image pairs have high similarity. Pretraining the projection heads with frozen encoders ensures alignment without catastrophic forgetting.
  - **Quick check question**: Given a batch of N text-image pairs, can you compute the InfoNCE loss where each pair is positive against all other N-1 negatives?

- **Concept: Mutual Information Maximization / Information Bottleneck**
  - **Why needed here**: Provides theoretical grounding for Top-K selection—retaining patches that maximize I(v;T) while minimizing I(v;Image) ensures compact, task-relevant representations.
  - **Quick check question**: Why does selecting the top-K patches by dot-product similarity approximate maximizing mutual information? What does the information bottleneck constraint add?

- **Concept: Multi-Scale Feature Hierarchies**
  - **Why needed here**: Visual cues in rumor images (e.g., manipulated faces, inconsistent text overlays) may appear at different scales; single-scale encoders miss coarse context or fine details.
  - **Quick check question**: How does varying convolutional kernel size (e.g., 32×32 vs. 64×64) change what semantic information each patch captures?

## Architecture Onboarding

- **Component map**: Input (text + image) → SCLIP (T, M) → Alignment (D matrix → Top-K → M') → Fusion (Vrefact) → Classification (F = T ⊕ Vrefact → ŷ)

- **Critical path**: Input (text + image) → SCLIP (T, M) → Alignment (D matrix → Top-K → M') → Fusion (Vrefact) → Classification (F = T ⊕ Vrefact → ŷ)

- **Design tradeoffs**:
  - **Number/size of receptive fields**: More scales add coverage but increase redundancy. Paper finds [32, 64] optimal; adding [24, 48] degrades performance.
  - **Top-K value**: K=2 balances information retention vs. noise suppression. Higher K introduces semantically irrelevant regions.
  - **λ fusion weight**: 0.65–0.70 favors semantic importance over raw relevance; higher λ may overfit to learned patterns, lower λ risks shallow correlation.

- **Failure signatures**:
  - **Without alignment (MICC-A/γ, MICC-A/δ)**: Accuracy drops 8–20 points; fine-grained unaligned features (γ) outperform global features (δ), confirming alignment necessity.
  - **Without fusion (MICC-Ms/α, MICC-Ms/β)**: Smaller drops; direct concatenation underperforms weighted fusion, suggesting learned weighting matters.
  - **With standard CLIP (MICC-SCLIP)**: Accuracy drops ~1–3 points, validating multi-scale design over single-scale ViT-based CLIP.

- **First 3 experiments**:
  1. **Reproduce SCLIP pretraining**: Train projection heads on Flickr30K/COCO-CN with frozen encoders; verify retrieval performance via text-to-image recall.
  2. **Ablate receptive field configurations**: Test [32], [64], [32, 64], [24, 32, 64] on Weibo/PHEME; plot accuracy/F1 vs. scale count to validate the reported optimum.
  3. **Vary Top-K and λ jointly**: Grid search K ∈ {1, 2, 4, 8} × λ ∈ {0.3, 0.5, 0.7, 0.9}; identify interaction effects between alignment granularity and fusion weighting.

## Open Questions the Paper Calls Out
- **Question:** Can the MICC framework be effectively compressed or accelerated for real-time inference on resource-constrained edge devices without significant loss of detection accuracy?
- **Basis in paper:** [explicit] Section 5 states, "Future work will explore lightweight model designs to support real-time deployment on edge devices or mobile platforms."
- **Why unresolved:** The current model relies on a Transformer-based SCLIP encoder and multi-scale convolutions, which are computationally intensive; the paper only evaluates performance on standard hardware.
- **What evidence would resolve it:** A study reporting model size (MB), FLOPs, and latency (ms) on mobile hardware alongside accuracy metrics after applying pruning or quantization techniques.

- **Question:** Does a static Top-K selection strategy (K=2) limit performance on complex images containing multiple distinct semantic regions relevant to the rumor?
- **Basis in paper:** [inferred] Section 4.2 identifies K=2 as the optimal fixed value, but Section 3.3 emphasizes minimizing redundancy via the information bottleneck. A fixed K assumes all images have the same optimal number of relevant regions, potentially over-pruning complex scenes or retaining noise in simple ones.
- **Why unresolved:** The paper treats K as a global hyperparameter rather than a sample-dependent variable.
- **What evidence would resolve it:** An ablation study comparing the fixed Top-K approach against an adaptive thresholding mechanism or a learnable K on a dataset with high variance in visual clutter.

- **Question:** How does the cross-modal alignment mechanism perform on AI-generated misinformation (e.g., deepfakes) where text and manipulated images may be semantically consistent but factually false?
- **Basis in paper:** [inferred] The Introduction mentions "deceptive characteristics" of rumors, and Section 2 reviews semantic alignment. However, the evaluation uses Weibo and PHEME (Section 4.1), which consist of traditional misinformation. High semantic alignment between AI-generated text and images might bypass the relevance filters designed to detect traditional inconsistencies.
- **Why unresolved:** The model assumes that maximizing mutual information and alignment helps distinguish rumors, but deepfakes may possess high alignment while still being deceptive.
- **What evidence would resolve it:** Experimental results testing the MICC model on a dataset specifically composed of AI-synthesized image-text pairs.

## Limitations
- The model's reliance on dot-product similarity may conflate superficial correlations with genuine rumor-specific inconsistencies, lacking explicit semantic inconsistency detection.
- Optimal hyperparameters (multi-scale receptive fields [32, 64], Top-K=2, λ=0.65–0.70) were tuned on two datasets without systematic cross-validation across diverse rumor types, risking overfitting.
- The multi-scale CNN design adds computational overhead without ablation studies isolating the benefit of scale diversity versus single-scale alternatives.

## Confidence
- **High confidence**: Multi-scale encoding improves cross-modal alignment vs. single-scale baselines; alignment module (Top-K selection) is critical for performance; scale-aware fusion with learned weighting is superior to naive concatenation.
- **Medium confidence**: Theoretical grounding via mutual information maximization and information bottleneck for Top-K selection; optimal λ range (0.65–0.70) is stable across datasets.
- **Low confidence**: Generalizability of optimal hyperparameters (receptive fields, K, λ) to other rumor detection tasks or domains; robustness of dot-product relevance to noise in real-world data.

## Next Checks
1. **Ablate receptive field configurations**: Systematically test [32], [64], [32, 64], [24, 32, 64] on Weibo/PHEME; plot accuracy/F1 vs. scale count to confirm the reported optimum and assess redundancy.
2. **Joint Top-K and λ sensitivity analysis**: Grid search K ∈ {1, 2, 4, 8} × λ ∈ {0.3, 0.5, 0.7, 0.9}; identify interaction effects between alignment granularity and fusion weighting, and check for overfitting.
3. **Cross-domain pretraining validation**: Replace Flickr30K/COCO-CN with rumor-specific text-image pairs (if available) or evaluate MICC's robustness to pretraining corpus domain shift.