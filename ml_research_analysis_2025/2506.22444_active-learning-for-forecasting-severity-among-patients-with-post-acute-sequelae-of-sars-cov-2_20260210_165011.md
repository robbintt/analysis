---
ver: rpa2
title: Active Learning for Forecasting Severity among Patients with Post Acute Sequelae
  of SARS-CoV-2
arxiv_id: '2506.22444'
source_url: https://arxiv.org/abs/2506.22444
tags:
- active
- risk
- learning
- attention
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces the first publicly available dataset of 18
  Postacute Sequelae of SARS-CoV-2 (PASC) patients, annotated with clinical risk levels
  and structured event-time series extracted from case reports using Llama-3.1-70B-Instruct.
  An Active Attention Network is proposed to predict clinical risk and identify progression
  events, incorporating uncertainty-based active learning to improve efficiency.
---

# Active Learning for Forecasting Severity among Patients with Post Acute Sequelae of SARS-CoV-2

## Quick Facts
- **arXiv ID:** 2506.22444
- **Source URL:** https://arxiv.org/abs/2506.22444
- **Reference count:** 17
- **Primary result:** Active Attention Network achieves higher accuracy with fewer labeled samples compared to random sampling for PASC risk prediction

## Executive Summary
This study introduces the first publicly available dataset of 18 Postacute Sequelae of SARS-CoV-2 (PASC) patients, annotated with clinical risk levels and structured event-time series extracted from case reports using Llama-3.1-70B-Instruct. An Active Attention Network is proposed to predict clinical risk and identify progression events, incorporating uncertainty-based active learning to improve efficiency. The model achieves higher accuracy with fewer labeled samples compared to random sampling, demonstrating effective identification of risk-related features such as age, symptoms, and treatments. The approach supports enhanced patient management and resource allocation in PASC care.

## Method Summary
The method involves extracting clinical events and timestamps from 18 PASC case reports using Llama-3.1-70B-Instruct, embedding events with PubMedBERT and projecting to 32 dimensions, and concatenating with normalized timestamps to create 4,950-dimensional feature vectors. An Active Attention Network with sigmoid-weighted attention and two feedforward layers predicts binary risk (Low/High). Active learning employs margin-based uncertainty sampling, iteratively querying the sample with smallest probability margin between classes and retraining. The model is evaluated with 5 repetitions using test sizes of 5 and 7 samples.

## Key Results
- Active Attention Network achieves higher accuracy with fewer labeled samples compared to random sampling
- Uncertainty-based active learning identifies risk-related features such as age, symptoms, and treatments
- The approach demonstrates 50% label efficiency improvement over random sampling

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Uncertainty-based active learning achieves higher accuracy with fewer labeled samples compared to random sampling for PASC risk prediction.
- **Mechanism:** The model computes prediction probabilities for each unlabeled sample and selects those with the smallest margin between class probabilities (|p₀ - p₁|). These samples are closest to the decision boundary, providing maximum information gain when labeled. Each queried label expands the labeled training set, and the model retrains iteratively.
- **Core assumption:** Samples near the decision boundary are more informative for model improvement than confident predictions. This assumes the model's uncertainty correlates with genuinely ambiguous cases rather than noise or outliers.
- **Evidence anchors:**
  - [abstract] "The model achieves higher accuracy with fewer labeled samples compared to random sampling"
  - [section: Introduction] "The uncertainty of unlabeled sample is based on margin of two class probabilities. The samples with smallest margin is considered with most informative for the model."
  - [corpus] Related work on AKI-to-CKD progression (arXiv:2511.14603) uses EHR data for dynamic tracking but does not employ active learning, suggesting this mechanism is novel to the PASC domain.
- **Break condition:** If initial labeled samples are unrepresentative or heavily biased, the uncertainty scores may reinforce poor decisions, causing the model to query unhelpful samples (cold start problem).

### Mechanism 2
- **Claim:** The attention mechanism identifies clinically meaningful features (treatments, demographics, symptoms) that correlate with risk levels.
- **Mechanism:** A learned attention weight matrix (W_attn) applies sigmoid activation to produce per-feature weights between 0 and 1. These weights are multiplied element-wise with input features, amplifying relevant signals and suppressing noise. Top-weighted features can be mapped back to original clinical events for interpretability.
- **Core assumption:** Risk-related features share consistent patterns across patients that can be captured through learned attention weights. This assumes the 32-dimensional embedding space preserves clinical semantics.
- **Evidence anchors:**
  - [abstract] "demonstrating effective identification of risk-related features such as age, symptoms, and treatments"
  - [section: Results, Table 4] AL identifies "leukocytosis" for PMC10173208 and "elevated erythrocyte" for PMC9451509, which are more discriminative than RS features
  - [corpus] Related PASC symptom extraction (arXiv:2508.12405) uses rule-based NER with BERT but lacks attention-based feature importance.
- **Break condition:** If the embedding space poorly represents clinical semantics (e.g., rare treatments have weak embeddings), attention weights may amplify noise or assign high weights to spurious correlations.

### Mechanism 3
- **Claim:** Structured event-time series representation captures temporal disease progression that improves risk prediction over static features.
- **Mechanism:** Clinical events are extracted from case reports using Llama-3.1-70B-Instruct, embedded via PubMedBERT (768 dims → 32 dims via projection), and concatenated with normalized timestamps. The admission event is set to t=0; prior events have negative timestamps. Max 150 event-time pairs per case yields 4,950 total features.
- **Core assumption:** The temporal ordering and distance of events from admission carry prognostic signal. This assumes the LLM accurately extracts events and timestamps from unstructured text.
- **Evidence anchors:**
  - [section: Clinical Event-Time Sequences] "The clinical risk level is defined as Table 1... The clinical timestamp is based on admission event set as time 0"
  - [section: Experiments] "The maximal number of event-time pairs for one case is set as 150... The total dimension of the feature space for one case report is 4,950"
  - [corpus] Weak corpus evidence—no directly comparable event-time architectures found in neighbors.
- **Break condition:** If timestamp extraction is inaccurate or inconsistent across reports, temporal patterns become noise. Sparse events (fewer than 150) with zero-padding may dilute attention signals.

## Foundational Learning

- **Concept: Margin-based uncertainty sampling**
  - **Why needed here:** The active learning loop relies on selecting samples where |p₀ - p₁| is minimized. Understanding that smaller margins indicate boundary proximity is essential for debugging why certain samples are queried.
  - **Quick check question:** Given probabilities [0.48, 0.52] vs [0.15, 0.85], which sample would margin-based active learning select and why?

- **Concept: Attention as learnable feature gating**
  - **Why needed here:** The architecture uses sigmoid-attention to weight features. Engineers must understand that these weights are learned end-to-end, not hand-specified, and can be inspected post-training for interpretability.
  - **Quick check question:** If attention weights are uniformly ~0.5 across all features, what does this suggest about the model's ability to discriminate relevant features?

- **Concept: Embedding space semantics**
  - **Why needed here:** Clinical events are projected from 768-dim PubMedBERT embeddings to 32 dims. Information loss is possible; understanding the tradeoff between dimensionality and representational capacity is critical.
  - **Quick check question:** Why might rare clinical events (e.g., uncommon treatments) have poor representations after dimensionality reduction?

## Architecture Onboarding

- **Component map:**
  Input -> Embedding (PubMedBERT → 32 dims) + normalized timestamp -> Attention Layer (Sigmoid) -> FC(ReLU, BN, Dropout) -> FC(ReLU, BN, Dropout) -> Softmax(2)

- **Critical path:**
  1. Data preparation: Verify event extraction quality and timestamp normalization
  2. Initial training: Start with 4 labeled samples, evaluate on held-out test set
  3. Active learning iterations: Select lowest-margin unlabeled sample, query label, retrain
  4. Feature importance analysis: Extract attention weights, map to original events

- **Design tradeoffs:**
  - Dataset size (N=18) limits generalizability; results should be interpreted as proof-of-concept
  - Zero-padding for sparse events may introduce noise; attention mechanism partially mitigates this
  - Binary risk classification simplifies clinical reality; ordinal or continuous risk scores may be more informative

- **Failure signatures:**
  - Accuracy plateaus early with active learning → Check for label noise or uninformative feature space
  - Attention weights uniform or chaotic → Inspect embedding quality; consider increasing projection dimension
  - Active learning queries outliers repeatedly → Uncertainty may reflect annotation errors; add diversity criterion

- **First 3 experiments:**
  1. **Baseline sanity check:** Train on all 18 labeled samples without active learning. Report accuracy and top attention-weighted features. This establishes the upper bound.
  2. **Ablation on initialization:** Vary initial labeled set size (n_train = 2, 4, 6) and measure convergence speed. Test sensitivity to cold start.
  3. **Feature importance validation:** Have clinical expert rate whether top attention-weighted features are clinically plausible predictors. Compare AL vs RS feature sets systematically.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Active Attention Network maintain its reported 50% label efficiency advantage when scaled to larger, multi-center cohorts of PASC patients?
- Basis in paper: [inferred] The conclusion states the system is "more label-efficient by 50%" and "may be used as more PASC case reports become available," yet the current study validates this claim on a dataset of only 18 patients.
- Why unresolved: A sample size of 18 lacks the statistical power to verify if the active learning curve and efficiency gains generalize to a broader population or if they are artifacts of the specific small dataset.
- What evidence would resolve it: Replicating the experiment on a larger dataset (e.g., hundreds of patients) to confirm that the active learning strategy consistently outperforms random sampling with statistical significance.

### Open Question 2
- Question: How does the margin-based uncertainty sampling strategy compare against diversity-based active learning methods in this context?
- Basis in paper: [inferred] The "Related works" section identifies "diversity-based" methods as a primary category of active learning, but the experimental section only evaluates "uncertainty-based" sampling.
- Why unresolved: It remains unclear if selecting samples based on uncertainty (margin) is superior or inferior to selecting samples that best represent the data distribution (diversity) for clinical time-series data.
- What evidence would resolve it: A comparative ablation study evaluating model performance using diversity-based selectors (e.g., Core-Set) versus the margin-based method on the same dataset.

### Open Question 3
- Question: How robust is the risk prediction model to potential hallucinations or extraction errors in the Llama-3.1-generated event-time series?
- Basis in paper: [inferred] Table 4 lists "Unknown Event" as a top feature selected by the model, suggesting the attention mechanism may be fitting to artifacts or "0 filling" generated by the LLM extraction process.
- Why unresolved: The study does not quantify the noise rate of the LLM extraction or test the model's sensitivity to these potential errors versus genuine clinical signals.
- What evidence would resolve it: Evaluating the model's performance on a dataset where LLM-extracted events are manually cleaned versus the raw LLM output to measure the impact of extraction noise.

## Limitations
- Extremely small dataset (N=18) restricts generalizability and statistical power
- Binary risk classification oversimplifies the continuous nature of clinical risk
- Model performance depends entirely on LLM accuracy for clinical event extraction

## Confidence

**High Confidence:** The Active Attention Network architecture is technically sound and the margin-based uncertainty sampling mechanism is well-established in active learning literature. The reported accuracy improvements over random sampling are plausible given the theoretical foundation.

**Medium Confidence:** The specific clinical risk categories and event extraction process are methodologically reasonable but not independently verified. The 18-case dataset, while novel, is too small to establish clinical validity.

**Low Confidence:** The generalizability of findings to broader PASC populations is uncertain. The attention mechanism's feature importance may not translate to different patient cohorts or clinical settings.

## Next Checks

1. **Clinical Expert Validation:** Have three independent clinicians review the top attention-weighted features identified by both Active Learning and Random Sampling to assess clinical plausibility and discriminative value.

2. **Dataset Expansion Impact:** Test the active learning approach on a larger, independently collected PASC dataset (N>100) to evaluate whether the sample efficiency advantage persists with more diverse cases.

3. **Temporal Pattern Verification:** Manually verify timestamp extraction accuracy for 10 randomly selected events to ensure temporal relationships between clinical events are correctly captured and normalized.