---
ver: rpa2
title: 'RideKE: Leveraging Low-Resource, User-Generated Twitter Content for Sentiment
  and Emotion Detection in Kenyan Code-Switched Dataset'
arxiv_id: '2502.06180'
source_url: https://arxiv.org/abs/2502.06180
tags:
- sentiment
- supervised
- emotion
- semi-supervised
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RideKE, a sentiment and emotion analysis
  dataset for Kenyan-accented English code-switched with Swahili and Sheng. The dataset
  contains over 29,000 tweets from ride-hailing services like Uber, Bolt, and Little
  Cab, with sentiment labels (positive, negative, neutral) and emotion labels (frustration,
  happy, angry, sad, empathy, fear, love, surprise).
---

# RideKE: Leveraging Low-Resource, User-Generated Twitter Content for Sentiment and Emotion Detection in Kenyan Code-Switched Dataset

## Quick Facts
- arXiv ID: 2502.06180
- Source URL: https://arxiv.org/abs/2502.06180
- Reference count: 40
- Key outcome: XLM-R achieved highest sentiment analysis accuracy (69.2%) and F1 score (66.1%), while DistilBERT led emotion analysis with accuracy 59.8% and F1 31%; all models tended to predict neutral sentiment, with AfriBERTa showing lowest performance.

## Executive Summary
This paper introduces RideKE, a novel dataset containing over 29,000 Kenyan-accented English tweets code-switched with Swahili and Sheng from ride-hailing services. The dataset includes sentiment labels (positive, negative, neutral) and emotion labels (8 categories: frustration, happy, angry, sad, empathy, fear, love, surprise). Four transformer models (DistilBERT, mBERT, XLM-R, AfriBERTa) were evaluated using supervised and semi-supervised learning approaches. XLM-R achieved the best sentiment analysis performance, while emotion detection proved more challenging across all models, with several emotion classes achieving near-zero F1 scores.

## Method Summary
The study fine-tuned four pre-trained transformer models on a small labeled dataset (Set A: 1,189 tweets with human and ChatGPT-generated labels) and tested on a human-annotated test set (Set B: 2,000 tweets). Semi-supervised learning augmented the labeled data by generating pseudo-labels from a larger unlabeled corpus (Set C: 27,090 tweets) using a 75th percentile confidence threshold. Models were trained with multitask learning for both sentiment and emotion classification using cross-entropy loss. Supervised training ran for 10 epochs with Adam optimizer and learning rate 1e-5, while semi-supervised fine-tuning used 4 epochs.

## Key Results
- XLM-R achieved highest sentiment analysis accuracy (69.2%) and F1 score (66.1%)
- DistilBERT led emotion analysis with accuracy 59.8% and F1 score 31%
- All models showed tendency to over-predict neutral sentiment
- AfriBERTa, despite being pre-trained on African languages, showed lowest performance (accuracy 39.8%)
- Semi-supervised learning provided mixed results, improving sentiment for some models but not emotion detection

## Why This Works (Mechanism)

### Mechanism 1
**Claim:** Supervised fine-tuning of large multilingual transformers on limited labeled code-switched data can yield improved sentiment classification performance compared to mono-lingual or regionally specialized models.
**Mechanism:** The XLM-R model, pre-trained on 100 languages including high-resource English, develops rich cross-lingual representations. When fine-tuned on the small (~1,189 labeled tweets) RideKE dataset containing Kenyan English-Swahili-Sheng code-switching, it leverages these general representations to adapt to the specific linguistic patterns of this low-resource domain.
**Core assumption:** The linguistic features learned from high-resource languages during pre-training transfer effectively to the specific, informal, and blended context of Kenyan code-switched tweets.
**Evidence anchors:**
- [abstract] "Our results show that XLM-R outperforms other models; for sentiment analysis, XLM-R supervised model achieves the highest accuracy (69.2\%) and F1 score (66.1\%)..."
- [section 4.1 & 5.1] The paper evaluates XLM-R alongside DistilBERT, mBERT, and AfriBERTa. Table 5 shows XLM-R supervised outperforming the others in sentiment analysis accuracy (69.2%) and F1 (66.1%). The authors note XLM-R is "trained on 100 languages with improved performance" (Section 4.1).
- [corpus] A related paper, "Code-Mix Sentiment Analysis on Hinglish Tweets" (arXiv:2601.05091), similarly finds that transformer models applied to code-mixed social media data are effective, supporting the general approach.

### Mechanism 2
**Claim:** Semi-supervised learning (SSL) can provide marginal performance gains for sentiment analysis in low-resource settings by leveraging a large pool of unlabeled data.
**Mechanism:** SSL augments a small human-labeled dataset with pseudo-labels generated by the model itself on a much larger unlabeled dataset. By selecting only high-confidence predictions (using a 75th percentile probability threshold in this study), the model iteratively refines its decision boundaries with more data than would be feasible to label manually.
**Core assumption:** The model's high-confidence predictions on unlabeled data are sufficiently accurate to serve as useful training signals, and these additional signals help the model learn patterns not captured by the small, manually-labeled set.
**Evidence anchors:**
- [abstract] "Semi-supervised learning shows mixed results, with some improvements in sentiment analysis tasks."
- [section 3.6 & 5.4] The methodology details an SSL loop where models generate pseudo-labels for unlabeled data. Results in Table 5 show that semi-supervised mBERT slightly improves sentiment F1 (59.6% vs 59.8%) and semi-supervised AfriBERTa also sees a small increase (35.8% to 36.6% F1).
- [corpus] No direct corpus evidence was found explicitly validating this specific SSL method for this specific low-resource African context. The evidence is primarily internal to the paper.

### Mechanism 3
**Claim:** Emotion detection is significantly more challenging than sentiment analysis in this low-resource, code-switched context, leading to lower and more variable model performance.
**Mechanism:** While sentiment is a broad, three-class problem (positive, negative, neutral), emotion is a finer-grained, eight-class problem (e.g., frustration, happy, angry, sad, empathy, fear, love, surprise). This increased granularity requires the model to capture more subtle and subjective nuances in the text. The smaller number of training examples for many emotion classes creates a class imbalance problem, making it difficult for the model to learn robust representations for all categories.
**Core assumption:** The annotators can reliably distinguish between eight fine-grained emotions in informal, code-switched text, and the text contains sufficient signal for this distinction.
**Evidence anchors:**
- [abstract] "In emotion analysis, DistilBERT supervised leads in accuracy (59.8%) and F1 score (31%)..." (compared to sentiment analysis F1 scores of 66.1%).
- [section 5.2 & Tables 7-9] The results section notes, "The models generally show lower performance than sentiment analysis. Since emotions are complex". Tables 7, 8, and 9 show highly variable performance across emotions, with many F1-scores near or at zero for classes like Happy and Fear.
- [corpus] A paper on "Emotion Recognition for Low-Resource Turkish" (arXiv:2505.12160) demonstrates the broader challenge of emotion detection in low-resource languages, providing external context for the difficulty.

## Foundational Learning

**Concept: Transformer Fine-tuning**
- Why needed here: The entire experimental framework relies on taking large, general-purpose models (like XLM-R) and adapting them to a specific, narrow task (RideKE sentiment/emotion) using a small labeled dataset.
- Quick check question: What is the key difference between the pre-training and fine-tuning phases of a transformer model?

**Concept: Code-Switching in NLP**
- Why needed here: The core challenge of the paper is handling text that blends English, Swahili, and Sheng. Understanding why this breaks standard, mono-lingual NLP tools is essential.
- Quick check question: Why might a standard English-only sentiment analysis model fail on a sentence like "Hawa madere ni wazimu walai" (These drivers are crazy, really)?

**Concept: Class Imbalance**
- Why needed here: The emotion analysis task has eight classes, which likely have an unequal distribution in the dataset. This explains the poor performance on several emotion categories and is a critical factor in interpreting the results.
- Quick check question: If a dataset has 90% 'neutral' tweets and only 1% 'fear' tweets, a model that always predicts 'neutral' will have 90% accuracy. Why is this a problem, and what metric (e.g., F1-score) better reflects performance on the minority class?

## Architecture Onboarding

**Component map:**
1. Data Pipeline: Ingests raw tweets from Twitter, filters by geo-location, and preprocesses text (normalization, slang handling)
2. Labeling Module: Starts with human annotation for a small seed set (Set A). Can be augmented by a Large Language Model (ChatGPT) or by the system's own SSL loop (Set C)
3. Model Backbone: Uses pre-trained transformer encoders (DistilBERT, mBERT, XLM-R, AfriBERTa) as feature extractors
4. Task-Specific Heads: Two separate classification heads on top of the transformer output: one for 3-class sentiment and one for 8-class emotion
5. Training Loop: A combined loss function for the multi-task setup, with distinct supervised and semi-supervised phases
6. Evaluation: Measures accuracy, precision, recall, and F1-score on a held-out human-annotated test set (Set B)

**Critical path:** The performance of the final system depends most heavily on the quality and representativeness of the initial human-annotated seed data (Set A) and the ability of the chosen transformer backbone to generalize from it to the broader unlabeled corpus. The SSL loop's pseudo-labeling threshold is a critical hyperparameter.

**Design tradeoffs:**
- XLM-R vs. AfriBERTa: The paper empirically shows that XLM-R, a globally pre-trained model, outperforms AfriBERTa, which is pre-trained specifically on African languages. This suggests the scale and diversity of pre-training data can be more important than narrow domain-specific pre-training for this task.
- Model Size vs. Performance: DistilBERT, a smaller and faster model, leads in emotion accuracy but lags in sentiment. XLM-R is larger and leads in sentiment. This presents a tradeoff between computational cost and task-specific performance.
- Human vs. Synthetic/SSL Labels: Using ChatGPT and SSL for annotation trades off the cost and time of human labeling against the risk of introducing noisy or incorrect labels.

**Failure signatures:**
- Neutral Prediction Bias: All models, especially AfriBERTa, tend to over-predict the 'neutral' class. This is a sign that the model is uncertain or that the training data is dominated by neutral examples.
- Zero-shot Failure on Emotion Classes: Many models achieve an F1-score of 0.0 on specific emotions like 'Happy' or 'Fear', indicating they failed to learn any distinguishing features for those classes from the training data.

**First 3 experiments:**
1. Establish a Baseline: Fine-tune a standard `bert-base-uncased` model on the provided human-labeled training set (Set A) for sentiment analysis. Measure performance on the test set (Set B) to establish a baseline for this specific task.
2. Reproduce Key Result: Fine-tune the `xlm-roberta-base` model on the same training data. Compare its F1-score to the baseline and to the results reported in the paper (66.1% for sentiment) to validate the experimental setup.
3. Analyze a Failure Mode: Take the best-performing sentiment model and evaluate it on a subset of tweets containing Sheng words. Analyze its predictions and errors to understand if and how the model handles the code-switched vocabulary.

## Open Questions the Paper Calls Out
- In the future, we aim to explore alternative semi-supervised approaches to enhance model performance
- The study acknowledges the subjective nature of sentiment analysis and potential label bias from both human and ChatGPT annotations
- The authors note that emotion detection remains particularly challenging and may require larger, more balanced datasets

## Limitations
- Semi-supervised learning showed only marginal improvements, suggesting pseudo-labeling confidence thresholds may not be optimal for this domain
- Emotion detection performance remains poor across all models, with many emotion classes achieving F1-scores near zero
- The paper doesn't report on how code-switching complexity correlates with model performance, leaving uncertainty about which linguistic features are most challenging

## Confidence
- **High confidence:** XLM-R outperforms other transformer models for sentiment analysis on this dataset (supported by direct comparison in Table 5)
- **Medium confidence:** Semi-supervised learning provides consistent improvements (results show mixed outcomes with some models improving and others degrading)
- **Low confidence:** Emotion detection is fundamentally more difficult than sentiment analysis in this context (data limitations and severe class imbalance make it difficult to distinguish between model capability and data quality issues)

## Next Checks
1. Test whether class-weighted loss functions improve emotion detection performance, particularly for underrepresented emotion classes
2. Evaluate model performance on a stratified sample of tweets by code-switching ratio (English-dominant vs. Swahili-dominant vs. Sheng-heavy) to identify linguistic feature challenges
3. Implement an ablation study comparing the impact of human-labeled vs. ChatGPT-generated training data on final model performance