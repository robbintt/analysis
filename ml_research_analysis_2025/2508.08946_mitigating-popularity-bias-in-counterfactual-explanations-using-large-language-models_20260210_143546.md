---
ver: rpa2
title: Mitigating Popularity Bias in Counterfactual Explanations using Large Language
  Models
arxiv_id: '2508.08946'
source_url: https://arxiv.org/abs/2508.08946
tags:
- user
- popularity
- explanations
- counterfactual
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using large language models (LLMs) to improve
  the user alignment of counterfactual explanations (CFEs) in recommender systems
  by filtering out-of-character items from users' history before generating explanations.
  The approach embeds a textual user profile generated by an LLM, removes items that
  most change the profile, and feeds the filtered history to ACCENT, a CFE framework.
---

# Mitigating Popularity Bias in Counterfactual Explanations using Large Language Models

## Quick Facts
- arXiv ID: 2508.08946
- Source URL: https://arxiv.org/abs/2508.08946
- Reference count: 40
- Primary result: LLM-augmented ACCENT reduces popularity bias in counterfactual explanations with statistically significant improvements in expected popularity deviation on MovieLens 1M

## Executive Summary
This paper addresses popularity bias in counterfactual explanations (CFEs) for recommender systems by incorporating large language models (LLMs) to filter users' interaction histories. The proposed method uses an LLM to generate textual user profiles from historical interactions, then removes items that most alter these profiles before generating CFEs. The approach is evaluated on MovieLens 1M and Amazon Video Games datasets, showing that LLM-augmented explanations better align with individual users' popularity preferences while maintaining explanation quality.

## Method Summary
The approach combines LLM-based filtering with the ACCENT counterfactual explanation framework. First, an LLM generates textual descriptions of users' interaction histories. Items that most change these textual profiles are then removed, creating a filtered history. This filtered history is fed to ACCENT to generate counterfactual explanations that are more aligned with each user's unique preferences. The method aims to balance explanation accuracy with diversity by considering both popularity and individual user characteristics in the filtering process.

## Key Results
- LLM-augmented ACCENT reduces popularity distribution similarity (PDS) and expected popularity deviation (EPD) metrics compared to ACCENT alone
- Statistically significant improvements in EPD observed on MovieLens 1M dataset
- The approach shows stronger performance on richer datasets, with weaker but safe performance on sparser data
- Method produces explanations more closely matching each user's popularity preferences

## Why This Works (Mechanism)
The LLM filtering mechanism works by creating a more nuanced representation of user preferences through textual profiling. By removing items that most alter these profiles, the approach filters out interactions that may be influenced by popularity bias rather than genuine user preference. This allows the ACCENT framework to generate explanations that better reflect each user's authentic interests rather than being dominated by popular items that may not align with individual tastes.

## Foundational Learning

**Counterfactual Explanations (CFEs)**: Explanations showing minimal changes needed to achieve different recommendations. Needed to understand the target explanation format being improved. Quick check: Verify CFEs provide actionable feedback for users.

**Popularity Bias in Recommender Systems**: Tendency for systems to over-recommend popular items regardless of individual preferences. Needed to understand the problem being addressed. Quick check: Measure popularity bias by comparing recommended item popularity to overall item popularity distribution.

**Large Language Models (LLMs) for User Profiling**: Using LLMs to generate textual representations of user behavior patterns. Needed to understand how LLM filtering works. Quick check: Validate LLM-generated profiles capture meaningful patterns in user interaction data.

**ACCENT Framework**: A specific counterfactual explanation generation approach for recommender systems. Needed to understand the baseline method being augmented. Quick check: Confirm ACCENT produces coherent and actionable counterfactuals.

**Popularity Distribution Similarity (PDS) and Expected Popularity Deviation (EPD)**: Metrics measuring how well explanations align with user popularity preferences. Needed to evaluate method effectiveness. Quick check: Verify these metrics appropriately capture the balance between popularity and diversity.

## Architecture Onboarding

**Component Map**: User History -> LLM Profile Generation -> Item Filtering -> Filtered History -> ACCENT -> Counterfactual Explanations

**Critical Path**: The LLM filtering step is critical as it directly influences which items are available for ACCENT to generate explanations from. The quality of LLM-generated profiles determines the effectiveness of the filtering.

**Design Tradeoffs**: The approach trades computational complexity (LLM inference) for improved user alignment. The filtering mechanism must balance removing popularity-biased items while preserving enough history for meaningful explanations.

**Failure Signatures**: Poor LLM profiling could result in over-filtering or under-filtering. The method may underperform on sparse datasets where meaningful textual profiles cannot be generated.

**First Experiments**:
1. Baseline comparison: ACCENT vs. LLM-augmented ACCENT on PDS and EPD metrics
2. Ablation study: Test filtering with different threshold values for profile change
3. Dataset diversity test: Compare performance across MovieLens and Amazon datasets

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but implicit questions include: How does the approach generalize to other recommendation domains? What is the computational overhead of LLM filtering? How do these improvements translate to user satisfaction?

## Limitations

- Limited evaluation on diverse user populations and recommendation domains
- Computational overhead of LLM embeddings not investigated for real-time systems
- Effectiveness may not generalize to users with highly eclectic or niche preferences
- Focus on popularity alignment may not capture other aspects of explanation quality

## Confidence

High confidence in the assertion that the approach produces more "user-aligned" explanations based on quantitative improvements in popularity-based metrics. Medium confidence in the claim that LLM-augmented ACCENT improves user alignment overall, given modest effect sizes and lack of user studies. Major uncertainties remain regarding robustness across diverse user populations and recommendation domains.

## Next Checks

1. Conduct ablation studies to isolate the contribution of LLM filtering versus other components of the ACCENT framework, testing whether similar improvements can be achieved through simpler heuristics.

2. Evaluate the approach on additional recommendation domains (e.g., music, books, news) with different user behavior patterns to assess generalizability.

3. Perform user studies to validate whether improved popularity metric scores correlate with enhanced user satisfaction and understanding of the explanations.