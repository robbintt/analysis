---
ver: rpa2
title: A Dual Large Language Models Architecture with Herald Guided Prompts for Parallel
  Fine Grained Traffic Signal Control
arxiv_id: '2511.00136'
source_url: https://arxiv.org/abs/2511.00136
tags:
- traffic
- signal
- control
- herald
- heraldlight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HeraldLight addresses the limitations of fixed-time and reinforcement
  learning (RL) based traffic signal control by introducing a dual large language
  model (LLM) architecture enhanced with Herald guided prompts. The Herald Module
  forecasts queue lengths and provides fine-grained signal duration guidance, while
  LLM-Agent and LLM-Critic collaboratively make and refine control decisions to reduce
  hallucination and improve reliability.
---

# A Dual Large Language Models Architecture with Herald Guided Prompts for Parallel Fine Grained Traffic Signal Control

## Quick Facts
- arXiv ID: 2511.00136
- Source URL: https://arxiv.org/abs/2511.00136
- Reference count: 28
- Primary result: 20.03% reduction in average travel time and 10.74% reduction in average queue length vs state-of-the-art methods

## Executive Summary
HeraldLight introduces a dual LLM architecture enhanced with Herald guided prompts to address traffic signal control limitations of fixed-time and RL-based methods. The Herald Module forecasts queue lengths and provides fine-grained second-level signal timing guidance, while LLM-Agent and LLM-Critic collaboratively make and refine control decisions to reduce hallucination. Experiments on 224 real-world intersections across Jinan, Hangzhou, and New York demonstrate significant improvements in traffic efficiency and scalability.

## Method Summary
HeraldLight employs a dual LLM architecture where the LLM-Agent (Llama-3.1-8B with LoRA fine-tuning) generates traffic signal actions and the LLM-Critic (ChatGPT) evaluates and corrects outputs. The Herald Module extracts contextual information and forecasts queue lengths up to 40 seconds ahead, providing structured guidance through fine-grained prompts. The system uses imitation learning with expert demonstrations from ChatGPT interacting with CityFlow simulator, followed by score-based ranking loss fine-tuning that prioritizes higher-quality trajectories. The approach achieves parallel fine-grained control with second-level timing precision across multi-intersection networks.

## Key Results
- 20.03% reduction in average travel time compared to state-of-the-art methods
- 10.74% reduction in average queue length across tested scenarios
- 98.4% reduction in hallucination rate (from 9.23% to 0.163%) through dual-LLM correction

## Why This Works (Mechanism)

### Mechanism 1: Herald Module Queue Forecasting
The Herald Module learns scenario-specific queue-to-release-time mappings and forecasts queue states up to 40 seconds ahead. It collects empirical data on queue lengths, vehicle speeds, and egress times during simulation, fitting a piecewise-linear model mapping queue length to release time. This structured guidance is injected into LLM prompts to enable fine-grained, second-level signal timing. The approach assumes queue dynamics are approximately monotonic and can be abstracted without vehicle-level kinematics.

### Mechanism 2: Dual LLM Hallucination Reduction
The LLM-Agent generates phase/duration actions while the LLM-Critic evaluates outputs for constraint violations, repetition, or incoherence, then produces corrected responses with quality scores. A score-based ranking loss trains the Agent to prefer higher-scored trajectories while preventing regression on lower-scored ones. This iterative correction process reduces hallucination by 98.4%, assuming the Critic model is reliably more capable than the Agent and error patterns are detectable via structured prompts.

### Mechanism 3: LoRA-Based Imitation Fine-Tuning
LoRA adapters inject low-rank updates into Llama-3.1-8B, preserving base knowledge while specializing for traffic signal control. The system distills expert demonstrations from ChatGPT interactions with CityFlow via Herald-guided prompts, logging structured actions as an imitation corpus. This approach assumes the expert's reasoning patterns are learnable via next-token prediction and that LoRA rank is sufficient to capture task-specific adaptations without catastrophic forgetting.

## Foundational Learning

- **Traffic Signal Phases and Timing**: Understanding 4-phase signal control (NTST, NLSL, ETWT, ELWL), yellow/all-red intervals, and the distinction between fixed-time vs. dynamic timing. Quick check: Given a 4-phase intersection with 3s yellow + 2s all-red per phase, what is the minimum cycle length if each phase runs for 15s green?
- **LLM Prompting and Structured Output Parsing**: Familiarity with structured prompt templates (d_intersection, d_task, d_details, d_req, d_rules, d_important) and parsing XML-like outputs. Quick check: How would you design a fallback rule if the LLM outputs a duration of 50s when the maximum allowed is 40s?
- **Preference Learning and Ranking Losses**: Understanding score-based fine-tuning that uses ranking loss prioritizing higher-scored trajectories. Quick check: In Equation 6, what happens to the loss if all corrected trajectories receive identical scores?

## Architecture Onboarding

- **Component map**: CityFlow Simulator → Herald Module → Structured Prompts → LLM-Agent (Llama-3.1-8B + LoRA) → Action Parser → LLM-Critic (ChatGPT) → Fine-tuning Dataset (scored corrections)
- **Critical path**: 1) Herald Module calibration (collect queue→release-time data per scenario), 2) Imitation data generation (ChatGPT + CityFlow + Herald prompts), 3) LoRA fine-tuning of LLM-Agent on imitation corpus, 4) Deploy Agent with Critic oversight; log hallucinations and corrections, 5) Periodic score-based fine-tuning using collected corrections
- **Design tradeoffs**: Critic choice (ChatGPT) provides higher reasoning quality but introduces API dependency and latency; duration granularity (1s vs. 5s) improves precision but increases action space complexity; LoRA vs. full fine-tuning reduces memory demands but may underfit on complex intersections
- **Failure signatures**: Hallucination symptoms include repetitive phase selections, durations exceeding 40s, invalid phase names, constraint violations; Herald Module failure manifests as implausible duration predictions (>40s for short queues); scalability bottleneck occurs at 196 intersections requiring distributed parallel inference with 9.09s batch-level latency
- **First 3 experiments**: 1) Reproduce ablation on Hangzhou-2 to verify base Llama-3.1-8B fails, base + Herald fails, imitation fine-tuning stabilizes control, and dual-LLM architecture reduces ATT, 2) Herald Module calibration on single intersection to collect queue→release-time data and fit piecewise-linear model, 3) Hallucination detection by running LLM-Agent alone for 1 hour, logging outputs, and measuring reduction rate when enabling Critic

## Open Questions the Paper Calls Out
- How can HeraldLight's LLM-Agent be adapted for online self-evolution, enabling continuous learning and policy refinement during live deployment without requiring offline retraining cycles?
- What is the sim-to-real gap when deploying HeraldLight on physical traffic signal infrastructure, and what adaptations are needed for real-world sensor noise, communication latency, and actuator constraints?
- Can HeraldLight maintain low hallucination rates under distribution shift scenarios (e.g., unusual traffic incidents, emergency vehicle preemption) not represented in imitation training corpus?
- How does inference latency scale with network size, and can HeraldLight meet real-time control deadlines on networks larger than 196 intersections?

## Limitations
- Unknown prompt templates for the six structured prompt blocks prevent exact reproduction of the LLM-Agent's behavior
- Missing LoRA hyperparameters (rank, alpha, target modules, learning rate) and score thresholds for ranking loss
- Simulation-based results may not translate directly to real-world deployments with sensor noise and communication delays

## Confidence
- High: Dual LLM architecture's ability to reduce hallucination (98.4% drop) is well-supported by reported ablation and correction rates
- Medium: 20.03% ATT reduction claim is supported by multi-city experiments but depends on exact implementation of unknown prompt templates
- Low: Scalability claims for New York's 196 intersections rely on synchronous batching performance not fully detailed

## Next Checks
1. Reconstruct the six structured prompt blocks from paper description and test whether they reproduce reported hallucination reduction with Llama-3.1-8B + ChatGPT critic
2. On single intersection, collect queue-to-release-time data, fit piecewise-linear model, and verify predicted vs. actual release times match abstraction claims
3. Train HeraldLight on Jinan-1, then deploy on Hangzhou-2 and measure ATT/AQL degradation to validate claimed strong transferability