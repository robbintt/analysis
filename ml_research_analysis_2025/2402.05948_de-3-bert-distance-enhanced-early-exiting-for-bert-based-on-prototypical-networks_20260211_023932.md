---
ver: rpa2
title: 'DE$^3$-BERT: Distance-Enhanced Early Exiting for BERT based on Prototypical
  Networks'
arxiv_id: '2402.05948'
source_url: https://arxiv.org/abs/2402.05948
tags:
- exiting
- early
- bert
- inference
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of early exiting methods for
  BERT that rely solely on local information from individual test samples to make
  exiting decisions, leading to unreliable predictions and performance degradation.
  The authors propose DE3-BERT, a distance-enhanced early exiting framework that incorporates
  global information through prototypical networks and a distance metric between samples
  and class prototypes.
---

# DE$^3$-BERT: Distance-Enhanced Early Exiting for BERT based on Prototypical Networks

## Quick Facts
- **arXiv ID:** 2402.05948
- **Source URL:** https://arxiv.org/abs/2402.05948
- **Reference count:** 40
- **Primary result:** DE$^3$-BERT achieves better trade-off between model performance and inference efficiency on GLUE benchmark, consistently outperforming state-of-the-art models under different speed-up ratios with minimal computational overhead.

## Executive Summary
This paper addresses the limitation of early exiting methods for BERT that rely solely on local information from individual test samples to make exiting decisions, leading to unreliable predictions and performance degradation. The authors propose DE3-BERT, a distance-enhanced early exiting framework that incorporates global information through prototypical networks and a distance metric between samples and class prototypes. This hybrid approach combines classic entropy-based local information with distance-based global information to improve the estimation of prediction correctness for more reliable early exiting decisions. Experiments on the GLUE benchmark demonstrate that DE3-BERT consistently outperforms state-of-the-art models under different speed-up ratios with minimal additional computational overhead, achieving a better trade-off between model performance and inference efficiency.

## Method Summary
DE$^3$-BERT modifies standard BERT by attaching both an internal classifier and a prototypical network to each intermediate encoder layer. During training, it jointly optimizes classification loss with a distance-aware regularization term that encourages intra-class compactness in a learned metric space. Class prototypes are maintained as sliding averages of training sample representations. At inference, the model computes a hybrid exiting indicator that combines normalized prediction entropy with normalized distance ratio to class prototypes, exiting early when this indicator falls below a threshold. This architecture allows the model to leverage both local confidence signals and global geometric information about class clusters.

## Key Results
- DE$^3$-BERT consistently outperforms state-of-the-art early exiting models on GLUE benchmark across different speed-up ratios
- Ablation studies validate the effectiveness of each component, particularly the distance-based global information
- The method achieves significant inference speed-up with minimal performance degradation
- Minimal computational overhead is added to the baseline BERT architecture

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The reliability of early exiting decisions is improved by fusing local entropy with global distance metrics.
- **Mechanism:** A "Hybrid Exiting Indicator" ($EDR$) is computed as the harmonic mean of (1) the normalized entropy of the prediction (local signal) and (2) a normalized distance ratio comparing the distance from the sample to the top-2 class prototypes (global signal).
- **Core assumption:** The geometry of the latent space correlates with semantic correctness; specifically, samples closer to their predicted class prototype are more likely to be correctly classified, regardless of the softmax entropy.
- **Evidence anchors:** [abstract] Mentions implementing a "hybrid exiting strategy that supplements classic entropy-based local information with distance-based global information." [section III-C] Defines the $EDR$ calculation (Eq. 12) and the distance ratio (Eq. 11). [corpus] Related work *Improving Prediction Certainty Estimation for Reliable Early Exiting* supports the premise that estimating prediction correctness is a distinct challenge from simple confidence.
- **Break condition:** In domains where the latent space geometry does not form distinct class clusters (high intrinsic dimensionality or significant class overlap), the distance ratio may act as noise rather than a corrective signal.

### Mechanism 2
- **Claim:** Population-level statistics (class prototypes) provide a stable reference frame for estimating correctness when ground-truth labels are unavailable during inference.
- **Mechanism:** During training, a prototypical network projects hidden states into a metric space. Class prototypes ($cp_k$) are updated via a sliding average of class centroids. During inference, the model calculates the cosine distance between the sample representation and these fixed prototypes.
- **Core assumption:** The training data distribution is representative of the inference data distribution (IID assumption), ensuring that the pre-computed prototypes remain relevant anchors for test samples.
- **Evidence anchors:** [section III-B] Describes the update strategy for class prototypes (Eq. 1) using a sliding average with factor $\gamma$. [section IV-E] Ablation studies show performance degradation when prototypical networks are removed. [corpus] Corpus evidence for *Prototypicality Bias* suggests that relying on prototypes can sometimes favor "socially prototypical" or biased representations.
- **Break condition:** Under significant covariate shift (Out-Of-Distribution data), the fixed prototypes may drift away from the actual test sample clusters, rendering the distance metric misleading.

### Mechanism 3
- **Claim:** Joint optimization using Distance-Aware Regularization (DAR) creates a metric space conducive to the distance-based exit criteria.
- **Mechanism:** The training objective adds a regularization term (based on Center Loss) that minimizes the cosine distance between a sample and its ground-truth class prototype. This forces intra-class compactness.
- **Core assumption:** The optimization landscape allows for simultaneous minimization of classification cross-entropy (which focuses on boundaries) and intra-class distance (which focuses on centroids) without destructive interference.
- **Evidence anchors:** [section III-B] Formulates the loss function $L_{DAR}$ (Eq. 2) and the total loss (Eq. 6). [fig 10] Visualizations (t-SNE) show that training with DAR results in clearer classification boundaries compared to training without it.
- **Break condition:** If the regularization coefficient $\alpha$ is set too high, the model may focus excessively on creating tight clusters at the expense of linear separability required for the classifier head.

## Foundational Learning

- **Concept:** **Early Exiting & Overthinking**
  - **Why needed here:** The paper addresses the "overthinking" problem where deep models degrade on easy samples. Understanding this trade-off is necessary to evaluate *why* the complexity of DE³-BERT is justified.
  - **Quick check question:** Why does the paper argue that a high confidence score (low entropy) is an insufficient signal for exiting?

- **Concept:** **Prototypical Networks**
  - **Why needed here:** This is the architectural core of the paper. One must understand that a "prototype" is a representation of the class centroid in a learned embedding space, not just a semantic label.
  - **Quick check question:** How is the distance between a sample and a prototype calculated in this architecture, and what distance metric is used?

- **Concept:** **Harmonic Mean**
  - **Why needed here:** The fusion strategy (EDR) uses harmonic mean rather than arithmetic mean.
  - **Quick check question:** Why is the harmonic mean preferred for combining Entropy and Distance Ratio? (Hint: Think about how it penalizes extreme values in one metric vs. the other).

## Architecture Onboarding

- **Component map:** Input -> Encoder Block m -> Internal Classifier -> Probability p^(m) and Entropy E; Input -> Prototypical Network -> Representation h'^(m) -> Distance calculation -> Distance Ratio DR; EDR = HarmonicMean(E, DR) -> Exit Decision

- **Critical path:**
  1. **Input** passes through Encoder Block $m$.
  2. **Branch A (Local):** Hidden state $h^{(m)} \to$ Internal Classifier $\to$ Probability $p^{(m)} \to$ Entropy $E$.
  3. **Branch B (Global):** Hidden state $h^{(m)} \to$ Prototypical Network $\to$ Representation $h'^{(m)} \to$ Distance calculation against top-2 prototypes $\to$ Distance Ratio $DR$.
  4. **Fusion:** Compute $EDR = \text{HarmonicMean}(E, DR)$.
  5. **Decision:** If $EDR < \tau$, **EXIT**; else proceed to layer $m+1$.

- **Design tradeoffs:**
  - **Latency vs. Overhead:** The prototypical network adds computational overhead (1.2M FLOPs) to every layer, even if the sample exits early. This is negligible compared to the encoder block (1813.5M FLOPs) but non-zero.
  - **Memory:** Requires storing $M \times K$ prototype vectors (where $M$=layers, $K$=classes).
  - **OOD Sensitivity:** The mechanism relies on fixed prototypes, making it potentially brittle to domain shifts compared to pure entropy-based methods.

- **Failure signatures:**
  - **High Entropy & Low Distance:** Sample is confident in geometry (close to prototype) but the classifier is uncertain. This might force deeper processing unnecessarily.
  - **OOD Drift:** If test data clusters shift away from training prototypes, $DR$ values will consistently be high/poor, forcing all samples to the final layer (losing speedup).

- **First 3 experiments:**
  1. **Threshold Sweep:** Plot Accuracy vs. Speed-up Ratio on QNLI by varying $\tau$ to replicate the trade-off curves (Fig 4). Identify the "knee" where performance drops sharply.
  2. **Ablation on $\lambda$:** Fix the speed-up ratio (e.g., 2.0x) and vary the fusion coefficient $\lambda$ (Eq 12) to determine the optimal balance between entropy and distance for a specific dataset (e.g., SST-2).
  3. **Cluster Visualization:** Extract embeddings $h'$ from a trained model and visualize with t-SNE (replicating Fig 10) to confirm that DAR is actually producing the tight clusters the mechanism relies upon.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a neural network effectively map continuous regression values to prototype representations to extend the DE³-BERT framework to regression tasks?
- **Basis:** [explicit] Appendix E-B lists extending the framework to regression tasks as a primary future study.
- **Why unresolved:** The current framework relies on discrete class prototypes, which are unavailable for continuous regression targets.
- **Evidence:** Successful application and performance maintenance on standard regression benchmarks.

### Open Question 2
- **Question:** How can the framework be modified to maintain reliable exiting decisions when test data distributions differ significantly from training data?
- **Basis:** [explicit] Appendix E-B notes that the assumption of identical distribution creates extreme challenges for the method in out-of-distribution (OOD) scenarios.
- **Why unresolved:** The method transfers knowledge via prototypes derived from training data, which may be misleading in OOD settings.
- **Evidence:** An adaptation method that maintains performance-efficiency trade-offs on cross-domain datasets without requiring offline batch adjustments.

### Open Question 3
- **Question:** Does integrating distance-enhanced exiting with orthogonal methods focused on training schemes or architecture yield cumulative acceleration benefits?
- **Basis:** [explicit] Appendix E-B suggests exploring combinations with methods like GPFEE, LeeBERT, and COSEE to investigate potential orthogonality.
- **Why unresolved:** It is currently unclear if the hybrid exiting strategy conflicts with or complements alternative internal classifier enhancements.
- **Evidence:** Experiments showing whether the combined approach outperforms the sum of individual improvements on the GLUE benchmark.

## Limitations

- The method's effectiveness relies on the assumption that test data follows the same distribution as training data, which may not hold in real-world scenarios with domain shifts.
- The distance metric may provide misleading signals in high-dimensional latent spaces where class clusters overlap significantly or have complex geometries.
- Fixed prototypes may not adapt to evolving data distributions over time, potentially degrading performance on streaming or continuously updated datasets.

## Confidence

- **High Confidence:** The core claim that combining local entropy with global distance metrics improves early exiting reliability is well-supported by ablation studies and consistent performance gains across GLUE tasks.
- **Medium Confidence:** The claim about Distance-Aware Regularization creating compact class clusters is supported by t-SNE visualizations but relies on visual inspection rather than quantitative metrics of cluster quality.
- **Low Confidence:** The claim about OOD robustness is only implicitly supported. The fixed prototype approach may degrade under significant domain shift, but this is not empirically tested.

## Next Checks

1. **OOD Robustness Test:** Evaluate DE3-BERT on domain-shifted versions of GLUE tasks (e.g., using different genres or time periods) to quantify degradation in the distance-based signal.

2. **Prototype Dimension Sensitivity:** Systematically vary the output dimension of the Prototypical Network (e.g., 128, 256, 768) and measure the impact on both performance and computational overhead.

3. **Computational Overhead Profiling:** Measure actual wall-clock time per layer with and without the Prototypical Network to verify that the claimed "negligible overhead" holds across different hardware configurations.