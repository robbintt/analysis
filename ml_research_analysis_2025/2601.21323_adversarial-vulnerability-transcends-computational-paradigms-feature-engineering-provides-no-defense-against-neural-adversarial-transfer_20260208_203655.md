---
ver: rpa2
title: 'Adversarial Vulnerability Transcends Computational Paradigms: Feature Engineering
  Provides No Defense Against Neural Adversarial Transfer'
arxiv_id: '2601.21323'
source_url: https://arxiv.org/abs/2601.21323
tags:
- adversarial
- fgsm
- accuracy
- transfer
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates whether adversarial examples
  crafted on deep neural networks transfer to classical machine learning classifiers
  using Histogram of Oriented Gradients (HOG) features. Testing four classical models
  (KNN, Decision Tree, Linear SVM, Kernel SVM) and a shallow neural network across
  eight HOG configurations on CIFAR-10, the research reveals that all classifiers
  suffer substantial accuracy degradation (16.6%-59.1% relative drop) when attacked
  via VGG16-crafted FGSM and PGD perturbations.
---

# Adversarial Vulnerability Transcends Computational Paradigms: Feature Engineering Provides No Defense Against Neural Adversarial Transfer

## Quick Facts
- arXiv ID: 2601.21323
- Source URL: https://arxiv.org/abs/2601.21323
- Authors: Achraf Hsain; Ahmed Abdelkader; Emmanuel Baldwin Mbaya; Hamoud Aljamaan
- Reference count: 19
- One-line primary result: HOG-based classical ML classifiers suffer substantial accuracy degradation (16.6%-59.1% relative drop) when attacked via VGG16-crafted FGSM and PGD perturbations, with FGSM consistently outperforming PGD in cross-paradigm transfer.

## Executive Summary
This study systematically evaluates whether adversarial examples crafted on deep neural networks transfer to classical machine learning classifiers using Histogram of Oriented Gradients (HOG) features. Testing four classical models (KNN, Decision Tree, Linear SVM, Kernel SVM) and a shallow neural network across eight HOG configurations on CIFAR-10, the research reveals that all classifiers suffer substantial accuracy degradation (16.6%-59.1% relative drop) when attacked via VGG16-crafted FGSM and PGD perturbations. Notably, the study discovers "attack hierarchy reversal" where FGSM consistently outperforms PGD in cross-paradigm transfer—contrary to established neural-to-neural patterns—suggesting iterative attacks overfit to surrogate-specific features that don't survive feature extraction. Block normalization provides partial mitigation but insufficient protection. These findings demonstrate that adversarial vulnerability is a fundamental property of image classification systems, not merely an artifact of end-to-end differentiability, with significant implications for security-critical deployments of classical ML systems.

## Method Summary
The study fine-tunes VGG16-BN on CIFAR-10 (98% train/89% test accuracy) and generates adversarial examples using FGSM and PGD (L∞-bounded, ε∈{4/255, 8/255}). Eight HOG configurations vary cell size (6,8,10), orientations (3,6,9), and block normalization (1,2,3). Classical ML classifiers (KNN, Decision Tree, Linear SVM, Kernel SVM, shallow ANN) are trained on clean HOG features only, then evaluated on clean, FGSM-perturbed, and PGD-perturbed test sets. The evaluation measures accuracy degradation relative to clean performance, comparing cross-paradigm transfer against neural-to-neural baselines.

## Key Results
- All classifiers show substantial accuracy degradation (16.6%-59.1% relative drop) under transferred attacks
- FGSM causes greater degradation than PGD in 100% of classical ML cases (32/32 configuration-model pairs)
- Block normalization provides partial mitigation but insufficient protection
- Transferred attack effectiveness comparable to or exceeding neural-to-neural baselines
- Attack hierarchy reversal contradicts established neural-to-neural patterns where PGD typically dominates FGSM

## Why This Works (Mechanism)

### Mechanism 1
Adversarial perturbations optimized against CNN surrogate models transfer effectively to HOG-based classifiers despite the gradient quantization and spatial binning bottleneck. VGG16-crafted FGSM/PGD perturbations corrupt fundamental edge structures in pixel space that HOG's gradient computation preserves and encodes, causing misclassification in downstream classical ML models trained on clean HOG features. The adversarial signal exploits low-level image properties (edge orientations) rather than model-specific high-level features.

### Mechanism 2
FGSM causes greater accuracy degradation than PGD in cross-paradigm transfer—the opposite of neural-to-neural patterns. PGD's iterative optimization exploits surrogate-specific high-level features (e.g., VGG's learned representations) that do not survive HOG's gradient-based encoding, while FGSM's single-step perturbation more directly corrupts low-level edge structures HOG preserves. Iterative attacks overfit to surrogate architecture-specific features; single-step attacks exploit more transferable gradient directions.

### Mechanism 3
Block normalization provides partial robustness improvement by inducing spatial averaging that attenuates high-frequency perturbations. Larger cells-per-block (B=3 vs. B=1) induces L2 contrast normalization over broader spatial regions, smoothing localized adversarial perturbations before they corrupt the full feature vector. Adversarial perturbations are partially high-frequency signals that spatial averaging can attenuate.

## Foundational Learning

- **Adversarial Transferability**: The entire paper examines whether attacks crafted on one model (VGG16) fool entirely different systems (HOG+classical ML) without knowledge of target architecture. Can you explain why an attack optimized against one model's loss landscape might fool a completely different model?
- **FGSM vs. PGD Attacks**: The "attack hierarchy reversal" discovery hinges on understanding that PGD normally dominates FGSM in neural-to-neural transfer, but this paper shows the opposite for cross-paradigm transfer. What is the key difference between single-step FGSM and iterative PGD in how they compute perturbations?
- **HOG Feature Extraction Pipeline**: The hypothesis being tested is that HOG's gradient quantization and spatial binning might filter adversarial signals—understanding this pipeline is essential. How does HOG encode image information, and what preprocessing steps might affect whether adversarial perturbations survive?

## Architecture Onboarding

- **Component map**: CIFAR-10 (32×32) → bilinear upsampling (64×64) → VGG16 surrogate (fine-tuned) → FGSM/PGD attacks (L∞, ε∈{4/255,8/255}) → HOG feature extraction (8 configurations) → classical ML classifiers (KNN, DT, LSVM, KSVM, ANN) → accuracy evaluation
- **Critical path**: Upsample CIFAR-10 images to 64×64 → generate adversarial examples via VGG16 → extract HOG features from clean/FGSM/PGD images → train each classifier on clean HOG features only → evaluate on all three test sets; compute relative accuracy drop
- **Design tradeoffs**: Single surrogate (VGG16) limits generalizability but enables controlled comparison; accuracy-only metric ignores per-class vulnerability differences; no repeated trials due to hardware constraints
- **Failure signatures**: If HOG filtered adversarial signals: accuracy degradation near zero; if PGD dominated FGSM: PGD curves below FGSM in Figure 2 (opposite observed); if block normalization worked fully: B=3 configurations with no degradation
- **First 3 experiments**: 1) Reproduce baseline transfer: replicate VGG→AlexNet transfer baseline to confirm attack generation pipeline works correctly; 2) Single HOG configuration test: pick C1 and verify FGSM causes larger drops than PGD on KNN and KSVM; 3) Block size ablation: compare B=1, B=2, B=3 on a single classifier to confirm partial mitigation effect

## Open Questions the Paper Calls Out

- **Do attacks crafted on architecturally diverse surrogates (e.g., ResNet, EfficientNet, Vision Transformers) exhibit different transfer characteristics to HOG-based classifiers compared to VGG16?**: Only VGG16 was tested as surrogate; architectural properties (skip connections, attention mechanisms) may affect how perturbations overfit to surrogate-specific features. Systematic evaluation using multiple surrogate architectures with controlled comparisons, measuring transfer success rates to identical HOG-based target classifiers would resolve this.

- **Does the attack hierarchy reversal phenomenon (FGSM outperforming PGD in transfer) generalize to higher-resolution datasets such as ImageNet?**: CIFAR-10's 32×32 resolution limits HOG's spatial binning capacity; higher-resolution images provide more gradient information that may interact differently with HOG's quantization. Replication of the experimental protocol on ImageNet or comparable datasets, comparing FGSM vs. PGD transfer effectiveness across HOG configurations would resolve this.

- **Can input preprocessing techniques (JPEG compression, spatial smoothing) or adversarial training on HOG features effectively mitigate cross-paradigm adversarial transfer?**: The study focused only on vulnerability characterization; no defense mechanisms were evaluated. Empirical evaluation of preprocessing pipelines and adversarial training protocols measuring accuracy retention under transferred attacks would resolve this.

## Limitations

- **Surrogate selection bias**: Using only VGG16 as surrogate may overestimate transferability to classical ML; results could differ with ResNet or EfficientNet surrogates.
- **Single-step hyperparameter search**: HOG configuration selection (C1 baseline) used only accuracy, not robustness, without specifying whether this captured the most robust configuration.
- **No confidence intervals**: Hardware constraints prevented repeated trials, leaving uncertainty about whether observed differences (especially the FGSM vs. PGD reversal) are statistically significant.

## Confidence

- **High confidence**: The core finding that HOG-based classical ML classifiers are vulnerable to adversarial transfer from neural surrogates (Section IV-A). This is supported by consistent degradation across all 32 configuration-model pairs with clear statistical patterns.
- **Medium confidence**: The "attack hierarchy reversal" (FGSM > PGD in cross-paradigm transfer). While the data shows this pattern consistently, the mechanism (iterative attacks overfitting to surrogate-specific features) lacks direct empirical validation and relies on indirect reasoning.
- **Low confidence**: Block normalization's partial mitigation effect. The evidence shows improvement but is based on a single block size comparison (B=1→B=3) without exploring intermediate values or testing whether this mitigates the fundamental vulnerability or merely shifts it.

## Next Checks

1. **Surrogate diversity test**: Repeat the experiment with ResNet-18 and MobileNet as surrogates to determine if VGG16-specific features drive the observed transferability patterns.
2. **Statistical significance verification**: Implement 10-fold cross-validation to generate confidence intervals for accuracy drops, particularly for the FGSM vs. PGD comparison where the reversal is most surprising.
3. **Per-class vulnerability analysis**: Extend evaluation to compute class-wise accuracy drops to determine if certain object categories (e.g., animals vs. vehicles) show systematically different vulnerability patterns across classical ML models.