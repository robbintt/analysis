---
ver: rpa2
title: Learning to Integrate Diffusion ODEs by Averaging the Derivatives
arxiv_id: '2505.14502'
source_url: https://arxiv.org/abs/2505.14502
tags:
- diffusion
- arxiv
- training
- sampling
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the slow inference speed of diffusion models
  by proposing a new method to reduce the number of required steps. The core idea
  is to learn the integration of diffusion ODEs using loss functions derived from
  the derivative-integral relationship, termed secant losses.
---

# Learning to Integrate Diffusion ODEs by Averaging the Derivatives

## Quick Facts
- arXiv ID: 2505.14502
- Source URL: https://arxiv.org/abs/2505.14502
- Reference count: 40
- Proposed method achieves 2.14 FID on CIFAR-10 with 10 steps and 2.27/1.96 FID on ImageNet-256×256 with 4/8 steps

## Executive Summary
This work addresses the slow inference speed of diffusion models by proposing a new method to reduce the required number of sampling steps. The core idea is to learn the integration of diffusion ODEs using loss functions derived from the derivative-integral relationship, termed secant losses. These losses operate by gradually extending the tangent to the secant, providing a geometric interpretation. By fine-tuning or distillation, the secant version of EDM achieves strong performance with fewer steps compared to standard diffusion models.

## Method Summary
The paper introduces secant losses that replace the standard denoising loss in diffusion models. These losses are derived from the integral relationship between the diffusion ODE and its derivative, operating by gradually extending the tangent to the secant. The method can be applied through fine-tuning existing diffusion models or via distillation. The geometric interpretation provides intuition for why this approach enables faster sampling, as it directly optimizes for the integral path rather than local derivatives.

## Key Results
- Secant version of EDM achieves 10-step FID of 2.14 on CIFAR-10
- Secant version of SiT-XL/2 attains 4-step FID of 2.27 and 8-step FID of 1.96 on ImageNet-256×256
- Demonstrates effective balance between performance and computational cost

## Why This Works (Mechanism)
The secant loss function optimizes for the integral path of the diffusion ODE rather than just the local derivative. By learning to integrate the ODE through averaging derivatives, the model can make larger, more accurate jumps during sampling. This geometric approach of extending tangents to secants allows the model to capture longer-range dependencies in the trajectory, enabling faster convergence with fewer sampling steps.

## Foundational Learning
- Diffusion models and their relationship to ODEs: Understanding how diffusion models can be formulated as ODEs is crucial for grasping the secant loss concept
  - Why needed: Provides the mathematical foundation for the secant approach
  - Quick check: Can you explain the forward and reverse processes in diffusion models using ODE notation?
- Integral relationships in differential equations: The secant loss leverages the fundamental theorem of calculus
  - Why needed: Essential for understanding how averaging derivatives relates to integration
  - Quick check: Can you derive the relationship between a function's derivative and its integral?
- Monte Carlo estimation: The secant loss uses expectation over intermediate time points
  - Why needed: Understanding how the loss is computed in practice
  - Quick check: Can you explain how Monte Carlo methods approximate expectations?

## Architecture Onboarding
- Component map: Diffusion model architecture -> Secant loss integration -> Sampling with fewer steps
- Critical path: Model training with secant loss -> Fine-tuning or distillation -> Fast inference
- Design tradeoffs: The method balances between maintaining diffusion model stability and achieving faster sampling
- Failure signatures: Poor performance may indicate issues with the bootstrapping process or inappropriate choice of sampling distribution
- First experiments: 1) Compare 1-step vs multi-step generation quality, 2) Analyze error propagation across chained secant steps, 3) Test different sampling distributions for the intermediate time point

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What is the theoretical relationship between classifier-free guidance (CFG) and secant losses, and does it differ fundamentally from standard diffusion guidance?
- Basis: The authors state in Section 6 (Limitations) that "the theoretical relationship between CFG and secant losses may be explored in future work."
- Why unresolved: While the paper applies CFG empirically, it does not derive how the guidance scale interacts with the integral-based secant objective compared to the derivative-based diffusion objective
- Evidence: A formal derivation of the optimal guidance formulation within the secant framework, or empirical analysis showing if standard CFG scaling laws apply to secant-trained models

### Open Question 2
- Question: Can the significant performance gap between 1-step and multi-step generation be closed without introducing adversarial training or instability?
- Basis: Section 6 identifies the "significant performance gap between 1-step and 8-step generation" as a specific limitation
- Why unresolved: The secant loss prioritizes local trajectory accuracy, which naturally aligns with multi-step solvers, whereas 1-step generation requires accurate global jumps
- Evidence: Modifications to the loss function or architecture that significantly improve 1-step FID scores to rival consistency or adversarial distillation methods without training divergence

### Open Question 3
- Question: What are the theoretical guarantees regarding error propagation when extending secant losses from small local neighborhoods to the full time interval via bootstrapping?
- Basis: Theorems 2 and 3 strictly guarantee convergence only in "sufficiently small neighborhoods" ($|s-t| \le h$), yet the practical application relies on bootstrapping
- Why unresolved: It is unclear if errors accumulated during bootstrapping remain bounded or if Lipschitz assumptions hold sufficiently to prevent divergence
- Evidence: A theoretical proof of global convergence bounds or an empirical analysis quantifying error accumulation rates across chained secant steps

### Open Question 4
- Question: What is the optimal sampling distribution $q(r)$ for the intermediate time point $r$ to minimize variance and maximize convergence speed in secant expectation?
- Basis: The paper utilizes uniform sampling for $r$ "for simplicity" but notes that alternative distributions could be used for importance sampling
- Why unresolved: While uniform sampling works, it is unknown if a density biased towards high-curvature regions could reduce variance
- Evidence: A derivation of the optimal importance sampling distribution for the secant integral or ablation studies comparing variance reduction strategies

## Limitations
- Performance improvements demonstrated primarily on CIFAR-10 and ImageNet-256×256 datasets, leaving uncertainty about generalizability
- Method requires fine-tuning or distillation of existing diffusion models, adding computational overhead
- Geometric interpretation lacks formal mathematical grounding showing why extending tangents to secants specifically improves sampling efficiency

## Confidence
- High confidence in experimental results on tested datasets
- Medium confidence in claimed performance improvements based on limited experimental validation
- Low confidence in theoretical contributions as geometric intuition is presented without formal proof

## Next Checks
1. Test the method on additional datasets including higher-resolution images and non-image domains to assess generalizability
2. Conduct ablation studies to quantify the contribution of each component and validate the claimed training stability
3. Provide formal mathematical analysis of the secant loss properties and their relationship to the original diffusion ODE