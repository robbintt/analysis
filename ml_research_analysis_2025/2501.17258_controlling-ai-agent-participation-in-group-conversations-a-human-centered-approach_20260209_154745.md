---
ver: rpa2
title: 'Controlling AI Agent Participation in Group Conversations: A Human-Centered
  Approach'
arxiv_id: '2501.17258'
source_url: https://arxiv.org/abs/2501.17258
tags:
- koala
- participants
- group
- agent
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores how conversational AI agents can effectively
  participate in group brainstorming sessions. Through two user studies, researchers
  examined participants' experiences with a Slack-based AI agent called Koala in group
  ideation tasks.
---

# Controlling AI Agent Participation in Group Conversations: A Human-Centered Approach

## Quick Facts
- arXiv ID: 2501.17258
- Source URL: https://arxiv.org/abs/2501.17258
- Reference count: 40
- Primary result: Users prefer reactive AI agents over proactive ones in group brainstorming due to interruption concerns, despite AI contributing 73% of ideas.

## Executive Summary
This paper explores how conversational AI agents can effectively participate in group brainstorming sessions without overwhelming participants. Through two user studies with a Slack-based AI agent called Koala, researchers found that participants valued having the AI agent present, with 73% of ideas generated coming from Koala and 33% of top ideas being AI-generated. However, participants strongly preferred the reactive variant over the proactive one due to concerns about being overwhelmed and interrupted. Based on these findings, the researchers developed a taxonomy of controls for AI agent participation in group conversations, covering when, what, and where to contribute, who can control the behavior, and how those controls are specified and implemented.

## Method Summary
The researchers conducted two user studies using a Slack-based AI agent called Koala that participated in group brainstorming sessions. The agent used an LLM (Llama 2 in Study 1, Llama 3 in Study 2) to generate and evaluate responses based on conversation context. Participants could configure the agent's behavior through a control panel, adjusting parameters like contribution threshold and reply timing. The agent evaluated its own responses by generating a structured JSON output containing source, target, reply, evaluation, value score (0-100), and decision (<SUBMIT> or <PASS>). The studies compared reactive (user-initiated) and proactive (agent-initiated) variants to understand user preferences for AI participation in group conversations.

## Key Results
- Participants generated significantly more ideas with Koala present (73% of all ideas were AI-generated)
- The reactive variant of Koala was strongly preferred over the proactive variant due to interruption concerns
- 33% of top-rated ideas were generated by the AI agent
- Users valued having AI participation but wanted control over when and how the agent contributed

## Why This Works (Mechanism)
The approach works by combining LLM-based contextual understanding with user-controlled participation parameters. The agent evaluates its own responses using a structured self-assessment process that generates a value score, allowing it to filter contributions based on relevance and quality. The control panel enables users to specify when the agent should contribute (e.g., only when directly addressed, or proactively based on a threshold), what types of contributions are appropriate, and how those behaviors are implemented. This creates a flexible system that can adapt to different group dynamics while respecting user preferences for conversational flow and minimizing interruptions.

## Foundational Learning

- **Mixed-Initiative Interaction:**
  - Why needed here: This is the core conceptual framework for the research. It moves beyond simple human-AI turn-taking to a dynamic partnership where initiative can shift between human and AI at any time. Understanding this is critical to grasping why a static agent design fails.
  - Quick check question: In a co-creative brainstorming session, what happens if the AI's initiative is fixed at a constant high level?

- **Turn-Taking in Multi-Party Conversation:**
  - Why needed here: The central problem the paper addresses is not *what* to say, but *when* to say it in a group. This concept involves understanding addressees ("who is talking to whom?") and conversational flow, which are non-trivial in a group chat with overlapping messages.
  - Quick check question: Why is a simple "reply to all messages" strategy ineffective for an AI agent in a multi-person Slack channel?

- **LLM-based Agent Architecture:**
  - Why needed here: The agent's implementation relies on an LLM for core functions like generating ideas and, crucially, evaluating their own relevance. A basic understanding of LLMs as non-deterministic engines guided by prompts and system instructions is required to appreciate the design challenges.
  - Quick check question: What are the two main ways the Koala agent uses its underlying LLM to determine whether to post a message?

## Architecture Onboarding

### Component Map
Slack API Event Listener -> Pre-Processing & External Control Logic -> LLM Inference Engine -> Self-Evaluation Module -> Post-Processing & Action Filter -> Slack Channel/Thread

### Critical Path
The critical path for every proactive turn is the decision cycle: `Message Received -> External Logic -> LLM Generation & Self-Score -> Threshold Check -> Post Message`. Any latency in this pipeline, especially the LLM inference, directly affects the agent's ability to participate in real-time.

### Design Tradeoffs
- **LLM vs. Rules-Based Control:** The authors chose to use the LLM for complex contextual judgment (valuing a contribution) but reverted to hard-coded rules for simple, high-stakes tasks (identifying a direct mention). The tradeoff is flexibility vs. reliability.
- **Control Panel Complexity:** Exposing controls empowers users but risks overwhelming them. The study found that users wanted high-level controls (like roles) but also access to low-level settings.
- **Proactivity vs. Intrusiveness:** A highly proactive agent can spur creativity but also disrupt flow. The main design challenge is tuning the `value` threshold.

### Failure Signatures
- **The "Overbearing Participant":** The agent posts too frequently, dominating the conversation. This indicates a `value` threshold that is set too low or a mis-calibrated self-evaluation prompt.
- **The "Ghost":** The agent fails to respond even when directly addressed or when a clear contribution opportunity exists. This points to a `value` threshold that is too high or external logic incorrectly suppressing replies.
- **The "Off-Topic Rambler":** The agent posts responses that are irrelevant or tangentially related. This suggests the conversation history in the prompt is insufficient or the LLM is failing to follow the persona instructions.

### First 3 Experiments
1. **Proactivity Tuning Test:** Run simulated brainstorming sessions with varying `proactive contribution threshold` values (e.g., 50, 75, 90). Measure the ratio of agent-to-human messages and have human raters score agent intrusiveness.
2. **Targeting Accuracy Test:** Feed the agent's pre-processing module a dataset of group chat messages. Measure the precision and recall of its hard-coded rules for identifying the correct addressee (e.g., `@Koala` vs. mentioning another participant's name).
3. **Control Elicitation Study:** Following the paper's method, conduct a small user study with a prototype agent. Intentionally configure it to be slightly over-proactive, then observe and interview users about what specific behaviors they want to control. Use this to populate the taxonomy.

## Open Questions the Paper Calls Out
- **Open Question 1:** How does the required taxonomy of controls change when an AI agent participates in group activities other than ideation, such as decision making, cooperative learning, or game playing?
- **Open Question 2:** Which specific behavioral controls are most critical to users, and how does the frequency of adjustment differ between various group compositions?
- **Open Question 3:** Can design-time aspects of an AI agent's behavior, typically fixed by developers, be effectively exposed as real-time customizable parameters for end-users?

## Limitations
- The control taxonomy was derived exclusively from group brainstorming and ideation tasks, leaving its applicability to other interaction types unknown
- The study relied on a single AI agent (Koala) with a specific persona and limited training data (Llama 3)
- Specific threshold values (90, 75, 50) for the "value" score and their relationship to perceived intrusiveness are based on limited experimentation

## Confidence
- **High Confidence:** The finding that participants strongly preferred reactive over proactive agent behavior is well-supported by both studies and aligns with established human-computer interaction principles about conversational flow and interruptions.
- **Medium Confidence:** The control taxonomy itself is well-constructed based on user feedback, but its completeness and optimal structure for different contexts remains uncertain.
- **Low Confidence:** The specific threshold values (90, 75, 50) for the "value" score and their relationship to perceived intrusiveness are based on limited experimentation.

## Next Checks
1. **Cross-Scenario Validation:** Test the control taxonomy with the same agent design in different collaborative contexts (e.g., decision-making, problem-solving) to identify which controls are universal versus context-specific.
2. **Threshold Sensitivity Analysis:** Systematically vary the "value" score thresholds across a wider range (e.g., 60, 80, 95) while measuring both contribution quality and perceived intrusiveness to establish more precise guidelines for different group sizes and task types.
3. **Longitudinal Group Dynamics Study:** Conduct extended sessions (multiple days/weeks) with the same groups to examine how control preferences and agent effectiveness evolve as participants develop working patterns and familiarity with the AI participant.