---
ver: rpa2
title: 'Plan Then Retrieve: Reinforcement Learning-Guided Complex Reasoning over Knowledge
  Graphs'
arxiv_id: '2510.20691'
source_url: https://arxiv.org/abs/2510.20691
tags:
- reasoning
- knowledge
- retrieval
- search
- graph-rft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Graph-RFT, a two-stage reinforcement fine-tuning
  framework for Knowledge Graph Question Answering (KGQA) that addresses the challenge
  of incomplete knowledge graphs and complex reasoning. The method first uses supervised
  fine-tuning to teach structured planning and retrieval, then applies reinforcement
  learning with a multi-reward design that combines outcome-based and retrieval-specific
  rewards.
---

# Plan Then Retrieve: Reinforcement Learning-Guided Complex Reasoning over Knowledge Graphs

## Quick Facts
- arXiv ID: 2510.20691
- Source URL: https://arxiv.org/abs/2510.20691
- Reference count: 40
- Primary result: 80.7% accuracy on CWQ and 90.6% on WebQSP with 7B model under incomplete KGs

## Executive Summary
Graph-RFT introduces a two-stage reinforcement fine-tuning framework for KGQA that addresses the challenge of incomplete knowledge graphs and complex reasoning. The method first uses supervised fine-tuning to teach structured planning and retrieval, then applies reinforcement learning with a multi-reward design that combines outcome-based and retrieval-specific rewards. By employing Cartesian-inspired planning to decompose questions and logical expressions to guide tool invocation, Graph-RFT achieves coherent multi-step reasoning and adaptive retrieval scheduling across different levels of KG incompleteness.

## Method Summary
Graph-RFT operates in two stages: First, supervised fine-tuning on a customized plan-retrieval CoT dataset teaches the model structured decomposition and tool selection (Relation Search, Neighbor Search, Web Search). Second, reinforcement learning with a multi-reward design (including outcome accuracy, KG retrieval success, web retrieval success, and penalty terms) optimizes the policy for adaptive retrieval scheduling. The approach uses Cartesian-inspired planning to decompose complex questions into ordered sub-questions with logical dependencies, enabling globally consistent multi-step reasoning that outperforms state-of-the-art baselines even when knowledge graphs are incomplete.

## Key Results
- Achieves 80.7% accuracy on CWQ and 90.6% on WebQSP with a 7B model
- Demonstrates superior performance under varying KG incompleteness (20%, 40%, 60% removal)
- Shows robust planning and retrieval capabilities compared to existing methods
- Effective across different knowledge graph schemas (Freebase and Wikidata)

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Training for Reasoning Activation and RL Stability
Separating supervised CoT fine-tuning from reinforcement learning stabilizes training and initializes the model for effective tool coordination. The SFT stage defines a valid action space and reasoning format, acting as a warm start for the subsequent RL stage (GRPO) and preventing cold-start rollouts where the model might otherwise explore invalid action sequences or fail to produce coherent multi-step reasoning.

### Mechanism 2: Explicit Planning and Logical Decomposition for Global Reasoning Coherence
Decoupling question decomposition from execution allows for more coherent multi-step reasoning than methods that perform these steps concurrently. The model first produces a global plan breaking a complex question into ordered sub-questions with explicit logical dependencies, then subsequent retrieval actions are conditioned on this pre-computed plan, ensuring each tool invocation serves a specific sub-goal within the larger strategy.

### Mechanism 3: Multi-Reward Reinforcement Learning for Adaptive Retrieval Scheduling
A composite reward combining outcome accuracy with retrieval-specific feedback teaches the model to dynamically schedule KG and web searches, effectively compensating for incomplete knowledge graphs. The RL objective is guided by an overall reward with multiple components that reward answer correctness, presence of ground-truth in retrieved KG triples or web documents, and include penalties for unnecessary web searches when KG information is sufficient.

## Foundational Learning

- **Knowledge Graph Question Answering (KGQA)**
  - Why needed here: This is the core task. Understanding it involves knowing that the goal is to map a natural language question to a subgraph or entities in a Knowledge Graph (KG) to derive an answer.
  - Quick check question: Can you explain how a multi-hop KGQA query differs from a single-hop query in terms of the reasoning path required?

- **Reinforcement Learning (RL) from Human Feedback / Verifiable Rewards**
  - Why needed here: Graph-RFT uses RL (specifically GRPO) to optimize the model's policy. Understanding policy optimization, reward signals, and the exploration-exploitation trade-off is essential to grasp why this stage improves upon the initial SFT model.
  - Quick check question: What is the role of the `β * D_KL` term in the GRPO objective function, and why is a reference model (`π_ref`) used?

- **Tool-Augmented LLMs / Agentic Frameworks**
  - Why needed here: The model is not just generating text; it is an agent that must learn to invoke external tools (relation search, neighbor search, web search) in a specific sequence.
  - Quick check question: In the Graph-RFT framework, what specific signal or state change indicates to the model that it should switch from a Knowledge Graph search tool to a Web Search tool?

## Architecture Onboarding

- **Component map:**
  1. LLM Backbone (e.g., Qwen2.5-7B) -> Toolbox (Relation Search, Neighbor Search, Web Search) -> Planning Module (Cartesian decomposition) -> Reward Calculator (multi-component reward) -> GRPO Trainer (policy optimization)

- **Critical path:**
  1. Data Preparation: Construct SFT dataset by prompting stronger model and filtering for plan/retrieval correctness
  2. Stage 1 (SFT): Fine-tune LLM on CoT dataset to learn structured format and basic reasoning flow
  3. Stage 2 (RL - Rollout): SFT model generates trajectories by interacting with KG and web tools
  4. Stage 2 (RL - Reward & Update): Reward Calculator evaluates trajectories, GRPO Trainer updates policy using rewards

- **Design tradeoffs:**
  - Investing more in high-quality SFT data reduces RL instability but requires upfront annotation/filtering cost
  - More detailed rewards provide better learning signal but are more complex to tune
  - An upfront plan ensures global coherence but may be brittle for highly ambiguous questions

- **Failure signatures:**
  - Format Errors: Model generates text outside defined tags, indicating SFT failure
  - Myopic Retrieval: Model retrieves irrelevant triples repeatedly, suggesting it hasn't learned relation search logic
  - Over-reliance on Web Search: Model uses web search even when KG data is complete, indicating mis-calibrated reward penalty
  - Planning Collapse: `<plan>` is empty or illogical, leading to incoherent tool sequences

- **First 3 experiments:**
  1. SFT Data Ablation: Train with SFT dataset but skip RL stage, compare to model trained only on question-answer pairs
  2. Reward Component Ablation: Run RL training with different reward combinations to quantify contribution of retrieval-specific rewards
  3. KG Incompleteness Robustness: Evaluate fully trained model across CKG, IKG-20%, IKG-40%, IKG-60% datasets, analyze correlation between KG incompleteness and web search frequency

## Open Questions the Paper Calls Out

- **Enhancing relation filtering performance from KGs and decomposing complex reasoning problems**
  - Basis: Conclusion states future work will explore enhancing relation filtering performance and decomposing complex reasoning tasks; error analysis identifies relation selection errors as major failure type
  - Why unresolved: Current greedy strategy for relation selection only considers local optimality, and errors persist even with correct reasoning processes
  - Evidence needed: Systematic study comparing alternative relation selection mechanisms on same benchmarks, showing reduced error rates while maintaining or improving accuracy

- **Generalization to diverse knowledge graphs beyond Freebase**
  - Basis: Appendix A.5 shows Graph-RFT performs differently on Freebase vs. Wikidata (CWQ: 67.2 vs 61.5; WebQSP: 86.3 vs 77.8)
  - Why unresolved: Paper focuses on Freebase for all primary experiments, significant performance gap with Wikidata indicates potential architectural or reward design limitations
  - Evidence needed: Experiments on additional KGs (DBpedia, YAGO, domain-specific graphs) with analysis of required adaptations, plus reward parameter sensitivity ablation across different KG structures

- **Mechanisms for answer normalization across heterogeneous sources**
  - Basis: Section 4.4 states reasoning errors represent largest proportion, more prevalent in IKG due to answer aliasing and discrepancies between retrieved document answers and reference answers
  - Why unresolved: Even with correct planning and retrieval, final answer alignment problem remains unaddressed
  - Evidence needed: Module for answer normalization or entity linking integrated into Graph-RFT, evaluated specifically on reasoning error cases with metrics measuring reduction in aliasing-related failures

## Limitations

- Performance degrades on Wikidata compared to Freebase, suggesting KG-specific dependencies in the framework
- Cartesian-inspired planning may struggle with highly ambiguous questions requiring dynamic replanning
- The specific mechanism for identifying "critical triples" to create incomplete KGs is not fully specified

## Confidence

- **High**: Two-stage training mechanism, explicit planning and logical decomposition, multi-reward RL formulation
- **Medium**: Completeness of error analysis, sensitivity of results to reward hyperparameters
- **Low**: Claims about generalization beyond Freebase and tested benchmarks, scalability to larger KGs or different domains

## Next Checks

1. Conduct ablation studies comparing Graph-RFT performance with varying SFT dataset sizes to quantify the warm-start effect's contribution
2. Test the model on questions requiring dynamic replanning to evaluate Cartesian planning robustness limits
3. Perform sensitivity analysis on reward penalty hyperparameters to determine optimal balance for coverage-aware retrieval