---
ver: rpa2
title: Modeling User Preferences as Distributions for Optimal Transport-based Cross-domain
  Recommendation under Non-overlapping Settings
arxiv_id: '2508.16210'
source_url: https://arxiv.org/abs/2508.16210
tags:
- user
- domain
- cross-domain
- recommendation
- domains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of cross-domain recommendation
  in non-overlapping settings where no users or items are shared between domains during
  training. The core innovation is modeling user preferences as Gaussian Mixture Models
  (GMMs) over item embeddings, capturing multi-aspect interests more expressively
  than discrete vectors.
---

# Modeling User Preferences as Distributions for Optimal Transport-based Cross-domain Recommendation under Non-overlapping Settings

## Quick Facts
- **arXiv ID:** 2508.16210
- **Source URL:** https://arxiv.org/abs/2508.16210
- **Reference count:** 29
- **Key outcome:** DUP-OT achieves RMSE of 1.2919 and MAE of 0.8965 on Digital Music→Electronics, outperforming TDAR (1.5688 RMSE, 1.0904 MAE) through GMM-based preference modeling and OT transfer.

## Executive Summary
This paper addresses cross-domain recommendation in non-overlapping settings where no users or items are shared between domains during training. The core innovation is modeling user preferences as Gaussian Mixture Models (GMMs) over item embeddings, capturing multi-aspect interests more expressively than discrete vectors. The DUP-OT framework aligns these distributional preferences across domains using optimal transport, transferring GMM weights from source to target domain. Experiments on Amazon Review datasets show DUP-OT consistently outperforms state-of-the-art baselines, with the distributional representation alone (DUP-OT w/o source) also outperforming single-domain models.

## Method Summary
DUP-OT models each user's preferences as a GMM over item embeddings, where domain-level Gaussian components are fit on item embeddings via EM and personalized mixture weights are learned via an MLP. Rating prediction uses weighted Mahalanobis distances between items and the user's GMM. For cross-domain transfer, optimal transport aligns GMM components across domains using Wasserstein-2 distance, transporting user weights via the optimal transport matrix. A shared sentence encoder and joint autoencoder create unified latent spaces for semantic alignment. For users appearing in both domains, transferred and target distributions are linearly fused with weight α.

## Key Results
- DUP-OT (w/ source) achieves RMSE of 1.2919 and MAE of 0.8965 on Digital Music→Electronics
- DUP-OT outperforms TDAR baseline (1.5688 RMSE, 1.0904 MAE) by significant margins
- DUP-OT (w/o source) already outperforms single-domain models, demonstrating GMM representation benefits
- Consistent performance improvements across all tested Amazon Review dataset pairs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GMM-based user preference representations capture multi-aspect interests more expressively than discrete vectors, improving rating prediction even within a single domain.
- **Mechanism:** Domain-level Gaussian components are fit on item embeddings via EM (BayesianGaussianMixture). Each user receives personalized mixture weights over these shared components via a weight-learner MLP. Rating prediction uses weighted Mahalanobis distances between items and the user's GMM.
- **Core assumption:** User preferences are multi-modal and can be meaningfully decomposed into mixtures over latent item clusters.
- **Evidence anchors:**
  - [abstract] "represent each user's interests as a Gaussian Mixture Model (GMM) over item embeddings; capturing multi-aspect interests more expressively than discrete vectors"
  - [section 4.3] "DUP-OT models each user's preference as a Gaussian Mixture Model (GMM) over item embeddings"
  - [corpus] Weak direct support; neighbor papers focus on ID-based or single-Gaussian representations.
- **Break condition:** If user-item interactions are extremely sparse per user or preferences are genuinely unimodal, GMM expressiveness may not help over vectors.

### Mechanism 2
- **Claim:** Optimal Transport (OT) on GMM component distributions enables preference transfer across domains without any user/item overlap during training.
- **Mechanism:** Compute Wasserstein-2 distance between source and target GMM component means/covariances, solve for optimal transport matrix T via Sinkhorn, then transport user weights as w_t = w_s · T. For users appearing in both domains, linearly fuse transferred and target distributions (α-weighted).
- **Core assumption:** GMM components have semantic correspondence across domains so that transport aligns similar preference aspects.
- **Evidence anchors:**
  - [abstract] "aligns these distributional preferences across domains using optimal transport, transferring GMM weights from source to target domain"
  - [section 4.4] "apply OT only between the two domains' GMM component sets, avoiding instance-level transport"
  - [corpus] Li et al. (2022) and Liu et al. (2024) apply OT to embeddings, not GMM components.
- **Break condition:** If domain discrepancy is too large (e.g., unrelated product categories), OT may map components incorrectly, degrading transfer.

### Mechanism 3
- **Claim:** A shared sentence encoder and joint autoencoder create a unified latent space that enables cross-domain semantic alignment even without overlapping entities.
- **Mechanism:** Freeze a pre-trained sentence encoder (all-MiniLM-L6-v2) to encode reviews into user/item embeddings. Train a shared autoencoder on both domains to reduce dimensionality and enforce domain-consistent representations. Time-aware weighting emphasizes recent reviews.
- **Core assumption:** Review text captures transferable semantic structure across domains, and the encoder generalizes sufficiently.
- **Evidence anchors:**
  - [section 4.2] "train a shared autoencoder on both domains to obtain compact, domain-consistent embeddings"
  - [section 5.1.2] "use a shared pre-trained sentence encoder...followed by a shared autoencoder to reduce dimensionality"
  - [corpus] Neighbor papers (e.g., KGBridge, Causal-Invariant CDR) also leverage semantic features but differ in architecture.
- **Break condition:** If reviews are missing, low-quality, or domain-specific jargon dominates, the shared encoder may fail to align semantics.

## Foundational Learning

- **Concept: Gaussian Mixture Models (GMMs)**
  - Why needed here: Core representation; user preferences are modeled as weighted mixtures of Gaussian components over item space.
  - Quick check question: Can you explain why a GMM can represent a user who likes both classical music and electronic gadgets?

- **Concept: Optimal Transport / Wasserstein Distance**
  - Why needed here: Aligns preference distributions across domains; requires understanding cost matrices and Sinkhorn algorithm.
  - Quick check question: What does the transport matrix T represent in terms of mapping probability mass between source and target components?

- **Concept: Cross-Domain Transfer under Non-Overlap**
  - Why needed here: Problem setting; no shared users/items at training time requires indirect alignment via semantic features.
  - Quick check question: Why does eliminating user/item overlap make domain alignment harder compared to standard CDR?

## Architecture Onboarding

- **Component map:**
  - Stage 1 (Shared Preprocessing): Sentence encoder (frozen) → Autoencoder (trained jointly) → User/item embeddings
  - Stage 2 (Per-Domain): GMM fit on item embeddings → Weight-learner MLP (user → weights) → Rating-predictor MLP (weights + item → rating)
  - Stage 3 (Cross-Domain): OT alignment of GMM components → Weight transport → Fusion (α) → Final prediction

- **Critical path:**
  1. Ensure embeddings from Stage 1 are domain-consistent (check autoencoder reconstruction loss parity across domains).
  2. Verify GMM component quality (elbow plot on component number, component clustering visualization).
  3. Inspect transport matrix T for sparsity/concentration (should not be near-uniform).

- **Design tradeoffs:**
  - Number of GMM components: Too few loses expressiveness; too many increases OT cost and may overfit.
  - Fusion weight α: Higher α trusts source transfer more; lower α relies on target-only distribution.
  - Autoencoder capacity vs. overfitting to one domain.

- **Failure signatures:**
  - DUP-OT (w/o source) underperforms single-domain baselines → GMM representation not learning meaningful structure.
  - DUP-OT (w/ source) underperforms w/o source → OT transport mapping is detrimental (check T alignment).
  - Large RMSE gap between domains → autoencoder not creating unified space.

- **First 3 experiments:**
  1. Ablation: Run DUP-OT (w/o source) vs. LightGCN/NeuMF on target-domain only to validate distributional representation benefit.
  2. Component sensitivity: Vary GMM component count (K) and observe RMSE/MAE trends.
  3. Transport inspection: Visualize T matrix for Digital Music→Electronics; check if high-weight mappings align semantically (e.g., audio equipment components map appropriately).

## Open Questions the Paper Calls Out
None

## Limitations
- The distributional representation mechanism has limited empirical validation against other distributional representations
- The OT-based transfer mechanism lacks thorough analysis of failure modes and transport quality metrics
- Claims about semantic alignment through shared encoders assume ideal conditions (quality reviews, consistent semantics) not fully validated

## Confidence
- **High Confidence**: The experimental results showing DUP-OT outperforming baselines on Amazon datasets are well-documented and reproducible.
- **Medium Confidence**: The core hypothesis that GMM-based distributions improve rating prediction within single domains has theoretical support but limited empirical validation.
- **Medium Confidence**: The OT-based transfer mechanism works under controlled conditions but lacks thorough analysis of failure modes.
- **Low Confidence**: Claims about semantic alignment through shared encoders assume ideal conditions not fully validated in the paper.

## Next Checks
1. **Transport Matrix Analysis**: Visualize and analyze the optimal transport matrix T for Digital Music→Electronics. Check for semantic coherence in high-weight mappings and identify cases where transport degrades performance.

2. **Ablation Under Sparse Data**: Test DUP-OT (w/o source) against single-domain baselines when user interaction counts are artificially reduced to 5, 10, and 20 interactions per user to validate whether GMM expressiveness provides benefits under realistic sparsity conditions.

3. **Cross-Domain Transfer Quality**: For users appearing in both domains, measure how well transferred GMM weights from the source domain predict ratings in the target domain compared to training only on target data, isolating the transfer contribution from the distributional representation benefit.