---
ver: rpa2
title: 'EvoMem: Improving Multi-Agent Planning with Dual-Evolving Memory'
arxiv_id: '2511.01912'
source_url: https://arxiv.org/abs/2511.01912
tags:
- memory
- planning
- arxiv
- constraints
- constraint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'EvoMem introduces a dual-evolving memory mechanism to enhance
  multi-agent planning by integrating task-specific constraints with iterative feedback
  refinement. The framework uses three agents (Constraint Extractor, Verifier, Actor)
  and two memory modules: CMem stores task-specific constraints across queries, while
  QMem accumulates feedback within a query to refine solutions iteratively.'
---

# EvoMem: Improving Multi-Agent Planning with Dual-Evolving Memory
## Quick Facts
- arXiv ID: 2511.01912
- Source URL: https://arxiv.org/abs/2511.01912
- Reference count: 40
- Key outcome: EvoMem achieves +11.17% improvement in trip planning, +2.56% in calendar scheduling, and +3.76% in meeting planning over strong baselines on the NaturalPlan benchmark

## Executive Summary
EvoMem introduces a dual-evolving memory mechanism to enhance multi-agent planning by integrating task-specific constraints with iterative feedback refinement. The framework uses three agents (Constraint Extractor, Verifier, Actor) and two memory modules: CMem stores task-specific constraints across queries, while QMem accumulates feedback within a query to refine solutions iteratively. Evaluated on trip planning, meeting planning, and calendar scheduling tasks using the NaturalPlan benchmark, EvoMem achieves significant performance gains over strong baselines. The results demonstrate that combining long-lived constraints with dynamically updated feedback memory substantially improves planning accuracy across diverse tasks and LLM backbones.

## Method Summary
EvoMem employs a dual-evolving memory system with CMem (Constraint Memory) and QMem (Query Memory) to improve multi-agent planning. The system uses three specialized agents: a Constraint Extractor to parse task constraints, a Verifier to validate plan feasibility, and an Actor to generate plans. CMem stores task-specific constraints across queries for long-term knowledge retention, while QMem accumulates feedback within a query to refine solutions iteratively. This architecture enables both persistent constraint learning and adaptive feedback incorporation, allowing the system to improve performance through repeated interactions and constraint enforcement.

## Key Results
- EvoMem achieves +11.17% performance improvement in trip planning tasks
- EvoMem demonstrates +3.76% improvement in meeting planning tasks
- EvoMem shows +2.56% improvement in calendar scheduling tasks

## Why This Works (Mechanism)
The dual-evolving memory mechanism works by maintaining both persistent and adaptive memory stores. CMem provides long-term retention of task-specific constraints, preventing repeated violations across different queries. QMem enables iterative refinement within individual queries by accumulating feedback and adjusting plans accordingly. This combination allows the system to learn from both historical patterns and real-time interactions, creating a more robust planning capability that adapts to both persistent constraints and dynamic requirements.

## Foundational Learning
- Multi-agent planning: Coordination of multiple specialized agents to solve complex planning problems through division of labor and iterative refinement
- Constraint extraction: Automated identification and parsing of task-specific requirements from natural language inputs to ensure plans adhere to explicit and implicit constraints
- Feedback accumulation: Systematic collection and integration of evaluation results to improve subsequent planning iterations within a single query
- Iterative refinement: Repeated cycles of plan generation, validation, and adjustment to progressively improve solution quality
- Memory persistence: Long-term storage of learned constraints across different queries to avoid repeating past mistakes
- Adaptive planning: Dynamic adjustment of planning strategies based on accumulated feedback and evolving requirements

## Architecture Onboarding
- Component map: User Query -> Constraint Extractor -> Verifier -> Actor -> Plan Output, with CMem and QMem providing memory support
- Critical path: User Query → Constraint Extractor (with CMem) → Verifier (with QMem) → Actor → Plan Output
- Design tradeoffs: Balance between memory persistence (CMem) and real-time adaptability (QMem) to optimize both long-term learning and immediate performance
- Failure signatures: Constraint violations indicate CMem inadequacy; plan rejection indicates QMem feedback insufficiency
- First experiments: 1) Test constraint extraction accuracy with known constraint sets, 2) Evaluate iterative refinement effectiveness on simple planning tasks, 3) Measure memory persistence across multiple similar queries

## Open Questions the Paper Calls Out
None

## Limitations
- Performance improvements vary across task types, with calendar scheduling showing smaller gains than trip planning
- Memory module effectiveness may depend on task complexity and constraint density
- System performance tied to underlying LLM capabilities and may not generalize beyond tested models

## Confidence
- Performance claims: High - based on systematic evaluation against established benchmarks
- Memory mechanism effectiveness: Medium - improvements demonstrated but underlying mechanisms could benefit from deeper analysis
- Generalizability: Medium - results show promise but may be influenced by specific task characteristics and LLM limitations

## Next Checks
1. Verify constraint extraction accuracy across diverse planning domains beyond the tested NaturalPlan benchmark
2. Test memory persistence effectiveness by measuring performance degradation when memory modules are disabled
3. Evaluate system robustness to varying constraint complexity and interdependency patterns