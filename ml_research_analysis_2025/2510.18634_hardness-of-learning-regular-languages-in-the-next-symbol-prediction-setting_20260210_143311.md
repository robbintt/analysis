---
ver: rpa2
title: Hardness of Learning Regular Languages in the Next Symbol Prediction Setting
arxiv_id: '2510.18634'
source_url: https://arxiv.org/abs/2510.18634
tags:
- setting
- learning
- labels
- dead
- prefix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the learnability of languages in the Next Symbol
  Prediction (NSP) setting, where a learner receives positive examples together with
  membership and continuation labels for each prefix. The authors formalize the NSP
  setting to make it amenable to PAC-learning analysis and show that despite the richer
  supervision, learning concept classes like DFAs and Boolean formulas remains computationally
  hard.
---

# Hardness of Learning Regular Languages in the Next Symbol Prediction Setting

## Quick Facts
- arXiv ID: 2510.18634
- Source URL: https://arxiv.org/abs/2510.18634
- Reference count: 2
- One-line primary result: NSP learning of DFAs is computationally hard under cryptographic assumptions, despite richer supervision

## Executive Summary
This paper studies the learnability of regular languages in the Next Symbol Prediction (NSP) setting, where learners receive positive examples with membership and continuation labels for each prefix. The authors formalize NSP learning for PAC analysis and prove that despite the richer supervision, learning DFAs and Boolean formulas remains computationally hard. The key technical result is a reduction from conventional learning to NSP learning, showing that efficient NSP learning would break cryptographic assumptions. This demonstrates that the NSP setting does not circumvent the computational hardness of learning regular languages, even when learners receive both positive and negative examples with NSP labels.

## Method Summary
The paper proves hardness of learning DFAs in the NSP setting by constructing a reduction from conventional PAC learning. Given a DFA A accepting strings of length N, the authors create a padded DFA A⊕ where NSP labels for prefixes of length < N are uninformative (always (1,1,0)), but the continuation bit at depth N reveals the original acceptance decision. The reduction transforms standard learning samples (u,y) into NSP samples (x,f_A⊕(x)) such that an efficient NSP learner would yield an efficient conventional learner, establishing computational equivalence. The hardness then follows from known results about the difficulty of learning DFAs under cryptographic assumptions.

## Key Results
- NSP learning of DFAs is as hard as conventional PAC learning under cryptographic assumptions
- The reduction works even for improper learning, including neural models trained on NSP labels
- Adding negative examples with NSP labels does not overcome computational hardness
- For restricted classes like Conjunctions, NSP can provide significant learning advantages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NSP labels can be made uninformative through a padding construction that delays all useful signal until a single decision point.
- Mechanism: Given a DFA A accepting strings of length N, construct a padded DFA A⊕ accepting strings of length N+1. For the first N positions, redirect any dead transitions to a chain of states q′₁ → q′₂ → ... → q′_N → q_fin, ensuring continuation labels are always (1,1,0) regardless of the underlying computation. Only at depth N does the continuation bit for symbol 0 reveal A(u) for the prefix u.
- Core assumption: The learner cannot extract information from the structural properties of the hypothesis space itself—only from the labels.
- Evidence anchors:
  - [abstract] "The proof is via a construction that makes almost all additional labels uninformative"
  - [section 3.1] Lemma 3.3 formally specifies that for every prefix y with |y| < N, the NSP labels equal (1, 1, 0)
  - [corpus] Related work on regular expression hardness (arxiv:2510.04834) confirms computational barriers persist across representation formalisms
- Break condition: If the hypothesis class is restricted such that padded constructions exceed allowed complexity bounds, the reduction fails.

### Mechanism 2
- Claim: Efficient NSP learning would imply efficient conventional PAC learning, establishing equivalence of computational complexity.
- Mechanism: Given samples (u, y) from distribution D, construct padded examples x = u·1 with NSP labels computable from y alone. An NSP learner outputs ĥ; define standard classifier h(u) as the bit predicted by ĥ for φ(x:N, 0). NSP error ≤ ε implies standard error ≤ ε by the padding property.
- Core assumption: The transformation preserves polynomial-time computability (adds at most N+1 states).
- Evidence anchors:
  - [abstract] "yielding a reduction from the conventional learning problem to learning with NSP labels"
  - [section 3.1] Theorem 3.4 proves the reduction formally
  - [corpus] Weak corpus signals on this specific reduction technique; related work focuses on different hardness proofs
- Break condition: If NSP learners exploit structure across multiple examples (batch effects) rather than treating each independently, the reduction may not capture all learning strategies.

### Mechanism 3
- Claim: Cryptographic hardness propagates from conventional learning to NSP learning via the established reduction.
- Mechanism: Kearns & Valiant (1994) showed weak PAC learning of DFAs is as hard as inverting RSA/factoring Blum integers. Since NSP learning of ADFAN_p(N) implies conventional learning (Theorem 3.4), any polynomial-time NSP learner would break these cryptographic assumptions.
- Core assumption: Standard cryptographic assumptions (RSA, factoring) hold.
- Evidence anchors:
  - [abstract] "Under cryptographic assumptions, the reduction implies that the problem of learning DFAs is computationally hard"
  - [section 3.1] Corollary 3.4.1 explicitly states the cryptographic implication
  - [corpus] No direct corpus corroboration for this specific propagation
- Break condition: If quantum computing advances invalidate factoring assumptions, or if alternative hardness foundations are used.

## Foundational Learning

- Concept: **PAC Learning Framework**
  - Why needed here: The entire hardness result is framed in PAC terms—probability of error ≤ ε with confidence 1-δ, in polynomial time.
  - Quick check question: Can you explain why "weak learning" (advantage over random guessing) is sufficient for hardness reductions?

- Concept: **Deterministic Finite Automata (DFAs) and State Depth**
  - Why needed here: Lemma 3.2's unique depth property is essential for the padding construction to work correctly.
  - Quick check question: Why does minimality guarantee each non-dead state is reachable by strings of exactly one length?

- Concept: **Reductions Between Learning Problems**
  - Why needed here: The core technical contribution is showing NSP-learning ⊇ standard-learning in complexity.
  - Quick check question: If problem A polynomial-time reduces to problem B, and A is hard, what does this imply about B?

## Architecture Onboarding

- Component map: Target DFA A (length N) → Padding Transform → Padded DFA A⊕ (length N+1) → Standard samples (u, y) → Label Transformation → NSP samples (x, f_A⊕(x)) → NSP Learner → Hypothesis ĥ → Extraction → Standard Classifier h

- Critical path: The padding construction (Lemma 3.3) is the bottleneck—understanding how dead-state redirection creates uniform labels is essential to grasp why additional NSP supervision provides no computational advantage.

- Design tradeoffs:
  - Acyclic vs. cyclic DFAs: Results proven for fixed-length acyclic case; general DFA hardness follows but requires additional steps
  - Proper vs. improper learning: Hardness applies even when hypothesis need not be a DFA (includes neural networks)
  - Positive-only vs. full supervision: Paper shows adding negative examples with NSP labels still doesn't help

- Failure signatures:
  - Assuming NSP is "easier" because labels seem richer—this is the key misconception the paper refutes
  - Conflating statistical efficiency (sample complexity) with computational efficiency (time complexity)—NSP may help the former but not the latter
  - Missing the depth-uniqueness requirement when attempting similar constructions

- First 3 experiments:
  1. Implement the padding transformation on small DFAs (N=3-5) and verify Lemma 3.3's label properties hold for all prefixes
  2. Train neural sequence models (LSTMs/Transformers) on padded vs. unpadded DFA languages; compare sample efficiency but recognize computational hardness remains
  3. Test whether the Conjunctions class (where NSP helps dramatically per Section 3) vs. general DFAs shows different empirical learning curves, illustrating the gap between lucky structure and worst-case hardness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Which restricted subclasses of regular languages permit efficient PAC learning in the NSP setting, distinguishing them from the general class of DFAs?
- Basis in paper: [inferred] The paper contrasts simple classes like Conjunctions (learnable from one example) with general DFAs (cryptographically hard), but does not map the boundary where NSP supervision transitions from being computationally beneficial to hard.
- Why unresolved: The reduction uses a general construction that obscures specific structural properties of languages that might facilitate efficient learning under NSP labels.
- What evidence would resolve it: Identification of specific language families (e.g., star-free, counter-free) where efficient NSP learning algorithms exist.

### Open Question 2
- Question: Does the computational hardness of learning DFAs in the NSP setting persist under realistic or average-case distributions, rather than the adversarial distribution constructed in the reduction?
- Basis in paper: [inferred] The reduction relies on a "padded" construction where NSP labels are deliberately uninformative $(1,1,0)$ for the first $N$ symbols, creating a worst-case scenario.
- Why unresolved: The PAC-analysis is distribution-free; it is unclear if "natural" distributions, where NSP labels are informative earlier in the string, avoid this computational barrier.
- What evidence would resolve it: Theoretical results showing hardness for average-case distributions or empirical demonstrations of efficient learning on non-adversarial data.

### Open Question 3
- Question: Does the NSP setting provide a significant statistical advantage (sample complexity reduction) for learning regular languages, even if the problem remains computationally hard?
- Basis in paper: [explicit] The authors observe that for some classes, NSP offers a "richer set of labels" (e.g., Conjunctions), but the paper focuses strictly on computational barriers rather than sample complexity bounds.
- Why unresolved: The reduction strategy renders the extra labels uninformative to prove hardness, leaving the potential statistical benefits of the richer supervision unquantified for hard classes.
- What evidence would resolve it: A formal analysis comparing the sample complexity of learning DFAs with NSP labels versus standard binary labels.

## Limitations

- The hardness result applies specifically to DFAs and Boolean formulas; extending to other concept classes requires additional analysis
- The reduction relies on acyclic DFAs with fixed length; the complexity for general cyclic DFAs is not explicitly addressed
- The cryptographic hardness assumptions depend on specific results from Kearns & Valiant (1994) that are referenced but not independently verified

## Confidence

- High Confidence: The technical construction in Lemma 3.3 is clearly specified and verifiable. The reduction mechanism in Theorem 3.4 follows standard proof techniques.
- Medium Confidence: The claim that NSP learning is as hard as conventional learning for DFAs is well-supported, but the extension to other concept classes (like Boolean formulas) relies on external references.
- Medium Confidence: The cryptographic implications are stated but not independently verified within the paper.

## Next Checks

1. Implement the padding construction on multiple DFAs with varying N and verify that NSP labels are indeed uninformative for all prefixes except depth N.

2. Test whether restricting the hypothesis class (e.g., to DFAs with fewer states than the padded construction) could potentially circumvent the hardness result.

3. Explore whether the reduction technique can be adapted to show hardness for other concept classes beyond those explicitly mentioned in the paper.