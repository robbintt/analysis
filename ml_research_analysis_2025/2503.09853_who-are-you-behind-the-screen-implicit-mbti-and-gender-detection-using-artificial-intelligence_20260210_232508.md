---
ver: rpa2
title: Who Are You Behind the Screen? Implicit MBTI and Gender Detection Using Artificial
  Intelligence
arxiv_id: '2503.09853'
source_url: https://arxiv.org/abs/2503.09853
tags:
- personality
- mbti
- classification
- gender
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates Transformer-based models (BERT, GPT-2, RoBERTa)
  for implicit MBTI personality and gender classification from Telegram chat data.
  By leveraging confidence thresholds, RoBERTa achieved 86.16% accuracy for personality
  classification and 74.4% for gender classification, demonstrating superior ability
  to capture linguistic patterns indicative of personality traits and gender differences.
---

# Who Are You Behind the Screen? Implicit MBTI and Gender Detection Using Artificial Intelligence

## Quick Facts
- arXiv ID: 2503.09853
- Source URL: https://arxiv.org/abs/2503.09853
- Reference count: 18
- Primary result: Transformer-based models achieve 86.16% accuracy for personality classification and 74.4% for gender classification from Telegram chat data

## Executive Summary
This study evaluates Transformer-based models (BERT, GPT-2, RoBERTa) for implicit MBTI personality and gender classification from Telegram chat data. By leveraging confidence thresholds, RoBERTa achieved 86.16% accuracy for personality classification and 74.4% for gender classification, demonstrating superior ability to capture linguistic patterns indicative of personality traits and gender differences. Results show introverted and intuitive personality types are more active in text-based interactions. The findings highlight the potential of Transformers for implicit psychological profiling while emphasizing trade-offs between accuracy and data coverage in real-world conversational environments.

## Method Summary
The study collected 309 participants' Telegram chat data and administered MBTI personality assessments to establish ground truth labels. Three Transformer architectures (BERT, GPT-2, RoBERTa) were fine-tuned on the chat text data for personality and gender classification tasks. The researchers implemented confidence threshold filtering to improve precision by only accepting predictions above certain confidence levels. Models were evaluated using accuracy metrics, with RoBERTa demonstrating superior performance across both classification tasks.

## Key Results
- RoBERTa achieved 86.16% accuracy for MBTI personality classification
- RoBERTa achieved 74.4% accuracy for gender classification
- Introverted and intuitive personality types showed higher text-based interaction activity

## Why This Works (Mechanism)
Transformer models excel at capturing long-range dependencies and contextual relationships in text, making them well-suited for identifying subtle linguistic patterns that correlate with personality traits and gender expression. The self-attention mechanism allows these models to weigh the importance of different words and phrases within conversational context, enabling them to detect stylistic markers, vocabulary choices, and communication patterns that implicitly signal personality dimensions and gender-related linguistic features.

## Foundational Learning
- **MBTI personality framework**: Understanding the four dimensions (Extraversion/Introversion, Sensing/Intuition, Thinking/Feeling, Judging/Perceiving) is essential for interpreting classification results and their psychological implications.
- **Transformer architecture fundamentals**: Knowledge of self-attention, positional encoding, and fine-tuning processes is needed to understand model behavior and limitations.
- **Confidence thresholding**: This technique improves precision by filtering predictions based on model certainty, trading coverage for accuracy in practical applications.
- **Persian language characteristics**: Awareness of linguistic features specific to Persian helps contextualize results and understand potential cross-linguistic differences.

## Architecture Onboarding
**Component Map**: Raw Chat Data -> Preprocessing -> Transformer Model -> Confidence Thresholding -> Classification Output

**Critical Path**: The core pipeline flows from chat data collection through preprocessing, model inference, confidence filtering, and final classification output. The confidence threshold step is critical as it directly impacts the trade-off between precision and recall.

**Design Tradeoffs**: The study prioritized accuracy over coverage by implementing confidence thresholds, accepting that some data points would be excluded from final predictions. This approach improved precision but limited the practical applicability in scenarios requiring complete coverage.

**Failure Signatures**: Lower confidence scores indicate ambiguous or mixed linguistic patterns that don't clearly align with typical personality or gender markers. These failures often occur in texts with formal language, code-switching, or non-standard communication patterns.

**First Experiments**:
1. Evaluate model performance on a held-out validation set to establish baseline metrics
2. Test different confidence threshold levels to optimize the precision-recall trade-off
3. Analyze misclassified samples to identify common failure patterns and linguistic features

## Open Questions the Paper Calls Out
None

## Limitations
- Telegram-specific data may introduce platform-specific communication biases
- Persian-language focus limits cross-linguistic generalizability of findings
- 309 participant sample size may not fully represent population distributions

## Confidence
- Personality classification results: High confidence due to robust Transformer architecture and clear performance metrics
- Gender classification results: Medium confidence due to lower accuracy and potential cultural biases
- Introverted/intuitive personality activity claim: Low confidence requiring further validation

## Next Checks
1. Cross-platform validation: Replicate the study using datasets from multiple messaging platforms (e.g., WhatsApp, Discord) to assess model robustness across different conversational contexts
2. Cross-linguistic testing: Evaluate model performance on datasets in multiple languages to determine if the linguistic patterns identified are universal or language-specific
3. External validation: Compare the model's implicit predictions against gold-standard personality assessments administered to the same participants to validate the accuracy of inferred personality traits