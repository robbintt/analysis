---
ver: rpa2
title: 'N-GLARE: An Non-Generative Latent Representation-Efficient LLM Safety Evaluator'
arxiv_id: '2511.14195'
source_url: https://arxiv.org/abs/2511.14195
tags:
- safety
- benign
- jailbreak
- latent
- refusal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: N-GLARE is a non-generative framework that evaluates LLM safety
  by analyzing latent representation trajectories rather than generating text. It
  constructs a benign manifold from benign inputs and uses Jensen-Shannon Separability
  (JSS) to measure deviations of jailbreak and refusal trajectories.
---

# N-GLARE: An Non-Generative Latent Representation-Efficient LLM Safety Evaluator

## Quick Facts
- arXiv ID: 2511.14195
- Source URL: https://arxiv.org/abs/2511.14195
- Reference count: 40
- Across 40+ models and 20 red-teaming strategies, JSS rankings correlate highly with traditional red-teaming while reducing cost to <1% of token/runtime usage.

## Executive Summary
N-GLARE evaluates LLM safety without generating text by analyzing latent representation trajectories across four probing conditions: benign, jailbreak, ideal refusal, and plain query. It constructs a benign manifold from benign activations and measures Jensen-Shannon Separability (JSS) between trajectory angle distributions to assess safety alignment. The framework achieves high correlation with traditional red-teaming rankings while using less than 1% of the token and runtime costs, and captures fine-grained safety dynamics that precede surface-level metric changes during training.

## Method Summary
N-GLARE extracts hidden states from four probing conditions (benign, jailbreak, ideal refusal, plain query) and constructs a benign manifold using whitened PCA on benign activations. It standardizes trajectories via arc-length parameterization, computes geometric turning angles between trajectory tangents and manifold normals, and measures separability using Jensen-Shannon divergence across discretized progress slices. The method aggregates JSS scores across layer groups to produce model-level safety rankings that correlate strongly with traditional red-teaming results while requiring minimal computation.

## Key Results
- JSS rankings correlate highly with traditional red-teaming rankings across 40+ models and 20 red-teaming strategies
- Evaluation cost reduced to less than 1% of token and runtime usage compared to standard red-teaming
- Latent trajectory changes detected earlier than surface safety metric changes during fine-tuning
- Framework captures fine-grained safety dynamics and differentiates between safety-aligned and unaligned models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A low-dimensional benign manifold serves as a geometric reference for normal behavior, enabling trajectory deviation quantification.
- Mechanism: Whitened PCA on benign activations constructs a subspace; residual vectors define "outward normal" directions. Geometric turning angles measure how far trajectory tangents deviate from this normal, with smaller angles indicating departure from benign behavior.
- Core assumption: Benign inputs occupy a consistent low-dimensional manifold; deviations correlate with safety-relevant behavior changes.
- Evidence anchors:
  - [abstract] "constructs a benign manifold from benign inputs"
  - [section 3.3] "construct a benign manifold using representations collected under benign conditions... defines a geometric turning angle"
  - [corpus] No direct corpus validation; neighboring papers focus on attack generation, not latent geometry.
- Break condition: If benign inputs span high-dimensional space without cluster structure, manifold approximation fails.

### Mechanism 2
- Claim: Trajectory separability between conditions (J vs R, J vs B) reflects internal safety awareness and refusal tendency.
- Mechanism: Discretize standardized progress into slices; compute Jensen-Shannon divergence between turning angle distributions per slice; aggregate across layers/slices to form JSS score.
- Core assumption: Safety alignment manifests as distinct latent trajectories for different input types; misaligned models show overlapping trajectories.
- Evidence anchors:
  - [abstract] "JSS rankings are highly consistent with traditional red-teaming rankings"
  - [section 3.5] "JSS(J,B)... measures whether the model internally distinguishes adversarial inputs from normal dialogue"
  - [corpus] No corpus papers validate trajectory separability as safety proxy.
- Break condition: If jailbreak and refusal trajectories converge despite different surface behavior, separability assumption invalid.

### Mechanism 3
- Claim: Latent trajectory changes precede surface-level safety metric changes during fine-tuning.
- Mechanism: Track JSS and behavioral metrics (Unsafe Rate, Refusal Rate) across DPO training steps; JSS turning points appear earlier in training.
- Core assumption: Internal representation dynamics are more sensitive to weight updates than output probabilities.
- Evidence anchors:
  - [abstract] "latent changes precede surface safety metric changes during training"
  - [section 4.1.2, Figure 6] "turning points of the dashed JSS curves appear earlier than the corresponding shifts in output behavior-based metrics"
  - [corpus] No corpus validation; this is a novel temporal claim.
- Break condition: If early JSS changes don't predict eventual behavioral shifts, early-warning utility fails.

## Foundational Learning

- Concept: **Jensen-Shannon Divergence**
  - Why needed here: Core metric for comparing turning angle distributions; symmetric and bounded unlike KL divergence.
  - Quick check question: Given two distributions P and Q, can you compute JSD(P||Q) and interpret it as a separability measure?

- Concept: **PCA and Subspace Projections**
  - Why needed here: Constructing the benign manifold requires projecting high-dimensional activations onto low-rank subspace; understanding residual vectors is essential.
  - Quick check question: For a matrix with rank-r approximation, what does the residual orthogonal to the principal subspace represent?

- Concept: **Arc-Length Parameterization**
  - Why needed here: Standardizing variable-length trajectories onto a unified progress axis [0,1] for slice-wise comparison.
  - Quick check question: Given a sequence of points in R^d, can you compute cumulative arc length and normalize to [0,1]?

## Architecture Onboarding

- Component map:
  Probing Module -> Hidden state extraction -> Manifold Constructor -> Trajectory Processor -> JSS Calculator -> Safety Scorer

- Critical path: Probing → Hidden state extraction → Manifold construction (benign only) → Trajectory standardization → Turning angle computation → Slice-wise JSD → JSS aggregation → Model-level safety score.

- Design tradeoffs:
  - **Rank r selection**: Higher r captures more manifold complexity but risks overfitting; paper uses r≪d (exact value in appendix).
  - **Layer grouping**: Averaging across groups reduces dimensionality but may lose fine-grained layer-specific signals.
  - **Slice count I**: More slices increase temporal resolution but require more samples per slice for stable distribution estimates.
  - **Offline vs online probing**: Pre-collected trajectories enable efficiency but limit adaptability to new attack types.

- Failure signatures:
  - **Flat JSS across models**: Indicates probing conditions don't elicit distinguishable trajectories; check template quality.
  - **High variance in bootstrap rankings**: Sample size too small; increase trajectory count per condition.
  - **Benign manifold collapse**: Residual energy near zero for all inputs; r too high or benign data insufficient.
  - **No correlation with red-teaming**: JSS proxy not capturing safety-relevant geometry; revisit probing condition design.

- First 3 experiments:
  1. **Sanity check on known models**: Run N-GLARE on a safety-aligned model vs its unaligned base version; verify JSS ordering matches expected safety difference (paper shows Qwen3-4B variants in Figure 1).
  2. **Trajectory visualization**: Plot APT trajectories for all four conditions on a single model; confirm visual separability aligns with JSS scores (reference Figure 3).
  3. **Cost benchmark**: Measure token count and wall-clock time for N-GLARE vs a standard red-teaming pipeline (e.g., PyRIT with GPTFuzzer) on the same model set; target <1% as claimed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can N-GLARE be extended to evaluate proprietary or API-only models where internal hidden representations are inaccessible?
- Basis in paper: [explicit] The authors state in the Limitations section that the "reliance on hidden states motivates future work on approximate or surrogate signals that could extend N-GLARE–style analysis to partially observable or API-only settings."
- Why unresolved: The current framework fundamentally depends on extracting intermediate layer activations (`h` vectors) during forward passes, which are unavailable in closed-source APIs.
- What evidence would resolve it: The development of a "surrogate" method that correlates external behavioral signals or output probabilities with the internal JSS metric, achieving similar ranking consistency on black-box models.

### Open Question 2
- Question: Can the trajectory-separability framework effectively measure alignment dimensions beyond safety, such as bias, factual reliability, or hallucination rates?
- Basis in paper: [explicit] The paper suggests the framework "can be generalized to capture other alignment dimensions, such as bias, factual reliability, or long-horizon agentic safety."
- Why unresolved: The current experimental validation is restricted to safety (jailbreaks vs. refusals) and does not demonstrate that "bias" or "truthfulness" exhibit similar separable trajectories relative to a benign manifold.
- What evidence would resolve it: Empirical results showing that JSS-derived rankings for specific bias or factuality datasets correlate with established benchmarks for those dimensions.

### Open Question 3
- Question: Does replacing the linear benign manifold with a non-linear or multi-manifold representation significantly improve the fidelity of safety evaluations?
- Basis in paper: [explicit] The authors note that the geometric abstraction "may oversimplify more complex latent structures in large models" and suggest "non-linear or multi-manifold representations" as a future direction.
- Why unresolved: The current method uses whitened PCA (linear) to define the benign manifold, which might not accurately capture the complex geometry of high-capacity models.
- What evidence would resolve it: Ablation studies comparing the ranking correlation of linear vs. non-linear manifold construction methods across the 40+ model set.

## Limitations

- Reliance on low-dimensional benign manifold assumption may fail for high-dimensional or non-manifold structured representations
- Critical hyperparameters (PCA rank, layer groupings, slice count) not fully specified in main text
- Refusal template specificity may limit generalizability to models with different refusal strategies

## Confidence

**High Confidence**: The geometric framework for trajectory analysis is mathematically sound and empirical correlation with red-teaming rankings is strong across diverse models.

**Medium Confidence**: Temporal precedence of JSS changes is demonstrated but limited to single fine-tuning experiment, requiring broader validation.

**Low Confidence**: Implementation details lack specification of critical hyperparameters, making faithful reproduction challenging.

## Next Checks

1. **Manifold Dimensionality Validation**: Analyze explained variance ratio of PCA components on benign data across multiple models; test sensitivity of JSS scores to different rank choices.

2. **Template Strategy Robustness**: Replace fixed Ideal Refusal template with alternative refusal strategies; measure JSS stability and ranking correlation to test template dependency.

3. **Cross-Training Generalization**: Apply N-GLARE to safety fine-tuning scenarios beyond DPO (RLHF, supervised fine-tuning); track whether JSS consistently predicts behavioral changes across different optimization methods and training durations.