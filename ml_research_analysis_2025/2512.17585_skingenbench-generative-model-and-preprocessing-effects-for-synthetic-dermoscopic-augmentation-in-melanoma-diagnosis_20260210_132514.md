---
ver: rpa2
title: 'SkinGenBench: Generative Model and Preprocessing Effects for Synthetic Dermoscopic
  Augmentation in Melanoma Diagnosis'
arxiv_id: '2512.17585'
source_url: https://arxiv.org/abs/2512.17585
tags:
- images
- melanoma
- image
- preprocessing
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces SkinGenBench, a systematic benchmark for\
  \ evaluating synthetic dermoscopic image generation and its impact on melanoma diagnosis.\
  \ The study compares two generative models\u2014StyleGAN2-ADA and Denoising Diffusion\
  \ Probabilistic Models (DDPMs)\u2014under different preprocessing pipelines, using\
  \ a curated dataset of 14,116 dermoscopic images."
---

# SkinGenBench: Generative Model and Preprocessing Effects for Synthetic Dermoscopic Augmentation in Melanoma Diagnosis

## Quick Facts
- arXiv ID: 2512.17585
- Source URL: https://arxiv.org/abs/2512.17585
- Reference count: 32
- Generative architecture choice has stronger influence on image fidelity and diagnostic performance than preprocessing complexity

## Executive Summary
SkinGenBench introduces a systematic benchmark for evaluating synthetic dermoscopic image generation and its impact on melanoma diagnosis. The study compares StyleGAN2-ADA and Denoising Diffusion Probabilistic Models (DDPMs) under different preprocessing pipelines using 14,116 dermoscopic images. Results show that generative architecture choice strongly influences image fidelity and diagnostic performance, with StyleGAN2-ADA consistently producing higher-quality images and improved melanoma detection by 8-15% in F1-score.

## Method Summary
The benchmark evaluates two generative models - StyleGAN2-ADA and DDPMs - under basic and advanced preprocessing pipelines using a curated dataset of 14,116 dermoscopic images from HAM10000 and ISIC 2025. StyleGAN2-ADA was fine-tuned from FFHQ with 240k images and adaptive augmentation, while DDPMs used CELEBA-HQ initialization for 120 epochs. Synthetic images were generated for the underrepresented melanoma class (11.03% of dataset) and combined with real samples for classifier training. Four classifier architectures (ResNet-18/50, VGG-16, ViT-B/16, EfficientNet-B0) were evaluated using standard metrics including FID, KID, F1-score, and ROC-AUC.

## Key Results
- StyleGAN2-ADA achieved FID ≈ 65.5 and KID ≈ 0.05, significantly outperforming DDPMs
- Synthetic augmentation improved melanoma detection by 8-15% in F1-score, with ViT-B/16 reaching F1 ≈ 0.88
- Advanced artifact removal provided limited diagnostic gains, suggesting possible suppression of clinically relevant texture cues

## Why This Works (Mechanism)

### Mechanism 1
StyleGAN2-ADA produces higher-fidelity dermoscopic images than DDPMs due to its constrained latent space and adaptive augmentation, leading to better downstream classification. The mapping network learns disentangled latent representations while Adaptive Discriminator Augmentation dynamically adjusts augmentation probability to prevent discriminator memorization on limited medical data.

### Mechanism 2
Targeted synthetic augmentation of minority classes improves melanoma detection by rebalancing the training distribution without introducing label noise. By generating synthetic images only for the underrepresented melanoma class and combining them with real samples, classifiers receive more balanced class priors during training.

### Mechanism 3
Basic preprocessing outperforms aggressive artifact removal for downstream diagnosis because clinically relevant texture cues are preserved. The DullRazor algorithm may inadvertently smooth diagnostically meaningful texture patterns, while basic geometric augmentation preserves original textures while increasing spatial invariance.

## Foundational Learning

- **Fréchet Inception Distance (FID) and Kernel Inception Distance (KID)**
  - Why needed here: Primary metrics for evaluating synthetic image quality; lower FID indicates generated images match real data distribution more closely
  - Quick check question: If Model A achieves FID=65 and Model B achieves FID=90, which produces images more similar to the real distribution?

- **Adaptive Discriminator Augmentation (ADA)**
  - Why needed here: Core mechanism preventing StyleGAN2-ADA from overfitting on the small melanoma dataset (1,563 images)
  - Quick check question: What happens to ADA augmentation probability if the discriminator starts memorizing training samples?

- **Class imbalance in medical imaging**
  - Why needed here: Entire motivation for synthetic augmentation stems from melanoma being only 11.03% of the dataset
  - Quick check question: Why would a classifier trained on this dataset achieve high accuracy but poor melanoma sensitivity?

## Architecture Onboarding

- **Component map:**
  Input: Dermoscopic images (HAM10000 + MILK10K, 14,116 total) -> Preprocessing branches: Basic vs Advanced -> Generative models: StyleGAN2-ADA and DDPM -> Augmentation: Synthetic melanoma + real data -> Classifiers: ResNet-18/50, VGG-16, ViT-B/16, EfficientNet-B0

- **Critical path:**
  1. Preprocess images through basic pipeline (resize to 256×256, normalize to [-1,1])
  2. Train StyleGAN2-ADA on melanoma-only subset (1,563 images) with ADA target r_t=0.7
  3. Generate synthetic melanoma images and combine with original training set
  4. Fine-tune ViT-B/16 or ResNet-50 on augmented dataset for maximum melanoma F1

- **Design tradeoffs:**
  - StyleGAN2-ADA: Better fidelity (FID~65) and classifier gains, but may reduce diversity
  - DDPM: Higher sample variance but lower perceptual fidelity (FID~83-90), potentially useful for coverage analysis
  - Advanced preprocessing: Cleaner images but risk of removing diagnostic texture; only use if artifacts correlate with labels

- **Failure signatures:**
  - FID plateauing above 100: Check discriminator overfitting (r_v approaching 1.0), increase ADA probability
  - Classifier F1 decreases with augmentation: Synthetic samples may have mode collapse; reduce generator learning rate or increase training diversity
  - ViT-B/16 shows dispersed Grad-CAM attention: Expected behavior; use ResNet-50 if compact saliency maps are required for clinical interpretability

- **First 3 experiments:**
  1. Reproduce A2 pipeline (StyleGAN2-ADA + basic preprocessing) and verify FID < 80 on generated melanoma samples
  2. Train ResNet-50 on augmented vs. non-augmented data; confirm >8% absolute F1 improvement for melanoma class
  3. Generate Grad-CAM visualizations for both CNN and ViT classifiers on synthetic samples; verify attention focuses on lesion regions rather than artifacts

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: Can conditional or self-supervised diffusion models (e.g., DiNO-Diffusion, Stable Diffusion variants) achieve better class anchoring and perceptual fidelity than unconditional DDPMs for dermoscopic synthesis?
  - Basis in paper: Future work section lists "exploring conditional and self-supervised diffusion models" as a priority direction
  - Why unresolved: The tested DDPM was unconditional and showed weaker class anchoring and higher FID (~90) compared to StyleGAN2-ADA (~65); modern conditional variants were not evaluated
  - What evidence would resolve it: Benchmark DiNO-Diffusion or class-conditional Stable Diffusion on the same dataset, reporting FID, KID, and downstream MEL F1-score under matched conditions

- **Open Question 2**
  - Question: Does human dermatologist evaluation of synthetic image realism correlate with automated metrics (FID, KID, IS)?
  - Basis in paper: Future work proposes "incorporating human-in-the-loop dermatological evaluation to complement automated realism metrics"
  - Why unresolved: The study relied solely on distributional metrics without clinical perceptual validation; automated metrics may not capture diagnostic utility
  - What evidence would resolve it: Conduct reader studies where dermatologists rate synthetic images for realism and diagnostic relevance, then correlate scores with FID/KID/IS

- **Open Question 3**
  - Question: Does synthetic data augmentation generalize to darker skin tones and multi-institutional datasets?
  - Basis in paper: Future work calls for "improving fairness and generalization via inclusion of darker skin tones and multi-institutional datasets"
  - Why unresolved: Current dataset (HAM10000, MILK10K) overrepresents fair-skinned populations; generalization to diverse skin tones remains untested
  - What evidence would resolve it: Train generative models on diverse skin-tone datasets (e.g., Fitzpatrick17k) and evaluate downstream classifier performance across skin-type strata

## Limitations
- The study did not validate results with human dermatologist evaluation of synthetic image quality
- Dataset limitations include potential bias toward fair-skinned populations from HAM10000 and MILK10K
- Exact train/validation/test split ratios and augmentation ratios were not specified

## Confidence
High: The benchmark methodology is well-defined with reproducible metrics and clear evaluation procedures
Medium: Some implementation details (exact split ratios, augmentation counts) are unspecified but do not affect core conclusions
Low: Clinical validation through dermatologist evaluation was not performed

## Next Checks
1. Verify that synthetic melanoma images preserve key diagnostic features by comparing Grad-CAM attention maps between synthetic and real samples
2. Test whether reducing synthetic augmentation ratio below 1:1 with real samples still maintains the 8-15% F1 improvement
3. Evaluate classifier performance on a held-out test set from a different institution to assess generalization beyond the training distribution