---
ver: rpa2
title: 'Automated Safety Benchmarking: A Multi-agent Pipeline for LVLMs'
arxiv_id: '2601.19507'
source_url: https://arxiv.org/abs/2601.19507
tags:
- harmful
- safety
- image
- text
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "VLSafetyBencher is an automated system for constructing safety\
  \ benchmarks for large vision-language models (LVLMs). It uses four collaborative\
  \ agents\u2014Data Preprocessing, Generation, Augmentation, and Selection\u2014\
  to streamline benchmark creation."
---

# Automated Safety Benchmarking: A Multi-agent Pipeline for LVLMs

## Quick Facts
- **arXiv ID**: 2601.19507
- **Source URL**: https://arxiv.org/abs/2601.19507
- **Reference count**: 40
- **Primary result**: VLSafetyBencher constructs safety benchmarks for LVLMs in one week at $1.34 cost, achieving 70% safety rate disparity between models

## Executive Summary
VLSafetyBencher introduces an automated system for constructing safety benchmarks for large vision-language models (LVLMs) using a collaborative multi-agent pipeline. The system employs four specialized agents - Data Preprocessing, Generation, Augmentation, and Selection - to streamline the benchmark creation process. The Generation agent synthesizes harmful image-question pairs through three cross-modal interaction strategies, while the Selection agent optimizes sample quality through iterative algorithms. The approach demonstrates significant efficiency gains over human-constructed benchmarks, achieving a 70% safety rate disparity between the safest and least safe models.

## Method Summary
The VLSafetyBencher system uses a multi-agent pipeline where the Data Preprocessing agent prepares datasets, the Generation agent creates harmful image-question pairs using modality dependency, complementarity, and conflict strategies, the Augmentation agent enhances diversity, and the Selection agent optimizes sample quality through iterative algorithms balancing separability, harmfulness, and diversity. The system constructs benchmarks within one week at minimal cost ($1.34) while demonstrating strong discriminative power compared to existing methods.

## Key Results
- Constructed benchmark in one week at $1.34 cost
- Achieved 70% safety rate disparity between safest and least safe models
- Demonstrated 15.67% higher discriminative power than human-constructed benchmarks

## Why This Works (Mechanism)
The system leverages automated generation of harmful content through cross-modal interaction strategies, enabling systematic exploration of safety vulnerabilities in LVLMs. The iterative selection algorithm ensures optimal balance between sample quality metrics, while the multi-agent architecture enables specialized processing of different benchmark construction phases. The approach's efficiency stems from automation of tasks traditionally requiring extensive human labor.

## Foundational Learning
- **Cross-modal interaction strategies**: Understanding how image and text modalities interact to generate harmful content (needed for generating diverse safety test cases, quick check: verify all three strategies are implemented)
- **Iterative selection algorithms**: Balancing multiple quality metrics simultaneously (needed for optimal sample selection, quick check: confirm convergence criteria)
- **Multi-agent collaboration**: Coordinating specialized agents for complex tasks (needed for efficient pipeline operation, quick check: verify agent communication protocols)
- **Safety rate disparity measurement**: Quantifying differences in model safety performance (needed for benchmark validation, quick check: ensure statistical significance)
- **Cost-effectiveness analysis**: Evaluating resource utilization in benchmark construction (needed for practical deployment assessment, quick check: verify cost calculations)

## Architecture Onboarding
- **Component map**: Data Preprocessing -> Generation -> Augmentation -> Selection
- **Critical path**: Generation agent synthesis → Selection agent optimization → Benchmark validation
- **Design tradeoffs**: Automated generation vs. human oversight, speed vs. quality, cost vs. comprehensiveness
- **Failure signatures**: Inconsistent safety metrics, generation bias, selection algorithm convergence issues
- **First experiments**: 1) Generate sample harmful pairs using each interaction strategy, 2) Run selection algorithm on small dataset, 3) Compare automated vs. human-constructed benchmark performance

## Open Questions the Paper Calls Out
None identified in the provided analysis.

## Limitations
- Limited external validation across diverse LVLM architectures
- Heavy reliance on quantitative metrics without extensive qualitative validation
- Potential gaps in capturing nuanced or context-dependent safety violations

## Confidence
- **Benchmark construction efficiency**: High
- **Benchmark quality and discriminative power**: Medium
- **Generalizability across LVLM architectures**: Medium

## Next Checks
1. External validation across diverse LVLM architectures beyond tested models
2. Human evaluation studies to verify automated safety assessments align with human judgment
3. Long-term stability testing to evaluate benchmark performance consistency over time