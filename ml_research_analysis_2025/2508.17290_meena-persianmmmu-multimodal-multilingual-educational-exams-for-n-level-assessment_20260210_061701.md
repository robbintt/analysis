---
ver: rpa2
title: 'MEENA (PersianMMMU): Multimodal-Multilingual Educational Exams for N-level
  Assessment'
arxiv_id: '2508.17290'
source_url: https://arxiv.org/abs/2508.17290
tags:
- image
- persian
- gpt-4o
- flash
- gpt-4o-mini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MEENA (PersianMMMU) is the first Persian multimodal-multilingual
  benchmark for evaluating vision-language models on scientific reasoning and human-level
  understanding. It contains ~7,500 Persian and 3,000 English multiple-choice questions
  from primary to upper secondary school levels, covering reasoning, mathematics,
  physics, diagrams, charts, and Persian art/literature.
---

# MEENA (PersianMMMU): Multimodal-Multilingual Educational Exams for N-level Assessment

## Quick Facts
- arXiv ID: 2508.17290
- Source URL: https://arxiv.org/abs/2508.17290
- Reference count: 40
- First Persian multimodal-multilingual benchmark for evaluating vision-language models on scientific reasoning and human-level understanding

## Executive Summary
MEENA (PersianMMMU) introduces the first Persian multimodal-multilingual benchmark for evaluating vision-language models on scientific reasoning and human-level understanding. The dataset contains ~7,500 Persian and 3,000 English multiple-choice questions from primary to upper secondary school levels, covering reasoning, mathematics, physics, diagrams, charts, and Persian art/literature. Experiments reveal that knowledge-based tasks outperform reasoning tasks by 10-19% accuracy, Gemini 2.0-flash achieves the highest hallucination detection rate especially in Persian, and model performance declines significantly with question difficulty at higher educational levels.

## Method Summary
The paper evaluates VLMs on Persian and English multimodal multiple-choice questions across five settings: Zero-Shot, In-Context Learning (k=4 examples), First Describe (visual description before answering), Wrong Image (image-question mismatch detection), and Without Image. The MEENA dataset was constructed from the "Pellekan Yadgiri" platform and Iranian national exams, with Persian-English translation performed using GPT-4o with LLM-as-Judge quality filtering (≥4/5 semantic alignment). Five models were tested: GPT-4o, GPT-4o-mini, GPT-4-Turbo, Gemini-2.0-flash, and InstructBLIP-T5. Answer extraction used regex patterns with GPT-4o-mini fallback, and accuracy was computed across subjects, difficulty levels, and languages.

## Key Results
- Knowledge-based tasks outperform reasoning tasks by 10-19% absolute accuracy across both languages
- Gemini 2.0-flash achieves highest hallucination detection rate, especially in Persian
- GPT-4-Turbo and GPT-4o maintain low image presence errors while Gemini 2.0-flash has higher rates (9.17% in Persian)
- Model performance declines with question difficulty, particularly in higher-level chemistry and mathematics

## Why This Works (Mechanism)

### Mechanism 1: Knowledge-Retrieval vs. Multi-Step Reasoning Gap
- Claim: Knowledge-based tasks consistently outperform reasoning tasks by 10-19% absolute accuracy across both languages, with a wider gap in Persian.
- Mechanism: VLMs retrieve factual patterns from training data more effectively than executing sequential logical operations. Reasoning tasks require integrating visual evidence with domain knowledge through structured inference chains—a capability that appears less robust in lower-resource languages like Persian due to sparser training coverage.
- Core assumption: The accuracy differential reflects a genuine architectural limitation in multimodal reasoning rather than benchmark construction bias.
- Evidence anchors:
  - [abstract] "knowledge-based tasks outperform reasoning tasks (+10–19% accuracy)"
  - [Page 8, Results] "Knowledge-based tasks consistently outperform reasoning tasks by a significant margin of +10–19% in absolute accuracy... the performance gap is more pronounced in Persian"
  - [corpus] Weak/no direct corpus evidence; related Persian benchmarks (PerCoR, PersianMedQA) focus on language-only reasoning

### Mechanism 2: Cross-Modal Misalignment Detection (Hallucination Sensitivity)
- Claim: Gemini 2.0-flash detects image-question mismatches at higher rates than GPT-4 variants, particularly in Persian.
- Mechanism: The "Wrong Image" setting substitutes the correct image with an irrelevant one. Models with stronger visual-textual grounding recognize the inconsistency and refuse to answer or flag the mismatch. Gemini's higher detection rate suggests either stricter internal consistency checks or more conservative response policies when cross-modal alignment is weak.
- Core assumption: Higher detection rates indicate better grounding, not merely over-cautious refusal behavior.
- Evidence anchors:
  - [abstract] "Gemini 2.0-flash achieves the highest hallucination detection rate, especially in Persian"
  - [Page 9, Figure 3] Detection rate difference between Gemini 2.0 Flash and GPT-4 Mini exceeds 400 detections on MEENA dataset
  - [corpus] No corpus evidence directly addresses Persian VLM hallucination detection

### Mechanism 3: Image Encoding Robustness Across Languages
- Claim: GPT-4-Turbo and GPT-4o exhibit low "no image" error rates (falsely reporting image absence), while Gemini 2.0-flash shows elevated rates (9.17% in Persian).
- Mechanism: Visual encoders may have language-conditional processing pathways. Higher "no image" errors in Persian suggest either: (a) visual features are not consistently routed to the language decoder for non-English inputs, or (b) preprocessing failures occur for images containing Persian text or culturally-specific visual patterns.
- Core assumption: "No image" responses when images are present reflect encoding or routing failures, not intentional design.
- Evidence anchors:
  - [abstract] "GPT-4-Turbo and GPT-4o maintain low image presence errors, while Gemini 2.0-flash has higher rates"
  - [Page 9, Figure 2b] Gemini 2.0 Flash reaches 9.17% "no image" error rate for Persian inputs
  - [corpus] PerCoR and MELAC papers highlight Persian-specific evaluation challenges, but no multimodal encoding studies

## Foundational Learning

- **Concept: Zero-Shot vs. In-Context Learning (ICL)**
  - Why needed here: The paper evaluates models under five settings; ZS provides baseline while ICL tests few-shot adaptation (k=4 examples).
  - Quick check question: In MEENA's ICL setting, how many demonstration examples precede the target query? (Answer: 4)

- **Concept: Visual Chain-of-Thought ("First Describe")**
  - Why needed here: This setting forces models to articulate visual content before answering, testing whether explicit description reduces shortcut reasoning.
  - Quick check question: What cognitive behavior does "First Describe" aim to prevent? (Answer: Guessing without fully processing visual evidence)

- **Concept: Multimodal Grounding and Hallucination**
  - Why needed here: The "Wrong Image" condition operationalizes hallucination detection—measuring whether models recognize when visual context contradicts the textual query.
  - Quick check question: How does the Wrong Image setting distinguish between grounded vs. hallucinating models? (Answer: Grounded models detect the mismatch; hallucinating models answer confidently despite irrelevant images)

## Architecture Onboarding

- **Component map:**
  - HTML parsing -> question/image extraction -> deduplication
  - Image merging for multi-image questions
  - Translation with quality filtering (LLM-as-Judge scoring)
  - Inference across 5 settings × 3 image cases × 2 languages
  - Answer extraction and accuracy computation by subject/level/difficulty

- **Critical path:**
  1. HTML parsing → question/image extraction → deduplication
  2. Image merging for multi-image questions
  3. Translation with quality filtering (LLM-as-Judge scoring)
  4. Inference across 5 settings × 3 image cases × 2 languages
  5. Answer extraction and accuracy computation by subject/level/difficulty

- **Design tradeoffs:**
  - Merging images preserves VLM compatibility but may obscure spatial relationships
  - Translation introduces potential semantic drift; mitigated by quality thresholding
  - LLM-as-Judge for answer extraction trades deterministic extraction for coverage

- **Failure signatures:**
  - ICL underperforms Zero-Shot (GPT-4o-mini: 0.310 ZS → 0.224 ICL on Persian)
  - "No image" errors concentrated in Persian (Gemini: 9.17%)
  - Accuracy collapse at upper secondary levels (Chemistry lvl 12: 0.23–0.47 across models)

- **First 3 experiments:**
  1. Run Zero-Shot baseline on Persian Mathematics subset; expect 0.28–0.42 accuracy range
  2. Apply Wrong Image setting to Art subset; compare detection rates (expect Gemini > GPT)
  3. Test First Describe on high-difficulty Natural Science; check if accuracy improves over Zero-Shot

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why is the performance gap between reasoning and knowledge-based tasks significantly more pronounced in Persian (+10–19%) compared to English?
- Basis in paper: [explicit] The paper (Page 8) reports that while knowledge-based tasks consistently outperform reasoning ones, the gap is "more pronounced in Persian, indicating that reasoning tasks in this language are more difficult than in English."
- Why unresolved: The authors quantify the accuracy drop but do not investigate whether this stems from a lack of Persian reasoning data in pre-training, tokenization inefficiencies, or linguistic structural barriers.
- What evidence would resolve it: A comparative analysis of model attention mechanisms on reasoning steps in Persian versus English, or experiments controlling for token count and training data volume.

### Open Question 2
- Question: What specific architectural or training factors allow Gemini 2.0-flash to detect mismatched images (hallucinations) at a significantly higher rate than GPT-4o, particularly in Persian?
- Basis in paper: [explicit] The results (Page 9) show Gemini 2.0-flash "surpasses GPT-4 and GPT-4o-Mini in detecting image mismatches" with a clear performance gap in Persian, but the paper offers no explanation for this specific cross-lingual robustness.
- Why unresolved: The paper identifies the behavioral outcome (superior detection) but leaves the cause—whether due to different vision encoders, contrastive loss functions, or data augmentation techniques—unexplored.
- What evidence would resolve it: Ablation studies on the vision encoders of these models or an analysis of cross-modal attention alignment scores when processing incorrect images in Persian contexts.

### Open Question 3
- Question: Does the use of GPT-4o as the primary translation engine for the English subset introduce evaluation bias (circularity) when evaluating GPT-4o itself?
- Basis in paper: [inferred] The methodology (Page 5) states that GPT-4o was used to translate Persian questions into English to create the bilingual subset, yet GPT-4o is one of the primary models evaluated on this subset (Table 2).
- Why unresolved: The paper relies on "LLM-as-a-Judge" to ensure translation quality, but does not address the potential for "self-preference" or stylistic alignment where a model performs better on text it (or a similar architecture) generated.
- What evidence would resolve it: Comparing GPT-4o's performance on the GPT-translated English subset versus a human-translated gold standard or a subset translated by a different model family (e.g., Gemini).

## Limitations

- Translation quality relies on LLM-as-Judge without human validation, introducing potential semantic drift
- Manual ICL example selection lacks detailed criteria, raising reproducibility concerns
- Dataset origin from Iranian educational sources may not generalize to broader Persian-speaking populations
- Limited sample sizes for higher grades, especially Chemistry lvl 12, without statistical significance testing

## Confidence

- **High confidence**: Knowledge-based tasks outperforming reasoning tasks (10-19% gap) - supported by clear numerical results across multiple subjects and languages
- **Medium confidence**: Gemini 2.0-flash having highest hallucination detection rates - result appears robust but interpretation (better grounding vs. over-cautious refusal) remains uncertain
- **Medium confidence**: GPT-4 variants maintaining low "no image" errors while Gemini has elevated rates - statistically significant but mechanism unclear (encoding failure vs. design choice)
- **Low confidence**: Difficulty-based performance degradation at upper secondary levels - limited sample sizes for higher grades (especially Chemistry lvl 12) and no statistical significance testing reported

## Next Checks

1. **Translation quality validation**: Have two independent bilingual experts rate 100 randomly sampled Persian-English question pairs for semantic equivalence. Compare human ratings against the LLM-as-Judge threshold to verify the ≥4/5 cutoff maintains translation fidelity.

2. **ICL example sensitivity analysis**: Systematically vary the ICL demonstration examples (using both optimal and deliberately suboptimal examples) and measure performance variance across all five settings. This will determine whether ICL's underperformance is due to poor example selection or inherent limitations of few-shot learning for this task.

3. **Cross-modal grounding test**: Implement a controlled experiment where models receive identical questions with three image conditions: correct image, semantically unrelated image (current "Wrong Image"), and semantically related but incorrect image. Compare detection rates to distinguish between genuine grounding ability versus generic refusal behavior.