---
ver: rpa2
title: AI-generated data contamination erodes pathological variability and diagnostic
  reliability
arxiv_id: '2601.12946'
source_url: https://arxiv.org/abs/2601.12946
tags:
- data
- synthetic
- clinical
- generation
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Self-referential training on synthetic medical data causes rapid
  erosion of diagnostic accuracy and pathological variability across clinical text,
  vision-language reports, and medical images. As models train on their own outputs,
  vocabulary shrinks (98.9% reduction in radiology reports), rare findings disappear,
  and false reassurance rates triple to 40%.
---

# AI-generated data contamination erodes pathological variability and diagnostic reliability

## Quick Facts
- arXiv ID: 2601.12946
- Source URL: https://arxiv.org/abs/2601.12946
- Reference count: 40
- Self-referential training on synthetic medical data causes rapid erosion of diagnostic accuracy and pathological variability across clinical text, vision-language reports, and medical images.

## Executive Summary
Self-referential training on synthetic medical data causes rapid model collapse through distributional narrowing, where vocabulary shrinks by 98.9% and rare pathological findings disappear. Models become overconfident on degraded content while failing to detect life-threatening conditions, with false reassurance rates tripling to 40%. Physician evaluation shows near-complete rewrite requirements after just two generations. The degradation is intrinsic to recursive training rather than architecture-specific, and mitigation requires mandatory human oversight, provenance tagging, and maintaining minimum real data proportions in retraining.

## Method Summary
The study employed a self-referential training pipeline where each generation of models trained exclusively on synthetic outputs from the previous generation. Text generation used GPT-2/Qwen3-8B with nucleus sampling (temperature=0.7-0.8, top_p=0.95), vision-language models used R2GenGPT with LoRA adaptation, and image synthesis used DDPM with U-Net backbone. Quality-aware filtering selected synthetic samples closest to real distribution using k-NN distances computed by an external GPT-2 Large model. The pipeline maintained fresh initialization from base checkpoints each generation to isolate data degradation effects.

## Key Results
- Vocabulary underwent near-complete extinction, plummeting from 12,078 unique words in generation 0 to approximately 200 by generation 4 (98.9% reduction)
- Generation-4 models achieved low perplexity on their own synthetic outputs (1.08 ± 0.04) while perplexity on authentic clinical text reached 211.5 ± 209.0
- False reassurance rates tripled to 40%, with physician evaluation showing near-complete rewrite requirements after just two generations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-referential training on synthetic medical data causes rapid model collapse through distributional narrowing.
- Mechanism: Each generation of models trains exclusively on outputs from the previous generation, compounding sampling bias toward high-probability tokens while systematically eliminating rare pathological signals. The model's probability distribution contracts around modal patterns, causing vocabulary extinction (98.9% reduction observed) and loss of diagnostic heterogeneity.
- Core assumption: The degradation is intrinsic to recursive training on synthetic data rather than architecture-specific.
- Evidence anchors:
  - [abstract] "models progressively converge toward generic phenotypes regardless of the model architecture"
  - [section] "Vocabulary underwent near-complete extinction, plummeting from 12,078 unique words in generation 0 to approximately 200 by generation 4 (98.9% reduction)"
  - [corpus] "On the Dangers of Bootstrapping Generation for Continual Learning and Beyond" (arXiv:2512.11867) directly addresses distribution drift from synthetic data training with FMR=0.608, confirming this is a recognized phenomenon beyond medicine
- Break condition: Introducing ≥50-75% authentic human data per training generation arrests collapse by anchoring the distribution to real clinical variability.

### Mechanism 2
- Claim: Model confidence decouples from diagnostic accuracy, creating dangerous false reassurance.
- Mechanism: As models train on their own outputs, perplexity on synthetic data decreases (indicating high confidence) while perplexity on authentic clinical text increases up to 45-fold. Models learn to predict their own degraded patterns fluently while losing comprehension of genuine medical language. This produces confident but clinically useless outputs.
- Core assumption: Standard evaluation metrics that measure performance on training-like data will fail to detect this degradation.
- Evidence anchors:
  - [abstract] "false reassurance rates tripling to 40%"
  - [section] "generation-4 models achieved low perplexity on their own synthetic outputs (1.08 ± 0.04) while perplexity on authentic clinical text reached 211.5 ± 209.0, indicating models became overconfident on degraded content"
  - [corpus] Weak direct evidence—corpus papers focus on synthetic data generation quality rather than confidence-accuracy decoupling specifically
- Break condition: Monitoring perplexity on held-out real clinical data (not synthetic outputs) exposes the confidence gap; quality-aware filtering using external reference models reduces overconfidence.

### Mechanism 3
- Claim: Synthetic volume scaling fails to mitigate collapse; quality-aware filtering with real data mixing is required.
- Mechanism: Increasing synthetic data volume amplifies rather than dilutes recursive errors because each generation's systematic bias becomes more deeply entrenched when reinforced by greater volume. The paper shows 5× volume expansion increased the confidence gap from 196-fold to 317-fold. Only continuous injection of authentic data preserves pathological diversity.
- Core assumption: Authentic human data functions as a non-renewable anchor that cannot be replaced by synthetic generation.
- Evidence anchors:
  - [abstract] "while synthetic volume scaling fails to prevent collapse, mixing real data with quality-aware filtering effectively preserves diversity"
  - [section] "increased volume not only failed to prevent degradation but actively accelerated it... mean perplexity of 346.6 on real clinical text, representing a 64-fold increase from baseline"
  - [corpus] Limited corpus evidence on mitigation strategies specifically; related work focuses on generation quality rather than training composition
- Break condition: Maintaining ≥50-75% real data proportion with quality-aware filtering (selecting synthetic samples closest to real distribution) provides partial protection.

## Foundational Learning

- Concept: **Model collapse (distributional contraction)**
  - Why needed here: The paper's central phenomenon—understanding why recursive training on synthetic data causes vocabulary extinction and loss of rare findings requires grasping how maximum likelihood training amplifies high-probability modes while suppressing distribution tails.
  - Quick check question: Can you explain why training a language model on its own outputs would cause vocabulary size to shrink rather than stay stable?

- Concept: **Perplexity as a confidence vs. capability signal**
  - Why needed here: The false confidence paradox depends on understanding that perplexity measured on synthetic outputs vs. real data reveals different capabilities—low perplexity on synthetic data with high perplexity on real data indicates overconfidence on degraded patterns.
  - Quick check question: If a model achieves perplexity of 1.1 on its own generated outputs but 200+ on held-out real clinical text, what does this tell you about its actual clinical capability?

- Concept: **Provenance and data mixing ratios**
  - Why needed here: The paper's mitigation strategy hinges on maintaining specific proportions of authentic vs. synthetic data; understanding why "more synthetic data" fails while "more real data" works is counterintuitive but critical.
  - Quick check question: Why would doubling synthetic training data from 5,000 to 10,000 samples worsen model collapse rather than help it?

## Architecture Onboarding

- Component map:
  - **Self-referential training loop**: Generation N model → produces synthetic data → trains Generation N+1 model (fresh initialization from base checkpoint)
  - **Text generation pipeline**: GPT-2/Qwen3-8B with nucleus sampling (temperature=0.7-0.8, top_p=0.95)
  - **Vision-language pipeline**: R2GenGPT (Swin-Transformer encoder + frozen Llama-2 7B) with LoRA adaptation
  - **Image synthesis pipeline**: DDPM with U-Net backbone (64→512 channels, 1000 timesteps)
  - **Quality-aware filtering**: External GPT-2 Large (774M) computes k-NN distances to real distribution; retain samples closest to authentic data

- Critical path:
  1. Initialize from pretrained base (not previous generation's weights)—this isolates data degradation from model drift
  2. Fine-tune exclusively on synthetic outputs from previous generation
  3. Generate synthetic corpus using nucleus sampling
  4. Apply quality filtering before next generation training
  5. Monitor perplexity on real held-out data (not synthetic) to detect collapse

- Design tradeoffs:
  - **Fresh initialization vs. continued training**: Paper resets to base weights each generation to isolate data effects; in production, this may underestimate accumulated drift
  - **Filtering threshold (25th percentile)**: More aggressive filtering preserves quality but reduces synthetic data utility; paper shows 40% quality improvement at fixed 25% real data budget
  - **Real data proportion (50-75%)**: Higher proportions protect better but may be infeasible with privacy constraints; quality-aware filtering can double efficiency at 25% real data

- Failure signatures:
  - Vocabulary contraction >90% from baseline
  - Perplexity gap >10× between synthetic outputs and real data
  - False reassurance rate >30% on pathological inputs
  - Demographic bias amplification (gender skew >15 percentage points)
  - Report uniqueness <10%

- First 3 experiments:
  1. Replicate the baseline collapse experiment: Train GPT-2 on 5,000 radiology reports for 4 generations with pure synthetic inheritance; measure vocabulary size, medical term density, and perplexity on real vs. synthetic data to confirm the degradation pattern.
  2. Test the real data mixing threshold: Compare 0%, 25%, 50%, and 75% real data conditions across 4 generations; identify which proportion maintains >50% of baseline vocabulary and keeps perplexity gap <10-fold.
  3. Validate quality-aware filtering: At 25% real data budget, compare random mixing vs. embedding-based k-NN filtering; measure whether filtering achieves comparable performance to unfiltered 50% real data (testing the efficiency doubling claim).

## Open Questions the Paper Calls Out

- How does recursive training on synthetic data alter clinical vocabulary and diagnostic accuracy over successive generations?
- What is the impact of self-referential training on model confidence versus actual clinical capability?
- Can synthetic data volume scaling prevent model collapse, or are other mitigation strategies required?
- How does quality-aware filtering compare to random mixing in preserving diagnostic reliability?

## Limitations
- The findings are based on controlled simulation using radiology reports and limited image synthesis, which may not generalize to all medical specialties or multimodal clinical data.
- The quality-aware filtering mechanism relies on embedding similarity to real data, but this assumes that embedding spaces adequately capture clinical relevance and diagnostic utility.
- The physician evaluation involved only two expert reviewers rating 100 samples, which may not capture full clinical practice variability.

## Confidence
- **High confidence**: The core mechanism of model collapse through recursive training on synthetic data is well-established in the broader literature (FMR=0.608 from "On the Dangers of Bootstrapping Generation") and the paper's controlled experiments clearly demonstrate distributional narrowing, vocabulary extinction, and confidence-accuracy decoupling.
- **Medium confidence**: The specific thresholds for mitigation (50-75% real data, 25th percentile filtering) are empirically derived but may require validation across different medical domains and model architectures.
- **Low confidence**: The scalability of the proposed mitigation strategies to large-scale production systems with privacy constraints is not demonstrated.

## Next Checks
1. **Cross-domain validation**: Replicate the complete experiment pipeline (4 generations, 5,000 samples per generation) using pathology reports, discharge summaries, and clinical notes from different medical specialties to test whether vocabulary contraction and diagnostic degradation patterns hold across clinical text types.
2. **Rare finding preservation**: Design a synthetic test set containing 50-100 rare but critical findings (pneumothorax, effusions, rare tumors) and measure detection rates across generations to validate whether the 90% vocabulary extinction directly translates to diagnostic failures on life-threatening conditions.
3. **Production feasibility study**: Implement the 25% real data + quality-aware filtering pipeline on a synthetic dataset 10× larger (50,000 samples) and measure computational overhead, memory requirements, and whether the 40% quality improvement at fixed budget holds at scale.