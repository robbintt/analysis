---
ver: rpa2
title: Structure-Aware Corpus Construction and User-Perception-Aligned Metrics for
  Large-Language-Model Code Completion
arxiv_id: '2505.13073'
source_url: https://arxiv.org/abs/2505.13073
tags:
- code
- metrics
- completion
- semantic
- adoption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes two user-aligned evaluation metrics\u2014\
  LCP and ROUGE-LCP\u2014for code completion tasks, derived from probabilistic modeling\
  \ of LLM output distributions. Through theoretical analysis and empirical validation\
  \ on 10,769 user completion samples, the metrics show strong correlation with user\
  \ adoption behavior (Pearson r 0.7, p < 0.05), outperforming traditional metrics\
  \ like EM and ROUGE-L."
---

# Structure-Aware Corpus Construction and User-Perception-Aligned Metrics for Large-Language-Model Code Completion

## Quick Facts
- arXiv ID: 2505.13073
- Source URL: https://arxiv.org/abs/2505.13073
- Reference count: 28
- Key outcome: LCP and ROUGE-LCP metrics show strong correlation with user adoption behavior (Pearson r > 0.7, p < 0.05), outperforming traditional metrics

## Executive Summary
This paper addresses two fundamental challenges in LLM code completion: evaluation metric alignment with user behavior and structural awareness in pretraining corpora. The authors propose LCP and ROUGE-LCP metrics derived from probabilistic modeling of autoregressive output distributions, demonstrating strong correlation with user adoption decisions. To improve repository-level completion, they introduce a Structure-Preserving and Semantically-Reordered Code Graph (SPSR-Graph) method that explicitly models cross-file and cross-module dependencies during corpus construction. Experimental results show consistent performance gains across C/C++ tasks when using the proposed structural-aware pretraining corpora.

## Method Summary
The method consists of two main innovations. First, the LCP metric measures consecutive correct characters from position 1, capturing the importance of prefix correctness in left-to-right code editing. ROUGE-LCP normalizes this by reference length. Second, the SPSR-Graph construction method parses code into semantic units, builds a directed graph encoding call/reference relationships, and generates training samples via BFS traversal ordered by dependency paths rather than file order. The approach replaces traditional random token masking with AST-based semantic segmentation during pretraining.

## Key Results
- LCP and ROUGE-LCP show Pearson correlation r > 0.7 (p < 0.05) with user adoption behavior, outperforming EM and ROUGE-L
- SPSR-Graph construction improves LCP values by up to 18.55% and BLEU scores by 2.74 points across C/C++ tasks
- Optimal graph breadth parameter k=4, with performance declining at k>4 due to noise introduction
- AST-based semantic segmentation achieves perfect structural preservation (Rs = 1.0) compared to greedy segmentation (Rs = 1/k)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LCP correlates more strongly with user adoption behavior than traditional metrics
- Mechanism: Users working left-to-right in code editors disproportionately value correctness at the start of completions. LCP captures this by measuring consecutive correct characters from position 1, whereas metrics like ROUGE-L permit non-contiguous matches that don't reflect real editing patterns.
- Core assumption: User adoption decisions are primarily driven by prefix correctness rather than overall similarity
- Evidence anchors: Pearson r > 0.7, p < 0.05 correlation; LCP correlation r = 0.8707 with p < 0.05; long-tail distribution matching theoretical model

### Mechanism 2
- Claim: AST-based semantic segmentation for FIM pretraining preserves structural integrity
- Mechanism: Traditional FIM masks arbitrary token spans, which can split semantic units mid-way. AST-based segmentation extracts complete subtrees as masking targets, ensuring the model learns to predict structurally coherent units.
- Core assumption: Models trained on structurally intact units better generalize to real completion scenarios
- Evidence anchors: +AST improves EM (16.54%→16.72% C++), though BLEU slightly decreases; semantic subtree extraction formalized

### Mechanism 3
- Claim: SPSR-Graph reordering enables cross-file dependency modeling
- Mechanism: Code is parsed into semantic units, then connected via a directed graph encoding call/reference relationships. BFS traversal generates training samples ordered by dependency paths rather than file order.
- Core assumption: Dependency-ordered sequences provide training signal that improves repository-level completion
- Evidence anchors: Graph construction and BFS path extraction formalized; +KGF yields 2.66% EM gain for C, 2.74 BLEU improvement; breadth-5 to -7 shows declining performance

## Foundational Learning

- Concept: **Fill-in-the-Middle (FIM) Pretraining**
  - Why needed here: Core training paradigm being modified—understanding baseline FIM (mask middle, predict masked) is prerequisite to appreciating AST-based variants
  - Quick check question: Can you explain why FIM differs from standard next-token prediction for code completion?

- Concept: **Abstract Syntax Trees (AST)**
  - Why needed here: Central to both semantic segmentation and SPSR-Graph construction—AST parsing extracts structural units that token-level methods miss
  - Quick check question: What type of code construct would appear as a complete subtree vs. span multiple subtrees?

- Concept: **Probabilistic Modeling of Autoregressive Generation**
  - Why needed here: LCP metric is derived from probability chains of correct token predictions—understanding P(st = rt | history) is essential
  - Quick check question: Why does error probability compound with sequence length in autoregressive models?

## Architecture Onboarding

- Component map: Raw corpus → AST parsing → Semantic unit extraction → Graph construction → BFS traversal → Training samples
- Critical path: 1) Raw corpus → AST parsing (per file) 2) Semantic unit extraction with granularity threshold θ 3) Graph construction from cross-file call/reference analysis 4) BFS traversal with depth limit D and breadth limit k 5) Training sample emission with file-path tagging at cross-file boundaries
- Design tradeoffs: Graph breadth (k): k=4 optimal; k>4 introduces noise. Traversal depth (D): Fixed at 1 for single-line; deeper for multi-line tasks. Granularity (θ): Controls subtree size range vs. computational overhead. AST vs. token FIM: Structural coherence vs. simplicity
- Failure signatures: BLEU drops with +AST (29.33→27.83 for C++) while EM rises—metric tradeoff. LCP stays flat across corpus strategies—prefix prediction may not benefit from structural enhancement. Adoption rate spikes at LCP=1 due to trivial completions
- First 3 experiments: 1) Baseline metric correlation: Compute LCP, ROUGE-L, EM on existing completion logs; measure Pearson correlation with adoption rate 2) AST segmentation ablation: Train identical model with (a) token-level FIM, (b) AST-FIM; compare EM/LCP/BLEU 3) Graph breadth sweep: With D=1 fixed, train separate models with k ∈ {3, 4, 5, 6, 7}; identify performance peak

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the LCP and ROUGE-LCP metrics be effectively adapted to evaluate non-autoregressive or masked language models for code completion?
- Basis in paper: [explicit] The authors state in the Limitations section that the "current metrics in terms of model affinity are applicable only to autoregressive generative models, and their adaptability to other types of models remains to be further studied."
- Why unresolved: The theoretical derivation of the metrics relies on the chain rule of probability specific to autoregressive generation, which differs fundamentally from the independent decoding assumptions of non-autoregressive models.
- What evidence would resolve it: An empirical study validating the correlation between these metrics and user adoption rates when applied to non-autoregressive code completion architectures.

### Open Question 2
- Question: Do LCP and ROUGE-LCP maintain a high correlation with user adoption behavior in multi-line or function-level code generation tasks?
- Basis in paper: [explicit] The paper notes that the work "focuses only on on-the-fly completion tasks" (typically single-line or inline) and that user "left-to-right code editing habits" drive the efficacy of the LCP metric.
- Why unresolved: The "prefix-first" importance established for single-line completion may not hold for multi-line generation, where users might prioritize functional correctness or structural integrity over the immediate correctness of the first few characters.
- What evidence would resolve it: A user study measuring the Pearson correlation coefficient between LCP/ROUGE-LCP scores and adoption rates specifically on datasets of multi-line code generation suggestions.

### Open Question 3
- Question: Does the SPSR-Graph corpus construction method improve model performance for dynamic programming languages (e.g., Python, JavaScript) to the same extent demonstrated for C/C++?
- Basis in paper: [inferred] The experimental validation is restricted to C/C++ tasks in the communication domain, relying on specific structures like "header inclusion" and "struct reference paths" which have different implementations or do not exist in dynamic languages.
- Why unresolved: The graph construction depends on AST parsing and explicit cross-file dependencies (like includes); dynamic languages often handle dependencies implicitly or at runtime, potentially reducing the effectiveness of the proposed "Structure-Preserving" reordering.
- What evidence would resolve it: Comparative benchmarks (EM, BLEU, LCP) of models trained on SPSR-Graph processed corpora versus standard corpora in dynamic languages like Python.

### Open Question 4
- Question: How does increasing the SPSR-Graph traversal depth beyond 1 impact performance on repository-level tasks that require long-range cross-module context?
- Basis in paper: [inferred] The authors fixed the traversal depth D=1 because the specific "on-the-fly" task involves local context, acknowledging that "deep dependency modeling is unnecessary" for that specific scope, but leaving the impact of deeper graphs unexplored.
- Why unresolved: While depth=1 optimizes for local single-line completion, it is unclear if deeper graph traversal introduces distracting noise or provides necessary semantic signals for more complex repository-level code synthesis.
- What evidence would resolve it: Ablation studies varying the graph depth parameter D on code completion benchmarks that specifically require reasoning across multiple non-adjacent files.

## Limitations
- LCP metric may overvalue trivial completions (punctuation, variable names) that drive strong correlation with adoption but don't reflect substantive completion quality
- Experimental design cannot definitively separate structural awareness effects from other concurrent changes like improved filtering/deduplication
- AST-based semantic segmentation and SPSR-Graph construction introduce significant engineering overhead with mixed metric results (EM ↑, BLEU ↓)

## Confidence

**High confidence**: The LCP metric's mathematical definition and its direct measurement of prefix correctness are sound. The theoretical basis for why prefix correctness matters (left-to-right editing patterns) is well-established.

**Medium confidence**: The structural preservation benefits of AST-based segmentation are demonstrated through controlled ablation experiments, though the mixed BLEU results suggest the approach isn't universally beneficial. The graph-based reordering mechanism shows consistent gains but requires careful parameter tuning (optimal breadth k=4).

**Low confidence**: The exact mechanism by which SPSR-Graph improves repository-level completion is not fully characterized. The relationship between dependency-ordered training sequences and cross-file completion performance lacks detailed ablation analysis to isolate specific contributing factors.

## Next Checks
1. Compute adoption rate correlation separately for high-information (function/complex type completions) vs. low-information (punctuation/variable name) LCP values to determine if the strong overall correlation is driven by trivial matches
2. Systematically vary the number of AST segments (k) in semantic segmentation and measure the relationship between structural preservation rate (Rs) and completion quality metrics to establish the functional form of the benefit
3. Extend the breadth sweep beyond k=7 to determine the precise point where additional dependencies begin degrading performance, and characterize the noise-to-signal ratio at different breadth levels