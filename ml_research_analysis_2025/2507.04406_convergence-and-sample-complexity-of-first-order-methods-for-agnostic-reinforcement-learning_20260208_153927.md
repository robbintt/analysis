---
ver: rpa2
title: Convergence and Sample Complexity of First-Order Methods for Agnostic Reinforcement
  Learning
arxiv_id: '2507.04406'
source_url: https://arxiv.org/abs/2507.04406
tags:
- policy
- algorithm
- learning
- lemma
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies agnostic policy learning in reinforcement learning,
  where the goal is to find a policy competitive with the best in a given policy class
  without assuming the optimal policy is in that class. The authors introduce a policy
  learning framework that reduces agnostic RL to first-order optimization in a non-Euclidean
  space, leading to new algorithms and improved convergence results for existing ones.
---

# Convergence and Sample Complexity of First-Order Methods for Agnostic Reinforcement Learning

## Quick Facts
- arXiv ID: 2507.04406
- Source URL: https://arxiv.org/abs/2507.04406
- Reference count: 40
- This paper studies agnostic policy learning in RL, introducing a framework that reduces it to first-order optimization in non-Euclidean spaces, leading to improved convergence results and sample complexity bounds.

## Executive Summary
This paper studies agnostic policy learning in reinforcement learning, where the goal is to find a policy competitive with the best in a given policy class without assuming the optimal policy is in that class. The authors introduce a policy learning framework that reduces agnostic RL to first-order optimization in a non-Euclidean space, leading to new algorithms and improved convergence results for existing ones. Under the assumption that the policy class is convex and satisfies a variational gradient dominance (VGD) condition, they obtain sample complexity upper bounds for three policy learning algorithms: Steepest Descent Policy Optimization (SDPO), Conservative Policy Iteration (CPI), and Policy Mirror Descent (PMD).

## Method Summary
The paper proposes a general policy learning framework that reduces agnostic RL to first-order optimization in a non-Euclidean space. The key innovation is the variational gradient dominance (VGD) condition, which is strictly weaker than completeness and coverage conditions traditionally used in RL theory. The authors develop three algorithms - SDPO, CPI, and PMD - and analyze their sample complexity under VGD. The empirical evaluation estimates VGD parameters during optimization across several standard environments using L2-SDPO, showing that the empirically observed VGD parameters are reasonable compared to theoretical ones.

## Key Results
- Introduces VGD as a strictly weaker condition than completeness + coverage, enabling sample-efficient learning in non-realizable settings
- Develops steepest descent methods for smooth non-convex constrained optimization in non-Euclidean spaces
- Improves CPI's iteration complexity by casting it as an instance of the Frank-Wolfe algorithm
- Achieves polynomial sample complexity for PMD independent of policy class parametrization
- Empirically validates VGD parameters across Cartpole, Acrobot, and MinAtar environments

## Why This Works (Mechanism)

### Mechanism 1: Reduction to Non-Euclidean First-Order Optimization
- Claim: Agnostic policy learning can be reduced to constrained first-order optimization over policy space, with convergence rates independent of state-space cardinality.
- Mechanism: The policy gradient theorem provides unbiased gradient estimates. Combined with local smoothness of the value function w.r.t. a non-Euclidean norm (∥·∥L2(μπ),1), each optimization oracle call produces an approximate gradient step in functional space. The local norm adapts to the on-policy occupancy measure, decoupling rates from |S|.
- Core assumption: The policy class Π is convex and satisfies (ν, εvgd)-VGD.
- Evidence anchors:
  - [abstract] "We propose a general policy learning framework that reduces this problem to first-order optimization in a non-Euclidean space."
  - [section 3] "smoothness of the value function is established w.r.t. a (non-Euclidean) norm that measures distance between policies in a manner that is independent of the size of the state space."
  - [corpus] Weak direct corpus support; related work (Policy Newton Algorithm in RKHS) addresses kernel-based policy optimization but not the non-Euclidean reduction directly.
- Break condition: VGD condition fails (i.e., ν → ∞ or εvgd → ∞); policy class non-convex (breaks steepest descent analysis).

### Mechanism 2: Variational Gradient Dominance Replaces Completeness/Coverage
- Claim: VGD is strictly weaker than completeness + coverage, enabling sample-efficient learning in non-realizable settings.
- Mechanism: VGD bounds suboptimality by a linear function of the directional gradient: V(π) − V⋆(Π) ≤ ν max_{π̃∈Π} ⟨∇V(π), π − π̃⟩ + εvgd. Completeness (E(Π) = 0) + bounded distribution mismatch (D∞ < ∞) implies VGD with ν = HD∞ and εvgd = E(Π)H²D∞ (Lemma 1), but VGD can hold when these don't.
- Core assumption: VGD parameters (ν, εvgd) are moderate for the policy class and environment.
- Evidence anchors:
  - [abstract] "an assumption known to be strictly weaker than more standard completeness and coverability conditions"
  - [section 5 / Figure 2] Empirical evaluation shows ν estimates remain moderate (≈1 or below) during convergence across Cartpole, Acrobot, and MinAtar environments.
  - [corpus] No direct corpus validation of VGD empirics; related work assumes completeness (e.g., PMD with TD evaluation).
- Break condition: Policy landscape has poor gradient information (flat regions, misleading local optima); εvgd dominates signal (large irreducible suboptimality).

### Mechanism 3: Actor-Oracle Efficiency via Approximate Updates
- Claim: Practical algorithms require O(1) actor memory and objective functions evaluable independently of |S|.
- Mechanism: CPI's exact convex combination requires storing all prior actors (linear memory). DA-CPI approximates this via a second oracle invocation, controlling error propagation through local-norm analysis. SDPO and PMD naturally maintain only the current and next policy.
- Core assumption: Approximate minimization errors (εerm) are bounded and controlled; local smoothness bounds error amplification.
- Evidence anchors:
  - [section 1.1] "CPI requires linear actor memory, while PMD, SDPO, and DA-CPI are all actor efficient, requiring at most two actor models at any given time."
  - [section 4.2 / Theorem 3] DA-CPI achieves O(ν²AH³/K^(2/3) + ν²ε^(2/3)K^(2/3) + εvgd) with actor-oracle efficiency.
  - [corpus] No corpus papers address actor memory constraints directly; most theoretical work assumes exact updates.
- Break condition: Optimization oracle errors accumulate uncontrollably; local smoothness constant β grows too large (requires small εex exploration, which may harm sample efficiency).

## Foundational Learning

- Concept: First-order optimization in constrained non-convex settings
  - Why needed here: All algorithms (SDPO, CPI, PMD) are instances of steepest descent, Frank-Wolfe, or proximal methods. Understanding O(1/√K) → O(1/K) rate transitions under VGD is essential.
  - Quick check question: Can you explain why VGD enables O(1/K) convergence for Frank-Wolfe while general non-convex objectives yield only O(1/√K)?

- Concept: Policy gradient theorem and occupancy measures
  - Why needed here: Connects sampled Q-estimates to true value gradients; μπ defines both sampling distribution and local geometry.
  - Quick check question: Given π, how would you obtain an unbiased estimate of ⟨∇V(π), π̃ − π⟩ from environment samples?

- Concept: Covering numbers and uniform convergence
  - Why needed here: Sample complexity bounds require generalization from empirical to population objectives over Π.
  - Quick check question: What is the ε-covering number N(ε, Π, ∥·∥∞,1), and how does it enter the sample complexity of SDPO?

## Architecture Onboarding

- Component map: Environment (MDP) -> On-policy Rollout (Algorithm 7) -> Q-estimates (bQ^k) -> Optimization Oracle -> Policy Update (SDPO/CPI/PMD) -> New Policy π_{k+1} -> VGD Monitoring (optional, for diagnostics) -> ν_k estimation

- Critical path: (1) Ensure Π is convex and VGD parameters are moderate (empirical check via ν_k monitoring). (2) Choose algorithm based on actor-memory constraints: SDPO/DA-CPI for practical settings, CPI for theoretical benchmarks. (3) Set exploration εex ≈ H²/K^(2/3) (SDPO) or K^(-2/3) (PMD) per theorems.

- Design tradeoffs:
  - SDPO: Better ε^(1/2) error dependence vs. PMD's ε^(1/4); requires L1 action norm for best A-dependence.
  - DA-CPI: Actor-efficient but O(1/K^(2/3)) rate vs. CPI's O(1/K) (theoretical, not practical).
  - PMD: Most flexible (arbitrary regularizers), but sample complexity degrades for non-L2 regularizers.
  - CPI: Best theoretical rate O(1/K), but linear actor memory—impractical for neural policies.

- Failure signatures:
  - ν_k estimates growing unboundedly → VGD condition failing; consider richer policy class or environment structure.
  - Suboptimality plateauing above εvgd → irreducible approximation error; policy class may be fundamentally mismatched.
  - Sample complexity scaling with |S| → local norm implementation error (should scale with log-covering number only).

- First 3 experiments:
  1. **VGD parameter estimation**: Implement Algorithm 12 on a tabular MDP; verify ν_k ≤ HD∞ when completeness holds, and observe ν_k behavior when it doesn't.
  2. **SDPO vs. PMD comparison**: Run both on Cartpole/Acrobot with identical sample budgets; confirm SDPO's superior ε-dependence via controlled noise injection (vary N, observe error floors).
  3. **Actor-memory stress test**: Implement CPI with K=1000 on a policy class requiring 10K+ parameters per actor; measure memory blowup, then verify DA-CPI resolves it with negligible performance loss.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the sample complexity bounds be tightened to show better dependence on the effective horizon H and action space cardinality A?
- Basis in paper: [explicit] The authors state in Appendix E that they "did not make a notable effort in obtaining optimal dependence on all problem parameters, and expect these can be easily tightened."
- Why unresolved: The current bounds have high polynomial dependencies (e.g., H^7), which may obscure the practical efficiency of the proposed algorithms.
- What evidence would resolve it: Refined theoretical analyses or lower bounds proving linear or logarithmic dependence on H and A.

### Open Question 2
- Question: Is the ε_ex-greedy exploration strictly necessary to ensure the local smoothness required for convergence?
- Basis in paper: [inferred] Appendix E highlights "the greedy exploration required by the current local-smoothness analysis" as a key area for future work to tighten results.
- Why unresolved: The current proofs rely on injected noise to maintain smoothness properties; it is unknown if natural on-policy sampling suffices.
- What evidence would resolve it: A convergence proof achieving similar rates without the explicit exploratory perturbation parameter ε_ex.

### Open Question 3
- Question: Does the VGD condition accurately predict convergence to global optima in environments prone to local minima?
- Basis in paper: [inferred] Section 5 notes that "local optima only plays a role in the two more challenging environments" despite reasonable VGD parameters observed empirically.
- Why unresolved: The theory allows for an error floor ε_vgd, but the relationship between VGD, local optima, and global convergence remains empirically uncharacterized.
- What evidence would resolve it: Empirical mapping of the VGD parameter ν against local landscape topology in complex, high-dimensional tasks.

## Limitations

- VGD Parameter Estimation: While empirical evaluation shows ν estimates remain moderate (≈1) across environments, the robustness of these estimates to policy class expressiveness and exploration quality is untested.
- Actor-Oracle Error Propagation: DA-CPI's sample complexity bounds assume εerm errors are controlled, but the gap between theoretical εerm and practical optimization errors is not characterized.
- Non-Euclidean Norm Implementation: The local norm ∥·∥L2(μπ),1 is defined via occupancy measures, but practical implementations may approximate this geometry imperfectly.

## Confidence

- **High Confidence**: The reduction of agnostic RL to non-Euclidean first-order optimization is theoretically sound. The connection between VGD and completeness/coverage conditions is rigorously proven (Lemma 1), and the policy gradient theorem provides unbiased gradient estimates.
- **Medium Confidence**: The empirical validation of VGD parameters across Cartpole, Acrobot, and MinAtar environments is convincing but limited to a small set of tasks.
- **Low Confidence**: The sample complexity bounds for DA-CPI and the actor-oracle efficiency claims rely on strong assumptions about optimization oracle accuracy.

## Next Checks

1. **VGD Robustness Across Tasks**: Evaluate νk estimates on a broader suite of environments (e.g., DM Control, Procgen) with varying state-action spaces and dynamics to test the generalizability of VGD.
2. **Actor-Oracle Error Sensitivity**: Conduct ablation studies on DA-CPI by injecting controlled optimization errors (e.g., via gradient clipping, reduced optimizer steps) and measure the impact on convergence rates.
3. **Memory vs. Performance Tradeoff**: Implement CPI with exact convex combinations on small tabular policies to verify the O(1/K) rate, then compare against DA-CPI's O(1/K^(2/3)) rate with linear actor memory constraints.