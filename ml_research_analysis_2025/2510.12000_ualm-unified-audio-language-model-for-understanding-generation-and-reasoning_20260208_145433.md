---
ver: rpa2
title: 'UALM: Unified Audio Language Model for Understanding, Generation and Reasoning'
arxiv_id: '2510.12000'
source_url: https://arxiv.org/abs/2510.12000
tags:
- audio
- generation
- reasoning
- arxiv
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents UALM, a unified audio language model capable
  of understanding, generating, and reasoning across audio and text modalities. The
  authors develop UALM-Gen, a text-to-audio language model using auto-regressive prediction
  that achieves state-of-the-art quality comparable to diffusion-based models through
  techniques including large-scale data scaling (30M samples), classifier-free guidance,
  and direct preference optimization.
---

# UALM: Unified Audio Language Model for Understanding, Generation and Reasoning

## Quick Facts
- arXiv ID: 2510.12000
- Source URL: https://arxiv.org/abs/2510.12000
- Authors: Jinchuan Tian; Sang-gil Lee; Zhifeng Kong; Sreyan Ghosh; Arushi Goel; Chao-Han Huck Yang; Wenliang Dai; Zihan Liu; Hanrong Ye; Shinji Watanabe; Mohammad Shoeybi; Bryan Catanzaro; Rafael Valle; Wei Ping
- Reference count: 20
- Key outcome: UALM achieves state-of-the-art text-to-audio quality comparable to diffusion models through autoregressive prediction with 30M samples, classifier-free guidance, and direct preference optimization.

## Executive Summary
This paper presents UALM, a unified audio language model capable of understanding, generating, and reasoning across audio and text modalities. The authors develop UALM-Gen, a text-to-audio language model using auto-regressive prediction that achieves state-of-the-art quality comparable to diffusion-based models through techniques including large-scale data scaling (30M samples), classifier-free guidance, and direct preference optimization. They then extend this to create UALM, a single unified model that maintains competitive performance across audio understanding, text-to-audio generation, and text reasoning tasks. Finally, they introduce UALM-Reason, which enables multimodal reasoning beyond text through a two-stage post-training approach using rich captions as intermediate representations and self-reflection capabilities.

## Method Summary
The UALM architecture uses a Qwen2.5-7B backbone with frozen acoustic encoder, MLP adapter, and decoder-only transformer. Training proceeds through four stages: modality alignment (freeze backbone, train adapter/embeddings for 1.8k steps), full pre-training (660k steps with blended data: 2× up-weight generation), post-training (SFT then DPO on synthetic audio with self-reflection), and finally DPO-2 on human-preferred samples. The model uses X-codec with delay pattern for RVQ tokenization, classifier-free guidance with λ=3.0, and an enhancement VAE for upsampling 16kHz to 48kHz stereo output.

## Key Results
- UALM-Gen achieves state-of-the-art text-to-audio quality with CLAP scores of 0.54 on SongDescriber and 0.65 on AudioCaps
- Unified UALM maintains strong performance across audio understanding (MMAU/MMAR), text reasoning (MMLU/GSM8K), and generation tasks
- UALM-Reason demonstrates superior controllability and detail through self-reflection using rich captions as intermediate representations
- The model shows objective metrics comparable to diffusion models while maintaining autoregressive generation speed

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Autoregressive Language Models (LMs) can match diffusion-based audio quality if data is scaled by an order of magnitude and Classifier-Free Guidance (CFG) is applied.
- **Mechanism:** Diffusion models often have better inductive biases for data efficiency. To compensate, the autoregressive LM requires massive scaling (30M samples) to cover the audio distribution adequately. Furthermore, CFG interpolates between conditional and unconditional generation, sharpening the model's adherence to text prompts during sampling.
- **Core assumption:** The loss function (cross-entropy) of standard LMs is sufficient if the data volume saturates the distribution coverage required for high-fidelity audio.
- **Evidence anchors:**
  - [Section 2.2]: "LM-based audio generation needs significantly more data than diffusion-based methods... scaled our training data volume up to 30M samples."
  - [Section 2.2]: "CFG... is critical to high-quality generation in our model."
  - [Corpus]: "Language Model Based Text-to-Audio Generation" confirms LMs paired with RVQ tokenizers still lag behind diffusion models without specific interventions.
- **Break condition:** Performance plateaus or degrades if training data is <10M samples or if CFG weight λ is set to 1.0 (disabled).

### Mechanism 2
- **Claim:** Unifying understanding and generation requires a "modality alignment" warm-up phase and deliberate data re-balancing.
- **Mechanism:** Generation tasks converge slower than understanding tasks. Without intervention, the unified model optimizes for the faster-converging understanding tasks, ignoring generation. The alignment stage trains only the adapter/embeddings to project audio into the LLM's static semantic space before full unfreezing, preventing catastrophic forgetting of text capabilities.
- **Core assumption:** The pre-trained text LLM represents a fixed "semantic anchor" that multimodal inputs must adapt to, rather than adapting the LLM to the audio immediately.
- **Evidence anchors:**
  - [Section 2.3]: "Up-weight generation data due to its slower convergence."
  - [Section 2.3]: "A dedicated modality alignment stage is critical... freeze the Transformer body... updating only the MLP adapter."
  - [Corpus]: "Audio-FLAN" highlights that treating understanding and generation as distinct tasks hinders unified development, supporting the need for specific blending recipes.
- **Break condition:** Loss spikes or text reasoning collapse (e.g., MMLU score drops >10%) if full training begins without the alignment warm-up.

### Mechanism 3
- **Claim:** Cross-modal reasoning is achievable via "Rich Captions" acting as an intermediate textual blueprint for self-reflection.
- **Mechanism:** The model generates a structured text plan (Rich Caption) before generating audio. It then re-encodes the generated audio back into a caption to compare against the plan. This "generate-understand-critique" loop leverages the model's existing text reasoning strength to correct audio generation errors.
- **Core assumption:** The model's audio understanding capability is accurate enough to reliably transcribe its own generated artifacts back into text for comparison.
- **Evidence anchors:**
  - [Section 2.4]: "Rich captions... serve as an intermediate blueprint."
  - [Section 2.4]: "The model compares the two rich captions (the plan vs. the result)... to generate a second, improved audio clip."
  - [Corpus]: Corpus papers on explainability (AudioGenX) suggest evaluating semantic alignment is crucial; UALM uses this alignment internally as a control signal.
- **Break condition:** If the audio encoder hallucinates non-existent sounds during the reflection phase, the refinement loop diverges.

## Foundational Learning

- **Concept: Residual Vector Quantization (RVQ) & Delay Pattern**
  - **Why needed here:** Audio is continuous; LMs need discrete tokens. RVQ compresses audio into multiple token streams (codebooks). The delay pattern allows the LM to predict these streams in parallel with temporal offsets, reducing sequence length from T × 8 to roughly T.
  - **Quick check question:** Can you explain why predicting 8 codebooks per frame sequentially is inefficient compared to the delay pattern?

- **Concept: Classifier-Free Guidance (CFG) in LMs**
  - **Why needed here:** Standard LMs maximize likelihood, which can result in "average" or blurry audio. CFG forces the model to contrast the prompt against a null condition, amplifying the specific features requested in the prompt.
  - **Quick check question:** What happens to generation diversity if the CFG weight (λ) becomes excessively high?

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed here:** Cross-entropy loss trains the model to be statistically correct, but not necessarily aesthetically pleasing. DPO aligns the model with human preferences (e.g., higher CLAP/AES scores) by directly optimizing a reward function using preference pairs.
  - **Quick check question:** Why is it necessary to adapt the model with cross-entropy on synthetic data before starting DPO training?

## Architecture Onboarding

- **Component map:** Audio → Acoustic Encoder (frozen) → MLP Adapter → Decoder-only Transformer (Qwen2.5-7B) → Text Tokens OR RVQ Delay Pattern → Audio Decoder → Enhancement VAE

- **Critical path:**
  1. **Modality Alignment:** Freeze Backbone, Train Adapter/Embeddings (1.8k steps)
  2. **Pre-training:** Unfreeze all (except Encoder), Blend Data (Text + Audio Understanding + 2× Audio Generation)
  3. **Post-training:** SFT (Enrichment/Dialogue) → DPO → SFT (Self-Reflection)

- **Design tradeoffs:**
  - **Continuous Input vs. Discrete Output:** Input uses continuous embeddings (better for understanding), output uses discrete tokens (better for generation). This asymmetry complicates the architecture but optimizes per-task performance.
  - **Data Blending:** Heavily weighting generation data (2×) is required for convergence but risks degrading text performance, necessitating text reasoning data inclusion.

- **Failure signatures:**
  - "Robotic" or muffled audio: Likely insufficient data scaling or misconfigured RVQ/Delay pattern
  - Text capability collapse: Skipping modality alignment or using improper data blending ratios
  - DPO Instability: Spiking loss indicates the model is drifting too far from the reference; requires cross-entropy regularization

- **First 3 experiments:**
  1. **CFG Sweep:** Validate audio quality (CLAP/FD) with CFG λ ∈ {1.0, 2.0, 3.0} on a held-out set to confirm the "critical" nature of guidance
  2. **Data Volume Ablation:** Train a smaller model on 1M vs. 10M vs. 30M samples to replicate the scaling finding
  3. **Modality Alignment Stress Test:** Train a unified model from scratch (no alignment warm-up) vs. the prescribed alignment stage; measure MMLU score retention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a single unified audio representation replace the current dual approach of continuous inputs and discrete codec outputs to facilitate scalable training?
- Basis in paper: [explicit] The Conclusion identifies "Unifying audio representation" as a primary future direction.
- Why unresolved: The current UALM architecture uses distinct representations for input (continuous encoder) and output (discrete codec), and the authors hypothesize that unification would aid scalability but have not yet implemented it.
- What evidence would resolve it: A model architecture using a single representation for both understanding and generation that maintains competitive performance metrics.

### Open Question 2
- Question: How can the quality of synthetic "rich captions" be quantitatively assessed at scale to reduce hallucinations in the training data?
- Basis in paper: [explicit] The Conclusion highlights "Quality Assessment of Synthetic Audio Captions" as a necessary step to build a robust data curation pipeline.
- Why unresolved: Manual inspections reveal hallucinations and misalignment in synthetic captions, but no automated quantitative method currently exists to detect these errors at scale.
- What evidence would resolve it: The development of an automated metric or pipeline that accurately identifies caption hallucinations and correlates with human judgment.

### Open Question 3
- Question: What evaluation metrics can better capture human perception of acoustic quality, diversity, and faithfulness in complex generative tasks?
- Basis in paper: [explicit] The Conclusion notes a "gap from human perception" in existing metrics (e.g., FD, CLAP) which limits the reinforcement learning of reasoning.
- Why unresolved: Current objective metrics fail to accurately measure the nuances of complex, reasoning-based audio generation.
- What evidence would resolve it: New benchmarks or metrics that demonstrate a stronger correlation with human subjective evaluation for complex generation scenarios.

## Limitations

- The exact dataset sources and pseudo-label generation pipeline for the 30M samples are not fully disclosed
- The acoustic encoder architecture and enhancement VAE training procedures are referenced but not fully detailed
- The self-reflection mechanism's reliability depends on the audio encoder's caption accuracy, which may degrade with complex or ambiguous audio

## Confidence

- **High Confidence:** The unified model architecture (modality alignment stage, data blending ratios) and its core training methodology are well-specified and reproducible
- **Medium Confidence:** The assertion that autoregressive models can match diffusion quality through scaling is empirically supported but requires the full 30M sample regime
- **Low Confidence:** The exact dataset sources and pseudo-label generation pipeline for the 30M samples are not fully disclosed

## Next Checks

1. **CFG Sensitivity Analysis:** Systematically vary CFG λ from 1.0 to 5.0 in 0.5 increments on a held-out set to quantify the "critical" threshold and measure CLAP/FD degradation without guidance
2. **Data Scaling Ablation:** Train identical models on 1M, 5M, 10M, and 30M samples to precisely quantify the scaling benefit and identify the inflection point where autoregressive quality plateaus
3. **Modality Alignment Necessity Test:** Train a baseline model from scratch (no alignment warm-up) versus the prescribed alignment stage, measuring MMLU score retention and audio generation quality (CLAP) to validate the catastrophic forgetting claim