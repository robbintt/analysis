---
ver: rpa2
title: Can LLM Agents Maintain a Persona in Discourse?
arxiv_id: '2502.11843'
source_url: https://arxiv.org/abs/2502.11843
tags:
- traits
- personality
- high
- agents
- discourse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how consistently LLM agents maintain assigned
  personality traits during dyadic conversations. Two agents, each with distinct Big
  Five personality profiles, engage in debates on 100 topics.
---

# Can LLM Agents Maintain a Persona in Discourse?

## Quick Facts
- arXiv ID: 2502.11843
- Source URL: https://arxiv.org/abs/2502.11843
- Authors: Pranav Bhandari; Nicolas Fay; Michael Wise; Amitava Datta; Stephanie Meek; Usman Naseem; Mehwish Nasim
- Reference count: 17
- Primary result: LLM agents can reflect assigned Big Five personality traits in dyadic discourse, but consistency varies significantly by trait type and model combination.

## Executive Summary
This study examines how consistently LLM agents maintain assigned personality traits during dyadic conversations. Two agents, each with distinct Big Five personality profiles, engage in debates on 100 topics. Judge agents then evaluate the generated discourse to measure prediction accuracy and inter-model agreement. Results show that while LLMs can reflect personality traits, their consistency varies significantly by trait type and model combination. High trait classification accuracy ranges from 81% to 99% depending on the trait and model, with Agreeableness and Openness showing higher consistency than Neuroticism and Extraversion. Inter-model agreement (Fleiss' Kappa) remains moderate to fair across traits, highlighting challenges in achieving stable, interpretable personality-driven interactions.

## Method Summary
The study uses two LLM agents with assigned Big Five personality traits (High/Low per dimension) to engage in dyadic debates on 100 topics. System prompts inject OCEAN traits as explicit constraints with behavioral rules. Five independent judge models (GPT-4o, GPT-4o-mini, LLaMA-3.3-70B, Qwen-2.5-14B, DeepSeek) analyze discourse and predict traits. High Trait Classification Accuracy (HTA) and Low Trait Classification Accuracy (LTA) measure prediction success, while Fleiss' Kappa quantifies inter-model agreement. LIWC-22 linguistic analysis validates trait expression through psychological word categories.

## Key Results
- HTA ranges from 81% to 99% depending on trait and model combination
- Inter-model agreement (Fleiss' Kappa) remains moderate to fair across traits
- Agreeableness and Openness show highest consistency; Neuroticism and Extraversion lowest
- Judge models follow instructions inconsistently, with DeepSeek showing >40% invalid responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit trait assignment via structured system prompts enables personality-conditioned discourse generation
- Mechanism: System prompts inject OCEAN traits as High/Low constraints with behavioral rules (under 50 words, maintain traits implicitly, address previous utterances). The model conditions its token distribution on these constraints throughout generation
- Core assumption: Models can map abstract trait labels to consistent linguistic patterns without explicit trait mentions in output
- Evidence anchors: [abstract], [section 3.1], [corpus]
- Break condition: If models default to neutral/balanced responses despite explicit prompting

### Mechanism 2
- Claim: Multi-judge evaluation with Fleiss' Kappa quantifies inter-model agreement on trait inference
- Mechanism: Five independent judge models analyze discourse and predict traits. Agreement measured via Fleiss' Kappa across topic-trait pairs
- Core assumption: High inter-model agreement indicates objective trait presence in text; disagreement indicates ambiguous or absent trait expression
- Evidence anchors: [abstract], [section 4.2], [corpus]
- Break condition: If all judges share systematic biases, Kappa inflates without validity

### Mechanism 3
- Claim: LIWC-22 linguistic markers validate trait expression independent of LLM judges
- Mechanism: LIWC-22 categorizes words into psychological categories. Traits mapped to linguistic features provide human-interpretable alignment scores
- Core assumption: Linguistic markers that predict human personality also validly measure LLM personality expression
- Evidence anchors: [section 4.3], [section 5.3], [corpus]
- Break condition: If LLMs use different linguistic patterns than humans to express traits, LIWC mappings become invalid

## Foundational Learning

- Concept: **Big Five Inventory (OCEAN) framework**
  - Why needed here: All trait assignment and evaluation relies on this taxonomy
  - Quick check question: Would "using many positive emotion words" indicate High Extraversion or High Agreeableness?

- Concept: **Fleiss' Kappa for inter-rater reliability**
  - Why needed here: Distinguishes agreement from chance; κ < 0.2 = poor, 0.2-0.4 = fair, 0.4-0.6 = moderate
  - Quick check question: If 5 judges unanimously rate all traits as High, what would Fleiss' Kappa equal?

- Concept: **Dyadic conversation dynamics**
  - Why needed here: Multi-turn exchanges create context-shifting pressure
  - Quick check question: Why might trait consistency degrade faster in dyadic debate than monologic essay generation?

## Architecture Onboarding

- Component map: System Prompt: OCEAN traits + rules → Agent A ←→ Agent B → Discourse Log → Judge Agents × 5 → Trait Predictions JSON → Fleiss' Kappa + HTA/LTA → LIWC-22 Analysis → Linguistic Alignment

- Critical path: Prompt design → discourse quality → judge reliability → metric interpretability

- Design tradeoffs:
  - High temperature (>0.8) enables creative trait expression but increases inconsistency
  - Direct trait assignment (vs. implicit) improves controllability but may reduce naturalism
  - Max_tokens=150 prevents verbosity but constrains argument depth

- Failure signatures:
  - DeepSeek: 40%+ invalid judge responses; inconsistent instruction following
  - Low Neuroticism detection: HTA near 0% for some judge configurations
  - Low Extraversion: Fleiss' Kappa consistently lowest (~0.1-0.3)

- First 3 experiments:
  1. Baseline replication: Run GPT-4o vs. GPT-4o-mini on 10 topics; compute HTA/LTA and Kappa
  2. Judge ablation: Compare single-judge vs. 5-judge consensus accuracy
  3. Conversation length sweep: Test 5, 10, 15 turn dialogues to quantify trait consistency degradation

## Open Questions the Paper Calls Out

- **Open Question 1**: Do fine-tuning or reinforcement learning strategies improve personality consistency better than prompting alone?
  - Basis: Conclusion states exploring these approaches would be valuable
  - Why unresolved: Study focused exclusively on system prompt induction
  - What evidence would resolve it: Comparative experiment between prompted and fine-tuned models

- **Open Question 2**: How does conversation length affect personality trait evaluation reliability?
  - Basis: Limitations note no standard for dialogue length to ensure reliable evaluation
  - Why unresolved: Fixed dialogue parameters without analyzing duration impact
  - What evidence would resolve it: Longitudinal analysis measuring trait drift across varying turn lengths

- **Open Question 3**: Can specific metrics capture personality expression nuances beyond binary classification?
  - Basis: Conclusion calls for developing metrics for assessing personality expression nuances
  - Why unresolved: Current evaluation relies on binary classification or broad LIWC categories
  - What evidence would resolve it: Creation and validation of continuous-scale evaluation metric

## Limitations

- Topic and trait coverage relies on samples rather than complete dataset, limiting reproducibility
- Judge model reliability may reflect systematic biases rather than true trait presence
- LIWC-22 linguistic alignment assumes LLM patterns match human personality expression

## Confidence

- High Confidence: Methodological framework is sound and well-documented
- Medium Confidence: Trait-dependent consistency variations are likely valid
- Low Confidence: Absolute trait classification accuracy claims require reproduction

## Next Checks

1. Judge Model Ablation: Compare single-judge vs. ensemble predictions to identify systematic biases
2. Cross-Corpus Linguistic Validation: Apply LIWC-22 to human-authored personality-consistent texts
3. Topic and Trait Sensitivity Analysis: Systematically vary debate topics to measure performance fluctuations