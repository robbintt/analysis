---
ver: rpa2
title: 'Towards Ancient Plant Seed Classification: A Benchmark Dataset and Baseline
  Model'
arxiv_id: '2512.18247'
source_url: https://arxiv.org/abs/2512.18247
tags:
- information
- classification
- size
- ancient
- seed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first Ancient Plant Seed Image Classification
  (APS) dataset, containing 8,340 images across 17 seed categories from 18 archaeological
  sites in China. The dataset spans 5400 BCE to 220 CE and addresses challenges of
  morphological similarity, intra-class variation, and long-tail distribution.
---

# Towards Ancient Plant Seed Classification: A Benchmark Dataset and Baseline Model

## Quick Facts
- arXiv ID: 2512.18247
- Source URL: https://arxiv.org/abs/2512.18247
- Reference count: 40
- First Ancient Plant Seed Image Classification dataset with 8,340 images across 17 seed categories

## Executive Summary
This paper introduces the APS dataset, the first benchmark for ancient plant seed classification, containing 8,340 microscope images from 18 archaeological sites spanning 5400 BCE to 220 CE. The dataset addresses critical challenges including morphological similarity, intra-class variation, and long-tail distribution. To tackle these issues, the authors propose APSNet, a deep learning framework that explicitly incorporates seed size information through a Size Perception and Embedding (SPE) module and an Asynchronous Decoupled Decoding (ADD) structure for fine-grained classification. APSNet achieves 90.2% accuracy, significantly outperforming 28 state-of-the-art baselines and demonstrating strong robustness in both quantitative and qualitative analyses.

## Method Summary
The authors propose APSNet, a deep learning framework specifically designed for ancient plant seed classification. The method combines a Size Perception and Embedding (SPE) module that extracts high-frequency components via FFT to capture seed size information, with an Asynchronous Decoupled Decoding (ADD) structure that processes features through multiple heads with different loss functions. The framework uses ResNet50 as backbone and employs a four-stage training procedure with progressive loss complexity, including cross-entropy and supervised contrastive losses. The model explicitly addresses the challenges of morphological similarity and fine-grained discrimination through this dual-perspective decoding approach.

## Key Results
- APSNet achieves 90.2% accuracy on the APS dataset, outperforming 28 state-of-the-art baselines
- The SPE module with high-frequency components provides effective size prior information for classification
- ADD structure with spatial contrastive supervision improves discriminative feature learning compared to traditional methods
- Strong robustness demonstrated through both quantitative results and qualitative feature visualization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-frequency components from FFT provide more effective size prior information than edge, mask, or raw image features for guiding seed classification.
- Mechanism: The SPE module applies Fast Fourier Transform to extract high-frequency components (entropy ~1.4 bit vs. edge ~0.1 bit, mask ~0.3 bit), which captures intermediate information between edges and masks—preserving both boundaries and details without overly sparse or overly masking representations. This prior is embedded into backbone features via channel concatenation followed by channel attention.
- Core assumption: High-frequency information effectively characterizes seed size and transfers stably through network layers.
- Evidence anchors: [abstract] "We design a Size Perception and Embedding (SPE) module in the encoder part to explicitly extract size information for the purpose of complementing fine-grained information." [section 4.1] "Among them, the original image is about 0.9 bit, the High-frequency is about 1.4 bit... The reason that the information entropy of high-frequency is the highest is that it magnifies the drastic changes of seed features."

### Mechanism 2
- Claim: Decoupling spatial information (e.g., size) from channel-based classification via an asynchronous dual-branch decoding improves discriminative feature learning.
- Mechanism: The ADD structure introduces Head Dc at the third training stage, which processes features through both a traditional channel branch (Max pooling + MLP for classification) and a spatial branch (channel projection + MLP to spatial representation + Supervised Contrastive Loss). The contrastive loss enforces intra-class compactness and inter-class separation in spatial representations.
- Core assumption: Spatial information like size provides complementary supervision that cross-entropy loss alone cannot capture; decoupling after sufficient high-level semantic learning (stage 3) is optimal.
- Evidence anchors: [abstract] "We propose an Asynchronous Decoupled Decoding (ADD) architecture based on traditional progressive learning to decode features from both channel and spatial perspectives." [section 4.2] "Therefore, the spatial branch preserves spatial information as an additional loss term by shrinking the channel information... We apply a Supervised Contrastive loss to the generated spatial representation."

### Mechanism 3
- Claim: Combining explicit size perception with dual-domain decoding achieves better generalization than either component alone or standard fine-grained methods.
- Mechanism: SPE provides size-aware initial features; ADD preserves size information through spatial contrastive supervision while learning fine-grained texture details. The multi-head ensemble (Head 1, Head 2, Head Dc, concatenated Head Con) balances predictions across scales.
- Core assumption: Ancient seed classification requires both inter-class size discrimination and intra-class fine-grained texture discrimination; neither is sufficient alone.
- Evidence anchors: [abstract] "APSNet achieves 90.2% accuracy, significantly outperforming 28 state-of-the-art baselines" [section 5.3.1/Table 3] Shows APSNet (90.2% accuracy) outperforms best CNN baseline GhostNetv2 (84.9%) and best Transformer fine-grained method FET-FGVC (76.9%).

## Foundational Learning

- Concept: **Fast Fourier Transform for image analysis**
  - Why needed here: SPE uses FFT to extract high-frequency components as size priors; understanding frequency domain decomposition and filtering is essential.
  - Quick check question: Can you explain what happens when you zero out low-frequency components in the frequency domain and transform back to spatial domain?

- Concept: **Supervised Contrastive Learning**
  - Why needed here: Head Dc uses supervised contrastive loss to enforce spatial representation clustering; understanding positive/negative pairs and margin κ is required.
  - Quick check question: Given a batch with 16 samples across 4 classes, how many positive and negative pairs exist for supervised contrastive loss?

- Concept: **Progressive/Multi-head Training for Fine-Grained Classification**
  - Why needed here: ADD uses four-stage asynchronous training with multiple classification heads; understanding staged loss composition (L1, L2, LComposite, LConcat) is necessary.
  - Quick check question: Why might training multiple heads with different loss functions at different stages improve fine-grained feature learning compared to single-head training?

## Architecture Onboarding

- Component map: Input image → FFT-based SPE module → ResNet50 backbone → ADD decoder with 4 heads (Head 1, Head 2, Head Dc, Head Con) → Output predictions
- Critical path:
  1. Image preprocessing (bilinear downsample to 512×512)
  2. SPE extracts high-frequency prior from input image
  3. Prior embedded into backbone at layers 2, 3, 4, 5 via channel concatenation + attention
  4. Multi-scale features decoded through four-stage ADD with escalating supervision complexity
  5. Final prediction from concatenated head ensemble
- Design tradeoffs:
  - ResNet50 backbone vs. GhostNet: GhostNet has higher accuracy but poor GPU utilization; ResNet50 chosen for practical archaeological GPU setups
  - High-frequency prior vs. Mask: Mask has better Recall/F1 in ablation but HF chosen for information richness; consider ensemble if inference budget allows
  - Spatial branch dimension (1000-d): Controls complexity vs. representation capacity; may need tuning for fewer classes
- Failure signatures:
  - Accuracy drops significantly on long-tail classes (e.g., Prunus persica: 3 test samples, Zizyphus jujube: excluded): dataset imbalance not addressed
  - Precision near 0% on small-sample classes for many baselines (Table B2): indicates overfitting to majority classes
  - Feature visualization shows scattered clusters (UMAP): SPE/ADD not effectively separating classes
- First 3 experiments:
  1. **SPE prior ablation**: Compare HF, Mask, Edge, Sobel priors on validation set; measure Accuracy and Recall to confirm HF superiority and identify edge cases where Mask wins
  2. **Head Dc spatial branch ablation**: Train with Head Dc (CE only) vs. Head Dc (CE + Contrastive) to isolate contrastive loss contribution on spatial clustering (visualize with UMAP)
  3. **Backbone swap test**: Replace ResNet50 with DenseNet201 or EfficientNetV2-small + SPE + Head Dc (following Table 5 setup) to verify module transferability and identify architecture-specific gains

## Open Questions the Paper Calls Out
- How can the severe long-tail distribution of ancient seed categories be effectively mitigated to improve recognition accuracy for minority classes?
- Does the reliance on high-frequency edge information for size estimation compromise robustness when classifying fragmented or physically damaged seeds?
- Can the explicit modeling of size information be effectively adapted for Transformer-based architectures to improve their performance on the APS dataset?

## Limitations
- Dataset availability is restricted to author requests, preventing independent verification of reported results
- The long-tail class distribution (e.g., Prunus persica with only 3 test samples) creates significant uncertainty about true generalization capability
- Several key implementation details are underspecified, including the exact SPE module architecture, contrastive loss margin κ, and optimizer configuration
- No comparison with established fine-grained classification benchmarks to contextualize APSNet's performance claims

## Confidence
- **High confidence**: APSNet achieves superior accuracy (90.2%) compared to traditional CNN and Transformer baselines on the APS dataset
- **Medium confidence**: The combination of SPE module and ADD structure provides meaningful improvement over individual components
- **Low confidence**: The FFT-based high-frequency approach is optimal for size perception without testing alternative frequency decomposition methods

## Next Checks
1. Request and evaluate the APS dataset with stratified sampling to ensure minority classes are adequately represented in validation
2. Implement a controlled ablation study comparing SPE priors (HF, Mask, Edge) with statistical significance testing on a held-out validation set
3. Conduct transferability experiments replacing ResNet50 with alternative architectures (DenseNet, EfficientNet) to verify SPE and ADD module robustness