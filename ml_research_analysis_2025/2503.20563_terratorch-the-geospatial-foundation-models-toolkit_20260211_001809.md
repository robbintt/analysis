---
ver: rpa2
title: 'TerraTorch: The Geospatial Foundation Models Toolkit'
arxiv_id: '2503.20563'
source_url: https://arxiv.org/abs/2503.20563
tags:
- terratorch
- data
- learning
- tasks
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TerraTorch is a modular toolkit for fine-tuning and benchmarking
  Geospatial Foundation Models (GeoFMs) on satellite, weather, and climate data. It
  integrates domain-specific data modules, pre-defined tasks, and a model factory
  that pairs any backbone with diverse decoder heads, enabling no-code fine-tuning
  through configuration files.
---

# TerraTorch: The Geospatial Foundation Models Toolkit

## Quick Facts
- arXiv ID: 2503.20563
- Source URL: https://arxiv.org/abs/2503.20563
- Reference count: 35
- Primary result: Modular toolkit enabling no-code fine-tuning of GeoFMs with 82-88% IoU on segmentation tasks

## Executive Summary
TerraTorch is a PyTorch Lightning-based toolkit that simplifies fine-tuning and benchmarking of Geospatial Foundation Models (GeoFMs) on satellite, weather, and climate data. It provides a modular architecture that pairs any backbone with diverse decoder heads through a model factory pattern, enabling configuration-driven experimentation without code changes. The toolkit integrates domain-specific data modules, pre-defined tasks, and automated hyperparameter optimization via the Iterate extension, supporting segmentation, regression, classification, object detection, and downscaling tasks. Experiments on Sen1Floods11 and BurnScars2 datasets demonstrate IoU scores of 82-88% for water and burned classes, with Bayesian optimization improving performance by up to 2 percentage points.

## Method Summary
TerraTorch implements a modular GeoFM fine-tuning framework using PyTorch Lightning abstractions. The core architecture consists of a model factory that instantiates backbones from a registry and pairs them with task-specific decoders through configurable necks that handle dimension transformations. The toolkit supports five task types: semantic segmentation, pixel-wise regression, classification, object detection, and downscaling. Data is organized through Generic and TorchGeo datasets with automatic temporal dimension handling. The Iterate extension provides Bayesian hyperparameter optimization using Optuna, with MLFlow tracking and Ray Tune parallelization. Experiments use AdamW optimizer with ReduceLROnPlateau scheduler, 100 epochs with early stopping (patience=20), and batch size 16 on 512x512 inputs. The framework automatically adapts patch embeddings for cross-sensor transfer by copying matching channel weights and randomly initializing unseen ones.

## Key Results
- IoU scores of 82-88% achieved on Sen1Floods11 and BurnScars2 segmentation tasks
- Hyperparameter optimization improves performance by up to 2 percentage points
- Pre-training data alignment with downstream sensors correlates with higher fine-tuning performance
- No-code fine-tuning possible through YAML configuration files
- Supports five task types with modular backbone-decoder pairing

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Modular backbone-decoder pairing enables task-adaptive fine-tuning without code changes
- **Mechanism**: The `EncoderDecoderFactory` instantiates backbones from a registry and connects them to task-specific decoders via configurable necks that handle dimension transformations (e.g., 1D ViT patches → 2D feature maps for UPerNet)
- **Core assumption**: EO tasks require different architectural combinations, and no single backbone-decoder pair is universally optimal
- **Evidence anchors**:
  - [abstract]: "model factory that pairs any backbone with diverse decoder heads"
  - [section IV-B]: "neck facilitates the conversion of layer-wise backbone outputs into the required decoder inputs. For instance, users can choose layer indices for hierarchical decoders such as UPerNet"
  - [corpus]: Weak direct validation; PyGALAX supports modular toolkit design but does not test backbone-decoder interchangeability
- **Break condition**: When backbone output dimensions lack compatible neck/decoder configurations, or when spectral band counts differ significantly between pre-training and downstream data without automated patch embedding adjustment

### Mechanism 2
- **Claim**: Bayesian hyperparameter optimization systematically improves fine-tuning performance over manual defaults
- **Mechanism**: Iterate uses Optuna's Bayesian optimization to explore learning rate and weight decay space, with MLFlow tracking and optional Ray Tune parallelization across trials
- **Core assumption**: Hyperparameter sensitivity exists and optimal values vary across model-dataset combinations
- **Evidence anchors**:
  - [abstract]: "hyperparameter optimization improving performance by up to 2 percentage points"
  - [section V]: "learning rates around 1e-4 tend to yield stable training and higher scores, while either very low or high rates often lead to suboptimal performance... small changes using HPO easily improve the model performance by 2pp"
  - [corpus]: No direct corpus evidence validating TerraTorch's specific HPO mechanism
- **Break condition**: When trial budget is insufficient for Bayesian convergence, or when validation loss is a poor proxy for test performance

### Mechanism 3
- **Claim**: Pre-training data alignment with downstream sensors correlates with higher fine-tuning performance
- **Mechanism**: Models pre-trained on matching sensor data (e.g., Sentinel-2, HLS) encode spectral representations that transfer more effectively than mismatched pre-training
- **Core assumption**: Spectral and spatial characteristics of pre-training data influence representation quality for specific downstream tasks
- **Evidence anchors**:
  - [section V]: "Prithvi-EO-2.0-300M-TL performs best on BurnScars, which relies on HLS data and thus matches the Prithvi pre-training data. DeCUR and Clay v1, pre-trained on Sentinel-2, show strong results on Sen1Floods11"
  - [corpus]: Harvesting AlphaEarth paper supports pre-training data importance for agricultural downstream tasks; Urban Heat Islands paper demonstrates fine-tuned GFM effectiveness but doesn't isolate pre-training alignment
- **Break condition**: When domain shift (resolution, geography, temporal patterns) overwhelms sensor-level alignment benefits

## Foundational Learning

- **Concept: PyTorch Lightning abstractions (Trainer, LightningModule, DataModule)**
  - **Why needed here**: TerraTorch inherits from Lightning; understanding the training loop abstraction and automatic GPU scaling is prerequisite to customizing tasks
  - **Quick check question**: Can you explain how `LightningModule.training_step()` separates training logic from optimizer configuration?

- **Concept: Encoder-decoder architectures with multi-scale features**
  - **Why needed here**: ViT backbones require necks to transform patch outputs; UPerNet decoders expect hierarchical feature maps at multiple resolutions
  - **Quick check question**: Why does a ViT backbone with patch size 16 require a neck before UPerNet decoder input?

- **Concept: Multi-band geospatial data organization (spectral bands, temporal stacks)**
  - **Why needed here**: TerraTorch automatically unstacks temporal dimensions from single TIF files; understanding `dataset_bands` configuration is essential
  - **Quick check question**: How would you structure a dataset where each sample contains 6 HLS bands across 3 timesteps in a single TIF?

## Architecture Onboarding

- **Component map:**
  ```
  YAML Config
      ↓
  Task (training/eval logic) ←→ Model Factory → Backbone + Neck + Decoder + Head
      ↓                                    ↑
  Data Module → Generic/TorchGeo Datasets  Registry (Prithvi, Clay, timm, SMP)
      ↓
  Lightning Trainer (GPU, checkpointing, logging)
      ↓
  Iterate Extension (optional) → Optuna + MLFlow + Ray Tune
  ```

- **Critical path:**
  1. Install: `pip install terratorch`
  2. Prepare dataset in ImageFolder (classification) or image/label structure (segmentation/regression)
  3. Create YAML config specifying: backbone, decoder, dataset paths, bands, hyperparameters
  4. Run: `terratorch fit --config my_config.yaml` or use Python API
  5. (Optional) Enable Iterate for HPO by adding `optimization_space` block

- **Design tradeoffs:**
  - No-code YAML vs. Python API: YAML limits customization; Python required for novel architectures
  - Registry backbones vs. custom: Registry models tested; custom modules require manual validation
  - HPO compute vs. gains: 10 trials at 4-46 hours each (A100) for ~2pp improvement

- **Failure signatures:**
  - Channel mismatch: Backbone expects 6 bands, dataset provides 13 → check `dataset_bands` and `output_bands` config
  - Memory overflow: 512px batch=16 exceeds GPU → reduce batch size or input size
  - Poor convergence: Learning rate outside 1e-4 to 1e-5 range → run Iterate or manually tune

- **First 3 experiments:**
  1. Baseline: Fine-tune Prithvi-EO-2.0 on your HLS dataset with learning rate 1e-4, UPerNet decoder, batch size 16
  2. HPO: Run Iterate with 10 trials optimizing learning rate (log scale 1e-5 to 1e-3) and weight decay
  3. Ablation: Compare backbone (Clay v1 vs. Prithvi) to validate pre-training alignment effect on your specific sensor data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TerraTorch's performance compare across the full range of supported task types beyond semantic segmentation?
- Basis in paper: [explicit] The paper states TerraTorch "currently supports semantic segmentation, pixel-wise regression, classification, object detection, and downscaling," yet experiments only demonstrate segmentation results on Sen1Floods11 and BurnScars datasets.
- Why unresolved: No empirical validation is provided for regression, classification, object detection, or downscaling tasks, leaving uncertainty about whether the modular architecture translates effectively across these diverse task types.
- What evidence would resolve it: Benchmark results on GEO-Bench or other datasets spanning all five task types, showing IoU/accuracy metrics and comparing GeoFM performance against traditional baselines for each task category.

### Open Question 2
- Question: What is the computational and accuracy trade-off when using TerraTorch's automated patch embedding adaptation for cross-sensor transfer?
- Basis in paper: [explicit] The paper mentions "TerraTorch automatically updates the patch embeddings for S2 data by copying the pre-trained weights for matching channels while randomly initializing unseen ones" when adapting HLS-pretrained models to Sentinel-2 data.
- Why unresolved: The paper does not quantify how much performance degrades from this random initialization of non-matching channels, or whether fine-tuning the entire patch embedding layer would yield better results than the current strategy.
- What evidence would resolve it: Ablation studies comparing (1) random initialization of mismatched channels, (2) interpolation/projection of pre-trained weights, and (3) full fine-tuning of patch embeddings, measuring convergence speed and final accuracy on cross-sensor tasks.

### Open Question 3
- Question: Does expanding the hyperparameter optimization search space beyond learning rate and weight decay yield meaningful performance improvements?
- Basis in paper: [explicit] The paper notes "small changes using HPO easily improve the model performance by 2pp" and observes that "weight decay plays a less critical role as no trend is visible," suggesting current HPO may be underexplored.
- Why unresolved: The Iterate extension only tuned two hyperparameters over 10 trials per model, leaving unexplored whether optimizer choice, batch size, data augmentation strategies, decoder architecture selection, or neck configurations could yield larger gains.
- What evidence would resolve it: Systematic HPO experiments expanding the search space to include optimizer type (AdamW vs. Adam vs. SGD), augmentation intensity, decoder depth, and learning rate schedulers, with statistical significance testing across multiple random seeds.

## Limitations

- Incomplete implementation details for YAML configuration syntax and backbone-decoder pairing specifications
- Lack of baseline performance comparisons without hyperparameter optimization to quantify HPO benefits
- Insufficient experimental validation of mechanisms beyond semantic segmentation, particularly cross-sensor transfer learning

## Confidence

- **High Confidence**: The modular architecture design (backbone-decoder pairing) is well-justified and technically sound. The evidence from the registry-based factory pattern and configuration-driven instantiation is compelling.
- **Medium Confidence**: The reported IoU scores (82-88%) and HPO benefits are supported by experimental results, but the lack of baseline comparisons and precise hyperparameter values reduces reproducibility confidence.
- **Low Confidence**: The mechanism by which spectral band alignment drives transfer learning is asserted but not experimentally isolated. The paper shows correlation between pre-training data and downstream performance but does not conduct ablation studies to prove causation.

## Next Checks

1. **Reproduce the 2pp HPO gain**: Run identical experiments on Sen1Floods11 with and without Iterate optimization, comparing mIoU improvements across 10 trials to verify the claimed performance boost.
2. **Channel adaptation validation**: Test the automatic patch embedding mechanism by loading 6-channel pre-trained weights on 13-channel input data, verifying that missing channel handling produces stable training rather than convergence failure.
3. **Pre-training alignment ablation**: Compare models pre-trained on matching vs. mismatched sensor data (e.g., Clay v1 on Sentinel-2 vs. HLS) while controlling for other variables to isolate the spectral alignment effect on downstream performance.