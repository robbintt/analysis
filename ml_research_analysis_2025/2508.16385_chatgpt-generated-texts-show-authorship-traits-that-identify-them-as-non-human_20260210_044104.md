---
ver: rpa2
title: ChatGPT-generated texts show authorship traits that identify them as non-human
arxiv_id: '2508.16385'
source_url: https://arxiv.org/abs/2508.16385
tags:
- texts
- human
- chatgpt
- variation
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined whether ChatGPT exhibits distinctive authorship
  traits and the extent to which it can adapt to different writing registers. Using
  stylometric analysis (FW-PCA) and multidimensional register analysis (MDA), researchers
  compared human-authored texts (college essays and Wikipedia entries) with ChatGPT-generated
  texts on identical topics.
---

# ChatGPT-generated texts show authorship traits that identify them as non-human

## Quick Facts
- **arXiv ID**: 2508.16385
- **Source URL**: https://arxiv.org/abs/2508.16385
- **Reference count**: 16
- **Primary result**: ChatGPT exhibits distinctive authorship traits that can identify it as non-human

## Executive Summary
This study investigates whether ChatGPT displays unique authorship traits and its ability to adapt to different writing registers. Researchers used stylometric analysis (Forward Weighted Principal Component Analysis) and multidimensional register analysis to compare ChatGPT-generated texts with human-authored college essays and Wikipedia entries on identical topics. The analysis revealed that ChatGPT texts exhibit a distinctive "idiolect" characterized by dense noun usage, nominalizations, and attributive adjectives, while showing limited use of function words (except "and"), verbs, adverbs, and pronouns.

ChatGPT failed to replicate the register variation seen in human writing, producing outputs that were more similar to each other than to their human counterparts. These findings suggest that ChatGPT's writing style is less anchored in grammatical complexity (tense, aspect, mood) and more context-insensitive than human language, potentially serving as a linguistic fingerprint for AI-generated content.

## Method Summary
Researchers conducted a comparative analysis of 30 human-authored texts (15 college essays and 15 Wikipedia entries) against 30 ChatGPT-generated texts on identical topics (Othello and capital punishment). The study employed two analytical approaches: Forward Weighted Principal Component Analysis (FW-PCA) for stylometric analysis and Multidimensional Register Analysis (MDA) to examine linguistic variation across different writing registers. The texts were analyzed for lexical features including nouns, verbs, adjectives, adverbs, function words, pronouns, and nominalizations to identify distinctive patterns between human and AI-generated content.

## Key Results
- ChatGPT texts display a unique "idiolect" with dense noun usage, nominalizations, and attributive adjectives
- ChatGPT shows limited use of function words (except "and"), verbs, adverbs, and pronouns compared to human writing
- ChatGPT fails to replicate register variation, producing more homogeneous outputs that are more similar to each other than to human counterparts

## Why This Works (Mechanism)
ChatGPT's distinctive linguistic patterns emerge from its training process and architecture. The model's tendency toward dense noun usage and nominalizations likely reflects patterns in its training data, which contains many formal and academic texts. Its limited use of function words, verbs, and pronouns suggests that the model prioritizes content words over grammatical structure, resulting in a writing style that lacks the nuanced register variation characteristic of human authors.

## Foundational Learning

**Stylometric Analysis**: Quantitative examination of writing style using statistical methods. Needed to identify linguistic patterns that distinguish AI-generated from human writing. Quick check: Can reveal authorship traits through frequency analysis of specific word classes.

**Multidimensional Register Analysis**: Framework for examining how language varies across different contexts and purposes. Needed to assess ChatGPT's ability to adapt to different writing styles. Quick check: Reveals whether AI can replicate human register variation.

**Forward Weighted PCA**: Statistical technique that emphasizes recent or more relevant data points. Needed to identify the most distinguishing features in authorship analysis. Quick check: Helps isolate the most diagnostic linguistic markers.

## Architecture Onboarding

**Component Map**: Input Text -> FW-PCA Analysis -> MDA Analysis -> Linguistic Feature Extraction -> Pattern Recognition -> Authorship Attribution

**Critical Path**: Text collection and preparation -> Feature extraction -> Statistical analysis (FW-PCA and MDA) -> Pattern identification -> Validation

**Design Tradeoffs**: The study prioritized breadth of linguistic features over depth of analysis of any single feature. This approach captures overall patterns but may miss nuanced variations in specific linguistic constructions.

**Failure Signatures**: The model's writing becomes detectable through: overuse of nouns and nominalizations, underuse of function words and pronouns, failure to vary register appropriately, and lack of grammatical complexity markers.

**3 First Experiments**:
1. Test the model's response to prompts requiring specific register shifts
2. Analyze how the model handles different sentence structures and grammatical complexity
3. Examine the model's use of discourse markers and cohesive devices

## Open Questions the Paper Calls Out
Major uncertainties remain about the generalizability of these findings to other large language models, as the study exclusively examined ChatGPT without comparing it to other generative AI systems. The sample size of 60 texts (30 human, 30 AI) and the restriction to specific topics (Othello, capital punishment) may limit the robustness of the identified linguistic patterns across different domains and writing tasks. Additionally, the study does not address whether these authorship traits might evolve as ChatGPT's training data and algorithms are updated.

## Limitations
- Limited generalizability to other large language models
- Small sample size (60 total texts) and restricted topic scope
- Uncertainty about whether linguistic patterns persist across ChatGPT updates

## Confidence

**High Confidence**:
- ChatGPT exhibits a distinctive "idiolect" with consistent patterns across different text types

**Medium Confidence**:
- ChatGPT fails to replicate register variation as seen in human writing

**Low Confidence**:
- These patterns could serve as a reliable linguistic fingerprint for AI detection without broader validation

## Next Checks
1. Replicate the analysis with larger and more diverse text corpora covering multiple topics, genres, and writing styles
2. Compare ChatGPT's authorship traits against other contemporary large language models and human writers with varying levels of expertise
3. Conduct temporal validation by testing whether these linguistic patterns persist across different versions of ChatGPT as the model is updated