---
ver: rpa2
title: 'DeepAgent: A General Reasoning Agent with Scalable Toolsets'
arxiv_id: '2510.21618'
source_url: https://arxiv.org/abs/2510.21618
tags:
- tool
- arxiv
- reasoning
- wang
- deepagent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeepAgent introduces a fully autonomous reasoning agent that dynamically
  discovers and invokes tools within a unified reasoning process, moving beyond predefined
  workflows. To handle long-horizon interactions, it uses an autonomous memory folding
  mechanism that compresses interaction history into structured episodic, working,
  and tool memories, reducing error accumulation and allowing the agent to "take a
  breath" and reconsider strategies.
---

# DeepAgent: A General Reasoning Agent with Scalable Toolsets

## Quick Facts
- **arXiv ID:** 2510.21618
- **Source URL:** https://arxiv.org/abs/2510.21618
- **Reference count:** 40
- **Primary result:** DeepAgent outperforms baselines on eight benchmarks including ToolBench, ALFWorld, and GAIA through unified reasoning and memory folding

## Executive Summary
DeepAgent introduces a fully autonomous reasoning agent that dynamically discovers and invokes tools within a unified reasoning process, moving beyond predefined workflows. To handle long-horizon interactions, it uses an autonomous memory folding mechanism that compresses interaction history into structured episodic, working, and tool memories, reducing error accumulation and allowing the agent to "take a breath" and reconsider strategies. The training method, ToolPO, employs LLM-simulated APIs and tool-call advantage attribution for stable, efficient learning with fine-grained credit assignment. Extensive experiments on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank, TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA, HLE), show DeepAgent consistently outperforms baselines in both labeled-tool and open-set tool retrieval scenarios, demonstrating superior adaptability and scalability to real-world tasks.

## Method Summary
DeepAgent is a reasoning agent that integrates tool discovery and execution into a single, coherent reasoning process. It uses a unified agentic reasoning stream where the model generates special tokens (e.g., `<tool_search>`, `<call>`) that are intercepted by the environment to perform actions. For long-horizon tasks, it employs autonomous memory folding that compresses interaction history into structured episodic, working, and tool memories. The training method, ToolPO, uses LLM-simulated APIs and applies tool-call advantage attribution to assign fine-grained credit. The agent is trained on a mixed dataset of ~4.6k instances using the VeRL framework on 64 NVIDIA H20 GPUs, with QwQ-32B as the policy model and Qwen2.5-32B-Instruct as the auxiliary LLM.

## Key Results
- DeepAgent achieves state-of-the-art performance on ToolBench with Pass@1 scores significantly higher than baselines
- The agent demonstrates superior adaptability in open-set tool retrieval scenarios compared to traditional methods
- Memory folding mechanism reduces context explosion while maintaining task performance across long-horizon interactions
- ToolPO training with simulated APIs shows stable learning and generalizes well to real-world benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Unified Agentic Reasoning
- **Claim:** Integrating tool discovery and execution directly into the model's reasoning stream improves adaptability in complex tasks
- **Mechanism:** The model generates special tokens (e.g., `<tool_search>`, `<call>`) within its continuous Chain-of-Thought (CoT). These are intercepted by the environment to perform actions, and results are injected back into the stream
- **Core assumption:** The underlying Large Reasoning Model (LRM) has sufficient context window and planning capability to maintain coherence across these interleaved thought-action states
- **Evidence anchors:** [abstract] "performs autonomous thinking, tool discovery, and action execution within a single, coherent reasoning process"; [section 3.1] Defines actions as unified outputs of policy $\pi$

### Mechanism 2: Structured Memory Folding
- **Claim:** Compressing interaction history into a structured JSON schema (Episodic, Working, Tool) likely reduces error accumulation better than unbounded context or naive truncation
- **Mechanism:** The agent autonomously emits a `<fold_thought>` token. An auxiliary LLM then processes the raw history into three specific JSON structures: Episodic (long-term progress), Working (immediate next steps), and Tool (usage experience)
- **Core assumption:** The auxiliary LLM is capable of extracting "agent-usable" signals without losing critical details
- **Evidence anchors:** [abstract] "compresses interaction history into structured episodic, working, and tool memories... allowing the agent to 'take a breath'"

### Mechanism 3: ToolPO with Fine-Grained Attribution
- **Claim:** Stable tool-use learning likely requires decoupling the final outcome reward from the process of correct tool invocation
- **Mechanism:** ToolPO uses two advantage terms: $A_{succ}$ (global task success) and $A_{action}$ (local tool correctness). Crucially, $A_{action}$ is masked to apply only to the specific tokens generating the tool call
- **Core assumption:** The "LLM-simulated APIs" provide a sufficiently realistic gradient signal compared to real, noisy APIs
- **Evidence anchors:** [abstract] "leverages LLM-simulated APIs and applies tool-call advantage attribution to assign fine-grained credit"

## Foundational Learning

- **Concept: Reinforcement Learning (Advantage Actor-Critic)**
  - **Why needed here:** To understand ToolPO, specifically how the paper splits "advantage" into global success and local action signals
  - **Quick check question:** How does masking the advantage function $A(y_i)$ to specific tokens differ from standard PPO on the full sequence?

- **Concept: Dense Retrieval (RAG)**
  - **Why needed here:** The agent uses a "Tool Retriever" based on embedding similarity to search scalable toolsets
  - **Quick check question:** What is the role of the embedding model $E$ in the tool search process described in Section 3.3?

- **Concept: Context Window Management**
  - **Why needed here:** The "Memory Folding" mechanism is explicitly designed to solve "context length explosion"
  - **Quick check question:** Why does the paper argue that simply increasing the context window is insufficient compared to structured folding?

## Architecture Onboarding

- **Component map:**
  Main LRM (QwQ-32B) -> Auxiliary LLM (Qwen2.5-32B-Instruct) -> Tool Environment (Retriever + Executor) -> Trainer (VeRL framework)

- **Critical path:**
  1. **Inference:** User Query → Main LRM generates `<tool_search>` → Retriever finds tools → Aux LLM summarizes tools → Main LRM generates `<tool_call_result>` → (Loop) → Main LRM generates `<fold_thought>` → Aux LLM compresses history → State Reset
  2. **Training:** Rollout with Simulator → Calculate $A_{succ}$ and $A_{action}$ → Apply masked loss → Update Main LRM

- **Design tradeoffs:**
  - **Simulation vs. Reality:** Using an LLM to simulate APIs trades ground-truth execution accuracy for training stability and cost reduction
  - **JSON vs. Text Memory:** Structured JSON memory trades potential richness of natural language for reliability in parsing and state management

- **Failure signatures:**
  - **Infinite Loop:** The model keeps calling tools without ever emitting `<fold_thought>` or concluding (hit max action limit)
  - **Memory Amnesia:** After a fold, the agent repeats failed steps because the Episodic Memory failed to log the negative outcome
  - **Simulator Drift:** The trained agent performs well on simulated APIs but fails on real benchmarks due to hallucinated tool behaviors during training

- **First 3 experiments:**
  1. **Unit Test the Token Interceptor:** Verify that generating `<tool_search>` actually triggers the retriever and returns data to the context window without breaking the generation stream
  2. **Memory Folding Stress Test:** Run a long-horizon task (e.g., WebShop), force a fold, and verify if the agent retains the "current_goal" from the Working Memory JSON
  3. **ToolPO Gradient Check:** Compare training runs with and without the tool-call advantage attribution ($A_{action}$) to confirm that the loss is indeed guiding tool selection tokens

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of DeepAgent trained via ToolPO with LLM-simulated APIs transfer to real-world environments characterized by high API latency, rate limits, or execution errors?
- **Basis in paper:** The authors introduce an LLM-based Tool Simulator to mitigate the "instability, slow execution, and high costs" of real-world APIs during training
- **Why unresolved:** While the method improves training stability, the paper does not quantify the "sim-to-real" gap where the agent might encounter API behaviors not captured by the simulator

### Open Question 2
- **Question:** Does the rigid JSON schema used for autonomous memory folding constrain the agent's ability to reason about tasks requiring ambiguous or unstructured historical context?
- **Basis in paper:** The paper specifies a fixed "agent-usable data schema in JSON format" to structure episodic, working, and tool memories
- **Why unresolved:** Enforcing a fixed schema reduces token overhead but may act as an information bottleneck, discarding subtle nuances necessary for complex, creative problem-solving

### Open Question 3
- **Question:** To what extent is the system's overall latency and capability bottlenecked by the auxiliary LLM required for memory folding and tool filtering?
- **Basis in paper:** The framework relies on a separate "auxiliary LLM" to process tool documentation and compress interaction history
- **Why unresolved:** The efficiency gains from reduced context length might be offset by the inference time and potential error propagation of the auxiliary model

## Limitations
- The reliance on LLM-simulated APIs introduces potential distribution shift risks when transferring to real-world tools
- The memory folding mechanism's effectiveness depends heavily on the auxiliary LLM's ability to compress complex interaction histories without losing critical task-specific details
- Performance metrics focus primarily on task completion rates without sufficient analysis of efficiency trade-offs introduced by the memory folding system

## Confidence

- **High Confidence:** The architectural design choices (unified reasoning stream, structured memory schema) are well-specified and technically sound. The experimental results showing consistent outperformance across eight benchmarks are methodologically robust.
- **Medium Confidence:** The ToolPO algorithm's theoretical framework appears solid, but the practical impact of the dual-advantage system versus simpler approaches needs more rigorous ablation studies.
- **Low Confidence:** The paper doesn't adequately address failure modes when the auxiliary LLM makes errors in tool summarization or memory folding.

## Next Checks

1. **Simulator-to-Reality Gap Analysis:** Run DeepAgent on both the simulated API environment and the actual real APIs for ToolBench tasks, measuring performance degradation to quantify sim-to-real transfer effectiveness
2. **Memory Compression Fidelity Test:** After each memory fold, compare the compressed JSON memory against the raw interaction history to identify what specific information types (numerical values, conditional logic, tool parameters) are most frequently lost or corrupted
3. **Context Window Stress Test:** Systematically increase task complexity until the agent fails to fold memory appropriately, mapping the relationship between task horizon length, memory fold frequency, and overall success rate