---
ver: rpa2
title: 'MoENAS: Mixture-of-Expert based Neural Architecture Search for jointly Accurate,
  Fair, and Robust Edge Deep Neural Networks'
arxiv_id: '2502.07422'
source_url: https://arxiv.org/abs/2502.07422
tags:
- fairness
- accuracy
- edge
- robustness
- moenas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MoENAS, a hardware-aware neural architecture
  search method that jointly optimizes accuracy, fairness, robustness, and generalization
  for edge DNNs. MoENAS addresses limitations in existing edge DNN designs, which
  often exhibit significant accuracy disparities across different skin tones (14.09%),
  poor robustness, and overfitting.
---

# MoENAS: Mixture-of-Expert based Neural Architecture Search for jointly Accurate, Fair, and Robust Edge Deep Neural Networks

## Quick Facts
- arXiv ID: 2502.07422
- Source URL: https://arxiv.org/abs/2502.07422
- Reference count: 40
- Primary result: MoENAS achieves 4.02% accuracy improvement, reduces skin tone accuracy disparity from 14.09% to 5.60%, and improves robustness by 3.80% while maintaining model size close to SOTA (+0.4M)

## Executive Summary
This paper introduces MoENAS, a hardware-aware neural architecture search method that jointly optimizes accuracy, fairness, robustness, and generalization for edge DNNs. MoENAS addresses limitations in existing edge DNN designs, which often exhibit significant accuracy disparities across different skin tones (14.09%), poor robustness, and overfitting. The key innovation lies in replacing standard FFN layers with switch FFN layers inspired by mixture-of-experts (MoE), enabling dynamic feature routing and improving fairness and robustness. A Bayesian optimization-based search process explores expert configurations within a defined search space, while expert pruning reduces model size by eliminating under-utilized experts. Evaluated on the FACET dataset for person classification, MoENAS achieves a 4.02% improvement in accuracy, reduces skin tone accuracy disparities from 14.09% to 5.60%, enhances robustness by 3.80%, and minimizes overfitting to 0.21%, all while maintaining a model size close to SOTA models (+0.4M). These results establish a new benchmark for edge DNN design, paving the way for more inclusive and robust models.

## Method Summary
MoENAS replaces the standard FFN layers in MobileViTv2 with Switch FFN layers that use a learned router to direct features to one of 1-8 expert FFNs per layer. The search space is encoded as a 9-element vector (experts per layer) representing 8^9 ≈ 134M possible configurations. Bayesian optimization with XGBoost surrogate models explores this space, jointly optimizing test accuracy, skin fairness (β - SPD/β, β=0.2), robustness to lighting, and generalization while constraining model size to 1M-3M parameters. After identifying promising architectures, expert pruning iteratively removes least-used experts and re-routes features, reducing model size by up to 26% while preserving performance.

## Key Results
- 4.02% improvement in test accuracy compared to SOTA edge DNNs
- Skin tone accuracy disparity reduced from 14.09% to 5.60%
- Robustness to lighting conditions improved by 3.80%
- Model size maintained at +0.4M compared to average SOTA models
- Overfitting minimized to 0.21% validation-test accuracy gap

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Feature Routing via Switch FFN Layers
Replacing standard FFN layers with Switch FFN layers enables input-adaptive processing that reduces skin tone accuracy disparities and improves robustness. A learned router directs each feature vector to the most appropriate expert FFN from a set, creating specialized processing pathways. Different input characteristics (e.g., skin tones, lighting conditions) activate different expert combinations, allowing the model to handle diverse cases without a single monolithic processing path amplifying biases. The core assumption is that feature representations contain learnable patterns that correlate with demographic or environmental attributes, and routing based on these patterns enables more equitable processing.

### Mechanism 2: Multi-Objective Bayesian Optimization for Architecture Search
Searching over expert configurations (number of experts per layer) with joint optimization of accuracy, fairness, robustness, and generalization produces architectures that outperform manually designed or single-objective-optimized models. XGBoost surrogate models predict performance metrics from architecture encodings. Bayesian optimization explores the search space (8^9 ≈ 134M configurations) to identify Pareto-optimal architectures balancing competing objectives. Model size is treated as a constraint rather than objective. The core assumption is that the search space contains architectures where fairness and accuracy improvements do not strictly trade off against each other, which standard scaling approaches miss.

### Mechanism 3: Post-Training Expert Pruning for Efficiency
Iteratively removing least-used experts after training preserves most performance gains while reducing model size, outperforming direct training of smaller expert configurations. After training, expert usage frequencies are measured across validation data. Low-usage experts are pruned, and affected features are re-routed to next-best experts. This leverages the "knowledge distillation" effect where over-parameterized training discovers better representations. The core assumption is that expert importance correlates with usage frequency, and high-usage experts encode critical processing pathways that should be preserved.

## Foundational Learning

- Concept: Mixture of Experts (MoE) and Sparse Routing
  - Why needed here: Understanding how routers select experts and why sparse activation improves efficiency without linear compute scaling is essential for debugging routing failures.
  - Quick check question: Can you explain why MoE increases parameter count but not FLOPs? (Answer: Only a subset of experts are activated per input.)

- Concept: Bayesian Optimization with Surrogate Models
  - Why needed here: The search process relies on surrogate models to predict multi-objective performance; understanding their limitations prevents over-trusting predictions.
  - Quick check question: What happens to BO search efficiency when the search space dimensionality increases? (Answer: Sample efficiency degrades; more training samples needed.)

- Concept: Multi-Objective Optimization and Pareto Fronts
  - Why needed here: MoENAS produces Pareto-optimal solutions; engineers must understand trade-offs when selecting final architectures.
  - Quick check question: If Model A has higher accuracy but lower fairness than Model B, which is better? (Answer: Neither dominates; selection depends on deployment requirements.)

## Architecture Onboarding

- Component map:
  - Base Architecture: MobileViTv2 (9 transformer blocks with FFN layers)
  - Switch FFN Layer: Router (learnable linear layer) + Expert Set (1-8 FFN experts per layer)
  - Search Pipeline: Architecture Encoder → XGBoost Surrogates → Bayesian Optimizer → Candidate Selection
  - Pruning Module: Usage Tracker → Expert Ranker → Pruning Executor → Re-routing Logic

- Critical path:
  1. Replace all FFN layers in MobileViTv2 with Switch FFN layers (max 8 experts each)
  2. Train 127+ random architectures to build surrogate training data (~5 days with 4×A6000)
  3. Run BO to identify Pareto-optimal configurations
  4. Train selected architecture fully
  5. Apply iterative expert pruning until performance threshold approached

- Design tradeoffs:
  - More experts per layer → Higher capacity but larger model size
  - Aggressive pruning → Smaller model but potential fairness degradation
  - Larger surrogate training set → Better predictions but higher search cost

- Failure signatures:
  - Router collapse: Single expert receives >90% of routing decisions (check expert usage histograms)
  - Fairness regression after pruning: Minority subgroup accuracy drops disproportionately (monitor per-group metrics during pruning)
  - Surrogate overfitting: Predicted performance far exceeds actual performance on held-out architectures (use cross-validation)

- First 3 experiments:
  1. Baseline validation: Train MobileViTv2 with Switch FFN (fixed 4 experts per layer) without search. Measure if any fairness improvement emerges from MoE structure alone.
  2. Expert usage analysis: Visualize routing patterns across skin tone groups to verify hypothesis that different groups activate different experts. If patterns are uniform, the mechanism is not working as intended.
  3. Pruning sensitivity: Apply expert pruning to the discovered architecture and plot performance vs. size curve. Identify the "knee point" where further pruning causes sharp degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the MoENAS methodology be effectively extended to convolutional neural networks (CNNs) to achieve similar improvements in fairness and robustness?
- Basis in paper: [explicit] The Conclusion states future research directions include "extending its capabilities to convolutional networks, aiming for broader applicability."
- Why unresolved: The current methodology relies on replacing the Feed-Forward Network (FFN) layer within attention-based blocks (specifically MobileViTv2); it is unclear how the Switch FFN layer would be integrated into or replace standard convolutional blocks without disrupting the spatial feature extraction inherent to CNNs.
- What evidence would resolve it: A successful application of the search space and expert routing to a standard CNN backbone (e.g., ResNet or MobileNet) on the FACET dataset, demonstrating reduced skin tone accuracy disparity and robustness gains comparable to the attention-based results.

### Open Question 2
- Question: How can the analysis of expert selection patterns be formalized to provide explainability and interpretability for the model's fairness decisions?
- Basis in paper: [explicit] The Conclusion identifies "investigating the explainability of MoENAS by analyzing the choice of experts within the architecture" as a promising research direction.
- Why unresolved: While the ablation study (Figure 10) observes that images from the same skin tone tend to follow similar paths, the paper does not establish a method to interpret *why* certain experts are better for specific subgroups or how to extract human-understandable rules from the router's decisions.
- What evidence would resolve it: Development of an explainability framework that maps specific expert activations to demographic features, verifying that the router is basing decisions on relevant semantic features rather than amplifying spurious correlations.

### Open Question 3
- Question: Can the model size overhead introduced by the Mixture-of-Experts (MoE) layers be reduced to facilitate deployment in extremely low-resource TinyML environments?
- Basis in paper: [explicit] The Limitations section notes that "due to the nature of MoEs, the model size cannot be reduced as much as standalone edge DNNs," making it "unsuitable for extremely tiny ML use cases."
- Why unresolved: The expert pruning strategy effectively reduces size but still results in models larger than the average SOTA (+0.4M parameters), suggesting a fundamental trade-off exists between the capacity required for diverse expert routing and the minimal footprint of tiny edge models.
- What evidence would resolve it: A variant of MoENAS that utilizes aggressive shared-expert architectures or more efficient routing to achieve parity in parameter count (<0.5M) with tiny models (like TinyNet-e) while retaining fairness benefits.

### Open Question 4
- Question: How does the inclusion of hardware latency constraints during the search process impact the multi-objective balance between accuracy, fairness, and robustness?
- Basis in paper: [explicit] The Discussion states future work could focus on "expanding the model's objectives to include factors like latency... thereby broadening the scope and application."
- Why unresolved: The current optimization uses model size as a proxy for efficiency and treats it as a constraint; it remains unknown if optimizing for strict real-time latency on specific hardware would force the NAS to sacrifice the specialized expert configurations that enable high fairness.
- What evidence would resolve it: A Pareto front analysis on target edge hardware (e.g., Raspberry Pi or Jetson) showing that architectures selected for low latency can still maintain a skin tone accuracy gap significantly lower than SOTA baselines.

## Limitations
- Relies on MobileViTv2 as base architecture without providing specific version or training recipe, introducing potential replication variance
- Expert routing dynamics are inferred from usage patterns but not explicitly validated or visualized
- Search space exploration via only 127+ samples raises questions about surrogate prediction robustness in under-sampled regions
- No analysis of how sensitive fairness/robustness gains are to training data composition or hyperparameter choices
- Expert pruning relies on usage frequency without ablation showing whether other importance metrics would better preserve fairness

## Confidence

- **High Confidence**: Accuracy improvement over SOTA (4.02%) and model size maintenance (+0.4M) are directly measurable from reported results.
- **Medium Confidence**: Skin tone accuracy disparity reduction (14.09% → 5.60%) and robustness gain (3.80%) are plausible given the MoE structure, but lack direct mechanistic validation or ablation.
- **Low Confidence**: The claim that fairness and robustness gains persist after aggressive expert pruning is weakly supported; no detailed analysis of subgroup impact during pruning is provided.

## Next Checks

1. **Routing Pattern Validation**: Visualize and cluster expert selection patterns across FACET skin tone groups and lighting conditions. Confirm that different subgroups activate meaningfully different expert combinations versus uniform or random routing.

2. **Pruning Impact by Subgroup**: After each pruning iteration, plot per-skin-tone group accuracy and robustness metrics. Identify if minority subgroups degrade faster, indicating biased pruning effects.

3. **Search Space Coverage Analysis**: Retrain a small set (5-10) of high-performing architectures from the surrogate predictions that were not in the original 127-sample set. Compare actual vs. predicted performance to quantify surrogate model accuracy and identify potential search failures.