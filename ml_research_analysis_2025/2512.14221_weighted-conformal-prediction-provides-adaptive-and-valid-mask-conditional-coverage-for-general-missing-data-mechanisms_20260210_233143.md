---
ver: rpa2
title: Weighted Conformal Prediction Provides Adaptive and Valid Mask-Conditional
  Coverage for General Missing Data Mechanisms
arxiv_id: '2512.14221'
source_url: https://arxiv.org/abs/2512.14221
tags:
- prediction
- imputation
- data
- missing
- xobs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of constructing valid prediction
  sets under missing covariates, which standard conformal prediction methods fail
  to guarantee. The authors propose a preimpute-mask-then-correct framework that uses
  distributional imputation followed by likelihood ratio weighting or acceptance-rejection
  correction to handle mask-induced distribution shifts.
---

# Weighted Conformal Prediction Provides Adaptive and Valid Mask-Conditional Coverage for General Missing Data Mechanisms

## Quick Facts
- arXiv ID: 2512.14221
- Source URL: https://arxiv.org/abs/2512.14221
- Reference count: 40
- Key outcome: Proposed methods reduce prediction interval width by 10-30% compared to state-of-the-art CP-MDA methods while maintaining target coverage under MCAR, MAR, and MNAR missingness.

## Executive Summary
This paper addresses the fundamental challenge of constructing valid prediction sets when covariates are missing, a problem where standard conformal prediction methods fail to guarantee coverage. The authors introduce a preimpute-mask-then-correct framework that uses distributional imputation followed by likelihood ratio weighting or acceptance-rejection correction to handle mask-induced distribution shifts. Two novel algorithms, Mask-Conditional Weighted CP and ARC-CP, are theoretically proven to provide both marginal and mask-conditional validity under general missingness mechanisms. Experiments demonstrate these methods achieve 10-30% narrower prediction intervals than existing approaches while maintaining target coverage, with ARC-CP offering computational advantages through acceptance-rejection sampling.

## Method Summary
The method employs a three-stage approach: first, calibration data is pre-imputed using distributional imputation (e.g., MICE with posterior sampling) to create complete data; second, each imputed calibration point is masked to match the test point's missingness pattern, creating paired data with identical observed features but potentially different underlying distributions; third, a correction step either weights calibration points by their likelihood ratios (Weighted CP) or subsamples them via acceptance-rejection sampling (ARC-CP) to restore exchangeability with the test point. The nonconformity scores are computed using Conformalized Quantile Regression with Quantile Regression Forest, and prediction sets are constructed based on the corrected empirical distribution of calibration scores. This framework ensures valid mask-conditional coverage under MCAR, MAR, and MNAR missingness mechanisms.

## Key Results
- ARC-CP achieves 12% narrower average prediction intervals compared to CP-MDA-Nested on synthetic MCAR data
- Both Weighted CP and ARC-CP maintain ≥90% target coverage across all mask patterns in experiments
- ARC-CP offers ~10× computational speedup over Weighted CP due to acceptance-rejection sampling
- Method successfully handles real-world UCI Concrete dataset with 50% missingness at random

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preimputing calibration data and masking it to match the test point's missingness pattern enables standard split CP to be applied, but introduces a distribution shift that requires correction.
- Mechanism: The calibration data is first imputed using distributional (multiple) imputation (e.g., MICE), then each calibration point is masked with the test point's mask M^{n+1}. This creates paired data with identical observed feature patterns, but the imputed calibration follows Q_m while the true test point follows P_m—inducing a distribution shift proportional to d_{TV}(P_m, Q_m).
- Core assumption: Assumption 1 (calibration and test data are i.i.d.; imputer is trained only on training data; predictive model fitted only on training set) and Assumption 2 (absolute continuity of P with respect to Q).
- Evidence anchors:
  - [abstract]: "preimpute-mask-then-correct framework that uses distributional imputation followed by likelihood ratio weighting or acceptance-rejection correction to handle mask-induced distribution shifts"
  - [Section 3.1, Proposition 3.2]: "P(Y_{n+1} ∈ Ĉ_α| M_{n+1}=m) ≥ 1-α - [d_{TV}(P,Q) + E_P[|dP(X,Y|M=m)/dP(X,Y) - 1|]]"
  - [corpus]: Related work on distribution shifts (WQLCP, arxiv 2505.19587) addresses covariate shift but not mask-induced heterogeneity specifically.
- Break condition: If imputation is purely deterministic, absolute continuity is violated, causing unbounded likelihood ratios and potential degeneration toward CP-MDA-Exact behavior.

### Mechanism 2
- Claim: Mask-Conditional Weighted CP corrects the distribution shift by weighting each calibration point's contribution to the quantile computation using the likelihood ratio ω_m = dP_m/dQ_m.
- Mechanism: Instead of computing the (1-α) quantile of the empirical score distribution uniformly, each calibration score S_i^m is weighted by W_i^m(x_obs(m), y) ∝ ω_i^m. The prediction set is then Ĉ_α^W = {y : s(ẽX_{n+1}, y) ≤ Q_{1-α}(Σ_i W_i^m δ_{S_i^m} + W_{n+1}^m δ_∞)}. This adjustment ensures valid coverage under the true conditional distribution P_m.
- Core assumption: The likelihood ratio ω_m(x_obs(m), y) must be known or accurately estimated; ω_m must be bounded for computational tractability.
- Evidence anchors:
  - [abstract]: "A key component of our approach is a reweighted conformal prediction procedure that corrects the prediction sets after distributional imputation"
  - [Section 3.2, Theorem 3.3]: "For any missingness mechanism (MCAR, MAR, or MNAR): P(Y_{n+1} ∈ Ĉ_α^W | M_{n+1}=m) ≥ 1-α"
  - [corpus]: Weighted p-values approaches (arxiv 2505.11785) share the reweighting principle but address different aggregation problems.
- Break condition: Poor likelihood ratio estimation leads to miscoverage bounded by d_{TV}(P̃_m, P_m) (Proposition 3.5); if estimation correlation with true ratio drops below ~0.3, coverage may systematically underperform.

### Mechanism 3
- Claim: ARC-CP uses acceptance-rejection sampling to select a subset of imputed calibration points that exactly follow the target distribution P_m, enabling standard (unweighted) split CP.
- Mechanism: For each imputed calibration point, sample U_i ∼ U(0,1) and accept if U_i < ω_i^m/K where K bounds the likelihood ratio. Accepted points follow P_m exactly, restoring exchangeability with the test point and allowing standard quantile-based prediction sets.
- Core assumption: The likelihood ratio ω_m is bounded by some constant K > 0; acceptance rate must be sufficient to yield enough calibration points.
- Evidence anchors:
  - [Section 3.3, Theorem 3.4]: "(a) The points in gCal_m are i.i.d. samples from P_m; (b) P(Y_{n+1} ∈ Ĉ_α^{AR} | M_{n+1}=m) ≥ 1-α"
  - [Section 4.1]: "ARC CP yields the narrowest average intervals—reducing width by 12% relative to MDA-Nested"
  - [corpus]: No direct corpus precedent for AR correction in missing-data CP; this is the paper's novel contribution.
- Break condition: If Assumption 2 is violated (unbounded ω), acceptance rate drops toward zero, potentially yielding empty calibration sets and infinite-width intervals.

## Foundational Learning

- Concept: **Total Variation Distance (d_{TV})**
  - Why needed here: Quantifies the distribution shift between imputed calibration (Q_m) and true test distribution (P_m); directly appears in coverage bounds.
  - Quick check question: If imputation perfectly reconstructs the joint distribution, what is d_{TV}(P, Q) and what coverage guarantee remains?

- Concept: **Likelihood Ratio / Density Ratio Estimation**
  - Why needed here: Central to both Weighted CP and ARC-CP; requires training a binary classifier to distinguish samples from P_m vs Q_m (Algorithm 2).
  - Quick check question: How does the Bayes classifier's error rate relate to total variation distance? (Hint: See Lemma C.1)

- Concept: **Acceptance-Rejection Sampling**
  - Why needed here: Enables ARC-CP to transform samples from proposal Q_m into exact samples from target P_m without explicit integration.
  - Quick check question: If the proposal distribution Q_m has low density where P_m is high, what happens to the acceptance rate?

## Architecture Onboarding

- Component map: Training split -> Quantile Regression Forest fitting -> Distributional imputation (MICE) -> Likelihood ratio estimator (GBDT classifier) -> Mask-conditional correction (Weighted/ARC-CP) -> Prediction set construction
- Critical path:
  1. Ensure training/imputation/likelihood estimation are strictly separated (no data leakage)
  2. Verify imputation is *distributional* (not deterministic)—check `sample_posterior=True` in MICE
  3. Validate likelihood ratio estimator quality before deployment (target: correlation >0.3 with oracle if ground truth available)
- Design tradeoffs:
  - **Weighted CP vs ARC-CP**: Weighted CP is more stable (no sampling variance) but requires grid search for interval bounds; ARC-CP is ~10× faster (Section E.1) but introduces AR sampling variance
  - **Imputation quality**: Better imputation → higher acceptance rate → narrower intervals; poor imputation → degenerates toward CP-MDA-Exact
  - **Calibration set size**: ARC-CP requires sufficient accepted samples; sparse masks may need larger calibration pools
- Failure signatures:
  - **Infinite/empty intervals**: Likely CP-MDA-Exact fallback due to insufficient accepted samples or unbounded likelihood ratios
  - **Systematic under-coverage (~86-89%)**: Distribution shift not corrected; check likelihood estimator training
  - **Over-coverage with wide intervals**: Imputation too conservative; verify distributional (not mean) imputation
- First 3 experiments:
  1. **Sanity check on synthetic MCAR data** (Section 4.1 setup): Verify coverage ≥90% across all masks with known ground truth; compare interval widths against MDA-Nested baseline
  2. **Ablation: correction vs no correction** (Section 4.3): Demonstrate that removing the weighting/AR step causes under-coverage (~89% worst-case), confirming the distribution shift is real and corrected
  3. **Likelihood estimator quality test** (Appendix D.4): Perturb estimated ω_m with controlled noise and plot coverage vs. correlation with true ratio; confirm >0.3 correlation yields stable coverage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can tighter theoretical bounds on miscoverage be derived under likelihood ratio estimation error, beyond the general TV-distance bounds in Proposition 3.6?
- Basis in paper: [explicit] "Future work includes deriving tighter robustness bounds under likelihood ratio estimation error or violations of Assumption 2"
- Why unresolved: Current bounds use total variation distance which may be loose; the empirical results in Appendix D suggest the method is robust even with moderate estimation error (correlation > 0.3), indicating potential gap between theory and practice.
- What evidence would resolve it: Explicit concentration inequalities relating likelihood estimation error (e.g., correlation with true ratio) to coverage deviation, validated empirically across diverse settings.

### Open Question 2
- Question: What adaptive strategies can maintain MCV guarantees in high-dimensional settings where likelihood ratio estimation becomes unreliable?
- Basis in paper: [explicit] Future work includes "developing adaptive strategies for high-dimensional or imperfect imputations"; [inferred] Section 3.5 notes "accurately estimating the likelihood ratio is never trivial... for instance in high dimensions"
- Why unresolved: The curse of dimensionality affects classifier-based density ratio estimation (Algorithm 2), potentially violating the quality requirements established in Section 4.4.
- What evidence would resolve it: Modified algorithms with dimension-adaptive regularization or alternative weighting schemes, with theoretical guarantees and empirical validation on datasets with d >> 100.

### Open Question 3
- Question: How should the subsampled calibration set fId_Cal be optimally selected in CP-MDA-Nested* to balance coverage validity and interval efficiency?
- Basis in paper: [explicit] "The CP-MDA-Nested⋆ with a more flexible target missingness pattern is a compromise between the two methods, but how to choose the best fID Cal remains a question."
- Why unresolved: The choice affects both feasibility (sufficient calibration points) and conservativeness; no principled criterion exists.
- What evidence would resolve it: Theoretical analysis of the bias-variance trade-off in selection strategies, or empirical guidelines derived from systematic simulation studies across missingness rates and calibration set sizes.

### Open Question 4
- Question: When does the absolute continuity assumption (Assumption 2) fail in practice, and what principled modifications can restore valid coverage?
- Basis in paper: [explicit] "If Assumption 2 is violated, the likelihood ratio may not be well-defined or bounded, compromising our methods' validity"; [inferred] Appendix A notes deterministic imputation violates this assumption, leading to extreme weights or infinite prediction intervals.
- Why unresolved: The current remedy (adding small Gaussian perturbation) is ad hoc; no systematic characterization exists of when imputation methods produce distributions with incompatible supports.
- What evidence would resolve it: Formal conditions linking imputation method properties to Assumption 2 satisfaction, plus fallback algorithms with modified guarantees when the assumption fails.

## Limitations
- Method critically depends on distributional (stochastic) imputation being available and accurate—deterministic imputation causes unbounded likelihood ratios
- Coverage guarantees require the likelihood ratio to be bounded, which may not hold when imputation systematically differs from true data
- Computational overhead from likelihood ratio estimation and potential acceptance-rejection inefficiency may limit scalability to high-dimensional data

## Confidence
- **High confidence**: Theoretical validity guarantees (Theorem 3.3, 3.4) under stated assumptions; empirical coverage results on synthetic MCAR and UCI Concrete datasets showing ≥90% target coverage
- **Medium confidence**: Real-world performance on complex missingness mechanisms (MAR/MNAR) and high-dimensional data; numerical stability of likelihood ratio estimation in practice
- **Low confidence**: Performance in streaming/online settings with evolving missingness patterns; behavior with categorical/discrete features where distributional imputation is challenging

## Next Checks
1. **Coverage sensitivity to imputation quality**: Systematically degrade imputation accuracy (e.g., using mean imputation vs. MICE) and measure coverage degradation across all methods to confirm the distribution shift correction is necessary
2. **Boundedness verification**: Test ARC-CP on datasets where Assumption 2 is violated (e.g., deterministic imputation or extreme distribution shifts) and measure acceptance rates and interval widths to identify failure modes
3. **High-dimensional scalability**: Apply the method to a d=50+ feature dataset with sparse missingness and measure likelihood ratio estimation runtime and AR acceptance rates to quantify computational bottlenecks