---
ver: rpa2
title: 'LLM-Specific Utility: A New Perspective for Retrieval-Augmented Generation'
arxiv_id: '2510.11358'
source_url: https://arxiv.org/abs/2510.11358
tags:
- passages
- utility
- llms
- gold
- utilitarian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of LLM-specific utility in retrieval-augmented
  generation (RAG), challenging the assumption that utility is universal across models.
  The authors formalize LLM-specific utility as the performance improvement a target
  LLM achieves when provided with evidence compared to answering without it.
---

# LLM-Specific Utility: A New Perspective for Retrieval-Augmented Generation

## Quick Facts
- **arXiv ID**: 2510.11358
- **Source URL**: https://arxiv.org/abs/2510.11358
- **Reference count**: 40
- **Primary result**: LLM-specific utility in RAG is model-dependent and non-transferable; human-annotated passages serve as a strong general baseline.

## Executive Summary
This paper introduces the concept of LLM-specific utility in retrieval-augmented generation (RAG), challenging the assumption that utility is universal across models. The authors formalize LLM-specific utility as the performance improvement a target LLM achieves when provided with evidence compared to answering without it. They construct a benchmark of LLM-specific gold utilitarian passages for four LLMs (Qwen3-8B/14B/32B and Llama3.1-8B) across three QA datasets. Their analysis reveals that utilitarian passages are model-dependent and non-transferable—each LLM performs best with its own evidence, while human-annotated passages serve as a strong but suboptimal general baseline. Additionally, they introduce the LLM-specific utility judgment task and find that existing utility-aware methods largely capture model-agnostic usefulness rather than true LLM-specific utility. These findings highlight the need for generator-tailored evidence selection to improve RAG performance.

## Method Summary
The authors construct a benchmark of LLM-specific gold utilitarian passages (GUP) by comparing each LLM's accuracy with and without evidence passages. For each query-passage pair, a passage is labeled as utilitarian if it improves the LLM's answer accuracy. They evaluate four LLMs (Qwen3-8B/14B/32B and Llama3.1-8B) across three QA datasets (Natural Questions, TriviaQA, MS MARCO-FQA) using a Wikipedia corpus. The paper introduces and evaluates several utility judgment methods including verbalized selection/ranking with pseudo-answers, likelihood-based ranking, and attention-based ranking. They compare these methods against the gold standard of LLM-specific GUP and human-annotated gold passages.

## Key Results
- LLM-specific gold utilitarian passages are non-transferable across models—each LLM performs best with its own evidence.
- Human-annotated gold passages serve as a strong general baseline but are suboptimal compared to model-specific evidence.
- Current utility-aware methods (verbalized and likelihood-based) capture model-agnostic usefulness rather than true LLM-specific utility.
- Perplexity serves as a reliable proxy for evidence comprehensibility and correlates with utility.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Passage utility is conditional on the specific LLM acting as the generator, driven by variations in parametric knowledge and evidence comprehension.
- **Mechanism**: A passage is defined as utilitarian only if `Acc(LLM(q, d)) > Acc(LLM(q, ∅))`. Because LLMs possess different pre-trained knowledge (some already know the answer) and different processing capabilities (perplexity/reading ability), a passage beneficial to one model may be redundant or confusing to another.
- **Core assumption**: Performance improvement over a zero-evidence baseline is the sole valid indicator of utility, disregarding potential stylistic improvements in answers.
- **Break condition**: If a universal retriever achieves parity with model-specific gold passages across heterogeneous architectures (e.g., Llama vs. Qwen), the mechanism of model-dependency is falsified.

### Mechanism 2
- **Claim**: Utility arises from the intersection of knowledge gaps and evidence comprehensibility, often indicated by lower perplexity.
- **Mechanism**: A passage must fill a specific knowledge void (the model cannot answer without it) AND be parseable. The paper finds that human-annotated gold passages often fail to be utilitarian if the LLM assigns them higher perplexity, suggesting the model cannot effectively "read" or integrate that specific text style/format.
- **Core assumption**: Perplexity serves as a reliable proxy for an LLM's ability to leverage evidence.
- **Break condition**: If high-perplexity passages consistently yield higher accuracy gains than low-perplexity ones, the comprehensibility link breaks.

### Mechanism 3
- **Claim**: Current utility estimation methods (verbalized or likelihood-based) capture a "general" notion of usefulness rather than true LLM-specific utility.
- **Mechanism**: Experiments show that weaker models often perform better when using passages selected by stronger models (via verbalized judgment) than their own selections. This implies current methods identify "good" text generally but fail to model the specific constraints or knowledge gaps of the target LLM.
- **Core assumption**: The verbalized judgment capabilities of LLMs are currently insufficient for precise self-diagnosis of knowledge gaps.
- **Break condition**: If a verbalized selection method is developed where the model strictly selects only what it *doesn't* know and results in zero performance degradation compared to ground-truth utility.

## Foundational Learning

- **Concept: Parametric Knowledge Boundary (Known vs. Unknown)**
  - **Why needed here**: The definition of utility explicitly relies on whether the LLM can answer the query without evidence (`Acc(q, ∅)`). You cannot measure utility gain without establishing this baseline.
  - **Quick check question**: Can you explain why a passage providing the correct answer might have *zero* utility for a specific LLM?

- **Concept: Context-Independent vs. Context-Dependent Utility**
  - **Why needed here**: The paper acknowledges it evaluates context-independent utility (single passage value). Understanding this is critical because real-world RAG requires combining passages (context-dependent), which may exhibit different utility behaviors.
  - **Quick check question**: Does the paper's gold utilitarian passage definition account for the interaction between multiple retrieved passages?

- **Concept: Verbalized vs. Probabilistic Utility Estimation**
  - **Why needed here**: These are the two primary methods evaluated (Prompting vs. Likelihood scores). The paper shows both struggle with specificity.
  - **Quick check question**: Why would "Likelihood" (probability of generating the answer given the passage) fail to capture true utility if the model hallucinates answers confidently?

## Architecture Onboarding

- **Component map:**
  - Query Input -> Target LLM (Qwen3-8B/14B/32B or Llama3.1-8B) -> Retriever (BGE-M3) -> Top-20 Candidates -> Utility Annotator -> Filtered Passages -> Generation

- **Critical path:**
  1. Query Input
  2. **Baseline Test:** Measure `Acc(q, ∅)` to determine if the query is "Known" or "Unknown" to the specific LLM
  3. **Candidate Generation:** Retrieve Top-K passages
  4. **Utility Filtering:** Apply a selector to find passages where `Acc(q, d) > Acc(q, ∅)`
  5. **Generation:** Feed filtered passages to the LLM

- **Design tradeoffs:**
  - **Pointwise vs. Listwise Annotation:** Listwise selection (showing all candidates at once) improves F1 but risks "over-selection" (selecting irrelevant context)
  - **Self-Selection vs. Stronger-Model-Selection:** Using a stronger model to select evidence for a weaker model may yield better performance than the weaker model selecting for itself (counter-intuitive but evidenced in paper)

- **Failure signatures:**
  - **Knowledge Conflict:** LLM ignores evidence if it contradicts parametric memory (though paper focuses more on comprehension gaps)
  - **Over-Selection:** Verbalized methods often select too many passages, introducing noise
  - **Negative Utility:** "Known" queries suffer slight performance drops when "helpful" human evidence is added (distracting the model)

- **First 3 experiments:**
  1. **Establish Baselines:** Pick two LLMs (one "strong" like Qwen-32B, one "weak" like Llama-8B). For a small set of NQ queries, measure their accuracy without evidence (`Acc(q, ∅)`) to identify their specific "Unknown" queries
  2. **Construct GUP:** For the "Unknown" queries, run retrieval and check if adding individual passages improves accuracy. Compare these sets—do they overlap?
  3. **Test Transfer:** Force the "weak" LLM to use the "strong" LLM's identified GUP. Compare its performance against using its own GUP and the Top-20 raw retrieval

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design utility judgment methods that genuinely capture LLM-specific utility rather than model-agnostic usefulness?
- Basis in paper: [explicit] The authors state: "Future research should focus on developing more sophisticated strategies for selecting or generating passages that are genuinely tailored to the unique characteristics of each LLM." They also find that "existing utility-aware selection and ranking methods largely capture model-agnostic utility rather than true LLM-specific utility."
- Why unresolved: Current verbalized and likelihood-based methods produce selections that generalize across models, suggesting they fail to capture model-specific characteristics like individual knowledge gaps and comprehension abilities.
- What evidence would resolve it: Development of a method that, when selecting passages for a specific LLM, achieves significantly higher RAG performance for that target LLM than when the same selections are used by other LLMs, along with lower cross-model transfer performance.

### Open Question 2
- Question: How should context-dependent utility be formalized and estimated when multiple passages jointly contribute to answer generation?
- Basis in paper: [explicit] The authors explicitly state: "While this analysis centers on context-independent utility, exploring context-dependent utility remains an important direction for future work." They note their current approach treats each passage independently.
- Why unresolved: The current definition assumes utility is context-independent (each passage evaluated in isolation), but in multi-hop reasoning or complex queries, passage combinations may provide synergistic utility that no single passage offers alone.
- What evidence would resolve it: A formalization of context-dependent utility that accounts for passage interactions, demonstrated through improved RAG performance on multi-hop datasets compared to context-independent selection methods.

### Open Question 3
- Question: What mechanisms explain why weaker LLMs often achieve better RAG performance using passages selected by stronger LLMs rather than their own utility judgments?
- Basis in paper: [explicit] The authors observe: "weaker models often perform better when using passages selected by stronger LLMs rather than their own selections." They conclude this indicates current methods capture general utility rather than truly model-specific utility, but the underlying mechanism remains unexplained.
- Why unresolved: This finding is counterintuitive—self-selection should theoretically optimize for a model's own knowledge gaps. Understanding why this fails could reveal fundamental limitations in how LLMs verbalize or estimate their own utility needs.
- What evidence would resolve it: Systematic analysis comparing self-selection vs. stronger-model-selection across models with controlled capability differences, identifying whether the effect stems from calibration issues, selection bias, or fundamental limitations in self-assessment.

## Limitations
- The study focuses on context-independent utility evaluation, which doesn't capture the real-world complexity of RAG where multiple passages interact.
- Current utility estimation methods may be capturing general usefulness rather than true LLM-specific utility, suggesting the evaluation framework may not fully validate the proposed mechanism.
- The perplexity-based comprehension proxy, while theoretically grounded, lacks direct validation against human comprehension metrics.

## Confidence
- **High Confidence**: The core finding that LLM-specific gold utilitarian passages are non-transferable across models is well-supported by experimental results showing distinct GUP sets for different LLMs.
- **Medium Confidence**: The mechanism linking perplexity to evidence comprehensibility is plausible but relies on an indirect proxy rather than direct comprehension validation.
- **Medium Confidence**: The conclusion that current utility estimation methods capture model-agnostic usefulness is supported but could benefit from more diverse estimation approaches.

## Next Checks
1. **Multi-passage Utility Test**: Extend the evaluation framework to measure context-dependent utility by testing how individual passage utility changes when combined with other passages in retrieval sets.
2. **Cross-architecture Transfer Test**: Systematically test utility transfer between fundamentally different architectures (e.g., decoder-only vs. encoder-decoder models) to validate the claimed model-dependency.
3. **Comprehension Proxy Validation**: Replace perplexity with direct human comprehension assessment for a subset of passages to validate whether perplexity truly captures the LLM's ability to leverage evidence.