---
ver: rpa2
title: Rethinking Visual Information Processing in Multimodal LLMs
arxiv_id: '2511.10301'
source_url: https://arxiv.org/abs/2511.10301
tags:
- visual
- tokens
- vision
- arxiv
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces LLaViT, a new approach to multimodal large
  language models (MLLMs) that improves visual information processing within the LLM.
  Instead of treating the LLM solely as a language processor, LLaViT extends its role
  to also serve as an integral part of the visual feature processing pipeline through
  three key modifications: learning separate QKV projections for visual tokens, enabling
  bidirectional attention on visual tokens, and incorporating both local and global
  visual representations.'
---

# Rethinking Visual Information Processing in Multimodal LLMs

## Quick Facts
- **arXiv ID:** 2511.10301
- **Source URL:** https://arxiv.org/abs/2511.10301
- **Reference count:** 30
- **Primary result:** LLaViT achieves 47.6% Vision Centric accuracy (3B) vs 37.5% for baseline LLaVA, with 7B model outperforming 14B LLaVA

## Executive Summary
This paper introduces LLaViT, a novel approach to multimodal large language models that fundamentally reimagines how visual information is processed within the LLM architecture. Rather than treating the LLM purely as a language processor, LLaViT extends its role to actively participate in visual feature processing through three key modifications: modality-specific QKV projections, bidirectional attention for visual tokens, and multi-layer CLIP feature extraction. The approach demonstrates significant performance improvements across 17 benchmarks, with the 7B LLaViT model even outperforming the 14B LLaVA baseline, establishing a promising new direction for MLLM architecture design.

## Method Summary
LLaViT modifies standard LLaVA by adding modality-specific QKV projections for visual tokens (initialized from text weights), implementing bidirectional attention among visual tokens while preserving causal structure for text, and extracting features from multiple CLIP layers (5, 15, 23) to capture both local and global visual information. The model is trained in two stages: pre-training on 622k PixMo-Cap samples to learn visual processing, followed by fine-tuning on LLaVA-1.5 dataset. The architecture uses CLIP ViT-L/14 as vision encoder and various Qwen2.5 LLM sizes as base models.

## Key Results
- Vision Centric accuracy: 47.6% (3B LLaViT) vs 37.5% (3B LLaVA), 53.3% (7B LLaViT) vs 39.3% (7B LLaVA)
- OCR & Chart: 34.7% (7B LLaViT) vs 31.8% (7B LLaVA)
- Knowledge: 38.2% (7B LLaViT) vs 37.8% (7B LLaVA)
- 7B LLaViT outperforms 14B LLaVA baseline on Vision Centric and OCR & Chart tasks

## Why This Works (Mechanism)

### Mechanism 1: Modality-Specific QKV Projections
- **Claim:** Separate QKV projections reduce optimization conflicts between visual and textual representation learning.
- **Mechanism:** Visual tokens have poor alignment with text embeddings at input layer (~0.1 cosine similarity). Separate projections allow dedicated optimization without degrading pre-trained text representations.
- **Core assumption:** Visual and text modalities require different attention dynamics that conflict when shared.
- **Evidence anchors:** Section 2.2 (poor visual-text alignment), Section 3.1 (addresses stability-plasticity tradeoff), weak corpus support from related work on token pruning.

### Mechanism 2: Bidirectional Visual Attention
- **Claim:** Full self-attention among visual tokens improves visual representation refinement.
- **Mechanism:** Removes causal mask for visual tokens while preserving it for text, allowing symmetric interactions since visual patches lack temporal ordering.
- **Core assumption:** Visual token interactions are symmetric and benefit from unconstrained attention.
- **Evidence anchors:** Section 3.2 (severe performance drop when removed: Vision Centric 39.3→24.9, OCR 27.4→10.7), Section 2.3 (visual tokens lack temporal ordering).

### Mechanism 3: Multi-Layer CLIP Features
- **Claim:** Extracting from multiple CLIP layers preserves fine-grained details lost in penultimate-layer-only extraction.
- **Mechanism:** CLIP's penultimate layer prioritizes global semantic alignment. Layers 5, 15, 23 capture local→global spectrum. Concatenation along feature dimension maintains token count and efficiency.
- **Core assumption:** Lower CLIP layers contain spatially precise information that survives projection to LLM dimension.
- **Evidence anchors:** Section 3.3 (struggles with fine-grained details), Table 3 (OCR & Chart improvement: 31.8→34.7), weak corpus support.

## Foundational Learning

- **Concept: QKV Projections in Transformers**
  - **Why needed here:** Essential to understand why separate projections help avoid gradient conflicts between modalities.
  - **Quick check question:** Can you explain why sharing QKV between modalities might create gradient conflicts during backpropagation?

- **Concept: Causal vs Bidirectional Attention Masks**
  - **Why needed here:** Paper modifies attention masks specifically for visual tokens; understanding causal masking is crucial.
  - **Quick check question:** What would happen to autoregressive text generation if all attention were bidirectional?

- **Concept: Feature Hierarchies in Vision Transformers**
  - **Why needed here:** Local/global extraction relies on different abstraction levels in CLIP layers.
  - **Quick check question:** Why does the penultimate layer of CLIP emphasize global semantics over local texture?

## Architecture Onboarding

- **Component map:** Image → CLIP ViT-L/14 (extract layers 5,15,23) → Concat features (3×dV) → MLP projector (3dV → dL) → LLM with [separate visual QKV + bidirectional visual attention] → Output logits

- **Critical path:** Visual token attention updates within LLM layers (Section 2.3 ablation shows this path is essential; removal causes -14.4pp drop on Vision Centric).

- **Design tradeoffs:**
  - +5-12% parameters (Table 4) vs 2× efficiency gain from using smaller base LLM for equivalent performance
  - Minimal FLOPs increase (Table 5: 5348.7→5426.1 GFLOPs for 3B model)
  - Risk: visual QKV may overfit to training distribution if pre-training data is insufficient

- **Failure signatures:**
  - Text-only benchmarks degrade → visual QKV contaminating text representations
  - High-resolution (Any-Res) shows smaller gains → local/global features may be redundant with more tokens
  - Newline token dominates visual token outputs (Figure 5-6) → insufficient alignment training

- **First 3 experiments:**
  1. **Baseline sanity check:** Train standard LLaVA-1.5 on PixMo-Cap + LLaVA-IT to establish comparable baseline numbers for your compute setup.
  2. **Single component ablation:** Add only separate QKV, measure Vision Centric and OCR gains (expect ~5-8pp per Table 3 Row 2).
  3. **Attention mask validation:** Verify bidirectional attention is correctly applied only to Iv indices by logging attention patterns for a sample batch.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the specific selection of intermediate CLIP layers (5th, 15th, 23rd) for local/global feature extraction be replaced by a learned or adaptive mechanism?
- **Basis:** Section 4.1 fixes extraction layers to specific indices without justification for optimality.
- **Why unresolved:** Treated as static hyperparameter; unclear if chosen layers are universally optimal.
- **What evidence would resolve it:** Ablation studies comparing fixed layer extraction against learnable attention-based aggregation across all vision encoder layers.

### Open Question 2
- **Question:** Does LLaViT's separation of visual and text QKV projections fully mitigate the stability-plasticity dilemma regarding the LLM's inherent knowledge?
- **Basis:** Section 3.1 claims addressing the dilemma, but Section 4.2 notes only "modest" gains in Knowledge benchmarks.
- **Why unresolved:** No evaluation on pure text benchmarks to confirm preservation of original language capabilities.
- **What evidence would resolve it:** Evaluation results on standard text-only LLM benchmarks (e.g., MMLU, GSM8K) comparing LLaViT against baseline LLaVA and original text-only LLM.

### Open Question 3
- **Question:** Does the "LLM as extended Vision Transformer" paradigm generalize to alternative vision encoders (e.g., SigLIP, DINOv2)?
- **Basis:** Section 5 states this establishes a "new perspective" and "promising direction," but experiments are limited to CLIP ViT-L/14.
- **Why unresolved:** Architectural modifications motivated by CLIP's specific properties; unclear if solutions are necessary/effective for other encoders.
- **What evidence would resolve it:** Experiments applying LLaViT framework to MLLMs built on non-CLIP vision encoders.

## Limitations
- No direct ablation study isolating each modification's contribution - 12% improvement could be from any combination of changes
- Learning rate configuration for non-visual parameters not fully specified ("follow LLaVA-1.5" without concrete values)
- Local/global feature extraction shows only marginal improvements (3.9 percentage points) on OCR and chart tasks
- Doesn't explore failure modes where visual QKV projections might degrade text-only performance

## Confidence

- **High Confidence:** The claim that LLaViT outperforms LLaVA across benchmarks is well-supported by experimental results. Parameter and FLOPs analysis appears accurate and reproducible.
- **Medium Confidence:** Mechanism explanations for separate QKV projections and bidirectional attention are plausible based on ablation studies but could benefit from more detailed gradient analysis.
- **Low Confidence:** The assertion that the approach "establishes a promising new direction" is somewhat overstated given lack of theoretical grounding and limited exploration of alternative architectural choices.

## Next Checks

1. **Ablation Isolation Test:** Train three separate models - one with only separate QKV projections, one with only bidirectional visual attention, and one with only multi-layer feature extraction. Compare their individual contributions to the 12% improvement to verify gains are additive.

2. **Text-Only Degradation Test:** Evaluate all LLaViT models on standard language benchmarks (e.g., MMLU for text-only) to confirm visual QKV projections don't negatively impact language understanding capabilities.

3. **Attention Pattern Analysis:** Implement logging of attention weight distributions for visual tokens across layers. Verify bidirectional attention is correctly applied and there's no attention leakage from visual to future text tokens.