---
ver: rpa2
title: 'UserRL: Training Interactive User-Centric Agent via Reinforcement Learning'
arxiv_id: '2509.19736'
source_url: https://arxiv.org/abs/2509.19736
tags:
- user
- agent
- training
- environment
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: UserRL addresses the challenge of training interactive, user-centric
  agents in diverse and dynamic multi-turn environments. It introduces a unified framework
  with standardized gym environments and simulated users, enabling systematic training
  and evaluation of user interaction abilities.
---

# UserRL: Training Interactive User-Centric Agent via Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.19736
- Source URL: https://arxiv.org/abs/2509.19736
- Reference count: 40
- Primary result: Introduces UserRL framework for training interactive agents with standardized gym environments and simulated users

## Executive Summary
UserRL addresses the challenge of training interactive, user-centric agents in diverse and dynamic multi-turn environments. It introduces a unified framework with standardized gym environments and simulated users, enabling systematic training and evaluation of user interaction abilities. The framework integrates reinforcement learning with turn-level reward shaping and trajectory-level scoring, using the GRPO algorithm to optimize policy learning. Experiments across Qwen3 models demonstrate that SFT cold start is essential for unlocking initial interaction ability and sustaining RL improvements. Trajectory-level scoring (e.g., R2G) is more effective than fine-grained turn differentiation, and open-source simulated users (e.g., Qwen3-32B) remain cost-effective while enabling strong transfer to stronger evaluators like GPT-4o. Trained models show improved effectiveness and efficiency in multi-turn interactions and even outperform their performance with simulated users when interacting with real users, highlighting the importance of adaptive, collaborative design.

## Method Summary
UserRL introduces a unified framework for training interactive agents through reinforcement learning with standardized gym environments. The framework includes eight gym environments combining rule-based task logic with LLM-based user simulation and reward calculation. The training pipeline uses supervised fine-tuning (SFT) cold start on high-quality trajectories followed by RL optimization via the VERL framework using GRPO. Key innovations include turn-level reward shaping with Equalized and trajectory-level scoring with R2G, which together address credit assignment challenges in multi-turn interactions. The framework enables systematic evaluation across different user simulators and real users, demonstrating transfer learning capabilities.

## Key Results
- SFT cold start is critical for unlocking initial interaction ability and enabling sustained RL improvements
- Trajectory-level scoring (e.g., R2G) is more effective than fine-grained turn differentiation for policy optimization
- Open-source simulated users (e.g., Qwen3-32B) remain cost-effective and transferable to stronger evaluators like GPT-4o
- Models trained on simulated users outperform their simulated user performance when interacting with real users

## Why This Works (Mechanism)

### Mechanism 1: SFT Cold Start Enables RL Policy Improvement
SFT transfers procedural knowledge into the policy distribution, creating a behavioral prior that keeps the policy in regions where reward signals are informative. This enables GRPO's advantage estimation to compute meaningful gradients. The core assumption is that SFT data quality is representative of optimal interaction patterns. Evidence shows models with SFT cold start begin from higher baselines and continue improving, while those without SFT plateau early at lower performance levels.

### Mechanism 2: Trajectory-Level R2G Scoring Captures Turn Interdependence
R2G trajectory scoring outperforms sum-based aggregation by implicitly crediting early exploratory turns that enable later reward-yielding actions. The temporal discounting in R2G computes rewards as $\sum_{j=1}^{T} \gamma^{j-1} r_j$, shaping advantage estimates to favor early progress. This mechanism assumes early progress correlates with interaction efficiency and user experience. Evidence shows equalized/R2G consistently outperforms other training settings.

### Mechanism 3: User Simulation Diversity Enables Transfer to Real Users
Training with open-source simulators transfers to stronger evaluators and real humans because learned interaction skills are simulator-agnostic collaborative patterns. The gym's rule-based task completion combined with LLM-generated user responses creates training diversity, enabling models to learn iterative refinement, feedback adaptation, and exploration-exploitation balance. The core assumption is that interaction skills learned from synthetic users generalize to real humans, who are more cooperative than adversarial.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed: UserRL builds on GRPO, which normalizes advantages across trajectory groups rather than using a learned value function
  - Quick check: Given 8 trajectories with rewards [0.2, 0.5, 0.5, 0.8, 0.3, 0.5, 0.7, 0.4], what is the normalized advantage for the trajectory with reward 0.8?

- **Concept: Reward Shaping for Credit Assignment**
  - Why needed: The paper's core contribution is decoupling turn-level reward shaping from trajectory-level scoring to address sparse rewards
  - Quick check: In a 5-turn interaction where only turn 5 yields reward 1.0, why does naive turn-level reward assignment (zeros for turns 1-4) cause learning failure?

- **Concept: Multi-Turn Markov Decision Processes**
  - Why needed: Gym environments are finite automata with state transitions where the policy conditions on conversation history
  - Quick check: In TelepathyGym, how does the agent's belief state change after receiving "Maybe" vs. "No" responses?

## Architecture Onboarding

- **Component map:** Gym environments (8 total) -> Standardized tool interface (interact_with_env) -> Training pipeline (SFT cold start -> RL via VERL framework) -> Reward modules (turn-level shaper + trajectory-level scorer) -> Token-level advantages

- **Critical path:**
  1. Implement new gym by defining: `reset()`, `step()`, reward function, and LLM user prompt
  2. Generate SFT data by running GPT-4o as agent through gym, ranking trajectories, selecting top-K
  3. Configure reward shaping (start with Equalized turn-level + R2G trajectory-level)
  4. Train with VERL: batch_size=128, n_rollouts=8, max_turns=16, epochs=15

- **Design tradeoffs:**
  - Stronger vs. cheaper user simulators: GPT-4o yields faster learning but costs ~4M requests per training run; Qwen3-32B is cost-effective but may require more epochs
  - R2G vs. Sum trajectory scoring: R2G favors efficiency (early rewards weighted higher); Sum favors completeness (all rewards equal weight)
  - Held-in vs. held-out gyms: Training on 5 gyms, evaluating on 3 held-out tests generalization

- **Failure signatures:**
  - Training collapse: Loss spikes, rewards → 0 → check if naive turn-level rewards used (switch to Equalized)
  - Early plateau: Rewards stabilize below baseline → check if SFT cold start was skipped
  - Poor transfer: Held-out gym performance << held-in → simulated user may be overfitting to specific response patterns

- **First 3 experiments:**
  1. Reproduce Equalized/R2G vs. Equalized/Sum: Train Qwen3-4B on TravelGym+TurtleGym with both configurations
  2. Ablate SFT cold start: Train with and without SFT initialization; plot training curves
  3. Test user simulator transfer: Train with Qwen3-32B simulator, evaluate with GPT-4o

## Open Questions the Paper Calls Out

### Open Question 1
Can adaptive or learned reward shaping mechanisms outperform fixed heuristics like Reward-to-Go (R2G) in capturing the true utility of intermediate interaction turns? The paper states that simple heuristics fail to distinguish productive zero-reward turns from unproductive ones and suggests future research should aim to design adaptive or learned reward shaping mechanisms. Evidence would require demonstrating a learned reward model that successfully differentiates between high-value and low-value exploratory questions, leading to higher final task completion scores than R2G.

### Open Question 2
How can user simulations balance the rigor required for reproducible benchmarking with the flexibility needed to approximate diverse, real-world user behaviors? The paper highlights an inherent tradeoff where rule-based structures ensure fairness but reduce naturalness, calling for future work on balancing the two for robust agent training. Evidence would require developing a user simulation framework incorporating richer user profiles that maintains high inter-run consistency while displaying the diverse strategic and tonal variations seen in human interactions.

### Open Question 3
Can a unified reward formulation jointly optimize for both interaction efficiency (conciseness) and effectiveness (task success) without compromising the user experience? The paper argues that metrics for effectiveness (using many turns) and efficiency (weighted timing) often conflict, suggesting training and evaluation should consider how to balance efficiency and effectiveness. Evidence would require a training run using a composite reward function that maximizes task success while minimizing turns, resulting in a Pareto improvement over the current "Equalized/R2G" baseline in terms of user satisfaction or interaction length.

## Limitations

- The transfer hypothesis from simulated users to real humans rests on a single qualitative observation and lacks systematic robustness testing across diverse user cohorts
- The core mechanism that SFT cold start enables RL improvement is supported only within the UserRL experimental framework and has not been validated on alternative tasks or domains
- The framework's reliance on GPT-4o for high-quality SFT data and evaluation creates potential circularity where the model evaluates itself

## Confidence

- **High confidence:** The necessity of SFT cold start for enabling RL improvement is strongly supported by training curve evidence showing plateau behavior without SFT
- **Medium confidence:** The superiority of trajectory-level R2G scoring over turn-level differentiation is supported by ablation studies but requires validation across more diverse task structures
- **Medium confidence:** The transfer hypothesis from open-source simulators to stronger evaluators and real users is supported by observed performance gains but lacks systematic stress-testing

## Next Checks

1. **Adversarial user transfer test:** Design a set of adversarial real user scenarios (e.g., contradictory preferences, time pressure, unclear instructions) to test whether models trained on cooperative simulated users maintain performance under stress conditions

2. **Cross-domain SFT validation:** Apply the SFT cold start approach to a completely different domain (e.g., medical diagnosis or legal consultation) to verify that the mechanism generalizes beyond the current task set

3. **Temporal reward structure analysis:** Systematically vary the temporal distribution of rewards in gym environments (e.g., require 8 turns of exploration before reward becomes available) to test whether R2G scoring remains superior when early progress is not rewarded