---
ver: rpa2
title: 'RSwinV2-MD: An Enhanced Residual SwinV2 Transformer for Monkeypox Detection
  from Skin Images'
arxiv_id: '2601.01835'
source_url: https://arxiv.org/abs/2601.01835
tags:
- rswinv2
- mpox
- monkeypox
- transformer
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces RSwinV2, a customized Swin Transformer architecture
  designed to enhance monkeypox lesion classification from skin images. The method
  employs a hierarchical transformer structure that splits input images into non-overlapping
  patches and processes them using shifted-window self-attention to improve cross-window
  information exchange while maintaining computational efficiency.
---

# RSwinV2-MD: An Enhanced Residual SwinV2 Transformer for Monkeypox Detection from Skin Images

## Quick Facts
- arXiv ID: 2601.01835
- Source URL: https://arxiv.org/abs/2601.01835
- Reference count: 40
- 96.51% accuracy on monkeypox lesion classification

## Executive Summary
This study introduces RSwinV2, a customized Swin Transformer architecture designed to enhance monkeypox lesion classification from skin images. The method employs a hierarchical transformer structure that splits input images into non-overlapping patches and processes them using shifted-window self-attention to improve cross-window information exchange while maintaining computational efficiency. A key innovation is the incorporation of an Inverse Residual Block (IRB), which integrates convolutional skip connections to address vanishing gradient issues and strengthen local feature extraction. RSwinV2 effectively combines global contextual modeling via multi-head attention with local pattern recognition, improving discrimination between monkeypox and visually similar conditions such as chickenpox, measles, and cowpox. Evaluated on a public Kaggle dataset, RSwinV2 achieves an accuracy of 96.51% and an F1-score of 96.13%, outperforming both standard CNN and SwinTransformer models. These results demonstrate its potential as a reliable computer-assisted diagnostic tool for monkeypox detection.

## Method Summary
RSwinV2 is a hierarchical vision transformer architecture built on Swin Transformer V2. It processes images by splitting them into 16×16 non-overlapping patches (196 tokens for 224×224 input), then applies a series of shifted-window multi-head self-attention blocks. The key innovation is replacing the standard Feed-Forward Network (FFN) with an Inverse Residual Block (IRB), which integrates convolutional skip connections to strengthen local feature extraction and mitigate vanishing gradients. The architecture uses 5-class classification (MPox, Chickenpox, Cowpox, Measles, Normal) trained on a public Kaggle dataset of 16,630 images with 80/20 train/test split. Training employs Adam optimizer (lr=1e-3, weight decay=0.04), cross-entropy loss, batch size 16, dropout 0.3, and learning rate decay of 0.85 every 20 epochs.

## Key Results
- Achieved 96.51% accuracy and 96.13% F1-score on 5-class monkeypox lesion classification
- Outperformed standard CNN and SwinTransformer baselines on the same dataset
- Demonstrated effective integration of global attention mechanisms with local convolutional feature extraction through IRB

## Why This Works (Mechanism)
RSwinV2 works by combining the global contextual modeling capabilities of transformers with enhanced local feature extraction through convolutional skip connections. The shifted-window self-attention mechanism allows information exchange across window boundaries while maintaining computational efficiency. The IRB structure addresses the vanishing gradient problem common in deep transformers by providing direct skip connections that preserve fine-grained spatial details essential for distinguishing between visually similar dermatological conditions.

## Foundational Learning

**Vision Transformers**: Needed to understand hierarchical processing of image patches into tokens for self-attention. Quick check: Verify patch size (16×16) creates correct token count (196) for 224×224 input.

**Shifted-Window Self-Attention**: Needed to comprehend how cross-window information exchange is achieved without quadratic complexity. Quick check: Confirm window shift pattern alternates between stages to enable global context.

**Inverse Residual Block (IRB)**: Needed to understand the convolutional skip connection mechanism that addresses vanishing gradients. Quick check: Verify IRB structure (1×1 conv expansion → 3×3 depthwise conv → 1×1 conv projection) with proper skip connection implementation.

## Architecture Onboarding

**Component Map**: Input Image -> Patch Embedding -> Stage 1-4 (Shifted-Window MSA + IRB) -> Classification Head

**Critical Path**: Patch embedding → shifted-window multi-head self-attention → IRB → CLS token pooling → classification

**Design Tradeoffs**: Global attention provides context but increases computational cost; IRB adds parameters but improves gradient flow and local feature preservation

**Failure Signatures**: Vanishing gradients (training loss plateaus early), overfitting (large train-val gap), class imbalance (biased predictions toward majority class)

**3 First Experiments**:
1. Implement IRB layer and verify gradient flow through skip connections
2. Test shifted-window attention with different window sizes (7×7 vs 8×8)
3. Compare performance with and without IRB to quantify contribution

## Open Questions the Paper Calls Out
- Can RSwinV2 be customized for few-shot learning scenarios with limited labeled training data?
- How does performance degrade on biomedical images with low-contrast patterns and high morphological heterogeneity?
- Does RSwinV2 maintain accuracy when validated on external, multi-center clinical datasets?

## Limitations
- Limited to a single-source Kaggle dataset, raising concerns about generalization
- Unspecified architectural hyperparameters (Swin stages, IRB expansion ratio, embedding dimensions)
- Unclear training details (total epochs, early stopping criteria) affecting reproducibility

## Confidence
- **High**: IRB architectural innovation and clear description
- **Medium**: Reported accuracy (96.51%) due to strong experimental setup but missing reproducibility details
- **Low**: Generalizability claims due to single dataset evaluation

## Next Checks
1. Verify class distribution in exact Kaggle dataset and implement stratified splits to confirm reported balance
2. Implement IRB layer as specified and compare against baseline SwinV2 without IRB
3. Reproduce training curves and metrics using reported hyperparameters to validate 96.51% accuracy claim