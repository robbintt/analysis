---
ver: rpa2
title: 'Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem Proving
  via Reinforcement Learning'
arxiv_id: '2507.08649'
source_url: https://arxiv.org/abs/2507.08649
tags:
- lean
- training
- reasoning
- arxiv
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Leanabell-Prover-V2, a 7B LLM that integrates
  Lean 4 verifier feedback directly into the reasoning process via multi-turn verification
  interactions. The model learns to self-correct proofs based on real-time compilation
  feedback, using reinforcement learning with token-level loss to stabilize training.
---

# Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem Proving via Reinforcement Learning

## Quick Facts
- arXiv ID: 2507.08649
- Source URL: https://arxiv.org/abs/2507.08649
- Reference count: 40
- Primary result: 7B LLM that integrates Lean 4 verifier feedback into reasoning chain, achieving 3.2% and 2.0% improvements in pass@128 on MiniF2F-test over Kimina-Prover-Preview-Distill-7B and DeepSeek-Prover-V2-7B respectively

## Executive Summary
Leanabell-Prover-V2 introduces a 7B LLM that learns formal theorem proving through direct integration with the Lean 4 verifier. The model generates proofs, receives compilation feedback, and learns to self-correct through iterative multi-turn interactions. Using reinforcement learning with token-level loss (DAPO), the system achieves state-of-the-art performance on formal theorem proving benchmarks, demonstrating that verifier-integrated reasoning enables 7B models to develop reflective error-correction capabilities previously limited to larger models.

## Method Summary
The method employs a two-stage training pipeline: cold-start supervised fine-tuning on synthesized correction pairs (~7K samples) followed by reinforcement learning with DAPO algorithm. The model learns to generate proofs in Lean 4, receives structured compiler feedback, and produces corrected versions. Verifier feedback tokens are masked during training to prevent gradient interference. The RL stage uses token-level policy gradients to stabilize training on variable-length chain-of-thought trajectories, with multi-turn verification interactions enabling self-correction learning.

## Key Results
- 3.2% improvement in pass@128 on MiniF2F-test over Kimina-Prover-Preview-Distill-7B
- 2.0% improvement in pass@128 on MiniF2F-test over DeepSeek-Prover-V2-7B
- Strong generalization to ProofNet and ProverBench benchmarks
- First correction iteration provides +3.7% improvement over Kimina-base

## Why This Works (Mechanism)

### Mechanism 1: Multi-turn Verifier Feedback Enables Self-Correction Learning
- **Claim:** Integrating Lean 4 verifier feedback into the reasoning chain allows 7B models to learn reflective error correction that they cannot acquire from single-pass generation alone.
- **Mechanism:** The model generates an initial proof, receives structured error messages from the Lean 4 compiler (e.g., "type mismatch at line 12"), and produces a corrected proof. RL optimizes the conditional probability of successful correction given feedback: $\max_\theta \mathbb{E}[P(\hat{y}_i=1)]$ where $\hat{y}_i=1(\hat{p}_i|p_i, o_i, s)$. This creates a learning signal from failures that would otherwise be discarded.
- **Core assumption:** The base model has sufficient formal reasoning capability to understand error messages and map them to correct repairs; verifier feedback is sufficiently informative to guide corrections.
- **Evidence anchors:**
  - [abstract] "verifier feedback...allows the LLM to become 'self-aware' of the correctness of its own reasoning process and learn to reflexively correct errors"
  - [Section 3.1, Table 3] First iteration of correction improves Kimina-based model from 64.7% to 68.4% (+3.7%) on MiniF2F-test
  - [corpus] Seed-Prover and ProofNet++ similarly leverage verification feedback for self-correction, suggesting convergent evidence
- **Break condition:** If the base model's error diagnosis capability is too weak, or if verifier messages are too cryptic, the correction signal becomes noise.

### Mechanism 2: Feedback Token Masking Stabilizes Multi-modal Sequence Training
- **Claim:** Masking verifier-generated tokens during loss computation prevents gradient interference from non-model-generated content.
- **Mechanism:** Verifier feedback tokens (between `<interpreter></interpreter>`) are excluded from both SFT and RL loss calculations. This ensures the model only learns to predict its own outputs (reasoning + corrected code), not to predict external system messages. Without masking, the model would receive conflicting signals trying to predict deterministic compiler output.
- **Core assumption:** The model can condition on verifier tokens at inference time without needing to learn their distribution; the useful learning signal is in the correction behavior, not feedback prediction.
- **Evidence anchors:**
  - [Section 2.2] "feedback token masking strategy that focuses solely on predicting the rewritten portions"
  - [Section 2.3] "mask the token sequences provided by the Lean 4 verifier for stable RL training"
  - [corpus] No direct corpus comparison found; this appears to be a design choice without ablation in this paper
- **Break condition:** If verifier feedback format varies unpredictably, the model may fail to parse it correctly at inference time despite masking during training.

### Mechanism 3: Token-Level Loss (DAPO) Prevents Collapse on Long Sequences
- **Claim:** Token-level policy gradient loss is necessary for stable RL on long CoT trajectories; sample-level averaging causes training collapse.
- **Mechanism:** DAPO computes gradients per-token with uniform weighting, rather than averaging loss across tokens within each sample. For variable-length responses (long CoT can reach 16K tokens), sample-level averaging disproportionately down-weights learning signals from longer sequences. Token-level loss maintains gradient contribution per reasoning step regardless of total length.
- **Core assumption:** All tokens in the reasoning trajectory contribute meaningful learning signal; longer responses are not inherently noisier.
- **Evidence anchors:**
  - [Appendix C] "sample-level loss calculation...proves problematic for this verifier-integrated reasoning task, where responses vary significantly in length"
  - [Appendix C, Figure 11] GRPO with sample-level loss shows training instability (entropy collapse pattern)
  - [corpus] Weak external validation; DAPO is recent (2025) with limited independent replication
- **Break condition:** If specific token positions (e.g., early vs. late in sequence) have systematically different noise profiles, uniform token weighting may be suboptimal.

## Foundational Learning

- **Concept: Policy Gradient with Clipping (PPO-family)**
  - Why needed here: DAPO builds on PPO-style clipped surrogate objectives. Understanding the clipping mechanism ($1-\epsilon_{low}, 1+\epsilon_{high}$) and advantage normalization is prerequisite to debugging RL instability.
  - Quick check question: Can you explain why asymmetric clipping bounds (0.2 low, 0.28 high in this paper) might help prevent entropy collapse?

- **Concept: Lean 4 Tactic State and Proof Goals**
  - Why needed here: The verifier feedback references proof states (e.g., "no goals to be solved"). Interpreting error messages requires basic literacy in Lean's goal-directed proof paradigm.
  - Quick check question: What does the error "no goals to be solved" indicate about the model's generated tactic?

- **Concept: Cold-Start SFT before RL**
  - Why needed here: The two-stage pipeline (SFT → RL) is critical. SFT teaches format compliance (special tokens for code/interpreter blocks); RL optimizes for correctness. Skipping cold-start would cause format violations that break the verifier loop.
  - Quick check question: Why might RL alone fail to learn the `<code></code>` formatting convention without SFT pretraining?

## Architecture Onboarding

- **Component map:**
  - Cold-start SFT module -> RL environment (DAPO) -> Verifier service -> Iteration controller

- **Critical path:**
  1. Start from pretrained prover (Kimina-Prover-Distill-7B or DeepSeek-Prover-V2-7B)
  2. Run cold-start SFT on correction data (3 epochs, lr=2e-5)
  3. Filter training theorems by difficulty (pass@8 between 1/8 and 1/2)
  4. Run RL training with DAPO (lr=1e-6, rollout=24, max_tokens=16K)
  5. At inference, enable iterative verification (1-3 correction rounds)

- **Design tradeoffs:**
  - **Rollout size vs. compute:** Paper uses 24 rollouts (matching pass@32 evaluation). Larger rollouts improve exploration but increase latency.
  - **Simple vs. structured rewards:** Paper tested tactic-count, automation-level, and state-change rewards (Appendix B). Simple success/failure (±1.0) outperformed structured rewards. **Assumption:** Structured rewards may require more tuning or different base models.
  - **Iteration depth:** First correction gives +3.7% (Kimina); diminishing returns after 2-3 iterations. Tree branching (4 rollouts per iteration) adds +0.4% but increases compute 4x.

- **Failure signatures:**
  - **Entropy collapse:** Indicates LR too high or clipping bounds misconfigured; check Figure 5 entropy curve for healthy slow decline
  - **Format violations:** Model outputs code without `<code>` tags → cold-start insufficient or LR too high during RL
  - **Zero verifier use rate:** Model ignores verification prompt → cold-start data quality issue

- **First 3 experiments:**
  1. **Verify cold-start format compliance:** Sample 50 outputs from post-SFT model; check all contain properly formatted `<code>` blocks. Target: >95% compliance.
  2. **Ablate token-level vs. sample-level loss:** Train two models with DAPO and GRPO losses on same data subset; compare entropy stability curves over 50 steps. Expect GRPO to show collapse.
  3. **Measure iteration gain curve:** Run inference with 1, 2, 3 correction iterations on MiniF2F-test subset (100 problems); plot cumulative pass@32 improvement. Expect diminishing returns; identify optimal iteration budget.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can elaborate rewards based on Abstract Syntax Tree (AST) feedback help models learn more effective reflection capabilities during RL training?
- Basis in paper: [explicit] The authors state: "This leaves behind a crucial question: whether we can design elaborate rewards based on Abstract Syntax Tree (AST) feedback (See Figure 9 in Appendix) to help models learn effective reflection capabilities in RL training." They investigated structured rewards (tactic count, automation level, state change efficiency) but "have not obtained favorable results."
- Why unresolved: The explored fine-grained rewards ($\lambda_{tc}$, $\lambda_{at}$, $\lambda_{sc}$) either showed only slight gains or performed worse than simple success/failure rewards, leaving the potential of AST-based rewards unproven.
- What evidence would resolve it: Demonstrated improvements in pass@k metrics on MiniF2F or ProofNet using successfully integrated AST-based structured rewards that outperform binary success/failure rewards.

### Open Question 2
- Question: How can reinforcement learning methods effectively enhance subgoal decomposition efficiency for complex formal theorem proving?
- Basis in paper: [explicit] The authors state: "how to effectively leverage RL methods to enhance subgoal decomposition efficiency remains an open research question." They note prompt-based approaches yielded "consistently low success rates."
- Why unresolved: While DeepSeek-Prover-V2 demonstrated subgoal decomposition is valuable, the authors' attempts at prompt-based decomposition failed, and RL-based approaches have not yet been successfully developed for this purpose.
- What evidence would resolve it: An RL-based subgoal decomposition method that achieves higher success rates than prompt-based approaches on complex theorem benchmarks (e.g., IMO-level problems).

### Open Question 3
- Question: How can verifier-integrated RL frameworks maintain effectiveness for very strong base models where pass-rate filtering yields limited active training prompts?
- Basis in paper: [inferred] The authors note as a limitation that "for very strong base models (such as DeepSeek-Prover-V2-7B), our RL framework may suffer from limited active training prompts used during training, making our benefit shows a diminishing trend." The proposed solution is "partial rollout."
- Why unresolved: Strong models solve or fail most problems completely, leaving few problems with intermediate pass rates suitable for RL exploration; partial rollout strategies remain unexplored in this work.
- What evidence would resolve it: Successful application of partial rollouts (concatenating hints like decomposed subgoals or partial proofs) that restores significant RL training gains for strong base models on challenging benchmarks.

## Limitations
- The effectiveness of token-level loss (DAPO) lacks independent validation, as DAPO was introduced concurrently with this work and has limited external replication.
- The filter thresholds for RL training data (pass@8 between 1/8 and 1/2) are not justified empirically—tuning these parameters could significantly affect results but their sensitivity is not explored.
- The contribution of each mechanism is entangled: the +3.2% improvement could stem from verification feedback, correction learning, cold-start SFT, or DAPO training algorithm. Ablation studies isolating these components are absent.

## Confidence

- **High Confidence:** The mechanism of multi-turn verifier feedback enabling self-correction is well-supported by the +3.7% improvement on Kimina-base and consistent with Seed-Prover/ProofNet++ approaches. The feedback token masking strategy is straightforward and logically necessary.
- **Medium Confidence:** The specific hyperparameter choices (rollout=24, clipping bounds, reward values) appear effective but their optimality is not established. The training pipeline is reproducible but sensitive to base model quality and data filtering.
- **Low Confidence:** The DAPO algorithm's superiority over sample-level GRPO is primarily demonstrated through entropy curves that may not translate directly to downstream performance. Independent verification of the +2.0% gain over DeepSeek-Prover-V2-7B is limited.

## Next Checks
1. **Ablation of Training Algorithms:** Train three versions on identical data: (a) DAPO with token-level loss, (b) GRPO with sample-level loss, (c) SFT-only baseline. Compare both entropy stability and pass@128 on MiniF2F-test to isolate the impact of token-level loss.
2. **Filter Threshold Sensitivity:** Train models with RL data filtered at pass@8 thresholds of 1/16–1/4, 1/8–1/2, and 1/4–3/4. Measure the effect on final pass@128 performance to determine optimal difficulty range.
3. **Base Model Dependency:** Repeat the full pipeline (SFT+RL) using a weaker base prover (e.g., Qwen2.5-Coder-7B-fine-tuned for Lean). Compare improvement magnitude to assess whether gains are additive or compound with base capability.