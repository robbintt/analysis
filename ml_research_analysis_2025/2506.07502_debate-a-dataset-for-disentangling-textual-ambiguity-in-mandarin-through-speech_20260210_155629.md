---
ver: rpa2
title: 'DEBATE: A Dataset for Disentangling Textual Ambiguity in Mandarin Through
  Speech'
arxiv_id: '2506.07502'
source_url: https://arxiv.org/abs/2506.07502
tags:
- speech
- dataset
- ambiguity
- debate
- disambiguation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "DEBATE introduces the first dataset for Disentangling Textual\
  \ Ambiguity in Mandarin Through Speech, containing 1,001 ambiguous utterances spoken\
  \ by 10 native speakers (10k+ recordings, 9.66 hours). The dataset focuses on how\
  \ speech cues\u2014pronunciation, pauses, stress, and intonation\u2014resolve ambiguity\
  \ in polyphonic characters, structural parsing, and semantic focus."
---

# DEBATE: A Dataset for Disentangling Textual Ambiguity in Mandarin Through Speech

## Quick Facts
- arXiv ID: 2506.07502
- Source URL: https://arxiv.org/abs/2506.07502
- Reference count: 40
- DEBATE introduces the first dataset for Disentangling Textual Ambiguity in Mandarin Through Speech, containing 1,001 ambiguous utterances spoken by 10 native speakers (10k+ recordings, 9.66 hours)

## Executive Summary
DEBATE addresses the challenge of resolving textual ambiguity in Mandarin through speech by creating a novel dataset that captures how pronunciation, pauses, stress, and intonation disambiguate polyphonic characters, structural parsing, and semantic focus. The dataset contains 1,001 ambiguous utterances with over 10,000 recordings totaling 9.66 hours of speech from 10 native Mandarin speakers. Through structured data collection and quality validation via ASR character error rates, DEBATE provides a foundation for advancing machine understanding of spoken intent in Mandarin. Benchmarking shows significant gaps between current large speech-language models and human performance, highlighting the dataset's potential to drive research in this domain.

## Method Summary
The authors constructed DEBATE through a structured data collection pipeline involving 10 native Mandarin speakers who recorded 1,001 ambiguous utterances, resulting in over 10,000 recordings totaling 9.66 hours. The dataset focuses on three types of textual ambiguity: polyphonic characters, structural parsing, and semantic focus. Quality validation was performed using ASR character error rates to ensure high-quality recordings. The dataset was then used to benchmark three large speech-language models (Qwen2-audio, Qwen2.5-omni, and Gemini 2.0 Flash) across disambiguation tasks, with results showing accuracy ranging from 51.6% to 68.1%, substantially below human performance levels.

## Key Results
- DEBATE is the first dataset specifically designed for disentangling textual ambiguity in Mandarin through speech
- The dataset contains 1,001 ambiguous utterances with over 10,000 recordings from 10 native speakers (9.66 hours total)
- Benchmarking three large speech-language models shows accuracy of 51.6%-68.1% across tasks, significantly below human performance

## Why This Works (Mechanism)
DEBATE works by capturing the prosodic and acoustic features of speech that humans naturally use to resolve ambiguity in Mandarin. The dataset leverages the fact that Mandarin's polyphonic characters and syntactic ambiguities can be disambiguated through speech cues including pronunciation variations, strategic pauses, stress patterns, and intonation contours. By systematically collecting these speech variations from native speakers and pairing them with ambiguous textual forms, the dataset creates a direct mapping between acoustic features and semantic resolution, enabling models to learn the relationship between how something is said and what it means.

## Foundational Learning
- **Mandarin polyphonic characters**: Characters with multiple pronunciations depending on meaning - needed to understand core ambiguity type; quick check: verify dataset includes common polyphonic characters like 行 (xíng/háng) in context
- **Speech prosody and disambiguation**: How pitch, stress, and rhythm resolve ambiguity - needed to connect acoustic features to meaning; quick check: examine annotation scheme for prosodic features
- **Speech-language model architecture**: Models that process both speech and text modalities - needed to understand benchmarked systems; quick check: verify model input/output specifications
- **ASR character error rate validation**: Using automatic speech recognition to validate recording quality - needed to ensure dataset reliability; quick check: review ASR validation methodology and thresholds
- **Cross-modal ambiguity resolution**: Using multiple modalities to resolve ambiguity - needed to contextualize DEBATE's contribution; quick check: compare with related cross-modal datasets

## Architecture Onboarding

Component map: Text Ambiguity Generator -> Native Speaker Recording Pipeline -> Quality Validation (ASR) -> Benchmark Models (Qwen2-audio, Qwen2.5-omni, Gemini 2.0 Flash) -> Performance Evaluation

Critical path: The critical path flows from generating ambiguous text instances, through collecting speech recordings from native speakers, validating quality via ASR, and finally benchmarking models on disambiguation tasks. Each stage depends on the successful completion of the previous stage.

Design tradeoffs: The dataset prioritizes quality over quantity with 10 speakers and 1,001 utterances versus larger but potentially noisier datasets. The focus on native speakers ensures authentic prosodic patterns but limits scalability. Using ASR for quality validation provides objective metrics but may miss nuanced linguistic features.

Failure signatures: Model performance below 70% accuracy indicates fundamental challenges in capturing prosodic disambiguation cues. High ASR error rates would suggest recording quality issues. Poor inter-annotator agreement would indicate ambiguity in the task definitions themselves.

First experiments:
1. Test individual model components (speech encoder vs. language decoder) to identify performance bottlenecks
2. Evaluate model performance on each disambiguation type separately (polyphonic, structural, semantic) to identify specific weaknesses
3. Conduct human evaluation studies to establish baseline human performance metrics for comparison

## Open Questions the Paper Calls Out
None

## Limitations
- The dataset is relatively small (1,001 utterances, 10 speakers) compared to other speech-language datasets, potentially limiting generalizability
- The benchmarking results show substantial gaps between model and human performance, but human performance evaluation methodology is not detailed
- The specific distribution of performance across disambiguation types is not broken down, making it difficult to assess which aspects pose the greatest challenges

## Confidence
High confidence in dataset construction methodology and quality validation claims
Medium confidence in benchmarking results and their implications for model limitations

## Next Checks
1. Conduct inter-annotator agreement studies on the human validation data to establish reliability metrics for the dataset labels
2. Perform ablation studies with models trained on subsets of the disambiguation types to identify which aspects contribute most to the performance gap
3. Test model performance on a held-out test set of speakers not included in the original 10 to assess speaker generalization