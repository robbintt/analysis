---
ver: rpa2
title: 'CARE: A QLoRA-Fine Tuned Multi-Domain Chatbot With Fast Learning On Minimal
  Hardware'
arxiv_id: '2503.14136'
source_url: https://arxiv.org/abs/2503.14136
tags:
- care
- medical
- dataset
- support
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CARE, a lightweight multi-domain chatbot
  built by fine-tuning the Phi3.5-mini model using QLoRA on minimal hardware. The
  authors address the challenge of creating efficient domain-specific chatbots by
  fine-tuning a single model across three domains: telecommunications, medical, and
  banking support.'
---

# CARE: A QLoRA-Fine Tuned Multi-Domain Chatbot With Fast Learning On Minimal Hardware

## Quick Facts
- arXiv ID: 2503.14136
- Source URL: https://arxiv.org/abs/2503.14136
- Authors: Ankit Dutta; Nabarup Ghosh; Ankush Chatterjee
- Reference count: 17
- Single model fine-tuned on three domains achieves state-of-the-art medical domain performance while requiring minimal computational resources

## Executive Summary
CARE is a lightweight multi-domain chatbot developed by fine-tuning the Phi3.5-mini model using QLoRA on minimal hardware. The system addresses the challenge of creating efficient domain-specific chatbots by training a single model across telecommunications, medical, and banking support domains. Using downsampled datasets and supervised fine-tuning with LoRA adapters, CARE achieves strong performance on medical benchmarks while requiring only a Tesla P100 GPU for training. The model demonstrates 78.1% accuracy on MMLU Clinical Knowledge and 76.4% on PubMedQA, surpassing comparable models like Llama-3.2-3b-Instruct and Gemma-2-2b-it.

## Method Summary
CARE was developed by fine-tuning the Phi3.5-mini model using QLoRA (Quantized Low-Rank Adaptation) on downsampled datasets from three domains: telecommunications, medical, and banking support. The training process utilized supervised fine-tuning with LoRA adapters to optimize the model for domain-specific tasks while maintaining computational efficiency. The entire training pipeline was executed on a Tesla P100 GPU, requiring only 118 training steps to achieve a final loss of 1.73. The downsampling approach enabled efficient training across multiple domains while preserving the model's ability to handle domain-specific queries effectively.

## Key Results
- Achieves state-of-the-art medical domain performance with 78.1% accuracy on MMLU Clinical Knowledge
- Demonstrates 76.4% accuracy on PubMedQA, surpassing comparable models like Llama-3.2-3b-Instruct and Gemma-2-2b-it
- Successfully fine-tuned on minimal hardware (Tesla P100) with only 118 training steps and final loss of 1.73

## Why This Works (Mechanism)
The effectiveness of CARE stems from the combination of QLoRA fine-tuning with domain-specific adaptation. By using LoRA adapters, the model can efficiently learn domain-specific patterns without requiring full fine-tuning of all parameters. The downsampling of training data allows for rapid convergence while maintaining generalization across multiple domains. The Phi3.5-mini architecture provides a strong foundation for instruction-following capabilities, which are enhanced through the supervised fine-tuning process on domain-specific datasets.

## Foundational Learning
- **QLoRA Fine-Tuning**: Why needed - Enables efficient parameter-efficient fine-tuning on limited hardware; Quick check - Verify memory usage stays under GPU capacity during training
- **LoRA Adapters**: Why needed - Allows selective adaptation of model parameters for domain-specific learning; Quick check - Monitor adapter weight changes during training
- **Domain Adaptation**: Why needed - Enables single model to handle multiple specialized domains; Quick check - Test model performance across all three target domains
- **Supervised Fine-Tuning**: Why needed - Provides structured learning from labeled domain-specific data; Quick check - Evaluate training loss convergence over steps
- **Data Downsampling**: Why needed - Reduces computational requirements while maintaining performance; Quick check - Compare performance with full dataset versions
- **Multi-Domain Training**: Why needed - Enables unified model for multiple customer support scenarios; Quick check - Assess cross-domain knowledge transfer

## Architecture Onboarding
- **Component Map**: Phi3.5-mini base model -> QLoRA quantization -> LoRA adapter fine-tuning -> Domain-specific output layers
- **Critical Path**: Input text -> Base model processing -> LoRA-adapted layers -> Domain-specific response generation
- **Design Tradeoffs**: Downsampled datasets vs. full coverage, single model vs. domain-specific models, computational efficiency vs. potential accuracy loss
- **Failure Signatures**: Domain confusion when queries overlap between categories, reduced accuracy on out-of-distribution examples, potential overfitting to downsampled training data
- **First Experiments**:
  1. Test model response quality on held-out examples from each domain
  2. Measure inference latency on target hardware configurations
  3. Evaluate cross-domain performance to identify knowledge transfer effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to medical domain due to lack of standardized banking and telecommunications datasets
- Training conducted on relatively small number of steps (118) raising questions about convergence
- Downsampled datasets may not capture full complexity of real-world customer support scenarios

## Confidence
- High confidence: Technical implementation of QLoRA fine-tuning and medical domain benchmark results
- Medium confidence: Multi-domain capability claims with limited cross-domain evaluation
- Low confidence: Generalizability to real-world deployment scenarios across all three domains

## Next Checks
1. Conduct systematic evaluation of CARE's performance on standardized banking and telecommunications customer support datasets
2. Perform extended training experiments with varying numbers of steps to verify convergence behavior
3. Replicate medical domain performance evaluation on alternative hardware configurations to validate "minimal hardware" claims