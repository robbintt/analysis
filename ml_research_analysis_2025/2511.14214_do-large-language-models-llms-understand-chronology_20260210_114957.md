---
ver: rpa2
title: Do Large Language Models (LLMs) Understand Chronology?
arxiv_id: '2511.14214'
source_url: https://arxiv.org/abs/2511.14214
tags:
- presidents
- ordering
- reasoning
- gpt-5
- list
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models are increasingly used in finance and economics,
  where prompt-based attempts against look-ahead bias implicitly assume that models
  understand chronology. We test this fundamental question with a series of chronological
  ordering tasks with increasing complexities over facts the model already knows from
  pre-training.
---

# Do Large Language Models (LLMs) Understand Chronology?

## Quick Facts
- **arXiv ID**: 2511.14214
- **Source URL**: https://arxiv.org/abs/2511.14214
- **Authors**: Pattaraphon Kenny Wongchamcharoen; Paul Glasserman
- **Reference count**: 5
- **Primary result**: Reasoning effort threshold critical for flawless chronological ordering; filtering bottlenecks disappear with extended thinking

## Executive Summary
This paper investigates whether large language models truly understand chronology by testing their ability to order known facts across increasingly complex tasks. Through systematic experiments with GPT-4.1, Claude-3.7 Sonnet (with and without Extended Thinking), and GPT-5, the authors reveal that LLMs can maintain local temporal coherence but struggle with global consistency as sequence lengths increase. The study demonstrates that allocating explicit reasoning budget—particularly for GPT-5 at medium/high effort—enables flawless chronological ordering and perfect conditional sorting, whereas minimal effort results in performance identical to earlier models. These findings have important implications for finance applications where prompt-based look-ahead bias prevention assumes chronological understanding.

## Method Summary
The authors evaluate LLMs on chronological ordering tasks using knowledge-verified item pools to isolate reasoning failures from knowledge gaps. They test three task types: basic chronological ordering of increasing list lengths (5, 10, 20 items), conditional sorting (filter then order), and anachronism detection. Models are tested with varying reasoning efforts (minimal, low, medium, high) and with/without Extended Thinking capabilities. Performance is measured using exact match rate, Spearman's rank correlation, Kendall's tau, and Cayley distance. All code and evaluation templates are released for reproducibility.

## Key Results
- Exact match rate drops sharply as sequences lengthen (0.00 for n≥20) while rank correlations stay high (ρ > 0.93), revealing local coherence despite global ordering failures
- GPT-5 at medium/high reasoning effort achieves flawless chronological ordering (exact-match = 1.00) and perfect conditional sorting, while minimal effort degrades identically to earlier models
- Filtering failures dominate conditional sorting tasks, with extended thinking eliminating the bottleneck (0.02 → 0.98 accuracy on OHIOORVIRGINIA criterion)
- Anachronism detection is easiest but performance still declines with overlapping timelines or entities

## Why This Works (Mechanism)

### Mechanism 1: Reasoning Budget Enables Global Timeline Consistency
- **Claim**: Allocating explicit reasoning tokens transforms LLMs from locally-accurate but globally-inconsistent sorters into flawless chronological reasoners, but only when reasoning effort reaches a sufficient threshold.
- **Mechanism**: The extended thinking scratchpad allows models to enumerate candidates, verify membership against constraints, perform pairwise chronology checks, and deduplicate before emitting output—separating computation from presentation reduces early-commitment errors.
- **Core assumption**: Models possess the knowledge; failure is computational, not informational.
- **Evidence anchors**:
  - [abstract]: "GPT-5 at medium/high reasoning effort achieving flawless ordering at all lengths and perfect conditional sorting"
  - [Section 5.3.1]: "Claude 3.7 with ET achieves perfect chronological ordering across all list sizes (exact-match = 1.00 at every n)"
  - [corpus]: Weak direct corpus support; neighbor papers address sorting algorithms but not reasoning-budget effects.

### Mechanism 2: Local Coherence Persists Despite Global Collapse
- **Claim**: Even when exact match rate collapses to zero, rank correlations remain high (ρ > 0.9), indicating LLMs preserve local pairwise ordering while failing to maintain a single globally consistent timeline.
- **Mechanism**: Models anchor on salient endpoints (early/late figures) and maintain relative ordering within temporal neighborhoods, but cannot reliably chain local decisions into a globally consistent permutation.
- **Core assumption**: High rank correlations on surviving pairs mask omission and hallucination errors in longer lists.
- **Evidence anchors**:
  - [abstract]: "Exact match rate drops sharply as sequences lengthen even while rank correlations stay high"
  - [Section 4.1]: Table 1 shows exact match at 0.00 for n≥20 while Spearman's ρ remains 0.93–0.96
  - [corpus]: No direct corpus corroboration for this local/global split pattern.

### Mechanism 3: Conditional Sorting Bottleneck Is Filtering, Not Ordering
- **Claim**: When tasks require filtering then ordering, failures concentrate almost entirely in the filtering step; once the correct subset is identified, ordering accuracy is comparable to given-subset baselines.
- **Mechanism**: Single-prompt filter-and-sort creates premature commitment to incomplete or over-selected sets; reasoning models with scratchpads can enumerate, verify membership, and correct before output.
- **Core assumption**: Filtering failures reflect limited working memory or early token commitment, not knowledge gaps.
- **Evidence anchors**:
  - [abstract]: "most failures stem from the filtering step rather than the ordering step"
  - [Section 6.2.1]: GPT-4.1 achieved only 2% perfect filtering on FIRSTNAMESSTARTINGWITHABORC; 0% on OHIOORVIRGINIA
  - [corpus]: No corpus papers examine filter-then-order decomposition.

## Foundational Learning

- **Concept: Exact Match Rate vs. Rank Correlation**
  - **Why needed here**: These metrics tell opposite stories—high correlations mask ordering failures; only exact match captures global consistency.
  - **Quick check question**: If a model achieves ρ = 0.96 on a 20-item list but exact match = 0.00, what does this reveal about its temporal reasoning?

- **Concept: Extended Thinking / Reasoning Budget**
  - **Why needed here**: The paper's central intervention; controls whether models get a private scratchpad for deliberation before output.
  - **Quick check question**: Why would separating thinking from outputting reduce early-commitment errors in conditional filtering tasks?

- **Concept: Look-Ahead Bias in Finance Applications**
  - **Why needed here**: The paper's motivating problem—prompt-based attempts to wall off future information implicitly assume chronological understanding.
  - **Quick check question**: If a model cannot reliably sort 15 historical events it knows, can prompt instructions like "use only information from before 2016" be trusted?

## Architecture Onboarding

- **Component map**: Knowledge verification query → filtered item pool → shuffled list → model ordering → evaluation (exact match, ρ, τ, Cayley distance)
- **Critical path**: Knowledge verification is non-negotiable; unverified items confound reasoning failures with knowledge gaps. All reported results use only knowledge-verified lists.
- **Design tradeoffs**:
  - Reasoning budget (cost/latency) vs. accuracy: Medium/high effort required for flawless ordering; minimal effort fails identically to non-reasoning models.
  - Self-filter vs. given-subset: Self-filter adds complexity but may improve ordering via enforced deliberation—only viable with reasoning models.
  - Temperature: ET requires T=1.0; non-ET baselines use T=0 for determinism.
- **Failure signatures**:
  - U-shaped per-position accuracy: Strong at list boundaries, weak in middle (anchoring effects).
  - Hallucination pattern: Obscure mid-19th-century presidents omitted; recent salient presidents incorrectly added.
  - Filtering over-selection: Models tend to include extras rather than omit ground-truth items.
- **First 3 experiments**:
  1. Replicate basic sorting on 20th-century events at n ∈ {5, 10, 20} with GPT-4.1; verify exact match collapse with high rank correlations.
  2. Compare Claude 3.7 with/without ET on conditional sorting (OHIOORVIRGINIA criterion); confirm filtering bottleneck disappears with ET.
  3. Test GPT-5 across reasoning efforts (minimal/low/medium/high) on president ordering; identify the threshold where exact match reaches 1.00.

## Open Questions the Paper Calls Out
None

## Limitations
- Findings based on controlled lab tasks; real-world chronological reasoning may involve longer sequences, ambiguous boundaries, and noisy data
- Exact reasoning-effort threshold for flawless performance may vary across domains and sequence complexities
- Self-filtering benefits observed for historically salient entities may not generalize to less salient chronology (corporate filings, macroeconomic series)
- Unclear whether reasoning budget gains are due to improved filtering, better ordering, or both in combination

## Confidence

- **High confidence**: Extended thinking improves conditional filtering accuracy and eliminates the filtering bottleneck. Local coherence persists despite global ordering collapse (high rank correlations with zero exact match).
- **Medium confidence**: Medium/high reasoning effort is necessary and sufficient for flawless chronological ordering across all tested lengths; the exact minimal sufficient reasoning effort may vary with sequence complexity.
- **Low confidence**: Generalizability of self-filtering benefits to non-salient entities; whether gains are purely computational (working memory) vs. informational (deeper reasoning).

## Next Checks

1. **Cross-domain filtering test**: Replicate conditional sorting on financial time series (e.g., S&P 500 constituents meeting market-cap criteria) to confirm self-filtering benefits beyond historically salient entities.
2. **Reasoning-effort sensitivity**: Systematically vary reasoning effort (minimal/low/medium/high) on 30-item sequences to pinpoint the minimal budget yielding perfect exact match, measuring cost-benefit trade-offs.
3. **Noisy chronology experiment**: Introduce ambiguous or overlapping dates (e.g., fiscal quarters spanning years) to test whether high reasoning effort still ensures global consistency when local pairwise ordering is uncertain.