---
ver: rpa2
title: Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings
arxiv_id: '2508.00632'
source_url: https://arxiv.org/abs/2508.00632
tags:
- content
- assets
- feedback
- generation
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of generating interactive multimedia
  content like video games, which requires handling multiple modalities (text, audio,
  video, images, 3D models) and human interaction. Current AI methods struggle with
  this complexity, particularly for content that typically requires teams of humans
  and artists working for months.
---

# Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings

## Quick Facts
- **arXiv ID:** 2508.00632
- **Source URL:** https://arxiv.org/abs/2508.00632
- **Reference count:** 40
- **Key outcome:** AVR-Agent significantly improves multimedia content quality over one-shot generation, with best-of-k selection outperforming extended iterations, though current models struggle to leverage assets and feedback effectively.

## Executive Summary
This paper tackles the challenge of generating interactive multimedia content like video games, which requires handling multiple modalities (text, audio, video, images, 3D models) and human interaction. Current AI methods struggle with this complexity, particularly for content that typically requires teams of humans and artists working for months.

The core method introduces AVR-Eval, a relative evaluation metric for multimedia quality using Audio-Visual Recordings (AVRs). An omni-modal model processes the AVRs of two contents and compares them, with a text model reviewing the evaluation to determine superiority. The paper also presents AVR-Agent, a multi-agent framework that generates JavaScript games and animations from a bank of multimedia assets. The framework uses a coding agent to select assets, generate initial code, employ AVR-Eval to identify the best version, and iteratively improve it through omni-modal agent feedback from the AVR.

## Method Summary
The AVR-Agent framework generates JavaScript games and animations using a multi-agent approach. It starts with asset selection from a bank of multimedia files, then generates multiple initial code candidates. AVR-Eval compares audio-visual recordings of these candidates using an omni-modal model (Qwen2.5-Omni-7B) with text review (Qwen3-32B), selecting the best initial implementation via best-of-k. The framework then iteratively improves the content using AVR feedback and console logs from browser rendering. Experiments show best-of-k selection outperforms extended iterations, but current models fail to effectively leverage custom assets and AVR feedback despite their value to human creators.

## Key Results
- AVR-Agent significantly improves content quality over one-shot generation, with best-of-k initial content selection proving more effective than extended iterations
- Current coding models show no significant win-rate improvement when provided with custom assets (44.4% better) or AVR feedback (58.3% better)
- Qwen3-Coder-480B and Kimi-K2-1T were the best-performing models, but still struggled with asset integration and feedback utilization
- AVR-Eval achieved 99.09% win rate against broken content and 93.53% against mislabeled content

## Why This Works (Mechanism)

### Mechanism 1: Multi-round AVR evaluation with hierarchical review
- Claim: Multi-round, relative comparison with text model review improves evaluation reliability
- Mechanism: Generate AVR for content A and B separately → omni-modal model describes each then compares → text model reviews the evaluation → both orderings tested (A vs B, B vs A)
- Core assumption: Text models have stronger reasoning and instruction-following capabilities than current omni-modal models
- Evidence anchors: AVR-Eval ablation shows removing multiround (90.00%), relative comparison (90.91%), or review (9.09%) worsens win rate against broken content vs. full AVR-Eval (99.09%)

### Mechanism 2: Best-of-k selection over extended iterations
- Claim: Selecting the best from k initial candidates outperforms k additional improvement iterations
- Mechanism: Generate k initial implementations → pairwise AVR-Eval comparison → select winner → proceed with fewer refinement steps
- Core assumption: Early-stage quality is more predictive of final quality than iteration depth
- Evidence anchors: Logistic regression shows Best-of-k significantly increases win rate; 75% of settings with Init-best beat alternatives

### Mechanism 3: Console log + AVR feedback loop (limited effectiveness)
- Claim: Combining browser console errors with omni-modal AVR feedback provides improvement signals, but current models struggle to leverage this effectively
- Mechanism: Render in browser → extract console logs → generate AVR → omni-modal model describes content and provides criteria-based feedback → coding model updates code
- Core assumption: Text-only coding models can translate textual descriptions of audio-visual observations into actionable code changes
- Evidence anchors: Performance does not generally improve from adding audio-visual feedback from an omni-modal model to the coding model (58.3% of the time better overall)

## Foundational Learning

- Concept: Omni-modal model processing (text + audio + video)
  - Why needed here: AVR-Eval requires processing non-speech audio (music, sound effects) alongside video frames
  - Quick check question: Can your model extract meaningful information from a video containing background music and sound effects, not just spoken dialogue?

- Concept: Relative evaluation metrics (pairwise comparison)
  - Why needed here: AVR-Eval compares two implementations rather than assigning absolute scores
  - Quick check question: Why might asking "which is better, A or B?" produce more reliable judgments than "rate A from 1-10"?

- Concept: Cross-modal agent communication bottlenecks
  - Why needed here: The coding agent (text-only) receives text summaries of audio-visual observations
  - Quick check question: What types of audio-visual issues (timing, synchronization, aesthetic quality) might be difficult to communicate through text descriptions?

## Architecture Onboarding

- Component map: Asset Bank -> Asset Selector Agent -> Code Generator Agent -> Browser Renderer -> AVR Recorder -> Console Log Extractor -> Omni-modal Feedback Agent -> Text Review Agent -> AVR-Eval Comparator

- Critical path: Asset selection → Generate k initial codes → Record AVR for each → Pairwise AVR-Eval → Select best initial → Improvement loop (render → AVR + console logs → feedback → code update) → Final content

- Design tradeoffs:
  - Best-of-k vs. iterations: Paper shows best-of-k wins (allocate compute to initial diversity, not depth)
  - Assets inclusion: No significant benefit in experiments—models not trained to leverage external assets
  - Feedback inclusion: No significant benefit—current models don't translate text feedback effectively
  - Model scale: Qwen3-Coder-480B and Kimi-K2-1T performed best but require substantial compute

- Failure signatures:
  - Broken content (black screen, title-only): AVR-Eval rejects 99.09%
  - Mislabeled content (wrong animation type): AVR-Eval rejects 93.53%
  - Asset integration failure: Models ignore or misuse provided assets
  - Feedback loop failure: Iterations with feedback show no win-rate improvement
  - Small model failures: Qwen3-32B frequently generates non-functional code
  - Audio autoplay blocking: Requires automated start-button interaction

- First 3 experiments:
  1. Validate AVR-Eval on your content types: Compare known-broken vs. known-working implementations to establish baseline rejection rates (target: >95% for obvious failures)
  2. Best-of-k ablation with fixed compute budget: Test k=1 with 10 iterations vs. k=5 with 2 iterations vs. k=10 with 1 iteration on same benchmark to replicate paper's finding
  3. Asset utilization test: Generate identical content with and without asset packs using your coding model to verify whether asset integration improves output

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can future omni-modal coding models effectively leverage multimedia assets and audio-visual feedback, unlike current text-only coding models?
- Basis in paper: The authors state: "Once omni-modal models are strong enough for coding, we expect them to be able to leverage multimedia assets and audio-visual feedback for improved performance."
- Why unresolved: Current omni-modal models cannot code at reasonable performance—"we found it unable to generate any working content"
- What evidence would resolve it: Testing AVR-Agent with a future omni-modal model that has strong coding capabilities, comparing win rates with and without assets/feedback

### Open Question 2
- Question: Why do coding models fail to benefit from high-quality assets and audio-visual feedback when humans derive significant value from both?
- Basis in paper: The authors identify this as a "critical gap" and hypothesize: "current models are trained to work without separate assets or using placeholders; hence, they are not trained to leverage real assets."
- Why unresolved: Experiments show no significant improvement from including assets (44.4% better) or feedback (58.3% better) across settings
- What evidence would resolve it: Fine-tuning coding models specifically on tasks requiring asset integration, then re-evaluating with AVR-Eval

### Open Question 3
- Question: How well does AVR-Eval correlate with human preferences for multimedia content quality?
- Basis in paper: "Note that AVR-Eval... has not been directly tested on human preference."
- Why unresolved: The paper validates AVR-Eval against broken/mislabeled content and human-made content, but not direct human preference ratings on generated content pairs
- What evidence would resolve it: Running a human evaluation study (similar to WebDev Arena) comparing the same content pairs, then correlating human judgments with AVR-Eval decisions

## Limitations

- Current text-only coding models cannot effectively leverage custom assets and audio-visual feedback despite their value to human creators
- The experimental benchmark is limited to 10 content types (5 games + 5 animations), which may not generalize to broader content generation tasks
- Best-performing models (Qwen3-Coder-480B, Kimi-K2-1T) require substantial computational resources, limiting practical deployment

## Confidence

**High Confidence:** AVR-Eval methodology is technically sound; ablation results are robust; current models don't effectively leverage audio-visual feedback and custom assets is well-supported

**Medium Confidence:** Best-of-k selection superiority is supported by logistic regression but may be task-dependent; gap between human and machine content creation is plausible but needs further investigation

**Low Confidence:** Claim that AVR-Agent significantly improves content quality over one-shot generation lacks strong statistical validation across all conditions

## Next Checks

1. **Replication of AVR-Eval effectiveness:** Implement AVR-Eval pipeline on new multimedia content types and verify >95% accuracy distinguishing high-quality from low-quality implementations

2. **Best-of-k vs iteration trade-off validation:** Design experiment with fixed compute budget testing multiple k and iteration combinations on diverse content generation tasks to verify best-of-k consistently outperforms deeper iterations

3. **Asset utilization capability test:** Create controlled experiment generating identical content with/without asset packs using state-of-the-art coding model to determine if inability to leverage assets is model-specific or represents broader limitation