---
ver: rpa2
title: 'RUMAA: Repeat-Aware Unified Music Audio Analysis for Score-Performance Alignment,
  Transcription, and Mistake Detection'
arxiv_id: '2507.12175'
source_url: https://arxiv.org/abs/2507.12175
tags:
- score
- alignment
- transcription
- music
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RUMAA is a transformer-based framework that unifies score-to-performance
  alignment, score-informed transcription, and mistake detection in a single model.
  It uses pre-trained score and audio encoders and a novel tri-stream decoder to jointly
  process MusicXML scores and audio performances, handling repeat symbols directly
  without unfolding.
---

# RUMAA: Repeat-Aware Unified Music Audio Analysis for Score-Performance Alignment, Transcription, and Mistake Detection

## Quick Facts
- arXiv ID: 2507.12175
- Source URL: https://arxiv.org/abs/2507.12175
- Reference count: 40
- Primary result: 98.4% F_align on repeated scores vs 36.4% baseline

## Executive Summary
RUMAA is a transformer-based framework that unifies score-to-performance alignment, score-informed transcription, and mistake detection in a single model. It uses pre-trained score and audio encoders and a novel tri-stream decoder to jointly process MusicXML scores and audio performances, handling repeat symbols directly without unfolding. The model outperforms existing methods on repeated scores for alignment, achieves near-perfect transcription with score guidance, and improves mistake detection accuracy over prior work.

## Method Summary
RUMAA employs a tri-stream decoder architecture that processes audio and score features through hierarchical cross-attention to jointly predict alignment, transcription, and mistake detection tokens. The model uses pre-trained M3 and YourMT3+ encoders for score and audio respectively, with scores converted from MusicXML to ABC notation preserving repeat symbols. Training involves heavy augmentation (10×) including pitch modulation, deletions, and simulated repeats, with a focus on maintaining strict one-to-one correspondence between score and performance events through explicit edit operation modeling.

## Key Results
- 98.4% F_align on repeated scores (vs 36.4% HMM baseline)
- 99.1% onset F1 for score-informed transcription (vs 95.9% baseline)
- Improved mistake detection accuracy over prior work

## Why This Works (Mechanism)

### Mechanism 1: Tri-stream proxy task tokenization
Three parallel decoder channels generate synchronized token streams (performance tokens, score tokens in performance order, edit operations) with exclusive tokens "-" maintaining perfect alignment. This explicit edit operation modeling (Insert/Delete) captures mistake patterns better than implicit learning from misaligned sequences.

### Mechanism 2: Hierarchical cross-attention
Sequential cross-attention (self-attention → audio cross-attention → score cross-attention) outperforms simple feature concatenation by mimicking iterative transcription/score-following behavior and processing audio before score to reflect natural performance analysis direction.

### Mechanism 3: Bar-level score patching with preserved repeats
MusicXML conversion to ABC notation retains repeat symbols, with each bar patched into a single 768-dim token. The decoder's cross-attention learns to navigate repeated sections by conditioning on full score context with barlines and repeat markers.

## Foundational Learning

- **Transformer encoder-decoder with cross-attention**
  - Why needed here: RUMAA's decoder conditions on two encoder outputs (audio + score) via cross-attention
  - Quick check question: Can you explain why cross-attention allows the decoder to "query" audio features while being informed by score features?

- **Compound tokenization / CP-Words**
  - Why needed here: Tri-stream tokenization extends CP-Words by adding parallel channels for performance, score, and edit operations
  - Quick check question: How does compound tokenization differ from sequential MIDI-like tokenization in terms of sequence length and alignment?

- **Music notation formats (MusicXML → ABC)**
  - Why needed here: RUMAA converts MusicXML to ABC notation with bar-level patching
  - Quick check question: What musical elements (repeats, dynamics, key signatures) are retained in the ABC conversion, and which might be lost?

## Architecture Onboarding

- **Component map:**
  - MusicXML → ABC conversion (bar-level, up to 64 chars/bar)
  - Audio → STFT (2048 window, 10ms hop) → ResNet pre-encoder → audio encoder
  - Decoder: 6-layer Transformer with hierarchical cross-attention, tri-stream LM heads

- **Critical path:**
  1. MusicXML → ABC conversion preserving repeat symbols
  2. Audio → spectrogram → pre-encoder → audio encoder
  3. Decoder autoregressively generates T1/T2/T3 tokens conditioned on both encoders
  4. Post-processing: T3 edit tags yield alignment; T1 yields transcription; T3 mistakes = Insert/Delete

- **Design tradeoffs:**
  - Lower audio frame rate (12 fps) enables longer sequences but may reduce fine timing resolution
  - Frozen pre-trained encoders reduce training cost but limit domain adaptation
  - Tri-stream output increases vocabulary complexity but enables unified task handling

- **Failure signatures:**
  - Repeated score sections aligned only at beginning/end → repeat handling failed
  - High onset F1 but poor offset-velocity F1 → audio encoder pre-training mismatch
  - Mistake detection degrades on real-world recordings → augmentation insufficient

- **First 3 experiments:**
  1. Ablate hierarchical cross-attention → expect ~1% F_align drop
  2. Evaluate on Vienna dataset with/without repeat symbols → baseline collapse vs RUMAA stability
  3. Test score-informed vs score-free transcription on (n)ASAP → expect onset F1 improvement from 95.9 → 99.1

## Open Questions the Paper Calls Out

1. **Memory-augmented architectures for long sequences**: How can memory-augmented architectures be integrated to handle audio sequences exceeding one minute without performance degradation? The current transformer implementation restricts the context window.

2. **Multi-instrument and real-world generalizability**: Can the unified framework maintain performance when extended to multi-instrument scores and noisy, real-world recordings? Current evaluation is limited to relatively clean, single-instrument piano data.

3. **Online processing suitability**: Is the RUMAA architecture suitable for adaptation to online, real-time processing scenarios? The current model relies on full audio context, differing from low-latency streaming requirements.

## Limitations
- Repeat symbol handling generalizability depends on M3 encoder pre-training data diversity
- Audio encoder pre-training opacity due to lack of detailed architectural specifications
- Real-world performance gap between controlled datasets and actual concert recordings

## Confidence
- **High confidence**: Tri-stream decoder effectiveness (98.4 F_align on repeated scores), score-informed transcription quality (99.1 onset F1), hierarchical cross-attention superiority
- **Medium confidence**: Mistake detection improvements (synthetic error injection vs natural mistakes)
- **Low confidence**: Generalization to arbitrary repeat patterns and real-world recording conditions

## Next Checks
1. Audit M3 encoder's pre-training corpus for repeat symbol diversity and frequency; test on synthetic scores with complex repeat structures
2. Evaluate RUMAA on unedited concert recordings with manual alignment ground truth to quantify domain gap
3. Create synthetic scores with increasingly complex repeat structures (nested repeats, irregular lengths) and measure alignment accuracy degradation to identify performance limits