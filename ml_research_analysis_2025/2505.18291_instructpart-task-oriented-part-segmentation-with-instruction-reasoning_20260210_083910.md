---
ver: rpa2
title: 'InstructPart: Task-Oriented Part Segmentation with Instruction Reasoning'
arxiv_id: '2505.18291'
source_url: https://arxiv.org/abs/2505.18291
tags:
- part
- segmentation
- object
- dataset
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InstructPart addresses the lack of benchmarks for task-oriented
  part segmentation, where models must understand instructions and ground them to
  specific object parts. The dataset contains 2,400 images with 9,600 task instructions
  and 2,400 part queries, covering 48 object classes and 44 part classes.
---

# InstructPart: Task-Oriented Part Segmentation with Instruction Reasoning

## Quick Facts
- **arXiv ID**: 2505.18291
- **Source URL**: https://arxiv.org/abs/2505.18291
- **Reference count**: 40
- **Primary result**: State-of-the-art vision-language models struggle with task-oriented part segmentation, but simple DINOv2 + SAM baseline achieves >2x improvement

## Executive Summary
InstructPart introduces a novel dataset and benchmark for task-oriented part segmentation, where models must understand natural language instructions and ground them to specific object parts. The dataset contains 2,400 images with 9,600 task instructions and 2,400 part queries across 48 object classes and 44 part classes. The work defines two distinct tasks: Task Reasoning Part Segmentation (TRPS) requiring inference from task instructions without explicit part names, and Oracle Referring Part Segmentation (ORPS) with direct part names provided. Evaluations demonstrate that current vision-language models perform poorly on both tasks, even when affordances are provided, highlighting a critical gap in fine-grained part understanding and task-oriented reasoning capabilities.

## Method Summary
The authors created a dataset with 2,400 images manually annotated with task instructions and part queries, covering 48 object classes and 44 part classes. They established two evaluation tasks: TRPS where models must infer parts from task instructions without explicit part names, and ORPS where models receive direct part names. State-of-the-art vision-language models were evaluated on both tasks, revealing significant performance gaps. A simple baseline using DINOv2 features with a SAM decoder, fine-tuned on the InstructPart dataset, achieved over twofold improvement compared to existing models, demonstrating both the dataset's quality and training potential.

## Key Results
- State-of-the-art vision-language models struggle significantly with both TRPS and ORPS tasks
- Even with affordances provided, model performance remains poor on task-oriented part segmentation
- Simple DINOv2 + SAM baseline fine-tuned on InstructPart achieves >2x performance improvement over existing models
- Dataset demonstrates high quality and strong training potential for task-oriented part understanding

## Why This Works (Mechanism)
The approach works by bridging natural language task understanding with precise part localization through instruction reasoning. By requiring models to map abstract task descriptions to concrete object parts, the dataset forces development of semantic understanding beyond simple object recognition. The two-task design (TRPS and ORPS) enables systematic evaluation of reasoning capabilities versus direct localization performance, revealing that current models lack fundamental understanding of how tasks relate to object parts.

## Foundational Learning
- **Vision-language grounding**: Understanding how text instructions map to visual regions - needed for instruction-based part localization, quick check: evaluate on referring expression comprehension datasets
- **Part-level segmentation**: Ability to identify and delineate specific object components - needed for precise part identification, quick check: evaluate on PartImageNet segmentation accuracy
- **Task-affordance mapping**: Understanding which object parts enable specific tasks - needed for TRPS reasoning, quick check: test zero-shot transfer to novel task-part pairs
- **Cross-modal alignment**: Aligning visual features with language semantics - needed for instruction understanding, quick check: evaluate with modified language queries
- **Fine-grained localization**: Precise spatial identification of small object parts - needed for accurate segmentation, quick check: measure part boundary accuracy

## Architecture Onboarding

**Component Map**: Vision Encoder -> Language Encoder -> Cross-modal Fusion -> Segmentation Decoder

**Critical Path**: Input Image & Instruction → Vision+Language Features → Cross-modal Reasoning → Part Mask Output

**Design Tradeoffs**: The choice of DINOv2 + SAM baseline prioritizes simplicity and fine-tuning efficiency over complex architecture design, trading potential performance gains from sophisticated vision-language models for faster training and better generalization from limited data.

**Failure Signatures**: Models fail on ambiguous instructions, rare part-task combinations, and parts that are small or visually similar to other regions. Performance degrades significantly when affordances are not explicitly provided, indicating limited task-part reasoning capabilities.

**3 First Experiments**:
1. Evaluate baseline on ORPS task with oracle part names to establish upper performance bound
2. Test zero-shot transfer to standard part segmentation datasets to assess generalization
3. Conduct ablation study varying instruction specificity in TRPS task

## Open Questions the Paper Calls Out
None

## Limitations
- Manual curation process may introduce selection bias across the 48 object classes and 44 part classes
- Simple baseline improvement may not represent ceiling of achievable performance with more sophisticated architectures
- Limited evaluation of cross-dataset generalization to objects and parts outside curated classes
- Performance gaps could stem from dataset artifacts rather than fundamental model limitations

## Confidence

**High Confidence**: Current vision-language models struggle significantly with task-oriented part segmentation, well-supported by experimental results.

**Medium Confidence**: The dataset demonstrates a meaningful gap in fine-grained part understanding and task-oriented reasoning capabilities, though extent of fundamental limitation remains unclear.

**Low Confidence**: The twofold improvement from simple baseline demonstrates dataset quality and training potential, requiring further validation with more sophisticated approaches.

## Next Checks
1. Evaluate models trained on InstructPart on established part segmentation datasets (e.g., PartImageNet) to assess cross-dataset generalization
2. Systematically vary instruction complexity in TRPS task to determine correlation with performance degradation
3. Test more sophisticated vision-language architectures (e.g., CLIP-based models) against DINOv2 + SAM baseline