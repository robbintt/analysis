---
ver: rpa2
title: SIGIR 2025 -- LiveRAG Challenge Report
arxiv_id: '2507.04942'
source_url: https://arxiv.org/abs/2507.04942
tags:
- challenge
- teams
- were
- liverag
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# SIGIR 2025 -- LiveRAG Challenge Report

## Quick Facts
- arXiv ID: 2507.04942
- Source URL: https://arxiv.org/abs/2507.04942
- Reference count: 35
- Primary result: Best team achieved 2.02/2.00 in correctness/faithfulness using hybrid BM25 + dense retrieval with LLM-based query rewriting and cross-encoder reranking

## Executive Summary
The LiveRAG Challenge brought together 16 teams to develop Retrieval-Augmented Generation (RAG) systems using a fixed corpus (Fineweb-10BT) and a common LLM (Falcon3-10B-Instruct). Participants focused on optimizing retrieval and prompting strategies, with the best systems achieving high correctness and faithfulness scores through hybrid search, reranking, and query rewriting. The evaluation used an LLM-as-a-judge approach followed by manual review, showing strong correlation between automated and human assessments.

## Method Summary
Teams built RAG systems using a 15M document corpus and Falcon3-10B-Instruct LLM, with hybrid retrieval combining BM25 sparse and E5-base-v2 dense indices. Most systems employed LLM-based query rewriting, cross-encoder reranking (BGE-m3, Jina-m0, Cohere-3.5), and prompt augmentation with 3-10 retrieved passages. Evaluation measured correctness (harmonic mean of coverage and relatedness) and faithfulness (claim entailment from passages) using Claude-3.5-sonnet for automated judgment, followed by manual review of top submissions.

## Key Results
- Best team (PRMAS-DRCA) achieved 2.02/2.00 in correctness/faithfulness using hybrid search with LLM-based query rewriting and cross-encoder reranking
- All top 13 teams used hybrid BM25 + dense retrieval, with most employing cross-encoder reranking and query rewriting
- Automated LLM-as-a-judge scores showed high correlation (r=0.87) with manual evaluation for top submissions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraining all teams to a fixed LLM isolates retrieval and prompting strategies as primary performance drivers
- Mechanism: Standardizing the answer generator means performance differences stem from retrieval effectiveness and prompt construction
- Core assumption: Falcon3-10B-Instruct has sufficient baseline competence across question types
- Evidence anchors: Challenge used fixed corpus and LLM to "ensure fair comparison of retrieval and prompting strategies"
- Break condition: If the LLM is underpowered for complex questions, retrieval improvements may yield diminishing returns

### Mechanism 2
- Claim: Hybrid retrieval combining BM25 and dense indices provides complementary coverage for diverse question types
- Mechanism: BM25 excels at lexical matching while dense embeddings capture semantic similarity, addressing broader query spectrum
- Core assumption: Question set includes both keyword-heavy and semantically nuanced queries
- Evidence anchors: "Most teams use a hybrid search over the two pre-built indices" with varied retrieval demands implied
- Break condition: If queries are heavily skewed toward one type, hybrid complexity may not justify overhead

### Mechanism 3
- Claim: Two-stage evaluation (LLM-as-a-judge + manual review) provides scalable and reliable quality signal
- Mechanism: Claude-3.5-sonnet computes correctness and faithfulness via claim extraction and NLI; top submissions manually validated
- Core assumption: Judge LLM's claim extraction and entailment logic sufficiently approximates human judgment
- Evidence anchors: High correlation between LLM-based Correctness scores and manual scores of leading teams
- Break condition: If judge LLM exhibits systematic bias, automated rankings may diverge from human judgment

## Foundational Learning

### Concept: BM25 vs. Dense Retrieval
- Why needed here: Understanding sparse vs. dense retrieval strengths is essential for designing indices
- Quick check question: Why might BM25 outperform dense embeddings on queries with rare proper nouns?

### Concept: LLM-as-a-judge (Claim Extraction + NLI)
- Why needed here: Evaluation pipeline relies on automated judgment; knowing limits informs result interpretation
- Quick check question: What is a potential failure mode when using NLI to verify claim entailment in generated answers?

### Concept: Prompt Augmentation with Retrieved Passages
- Why needed here: All teams used retrieved passages to augment prompts; context window management is critical
- Quick check question: How does aggressive passage truncation potentially impact the faithfulness score?

## Architecture Onboarding

Component map: Query Rewriter → Hybrid Retriever (OpenSearch Sparse + Pinecone Dense) → Re-ranker (e.g., BGE-m3) → Prompt Assembler → Falcon3-10B-Instruct → Answer

Critical path: Retrieval and re-ranking must average ~14 seconds per question to meet the 2-hour, 500-question deadline

Design tradeoffs:
- Pre-built indices reduce setup time but limit embedding model flexibility
- Using more passages (e.g., 50 vs. 3-10) can improve coverage but risks context dilution and increased latency

Failure signatures:
- Negative faithfulness scores often trace to aggressive passage truncation or irrelevant retrieved content
- Low correctness despite high retrieval scores may indicate poor prompt assembly

First 3 experiments:
1. Baseline Hybrid Retrieval: Measure correctness/faithfulness using default BM25 + E5 indices without re-ranking
2. Re-ranker Ablation: Compare BGE-m3, Jina-m0, and Cohere-3.5 on a 50-question dev set
3. Prompt Passage Count Sweep: Test 3, 5, 10, and 20 retrieved passages to identify saturation point for correctness gains vs. latency cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does high correlation between LLM-based and manual evaluation scores hold for lower-ranked submissions?
- Basis in paper: Manual evaluation only conducted for top-13 teams; correlation may not generalize to lower-performing systems
- Why unresolved: Limited manual evaluation scope; errors could compound differently in lower-tier systems
- What evidence would resolve it: Manual evaluation on stratified sample across all performance levels with correlation coefficients per tier

### Open Question 2
- Question: How does truncating evaluation to 300 words and 10 passages affect final rankings and metric reliability?
- Basis in paper: Budget constraints limited evaluation to first 300 words and 10 passages, despite some teams using up to 50 passages
- Why unresolved: Truncation may systematically disadvantage certain retrieval strategies
- What evidence would resolve it: Re-evaluate without truncation limits and compare ranking changes; analyze variance in answer lengths

### Open Question 3
- Question: What specific question types or complexity levels remain most challenging for current RAG systems?
- Basis in paper: Authors considering extended variety of question types next year; questions were "intentionally generated to provide diverse test cases"
- Why unresolved: Report provides aggregate scores but no per-category breakdown of failure points
- What evidence would resolve it: Report scores disaggregated by DataMorgana's seven question categorizations

## Limitations

- Reliance on fixed LLM and corpus limits generalizability to other RAG settings
- Key methodological details (LLM-as-judge prompts, retrieval hyperparameters) not specified, making exact reproduction difficult
- Automated evaluation reliability depends on judge LLM's robustness, which has documented limitations in broader literature

## Confidence

- High confidence: Hybrid retrieval improving coverage across diverse query types is well-supported by challenge results and related literature
- Medium confidence: LLM-as-a-judge providing reliable automated evaluation is supported by observed correlation with manual review, but potential systematic biases remain
- Low confidence: Retrieval and prompting are primary performance drivers assumes fixed LLM is non-limiting across all question types, not explicitly validated

## Next Checks

1. Test Falcon3-10B-Instruct against complex multi-aspect questions to verify it's not the bottleneck across full question distribution
2. Validate LLM-as-a-judge evaluation pipeline by comparing rankings against human judges on subset, checking for systematic biases
3. Experiment with varying hybrid fusion weights between BM25 and dense retrieval to quantify actual contribution of each component across different query types