---
ver: rpa2
title: A Proposal for Networks Capable of Continual Learning
arxiv_id: '2503.22068'
source_url: https://arxiv.org/abs/2503.22068
tags:
- sources
- learning
- state
- active
- csvs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Modelleyen, a novel computational framework
  for continual learning that addresses the fundamental limitation of neural networks
  in preserving past responses during parameter updates. The key innovation is a local
  variation and selection mechanism where computational units (conditioning state
  variables) inherently preserve past responses through refinement and rerelation
  operations on network structures.
---

# A Proposal for Networks Capable of Continual Learning

## Quick Facts
- arXiv ID: 2503.22068
- Source URL: https://arxiv.org/abs/2503.22068
- Reference count: 40
- One-line primary result: Modelleyen achieves continual learning without replay buffers through local structural refinement mechanisms

## Executive Summary
This paper proposes Modelleyen, a novel computational framework for continual learning that addresses the fundamental limitation of neural networks in preserving past responses during parameter updates. The key innovation is a local variation and selection mechanism where computational units (conditioning state variables) inherently preserve past responses through refinement and rerelation operations on network structures. Experiments on environment modeling and MNIST digit classification demonstrate that Modelleyen achieves continual learning without replay buffers or task boundaries, maintaining stable performance across changing environments and class distributions.

## Method Summary
Modelleyen uses a hierarchy of state variables (BSV, DSV, CSV) with monotonic refinement mechanisms. CSVs initialize with all active sources connected, then permanently remove inactive positive sources and active negative sources as patterns are observed. The Modelleyen Network Refinement (MNR) component converts images to State Polynetworks (SPNs) and refines them through rerelation operations that preserve path connectivity. The system learns online with single-sample updates, maintaining past knowledge without explicit memory storage or task boundaries.

## Key Results
- Achieved continual learning on FSM environment without replay buffers, maintaining performance when environment switches from RS-L to SGS-L
- Maintained stable MNIST classification accuracy across class-incremental learning cycles, with early-learned classes retaining high accuracy
- Demonstrated human-comprehensible internal representations through SPN structures that can be inspected and interpreted

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continual learning emerges from monotonic refinement of over-connected hypotheses, not from explicit memory storage
- Mechanism: CSVs initialize with ALL currently active SVs as positive sources (exhaustive hypothesis). When the CSV activates with only a subset of sources active, inactive sources are permanently removed
- Core assumption: Past inputs that established a connection will remain constant (xᵢ₀ = xᵢ₁ = ... = xᵢₜ for all connected sources per Eq. 4)
- Evidence anchors: [abstract] "computational units (conditioning state variables) inherently preserve past responses through refinement and rerelation operations"
- Break condition: If environment non-stationarity means past input patterns fundamentally change, the assumption that "connected sources were always active in past" may fail

### Mechanism 2
- Claim: Network refinement with rerelation preserves common substructure across observations by maintaining path connectivity, not just edge identity
- Mechanism: When refining source SPN P₀ by refiner P₁: (1) Remove nodes/edges absent in P₁, (2) For each removed edge (n₀, n₁), create new edges connecting ALL predecessors of n₀ to ALL successors of n₁
- Core assumption: A valid node assignment f: V(P₀) → V(P₁) exists and can be found efficiently
- Evidence anchors: [section 5, Definition 5] "P₀ is satisfied by P₁... if every edge in P₀ has a path in P₁ connecting the assigned targets of its endpoints"
- Break condition: If no valid assignment exists between SPNs, refinement fails

### Mechanism 3
- Claim: Upstream conditioning enables arbitrarily complex logical relations without requiring compositional depth limits
- Mechanism: CSVs can condition not only DSVs but also other CSVs. When C₀ is active but sometimes lacks explanation, new CSV C₁ forms to condition C₀, representing alternative sufficient conditions
- Core assumption: Upstream conditioners stabilize before downstream ones require them; no circular dependencies
- Evidence anchors: [section 4, Figure 2] Shows C₁ conditioning C₀, then C₂/C₃ refining C₁'s sources
- Break condition: If upstream conditioning proliferates without stabilization, model complexity becomes unbounded

## Foundational Learning

- **Catastrophic Forgetting in Gradient Descent**: Why needed here: The paper proves neural networks cannot satisfy ∑Δwᵢxᵢₜ + Δb = 0 (Eq. 1) because weights lack direct correspondence to input values and past inputs aren't guaranteed identical across encounters. Quick check: For a neuron with past input Xₜ = [x₀ₜ, ..., xₙₜ] and weight update Δw, why can't gradient descent guarantee the new response y'ₜ equals the original yₜ?

- **Finite State Machines with Non-Deterministic Transitions**: Why needed here: Modelleyen's base version uses discrete state variables with states {-1, 0, 1} and models transitions via DSVs; the test environment includes alternative outcomes, correlated changes, and negative conditions that map to FSM concepts. Quick check: In the FSM environment (Figure 10), what does it mean for state G to have multiple incoming arrows from different predecessors, and how would Modelleyen represent this?

- **Subgraph Isomorphism and Graph Matching**: Why needed here: MNR's refinement requires finding assignments f: V(P₀) → V(P₁) such that edges in source map to paths in refiner—fundamentally a constrained subgraph matching problem. Quick check: Given two directed graphs G₁ and G₂, explain why the paper requires paths (not just edges) in G₂ to match edges in G₁.

## Architecture Onboarding

- Component map: Observation → BSV (external state, {1,-1}) → DSV (activation/deactivation per BSV) → CSV (mutable sources {Xₚ, Xₙ}, targets Y) → Upstream CSVs (hierarchical relations)

- Critical path:
  1. Environment step → assign BSV states from observations
  2. Compute DSV states from BSV transitions
  3. Traverse CSV network reverse-computation-order
  4. Per CSV: check source satisfaction → determine state → refine sources (remove inactive positive, active negative) → duplicate if mixed target states
  5. Create new CSVs for unexplained active DSVs/CSVs (all active eligible SVs as sources)
  6. Model refinement: remove empty/duplicate CSVs
  7. (Planner) Generate upstream action network from goal backward

- Design tradeoffs:
  - **Exhaustive initial connection vs. selective**: Current design connects ALL active SVs—computationally expensive but guarantees no necessary condition missed
  - **One-time negative source formation**: Prevents complexity explosion but may miss true negative conditions encountered later
  - **Single target per CSV (MNR implementation)**: Simplifies assignment propagation but inflates CSV count vs. base Modelleyen's multi-target design
  - **Statistical threshold Tᵣₑf**: Lower preserves more features (risk overfitting); higher simplifies (risk missing rare-but-important features)

- Failure signatures:
  - **CSV explosion**: If eligibility filtering (trivial source detection) fails, model grows unbounded
  - **Stuck "possibly conditional"**: CSV repeatedly encounters unpredictable states, never converges
  - **Assignment conflicts (MNR)**: Multiple valid node mappings produce different refined SPNs—non-deterministic
  - **Over-refinement with low Tᵣₑf**: Noisy samples cause SPNs to become overly specific, fail to match new instances

- First 3 experiments:
  1. **Replicate FSM baseline (Table 1)**: Implement base Modelleyen on RS/SGS/NEG environment subtypes. Verify episode durations don't spike when environment switches without readaptation
  2. **Ablate negative source formation timing**: Test allowing multiple negative source formation events vs. one-time on NEG subtype. Measure both task performance and CSV count
  3. **MNR MNIST with class-incremental validation**: Replicate NC = 3, 5, 10 experiments. Verify early-learned classes maintain stable accuracy across iterations

## Open Questions the Paper Calls Out

- **Can Modelleyen be adapted to model the temporal dynamics of environments represented as networks, integrating the separate dynamics modeling and network refinement (MNR) components?**
- **Can the computational complexity of Modelleyen be optimized to scale effectively to high-dimensional observation spaces without succumbing to the overhead of redundant modeling?**
- **Will adopting established feature representations (e.g., SIFT, frequency components, or pretrained visual features) improve the representational expressivity and classification accuracy of Modelleyen?**
- **Can a mechanism for "soft satisfaction" of State Polynetworks (SPNs) be implemented to handle noise and outliers without disrupting the local continual learning guarantee?**

## Limitations

- **Computational Complexity**: The implementation is computationally demanding due to redundant modeling, restricting simulations to shorter durations
- **Feature Representation Expressivity**: Current gradient sign change features miss shape details, limiting accuracy compared to neural networks
- **Underspecified Environment Details**: Specific FSM environment parameters and SPN assignment algorithms are not fully detailed in the paper

## Confidence

- **High Confidence**: The core Modelleyen framework with CSV refinement and preservation mechanisms is well-specified and theoretically sound
- **Medium Confidence**: The MNR implementation details are mostly complete but have critical gaps in assignment and population generation
- **Low Confidence**: Reproduction of exact FSM environment dynamics and MNIST SPN generation may require assumptions due to underspecification

## Next Checks

1. **Implement base Modelleyen SV types (BSV, DSV, CSV)** with three-valued states and verify CSV state computation, source refinement, and target duplication logic match specifications
2. **Build FSM environment** per Figure 10 with all 20 actions, three subtypes, and random variant; test continual learning by switching subtypes every 500 steps
3. **Implement MNR MNIST pipeline** including image preprocessing, gradient sign change detection, SPN construction, and statistical refinement with thresholds Tref=0.05, Tsign=0.05