---
ver: rpa2
title: Capturing Classic Authorial Style in Long-Form Story Generation with GRPO Fine-Tuning
arxiv_id: '2512.05747'
source_url: https://arxiv.org/abs/2512.05747
tags:
- author
- same
- cross
- style
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of generating long-form stories
  in a specific authorial style using large language models. It proposes a two-stage
  pipeline: first, a style-similarity judge is trained by fine-tuning a sentence transformer
  with authorship verification signals; second, this judge is used as the primary
  reward in Group Relative Policy Optimization (GRPO) to fine-tune an 8B model for
  style-conditioned story generation.'
---

# Capturing Classic Authorial Style in Long-Form Story Generation with GRPO Fine-Tuning

## Quick Facts
- **arXiv ID**: 2512.05747
- **Source URL**: https://arxiv.org/abs/2512.05747
- **Reference count**: 40
- **Primary result**: GRPO-trained 8B model achieves 0.893 average style score across four authors, outperforming GPT-4o and Claude Sonnet 4 in stylistic imitation.

## Executive Summary
This paper proposes a two-stage pipeline for generating long-form stories in a specific authorial style using large language models. The approach first trains a style-similarity judge by fine-tuning a sentence transformer with authorship verification signals, then uses this judge as the primary reward in Group Relative Policy Optimization (GRPO) to fine-tune an 8B model. The method includes auxiliary rewards for content quality and completeness to stabilize long-form generation. Experiments with fiction by Mark Twain, Jane Austen, Charles Dickens, and Thomas Hardy demonstrate that the GRPO-trained 8B model achieves higher style scores than larger baselines, though narrative completeness remains a challenge.

## Method Summary
The approach consists of two main stages: (1) style judge training via fine-tuning a sentence transformer on authorship verification pairs to create a differentiable style reward signal, and (2) GRPO fine-tuning using this judge as the primary reward combined with content and completeness auxiliary rewards. The style judge is trained on 100K pairwise samples using chunk refilling to generate masked sentences, with graded labels based on author matching. GRPO optimization is performed with a KL-regularized loss and tuned coefficient to balance reward maximization against divergence from the reference model.

## Key Results
- GRPO-trained 8B model achieves 0.893 average style score across four authors, outperforming GPT-4o and Claude Sonnet 4
- Style scores reach 0.984 for Jane Austen and 0.962 for Mark Twain, with lower scores for Dickens (0.807) and Hardy (0.819)
- KL coefficient β=0.0005 provides optimal balance between reward improvement and training stability
- Multi-reward weighting (0.6 style + 0.3 content + 0.1 completeness) prevents style-only optimization collapse

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AV-calibrated sentence transformers provide learnable gradient signals for stylistic imitation
- Mechanism: Fine-tuning on authorship-verification pairs amplifies stylistic features while suppressing topic confounds, enabling gradient-based policy updates with partial credit for imperfect imitation
- Core assumption: Stylistic signatures are consistent within authors and discriminable across authors
- Evidence anchors: Δ separation increases from +0.057 to +0.406 for GTE-large-en-v1.5; IQR overlap drops from 34.5% to 0.0%
- Break condition: Judge collapses to content-matching, rewarding topic imitation instead of style

### Mechanism 2
- Claim: GRPO enables stable long-form style optimization without curated preference pairs
- Mechanism: Group-relative advantages computed from multiple outputs per prompt provide meaningful learning signals while maintaining output diversity
- Core assumption: Reward function correlates with true style quality for meaningful ranking
- Evidence anchors: GRPO avoids accept/reject supervision requirement; maintains diversity critical for creative tasks
- Break condition: Low reward variance within groups causes noisy advantage estimates and training instability

### Mechanism 3
- Claim: Multi-reward weighting prevents style-optimization collapse at cost of narrative coherence
- Mechanism: Weighted combination (0.6 style + 0.3 content + 0.1 completeness) provides competing optimization pressures
- Core assumption: Auxiliary rewards are sufficiently correlated with narrative quality to counteract reward hacking
- Evidence anchors: Explicit acknowledgment that style alignment is strong but completeness remains a challenge
- Break condition: Content/completeness rewards are poorly calibrated or gamed by the model

## Foundational Learning

- **Concept: Contrastive representation learning**
  - Why needed here: Judge training uses contrastive loss to pull same-author embeddings closer and push cross-author embeddings apart
  - Quick check question: Can you explain why increasing Δ (same-author minus cross-author similarity) after fine-tuning indicates the judge is learning style rather than topic?

- **Concept: KL-regularized policy optimization**
  - Why needed here: GRPO uses KL divergence penalty to prevent policy from deviating too far from reference model
  - Quick check question: What happens to training dynamics if β is set too high vs. too low, based on Figure 5 in the paper?

- **Concept: Reward calibration and scaling**
  - Why needed here: Raw similarity scores must be mapped to bounded [0,1] reward using benchmark quartiles
  - Quick check question: Why does the paper use Q25_cross and Q75_same as calibration bounds rather than the theoretical [0,1] range?

## Architecture Onboarding

- **Component map**: Style Judge (GTE-large-en-v1.5) -> Content Judge (openchat-3.5-0106) -> Completeness Check (rule-based) -> Reward Aggregator -> GRPO Trainer -> Reference Model

- **Critical path**: Construct pairwise style-judge training data via chunk refilling → Fine-tune sentence transformer on 100K pairs → Validate judge calibration (Δ, IQR overlap) → Build SFT dataset → Run GRPO fine-tuning per author → Evaluate on held-out plots

- **Design tradeoffs**:
  - Judge model selection: GTE-large-en-v1.5 chosen for highest separation despite BGE-M3 being better centered
  - KL coefficient: β=0.0005 outperforms β=0 and higher values; weak constraint sufficient for stability
  - Reward weights: 0.6/0.3/0.1 prioritizes style but trades off narrative completeness

- **Failure signatures**:
  - Judge collapse: Significant overlap between same-author and cross-author distributions
  - Reward hacking: High style scores through vocabulary copying without deeper stylistic capture
  - Training instability: KL divergence spikes with β ≥ 0.002
  - Narrative truncation: Completeness reward fails to enforce genuine story resolution

- **First 3 experiments**:
  1. Validate judge before GRPO by computing Δ and IQR overlap; revisit data construction if Δ < 0.2 or IQR > 10%
  2. Beta sweep on single author (Mark Twain) to select optimal β based on reward trajectory and KL stability
  3. Ablate auxiliary rewards to quantify tradeoff between style alignment and narrative quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can narrative completeness and global coherence be improved while maintaining strong style alignment in GRPO-trained models?
- Basis in paper: "While style alignment is strong, narrative completeness remains a challenge, indicating the need for further work on global coherence and story resolution."
- Why unresolved: Current completeness reward is rule-based (word count + "THE END") rather than semantic
- What evidence would resolve it: Experiments with coherence-focused rewards showing improved narrative resolution without degrading style scores

### Open Question 2
- Question: Does the trained style judge conflate stylistic features with residual lexical or semantic content matching from its pretrained embedding model?
- Basis in paper: Judge initialized from semantic retrieval model, similarity signal may reflect residual lexical/semantic matching
- Why unresolved: No ablation isolates pure style similarity from content similarity
- What evidence would resolve it: Ablation comparing judge scores on paraphrased vs. stylistically-distinct versions of same content

### Open Question 3
- Question: How does the framework perform on authors with prominent dialect, archaic vocabulary, or code-switching?
- Basis in paper: Dickens (0.807) and Hardy (0.819) achieved lower style scores than Austen (0.984) and Twain (0.962), suggesting normalization of distinctive linguistic features
- Why unresolved: Model may normalize dialect markers, French phrases, or archaic vocabulary during generation
- What evidence would resolve it: Analysis of generated outputs for presence/absence of dialect markers and targeted reward ablation

### Open Question 4
- Question: Would increasing plot diversity and prompt format variation improve generalization and reduce reward hacking?
- Basis in paper: Limited plot set (50 plots) may encourage repeated high-reward structures and format-specific performance
- Why unresolved: Small fixed plot set may cause model to exploit structural shortcuts
- What evidence would resolve it: Training with expanded plot sets and varied prompt templates, evaluating generalization on held-out structures

## Limitations

- The completeness reward is rule-based rather than semantic, potentially incentivizing superficial compliance rather than genuine story resolution
- The SFT dataset includes only 5,000 examples across four authors with limited plot diversity, risking overfitting to training patterns
- Style judge calibration relies on fixed benchmark quartiles without testing robustness to different percentile choices or out-of-domain authors

## Confidence

- **High confidence**: AV-fine-tuned sentence transformers providing differentiable style rewards is well-supported by empirical separation metrics and contrastive learning literature
- **Medium confidence**: GRPO's ability to optimize long-form style without preference pairs is plausible but lacks ablation studies isolating its contribution
- **Medium confidence**: Multi-reward weighting tradeoff is explicitly acknowledged as a limitation with unresolved completeness challenge

## Next Checks

1. **Judge calibration robustness**: Recompute Δ and IQR overlap using different percentile thresholds (Q10/Q90) and test on an out-of-domain author (Edgar Allan Poe) to verify generalization
2. **Ablate GRPO vs. judge quality**: Train with same reward function but vanilla policy gradient (no KL penalty) to quantify GRPO's contribution
3. **Semantic completeness evaluation**: Replace rule-based completeness check with LLM-based evaluator assessing narrative resolution and coherence, then compare story quality metrics