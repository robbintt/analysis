---
ver: rpa2
title: Selecting Fine-Tuning Examples by Quizzing VLMs
arxiv_id: '2511.12002'
source_url: https://arxiv.org/abs/2511.12002
tags:
- images
- quizrank
- lora
- fine-tuning
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces QZLoRA, a method for selecting high-quality
  training images for fine-tuning text-to-image diffusion models using a visual question-answering
  approach. It leverages QuizRank, which ranks images based on how well a vision-language
  model can answer questions about their visual properties.
---

# Selecting Fine-Tuning Examples by Quizzing VLMs

## Quick Facts
- arXiv ID: 2511.12002
- Source URL: https://arxiv.org/abs/2511.12002
- Authors: Tenghao Ji; Eytan Adar
- Reference count: 25
- Primary result: QZLoRA achieves 54.99% QuizRank accuracy on photorealistic images using only 15 top-ranked examples, outperforming random selection (46.74%) and no LoRA (35.37%).

## Executive Summary
QZLoRA introduces a novel method for selecting high-quality training images for LoRA fine-tuning of text-to-image diffusion models by "quizzing" a visual-language model (VLM) with multiple-choice questions derived from Wikipedia descriptions. The approach ranks images based on how well the VLM can answer visual questions about their distinguishing features, operating under the assumption that images enabling better VLM performance are more representative of the target concept. By selecting only the top-ranked images for fine-tuning, QZLoRA produces more visually accurate and concept-aligned outputs than random sampling or baseline models, with improved input-output correlation (r=0.920) and reduced variance across topics.

## Method Summary
The QZLoRA pipeline begins with Wikipedia text describing a target concept, which is used to generate multiple-choice quiz questions about distinguishing visual features. A VLM (GPT-4o) evaluates candidate images from Wikimedia Commons by attempting to answer these questions, and images are ranked by the VLM's accuracy. The top-k images are then used to fine-tune a Stable Diffusion 1.5 model using LoRA with specific hyperparameters (epochs=20, num_repeats=5, batch_size=1, lr=1e-4). The resulting fine-tuned model generates images that are evaluated using the same QuizRank test, demonstrating improved semantic alignment with the target concept compared to random selection or no fine-tuning.

## Key Results
- QZLoRA top-15 achieves 54.99% QuizRank accuracy vs. random LoRA 46.74% and no LoRA 35.37%
- Input-output accuracy correlation increases from r=0.732 (k=2 images) to r=0.920 (k=15 images)
- Using only top-2 images yields 50.03% accuracy, remaining above random baseline
- Method works for both photorealistic and stylized image generation

## Why This Works (Mechanism)

### Mechanism 1: VLM-based Educational Proxy for Image Ranking
The system treats candidate images as "educational interventions" for a VLM, ranking them based on how well the VLM can answer questions about visual properties when conditioned on each image. Images that enable higher VLM accuracy are assumed to better exemplify the target concept. This operates on the assumption that VLM question-answering accuracy serves as a valid proxy for human-perceived "representativeness."

### Mechanism 2: Quality-Dominant Data Filtering
By filtering out noisy or unrepresentative samples common in random web-scraped datasets, LoRA avoids overfitting to irrelevant visual features and focuses adaptation capacity on core concepts. The diffusion model's prior knowledge is assumed sufficient to generalize from a small set of high-quality, semantically aligned examples.

### Mechanism 3: Input-Output Stability Scaling
Increasing the number of high-quality images selected via QuizRank improves fine-tuning stability, creating stronger correlation between input image quality and generated output quality. As more verified representative images are added (up to 15), noise in gradient updates decreases, resulting in a fine-tuned model that reliably produces outputs mirroring input quality.

## Foundational Learning

- **Low-Rank Adaptation (LoRA)**
  - Why needed: Core adaptation technique that injects trainable rank decomposition matrices into model layers rather than retraining the whole model
  - Quick check: Does LoRA modify the pre-trained weights of the diffusion model directly, or does it add parallel trainable layers?

- **Vision-Language Models (VLMs)**
  - Why needed: Selection mechanism relies entirely on VLM's ability to "see" images and answer text questions, mapping visual inputs into shared semantic space with text
  - Quick check: In the QZLoRA pipeline, is the VLM generating images, or is it acting as a classifier/evaluator for existing images?

- **Contrastive Question Generation**
  - Why needed: System must know what makes the target distinct by contrasting it with similar "distractor" classes
  - Quick check: Why would asking "Is this round?" be a worse question than "Does this have a half-moon shape typical of Gujia vs. the rectangular shape of Chandrakala?"

## Architecture Onboarding

- **Component map:** Topic Source -> Question Generator (LLM) -> Evaluator (VLM) -> Ranker -> Fine-Tuner (LoRA) -> Generator

- **Critical path:** Quality of the Contrastive Question Generator is critical. If questions do not target distinguishing visual features, VLM ranking will fail to select visually representative images, rendering LoRA ineffective.

- **Design tradeoffs:**
  - k-value: Top-2 yields surprisingly strong results with minimal compute; Top-15 offers higher stability and peak performance (54.99%) but requires 7x more data processing and training time
  - VLM choice: Using GPT-4o provides reliable quiz scores; smaller open-source VLM might reduce cost but could lower ranking reliability

- **Failure signatures:**
  - Broad Concepts: Topics without single "prototypical" look result in poor performance
  - Specific Instances: Monuments or specific artworks may still hallucinate details
  - Style Mismatch: Training on photorealistic inputs to generate illustrations may underperform

- **First 3 experiments:**
  1. Validation of Ranking: Train two LoRAsâ€”one on Top-5 images, one on Bottom-5 images. Verify Top-5 produces significantly higher QuizRank accuracy in generated outputs (should be ~50% vs <35%).
  2. Sensitivity to k: Run fine-tuning with k=[2, 5, 10, 15] on fixed topic set to reproduce accuracy plateau and correlation tightening.
  3. Cross-Topic Generalization: Select 5 topics from "Art" category and 5 from "Biology" to ensure question generation mechanism works across domains with different visual textures.

## Open Questions the Paper Calls Out

- **Can clustering images based on which specific QuizRank questions they answer correctly improve fine-tuning for broad or highly variable topics?**
  - Basis: Discussion proposes identifying "clusters of images based on which questions an image helps answer correctly" to handle topics without single representative prototype
  - Why unresolved: Current method assumes singular target concept, which fails for broad categories where distinct sub-types possess different visual features
  - Evidence would resolve: Study demonstrating training separate LoRA models for each semantic cluster outperforms single QZLoRA model on compound topics

- **Does the effectiveness of QZLoRA transfer to diffusion architectures other than Stable Diffusion 1.5?**
  - Basis: Discussion acknowledges while benefits expected to transfer, "experiments focus on particular diffusion model (1.5)" and require additional testing
  - Why unresolved: Results may be specific to architectural inductive biases or latent space of Stable Diffusion 1.5
  - Evidence would resolve: Benchmarks replicating QZLoRA pipeline on diverse architectures (e.g., SDXL, DALL-E) showing consistent semantic accuracy improvements

- **Can the distribution of QuizRank scores for real images predict the optimal number of samples (k) required for fine-tuning?**
  - Basis: Section 4.5 suggests correlation between input and output accuracy "may enable future optimization where initial distribution indicates how many are needed"
  - Why unresolved: Paper tests fixed values of k but does not define mechanism to dynamically determine sufficient sample size for specific topic
  - Evidence would resolve: Predictive model successfully sets optimal k values based on input image score variance, validated against exhaustive search baselines

## Limitations

- VLM-based selection validity depends entirely on correlation between VLM accuracy and human-perceived image quality
- Method shows diminishing returns when topics lack clear visual prototypes (e.g., broad taxonomic categories)
- Reliance on GPT-4o creates cost and accessibility barriers for practical deployment
- Cannot generate novel visual features beyond what appears in training set

## Confidence

**High Confidence:** Core mechanism of using VLM quiz scores to rank images is well-supported by data with clear quantitative improvements in QuizRank accuracy (54.99% vs 46.74% random) and documented input-output correlation improvement from r=0.732 to r=0.920.

**Medium Confidence:** Claim that quality dominates quantity in fine-tuning is supported by observation that top-2 images achieve 50.03% accuracy, but broader generalizability to other model architectures requires further validation.

**Low Confidence:** Scalability claims beyond k=15 images and method's performance on highly abstract or multimodal concepts remain untested, as paper only reports results up to 15 images using primarily concrete visual topics.

## Next Checks

1. **Cross-Architecture Generalization:** Apply QZLoRA pipeline to fine-tune a different diffusion model architecture (e.g., SDXL or ControlNet variant) on same topic set to verify VLM ranking correlation holds across model families.

2. **VLM Independence Test:** Replace GPT-4o with smaller open-source VLM (e.g., LLaVA-1.5) in ranking pipeline and measure degradation in QuizRank accuracy to establish sensitivity to VLM quality.

3. **Concept Complexity Stress Test:** Systematically evaluate performance across topics with varying semantic complexity (single visual prototype vs. multimodal concepts) to quantify method's failure modes and establish guidelines for topic selection.