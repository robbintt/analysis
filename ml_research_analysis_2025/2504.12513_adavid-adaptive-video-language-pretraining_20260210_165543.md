---
ver: rpa2
title: 'AdaVid: Adaptive Video-Language Pretraining'
arxiv_id: '2504.12513'
source_url: https://arxiv.org/abs/2504.12513
tags:
- video
- compute
- transformer
- adavid
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AdaVid, an adaptive video-language pretraining
  framework that enables efficient video encoding on compute-constrained edge devices.
  The core innovation is an adaptive transformer block that dynamically adjusts hidden
  embedding dimensions during inference, allowing a single model to operate across
  different computational footprints.
---

# AdaVid: Adaptive Video-Language Pretraining

## Quick Facts
- arXiv ID: 2504.12513
- Source URL: https://arxiv.org/abs/2504.12513
- Authors: Chaitanya Patel; Juan Carlos Niebles; Ehsan Adeli
- Reference count: 40
- Key outcome: Adaptive video-language pretraining framework enabling efficient inference across computational budgets by dynamically adjusting hidden embedding dimensions

## Executive Summary
AdaVid introduces an adaptive video-language pretraining framework that enables efficient video encoding on compute-constrained edge devices. The core innovation is an adaptive transformer block that dynamically adjusts hidden embedding dimensions during inference, allowing a single model to operate across different computational footprints. AdaVid-EgoVLP, trained on Ego4D, matches standard EgoVLP performance on short video benchmarks while using only half the compute, and outperforms it with equal resources. AdaVid-Agg, a lightweight hierarchical network, aggregates short clip features for long video understanding, achieving strong accuracy across multiple long video benchmarks with significantly reduced computation.

## Method Summary
AdaVid consists of two main components: AdaVid-EgoVLP for short video clips (4 frames) and AdaVid-Agg for long videos via hierarchical aggregation. The core innovation is adaptive transformer layers that sample embedding dimensions during training from {768, 576, 384, 192}, creating nested models where lower-dimensional subspaces are meaningful. During inference, dimensions can be selected based on computational constraints. AdaVid-EgoVLP uses space-time attention and is trained with EgoNCE contrastive loss on video-narration pairs. AdaVid-Agg is a 12-layer transformer that aggregates 16 segment features from frozen AdaVid-EgoVLP features using InfoNCE loss, enabling long video understanding with significantly reduced computation.

## Key Results
- AdaVid-EgoVLP matches EgoVLP performance on short video benchmarks using half the compute (285 vs 714 GFLOPs)
- AdaVid-EgoVLP-FT with 128 frames at 285 GFLOPs achieves 82.2% accuracy on Diving48 vs TimeSformer-L at 714 GFLOPs with 81.0%
- AdaVid-Agg enables long video understanding with strong performance across multiple benchmarks while using significantly reduced computation

## Why This Works (Mechanism)

### Mechanism 1: Coarse-to-Fine Weight Structure via Random Dimension Sampling
Training with randomly sampled embedding dimensions creates a nested model structure that supports inference at multiple compute levels from a single trained model. During each training iteration, embedding dimensions are randomly selected, enforcing a coarse-to-fine structure where the first d values of any weight matrix must produce meaningful outputs for any d ≤ D. This exploits video feature redundancy, allowing lower-dimensional subspaces to capture sufficient semantic information for many downstream tasks.

### Mechanism 2: Hierarchical Dimension Reduction Across Layers
Decreasing dimensions progressively across transformer layers (e.g., 768→576→384→192) yields better accuracy-compute trade-offs than uniform or increasing dimensions. Early layers operate at full dimension to process low-level spatial details, while deeper layers progressively reduce dimension as they encode high-level semantic concepts in smaller subspaces. This aligns with information bottleneck theory—early layers must preserve rich input representations while later layers can compress to task-relevant abstractions.

### Mechanism 3: Frame-Compute Substitution via Adaptive Dimensionality
Within a fixed compute budget, processing more frames at lower dimensions can outperform processing fewer frames at full dimensions for temporally complex video understanding tasks. Reduce embedding dimension (e.g., 768→384, ~4× FLOPs reduction) to free compute budget, then reallocate to process more frames (e.g., 4→16 frames). Since transformer complexity is quadratic in both token count and dimension, this substitution can be compute-neutral while providing richer temporal coverage.

## Foundational Learning

- **Transformer Compute Complexity**: Understanding FLOPs formulas (FFN: 16ND², attention: 8ND² + 4N²D) is essential for calculating compute savings and designing dimension schedules. Quick check: If you reduce embedding dimension from 768 to 384 while keeping token count constant, what is the approximate FLOPs reduction for the FFN layer?
- **Matryoshka Representation Learning**: AdaVid's adaptive training directly applies MRL principles—training with random dimension sampling creates "nested" models where the first d dimensions are meaningful representations. Quick check: Why does MRL training sample dimensions randomly rather than always training at full dimension and truncating at inference?
- **Space-Time Attention**: AdaVid builds on TimeSformer/EgoVLP architectures with divided space-time attention. Understanding this decomposition (spatial attention per frame, then temporal attention across frames) explains why complexity scales linearly rather than quadratically with frame count. Quick check: In space-time attention, why does computational complexity scale linearly with the number of frames T rather than quadratically?

## Architecture Onboarding

- **Component map**: Ego4D video-narration pairs → AdaVid-EgoVLP (12 adaptive transformer layers, max D=768) → DistilBERT text encoder → EgoNCE contrastive loss → AdaVid-Agg (12-layer transformer) → InfoNCE loss with summary annotations
- **Critical path**: Initialize from pretrained EgoVLP weights → Replace all transformer layers with adaptive variants → Training loop: For each batch, randomly sample dimension for each layer → Forward pass with sampled dimensions → Inference: Select evaluation configuration based on compute budget and task requirements
- **Design tradeoffs**: Decreasing dimensions (768→192) outperforms uniform low dimensions at same FLOPs but requires more complex implementation; more frames at lower dimensions wins for temporal tasks; AdaVid-Agg is lightweight but requires separate training
- **Failure signatures**: Early-layer bottlenecks with increasing dimensions cause significant accuracy drops; excessive dimension reduction at d-192 shows sharp accuracy drop on fine-grained tasks; misaligned frame-dimension tradeoff underperforms on temporally complex tasks
- **First 3 experiments**: (1) Baseline replication: Train AdaVid-EgoVLP-dec on Ego4D with 4 frames, evaluate on EgoMCQ at d-768, d-576, d-384 configurations; (2) Dimension schedule ablation: Compare d-dec vs. d-inc vs. uniform configurations at matched FLOPs on EgoMCQ-intra; (3) Frame-dimension tradeoff: On Diving48, evaluate fixed compute budget (~140 GFLOPs) comparing (64 frames, d-768) vs. (128 frames, d-384)

## Open Questions the Paper Calls Out

**Adaptive Retrieval Pipeline**: The authors note it's possible to implement adaptive retrieval where candidate videos are initially ranked with low-compute configurations and selectively re-ranked with high-compute configurations, but leave this exploration for further research.

**Scaling to Larger Datasets**: The paper speculates that with extensive datasets containing high-quality annotations, AdaVid has the potential to train accurate models, but experiments are currently restricted to the Ego4D dataset.

**Dynamic Router Integration**: While the framework provides capability to adapt dimensions, it currently requires manual selection of inference configurations rather than automatically adjusting compute based on semantic complexity of specific video clips.

## Limitations

- Training procedure ambiguity regarding exact dimension sampling probabilities during adaptive training
- Missing optimizer configuration details for AdaVid-EgoVLP training
- Limited ablation studies examining interaction between dimension configurations and frame rates across all benchmark tasks
- No exploration of framework behavior on extremely compute-constrained devices (<100 GFLOPs)

## Confidence

**High Confidence**: Adaptive training creates nested models supporting multiple inference dimensions; decreasing dimension schedules outperform increasing schedules at matched FLOPs; AdaVid-EgoVLP achieves comparable performance to EgoVLP with half the compute; AdaVid-Agg successfully extends short-clip models to long video understanding.

**Medium Confidence**: Frame-dimension tradeoff benefits are task-dependent and require careful calibration; 16× segment aggregation in AdaVid-Agg is optimal for long video tasks; decreasing dimension strategy generalizes across video understanding domains.

**Low Confidence**: Claims about AdaVid's performance on "any computational budget" are not systematically validated; framework's behavior on extremely compute-constrained devices is not explored; long-term temporal dependencies beyond 16 segments are not evaluated.

## Next Checks

1. **Dimension Sampling Ablation**: Systematically evaluate AdaVid-EgoVLP performance across different dimension sampling distributions (uniform, decreasing probability, curriculum-based) during training. Measure impact on convergence speed and final accuracy at various inference configurations.

2. **Frame-Dimension Sweep**: Conduct comprehensive experiments varying both frame count (4, 8, 16, 32, 64) and dimension configurations (d-768, d-384, d-192) on Diving48 and EgoMCQ-intra. Quantify the exact compute-accuracy Pareto frontier and identify task-specific optimal configurations.

3. **Long-Video Boundary Test**: Evaluate AdaVid-Agg's performance with different segment counts (8, 16, 32, 64) on the most temporally complex long video tasks in the benchmark suite. Measure degradation points and identify the practical limits of the hierarchical aggregation approach.