---
ver: rpa2
title: 'FastMMoE: Accelerating Multimodal Large Language Models through Dynamic Expert
  Activation and Routing-Aware Token Pruning'
arxiv_id: '2511.17885'
source_url: https://arxiv.org/abs/2511.17885
tags:
- tokens
- experts
- pruning
- token
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FastMMoE addresses the high computational cost of multimodal large
  language models (MLLMs) caused by long sequences of high-resolution visual tokens.
  The method introduces a training-free acceleration framework for MoE-based MLLMs
  that combines dynamic expert activation reduction for vision tokens with routing-aware
  token pruning based on routing-probability similarity.
---

# FastMMoE: Accelerating Multimodal Large Language Models through Dynamic Expert Activation and Routing-Aware Token Pruning

## Quick Facts
- arXiv ID: 2511.17885
- Source URL: https://arxiv.org/abs/2511.17885
- Reference count: 40
- Primary result: Training-free MoE acceleration achieving up to 55.0% FLOPs reduction while retaining ~95.5% performance on DeepSeek-VL2 and InternVL3.5

## Executive Summary
FastMMoE introduces a training-free acceleration framework for MoE-based multimodal large language models (MLLMs) that addresses the high computational cost of processing long sequences of high-resolution visual tokens. The method combines two complementary strategies: dynamic expert activation reduction for vision tokens and routing-aware token pruning based on routing-probability similarity. Experiments demonstrate significant computational savings (up to 55% FLOPs reduction) while maintaining approximately 95.5% of the original model performance across multiple benchmarks, consistently outperforming dense-model pruning baselines.

## Method Summary
FastMMoE accelerates MoE-based MLLMs by reducing computation for vision tokens through two mechanisms: (1) Expert activation reduction - from a specified layer onwards, vision tokens activate only a subset of experts (Kv < K) while text tokens maintain full activation, with gating weights renormalized among selected experts; (2) Routing-aware token pruning - adjacent visual tokens with highly similar routing probability distributions are identified as redundant and merged or pruned using a sliding window approach that combines routing similarity and attention scores. The method is training-free and works by intercepting the router's output during inference to modify Top-K selection for vision tokens and identify redundancy clusters.

## Key Results
- Achieves up to 55.0% FLOPs reduction on InternVL3.5-30B-A3B while retaining 95.5% performance
- Outperforms FastV and SparseVLM baselines by 4.8% and 4.6% respectively at 75% retention rate
- Vision tokens are more robust to expert activation reduction than text tokens, with Table 3 showing 80.03 average performance vs 77.98 when reducing text experts
- OCR performance degrades significantly with aggressive pruning, dropping to ~66 vs 88 baseline at 25% retention

## Why This Works (Mechanism)

### Mechanism 1: Vision-Specific Expert Activation Reduction
Reducing the number of activated experts (Kv < K) for vision tokens preserves performance better than reducing them for text tokens, enabling computation savings without retraining. The framework applies a modality mask to identify vision tokens and from layer lv onwards selects only the top Kv experts for vision tokens while maintaining full expert activation for text tokens. Vision tokens in MoE-MLLMs exhibit lower "expert utilization" or semantic density than text tokens, meaning they rely less on specialized ensemble knowledge.

### Mechanism 2: Routing-Probability Similarity Pruning
Adjacent visual tokens with highly similar routing probability distributions are semantically redundant and can be merged or pruned with minimal information loss. The method segments visual tokens into sliding windows, computes the cosine similarity of routing probability distributions within each window, and uses high similarity combined with low attention scores to trigger merging or pruning. Tokens activating the same experts (high routing similarity) are functionally equivalent in the MoE context, serving as a superior redundancy proxy compared to attention-only metrics.

### Mechanism 3: Magnitude-Constrained Output Stability
Vision tokens exhibit "magnitude concentration" in expert outputs, meaning the vector norms of outputs from different experts are similar. Because output norms are consistent across experts for vision tokens, removing lower-weighted experts and re-normalizing minimally changes the final vector's magnitude, with deviation primarily angular rather than normative. Expert outputs for vision tokens have a low Coefficient of Variation (CV) in their L2 norms compared to text tokens, ensuring stable magnitude preservation.

## Foundational Learning

- **MoE Gating & Top-K Routing**
  - Why needed here: The entire method relies on intercepting the router's output (softmax over expert centroids) and modifying the Top-K selection specifically for visual tokens
  - Quick check question: How does the gating weight gj change if you reduce the activated expert count K but keep the same top experts?

- **Token Merging vs. Pruning**
  - Why needed here: FastMMoE distinguishes between dropping tokens entirely (removing computation) and merging them (averaging features) to preserve information in high-similarity windows
  - Quick check question: Why might simple arithmetic averaging (mean) disturb expert routing more than magnitude-aware merging (MLERP)?

- **Multimodal Computational Bottlenecks**
  - Why needed here: High-resolution inputs in MLLMs create massive token sequences (Nv). The FLOPs reduction depends on whether the cost is dominated by attention (O(N²)) or the MoE FFN (O(N·K))
  - Quick check question: At what sequence length does the quadratic attention cost typically overshadow the linear MoE computation cost in these architectures?

## Architecture Onboarding

- **Component map**: Input (Vision Tokens + Text Tokens) -> MoE Router (computes Zv, Pv) -> Activation Controller (masks text, reduces Kv for vision from lv) -> Pruning Module (sliding window, computes Cv, merges/prunes)

- **Critical path**: The routing logits (Zv) are the single source of truth for both expert selection (Mechanism 1) and redundancy scoring (Mechanism 2). Errors in routing propagation will compound.

- **Design tradeoffs**:
  - Retention Rate (η) vs. OCR Accuracy: Aggressive pruning (25% retention) destroys OCRBench performance (drops to ~66 vs 88 baseline) due to loss of fine-grained text features
  - Window Size (W) vs. Granularity: Larger windows improve merging efficiency but risk merging semantically distinct objects if routing similarity isn't uniform

- **Failure signatures**:
  - Random/MinK Selection: Table 4 shows selecting experts with lowest weights (MinK) crashes performance (74.16 avg vs 80.52 TopK), proving the importance of the primary experts
  - Shared Expert Reduction: In DeepSeek-VL2, reducing shared experts (Table 8) causes a catastrophic drop in OCR, indicating shared experts hold general knowledge critical for text-in-image tasks

- **First 3 experiments**:
  1. Ablation on Modality: Apply activation reduction (lv=3, Kv=4) to (a) Vision only, (b) Text only, (c) All. Verify Table 3 to confirm vision tokens are the robust target
  2. Similarity Threshold Sensitivity: Vary α (routing vs. attention weight) at 50% retention on InternVL3.5 to find the optimal balance point (expected around α=0.6)
  3. Shared vs. Routed Expert Stability: For DeepSeek-VL2, measure performance drop when reducing only routed experts vs. only shared experts for vision tokens to confirm architectural constraints

## Open Questions the Paper Calls Out

### Open Question 1
Can the pre-training or fine-tuning paradigms of MoE-MLLMs be explicitly optimized to encourage sparser expert activation for vision tokens, rather than relying on post-hoc reduction? The authors conclude that their "findings highlight that current MoE-MLLMs often underutilize visual experts, suggesting new opportunities for... future research" regarding training refinement.

### Open Question 2
Would an adaptive, token-specific expert activation strategy outperform the uniform reduction strategy employed in FastMMoE? The introduction mentions the concept of assigning fewer experts to "easy" tokens, but the method implements a uniform reduction for "all visual tokens—without differentiating among them."

### Open Question 3
How does the ratio of shared-to-routed experts in the backbone architecture influence the robustness of training-free activation reduction? The ablation study (Appendix B.4) reveals that shared experts carry critical modality-agnostic knowledge; reducing them causes a >10% drop in OCR performance, whereas reducing routed experts is safe.

## Limitations
- Modality identification granularity - the method assumes clean separation between vision and text tokens, but mixed-token sequences in MLLMs blur these boundaries
- Routing probability interpretability - the pruning mechanism depends on stable, interpretable routing distributions that may not hold for poorly trained or stochastic routers
- OCR and text-in-image fragility - aggressive pruning severely degrades OCR performance (down to ~66 vs 88 baseline at 25% retention), limiting applicability for text-heavy tasks
- Architecture constraints - the expert activation reduction cannot be applied to shared experts in DeepSeek-VL2, restricting the method's generality

## Confidence

**High Confidence**: FLOPs reduction measurements (55.0% max) and general performance retention (~95.5%) across multiple benchmarks (MMMU, ScienceQA, MMBench) are well-supported by ablation tables and baseline comparisons.

**Medium Confidence**: The vision-specific activation reduction mechanism is well-supported by ablation (Table 3) and logical reasoning about modality differences, though the assumption about vision tokens relying less on specialized ensemble knowledge needs external validation.

**Low Confidence**: The routing-probability similarity pruning mechanism relies heavily on the paper's internal routing distribution analysis (Figure 1). Without access to the same models or routing visualizations, the claimed 95% similarity in adjacent tokens cannot be independently verified.

## Next Checks

1. **Routing Distribution Stability Test**: For InternVL3.5, extract routing distributions from multiple forward passes on the same image. Compute routing entropy and cosine similarity variance across passes. If similarity is not consistently >95% for adjacent tokens, the pruning mechanism's redundancy assumption fails.

2. **OCR Token Sensitivity Analysis**: On DeepSeek-VL2, identify OCR tokens (e.g., via OCRBench inputs). Apply the full FastMMoE pipeline with 25% retention. Measure OCR accuracy drop. If drop exceeds 20 points, the method is unsafe for text-in-image tasks and requires OCR-specific safeguards.

3. **Shared Expert Reduction Failure Reproduction**: For DeepSeek-VL2, implement the activation reduction on only the shared experts (not routed experts) for vision tokens. Measure OCRBench performance. If performance drops below 60 (vs 88 baseline), it confirms the architectural constraint and the method's incompatibility with shared expert-heavy designs.