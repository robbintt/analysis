---
ver: rpa2
title: Data-Free Continual Learning of Server Models in Model-Heterogeneous Cloud-Device
  Collaboration
arxiv_id: '2509.25977'
source_url: https://arxiv.org/abs/2509.25977
tags:
- knowledge
- learning
- data
- federated
- server
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling heterogeneous models
  deployed on edge devices to collaboratively and continuously train a central server
  model in federated continual learning settings, while overcoming catastrophic forgetting,
  model heterogeneity, and knowledge misalignment. The authors propose FedDCL, which
  leverages pre-trained diffusion models to extract lightweight class-specific prototypes.
---

# Data-Free Continual Learning of Server Models in Model-Heterogeneous Cloud-Device Collaboration

## Quick Facts
- arXiv ID: 2509.25977
- Source URL: https://arxiv.org/abs/2509.25977
- Reference count: 40
- Key outcome: Proposed FedDCL framework achieves 9.00%/6.48%/23.67%/16.40% improvements in server accuracy compared to state-of-the-art baselines across different non-IID settings while maintaining lower forgetting measures.

## Executive Summary
This paper addresses the challenge of enabling heterogeneous models deployed on edge devices to collaboratively and continuously train a central server model in federated continual learning settings. The authors propose FedDCL, which leverages pre-trained diffusion models to extract lightweight class-specific prototypes that enable data-free knowledge distillation and exemplar-free knowledge replay. This approach dynamically aligns with evolving task distributions while overcoming catastrophic forgetting, model heterogeneity, and knowledge misalignment. Experimental results on Grayscale and RGB datasets demonstrate significant performance improvements over existing baselines while enabling effective collaboration among heterogeneous models without requiring static public datasets.

## Method Summary
FedDCL introduces a novel approach to federated continual learning that addresses the challenges of model heterogeneity and data privacy. The framework extracts class-specific prototypes from pre-trained diffusion models, which serve as lightweight representations for knowledge distillation and replay. These prototypes enable data-free knowledge transfer between heterogeneous edge models and the central server, while also supporting exemplar-free knowledge replay to mitigate catastrophic forgetting. The system employs a multi-teacher distillation strategy that preserves both current and historical task knowledge, allowing the server model to continuously learn from diverse edge devices with different model architectures. By eliminating the need for static public datasets and leveraging diffusion model-generated prototypes, FedDCL creates a flexible and privacy-preserving collaborative learning environment.

## Key Results
- FedDCL achieves 9.00% improvement in server accuracy compared to state-of-the-art baselines in non-IID settings
- The framework demonstrates 6.48% accuracy gains in RGB dataset evaluations under heterogeneous model conditions
- FedDCL shows 23.67% improvement in knowledge preservation metrics while maintaining 16.40% lower forgetting measures across experimental conditions

## Why This Works (Mechanism)
The proposed method works by leveraging pre-trained diffusion models to extract class-specific prototypes that serve as universal knowledge representations. These lightweight prototypes bridge the gap between heterogeneous edge models by providing a common semantic space for knowledge transfer. The data-free knowledge distillation mechanism uses these prototypes to guide the server model's learning without requiring access to raw training data, preserving privacy while enabling effective collaboration. The exemplar-free knowledge replay component stores and utilizes these prototypes to prevent catastrophic forgetting of previously learned tasks. The multi-teacher distillation strategy aggregates knowledge from multiple heterogeneous edge models, ensuring the server model benefits from diverse perspectives while maintaining task-specific expertise.

## Foundational Learning
- **Federated Continual Learning**: Distributed learning paradigm where multiple clients collaboratively train a global model while preserving data privacy and handling sequential task arrivals. Needed to address real-world scenarios where edge devices must learn continuously without sharing raw data. Quick check: Verify federated averaging and privacy-preserving aggregation mechanisms function correctly.
- **Model Heterogeneity**: Situation where edge devices deploy different model architectures or capacities, requiring adaptive training strategies. Needed because real-world deployments rarely have uniform model specifications across all devices. Quick check: Confirm knowledge transfer mechanisms work across varying model depths and widths.
- **Catastrophic Forgetting**: Phenomenon where neural networks lose previously learned information when trained on new tasks sequentially. Needed to ensure the server model maintains long-term knowledge while adapting to new tasks. Quick check: Measure forgetting metrics across multiple task sequences.
- **Knowledge Distillation**: Technique for transferring knowledge from larger or ensemble models to smaller ones by matching output distributions. Needed to enable effective knowledge transfer between heterogeneous models without direct parameter sharing. Quick check: Validate distillation loss convergence and knowledge transfer quality.
- **Diffusion Models**: Generative models that learn to reverse a gradual noising process to generate data. Needed to extract meaningful class-specific prototypes without requiring access to original training data. Quick check: Ensure prototype quality and diversity across different classes.

## Architecture Onboarding

**Component Map**: Edge Devices (Heterogeneous Models) -> Prototype Extraction (Diffusion Models) -> Knowledge Distillation (Multi-Teacher) -> Server Model -> Knowledge Replay (Exemplar-Free)

**Critical Path**: The core workflow involves edge devices generating predictions, which are then used with diffusion model prototypes for knowledge distillation. The server model aggregates this knowledge while maintaining historical information through exemplar-free replay mechanisms.

**Design Tradeoffs**: The approach trades computational overhead of diffusion model operations against the benefits of data-free knowledge transfer and privacy preservation. While prototype extraction adds complexity, it eliminates the need for raw data sharing and static public datasets, making the system more scalable and privacy-compliant.

**Failure Signatures**: System failures may manifest as degraded prototype quality leading to ineffective knowledge transfer, catastrophic forgetting during sequential task learning, or communication bottlenecks when aggregating knowledge from numerous heterogeneous edge devices. Performance degradation may also occur if diffusion models are not well-aligned with the task distribution.

**First 3 Experiments**: 1) Validate prototype extraction quality across different classes and tasks using standard similarity metrics. 2) Test knowledge distillation effectiveness between heterogeneous model pairs with varying architectural differences. 3) Evaluate forgetting metrics during sequential task learning with controlled task sequences.

## Open Questions the Paper Calls Out
None

## Limitations
- The paper does not provide implementation details or code availability, raising reproducibility concerns for the proposed FedDCL framework.
- Heavy reliance on pre-trained diffusion models introduces significant dependencies on external resources that may not be universally accessible or computationally feasible for all edge devices.
- Evaluation focuses primarily on image classification tasks with specific Grayscale and RGB datasets, leaving unclear whether the approach generalizes to other modalities or more complex vision tasks.
- The paper does not address potential privacy concerns that may arise from using diffusion models to generate class-specific prototypes, which could inadvertently leak information about training data distributions.

## Confidence

**High Confidence**: The claim about addressing model heterogeneity in federated continual learning is well-supported by the proposed architecture and experimental design. The demonstration of improved accuracy over baselines in non-IID settings is methodologically sound.

**Medium Confidence**: The effectiveness of data-free knowledge distillation using diffusion model prototypes is promising but requires further validation across diverse datasets and real-world deployment scenarios. The claim about achieving exemplar-free knowledge replay without compromising performance needs additional empirical support.

**Low Confidence**: The assertion that FedDCL can effectively collaborate among heterogeneous models "without requiring static public datasets" needs more rigorous validation, as the diffusion model dependency may introduce implicit data requirements.

## Next Checks
1. Conduct ablation studies to quantify the contribution of each component (prototype extraction, knowledge distillation, knowledge replay) to the overall performance improvements, particularly focusing on the diffusion model's impact.

2. Evaluate the framework's robustness to varying degrees of model heterogeneity beyond the tested configurations, including extreme cases where device models have significantly different architectures or capacities.

3. Test the approach on non-vision tasks and more diverse datasets (e.g., natural language processing, time series) to assess generalizability beyond the current image classification focus.