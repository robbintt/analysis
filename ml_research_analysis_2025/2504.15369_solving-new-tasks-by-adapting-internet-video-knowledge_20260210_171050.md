---
ver: rpa2
title: Solving New Tasks by Adapting Internet Video Knowledge
arxiv_id: '2504.15369'
source_url: https://arxiv.org/abs/2504.15369
tags:
- video
- adaptation
- tasks
- in-domain
- inverse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates adapting internet-scale pretrained video\
  \ generative models to in-domain robotic data to enable novel task generalization.\
  \ The authors compare three adaptation techniques\u2014direct finetuning, subject\
  \ customization, and probabilistic adaptation\u2014across visual planning and policy\
  \ supervision tasks."
---

# Solving New Tasks by Adapting Internet Video Knowledge

## Quick Facts
- arXiv ID: 2504.15369
- Source URL: https://arxiv.org/abs/2504.15369
- Reference count: 22
- Primary result: Inverse probabilistic adaptation consistently outperforms standard probabilistic adaptation and direct finetuning for text-conditioned generalization to novel robotic tasks using adapted video generative models.

## Executive Summary
This work investigates adapting internet-scale pretrained video generative models to in-domain robotic data to enable novel task generalization. The authors compare three adaptation techniques—direct finetuning, subject customization, and probabilistic adaptation—across visual planning and policy supervision tasks. A novel inverse probabilistic adaptation method consistently achieves strong performance, showing robustness even with suboptimal demonstrations. Across MetaWorld and DeepMind Control Suite benchmarks, adapted models enable text-conditioned generalization to unseen tasks. Probabilistic adaptation and its inverse variant notably improve performance over vanilla models, with inverse probabilistic adaptation achieving the highest overall success rates. The findings demonstrate that cheap, data-efficient adaptation can effectively integrate large-scale video priors for downstream robotics, opening avenues for flexible, language-driven robot behavior learning.

## Method Summary
The method adapts pretrained text-to-video models (AnimateDiff) to in-domain robotic environments through three techniques: (1) Direct Finetuning of the motion module on 20-25 expert demonstrations; (2) Subject Customization using DreamBooth fine-tuning on 20 static images to bind visual appearance while preserving motion priors; and (3) Probabilistic Adaptation training a small AVDC-based model on demonstrations and composing its scores with the frozen AnimateDiff model. The novel inverse probabilistic adaptation inverts the score composition direction, letting the pretrained model control generation while consulting the small in-domain model for domain-specific information. Two evaluation modes are used: visual planning via inverse dynamics models and policy supervision using Video-TADPoLe rewards. All methods are tested on MetaWorld and DeepMind Control Suite benchmarks for text-conditioned generalization to novel tasks.

## Key Results
- Inverse probabilistic adaptation achieves highest overall success rates across MetaWorld and DeepMind Control Suite benchmarks
- Subject customization significantly improves locomotion performance using only static images by transferring pretrained motion priors
- Probabilistic adaptation remains effective with suboptimal demonstrations, showing robustness to 70% random actions in training data
- Direct finetuning shows marginal improvement over vanilla AnimateDiff in continuous control tasks
- Adapted models enable text-conditioned generalization to unseen tasks across both visual planning and policy supervision modes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Inverse probabilistic adaptation enables robust generalization by letting the pretrained model control generation while consulting a small in-domain model for environment-specific information.
- Mechanism: Score composition inverts the standard direction: rather than the small model generating with pretrained guidance, the pretrained model generates with domain-specific correction. Equation 3: ˜ϵ_inv = ϵ_pretrained(τ_t, t) + α(ϵ_pretrained(τ_t, t | text) + γϵ_θ(τ_t, t | text) − ϵ_pretrained(τ_t, t)). The pretrained model's stronger text-conditioning capability handles novel prompts, while the small model injects domain dynamics.
- Core assumption: The pretrained model's text priors are more reliable for novel prompts than the small in-domain model's text-conditioned scores.
- Evidence anchors:
  - [Section 3.1.3] "In inverse probabilistic adaptation, the pretrained video model controls the generation process, while consulting the small model for domain-specific information."
  - [Section 4.2] "inverse probabilistic adaptation is more performant; it may be more robust to novel text-conditioning, as more weight is put on leveraging textual priors from the pretrained model."
  - [corpus] Neighbor paper "Self-Adapting Improvement Loops for Robotic Learning" similarly addresses generalization to unseen tasks with video models, but does not test the inverse composition direction explicitly—suggesting this inversion may be this work's specific contribution.
- Break condition: If the in-domain model's text-conditioned scores are more accurate than the pretrained model's for a given prompt, the inverse direction may underperform standard probabilistic adaptation.

### Mechanism 2
- Claim: Subject customization can improve locomotion performance using only static images by transferring pretrained motion priors to domain-specific visual appearance.
- Mechanism: DreamBooth fine-tunes the image encoder and U-Net on ~20 static images with a special identifier (e.g., "[D] robot arm"). The motion module remains frozen, preserving internet-scale motion priors while binding visual appearance to the identifier. At inference, text prompts with the identifier invoke domain-specific visuals with pretrained dynamics.
- Core assumption: The pretrained motion module's generic motion priors transfer to the domain-specific embodiment without modification.
- Evidence anchors:
  - [Section 4.2, Table 1] Subject customization achieves 117.9 episode return on Dog Walking vs. 76.2 for vanilla AnimateDiff, using only static images.
  - [Section 3.1.2] "this adaptation technique will not expose any subject motions to the video model, it also allows us to study whether the pretrained video model can directly transfer its motion prior... onto arbitrary in-domain subjects."
  - [corpus] Corpus does not provide direct validation of motion prior transfer; this remains an inferred mechanism based on empirical results.
- Break condition: If domain-specific dynamics differ substantially from internet motion priors (e.g., unusual robot morphologies), frozen motion modules may generate physically implausible trajectories.

### Mechanism 3
- Claim: Probabilistic adaptation remains effective with suboptimal demonstrations because pretrained priors can bridge the gap between data quality and task performance.
- Mechanism: The small in-domain model trained on suboptimal data learns approximate domain dynamics but produces noisy text-conditioned scores. Inverse probabilistic adaptation relies primarily on the pretrained model for text alignment, using the small model only for coarse domain grounding. This decouples data quality from generalization capability.
- Core assumption: The pretrained model's motion priors are sufficiently general to correct for in-domain model inaccuracies when composed via inverse weighting.
- Evidence anchors:
  - [Section 4.3, Table 5] With 70% random actions in training data, inverse probabilistic adaptation maintains 27.4% overall success vs. 20.0% for in-domain-only.
  - [Section 4.3] "the overall average task success rate remains robust for inverse probabilistic adaptation... expert demonstrations may not be explicitly needed."
  - [corpus] Neighbor paper "ROSA" addresses self-adaptation under uncertainty but through knowledge-based architectures rather than probabilistic composition—mechanism for handling suboptimal data via pretrained priors is not directly validated in corpus.
- Break condition: If suboptimal data contains systematic errors (e.g., consistently wrong actions for a task), the in-domain model may reinforce these errors despite pretrained correction.

## Foundational Learning

- Concept: **Diffusion denoising objective and classifier-free guidance**
  - Why needed here: All adaptation methods build on the standard denoising loss L_denoise = E[||ε − ε_θ(τ_t, t | text)||²] and use classifier-free guidance for text-conditioning. Without this foundation, score composition equations (2-3) are opaque.
  - Quick check question: Given noise level t and noisy video τ_t, what does ε_θ(τ_t, t | text) predict?

- Concept: **Score composition for diffusion models**
  - Why needed here: Probabilistic adaptation combines scores from two models using weighted sum. Understanding why low-temperature sampling and prior strength (γ) control the balance is essential for tuning.
  - Quick check question: In Equation 2, what happens when γ → 0 vs. γ → ∞?

- Concept: **Inverse dynamics models for visual planning**
  - Why needed here: Visual planning requires converting predicted video frames into executable actions. The inverse dynamics model p(a | s, s') is trained separately and its errors compound with video prediction errors.
  - Quick check question: If the inverse dynamics model has 10% action prediction error, how does this affect closed-loop vs. open-loop planning?

## Architecture Onboarding

- Component map:
  - Pretrained backbone: AnimateDiff (~1.5B params) with frozen StableDiffusion U-Net + motion module trained on WebVid-10M
  - In-domain model: A VDC-based pixel-space diffusion (~109M params), modified to share AnimateDiff's latent space for score composition
  - Inverse dynamics model: Small MLP (1536-dim VC-1 embeddings → 4-dim action)
  - Policy backbone: TDMPC for policy supervision experiments

- Critical path:
  1. Collect 20-25 in-domain demonstrations with text labels
  2. Train in-domain model on demonstrations (60K steps, lr=1e-4)
  3. For subject customization: fine-tune StableDiffusion with DreamBooth on 20 static images
  4. For inverse probabilistic adaptation: load both models, set prior strength γ=0.5, guidance scale α
  5. Generate video plans via DDIM (25 steps) or compute Video-TADPoLe rewards

- Design tradeoffs:
  - **Direct finetuning** (~highest resource, risks catastrophic forgetting): Best for novel text-conditioned motions (e.g., "dog jumping")
  - **Subject customization** (~lowest resource, no motion learning): Strong for locomotion with standard motions
  - **Inverse probabilistic adaptation** (~moderate resource, best generalization): Robust to suboptimal data and novel tasks

- Failure signatures:
  - Generated videos show domain-incorrect objects → in-domain model not trained sufficiently or latent space mismatch
  - Policy achieves high reward but low task success → reward misalignment; tune Video-TADPoLe noise level via policy discrimination
  - Visual planning fails on novel tasks but succeeds on seen tasks → pretrained text priors not being leveraged; increase prior strength γ

- First 3 experiments:
  1. Replicate subject customization on DeepMind Control Dog Walking with 20 static images; verify improvement over vanilla AnimateDiff (target: >100 episode return)
  2. Train in-domain model on 25 MetaWorld demonstrations; compare standard vs. inverse probabilistic adaptation on 2 held-out tasks (target: inverse variant achieves >50% success on Door Open)
  3. Corrupt training data with 70% random actions; verify inverse probabilistic adaptation maintains >25% overall success rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does inverse probabilistic adaptation significantly outperform probabilistic adaptation under policy supervision, yet both achieve similar success rates under visual planning?
- Basis in paper: [explicit] The authors explicitly state in the conclusion: "performance differences that are difficult to explain may arise. For example, while inverse probabilistic adaptation outperforms its inverse version significantly under the policy supervision setup, both achieve similar success rates under the video planning setup."
- Why unresolved: The mechanisms underlying the two evaluation approaches differ fundamentally (discriminative evaluation vs generative planning), but the interaction with the inversion of score composition direction remains poorly understood.
- What evidence would resolve it: Ablation studies isolating the contribution of each score component across both setups, paired with analysis of how text-conditioning robustness affects each evaluation mode differently.

### Open Question 2
- Question: How can suboptimal demonstration data be more systematically leveraged for adaptation?
- Basis in paper: [explicit] The conclusion identifies "a natural next step to further study is the use of suboptimal data, which can be collected cheaply by running random policies on tasks of interest, or iteratively augmented from on-policy observations."
- Why unresolved: While inverse probabilistic adaptation showed robustness to suboptimal data, probabilistic adaptation degraded substantially; the principled integration of low-quality data remains an open challenge.
- What evidence would resolve it: Systematic experiments varying data quality levels across adaptation techniques, combined with iterative data augmentation strategies from on-policy rollouts.

### Open Question 3
- Question: What determines whether direct finetuning will improve or degrade downstream task performance?
- Basis in paper: [inferred] Direct finetuning showed marginal improvement over vanilla AnimateDiff in continuous control (Table 1) but performed on par with the small in-domain model in visual planning (Table 3), suggesting sensitivity to domain and task complexity that is not explained.
- Why unresolved: The paper notes finetuning may cause model collapse or catastrophic forgetting with limited samples, but does not characterize the boundary conditions for when adaptation succeeds versus fails.
- What evidence would resolve it: Scaling curves relating demonstration quantity and diversity to finetuning success, alongside diagnostics measuring forgetting of pretraining priors during adaptation.

## Limitations
- Latent space alignment between AnimateDiff and custom AVDC model is only abstractly described without validation
- Motion prior transfer claims for subject customization are inferred from results rather than directly measured
- Robustness to suboptimal data lacks systematic ablation studies isolating pretrained priors versus in-domain corrections
- Inverse probabilistic adaptation assumes additive score composition works across models trained on different datasets without further alignment

## Confidence

- **High confidence**: Direct finetuning results and hyperparameters are fully specified; subject customization follows established DreamBooth protocol
- **Medium confidence**: Probabilistic adaptation and inverse variant mechanisms are described but latent space matching is hand-waved; suboptimal data results show trends but lack systematic ablation
- **Low confidence**: Motion prior transfer claims for subject customization; exact AVDC architecture modifications for latent space compatibility

## Next Checks

1. **Latent space alignment verification**: Generate video plans using both AnimateDiff and the custom AVDC model independently on the same text prompt; measure latent space similarity (e.g., MSE between latents) to verify the assumption enabling score composition.

2. **Motion prior isolation**: For subject customization, generate videos using the vanilla identifier vs. "[D]" identifier on the same static image; measure FVD difference to quantify motion prior contribution separate from visual appearance.

3. **Suboptimal data ablation**: Train inverse probabilistic adaptation with 0%, 30%, 70%, and 100% random actions; measure the gap between pretrained-only performance and the inverse variant to isolate the value of in-domain data.