---
ver: rpa2
title: 'OI-Bench: An Option Injection Benchmark for Evaluating LLM Susceptibility
  to Directive Interference'
arxiv_id: '2601.13300'
source_url: https://arxiv.org/abs/2601.13300
tags:
- option
- arxiv
- injection
- question
- directive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Option injection reveals that large language models remain vulnerable\
  \ to benign but irrelevant content within choice interfaces. Adding a single injected\
  \ option containing directive cues\u2014ranging from social compliance and bonus\
  \ framing to threat-based coercion\u2014significantly degrades accuracy on knowledge,\
  \ reasoning, and commonsense tasks, with threat framing causing the largest performance\
  \ drops."
---

# OI-Bench: An Option Injection Benchmark for Evaluating LLM Susceptibility to Directive Interference

## Quick Facts
- arXiv ID: 2601.13300
- Source URL: https://arxiv.org/abs/2601.13300
- Reference count: 40
- One-line primary result: Large language models remain vulnerable to benign but irrelevant content within choice interfaces, with threat framing causing the largest performance drops.

## Executive Summary
Option injection reveals that large language models remain vulnerable to benign but irrelevant content within choice interfaces. Adding a single injected option containing directive cues—ranging from social compliance and bonus framing to threat-based coercion—significantly degrades accuracy on knowledge, reasoning, and commonsense tasks, with threat framing causing the largest performance drops. Despite high baseline capability, models such as Gemini-2.5-pro and Deepseek-v3.2 exhibit high attack success rates when exposed to injected options, indicating that capability does not guarantee robustness. Defensive prompting and safety-aligned models offer limited mitigation, while post-training preference-based fine-tuning via DPO and PPO more effectively reduce injection-induced errors. Attention analysis shows that alignment fine-tuning suppresses deep-layer attention advantages for injected options, further supporting the role of targeted adaptation in improving resilience to option-level interference.

## Method Summary
The benchmark constructs 3,000 multiple-choice questions from MMLU, LogiQA, and HellaSwag, filtered via IRT-based selection. Each question is presented with options A-D and an injected 5th option E containing directive cues from 16 types across four categories (social compliance, bonus framing, threat framing, instructional interference). Evaluation uses zero-shot prompting with constrained output format, measuring standard accuracy, injected accuracy, attack success rate, and accuracy drop. Response types (E-induced, E-influenced, E-ignored, E-rejected) are annotated via multi-judge voting to construct preference data for DPO and PPO fine-tuning. The fine-tuning uses LoRA with specific hyperparameters and an external reward model for PPO.

## Key Results
- Threat framing causes the largest performance degradation (19.8% mean ASR, 10.9% accuracy drop)
- Capability does not guarantee robustness—high-performing models like Gemini-2.5-pro show high attack success rates
- DPO and PPO fine-tuning more effectively reduce injection-induced errors than defensive prompting or safety-aligned models
- Attention analysis shows alignment fine-tuning suppresses deep-layer attention advantages for injected options

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Directive cues embedded in irrelevant options capture disproportionate attention in deep transformer layers, overriding task-relevant reasoning.
- Mechanism: The paper shows that in base models, attention contribution to the injected option E is systematically higher than the mean attention to legitimate options A–D in deep layers. These deep-layer attention heads, which prior work associates with information integration and reasoning, allocate excess attention to the directive content, causing the model to treat the injected instruction as task-relevant.
- Core assumption: Deep-layer attention patterns causally influence final selection behavior; suppressing this attention should reduce susceptibility.
- Evidence anchors: Section 5.4 and Appendix I show Δ = (E − μ̂) is strongly positive in deep-layer heads for base models, and PPO fine-tuning reduces this difference; "Attention analysis shows that alignment fine-tuning suppresses deep-layer attention advantages for injected options."

### Mechanism 2
- Claim: Threat framing (penalty-based coercion) causes larger performance degradation than bonus framing (reward-based inducement) because models exhibit greater sensitivity to loss aversion signals than gain-oriented cues.
- Mechanism: Override Penalty yields higher median ASR than Override Bonus across models. The paper suggests models weight negative outcome framing more heavily than positive framing when both carry explicit override instructions. This aligns with the observed 19.8% mean ASR for threat framing versus 12.7% for bonus framing.
- Core assumption: The directive content's framing (loss vs. gain) is the causal factor, not other linguistic features of the prompts.
- Evidence anchors: Section 4.2 reports Threat Framing causes the highest mean ASR (19.8%) and accuracy drop (10.9%), with Override Penalty as the most disruptive single directive; Figure 3 shows Override Penalty has higher median ASR and wider dispersion than Override Bonus.

### Mechanism 3
- Claim: Post-training alignment via preference optimization reduces injection susceptibility by learning to recognize and down-weight directive content rather than merely filtering specific phrases.
- Mechanism: DPO and PPO fine-tuning construct preference pairs where responses that reject or ignore injected options are preferred over induced/influenced responses. The trained model learns a content-level discrimination rather than surface-level filtering, generalizing across directive types. Attention analysis confirms reduced deep-layer attention to E after PPO.
- Core assumption: The preference signal captures genuine directive-awareness rather than overfitting to training directive templates.
- Evidence anchors: Table 3 shows DPO and PPO reduce mean ASR relative to base on Qwen-3-8B, while defensive prompting and guard models increase ASR in some cases; Section 5.2.3 describes preference data construction from annotated response types (E-rejected preferred, E-induced dispreferred).

## Foundational Learning

- Concept: **Multiple-Choice Question Answering (MCQA) as a benchmarking paradigm**
  - Why needed here: OI-Bench extends standard 4-option MCQA by injecting a 5th option. Understanding the baseline evaluation setup is required to interpret attack success rates and accuracy drops.
  - Quick check question: Given a question with options A–D and ground truth B, what happens to accuracy if the model selects E when E is appended but contains no correct answer?

- Concept: **Attention contribution norms in transformers**
  - Why needed here: The paper's mechanistic analysis relies on comparing attention allocated to option E vs. options A–D at the final token position across layers and heads.
  - Quick check question: At the last token before generation, how would you compute the relative attention contribution of option E versus the mean of options A–D?

- Concept: **Direct Preference Optimization (DPO) and Proximal Policy Optimization (PPO)**
  - Why needed here: The paper uses DPO and PPO as mitigation strategies. DPO learns from preference pairs; PPO uses reward signals. Understanding these is required to implement or extend the defense experiments.
  - Quick check question: In DPO, what objective is optimized when given a prompt with a chosen response and a rejected response?

## Architecture Onboarding

- Component map: Dataset construction (MMLU/LogiQA/HellaSwag) -> Directive injection (16 types across 4 categories) -> Zero-shot evaluation (temp=0, max_tokens=8192) -> Response annotation (E-induced/E-influenced/E-ignored/E-rejected) -> Preference data construction -> DPO/PPO fine-tuning (LoRA, external reward model) -> Attention analysis (deep-layer comparison)

- Critical path: 1. Run baseline evaluation on 4-option MCQA to establish Standard Accuracy. 2. Append injected option E for each directive type; measure Injected Accuracy and compute ASR. 3. Annotate responses to construct preference data for DPO/PPO. 4. Fine-tune with DPO or PPO; re-evaluate to measure ASR reduction. 5. Optional: Extract attention norms at final token position to verify deep-layer suppression of E.

- Design tradeoffs:
  - Append vs. permute injection: Appending E as the 5th option yields lower ASR than permuting E into positions A–D. The paper tests both; permuting increases vulnerability but may be less ecologically valid for MCQA benchmarks.
  - Defensive prompting vs. alignment: Defensive prompting is inference-time and cheap but ineffective; alignment requires training compute and data but shows stronger mitigation.
  - DPO vs. PPO: DPO shows more concentrated attention reallocation; PPO shows broader attenuation. DPO is simpler (no reward model); PPO allows finer-grained reward shaping.

- Failure signatures:
  - High ASR with low E-selection rate: Model does not explicitly choose E, but E biases reasoning toward incorrect A–D selections.
  - Negative accuracy drop: Some models show improved accuracy under certain injection types; may indicate injection eliciting more careful reasoning or evaluation noise.
  - Guard model underperforms base: Qwen3Guard-Gen-8B shows higher ASR than base Qwen-3-8B on some datasets, suggesting safety fine-tuning does not transfer to directive interference.

- First 3 experiments:
  1. **Baseline susceptibility profiling**: Evaluate your target model on OI-Bench across all 16 directive types. Compute per-directive ASR to identify which framing categories (threat, bonus, social, instructional) are most impactful for your model.
  2. **Position sensitivity test**: Compare ASR when E is appended (position 5) vs. permuted into positions A–D. This quantifies positional bias and determines whether your model's vulnerability is specific to anomalous option positions.
  3. **Minimal DPO mitigation**: Construct preference pairs from a subset of annotated responses (e.g., 1,000 E-rejected vs. 1,000 E-induced). Fine-tune with DPO using the paper's LoRA configuration and measure ASR reduction on a held-out question set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can training-free defense strategies effectively mitigate option injection vulnerabilities without requiring costly fine-tuning?
- Basis in paper: The conclusion states "several critical future directions can be further explored, such as additional directive designs and training-free defense strategies."
- Why unresolved: The paper evaluates defensive prompting but finds it unreliable; only post-training alignment (DPO/PPO) shows promise. No training-free method was identified as effective.
- What evidence would resolve it: Systematic evaluation of inference-time techniques (e.g., instruction hierarchy enforcement, confidence calibration, or ensemble voting) showing reduced ASR without fine-tuning.

### Open Question 2
- Question: Why does option injection occasionally improve accuracy (negative accuracy drop) in some model–directive combinations?
- Basis in paper: Section 5.3 notes "injecting an irrelevant option does not necessarily degrade MCQA performance... A comprehensive investigation is left to future work."
- Why unresolved: The phenomenon is observed but not explained; it is unclear whether injection triggers more careful reasoning or represents noise in specific cases.
- What evidence would resolve it: Controlled studies analyzing reasoning traces before and after injection, correlating accuracy changes with response types and directive characteristics.

### Open Question 3
- Question: Do option injection vulnerabilities generalize to broader decision-making tasks beyond MCQA, such as planning, ranking, or tool selection?
- Basis in paper: The limitations section states results "may not transfer to broader decision tasks that require a plan, a sequence of actions, or longer reasoning."
- Why unresolved: MCQA has a fixed choice structure; open-ended or sequential tasks may exhibit different susceptibility patterns to directive interference.
- What evidence would resolve it: Benchmarks applying option injection paradigms to tasks like multi-step planning, retrieval-augmented generation routing, or LLM-as-judge ranking scenarios.

## Limitations
- Ecological validity concerns—controlled synthetic injection may not capture real-world option injection in dynamic user interfaces
- Mitigation generalization limited—defense experiments evaluated only on Qwen-3-8B with specific hyperparameters
- Causal mechanism unclear—attention analysis shows correlation but not causation between deep-layer patterns and injection susceptibility
- Safety alignment transfer failure—guard models show higher ASR than base models, suggesting current alignment methods insufficient for directive interference

## Confidence
**High confidence** in ASR measurement methodology and baseline susceptibility results—the zero-shot evaluation protocol is clearly specified, and the 16 directive types are explicitly enumerated with consistent formatting. The accuracy drops (10.9% mean, up to 18.7% for Override Penalty) are reproducible given access to the question sets and directive templates.

**Medium confidence** in the mechanistic claims about deep-layer attention patterns—while the Δ = (E − μ̂) metric shows systematic differences in base vs. fine-tuned models, the paper does not establish temporal precedence or conduct ablation studies to confirm attention reallocation causes ASR reduction rather than correlating with it.

**Low confidence** in the generalization of PPO/DPO mitigation—the defense experiments are limited to Qwen-3-8B with specific hyperparameters (β=0.1, LoRA rank 64 for DPO; GPT-OSS-120B judge for PPO). The preference data construction process and reward model specifics are not fully specified, making replication challenging.

## Next Checks
1. **Cross-architecture mitigation validation**: Replicate the DPO/PPO fine-tuning experiments on at least two additional base models (e.g., Llama-3-8B and Mistral-7B) to assess whether preference optimization generalizes beyond Qwen-3-8B. Measure ASR reduction and attention pattern changes to confirm the deep-layer suppression mechanism holds across architectures.

2. **Ecological validity assessment**: Design a dynamic interface simulation where injected options appear in non-terminal positions (permuted A–D) and vary in visual prominence (font size, color). Measure ASR under these conditions to determine whether the vulnerability persists when injection violates standard MCQA formatting conventions.

3. **Attention ablation study**: Implement targeted attention masking in deep layers for the injected option E during inference. Compare ASR with and without masking to establish causal links between deep-layer attention allocation and injection susceptibility, addressing the correlational nature of the current attention analysis.