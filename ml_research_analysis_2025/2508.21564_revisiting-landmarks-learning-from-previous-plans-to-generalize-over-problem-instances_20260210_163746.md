---
ver: rpa2
title: 'Revisiting Landmarks: Learning from Previous Plans to Generalize over Problem
  Instances'
arxiv_id: '2508.21564'
source_url: https://arxiv.org/abs/2508.21564
tags:
- landmarks
- state
- generalized
- landmark
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to learn generalized landmarks that
  automatically apply across a domain by using state functions rather than object-specific
  atoms. These landmarks are discovered from plans of a small set of solved instances
  and can capture repetitive patterns with loops in a directed landmark graph.
---

# Revisiting Landmarks: Learning from Previous Plans to Generalize over Problem Instances

## Quick Facts
- **arXiv ID:** 2508.21564
- **Source URL:** https://arxiv.org/abs/2508.21564
- **Reference count:** 40
- **Primary result:** Learned generalized landmark graphs with loops improve planning efficiency and scalability compared to traditional landmarks.

## Executive Summary
This paper proposes a method to learn generalized landmarks that automatically apply across a domain by using state functions rather than object-specific atoms. These landmarks are discovered from plans of a small set of solved instances and can capture repetitive patterns with loops in a directed landmark graph. A heuristic counting accepted landmarks is used to guide planning in new instances. Experiments show that the learned landmark graphs, especially those with loops, significantly improve planning efficiency and scalability compared to baselines.

## Method Summary
The method learns generalized landmarks from solved training instances using a two-phase approach. First, a feature pool of state functions is generated from the domain using description logic tools. Then, an answer set program discovers a directed graph of generalized landmarks that satisfy all training trajectories. The framework includes loop detection to capture repetitive subplans and uses a path-dependent counting heuristic with aggressive pruning during planning. This allows the learned landmarks to generalize to new problem instances with different numbers of objects than the training set.

## Key Results
- Generalized landmarks generalize over problem instances, requiring only a few training examples
- Learned landmark graphs with loops significantly improve planning efficiency compared to baselines
- The method outperforms traditional landmarks in capturing domain structure and solving larger instances faster
- Provides interpretable high-level plans that scale well across domains

## Why This Works (Mechanism)

### Mechanism 1: Abstraction via State Functions
Generalization over problem instances occurs because generalized landmarks are defined using state functions (first-order features) rather than object-specific grounded atoms. By defining landmarks as conjunctions of state descriptors (e.g., "exists empty truck" vs. "truck t1 is empty"), the landmark becomes independent of the specific set of objects in a problem instance. If the feature pool lacks the complexity to capture necessary domain relations, generalized landmarks will underfit and fail to guide the planner.

### Mechanism 2: Iterative Discovery and Loop Detection
The framework learns compact planning strategies from few examples by identifying repetitive subplans and encoding them as loops within a directed landmark graph. The discovery process iterates through trajectories to find a chain of landmarks and detects loops if the sequence of states implies repeated behavior. This allows the graph to scale to instances with more objects than the training set. If training instances do not demonstrate repetition, the loop detection mechanism fails to engage, significantly reducing the heuristic's benefit.

### Mechanism 3: Path-Dependent Heuristic & Pruning
Planning efficiency improves by using a path-dependent counting heuristic that forces adherence to the learned landmark graph, coupled with aggressive queue pruning. The heuristic tracks the next required landmark based on the current graph state, and if a loop is traversed, the algorithm prunes the search queue, forcing the planner to commit to the loop's subplan. If the landmark graph is incorrect or loop conditions are too restrictive, the pruning mechanism will cause the planner to fail even if a solution exists.

## Foundational Learning

- **Concept:** Classical Planning (PDDL)
  - **Why needed here:** The entire framework operates on PDDL domains (predicates, actions, objects) and state trajectories.
  - **Quick check question:** Can you explain why `at(Truck, Cell)` is a predicate while `at(t1, c1)` is a grounded atom?

- **Concept:** Landmarks in AI Planning
  - **Why needed here:** The paper "revisits" landmarks. You need to know that a landmark is a fact that must be true at some point in every valid plan.
  - **Quick check question:** Why is a "landmark counting heuristic" considered a weak method for optimal planning but strong for satisficing planning?

- **Concept:** Description Logic (DL) & Feature Generation
  - **Why needed here:** The implementation relies on the DLplan library to generate a "feature pool" of state functions.
  - **Quick check question:** How would a description logic feature represent "the number of delivered packages"?

## Architecture Onboarding

- **Component map:** PDDL Domain + Training Instances -> DLplan (Feature Generator) -> ASP Solver (Discovery Engine) -> Generalized Landmark Graph -> A* Planner with $LM_G$ Heuristic
- **Critical path:** The Discovery Engine is the bottleneck. Generating the feature pool and solving the ASP constraints for complex domains can timeout.
- **Design tradeoffs:**
  - *Feature Complexity vs. Runtime:* Higher complexity features capture more detail but drastically increase discovery time and risk timeouts.
  - *Pruning vs. Completeness:* The pruning strategy speeds up planning significantly but makes the planner incomplete.
- **Failure signatures:**
  - *Timeout in Discovery:* The feature pool is too large; preprocessing didn't filter enough redundant features.
  - *Zero instances solved:* The discovered graph likely has incorrect loop conditions or landmark ordering.
- **First 3 experiments:**
  1. **Verify Discovery Pipeline:** Run discovery on the Delivery domain with β1 configuration. Verify that a loop is found and the graph matches the expected "Pickup → Move → Drop" pattern.
  2. **Ablation of Pruning:** Run the planner on a medium-sized instance with pruning enabled vs. disabled. Confirm the speedup vs. the risk of failure.
  3. **Stress Test Generalization:** Train the discovery on the smallest instance (e.g., 2 packages) and test on the largest available instance (e.g., 9 packages). Check if the Loop Landmark Counter scales correctly.

## Open Questions the Paper Calls Out

- **Question:** How can the generalized landmark graph structure be extended to support disjunctive paths or simultaneous subgoals?
  - **Basis in paper:** Section 10 suggests future work could "allow disjunctive paths in the generalized landmark graph" or introduce "k-possible generalized landmarks."
  - **Why unresolved:** The current discovery algorithm enforces a strict linear chain of landmarks, causing failure in domains with intertwined or interchangeable subgoals.
  - **What evidence would resolve it:** A formal extension of the discovery logic and experiments on multi-modal domains like Logistics or Grid showing successful loop identification without requiring strict consecutiveness of landmarks.

- **Question:** How can the reliance on manual feature configuration tuning be reduced to ensure robust loop discovery?
  - **Basis in paper:** Section 9.1 states that loops "can only be identified with a specific configuration" and "parameter tuning could be optimized and specialized."
  - **Why unresolved:** The feature generation process is parameter-sensitive; specific complexity limits are needed to find relevant features without exhausting time limits.
  - **What evidence would resolve it:** An adaptive feature generation mechanism or a theoretical bound on feature complexity that consistently discovers graphs with loops across the benchmark suite without domain-specific manual adjustments.

## Limitations
- Performance heavily depends on quality and diversity of training instances
- Discovery process can timeout for complex feature pools (β5 configuration), limiting scalability
- Pruning strategy sacrifices completeness and may discard valid solutions if the landmark graph is incorrect

## Confidence

- **High Confidence:** The core mechanism of generalizing landmarks via state functions and basic landmark graph construction are well-validated across experiments.
- **Medium Confidence:** The loop detection and pruning strategies show strong performance in reported results, but their correctness relies heavily on specific ASP encoding and pruning implementation details.
- **Low Confidence:** The claim of "strong generalization" from a single small instance is demonstrated but the robustness across all 16 domains is not fully explored.

## Next Checks

1. **Discovery Engine Stress Test:** Run the landmark discovery pipeline (DLplan + ASP) on the most complex domain (e.g., Hiking with β5 features) to measure timeout rates and verify the implementation handles large feature pools.

2. **Pruning Ablation Study:** For a domain with loops (e.g., Delivery), run the planner with and without the queue pruning strategy on medium-sized instances to quantify the speedup vs. the rate of planner failures.

3. **Feature Sensitivity Analysis:** Systematically vary the DLplan feature complexity (β1 to β5) for a single domain and measure its impact on: (a) discovery time, (b) presence/absence of loops in the graph, and (c) final planning performance on test instances.