---
ver: rpa2
title: Fun-Audio-Chat Technical Report
arxiv_id: '2512.20156'
source_url: https://arxiv.org/abs/2512.20156
tags:
- speech
- audio
- fun-audio-chat
- training
- voice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Fun-Audio-Chat, a large-scale Large Audio Language
  Model (LALM) that addresses key limitations in existing joint speech-text models
  through dual-resolution speech representations and multi-stage post-training. The
  model uses a dual-resolution architecture where the Shared LLM backbone processes
  audio at an efficient 5Hz frame rate while the Speech Refined Head generates high-quality
  speech tokens at 25Hz resolution, reducing GPU hours by nearly 50%.
---

# Fun-Audio-Chat Technical Report

## Quick Facts
- arXiv ID: 2512.20156
- Source URL: https://arxiv.org/abs/2512.20156
- Authors: Tongyi Fun Team; Qian Chen; Luyao Cheng; Chong Deng; Xiangang Li; Jiaqing Liu; Chao-Hong Tan; Wen Wang; Junhao Xu; Jieping Ye; Qinglin Zhang; Qiquan Zhang; Jingren Zhou
- Reference count: 23
- Key outcome: Fun-Audio-Chat achieves competitive performance on Spoken QA (76.61% on OpenAudioBench), Audio Understanding (top performance on MMAU, MMAU-Pro, MMSU), Speech Function Calling (79.63% overall), and Speech Instruction-Following and Voice Empathy tasks, with perfect turn-taking success rate in full-duplex interactions.

## Executive Summary
Fun-Audio-Chat is a large-scale Large Audio Language Model (LALM) that addresses key limitations in existing joint speech-text models through dual-resolution speech representations and multi-stage post-training. The model uses a dual-resolution architecture where the Shared LLM backbone processes audio at an efficient 5Hz frame rate while the Speech Refined Head generates high-quality speech tokens at 25Hz resolution, reducing GPU hours by nearly 50%. The Core-Cocktail Training strategy, a two-stage fine-tuning approach with intermediate model merging, effectively mitigates catastrophic forgetting of text LLM knowledge. Multi-Task DPO Training further enhances robustness, audio understanding, instruction-following, and voice empathy capabilities. Fun-Audio-Chat achieves competitive performance on Spoken Question Answering, Audio Understanding, Speech Function Calling, and Speech Instruction-Following and Voice Empathy tasks.

## Method Summary
Fun-Audio-Chat employs a dual-resolution speech representation architecture (DRSR) with a Whisper-Large-v3 encoder, Adapter, Speech Tokenizer, grouping operation (k=5), Shared LLM (Qwen3-30B-A3B or Qwen3-VL-8B), Text Head, Speech Refined Head (SRH), and Flow Matching + HiFi-GAN detokenizer. The model undergoes three-stage post-training: Pre-alignment (freeze LLM, train encoder/adapter/SRH), Core-Cocktail Training (Stage-1 high LR fine-tuning → merge with α=0.5 → Stage-2 low LR fine-tuning), and Multi-Task DPO across robustness, instruction-following, audio understanding, and voice empathy. Training uses millions of hours of diverse speech data including open-source and in-house datasets, with high-quality speech synthesized via CosyVoice 3.

## Key Results
- Spoken Question Answering: 76.61% on OpenAudioBench, 83.21% on VoiceBench among ~8B-scale models
- Audio Understanding: Top performance on MMAU, MMAU-Pro, and MMSU benchmarks
- Speech Function Calling: 79.63% overall score on ACEBench, BFCL, and SmartInteract
- Speech Instruction-Following and Voice Empathy: Competitive performance on VStyle benchmark
- Full-duplex interactions: Perfect turn-taking success rate

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Processing speech at 5Hz in the Shared LLM while generating at 25Hz via a specialized head reduces computational cost by ~50% without degrading output quality.
- **Mechanism:** A grouping operation compresses 25Hz speech tokens into 5Hz representations via linear projection over concatenated groups of k=5 tokens. The Shared LLM operates on this compressed sequence. A Speech Refined Head then ungroups the hidden state back to 25Hz and autoregressively generates fine-grained speech tokens.
- **Core assumption:** Semantic reasoning can operate on compressed temporal resolution while high-fidelity acoustic synthesis only needs the upsampled conditioning from SRH.
- **Evidence anchors:** [abstract] "the Shared LLM backbone processes audio at an efficient 5Hz frame rate while the Speech Refined Head generates high-quality speech tokens at 25Hz resolution, reducing GPU hours by nearly 50%"; [section 2.2] "This mechanism reduces sequence length from T to T/k, allowing the Shared LLM to operate at a 5Hz frame rate, which substantially reduces computational overhead"; [corpus] DrVoice (FMR=0.60) introduces the same DRSR architecture; Fun-Audio-Chat scales it to larger data and models.
- **Break condition:** If the task requires fine-grained temporal reasoning (e.g., precise prosody alignment or sub-word timing control), the 5Hz compression may lose discriminative detail.

### Mechanism 2
- **Claim:** Two-stage fine-tuning with an intermediate model merge between high-LR and low-LR phases mitigates catastrophic forgetting of the original text LLM's knowledge.
- **Mechanism:** Stage 1 uses LR 1e-4→1e-5 to rapidly adapt all parameters. Then, the Stage-1 model is merged with the original pretrained LLM via linear interpolation with α=0.5. Stage 2 fine-tunes the merged model with LR 1e-5→1e-6.
- **Core assumption:** High-LR training moves parameters to multimodal-favorable regions but overshoots; merging restores a portion of original text capability that low-LR refinement can stabilize.
- **Evidence anchors:** [abstract] "Core-Cocktail Training strategy, a two-stage fine-tuning approach with intermediate model merging, effectively mitigates catastrophic forgetting of text LLM knowledge"; [section "Core-Cocktail Training"] "This merging operation reintroduces the foundational knowledge from the base LLM, safeguarding the original text understanding capabilities"; [corpus] DrVoice proposes the Core-Cocktail strategy; Fun-Audio-Chat validates scalability to 8B and 30B-A3B.
- **Break condition:** If the original LLM's knowledge is highly task-specific or if the multimodal data distribution is extremely divergent, a single merge at α=0.5 may not suffice—multiple merges or task-weighted interpolation may be required.

### Mechanism 3
- **Claim:** Multi-Task DPO applied after Core-Cocktail improves robustness to real speech, instruction-following, audio understanding, and voice empathy.
- **Mechanism:** After supervised fine-tuning, DPO is applied across four preference dimensions: robustness (noisy/diverse inputs), instruction-following (emotion/style/prosody), audio understanding, and voice empathy. The loss aggregates preference signals across tasks.
- **Core assumption:** Preference optimization can jointly align multiple capabilities without heavy conflict, and the post-SFT checkpoint has sufficient multimodal grounding for DPO to refine.
- **Evidence anchors:** [abstract] "Multi-Task DPO Training further enhances robustness, audio understanding, instruction-following, and voice empathy capabilities"; [section "Multi-Task DPO Training"] "The DPO training loss is computed across these multiple preference dimensions, allowing the model to learn a unified preference signal that balances all these capabilities"; [corpus] Kimi-Audio (FMR=0.69) and Qwen2.5-Omni (FMR=0.60) report strong results with extensive post-training but do not explicitly describe multi-task DPO; corpus does not provide direct comparative evidence for this mechanism.
- **Break condition:** If preference data for different tasks are contradictory or poorly balanced, DPO may exhibit gradient conflicts, leading to unstable optimization or capability regressions.

## Foundational Learning

### Concept: Temporal resolution mismatch between speech and text tokens
- **Why needed here:** Speech tokens at 25Hz are ~8× denser than text (~3Hz), causing longer sequences, higher compute, and potential semantic dilution. DRSR directly addresses this.
- **Quick check question:** Given 10 seconds of speech, approximately how many speech tokens would a 25Hz tokenizer produce versus how many text tokens at ~3Hz?

### Concept: Catastrophic forgetting in multimodal fine-tuning
- **Why needed here:** Fine-tuning a text LLM on speech-text data can overwrite textual knowledge. Core-Cocktail Training explicitly mitigates this via merging.
- **Quick check question:** Why might simply fine-tuning a pretrained text LLM on audio data degrade its ability to answer general text-based questions?

### Concept: Direct Preference Optimization (DPO) for alignment
- **Why needed here:** Multi-Task DPO aligns the model to human preferences across multiple dimensions after supervised training.
- **Quick check question:** How does DPO differ from supervised fine-tuning in terms of data requirements and optimization objective?

## Architecture Onboarding

### Component map:
Whisper-Large-v3 encoder -> Adapter -> Speech Tokenizer -> Grouping (k=5) -> Shared LLM (5Hz) -> Text Head + Speech Refined Head -> 25Hz speech tokens -> Flow Matching + HiFi-GAN detokenizer

### Critical path:
1. Pre-alignment (freeze Shared LLM; train Encoder, Adapter, SRH on speech-text pairs)
2. Core-Cocktail Training: Stage 1 (high LR full fine-tuning) → merge (α=0.5) → Stage 2 (low LR fine-tuning)
3. Multi-Task DPO across robustness, instruction-following, audio understanding, voice empathy
4. (Optional) Full-duplex training with parallel input streams

### Design tradeoffs:
- **5Hz vs. 25Hz backbone:** ~50% GPU reduction; may lose fine temporal detail for tasks requiring precise timing
- **Post-training only (no large-scale audio-text pretraining):** Faster iteration but may limit generalization compared to models with massive multimodal pretraining
- **α=0.5 merge:** Balanced knowledge retention vs. multimodal adaptation; task-specific tuning may be needed
- **Max context 2048 tokens (~6 min speech):** Sufficient for typical conversations; insufficient for long-form audio reasoning

### Failure signatures:
- **Memory loss in multi-turn conversations:** Earlier-turn context is inconsistently retained (explicitly noted in Limitations)
- **Unstable expressiveness in instruction-following:** Emotional nuances or prosody control may not match instructions
- **Inconsistent voice empathy:** Paralinguistic-cue-based empathy varies across emotional contexts
- **Degraded text-only performance:** If Core-Cocktail merge is insufficient, original text LLM capabilities may regress

### First 3 experiments:
1. **Ablate grouping factor k:** Compare k=5 vs. k=4 vs. k=6 on Spoken QA accuracy and GPU hours to validate efficiency/quality tradeoff
2. **Vary merge coefficient α:** Test α ∈ {0.3, 0.5, 0.7} on text LLM benchmarks (e.g., MMLU subset) vs. Spoken QA to quantify forgetting/retention balance
3. **Per-task DPO ablation:** Train DPO on each preference dimension independently vs. multi-task, then evaluate on VStyle and internal empathy test sets to check for gradient conflicts

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What architectural or training modifications could reliably mitigate context memory loss in multi-turn spoken conversations while preserving the dual-resolution efficiency gains?
- **Basis in paper:** [explicit] Authors state the model "occasionally exhibits memory loss of context, where information from earlier turns may not be consistently retained" in multi-turn conversations, particularly noticeable for "long-context comprehension and complex reasoning across multiple turns."
- **Why unresolved:** The 2048-token context limit (~6 minutes of speech) constrains multi-turn retention, and the dual-resolution grouping may further dilute cross-turn semantic bindings.
- **What evidence would resolve it:** Ablation studies comparing context retention accuracy across varying context lengths, grouping factors, and explicit memory-augmentation mechanisms on multi-turn dialogue benchmarks.

### Open Question 2
- **Question:** What causes instability in speech instruction-following expressiveness, and how can emotional nuance and prosodic variation be controlled more reliably?
- **Basis in paper:** [explicit] Authors note "there are cases where the generated speech may not fully capture the intended emotional nuances, speaking styles, or prosodic variations specified in the instructions."
- **Why unresolved:** The VStyle benchmark shows moderate performance on style control (4.09/3.14 for English/Mandarin), suggesting the Speech Refined Head may not consistently decode fine-grained prosodic instructions from the 5Hz LLM representations.
- **What evidence would resolve it:** Controlled experiments varying prosodic instruction complexity, combined with acoustic analysis of pitch/energy/duration deviations from target specifications.

### Open Question 3
- **Question:** How can voice empathy capabilities achieve consistent performance across diverse emotional contexts and paralinguistic cues?
- **Basis in paper:** [explicit] Authors report that "the model's ability to consistently recognize and respond with appropriate emotional empathy can vary across different scenarios and emotional contexts," noting that this inconsistency may impact real-world applications.
- **Why unresolved:** The gap between Semantics-based Empathy (4.80) and Paralinguistic-Cue-based Empathy (3.55-3.85) scores suggests the 5Hz token grouping may lose critical emotional acoustic information needed for reliable empathy.
- **What evidence would resolve it:** Fine-grained analysis of empathy performance across specific emotion categories with and without access to high-resolution acoustic features.

### Open Question 4
- **Question:** To what extent does reliance on TTS-synthesized training data limit generalization to real, noisy, and diverse speech conditions?
- **Basis in paper:** [inferred] Core-Cocktail Training uses "high quality speech data synthesized from billions of text tokens using CosyVoice 3," while Multi-Task DPO is explicitly added to enhance "robustness to real speech data," suggesting a gap between synthetic training and real-world performance.
- **Why unresolved:** The paper does not report ablations isolating synthetic vs. real training data, and synthesized speech may not cover the full diversity of accents, noise conditions, and speaking styles encountered in deployment.
- **What evidence would resolve it:** Ablation studies comparing models trained on varying proportions of synthetic vs. real speech, evaluated on out-of-domain noisy speech benchmarks.

## Limitations

- **Memory loss in multi-turn conversations:** Earlier-turn context is inconsistently retained, particularly noticeable for long-context comprehension and complex reasoning across multiple turns
- **Unstable expressiveness in instruction-following:** The model may not fully capture intended emotional nuances, speaking styles, or prosodic variations specified in instructions
- **Inconsistent voice empathy:** The model's ability to consistently recognize and respond with appropriate emotional empathy varies across different scenarios and emotional contexts

## Confidence

**High Confidence:** Spoken Question Answering performance metrics (OpenAudioBench 76.61%, VoiceBench 83.21%) and Audio Understanding benchmark results (MMAU, MMAU-Pro, MMSU) - These are measured against established benchmarks with clear protocols and comparable peer models.

**Medium Confidence:** GPU efficiency claims (50% reduction) and Core-Cocktail training effectiveness - These claims are supported by mechanism descriptions but lack comprehensive ablation studies or baseline comparisons.

**Low Confidence:** Voice empathy and instruction-following improvements from Multi-Task DPO - While benchmark results are reported, the underlying implementation details and preference data construction are not specified, making independent verification difficult.

## Next Checks

1. **DRSR Architecture Ablation:** Implement and evaluate a baseline architecture without grouping (25Hz throughout) and compare against the proposed DRSR design on both GPU hours and Spoken QA accuracy to directly validate the 50% efficiency claim.

2. **Core-Cocktail Merge Sensitivity:** Systematically vary the merge coefficient α (0.3, 0.5, 0.7) and evaluate on text LLM benchmarks (MMLU subset) versus Spoken QA to quantify the exact tradeoff between catastrophic forgetting prevention and multimodal adaptation.

3. **Multi-Task DPO Task Independence:** Train separate DPO models for each preference dimension (robustness, instruction-following, audio understanding, empathy) and evaluate them independently versus the multi-task model on VStyle and internal empathy test sets to identify potential gradient conflicts or capability trade-offs.