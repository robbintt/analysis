---
ver: rpa2
title: 'IceBerg: Debiased Self-Training for Class-Imbalanced Node Classification'
arxiv_id: '2502.06280'
source_url: https://arxiv.org/abs/2502.06280
tags:
- graph
- node
- iceberg
- nodes
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses class-imbalanced node classification in graph
  neural networks by proposing IceBerg, a debiased self-training framework that leverages
  unlabeled nodes to improve performance. The core method, Double Balancing, mitigates
  Matthew effect by dynamically estimating class distributions from pseudo labels
  and applying balanced softmax loss.
---

# IceBerg: Debiased Self-Training for Class-Imbalanced Node Classification

## Quick Facts
- arXiv ID: 2502.06280
- Source URL: https://arxiv.org/abs/2502.06280
- Reference count: 40
- Key outcome: Achieves state-of-the-art performance on class-imbalanced node classification with notable improvements in balanced accuracy (e.g., 75.70 on Cora with IR=10) and macro-F1 scores

## Executive Summary
This paper addresses class-imbalanced node classification in graph neural networks by proposing IceBerg, a debiased self-training framework that leverages unlabeled nodes to improve performance. The core method, Double Balancing, mitigates Matthew effect by dynamically estimating class distributions from pseudo labels and applying balanced softmax loss. Additionally, the approach decouples propagation and transformation in GNNs to enhance long-range supervision signal propagation, addressing few-shot learning challenges. Experiments across multiple benchmark datasets show that IceBerg achieves state-of-the-art performance with notable improvements in balanced accuracy and macro-F1 scores.

## Method Summary
IceBerg addresses class-imbalanced node classification through two key innovations: (1) Double Balancing, which dynamically estimates class distributions from pseudo labels and applies balanced softmax loss to mitigate Matthew effect, and (2) PPTT architecture that decouples feature propagation from transformation in GNNs to enable long-range supervision signal propagation. The method combines these approaches to create a self-training framework that effectively utilizes unlabeled data, particularly in few-shot scenarios where only 1-5 labels per class are available.

## Key Results
- Achieves 75.70 balanced accuracy on Cora dataset with imbalance ratio (IR) of 10
- Demonstrates state-of-the-art performance across 9 benchmark datasets
- Shows significant improvements in macro-F1 scores compared to baseline methods
- Exhibits efficiency gains due to precomputed feature propagation in PPTT architecture

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Class Distribution Re-balancing
Standard self-training on imbalanced data causes models to predict majority classes with higher confidence, flooding the pseudo-label set with majority samples. IceBerg counts pseudo-labels per class (π_c) and adjusts logits using Balanced Softmax (q_j[c] = f_θ(v_j)[c] + μ log π_c) before computing loss. This mitigates Matthew effect by ensuring minority classes are not further marginalized during training.

### Mechanism 2: Decoupled Long-Range Propagation
Standard GNNs entangle propagation and transformation, causing over-smoothing as depth increases. IceBerg pre-computes multiple hops of propagation using diffusion process, allowing a simple MLP to classify nodes using information from a much wider receptive field without training instability. This enables effective few-shot learning by allowing supervision signals to reach distant nodes.

### Mechanism 3: Noise-Tolerant Optimization
Self-training risks reinforcing incorrect predictions. IceBerg adds a symmetric cross-entropy term ℓ(ŷ_j, max(q_j)) to the loss function, which acts as regularization to prevent the model from becoming overconfident in potentially wrong pseudo-labels. This addresses confirmation bias in self-training loops.

## Foundational Learning

- **Balanced Softmax**: Why needed - standard cross-entropy assumes uniform prior; Balanced Softmax adjusts logits based on class priors to handle distribution shift between training (imbalanced) and testing (balanced). Quick check - How does log-likelihood adjustment change when prior probability of a class approaches zero?
- **Propagation vs. Transformation in GNNs**: Why needed - understanding the difference between aggregating neighbor features (Propagation) and applying learnable weights (Transformation) is essential to grasp why decoupling them improves long-range signal transfer. Quick check - In a standard GCN layer, are aggregations and transformations coupled or decoupled?
- **Confirmation Bias in Self-Training**: Why needed - this is the failure mode IceBerg aims to solve; students must understand that self-training can amplify early errors if loss function isn't debiased. Quick check - If a model is 90% confident on a majority class sample and 60% confident on a minority sample, which sample is more likely to be selected as a pseudo-label in a standard threshold-based approach?

## Architecture Onboarding

- **Component map**: Preprocessing (Diffusion-based Propagation -> Pre-computed Node Features) -> Backbone (Shallow MLP) -> Head (Double Balancing Module)
- **Critical path**: 1) Compute X^(T) via diffusion (offline), 2) Train MLP on labeled set to generate initial pseudo-labels, 3) Estimate distribution π from pseudo-labels using dynamic threshold τ', 4) Compute adjusted logits q and apply Noise-Tolerant loss
- **Design tradeoffs**: Threshold Strategy (Dynamic threshold vs. Fixed high threshold), Propagation Hops (Higher hops improve few-shot but may blur local structure)
- **Failure signatures**: Performance Collapse (Macro-F1 drops while accuracy rises indicates overfitting to majority), OOM (Out of Memory on large graphs storing pre-propagated features)
- **First 3 experiments**: 1) Sanity Check - run with increasing propagation hops (2, 4, ..., 14) on Cora, 2) Ablation - compare Base vs. +DB vs. +IceBerg, 3) Threshold Sensitivity - plot Utilization Rate and Pseudo Accuracy

## Open Questions the Paper Calls Out

The paper identifies three key open questions: (1) How performance degrades under severe "Label Missing Not At Random" scenarios where class-conditional distribution of unlabeled set differs from labeled set, (2) Whether reliance on graph diffusion limits effectiveness on highly heterophilic graphs compared to adaptive message-passing methods, and (3) At what noise ratio the "Noise-Tolerant Double Balancing" mechanism fails to correct Matthew Effect.

## Limitations
- Lack of explicit hyperparameter values for critical components like diffusion restart probability α, propagation steps T, and balancing scales μ and β
- Method assumes unlabeled set has sufficient minority-class samples for pseudo-label distribution estimation to be meaningful
- May face challenges in extreme imbalance scenarios where minority classes are severely underrepresented

## Confidence
- **High confidence** in core architectural claims (PPTT improves long-range propagation, Table 1 validates this)
- **Medium confidence** in balancing mechanism's effectiveness (mechanism is sound, but exact gains depend on hyperparameter tuning)
- **Medium confidence** in noise-tolerant optimization (symmetric loss is theoretically justified but practical impact requires careful implementation)

## Next Checks
1. **Threshold stability test**: Plot dynamic threshold τ' and pseudo-label accuracy over training epochs to verify mechanism stabilizes rather than oscillates
2. **Extreme imbalance test**: Evaluate on IR=50 or IR=100 to identify when pseudo-label distribution estimation breaks down
3. **Heterophily stress test**: Apply on strongly heterophilic graphs (like Chameleon) with high propagation hops to identify noise propagation limit