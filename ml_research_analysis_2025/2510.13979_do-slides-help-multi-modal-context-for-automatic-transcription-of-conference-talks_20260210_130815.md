---
ver: rpa2
title: Do Slides Help? Multi-modal Context for Automatic Transcription of Conference
  Talks
arxiv_id: '2510.13979'
source_url: https://arxiv.org/abs/2510.13979
tags:
- words
- context
- speech
- text
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of accurately transcribing domain-specific
  terminology in scientific conference talks using automatic speech recognition (ASR).
  The authors propose integrating visual context from presentation slides into existing
  ASR models to improve performance on specialized vocabulary.
---

# Do Slides Help? Multi-modal Context for Automatic Transcription of Conference Talks

## Quick Facts
- arXiv ID: 2510.13979
- Source URL: https://arxiv.org/abs/2510.13979
- Authors: Supriti Sinhamahapatra; Jan Niehues
- Reference count: 5
- Key outcome: Integrating slide-derived context into ASR models improves transcription of domain-specific terminology, achieving up to 34% relative WER reduction overall and 35% for domain terms.

## Executive Summary
This work addresses the challenge of accurately transcribing domain-specific terminology in scientific conference talks using automatic speech recognition (ASR). The authors propose integrating visual context from presentation slides into existing ASR models to improve performance on specialized vocabulary. They develop a method to extract text from slides, filter it to retain domain-specific terms, and incorporate these as prompts or visual inputs into multimodal ASR systems. To address data limitations, they augment existing speech datasets with automatically generated slide images using large language models. Experiments show that integrating slide-derived context significantly improves ASR performance, demonstrating that leveraging slide context substantially boosts ASR accuracy for technical vocabulary in academic settings.

## Method Summary
The method involves extracting text from presentation slides using vision-language models or traditional OCR, filtering to retain domain-specific terms absent from a general corpus, and incorporating these as prompts or direct image inputs into multimodal ASR systems. To address data scarcity, synthetic slides are generated from speech transcripts using large language models and used to fine-tune ASR models with slide context. The approach is evaluated on the ACL 60/60 dataset, comparing overall and domain-specific word error rates across zero-shot prompting and fine-tuned multimodal ASR models.

## Key Results
- Up to 34% relative reduction in overall word error rate when incorporating slide-derived context
- 35% relative reduction in word error rate for domain-specific terminology
- Fine-tuning with augmented data further enhances results, especially when using images directly alongside speech

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contextual vocabulary extracted from presentation slides and injected as textual prompts improves transcription of domain-specific terms that general ASR models misrecognize.
- Mechanism: Slides are extracted from video frames, text is pulled via OCR/VLM, filtered to retain terms absent from a general corpus, and these domain-specific words are added to the ASR model's text prompt. This conditions the model's decoding to favor these terms when acoustically plausible.
- Core assumption: Domain-specific terms not in general training data are a primary error source; explicitly surfacing them as candidates reduces substitution and deletion errors.
- Evidence anchors:
  - [abstract] "relative reduction in word error rate of approximately 34%, across all words and 35%, for domain-specific terms compared to the baseline model."
  - [section 3.4] "We find that for all models, the word error rate (WERtref and WERthyp) on domain-specific words is significantly higher compared to WER on all words."
  - [corpus] Related multi-modal ASR/AVSR datasets (e.g., Chinese-LiPS) support slide-context integration, but direct replication of this prompting approach across diverse domains is limited.
- Break condition: If domain-specific terms are not the dominant error type, or if extracted slide text has low overlap with spoken terms, this mechanism's gains will diminish.

### Mechanism 2
- Claim: Directly providing slide images as visual context to a multi-modal ASR model can outperform cascaded text extraction, especially when slides contain figures, tables, or layout cues beyond raw text.
- Mechanism: An end-to-end multi-modal model (e.g., Phi-4-multimodal) receives both audio and the slide image. Visual features are encoded and projected into the model's joint embedding space, allowing it to leverage non-textual semantics and avoid OCR error cascades.
- Core assumption: The vision encoder can extract task-relevant semantic information from complex slide images and align it with the audio stream.
- Evidence anchors:
  - [abstract] "Fine-tuning with augmented data further enhances results, especially when using images directly alongside speech."
  - [section 7.3] "We find this to be our best possible overall setup for Phi, even outperforming the setup containing context words from reference."
  - [corpus] Evidence for end-to-end image integration on scientific terminology is emergent; related datasets incorporate slides but do not fully validate this specific architecture choice.
- Break condition: If the vision encoder fails to capture fine-grained slide text or misaligns visual and audio modalities, performance may not exceed text-based prompting.

### Mechanism 3
- Claim: Synthetic slide generation via LLMs enables effective multi-modal fine-tuning when real paired data is scarce.
- Mechanism: An LLM (e.g., LLaMA 3) generates LaTeX slide content from speech transcripts, which is compiled into slide images. These synthetic slides are paired with the original audio to create a multi-modal training set, teaching the model to attend to and integrate slide context during ASR.
- Core assumption: Synthetic slides provide sufficient signal for the model to learn multi-modal integration, and the distribution shift between synthetic and real slides is manageable.
- Evidence anchors:
  - [abstract] "We mitigate the lack of datasets with accompanying slides by a suitable approach of data augmentation."
  - [section 5.1] "we generate presentation slides for existing speech content through a series of steps... employing LLaMA 3 to generate LaTeX code..."
  - [corpus] No direct corpus evidence for this LLM-based augmentation; generalization to real-world conference slides needs further validation.
- Break condition: If synthetic slides are systematically different from real ones in content, style, or term distribution, fine-tuning may not transfer effectively.

## Foundational Learning

- **Concept**: Multi-modal Foundation Models (e.g., SALMONN, Phi-4-multimodal)
  - Why needed here: These models provide the architecture to fuse audio and visual inputs, enabling slide context integration as prompts or images.
  - Quick check question: How does a multi-modal model project visual and audio features into a shared embedding space for joint reasoning?

- **Concept**: Domain-Specific WER Metrics (WERtref, WERthyp)
  - Why needed here: Standard WER may mask problems with specialized vocabulary; these metrics isolate errors on rare or technical terms.
  - Quick check question: What does a high WERtref but low overall WER indicate about an ASR system's performance?

- **Concept**: Vision-Language Models for OCR (e.g., LLaVa-NeXT) vs. Traditional OCR
  - Why needed here: VLMs extract text from slides for context; understanding their capabilities and hallucination risks is critical for reliable prompting.
  - Quick check question: What are the trade-offs between using a VLM versus a traditional OCR library for extracting text from presentation slides?

## Architecture Onboarding

- **Component map**: Video/Audio → Frame Extraction → Text Extraction (Llava/Pytesseract) → Filtering (domain-specific) → Context Integration (Prompting) → Multi-modal ASR Model → Transcription. Alternative path: Video/Audio → Frame Extraction → Direct Image Input → Multi-modal ASR Model → Transcription.
- **Critical path**:
  1. Align extracted frames with audio segments.
  2. Extract and filter domain-specific text or prepare images.
  3. Inject context via prompts or as image input.
  4. Generate transcription and evaluate with WER and domain-specific metrics.
- **Design tradeoffs**: Cascaded (text prompts) is simpler and interpretable but vulnerable to OCR errors. End-to-end (image input) leverages richer visual context but requires robust vision encoding. Synthetic slides enable scaling but may introduce domain shift.
- **Failure signatures**: High WERtref with low overall WER; VLM hallucinations in extracted text; timing mismatches between slides and speech.
- **First 3 experiments**:
  1. Run baseline ASR on a domain-specific test set; compute overall WER and WERtref/WERthyp to quantify the domain-term problem.
  2. Extract text from real slides via Llava, filter to domain terms, inject as prompts into a multi-modal ASR model; compare transcription quality to baseline.
  3. Generate synthetic slides from a speech corpus using an LLM, pair with audio, and fine-tune a multi-modal ASR model to use slide images directly; evaluate on held-out real presentations.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does direct image integration outperform cascaded text-prompting approaches in multimodal ASR architectures other than Phi-4-multimodal?
  - Basis in paper: [explicit] The Limitations section states that the image integration experiment was "limited to the Phi-4-multimodal model" and notes that "further comprehensive studies are required to draw conclusive insights."
  - Why unresolved: The paper demonstrates success with Phi-4 but does not verify if the "Fine-tuned with image" approach yields similar improvements for other architectures like SALMONN or audio-visual LLMs.
  - Evidence: Replicating the fine-tuning protocol with direct image inputs on alternative multimodal models (e.g., SALMONN, Video-LLaVA) and comparing WER against text-prompt baselines.

- **Open Question 2**: Can state-of-the-art unimodal ASR models be effectively adapted to incorporate visual context in an end-to-end manner?
  - Basis in paper: [explicit] The Conclusion states the authors "propose to investigate on SOTA ASR uni-model performances on such end-to-end approaches" as future work.
  - Why unresolved: The current study relies on models already designed to handle multiple modalities (Phi, SALMONN); the feasibility of retrofitting acoustic-only models with visual encoders remains untested.
  - Evidence: Experiments integrating visual encoders (e.g., CLIP/ViT) into audio-only ASR architectures (e.g., Whisper) and evaluating performance on the ACL dataset benchmark.

- **Open Question 3**: How does the mismatch between spoken elaboration and static slide content affect the reliability of context-aware ASR?
  - Basis in paper: [inferred] The Limitations section notes that "speakers often elaborate the slides with their own words introducing mismatch," and that VLMs used for extraction are susceptible to "hallucination."
  - Why unresolved: It is unclear if the model prioritizes noisy visual context (hallucinated or mismatched text) over the audio, potentially increasing insertion errors when the speaker deviates from the slide.
  - Evidence: An error analysis comparing performance on speech segments highly aligned with slide text versus segments where the speaker diverges significantly from the visual prompt.

## Limitations

- The evaluation set (ACL 60/60) contains only five talks per split, limiting statistical power and generalizability.
- Key hyperparameters for fine-tuning are not reported, making exact replication difficult.
- The effectiveness of synthetic slide augmentation is asserted but not validated with ablation studies or comparison to real slide data.

## Confidence

- **High confidence**: The baseline finding that domain-specific terms are a primary source of ASR errors (WERtref >> WER) is strongly supported by results.
- **Medium confidence**: The relative WER improvements from slide context integration are plausible given the mechanism, but limited evaluation size prevents strong generalization claims.
- **Low confidence**: The effectiveness of synthetic slide augmentation for fine-tuning is asserted but not validated with ablation studies or comparison to real slide data.

## Next Checks

1. Replicate the ACL 60/60 baseline and context-augmented ASR evaluation on at least two additional scientific domains (e.g., medical and engineering conferences) to test generalizability.
2. Conduct an ablation study comparing synthetic slides vs. real slides (where available) for multi-modal fine-tuning to quantify domain shift effects.
3. Implement hallucination detection for VLM-extracted text and measure its impact on WER improvements when filtering out high-confidence extractions.