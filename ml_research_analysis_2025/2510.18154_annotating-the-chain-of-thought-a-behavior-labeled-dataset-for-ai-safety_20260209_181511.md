---
ver: rpa2
title: 'Annotating the Chain-of-Thought: A Behavior-Labeled Dataset for AI Safety'
arxiv_id: '2510.18154'
source_url: https://arxiv.org/abs/2510.18154
tags:
- reasoning
- safety
- behaviors
- steering
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a sentence-level labeled dataset enabling activation-based
  safety monitoring of chain-of-thought reasoning. The dataset contains over 50,000
  annotated sentences from 20 distinct safety behaviors across reasoning traces from
  multiple models responding to harmful prompts.
---

# Annotating the Chain-of-Thought: A Behavior-Labeled Dataset for AI Safety

## Quick Facts
- **arXiv ID:** 2510.18154
- **Source URL:** https://arxiv.org/abs/2510.18154
- **Reference count:** 3
- **One-line primary result:** Sentence-level behavior-labeled dataset enables activation-based safety monitoring with middle layers (13-15) achieving highest separation scores (up to 0.28)

## Executive Summary
This paper introduces a sentence-level labeled dataset enabling activation-based safety monitoring of chain-of-thought reasoning. The dataset contains over 50,000 annotated sentences from 20 distinct safety behaviors across reasoning traces from multiple models responding to harmful prompts. Using LLM-as-a-judge annotation, the dataset identifies precisely when specific behaviors occur during reasoning, addressing the gap where existing datasets only label reasoning holistically. The authors demonstrate the dataset's utility by extracting steering vectors that can detect and influence safety behaviors in model activations.

## Method Summary
The dataset is created by annotating reasoning traces from four models (DeepSeek-R1-8B, DeepSeek-R1-8B-0528, DeepSeek-R1-32B, Qwen3-8B) responding to ~500 harmful prompts. Each sentence is labeled by Gemini 2.0-flash (temperature=0) with up to 3 labels from a 20-behavior taxonomy across 6 categories. Steering vectors are computed as the mean activation difference between behavior-present and behavior-absent sentences using NNSight for activation extraction. The vectors are applied at layers 13-15 with strength α typically 1.0-2.0 for both detection (cosine similarity) and steering (activation addition).

## Key Results
- Middle layers (13-15) achieve highest separation scores (up to 0.28) for distinguishing behaviors
- Cross-model training yields better detection than single-model training
- Steering experiments successfully guide models toward safety-oriented behaviors
- Behavior detection reveals distinct activation patterns between harmful and safe prompts

## Why This Works (Mechanism)

### Mechanism 1: Sentence-Level Behavioral Annotation via LLM-as-Judge
Fine-grained behavioral labels enable precise steering vector extraction by pinpointing when target behaviors emerge during reasoning. Gemini 2.0-flash annotates each sentence in reasoning traces with up to 3 labels from a 20-behavior taxonomy. Core assumption: textual manifestations correspond sufficiently with internal representations to enable effective steering vector extraction. Evidence: Cohen's d effect sizes validate taxonomy (e.g., "Detail harmful method" d=+1.28, "Suggest Safe Alternative" d=-1.45).

### Mechanism 2: Mean-Difference Steering Vector Computation
The difference between mean activations of behavior-present and behavior-absent sentences creates effective steering vectors for detection and control. Compute v_behavior = ā_with − ā_without across all sentences using mean pooling across token positions. Core assumption: behavioral representations are approximately linearly separable in activation space. Evidence: cross-model training yields better detection than single-model training.

### Mechanism 3: Middle-Layer Activation Intervention
Layers 13-15 most effectively encode safety behaviors for detection and steering in the tested architectures. Extract activations from middle layers and apply steering via a′_l,t = a_l,t + α·v_behavior. Core assumption: safety behaviors are most distinctly represented in middle transformer layers, consistent across similar architectures. Evidence: Figure 2 shows consistent middle-layer peak across behaviors; Figure 5 confirms pattern across DeepSeek-R1-8B and Qwen3-8B.

## Foundational Learning

- **Concept: Representation Engineering (RepE)**
  - Why needed here: Core paradigm for extracting and applying steering vectors from activation space rather than training-based approaches.
  - Quick check question: Why might activation-space monitoring detect behaviors that textual monitoring misses?

- **Concept: Chain-of-Thought (CoT) Monitoring**
  - Why needed here: Reasoning traces provide the temporal structure enabling sentence-level behavioral analysis.
  - Quick check question: What fundamental limitation of textual CoT monitoring motivates activation-based approaches?

- **Concept: Separation Score**
  - Why needed here: Quantifies how well steering vectors distinguish behavior-present from behavior-absent activations (mean cosine similarity difference).
  - Quick check question: What does a separation score of 0.28 versus 0.10 imply for detection reliability?

## Architecture Onboarding

- **Component map:** Harmful prompts (HarmBench/StrongReject) → Model reasoning traces → LLM-as-judge sentence labeling → NNSight activation extraction → Layer-wise mean pooling → Steering vector computation → Detection (cosine similarity) / Steering (activation addition)

- **Critical path:**
  1. Accurate sentence-level labels (taxonomy validity via effect sizes)
  2. Correct layer selection (layers 13-15 for 8B models)
  3. Appropriate steering strength (α typically 1.0-2.0, balancing effectiveness vs. coherence)

- **Design tradeoffs:**
  - Cross-model training improves robustness but may dilute architecture-specific signals
  - Higher α increases behavioral influence but risks incoherent generation
  - Mean pooling is simple but may miss token-level behavioral nuance

- **Failure signatures:**
  - Separation scores < 0.1 indicate poor behavioral representation
  - "Flag prompt as harmful" activating on benign prompts suggests initial screening process
  - Text incoherence after steering indicates α too high
  - Low effect sizes (|d| < 0.3) suggest label-behavior mismatch

- **First 3 experiments:**
  1. Replicate Cohen's d analysis to validate behavior-harmfulness relationships on your target model
  2. Sweep layers 0-30 to identify optimal detection layers for your architecture (may differ from 13-15)
  3. Test steering vector transfer: train on combined multi-model data, evaluate on held-out single-model data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can steering vectors trained on textually-manifest behaviors detect the same behaviors when they occur without textual expression?
- Basis in paper: Future Work section states this as "a key direction" that "address[es] the core motivation for activation-based monitoring."
- Why unresolved: The dataset only labels behaviors visible in text; models may employ reasoning patterns internally without manifesting them textually.
- What evidence would resolve it: Experiments comparing steering vector detection rates on reasoning traces where behaviors occur internally but not textually.

### Open Question 2
- Question: Do larger model architectures (beyond 8B parameters) better represent safety behaviors in their activations?
- Basis in paper: Future Work notes that "testing with larger models (beyond 8B parameters) may reveal whether bigger architectures better represent safety behaviors."
- Why unresolved: All experiments use 8B-32B parameter models; scaling properties of behavior representation remain untested.
- What evidence would resolve it: Layer performance and separation score comparisons across model scales (e.g., 70B, 405B) on identical behaviors.

### Open Question 3
- Question: Can steering vectors trained on textual examples transfer effectively across languages?
- Basis in paper: Future Work mentions creating "more robust steering vectors that work across language boundaries" through multilingual expansion.
- Why unresolved: Current dataset contains only English reasoning traces; cross-linguistic consistency of safety behavior representations is unknown.
- What evidence would resolve it: Evaluation of English-trained steering vectors on non-English reasoning traces.

### Open Question 4
- Question: Does the assumption that textual behavioral manifestations correspond to consistent internal representations hold?
- Basis in paper: Introduction acknowledges this as a "key assumption" without validation.
- Why unresolved: The paper hypothesizes consistency but acknowledges models may exhibit behaviors internally without textual manifestation due to competing processes.
- What evidence would resolve it: Probing experiments comparing activation patterns when behaviors manifest textually versus when they occur covertly.

## Limitations

- The dataset relies on textual LLM-as-judge annotations which may not capture all internal safety behaviors
- Cross-model training may introduce noise from architecture-specific representation differences
- Fixed layer selection (13-15) for 8B models may not generalize to larger architectures
- Claims about utility for preventing reasoning obfuscation lack direct empirical support

## Confidence

**High Confidence:** Dataset creation methodology and basic separation score measurements are well-documented and reproducible. Middle layers (13-15) show highest separation scores for tested 8B models with consistent experimental results.

**Medium Confidence:** Effectiveness of mean-difference steering vectors for detection and control is demonstrated, but robustness to model obfuscation and cross-architecture transfer remains uncertain. LLM-as-judge annotation appears reasonable given effect size validation.

**Low Confidence:** Claims about dataset's utility for preventing reasoning obfuscation lack direct empirical support. Paper demonstrates steering vector effectiveness but doesn't conclusively show these vectors remain effective when models actively attempt to hide behaviors.

## Next Checks

1. **Obfuscation Robustness Test:** Apply the dataset's steering vectors to models fine-tuned to hide specific safety behaviors. Measure whether separation scores degrade and by how much, establishing the dataset's vulnerability to adversarial behavior hiding.

2. **Architecture Transfer Validation:** Test steering vector transfer from 8B models used in dataset creation to larger models (e.g., 70B parameters) and different architectures (e.g., Mamba). Identify optimal detection layers for each architecture and measure performance degradation.

3. **Multi-Sentence Behavioral Persistence Study:** Conduct systematic experiments varying α values (0.5 to 3.0) across multiple consecutive sentences to determine minimum sequence length and steering strength required for reliable behavioral influence. Measure both effectiveness and text coherence degradation rates.