---
ver: rpa2
title: 'Under the Shadow of Babel: How Language Shapes Reasoning in LLMs'
arxiv_id: '2506.16151'
source_url: https://arxiv.org/abs/2506.16151
tags:
- causal
- attention
- english
- chinese
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether language shapes reasoning patterns
  in large language models (LLMs) through a bilingual causal reasoning study. The
  authors construct BICAUSE, a semantically and syntactically aligned dataset of Chinese-English
  causal chains, to examine attention patterns and representation alignment across
  languages.
---

# Under the Shadow of Babel: How Language Shapes Reasoning in LLMs

## Quick Facts
- arXiv ID: 2506.16151
- Source URL: https://arxiv.org/abs/2506.16151
- Authors: Chenxi Wang; Yixuan Zhang; Lang Gao; Zixiang Xu; Zirui Song; Yanbo Wang; Xiuying Chen
- Reference count: 14
- Key outcome: This paper investigates whether language shapes reasoning patterns in large language models (LLMs) through a bilingual causal reasoning study. The authors construct BICAUSE, a semantically and syntactically aligned dataset of Chinese-English causal chains, to examine attention patterns and representation alignment across languages. They find that LLMs exhibit language-specific attention biases: Chinese inputs draw more focus on subjects and connectives, while English inputs elicit greater attention to verbs. Moreover, Chinese-specific causal ordering preferences are rigidly internalized, leading to degraded performance on reversed causal chains. However, when reasoning succeeds, model representations converge toward shared semantic abstractions across languages, regardless of surface form. These findings provide empirical evidence that linguistic structures influence not only LLM outputs but also internal attention allocation strategies, echoing the linguistic relativity hypothesis.

## Executive Summary
This paper explores how linguistic structures influence LLM reasoning through a bilingual causal reasoning study. Using the BICAUSE dataset of semantically and syntactically aligned Chinese-English causal chains, the authors analyze attention patterns and representation alignment across languages. They demonstrate that LLMs develop language-specific attention biases and internalize rigid causal ordering preferences, leading to performance degradation when these patterns are violated. However, successful reasoning produces convergent semantic abstractions across languages, suggesting a shared understanding beyond surface form. These findings provide empirical evidence for linguistic relativity in LLMs, showing that language shapes not just outputs but also internal processing strategies.

## Method Summary
The authors construct BICAUSE, a bilingual dataset containing 400 semantically and syntactically aligned causal chains in Chinese and English. Each chain consists of three causal steps [cause]→[intermediate effect]→[final effect], with both forward and reversed variants. The study analyzes pretrained models (primarily Qwen1.5-1.8B-Chat) by extracting per-layer attention matrices and hidden states during inference. The Relative Component Attention Ratio (RCAR) quantifies attention allocation across 13 syntactic components and 3 causal components. SVCCA measures attention trajectory similarity, while cosine similarity tracks representational convergence at the final causal token. The analysis focuses on comparing attention patterns and representations across languages and causal structures.

## Key Results
- LLMs exhibit language-specific attention biases: Chinese inputs draw more focus on subjects and connectives, while English inputs elicit greater attention to verbs
- Models rigidly internalize language-specific causal ordering preferences, leading to degraded performance on reversed Chinese chains
- When causal reasoning succeeds, model representations converge toward shared semantic abstractions across languages regardless of surface form

## Why This Works (Mechanism)

### Mechanism 1: Language-Specific Attention Allocation
- Claim: LLMs develop distinct attention patterns based on input language typology.
- Mechanism: Attention heads weight syntactic components differently based on learned statistical regularities from language-specific training data. Chinese inputs trigger higher attention to subjects and sentence-initial connectives; English inputs elicit greater attention to verbs and result-oriented connectives.
- Core assumption: Attention patterns reflect internalized processing strategies rather than random variance or tokenization artifacts.
- Evidence anchors:
  - [abstract]: "LLMs exhibit typologically aligned attention patterns, focusing more on causes and sentence-initial connectives in Chinese, while showing a more balanced distribution in English."
  - [section 4.3]: "For Chinese inputs, the model tends to focus more on subjects and conditional connectives such as 'once' and 'if'; in contrast, English inputs elicit higher attention to verbs and logical progression connectives like 'then' and 'therefore.'"
  - [corpus]: No direct corpus support for this causal mechanism; related multilingual work (Babel) addresses coverage, not attention patterns.
- Break condition: If attention differences disappear when controlling for tokenization and sequence length across languages, the mechanism may be artifact-driven.

### Mechanism 2: Rigid Causal Ordering Preferences
- Claim: LLMs internalize language-specific causal word order preferences and fail when these patterns are violated.
- Mechanism: Models learn probabilistic associations between syntactic position and semantic role (e.g., sentence-initial = cause in Chinese). When inputs violate these patterns, attention misallocates to the wrong causal component, degrading reasoning.
- Core assumption: Performance gaps stem from structural overfitting rather than lexical or semantic processing failures.
- Evidence anchors:
  - [abstract]: "Models internalize language-specific preferences for causal word order and often rigidly apply them to atypical inputs, leading to degraded performance, especially in Chinese."
  - [section 5.2]: "In paraphrased Chinese inputs, the model disproportionately focuses on the sentence-initial [final effect] instead of the true causal origin ([cause]), indicating an overreliance on the prior that 'sentence-initial = cause.'"
  - [corpus]: Weak support; ExpliCa (arXiv:2502.15487) evaluates causal reasoning across linguistic orders but does not analyze internal mechanisms.
- Break condition: If accuracy recovers when reversed structures include explicit disambiguating connectives, the mechanism may be cue-based rather than structural.

### Mechanism 3: Convergent Semantic Abstraction
- Claim: Successful causal reasoning produces language-agnostic internal representations regardless of surface form.
- Mechanism: When reasoning succeeds, hidden states in mid-to-late layers converge to a shared semantic space, indicating a universal abstraction layer atop language-specific attention patterns.
- Core assumption: Cosine similarity of hidden states at specific token positions reflects functional equivalence of causal representations.
- Evidence anchors:
  - [abstract]: "When causal reasoning succeeds, model representations converge toward semantically aligned abstractions across languages, indicating a shared understanding beyond surface form."
  - [section 6]: "All input forms converge to relatively high cosine similarity in the final layers, implying that once the model succeeds... it forms functionally equivalent causal representations."
  - [corpus]: Indirect support from Semantic Hub Hypothesis (Wu et al., cited in paper) on centralized multilingual meaning.
- Break condition: If convergence only occurs for correctly predicted samples, the mechanism may reflect output alignment rather than reasoning process convergence.

## Foundational Learning

- Concept: Linguistic Relativity (Sapir-Whorf Hypothesis)
  - Why needed here: The paper frames its investigation around whether language shapes LLM reasoning analogously to human cognition.
  - Quick check question: What is the difference between the strong and weak versions of linguistic relativity, and which does this paper's evidence support?

- Concept: Attention Mechanisms in Transformers
  - Why needed here: The primary analytical tool is measuring attention allocation using the Relative Component Attention Ratio (RCAR).
  - Quick check question: Given an attention matrix A(l,h), how would you compute the attention received by a token set Tc from all valid query positions in an autoregressive model?

- Concept: Typological Linguistics (Topic-Prominent vs. Verb-Centric Languages)
  - Why needed here: Attention differences are attributed to typological features: Chinese as topic-prominent, English as verb-centric.
  - Quick check question: What discourse features distinguish topic-prominent from verb-centric languages, and how might these affect causal chain processing?

## Architecture Onboarding

- Component map: The framework decomposes causal chains into 13 syntactic components (subjects, verbs, connectives) and 3 causal components (cause, intermediate effect, final effect). RCAR quantifies per-layer attention. SVCCA measures attention trajectory similarity. Hidden state cosine similarity tracks representational convergence.

- Critical path: (1) Construct semantically aligned bilingual dataset with forward/reversed chains → (2) Compute RCAR per component across layers → (3) Compare attention patterns across languages/structures → (4) Measure SVCCA similarity between attention trajectories → (5) Compute hidden state cosine similarity at final causal token for correct predictions.

- Design tradeoffs: Using Qwen1.5-1.8B-Chat trades scale for interpretability (24 layers, 16 heads) but limits generalization. Chinese-English focus ensures alignment quality but restricts typological diversity. Analyzing only correct predictions for representational similarity avoids conflating failures but introduces selection bias.

- Failure signatures: (1) High attention to sentence-initial position regardless of causal role in Chinese reversed chains → rigid positional prior. (2) SVCCA similarity < 0.5 between forward/reversed Chinese chains → structural processing divergence. (3) 14+ percentage point accuracy drop on Chinese reversed chains → failed generalization.

- First 3 experiments:
  1. Replicate RCAR analysis on LLaMA-3.2-1B using BICAUSE to test cross-family generalization.
  2. Add explicit causal direction markers (e.g., "The cause is...") to reversed Chinese chains and measure attention reallocation and accuracy recovery.
  3. Extract middle-layer hidden states (layers 12-18) for correct vs. incorrect predictions and compute within-language cosine similarity to test whether convergence is specific to successful reasoning.

## Open Questions the Paper Calls Out
The paper explicitly identifies several open questions in its Limitations section. The authors note that future work is needed to extend the research to a broader set of languages, "especially those with greater structural divergence or lower resource availability." This suggests the current findings may not generalize to all language typologies. The study's focus on causal reasoning leaves open whether linguistic relativity effects extend to other cognitive domains like temporal or spatial reasoning. Additionally, the paper does not investigate methods to mitigate the rigid causal ordering preferences that cause performance degradation, leaving open the question of whether these limitations are fundamental or can be corrected through fine-tuning or architectural interventions.

## Limitations
- Analysis relies on a single 1.8B-parameter model with limited cross-model validation, leaving open whether effects scale to frontier models
- Chinese-English typological focus provides clean comparative conditions but limits broader claims about topic-prominent versus verb-centric languages
- Controlled dataset may not generalize to natural, unstructured text with messier syntax and pragmatic variations

## Confidence
- Language-specific attention allocation patterns: High confidence (quantitative RCAR analysis and SVCCA trajectory comparisons provide strong empirical support)
- Rigid causal ordering preferences driving performance degradation: Medium confidence (supported by correlational evidence from attention analysis but lacks interventional proof)
- Convergent semantic abstraction mechanism: Medium confidence (empirically grounded in hidden-state similarity measurements but selection bias toward successful samples weakens causal interpretation)

## Next Checks
1. Test whether the observed attention pattern differences persist when controlling for tokenization and sequence length across languages through matched synthetic chains.
2. Evaluate whether explicit syntactic markers (e.g., "because" in reversed Chinese chains) can reallocate attention and recover performance, distinguishing structural from cue-based processing.
3. Analyze whether attention patterns and representational convergence generalize to typologically distinct language pairs (e.g., English-Japanese) using the BICAUSE framework.