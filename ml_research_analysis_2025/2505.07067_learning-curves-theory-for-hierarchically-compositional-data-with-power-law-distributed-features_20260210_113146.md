---
ver: rpa2
title: Learning curves theory for hierarchically compositional data with power-law
  distributed features
arxiv_id: '2505.07067'
source_url: https://arxiv.org/abs/2505.07067
tags:
- learning
- rules
- data
- production
- curves
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes learning curves for deep networks trained on
  hierarchically compositional data with power-law distributed features. The authors
  combine the Random Hierarchy Model (RHM) with Zipf-distributed production rules
  to create synthetic datasets that capture both hierarchical structure and Zipfian
  feature distributions found in natural data.
---

# Learning curves theory for hierarchically compositional data with power-law distributed features

## Quick Facts
- arXiv ID: 2505.07067
- Source URL: https://arxiv.org/abs/2505.07067
- Reference count: 40
- Primary result: Shows that Zipf-distributed production rules transform learning curves from sigmoidal to power-law decay P^(-a/(1+a)) for classification, while next-token prediction scaling remains controlled by hierarchical structure alone

## Executive Summary
This paper develops a theoretical framework for understanding learning curves when training deep networks on hierarchically compositional data with power-law distributed features. The authors combine the Random Hierarchy Model with Zipf-distributed production rules to create synthetic datasets that capture both the hierarchical structure and Zipfian feature distributions found in natural data. They show that for classification tasks, Zipf-distributed production rules fundamentally change the learning curve from sigmoidal to power-law decay, with the asymptotic exponent determined by the Zipf parameter. For next-token prediction, the asymptotic scaling exponent remains controlled by the hierarchical structure regardless of the production rule distribution, though local curve details and asymptotic loss are affected by Zipf distributions.

## Method Summary
The authors analyze learning curves for deep networks trained on synthetic data generated from a modified Random Hierarchy Model (RHM) where production rules follow Zipf distributions. They consider two tasks: classification (predicting root labels from hierarchical sequences) and next-token prediction. For classification, they use deep CNNs with hierarchical filter/stride sizes matched to the data structure, trained with SGD and cosine annealing. For next-token prediction, they use depth-4 transformers without residuals, layernorm, or MLP layers, trained with Adam. Both models are trained across a wide range of training set sizes (10² to 10⁷) to characterize learning curves. The theoretical predictions are derived from a random features model and validated through extensive experiments.

## Key Results
- For classification with Zipf-distributed production rules, learning curves follow power-law decay P^(-a/(1+a)) rather than sigmoidal behavior
- The hierarchical structure controls the pre-asymptotic phase size while the Zipf distribution determines the asymptotic decay rate
- For next-token prediction, the asymptotic scaling exponent remains P^(-log(m/v^(s-1))/(2log m)) regardless of production rule distribution
- Deep CNNs and transformers show excellent agreement with theoretical predictions across multiple parameter settings
- The asymptotic test loss for next-token prediction is directly determined by the Zipf distribution of production rules

## Why This Works (Mechanism)
The mechanism relies on the interplay between hierarchical data generation and feature distribution statistics. In the RHM with Zipf-distributed production rules, each symbol at a given layer is generated with probability proportional to k^(-(1+a)), creating a power-law feature distribution. During training, neural networks first learn frequent (low-index) production rules before rare ones, following a "memorization" strategy. For classification, this sequential learning of production rules translates into power-law learning curves, while for next-token prediction, the hierarchical structure dominates the asymptotic scaling behavior regardless of feature distribution.

## Foundational Learning
- Random Hierarchy Model (RHM): A generative model creating hierarchically compositional data with tree-like structure; needed to capture the compositional nature of natural data; quick check: verify tree depth L and branching factor s match specifications
- Zipf distribution: Power-law distribution f_k ∝ k^(-(1+a)) where a is the Zipf exponent; needed to model the heavy-tailed nature of natural features; quick check: plot production rule frequencies to confirm Zipf scaling
- Learning curves: Test error/loss vs training set size on log-log scale; needed to characterize generalization behavior; quick check: verify power-law scaling by fitting log-log regression
- Random features model: Theoretical framework for analyzing neural network learning with random feature extraction; needed for deriving asymptotic learning curve predictions; quick check: confirm feature dimensionality matches theoretical assumptions

## Architecture Onboarding

### Component Map
Data Generator (RHM with Zipf rules) -> CNN/Transformer -> Training Loop -> Validation Set -> Learning Curve Plot

### Critical Path
1. RHM data generation with specified L, s, v, m, a parameters
2. CNN/Transformer architecture matching hierarchical structure
3. Training across P ∈ [10², 10⁷] with convergence monitoring
4. Test error/cross-entropy measurement and log-log plotting
5. Exponent fitting to verify theoretical predictions

### Design Tradeoffs
- CNN stride/filter size = s: Matches data hierarchy but limits architectural flexibility
- Transformer without residuals/layernorm/MLP: Simplifies analysis but may underperform on complex data
- Minimal batch size for convergence: Ensures clean learning curves but may increase training time
- Validation set size 2²¹/2¹⁷: Large validation sets provide reliable metrics but increase memory requirements

### Failure Signatures
- Sigmoidal curves for classification with Zipf rules: Indicates incorrect Zipf parameterization or normalization
- Wrong asymptotic exponent: Suggests errors in Zipf parameter a or insufficient data for asymptotic regime
- Transformer underperforms: May indicate missing architectural components or incorrect RHM data generation
- Noisy learning curves: Could result from batch sizes too small for stable convergence

### 3 First Experiments
1. Generate RHM data with L=3, s=3, v=10, m=5, a=1 and verify Zipf distribution visually
2. Train CNN on small dataset (P=100) and confirm hierarchical filter/stride configuration
3. Plot learning curve for next-token prediction with uniform production rules to establish baseline

## Open Questions the Paper Calls Out
### Open Question 1
Does the theoretical framework hold when Zipf-distributed production rules appear at multiple hierarchical levels rather than just one layer? The authors believe the main conclusions would still hold, but only analyze single-layer Zipf distributions for analytical tractability.

### Open Question 2
What mechanisms cause the fundamental asymmetry between classification and next-token prediction in how Zipf distributions affect scaling exponents? The paper documents this difference but doesn't explain why the two tasks respond differently to the same distributional structure.

### Open Question 3
Can the Random Hierarchy Model framework be extended to handle varying tree topologies and context-dependent effects present in natural language? The current RHM assumes fixed branching factors and depth, which doesn't capture all aspects of natural data structure.

### Open Question 4
How does the assumption that production rules are learnable when their correlation effects become detectable hold for real neural networks with finite capacity? This assumption underlies the entire theoretical framework but hasn't been validated for actual deep networks.

## Limitations
- Theoretical framework assumes idealized data distribution that may not fully represent real-world natural data
- Analysis focuses on specific architectures (deep CNNs and transformers) that may not generalize to all network types
- Assumes sufficient training data to reach asymptotic regimes, but practical applications often operate in pre-asymptotic regimes
- Zipf distribution assumption is a simplification that may not hold exactly in practice

## Confidence
- Classification power-law decay predictions: High confidence based on mathematical derivation and strong experimental agreement
- Next-token prediction scaling exponent predictions: Medium confidence, supported by experiments but sensitive to architectural details
- Asymptotic test loss predictions: Medium confidence, theoretically sound but dependent on accurate production rule distribution modeling

## Next Checks
1. **Convergence threshold validation**: Test multiple convergence criteria values and observe how learning curve shapes change, particularly for classification with Zipf-distributed rules
2. **Architectural robustness test**: Reproduce key experiments with alternative architectures (e.g., ResNets for classification, transformers with residual connections) to assess sensitivity to architectural choices
3. **Real-world data validation**: Apply theoretical framework to natural language or vision datasets with known hierarchical structure and compare empirical learning curves against predictions