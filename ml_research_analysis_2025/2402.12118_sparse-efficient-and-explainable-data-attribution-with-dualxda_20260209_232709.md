---
ver: rpa2
title: Sparse, Efficient and Explainable Data Attribution with DualXDA
arxiv_id: '2402.12118'
source_url: https://arxiv.org/abs/2402.12118
tags:
- training
- attribution
- data
- test
- dualda
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DualXDA, a framework for sparse, efficient,
  and explainable data attribution in deep learning models. The key problem addressed
  is the computational inefficiency and lack of sparsity in existing data attribution
  methods, which limit their scalability and interpretability.
---

# Sparse, Efficient and Explainable Data Attribution with DualXDA

## Quick Facts
- arXiv ID: 2402.12118
- Source URL: https://arxiv.org/abs/2402.12118
- Authors: Galip Ãœmit Yolcu; Moritz Weckbecker; Thomas Wiegand; Wojciech Samek; Sebastian Lapuschkin
- Reference count: 40
- Primary result: Introduces DualXDA, a framework achieving up to 4.1 million times speedup over Influence Functions with naturally sparse, interpretable data attributions.

## Executive Summary
This paper introduces DualXDA, a framework for sparse, efficient, and explainable data attribution in deep learning models. The key problem addressed is the computational inefficiency and lack of sparsity in existing data attribution methods, which limit their scalability and interpretability. DualDA, a novel component of DualXDA, leverages Support Vector Machine (SVM) theory to provide fast, naturally sparse attributions by replacing the final layer of neural networks with a multiclass SVM. This approach achieves up to 4.1 million times speedup compared to Influence Functions while maintaining competitive attribution quality. XDA, the second component, enhances interpretability by mapping data attributions back to the feature space, identifying relevant input features for both training and test samples simultaneously. The framework is evaluated across three datasets and models, demonstrating superior performance in multiple metrics including mislabeling detection and shortcut detection. The authors also showcase applications on large language models, highlighting the framework's scalability. Together, DualDA and XDA enable deeper, more actionable analyses of neural network behavior, addressing key limitations in explainability and scalability.

## Method Summary
DualXDA is a framework that combines DualDA and XDA to achieve sparse, efficient, and interpretable data attribution. DualDA leverages Support Vector Machine (SVM) theory by replacing the final layer of neural networks with a multiclass SVM, enabling naturally sparse and computationally efficient attributions. This approach achieves up to 4.1 million times speedup compared to Influence Functions. XDA enhances interpretability by mapping data attributions back to the feature space, identifying relevant input features for both training and test samples simultaneously. The framework is evaluated across three datasets and models, demonstrating superior performance in metrics like mislabeling detection and shortcut detection. Applications on large language models further highlight its scalability and practical utility.

## Key Results
- Achieves up to 4.1 million times speedup compared to Influence Functions.
- Provides naturally sparse attributions via SVM-based DualDA.
- Demonstrates superior performance in mislabeling detection and shortcut detection across three datasets.
- Enables interpretable feature-level analysis through XDA.
- Successfully applied to large language models, showcasing scalability.

## Why This Works (Mechanism)
DualXDA works by combining DualDA and XDA. DualDA leverages SVM theory to replace the final layer of neural networks with a multiclass SVM, enabling naturally sparse and computationally efficient attributions. This sparsity is inherent to SVM decision boundaries, which rely on support vectors, reducing the number of relevant training samples. XDA then maps these sparse data attributions back to the feature space, identifying relevant input features for both training and test samples simultaneously. This dual approach addresses the computational inefficiency and lack of sparsity in existing methods, while enhancing interpretability. The framework's scalability is further demonstrated through applications on large language models.

## Foundational Learning
- **Support Vector Machines (SVMs)**: Why needed: SVMs provide naturally sparse decision boundaries by relying on support vectors, which reduces computational complexity and enables efficient data attribution. Quick check: Verify that the SVM-based approach indeed results in sparse attributions compared to other methods.
- **Influence Functions**: Why needed: Influence functions are a baseline for data attribution but are computationally expensive, motivating the need for faster alternatives like DualDA. Quick check: Compare the computational cost of DualDA with influence functions on a standard dataset.
- **Feature Space Mapping**: Why needed: Mapping data attributions to the feature space (via XDA) enhances interpretability by identifying relevant input features, making the attributions more actionable. Quick check: Assess whether XDA's feature-level attributions align with domain knowledge or known patterns in the data.

## Architecture Onboarding
- **Component Map**: Neural Network -> Multiclass SVM (DualDA) -> Feature Space Mapping (XDA)
- **Critical Path**: DualDA computes sparse data attributions via SVM theory, then XDA maps these attributions to the feature space for interpretability.
- **Design Tradeoffs**: DualDA sacrifices some flexibility of standard neural networks for computational efficiency and sparsity, while XDA adds interpretability at the cost of additional computation.
- **Failure Signatures**: If the SVM-based approach fails to capture relevant training influences, attributions may be overly sparse or miss subtle patterns. XDA may struggle with highly complex, non-linear feature interactions.
- **First Experiments**:
  1. Apply DualDA to a simple binary classification task and verify speedup over influence functions.
  2. Use XDA to map attributions to feature space on a tabular dataset and assess interpretability.
  3. Test DualXDA on a small language model to validate scalability claims.

## Open Questions the Paper Calls Out
None

## Limitations
- The sparsity of DualDA may miss subtle but important training influences in certain domains.
- Performance gains on large language models may not generalize beyond tested tasks.
- The extent to which XDA's feature-level attributions preserve information for complex, high-dimensional data remains unclear.

## Confidence
- **High confidence**: DualDA's theoretical foundation in SVM theory, the claimed computational speedups, and the overall framework design.
- **Medium confidence**: Empirical performance on LLMs and generalization across diverse model architectures.
- **Low confidence**: The absolute sparsity levels achieved in all tested scenarios and the preservation of attribution quality in extremely high-dimensional spaces.

## Next Checks
1. Test DualXDA on additional diverse tasks with large language models to assess generalization beyond the reported applications.
2. Quantitatively compare the sparsity levels of DualDA against existing methods across multiple datasets to validate the "naturally sparse" claim.
3. Evaluate whether XDA's feature-level attributions maintain fidelity when applied to highly complex, non-linear feature interactions common in deep learning.