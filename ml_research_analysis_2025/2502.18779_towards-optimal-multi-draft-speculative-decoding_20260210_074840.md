---
ver: rpa2
title: Towards Optimal Multi-draft Speculative Decoding
arxiv_id: '2502.18779'
source_url: https://arxiv.org/abs/2502.18779
tags:
- pdraft
- draft
- replacement
- optimal
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new approach to measuring and optimizing
  the efficiency of multi-draft speculative decoding for large language models. The
  key contribution is transforming the optimal acceptance rate computation into a
  subset selection problem via the dual of an optimal transport formulation.
---

# Towards Optimal Multi-draft Speculative Decoding

## Quick Facts
- arXiv ID: 2502.18779
- Source URL: https://arxiv.org/abs/2502.18779
- Reference count: 27
- This paper introduces a new approach to measuring and optimizing the efficiency of multi-draft speculative decoding for large language models.

## Executive Summary
This paper introduces a new approach to measuring and optimizing the efficiency of multi-draft speculative decoding for large language models. The key contribution is transforming the optimal acceptance rate computation into a subset selection problem via the dual of an optimal transport formulation. The authors develop efficient algorithms to compute the theoretical upper bound of acceptance rates for different draft sampling methods, including sampling with replacement, without replacement, and a novel greedy approach. Experiments on real text distributions show that existing verification algorithms like RRS and K-SEQ have significant gaps from the theoretical upper bound, while the greedy method achieves the optimal rate in certain cases. The work provides practical methods to compute the optimal acceptance rate and suggests directions for designing better verification algorithms.

## Method Summary
The paper transforms the optimal acceptance rate computation for multi-draft speculative decoding into a subset selection problem by deriving the dual of an optimal transport formulation. Using total unimodularity of the constraint matrix, the problem reduces to finding the minimum of f(H) = P(H) - Q(H) over subsets H of the vocabulary. For sampling with and without replacement, the function Q(H) is proven to be q-convex, enabling efficient O(n|Σ|) algorithms. A novel greedy draft sampling method is proposed that achieves the theoretical optimal acceptance rate for specific distributions when paired with a corresponding verification algorithm.

## Key Results
- The optimal acceptance rate for multi-draft speculative decoding can be computed efficiently via a subset selection formulation derived from the dual of an optimal transport problem
- For common draft sampling methods (with and without replacement), efficient O(n|Σ|) algorithms exist due to q-convexity properties
- A novel greedy draft construction combined with tailored verification achieves the optimal acceptance rate for specific distributions
- Existing verification algorithms like RRS and K-SEQ have significant gaps (0.5-3.3 percentage points) from the theoretical upper bound

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The optimal acceptance rate for multi-draft speculative decoding (MDSD) can be computed efficiently via a subset selection formulation derived from the dual of an optimal transport problem.
- Mechanism: The paper transforms the primal optimal transport LP (computing a joint distribution π ∈ Π(p, pdraft)) into its dual, a weighted vertex cover problem. Exploiting total unimodularity of the constraint matrix, the dual is reduced to a subset selection problem over the vocabulary Σ. The objective becomes min_{H⊂Σ} [P(H) - Q(H)], where P(H) and Q(H) are aggregated probabilities from the target and draft distributions. This reduces complexity from exponential in |Σ|^n to O(|Σ| log|Σ|) for replacement sampling and O(n|Σ|) for without-replacement sampling (after sorting).
- Core assumption: The coefficient matrix of the constraints (incidence matrix of a bipartite graph plus identity matrices) being totally unimodular guarantees an integer optimal solution to the dual LP.
- Evidence anchors:
  - [abstract] "This paper discusses the dual of the optimal transport problem, providing a way to efficiently compute the optimal acceptance rate."
  - [section 3] "We show that the optimal acceptance rate can be expressed as a subset selection problem... derived the dual problem... Total Unimodularity."
  - [corpus] Weak/missing for this specific dual/subset-selection technique in speculative decoding context among the provided neighbors.

### Mechanism 2
- Claim: For common draft sampling methods (with and without replacement), the set function Q(H) is q-convex, enabling a greedy linear-scan algorithm to find the optimal subset H*.
- Mechanism: A function Q is q-convex if marginal gains normalized by weights q(x) obey a diminishing-returns-like property. The paper proves Q(H) is q-convex for both sampling types. This property, combined with Theorem 6 (marginal monotonicity under a certain ordering), allows sorting vocabulary by q(i)/p(i) in non-increasing order and scanning to find the minimizer of f(H) = P(H) - Q(H), rather than requiring general submodular minimization.
- Core assumption: The target distribution has p(x) > 0 for all x; if p(x)=0, the optimal set H* is assumed to contain x since Q is monotone increasing. Practically, numerical stability or epsilon-smoothing is assumed for zero probabilities.
- Evidence anchors:
  - [abstract] "For certain special cases, we propose efficient methods to solve the subset selection problem by noticing convexity-like structures..."
  - [section 4] "Theorem 3/4... F or sampling with/without replacement, the function Q is a q-convex function... can be formulated as an unconstrained submodular minimization problem... we can solve... even faster."
  - [corpus] Weak/missing direct evidence for q-convexity in MDSD from neighbors.

### Mechanism 3
- Claim: A novel greedy draft construction (top n-1 tokens deterministic, last token sampled without replacement) combined with a tailored verification algorithm achieves the optimal acceptance rate for that distribution.
- Mechanism: By fixing n-1 drafts to be the top tokens from the draft model q, the draft distribution pdraft becomes highly concentrated and structured. The verification algorithm πGreedy applies the optimal single-draft verification (standard speculative sampling) between the target p and the modified distribution of the last random token. Theorem 7 proves this matching by showing the derived acceptance rate equals the computed theoretical upper bound for this specific pdraft.
- Core assumption: The target and draft models are fixed; the benefit comes from the alignment between the draft sampling strategy and the verification logic. The verification algorithm must be exactly πGreedy as specified; using a standard algorithm like RRS would yield a suboptimal rate for this draft distribution.
- Evidence anchors:
  - [abstract] "We propose a novel draft sampling method that greedily selects high-probability drafts... We also propose a corresponding verification algorithm that perfectly reaches the theoretical acceptance rate upper bound."
  - [section 5] "The corresponding optimal transport problem for this draft distribution is simple... can design a verification algorithm that strictly achieves the optimal acceptance rate... Theorem 7."
  - [corpus] "Speculative Decoding: Performance or Illusion?" (arXiv:2601.11580) questions real-world SD effectiveness, making the pursuit of optimal rates practically relevant.

## Foundational Learning

- **Optimal Transport & Wasserstein Distance**
  - Why needed here: The core problem is formulated as an optimal transport between the draft token distribution and the target model distribution. Understanding LP duality (Kantorovich duality) is essential to follow the derivation of the subset selection formulation.
  - Quick check question: Explain how the dual of an optimal transport problem provides a lower bound on the primal cost and, under strong duality, achieves the same optimal value.

- **Total Unimodularity (TUM) & Integer Programming**
  - Why needed here: The paper relies on the constraint matrix of the dual LP being totally unimodular to guarantee that the optimal solution is integral (binary), which simplifies the problem to a subset selection over a discrete set.
  - Quick check question: What property of a matrix makes it totally unimodular, and how does that ensure integer solutions for an LP with integer right-hand sides?

- **Speculative Decoding Basics**
  - Why needed here: The paper builds directly on the speculative decoding framework, where a fast draft model proposes tokens that are verified by a larger target model. Familiarity with acceptance rates, draft/verification, and distribution matching is required.
  - Quick check question: In standard speculative decoding, why must the verification process adjust the distribution to ensure the final output exactly matches the target model's distribution?

## Architecture Onboarding

- **Component map:**
  - Primal LP Formulation (Eq. 7) -> Dual LP Formulation (Eq. 10) -> Subset Selection Problem (Eq. 8) -> Q(H) Computation -> Greedy Draft Constructor -> Verification Algorithm (πGreedy)

- **Critical path:**
  1. For a given context, obtain logits from target p and draft model q.
  2. Convert to probabilities (apply temperature, softmax).
  3. If computing the theoretical upper bound: run the subset selection algorithm (sort by q/p, scan, compute Q(H) via generating function). This yields α*.
  4. If running actual decoding with the greedy method: construct drafts as per Section 5.1, then verify using πGreedy. The achieved α should match αGreedy from Theorem 7.
  5. Compare against existing methods (RRS, K-SEQ) to quantify the optimality gap.

- **Design tradeoffs:**
  - **Computational Complexity vs. Accuracy:** The subset selection method is efficient (O(n|Σ|)) but relies on the q-convexity assumption for correctness.
  - **Draft Strategy vs. Optimal Rate:** Sampling without replacement gives a higher theoretical upper bound than with replacement. The greedy method can sometimes exceed both, but its advantage diminishes at high temperatures (T≈1).
  - **Theoretical vs. Practical Verification:** The greedy verification achieves the optimal rate for its draft distribution, but is specialized to that distribution. RRS is more general but suboptimal.

- **Failure signatures:**
  - **Numerical Instability:** When p(x) or q(x) are extremely small, the ratio q/p becomes unstable, and the generating function computation may underflow.
  - **Assumption Violation:** If the underlying draft sampling does not match the assumed model (e.g., drafts are not generated without replacement when claimed), the computed theoretical bound will not match reality.
  - **High-Temperature Regime:** At T=1, the greedy method's performance drops, potentially falling below the optimal bound of the without-replacement method (see Figure 1). Using it blindly in this regime is suboptimal.

- **First 3 experiments:**
  1. **Reproduce Optimal Bound Calculation:** Implement the O(n|Σ|) algorithm for sampling without replacement. Verify correctness on a small vocabulary (e.g., |Σ|=100, n=3) by comparing against a standard LP solver on the primal problem. Measure runtime scaling to realistic sizes (|Σ|=50,000).
  2. **Quantify the Gap for RRS/K-SEQ:** On a real dataset (e.g., Alpaca with LLaMA-7B target, LLaMA-68M draft), compute α* for without-replacement sampling. Then run RRS on the same distribution. The difference (Δα) measures sub-optimality. Compare your Δα against the values in Table 1 to validate the implementation.
  3. **Validate Greedy Method:** Implement the greedy draft constructor and its verification algorithm (Eq. 19). Run it on the MT-Bench dataset with the Eagle-0.24B/Vicuna-7B pair. Measure the acceptance rate α and verify it matches the theoretical αGreedy computed via Eq. 20. Compare the speedup (tokens per step) against RRS without replacement and the default EAGLE sparse tree configuration.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the subset selection formulation and efficient algorithms for optimal acceptance rate be extended to the multi-step speculative decoding setting, where drafts form a tree structure across multiple generation steps?
  - Basis in paper: [explicit] Section B.3 states: "Combining both improvements in the multi-draft and multi-step scenario would be ideal, and could be a direction for future research."
  - Why unresolved: The paper's theoretical framework only addresses single-step MDSD. Multi-step introduces exponentially larger joint distributions across sequences, and the q-convexity properties may not extend.
  - What evidence would resolve it: A formulation of the multi-step optimal acceptance rate as a tractable optimization problem, or proof that it remains intractable.

- **Open Question 2:** Can new verification algorithms be designed to close the gap between existing methods (RRS, K-SEQ) and the theoretical upper bound for sampling with and without replacement?
  - Basis in paper: [explicit] Abstract states the findings "suggest that carefully designed draft sampling methods can potentially improve the optimal acceptance rate and enable the development of verification algorithms that closely match the theoretical upper bound." Table 1 shows consistent gaps of 0.5–3.3 percentage points.
  - Why unresolved: Existing algorithms were designed without knowledge of the theoretical bound. The subset selection formulation may enable new algorithmic approaches.
  - What evidence would resolve it: A verification algorithm with provable approximation guarantees or empirical acceptance rates within 0.1% of optimal.

- **Open Question 3:** Is there a unified draft sampling method that achieves near-optimal acceptance rates across all temperature settings, including high temperatures where the greedy method underperforms?
  - Basis in paper: [inferred] Figure 1 and Table 2 show the greedy method achieves optimal rates at low temperatures but performance diminishes at T=1.0, while sampling without replacement is more robust but suboptimal.
  - Why unresolved: The greedy method's deterministic selection of top tokens is well-suited to peaked distributions but not uniform ones. The optimal strategy may depend on both temperature and the alignment between draft and target distributions.
  - What evidence would resolve it: A hybrid or adaptive sampling strategy with theoretical guarantees across temperature ranges, validated on diverse datasets.

## Limitations

- The dual/subset selection formulation relies critically on the assumption that the coefficient matrix of the primal LP constraints is totally unimodular, which is asserted but not empirically validated for the specific bipartite incidence matrix used.
- The q-convexity property enabling efficient greedy algorithms is proven but its robustness to practical numerical issues (small probabilities, smoothing) is not explored.
- The greedy draft method's performance degradation at high temperatures (T=1) suggests fundamental limitations in the alignment between draft strategy and verification logic.

## Confidence

- **High confidence:** The transformation of the optimal transport problem to its dual and the subsequent reduction to subset selection via total unimodularity is mathematically sound given the stated assumptions about the constraint matrix structure.
- **Medium confidence:** The q-convexity proofs for sampling with and without replacement appear rigorous, but the practical implementation of the dynamic programming for Q(H) computation may encounter numerical stability issues with large vocabularies or extreme probability distributions.
- **Low confidence:** The performance claims for the greedy draft method, particularly its superiority over without-replacement sampling, are based on synthetic distribution experiments. Real-world model distributions may not exhibit the same structure, making the practical benefit uncertain.

## Next Checks

1. **Validate Total Unimodularity Assumption:** Implement the constraint matrix for the dual LP with n=3 drafts and a small vocabulary (e.g., |Σ|=20). Use a combinatorial matrix library to verify that every square submatrix has determinant in {-1, 0, 1}. This confirms the theoretical basis for the subset selection reduction.

2. **Stress Test Q(H) Numerical Stability:** Generate extreme probability distributions where p(x) or q(x) are extremely small (e.g., 1e-10) for many tokens. Implement the generating function DP and measure: (a) accuracy of W_{n,H} coefficients against high-precision arithmetic, (b) runtime growth with |Σ|, and (c) sensitivity to epsilon-smoothing values. This validates the practical feasibility of the without-replacement algorithm.

3. **Empirical Performance of Greedy Draft:** On the Alpaca dataset with LLaMA-68M/LLaMA-7B, run the greedy draft constructor at T=0.7 and T=1.0. For each temperature: (a) measure the achieved acceptance rate α against the theoretical α*Greedy, (b) compare speedup against RRS without replacement, and (c) analyze the draft distribution pdraft to verify it matches the assumed structure (top n-1 deterministic + 1 random). This quantifies the practical benefit and validates the method's assumptions.