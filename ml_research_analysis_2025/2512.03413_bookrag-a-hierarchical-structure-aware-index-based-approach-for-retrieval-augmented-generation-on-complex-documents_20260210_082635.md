---
ver: rpa2
title: 'BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented
  Generation on Complex Documents'
arxiv_id: '2512.03413'
source_url: https://arxiv.org/abs/2512.03413
tags:
- arxiv
- document
- query
- retrieval
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BookRAG, a novel retrieval-augmented generation
  approach designed for complex documents with hierarchical structures. BookRAG constructs
  a document-native BookIndex that integrates a hierarchical tree structure derived
  from document layout with a knowledge graph capturing fine-grained entity relations,
  linking them via a Graph-Tree Link.
---

# BookRAG: A Hierarchical Structure-aware Index-based Approach for Retrieval-Augmented Generation on Complex Documents

## Quick Facts
- arXiv ID: 2512.03413
- Source URL: https://arxiv.org/abs/2512.03413
- Authors: Shu Wang; Yingli Zhou; Yixiang Fang
- Reference count: 40
- Primary result: BookRAG achieves 43.8% EM on MMLongBench, 61.0% on M3DocVQA, and 55.2% accuracy on Qasper, outperforming state-of-the-art baselines.

## Executive Summary
This paper introduces BookRAG, a novel retrieval-augmented generation approach designed for complex documents with hierarchical structures. BookRAG constructs a document-native BookIndex that integrates a hierarchical tree structure derived from document layout with a knowledge graph capturing fine-grained entity relations, linking them via a Graph-Tree Link. An agent-based retrieval method dynamically classifies queries and generates tailored retrieval workflows inspired by Information Foraging Theory. Extensive experiments on three benchmarks show BookRAG significantly outperforms state-of-the-art baselines while maintaining competitive efficiency.

## Method Summary
BookRAG addresses the challenge of retrieving relevant information from complex, hierarchical documents by constructing a document-native index. The approach involves three main steps: (1) Tree Construction - parsing document layout and filtering titles to build a hierarchical tree structure, (2) Graph Construction - extracting entities and relations, then linking them to tree nodes using a gradient-based entity resolution algorithm, and (3) Agent-based Retrieval - classifying queries and executing tailored retrieval workflows using operators like Selector, Reasoner, and Skyline. The system integrates a knowledge graph with hierarchical structure to improve retrieval accuracy for complex documents.

## Key Results
- Achieves 43.8% Exact Match on MMLongBench
- Achieves 61.0% EM on M3DocVQA
- Achieves 55.2% accuracy on Qasper
- Significantly outperforms state-of-the-art baselines across all three benchmarks
- Maintains competitive efficiency compared to baseline methods

## Why This Works (Mechanism)
BookRAG works by leveraging the inherent hierarchical structure of complex documents while capturing fine-grained entity relationships through a knowledge graph. The hierarchical tree structure preserves document organization, while the knowledge graph captures semantic relationships between entities. The Graph-Tree Link connects these two structures, enabling efficient navigation between high-level document organization and detailed entity relationships. The agent-based retrieval system uses query classification to select appropriate retrieval strategies, improving precision by matching the retrieval approach to query complexity.

## Foundational Learning
- **Gradient-based Entity Resolution**: A novel algorithm that merges entities based on sharp drops in similarity scores (threshold g=0.6). Why needed: To prevent knowledge graph fragmentation and maintain semantic consistency. Quick check: Monitor graph diameter and connected components to ensure reasonable density.
- **Graph-Tree Link**: Connects hierarchical tree nodes with knowledge graph entities. Why needed: Enables navigation between document structure and entity relationships. Quick check: Verify each entity links to at least one tree node.
- **Information Foraging Theory**: Guides the agent's retrieval workflow design. Why needed: Provides theoretical foundation for optimal information-seeking behavior. Quick check: Ensure retrieval paths follow predictable, efficient patterns.
- **Pareto-frontier Ranking**: Combines graph and text scores for retrieval ranking. Why needed: Balances multiple retrieval criteria without arbitrary weighting. Quick check: Verify retrieved results appear on the Pareto front across multiple dimensions.

## Architecture Onboarding

**Component Map:** PDF Parsing -> Tree Construction -> Graph Construction -> Graph-Tree Link -> Agent-based Retrieval -> Answer Generation

**Critical Path:** The most critical path is from Graph-Tree Link through Agent-based Retrieval to Answer Generation. The quality of the graph-tree link directly impacts retrieval accuracy, while the agent's query classification and operator selection determine retrieval efficiency and relevance.

**Design Tradeoffs:** BookRAG trades some indexing time and complexity for improved retrieval accuracy. The hierarchical structure and knowledge graph increase index size but provide more precise navigation. The agent-based system adds computational overhead but enables dynamic, context-aware retrieval strategies.

**Failure Signatures:**
- Poor Retrieval Recall: Check MinerU parsing quality; missing layout blocks make ground truth unreachable
- Graph Fragmentation: Inspect KG statistics; low density indicates entity resolution threshold may be too strict
- VLM Hallucination: If EM is low but recall is high, VLM may misinterpret visual content; verify content extraction accuracy

**First Experiments:**
1. Test Tree Construction with various document layouts to verify hierarchical structure preservation
2. Validate Gradient-based Entity Resolution with controlled entity sets to tune threshold g
3. Benchmark Agent-based Retrieval with single-hop queries to verify basic functionality before multi-hop testing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can BookRAG be integrated into a comprehensive document-native database system?
- Basis in paper: [explicit] The authors conclude by stating an intention to "explore an integrated document-native database system that supports data formatting, knowledge extraction, and intelligent querying."
- Why unresolved: The current implementation focuses on the RAG pipeline (indexing and retrieval) rather than database management or data formatting operations.
- What evidence would resolve it: A system architecture proposal and performance benchmarks for an integrated database handling formatting tasks alongside querying.

### Open Question 2
- Question: How can the agent's planning module be refined to prevent over-decomposition of detailed single-hop queries?
- Basis in paper: [inferred] The error analysis identifies a specific "Plan Error" where the planner tends to "over-decompose detailed single-hop queries into unnecessary multi-hop sub-tasks."
- Why unresolved: The current classification logic occasionally misinterprets query detail as complexity, leading to disjointed retrieval paths and incoherent answers.
- What evidence would resolve it: A comparative evaluation showing a reduction in "Plan Error" rates on the MMLongBench dataset using an improved planning strategy.

### Open Question 3
- Question: How can BookRAG maintain retrieval recall when the upstream layout parser fails to extract ground-truth blocks?
- Basis in paper: [inferred] The metrics section notes that if a PDF parsing error makes a ground-truth item unavailable, the retrieval recall is strictly recorded as 0.
- Why unresolved: The system relies entirely on the MinerU parser without robust fallback mechanisms for missing or corrupted layout blocks.
- What evidence would resolve it: An ablation study measuring recall degradation under simulated parsing noise and testing the efficacy of recovery or re-parsing mechanisms.

## Limitations
- The KG Construction prompt schema is not provided in the appendix, potentially affecting reproducibility
- The system's performance depends heavily on the quality of the upstream layout parser (MinerU)
- The Skyline_Ranker implementation is mentioned but not fully defined, which could lead to variation in retrieval performance

## Confidence

**High:** Hierarchical Tree Construction, Graph-Tree Link, Agent-based Query Classification, and overall retrieval workflow

**Medium:** Gradient-based Entity Resolution, Skyline_Ranker, and empirical performance gains

**Low:** N/A (all major components have at least medium confidence)

## Next Checks

1. **Prompt Validation:** Implement and test the missing KG Construction prompt to ensure consistent entity/relation extraction across different document types
2. **Entity Resolution Sensitivity:** Experiment with different values of the gradient threshold g to assess its impact on graph density and retrieval recall
3. **Skyline_Ranker Testing:** Implement and benchmark the Pareto-frontier ranking to verify it achieves the reported efficiency and accuracy gains