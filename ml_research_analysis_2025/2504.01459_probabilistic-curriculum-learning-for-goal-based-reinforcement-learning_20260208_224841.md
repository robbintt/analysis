---
ver: rpa2
title: Probabilistic Curriculum Learning for Goal-Based Reinforcement Learning
arxiv_id: '2504.01459'
source_url: https://arxiv.org/abs/2504.01459
tags:
- learning
- goal
- goals
- curriculum
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a probabilistic curriculum learning method
  for reinforcement learning in continuous control and navigation tasks. The approach
  models task difficulty using a probabilistic model (specifically a deep mixture
  density network) to estimate the likelihood of achieving goals given the current
  policy.
---

# Probabilistic Curriculum Learning for Goal-Based Reinforcement Learning

## Quick Facts
- arXiv ID: 2504.01459
- Source URL: https://arxiv.org/abs/2504.01459
- Authors: Llewyn Salt; Marcus Gallagher
- Reference count: 40
- Key result: Probabilistic curriculum learning achieves 90% coverage in DC motor tasks versus 50% for uniform baselines

## Executive Summary
This paper introduces a probabilistic curriculum learning method that models goal difficulty using a deep mixture density network to estimate success probabilities. By filtering goals through probability density quantiles, the algorithm selects appropriately challenging goals for efficient policy learning. The approach is evaluated on continuous control (DC motor) and navigation (point robot in mazes) tasks, demonstrating significant improvements over uniform curriculum baselines. The method achieves higher goal coverage while maintaining flexibility in goal generation without restrictive initializations.

## Method Summary
The method uses Soft Actor-Critic (SAC) as the base RL algorithm, augmented with a Deep Mixture Density Network (MDN) that models the probability distribution of achieving goals given current state and action. The MDN is trained on experience stored in a replay buffer, learning to predict p(s_{t+N}|s_t, a_t) as a Gaussian mixture. Goals are sampled from a distribution, filtered by probability density quantiles (Q_lower, Q_upper), and selected using strategies like weighted or multiweighted selection. An adaptive quantile controller adjusts the difficulty bounds based on recent success rates, creating a curriculum that balances exploration and exploitation throughout learning.

## Key Results
- In DC motor task: PCL achieves 90% coverage versus 50% for uniform sampling (standard deviation: 7.1%)
- In 21x21 maze with 72 goals: PCL reaches 79 out of 288 goals versus 54 for uniform sampling
- Weighted selection strategy with adaptive quantiles achieves best performance (coverage: 0.351)
- Performance advantage is most pronounced in longer time horizon tasks and diverse goal sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Estimating goal achievement probability via a learned density model enables filtering goals to intermediate difficulty.
- Mechanism: A deep mixture density network (MDN) learns p(g|s_t, a_t) ≈ p(s_{t+N}|s_t, a_t) from agent experience. Goals are sampled, then filtered by quantile bounds (Q_lower, Q_upper) on their probability density values, selecting those neither trivial (high probability) nor infeasible (low probability). This maintains training signal quality throughout learning.
- Core assumption: Probability density values correlate with actual goal achievement likelihood under the current policy.
- Evidence anchors:
  - [abstract] "By predicting goal success probabilities, the algorithm filters goals to an appropriate difficulty level for efficient policy learning."
  - [Section 3.1.2] "we decide to bound our pdf values by some quantile's upper and lower, we can then select from within this range"
- Break condition: If the MDN fails to calibrate (e.g., modal collapse, overfitting to easy goals), density values no longer indicate difficulty, causing curriculum degradation.

### Mechanism 2
- Claim: Adaptive quantile adjustment maintains curriculum pacing by responding to success signals.
- Mechanism: A short-term memory tracks recent goal outcomes. When success rate exceeds target or a success streak occurs, quantiles shift toward harder goals (lower Q_lower, Q_upper). On failures, quantiles expand toward easier goals. This creates a feedback loop balancing exploration and exploitation.
- Core assumption: Success rate is a reliable signal for when the agent is ready for harder goals.
- Evidence anchors:
  - [abstract] "weighted selection strategy combined with adaptive quantiles achieves the best performance"
  - [Section 3.2.2] Equations 13-16 define success rate, streak detection, and quantile updates
- Break condition: If the agent oscillates between easy and hard goals without stable progress (e.g., highly stochastic environments), quantile adaptation becomes unstable.

### Mechanism 3
- Claim: Weighted selection within quantile bounds improves over uniform sampling by biasing toward learnable goals.
- Mechanism: Instead of uniform selection from filtered goals, probability density values are normalized to form a sampling distribution. This preferentially selects goals with higher estimated achievability while maintaining curriculum structure via quantile bounds.
- Core assumption: Goals with higher estimated probability offer better learning signal than uniformly sampled intermediate-difficulty goals.
- Evidence anchors:
  - [abstract] "balancing exploration and exploitation based on goal success probabilities is beneficial"
  - [Section 4.3] "weighted selection strategy with adaptive quantiles achieves a coverage of 0.351... outperforming the uniform selection strategy"
- Break condition: If the MDN overestimates probability for certain regions, weighted selection creates a feedback loop avoiding those areas entirely.

## Foundational Learning

- **Mixture Density Networks**:
  - Why needed here: Standard neural networks output point estimates; MDNs output distribution parameters (mixture weights, means, covariances) enabling probabilistic goal filtering.
  - Quick check question: Can you explain why a Gaussian mixture model can represent multimodal goal distributions better than a single Gaussian?

- **Goal-Conditioned Reinforcement Learning**:
  - Why needed here: The policy π(s, g) must generalize across goals; understanding UVFAs (Universal Value Function Approximators) clarifies why goal-conditioned Q-functions enable multi-task learning.
  - Quick check question: How does conditioning on goals change the Bellman equation compared to standard RL?

- **Quantile-Based Filtering**:
  - Why needed here: Direct probability thresholds are brittle; quantiles adapt automatically to the distribution shape and scale.
  - Quick check question: Why might filtering by absolute probability threshold fail as the MDN's output distribution shifts during training?

## Architecture Onboarding

- **Component map**:
  Agent (SAC) -> MDN -> Goal Selector -> Replay Buffer -> Adaptive Quantile Controller

- **Critical path**:
  1. Warmup phase: Random goals populate replay buffer
  2. MDN trains on buffer data (minimize negative log-likelihood + regularization)
  3. Goal sampling → quantile filtering → selection strategy → episode rollout
  4. Success/failure signal → adaptive quantile update
  5. Repeat from step 2

- **Design tradeoffs**:
  - **Number of mixtures (K)**: More mixtures capture complex distributions but risk overfitting; paper uses 6-12
  - **Quantile width (Q_upper - Q_lower)**: Narrow band = focused curriculum but less diversity; wide band = more exploration but weaker difficulty targeting
  - **Selection strategy**: Uniform = maximum diversity; weighted = exploitation; multiweighted = adds uncertainty/learning progress/novelty terms
  - **MDN update frequency**: Frequent updates track policy changes but may destabilize; paper uses 2-10 steps

- **Failure signatures**:
  - Coverage plateaus early: Quantiles may be stuck too narrow; check adaptive quantile learning rate λ
  - Modal collapse in MDN: KL divergence term (λ_3) may be too low; increase regularization
  - High variance across runs: Curriculum is sensitive to hyperparameters; needs more tuning or ensemble approaches
  - Agent only reaches easy goals: Q_lower set too high; MDN may be miscalibrated

- **First 3 experiments**:
  1. **Calibration check**: Train MDN on fixed policy; compare predicted p(g|s,a) against empirical success rates. Verify density correlates with difficulty.
  2. **Ablation on quantile settings**: Fix selection strategy to uniform; sweep static (Q_lower, Q_upper) pairs. Identify the curriculum sensitivity surface before enabling adaptation.
  3. **Selection strategy comparison**: In a simple environment (e.g., DC motor), compare uniform vs. weighted vs. multiweighted with fixed quantiles. Isolate the contribution of selection bias from adaptive curriculum effects.

## Open Questions the Paper Calls Out
- Can alternative probabilistic models like normalizing flows outperform MDNs for goal success probability estimation?
- Can the framework be extended to handle abstract goals that don't directly correspond to state space subsets?
- Does PCL maintain efficiency advantages when scaled to high-dimensional tasks with longer time horizons?
- How do selection strategies interact with adaptive quantiles in environments with dense obstacles?

## Limitations
- Relies on density values from learned MDN as proxy for goal difficulty, which may not generalize to highly stochastic environments
- Adaptive quantile mechanism introduces additional hyperparameters requiring careful tuning
- Assumes access to suitable goal sampling distribution, which may not be trivial to construct in all domains
- Performance gains show variability across runs, suggesting robustness issues

## Confidence
- Mechanism 1 (MDN-based filtering): Medium confidence
- Mechanism 2 (Adaptive quantiles): Medium confidence  
- Mechanism 3 (Weighted selection): Medium confidence
- Overall approach: Medium confidence

## Next Checks
1. **MDN Calibration Test**: Run controlled experiment where MDN is trained on fixed policy, then systematically vary goal difficulty and measure correlation between predicted probability density and actual success rate.
2. **Curriculum Sensitivity Analysis**: Perform ablation study fixing selection strategy to uniform while sweeping static quantile pairs (Q_lower, Q_upper) to isolate curriculum targeting effects.
3. **Transfer Robustness Test**: Train PCL in one task variant (e.g., 11x11 maze), then evaluate performance when transferred to different variant (e.g., 21x21 maze) without retraining MDN.