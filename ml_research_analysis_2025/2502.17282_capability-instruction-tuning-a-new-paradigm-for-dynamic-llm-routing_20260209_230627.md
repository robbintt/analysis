---
ver: rpa2
title: 'Capability Instruction Tuning: A New Paradigm for Dynamic LLM Routing'
arxiv_id: '2502.17282'
source_url: https://arxiv.org/abs/2502.17282
tags:
- capability
- instruction
- routing
- performance
- instructions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Model-SAT, a novel paradigm called capability
  instruction tuning for dynamic LLM routing. The method constructs capability instructions
  that combine model capability representations, user instructions, and performance
  inquiry prompts to assess which LLM performs best on each instruction.
---

# Capability Instruction Tuning: A New Paradigm for Dynamic LLM Routing

## Quick Facts
- arXiv ID: 2502.17282
- Source URL: https://arxiv.org/abs/2502.17282
- Authors: Yi-Kai Zhang; De-Chuan Zhan; Han-Jia Ye
- Reference count: 8
- Key outcome: Model-SAT achieves state-of-the-art performance in dynamic LLM routing using capability instruction tuning, maintaining 75.28% average accuracy on smaller-scale LLMs (under 10B parameters) while providing efficient routing without inference overhead.

## Executive Summary
This paper introduces Model-SAT, a novel paradigm for dynamic LLM routing that uses capability instruction tuning to select the best-performing model from a zoo for each instruction. The method constructs capability instructions by combining model capability representations, user instructions, and performance inquiry prompts, enabling end-to-end learning of routing decisions without requiring candidate inference or ground truth access. Model-SAT achieves state-of-the-art performance across five comprehensive LLM zoo setups, including smaller-scale, mixed-scale, and high-performance models, as well as multimodal scenarios.

## Method Summary
Model-SAT uses a two-stage training approach with a lightweight LLM router (Phi-3-Mini 3.8B) augmented with a model capability encoder (E5-Large ~0.5B) and MLP connector. First, candidate models are profiled on 50 standardized MMLU tasks to generate capability text representations. These representations are combined with user instructions and inquiry prompts to form capability instructions. The router is trained using in-batch contrastive learning with positive/negative samples based on model performance. During inference, the router scores all candidates' capability instructions and selects the highest-scoring model without requiring candidate inference or ground truth access.

## Key Results
- Model-SAT achieves 75.28% average accuracy on smaller-scale LLMs (under 10B parameters), comparable to ~70B parameter models
- Maintains strong performance across five LLM zoo setups including smaller-scale, mixed-scale, and high-performance models
- Demonstrates excellent generalization to unseen data and new models with only 50 core tasks and 20 shots per model
- Outperforms existing routing methods on seven benchmarks (MMLU, WinoGrande, ARC-C, BoolQ, TruthfulQA, MRPC, MNLI)

## Why This Works (Mechanism)

### Mechanism 1: Capability Representation via Aptitude Test
A standardized 50-task aptitude test creates sufficient model representations for routing decisions. Candidate models are evaluated on 50 core MMLU tasks (20-shot each), with results converted to natural language descriptions. This text-based representation encodes performance dimensions without requiring model weights or architecture details. The core assumption is that performance on these 50 tasks generalizes to unseen instructions and captures distinguishable capability profiles across models.

### Mechanism 2: Capability Instruction as Unified Input Format
The router receives concatenated inputs of capability text, user instruction, and inquiry prompt. This format enables the LLM to learn capability-to-instruction mappings through contrastive training. The lightweight LLM outputs probability scores for "Yes" tokens indicating whether the candidate model can handle the instruction. The core assumption is that the router can learn implicit relationships between capability profiles and instruction semantics through this unified input format.

### Mechanism 3: Capability Encoder Alignment via MLP Connector
A separate encoder (E5-Large) with a learned connector better aligns capability text to instruction semantics than using the base LLM alone. The capability encoder processes representation text, then a single-layer MLP projects embeddings to match the base LLM's input dimension. Two-stage training first freezes the LLM and trains the connector only, then fine-tunes all parameters with higher learning rates on the encoder/connector. The core assumption is that explicit capability-to-semantic alignment improves routing accuracy over treating capability text as generic input.

## Foundational Learning

- **Concept: Contrastive Learning / In-Batch Negatives**
  - Why needed here: Model-SAT uses homogeneous in-batch negative sampling—each batch contains 1 positive instruction and k-1 negatives for a given capability representation. Understanding contrastive loss helps diagnose why routing learns to distinguish model-specific strengths.
  - Quick check question: Given a batch with instructions A (positive for Model X) and B (negative for Model X), which should receive higher probability from the router for Model X's capability representation?

- **Concept: Transferability / Model Selection Metrics**
  - Why needed here: The paper builds on prior work (LEEP, LogME, NCE) that predicts fine-tuning performance. Model-SAT extends this to zero-shot routing without target labels. Knowing this lineage clarifies what problem constraints the paper addresses.
  - Quick check question: Why does Model-SAT avoid requiring candidate inference outputs, and what tradeoff does this introduce?

- **Concept: Instruction Tuning Formats**
  - Why needed here: Capability instructions follow instruction-tuning conventions (input + prompt + expected output format). Understanding how LLMs learn from formatted instructions explains why "Yes/No" prediction works for routing.
  - Quick check question: What would happen if the inquiry prompt asked for a numerical score instead of Yes/No? How would training change?

## Architecture Onboarding

- Component map: Input text → Capability Encoder (E5-Large ~0.5B) → MLP Connector → Base LLM (Phi-3-Mini 3.8B) → Output probability

- Critical path:
  1. Profile each candidate model on 50 MMLU tasks → generate capability text
  2. Construct capability instructions for training (positive/negative samples per model)
  3. Stage 1 training: freeze base LLM, train connector only on in-domain data
  4. Stage 2 training: fine-tune all parameters with higher LR on encoder/connector
  5. Deployment: for new instruction, score all candidates' capability instructions, select highest

- Design tradeoffs:
  - Router size vs. routing accuracy: Using Phi-3-Mini (3.8B) balances capability with inference cost; smaller routers may fail to learn complex mappings
  - Core task count vs. representation fidelity: 50 tasks is a design choice; fewer tasks reduce profiling cost but may miss capability dimensions
  - Text-based vs. learned representations: Text enables quick new-model onboarding but may lose fine-grained signals that learned embeddings capture

- Failure signatures:
  - All candidates receive similar scores: Encoder/connector failed to learn discriminative alignment; check Stage 1 training convergence
  - Router always selects same model: Likely overfitting to that model's representation; verify positive/negative sampling balance
  - New models routed poorly: 50 core tasks may not cover new model's strengths; consider expanding task set or adding domain-specific probes
  - Inference too slow: Router (4.3B) is larger than some candidates; profile router latency vs. candidate inference time

- First 3 experiments:
  1. Verify aptitude test coverage: Run the 50-task evaluation on 3-5 diverse models in your zoo; confirm representations capture known strengths (e.g., code models should show high programming task scores). If representations look uniform, increase task diversity.
  2. Ablate connector vs. no-encoder baseline: Train Model-SAT with and without the capability encoder (using Phi-3 alone per Table 2). Measure accuracy gap on held-out instructions. If gap <2%, encoder may be unnecessary for your scale.
  3. Test new-model generalization: Hold out 1-2 models from training, generate their representations via aptitude test only, and evaluate routing accuracy on these unseen models. If performance drops >10% vs. seen models, core tasks may not generalize—consider expanding task set or adding domain-specific probes.

## Open Questions the Paper Calls Out
None

## Limitations
- Several critical details are underspecified, including exact 50 core task selection criteria, training hyperparameters, and prompt templates
- The evaluation focuses primarily on academic datasets, leaving real-world deployment performance uncertain
- Method's scalability to very large model zoos (>100 candidates) or cross-domain scenarios hasn't been thoroughly tested

## Confidence

**High Confidence**: The core routing mechanism works as described. The empirical results showing Model-SAT outperforming existing routing methods on the tested benchmarks appear robust.

**Medium Confidence**: The claim about strong generalization to unseen data and new models. While the paper provides evidence of good out-of-domain performance, the evaluation covers a limited range of unseen scenarios.

**Low Confidence**: The scalability and deployment claims. The paper doesn't provide detailed latency measurements for real-world deployment scenarios, and the computational overhead of maintaining the 4.3B parameter router for large model zoos is not thoroughly analyzed.

## Next Checks
1. Verify aptitude test coverage: Run the 50-task evaluation on 3-5 diverse models in your zoo; confirm representations capture known strengths (e.g., code models should show high programming task scores). If representations look uniform, increase task diversity.

2. Ablate connector vs. no-encoder baseline: Train Model-SAT with and without the capability encoder (using Phi-3 alone per Table 2). Measure accuracy gap on held-out instructions. If gap <2%, encoder may be unnecessary for your scale.

3. Test new-model generalization: Hold out 1-2 models from training, generate their representations via aptitude test only, and evaluate routing accuracy on these unseen models. If performance drops >10% vs. seen models, core tasks may not generalize—consider expanding task set or adding domain-specific probes.