---
ver: rpa2
title: 'InsertRank: LLMs can reason over BM25 scores to Improve Listwise Reranking'
arxiv_id: '2506.14086'
source_url: https://arxiv.org/abs/2506.14086
tags:
- bm25
- reasoning
- retrieval
- scores
- reranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces InsertRank, a listwise reranking method that
  injects BM25 scores into LLM prompts to improve retrieval for reasoning-centric
  queries. By providing lexical relevance signals alongside documents, InsertRank
  helps LLMs ground their reasoning and avoid issues like overthinking.
---

# InsertRank: LLMs can reason over BM25 scores to Improve Listwise Reranking

## Quick Facts
- arXiv ID: 2506.14086
- Source URL: https://arxiv.org/abs/2506.14086
- Reference count: 40
- InsertRank improves listwise reranking by injecting BM25 scores into LLM prompts, achieving NDCG@10 of 37.5 on BRIGHT and 51.1 on R2MED

## Executive Summary
This paper introduces InsertRank, a method that enhances LLM-based listwise reranking by injecting BM25 scores directly into prompts. The approach addresses challenges in reasoning-centric retrieval where LLMs struggle to ground their reasoning without explicit lexical relevance signals. By providing BM25 scores alongside documents, InsertRank helps LLMs avoid overthinking and make more informed ranking decisions. Evaluated on two benchmarks (BRIGHT and R2MED) using multiple LLM families, InsertRank consistently improves retrieval effectiveness and demonstrates robustness to score normalization and document ordering variations.

## Method Summary
InsertRank implements a zero-shot listwise reranking approach where BM25 scores are injected into LLM prompts to guide document ranking. The process begins with BM25 first-stage retrieval on reasoning-centric queries, followed by prompt construction that includes both document content and their corresponding BM25 scores. Documents are ordered by decreasing BM25 score in the prompt. The method was evaluated on BRIGHT (12 domains, ~1.3K queries, ~1M docs) and R2MED (8 medical tasks, 876 queries) using query reformulation via GPT-4 CoT and HyDE respectively. Multiple LLM families including Gemini 2.0/2.5 Flash, GPT-4o, and Deepseek-R1 were tested with document truncation to 1800 tokens for GPT-4o/Deepseek.

## Key Results
- Achieved average NDCG@10 of 37.5 on BRIGHT benchmark
- Achieved average NDCG@10 of 51.1 on R2MED benchmark
- Outperformed prior methods on both benchmarks
- Demonstrated robustness to score normalization and document ordering

## Why This Works (Mechanism)
InsertRank works by providing LLMs with explicit lexical relevance signals through BM25 scores, helping them ground their reasoning process. The method addresses the common LLM issue of overthinking by giving concrete relevance metrics to consider alongside document content. By injecting these scores into prompts and ordering documents by decreasing relevance, LLMs can make more informed ranking decisions without needing additional training or fine-tuning.

## Foundational Learning
- **BM25 retrieval fundamentals**: Why needed - forms the first-stage retrieval basis; Quick check - verify BM25 implementation produces reasonable top-k results
- **Listwise reranking concepts**: Why needed - understands how LLMs rank document lists; Quick check - test basic listwise ranking without score injection
- **Prompt engineering for LLMs**: Why needed - constructs effective prompts with injected metadata; Quick check - validate prompt template produces expected LLM responses
- **NDCG@10 metric**: Why needed - evaluation metric for ranking quality; Quick check - confirm NDCG@10 calculations match benchmark standards

## Architecture Onboarding
- **Component map**: BM25 retrieval -> Query reformulation -> Prompt construction -> LLM reranking -> NDCG@10 evaluation
- **Critical path**: First-stage BM25 retrieval → query reformulation → prompt injection with BM25 scores → LLM reranking → evaluation
- **Design tradeoffs**: Zero-shot vs. fine-tuning (InsertRank chooses zero-shot for cost-effectiveness), score injection vs. metadata complexity (starts with simple BM25), document ordering strategies (uses decreasing BM25 order)
- **Failure signatures**: Performance degradation with shuffled document order, sensitivity to score normalization scale, output format parsing errors
- **First experiments**: 1) Basic BM25 retrieval on test queries, 2) Simple prompt construction without scores, 3) LLM reranking with shuffled document order

## Open Questions the Paper Calls Out
- What specific types of low-cost relevance signals or metadata beyond BM25 scores can be effectively injected into prompts to further improve listwise reranking?
- Does the effectiveness of InsertRank generalize to standard non-reasoning retrieval benchmarks, or is it specific to complex, reasoning-centric queries?
- Why does the combination of document shuffling and BM25 injection yield divergent results across domains (improving BRIGHT but degrading R2MED)?

## Limitations
- BM25 implementation details (tokenizer, k1/b parameters, exact variant) not specified
- Exact top-k value for reranking not reported
- Query reformulation prompts (GPT-4 CoT and HyDE) not provided
- Context window size and exact prompt variations per model unspecified

## Confidence
- High confidence: General method of injecting BM25 scores into LLM prompts for listwise reranking is sound and reproducible
- Medium confidence: Reported NDCG@10 improvements (37.5 on BRIGHT, 51.1 on R2MED) are likely real but may be influenced by unspecified BM25 implementation details
- Low confidence: Claim that InsertRank is "robust" to document ordering and score normalization is weakly supported by single ablation points without statistical testing

## Next Checks
1. **BM25 Parameter Sensitivity**: Reproduce first-stage retrieval using multiple BM25 parameter configurations (k1=1.2, b=0.75 vs k1=2.0, b=0.75) and verify InsertRank improvements persist across variants

2. **Order Robustness Validation**: Systematically test InsertRank with shuffled document orders (10+ permutations) and report mean ± standard deviation of NDCG@10 to quantify positional bias effects

3. **Score Normalization Impact**: Evaluate raw BM25 scores, min-max normalized (0-1), and scaled (0-100) versions across all models to measure actual performance variation rather than relying on qualitative robustness claims