---
ver: rpa2
title: Predictive Scheduling for Efficient Inference-Time Reasoning in Large Language
  Models
arxiv_id: '2602.01237'
source_url: https://arxiv.org/abs/2602.01237
tags:
- token
- budget
- allocation
- reasoning
- difficulty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces predictive scheduling for efficient inference-time\
  \ reasoning in large language models. The key insight is that allocating a fixed\
  \ token budget per query leads to inefficiency\u2014over-computing on easy inputs\
  \ and under-computing on hard ones."
---

# Predictive Scheduling for Efficient Inference-Time Reasoning in Large Language Models

## Quick Facts
- **arXiv ID**: 2602.01237
- **Source URL**: https://arxiv.org/abs/2602.01237
- **Reference count**: 14
- **Primary result**: 7.9 percentage points higher accuracy than uniform budgeting at same token cost on GSM8K

## Executive Summary
This paper addresses the inefficiency of fixed token budgets in inference-time reasoning by introducing predictive scheduling. The key insight is that allocating the same number of tokens to every query wastes compute on easy inputs and undercomputes hard ones. The authors propose lightweight predictors that estimate each query's optimal reasoning length before generation, enabling dynamic allocation of a fixed total token budget across queries to maximize expected accuracy. On GSM8K, predictive scheduling achieves significant gains over uniform allocation, demonstrating that pre-run budget prediction enables fine-grained control of the compute-accuracy trade-off.

## Method Summary
The method involves training lightweight predictors to estimate per-query reasoning length or difficulty before generation. Two approaches are explored: MLPs trained on transformer hidden states and LoRA-fine-tuned classifiers operating on raw question text. A greedy batch allocator then dynamically distributes a fixed total token budget across queries to maximize expected accuracy. The MLP approach uses hidden states from middle transformer layers (particularly layer 16) as features, while the LoRA approach classifies queries into discrete difficulty buckets. The allocator iteratively assigns additional token windows to queries predicted to yield the highest marginal accuracy gain.

## Key Results
- Predictive scheduling achieves up to 7.9 percentage points higher accuracy than uniform budgeting at the same token cost on GSM8K
- Middle transformer layers (12-17) carry the richest signals for reasoning-length estimation, with layer 16 achieving highest correlation
- Difficulty-based allocation outperforms size-based allocation across all evaluated token budgets
- Predictive scheduling closes over half the gap to an oracle with perfect foresight

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Intermediate transformer layers (12-17) encode task complexity information usable for budget prediction
- **Mechanism:** MLP trained on hidden states from middle layers during pre-fill phase maps query representation to early stopping probabilities
- **Core assumption:** Model's internal representation at specific depth correlates with reasoning steps required
- **Evidence:** Middle layers significantly outperform early/late layers; layer 16 achieves highest test correlation of 0.742
- **Break condition:** If model architecture changes significantly, middle layer heuristic may no longer hold

### Mechanism 2
- **Claim:** Discrete difficulty classification provides more robust allocation signals than continuous length prediction
- **Mechanism:** LoRA adapter fine-tuned on raw question text classifies difficulty; uses pre-computed accuracy curves for each bucket
- **Core assumption:** Queries within same difficulty bucket share similar accuracy-to-budget trajectories
- **Evidence:** Difficulty-based allocation outperforms size-based across all token budgets; LoRA achieves 66.3% test accuracy
- **Break condition:** If test distribution shifts significantly, discrete mapping may misclassify queries

### Mechanism 3
- **Claim:** Greedy allocation strategy maximizes global accuracy under fixed token budget
- **Mechanism:** Allocator initializes minimum budget per query, iteratively distributes tokens to highest marginal gain queries
- **Core assumption:** Predictor provides reliable ranking of where next unit of compute will be most effective
- **Evidence:** Yields up to 7.9pp accuracy gain over uniform budgeting at identical token cost
- **Break condition:** High prediction errors may waste tokens on easy queries, starving difficult ones

## Foundational Learning

- **Concept: Inference-Time Compute (Test-Time Scaling)**
  - **Why needed:** Paper manipulates reasoning budget at inference time rather than model parameters
  - **Quick check:** If model given 10 tokens vs 100 tokens to solve math problem, does accuracy typically increase or decrease? (Answer: Increase)

- **Concept: LoRA (Low-Rank Adaptation)**
  - **Why needed:** Trains efficient difficulty classifier without full fine-tuning cost
  - **Quick check:** Does LoRA update all model weights or just small adapter matrices? (Answer: Adapter matrices only)

- **Concept: Greedy vs. Optimal Allocation**
  - **Why needed:** System uses greedy algorithm making locally optimal choices at each step
  - **Quick check:** Why might greedy algorithm fail here? (Answer: May over-invest in query that looks promising initially but hits diminishing returns faster than expected)

## Architecture Onboarding

- **Component map:** Feature Extractor -> Predictor Model -> Lookup Database -> Allocator -> Generator
- **Critical path:** Prediction must occur before generation; predictor+allocator latency must be less than time saved by reduced generation length
- **Design tradeoffs:** Continuous prediction (MLP) is precise but noisy; discrete classification (LoRA) is robust but coarse
- **Failure signatures:** High-budget inversion (adaptive accuracy drops below uniform); misclassification starvation (hard queries misclassified as easy)
- **First 3 experiments:** 1) Layer sensitivity sweep (train MLPs on layers 1-28); 2) Allocator benchmark at tight budget (32 tokens/query); 3) Robustness check (inject noise into predictor inputs)

## Open Questions the Paper Calls Out

1. Can hybrid approaches combining hidden-state features and linguistic pattern recognition improve predictive accuracy and allocation efficiency?

2. How can predictive scheduling be extended to multi-trace aggregation methods such as self-consistency or tree-of-thoughts?

3. Do predictive signals in middle transformer layers (12-17) generalize to non-arithmetic reasoning domains or different model architectures?

4. How can performance degradation of adaptive allocation at high token budgets be mitigated?

## Limitations

- Results are specific to GSM8K arithmetic benchmark and may not generalize to other domains
- Greedy allocation may accumulate errors when prediction confidence is low
- Method's performance degrades at high token budgets due to prediction errors
- Added latency from prediction and allocation not quantified for latency-sensitive applications

## Confidence

**High Confidence**: Core mechanism of using pre-generation predictors for token allocation is well-supported with rigorous experimental design within GSM8K domain.

**Medium Confidence**: Generalizability of middle-layer heuristic and superiority of discrete classification are less certain and tied to specific model architecture.

**Low Confidence**: Scalability to production environments unclear; does not address bottlenecks from increased prediction latency or robustness under distribution shift.

## Next Checks

1. Validate predictive scheduling on different reasoning benchmark (MATH or StrategyQA) to test generalizability beyond GSM8K.

2. Systematically inject calibrated noise into predictor outputs to measure tolerance threshold where adaptive allocation ceases to outperform uniform budgeting.

3. Measure total system latency (prediction + allocation + generation) under realistic batch sizes to determine viability in latency-constrained settings.