---
ver: rpa2
title: 'Instruction Agent: Enhancing Agent with Expert Demonstration'
arxiv_id: '2509.07098'
source_url: https://arxiv.org/abs/2509.07098
tags:
- agent
- action
- agents
- tasks
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Instruction Agent, a GUI agent that leverages
  expert demonstrations to automate complex tasks that previous agents failed to complete.
  The agent extracts step-by-step instructions from a single human demonstration and
  executes them strictly, avoiding mistakes through built-in verifier and backtracker
  modules.
---

# Instruction Agent: Enhancing Agent with Expert Demonstration

## Quick Facts
- arXiv ID: 2509.07098
- Source URL: https://arxiv.org/abs/2509.07098
- Reference count: 15
- Achieved 60% success rate on 20 challenging OSWorld tasks that all top-ranked agents failed

## Executive Summary
Instruction Agent is a GUI automation agent that leverages expert demonstrations to complete complex tasks that previous agents failed. The agent extracts step-by-step instructions from a single human demonstration and executes them strictly, using built-in verifier and backtracker modules to avoid and recover from errors. On 20 challenging OSWorld tasks where top-ranked agents achieved 0% success, Instruction Agent achieved 60% success by translating visual demonstrations into precise natural language instructions and validating each action through visual verification.

## Method Summary
The system uses a training-free, test-time inference approach. The Instructor module (GPT-4o) converts human demonstrations into step-by-step instructions by capturing screenshots and user inputs, then generating spatially-annotated descriptions. The Actor has four components: UI-Tars grounder finds click coordinates from instructions, Executor (GPT-4o) generates PyAutoGUI code, Verifier (GPT-4o) compares before/after screenshots to validate success, and Backtracker (GPT-4o) recovers from failures by restoring previous states. The system requires human demonstrations with screenshots before/after each action, mouse clicks, and keyboard inputs.

## Key Results
- Achieved 60% success rate on 20 OSWorld tasks that all top-3 agents failed (0% success)
- Outperformed human demonstrations (72.36% success) while maintaining task completion reliability
- Ablation studies showed verifier and backtracker modules were critical, with full system outperforming variants lacking these components

## Why This Works (Mechanism)

### Mechanism 1: Trajectory-Grounded Instruction Translation
Translating visual demonstrations into location-aware natural language instructions reduces planning errors and grounding ambiguity. The Instructor captures screenshots and user inputs, then prompts an LLM to describe actions with spatial annotations (e.g., "blue, underlined hyperlink") and functional intent. This creates a high-fidelity plan that constrains the Actor's search space.

### Mechanism 2: Iterative State Verification
Post-hoc verification mitigates error accumulation by validating the visual outcome of every step. The Verifier compares screenshots before and after actions to check if visual changes match intended effects, forcing a stop signal if actions fail rather than proceeding blindly.

### Mechanism 3: State Restoration via Backtracking
A dedicated recovery loop handles environmental stochasticity (e.g., pop-ups) without restarting tasks. If verification fails, the Backtracker analyzes divergence between current and expected states, generating recovery plans to restore valid states before retrying failed steps.

## Foundational Learning

- **GUI Grounding**: Decouples high-level planning (Instructor) from low-level interaction (Grounder). The system relies on UI-Tars to map text instructions to specific coordinates. *Why needed*: Enables precise UI interaction. *Quick check*: How does the agent handle UI elements that look different in demo vs. test time?

- **POMDP (Partially Observable Markov Decision Process)**: Treats screenshots as observations rather than full states. *Why needed*: Explains why verification is necessary - the agent cannot "know" system state, only "see" the screen. *Quick check*: Why is a screenshot considered a "partial" observation?

- **Test-Time Compute / Inference**: Runs purely on LLM APIs during execution rather than requiring fine-tuning. *Why needed*: Key to understanding the "training-free" nature. *Quick check*: What is the trade-off between fine-tuning costs vs. test-time API latency?

## Architecture Onboarding

- **Component map**: Recorder (captures screen/input) → Instruction Generator (LLM creates text plan) → Grounder (UI-Tars finds coords) → Executor (PyAutoGUI runs code) → Verifier (LLM checks screen) → Backtracker (LLM plan + Executor) on failure

- **Critical path**: The Recorder's timing alignment - must capture screenshot immediately before user action. Timing drift poisons the entire plan.

- **Design tradeoffs**: Strict adherence vs. adaptability (ensures success for personalized workflows but reduces flexibility), complexity vs. robustness (LLM loops increase latency/costs but appear necessary for long-horizon tasks)

- **Failure signatures**: Grounding Drift (vague descriptions or UI changes cause coordinate failures), False Negative Verification (subtle changes trigger unnecessary backtracking), Irreversible State Navigation (backtracker cannot recover from severe misnavigation)

- **First 3 experiments**: 1) Unit Test the Instructor: Record 5-step workflow, inspect JSON instructions for spatial precision. 2) Verifier Stress Test: Inject network disconnection to test detection and backtracker triggering. 3) Ablation Reproduction: Run 10-task subset with backtracker disabled to confirm success rate drop.

## Open Questions the Paper Calls Out

- Can the framework maintain competitive performance using smaller LLMs that run efficiently on edge devices? (Future work mentions testing with different LLMs, particularly smaller models)

- How can the backtracking mechanism be improved to reliably recover from severe environmental divergences? (Future work includes improving backtracking; analysis notes struggles with severe divergence)

- Can expert demonstrations be systematically generalized to support similar but unseen tasks without additional recordings? (Broader Use Cases mentions potential augmentation and generalization but provides no method)

## Limitations

- Critical dependence on LLM's ability to produce spatially precise descriptions - exact prompt templates are unspecified
- Verification assumes screenshots capture all relevant state changes, potentially failing for subtle or delayed effects
- Strict adherence to demonstration workflow reduces flexibility for environmental changes (e.g., specific browsers required)

## Confidence

- **High Confidence**: Core architecture and 60% success rate on previously-failed tasks are well-defined and empirically robust
- **Medium Confidence**: Claimed mechanisms are logically sound but depend on unspecified prompt engineering details
- **Low Confidence**: Assertion that system works "without requiring large-scale trajectory datasets" may overstate the case - still requires multiple high-quality human demonstrations per task

## Next Checks

1. **Instruction Generation Precision**: Record a 5-step workflow and analyze generated instructions for spatial precision - do descriptions like "blue, underlined hyperlink" consistently map to unique UI elements?

2. **Verifier Robustness Test**: Create controlled experiment with subtle visual changes (status indicators, loading spinners) to measure false negative rates and verify system doesn't trigger unnecessary backtracking loops.

3. **Cross-Environment Transfer**: Test same demonstration on modified environments (different browser, dark mode) to quantify performance degradation from strict workflow adherence and identify adaptation limits.