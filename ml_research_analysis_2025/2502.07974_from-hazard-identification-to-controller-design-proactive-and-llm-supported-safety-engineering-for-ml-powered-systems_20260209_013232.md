---
ver: rpa2
title: 'From Hazard Identification to Controller Design: Proactive and LLM-Supported
  Safety Engineering for ML-Powered Systems'
arxiv_id: '2502.07974'
source_url: https://arxiv.org/abs/2502.07974
tags:
- hazard
- system
- safety
- engineering
- control
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes integrating hazard analysis into ML-powered
  software development to proactively anticipate and mitigate risks. The authors demonstrate
  how large language models (LLMs) can partially automate a modified STPA (System-Theoretic
  Process Analysis) process with human oversight at critical steps, addressing the
  challenges of requiring highly experienced safety engineering experts and the time-consuming
  nature of traditional hazard analysis.
---

# From Hazard Identification to Controller Design: Proactive and LLM-Supported Safety Engineering for ML-Powered Systems

## Quick Facts
- arXiv ID: 2502.07974
- Source URL: https://arxiv.org/abs/2502.07974
- Authors: Yining Hong; Christopher S. Timperley; Christian Kästner
- Reference count: 40
- One-line primary result: LLM-assisted STPA can identify 1,159 hazards in minutes for under USD 2, enabling proactive safety engineering for ML systems

## Executive Summary
This paper proposes integrating hazard analysis into ML-powered software development to proactively anticipate and mitigate risks. The authors demonstrate how large language models (LLMs) can partially automate a modified STPA (System-Theoretic Process Analysis) process with human oversight at critical steps, addressing the challenges of requiring highly experienced safety engineering experts and the time-consuming nature of traditional hazard analysis. The approach is illustrated with a running example of a trail recommendation system, showing that many seemingly unanticipated issues can be anticipated through systematic analysis. The authors advocate for making this LLM-supported hazard analysis a routine step in responsible engineering of ML-powered applications.

## Method Summary
The paper presents a modified STPA process enhanced with LLM automation to identify hazards, constraints, and controllers for ML-powered systems. The method starts with defining the system purpose and modeling control structures, then uses GPT-4o to generate stakeholders, losses, hazards, and constraints through iterative prompting. The LLM also proposes controllers and analyzes unsafe control actions. Human oversight remains essential for filtering outputs, consolidating hazards, and making final decisions. The approach is demonstrated on a trail recommendation system, identifying 20 stakeholders, 145 losses, and 1,159 raw hazards (reduced to 50 distinct ones with human supervision).

## Key Results
- LLM-assisted STPA identified 1,159 hazards from 145 losses across 20 stakeholders in minutes at under USD 2 cost
- Many seemingly unanticipated issues were revealed through systematic backward-chaining analysis from losses to hazards
- Control structure modeling revealed missing mechanisms for user feedback, consent withdrawal, and centralized monitoring in the running example

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A backward-chaining analysis from losses to controllers may reveal hazards that forward-looking brainstorming misses.
- Mechanism: STPA starts with stakeholder values and losses, derives system-level hazards, inverts them into safety constraints, then maps controllers to enforce those constraints. This structured inversion forces consideration of conditions that could lead to harm, rather than starting from components and speculating about failures.
- Core assumption: Most "unintended consequences" are not inherently unpredictable; they emerge from incomplete analysis scope rather than fundamental unknowability.
- Evidence anchors:
  - [abstract] "demonstrating that many seemingly unanticipated issues can, in fact, be anticipated"
  - [Section IV-A] The authors identified 1,159 hazards from 145 losses across 20 stakeholders, including non-obvious hazards like "System recommends trails in ecologically sensitive areas or during hazardous conditions"
  - [corpus] Neighbor paper "An LLM-Integrated Framework for Completion, Management, and Tracing of STPA" (arXiv:2503.12043) similarly positions STPA as enabling systematic hazard traceability
- Break condition: The mechanism degrades if stakeholder/loss identification is prematurely narrowed, or if analysts lack sufficient domain knowledge to recognize which hazards are plausible vs. far-fetched.

### Mechanism 2
- Claim: LLM-assisted enumeration can expand analysis scope faster than manual effort alone, reducing the cost barrier that prevents adoption.
- Mechanism: LLMs (specifically GPT-4o in this paper) generate lists of stakeholders, values, losses, hazards, and controller designs when prompted with application descriptions. This allows analyzing "hundreds of potential losses" and "thousands of hazards" in minutes at low cost (~USD 2 for the hazard identification phase).
- Core assumption: LLM-generated suggestions, while including irrelevant or impractical options, contain enough useful signal that human curation yields net benefit.
- Evidence anchors:
  - [abstract] "By using large language models (LLMs) to partially automate a modified STPA process with human oversight at critical steps"
  - [Section IV-A] "we found LLMs effectively support analyzing all losses in a few minutes for under USD 2"
  - [corpus] Evidence is limited; neighbor papers on LLM-STPA integration (arXiv:2503.12043) are concurrent work without empirical validation of effectiveness
- Break condition: Over-reliance on LLM output without human judgment may produce superficial, boilerplate hazards that miss domain-specific risks; quality depends on prompt design and the analyst's ability to filter.

### Mechanism 3
- Claim: Explicitly modeling control structures (including non-technical controllers) surfaces gaps in mitigation that model-centric approaches overlook.
- Mechanism: After identifying hazards, the process requires mapping each constraint to at least one controller—technical (safeguards, I/O validation) or non-technical (human oversight, procedures, monitoring). This reveals where no controller exists and prompts proactive design.
- Core assumption: System-level controllers beyond the ML model (UI design, monitoring dashboards, human review processes) are underutilized in current ML practice but are often more practical than model-level fixes.
- Evidence anchors:
  - [Section I] "existing studies largely discuss only the potential application of hazard analysis...overlooking controllers beyond the model such as safeguards, trend monitoring, human oversight, and user interface modifications"
  - [Section IV-B] The running example's initial control structure revealed missing mechanisms for user feedback, consent withdrawal, and centralized monitoring
  - [Section IV-C] Analysis of unsafe control actions led to explicit procedures for alert fatigue management and accountability check-ins
- Break condition: The mechanism fails if control structures are modeled superficially without tracing feedback loops, or if organizational incentives prevent implementing non-technical controllers.

## Foundational Learning

- Concept: **STPA four-step process (Define Purpose → Model Control Structure → Identify UCAs → Identify Loss Scenarios)**
  - Why needed here: The paper modifies but does not replace STPA; understanding the baseline method is prerequisite to appreciating what changes and why.
  - Quick check question: Can you explain the difference between a "hazard" and a "loss," and why STPA works backward from losses rather than forward from components?

- Concept: **Control theory basics (feedback loops, controllers, controlled processes)**
  - Why needed here: STPA models systems as control structures; identifying unsafe control actions requires understanding how control actions can fail (absent, incorrect, mistimed, wrong sequence, prolonged, stopped early).
  - Quick check question: For a content recommendation system, what would constitute a "feedback" signal to the controller, and what could cause that feedback to be missing or misleading?

- Concept: **ML system failure modes beyond accuracy (hallucination, distribution shift, shortcut learning, reward hacking)**
  - Why needed here: The paper treats ML components as "inherently unreliable"; effective hazard analysis requires knowing how ML fails, not just that it fails.
  - Quick check question: If an ML model's input distribution shifts after deployment, what control structures could detect or mitigate resulting harms?

## Architecture Onboarding

- Component map:
  Application Description → Stakeholder Identification → Loss Enumeration → Hazard Derivation → Constraint Formulation → Controller Design → Control Structure Modeling → Unsafe Control Action Analysis → Loss Scenario Identification → Controller Revision/Iteration

- Critical path: Loss and hazard identification (Section IV-A). If this scope is too narrow, downstream analysis cannot recover; if too broad, analysis becomes unmanageable. The paper reports identifying 1,159 hazards reduced to 50 distinct ones with human supervision—this consolidation step is load-bearing.

- Design tradeoffs:
  - Comprehensiveness vs. tractability: The authors expanded STPA beyond "severe losses" to include fairness, usability, and societal harms, but note the analysis "can become unmanageable" without tool support.
  - LLM automation vs. human judgment: Full automation risks superficial analysis; human-only analysis is too slow. The paper positions LLMs as "support tools, not replacements."
  - Model-centric vs. system-level controllers: Model fixes are often prioritized in ML research, but system-level guards (monitoring, human oversight, UI constraints) may be more implementable.

- Failure signatures:
  - Hazard lists dominated by generic/template items (e.g., "system produces incorrect output") without domain-specific texture
  - Control structures with no feedback loops or where controllers have no defined accountability
  - Analysis that identifies many hazards but proposes no new controllers, suggesting scope-creep without actionability
  - LLM suggestions accepted without filtering, producing impractical or redundant controller designs

- First 3 experiments:
  1. Run the stakeholder→loss→hazard pipeline on a simple ML application you maintain. Compare LLM-generated stakeholder lists against your own enumeration; measure how many hazards you would have missed without the systematic expansion.
  2. For one identified hazard, model the control structure explicitly. Identify at least one controller (technical or non-technical) that does not currently exist in your system. Document what feedback loop would be needed to make that controller effective.
  3. Select one existing controller in your system and apply the UCA checklist (absent, incorrect, mistimed, wrong sequence, prolonged, stopped early). Identify at least one loss scenario and document whether current processes would catch it.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLM support be balanced with human critical judgment to prevent over-reliance while preserving creativity in hazard analysis?
- Basis in paper: [explicit] "Balancing LLM support while preserving human creativity and judgment is a crucial question for future research."
- Why unresolved: While LLMs can scale the process, the authors warn against using them to fully automate compliance, noting that developers are essential for assessing severity and thinking beyond suggestions.
- What evidence would resolve it: User studies comparing the depth and novelty of hazards identified by developers using varying levels of LLM assistance versus manual analysis.

### Open Question 2
- Question: Does LLM-supported automation enable non-experts to perform rigorous hazard analysis, or does it increase the risk of superficial results?
- Basis in paper: [explicit] The authors note that while automation enhances accessibility, "there is a risk that these developers may perform a more superficial analysis."
- Why unresolved: Lowering the barrier to entry via LLMs may allow practitioners to bypass the deep understanding usually required by traditional methods like STPA.
- What evidence would resolve it: Comparative studies measuring the completeness and validity of hazard analyses conducted by safety experts versus non-experts using LLM tools.

### Open Question 3
- Question: What organizational and technical interventions are required to foster the routine adoption of hazard analysis in non-safety-critical, fast-paced development environments?
- Basis in paper: [explicit] "Further research is required to embed hazard analysis into routine engineering, even in non-safety-critical contexts..."
- Why unresolved: Practitioners currently hesitate to integrate these methods into agile workflows due to a lack of incentives, managerial support, and resources.
- What evidence would resolve it: Longitudinal case studies of software teams identifying specific incentives or process modifications that lead to sustained adoption.

## Limitations
- The paper does not provide formal validation that LLM-generated hazards improve actual safety outcomes compared to traditional methods or no analysis
- The reduction from 1,159 to 50 hazards relies on unspecified human consolidation heuristics that may introduce bias
- No quantitative comparison of LLM-assisted vs. manual STPA timelines or effectiveness is presented
- The running example (trail recommendation system) represents a relatively contained domain; scalability to complex, safety-critical systems remains unproven

## Confidence
- **High confidence:** The systematic approach of working backward from losses to hazards is well-grounded in control theory and safety engineering principles
- **Medium confidence:** LLM assistance significantly reduces time/cost barriers for hazard analysis based on reported USD 2 cost and "few minutes" timeframe
- **Low confidence:** The claim that many "seemingly unanticipated issues can be anticipated" lacks empirical validation beyond the single running example

## Next Checks
1. Apply the LLM-assisted STPA process to a safety-critical ML system (e.g., medical diagnosis or autonomous vehicle perception) and compare hazard coverage against domain expert analysis
2. Conduct a controlled experiment measuring whether LLM-assisted hazard identification reduces actual incident rates when implemented in deployed systems
3. Perform inter-rater reliability testing with multiple analysts using the same LLM-assisted process to quantify consistency and reproducibility of results