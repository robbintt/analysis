---
ver: rpa2
title: Frequency-Aware Autoregressive Modeling for Efficient High-Resolution Image
  Synthesis
arxiv_id: '2507.20454'
source_url: https://arxiv.org/abs/2507.20454
tags:
- tokens
- stage
- low-frequency
- image
- regions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SparseV AR accelerates high-resolution autoregressive image synthesis
  by dynamically excluding low-frequency tokens during inference, leveraging the observation
  that such tokens have minimal impact on image quality. The method identifies low-frequency
  regions via lightweight MSE-based metrics and preserves quality using a small set
  of uniformly sampled anchor tokens.
---

# Frequency-Aware Autoregressive Modeling for Efficient High-Resolution Image Synthesis

## Quick Facts
- arXiv ID: 2507.20454
- Source URL: https://arxiv.org/abs/2507.20454
- Reference count: 40
- Authors: Zhuokun Chen; Jugang Fan; Zhuokang Yu; Bohan Zhuang; Mingkui Tan
- Key outcome: SparseV AR accelerates high-resolution autoregressive image synthesis by dynamically excluding low-frequency tokens during inference, achieving up to 2× speedup with negligible quality degradation—for example, reducing Infinity's inference latency by 51% while only decreasing the GenEval score by 0.0051.

## Executive Summary
SparseV AR accelerates high-resolution autoregressive image synthesis by dynamically excluding low-frequency tokens during inference, leveraging the observation that such tokens have minimal impact on image quality. The method identifies low-frequency regions via lightweight MSE-based metrics and preserves quality using a small set of uniformly sampled anchor tokens. SparseV AR is a plug-and-play framework requiring no additional training. Applied to Infinity-2B and HART-0.7B, it achieves up to 2× speedup with negligible quality degradation—for example, reducing Infinity's inference latency by 51% while only decreasing the GenEval score by 0.0051. The approach is effective across diverse image types and outperforms token merging/selection baselines, demonstrating robust acceleration in next-scale prediction models.

## Method Summary
SparseV AR accelerates next-scale prediction autoregressive models by dynamically excluding low-frequency tokens during inference. The method computes MSE changes in feature maps at a selected transformer block to identify low-frequency regions, then excludes tokens in these regions except for uniformly sampled anchor tokens. Excluded tokens receive logits from their most similar anchor token if cosine similarity exceeds a threshold; otherwise, they receive zero feature maps. This plug-and-play approach requires no additional training and operates by skipping inference for excluded tokens, achieving significant speedup with minimal quality degradation.

## Key Results
- Achieves up to 2× speedup in inference for high-resolution image synthesis
- Reduces Infinity-2B latency by 51% with only 0.0051 decrease in GenEval score
- Maintains comparable quality across diverse image types and benchmarks
- Outperforms token merging/selection baselines including ToMe and ZipVL

## Why This Works (Mechanism)

### Mechanism 1: Frequency-Aware Computational Redundancy
High-resolution stages in next-scale prediction predominantly refine high-frequency regions while low-frequency regions receive minimal updates. The paper visualizes ℓ1 differences between consecutive stages, showing residuals concentrate on high-frequency regions (contours, edges) with negligible change in low-frequency areas (backgrounds). This spatial selectivity creates an opportunity for early token exclusion in redundant regions.

### Mechanism 2: Block-Specific Frequency Sensitivity
Different transformer blocks exhibit distinct regional attention patterns, with some blocks showing pronounced feature changes in high-frequency regions. By computing MSE changes in features before and after inference at specific blocks, the method identifies which tokens are being actively refined versus those remaining stable. Tokens with low MSE change at frequency-sensitive blocks are classified as low-frequency.

### Mechanism 3: Anchor Token Proxy Representation
Retaining a sparse uniform grid of anchor tokens preserves low-frequency region quality through local similarity propagation. Excluded tokens are assigned logits from their most similar anchor token within α×α grid if cosine similarity exceeds threshold β. This exploits the observed high similarity between neighboring low-frequency tokens, allowing a few representatives to proxy for their regions.

## Foundational Learning

- **Next-scale prediction paradigm (VAR)**: Understanding that each stage predicts an entire resolution level is essential for grasping why high-resolution stages dominate compute.
- **Residual VAE / Multi-scale codebook**: The redundancy analysis depends on understanding how residuals compose into final images across stages.
- **Attention locality in autoregressive models**: VAR models show "strong local dependencies"—nearly all tokens attend highly to neighbors, explaining why alternative token selection methods fail.

## Architecture Onboarding

- **Component map**: Input feature map r_{k-1} → Block s (MSE computation) → Threshold comparison → M_low exclusion set → Anchor grid (α×α) → Similarity computation → Logit assignment for excluded tokens
- **Critical path**: At stage P-1, compute and cache ΔF^s_{P-1} (MSE change map at selected block s). At stage k ≥ P, interpolate cached ΔF to current resolution, identify M_low_k via Eq. 2. Exclude tokens in M_low_k except anchors, run inference on remaining tokens. For each excluded token, copy logits from most similar anchor if similarity ≥ β.
- **Design tradeoffs**: τ (threshold) controls exclusion aggressiveness; α (anchor grid size) affects quality-speed balance; P (start stage) determines when to begin exclusion; block selection requires empirical validation per model.
- **Failure signatures**: GenEval score drops sharply when τ is too high or P is too low; HART more sensitive to anchor removal than Infinity; attention-based selection methods underperform significantly.
- **First 3 experiments**:
  1. Validate MSE-based frequency identification by visualizing M_low_k masks overlaid on generated images
  2. Block ablation sweep to select optimal block for MSE calculation
  3. Anchor necessity test comparing α=4 vs α="none" (no anchors)

## Open Questions the Paper Calls Out

### Open Question 1
Can the early exclusion of low-frequency tokens be effectively generalized to non-next-scale autoregressive frameworks? The authors note that extending the method to a wider range of autoregressive frameworks remains an important direction.

### Open Question 2
Can a dynamic, content-aware anchor selection strategy improve generation quality over the current uniform sampling approach? The authors suggest developing a dynamic adaptive method as fixed window frequencies may not be optimal for all images.

### Open Question 3
Is there an automated, architecture-agnostic method to identify the optimal transformer block for MSE-based frequency estimation? The method relies on empirically selecting a specific block, which acts as a hyperparameter requiring manual tuning.

## Limitations
- Performance depends on model architecture and content type, with HART showing higher sensitivity to anchor removal than Infinity
- Current uniform anchor sampling may not optimally capture complex low-frequency structures
- Method requires empirical block selection and hyperparameter tuning for each model

## Confidence

| Claim | Confidence |
|-------|------------|
| Up to 2× speedup with minimal quality loss | High |
| Plug-and-play compatibility with pretrained models | High |
| Frequency-selective refinement patterns generalize across image types | Medium |
| Block selection methodology is optimal | Medium |

## Next Checks

1. Validate MSE-based frequency identification by visualizing exclusion masks on generated images to confirm low-frequency regions are correctly identified
2. Replicate block ablation study to identify optimal transformer block for MSE computation in target model
3. Test anchor necessity by comparing GenEval scores with and without anchor tokens in smooth gradient regions