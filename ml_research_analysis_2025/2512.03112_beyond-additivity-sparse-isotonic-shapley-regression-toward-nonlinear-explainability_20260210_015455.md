---
ver: rpa2
title: 'Beyond Additivity: Sparse Isotonic Shapley Regression toward Nonlinear Explainability'
arxiv_id: '2512.03112'
source_url: https://arxiv.org/abs/2512.03112
tags:
- shapley
- sisr
- values
- payoff
- sparsity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Sparse Isotonic Shapley Regression (SISR) is introduced to address
  the non-additivity and sparsity challenges in Shapley value-based explainability.
  SISR learns a monotonic transformation to restore additivity and enforces an L0
  sparsity constraint on the Shapley vector, overcoming limitations of ad hoc post-hoc
  thresholding methods.
---

# Beyond Additivity: Sparse Isotonic Shapley Regression toward Nonlinear Explainability

## Quick Facts
- **arXiv ID**: 2512.03112
- **Source URL**: https://arxiv.org/abs/2512.03112
- **Reference count**: 12
- **Primary result**: SISR learns monotonic transformations to restore additivity and enforces L0 sparsity on Shapley values, achieving better feature selection and attribution stability than standard methods.

## Executive Summary
This paper introduces Sparse Isotonic Shapley Regression (SISR), a method that addresses two fundamental challenges in Shapley value-based explainability: non-additivity of payoff functions and the need for sparse feature attributions. SISR jointly learns a monotonic transformation to restore additivity to non-Gaussian payoff structures and enforces an L0 sparsity constraint on the Shapley vector. The framework leverages Pool-Adjacent-Violators Algorithm for isotonic regression and normalized hard-thresholding for support selection, with theoretical convergence guarantees. Experiments demonstrate that SISR correctly identifies relevant features, recovers true transformations, and stabilizes attributions across diverse payoff schemes.

## Method Summary
SISR addresses non-additivity in Shapley values by learning a monotonic transformation T(·) that converts the coalition worths into an approximately additive form, while simultaneously enforcing L0 sparsity on the attribution vector γ. The method alternates between isotonic regression updates (via PAVA) for the transformation and normalized hard-thresholding updates for sparsity. The objective minimizes weighted squared error between transformed coalition values and linear combinations of attributions, subject to monotonicity constraints on T and cardinality constraints on γ. The approach avoids ad hoc post-hoc thresholding by directly optimizing for sparsity during training.

## Key Results
- SISR recovers true monotonic transformations in synthetic experiments with correlation >0.95 between estimated and true transformed values
- Support recovery rates exceed 65% even at high noise levels (σ₀ = 0.2), with 100% recovery at low noise (σ₀ = 1e-3)
- In prostate cancer regression, SISR correctly identifies relevant features while standard Shapley values are distorted by irrelevant features
- The method achieves stable attributions across different payoff schemes and outperforms L1-penalized alternatives

## Why This Works (Mechanism)

### Mechanism 1
A learned monotonic transformation T(·) can restore additivity to non-Gaussian payoff structures without requiring a predefined analytical form. The framework reformulates Shapley estimation as finding T such that T(ν_A) follows a Gaussian additive model: T*(ν_A) ~ N(Σ_{j∈A} γ*_j, σ²_A). The Pool-Adjacent-Violators Algorithm (PAVA) efficiently solves the weighted isotonic regression to learn T from the pairwise ordering constraints induced by ν. The core assumption is that the underlying payoff distortion is monotonic—preserving the relative ordering of feature importance—rather than arbitrarily non-invertible.

### Mechanism 2
Direct L0 constraint with normalized hard-thresholding provides shrinkage-free sparsity that recovers the true support more reliably than L1-penalized alternatives. Given current estimate γ^(k), the update computes y = γ^(k) - (1/ρ)∇ℓ(γ^(k)), then applies H°(y; s) = H(y; s) / ||H(y; s)||₂, where H retains only the s largest absolute entries. This yields a closed-form global optimizer for the constrained subproblem. The core assumption is that the true support size s* is bounded and approximately known (or selected via RIC criterion).

### Mechanism 3
Alternating optimization between T (via PAVA) and γ (via normalized hard-thresholding) converges to a stationary point with non-increasing loss. A surrogate function g(γ, γ⁻) = ℓ(γ⁻) + ⟨∇ℓ(γ⁻), γ - γ⁻⟩ + (ρ/2)||γ - γ⁻||² upper-bounds the true loss when ρ ≥ ||Z^T W Z||₂. Each alternation decreases the surrogate, hence the original loss. The core assumption is that the weight matrix W (Shapley weights) and design matrix Z (incidence matrix) yield bounded spectral norm for ρ selection.

## Foundational Learning

- **Shapley Values and Additivity Assumption**
  - Why needed here: SISR is built on the recognition that standard Shapley values implicitly assume ν_A = Σ β_j, which is violated by R² payoffs with correlated/irrelevant features. Understanding this assumption clarifies why transformation is necessary.
  - Quick check question: Given a 3-feature model where ν_{1,2} = max(β₁, β₂), would standard Shapley values produce correct attributions? (Answer: No—the max operation violates additivity.)

- **Isotonic Regression and PAVA**
  - Why needed here: The t-update in Algorithm 1 solves a weighted isotonic regression problem. PAVA is an O(n) stack-based algorithm that pools adjacent violators to enforce monotonicity.
  - Quick check question: If input points (x, y) = [(1, 5), (2, 3), (3, 4)] must be fit monotonically, what is the isotonic solution? (Answer: Pool positions 2-3 to yield [(1, 5), (2, 3.5), (3, 3.5)].)

- **Hard Thresholding and Normalization**
  - Why needed here: The γ-update requires selecting top-s entries and normalizing to unit norm. This differs from soft thresholding (L1) which shrinks all coefficients.
  - Quick check question: For vector [3, -1, 2, 0.5] with s=2, what is H°([3,-1,2,0.5]; 2)? (Answer: Keep |3| and |2|, normalize: [3/√13, 0, 2/√13, 0].)

## Architecture Onboarding

- **Component map**: Input: ν = [ν_A]_{A⊆2^F} (baseline-adjusted coalition values), Z ∈ R^{2^p × p} (incidence matrix: which features in each subset), W = diag(w_SH(A)) (Shapley weights per Eq. 5), s (sparsity upper bound). Iteration: 1. γ-update: y = γ - (1/ρ)Z^T W(Zγ - t), γ = H°(y; s). 2. t-update: δ = Zγ, t = PAVA(δ, W) with monotonicity constraints from ν. Output: γ (sparse attributions), t ≈ T̂(ν), β̂ = T̂⁻¹(γ)

- **Critical path**: 1. Precompute Z^T W and Z^T W t once before iteration loop. 2. Initialize t^(0) = C·ν with C scaling for numerical stability. 3. Inner loop: iterate γ-updates until convergence (gradient-based). 4. Outer loop: single t-update via PAVA per iteration. 5. Convergence: check ||γ^{k+1} - γ^k||₂ → 0

- **Design tradeoffs**: Full vs. sampled coalitions: For p > 15-20, computing all 2^p coalition values is intractable. The Bank Credit experiment uses 1,000 sampled coalitions (Section 4.6). Sampling reduces accuracy but enables scalability. Strict vs. non-strict monotonicity: Implementation uses non-decreasing constraints (t_i ≤ t_j when ν_i ≤ ν_j) rather than strict monotonicity for numerical stability. For invertible T̂, consider ϵ-margin enforcement. RIC vs. manual s selection: RIC criterion (Foster & George, 1994) automates s selection; experiments show it identifies reasonable values (e.g., s=6 for prostate data).

- **Failure signatures**: Flat or degenerate T̂: If estimated transformation is nearly constant, check that ν values are properly baseline-adjusted (ν_∅ = 0) and normalization constraint is active. Oscillating loss: ρ may be too small; verify ρ ≥ ||Z^T W Z||₂ via spectral norm computation. All-zero γ: Initialization may be inadequate; try t^(0) = C·ν with larger C, or check if s is too small relative to signal.

- **First 3 experiments**: 1. Validate transformation recovery: Generate synthetic data with known T* (e.g., T* = √·) following Section 4.1 protocol. Plot estimated T̂ vs. true T* to verify alignment. Success criterion: high correlation (>0.95) between estimated and true transformed values. 2. Test sparsity recovery: Generate sparse γ* with s* = 3 nonzeros, varying noise σ₀ and dimension p per Section 4.2. Report support recovery rate and affinity score. Success criterion: 100% support recovery at low noise (σ₀ = 1e-3), >65% at high noise (σ₀ = 0.2). 3. Compare to standard Shapley on correlated features: Replicate Section 4.3 regression experiment with Toeplitz covariance (θ = 0.9) and sparse true coefficients. Verify that standard Shapley shows distorted attributions while SISR recovers correct ranking. Success criterion: SISR correctly downweights irrelevant features that standard Shapley elevates.

## Open Questions the Paper Calls Out

### Open Question 1
Can the SISR framework be extended to simultaneously estimate a monotonic transformation and explicit higher-order feature interaction indices? The current formulation strictly enforces an additive main-effect structure on the transformed scale. Incorporating interactions requires generalizing the T-additive model and defining new constraints to manage the combinatorial explosion of parameters. What evidence would resolve it: A unified optimization framework that converges on both a transformation T̂ and a sparse set of interaction coefficients β_{jk}, along with empirical results showing improved fidelity on synthetic data with known interaction effects.

### Open Question 2
What are the theoretical convergence guarantees and estimation error bounds when the SISR algorithm relies on sampled coalition approximations rather than the full power set? The theoretical analysis in Section 3 assumes the full incidence matrix Z ∈ R^{2^p} is available, but Section 4.6 acknowledges that for p=20, computing the full game is intractable and uses a sampled subset of 1,000 coalitions. The global convergence proof relies on specific gradient and Hessian calculations derived from the complete set of Shapley weights. What evidence would resolve it: Theoretical bounds on the recovery error ||T̂ - T*|| relative to the number of sampled coalitions, and empirical stability analysis of the transformation T̂ as the sample size decreases.

### Open Question 3
How does the specific choice of the sparsity parameter s impact the estimated monotonic transformation T̂, and can s be optimized adaptively? While SISR enforces ||γ||_0 ≤ s, the interaction between the sparsity level and the isotonic regression fitting is complex. Over-sparsification might force the transformation T to overcompensate for missing features, potentially distorting the "true" nonlinear shape of the payoff function. What evidence would resolve it: Sensitivity analysis showing the variance of T̂ across different levels of s, or the derivation of an adaptive thresholding scheme that minimizes reconstruction error without a fixed cardinality constraint.

## Limitations
- Scalability constraints: Full coalition computation (2^p subsets) becomes intractable for p > 15-20, requiring sampling approximations
- Sparsity parameter dependence: Method performance depends critically on accurate selection of sparsity parameter s, which may be difficult to estimate in real applications
- Implementation complexity: Requires careful numerical implementation of PAVA and normalized hard-thresholding with proper initialization and convergence monitoring

## Confidence

- **Mechanism 1 (Transformation learning via isotonic regression)**: High - Supported by theoretical guarantees and experimental validation showing successful recovery of known transformations
- **Mechanism 2 (L0 sparsity with normalized hard-thresholding)**: Medium - Theorem 1 provides global optimality proof, but real-world performance depends on accurate s selection
- **Mechanism 3 (Alternating optimization convergence)**: High - Theorem 2 establishes non-increasing loss property with explicit conditions on ρ

## Next Checks

1. **Scalable implementation test**: Implement SISR with 1,000-sampled coalitions on a 20-feature dataset and verify that support recovery and attribution quality remain comparable to full coalition computation

2. **Robustness to s-misspecification**: Systematically vary the sparsity parameter s around the true support size s* in synthetic experiments to quantify sensitivity of support recovery rates

3. **Comparison with L1 alternatives**: Replicate the prostate cancer experiment using LASSO-regularized Shapley regression and compare attribution accuracy and stability against SISR's L0 approach