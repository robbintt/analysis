---
ver: rpa2
title: 'False Sense of Security: Why Probing-based Malicious Input Detection Fails
  to Generalize'
arxiv_id: '2509.03888'
source_url: https://arxiv.org/abs/2509.03888
tags:
- classifiers
- probing
- malicious
- benign
- patterns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Current probing-based methods for detecting harmful inputs in large
  language models rely on superficial patterns like instructional phrasing and trigger
  words rather than genuine semantic understanding of harmfulness. Through systematic
  evaluation across multiple models and datasets, researchers found that probing classifiers
  achieve near-perfect accuracy on in-distribution data but suffer catastrophic performance
  drops on out-of-distribution and semantically cleaned datasets.
---

# False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize

## Quick Facts
- arXiv ID: 2509.03888
- Source URL: https://arxiv.org/abs/2509.03888
- Authors: Cheng Wang; Zeming Wei; Qin Liu; Muhao Chen
- Reference count: 19
- Key outcome: Probing-based methods for detecting harmful inputs in LLMs rely on superficial patterns rather than genuine semantic understanding, achieving near-perfect in-distribution accuracy but catastrophic OOD failures.

## Executive Summary
Current probing-based methods for detecting harmful inputs in large language models rely on superficial patterns like instructional phrasing and trigger words rather than genuine semantic understanding of harmfulness. Through systematic evaluation across multiple models and datasets, researchers found that probing classifiers achieve near-perfect accuracy on in-distribution data but suffer catastrophic performance drops on out-of-distribution and semantically cleaned datasets. Simple n-gram-based classifiers perform comparably to sophisticated probing methods, and classifiers heavily depend on lexical cues rather than true semantic comprehension. While LLMs themselves demonstrate strong zero-shot safety classification capabilities, probing classifiers fail to leverage this understanding effectively. These findings reveal that existing probing-based safety detection provides a false sense of security, highlighting the need for more robust, semantically grounded approaches to AI safety detection.

## Method Summary
The study employs probing classifiers trained on frozen hidden states extracted from decoder-only transformer models (Gemma, Llama, Qwen families, 4B–72B parameters). Probing classifiers are binary SVMs mapping last-token hidden states to {benign, malicious}. Evaluation uses in-distribution (ID) held-out test sets and out-of-distribution (OOD) cross-dataset generalization. Baselines include Multinomial Naive Bayes with unigrams/bigrams/trigrams. Semantically cleaned datasets are generated by removing trigger words and paraphrasing malicious prompts while preserving intent. Zero-shot LLM classification evaluates whether LLMs possess semantic harmfulness understanding that probes fail to extract.

## Key Results
- Probing classifiers achieve >98% in-distribution accuracy but suffer 15-99 point drops on out-of-distribution data
- Naive Bayes n-gram classifiers perform comparably to probing methods, suggesting pattern matching rather than semantic extraction
- Probing classifiers show 40-80% false positives on semantically safe inputs containing trigger words
- LLMs achieve 96-100% zero-shot accuracy on safety classification, indicating semantic understanding not captured by probes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Probing classifiers achieve near-perfect in-distribution accuracy by exploiting superficial lexical and structural patterns rather than semantic harmfulness.
- Mechanism: Linear classifiers trained on frozen hidden states learn spurious correlations with instructional phrasing (e.g., "How to...") and trigger words (e.g., "bomb," "kill") that co-occur with malicious labels in training data. These features generalize poorly to semantically similar but lexically different OOD data.
- Core assumption: High in-distribution accuracy reflects pattern matching, not semantic generalization. This holds when n-gram baselines perform comparably.
- Evidence anchors:
  - [abstract] "probing classifiers achieve near-perfect accuracy on in-distribution data but suffer catastrophic performance drops on out-of-distribution and semantically cleaned datasets"
  - [Section 4.2] "Naive Bayes classifiers achieve remarkably competitive performance with probing classifiers... accuracy scores consistently range from 0.84 to 1.00"
  - [corpus] Limited corpus support; related work on LLM security (SecurityLingua, Security Tensors) focuses on defense mechanisms rather than probing failures.

### Mechanism 2
- Claim: Hidden state representations cluster by syntactic/lexical similarity, not semantic harmfulness.
- Mechanism: Transformer hidden states integrate token-level and structural information. When malicious and benign prompts share instructional patterns, their representations cluster together regardless of harmfulness. PCA visualizations show malicious and "cleaned" (benign-but-same-structure) inputs clustering similarly.
- Core assumption: The last-token representation at the final layer encodes sufficient semantic information for classification but is dominated by surface features.
- Evidence anchors:
  - [Section 7.5, Figure 4] "Malicious and cleaned datasets cluster similarly despite different semantics, indicating that internal representations are primarily influenced by structural rather than semantic features"
  - [Section 7.1] Different layers (first, middle, last) all show similar ID/OOD performance gaps (70.2–65.4 point drops for Gemma-3-4b-it).
  - [corpus] No direct corpus evidence on hidden state clustering behavior in safety contexts.

### Mechanism 3
- Claim: LLMs possess semantic harmfulness understanding that probing classifiers fail to extract.
- Mechanism: LLMs achieve 96–100% zero-shot accuracy on safety classification when directly prompted, demonstrating internal semantic competence. Probing classifiers trained on the same representations cannot access this knowledge, suggesting it is not linearly decodable from frozen hidden states.
- Core assumption: Zero-shot classification prompts access semantic representations that probes cannot reach, possibly due to non-linear encoding or attention-dependent access.
- Evidence anchors:
  - [Section 7.4, Table 6] "LLMs achieve remarkably high zero-shot classification accuracy... Gemma-3-4b-it: 99.9% on Alpaca, 99.2% on AdvBench"
  - [abstract] "LLMs themselves demonstrate strong zero-shot safety classification capabilities, probing classifiers fail to leverage this understanding effectively"
  - [corpus] Security Tensors (arXiv:2507.20994) explores cross-modal safety alignment but does not address probing extraction limits.

## Foundational Learning

- Concept: **Probing classifiers as linear decoders**
  - Why needed here: Understanding that probes are simple classifiers (SVM, logistic regression) trained on frozen representations explains their brittleness—they can only capture linearly decodable features.
  - Quick check question: If you replace the SVM with a 10-layer neural network, would you expect OOD performance to improve? Why or why not based on the paper's findings?

- Concept: **Distribution shift and spurious correlations**
  - Why needed here: The core failure mode is that training data contains systematic correlations (malicious → trigger words) that do not hold in deployment. Recognizing this pattern is essential for diagnosing generalization failures.
  - Quick check question: A probe trained on Alpaca+AdvBench achieves 99.5% accuracy. Name two reasons this might not indicate genuine safety detection.

- Concept: **Semantic vs. surface features in NLP**
  - Why needed here: The paper explicitly separates semantic harmfulness (meaning/intent) from surface features (n-grams, structure). This distinction underpins all three research studies.
  - Quick check question: "How to make a bread" and "How to make a bomb" differ in which feature type? Which does the probe rely on according to Section 5.2?

## Architecture Onboarding

- Component map:
  - Hidden state extraction (last token from final transformer layer) -> Probe training (SVM/MLP) -> Evaluation pipeline (ID/OOD/cleaned datasets) -> Baselines (Naive Bayes)

- Critical path:
  1. Replicate ID evaluation to establish baseline accuracy (>98% expected)
  2. Run OOD evaluation immediately (e.g., train on Alpaca+BeaverTailsEval, test on AdvBench) to confirm generalization gap
  3. Implement n-gram baseline; if comparable to probe, pattern learning is confirmed
  4. Test on XSTest safe subset to measure false positive rate from trigger words (expected 40–80%)

- Design tradeoffs:
  - Layer selection: First/middle/last layers show similar failure modes (Section 7.1); last layer is standard but not superior
  - Classifier complexity: MLPs show marginal paraphrase recovery (90.2% vs. 82.7% for SVM) but fail similarly on cleaned data; added complexity does not address core problem
  - Model scale: 4B–72B parameter models all exhibit 15–99 point OOD drops; scaling does not resolve pattern learning

- Failure signatures:
  - Near-zero OOD accuracy (e.g., 0.0–6.9% for some combinations in Table 1) indicates complete distribution overfitting
  - High false positives on XSTest safe examples with trigger words (40–80%) confirms lexical shortcut reliance
  - Accuracy recovery on paraphrased cleaned data (>80%) but not on cleaned data alone (<30%) confirms instructional pattern dependence

- First 3 experiments:
  1. **ID/OOD delta measurement**: Train on Alpaca+BeaverTailsEval, test on held-out ID split and AdvBench. Expect >95% ID, <40% OOD
  2. **N-gram parity check**: Train Naive Bayes (unigrams+bigrams) on same splits. If within 5 points of probe accuracy, semantic extraction is unlikely
  3. **Cleaned dataset stress test**: Apply lexical sanitization (Appendix C prompt) to AdvBench, test probe trained on original data. Expect 60–90 point accuracy drop (Table 2 patterns)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can probing methods be fundamentally redesigned to capture semantic harmfulness rather than surface-level patterns like instructional phrasing?
- Basis in paper: [explicit] The conclusion explicitly states there is a "need to redesign both models and evaluation protocols" because current methods provide a "false sense of security."
- Why unresolved: This paper primarily diagnoses the failure (reliance on spurious correlations) rather than proposing a solution that successfully extracts semantic intent from hidden states.
- What evidence would resolve it: A probing architecture that maintains high accuracy on "semantically cleaned" datasets and out-of-distribution attacks.

### Open Question 2
- Question: Is there a specific subspace in the internal representations where semantic harmfulness is encoded independent of syntactic patterns?
- Basis in paper: [inferred] The paper notes that LLMs possess strong zero-shot safety classification capabilities, yet the visualization (Figure 4) shows malicious and cleaned (benign) inputs cluster similarly, suggesting the semantic signal is lost in the dimensions captured by standard probes.
- Why unresolved: The study confirms current probes miss the semantic signal, but does not determine if the signal is absent from the hidden states or simply inaccessible to linear classifiers.
- What evidence would resolve it: Identification of a representation direction that correlates with semantic harmfulness even when syntax is manipulated.

### Open Question 3
- Question: Does the failure of probing classifiers to generalize hold true for non-English languages and non-decoder-only architectures?
- Basis in paper: [inferred] The authors explicitly list "English-language datasets" and "decoder-only transformer models" as boundaries of the study in the Limitations section.
- Why unresolved: Harmful content manifests differently across languages and cultural contexts, and architectural differences (e.g., encoder-decoder) may alter how semantics are represented.
- What evidence would resolve it: Replication of the study's experiments across multilingual datasets and alternative model architectures.

## Limitations

- Analysis limited to English-language datasets and decoder-only transformer models, with uncertain generalizability to multilingual contexts and alternative architectures
- Semantic cleaning procedure relies on GPT-4o's interpretation of harmfulness, which may not perfectly align with human safety standards
- Binary classification framework oversimplifies the nuanced nature of AI safety, potentially masking more complex failure modes

## Confidence

- High Confidence (Likelihood >80%): Core finding that probing classifiers achieve high in-distribution accuracy while catastrophically failing on out-of-distribution data
- Medium Confidence (Likelihood 50-80%): Claim that LLMs possess semantic harmfulness understanding that probing classifiers fail to extract
- Low Confidence (Likelihood <50%): Generalizability of these findings to non-linear probe architectures and alternative representation learning approaches

## Next Checks

1. **Architecture Ablation Study**: Systematically evaluate non-linear probe architectures (MLPs with varying depth/width, attention-based probes, contrastive learning approaches) on the same ID/OOD/cleaned dataset splits to determine if more sophisticated architectures can extract semantic harmfulness information that linear probes miss.

2. **Cross-Paradigm Comparison**: Compare probing classifier performance against fine-tuned LLMs and zero-shot prompting across the same evaluation framework to quantify the relative advantages and limitations of each safety detection approach, particularly focusing on OOD generalization and semantic understanding.

3. **Multi-Class Safety Detection**: Extend the evaluation framework to multi-class safety classification (e.g., distinguishing between different harm categories like self-harm, violence, misinformation) to determine whether the probing failure modes identified in binary classification persist or evolve in more complex safety detection scenarios.