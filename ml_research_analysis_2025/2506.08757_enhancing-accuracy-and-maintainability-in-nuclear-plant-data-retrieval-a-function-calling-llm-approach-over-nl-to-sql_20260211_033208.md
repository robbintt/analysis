---
ver: rpa2
title: 'Enhancing Accuracy and Maintainability in Nuclear Plant Data Retrieval: A
  Function-Calling LLM Approach Over NL-to-SQL'
arxiv_id: '2506.08757'
source_url: https://arxiv.org/abs/2506.08757
tags:
- function
- query
- calling
- data
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accurate and transparent
  data retrieval from complex nuclear plant databases, where traditional NL-to-SQL
  methods pose risks due to validation difficulties and legacy system complexity.
  The proposed solution replaces direct SQL generation with a function-calling LLM
  approach, where pre-approved, purpose-specific functions encapsulate validated SQL
  logic, reducing errors and enhancing trust.
---

# Enhancing Accuracy and Maintainability in Nuclear Plant Data Retrieval: A Function-Calling LLM Approach Over NL-to-SQL

## Quick Facts
- arXiv ID: 2506.08757
- Source URL: https://arxiv.org/abs/2506.08757
- Reference count: 0
- Key outcome: Function-calling LLM approach achieved higher correctness scores (average 4.2 vs. 2.6) compared to NL-to-SQL in nuclear plant data retrieval.

## Executive Summary
This paper addresses the challenge of accurate and transparent data retrieval from complex nuclear plant databases, where traditional NL-to-SQL methods pose risks due to validation difficulties and legacy system complexity. The proposed solution replaces direct SQL generation with a function-calling LLM approach, where pre-approved, purpose-specific functions encapsulate validated SQL logic, reducing errors and enhancing trust. Using GPT-4o, the system routes user queries through specialized agents, enforces strict data schemas with Pydantic models, and incorporates retry logic and structured logging. Human evaluation showed the function-calling method achieved higher correctness scores (average 4.2 vs. 2.6) compared to the non-function-calling approach, while both performed similarly on LLM-computed relevance and faithfulness metrics. This approach improves accuracy and maintainability by shifting validation from end-users to expert-reviewed functions.

## Method Summary
The paper proposes a multi-agent architecture using GPT-4o that routes natural language queries to domain-specific sub-agents, each with pre-approved functions encapsulating validated SQL logic. The system enforces parameter schemas using Pydantic models, employs OpenAI constrained decoding to ensure output format compliance, and implements retry logic for validation failures. A main agent classifies queries and dispatches them to specialized sub-agents (e.g., work orders, equipment), while structured logging records all interactions for troubleshooting. The function-calling approach contrasts with a baseline NL-to-SQL method using RAG and schema-aware generation, evaluated on correctness, relevance, and faithfulness metrics.

## Key Results
- Function-calling approach achieved average correctness score of 4.2 vs. 2.6 for non-function calling method in human evaluation
- Both approaches performed similarly on LLM-computed relevance and faithfulness metrics
- System successfully routes queries through specialized agents while maintaining validation through Pydantic models
- Human evaluators found function-calling method provided more reliable answers for nuclear plant operational queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Encapsulating SQL in pre-approved functions improves correctness by reducing generation variability.
- Mechanism: Expert-reviewed functions hide schema complexity; the LLM only selects and parameterizes, eliminating direct SQL hallucination paths.
- Core assumption: The function library covers most user intents; uncovered intents return controlled failures rather than wrong answers.
- Evidence anchors:
  - [abstract] "pre-approved, purpose-specific functions encapsulate validated SQL logic… reducing errors"
  - [section 3.1] "limiting the SQL generation to predefined queries and only using the reasoning of the LLM to decide which interaction is appropriate"
  - [corpus] FinAI Data Assistant (arXiv:2510.14162) reports similar pattern: routing to vetted, parameterized functions instead of full SQL synthesis.
- Break condition: Coverage gaps grow faster than curation capacity, or function parameter schemas become too complex for reliable LLM population.

### Mechanism 2
- Claim: Constrained decoding + Pydantic validation prevents malformed outputs and enables safe retries.
- Mechanism: JSON Schema defines a grammar; token masking zeros out invalid continuations; Pydantic enforces types post-hoc and triggers retries on violations.
- Core assumption: Errors are recoverable by re-prompting with the same or refined instructions.
- Evidence anchors:
  - [abstract] "enforces strict data schemas with Pydantic models, and incorporates retry logic"
  - [section 2.3] "constrained decoding… ensures 100% consistency with the specified output format"
  - [corpus] No direct corpus evidence on constrained decoding; generalization beyond this paper is uncertain.
- Break condition: Schema complexity exceeds efficient CFG compilation, or latency budgets cannot absorb retry overhead.

### Mechanism 3
- Claim: Multi-agent routing with limited tool scope reduces function-misselection errors.
- Mechanism: A main agent dispatches to domain-specific sub-agents; each sub-agent only sees a subset of functions, mitigating the 20–25 tool overload threshold.
- Core assumption: Queries map cleanly to a single domain; cross-domain queries are rare or can be decomposed.
- Evidence anchors:
  - [abstract] "system routes user queries through specialized agents"
  - [section 2.1] "studies indicate that performance declines after 20-25 functions due to cognitive overload"
  - [corpus] Gorilla (arXiv:2305.15334) uses retrieval-based tool filtering to reduce candidate tools per query.
- Break condition: Queries require cross-domain reasoning or chained tool calls—currently unsupported (section 2.5 notes this limitation).

## Foundational Learning

- **Function-calling vs. direct SQL generation**: Why needed here: The paper's central design choice; you must understand why a model chooses/parameterizes a function versus synthesizing raw SQL. Quick check question: If a user asks "show overdue work orders for unit 4," which approach generates the WHERE clause: the LLM or the encapsulated function?
- **Constrained decoding / structured outputs**: Why needed here: Guarantees that LLM outputs conform to schemas required by downstream tools; enables automated validation and retry. Quick check question: What token-level mechanism prevents an LLM from emitting a string where an integer is expected?
- **Tool overload in LLMs**: Why needed here: Motivates the multi-agent, domain-partitioned design; explains why a monolithic function set would degrade accuracy. Quick check question: At roughly how many simultaneous tools does function-selection accuracy drop in prior benchmarks?

## Architecture Onboarding

- **Component map**: Main agent -> Sub-agents (domain-specific) -> Function library -> Pydantic models -> Database -> Logging DB
- **Critical path**: 1. User query → Main agent (classify intent) → Sub-agent (select function) → Pydantic validation → Execute SQL → Return JSON → Sub-agent answer → Main agent answer. 2. On validation failure or empty result: retry (within limits) or surface controlled error.
- **Design tradeoffs**: Control vs. flexibility: pre-approved functions reduce risk but constrain expressiveness; NL-to-SQL offers flexibility at higher error risk. Upfront cost vs. maintenance: function curation is front-loaded; paper suggests NL-to-SQL tools can assist initial code generation, shifting effort to validation. Model size vs. capability: multi-turn conversation + function calling appears to need ≥70B parameters (per Meta guidance cited); smaller specialized models may handle subsets.
- **Failure signatures**: Tool misselection: query routed to wrong sub-agent (e.g., "work orders against SG2" → equipment agent instead of work-order agent). Parameter formatting errors: dates, IDs in wrong format; caught by Pydantic, retried. Empty/incorrect results from non-function-calling path: human correctness scores show more frequent completely wrong answers. Unhandled chained queries: system cannot yet decompose multi-step tool chains (noted as limitation).
- **First 3 experiments**: 1. Reproduce the correctness gap on a sample query set: compare function-calling vs. direct NL-to-SQL on 20 domain queries, score with SMEs using the paper's 0–5 rubric. 2. Test tool-count sensitivity: give a single agent 30 vs. 10 functions and measure selection accuracy and latency. 3. Validate retry effectiveness: log Pydantic rejection rate and resolution on retry; estimate latency overhead per successful query.

## Open Questions the Paper Calls Out

- Can advanced reasoning models effectively decompose and execute chained queries requiring sequential tool calls in nuclear plant data retrieval? Basis: "This is currently not supported in this version of our system, but we plan to add a reasoning model as the first step to make a detailed plan for the other agents to execute." Why unresolved: The current system cannot handle multi-step workflows (e.g., equipment-to-parts-to-stock lookups) that require orchestrating multiple functions. What evidence would resolve it: Implementation and benchmarking of a reasoning model that successfully executes chained queries with measurable accuracy improvements.
- Can a pure function-calling approach succeed with complex, jargon-heavy nuclear database schemas without pre-approved functions? Basis: "While our preliminary experiments with complex, jargon-heavy schemas were unsuccessful, this approach might be effective for databases with clearer and more standardized schemas." Why unresolved: The authors attempted schema-only approaches but found them inadequate for domain-specific complexity; the boundary conditions remain unknown. What evidence would resolve it: Comparative testing across databases with varying schema complexity and jargon density, measuring retrieval accuracy.
- How does retrieval accuracy and latency change as the function library scales beyond the 20-25 function cognitive overload threshold? Basis: The literature review notes performance declines after 20-25 functions, but the paper does not test how their multi-agent routing approach scales with larger function libraries. Why unresolved: No experiments reported on system behavior with expanding function sets typical of production deployments. What evidence would resolve it: Controlled experiments measuring function selection accuracy and response time across libraries of 10, 25, 50, and 100+ functions.

## Limitations

- Limited empirical validation: Claims rest on single expert evaluation (5 annotators) on proprietary dataset without ablation studies or statistical significance testing
- Schema and function coverage: Success depends on expert-curated function libraries; coverage gaps and failure cases are not quantified
- Latency and cost implications: Retry logic and multi-agent routing likely increase response times and API costs, but these trade-offs aren't quantified

## Confidence

- **High confidence**: Function-calling approach demonstrably outperforms direct NL-to-SQL on this dataset's correctness metric (4.2 vs 2.6 on 0-5 scale)
- **Medium confidence**: Mechanisms (Pydantic validation, constrained decoding, multi-agent routing) are sound but their relative contribution to accuracy gains is unmeasured
- **Low confidence**: Claims about latency, cost, and maintenance benefits are unsupported by data; the 20-25 tool threshold is cited but not tested

## Next Checks

1. **Ablation study**: Run the same evaluation with (a) function-calling without Pydantic validation, (b) function-calling without retry logic, (c) single-agent vs. multi-agent routing to isolate each mechanism's contribution to accuracy gains
2. **Statistical significance testing**: Apply paired t-tests or non-parametric equivalents to the correctness scores to determine if the 4.2 vs 2.6 difference is statistically significant given the small sample size
3. **Coverage analysis**: Systematically document which query intents fail due to missing functions versus other failure modes (misrouting, parameter errors) to quantify the approach's practical coverage limits