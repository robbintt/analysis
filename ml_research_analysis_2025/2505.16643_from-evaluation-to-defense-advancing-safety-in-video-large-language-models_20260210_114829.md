---
ver: rpa2
title: 'From Evaluation to Defense: Advancing Safety in Video Large Language Models'
arxiv_id: '2505.16643'
source_url: https://arxiv.org/abs/2505.16643
tags:
- video
- safety
- harmful
- qwen2
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the under-examined safety risks of video-based
  large language models (Video LLMs) compared to image-based models. To study this
  problem, the authors introduce VideoSafetyBench (VSB-77k), the first large-scale,
  culturally diverse benchmark for Video LLM safety, containing 77,646 video-query
  pairs spanning 19 principal risk categories across 10 language communities.
---

# From Evaluation to Defense: Advancing Safety in Video Large Language Models

## Quick Facts
- **arXiv ID**: 2505.16643
- **Source URL**: https://arxiv.org/abs/2505.16643
- **Reference count**: 40
- **Primary result**: VideoSafety-R1 achieves 65.1% improvement on VSB-Eval-HH, addressing 42.3% average safety degradation from video modality integration

## Executive Summary
This paper tackles the under-examined safety risks of video-based large language models (Video LLMs) by introducing VideoSafetyBench (VSB-77k), the first large-scale, culturally diverse benchmark for Video LLM safety. The evaluation reveals that integrating video modality degrades safety performance by an average of 42.3%, exposing systemic vulnerabilities. To address this, the authors propose VideoSafety-R1, a dual-stage framework that achieves unprecedented safety gains through alarm token-guided safety fine-tuning and safety-guided reinforcement learning, improving defense success rates by 65.1% on their benchmark while maintaining reasonable performance on benign inputs.

## Method Summary
The research introduces a two-stage defense framework for Video LLMs. First, VideoSafetyBench (VSB-77k) is constructed as a comprehensive benchmark containing 77,646 video-query pairs across 19 risk categories and 10 languages. The defense framework, VideoSafety-R1, employs Alarm Token-Guided Safety Fine-Tuning (AT-SFT) that injects learnable alarm tokens into visual and textual sequences for explicit harm perception, followed by Safety-Guided GRPO that enhances defensive reasoning through dynamic policy optimization with rule-based rewards. The framework is trained on VideoLLaMA3-2B using a combination of safety reasoning data and carefully designed reward mechanisms.

## Key Results
- VideoSafety-R1 achieves 65.1% improvement on VSB-Eval-HH safety benchmark
- Framework improves by 59.1%, 44.3%, and 15.0% on image safety datasets MMBench, VLGuard, and FigStep respectively
- Video modality integration degrades safety performance by average 42.3% across tested models
- False Refusal Rate of 13.4% on benign queries, up from 0.7% baseline

## Why This Works (Mechanism)
The framework addresses video-specific safety challenges through multimodal harm detection and reasoning. AT-SFT introduces alarm tokens that learn to signal harmful content across both visual and textual modalities, while the multi-task objective ensures the model simultaneously maintains base capabilities and safety awareness. Safety-Guided GRPO employs dynamic reward adaptation that adjusts based on model performance, enabling more nuanced safety reasoning than static reward approaches. The combination allows the model to explicitly perceive and reason about harmful content in video contexts rather than relying solely on post-hoc filtering.

## Foundational Learning

**Video LLM Safety Degradation**: Video modality integration can reduce safety performance by 42.3% on average. *Why needed*: Understanding baseline vulnerability before implementing defenses. *Quick check*: Compare safety metrics between image-only and video-based models on same benchmarks.

**Multimodal Harm Detection**: Alarm tokens enable explicit harm perception across visual and textual sequences simultaneously. *Why needed*: Single-modality safety mechanisms insufficient for video content. *Quick check*: Verify alarm tokens activate for harmful content in both modalities during inference.

**Dynamic Policy Optimization**: Safety-Guided GRPO uses adaptive rewards based on real-time model performance. *Why needed*: Static rewards cannot capture complex safety reasoning patterns in videos. *Quick check*: Monitor reward distribution changes during training to ensure proper adaptation.

**Cross-Cultural Safety**: VSB-77k spans 10 language communities to capture diverse safety norms. *Why needed*: Safety perceptions vary significantly across cultures and languages. *Quick check*: Verify benchmark coverage across all specified language communities.

## Architecture Onboarding

**Component Map**: Video Input -> Alarm Token Injection -> AT-SFT Fine-tuning -> Safety-Guided GRPO -> Enhanced Video LLM

**Critical Path**: Alarm token generation → Multi-task loss computation → Dynamic reward calculation → Policy update sequence

**Design Tradeoffs**: 
- Safety vs usability: 13.4% FRR increase indicates over-refusal of safe content
- Performance vs generalization: Framework optimized for VideoLLaMA3-2B may not transfer directly
- Complexity vs effectiveness: Dual-stage approach adds training complexity but achieves significant gains

**Failure Signatures**: 
- High FRR indicates over-conservative safety filtering
- Video understanding degradation suggests safety training interferes with core capabilities
- Reward collapse in GRPO indicates improper reward scaling or adaptation

**Three First Experiments**:
1. Evaluate baseline VideoLLaMA3-2B on VSB-Eval to establish degradation baseline
2. Test alarm token activation patterns on mixed-harm/non-harm video pairs
3. Run single-stage AT-SFT without GRPO to isolate contribution of each component

## Open Questions the Paper Calls Out

**Open Question 1**: How can Video LLM safety alignment be improved to significantly reduce False Refusal Rate (FRR) for benign queries without compromising defense success? The current framework achieves 65.1% safety improvement but increases FRR to 13.4%, requiring mitigation strategies.

**Open Question 2**: What methodologies are required to generate large-scale safety reasoning data that ensures response naturalness and flexibility comparable to human answers? Current VSB-R1-46k relies on templates that limit output diversity and naturalness.

**Open Question 3**: How robust are current Video LLM safety mechanisms against complex harmful semantic attacks and model-specific adversarial techniques? The study focuses on common scenarios, leaving sophisticated attack resistance untested.

## Limitations

- Critical implementation details including exact hyperparameters (λ1, λ2, γ1, γ2, DRA bounds) and group sizes are omitted, hindering reproducibility
- 13.4% False Refusal Rate represents substantial over-refusal of safe content, impacting real-world usability
- Framework effectiveness limited to VideoLLaMA3-2B without cross-model validation or long-term stability studies
- Complete code and dataset unavailability prevents independent verification of reported results

## Confidence

**High Confidence**: The identification of 42.3% average safety degradation from video modality integration is well-supported by direct comparisons and ablation studies across multiple models.

**Medium Confidence**: VideoSafety-R1 demonstrates effectiveness on VSB-77k benchmark, but generalizability to other Video LLM architectures and real-world performance remain uncertain without cross-model validation.

**Low Confidence**: Complete reproducibility is compromised by missing code, datasets, and critical hyperparameters, with external dependency on Qwen-Long API for evaluation introducing potential variability.

## Next Checks

1. **Dataset Accessibility Validation**: Obtain and verify VSB-77k contains 77,646 video-query pairs across 19 risk categories and 10 languages with correct evaluation/training splits.

2. **Hyperparameter Sensitivity Analysis**: Systematically test impact of missing hyperparameters (λ1, λ2, γ1, γ2, DRA bounds, group size) on DSR and FRR performance to identify critical ranges.

3. **Cross-Model Generalization Test**: Apply VideoSafety-R1 to at least two additional Video LLM architectures and evaluate DSR improvements on VSB-Eval-HH to assess framework robustness.