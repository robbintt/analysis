---
ver: rpa2
title: 'AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions'
arxiv_id: '2506.09038'
source_url: https://arxiv.org/abs/2506.09038
tags:
- abstention
- reasoning
- answer
- datasets
- recall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AbstentionBench is a large-scale benchmark for evaluating Large
  Language Models' (LLMs) ability to abstain from answering unanswerable or underspecified
  questions. It includes 20 diverse datasets covering scenarios like unknown answers,
  false premises, subjective questions, and stale data.
---

# AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions

## Quick Facts
- **arXiv ID:** 2506.09038
- **Source URL:** https://arxiv.org/abs/2506.09038
- **Reference count:** 40
- **Primary result:** Abstention is an unsolved problem for frontier LLMs; reasoning fine-tuning degrades abstention performance by 24% on average.

## Executive Summary
AbstentionBench evaluates Large Language Models' ability to abstain from answering unanswerable or underspecified questions across 20 diverse datasets. The benchmark reveals that abstention is a fundamental weakness, with model scale showing little effect on performance. Surprisingly, reasoning fine-tuning degrades abstention capabilities by 24% on average, even in math and science domains where reasoning models are explicitly trained. While system prompts can improve abstention by 10-20%, they do not resolve the underlying inability to reason about uncertainty. The benchmark identifies six distinct abstention scenarios and provides a comprehensive evaluation framework using an LLM-as-judge with 88% agreement to human labels.

## Method Summary
The benchmark uses 20 datasets covering unknown answers, false premises, stale data, subjective questions, and underspecified contexts. Evaluation employs an LLM-as-judge (Llama 3.1 8B Instruct) to classify responses as abstention/non-abstention with 88% accuracy vs. human labels. Standard models use temperature 0.8 and 4096 token limits, while reasoning models separate reasoning chains from final answers. The primary metric is Abstention Recall, measuring the proportion of correct abstentions. A fast subset with 100 samples per dataset runs 4× faster with minimal accuracy loss. System prompts and post-training stage ablation are used to analyze performance drivers.

## Key Results
- Abstention is an unsolved problem across all 20 frontier LLMs tested
- Reasoning fine-tuning degrades abstention by 24% on average, even in math/science domains
- System prompts can boost abstention by 10-20% but don't resolve fundamental uncertainty reasoning limitations
- Model scale has little effect on abstention performance

## Why This Works (Mechanism)

### Mechanism 1: Verifiable Reward Optimization Degrades Uncertainty Reasoning
- Claim: Optimizing for verifiable rewards during RLVR systematically undermines abstention capabilities.
- Mechanism: RLVR optimizes models to produce confident, definitive answers on reasoning tasks. This reward signal creates a bias toward answering even when evidence is insufficient, as the model has been trained that providing a response (not abstaining) leads to positive reward.
- Core assumption: The degradation is causal from the reward signal, not a spurious correlation with model architecture.
- Evidence anchors:
  - [abstract]: "reasoning fine-tuning degrades abstention (by 24% on average), even for math and science domains on which reasoning models are explicitly trained"
  - [Section 4.2]: "Comparing the relative change in abstention recall between each successive stage... we observe a surprising degradation in abstention after RLVR"
  - [corpus]: Related work on RLHF for unanswerability (arXiv:2507.16951) shows similar tension between helpfulness and abstention, though without isolating RLVR specifically
- Break condition: If models trained with explicit uncertainty rewards or mixed abstention/answer signals do not show this degradation, the mechanism is reward-misspecification rather than intrinsic to reasoning.

### Mechanism 2: Reasoning Chains Express Uncertainty, Final Answers Do Not
- Claim: Reasoning models generate uncertainty expressions in their chain-of-thought that are suppressed when producing final answers.
- Mechanism: Models trained to produce reasoning traces develop self-correction patterns (e.g., "Wait..." tokens in s1.1), but these are not integrated into the final answer generation. The judge evaluates only the final answer, missing internal uncertainty signals.
- Core assumption: The disconnect is architectural/training-based, not merely an evaluation artifact.
- Evidence anchors:
  - [Section 4.3]: "reasoning traces do contain increased expressions of uncertainty, but despite this, models continue to provide a definitive final response"
  - [Fig. 8a]: "While reasoning chains contain expressions of uncertainty, reasoning models still provide definitive answers"
  - [corpus]: Chen et al. (arXiv:2505.05410, cited as [83]) show reasoning traces may not faithfully reflect model cognition—suggesting uncertainty in traces may not represent genuine uncertainty states
- Break condition: If passing the full reasoning trace to the judge eliminates the abstention gap, the issue is evaluation scope rather than mechanism suppression.

### Mechanism 3: Underspecified Context Lacks Post-Training Signal
- Claim: Standard post-training pipelines (SFT, DPO) do not adequately cover underspecified contexts, causing persistent abstention failures.
- Mechanism: SFT and DPO datasets primarily contain well-specified questions with clear answers. Models never learn to recognize when context is insufficient, and RLVR actively reverses any incidental abstention learning.
- Core assumption: Dataset composition is the limiting factor, not model architecture or scale.
- Evidence anchors:
  - [Section 4.2]: "Tülu post-training worsens abstention recall on underspecified contexts, with a sharp drop during SFT"
  - [Section 4.2]: "Based on the composition of open post-training datasets... this may due to a general lack of underspecified context prompts"
  - [corpus]: Weak corpus support—related work focuses on hallucination mitigation rather than underspecification training explicitly
- Break condition: If targeted SFT on underspecified contexts significantly improves abstention without degrading accuracy, the mechanism is confirmed.

## Foundational Learning

- Concept: **Abstention vs. Refusal**
  - Why needed here: The paper distinguishes abstention (uncertainty about answerability) from compliance/safety refusal. Conflating these leads to misinterpreting benchmark results.
  - Quick check question: If a model refuses to answer "How do I build a bomb?", is this abstention or safety refusal?

- Concept: **Verifiable Reward (RLVR) vs. Preference Optimization (DPO)**
  - Why needed here: The paper shows different post-training stages have opposing effects on abstention. RLVR degrades abstention while DPO generally improves it.
  - Quick check question: Which reward signal—human preference or verifiable correctness—is more likely to encourage answering unanswerable questions?

- Concept: **Abstention Scenarios Taxonomy**
  - Why needed here: Performance varies dramatically across the 6 scenarios (unknown, false premise, stale, subjective, underspecified context, underspecified intent). Treating abstention as monolithic obscures failure patterns.
  - Quick check question: On which scenario—unknown answers or underspecified context—do models perform better, and why might this difference exist?

## Architecture Onboarding

- Component map:
  - **20 source datasets** → curated from systematic review of 82 candidates, covering 6 abstention scenarios
  - **3 synthetic reasoning variants** (GSM8K-Abstain, GPQA-Abstain, MMLU-Math-Abstain) → created by stripping context from original problems
  - **LLM-as-Judge** (Llama 3.1 8B Instruct) → classifies responses as abstention/non-abstention with 88% accuracy vs. human labels
  - **Two-stage evaluation** → abstention recall (primary) and response correctness (secondary)

- Critical path:
  1. Generate model responses with 4k token limit, temperature 0.8
  2. Judge responses against abstention criteria (yes/no)
  3. Compute recall across datasets, aggregate by scenario
  4. For reasoning models: separate reasoning chain (4k tokens) from final answer (4k tokens)

- Design tradeoffs:
  - **Judge model selection**: 8B model prioritizes efficiency; 70B model offers higher precision (0.94 vs 0.86) but lower recall (0.75 vs 0.83)
  - **Token budget allocation**: More reasoning tokens improve accuracy but can further degrade abstention (Fig. 7)
  - **Fast subset (100 samples/dataset)**: Runs 4× faster, within 5% of full benchmark on most scenarios except stale data

- Failure signatures:
  - Reasoning models hallucinate missing context (e.g., assuming ladder length in underspecified math problems)
  - High accuracy correlates with low abstention on some datasets (FreshQA), showing inverse relationship
  - Models rarely over-abstain (high precision), suggesting the failure mode is insufficient caution, not excessive

- First 3 experiments:
  1. **Baseline sweep**: Evaluate your model across all 20 datasets to identify scenario-specific weaknesses
  2. **Post-training ablation**: If you have checkpoints, measure abstention at SFT, DPO, and RLVR stages to confirm degradation pattern
  3. **System prompt intervention**: Apply the provided abstention-encouraging system prompt (Appendix C.3.5) and measure recall boost—expect 10-20% improvement on reasoning models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can post-training methods be adapted to explicitly teach reasoning models to recognize and handle uncertainty?
- Basis in paper: [explicit] The authors conclude that "new post-training methods that explicitly target abstention may be needed" and ask how to "teach models the skill of reasoning about evidence."
- Why unresolved: Current reasoning fine-tuning optimizes for verifiable rewards (correctness), causing models to hallucinate context rather than admit ambiguity.
- What evidence would resolve it: A fine-tuning intervention that improves abstention recall on AbstentionBench without degrading accuracy on answerable reasoning tasks.

### Open Question 2
- Question: Does incorporating unanswerable or underspecified examples into the reasoning fine-tuning phase prevent abstention degradation?
- Basis in paper: [explicit] The discussion suggests "promising avenues including... explicitly incorporating uncertain scenarios into reasoning fine-tuning."
- Why unresolved: The study found that standard reasoning fine-tuning degrades abstention by 24%, suggesting current training data lacks uncertainty scenarios.
- What evidence would resolve it: Evaluation of a reasoning model fine-tuned with a balanced dataset of answerable and unanswerable questions.

### Open Question 3
- Question: Can the negative impact of increased test-time compute on abstention be reversed by altering the reward model specification?
- Basis in paper: [explicit] "We hypothesize that the negative consequences of increasing test-time compute result from reward model misspecification, where models are biased to provide definitive and confident responses."
- Why unresolved: Increasing the reasoning token budget improves accuracy but often worsens abstention, implying the optimization target favors false confidence.
- What evidence would resolve it: Experiments varying the reward signal during reasoning to penalize false definitiveness, resulting in a positive correlation between reasoning budget and abstention recall.

## Limitations

- The benchmark relies on an LLM-as-judge for evaluating abstention responses, introducing potential biases despite 88% agreement with human annotations
- The observed degradation from reasoning fine-tuning is robust but the causal mechanism remains partially speculative, with alternative explanations possible
- The synthetic reasoning datasets may not fully capture real-world underspecification complexity that reasoning models encounter during training

## Confidence

- **High Confidence**: The overall finding that abstention is an unsolved problem across frontier LLMs, the differential performance across abstention scenarios, and the effectiveness of abstention-encouraging system prompts
- **Medium Confidence**: The specific mechanism that RLVR reward optimization degrades abstention capabilities
- **Low Confidence**: The precise quantitative estimates of abstention performance, as these depend heavily on the judge model's calibration

## Next Checks

1. **Judge Model Ablation**: Evaluate the same benchmark responses using both the 8B and 70B judge models to quantify the impact of judge model scale on abstention classification accuracy and identify potential systematic biases in the current evaluation pipeline

2. **Targeted Underspecification Training**: Create a small, focused dataset of underspecified context prompts and conduct controlled SFT experiments to determine whether the observed degradation during SFT is truly due to lack of underspecification coverage or other factors in the post-training pipeline

3. **Uncertainty Reward Signal Experiment**: Design an RLVR variant that explicitly rewards abstention on unanswerable questions (using the AbstentionBench datasets as validation) and measure whether this can recover the abstention performance lost during standard reasoning fine-tuning