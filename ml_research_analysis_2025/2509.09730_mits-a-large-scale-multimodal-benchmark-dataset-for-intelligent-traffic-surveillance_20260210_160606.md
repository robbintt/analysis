---
ver: rpa2
title: 'MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance'
arxiv_id: '2509.09730'
source_url: https://arxiv.org/abs/2509.09730
tags:
- dataset
- image
- multimodal
- traffic
- mits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MITS introduces the first large-scale multimodal benchmark dataset
  for Intelligent Traffic Surveillance, comprising 170,400 real-world ITS images with
  24 subcategories of objects and events. A systematic data generation pipeline produces
  5 million high-quality instruction-following visual question-answer pairs across
  five critical ITS tasks.
---

# MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance

## Quick Facts
- **arXiv ID:** 2509.09730
- **Source URL:** https://arxiv.org/abs/2509.09730
- **Reference count:** 40
- **Primary result:** 170,400 ITS images with 5M VQA pairs; fine-tuning LMMs yields 27-83% performance gains across five tasks

## Executive Summary
MITS introduces the first large-scale multimodal benchmark dataset for Intelligent Traffic Surveillance (ITS), featuring 170,400 real-world images with 24 object/event subcategories. The dataset includes 5 million high-quality instruction-following visual question-answer pairs generated through a systematic pipeline across five critical ITS tasks: object/event recognition, counting, localization, background analysis, and reasoning. Fine-tuning mainstream LMMs on MITS demonstrates significant performance improvements, with LLaVA-1.5 improving by 83.2% and Qwen2-VL by 58.6%. The dataset, code, and models are released open-source, providing valuable resources for both ITS and LMM research communities.

## Method Summary
The MITS dataset addresses the lack of large-scale multimodal data for ITS by creating a comprehensive benchmark covering five critical tasks. The systematic data generation pipeline produces high-quality instruction-following VQA pairs from 170,400 real-world ITS images. The training methodology employs LoRA-based fine-tuning of mainstream LMMs (LLaVA and Qwen series) using the MS-Swift Toolkit framework, with frozen vision encoders and optimized hyperparameters for each model. Evaluation combines multiple metrics including IoU for localization, relative absolute error for counting, accuracy for recognition, and DeepSeek-R1 semantic alignment for reasoning tasks.

## Key Results
- LLaVA-1.5 fine-tuned on MITS achieves 83.2% performance improvement
- Qwen2-VL demonstrates 58.6% improvement on ITS tasks
- Dataset provides first comprehensive benchmark for multimodal ITS applications
- Systematic pipeline generates 5M high-quality VQA pairs across five task categories

## Why This Works (Mechanism)
The MITS dataset works by addressing the critical gap in multimodal ITS data through systematic collection and annotation of real-world traffic surveillance images. The instruction-following VQA pairs enable models to understand complex traffic scenarios requiring reasoning beyond simple recognition. The comprehensive coverage across five task categories ensures models learn diverse ITS capabilities, from basic object detection to complex reasoning about traffic events and backgrounds.

## Foundational Learning
- **Large Multimodal Models (LMMs):** AI models processing both text and images for complex reasoning tasks
  - *Why needed:* Enable holistic understanding of traffic scenes beyond single-modality approaches
  - *Quick check:* Verify model accepts both image and text inputs simultaneously

- **LoRA Fine-tuning:** Parameter-efficient adaptation technique freezing base model weights
  - *Why needed:* Enables efficient adaptation of large models without full retraining
  - *Quick check:* Confirm training only updates LoRA adapter weights

- **Instruction-Following VQA Pairs:** Question-answer pairs designed for complex reasoning
  - *Why needed:* Trains models to handle real-world ITS queries requiring multi-step reasoning
  - *Quick check:* Validate VQA pairs cover all five ITS task categories

- **Bounding Box Coordinate Systems:** Absolute vs normalized coordinate representations
  - *Why needed:* Critical for accurate object localization and evaluation
  - *Quick check:* Verify coordinate conversion between training and evaluation formats

- **Semantic Alignment Scoring:** Using language models to evaluate reasoning quality
  - *Why needed:* Provides nuanced assessment of model understanding beyond accuracy metrics
  - *Quick check:* Confirm DeepSeek-R1 prompt and scoring methodology are correctly implemented

## Architecture Onboarding

**Component Map:** Images → Vision Encoder → Projector → LLM → Output (VQA)  
**Critical Path:** Data preprocessing → LoRA training → Evaluation (IoU, RAE, Accuracy, Semantic alignment)  
**Design Tradeoffs:** Large dataset size (5M pairs) vs. training efficiency through LoRA; comprehensive task coverage vs. evaluation complexity  
**Failure Signatures:** 
- Localization failure: IoU scores near zero (coordinate system mismatch)
- Counting failure: High RAE values (training instability or poor data quality)
- Reasoning failure: Low semantic alignment scores (insufficient reasoning data)

**First Experiments:**
1. Train Qwen2-VL-7B with minimal dataset subset (10K samples) to verify end-to-end pipeline
2. Test coordinate conversion for bounding boxes (absolute → normalized) to prevent localization failures
3. Run DeepSeek-R1 scoring on sample outputs to validate semantic alignment evaluation

## Open Questions the Paper Calls Out
None

## Limitations
- Incomplete LoRA hyperparameter specification (rank, alpha, target modules) limits exact reproduction
- DeepSeek-R1 semantic alignment scoring methodology lacks implementation transparency
- Dataset and code repository URLs not provided in paper text despite open-source claims

## Confidence
- **High confidence:** Dataset scale and diversity (170,400 images, 5M pairs) with systematic generation pipeline
- **Medium confidence:** Substantial performance improvements (83.2% for LLaVA-1.5) but opaque evaluation details
- **Low confidence:** Complete reproducibility hindered by missing critical implementation details and data access information

## Next Checks
1. Verify bounding box coordinate handling: Confirm Qwen2-VL training uses absolute coordinates while evaluation uses normalized coordinates
2. Validate DeepSeek-R1 scoring implementation: Obtain and test exact prompt and API configuration for reasoning task evaluation
3. Test LoRA training stability: Run initial experiments with multiple LoRA configurations to identify optimal parameters matching reported performance gains