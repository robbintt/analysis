---
ver: rpa2
title: Qwen3 Technical Report
arxiv_id: '2505.09388'
source_url: https://arxiv.org/abs/2505.09388
tags:
- thinking
- qwen2
- mode
- qwen3
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Qwen3 is a series of open-weight large language models (LLMs) with
  dense and Mixture-of-Expert (MoE) architectures ranging from 0.6 to 235 billion
  parameters. The flagship model, Qwen3-235B-A22B, integrates thinking and non-thinking
  modes in a unified framework with a thinking budget mechanism for adaptive resource
  allocation.
---

# Qwen3 Technical Report

## Quick Facts
- arXiv ID: 2505.09388
- Source URL: https://arxiv.org/abs/2505.09388
- Reference count: 9
- Key outcome: Qwen3-235B-A22B achieves 85.7 on AIME'24 and 81.5 on AIME'25 in thinking mode, and 40.1 on AIME'24 in non-thinking mode, surpassing comparable open-source models.

## Executive Summary
Qwen3 is a series of open-weight large language models ranging from 0.6 to 235 billion parameters, featuring both dense and Mixture-of-Expert (MoE) architectures. The flagship model integrates thinking and non-thinking modes in a unified framework with a thinking budget mechanism for adaptive resource allocation. Trained on 36 trillion tokens across 119 languages, Qwen3 achieves state-of-the-art performance on reasoning benchmarks while enabling efficient small model distillation from larger teacher models.

## Method Summary
Qwen3 uses a three-stage pre-training process (30T general tokens, 5T reasoning-heavy tokens, long-context extension) followed by four-stage post-training including reasoning RL (GRPO), thinking mode fusion, and general RL. The unified framework supports both thinking mode for complex reasoning and non-thinking mode for rapid responses, with a thinking budget mechanism that emerges naturally during training. Small models are trained via strong-to-weak distillation using a two-phase approach (off-policy SFT + on-policy KL distillation) that achieves 10x efficiency gains over RL training.

## Key Results
- Qwen3-235B-A22B achieves 85.7 on AIME'24 and 81.5 on AIME'25 in thinking mode
- Same model scores 40.1 on AIME'24 in non-thinking mode
- Outperforms DeepSeek-V3 and competes with GPT-4o and o1 on key benchmarks
- Small models (Qwen3-8B) achieve 74.4 AIME'24 score via distillation vs 67.6 with RL

## Why This Works (Mechanism)

### Mechanism 1: Unified Thinking/Non-Thinking Mode Framework
The model integrates reasoning and rapid-response modes using `/think` and `/no_think` flags in chat templates. During Thinking Mode Fusion, the model learns to switch modes based on user input while maintaining format consistency. This enables a single checkpoint to handle both fast inference and deliberate reasoning without deployment complexity.

### Mechanism 2: Thinking Budget for Inference-Time Scaling
A user-defined token threshold triggers a stop-thinking instruction when reached, allowing the model to generate coherent outputs from incomplete reasoning traces. This emergent capability enables predictable trade-offs between latency and reasoning quality, with smooth performance scaling curves as budget increases.

### Mechanism 3: Strong-to-Weak Distillation for Efficient Small Models
Two-phase distillation (off-policy SFT + on-policy KL divergence) transfers knowledge from large teacher models to small students. This approach achieves 74.4 AIME'24 score on Qwen3-8B versus 67.6 for RL training, using 1,800 GPU hours versus 17,920.

## Foundational Learning

- **Mixture-of-Experts (MoE) Routing**: Qwen3 MoE models use 128 experts with 8 activated per token; critical for understanding load balancing and expert specialization
  - Quick check: Can you explain why Qwen3 removed shared experts compared to Qwen2.5-MoE?

- **Group Relative Policy Optimization (GRPO)**: Used in reasoning RL stage for policy updates; understanding on-policy vs off-policy sampling affects batch size tuning
  - Quick check: How does GRPO differ from PPO in handling multiple rollouts per query?

- **Chain-of-Thought Cold-Start Training**: Stage 1 post-training establishes foundational reasoning patterns; over-training here can limit RL-phase improvement potential
  - Quick check: Why does the paper recommend minimizing training samples during cold-start?

## Architecture Onboarding

- **Component map**: Dense models (0.6B, 1.7B, 4B, 8B, 14B, 32B) → MoE models (30B-A3B, 235B-A22B) → All use GQA, SwiGLU, RoPE, RMSNorm → MoE adds fine-grained expert segmentation + global-batch load balancing loss
- **Critical path**: Pre-training (S1→S2→S3) → Post-training (Long-CoT cold start → Reasoning RL → Thinking Mode Fusion → General RL) → For small models: Skip stages 1-4; apply off-policy + on-policy distillation from Qwen3-32B or Qwen3-235B-A22B
- **Design tradeoffs**: Thinking Mode Fusion + General RL improve instruction-following but slightly degrade specialized math/coding performance (AIME'24 drops from 83.8 to 81.4 for Qwen3-32B thinking mode); MoE without shared experts improves specialization but increases routing complexity
- **Failure signatures**: Mode-switching failures (ThinkFollow score <95%), budget control incoherence (contradictory conclusions), distillation collapse (student pass@64 decreases)
- **First 3 experiments**: 
  1. Budget scaling validation: Test Qwen3-235B-A22B on AIME'24 with budgets [1K, 4K, 8K, 16K, 32K] tokens
  2. Mode-switching robustness: Run ThinkFollow benchmark on Qwen3-32B with alternating `/think` and `/no_think` flags
  3. Distillation vs RL comparison: Train Qwen3-8B with on-policy distillation from Qwen3-32B vs GRPO on same math/code queries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does extending the output length for thinking tokens beyond 32,768 tokens yield further performance improvements on reasoning benchmarks?
- Basis in paper: Section 4.7 states performance is expected to improve if output length is further extended beyond 32K
- Why unresolved: Current evaluations and thinking budgets were capped at standard context limits
- What evidence would resolve it: Controlled experiments measuring benchmark performance with maximum output lengths scaled to 64K and 128K tokens

### Open Question 2
- Question: How can the model's long-context retrieval capability be preserved when operating in thinking mode?
- Basis in paper: Appendix A.1.1 notes thinking mode slightly degrades performance on RULER benchmark
- Why unresolved: Generated thinking content interferes with the retrieval process, but no solution is currently implemented
- What evidence would resolve it: Future model iteration where thinking mode matches non-thinking mode's performance on long-context retrieval benchmarks

### Open Question 3
- Question: Can the performance degradation in specialized reasoning tasks during general alignment stages be mitigated?
- Basis in paper: Section 4.7 and Table 22 show performance decreased on challenging tasks after Thinking Mode Fusion and General RL
- Why unresolved: Unclear if training data mixture or learning objective in later stages inevitably dilutes specialized reasoning skills
- What evidence would resolve it: Training ablation demonstrating retention of peak reasoning scores while integrating general instruction-following capabilities

## Limitations
- Thinking budget mechanism's emergent behavior lacks theoretical guarantees of coherence
- Strong-to-weak distillation claims may be architecture-specific without broader validation
- Mode-switching framework's robustness in noisy, real-world deployment scenarios is not thoroughly validated

## Confidence
- **High Confidence**: Qwen3-235B-A22B's benchmark performance, three-stage pre-training pipeline, basic MoE architecture implementation
- **Medium Confidence**: Unified thinking/non-thinking framework's practical utility, thinking budget mechanism's emergent capabilities, distillation efficiency claims for small models
- **Low Confidence**: Long-term stability of mode-switching behavior across diverse deployment scenarios, generalization of thinking budget mechanism to non-mathematical reasoning tasks, scalability of distillation approach to other model families

## Next Checks
1. **Budget Mechanism Coherence Test**: Deploy Qwen3-235B-A22B on mixed-task benchmark with variable thinking budgets (100 to 16K tokens) across reasoning, code generation, and multi-step planning tasks; verify partial reasoning traces produce coherent final outputs without logical contradictions
2. **Mode-Switching Robustness in Noisy Inputs**: Evaluate mode-switching accuracy on ThinkFollow benchmark with adversarial conditions (noisy templates, mixed languages, inconsistent flag usage); measure degradation from 98.9% accuracy
3. **Distillation Transferability Study**: Apply exact two-phase distillation pipeline to non-Qwen architecture (Llama/Mistral base model); compare performance against direct RL training and original Qwen3-8B results to determine generalizability of efficiency gains