---
ver: rpa2
title: 'EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods on
  NLP Models'
arxiv_id: '2505.01238'
source_url: https://arxiv.org/abs/2505.01238
tags:
- explanations
- methods
- explainability
- evaluation
- evalxnlp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "EvalxNLP addresses the lack of standardized evaluation frameworks\
  \ for explainability methods in NLP, where transformer-based models operate as black\
  \ boxes. The framework benchmarks eight post-hoc feature attribution methods\u2014\
  including gradient-based (Saliency, Gradient\xD7Input, Integrated Gradients, DeepLIFT,\
  \ Guided BackProp) and perturbation-based (LIME, SHAP, SHAP-I)\u2014across three\
  \ key properties: faithfulness, plausibility, and complexity."
---

# EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods on NLP Models

## Quick Facts
- arXiv ID: 2505.01238
- Source URL: https://arxiv.org/abs/2505.01238
- Authors: Mahdi Dhaini; Kafaite Zahra Hussain; Efstratios Zaradoukas; Gjergji Kasneci
- Reference count: 36
- Key outcome: Framework benchmarks 8 post-hoc feature attribution methods across faithfulness, plausibility, and complexity metrics on rationale-annotated datasets

## Executive Summary
EvalxNLP addresses the lack of standardized evaluation frameworks for explainability methods in NLP, where transformer-based models operate as black boxes. The framework benchmarks eight post-hoc feature attribution methods—including gradient-based (Saliency, Gradient×Input, Integrated Gradients, DeepLIFT, Guided BackProp) and perturbation-based (LIME, SHAP, SHAP-I)—across three key properties: faithfulness, plausibility, and complexity. It integrates LLM-generated textual explanations to improve interpretability and supports three rationale-annotated datasets (MovieReviews, HateXplain, e-SNLI) for sentiment analysis, hate speech detection, and NLI tasks. Human evaluation (N=20) showed user satisfaction with framework usability, with higher scores from less experienced participants, indicating room for improvement for advanced users.

## Method Summary
The framework implements eight post-hoc explainability methods using Captum for gradient-based methods, ferret, LIME original, Partition SHAP, and shapiq for SHAP-I. It evaluates these methods on three rationale-annotated datasets: MovieReviews (sentiment analysis), HateXplain (hate speech detection), and e-SNLI (natural language inference). The evaluation uses soft sufficiency/comprehensiveness for faithfulness, IOU-F1 and Token-F1 for plausibility, and complexity metrics including Sparseness. An LLM verbalization module translates numerical attributions into natural language explanations via the Together AI API with Llama-3.3-70B-Instruct-Turbo. The codebase is publicly available at github.com/dmah10/EvalxNLP.

## Key Results
- DeepLIFT achieved the highest faithfulness scores on the MovieReviews dataset
- Integrated Gradients showed the best complexity metrics while maintaining reasonable faithfulness
- SHAP demonstrated the highest plausibility scores, confirming no single method excels across all properties
- Human evaluation (N=20) indicated higher usability satisfaction among less experienced NLP participants
- Soft perturbation metrics proved more accurate than hard token removal for faithfulness evaluation

## Why This Works (Mechanism)

### Mechanism 1
Soft-perturbation metrics provide more accurate faithfulness measurement than hard token removal. Instead of completely masking important tokens (which disrupts grammatical structure), the framework applies soft masking like Bernoulli masks. This measures model confidence decay while preserving linguistic context, resulting in smoother, more reliable assessment of feature importance. Core assumption: hard removal creates unrealistic inputs that damage performance for reasons other than missing feature importance.

### Mechanism 2
Constrained LLM verbalization improves user interpretability without hallucination. The framework integrates an LLM via API to translate numerical scores into natural language summaries. Crucially, it restricts the LLM from generating its own reasoning ("why the model decided X") and forces it to describe only importance scores ("which words were important"). Core assumption: users struggle to interpret raw heatmaps but can understand textual summaries of scores.

### Mechanism 3
Multi-dimensional benchmarking prevents the "single best method" fallacy. By forcing simultaneous evaluation across Faithfulness, Plausibility, and Complexity, the framework exposes inherent trade-offs in explainability methods (gradient methods are fast but noisy; perturbation methods are plausible but slow). This prevents practitioners from selecting methods based on single favorable metrics. Core assumption: no single post-hoc method optimizes all desirable properties simultaneously.

## Foundational Learning

**Feature Attribution (Gradient vs. Perturbation)**
- Why needed here: The framework benchmarks 8 distinct explainers split into these two camps. Gradient methods analyze internal model weights (fast, model-aware), while perturbation methods test input-output behavior (slow, model-agnostic).
- Quick check question: Can you explain why Integrated Gradients requires a "baseline" input, whereas LIME requires sampling near the specific input instance?

**Plausibility vs. Faithfulness**
- Why needed here: These are the two primary axes of evaluation. Faithfulness asks "Is this explanation true to the model's mechanics?" while Plausibility asks "Does this explanation match human intuition?"
- Quick check question: If a model relies on a spurious correlation (e.g., the word "Amazon" implies delivery logistics), would a faithful explanation highlight that word? Would a plausible explanation highlight it?

**Rationale-annotated Datasets**
- Why needed here: To calculate Plausibility metrics (IOU-F1, Token-F1), the framework requires datasets where humans have highlighted the "ground truth" important words (rationales).
- Quick check question: Why would a high Token-F1 score on the MovieReviews dataset not necessarily imply the explainer is useful for debugging a model error?

## Architecture Onboarding

**Component map:**
Core Engine -> Captum/ferret/LIME/SHAP libraries -> Model Interface (HuggingFace Transformers) -> Evaluation Module (Soft Suff/Comp, IOU-F1) -> LLM API client (Together AI)

**Critical path:**
1. Select Dataset (e.g., HateXplain) and load Transformer model
2. Run Explainer (e.g., SHAP-I) to generate feature attributions for target instances
3. Apply Metrics (e.g., Soft Sufficiency) to score the explainer's performance against the model
4. (Optional) Call LLM API to verbalize the top-k important tokens

**Design tradeoffs:**
- Speed vs. Completeness: Perturbation methods (SHAP, LIME) are significantly slower than gradient methods but often yield higher plausibility scores
- Standard vs. Soft Metrics: The framework defaults to "soft" perturbation for faithfulness. While more accurate, this is computationally denser than simple hard masking

**Failure signatures:**
- Context Window Errors: Perturbation-based explainers may fail on very long sequences if context exceeds model's max length
- LLM Hallucination: If prompt template is modified to be open-ended, verbalizer may invent reasons not present in attribution data
- Gradient Saturation: Gradient-based methods may return zero attribution for saturated neurons, potentially misleading verbalizer

**First 3 experiments:**
1. Baseline Consistency Check: Run all 8 explainers on a single misclassified instance from MovieReviews to verify if methods highlight contradictory tokens
2. Metric Correlation Analysis: Run benchmark on full e-SNLI test set to calculate correlation matrix between Faithfulness (Soft Suff) and Plausibility (IOU-F1)
3. Verbalizer Stress Test: Intentionally feed LLM verbalizer adversarial example (random noise attributions) to test resistance to describing noise as "meaningful"

## Open Questions the Paper Calls Out

**Open Question 1**
Does the LLM-based textual explanation module significantly improve user comprehension compared to visual-only outputs? The reported human evaluation was conducted prior to LLM module integration, leaving its specific impact unverified. Comparative user study results measuring task performance with LLM module enabled versus disabled would resolve this.

**Open Question 2**
What specific design modifications are required to better serve expert NLP users? Section 5 notes that participants with greater NLP experience provided lower ratings than novices, indicating room for improvement. A follow-up user study showing increased satisfaction scores from experienced users after implementing advanced customization features would resolve this.

**Open Question 3**
Can the framework effectively extend support to non-feature attribution techniques and robustness metrics? Section 6 lists future directions including integrating recent non-feature attribution techniques and robustness metrics. Successful implementation and consistent benchmarking of methods like those cited [34] within EvalxNLP would resolve this.

## Limitations
- Evaluation relies on three datasets with varying rationale quality, introducing uncertainty about whether high plausibility scores reflect useful explanations or annotation artifacts
- LLM verbalization introduces additional abstraction layer between model reasoning and user interpretation, potentially confounding evaluation results
- Human evaluation with only 20 participants provides preliminary insights but lacks statistical power for robust conclusions about user preferences

## Confidence

**High confidence:** Technical implementation and metric calculations follow established methods from Captum, ferret, and related libraries. Code architecture appears sound and evaluation pipeline is reproducible.

**Medium confidence:** Comparative results between explainability methods depend on specific model architectures, hyperparameters, and datasets used. Method rankings may shift with different experimental conditions.

**Low confidence:** Generalizability of human evaluation results due to small sample size and potential selection bias. Observation that less experienced users found framework more intuitive requires deeper investigation with larger, more diverse user groups.

## Next Checks
1. Conduct replication study using BERT instead of XLM-RoBERTa on MovieReviews dataset to verify if method rankings remain consistent across model families
2. Perform ablation testing on LLM verbalization component by comparing user satisfaction scores with and without verbalized explanations on same test instances
3. Test soft perturbation metrics against alternative faithfulness measures (e.g., model confidence decay under input noise) on held-out dataset to validate soft suff/comp consistently outperform traditional hard removal metrics