---
ver: rpa2
title: Nearly Tight Bounds for Cross-Learning Contextual Bandits with Graphical Feedback
arxiv_id: '2502.04678'
source_url: https://arxiv.org/abs/2502.04678
tags:
- algorithm
- regret
- feedback
- probability
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the cross-learning contextual bandit problem
  with graphical feedback, where pulling an arm reveals losses for neighboring arms
  across all contexts. The key theoretical question is whether an algorithm with $\tilde{O}(\sqrt{\alpha
  T})$ regret exists, where $\alpha$ is the independence number of the feedback graph.
---

# Nearly Tight Bounds for Cross-Learning Contextual Bandits with Graphical Feedback

## Quick Facts
- arXiv ID: 2502.04678
- Source URL: https://arxiv.org/abs/2502.04678
- Reference count: 40
- Primary result: Achieves Õ(√αT) regret bound for cross-learning contextual bandits with graphical feedback, where α is the independence number

## Executive Summary
This paper addresses the cross-learning contextual bandit problem with graphical feedback, where pulling an arm reveals losses for neighboring arms across all contexts. The key theoretical question is whether an algorithm with Õ(√αT) regret exists, where α is the independence number of the feedback graph. The authors provide an affirmative answer by presenting an algorithm that achieves this minimax Õ(√αT) regret bound for both stochastic and adversarial contexts. The core technical idea involves decomposing the regret and leveraging the stochastic nature of contexts while applying adversarial bandit techniques.

## Method Summary
The paper presents two algorithms: one for known context distributions and one for unknown distributions. For known distributions, the algorithm uses importance-weighted estimators and Follow-The-Regularized-Leader (FTRL) with negative entropy regularization. When the distribution is unknown, it employs epoch-based sampling with rejection sampling procedures and concentration inequalities. The analysis handles both stochastic and adversarial losses, with the stochastic case following as a corollary. The key insight is that since contexts are drawn i.i.d. from a fixed distribution, the importance-weighted estimator can be used to construct an unbiased estimate of the loss.

## Key Results
- Achieves Õ(√αT) regret bound for cross-learning contextual bandits with graphical feedback
- Bound is independent of the number of contexts M
- Results hold for both stochastic and adversarial contexts
- Algorithm works with self-loops required on the feedback graph

## Why This Works (Mechanism)

### Mechanism 1: Importance-Weighted Loss Estimation with Cross-Learning
The algorithm constructs unbiased loss estimates that leverage the cross-learning structure to achieve context-independent regret bounds. When arm at is pulled, the learner observes losses ℓt,c(a) for all a ∈ Nout(at) and all contexts c. The importance weight wt(a) = Eν[pt,c(Nin(a))] represents the probability of observing arm a's loss at round t. The estimator ℓ̃t,c(a) = ℓt,c(a)/wt(a) · 1(at → a) is unbiased because contexts are drawn i.i.d. from fixed distribution ν, making wt(a) context-independent via the cross-learning structure.

### Mechanism 2: Regret Decomposition via Stochastic Contexts
Regret decomposes into per-context components, allowing independent control of each component to achieve Õ(√αT) overall. Reg(π) = Σc Pr(c)(Σt⟨pt,c - πc, ℓt,c⟩). The algorithm ensures Σt⟨pt,c - πc, ℓt,c⟩ ≤ Õ(√αT) for each fixed context c using adversarial bandit techniques, then combines via the decomposition.

### Mechanism 3: Epoch-Based Frequency Estimation with Conditional Independence
When context distribution is unknown, dividing time into epochs with paired timesteps enables concentration despite complex dependencies. Time divided into epochs of length L = Θ̃(√αT). Observation probability we(a) is fixed within each epoch using snapshots from two epochs prior. Timesteps are paired: one for loss estimation (Tℓe), one for frequency estimation (Tfe), ensuring ŵe and ℓ̃t are conditionally independent given the epoch-2 snapshot.

## Foundational Learning

- Concept: **Feedback Graphs and Independence Number**
  - Why needed here: Regret bound Õ(√αT) depends on α, the independence number. Understanding Nin(a) and Nout(a) is essential for implementing importance weights.
  - Quick check question: For a directed graph with self-loops on K nodes, can you compute α and explain why Σa p(a)/w(a) ≤ 4α ln(4K/αε)?

- Concept: **FTRL with Negative Entropy Regularization**
  - Why needed here: Core subroutine generates distributions pt,c = argminp ⟨p, Σs<t ℓ̂s,c⟩ - η-1 Σi pi log pi.
  - Quick check question: Why does negative entropy yield Exp3-style updates, and what is the standard FTRL regret bound?

- Concept: **Importance Sampling in Bandits**
  - Why needed here: Unbiased loss estimation from partial observations is the core technical device.
  - Quick check question: If arm a is pulled with probability qt(a), what is the importance-weighted estimator for loss of arm a' given observation indicator 1(a → a')?

## Architecture Onboarding

- Component map:
  1. **FTRL Core**: Per-context distributions via pt,c = argminp ⟨p, Σs<t ℓ̂s,c⟩ - η-1 F(p)
  2. **Importance Weight Calculator**: wt(a) = Eν[pt,c(Nin(a))] (known) or ŵe(a) = (2/L)Σt∈Tfe-1 st,c(Nin(a))/2 (unknown)
  3. **Loss Estimator**: ℓ̃t,c(a) = 2ℓt,c(a)/(ŵe(a) + (3/2)γ) · 1(at → a, St,a=1, t ∈ Tℓe)
  4. **Rejection Sampler**: Ensures qt matches target observation probability when pt drifts from snapshot
  5. **Epoch Manager**: Coordinates snapshots se+2 = peL, manages paired timestep assignment

- Critical path:
  1. Receive context ct ~ ν
  2. Sample at ~ qt,ct (rejection-sampled version of pt,ct)
  3. Observe ℓt,c(a) for all c, all a ∈ Nout(at)
  4. If t ∈ Tℓe: compute ℓ̃t,c, update FTRL; if t ∈ Tfe: update ŵe+1
  5. At epoch boundary: finalize ŵe+1, snapshot se+2

- Design tradeoffs:
  - **Known vs Unknown Distribution**: Algorithm 1 is simpler; Algorithm 2 adds epoch overhead but handles unknown ν
  - **Epoch length L**: Θ̃(√αT) balances concentration vs. adaptation delay
  - **Implicit exploration γ**: Thetã(1/√αT) trades bias for concentration
  - **Self-loop requirement**: Current analysis requires self-loops; extension is open

- Failure signatures:
  - Regret scales with √M: wt(a) is not properly context-independent
  - High variance in ℓ̃: ŵe not concentrating; increase L or γ
  - Frequent rejection sampling (qt ≠ pt): Snapshot se too stale relative to pt

- First 3 experiments:
  1. **Known distribution sanity check**: Algorithm 1 on 4-arm complete graph (α=1), 10 contexts, uniform ν. Verify regret Õ(√T) independent of M.
  2. **Unknown distribution validation**: Algorithm 2 on same setup. Compare to Algorithm 1; verify only logarithmic overhead.
  3. **Independence number scaling**: Test graphs with α ∈ {1, 2, 4, 8}. Plot regret/√αT vs. time to verify scaling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Õ(√αT) regret bound be extended to feedback graphs without self-loops?
- Basis in paper: [explicit] "We leave the extension of our results to the case without self-loops as an interesting open question."
- Why unresolved: The algorithm breaks the delicate balance required for high-probability bounds in the no-self-loop case (which relies on techniques from Luo et al., 2023). Lemma 3 does not hold without self-loops.
- What evidence would resolve it: An algorithm achieving Õ(√αT) regret for strongly observable graphs without self-loops, or a lower bound showing impossibility.

### Open Question 2
- Question: Can similar regret bounds be achieved for graphs that are not strongly observable?
- Basis in paper: [explicit] "Another interesting research direction is to extend our results to graphs that are not strongly observable."
- Why unresolved: Non-strongly observable graphs typically require different algorithmic approaches and analysis techniques; the minimax regret structure differs fundamentally from the strongly observable case.
- What evidence would resolve it: Characterization of minimax regret for cross-learning contextual bandits with weakly observable or unobservable feedback graphs.

### Open Question 3
- Question: Can the logarithmic factors in the Õ(√αT) bound be removed to achieve truly tight Θ(√αT) regret?
- Basis in paper: [inferred] The title claims "Nearly Tight Bounds" and the abstract mentions the bound is tight "up to logarithmic factors."
- Why unresolved: The FTRL analysis and concentration inequalities used introduce logarithmic dependencies on K and T that may be inherent to the approach.
- What evidence would resolve it: Either an algorithm with O(√αT) regret (no logarithmic factors) or a refined lower bound showing logarithmic factors are necessary.

## Limitations

- The algorithm requires self-loops on the feedback graph, with extension to graphs without self-loops remaining an open question
- The epoch-based algorithm introduces complexity and overhead when the context distribution is unknown
- Logarithmic factors in the regret bound prevent achieving truly tight Θ(√αT) regret

## Confidence

- **Major uncertainties**: The paper's analysis critically depends on self-loops in the feedback graph, with the authors explicitly noting that extending results to graphs without self-loops remains an open question. The epoch-based algorithm (Algorithm 2) introduces complex dependencies between frequency estimation and loss estimation that require careful implementation to maintain conditional independence.
- **Confidence assessment**: High confidence in the theoretical framework and main results. The importance-weighted estimator approach and regret decomposition via stochastic contexts are well-established techniques. Medium confidence in the epoch-based algorithm implementation details, as some edge cases (division by near-zero weights, exact initialization) are not fully specified.

## Next Checks

1. **Self-loop dependence test**: Verify that removing self-loops from the feedback graph breaks the regret bound, confirming the theoretical limitation.
2. **Epoch length sensitivity**: Test Algorithm 2 with different epoch lengths L to identify the optimal scaling behavior and verify the √αT requirement.
3. **Known vs unknown distribution comparison**: Implement both algorithms on the same problem instances to empirically validate that Algorithm 2 achieves only logarithmic overhead compared to Algorithm 1 when the context distribution is unknown.