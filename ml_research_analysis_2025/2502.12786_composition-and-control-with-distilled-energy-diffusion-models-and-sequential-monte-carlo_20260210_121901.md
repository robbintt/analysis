---
ver: rpa2
title: Composition and Control with Distilled Energy Diffusion Models and Sequential
  Monte Carlo
arxiv_id: '2502.12786'
source_url: https://arxiv.org/abs/2502.12786
tags:
- diffusion
- arxiv
- energy
- score
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses training instability in energy-parameterized
  diffusion models and introduces a novel method for controllable generation. The
  authors propose distilling pre-trained diffusion models into energy-based models
  using a conservative projection loss, avoiding the high-variance loss of denoising
  score-matching.
---

# Composition and Control with Distilled Energy Diffusion Models and Sequential Monte Carlo

## Quick Facts
- arXiv ID: 2502.12786
- Source URL: https://arxiv.org/abs/2502.12786
- Reference count: 0
- This paper introduces a method to stabilize training of energy-parameterized diffusion models through distillation and enables controllable generation via Sequential Monte Carlo

## Executive Summary
This paper addresses the training instability problem in energy-parameterized diffusion models by introducing a novel distillation framework. Instead of training from scratch using denoising score-matching, the authors propose distilling a pre-trained diffusion model into an energy-based model through a conservative projection loss. They also cast the diffusion sampling process as a Feynman Kac model, enabling principled composition and low-temperature sampling through Sequential Monte Carlo. The method achieves superior generative performance across multiple datasets and provides a framework for using energy functions beyond just gradient computation.

## Method Summary
The authors propose distilling pre-trained diffusion models into energy-based models using a conservative projection loss, avoiding the high-variance denoising score-matching objective. They introduce a specific network parameterization where the energy network's gradient can be computed efficiently. The distillation process projects the teacher's denoiser output onto a conservative vector field. Additionally, they reframe diffusion sampling as a Feynman Kac model where the learned energy function provides potentials for Sequential Monte Carlo sampling, enabling controlled generation through principled particle filtering.

## Key Results
- Improved generative performance with superior FID scores compared to prior energy-parameterized models on CIFAR-10, CelebA-64, AFHQv2-64, and FFHQ-64 datasets
- Demonstrated composition of diffusion models via SMC for generating combined distributions (e.g., male AND glasses)
- Achieved low-temperature sampling for conditional generation with better condition adherence
- Showed that energy-parameterized models trained via distillation converge faster than those trained from scratch

## Why This Works (Mechanism)

### Mechanism 1: Distillation via Conservative Projection
The distillation loss reduces training instability by replacing the high-variance denoising score-matching objective with a stable, teacher-guided projection. The method introduces a "conservative projection loss" that projects a pre-trained (non-conservative) score vector field onto a conservative one. By using the teacher's expectation E[X0|xt] as the target rather than ground-truth X0, the training objective becomes lower-variance and more stable.

### Mechanism 2: SMC for Principled Control
The Feynman Kac model formulation enables principled composition and control by using energy functions to construct potentials for Sequential Monte Carlo sampling. The method reframes the diffusion sampling process as a Feynman Kac model where the standard reverse diffusion process provides the proposal distribution, while the learned energy function provides the potential functions. This allows for controlled change of measure, guiding samples toward a desired target distribution without retraining.

### Mechanism 3: Teacher-Student Weight Initialization
Initializing the energy-parameterized network with weights from the pre-trained teacher model accelerates convergence and improves final performance. The authors use a specific network parameterization Fθ(xt, t) = hθ(xt, t) · xt, where hθ is the same architecture as the teacher denoiser. By initializing hθ with the teacher's weights, the student model starts from a strong baseline, making the distillation task a fine-tuning problem rather than learning from scratch.

## Foundational Learning

- **Energy-Based Models (EBMs) and the Score Function**
  - Why needed: The entire paper is built on parameterizing a diffusion model's score as the gradient of an energy function (sθ = -∇Eθ).
  - Quick check: What is the mathematical relationship between the score of a distribution and its energy function?

- **Sequential Monte Carlo (SMC) / Particle Filtering**
  - Why needed: The paper's second major contribution is using SMC to sample from the diffusion model. SMC is the engine that uses the energy-derived potentials to steer the generation process.
  - Quick check: In an SMC algorithm, what is the purpose of resampling particles based on their weights?

- **Denoising Score Matching (DSM) vs. Distillation**
  - Why needed: The paper's first major contribution is a new training method (distillation) that they claim is more stable than the standard approach (DSM) for energy-parameterized models.
  - Quick check: In standard DSM, what is the target that the model is trained to predict, and how does the distillation method's target differ?

## Architecture Onboarding

- **Component map:** Pre-trained Diffusion Model (Teacher) -> Energy Network (Student) -> Distillation Training Loop -> SMC Sampler
- **Critical path:** 1) Obtain/Train a high-quality base diffusion model, 2) Design and initialize the student energy network with the teacher's weights, 3) Run the distillation training loop to get the energy model, 4) At inference, use the SMC algorithm with the energy model's potentials for tasks like composition or low-temperature sampling
- **Design tradeoffs:** Performance vs. Training Stability (sacrifices end-to-end training for stability), Inference Cost (SMC sampling introduces significant overhead), Control Quality (SMC provides more principled control but requires a good energy model)
- **Failure signatures:** Weight Degeneracy (particle collapse in SMC), Training Divergence (unstable distillation loss), Conservativity Failure (teacher model's score not approximately conservative)
- **First 3 experiments:** 1) Ablation on Initialization (scratch vs. teacher initialization), 2) Compositionality Test (attribute combination via SMC), 3) SMC vs. MCMC (conditional generation comparison)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the energy function and score be trained jointly within a multi-headed network architecture to eliminate the requirement for a separate pretrained teacher model?
- **Basis in paper:** Section 6.3 (Limitations) states, "Exploring multi-headed networks for joint training of both score and energy may be an interesting direction to pursue, this would avoid the need for pretrained networks and reduce NFE at sampling time."
- **Why unresolved:** The current method relies entirely on distilling a pre-trained score-based teacher into an energy-based model, leaving the dependency on prior training unaddressed.

### Open Question 2
- **Question:** Does the energy distillation framework and SMC sampling scale effectively to latent-space diffusion models or high-resolution image datasets beyond 64x64 pixels?
- **Basis in paper:** Section 6.4 (Future Considerations) notes, "We are yet to verify the performance on other modalities or larger datasets. A first step would be to apply this to latent space... for higher resolution images."
- **Why unresolved:** The experiments were limited to medium-sized image datasets (CIFAR-10, CelebA-64), and the authors have not tested the method's stability or performance in latent spaces.

### Open Question 3
- **Question:** Can approximate resampling techniques effectively mitigate the loss of sample diversity caused by weight degeneracy in high-dimensional SMC sampling?
- **Basis in paper:** Section 6.3 (Limitations) observes that "Resampling may result in a loss of diversity," and suggests, "Investigating approximate resampling techniques which preserve diversity... may be practical mitigation strategy."
- **Why unresolved:** Standard SMC resampling focuses computation on high-probability particles, which can collapse diversity, a known issue the paper identifies but does not solve experimentally.

## Limitations
- The method relies heavily on a well-trained pre-trained diffusion model as a teacher, which may limit generalizability
- SMC sampling introduces significant computational overhead through particle management and resampling
- The conservativity assumption for the teacher model's score may not hold in practice for all well-trained diffusion models
- Empirical validation is somewhat narrow in scope, focusing primarily on standard image benchmarks without exploring more complex, high-dimensional domains

## Confidence
- **High confidence:** The core mechanism of using distillation to stabilize energy-parameterized diffusion training is well-supported by theoretical framework and empirical results
- **Medium confidence:** The SMC formulation for composition and control is theoretically sound but empirical validation is limited to specific attribute combination tasks
- **Medium confidence:** The claim of superior FID scores is supported by experiments but primarily compared against other energy-parameterized models rather than the full spectrum of diffusion approaches

## Next Checks
1. **Scalability to Higher Dimensions:** Test the SMC sampling approach on higher-resolution image datasets (e.g., 256x256 or 512x512) to assess whether particle degeneracy becomes prohibitive as dimensionality increases. Measure both sample quality and computational overhead scaling.

2. **Robustness to Teacher Quality:** Systematically vary the quality of the pre-trained teacher diffusion model (using models with different training durations or architectures) and measure the impact on the distilled energy model's stability and final performance. This would validate the dependency on teacher model quality.

3. **Direct Comparison with Classifier-Free Guidance:** Implement a fair comparison where both methods are used for the same conditional generation tasks (e.g., attribute-based generation), measuring not just sample quality but also inference speed and sample diversity to determine when SMC-based control is actually advantageous over simpler alternatives.