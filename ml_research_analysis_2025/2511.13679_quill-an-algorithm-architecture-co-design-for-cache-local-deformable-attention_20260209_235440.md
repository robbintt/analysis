---
ver: rpa2
title: 'QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention'
arxiv_id: '2511.13679'
source_url: https://arxiv.org/abs/2511.13679
tags:
- detr
- deformable
- dooq
- quill
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QUILL introduces a schedule-aware accelerator that transforms deformable
  attention into cache-friendly, single-pass work by combining Distance-based Out-of-Order
  Querying (DOOQ) with a fused MSDeformAttn engine. DOOQ reorders queries by spatial
  proximity and drives double-buffered prefetch, while the fused core executes interpolation,
  Softmax, aggregation, and projection without intermediate spills.
---

# QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention

## Quick Facts
- arXiv ID: 2511.13679
- Source URL: https://arxiv.org/abs/2511.13679
- Reference count: 36
- Primary result: Achieves up to 7.29× higher throughput and 47.3× better energy efficiency than RTX 4090 for deformable attention

## Executive Summary
QUILL is an algorithm-architecture co-design that eliminates memory bottlenecks in deformable attention by combining a novel scheduling algorithm with a specialized fused hardware engine. The system transforms the traditionally memory-bound deformable attention operation into a cache-friendly, single-pass computation through Distance-based Out-of-Order Querying (DOOQ) and a fused MSDeformAttn core. This approach achieves significant performance and efficiency gains while maintaining FP32-level accuracy across multiple Deformable and Sparse DETR variants.

## Method Summary
The QUILL approach restructures deformable attention computation by first reordering queries spatially using DOOQ, which groups nearby queries together to maximize cache locality and enable effective prefetching. This reordered schedule is then executed on a fused hardware accelerator that performs all deformable attention operations (interpolation, Softmax, aggregation, and projection) in a single pass without intermediate memory spills. The architecture employs double-buffering and a stream prefetcher to overlap computation with memory access, ensuring continuous data flow through the fused core.

## Key Results
- Achieves up to 7.29× higher throughput compared to RTX 4090
- Delivers 47.3× better energy efficiency than RTX 4090
- Outperforms prior accelerators by 3.26–9.82× in throughput and 2.01–6.07× in energy efficiency
- Maintains FP32-level accuracy with ≤0.9 AP degradation across Deformable and Sparse DETR variants

## Why This Works (Mechanism)
QUILL works by fundamentally changing the memory access pattern of deformable attention. Traditional implementations suffer from irregular memory access patterns that cause cache misses and memory spills. DOOQ reorders queries to maximize spatial locality, allowing the fused MSDeformAttn engine to process multiple related operations in cache without spilling to main memory. The fused architecture eliminates intermediate storage by directly chaining operations, while double-buffering and prefetching ensure that memory bandwidth is fully utilized without stalls.

## Foundational Learning
- **Deformable Attention**: A variant of attention that samples features at learnable offset positions rather than regular grids, crucial for handling scale variations in vision tasks
  - Why needed: Standard attention's regular sampling is inefficient for objects of varying sizes
  - Quick check: Verify offset sampling patterns in the reference implementation

- **Spatial Locality Optimization**: Grouping nearby queries together to maximize cache hit rates
  - Why needed: Deformable attention's irregular access patterns cause frequent cache misses
  - Quick check: Measure cache miss rates before and after DOOQ reordering

- **Fused Operation Execution**: Combining multiple computational stages into a single hardware pass
  - Why needed: Intermediate results in traditional pipelines require memory storage, causing stalls
  - Quick check: Compare memory traffic with and without fusion

- **Double-Buffering**: Using two memory buffers to overlap computation with data transfer
  - Why needed: Prevents pipeline stalls during memory access in streaming workloads
  - Quick check: Monitor pipeline utilization with single vs. double buffering

## Architecture Onboarding
**Component Map**: Input Feature Maps -> DOOQ Scheduler -> Double-Buffered Prefetcher -> Fused MSDeformAttn Core -> Output Features

**Critical Path**: The fused MSDeformAttn core represents the critical path, where interpolation, Softmax, aggregation, and projection operations must complete in sequence without stalls. Memory bandwidth to feed this core and register file size to hold intermediate values are the primary constraints.

**Design Tradeoffs**: The architecture trades flexibility for efficiency - the fused core is optimized for a specific deformable attention pattern and cannot easily adapt to arbitrary attention variants. The DOOQ algorithm requires additional scheduling overhead but pays off through reduced memory traffic. Double-buffering increases hardware complexity but eliminates pipeline stalls.

**Failure Signatures**: Performance degradation occurs when query spatial distribution becomes too scattered (DOOQ benefits diminish), when offset patterns change significantly (fused core assumptions break), or when memory bandwidth cannot keep up with the fused core's consumption rate. Accuracy degradation beyond the 0.9 AP threshold indicates issues with the fused computation's numerical precision.

**First Experiments**:
1. Measure cache miss rates and memory traffic with and without DOOQ on representative deformable attention workloads
2. Characterize the fused core's throughput and latency for each computational stage (interpolation, Softmax, aggregation, projection)
3. Validate accuracy preservation across different deformable attention variants by comparing against FP32 baseline implementations

## Open Questions the Paper Calls Out
None

## Limitations
- DOOQ's reordering scheme is optimized for 2D grid-like feature maps and may not generalize well to irregular layouts like point clouds or 3D voxels
- The fused MSDeformAttn engine assumes fixed offset sampling patterns, and changes to sampling patterns could break the single-pass guarantee
- RTL validation is limited to 28nm technology node, and results may not scale predictably to newer process nodes with different cache hierarchies

## Confidence
- **High** confidence in claimed throughput and energy-efficiency gains vs. RTX 4090 and prior accelerators (post-synthesis RTL measurements)
- **Medium** confidence that FP32-accuracy preservation holds across all variants (0.9 AP tolerance is narrow)
- **Low** confidence in absolute efficiency numbers when extrapolated to other process nodes or platforms (missing cross-technology comparisons)

## Next Checks
1. Measure accuracy loss under distribution shifts (e.g., MS COCO to LVIS) to confirm ≤0.9 AP margin holds
2. Port DOOQ to a non-2D attention variant (e.g., 3D voxel-based deformable attention) and quantify prefetch benefit retention
3. Synthesize and benchmark the fused core in a modern 5nm or 7nm technology to reassess throughput and energy-efficiency scaling