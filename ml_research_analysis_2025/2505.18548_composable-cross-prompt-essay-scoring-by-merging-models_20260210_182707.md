---
ver: rpa2
title: Composable Cross-prompt Essay Scoring by Merging Models
arxiv_id: '2505.18548'
source_url: https://arxiv.org/abs/2505.18548
tags:
- source
- adaptation
- merging
- target
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses cross-prompt automated essay scoring (AES)\
  \ under source-free domain adaptation, where models trained on multiple source prompts\
  \ must adapt to a new target prompt without access to source data. The key idea\
  \ is to merge individually fine-tuned source models via linear combinations of their\
  \ LoRA adapters (task vectors) instead of retraining jointly, and to optimize the\
  \ merging coefficients using an unsupervised objective\u2014Prior-encoded Information\
  \ Maximization (PIM)\u2014which encourages score discriminability regularized by\
  \ source-derived priors."
---

# Composable Cross-prompt Essay Scoring by Merging Models

## Quick Facts
- **arXiv ID:** 2505.18548
- **Source URL:** https://arxiv.org/abs/2505.18548
- **Reference count:** 40
- **Primary result:** Merges LoRA adapters from individually fine-tuned source models using Bayesian-optimized Prior-encoded Information Maximization (PIM) to outperform joint training in source-free cross-prompt AES.

## Executive Summary
This paper addresses source-free domain adaptation for automated essay scoring, where models trained on multiple source prompts must adapt to a new target prompt without access to source data. The key innovation is merging individually fine-tuned source models via linear combinations of their LoRA adapters (task vectors) and optimizing the merging coefficients using an unsupervised objective—Prior-encoded Information Maximization (PIM)—which encourages score discriminability regularized by source-derived priors. PIM is optimized efficiently via Bayesian optimization. Experiments on in-dataset and cross-dataset adaptation with LLMs show that the method consistently outperforms joint training on all sources, exceeds other merging strategies, maintains robustness under severe domain shifts, and is more computationally efficient than retraining-based baselines.

## Method Summary
The method consists of two phases: First, LoRA adapters are fine-tuned on each source prompt independently, and Beta distributions are fitted to scaled scores to compute priors. Second, merging coefficients are optimized using Bayesian optimization on 64 unlabeled target samples to maximize the PIM objective: f(λ) = -KL(p(y|λ)||q(y)) - H(p(y|x, λ)). The final model is obtained by merging the adapters: θ_mrg = θ_pre + Σλ_j·τ_j, where τ_j are the task vectors from source fine-tuning.

## Key Results
- Consistently outperforms joint training on all sources across in-dataset and cross-dataset adaptation
- Exceeds other merging strategies (equal weighting, linear combination) in QWK metrics
- Maintains robustness under severe domain shifts (ASAP to PERSUADE2.0)
- More computationally efficient than retraining-based baselines due to offline fine-tuning and online adaptation

## Why This Works (Mechanism)

### Mechanism 1: Task Vector Merging Approximates Selective Joint Training
Linear combinations of LoRA-based task vectors can approximate joint training on selected source domains without accessing source data. Each source prompt yields a task vector τ_j = θ_j − θ_pre, and weighted combination enables post-hoc "soft selection" of beneficial sources.

### Mechanism 2: Prior-Encoded Information Maximization Guides Coefficient Search
An unsupervised objective combining source-derived priors with mutual information maximization identifies effective merging coefficients without target labels. PIM maximizes f(λ) = −KL(p(y|λ)||q(y)) − H(p(y|x,λ)), where the first term encourages predictions matching source-derived prior and the second term promotes confident predictions.

### Mechanism 3: Bayesian Optimization Efficiently Searches Continuous Coefficient Space
Bayesian optimization with Expected Improvement acquisition efficiently finds merging coefficients without gradient computation. The method treats f(λ) as black-box, using Gaussian Process surrogate models and balancing exploitation and exploration through 40 iterations (10 random + 30 guided).

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**: Why needed: Task vectors are computed from LoRA adapters, not full parameters. Quick check: Given weight matrix W∈R^(m×n) with LoRA matrices B∈R^(m×r), A∈R^(r×n), what is the effective task vector dimensionality per layer?

- **Mutual Information in Discriminative Models**: Why needed: PIM modifies standard MI maximization by replacing uniform prior with learned prior. Quick check: Why does maximizing I(y;x) = H(p(y)) − H(p(y|x)) encourage both prediction confidence and class balance?

- **Beta Distribution Properties**: Why needed: Source scores are modeled as Beta(α,β) distributions; understanding mean μ=α/(α+β) and variance σ²=αβ/[(α+β)²(α+β+1)] is needed for prior computation. Quick check: What constraints on α,β ensure unimodality? How does the unified Beta match mixture moments?

## Architecture Onboarding

**Component map:**
Pre-trained LLM (Llama-3.1-8B / Phi-4-mini) -> Per-source fine-tuning -> LoRA adapters {τ_1, ..., τ_M} -> Source statistics {S_1, ..., S_M} -> Bayesian Optimization loop -> Final merged model

**Critical path:** PIM objective computation is called ~40 times per target prompt. Must efficiently extract p(y|x,λ) from LLM next-token probabilities over score tokens.

**Design tradeoffs:**
- Sample size for PIM: Paper uses 64 samples; fewer reduces cost but may increase variance
- Prior unification: Averaging Beta parameters vs. moment-matching (paper uses latter)
- Coefficient bounds: [0,1] per λ_j vs. allowing negative values; current choice prevents sign-flipping

**Failure signatures:**
- All source models predict same score range → PIM maximizes "confidence" on wrong predictions
- Target score range far from sources (e.g., [0,60] vs. [0,6]) → discretized prior q(y) may not cover target space
- GP surrogate fails to improve after ~15 iterations → suggests f(λ) landscape is flat or highly noisy

**First 3 experiments:**
1. Verify individual source models outperform zero-shot on their own prompts (Table 3 zero-shot vs. any single-source fine-tune)
2. Replicate Table 5 ablation (q(y)→U, remove −H term, remove −KL term) on held-out prompt
3. Train sources on ASAP, adapt to single PERSAUDE2.0 prompt; compare PIM vs. simple averaging vs. joint-training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the PIM objective be safeguarded against performance degradation when no source model captures the target prompt's semantics?
- Basis: Limitations section notes that if all source models fail to capture semantics, optimizing for discriminability becomes meaningless
- Why unresolved: Current method assumes at least one source model provides reasonable initial predictions
- What evidence would resolve it: Mechanism that detects semantic mismatch or objective modification maintaining baseline performance in zero-transfer scenarios

### Open Question 2
- Question: Can the method be adapted to handle extreme deviations between source and target score ranges?
- Basis: Limitations section notes extreme range differences (e.g., 0–60 vs. 0–4) lead to suboptimal predictions due to lack of diversity in source model outputs
- Why unresolved: Current framework struggles when source models produce homogeneous predictions for high-range target prompts
- What evidence would resolve it: Successful evaluation on datasets with high-variance score ranges without manual range normalization

### Open Question 3
- Question: Can the high inference latency inherent to LLM-based scoring be reduced to allow for scalability to large volumes of target essays?
- Basis: Limitations section acknowledges inference latency is significantly higher than encoder-based models, potentially limiting scalability
- Why unresolved: Paper focuses on adaptation effectiveness and source-free efficiency but doesn't address inference-time optimization
- What evidence would resolve it: Integration of model compression techniques preserving PIM adaptation capabilities

## Limitations

- **Prior Construction Validity**: Assumes Beta distributions well-model source score distributions across prompts; no corpus evidence validates this for cross-prompt AES
- **Coefficient Space Completeness**: Restricting coefficients to [0,1]^M excludes potentially beneficial negative combinations that could correct systematic biases
- **Target Score Range Coverage**: Discretization may poorly represent target space when ranges differ substantially from sources (e.g., [0,60] vs. [0,6])

## Confidence

**High Confidence**: Computational efficiency advantage over joint training is well-established; linear LoRA merging mechanism is technically sound; Bayesian optimization framework is correctly specified.

**Medium Confidence**: Superiority over other merging strategies is demonstrated but relies on specific hyperparameter choices; performance may vary with different sample budgets or optimization configurations.

**Low Confidence**: Transferability of Beta-based priors across heterogeneous prompts lacks empirical validation; assumption that linear combinations preserve functional scoring properties when source models have conflicting criteria is asserted but not tested.

## Next Checks

1. **Prior Sensitivity Analysis**: Systematically vary Beta distribution parameters by adding synthetic sources with different distribution shapes (bimodal, heavy-tailed) and measure how PIM performance degrades.

2. **Coefficient Constraint Relaxation**: Implement version allowing negative coefficients (with appropriate bounds to prevent divergence) and compare performance on held-out prompts.

3. **Cross-Domain Stress Test**: Train sources on ASAP, adapt to completely different domain (e.g., medical case summaries or scientific abstracts scored for quality); compare PIM against joint training and fine-tuning.