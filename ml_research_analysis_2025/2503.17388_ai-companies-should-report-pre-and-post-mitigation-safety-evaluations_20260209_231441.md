---
ver: rpa2
title: AI Companies Should Report Pre- and Post-Mitigation Safety Evaluations
arxiv_id: '2503.17388'
source_url: https://arxiv.org/abs/2503.17388
tags:
- safety
- evaluations
- post-mitigation
- capabilities
- dangerous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This position paper argues that AI companies should report both
  pre- and post-mitigation safety evaluations to enable informed policy decisions
  about model deployment and safety standards. The authors show that evaluating models
  at both stages provides essential evidence that neither evaluation alone can offer,
  as pre-mitigation assessments reveal unmitigated capabilities while post-mitigation
  evaluations measure safeguard effectiveness.
---

# AI Companies Should Report Pre- and Post-Mitigation Safety Evaluations

## Quick Facts
- **arXiv ID**: 2503.17388
- **Source URL**: https://arxiv.org/abs/2503.17388
- **Authors**: Dillon Bowen; Ann-Kathrin Dombrowski; Adam Gleave; Chris Cundy
- **Reference count**: 13
- **Primary result**: Joint pre- and post-mitigation safety evaluations provide essential evidence that neither evaluation alone can offer for informed AI deployment decisions.

## Executive Summary
This position paper argues that AI companies should report both pre- and post-mitigation safety evaluations to enable informed policy decisions about model deployment and safety standards. The authors demonstrate that evaluating models at both stages provides critical evidence about dangerous capabilities and safeguard effectiveness. Current AI safety disclosures from leading frontier labs have three critical gaps: companies rarely evaluate both pre- and post-mitigation versions, evaluation methods lack standardization, and reported results are often too vague to inform policy.

## Method Summary
The methodology involves evaluating frontier AI models on both pre-mitigation dangerous capabilities and post-mitigation refusal rates using WMDP-Chem datasets and open-ended dangerous requests. For closed-weight models, the post-mitigation API is queried directly. For open-weight models, a helpful-only version is created via LoRA fine-tuning on harmful request/response pairs to simulate the pre-mitigation state. The WMDP-Chem multiple-choice accuracy serves as a proxy for dangerous capabilities, while StrongREJECT evaluator measures compliance rates with dangerous requests.

## Key Results
- Pre-mitigation evaluations reveal latent dangerous capabilities that persist regardless of safety training
- Post-mitigation evaluations measure operational effectiveness of safety guardrails in deployed environments
- Joint evaluation enables tiered deployment decisions rather than binary safe/unsafe judgments

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Pre-mitigation evaluations expose latent dangerous capabilities that persist in model weights regardless of deployed safeguards
- **Mechanism**: Safety training creates a "refusal" surface that can be stripped via fine-tuning or bypassed via jailbreaks. Evaluating the pre-mitigation version reveals underlying knowledge base that adversaries could access
- **Core assumption**: Adversaries can eventually bypass software-level restrictions to access raw model capabilities
- **Evidence anchors**: Fine-tuning APIs pose major security risk; Llama-3 fine-tuning removes refusals while preserving WMDP-Chem accuracy

### Mechanism 2
- **Claim**: Post-mitigation evaluations measure operational effectiveness of safety guardrails
- **Mechanism**: Testing deployed model with adversarial prompts determines if refusal mechanisms prevent misuse in standard interactions
- **Core assumption**: Evaluation prompt set accurately simulates realistic malicious requests
- **Evidence anchors**: Post-mitigation assessments measure effectiveness of safety mechanisms; StrongREJECT used to evaluate model responses

### Mechanism 3
- **Claim**: Comparing pre- and post-mitigation results enables tiered deployment decisions
- **Mechanism**: Mapping capability vs. refusal axes creates four scenarios for deployment decisions (API vs. Open-Weight)
- **Core assumption**: Policymakers can define acceptable risk thresholds based on these quadrants
- **Evidence anchors**: Figure 1 visualizes four scenarios; GPT-4o vs Claude 3.5 Sonnet analysis demonstrates application

## Foundational Learning

- **Concept: Safety Alignment (RLHF/SFT)**
  - **Why needed here**: The paper distinguishes between "pre-mitigation" (raw/base model) and "post-mitigation" (aligned model). Understanding that alignment adds a refusal layer that masks capability is essential
  - **Quick check question**: Can you explain why a model might refuse a request it technically has the knowledge to answer?

- **Concept: Elicitation and Jailbreaking**
  - **Why needed here**: The paper argues pre-mitigation evals are necessary because safeguards can be removed. You must understand that "safety" is often just a surface-level constraint
  - **Quick check question**: How does "fine-tuning" on a harmful dataset affect the safety alignment of a model?

- **Concept: Evaluation Benchmarks (WMDP)**
  - **Why needed here**: The paper uses WMDP-Chem as a proxy for dangerous knowledge. Understanding the difference between multiple-choice (knowledge verification) and open-ended (generation) evaluations is critical
  - **Quick check question**: Why might a model score high on a multiple-choice hazard test but refuse to answer an open-ended question on the same topic?

## Architecture Onboarding

- **Component map**: Pre-mitigation model -> WMDP-Chem MC evaluation -> Capability score; Post-mitigation model -> Open-ended dangerous requests -> StrongREJECT scoring -> Refusal rate
- **Critical path**: 1) Generate Pre-mitigation Proxy via fine-tuning if needed, 2) Estimate Capability via WMDP-Chem MC on both versions, 3) Measure Refusal via open-ended requests on post-mitigation model
- **Design tradeoffs**: Proxy Validity (MC underestimates complex knowledge application) vs. Disclosure (Public vs. Government-only reporting thresholds)
- **Failure signatures**: High refusal in "pre-mitigation" indicates invalid proxy; Capability drop after fine-tuning breaks comparison; Vague reporting prevents comparison
- **First 3 experiments**: 1) Replicate Llama-3 ablation fine-tuning, 2) GPT-4o compliance check with filtered WMDP questions, 3) Threshold sensitivity analysis varying capability thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can pre- and post-mitigation evaluation frameworks be extended to address misalignment risks rather than focusing solely on misuse risks?
- **Basis in paper**: Section 6.3.1 states the position "does not address misalignment risks" and calls for complementary evaluation approaches focused on alignment properties
- **Why unresolved**: Current framework targets human misuse; misalignment involves autonomous model behavior not captured by standard capability or refusal evaluations
- **What evidence would resolve it**: Development and validation of evaluation protocols that reliably detect deceptive alignment or scheming tendencies in both pre- and post-mitigation models

### Open Question 2
- **Question**: What specific standardized methodologies and quantitative thresholds should define industry-wide pre- and post-mitigation safety evaluations?
- **Basis in paper**: Section 4.2 identifies that "companies use ambiguous terms like 'low-refusal' or 'without guardrails' without clear definitions" and recommends standardization without prescribing concrete methods
- **Why unresolved**: Paper advocates standardization but stops short of proposing exact protocols
- **What evidence would resolve it**: Empirical comparison of candidate evaluation methodologies across multiple frontier models demonstrating reproducibility and cross-model comparability

### Open Question 3
- **Question**: Can finetuning-resistant safety mechanisms be developed to prevent adversaries from recovering pre-mitigation capabilities in open-weight models?
- **Basis in paper**: Appendix B states this "is likely to remain the case unless breakthroughs are made in the area of finetuning-resistant models"
- **Why unresolved**: Current safety fine-tuning can be cheaply undone, as demonstrated by the authors' own experiments recovering helpful-only versions
- **What evidence would resolve it**: Demonstration of safety mechanisms that persist after adversarial fine-tuning attempts with quantified robustness bounds

## Limitations

- Evaluation proxy validity: Using WMDP multiple-choice accuracy as a proxy for pre-mitigation dangerous capabilities may significantly underestimate actual risk
- Open-weight model generalization: Llama-3 fine-tuning results may not generalize to other architectures
- Real-world adversary modeling: Pre-mitigation evaluation relevance diminishes if perfect model weight security is achieved
- Temporal validity: Safety evaluations are snapshots that can become outdated as jailbreak techniques evolve

## Confidence

- **High Confidence**: Joint pre- and post-mitigation evaluation methodology provides more actionable information than either evaluation alone
- **Medium Confidence**: Pre-mitigation evaluations reveal latent dangerous capabilities that persist regardless of safety training
- **Medium Confidence**: Post-mitigation evaluations accurately measure deployed safety guardrail effectiveness
- **Low Confidence**: Specific threshold recommendations (e.g., 60% WMDP accuracy) would translate directly to effective policy decisions across different contexts

## Next Checks

1. **Evaluation Proxy Validation Study**: Conduct controlled experiments comparing WMDP multiple-choice accuracy against actual open-ended generation capability on the same topics to quantify the underestimation factor

2. **Cross-Model Methodology Replication**: Apply the exact pre-/post-mitigation evaluation methodology to at least three additional open-weight models to test whether the fine-tuning approach consistently reduces refusals while preserving capabilities

3. **Temporal Degradation Analysis**: Implement a quarterly evaluation cycle for a fixed set of models, tracking how jailbreak success rates and refusal robustness change over time to quantify the "lower bound" acknowledgment