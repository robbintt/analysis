---
ver: rpa2
title: 'UEOF: A Benchmark Dataset for Underwater Event-Based Optical Flow'
arxiv_id: '2601.10054'
source_url: https://arxiv.org/abs/2601.10054
tags:
- underwater
- flow
- optical
- dataset
- event
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the UEOF dataset, the first synthetic underwater
  benchmark for event-based optical flow, addressing the lack of underwater datasets
  with ground-truth motion. The authors use physically-based ray-traced RGBD sequences
  rendered in Blender, converting them to realistic event streams via the v2e toolbox.
---

# UEOF: A Benchmark Dataset for Underwater Event-Based Optical Flow

## Quick Facts
- **arXiv ID:** 2601.10054
- **Source URL:** https://arxiv.org/abs/2601.10054
- **Reference count:** 40
- **Primary result:** Introduces first synthetic underwater benchmark for event-based optical flow with accurate ground-truth motion, revealing significant performance degradation of state-of-the-art methods under underwater conditions.

## Executive Summary
This paper introduces the UEOF dataset, the first synthetic underwater benchmark for event-based optical flow, addressing the lack of underwater datasets with ground-truth motion. The authors use physically-based ray-traced RGBD sequences rendered in Blender, converting them to realistic event streams via the v2e toolbox. This approach produces high-resolution, temporally dense event data with accurate ground-truth optical flow, depth, and camera velocities, enabling evaluation under realistic underwater physics. Benchmarking state-of-the-art learning-based and model-based optical flow methods reveals that underwater phenomena (attenuation, backscatter, caustics, turbidity) significantly degrade performance, with multimodal model-based approaches outperforming others. The dataset establishes a new baseline for developing robust underwater event-based perception algorithms for autonomous underwater vehicles.

## Method Summary
The UEOF dataset is constructed from two synthetic underwater datasets (VAROS and LOFUE) rendered in Blender Cycles. VAROS provides RGB frames with depth maps and 6-DoF camera poses, while LOFUE provides RGB frames with native optical flow. The v2e toolbox converts RGB frames to event streams using slow-motion interpolation (ensuring ≤1px displacement) and adapted contrast thresholds for low-contrast underwater environments. For VAROS, dense optical flow is reconstructed via geometric back-projection using depth maps and camera poses with occlusion checks. The dataset stores events in HDF5 format, optical flow in .flo files, and camera velocities in .npz files, enabling evaluation of both learning-based (E-RAFT, MotionPriorCMax) and model-based (EINCM, MultiCM, OPCM) optical flow methods.

## Key Results
- Underwater conditions (attenuation, backscatter, caustics, turbidity) significantly degrade optical flow performance compared to terrestrial environments
- Model-based approaches (EINCM, MultiCM, OPCM) outperform learning-based methods (E-RAFT, MotionPriorCMax) under underwater conditions
- Multimodal model-based methods (MultiCM) achieve the best overall performance across different underwater scenarios
- Supervised learning approaches suffer from severe feature distribution shifts due to the transition from rigid edges to soft, low-frequency underwater textures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Synthetic event streams can approximate real underwater dynamics if video-to-event conversion parameters are adapted for low-contrast environments.
- **Mechanism:** The `v2e` toolbox simulates a Dynamic Vision Sensor (DVS) by interpolating input frames to ensure pixel motion remains under 1px, triggering events based on log-intensity changes. In underwater scenes, where attenuation reduces contrast, the standard threshold for triggering an event is too high. By lowering the positive/negative thresholds ($\theta_{nominal}$) and reducing threshold variation ($\sigma_\theta$), the system increases sensitivity to faint illumination changes (e.g., sandy seafloors) while suppressing fixed-pattern noise.
- **Core assumption:** The relationship between contrast sensitivity and event density holds true for pseudo-events generated from interpolated video just as it does for physical silicon retinas.
- **Evidence anchors:**
  - [section 3.3]: "To increase DVS sensitivity in low-texture or heavily attenuated regions... we lowered the positive/negative thresholds from the default values."
  - [abstract]: "...converting them to realistic event streams via the v2e toolbox."
  - [corpus]: Limited direct support in neighbors; generalizability relies on the fidelity of the `v2e` model described in the text.
- **Break condition:** If the slow-motion interpolation creates artifacts that manifest as false events, or if the contrast thresholds are lowered to the point of saturating the sensor with noise in highly turbid water.

### Mechanism 2
- **Claim:** Ground-truth optical flow can be reconstructed for static-scene datasets lacking native flow labels via geometric back-projection using depth and pose.
- **Mechanism:** Valid pixels $u_t$ are back-projected into 3D space using depth maps $D_t$. These 3D points are transformed to the next timestamp using the rigid-body motion $T_{t \to t+1}$ and re-projected onto the image plane to calculate displacement $\Delta u$. An occlusion check filters points that move behind visible surfaces.
- **Core assumption:** The depth maps $D_t$ and camera poses $T$ are sufficiently accurate (metric-scale) to derive pixel-precise flow vectors.
- **Evidence anchors:**
  - [section 3.4]: "...reconstructed dense optical flow via geometric back-projection... Correspondences were considered valid only if [they] passed the visibility test ($z_{t+1} \le D_{t+1} + \epsilon$)."
  - [section 3.2]: Notes that VAROS data allows geometric reconstruction because "all dynamic motion arises purely from camera ego-motion."
  - [corpus]: Not applicable; this is a geometric procedure specific to the dataset construction.
- **Break condition:** In scenes with independently moving objects (not present in VAROS but present in LOFUE), this geometric method would fail, necessitating native renderer-based flow extraction (used for LOFUE via VisionBlender).

### Mechanism 3
- **Claim:** Model-based (MB) approaches outperform Learning-based (LB) methods in underwater environments due to the violation of brightness constancy and texture assumptions.
- **Mechanism:** LB methods (e.g., E-RAFT) trained on terrestrial data expect high-frequency rigid edges. Underwater scattering creates soft, low-frequency textures, causing feature distribution shifts. Furthermore, caustics create high-contrast patterns that move independently of the scene geometry. Contrast Maximization (CM) methods assume events correlate to motion; caustics violate this, but multimodal MB methods appear more robust to these "false" events than LB networks trained on clean terrestrial data.
- **Core assumption:** The performance drop is intrinsic to the algorithms' priors rather than just the quality of the synthetic events.
- **Evidence anchors:**
  - [section 4.3]: "Supervised approaches such as E-RAFT... suffered from severe feature distribution shifts... underwater scenes... are defined by soft, low-frequency textures."
  - [section 4.3]: "...refractive caustics created high-contrast light patterns... contrast maximization approaches... reveal a limitation in estimating optical flow from events that are not caused by motion."
  - [corpus]: Neighbors like *Perturbed State Space Feature Encoders* suggest improvements to optical flow networks, but do not address the specific underwater domain gap described here.
- **Break condition:** If an LB network were trained specifically on this synthetic underwater dataset, the domain gap might close, potentially outperforming MB methods.

## Foundational Learning
- **Concept:** **Dynamic Vision Sensors (DVS) / Event Cameras**
  - **Why needed here:** The dataset converts RGB frames into "events" (asynchronous brightness changes). Understanding that these are sparse, temporal signals rather than dense frames is critical for interpreting the evaluation results.
  - **Quick check question:** Does an event camera output a full image at a fixed frame rate, or a stream of pixel-level timestamps?
- **Concept:** **Physically-Based Ray Tracing (PBRT) vs. Rasterization**
  - **Why needed here:** The paper argues previous simulators (Gazebo/Stonefish) fail because they use rasterization, which cannot model volumetric scattering or spectral attenuation.
  - **Quick check question:** Why can't a standard game engine (rasterizer) accurately simulate the "fog" or color shift of deep water without heavy modification?
- **Concept:** **Optical Flow & Brightness Constancy**
  - **Why needed here:** The paper evaluates the error (AEE) of flow algorithms. These algorithms typically assume the brightness of a pixel doesn't change as it moves—a constraint broken by caustics and turbidity in this dataset.
  - **Quick check question:** If a light pattern (caustic) sweeps across a stationary white wall, will a standard optical flow algorithm calculate zero flow or non-zero flow?

## Architecture Onboarding
- **Component map:** Source Data (LOFUE, VAROS) -> Event Engine (v2e) -> GT Generation (VisionBlender, Geometric Back-Projection) -> Storage (HDF5, .flo, .npz)
- **Critical path:** The adaptation of `v2e` parameters. Using default terrestrial parameters results in sparse/noisy events in turbid scenes. The critical tuning involves lowering the contrast sensitivity threshold ($\theta$) while tightening noise variance ($\sigma_\theta$).
- **Design tradeoffs:**
  - *Synthetic vs. Real:* The dataset trades physical realism (real capture) for ground-truth accuracy (perfect pose/flow). Real underwater GT is impossible to get with current hardware.
  - *Resolution:* The dataset splits resolution (960×540 vs 1280×720) to match different sensor specs, requiring users to check resolution compatibility.
- **Failure signatures:**
  - **Haloing:** High-intensity rings from artificial lights in deep water causing event saturation.
  - **Caustic Drift:** Flow vectors pointing in the direction of light ripples rather than camera/scene motion.
  - **Turbidity Dropout:** Uniformly low event density in regions of heavy particle suspension.
- **First 3 experiments:**
  1. **Baseline Run:** Execute E-RAFT (Learning-Based) vs. MultiCM (Model-Based) on `scene1` (shallow). Verify that E-RAFT errors are higher due to the "domain gap" described in Section 4.
  2. **Parameter Sensitivity:** Vary the `v2e` contrast threshold ($\theta$) on a deep-water clip. Plot event density vs. threshold to visualize the trade-off between "missing data" (threshold too high) and "noise" (threshold too low).
  3. **Caustic Ablation:** Isolate a sequence with heavy caustics. Measure the Average Endpoint Error (AEE) specifically on the seafloor pixels vs. object pixels to quantify the "false motion" error discussed in Section 4.3.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can physically-grounded event noise models be developed to better simulate the stochastic nature of underwater light transport compared to the current v2e parameter adaptation?
  - **Basis in paper:** [explicit] The conclusion states that future work includes "extending the UEOF dataset with... physically-grounded event noise models."
  - **Why unresolved:** The current v2e pipeline adapts generic threshold parameters to increase sensitivity for low-contrast regions but does not model the specific noise characteristics caused by underwater photon scattering or sensor physics in aqueous environments.
  - **What evidence would resolve it:** A comparative analysis showing that flow algorithms trained on datasets with these new noise models generalize better to real-world underwater event data than those trained on the current UEOF version.

- **Open Question 2:** How can contrast maximization frameworks be modified to maintain accuracy in the presence of refractive caustics that violate the brightness constancy assumption?
  - **Basis in paper:** [inferred] The analysis notes that "refractive caustics created high-contrast light patterns... [causing] CM objectives to falsely align and produce incorrect motion estimates."
  - **Why unresolved:** Standard contrast maximization relies on the assumption that events are generated solely by motion, which fails when dynamic illumination (caustics) generates dense, high-frequency events independent of scene geometry.
  - **What evidence would resolve it:** The development of a robust event-based optical flow method that can explicitly detect or filter caustic-induced events, resulting in lower Average Endpoint Error (AEE) in shallow-water scenes.

- **Open Question 3:** Can supervised learning architectures overcome the "soft texture" domain gap to outperform model-based methods when trained exclusively on the UEOF dataset?
  - **Basis in paper:** [inferred] The results show learning-based methods (E-RAFT) pretrained on terrestrial data failed due to "severe feature distribution shifts" from rigid edges to soft, low-frequency underwater textures.
  - **Why unresolved:** The paper benchmarks existing pre-trained models but leaves open the question of whether training these architectures from scratch on UEOF's high-fidelity synthetic data would allow them to surpass the current top-performing model-based approaches (e.g., MultiCM).
  - **What evidence would resolve it:** Benchmarking results showing a neural network trained on UEOF achieving a lower AEE than MultiCM or EINCM across both shallow and deep-water environments.

## Limitations
- **Threshold parameter uncertainty:** Exact v2e threshold (θ) and variation (σ_θ) values used for underwater event generation are not fully specified, introducing uncertainty in reproducing the event characteristics.
- **Synthetic nature:** As a synthetic dataset, UEOF may not fully capture all real-world underwater phenomena, particularly the stochastic nature of underwater light transport and sensor noise.
- **Scene composition:** The dataset lacks independently moving objects, limiting the evaluation of optical flow methods in more complex underwater scenarios.

## Confidence
- **High Confidence:** The methodology for synthetic dataset generation and the core finding that underwater conditions degrade optical flow performance are well-supported by the paper's results.
- **Medium Confidence:** The specific parameter tuning for `v2e` and the geometric back-projection method for flow computation are described but lack complete implementation details.
- **Low Confidence:** The generalization of findings to real underwater environments is limited, as the dataset is synthetic.

## Next Checks
1. **Parameter Sensitivity Analysis:** Systematically vary the `v2e` contrast threshold (θ) on deep-water clips and plot event density versus threshold to visualize the trade-off between data coverage and noise.
2. **Caustic Error Isolation:** Measure the Average Endpoint Error (AEE) specifically on seafloor pixels versus object pixels in sequences with heavy caustics to quantify the impact of "false motion" from light patterns.
3. **Real-to-Synthetic Transfer:** Test whether a learning-based method (E-RAFT) fine-tuned on this dataset outperforms the same method trained on terrestrial data, directly validating the domain gap hypothesis.