---
ver: rpa2
title: 'Prompting ChatGPT for Chinese Learning as L2: A CEFR and EBCL Level Study'
arxiv_id: '2501.15247'
source_url: https://arxiv.org/abs/2501.15247
tags:
- language
- chinese
- learning
- chatgpt
- level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Generative AI chatbots can be guided via prompts to support Chinese
  L2 learning at defined CEFR/EBCL levels. A systematic test of ChatGPT-4o and ChatGPT-4o-mini
  models showed that prompts explicitly including the target-level character lists
  reduced out-of-list character use by 2.5-14.3% at A1 and A1+ levels, improving compliance
  with EBCL thresholds.
---

# Prompting ChatGPT for Chinese Learning as L2: A CEFR and EBCL Level Study

## Quick Facts
- arXiv ID: 2501.15247
- Source URL: https://arxiv.org/abs/2501.15247
- Reference count: 0
- LLMs can be guided via prompts to support Chinese L2 learning at defined CEFR/EBCL levels, with character list prompts reducing out-of-list character use by 2.5-14.3% at A1/A1+ levels.

## Executive Summary
This study evaluates whether ChatGPT-4o and ChatGPT-4o-mini can adhere to CEFR/EBCL character lists when generating Chinese L2 learning content. Using structured system prompts with and without explicit character constraints, the researchers tested 10 EBCL activities across three proficiency levels (A1, A1+, A2). Results show that prompts explicitly including target-level character lists significantly reduce out-of-list character usage at beginner levels, though instruction adherence remains imperfect. The methodology provides a framework for evaluating LLM outputs against pedagogical constraints, though further validation with actual learners is needed.

## Method Summary
The researchers used automated API calls to test two OpenAI models (gpt-4o, gpt-4o-mini) across three EBCL proficiency levels and 10 learning activities. They compared two prompt conditions: system prompts with explicit character lists versus prompts without lists. Each condition was tested 10 times per model-level-task combination (600 total calls). Temperature was set to 0.7. Responses were analyzed by counting characters not present in the target level's EBCL list, measuring "instruction deviation" as the percentage of out-of-list characters.

## Key Results
- Explicit character list inclusion reduced out-of-list character usage by 2.5-14.3% at A1 and A1+ levels
- At A2 level, character list prompts had minimal effect on reducing out-of-list characters
- Both models showed similar performance patterns across all conditions
- Instruction adherence remained imperfect even with explicit constraints

## Why This Works (Mechanism)
Structured system prompts guide LLMs to follow specific constraints by providing clear, step-by-step instructions that include the target character lists. The Chain-of-Thought approach helps models reason through the task before generating output.

## Foundational Learning
- **CEFR/EBCL Framework**: Standardized proficiency levels for language learning. Needed to establish measurable learning objectives. Quick check: Verify EBCL lists contain 320 characters for A1.
- **Character-level vs Word-level Analysis**: Chinese text can be evaluated at different granularities. Needed to measure compliance with character constraints. Quick check: Confirm "你好" counts as 2 characters in analysis.
- **Chain-of-Thought Prompting**: Step-by-step reasoning instructions for LLMs. Needed to improve instruction following. Quick check: Verify prompts include "First answer... Second check... Third rephrase..." structure.
- **Instruction Deviation**: Percentage of model output violating constraints. Needed to quantify prompt effectiveness. Quick check: Confirm deviation measured as out-of-list character percentage.
- **Temperature Setting**: Controls randomness in LLM outputs. Needed to balance creativity and constraint adherence. Quick check: Verify temperature=0.7 throughout experiments.
- **Automated API Testing**: Systematic evaluation of model behavior. Needed to collect reproducible results. Quick check: Confirm 600 total API calls across all conditions.

## Architecture Onboarding

**Component Map**: EBCL Lists -> System Prompts -> API Calls -> Character Extraction -> Compliance Analysis

**Critical Path**: Character List Definition → Prompt Engineering → Model Generation → Text Processing → Evaluation Metrics

**Design Tradeoffs**: Character-level analysis provides fine-grained control but may miss word-level appropriateness; higher temperature increases naturalness but reduces constraint adherence.

**Failure Signatures**: High instruction deviation (>20% out-of-list) indicates prompt constraints are not being followed; minimal difference between with-list and without-list conditions suggests prompts are ineffective.

**First Experiments**:
1. Test whether reducing temperature from 0.7 to 0.3 improves instruction adherence at A2 level
2. Verify character extraction logic correctly handles mixed Chinese-English responses
3. Confirm baseline out-of-list rates for "without list" condition match expected ranges (10-25% for A1)

## Open Questions the Paper Calls Out
- Can automated prompt optimization techniques further reduce instruction deviation compared to manually engineered Chain-of-Thought prompts?
- Do structured system prompts tested on ChatGPT-4o transfer effectively to other open-weight LLMs regarding character constraint adherence?
- Does reducing out-of-list characters via structured prompts correlate with improved reading comprehension and character retention for L2 learners?

## Limitations
- Reliance on character-level rather than word-level analysis may underestimate true readability for learners
- Temperature setting of 0.7 may be too high for strict instruction-following, explaining poor A2 compliance
- Automated evaluation cannot assess semantic appropriateness or pedagogical soundness of generated content

## Confidence
- **High Confidence**: Experimental methodology is sound and reproducible; finding that explicit character lists reduce out-of-list usage at A1/A1+ is robust
- **Medium Confidence**: Prompt engineering effectiveness is well-supported within tested constraints but may not generalize to other languages or models
- **Low Confidence**: Claims about enabling practical CEFR-aligned learning tools are overstated without learner outcome data

## Next Checks
1. Replicate experiment with temperature set to 0.3 or 0.5 to test if stricter sampling improves A2-level compliance
2. Conduct pilot study with actual Chinese L2 learners to assess correlation between lower out-of-list rates and improved comprehension
3. Extend analysis to word-level evaluation using Chinese word segmentation to better predict pedagogical appropriateness