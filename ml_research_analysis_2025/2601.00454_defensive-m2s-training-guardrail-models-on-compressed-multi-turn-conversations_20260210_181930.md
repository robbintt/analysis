---
ver: rpa2
title: 'Defensive M2S: Training Guardrail Models on Compressed Multi-turn Conversations'
arxiv_id: '2601.00454'
source_url: https://arxiv.org/abs/2601.00454
tags:
- training
- multi-turn
- guardrail
- compression
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Defensive M2S introduces a training paradigm that fine-tunes guardrail\
  \ models on compressed single-turn representations of multi-turn conversations,\
  \ reducing computational complexity from O(n\xB2) to O(n). The method leverages\
  \ M2S compression templates (hyphenize, numberize, pythonize) that preserve adversarial\
  \ semantics while eliminating assistant responses."
---

# Defensive M2S: Training Guardrail Models on Compressed Multi-turn Conversations

## Quick Facts
- arXiv ID: 2601.00454
- Source URL: https://arxiv.org/abs/2601.00454
- Authors: Hyunjun Kim
- Reference count: 15
- Primary result: Up to 94.6% token reduction while maintaining or improving safety detection accuracy

## Executive Summary
Defensive M2S introduces a training paradigm that fine-tunes guardrail models on compressed single-turn representations of multi-turn conversations, reducing computational complexity from O(n²) to O(n). The method leverages M2S compression templates (hyphenize, numberize, pythonize) that preserve adversarial semantics while eliminating assistant responses. Across three guardrail model families and SafeDialBench evaluation, the approach achieves up to 94.6% token reduction while maintaining or improving detection accuracy. The best configuration, Qwen3Guard with hyphenize compression, reaches 93.8% attack detection recall compared to 54.9% baseline recall.

## Method Summary
Defensive M2S fine-tunes guardrail models on compressed single-turn representations of multi-turn conversations. The approach extracts only user turns from conversations and formats them using one of three templates (hyphenize, numberize, pythonize). Models are trained using QLoRA on these compressed representations, reducing token count from ~3000 to ~173 while maintaining safety detection performance. The method addresses the quadratic complexity of processing long conversations by converting them to linear-time operations.

## Key Results
- Up to 94.6% token reduction (3020→173 tokens) while maintaining or improving detection accuracy
- Qwen3Guard with hyphenize compression achieves 93.8% attack detection recall vs 54.9% baseline recall
- Model-template compatibility proves critical, with different models performing optimally under different compression schemes

## Why This Works (Mechanism)

### Mechanism 1
Compressed single-turn representations preserve sufficient adversarial semantics for accurate safety classification. M2S templates extract only user turns and format them into structured single-turn prompts, concentrating adversarial intent in user messages while eliminating assistant response overhead. This works because safety-relevant semantic features are robust to loss of conversational context and assistant response signals.

### Mechanism 2
Model-template compatibility is critical—different models have distinct optimal compression formats. Each guardrail model's tokenization schemes, chat formats, and pre-training distributions interact differently with compression templates. Qwen3Guard's `im_start/im_end` format aligns well with hyphenize structure, while Nemotron performs better with numberize's sequential indexing.

### Mechanism 3
Single-template training outperforms mixed-template training due to stronger, more consistent learning signals. Training on one compression format allows the model to develop stable internal representations, while mixed templates introduce inconsistent format signals that create optimization conflicts and higher variance.

## Foundational Learning

- **Guardrail Models as Binary Safety Classifiers**: Understanding their input-output structure is essential for adapting training pipelines. Given a conversation `(u₁, a₁, u₂, a₂)`, what does a guardrail output, and what token count does it process in baseline vs. M2S mode?

- **Multi-turn Jailbreak Attack Taxonomy**: Defensive M2S's effectiveness depends on attack structure—response-dependent methods (Crescendo, PAIR) vs. pre-scripted methods have different implications for compression validity. If an attack requires model responses to construct subsequent turns, does M2S compression still preserve attack semantics? Under what conditions?

- **Quadratic vs. Linear Complexity in Transformer Attention**: The paper claims O(n²) to O(n) reduction; understanding whether this applies to attention computation, training data generation, or both is critical for accurate cost modeling. For a 20-turn conversation with U=R=100 tokens, calculate the theoretical token ratio between baseline and M2S approaches. Which phase (generation vs. training) contributes most to the quadratic scaling?

## Architecture Onboarding

- Component map: Raw Multi-turn Conversation C = {(u₁,a₁), ..., (uₙ,aₙ)} -> [M2S Compression Module] f_θ(C) → extracts user turns only -> [Template Formatter] → hyphenize | numberize | pythonize -> Compressed Single-turn Representation C̃ (~5% of |C|) -> [Guardrail Model G] (QLoRA fine-tuned on compressed format) -> Safety Classification ŷ ∈ {safe, unsafe}

- Critical path: Template selection → Training data compression → QLoRA fine-tuning → Inference-time compression → Guardrail prediction. Template mismatch at any stage degrades performance severely.

- Design tradeoffs:
  - **Accuracy vs. Efficiency**: Nemotron baseline achieves 99.0% recall (3020 tokens); Qwen3Guard M2S hyphenize achieves 93.8% recall (173 tokens)—5.2% recall drop for 94.6% token reduction
  - **Template Specificity vs. Generalization**: Single-template training outperforms mixed but creates format lock-in. Consider deployment constraints before selecting template
  - **Context Preservation vs. Compression Ratio**: Discarding assistant responses loses compliance detection signals but enables 93× training reduction

- Failure signatures:
  - **LlamaGuard pattern**: Baseline 75.1% → all M2S templates 17-25%. Indicates model-specific representation mismatch
  - **Mixed-template variance**: Nemotron mixed training shows ±43.5% variance vs. ±8.7% for single-template. High variance signals template interference
  - **Seed instability**: Qwen3Guard baseline shows 54.9% across all seeds (stable) but M2S configurations vary (92.5%, 93.2%, 95.8%). Always run multiple seeds

- First 3 experiments:
  1. **Template ablation on validation set**: Train Qwen3Guard on each template separately; evaluate on held-out SafeDialBench subset. Confirm hyphenize superiority for your specific data distribution
  2. **Compression ratio vs. recall curve**: Vary conversation length (5, 10, 15, 20 turns) and measure both token reduction and recall degradation. Identify break-even point for your latency budget
  3. **Cross-template inference test**: Train on hyphenize, evaluate on numberize-formatted inputs. Quantify format mismatch penalty to assess template lock-in risk

## Open Questions the Paper Calls Out
None

## Limitations
- Attack type compatibility: Strong performance on pre-scripted attacks but no evaluation of response-adaptive attacks where user turns depend on model responses
- Model-specific performance variability: Extreme performance differences across guardrail models suggest strong model-template compatibility effects, but mechanisms are not investigated
- Real-world deployment assumptions: Evaluation assumes attacks are detected at conversation end, but real deployments may need streaming detection or partial conversation evaluation

## Confidence
**High Confidence**: 
- Token reduction claims (94.6% reduction verified through calculation: 3020→173 tokens)
- Computational complexity reduction from O(n²) to O(n) for training data generation
- Single-template training consistently outperforming mixed-template training across multiple experiments

**Medium Confidence**:
- Safety detection accuracy claims (93.8% recall for Qwen3Guard hyphenize) based on SafeDialBench evaluation
- Model-template compatibility findings (Qwen3Guard/hyphenize vs. Nemotron/numberize) from systematic ablation studies
- Cost reduction estimates for training and inference

**Low Confidence**:
- Generalizability across all guardrail models (LlamaGuard results suggest model-specific failures)
- Performance on response-adaptive attacks (no empirical validation provided)
- Mixed-template training variance claims (based on limited Nemotron experiments)

## Next Checks
1. **Response-Adaptive Attack Evaluation**: Evaluate M2S compression on attacks where user messages depend on model responses (e.g., Crescendo-style pattern-following attacks). Generate conversation pairs where the same user script produces different safety outcomes based on model responses. Measure whether compressed representations preserve these safety-relevant differences.

2. **Model Architecture Ablation Study**: Systematically investigate why LlamaGuard fails with M2S compression (17-25% recall) while Qwen3Guard succeeds (93.8% recall). Test M2S templates on intermediate models with varying tokenization schemes, attention mechanisms, and pre-training objectives to identify architectural features that enable or prevent successful M2S adaptation.

3. **Streaming Detection Benchmark**: Create a benchmark where conversations are evaluated incrementally as they develop. Measure detection accuracy at different conversation depths (turns 5, 10, 15, 20), false positive rates when model responses are incomplete, and template selection performance without full conversation context.