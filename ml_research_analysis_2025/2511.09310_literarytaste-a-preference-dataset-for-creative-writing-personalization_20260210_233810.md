---
ver: rpa2
title: 'LiteraryTaste: A Preference Dataset for Creative Writing Personalization'
arxiv_id: '2511.09310'
source_url: https://arxiv.org/abs/2511.09310
tags:
- preference
- preferences
- text
- writing
- creative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LiteraryTaste, a dataset for personalizing
  creative writing technologies by capturing individual reading preferences. The dataset
  includes 60 annotators' self-reported reading habits (stated preferences) and 100
  binary preference annotations over pairs of short creative writing texts (revealed
  preferences).
---

# LiteraryTaste: A Preference Dataset for Creative Writing Personalization

## Quick Facts
- arXiv ID: 2511.09310
- Source URL: https://arxiv.org/abs/2511.09310
- Reference count: 40
- Primary result: Finetuned transformer achieved 75.8% accuracy in modeling personal revealed preferences for creative writing texts

## Executive Summary
LiteraryTaste introduces a dataset capturing individual preferences for creative writing texts, enabling personalized AI systems for literature. The dataset includes 60 annotators' self-reported reading habits and 100 binary preference annotations over pairs of short creative writing texts. Analysis reveals people have highly diverse tastes with minimal agreement on both stated and revealed preferences. A finetuned transformer encoder achieved 75.8% accuracy in modeling personal preferences and 67.7% for aggregated preferences, outperforming other approaches including LLM prompting. The work establishes a foundation for developing personalized creative writing systems and provides guidelines for preference elicitation in this domain.

## Method Summary
The dataset consists of 2000 text pairs from five sources (Gutenberg, Sterman et al., r/WritingPrompts, Poetry, Tell-me-a-story) with 60 annotators providing binary preference annotations for each pair. Each annotator also completed a survey with 34 questions about their reading habits and preferences. The best-performing approach used full finetuning of ModernBERT-large as a reward model with binary ranking loss, achieving 75.8% accuracy for personal preference modeling and 67.7% for aggregated preferences through 10-fold cross-validation.

## Key Results
- Annotators agreed minimally on both stated and revealed preferences, with Fleiss' Kappa of 0.14 indicating slight agreement
- Finetuned transformer encoder achieved 75.8% accuracy for personal preference modeling versus 67.7% for aggregated preferences
- LLM-driven interpretability pipeline identified 13 key dimensions of preference variation, clustering annotators into 10 distinct literary taste groups
- Stated preferences had limited utility in predicting revealed preferences, with cross-annotator models performing at random chance levels

## Why This Works (Mechanism)

### Mechanism 1
Finetuning a transformer encoder as a reward model captures individual creative writing preferences with moderate accuracy. Binary ranking loss trains ModernBERT-large to assign higher scores to preferred texts, with each user getting a personalized model trained on ~90 revealed preference pairs. Individual preferences are sufficiently consistent within a person to form learnable patterns from small sample sizes.

### Mechanism 2
Stated preferences provide limited predictive value for revealed preferences in creative writing. Survey responses about reading habits and textual qualities are encoded as feature vectors but models struggle to bridge the gap between what people say they like and what they actually prefer when confronted with specific texts.

### Mechanism 3
LLM-driven interpretability pipelines extract meaningful preference dimensions from text comparisons. Lloom extracts high-level concepts via iterative clustering and synthesis, with preference vectors computed as differences between chosen/rejected text embeddings and aggregated via logistic regression coefficients.

## Foundational Learning

- **Binary preference modeling / Bradley-Terry models**
  - Why needed here: Core mathematical framework for converting pairwise comparisons into reward scores
  - Quick check question: Can you explain why the loss function uses σ(r_θ(x_c) - r_θ(x_r)) rather than raw score differences?

- **Transfer learning with transformer encoders**
  - Why needed here: Understanding how pretrained ModernBERT-large can be adapted with ~15-90 samples per user
  - Quick check question: What properties of pretrained encoders enable sample-efficient finetuning on preference tasks?

- **Inter-annotator agreement metrics (Fleiss' Kappa, Krippendorff's alpha)**
  - Why needed here: Essential for quantifying preference diversity and justifying personalization approaches
  - Quick check question: Why does the paper interpret Kappa of 0.14 as "slight agreement" rather than random noise?

## Architecture Onboarding

- **Component map:**
  - Data layer: 2000 text pairs from 5 sources (Gutenberg, Sterman, r/WritingPrompts, Poetry, Tell-me-a-story), plus human vs. LLM comparisons
  - Annotation layer: 60 annotators × 100 pairs (revealed) + 34 survey questions (stated)
  - Embedding layer: Semantic embeddings (jina-embeddings-v4) + style embeddings (finetuned ModernBERT)
  - Modeling layer: Full-finetuning (best), neural networks over embeddings, logistic regression, decision trees, LLM prompting
  - Interpretability layer: Lloom concept extraction → preference vectors → clustering (10 annotator clusters)

- **Critical path:**
  1. Prepare text pairs → 2. Collect revealed preferences (binary choices) → 3. Train personalized reward model per user (if resources available) OR aggregate preferences → 4. Evaluate via 10-fold cross-validation

- **Design tradeoffs:**
  - Personal vs. aggregated models: Personal (75.8%) outperforms aggregated (67.7%), but requires per-user training data
  - Full-finetuning vs. embedding-based models: Finetuning achieves highest accuracy but requires GPU resources; neural networks over embeddings are lighter
  - Sample size: Performance gains diminish after ~30 samples but don't plateau at 90

- **Failure signatures:**
  - Test accuracy near 50% (random chance): Seen in cross-annotator models trying to generalize from stated preferences
  - Low inter-annotator agreement (Kappa < 0.2): Indicates personal taste divergence, making aggregated models difficult
  - Disagreement between stated and revealed preferences: Survey responses may mislead if used alone

- **First 3 experiments:**
  1. Replicate Full-Finetuning baseline: Train ModernBERT-large on 90 samples per user, verify ~75% accuracy on held-out pairs
  2. Ablate sample size: Test with 15, 30, 60 samples to confirm performance trajectory
  3. Evaluate stated→revealed transfer: Train Cross-LR-Weight model, compare cross-annotator vs. within-annotator accuracy to establish stated preference utility bounds

## Open Questions the Paper Calls Out

- How can the preference data be effectively leveraged for personalized creative text generation? The authors note they haven't investigated how to leverage the data for personalized text generation or scenarios where users interact further after initial preference elicitation.

- Can improved survey designs or larger annotator pools successfully bridge the gap between stated and revealed preferences? The paper suggests the failure might be due to questions not being comprehensive enough or insufficient annotator numbers.

- Do these findings generalize to longer narratives, or are they specific to the 150-word snippets used? The study only dealt with short text snippets, so results don't convey insights about preferences on aspects that only manifest in longer texts like narrative arcs.

## Limitations

- The dataset's sample size per annotator (90 revealed preferences) and total annotator count (60) may not fully capture preference diversity across broader populations
- The binary preference format constrains expressiveness compared to rating scales, potentially oversimplifying nuanced preferences
- Stated preferences showed limited predictive value for revealed preferences, suggesting fundamental gaps between declared and actual tastes

## Confidence

- **High confidence**: The core finding that individual preferences are highly diverse and that personal models significantly outperform aggregated models is well-supported by the data
- **Medium confidence**: The specific accuracy numbers (75.8% personal, 67.7% aggregated) are reliable within the dataset but may not generalize to different text domains or larger annotator pools
- **Low confidence**: The interpretability pipeline's identification of 13 key preference dimensions relies heavily on LLM capabilities that may not generalize across different creative writing contexts

## Next Checks

1. **Test-retest reliability**: Re-administer the revealed preference task to a subset of annotators after 2-4 weeks to assess consistency and establish whether preferences are stable enough for personalization systems

2. **Cross-cultural validation**: Collect data from annotators with different cultural backgrounds to determine whether the identified preference dimensions are universal or culturally specific

3. **Full-text preference modeling**: Extend the approach to longer text segments (500-1000 words) or complete short stories to validate whether the same modeling approaches work when preferences can develop over extended narrative arcs