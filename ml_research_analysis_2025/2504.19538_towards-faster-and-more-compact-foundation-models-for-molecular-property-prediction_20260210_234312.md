---
ver: rpa2
title: Towards Faster and More Compact Foundation Models for Molecular Property Prediction
arxiv_id: '2504.19538'
source_url: https://arxiv.org/abs/2504.19538
tags:
- block
- blocks
- performance
- reduction
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the computational inefficiency of large foundation
  models for molecular property prediction by investigating model compression strategies.
  It analyzes the layer contributions in the Joint Multi-domain Pre-training (JMP)
  model and finds that later interaction blocks contribute less to overall performance,
  indicating over-parameterization.
---

# Towards Faster and More Compact Foundation Models for Molecular Property Prediction

## Quick Facts
- arXiv ID: 2504.19538
- Source URL: https://arxiv.org/abs/2504.19538
- Reference count: 40
- A 32% reduction in model size (160M to 108M parameters) yields 1.3x inference speedup with minimal performance loss on molecular property prediction tasks

## Executive Summary
This paper investigates model compression strategies for Joint Multi-domain Pre-training (JMP) foundation models used in molecular property prediction. The authors identify that later interaction blocks in the architecture contribute less to overall performance, indicating over-parameterization. Through systematic pruning experiments, they demonstrate that removing two interaction blocks reduces the model size by 32% while increasing inference throughput by 1.3x, with minimal degradation in predictive accuracy across various downstream tasks. The study provides practical insights for developing more efficient foundation models for molecular and materials discovery.

## Method Summary
The authors employ a three-pronged approach to model compression: layer relevance analysis using GradCAM to identify redundant interaction blocks, block reduction strategies that prune these identified blocks, and weight realignment techniques (Sliced MLP) to preserve learned representations. For aggressive pruning scenarios, knowledge distillation is applied to recover lost performance by enforcing feature-level alignment between the compressed student model and the original teacher model. The compressed models are then fine-tuned on standard molecular property prediction benchmarks including QM9, QMOF, and OC20.

## Key Results
- Removing two interaction blocks reduces model size by 32% (160M to 108M parameters) while achieving 1.3x inference speedup
- Sliced MLP initialization preserves performance better than random initialization during fine-tuning
- Knowledge distillation effectively recovers accuracy lost during aggressive pruning (below 4 blocks)
- The compressed model maintains competitive performance across energy prediction, force prediction, and material property tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Removing later interaction blocks preserves performance because deeper layers in this architecture exhibit diminishing returns.
- **Mechanism:** The authors utilize GradCAM to score layer relevance. They find that earlier interaction blocks (f2–f5) capture most of the necessary physical interactions, while later blocks (f6–f7) show significantly lower relevance scores, suggesting functional redundancy or saturation.
- **Core assumption:** The GradCAM relevance score is a valid proxy for a layer's functional importance to the final energy/force prediction.
- **Evidence anchors:**
  - [Abstract] "later interaction blocks provide diminishing returns"
  - [Section 4.1] "sixth and seventh blocks exhibiting the lowest contribution... suggests that blocks with low contribution can be removed"
  - [Corpus] "Superior Molecular Representations from Intermediate Encoder Layers" (corpus neighbor) supports the general finding that intermediate layers often outperform final layers in molecular encoders.
- **Break condition:** If the downstream task requires reasoning about extremely complex long-range interactions not present in the pre-training data, the "redundant" deep layers may become necessary.

### Mechanism 2
- **Claim:** Performance retention relies on "Sliced MLP" initialization rather than random re-initialization of the prediction head.
- **Mechanism:** The architecture concatenates outputs from all blocks before feeding them into the FinalMLP. When a block is pruned, the authors slice the corresponding weights from the first layer of the FinalMLP ($g'_1$) rather than learning a new projection from scratch. This preserves the learned feature alignment for the remaining blocks.
- **Core assumption:** The pre-trained weights in the FinalMLP contain independent substructures corresponding to specific block outputs that remain valid even when the block count changes.
- **Evidence anchors:**
  - [Section 3.3] "Sliced MLP... retain the parameters of the first MLP layer... truncating it to match the reduced dimensionality"
  - [Section 4.3] "BR/RandomMLP... exhibits inconsistent behavior... noticeable performance drop in rMD17 and QM9"
  - [Corpus] Weak direct evidence in the provided corpus for this specific weight splicing technique.
- **Break condition:** If the FinalMLP features are highly entangled (non-orthogonal) across block dimensions, slicing weights will disconnect the remaining features from the prediction head.

### Mechanism 3
- **Claim:** Knowledge Distillation (KD) recovers accuracy lost during aggressive pruning by enforcing feature-level alignment.
- **Mechanism:** Pruning often degrades performance (e.g., a 2-block model performs poorly). The authors apply L1 losses on node and edge features (n2n, e2e) from the teacher to the student. This forces the pruned model to mimic the internal representations of the full model, closing the performance gap.
- **Core assumption:** The teacher's internal representations (which the student is forced to match) are optimal and do not contain the redundancy that justified pruning in Mechanism 1.
- **Evidence anchors:**
  - [Section 3.3] "distilling the force predictions... [and] node-to-node (n2n) and edge-to-edge (e2e) distillation"
  - [Section 4.2] "Applying both BR and KD greatly reduces the performance gap... with 5 blocks, the difference narrows to just -2.8 [meV/A]"
  - [Corpus] "Accelerating molecular graph neural networks via knowledge distillation" (referenced in paper text, though not detailed in provided corpus snippets) validates KD for GNNs.
- **Break condition:** If the student model capacity is too small (e.g., < 3 blocks), it may lack the representational power to mimic the teacher, causing distillation to fail.

## Foundational Learning

- **Concept:** **Message Passing / Interaction Blocks (GemNet-OC)**
  - **Why needed here:** The pruning target is the *interaction block*. You must understand that these blocks aggregate atomic information (message passing) to build higher-order representations of the molecular system.
  - **Quick check question:** Can you explain why removing an interaction block reduces the "receptive field" or the complexity of atomic relationships the model can capture?

- **Concept:** **Foundation Model Fine-tuning**
  - **Why needed here:** The paper operates within the "pre-train then fine-tune" paradigm. The goal is to maintain the *generalized knowledge* from pre-training (JMP) while changing the architecture.
  - **Quick check question:** Why is "Sliced MLP" theoretically better than random initialization when fine-tuning a pruned foundation model?

- **Concept:** **Knowledge Distillation (KD) Losses**
  - **Why needed here:** The paper uses specific distillation losses (n2n, e2e) for graphs. Standard KD (soft labels) is insufficient for regression tasks like molecular property prediction.
  - **Quick check question:** Why would you enforce alignment on intermediate node features ($\hat{g}_i$) rather than just the final energy output?

## Architecture Onboarding

- **Component map:**
  - Input: 3D positions + Atomic numbers
  - Backbone: 1 Embedding Block ($f_1$) → Sequential Interaction Blocks ($f_2$ to $f_7$)
  - Head: Concatenation of all block outputs → FinalMLP ($g$) → Energy/Force Prediction

- **Critical path:**
  1. **Analysis:** Run GradCAM on the validation set to confirm which blocks have low relevance (typically $f_6, f_7$)
  2. **Pruning:** Remove low-relevance blocks (e.g., reduce 6 → 4 blocks)
  3. **Realignment:** Adjust the FinalMLP input layer dimensions (Slicing)
  4. **Recovery (Optional):** Run KD using the original model as the teacher on 1.5% of pre-training data

- **Design tradeoffs:**
  - **Speed vs. Capacity:** Dropping from 6 to 4 blocks yields 1.3x speedup and 32% parameter reduction with "minimal" drop. Dropping to 2 blocks causes significant accuracy loss (underfitting)
  - **Sliced vs. Random MLP:** Random initialization is faster to set up but suffers from distribution shift; Sliced MLP preserves learned features but requires careful weight manipulation

- **Failure signatures:**
  - **Catastrophic Drop:** Performance collapses when removing early blocks ($f_2, f_3$) or the embedding layer ($f_1$)
  - **Stagnant Loss:** If KD is applied but the student is too small (e.g., 2 blocks), the loss may plateau at a high value because the student cannot match the teacher's capacity
  - **Distribution Mismatch:** If using RandomMLP, performance on small datasets (rMD17) may drop significantly while large datasets (QMOF) remain stable

- **First 3 experiments:**
  1. **Validation of Relevance:** Load the pre-trained JMP-L checkpoint and run the provided GradCAM script on the OC20 validation set to reproduce Figure 2 (Block Relevance)
  2. **The "Sliced" Ablation:** Create a 4-block variant of JMP-L by slicing the FinalMLP weights. Fine-tune on rMD17 (Aspirin) and compare MAE against the 6-block baseline
  3. **Distillation Recovery:** Take the failing 2-block or 3-block model from Experiment 2 and apply the n2n/e2e distillation loss against the full 6-block teacher to quantify the recoverable accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- The layer irrelevance observed may be architecture-specific rather than universal across different molecular foundation models
- Performance degradation at extreme pruning levels (below 4 blocks) indicates hard limits to compression efficiency
- Evaluation focuses on regression tasks (energies, forces) rather than diverse molecular properties like reactivity or drug-likeness

## Confidence
- **High Confidence:** The 1.3x inference speedup and 32% parameter reduction from removing 2 blocks (6→4) is well-supported by the experimental results
- **Medium Confidence:** The claim that Sliced MLP initialization is superior to random initialization has strong experimental backing but limited theoretical justification
- **Medium Confidence:** The effectiveness of knowledge distillation in recovering pruned model performance is demonstrated but relies on access to the full pre-trained model and pre-training data

## Next Checks
1. **Architecture Transferability Test:** Apply the same pruning methodology to a different molecular foundation model (e.g., AIMNet or TorchMD-Net) to determine if layer irrelevance is a universal phenomenon or JMP-specific
2. **Task Diversity Evaluation:** Evaluate the pruned models on classification tasks (e.g., drug-likeness, toxicity prediction) and quantum chemistry tasks beyond the QM9 benchmark to assess generalizability of the compression benefits
3. **Training Dynamics Analysis:** Track the fine-tuning trajectories of sliced vs. randomly initialized models to identify whether the performance difference stems from faster convergence or better final minima