---
ver: rpa2
title: Vulnerability Mitigation for Safety-Aligned Language Models via Debiasing
arxiv_id: '2502.02153'
source_url: https://arxiv.org/abs/2502.02153
tags:
- safety
- tsdi
- arxiv
- language
- helpfulness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses vulnerabilities in safety-aligned language
  models that, despite overall safety improvements, still exhibit harmful responses
  in specific categories. The authors propose Token-level Safety-Debiased Inference
  (TSDI), a learning-free method that estimates and corrects safety bias during generation
  using randomly constructed prompts.
---

# Vulnerability Mitigation for Safety-Aligned Language Models via Debiasing

## Quick Facts
- **arXiv ID**: 2502.02153
- **Source URL**: https://arxiv.org/abs/2502.02153
- **Reference count**: 40
- **Primary result**: TSDI improves safety-aligned models' helpfulness while maintaining safety, shifting the Pareto front outward

## Executive Summary
This paper addresses a critical vulnerability in safety-aligned language models: unintended context-free bias that causes over-rejection of harmless prompts. The authors propose Token-level Safety-Debiased Inference (TSDI), a learning-free method that estimates and corrects this bias during generation using randomly constructed prompts. Their experiments show that TSDI significantly improves the safety-helpfulness trade-off by reducing false refusals while maintaining protection against harmful content.

## Method Summary
TSDI operates by estimating position-specific safety bias from randomly constructed prompts and subtracting this bias from model logits during inference. The method constructs synthetic datasets of random token sequences, computes average logit differences between safety-aligned and reference models at each output position, and applies these bias corrections via a LogitProcessor. This approach targets the context-free rejection patterns learned during safety alignment without requiring additional training or modifications to the base model architecture.

## Key Results
- TSDI improves compliance rate from 0.57 to 0.67 on harmless prompts while maintaining safety scores across all categories
- The method is robust across different bias estimation lengths (L=5, 10, 20) and random prompt pool sources
- Safety bias magnitude scales with alignment strength (smaller β/λ increases both safety and bias)
- Adult Content category shows the most significant vulnerability to the safety-helpfulness trade-off

## Why This Works (Mechanism)

### Mechanism 1: Context-Free Safety Bias from Alignment Overfitting
Safety alignment overfits to rejection patterns in the preference dataset, creating a token-level bias that activates regardless of input context. When models are trained with safety constraints (e.g., SACPO), they learn not only which topics are unsafe, but also develop a context-free preference for negative tokens like "I'm sorry," "Unfortunately," and "None." This manifests as elevated logits for these tokens even on randomly constructed, semantically meaningless prompts. The bias observed on random prompts generalizes to real inputs, and removing it does not harm legitimate safety behaviors for truly harmful queries.

### Mechanism 2: Position-Specific Bias Subtraction via Random Prompts
The safety bias is position-dependent and can be estimated and corrected using synthetic data with random tokens. TSDI constructs a synthetic dataset with random token sequences, computes the average logit difference between safety-aligned and reference models at each output position i, and subtracts this bias vector b_i during generation. Random prompts are sufficiently "out-of-distribution" relative to the safety preference dataset that they isolate context-free bias rather than legitimate safety knowledge.

### Mechanism 3: Pareto-Front Improvement via Selective Helpfulness Recovery
Debiasing primarily improves helpfulness on benign prompts while maintaining safety on harmful ones, shifting the safety-helpfulness Pareto frontier outward. By reducing false refusals (over-rejection of harmless prompts), TSDI increases compliance rate and helpfulness win rates. Since the bias is estimated from random prompts (unrelated to actual safety-critical content), safety scores on harmful categories are preserved.

## Foundational Learning

- **Concept**: KL Divergence Penalty in Alignment (β in RLHF/DPO)
  - Why needed: The paper shows smaller β/λ (stronger deviation from reference) increases safety but also increases rejection bias. Understanding this trade-off is essential for diagnosing why TSDI helps.
  - Quick check: If you increase the KL penalty coefficient β during safety alignment, would you expect more or fewer false refusals on harmless prompts?

- **Concept**: Logit-Level Interventions in LLMs
  - Why needed: TSDI operates by modifying output logits before softmax. Understanding how logits translate to token probabilities and generation behavior is prerequisite.
  - Quick check: If you subtract a constant vector b from logits before softmax, how does this change the probability distribution over tokens?

- **Concept**: Overfitting vs. Generalization in Preference Learning
  - Why needed: The paper frames safety bias as an overfitting problem—the model learns rejection patterns that don't generalize correctly. Distinguishing useful safety generalization from harmful overfitting is central.
  - Quick check: A model refuses to answer "How do I make a cake?" with "I'm sorry, I cannot help with illegal activities." Is this a safety failure, overfitting, or both?

## Architecture Onboarding

- **Component map**: [Safety-Aligned Model π_θ] -> [Logit Output f_π(x⊕y_{1:i-1})] -> [Random Prompt Generator] -> [Bias Estimation: b_i = E[f_π - f_π*]] -> [TSDI LogitProcessor: subtract b_i at position i] -> [Softmax → Token Probability → Generation]

- **Critical path**: 1) Train safety-aligned model using DPO/SACPO 2) Construct random token pool 3) Generate random prompt-response pairs 4) Compute bias vectors b_i for each position 5) At inference, subtract b_i via LogitProcessor

- **Design tradeoffs**:
  - Bias estimation length (L): Paper uses L=20; robust for L=5 and L=10. Longer L captures more positional variation but increases estimation cost.
  - Number of random pairs (|D̃|): Paper uses 500. Fewer pairs increase noise; more pairs have diminishing returns.
  - Token pool source: MMLU and MS MARCO both work. Should avoid token pools that overlap with safety preference data distribution.
  - KL strength during alignment (β/λ): Smaller values increase safety but also increase bias magnitude, requiring stronger debiasing.

- **Failure signatures**:
  - Over-debiasing: If b_i is overestimated, legitimate safety refusals on harmful content may be suppressed.
  - Under-debiasing: If bias manifests differently on real inputs vs. random prompts, residual false refusals persist.
  - Generation corruption: Very small β/λ with excessive training iterations causes generation corruption—Greek letters, Russian text.
  - Position mismatch: If inference uses different prompt templates than estimation, b_i values won't align correctly.

- **First 3 experiments**:
  1. Validate bias estimation: Generate 100 random prompts and measure logit differences for rejection tokens vs. neutral tokens.
  2. Ablation on bias length (L): Implement TSDI with L ∈ {5, 10, 20} on harmless prompts and measure compliance rate.
  3. Category-wise safety audit: Apply TSDI and evaluate on SALAD-Bench categories, confirming safety is maintained while helpfulness improves.

## Open Questions the Paper Calls Out

- Can debiasing methods that operate on hidden states or representations outperform token-level debiasing (TSDI) for improving the safety-helpfulness trade-off?
- Why does safety alignment induce context-free biases toward negative tokens, and is this phenomenon fundamental to current alignment paradigms?
- Can vulnerabilities in specific safety categories be predicted before alignment by analyzing pre-trained models or training data?
- Does the safety bias phenomenon generalize across diverse alignment methods (e.g., RLHF, KTO, ORPO) and model architectures beyond Alpaca-7B?

## Limitations

- The core assumption that safety bias estimated from random prompts generalizes to real inputs remains untested.
- The paper doesn't explore whether certain safety-critical categories rely more heavily on initial rejection tokens as their primary defense mechanism.
- TSDI is evaluated as a standalone intervention without examining interactions with other inference-time safety methods.

## Confidence

**High Confidence** (supported by direct experimental evidence):
- TSDI effectively reduces false refusals on harmless prompts while maintaining safety on known harmful categories
- Smaller β/λ coefficients during safety alignment increase both safety scores and rejection bias magnitude
- The method is learning-free and computationally lightweight during inference

**Medium Confidence** (mechanistic claims with supporting evidence but untested assumptions):
- Safety bias is primarily context-free and can be accurately estimated from random prompts
- The bias structure is position-dependent and consistent across different prompt lengths
- Debiasing primarily affects false refusals rather than legitimate safety behaviors

**Low Confidence** (extrapolations beyond tested scope):
- TSDI will generalize to models larger than 7B parameters without modification
- The method will maintain effectiveness as model capabilities and alignment techniques evolve
- Random prompt estimation will work equally well for all safety-critical domains

## Next Checks

1. **Cross-Domain Bias Validation**: Generate prompts spanning multiple safety categories and measure whether bias patterns estimated from random prompts match those observed on real safety-relevant inputs.

2. **Adversarial Prompt Testing**: Systematically test TSDI on prompts designed to exploit safety alignment to measure whether debiasing creates new vulnerabilities.

3. **Multi-Intervention Compatibility Study**: Implement TSDI alongside other inference-time safety methods to evaluate whether combined interventions show additive benefits or interference effects.