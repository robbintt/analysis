---
ver: rpa2
title: 'Beyond Correctness: Harmonizing Process and Outcome Rewards through RL Training'
arxiv_id: '2509.03403'
source_url: https://arxiv.org/abs/2509.03403
tags:
- reasoning
- process
- correct
- arxiv
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving reasoning quality
  in mathematical problem-solving using reinforcement learning. While Outcome Reward
  Models (ORMs) only provide coarse feedback on final answers, they cannot distinguish
  flawed reasoning within correct solutions or valid reasoning within incorrect ones.
---

# Beyond Correctness: Harmonizing Process and Outcome Rewards through RL Training

## Quick Facts
- arXiv ID: 2509.03403
- Source URL: https://arxiv.org/abs/2509.03403
- Reference count: 17
- One-line primary result: PROF improves final accuracy by over 4% compared to blending approaches on mathematical reasoning benchmarks.

## Executive Summary
This paper addresses the challenge of improving reasoning quality in mathematical problem-solving using reinforcement learning. While Outcome Reward Models (ORMs) provide coarse feedback on final answers, they cannot distinguish flawed reasoning within correct solutions or valid reasoning within incorrect ones. Process Reward Models (PRMs) offer fine-grained step-wise feedback but are often noisy and susceptible to reward hacking. The authors propose PROF, a data curation method that harmonizes PRMs and ORMs through consistency-driven sample selection, filtering responses by retaining correct ones with higher averaged process values and incorrect ones with lower averaged process values while maintaining balanced training ratios.

## Method Summary
The paper proposes PROF (PRocess cOnsistency Filter), a data curation method that harmonizes Process Reward Models (PRMs) and Outcome Reward Models (ORMs) for mathematical reasoning via reinforcement learning. PROF operates as a pre-gradient filter within GRPO's rollout-then-update cycle, generating multiple rollouts per prompt and computing trajectory-wise consistency scores by averaging PRM step rewards and multiplying by outcome reward. The method separates correct and incorrect responses, ranks each group by consistency, and removes samples to maintain balanced training data while filtering out inconsistent trajectories where process and outcome signals conflict.

## Key Results
- PROF consistently improves final accuracy by over 4% compared to blending approaches on mathematical reasoning benchmarks
- PROF strengthens intermediate reasoning steps, validated by Monte Carlo estimation and LLM-as-a-judge metrics
- PROF avoids reward hacking and entropy collapse while maintaining stable performance throughout training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Filtering samples where process and outcome signals conflict reduces noisy gradients that mislead training.
- Mechanism: PROF computes a trajectory-wise consistency score by averaging PRM step rewards and multiplying by outcome reward (+1/−1). Samples with low consistency (correct answers with low process scores, or incorrect answers with high process scores) are removed before policy updates.
- Core assumption: PRM step-level scores correlate meaningfully with reasoning validity within outcome-matched groups.

### Mechanism 2
- Claim: Separating correct and incorrect groups before filtering preserves training signal balance and prevents disproportionate removal of negative samples.
- Mechanism: Incorrect responses often contain correct intermediate steps, raising their average PRM scores. Without separation, uniform ranking would over-remove incorrect samples, distorting the correct/incorrect ratio. PROF ranks each group independently and balances final counts.

### Mechanism 3
- Claim: Using PRM only for ranking/filtering (not in the loss) avoids reward hacking and entropy collapse observed when blending PRM into the objective.
- Mechanism: Blend methods add weighted PRM scores to outcome rewards, creating exploitable gradients for verbose or repetitive outputs. PROF decouples PRM influence—PRM determines which samples enter training, but policy gradients come only from outcome rewards.

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: PROF operates as a pre-gradient filter within GRPO's rollout-then-update cycle; understanding GRPO's advantage computation (standardizing rewards within groups) clarifies why outcome-only signals are limiting and where PROF inserts process awareness.
  - Quick check question: Can you explain why GRPO removes the need for a separate value network?

- Concept: **Process Reward Models (PRMs) vs Outcome Reward Models (ORMs)**
  - Why needed here: The entire PROF framework hinges on PRM-ORM complementarity—ORM provides sparse but accurate outcome labels; PRM provides dense but noisy step-level scores. Recognizing this tradeoff is prerequisite to understanding why filtering (not blending) is the proposed integration strategy.
  - Quick check question: What failure mode does the paper identify when PRM scores are naively added to outcome rewards?

- Concept: **Reward Hacking in RLHF/RLVR**
  - Why needed here: PROF's core motivation is robustness to reward hacking (exploitation of imperfect reward models). Recognizing symptoms—entropy collapse, length inflation, gaming via repetitive outputs—helps interpret the ablation results and design monitoring.
  - Quick check question: In Figure 3, what two behavioral signatures indicate Blend-PRM-GRPO is reward hacking?

## Architecture Onboarding

- Component map: Rollout Generator -> Outcome Verifier + PRM Scorer -> PROF Filter -> GRPO Updater
- Critical path: Rollout → Outcome + PRM scoring → PROF filter → GRPO update. Latency dominated by PRM inference (H steps per response × n responses × batch size).
- Design tradeoffs:
  - Rollout size n: Higher n improves ranking granularity but increases PRM cost. Paper tests n ∈ {4, 8, 12, 16}; performance peaks then drops.
  - Filter-Correct vs Filter-Both: When PRM is less reliable, filtering only correct samples is more robust; when PRM is well-matched, filtering both groups is more efficient.
  - Regularization λ: Penalizes degenerate step counts (H=1 or H≥Hλ). Required to suppress edge-case gaming.

- Failure signatures:
  - Entropy → 0 rapidly: indicates reward hacking (should not happen with PROF; if observed, check PRM calibration or filter logic)
  - Response length growing unboundedly: PRM may be rewarding verbosity; verify step segmentation and λ regularization
  - Training reward gap pre/post filter > 2%: suggests incorrect group imbalance; verify separation logic

- First 3 experiments:
  1. Reproduce GRPO baseline on a small math benchmark (e.g., Math500 subset) with outcome-only rewards; log accuracy, entropy, and response length over training steps.
  2. Implement PROF filter logic (Algorithm 1) with a pretrained PRM (e.g., Qwen2.5-Math-PRM-7B). Start with n=8, m=4, λ=10, Hλ=30. Compare training dynamics (entropy, length, accuracy) against GRPO and Blend-PRM-GRPO baselines.
  3. Ablate separation by running PROF without splitting G+/G− (rank all samples together). Log the reward gap pre/post filter. If gap > 2%, the ablation should underperform.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the PROF framework be effectively generalized to complex reasoning domains beyond mathematics, such as coding or web navigation?
- Basis in paper: [explicit] The conclusion states that "how to extend our method to other reasoning tasks, such as coding... and web navigation also deserves to be explored."
- Why unresolved: The current experimental scope is strictly limited to mathematical benchmarks, which may have different step-wise consistency structures than code execution traces.
- What evidence would resolve it: Successful application of PROF on reasoning benchmarks like SWE-bench or WebArena, demonstrating improved process consistency and outcome accuracy.

### Open Question 2
- Question: How does PROF perform when integrated with more advanced or diverse Process Reward Models (PRMs) rather than a single pre-trained instance?
- Basis in paper: [explicit] The authors note that "Exploring the integration of PROF with more accurate or diverse PRMs remains an interesting direction for future work."
- Why unresolved: The study primarily relies on Qwen2.5-Math-PRM-7B, leaving the framework's robustness against other PRM architectures unverified.
- What evidence would resolve it: Experiments combining PROF with various PRM architectures, including generative or online-updating reward models, showing maintained or improved performance.

### Open Question 3
- Question: Can the selection between filtering strategies (Filter-Both vs. Filter-Correct) be automated based on real-time estimates of PRM reliability?
- Basis in paper: [inferred] Section 5.4 concludes that "Filter-Correct" is better for less reliable PRMs while "Filter-Both" suits reliable ones, implying a need for dynamic selection.
- Why unresolved: The paper currently relies on manual heuristic selection based on the specific base model rather than an adaptive mechanism.
- What evidence would resolve it: Development of an adaptive algorithm that monitors PRM confidence or distribution shift during training to switch filtration modes automatically.

## Limitations
- PROF's effectiveness hinges on the PRM providing meaningful step-level signals; if the PRM is poorly calibrated or mismatched, filtering may discard valid reasoning or retain flawed patterns.
- All experiments focus on mathematical reasoning, leaving the framework's generalizability to other structured reasoning tasks (e.g., code generation) untested.
- The optimal hyperparameters (rollout size n=8, filter count m=4, penalty λ=10) are empirically chosen and may require retuning for different PRM models or base policies.

## Confidence
- **High**: PROF outperforms naive blending (Blend-PRM-GRPO) on all tested benchmarks; avoids reward hacking symptoms (entropy collapse, length explosion).
- **Medium**: Separation of correct/incorrect groups is necessary to prevent class imbalance in filtered data (supported by ablation).
- **Low**: PROF's robustness to PRM miscalibration is demonstrated but only through limited cross-model comparisons; more extensive PRM ablation studies would strengthen this claim.

## Next Checks
1. **PRM ablation study**: Train PROF with progressively noisier or misaligned PRMs (e.g., PRMs trained on different domains or with injected noise). Measure accuracy degradation and compare against baselines to quantify PROF's robustness threshold.

2. **Cross-domain generalization**: Apply PROF to a non-mathematical structured reasoning task (e.g., multi-step code generation or logical reasoning). Compare performance against GRPO and Blend-PRM-GRPO to test domain transferability.

3. **Hyperparameter scaling**: Systematically vary n (rollout size) and m (filter count) across a wider range (e.g., n ∈ {4, 8, 16, 32}, m ∈ {2, 4, 8, 16}). Plot accuracy vs. computational cost (PRM inference time) to identify Pareto-optimal operating points.