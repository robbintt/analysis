---
ver: rpa2
title: "Deep Reinforcement Learning for Dynamic Algorithm Configuration: A Case Study\
  \ on Optimizing OneMax with the (1+($\u03BB$,$\u03BB$))-GA"
arxiv_id: '2512.03805'
source_url: https://arxiv.org/abs/2512.03805
tags:
- reward
- learning
- problem
- policy
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates the application of deep reinforcement\
  \ learning (RL) to dynamic algorithm configuration (DAC), focusing on controlling\
  \ the population size parameter of the (1+(\u03BB,\u03BB))-GA on OneMax instances.\
  \ Two fundamental challenges\u2014scalability degradation and learning instability\u2014\
  were identified as limiting the effectiveness of DDQN and PPO algorithms in DAC."
---

# Deep Reinforcement Learning for Dynamic Algorithm Configuration: A Case Study on Optimizing OneMax with the (1+($λ$,$λ$))-GA

## Quick Facts
- **arXiv ID:** 2512.03805
- **Source URL:** https://arxiv.org/abs/2512.03805
- **Reference count:** 34
- **Key outcome:** DDQN with adaptive reward shifting outperforms PPO and achieves performance comparable to theoretically derived policies with significantly improved sample efficiency on OneMax-DAC benchmark.

## Executive Summary
This study investigates deep reinforcement learning (RL) for dynamic algorithm configuration (DAC), specifically controlling the population size parameter λ of the (1+(λ,λ))-GA on OneMax instances. The research identifies two fundamental challenges—scalability degradation and learning instability—that limit the effectiveness of DDQN and PPO algorithms in DAC environments. An adaptive reward shifting mechanism is introduced to enhance exploration by leveraging reward distribution statistics, eliminating the need for instance-specific hyperparameter tuning. The DDQN algorithm with adaptive reward shifting achieves performance comparable to theoretically derived policies with significantly improved sample efficiency, outperforming prior DAC approaches by several orders of magnitude.

## Method Summary
The study applies deep RL to DAC by controlling the population size parameter λ of the (1+(λ,λ))-GA on OneMax optimization instances. The state space consists of the current fitness value f(x), while the action space comprises discrete λ values as powers of two. Three reward functions are evaluated: naïve (reward = fitness improvement minus expected improvement), scaled (normalized by problem size), and adaptive shifted (incorporating reward distribution statistics from warm-up episodes). DDQN is implemented with ε-greedy exploration, replay buffer, and soft target updates, while PPO uses standard Stable Baselines configurations. Training occurs over 500K steps with evaluation every 2K steps across 100 random seeds. Performance is measured using Expected Runtime (ERT), Hitting Rate (HR), and Area Under Curve (AUC) compared against theoretically derived optimal policies.

## Key Results
- DDQN with adaptive reward shifting achieved performance comparable to theoretically derived policies π_disc with significantly improved sample efficiency
- PPO demonstrated fundamental variance issues that prevented effective learning even with extensive hyperparameter optimization
- The adaptive reward shifting mechanism eliminated the need for instance-specific hyperparameter tuning while improving exploration

## Why This Works (Mechanism)
The effectiveness stems from addressing two core challenges in DAC: scalability degradation (where learned policies fail on larger problem instances) and learning instability (premature convergence to suboptimal policies). The adaptive reward shifting mechanism enhances exploration by incorporating reward distribution statistics, preventing under-exploration that leads to poor policy generalization. For DDQN, using an undiscounted learning rate (γ=1.0) resolves the planning horizon coverage issue, allowing the agent to effectively learn long-term dependencies in the DAC environment.

## Foundational Learning
- **Dynamic Algorithm Configuration (DAC):** Automated adjustment of algorithm parameters during optimization runs. Why needed: Enables algorithms to adapt to problem instance characteristics dynamically. Quick check: Verify the MDP formulation correctly captures state transitions and reward structure.
- **Expected Runtime (ERT):** Expected number of fitness evaluations until optimal solution is found. Why needed: Primary performance metric for optimization algorithms. Quick check: Ensure ERT calculations use consistent cutoff times and handling of unsolved instances.
- **Adaptive Reward Shifting:** Modification of reward function using reward distribution statistics (mean, quartiles) from warm-up episodes. Why needed: Enhances exploration by preventing reward scale collapse. Quick check: Verify R̄, Q₁, Q₃ are computed from the same warm-up distribution used in training.
- **Planning Horizon Coverage:** Agent's ability to learn long-term dependencies in sequential decision problems. Why needed: Critical for DAC where decisions affect future algorithm behavior. Quick check: Monitor policy performance across different problem sizes to detect scalability issues.
- **Undiscounted Learning (γ=1.0):** Using full future reward credit in temporal difference updates. Why needed: Prevents truncation of reward credit in long-horizon DAC problems. Quick check: Compare learning curves with discounted vs undiscounted settings.

## Architecture Onboarding
**Component Map:** Environment (OneMax + (1+(λ,λ))-GA) -> RL Agent (DDQN/PPO) -> Policy Evaluation -> Performance Metrics
**Critical Path:** Environment state observation → Action selection → Algorithm execution → Reward calculation → Policy update → New state observation
**Design Tradeoffs:** Simplified scalar state (fitness only) vs richer population statistics; Model-free RL (sample efficiency) vs model-based approaches (planning capability); Undiscounted learning (horizon coverage) vs discounted learning (stability).
**Failure Signatures:** Learning stagnation (near-zero pairwise policy differences indicating under-exploration); Scalability degradation (declining hitting rates on larger problem instances); Variance explosion (unstable policy updates in PPO).
**First Experiments:** 1) Train DDQN on n=100 with γ=1.0 and naïve reward to establish baseline performance; 2) Apply adaptive reward shifting and compare ERT against theoretical policy π_disc; 3) Test scalability by evaluating policies on n=200, 500, and 1000 instances.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can model-based reinforcement learning approaches improve sample efficiency and performance in Dynamic Algorithm Configuration compared to the model-free DDQN method? [explicit] The conclusion states that model-based approaches "remain largely under-investigated in DAC and may offer promising avenues for improving learning's sample efficiency." Why unresolved: This study focused exclusively on evaluating model-free algorithms (DDQN and PPO) and did not test model-based paradigms. Evidence: Empirical results showing a model-based agent achieving comparable or superior policy quality to DDQN with fewer environment interactions on the OneMax-DAC benchmark.

- **Open Question 2:** How do deep RL-based DAC methods perform when applied to richer state representations, such as raw populations or diverse internal statistics, rather than scalar fitness values? [explicit] The authors restrict the state space to fitness to utilize ground truth but explicitly list "richer state representations (e.g., incorporating additional statistics or raw populations)" as a direction for future work. Why unresolved: The current study deliberately simplified the state space to maintain a controlled environment for analyzing algorithmic challenges. Evidence: Successful training and convergence of RL agents on DAC benchmarks where the input state includes high-dimensional population data without losing the sample efficiency demonstrated in the simplified setting.

- **Open Question 3:** Can specific algorithmic modifications resolve the fundamental variance and convergence issues inherent to PPO in DAC environments, or is PPO inherently unsuitable for this domain? [explicit] The paper concludes PPO faces "fundamental variance issues that necessitate alternative algorithmic designs" and notes that even extensive hyperparameter optimization failed to identify effective policies. Why unresolved: Standard solutions like entropy regularization and reward scaling failed to stabilize PPO, and the authors did not identify a specific modification to overcome these variance issues. Evidence: A modified PPO variant or policy-gradient method demonstrating stable learning curves and competitive ERT on the OneMax-DAC benchmark.

## Limitations
- Conclusions are based on a single optimization problem (OneMax) and one specific algorithm ((1+(λ,λ))-GA), limiting generalizability to other DAC problems and evolutionary algorithms
- The adaptive reward shifting mechanism relies on specific reward distribution statistics that may not translate directly to problems with different fitness landscapes or optimization dynamics
- Performance comparisons are limited to specific problem instances and may not reflect behavior on more complex optimization landscapes

## Confidence
- **High confidence**: The identification of scalability degradation and learning instability as fundamental challenges in DAC with deep RL. The experimental evidence showing DDQN's superior sample efficiency compared to PPO is robust across problem sizes.
- **Medium confidence**: The effectiveness of adaptive reward shifting as a general solution for exploration problems in DAC. While well-validated on OneMax, its performance on other optimization landscapes requires further investigation.
- **Medium confidence**: The claim that DDQN with adaptive reward shifting achieves performance comparable to theoretically derived policies. The gap measurements are reliable, but the comparison is limited to specific problem instances.

## Next Checks
1. Test the adaptive reward shifting mechanism on alternative benchmark problems (e.g., LeadingOnes, Linear Fitness Landscape) to evaluate generalizability across different fitness landscapes
2. Implement and evaluate alternative DAC approaches (e.g., Bayesian optimization, evolutionary strategies) on the same OneMax problem to provide comparative benchmarks against deep RL methods
3. Conduct ablation studies on the (1+(λ,λ))-GA configuration space by varying mutation operators and selection mechanisms to assess robustness of the learned policies across algorithm variants