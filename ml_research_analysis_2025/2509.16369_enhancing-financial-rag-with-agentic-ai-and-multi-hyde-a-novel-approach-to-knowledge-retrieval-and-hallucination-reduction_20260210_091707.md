---
ver: rpa2
title: 'Enhancing Financial RAG with Agentic AI and Multi-HyDE: A Novel Approach to
  Knowledge Retrieval and Hallucination Reduction'
arxiv_id: '2509.16369'
source_url: https://arxiv.org/abs/2509.16369
tags:
- retrieval
- financial
- arxiv
- multi-hyde
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Multi-HyDE, a multi-perspective retrieval
  mechanism that generates multiple, non-equivalent hypothetical documents to improve
  coverage and accuracy in financial question answering. By integrating Multi-HyDE
  with BM25 keyword retrieval and an agentic pipeline capable of multi-hop reasoning
  and tool usage, the approach achieves 11.2% higher accuracy and 15% lower hallucination
  rates compared to baseline methods.
---

# Enhancing Financial RAG with Agentic AI and Multi-HyDE: A Novel Approach to Knowledge Retrieval and Hallucination Reduction

## Quick Facts
- **arXiv ID**: 2509.16369
- **Source URL**: https://arxiv.org/abs/2509.16369
- **Reference count**: 27
- **Primary result**: 11.2% higher accuracy and 15% lower hallucination rates compared to baseline methods on financial QA benchmarks

## Executive Summary
This paper introduces Multi-HyDE, a multi-perspective retrieval mechanism that generates multiple non-equivalent hypothetical document queries to improve coverage and accuracy in financial question answering. By integrating Multi-HyDE with BM25 keyword retrieval and an agentic pipeline capable of multi-hop reasoning and tool usage, the approach addresses key limitations in financial RAG systems where semantic similarity alone fails to distinguish between temporally similar but numerically different passages. The framework demonstrates significant improvements in both accuracy and hallucination reduction when evaluated on FinanceBench and ConvFinQA benchmarks.

## Method Summary
The proposed framework combines Multi-HyDE with hybrid dense-sparse retrieval and an agentic pipeline. Multi-HyDE generates multiple non-equivalent queries from a single input question, creating diverse hypothetical documents that capture different aspects of the information need. These are combined with BM25 retrieval specifically for tables to ensure exact matching of numerical data. The agentic pipeline uses structured reasoning to decompose complex queries, select appropriate tools (web search, financial APIs, calculator), and iteratively verify intermediate results before synthesis. The system is evaluated on financial documents including SEC 10-K filings, achieving 11.2% accuracy improvement and 15% hallucination reduction over baseline methods.

## Key Results
- 11.2% accuracy improvement over baseline methods on FinanceBench and ConvFinQA benchmarks
- 15% reduction in hallucination rates as measured by RAGAS metrics
- Enhanced retrieval reliability through multi-perspective query generation
- Improved handling of temporal disambiguation in financial documents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generating multiple non-equivalent hypothetical document queries improves retrieval coverage without increasing token costs over standard HyDE
- Mechanism: Multi-HyDE uses an LLM to generate contextually related but distinct queries (e.g., "fraud investigations by Company A" AND "criminal cases by Company A") rather than semantically similar ones. Each query produces a hypothetical document embedding; results are concatenated and reranked, surfacing complementary information that similar queries would miss
- Core assumption: Financial documents contain information responsive to multiple distinct query formulations within shared contexts
- Evidence anchors: Abstract mentions "generates multiple, nonequivalent queries to boost the effectiveness and coverage of retrieval"; Section 3.1 Algorithm 1 describes query decomposition into atomic steps

### Mechanism 2
- Claim: Hybrid dense-sparse retrieval disambiguates semantically similar financial passages that differ in critical numerical or temporal details
- Mechanism: Dense retrieval captures semantic relationships but conflates passages with similar phrasing across years. BM25 provides exact keyword matching for structured data (tables) and temporal/numerical precision. Combined results are reranked
- Core assumption: Financial reports across years contain near-identical phrasing with materially different figures that require exact matching
- Evidence anchors: Abstract integrates "dense and sparse retrieval (including BM25 for tables)"; Appendix C shows SEC filings from 2015 vs. 2017 with nearly identical amortization language but different dollar values

### Mechanism 3
- Claim: A state-governed agentic pipeline with iterative tool calls reduces hallucinations by validating intermediate results before synthesis
- Mechanism: The agent decomposes queries into atomic steps, selects appropriate tools (retrieval, web search, financial APIs, calculator), evaluates outputs at each stage, and adapts plans when evidence is insufficient. This prevents the model from filling gaps with parametric knowledge
- Core assumption: Complex financial queries can be decomposed into resolvable sub-queries with verifiable intermediate outputs
- Evidence anchors: Abstract mentions "multi-stage agentic pipeline for dynamic reasoning and verification... reduces hallucinations by 15%"; Section 3.2 Algorithm 2 describes breaking down queries into atomic steps

## Foundational Learning

- **RAG (Retrieval-Augmented Generation)**
  - Why needed here: Base paradigm this work extends; understanding why LLMs hallucinate without external grounding is prerequisite
  - Quick check question: Why does semantic similarity alone fail to distinguish a 2015 10-K from a 2017 10-K with identical phrasing?

- **HyDE (Hypothetical Document Embeddings)**
  - Why needed here: Multi-HyDE extends this; must understand how synthetic answers improve query-document alignment
  - Quick check question: What does HyDE assume about the relationship between query phrasing and document phrasing?

- **Sparse vs. Dense Retrieval (BM25 vs. Vector Search)**
  - Why needed here: The hybrid approach depends on understanding when exact matching outperforms semantic similarity
  - Quick check question: For a query asking "revenue growth 2023 to 2024," which retriever would you trust for the exact figures?

## Architecture Onboarding

- **Component map**: User query -> Query clarification -> Multi-HyDE + BM25 parallel retrieval -> Concatenate + rerank -> Agent evaluates sufficiency -> If insufficient: decompose into sub-queries, invoke tools -> Synthesize final answer
- **Critical path**: User query → Query clarification (if ambiguous) → Multi-HyDE + BM25 parallel retrieval → Concatenate + rerank → Agent evaluates sufficiency → If insufficient: decompose into sub-queries, invoke tools → Synthesize final answer
- **Design tradeoffs**: BGE reranker more accurate than MiniLM but significantly slower (Table 4); Tool calling adds resiliency but doesn't improve raw accuracy (ablation shows similar scores); Full PDF indexing (vs. evidence pages only) better simulates production but prevents comparison with methods using curated context
- **Failure signatures**: Low ROUGE scores on numerical answers (expected—ground truth is single numbers while models explain reasoning); RAGAS gives high Faithfulness to refusals (Appendix F.2); Retrieval returns wrong year's filing for temporal queries without BM25
- **First 3 experiments**: 
  1. Baseline comparison: Standard HyDE vs. Multi-HyDE on FinanceBench subset; measure Recall and Factual Correctness (expected: +0.2-0.3 recall improvement per Table 2)
  2. Hybrid retrieval ablation: Multi-HyDE alone vs. Multi-HyDE + BM25 on temporal/numerical queries; manually verify year disambiguation
  3. Agent loop validation: Enable/disable tool calling on complex multi-hop queries; measure reliability (confident correct answers / total confident answers)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Small Language Models (SLMs) fine-tuned via parameter-efficient techniques (like LoRA) match the performance of large closed-source models for specific agentic tasks like hypothetical document generation in financial RAG?
- Basis in paper: [explicit] The "Future Work" section suggests investigating SLMs fine-tuned with LoRA to replace large models for tasks like query re-writing
- Why unresolved: The current implementation relies on GPT-4o mini; it is unknown if smaller, fine-tuned models can handle the specific linguistic nuances of financial reports without significant performance degradation
- What evidence would resolve it: A comparative study benchmarking the retrieval accuracy and token efficiency of LoRA-tuned SLMs against GPT-4o mini within the Multi-HyDE framework

### Open Question 2
- Question: How can evaluation metrics be improved to accurately assess numerical precision and factual correctness in financial RAG systems without relying on flawed LLM-based judges?
- Basis in paper: [explicit] The "Limitations" and "Future Work" sections note that current LLM-based evaluation (e.g., RAGAS) fails on numerical examples, often penalizing correct answers that are more precise than ground truth or failing to catch confident hallucinations
- Why unresolved: RAGAS and similarity metrics do not correlate well with human evaluation for financial QA (high false positives/negatives)
- What evidence would resolve it: A new metric or evaluation protocol that demonstrates high correlation with human expert evaluation on the FinanceBench dataset, specifically handling numerical tolerance and evidence attribution

### Open Question 3
- Question: Does the Multi-HyDE approach generalize effectively to other high-stakes domains with complex document structures, such as legal or medical records, without requiring domain-specific architectural changes?
- Basis in paper: [inferred] The abstract claims the framework is "modular, adaptable" and applicable to "high-stakes contexts," yet the evaluation is strictly limited to financial data
- Why unresolved: The method is optimized for financial reports (numerical focus, specific table structures); it is unproven whether generating "non-equivalent" queries helps or hinders in domains with different ambiguity profiles
- What evidence would resolve it: Evaluation results from applying the unmodified pipeline to standard legal or medical QA benchmarks (e.g., LexGLUE or MedQA)

### Open Question 4
- Question: Do the reported improvements in hallucination reduction and accuracy persist when the system is scaled to evaluate the complete FinanceBench and ConvFinQA datasets?
- Basis in paper: [explicit] The "Limitations" section states the evaluation was conducted on a "relatively small dataset" due to resource constraints, which "may limit the generalizability of the results"
- Why unresolved: The 11.2% accuracy improvement was observed on a subset; statistical significance and robustness across the full distribution of financial queries remain unverified
- What evidence would resolve it: Full-scale experimental results replicating the reported metrics across the entirety of the FinanceBench and ConvFinQA corpora

## Limitations

- The study evaluates on relatively small benchmark subsets (150 annotated examples) rather than full benchmark suites
- Exact number of non-equivalent queries (N) in Multi-HyDE and specific LLM prompting strategies remain unspecified
- BM25 configuration details for table retrieval and the aggregation logic for financial tables are not fully detailed

## Confidence

- **High Confidence**: The hybrid dense-sparse retrieval mechanism's effectiveness in disambiguating temporally similar financial passages is well-supported by concrete examples
- **Medium Confidence**: The 11.2% accuracy improvement claim, while methodologically sound, is based on limited sample sizes and human evaluation which introduces subjectivity
- **Medium Confidence**: The 15% hallucination reduction is supported by RAGAS metrics but requires careful interpretation given the noted limitations of automated hallucination detection

## Next Checks

1. Replicate the temporal disambiguation experiment with SEC filings from different years using only semantic similarity vs. hybrid retrieval to verify year separation accuracy
2. Implement Multi-HyDE with varying values of N (3, 5, 7) to empirically determine the optimal number of non-equivalent queries for coverage vs. precision trade-offs
3. Conduct ablation studies removing the agentic tool-calling component on complex multi-hop queries to quantify its contribution to hallucination reduction vs. raw accuracy