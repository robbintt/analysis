---
ver: rpa2
title: Unveiling Latent Knowledge in Chemistry Language Models through Sparse Autoencoders
arxiv_id: '2512.08077'
source_url: https://arxiv.org/abs/2512.08077
tags:
- features
- feature
- molecules
- molecular
- chemical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper applies sparse autoencoders (SAEs) to the SMI-TED chemistry
  foundation model, decomposing its 768-dimensional molecular embeddings into interpretable,
  sparse features. This is the first application of SAE techniques to chemistry language
  models.
---

# Unveiling Latent Knowledge in Chemistry Language Models through Sparse Autoencoders
## Quick Facts
- arXiv ID: 2512.08077
- Source URL: https://arxiv.org/abs/2512.08077
- Authors: Jaron Cohen; Alexander G. Hasson; Sara Tanovic
- Reference count: 40
- First application of SAE techniques to chemistry language models, decomposing molecular embeddings into interpretable, sparse features

## Executive Summary
This paper introduces sparse autoencoders (SAEs) to chemistry language models, specifically applying them to the SMI-TED model to decompose 768-dimensional molecular embeddings into interpretable, sparse features. This pioneering approach demonstrates that SAE features outperform individual neurons in detecting chemical substructures while maintaining high reconstruction fidelity (94.7% stereo accuracy). The authors show these features correlate strongly with physicochemical descriptors, are more compact and less redundant than neuron-based analyses, and can group molecules by shared pharmacological functions despite weak structural similarity.

The work establishes SAEs as a powerful framework for uncovering latent chemical knowledge in AI models, advancing interpretability and control in computational chemistry. Through causal steering experiments, the authors confirm these features encode functional chemical information, enabling targeted molecular modifications. This represents a significant advance in making chemistry language models more interpretable and controllable, with potential applications in drug discovery and molecular design.

## Method Summary
The authors apply sparse autoencoders to the SMI-TED chemistry foundation model, which processes SMILES representations of molecules. The SAE architecture decomposes the model's 768-dimensional molecular embeddings into a smaller set of interpretable, sparse features. Training involves optimizing reconstruction accuracy while enforcing sparsity through L1 regularization. The extracted features are then analyzed for their ability to detect chemical substructures, correlate with physicochemical properties, and enable functional molecular modifications through causal steering experiments.

## Key Results
- SAE features outperform individual neurons in detecting chemical substructures while achieving 94.7% stereo accuracy reconstruction
- Single features can group molecules by shared pharmacological functions (e.g., opioid receptor activity) despite weak structural similarity
- Features correlate strongly with physicochemical descriptors while being more compact and less redundant than neuron-based analyses
- Causal steering experiments confirm features encode functional chemical information, enabling targeted molecular modifications

## Why This Works (Mechanism)
Sparse autoencoders work by learning to compress high-dimensional molecular embeddings into a smaller set of interpretable features while maintaining reconstruction accuracy. The L1 regularization enforces sparsity, ensuring each feature activates only for specific chemical patterns. This decomposition reveals latent chemical knowledge that individual neurons cannot capture, as features can represent complex functional relationships rather than just local substructures. The causal steering experiments demonstrate that these features encode meaningful chemical information that can be manipulated to achieve desired molecular properties.

## Foundational Learning
- **SMILES representation**: Why needed - Standard text-based molecular encoding; Quick check - Can represent any organic molecule
- **Sparse autoencoders**: Why needed - Extract interpretable features from high-dimensional embeddings; Quick check - L1 regularization enforces feature sparsity
- **Causal steering**: Why needed - Validate that features encode functional chemical information; Quick check - Manipulating feature activation changes molecular properties
- **Physicochemical descriptors**: Why needed - Benchmark feature interpretability against established chemical properties; Quick check - Correlation with descriptors validates feature meaning
- **Reconstruction fidelity**: Why needed - Ensure compressed features retain essential molecular information; Quick check - High stereo accuracy (94.7%) demonstrates preservation

## Architecture Onboarding
Component map: SMI-TED model -> 768D embeddings -> Sparse Autoencoder -> Interpretable features -> Analysis/Causal steering

Critical path: The SAE layer sits between the frozen SMI-TED model and the analysis pipeline. Input molecules are encoded by SMI-TED, then compressed by the SAE into sparse features that are analyzed for substructure detection, property correlation, and causal manipulation.

Design tradeoffs: The authors chose 3,000 features to balance interpretability with reconstruction accuracy. More features would improve reconstruction but reduce sparsity and interpretability. The L1 regularization strength was tuned to enforce meaningful sparsity without losing essential chemical information.

Failure signatures: Poor reconstruction fidelity indicates insufficient feature capacity. Features that don't correlate with known properties suggest the SAE isn't capturing meaningful chemical patterns. Inability to perform causal steering would indicate features don't encode functional information.

First experiments:
1. Test feature reconstruction accuracy on held-out molecular structures
2. Verify feature correlations with established physicochemical descriptors
3. Perform small-scale causal steering to confirm functional encoding

## Open Questions the Paper Calls Out
The paper acknowledges several limitations including the focus on a single model (SMI-TED) and architecture (SMILES-based), uncertainty about generalization to other molecular representations or chemistry models, and the need for broader validation of causal steering experiments across diverse chemical scaffolds.

## Limitations
- Results are limited to a single model (SMI-TED) and SMILES-based architecture
- Causal steering experiments involve relatively small sample sizes (5 examples per task)
- Computational scalability and feature coverage for larger models or underrepresented chemical classes remain unexplored

## Confidence
- High confidence in SAE reconstruction accuracy and comparison to single neurons
- Medium confidence in feature interpretability and correlation with physicochemical properties
- Medium confidence in causal steering effectiveness for molecular modification
- Low confidence in generalizability to other models and broader chemical space

## Next Checks
1. Test SAE feature transferability by applying the same learned features to a different chemistry model or molecular representation
2. Conduct large-scale causal steering experiments across diverse chemical scaffolds with quantitative activity measurements
3. Evaluate feature coverage by testing whether SAE-extracted features can reconstruct molecules from underrepresented chemical classes