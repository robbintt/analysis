---
ver: rpa2
title: 'SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of LMMs
  on Multi-modal Scientific Problems'
arxiv_id: '2503.10627'
source_url: https://arxiv.org/abs/2503.10627
tags:
- lmms
- scientific
- knowledge
- input
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SciVerse introduces a multi-modal benchmark with 5,735 instances
  across five versions to assess scientific reasoning in LMMs, focusing on knowledge
  comprehension, visual perception, and CoT reasoning. The benchmark transforms each
  problem into three knowledge-depth variants (free, lite, rich) and two visual variants
  (rich, only).
---

# SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of LMMs on Multi-modal Scientific Problems

## Quick Facts
- arXiv ID: 2503.10627
- Source URL: https://arxiv.org/abs/2503.10627
- Authors: Ziyu Guo; Ray Zhang; Hao Chen; Jialin Gao; Dongzhi Jiang; Jiaze Wang; Pheng-Ann Heng
- Reference count: 22
- Key outcome: Introduces a multi-modal benchmark with 5,735 instances across five versions to assess scientific reasoning in LMMs, showing closed-source models outperform open-source in knowledge and visual interpretation, but both struggle with Vision-only problems.

## Executive Summary
SciVerse presents a comprehensive multi-modal benchmark to evaluate Large Language Models' (LMMs) capabilities in scientific reasoning, specifically targeting knowledge comprehension, visual perception, and Chain-of-Thought (CoT) reasoning quality. The benchmark transforms each scientific problem into five distinct versions varying in knowledge depth (free, lite, rich) and visual presentation (rich, only), creating 5,735 test instances across Physics, Chemistry, and Biology. A novel step-wise CoT evaluation strategy analyzes intermediate reasoning steps separately for knowledge and logical errors, revealing that while closed-source models demonstrate superior overall performance, both closed and open-source models struggle significantly with Vision-only problems where all information is embedded in diagrams.

## Method Summary
SciVerse curates 1,147 scientific problems from existing benchmarks (SceMQA, MMMU, CMMMU) and transforms each into five versions: three knowledge variants (free, lite, rich) and two visual variants (rich, only), totaling 5,735 instances. Models are evaluated zero-shot using two prompting strategies: direct answer generation for binary accuracy, and step-by-step reasoning for CoT evaluation. A GPT-4o-based two-stage pipeline categorizes intermediate reasoning steps into knowledge review and logical deduction, then scores each step for correctness. The framework computes binary accuracy (Acc) and two step-wise scores (Sci-CoT_K for knowledge, Sci-CoT_L for logic) along with their average (Sci-CoT), enabling granular analysis of where reasoning fails.

## Key Results
- Closed-source LMMs (GPT-4V, Gemini 1.5 Pro, Claude 3 Opus) outperform open-source models (LLaVA-Next, LLaVA-1.5, Qwen2-VL-7B-32) across all metrics and variants
- Largest performance gap occurs between Vision-rich and Vision-only versions, indicating significant visual perception and OCR limitations
- Closed-source models show stable performance across knowledge variants, suggesting stronger internal expertise compared to open-source models
- CoT analysis reveals intermediate steps are often correct even when final answers are wrong, particularly for closed-source models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Varying embedded knowledge depth isolates knowledge comprehension from reasoning ability in LMMs.
- Mechanism: By transforming each problem into Knowledge-free (no background), Knowledge-lite (theorem names only), and Knowledge-rich (full equations and explanations), performance deltas between versions reveal whether failures stem from missing domain expertise or flawed reasoning chains.
- Core assumption: Models that improve significantly from Knowledge-free to Knowledge-rich lack internal expertise, while stable high performers possess sufficient parametric knowledge.
- Evidence anchors:
  - [abstract] "transform each problem into three versions containing different levels of knowledge required for solving"
  - [section 3.2] "closed-source LMMs display relatively stable results across all three versions... suggesting that these models inherently possess a greater depth of expertise knowledge"
  - [corpus] MME-CoT paper corroborates that CoT evaluation requires separating reasoning quality from knowledge retrieval, though not directly validating this specific transformation approach.

### Mechanism 2
- Claim: Shifting information from text to visual modality stress-tests cross-modal integration and OCR robustness.
- Mechanism: Vision-rich embeds problem conditions in diagrams; Vision-only embeds the entire question visually. Performance drops from text-heavy to vision-heavy versions expose weaknesses in visual encoding and text-diagram integration pipelines.
- Core assumption: Scientific diagrams require specialized visual perception beyond general image understanding, and OCR errors cascade into reasoning failures.
- Evidence anchors:
  - [section 2.3] "Vision-only problems present the most challenging evaluation for LMMs, which assess their capabilities in knowledge comprehension, OCR, and visual perception"
  - [section 3.2] "The largest performance drop is observed between Vision-rich and Vision-only versions for both closed-source and open-source LMMs"
  - [corpus] LIRA paper identifies "weak visual comprehension" as a key limitation in current LMMs, supporting the hypothesis that fine-grained visual perception remains underdeveloped.

### Mechanism 3
- Claim: Step-wise CoT evaluation with separate knowledge and logic scoring captures partial competence missed by binary accuracy.
- Mechanism: GPT-4o categorizes reasoning steps into Knowledge Review (quoting theorems, recalling facts) and Logical Deduction (calculations, inferences), then scores each step. This produces Sci-CoT_K and Sci-CoT_L scores that reveal whether final-answer errors stem from knowledge gaps, calculation mistakes, or inference failures.
- Core assumption: Intermediate step correctness is meaningful even when final answers are wrong, and GPT-4o can reliably categorize and evaluate scientific reasoning steps.
- Evidence anchors:
  - [section 2.4] "we compute two average scores: one for the knowledge comprehension steps and another for the logical deduction steps"
  - [section 3.2] "the gap between the two scores is more pronounced in closed-source LMMs, indicating that closed-source models excel at CoT reasoning, producing higher-quality intermediate steps"
  - [corpus] SDIGLM and MME-CoT both use multi-modal CoT evaluation strategies, but SciVerse's step-wise knowledge/logic separation is novel and not yet externally validated.

## Foundational Learning

- Concept: **Chain-of-Thought (CoT) Reasoning**
  - Why needed here: The entire evaluation framework depends on eliciting and assessing step-by-step reasoning. Without understanding how CoT prompting works ("perform reasoning step-by-step"), the step-wise evaluation mechanism is opaque.
  - Quick check question: Can you explain why CoT might improve accuracy on multi-step problems but also increase exposure to intermediate errors?

- Concept: **Vision-Language Model Architecture (Vision Encoder + LLM)**
  - Why needed here: The benchmark's distinction between text-based and vision-based information assumes a modular architecture where visual encoding quality can bottleneck downstream reasoning.
  - Quick check question: In a typical LMM (e.g., CLIP encoder + LLM backbone), where does cross-modal integration occur, and what component would most affect Vision-only performance?

- Concept: **Scientific Knowledge Representation in LLMs**
  - Why needed here: The Knowledge-free/lite/rich transformation assumes that domain expertise is stored parametrically and can be externally supplemented. Understanding how LLMs encode scientific facts informs why some models are "knowledge-stable" across versions.
  - Quick check question: If a model performs equally well on Knowledge-free and Knowledge-rich problems, what does that suggest about its pre-training data?

## Architecture Onboarding

- Component map:
  - Problem Selection -> Problem Transformation Pipeline (5 versions) -> Model Evaluation (Acc/CoT) -> CoT Evaluation Module (GPT-4o) -> Metrics Layer (Acc, Sci-CoT_K, Sci-CoT_L, Sci-CoT)

- Critical path:
  1. Problem selection and LaTeX normalization
  2. Expert annotation to create 5 versions per problem
  3. Model inference across all versions
  4. CoT response parsing and GPT-4o evaluation
  5. Score aggregation and comparative analysis

- Design tradeoffs:
  - Annotation cost vs. granularity: 5 versions per problem enables fine-grained analysis but requires significant expert effort (8 PhD-level annotators)
  - GPT-4o as evaluator vs. human evaluation: Scalable but introduces model-dependent bias; prompts provided in Appendix A but reliability unverified
  - Zero-shot vs. few-shot evaluation: Authors chose zero-shot to test inherent capabilities, but this may underrepresent model potential

- Failure signatures:
  - Large Acc-SciCoT gap with low Sci-CoT_L: Model has good knowledge but poor calculation/inference
  - High Sci-CoT_K with low Sci-CoT_L: Knowledge retrieval works but logical deduction fails
  - Sharp drop from Vision-rich to Vision-only: OCR or cross-modal integration bottleneck
  - Random performance across Knowledge-free/lite/rich: Prompt length or formatting confounds

- First 3 experiments:
  1. **Baseline verification**: Run random-chance baseline and a simple open-source model (e.g., LLaVA-1.5) on all 5 versions to reproduce reported Acc and Sci-CoT scores; verify GPT-4o evaluation pipeline output matches paper format.
  2. **Ablation on vision encoder**: Swap the vision encoder in a modular LMM (e.g., replace CLIP with SigLIP) and measure Vision-only performance delta to isolate visual encoding contribution.
  3. **Knowledge injection test**: Take a model with large Knowledge-freeâ†’Knowledge-rich improvement and test whether RAG-based knowledge retrieval (corpus: Logic-RAG paper) produces comparable gains, validating whether the transformation mechanism generalizes.

## Open Questions the Paper Calls Out

- **Generalization to non-STEM disciplines**: How does LMM performance on multi-modal scientific problems generalize to non-STEM disciplines such as art, business, and social sciences? The current benchmark is restricted to Physics, Chemistry, and Biology, leaving the model's efficacy in other expert domains unknown.

- **Architectural improvements for Vision-only problems**: What specific architectural improvements are required to close the performance gap between text-based and "Vision-only" scientific problem solving? Current LMMs fail to robustly extract and process question information when it is fully embedded in diagrams rather than provided as text.

- **Optimizing open-source LMMs for internal knowledge utilization**: How can open-source LMMs be optimized to utilize internal knowledge reserves rather than relying heavily on explicit knowledge cues? Results show open-source models exhibit greater performance gains when provided with knowledge details, whereas closed-source models display stable performance across knowledge variants.

## Limitations

- GPT-4o evaluation pipeline reliability: Step-wise knowledge/logic scoring depends entirely on a single external model's judgment without human validation or inter-annotator agreement statistics.
- Potential prompt length confounds: The Knowledge-free/lite/rich transformation may introduce confounds through varying prompt lengths across versions, not controlled for in the analysis.
- Visual variant transformation specifications: Lack of detailed specifications on diagram quality standards and OCR robustness testing for the visual variants.

## Confidence

- **High confidence**: Closed-source vs open-source performance gaps (Acc and Sci-CoT scores clearly favor closed-source models)
- **Medium confidence**: CoT reasoning quality assessment (Sci-CoT scores are reported but GPT-4o reliability unverified)
- **Medium confidence**: Cross-modal integration limitations (Vision-only performance drops are observed but diagram quality effects not isolated)

## Next Checks

1. **GPT-4o evaluation validation**: Take 100 randomly selected SciVerse responses and have human experts (or multiple GPT-4o instances with different prompts) independently score the same steps. Calculate Cohen's kappa to establish inter-rater reliability for step categorization and correctness scoring.

2. **Prompt length control experiment**: Re-run evaluations on a subset of problems where Knowledge-free, Knowledge-lite, and Knowledge-rich versions are normalized to identical token counts through controlled truncation/expansion. Compare performance deltas to identify prompt length confounds.

3. **Visual quality ablation**: Systematically degrade diagram quality (e.g., lower resolution, add noise, remove labels) in Vision-only problems and measure performance decay. This would separate visual perception limitations from cross-modal integration ability, as current results cannot distinguish between these failure modes.