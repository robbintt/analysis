---
ver: rpa2
title: Structure-Aware Fusion with Progressive Injection for Multimodal Molecular
  Representation Learning
arxiv_id: '2510.23640'
source_url: https://arxiv.org/abs/2510.23640
tags:
- molecular
- tasks
- mumo
- structural
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MuMo, a multimodal molecular representation
  learning framework that addresses the challenges of 3D conformer unreliability and
  modality collapse in molecular property prediction. The authors introduce a Structured
  Fusion Pipeline (SFP) that combines 2D topology and 3D geometry into a stable structural
  prior, along with a Progressive Injection (PI) mechanism that asymmetrically integrates
  this prior into the sequence stream.
---

# Structure-Aware Fusion with Progressive Injection for Multimodal Molecular Representation Learning

## Quick Facts
- arXiv ID: 2510.23640
- Source URL: https://arxiv.org/abs/2510.23640
- Reference count: 40
- Key outcome: Achieves 2.7% average improvement over baselines on 29 benchmark tasks, ranking first on 22 of them including a 27% improvement on LD50.

## Executive Summary
This paper introduces MuMo, a multimodal molecular representation learning framework that addresses the challenges of 3D conformer unreliability and modality collapse in molecular property prediction. The authors propose a Structured Fusion Pipeline (SFP) that combines 2D topology and 3D geometry into a stable structural prior, along with a Progressive Injection (PI) mechanism that asymmetrically integrates this prior into the sequence stream. Built on a state space backbone, MuMo supports long-range dependency modeling and robust information propagation. The model achieves an average improvement of 2.7% over the best-performing baseline on each of 29 benchmark tasks from Therapeutics Data Commons (TDC) and MoleculeNet, ranking first on 22 of them, including a 27% improvement on the LD50 task.

## Method Summary
MuMo is a 505M parameter multimodal molecular representation learning framework built on a Mamba backbone. It uses a Structured Fusion Pipeline (SFP) to combine 2D topology and 3D geometry into a rotation-invariant unified graph representation. The Progressive Injection (PI) mechanism asymmetrically integrates this structural prior into the sequence stream starting at layer 9, preserving modality-specific modeling. The model undergoes pretraining on ChEMBL-1.6M using Masked Language Modeling for 2 epochs, followed by finetuning on 29 downstream tasks from TDC and MoleculeNet with various evaluation metrics.

## Key Results
- Achieves 2.7% average improvement over best-performing baseline on each of 29 benchmark tasks
- Ranks first on 22 out of 29 tasks, including 27% improvement on LD50 task
- Demonstrates robustness to 3D conformer noise while maintaining stable performance across diverse molecular prediction tasks

## Why This Works (Mechanism)

### Mechanism 1: Rotation-Invariant Unified Graph for Conformer Stability
The Structured Fusion Pipeline (SFP) stabilizes molecular representation by converting noisy 3D coordinates into rotation-invariant geometric descriptors, reducing sensitivity to conformer generation artifacts. The model constructs a "Unified Graph" that replaces raw (x,y,z) coordinates with geometric triplets (bond lengths and angles), which do not change under rotation, allowing consistent structural learning regardless of conformer orientation.

### Mechanism 2: Asymmetric Injection for Modality Collapse Prevention
Delaying the fusion of structural priors until later layers (Progressive Injection) prevents "modality collapse," where a dominant or noisy modality suppresses the signal of another. The model allows the Sequence Stream to train independently for the first half of the network to establish stable semantic context before structural injection via Injection-Enhanced Attention into a global token.

### Mechanism 3: State-Space Evolution of Structural Priors
Using a Mamba (State Space Model) backbone enables the structural prior to evolve independently across layers with linear complexity, maintaining long-range dependencies better than standard attention. Unlike Transformers, which rely on token-to-token attention, Mamba maintains a recurrent latent state that allows continuous propagation of the fused structural prior.

## Foundational Learning

- **Concept: Unified Graph Representation**
  - Why needed here: MuMo does not process 3D coordinates directly. You must understand how to encode 3D space into graph edges (lengths/angles) to grasp the SFP.
  - Quick check question: Can you explain why calculating the Euclidean distance between two atoms is invariant to rotation, whereas raw (x,y,z) coordinates are not?

- **Concept: Modality Collapse**
  - Why needed here: This is the core failure mode MuMo solves. Without understanding this, the "Progressive Injection" looks like unnecessary complexity.
  - Quick check question: In a naive fusion (early concatenation), why might a neural network ignore the SMILES string if the 3D conformer features are high-dimensional but noisy?

- **Concept: State Space Models (SSMs) vs. Transformers**
  - Why needed here: The paper chooses Mamba over standard Transformers for the backbone.
  - Quick check question: What is the computational complexity of a Transformer attention layer vs. an SSM scan regarding sequence length (L)? (O(L^2) vs O(L)).

## Architecture Onboarding

- **Component map:** Input: SMILES (Sequence) + RDKit Conformer (Geometry) -> SFP: Encodes Geometry + Topology into Unified Graph -> Multiscale Partitioning -> Backbone: Hybrid Attention-Mamba blocks (16 layers) -> Fusion: Injection-Enhanced Attention (IEA) activates at Layer 9 -> Output: [GTK] global token for downstream tasks

- **Critical path:** The most critical path is the Injection Timing. The model must run 8 layers of pure sequence modeling before the IEA module touches the [GTK] token. Altering this ratio impacts performance significantly.

- **Design tradeoffs:**
  - Stability vs. Precision: Using lengths/angles instead of coordinates gains stability against conformer noise but may lose fine-grained chirality details.
  - Depth vs. Convergence: The model requires sufficient depth (16 layers) to allow the bifurcated "early semantic, late structural" learning strategy.

- **Failure signatures:**
  - Conformer Overfitting: If validation loss diverges while training loss drops, check if the Unified Graph construction is failing.
  - Collapse: If the model performs worse than a SMILES-only baseline, the injection is likely happening too early, or the structural prior weight Î± is too high.

- **First 3 experiments:**
  1. Timing Ablation: Verify Table 6 by running MuMo with injection at Layer 1, Layer 8, and Layer 16 to observe the performance curve.
  2. Noise Robustness: Verify Table 29 by adding Gaussian noise to input coordinates and confirming minimal performance drops compared to baselines.
  3. Modality Ablation: Verify Table 17 by comparing Full MuMo vs. SMILES-Only vs. SMILES+Graph to quantify the value added by 3D geometry.

## Open Questions the Paper Calls Out

- Can the Progressive Injection mechanism be adapted to create a general-purpose multitask framework that eliminates the need for per-task fine-tuning?
- Does the integration of chirality-sensitive features (e.g., signed dihedral angles) allow the Unified Graph to effectively distinguish between enantiomers?
- Can the asymmetric fusion design of MuMo generalize to cross-modal domains such as vision-language tasks or protein-compound modeling?

## Limitations

- The current architecture requires resource-intensive fine-tuning for each of the 29 downstream tasks, limiting efficiency in low-resource settings.
- The Unified Graph cannot distinguish enantiomers due to unsigned geometric descriptors, limiting applications requiring chirality sensitivity.
- The model has only been validated on molecular data (SMILES, 2D, 3D) and its generalizability to other multimodal domains remains unproven.

## Confidence

- Pretraining details: High confidence - explicit hyperparameters provided
- Downstream evaluation metrics: High confidence - clearly specified per task
- Finetuning hyperparameters: Low confidence - stated to be in code but not specified in text
- Conformer generation protocol: Medium confidence - ablation tests provided but main setup details limited
- Optimizer settings: Medium confidence - LR specified for pretraining but not for finetuning

## Next Checks

1. Verify the Progressive Injection timing by running ablation experiments at Layer 1, Layer 8, and Layer 16 as described in Table 6
2. Test noise robustness by adding Gaussian noise to 3D coordinates and comparing performance degradation against Uni-Mol baseline
3. Validate the modality ablation by comparing Full MuMo against SMILES-Only and SMILES+Graph variants to quantify 3D geometry contribution