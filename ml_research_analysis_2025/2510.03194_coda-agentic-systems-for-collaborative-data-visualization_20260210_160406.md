---
ver: rpa2
title: 'CoDA: Agentic Systems for Collaborative Data Visualization'
arxiv_id: '2510.03194'
source_url: https://arxiv.org/abs/2510.03194
tags:
- data
- visualization
- code
- plot
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoDA, a collaborative multi-agent system
  for automating data visualization from natural language queries. CoDA employs specialized
  LLM agents for metadata analysis, task planning, code generation, and self-reflection,
  enabling robust handling of complex, multi-file datasets and iterative refinement.
---

# CoDA: Agentic Systems for Collaborative Data Visualization

## Quick Facts
- arXiv ID: 2510.03194
- Source URL: https://arxiv.org/abs/2510.03194
- Reference count: 40
- Primary result: CoDA achieves up to 41.5% improvement in overall score over baselines for automated data visualization from natural language queries

## Executive Summary
This paper introduces CoDA, a collaborative multi-agent system for automating data visualization from natural language queries. CoDA employs specialized LLM agents for metadata analysis, task planning, code generation, and self-reflection, enabling robust handling of complex, multi-file datasets and iterative refinement. Extensive experiments on MatplotBench and Qwen benchmarks show CoDA achieves up to 41.5% improvement in overall score over baselines like MatplotAgent, VisPath, and CoML4VIS. The system addresses limitations of prior approaches by focusing on metadata preprocessing and quality-driven feedback loops, demonstrating that integrated agentic workflows, rather than isolated code generation, are key to robust visualization automation.

## Method Summary
CoDA uses 8 specialized agents (Query Analyzer, Data Processor, VizMapping, Search, Design Explorer, Code Generator, Debug, Visual Evaluator) with gemini-2.5-pro as backbone. The system extracts metadata summaries to bypass token limits, decomposes queries into structured TODO lists, and iteratively refines visualizations through quality-driven feedback loops. Agents communicate via shared memory buffer, with max 3 refinement iterations and quality threshold θq=0.85. Evaluation uses MatplotBench (100 queries), Qwen Code Interpreter (163), and DA-Code (78) benchmarks, measuring Execution Pass Rate, Visualization Success Rate, and Overall Score.

## Key Results
- 41.5% improvement in Overall Score (OS) compared to state-of-the-art baselines
- OS increases from 75.6% to 79.5% with 3 refinement iterations (+3.9% absolute)
- Multi-file dataset handling via metadata extraction avoids token limit violations
- Ablation studies show TODO list removal drops OS by 4.4% and Visual Evaluator removal drops OS by 3.9%

## Why This Works (Mechanism)

### Mechanism 1: Metadata-centric preprocessing
- Metadata extraction enables handling of complex datasets without hitting token limits
- Data Processor extracts schemas, statistics, and patterns using lightweight tools, producing structured summaries instead of raw data
- Core assumption: Summarized metadata retains sufficient information for accurate visualization planning
- Evidence: Abstract states "metadata-focused analysis bypasses token limits" and section 3.2 describes extraction of shapes, columns, and insights
- Break condition: If datasets require fine-grained row-level patterns that summaries obscure, visualization quality may degrade

### Mechanism 2: Iterative self-reflection through quality-driven feedback loops
- Quality-driven refinement improves visualization correctness and aesthetics
- Visual Evaluator assesses output images across clarity, accuracy, aesthetics, layout metrics, routing issues back to upstream agents when scores fall below θq=0.85
- Core assumption: LLM-based evaluator provides reliable quality signals correlating with human judgment
- Evidence: OS improves from 75.6% to 79.5% with 3 iterations; ablation shows 3.9% drop without feedback loop
- Break condition: If evaluator is miscalibrated or feedback loops exceed budget without convergence, latency increases without proportional accuracy gains

### Mechanism 3: Specialized agents with structured communication
- Specialized agents with structured communication improve decomposition of complex visualization tasks
- Global TODO list from Query Analyzer decomposes queries into agent-specific subtasks with priorities and status tracking
- Core assumption: Task decomposition quality is higher when a single agent creates the blueprint rather than emergent multi-agent negotiation
- Evidence: Ablation shows TODO list removal drops OS by 4.4%; section 3.1 emphasizes structured communication and quality-driven feedback loops
- Break condition: If queries are ambiguous or domain-specific in ways Query Analyzer's prompts don't cover, TODO decomposition may miss critical subtasks

## Foundational Learning

- **LLM Context Window Constraints**: Why needed: Understanding why metadata extraction matters requires grasping that raw multi-file datasets can exceed token limits, causing truncation or hallucination. Quick check: Can you explain why feeding a 100MB CSV directly to an LLM is problematic, and what summaries preserve vs. discard?

- **Multi-Agent Coordination Patterns**: Why needed: CoDA uses sequential agent handoffs with shared state, not parallel debate or negotiation—you need to understand when to use each pattern. Quick check: What's the difference between sequential handoff with shared memory vs. multi-agent debate, and why might CoDA choose the former?

- **Visualization Quality Dimensions**: Why needed: Visual Evaluator scores across multiple axes (clarity, accuracy, aesthetics, layout, correctness)—you need to know what each captures to interpret feedback loops. Quick check: If a visualization has correct data mapping but poor color contrast, which quality dimensions are affected and which agent handles the fix?

## Architecture Onboarding

- **Component map**: Query → Query Analyzer (TODO) → Data Processor (metadata) → VizMapping (chart spec) → Design Explorer (design spec) → Code Generator → Debug Agent → Visual Evaluator → (if score < θq) refine loop back to Design/Code → output

- **Critical path**: Natural language query flows through Query Analyzer for intent extraction and TODO list creation, then Data Processor for metadata extraction, VizMapping for chart type selection, Design Explorer for aesthetic specifications, Code Generator for code synthesis, Debug Agent for execution and error handling, and finally Visual Evaluator for quality assessment with potential refinement loop

- **Design tradeoffs**: Accuracy vs. efficiency (more iterations improve OS but increase latency; paper sets 3-iteration default as practical balance), modularity vs. complexity (specialized agents improve reasoning depth but require more coordination overhead), external knowledge vs. reliability (Search Agent improves EPR but adds dependency on external sources)

- **Failure signatures**: Low EPR suggests syntactic errors in specialized visualizations (check Search Agent or Code Generator); high EPR but low VSR indicates code runs but output doesn't match intent (check TODO decomposition or VizMapping); iterative loop doesn't converge suggests evaluator miscalibration or Design Explorer not addressing feedback

- **First 3 experiments**: 1) Baseline replication: Implement single-agent MatplotAgent equivalent on 10 MatplotBench queries to establish EPR/VSR baseline before adding CoDA components, 2) Ablation checkpoint: Disable Visual Evaluator feedback loop and measure OS drop (should see ~4% decrease per paper's ablation), 3) Stress test with multi-file data: Feed CoDA a 3-file dataset requiring joins/aggregations and verify Data Processor metadata extraction handles cross-file references correctly

## Open Questions the Paper Calls Out

- **Question 1**: Can the multi-agent collaborative behaviors in CoDA be distilled into a single model to reduce computational overhead without sacrificing performance? Basis: Paper concludes "Future efforts could distill agents... A key limitation is the computational overhead from multi-turn agent communications." Evidence needed: Comparative analysis showing distilled version achieving similar OS with significantly lower average token count and call count.

- **Question 2**: Does using the same LLM family (Gemini) for both generation and evaluation introduce systematic bias in reported scores? Basis: Evaluation methodology uses gemini-2.5-pro as both backbone and evaluator. Evidence needed: Cross-model evaluation where CoDA outputs are judged by different strong models or human experts, comparing scores against baseline Gemini evaluation.

- **Question 3**: How does extending CoDA to support multimodal inputs affect Query Analyzer's ability to decompose complex design requirements? Basis: Conclusion lists adapting to "multimodal inputs" as future direction. Evidence needed: Experiments on benchmarks requiring sketch-to-visualization or image-conditional generation, measuring fidelity to multimodal input.

## Limitations

- Computational overhead from 14.8 average LLM calls per query, with diminishing returns beyond 3 iterations
- Limited details on Search Agent implementation and web search ranking mechanism
- Potential evaluator bias using same LLM family for generation and quality assessment

## Confidence

- **High Confidence**: Metadata extraction mechanism bypassing token limits, structured agent coordination via TODO lists, general architecture of sequential handoffs with shared memory buffer
- **Medium Confidence**: Claims about quality-driven refinement improving visualization correctness, given evaluator reliability and diminishing returns uncertainty
- **Low Confidence**: Absolute performance numbers (41.5% improvement) due to limited details on Search Agent implementation, evaluator calibration, and potential domain-specific optimizations

## Next Checks

1. **Evaluator Calibration**: Test CoDA's Visual Evaluator on held-out validation set with human-annotated quality scores to verify correlation and identify potential bias in multi-dimensional assessment

2. **Search Agent Dependency**: Implement CoDA with Search Agent disabled and measure EPR/VSR changes, particularly for specialized visualization types that may rely on external examples

3. **Cross-Dataset Generalization**: Evaluate CoDA on third-party dataset (e.g., Vega-Lite examples) not seen during training to verify metadata extraction and TODO decomposition generalize beyond benchmark corpora