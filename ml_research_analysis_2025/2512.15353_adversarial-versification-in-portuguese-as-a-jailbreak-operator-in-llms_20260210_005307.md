---
ver: rpa2
title: Adversarial versification in portuguese as a jailbreak operator in LLMs
arxiv_id: '2512.15353'
source_url: https://arxiv.org/abs/2512.15353
tags:
- adversarial
- prose
- versification
- poetry
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper demonstrates that versification\u2014a form of stylistic\
  \ and structural poetic transformation\u2014acts as a powerful adversarial jailbreak\
  \ mechanism against aligned LLMs. When harmful prompts are rewritten as verse, guardrails\
  \ based on surface-level prose patterns fail to trigger, resulting in dramatically\
  \ increased success rates: up to 18x higher attack success than prose versions,\
  \ with manual poems achieving ~62% ASR and automatic versions ~43%, and some models\
  \ exceeding 90%."
---

# Adversarial versification in portuguese as a jailbreak operator in LLMs

## Quick Facts
- **arXiv ID:** 2512.15353
- **Source URL:** https://arxiv.org/abs/2512.15353
- **Reference count:** 0
- **Primary result:** Versification acts as a powerful adversarial jailbreak mechanism against aligned LLMs, with poetic rewrites achieving up to 18x higher attack success rates than prose versions

## Executive Summary
This paper demonstrates that versification—transforming harmful prompts into poetic form—acts as a powerful adversarial jailbreak mechanism against aligned LLMs. When prompts are rewritten as verse, guardrails based on surface-level prose patterns fail to trigger, resulting in dramatically increased success rates: up to 18x higher attack success than prose versions, with manual poems achieving ~62% ASR and automatic versions ~43%. The vulnerability stems from models relying on narrow textual cues tied to instructional prose, which versification disrupts by moving prompts into under-supervised latent regions. The study highlights a critical security gap: current alignment strategies are insufficiently robust to formal semiotic variation.

## Method Summary
The study investigates how versification can bypass LLM safety guardrails by examining whether poetic transformations of harmful prompts reduce refusal rates. The methodology involves creating both manual and automated versified versions of harmful prompts and measuring their success rates compared to prose baselines. The paper analyzes multiple architectures and safety training regimes (RLHF, Constitutional AI, hybrids) to assess consistency of the effect. The evaluation uses ASR (Attack Success Rate) as measured by MLCommons AILuminate taxonomy, with a focus on single-turn jailbreak attacks. The study also identifies Portuguese as a critical research gap due to its rich prosodic traditions and morphosyntactic complexity.

## Key Results
- Versified prompts achieve up to 18x higher attack success rates than prose versions, with some models exceeding 90% ASR
- Manual poems show ~62% ASR while automated versions achieve ~43% ASR, demonstrating scalability tradeoffs
- The effect is consistent across diverse architectures and safety training regimes (RLHF, Constitutional AI, hybrids)
- Current guardrails are excessively dependent on surface patterns rather than semantic understanding

## Why This Works (Mechanism)

### Mechanism 1: Latent Space Displacement via Semiotic Variation
Versification shifts prompts into sparsely-supervised regions of embedding space where safety classifiers have weak coverage. Poetic forms (meter, enjambment, parallelism) reorganize input surface geometry without altering pragmatic intent, navigating around "armed" guardrail regions calibrated primarily for instructional prose. This assumes safety filters are non-uniformly distributed across representational space.

- **Evidence:** [abstract] "Versification displaces the prompt into sparsely supervised latent regions, revealing guardrails that are excessively dependent on surface patterns." [section] Page 6: "A versified poem, by selecting low-probability lexical trajectories ('high-temperature language'), displaces the input into subspaces where refusal policies are weak or nonexistent." [corpus] Neighbor paper (Bisconti et al., 2511.15304) reports ASR exceeding 90% on some models with poetic prompts.

### Mechanism 2: Mismatched Generalization in Alignment Regimes
Current alignment methods (RLHF, Constitutional AI) exhibit robust refusal under benchmark conditions but degrade under minimal formal-semantic variation. Alignment datasets over-represent instructional prose, causing safety classifiers to learn surface-pattern correlates of harm rather than abstract intent recognition. When formal markers deviate, the classifier's activation threshold is not reached.

- **Evidence:** [abstract] "Systems trained with RLHF, Constitutional AI, and hybrid pipelines exhibit consistent degradation under minimal semiotic–formal variation." [section] Page 6: "Models exhibit strong semantic competence, but their safety systems depend on surface-level patterns." [corpus] Wei et al. (2023, cited in paper) documented mismatched generalization.

### Mechanism 3: Morphosyntactic Amplification in High-Complexity Languages
Portuguese-specific versification may amplify jailbreak effectiveness due to higher morphosyntactic plasticity and under-representation in safety training data. Portuguese offers flexible word order, rich inflectional morphology, and diverse metric traditions, enabling more varied trajectory paths through latent space.

- **Evidence:** [abstract] "The absence of evaluations in Portuguese, a language with high morphosyntactic complexity... constitutes a critical gap." [section] Page 8-9: Author notes open questions about whether Brazilian performative forms "displace the prompt into subspaces even more distant from the communicational/referential prose used in alignment." [corpus] No direct corpus evidence for Portuguese-specific effects.

## Foundational Learning

- **Concept: Latent Space Geometry in Transformer Models**
  - **Why needed here:** The displacement mechanism assumes safety filters occupy specific regions in high-dimensional embedding space. Understanding how inputs map to vectors, and how transformations alter trajectory, is foundational.
  - **Quick check question:** Can you explain why a versified prompt and its prose equivalent might occupy different regions in embedding space despite similar semantic content?

- **Concept: RLHF and Constitutional AI Alignment**
  - **Why needed here:** The paper attributes vulnerability to alignment regimes trained on surface patterns. Understanding what RLHF optimizes (preference rankings) vs. Constitutional AI (rule-based critique) clarifies why formal variation evades both.
  - **Quick check question:** What type of training data would RLHF need to generalize refusal to versified harmful prompts?

- **Concept: Scansion, Meter, and Prosodic Variation (Portuguese-specific)**
  - **Why needed here:** The proposed evaluation protocol requires parameterizing these variables. Terms like "redondilha maior," "sinalefas," "enjambment," and "heroic vs. sapphic decasyllables" are technical prerequisites.
  - **Quick check question:** How might enjambment (syntactic break across line boundaries) alter token-level representations differently than end-stopped lines?

## Architecture Onboarding

- **Component map:** Input layer (Versified prompt → Tokenizer → Embedding) → Representation space (high-dimensional latent geometry with non-uniform safety coverage) → Safety classifiers (guardrails trained predominantly on prose) → Output layer (refusal vs. compliance) → Evaluation layer (ASR measurement via MLCommons AILuminate taxonomy)

- **Critical path:** 1. Formal transformation (prose → verse) restructures surface tokens, 2. Embedding maps versified input to low-supervision latent region, 3. Safety classifier fails to activate (below threshold), 4. Model generates prohibited content

- **Design tradeoffs:** Semiotic complexity vs. reproducibility (manual poems achieve higher ASR ~62% vs. automated ~43% but lack scalability); Security vs. verifiability (non-disclosure of adversarial poems impedes peer validation); Single-turn vs. multi-turn (current study limited to single-turn)

- **Failure signatures:** High ASR differential between prose and verse versions of identical intent; Guardrail activation correlates with prose-specific surface markers, not semantic harm; Cross-architectural transfer (similar ASR across models with different training pipelines)

- **First 3 experiments:** 1. **Baseline replication in Portuguese:** Translate English/Italian adversarial poems to Portuguese; measure ASR differential against prose controls across 5+ models, 2. **Metric parameterization:** Construct Portuguese prompt set varying meter (heptasyllabic vs. decasyllabic vs. alexandrine), enjambment presence, and rhyme density; isolate which formal variables modulate ASR, 3. **Non-referential prose comparison:** Test experimental prose (syntactic perturbation, ellipsis, hyperbaton) vs. verse to determine whether displacement is poetry-specific or generalizes to any formal deviation from instructional prose

## Open Questions the Paper Calls Out
- Whether Brazilian performative forms (cordel, repente, rap) can displace prompts into even more distant subspaces than standard verse
- How to systematically parameterize and evaluate scansion, meter, and prosodic variation in safety assessments
- The extent to which morphosyntactic amplification effects are specific to Portuguese versus generalizable to other high-complexity languages

## Limitations
- The morphosyntactic amplification hypothesis (Mechanism 3) is explicitly speculative with no experimental evidence in the paper
- Non-disclosure of actual adversarial examples prevents independent verification of prompt quality and semantic preservation
- Cross-architectural consistency is asserted but not quantified per model in the abstract

## Confidence
- **High confidence:** Mechanism 1 (latent space displacement) - supported by multiple textual anchors and consistent with corpus neighbor findings showing >90% ASR with poetic prompts
- **Medium confidence:** Mechanism 2 (mismatched generalization in alignment) - theoretically sound and consistent with cited literature (Wei et al. 2023), but ASR differential quantification requires model-specific data
- **Low confidence:** Mechanism 3 (morphosyntactic amplification in Portuguese) - explicitly labeled as open question with no empirical evidence in the paper

## Next Checks
1. **Empirical displacement mapping:** Use embedding similarity analysis to measure cosine distance between prose and versified versions of identical harmful prompts across Portuguese and English, testing whether Portuguese verse occupies more distant regions
2. **Safety data density audit:** Analyze training corpus statistics for Portuguese safety examples vs. English, quantifying coverage of formal variations (poetry, experimental prose) in alignment datasets
3. **Cross-linguistic prosody comparison:** Construct parallel harmful prompt sets in Portuguese and English with controlled metrical variations (same meter types, same enjambment patterns), measuring ASR differentials to isolate language-specific amplification effects