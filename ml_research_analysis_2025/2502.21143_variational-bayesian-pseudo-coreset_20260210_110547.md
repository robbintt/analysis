---
ver: rpa2
title: Variational Bayesian Pseudo-Coreset
arxiv_id: '2502.21143'
source_url: https://arxiv.org/abs/2502.21143
tags:
- vbpc
- dataset
- training
- learning
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational challenges of training Bayesian
  Neural Networks (BNNs) on large datasets by introducing Variational Bayesian Pseudo-Coreset
  (VBPC), a method that learns small synthetic datasets to efficiently approximate
  posterior distributions. VBPC leverages variational inference to obtain closed-form
  solutions for the last-layer weights, eliminating the need for computationally expensive
  sampling methods and reducing memory requirements.
---

# Variational Bayesian Pseudo-Coreset

## Quick Facts
- arXiv ID: 2502.21143
- Source URL: https://arxiv.org/abs/2502.21143
- Reference count: 40
- Key outcome: Introduces VBPC, a method that learns small synthetic datasets to efficiently approximate posterior distributions in Bayesian Neural Networks, outperforming existing Bayesian Pseudo-Coreset methods in accuracy and negative log-likelihood while demonstrating robustness to out-of-distribution data.

## Executive Summary
This paper addresses the computational challenges of training Bayesian Neural Networks (BNNs) on large datasets by introducing Variational Bayesian Pseudo-Coreset (VBPC), a method that learns small synthetic datasets to efficiently approximate posterior distributions. VBPC leverages variational inference to obtain closed-form solutions for the last-layer weights, eliminating the need for computationally expensive sampling methods and reducing memory requirements. The method uses a model pool to prevent overfitting and employs memory-efficient techniques for both training and inference. Experiments on benchmark datasets (MNIST, CIFAR10/100, Tiny-ImageNet) show that VBPC outperforms existing Bayesian Pseudo-Coreset methods in terms of accuracy and negative log-likelihood, while also demonstrating robustness to out-of-distribution data and generalization to different model architectures.

## Method Summary
VBPC learns a small synthetic dataset (pseudo-coreset) that approximates the posterior distribution of a Bayesian Neural Network through bilevel optimization. The method uses Gaussian likelihood for the coreset variational inference problem, yielding closed-form solutions for the last-layer posterior weights and eliminating the need for iterative sampling. A model pool of P feature extractors prevents overfitting to a single model, while Woodbury identity-based computations reduce memory usage from O(h²) to O(n̂²). The approach combines variational inference with coreset learning, using the Bayesian Learning Rule for efficient optimization and memory-efficient techniques for both training and inference.

## Key Results
- VBPC achieves higher accuracy and lower negative log-likelihood than existing Bayesian Pseudo-Coreset methods on MNIST, CIFAR10/100, and Tiny-ImageNet
- The method demonstrates robustness to out-of-distribution data, with smaller performance degradation on corrupted datasets compared to baselines
- VBPC generalizes well to different model architectures, maintaining performance across various network designs
- Memory-efficient computation via Woodbury identity reduces BMA memory requirements from 542.9 MB to 268.9 MB

## Why This Works (Mechanism)

### Mechanism 1
Using Gaussian likelihood for the coreset variational inference problem yields closed-form solutions for the last-layer posterior, eliminating the need for iterative sampling or gradient unrolling. The paper sets p_S(y|x,θ) = N(y|W^⊤ϕ(x), γ⁻¹I_k) instead of softmax categorical likelihood. This choice allows the expected negative log-likelihood to decompose into quadratic terms in W, which when combined with Gaussian priors yields closed-form updates via conjugacy. The mean and variance are given by m*ⱼ = Φ^⊤(ρβ_S/γ · I_n̂ + ΦΦ^⊤)⁻¹ŷ_{:,j} and V* = 1/ρ · I_h − γ/(ρ²β_S) · Φ^⊤(I_n̂ + γ/(ρβ_S) · ΦΦ^⊤)⁻¹Φ.

### Mechanism 2
The bilevel optimization formulation enables gradient-based learning of the pseudo-coreset S without storing unrolled computation graphs, by exploiting the closed-form inner solution. The objective S* = argmin_S L_D(λ*_S) where λ*_S = argmin_λ L_S(λ) is differentiated via implicit function theorem. Because ∇²_μ L_S(λ*_S) = ∇_μ λ*_S (since ∇²_μ ℓ_S = 0 for Gaussian likelihood), the gradient simplifies to an influence-function form that avoids storing iterative updates.

### Mechanism 3
Memory-efficient computation of KL divergence and predictive variance via Woodbury Identity enables scaling to high-dimensional feature spaces. Rather than materializing V* ∈ R^(h×h), the method stores Φ ∈ R^(n̂×h) and (I_n̂ + γ/(ρβ_S) · ΦΦ^⊤)⁻¹ ∈ R^(n̂×n̂). Determinant and trace of V* are computed via Weinstein–Aronszajn identity and matrix trace properties, reducing memory from O(h²) to O(n̂²).

## Foundational Learning

- Concept: Variational Inference (VI) with Exponential Family
  - Why needed here: VBPC frames posterior approximation as minimizing KL[q_λ||p(θ|D)] where q_λ belongs to an exponential family (Gaussian for last-layer weights). Understanding natural parameters λ, mean parameters μ, and the Bayesian Learning Rule is essential.
  - Quick check question: Can you derive the natural-to-mean parameter mapping for a multivariate Gaussian, i.e., λ = [V⁻¹m, -½V] → μ = [m, V + mm^⊤]?

- Concept: Bilevel Optimization and Implicit Differentiation
  - Why needed here: Learning S requires differentiating through the solution of an inner optimization (coreset VI). The implicit function theorem provides ∇_S μ*_S without unrolling iterations.
  - Quick check question: Given F(S, μ) = 0, write the expression for ∇_S μ* in terms of ∇_S F and ∇_μ F.

- Concept: Coreset and Dataset Distillation
  - Why needed here: VBPC builds on the idea that a small synthetic dataset S can approximate posteriors from the full dataset D. Understanding the trade-off between coreset size n̂ and approximation quality contextualizes the method's goals.
  - Quick check question: Why might a learned pseudo-coreset outperform a randomly selected subset for high-dimensional posteriors?

## Architecture Onboarding

- Component map:
  Pseudo-coreset S (learnable images and labels) -> Model pool M (set of feature extractors) -> Last-layer VI module (computes closed-form posterior) -> Outer optimizer (updates S via Adam)

- Critical path:
  1. Initialize S by sampling n̂ images/labels per class from D
  2. Initialize model pool M with P random θ values
  3. For each training step: sample batch B ⊂ D, sample θ from M, compute ϕ, evaluate L̃_D via Eq. 27 with efficient KL/variance, update S via Adam, update θ for one step on S with Gaussian likelihood, replace θ in M; reinitialize after T updates
  4. At inference: train a new θ on S for T' steps, compute closed-form m* and V*, evaluate BMA using Σ* from Eq. 31

- Design tradeoffs:
  - Gaussian vs. Softmax likelihood for coreset VI: Gaussian yields closed-form but may mis-specify classification; softmax is correct but requires sampling/approximation
  - Model pool size P and update steps T: Larger P/T increase diversity but raise training cost; paper uses P=10, T=100 as defaults
  - Coreset size n̂ (ipc): Larger n̂ improves approximation but increases O(n̂²) memory; paper tests ipc ∈ {1, 10, 50}

- Failure signatures:
  - Overfitting to single ϕ: Without model pool, S overfits to one feature extractor; mitigated by sampling θ from M each step
  - Numerical instability: Direct computation of m* and V* via Eq. 21 can be unstable when n̂ ≪ h; use Eq. 48 and Eq. 52 (kernel/Woodbury forms) instead
  - OOD degradation: On severe corruptions (e.g., Motion Blur), accuracy drops from 69.8% → 55.9%; this is expected but less severe than baselines

- First 3 experiments:
  1. Sanity check on MNIST ipc=10: Train VBPC for 10K steps with P=5, T=50; verify closed-form m* produces reasonable images and ACC > 95%. Compare naïve vs. efficient memory usage.
  2. Ablation on model pool: Run CIFAR10 ipc=10 with P=1 (no pool) vs. P=10; expect ACC drop and visual overfitting in learned images.
  3. OOD robustness test: Train on CIFAR10 ipc=10, evaluate on CIFAR10-C (Gaussian Blur, Snow);