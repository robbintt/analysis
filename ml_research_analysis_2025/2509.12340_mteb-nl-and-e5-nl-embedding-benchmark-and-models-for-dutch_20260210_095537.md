---
ver: rpa2
title: 'MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch'
arxiv_id: '2509.12340'
source_url: https://arxiv.org/abs/2509.12340
tags:
- dutch
- dataset
- retrieval
- datasets
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces MTEB-NL, a comprehensive benchmark for Dutch
  text embeddings, and E5-NL, a suite of compact Dutch embedding models. MTEB-NL combines
  existing Dutch datasets with newly created ones across classification, retrieval,
  clustering, and semantic similarity tasks.
---

# MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch

## Quick Facts
- arXiv ID: 2509.12340
- Source URL: https://arxiv.org/abs/2509.12340
- Authors: Nikolay Banar; Ehsan Lotfi; Jens Van Nooten; Cristina Arhiliuc; Marija Kliocaite; Walter Daelemans
- Reference count: 0
- One-line primary result: Introduces MTEB-NL benchmark and E5-NL models for Dutch, achieving state-of-the-art performance with compact architectures

## Executive Summary
This work introduces MTEB-NL, a comprehensive benchmark for Dutch text embeddings, and E5-NL, a suite of compact Dutch embedding models. MTEB-NL combines existing Dutch datasets with newly created ones across classification, retrieval, clustering, and semantic similarity tasks. To train E5-NL models, the authors compile human-annotated Dutch retrieval data and augment it with synthetic data generated by LLMs, using topic sampling and filtering to improve diversity and quality. Experiments show that E5-NL models outperform non-instruct baselines and match or exceed some larger instruct models, while being significantly more parameter-efficient. The benchmark and models are publicly available to support future Dutch NLP research.

## Method Summary
The authors create MTEB-NL by combining existing Dutch datasets with newly created ones across 7 tasks: classification, multi-label classification, pair classification, reranking, retrieval, clustering, and semantic similarity. For E5-NL model training, they compile 620K human-annotated examples (mMARCO-NL, FEVER-NL, HotPotQA-NL) and generate 350K synthetic triplets using GPT-4.1 with topic sampling and filtering via Qwen3-Reranker-4B. Models are fine-tuned using two strategies: vocabulary trimming (250Kâ†’50K tokens) and tokeniser alignment to BERTje. Training uses InfoNCE contrastive loss with source-homogeneous batching, and hyperparameters vary by model size.

## Key Results
- E5-NL models outperform non-instruct baselines and match or exceed some larger instruct models
- Vocabulary trimming achieves significant model compression while maintaining performance
- Synthetic data generation with topic sampling improves performance on diverse tasks beyond retrieval
- E5-small-trm-nl achieves competitive results with substantially smaller parameter count

## Why This Works (Mechanism)

### Mechanism 1
Expanding training data beyond pure retrieval via synthetic generation improves embedding performance on diverse tasks like classification and clustering. Standard retrieval datasets bias models toward query-document matching, but synthetic triplets for non-retrieval tasks teach semantic relationships required for grouping and categorization, not just relevance ranking.

### Mechanism 2
Vocabulary trimming allows significant model compression while maintaining or improving performance on Dutch tasks compared to generic multilingual models. By reducing vocabulary from 250K to 50K tokens, the model focuses representational capacity on Dutch-specific semantics, reducing noise from unused language subspaces.

### Mechanism 3
Strict filtering of "hard negatives" using a high-quality reranker prevents pollution of the embedding space caused by false negatives. In contrastive learning, treating relevant documents as negatives damages the embedding space, so a superior teacher model filters out questionable negative pairs based on score thresholds.

## Foundational Learning

- **Concept: Contrastive Learning (InfoNCE Loss)**
  - Why needed here: This is the mathematical engine used to train E5-NL. It works by pulling the embedding of a "query" closer to its "positive" document while pushing it away from "negatives."
  - Quick check question: Why does including a "false negative" (a relevant document treated as irrelevant) in the training batch destroy the quality of the learned embedding space?

- **Concept: Vector Space & Semantic Similarity**
  - Why needed here: The benchmark (MTEB-NL) evaluates how well the model maps text to vectors where "closeness" equals semantic meaning. Understanding cosine similarity vs. Euclidean distance is vital for interpreting the results.
  - Quick check question: If two Dutch sentences have a cosine similarity of 0.98, what does that imply about their meanings compared to a pair with 0.45?

- **Concept: Vocabulary Adaptation (Trimming vs. Translation)**
  - Why needed here: The paper introduces two distinct ways to adapt an English/Multilingual model to Dutch. You must understand how changing the tokenizer vocabulary forces a remapping of the initial embedding layer weights.
  - Quick check question: In the "translation-based initialization" (t2t), what happens to the embedding weights of a Dutch token that did not exist in the original English model's vocabulary?

## Architecture Onboarding

- **Component map:** Tokenizer (Trimmed/Translated) -> Transformer Encoder -> Embedding Matrix (resized) -> InfoNCE Loss with Hard Negative Miner
- **Critical path:** Vocab Adaptation -> Data Curation with Topic Sampling and Filtering -> Source-Homogeneous Batching -> Contrastive Training
- **Design tradeoffs:** -trm (Trimmed) vs. -t2t (Translated): Results indicate -trm models generally perform better. -trm is more efficient but relies on multilingual tokenizer's ability to handle Dutch. -t2t aligns to native Dutch tokenizer but introduces noise if weight translation is imperfect. Efficiency vs. SOTA: e5-small-trm-nl matches larger models, offering massive speedup for production deployments at marginal accuracy cost.
- **Failure signatures:** Training Collapse / Loss plateau (polluted negatives), Poor Retrieval performance (too easy negatives), OOM (long-long pairs require significant memory).
- **First 3 experiments:** 1) Tokenizer Validation: Verify vocabulary trimming process and compare token overlap with Bertje. 2) Negative Mining Sanity Check: Inspect mined hard negatives for true non-relevance. 3) Zero-Shot Baseline: Evaluate unmodified multilingual-e5-small vs. e5-small-trm-nl (after vocab adaptation only) to measure vocabulary impact.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Dutch-specific instruct-tuned embedding models outperform current multilingual instruct baselines?
- Basis in paper: The conclusion states that "the next logical step is the development of instruct models for Dutch."
- Why unresolved: Current work focused on non-instruct models, leaving Dutch-instruct variant performance untested.
- What evidence would resolve it: Dutch-instruct model evaluated on MTEB-NL showing superior performance over multilingual-e5-large-instruct.

### Open Question 2
- Question: To what extent does replacing machine-translated data with native resources improve capturing linguistic nuances?
- Basis in paper: Authors list "reliance on translated data" as limitation that may overlook cultural context.
- Why unresolved: Training dataset is mixture of translated and synthetic data; impact of removing translation artifacts unknown.
- What evidence would resolve it: Performance comparisons between models trained on translated vs. native Dutch corpora on culturally specific tasks.

### Open Question 3
- Question: Can weak supervision pre-training close performance gap between Dutch self-supervised encoders and supervised models?
- Basis in paper: Authors suggest Dutch encoders could be effective candidates "if weak supervision is involved."
- Why unresolved: Self-supervised models tested (RobBERT) were only fine-tuned on labeled data, skipping weak supervision stage.
- What evidence would resolve it: Training RobBERT with large-scale weak supervision followed by fine-tuning, measuring performance delta.

## Limitations
- Synthetic data generation pipeline effectiveness depends heavily on prompt engineering and filtering heuristics that lack full specification
- Vocabulary trimming from 250K to 50K tokens may not preserve all necessary Dutch semantic information for code-switched or specialized domains
- Benchmark combines existing datasets with newly created ones, potentially introducing evaluation inconsistencies across tasks

## Confidence

- **High Confidence:** Vocabulary trimming mechanism and efficiency benefits are well-supported by ablation studies and align with established literature on language-specific adaptation
- **Medium Confidence:** Synthetic data generation approach shows promising results but lacks detailed prompt templates and filtering thresholds for full replication
- **Medium Confidence:** Hard negative filtering using Qwen3-Reranker-4B demonstrates sound methodology but depends on reranker's domain-specific Dutch performance

## Next Checks

1. **Synthetic Data Quality Audit:** Generate sample of 100 synthetic triplets using described topic sampling, manually evaluate for semantic relevance and diversity to verify 70% retention rate and assess false positive/negative rates.

2. **Vocabulary Coverage Analysis:** Compare actual token distribution in mMARCO-NL training corpus against 50K trimmed vocabulary to identify potential coverage gaps, particularly for domain-specific terminology or loanwords.

3. **Zero-Shot vs. Fine-Tuned Performance Gap:** Evaluate vanilla mE5 model on MTEB-NL, then measure performance change after only vocabulary adaptation (without fine-tuning) to isolate contribution of vocabulary trimming from full fine-tuning process.