---
ver: rpa2
title: Context Attribution with Multi-Armed Bandit Optimization
arxiv_id: '2506.19977'
source_url: https://arxiv.org/abs/2506.19977
tags:
- attribution
- context
- segments
- segment
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of identifying which segments of
  retrieved context are most responsible for a large language model's generated answer
  in question-answering systems. The authors formulate context attribution as a combinatorial
  multi-armed bandit (CMAB) problem, treating each context segment as a bandit arm
  and employing Combinatorial Thompson Sampling (CTS) to efficiently explore the exponentially
  large space of context subsets under limited query budgets.
---

# Context Attribution with Multi-Armed Bandit Optimization

## Quick Facts
- arXiv ID: 2506.19977
- Source URL: https://arxiv.org/abs/2506.19977
- Authors: Deng Pan; Keerthiram Murugesan; Nuno Moniz; Nitesh Chawla
- Reference count: 5
- Primary result: CAMAB achieves competitive context attribution quality with significantly fewer model queries than perturbation-based methods

## Executive Summary
This paper addresses the critical problem of identifying which segments of retrieved context are most responsible for a large language model's generated answer in question-answering systems. The authors formulate context attribution as a combinatorial multi-armed bandit (CMAB) problem, where each context segment represents a bandit arm. By employing Combinatorial Thompson Sampling (CTS), the method efficiently explores the exponentially large space of context subsets while operating under limited query budgets. The reward function is based on normalized token likelihoods, capturing how well a subset of segments supports the original model response.

The proposed method, CAMAB, demonstrates competitive attribution quality while requiring significantly fewer model queries than traditional perturbation-based methods. Extensive experiments on two diverse datasets (SST2 and HotpotQA) using two open-source LLMs (LLaMA3-8B and SmolLM-1.7B) show that CAMAB achieves superior or comparable performance to baselines like SHAP and ContextCite, particularly excelling in low-query-budget scenarios. The method maintains attribution fidelity while reducing computational demands, making it a practical and scalable solution for faithful context attribution in generative QA systems.

## Method Summary
The authors formulate context attribution as a combinatorial multi-armed bandit (CMAB) problem, treating each context segment as a bandit arm. The key innovation is using Combinatorial Thompson Sampling (CTS) to explore the exponentially large space of context subsets efficiently under limited query budgets. The reward function measures how well a subset of segments supports the original model response through normalized token likelihoods. Each segment is assigned a binary selection variable, and the CTS algorithm iteratively samples arm selections based on posterior distributions, balancing exploration and exploitation. The method progressively identifies the most relevant segments by maximizing the expected reward under the posterior, converging to an optimal or near-optimal attribution set while minimizing the number of required LLM queries.

## Key Results
- CAMAB achieves competitive or superior attribution quality compared to SHAP and ContextCite baselines while using 5-10x fewer model queries
- On SST2 and HotpotQA datasets, CAMAB consistently identifies relevant context segments with high accuracy, particularly excelling when query budgets are constrained
- Performance degradation is observed on less robust models like SmolLM-1.7B, suggesting attribution quality depends on model reliability
- The method demonstrates effective attribution across both single-sentence sentiment analysis and multi-hop reasoning tasks

## Why This Works (Mechanism)
CAMAB works by transforming context attribution into a principled exploration-exploitation problem. The combinatorial bandit framework allows efficient search through the exponential space of possible context subsets by treating each segment as an arm with an associated reward (likelihood of supporting the answer). CTS provides a probabilistic approach to balance trying new segment combinations (exploration) against refining known good ones (exploitation). The normalized token likelihood reward captures the semantic contribution of each segment to the final answer, while the Bayesian updating ensures that the algorithm learns which segments consistently contribute to high-quality responses. This approach avoids the brute-force nature of perturbation methods while still maintaining rigorous attribution guarantees.

## Foundational Learning
- **Combinatorial Multi-Armed Bandits**: Why needed - To efficiently explore exponentially large space of context subsets under query budget constraints; Quick check - Can the algorithm converge to optimal attribution with fewer than 2^n queries for n segments
- **Thompson Sampling**: Why needed - To balance exploration and exploitation in uncertain attribution scenarios; Quick check - Does the posterior update correctly reflect observed rewards and converge over time
- **Normalized Token Likelihoods**: Why needed - To quantify how well a context subset supports the original answer in a differentiable way; Quick check - Are rewards properly scaled between 0 and 1 regardless of context length
- **Posterior Bayesian Updating**: Why needed - To maintain and update beliefs about segment importance as evidence accumulates; Quick check - Does the observation noise variance appropriately control exploration in posterior sampling
- **Context Segmentation**: Why needed - To break continuous context into discrete units that can be individually evaluated; Quick check - Does segmentation granularity affect attribution quality and computational efficiency

## Architecture Onboarding

**Component Map**
Context Segments -> Reward Calculator -> CTS Algorithm -> Attribution Output

**Critical Path**
The critical path flows from context segmentation through reward calculation to the CTS algorithm, which iteratively refines segment importance estimates until convergence or budget exhaustion.

**Design Tradeoffs**
The primary tradeoff is between attribution accuracy and computational efficiency. Higher query budgets enable more thorough exploration and better attribution quality, while constrained budgets prioritize efficiency at the cost of some accuracy. The choice of observation noise variance σ² also represents a key tradeoff between exploration (high σ²) and exploitation (low σ²).

**Failure Signatures**
Attribution failures typically manifest as either: (1) false negatives where truly relevant segments are excluded due to insufficient exploration, or (2) false positives where noisy or coincidental segments are included due to reward fluctuations. These failures are more likely under tight query budgets or with less robust LLMs that produce inconsistent responses.

**First Experiments**
1. Verify reward normalization works across different context lengths and model outputs
2. Test CTS convergence with synthetic data where ground truth attributions are known
3. Compare attribution stability across multiple runs with identical parameters

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does CAMAB's convergence behavior and attribution quality degrade in highly noisy or ambiguous contexts compared to exhaustive baselines?
- Basis in paper: [explicit] The Limitations section states the approach "may converge to suboptimal local solutions... especially in highly noisy or ambiguous settings."
- Why unresolved: The experiments utilize standard benchmarks (SST2, HotpotQA) but do not specifically evaluate performance on adversarial, obfuscated, or highly ambiguous data samples.
- What evidence would resolve it: A comparative study on synthetic or adversarial datasets measuring convergence rates and attribution error relative to ground truth in high-noise regimes.

### Open Question 2
- Question: Can the exploration-exploitation parameters be adapted to maintain attribution fidelity on less robust or smaller-scale models?
- Basis in paper: [explicit] The results section notes performance degradation on SmolLM-1.7B and hypothesizes that "reliable attribution is more challenging on less robust models."
- Why unresolved: The paper identifies the issue but does not propose or test mechanisms to adjust the sensitivity of the posterior updates for models with higher intrinsic variance.
- What evidence would resolve it: Experiments tuning the observation noise variance (σ²) or prior initialization specifically for low-capacity models to recover attribution performance.

### Open Question 3
- Question: Does the assumption of additive segment contributions in the reward model bias attribution for tasks requiring non-linear reasoning?
- Basis in paper: [inferred] The posterior update assumes v_t = θ_j + ε (Eq. 5), implying segment contributions are additive, whereas multi-hop reasoning (HotpotQA) often relies on synergistic interactions between segments.
- Why unresolved: The paper evaluates multi-hop datasets but does not analyze specific failure cases where the importance of a segment is dependent on the presence of another.
- What evidence would resolve it: Analysis of attribution accuracy on synthetic datasets where ground-truth importance relies on the intersection of multiple context segments.

## Limitations
- Attribution quality is fundamentally coupled to the original LLM response quality, with errors propagating through the reward calculation
- The independence assumption between context segments may oversimplify complex contextual dependencies in real-world scenarios
- Computational savings come at the cost of approximation accuracy, with some true relevant segments potentially excluded under severe budget constraints

## Confidence
- **Attribution Quality Claims** (Medium): Strong experimental evidence but limited to two datasets and two models
- **Query Efficiency Claims** (High): Dramatic reduction in model queries is clearly demonstrated and represents a robust finding
- **Scalability Claims** (Medium): Theoretical framework supports scalability but empirical validation is limited to modest context sizes

## Next Checks
1. **Cross-Model Generalization Test**: Evaluate CAMAB on at least three additional diverse LLMs (including both encoder-decoder and decoder-only architectures) to assess whether the attribution patterns and query efficiency gains generalize beyond the two models tested.

2. **Error Propagation Analysis**: Systematically measure how attribution quality degrades as a function of the original response error rate, establishing the correlation between response accuracy and attribution fidelity under CAMAB.

3. **Contextual Dependency Stress Test**: Design experiments with explicitly constructed context segments that have strong conditional dependencies (e.g., pronoun resolution requiring specific antecedents) to quantify the impact of the independence assumption on attribution accuracy.