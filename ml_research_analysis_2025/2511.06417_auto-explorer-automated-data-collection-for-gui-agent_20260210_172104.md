---
ver: rpa2
title: 'AUTO-Explorer: Automated Data Collection for GUI Agent'
arxiv_id: '2511.06417'
source_url: https://arxiv.org/abs/2511.06417
tags:
- data
- arxiv
- elements
- software
- exploration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of acquiring GUI data for training
  GUI agents, particularly for desktop software and newly launched websites where
  traditional methods using webpage HTML or UI Automation (UIA) fall short. The authors
  propose Auto-Explorer, an automated data collection method with minimal annotation
  costs.
---

# AUTO-Explorer: Automated Data Collection for GUI Agent

## Quick Facts
- arXiv ID: 2511.06417
- Source URL: https://arxiv.org/abs/2511.06417
- Authors: Xiangwu Guo; Difei Gao; Mike Zheng Shou
- Reference count: 40
- Primary result: Proposes Auto-Explorer for automated GUI data collection using novelty-based exploration and hybrid parsing (UIA/OCR/template), achieving superior grounding accuracy (e.g., 0.42 for Creative icons/text, 0.57 for Commercial icons) compared to baselines on the UIXplore benchmark.

## Executive Summary
This paper tackles the challenge of acquiring high-quality data for training GUI agents, particularly for desktop software and websites lacking standard HTML or UI Automation (UIA) access. The authors introduce Auto-Explorer, an automated system that collects diverse screenshots and UI element annotations with minimal manual effort. By employing a hierarchical parser (UIA → OCR → template matching) and a novelty-driven exploration strategy, Auto-Explorer autonomously navigates interfaces to gather information-rich trajectories. The collected data is then synthesized into training queries via GPT-4o and used to fine-tune a small Multimodal Large Language Model (MLLM), significantly improving GUI element grounding accuracy compared to existing methods.

## Method Summary
Auto-Explorer is an automated data collection pipeline for GUI agents operating on Windows desktop software and websites. It uses a hybrid GUI parser combining UI Automation, Google Cloud OCR, and template matching to detect interactive elements when standard methods fail. The exploration module employs a novelty-based sampling strategy, prioritizing interactions with newly discovered UI elements via a "Difference Spot Module" that compares pre- and post-action states. Collected data (screenshots, element names, bounding boxes) is processed by GPT-4o to generate diverse instructional queries (Name, Shape, Function, Referring Expression), which are then used to fine-tune a small MLLM (Qwen2-VL-2B) via LoRA for improved GUI grounding.

## Key Results
- Auto-Explorer achieves grounding accuracy of 0.42 for Creative icons and 0.42 for Creative text in software, and 0.57 for Commercial icons and 0.50 for Commercial text in web environments.
- The method collects significantly more unique exploration actions compared to baselines, demonstrating higher data diversity.
- Fine-tuned models using Auto-Explorer-collected data surpass zero-shot state-of-the-art (OmniParser) on the 4,800-sample UIXplore test set.
- Auto-Explorer shows strong generalization, performing well on unseen software versions and websites.

## Why This Works (Mechanism)

### Mechanism 1: Novelty-Based Action Sampling
- **Claim:** Prioritizing interactions with newly appeared UI elements, rather than random selection, correlates with higher data diversity and downstream model accuracy.
- **Mechanism:** The "Difference Spot Module" compares parsed element lists before and after an action. If new elements appear, these are added to a priority queue. The agent samples subsequent actions from this queue of *new* discoveries rather than revisiting static elements.
- **Core assumption:** Interacting with new interface states yields information gain superior to re-interacting with known static elements.
- **Evidence anchors:**
  - [section 4.2]: "After each action, the agent compares the elements before and after the action to discover new ones. It then randomly samples an action from the newly discovered elements."
  - [section 5.3]: Shows a clear correlation where methods with higher "unique actions" (Auto-Explorer) achieve significantly higher accuracy than those with repetitive actions.
  - [corpus]: GUI-ReWalk and EchoTrail-GUI also emphasize stochastic or memory-guided exploration to maximize state coverage, supporting the focus on non-redundant trajectories.
- **Break condition:** If the "Difference Spot Module" fails to detect state changes (e.g., visual transitions without DOM/OCR changes), the agent terminates the trajectory prematurely.

### Mechanism 2: Fallback Parsing Hierarchy (UIA $\to$ OCR $\to$ Template)
- **Claim:** A hierarchical parsing strategy rescues element detection in environments where standard accessibility tools (UIA) fail, enabling interaction with otherwise opaque software.
- **Mechanism:** The system attempts UI Automation first. If unavailable or insufficient, it switches to Google Cloud OCR for text. For non-text icons (common in creative software), it employs template matching using pre-cropped icon images to locate interactive coordinates.
- **Core assumption:** Icon templates remain visually consistent across explored states, and OCR effectively identifies interactive text buttons.
- **Evidence anchors:**
  - [section 4.1]: "The GUI parser is able to use HTML, and UIA, but more importantly, it can also use OCR... and template matching... when HTML and UIA fail."
  - [table 2]: "Random Walk w. GUI Parser" vastly outperforms "Random Walk w. OCR" (e.g., 0.34 vs 0.07 accuracy on Productive Icons), validating the need for the hybrid approach over OCR alone.
  - [corpus]: Related papers like GUI-Xplore and GUI-Shift rely heavily on VLMs or pure OCR; this paper's explicit fallback to template matching is a distinct structural choice for handling legacy/custom UI frameworks.
- **Break condition:** If UI elements are neither text-based nor match existing templates (e.g., dynamic vector icons), the agent becomes blind to interaction opportunities.

### Mechanism 3: Automated Instructional Synthesis for Fine-Tuning
- **Claim:** Raw exploration logs (screenshots + bounding boxes) can be algorithmically converted into effective training pairs for Multimodal LLMs (MLLMs) using an LLM-in-the-loop (GPT-4o).
- **Mechanism:** The collected data (Name + BBox) is fed to GPT-4o to synthesize diverse query types (Shape, Function, Referring Expression). This synthetic dataset is then used to fine-tune a small MLLM (Qwen2-VL-2B) via LoRA.
- **Core assumption:** The synthesized queries accurately reflect the visual reality of the screenshot, and diversity in query types improves grounding generalization.
- **Evidence anchors:**
  - [section 3.4]: "By using GPT-4o, we aim to generate diverse queries... These data are then used to fine-tune the Qwen2-VL-2B."
  - [section 5.6]: Demonstrates that with sufficient unique samples, the fine-tuned model surpasses the zero-shot SOTA (OmniParser).
  - [corpus]: The general goal of scaling GUI grounding data is consistent across the corpus (e.g., GUI-Xplore), though the specific use of GPT-4o for *query expansion* from raw logs is specific to this architecture.
- **Break condition:** If GPT-4o hallucinates functional descriptions that do not match the visual element, the model trains on noisy labels, degrading grounding accuracy.

## Foundational Learning

- **Concept: UI Automation (UIA) vs. Visual Parsing**
  - **Why needed here:** Auto-Explorer relies on UIA for "free" annotations in standard apps but must switch to visual parsing (OCR/Templates) when UIA is absent. Understanding this distinction is critical for debugging why the agent succeeds in PowerPoint but struggles in custom tools.
  - **Quick check question:** Does the target application expose accessibility tree data to the OS, or is it a "black pixel" canvas requiring visual inference?

- **Concept: GUI Grounding (Visual Localization)**
  - **Why needed here:** The primary evaluation metric (Accuracy @ IoU > 0.3) measures how well the model maps natural language to screen coordinates. The entire data collection loop is optimized to improve this specific capability.
  - **Quick check question:** When the model predicts a bounding box, is it evaluated against a strict pixel match or an intersection-over-union threshold?

- **Concept: Exploration vs. Exploitation in GUI Navigation**
  - **Why needed here:** The "Critic Module" and "Difference Spot Module" embody this trade-off. The agent must decide whether to keep clicking new items (exploration) or stop when the state stabilizes (termination), directly impacting data collection efficiency.
  - **Quick check question:** How does the agent distinguish between a "dead end" (no new elements) and a "loading screen" (temporary stasis)?

## Architecture Onboarding

- **Component map:**
  1. **Environment:** Windows OS + Target Software.
  2. **GUI Parser:** Hybrid module (UIA $\to$ OCR $\to$ Template Matching) outputting {Name, BBox}.
  3. **Explorer:** "Difference Spot Module" (detects state changes) + "Critic Module" (termination logic) + "Sampler" (action selection).
  4. **Data Engine:** GPT-4o query generator $\to$ Qwen2-VL-2B LoRA fine-tuner.

- **Critical path:**
  1. Environment opens.
  2. GUI Parser extracts initial elements.
  3. Explorer selects an action.
  4. Environment updates.
  5. **Critical Step:** Difference Spot Module validates if the action yielded new information. If yes $\to$ continue; if no $\to$ Critic Module halts trajectory.
  6. Data is saved and synthesized for fine-tuning.

- **Design tradeoffs:**
  - **Randomness vs. Efficiency:** The sampler selects randomly from *new* elements (not "best"). This prevents bias but may lead to unproductive paths (e.g., clicking "Help" repeatedly) compared to a reasoning-based planner.
  - **Annotation Cost vs. Precision:** Using automated parsing and GPT-4o synthesis is cheap but noisy compared to human labeling (used only for the test set).

- **Failure signatures:**
  - **Infinite Loops:** Agent clicks a button that changes visual state but not the parsed element list (e.g., toggling a checkbox), confusing the termination logic.
  - **Parser Blindness:** Template matching fails because the UI scaled or changed themes, causing the agent to ignore interactive elements.
  - **Error State Lock:** Agent triggers a modal error popup that the "Critic" fails to classify as an error state, blocking further exploration.

- **First 3 experiments:**
  1. **Parser Ablation:** Run Auto-Explorer on Adobe Premiere (low UIA support) with only OCR vs. Full Parser to validate the template matching contribution.
  2. **Uniqueness Analysis:** Plot "Number of Unique Actions" vs. "Grounding Accuracy" to verify the paper's claim that diversity drives performance (replicating Figure 4).
  3. **Generalization Test:** Fine-tune on data collected from "Commercial" websites and test on "Informational" websites to measure domain transfer capability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the meaningful action trajectories collected by Auto-Explorer be utilized to enhance the planning capabilities of GUI agents?
- Basis in paper: [explicit] The visualization section (5.8) and conclusion state that the collected trajectories "could also be helpful for the planning module in GUI Agent, which we will leave for future work."
- Why unresolved: The current work focuses on data collection for grounding (element detection) rather than training the agent's navigation logic or high-level planning.
- What evidence would resolve it: A follow-up study using the saved trajectories to train a policy network or planning module, showing improved task completion rates on complex, multi-step GUI operations.

### Open Question 2
- Question: How can the recorded error states and anomaly trajectories be integrated into training to improve the robustness and error-recovery capabilities of GUI agents?
- Basis in paper: [explicit] Section 4.2 notes that the Critic Module records error states (e.g., unresponsive actions, warning dialogs), describing this data as "invaluable for developing more robust AI agents capable of handling errors effectively."
- Why unresolved: The paper collects and flags these error states but does not demonstrate a training methodology that leverages this negative data to teach agents how to avoid or recover from such states.
- What evidence would resolve it: An experiment where models fine-tuned with a mixture of successful trajectories and error-recovery pairs show higher resilience to execution failures compared to models trained on successful trajectories alone.

## Limitations
- The novelty-based exploration relies heavily on the accuracy of the "Difference Spot Module"; if visual state changes are not reflected in the parsed element list, the agent may terminate prematurely or enter infinite loops.
- The reliance on template matching for icons introduces brittleness; some icons lack templates, rendering those elements invisible to the agent.
- The automated query synthesis via GPT-4o is a significant efficiency gain but introduces potential hallucination risks that are not fully quantified.

## Confidence
- **High Confidence:** The superiority of Auto-Explorer over baselines in terms of unique actions collected and grounding accuracy is well-supported by the experimental data. The hierarchical parser design (UIA → OCR → Template) is a logical and validated solution for the stated problem.
- **Medium Confidence:** The claim that novelty-based sampling directly causes higher data diversity and accuracy is inferred from correlation (Figure 4) but not proven through ablation studies isolating the exploration strategy. The effectiveness of GPT-4o-generated queries also rests on indirect evidence.
- **Low Confidence:** The generalization claims to unseen software versions and websites are asserted but only briefly tested; the sample size and methodology for these tests are not detailed enough to fully assess robustness.

## Next Checks
1. **Parser Ablation Test:** Implement Auto-Explorer on Adobe Premiere with only OCR vs. the full UIA+OCR+Template parser. Compare unique actions collected and grounding accuracy to isolate the contribution of template matching.
2. **Exploration Strategy Isolation:** Modify the "Sampler" to use random selection from *all* elements (not just new ones) while keeping the rest of Auto-Explorer intact. Compare the number of unique actions and grounding accuracy to the novelty-based approach to directly test the paper's central hypothesis.
3. **Error State Robustness:** Intentionally trigger common error dialogs (e.g., file not found, invalid input) during exploration. Evaluate whether the "Critic Module" correctly identifies and bypasses these states, or if the agent becomes stuck, to validate the robustness of the termination logic.