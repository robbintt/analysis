---
ver: rpa2
title: Comprehensive and Efficient Distillation for Lightweight Sentiment Analysis
  Models
arxiv_id: '2510.24425'
source_url: https://arxiv.org/abs/2510.24425
tags:
- sentiment
- task
- data
- zhang
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COMPEFFDIST, a comprehensive and efficient
  distillation framework for lightweight sentiment analysis models. The method automatically
  constructs diverse sentiment-related instructions by identifying and clustering
  attributes from user texts into analytical perspectives, ensuring comprehensive
  knowledge coverage.
---

# Comprehensive and Efficient Distillation for Lightweight Sentiment Analysis Models

## Quick Facts
- arXiv ID: 2510.24425
- Source URL: https://arxiv.org/abs/2510.24425
- Reference count: 33
- Key outcome: Lightweight 3B student models match 20x larger teacher models using only 10% of the distillation data

## Executive Summary
This paper introduces COMPEFFDIST, a comprehensive and efficient distillation framework for lightweight sentiment analysis models. The method automatically constructs diverse sentiment-related instructions by identifying and clustering attributes from user texts into analytical perspectives, ensuring comprehensive knowledge coverage. It also incorporates a difficulty-based data filtering mechanism that reduces the proportion of simple samples, improving data efficiency. Experiments across Llama-3, Qwen-3, and Gemma-3 model series show that 3B student models can match the performance of 20x larger teacher models on most tasks, and the approach achieves the same performance with only 10% of the distillation data compared to baseline methods.

## Method Summary
COMPEFFDIST employs a two-stage approach for targeted sentiment analysis distillation. First, it constructs comprehensive instructions by extracting sentiment-related attributes from a user-text corpus, clustering them into analytical perspectives, and generating diverse tasks and detailed instructions for each perspective. Second, it filters the distillation data using a difficulty-based mechanism that ranks samples by their complexity and prioritizes harder samples for training. The framework combines these modules to produce a compact but high-quality distillation corpus that transfers teacher knowledge more effectively than baselines.

## Key Results
- 3B student models can match the performance of 20x larger teacher models on most sentiment analysis tasks
- Achieves the same performance level with only 10% of the distillation data compared to baseline methods
- COMPEFFDIST outperforms previous distillation methods on SENTIBENCH benchmark (69.89 avg-F1 vs 68.55 for KNOW&ICLDIST)

## Why This Works (Mechanism)

### Mechanism 1: Attribute-based instruction construction
- **Claim:** Automatic, attribute-based instruction construction improves sentiment analysis distillation by ensuring broader and more diverse coverage of analytical perspectives than manually written instructions.
- **Mechanism:** Teacher LLM extracts sentiment-related attributes from a user-text corpus → attributes are clustered into analytical perspectives → teacher generates diverse tasks and full instructions for each perspective → instruction-user text pairs prompt teacher for responses → student fine-tunes on the resulting triples.
- **Core assumption:** Real-world user texts contain a sufficiently diverse set of sentiment-related attributes that can be systematically surfaced and organized.
- **Evidence anchors:** [abstract] states comprehensive knowledge coverage through attribute clustering; [Section 3.1] details the four-step pipeline and reports ~1,800 attributes and 180 analytical perspectives from 20K user texts.

### Mechanism 2: Difficulty-based data filtering
- **Claim:** Difficulty-based data filtering (via ranking-based scoring and difficulty-prioritized sampling) improves data efficiency by down-weighting simple samples that contribute less to student optimization.
- **Mechanism:** Warm-up student model on a small subset → compute ranking-based difficulty scores for each token in teacher responses → optionally train a proxy model to predict difficulty from (instruction, user text) alone → apply sampling strategy that preferentially retains harder samples.
- **Core assumption:** Simple samples (where student easily replicates teacher output) provide minimal marginal learning signal; curriculum-style emphasis on harder samples improves sample efficiency.
- **Evidence anchors:** [abstract] states reduced proportion of simple samples improves data efficiency; [Section 3.2] defines the ranking-based metric; [Section 5.4] shows filtered 50K data matches unfiltered 90K performance.

### Mechanism 3: Combined comprehensive instruction and efficient filtering
- **Claim:** Combining comprehensive instruction construction with difficulty-based filtering enables small (3B) student models to approximate large (20x bigger) teacher performance on sentiment analysis tasks.
- **Mechanism:** The two modules jointly address instruction diversity (coverage) and data efficiency (signal density), producing a compact but high-quality distillation corpus that transfers teacher knowledge more effectively than baselines.
- **Core assumption:** Both instruction diversity and data difficulty are jointly necessary; improving one without the other yields suboptimal transfer.
- **Evidence anchors:** [abstract] states 3B students match 20x larger teachers; [Table 1] shows Llama-3-3B with COMPEFFDIST achieving 69.89 avg-F1 vs. Llama-3-70B at 72.83 and prior KNOW&ICLDIST at 68.55.

## Foundational Learning

- **Concept: Knowledge Distillation (Targeted)**
  - Why needed here: The entire framework builds on transferring specific capabilities (sentiment analysis) from large to small models; understanding the two-stage distillation pipeline (teacher extraction → student fine-tuning) is prerequisite.
  - Quick check question: Can you explain the difference between generic distillation (e.g., on general instruction corpora) and targeted distillation focused on a domain like sentiment analysis?

- **Concept: Instruction Tuning & Diversity**
  - Why needed here: The paper's first module explicitly constructs diverse instructions to ensure comprehensive knowledge coverage; familiarity with instruction-following LLM tuning and the role of instruction diversity is essential.
  - Quick check question: Why might manually written instructions limit distillation effectiveness, and how does automatic generation help?

- **Concept: Data Efficiency & Curriculum Learning**
  - Why needed here: The difficulty-based filtering module is motivated by curriculum-like ideas; understanding why sample difficulty matters for efficient training is key.
  - Quick check question: In what scenarios might removing simple samples hurt rather than help student model training?

## Architecture Onboarding

- **Component map:** User text corpus → Attribute enumeration → Clustering → Task & Instruction Generation → Pairing with user texts → Difficulty Assessment → Difficulty-Prioritized Sampling → Teacher response generation → Student Fine-tuning

- **Critical path:** User text corpus → Attribute enumeration → Clustering → Instruction generation → Pairing with user texts → Difficulty scoring → Sampling → Teacher response generation → Student fine-tuning

- **Design tradeoffs:**
  - Using the proxy model saves teacher prompting cost but yields lower filtering accuracy than direct ranking-based scoring (Table 3)
  - Random pairing of instructions with user texts performs comparably to attribute-based matching but is simpler and more balanced (Table 15)
  - Hard-only or global sampling underperforms per-instruction difficulty-prioritized sampling (Table 3), likely due to reduced diversity

- **Failure signatures:**
  - Student underperforms on fine-grained tasks (FSA) despite good basic sentiment scores → may indicate insufficient structured-output tasks or imbalance in instruction types
  - Performance gains are marginal on Qwen-3/Gemma-3 → these models already use distillation in pretraining; incremental benefit is smaller
  - Proxy model difficulty predictions correlate poorly with actual student learning → re-evaluate warm-up data size or feature representation

- **First 3 experiments:**
  1. **Ablate instruction diversity:** Train student with only 100 vs. 500 vs. full 3,707 instructions (no filtering) to validate the comprehensiveness claim (replicate Figure 6 trend)
  2. **Ablate filtering strategy:** Compare full COMPEFFDIST with (a) no filtering, (b) hard-only sampling, and (c) global sampling to isolate the contribution of difficulty-prioritized sampling (replicate Table 3)
  3. **Cross-architecture test:** Apply the same COMPEFFDIST corpus to a different student architecture not in the original study (e.g., Mistral-7B or a smaller encoder-decoder model) to assess generalizability beyond Llama/Qwen/Gemma families

## Open Questions the Paper Calls Out

- Can automated task-level deduplication and quality filtering significantly improve data efficiency by removing redundant or low-quality analytical perspectives?
- Can quality assurance mechanisms (e.g., consistency checks) be integrated into the distillation pipeline without negating the computational efficiency gains achieved by data filtering?
- How can the proxy model for difficulty estimation be improved to avoid overestimating the difficulty of complex samples?

## Limitations

- Corpus diversity dependency: Results may not transfer to domains with less attribute variety in user texts
- Difficulty metric calibration: No ablation shows sensitivity to alternative difficulty measures or threshold choices
- Generalizability across architectures: Experiments limited to three model families (Llama-3, Qwen-3, Gemma-3)

## Confidence

- **High confidence:** Core distillation pipeline (teacher response generation, student fine-tuning, SENTIBENCH evaluation) is well-specified and reproducible
- **Medium confidence:** Instruction construction and difficulty filtering mechanisms are supported by ablation studies within the paper, but external validation is absent
- **Low confidence:** Claims about cost-effectiveness and scalability to other domains rely on assumptions not tested in the study

## Next Checks

1. **Cross-architecture ablation:** Apply COMPEFFDIST to a different student architecture (e.g., Mistral-7B or a smaller encoder-decoder model) to assess generalizability beyond the tested Llama/Qwen/Gemma families

2. **Instruction diversity ablation:** Train students with only 100 vs. 500 vs. full 3,707 instructions (no filtering) to validate the comprehensiveness claim and quantify the marginal benefit of instruction diversity

3. **Difficulty metric robustness:** Compare the ranking-based difficulty scores and proxy model predictions against alternative measures (e.g., cross-entropy loss, KL divergence) on a held-out validation set to assess calibration and sensitivity