---
ver: rpa2
title: Learning Time-Scale Invariant Population-Level Neural Representations
arxiv_id: '2511.13022'
source_url: https://arxiv.org/abs/2511.13022
tags:
- interval
- across
- tsap
- neural
- time-scales
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses a critical limitation in neural foundation
  models: sensitivity to time-scale mismatches in preprocessing between pretraining
  and downstream tasks. The authors systematically demonstrate that population-level
  transformer models perform poorly when the input time-scale during finetuning differs
  from the scale used during pretraining.'
---

# Learning Time-Scale Invariant Population-Level Neural Representations

## Quick Facts
- arXiv ID: 2511.13022
- Source URL: https://arxiv.org/abs/2511.13022
- Authors: Eshani Patel; Yisong Yue; Geeling Chau
- Reference count: 23
- One-line primary result: TSAP improves time-scale generalization with significant ROC AUC gains (0.0074-0.0083 for word onset, 0.0045-0.0126 for sentence onset)

## Executive Summary
This paper addresses a critical limitation in neural foundation models: sensitivity to time-scale mismatches in preprocessing between pretraining and downstream tasks. The authors systematically demonstrate that population-level transformer models perform poorly when the input time-scale during finetuning differs from the scale used during pretraining. To solve this, they introduce Time-scale Augmented Pretraining (TSAP), which exposes the model to multiple interval lengths during pretraining. Results show that TSAP consistently improves generalization across seen and unseen time-scales, with significant performance gains (p < 0.05) on decoding tasks like word onset (ROC AUC improvements of 0.0074-0.0083) and sentence onset (0.0045-0.0126). PCA analysis reveals that TSAP produces more invariant representations by reducing time-scale clustering in the embedding space. This work advances the development of robust neural foundation models that can handle diverse preprocessing requirements.

## Method Summary
The authors build on Population Transformer (PopT) architecture, which uses a frozen temporal encoder (BrainBERT) to generate channel-wise embeddings from iEEG signals, followed by a population-level transformer that aggregates spatial information. TSAP modifies the pretraining procedure by sampling intervals from {1s, 2s, 4s, 5s} seconds during training rather than using fixed 5s windows. The population transformer learns to aggregate spatial patterns across temporally heterogeneous inputs, forcing scale-invariant representations. The model is pretrained for 1M steps (double the baseline) at learning rate 1e-4, then finetuned on binary classification tasks (word onset and sentence onset detection) across subjects with 5 random seeds each.

## Key Results
- Non-pretrained PopT shows significant performance drops (0.034-0.184 ROC AUC) when pretraining and finetuning intervals mismatch
- TSAP achieves consistent improvements across all interval lengths, including the held-out 3s interval
- PCA analysis shows TSAP [CLS] tokens have reduced clustering by interval length compared to non-pretrained models
- Statistical significance (p < 0.05) confirmed via paired t-tests across subjects and seeds

## Why This Works (Mechanism)

### Mechanism 1
Population-level transformers trained on fixed time-scales develop brittle, scale-specific representations that degrade when input lengths change. When PopT trains exclusively on 5s intervals, the population layer learns to aggregate spatial information conditioned on temporal embeddings that cluster strongly by duration. At inference time, mismatched intervals produce embeddings outside the learned distribution, causing spatial aggregation to fail.

### Mechanism 2
Exposing the population transformer to multiple interval lengths during pretraining forces it to learn time-scale invariant spatial aggregation patterns. TSAP samples intervals from {1s, 2s, 4s, 5s}, requiring the model to aggregate spatial information despite receiving temporally heterogeneous inputs. This multi-scale objective discourages interval-specific aggregation weights, instead learning spatial relationships that persist across scales.

### Mechanism 3
Time-scale invariant representations transfer to unseen interval lengths through learned interpolation in embedding space. TSAP models trained on {1s, 2s, 4s, 5s} perform competitively on held-out 3s intervals because the population layer has learned smooth, continuous aggregation functions across the interval spectrum rather than discrete scale-specific patterns.

## Foundational Learning

- **Concept: Population-level vs. channel-independent representation learning**
  - Why needed here: The paper builds on PopT which aggregates across channels after temporal encoding. Understanding this two-stage architecture is prerequisite to grasping why time-scale mismatches specifically affect the population layer.
  - Quick check question: Can you explain why the population transformer receives pre-computed embeddings rather than raw signals, and how this design choice creates the time-scale sensitivity problem?

- **Concept: Frozen encoder transfer learning**
  - Why needed here: BrainBERT is frozen during PopT/TSAP training. The population layer cannot adapt the temporal encoder's representations, so it must learn to work with whatever embeddings result from different interval lengths.
  - Quick check question: If BrainBERT were fine-tuned alongside the population layer during TSAP, would you expect more or less time-scale invariance in the final representations? Why?

- **Concept: Invariance vs. equivariance in representation learning**
  - Why needed here: TSAP aims for invariance (same underlying neural activity produces similar representations regardless of interval length). This differs from equivariance (predictable transformations under input changes).
  - Quick check question: For a BCI decoding task, would you always want time-scale invariance, or are there scenarios where preserving temporal scale information would be beneficial?

## Architecture Onboarding

- **Component map:**
  Raw iEEG signals (variable duration) → BrainBERT (frozen temporal encoder) → channel-wise embeddings → Positional embeddings (from 3D electrode coordinates) → Population Transformer encoder → spatially-contextualized channel tokens + [CLS] → Linear projection head (task-specific)

- **Critical path:**
  1. Data preprocessing: Verify interval extraction preserves temporal alignment across channels
  2. BrainBERT embedding generation: Pre-computed and cached for all interval lengths
  3. TSAP pretraining: 1M steps at lr=1e-4, monitor validation loss for checkpoint selection
  4. Finetuning: Per-subject, per-interval experiments with 5 seeds; select best checkpoint by validation ROC-AUC

- **Design tradeoffs:**
  - Compute vs. invariance: TSAP requires 2x pretraining steps
  - Held-out scale selection: Paper holds out 3s (middle of range)
  - Learning rate sensitivity: Authors reduced lr from unspecified original to 1e-4 for stability

- **Failure signatures:**
  - ROC-AUC matches non-pretrained baseline (~0.65-0.68): Population layer failed to learn
  - Strong performance on training scales, near-random on held-out: Overfit to specific intervals
  - High variance across seeds: Instability in finetuning; reduce learning rate or increase warmup
  - Performance degrades monotonically with interval length: Temporal encoder may have length bias

- **First 3 experiments:**
  1. Baseline validation: Reproduce Table 1 mismatch results (train on 5s, test on 1s, 2s, 3s, 4s)
  2. TSAP ablation: Train with only {1s, 5s} vs. full {1s, 2s, 4s, 5s} to test whether dense sampling matters
  3. Embedding inspection: Before/after TSAP, compute pairwise distances between [CLS] tokens from same sample at different intervals

## Open Questions the Paper Calls Out
- Does combining TSAP with architectural invariance methods (e.g., modifying temporal encoders or using fixed patches) yield better generalization than TSAP alone?
- How does the sample efficiency and computational cost of TSAP compare to other approaches for achieving time-scale invariance?
- Does TSAP maintain robust performance on neural signal modalities with different spatial and spectral characteristics than iEEG, such as non-invasive EEG or spike trains?

## Limitations
- The choice of interval lengths {1, 2, 4, 5} seconds appears somewhat arbitrary and not explored for optimality
- Generalization to held-out 3-second intervals represents only moderate extrapolation within a narrow range
- The mechanism by which TSAP achieves invariance remains somewhat speculative - analysis shows clustering differences but doesn't establish true scale-invariant aggregation vs. memorization

## Confidence
- **High confidence**: The empirical demonstration that PopT models are sensitive to time-scale mismatches and that TSAP reduces this sensitivity with statistically significant improvements
- **Medium confidence**: The PCA-based analysis showing reduced clustering by interval in TSAP representations provides visual support but doesn't prove true scale-invariant aggregation
- **Medium confidence**: The claim that TSAP representations are more "useful" because they enable scale-agnostic downstream training, though this is primarily demonstrated through improved ROC-AUC

## Next Checks
1. **Mechanism dissection experiment**: After TSAP pretraining, measure the similarity between [CLS] tokens from the same neural sample at different intervals (e.g., 1s vs 5s). Compare TSAP vs non-TSAP models to quantify the degree of scale-invariance learned.

2. **Extreme interval generalization test**: Train TSAP models with intervals {1s, 2s, 4s, 5s} but evaluate on held-out intervals at the extremes (0.5s and 10s) to determine whether the learned invariance extrapolates beyond the training range.

3. **Representation probing analysis**: Use a simple linear probe to classify interval lengths from frozen TSAP representations vs non-TSAP representations. TSAP should show worse interval classification accuracy (more invariant), while maintaining or improving task performance.