---
ver: rpa2
title: 'Towards Transparent AI Grading: Semantic Entropy as a Signal for Human-AI
  Disagreement'
arxiv_id: '2508.04105'
source_url: https://arxiv.org/abs/2508.04105
tags:
- entropy
- disagreement
- semantic
- human
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces semantic entropy\u2014a measure of variability\
  \ across multiple GPT-4-generated explanation rationales\u2014as a proxy for human\
  \ grader disagreement in automated short-answer grading. The method clusters rationales\
  \ using bidirectional entailment and computes entropy to quantify semantic diversity,\
  \ aiming to flag cases where grading decisions are uncertain or contentious."
---

# Towards Transparent AI Grading: Semantic Entropy as a Signal for Human-AI Disagreement

## Quick Facts
- arXiv ID: 2508.04105
- Source URL: https://arxiv.org/abs/2508.04105
- Reference count: 23
- Primary result: Semantic entropy correlates with human grader disagreement (r = 0.172) and varies meaningfully across subjects and task structures

## Executive Summary
This paper introduces semantic entropy—a measure of variability across multiple GPT-4-generated explanation rationales—as a proxy for human grader disagreement in automated short-answer grading. The method clusters rationales using bidirectional entailment and computes entropy to quantify semantic diversity, aiming to flag cases where grading decisions are uncertain or contentious. Experiments on the ASAP-SAS dataset show semantic entropy correlates with human disagreement, varies meaningfully across subjects (strongest in Biology/English, weakest in Science), and increases in source-dependent tasks. These findings position semantic entropy as an interpretable, task-sensitive signal for surfacing grading uncertainty and supporting transparent, trustworthy AI-assisted assessment workflows.

## Method Summary
The method generates K=6 short rationales (≤30 words) per response using GPT-4, clusters them via pairwise bidirectional entailment (also using GPT-4), and computes semantic entropy over the resulting cluster distribution. Entropy is then correlated with human grader disagreement (δᵢ = |y₁ - y₂|) to assess its effectiveness as an uncertainty proxy. The pipeline operates domain-agnostically using a single prompt template, with multimodal inputs preprocessed to text when necessary.

## Key Results
- Semantic entropy correlates with human disagreement at r = 0.172 (p = 1.39 × 10⁻¹⁹)
- Entropy varies significantly across subjects (Biology/English highest, Science lowest)
- Source-dependent tasks show substantially higher entropy (H = 0.551 vs 0.257) and stronger disagreement correlation (r = 0.218 vs 0.109)

## Why This Works (Mechanism)

### Mechanism 1: Semantic Entropy as Uncertainty Signal
- Claim: Measuring semantic variability across multiple AI-generated rationales serves as a proxy for epistemic uncertainty in grading decisions
- Mechanism: Multiple independent rationale samples capture different reasoning paths; clustering via entailment groups semantically equivalent justifications; entropy quantifies diversity across clusters (H = -Σpⱼlog pⱼ)
- Core assumption: When rationales diverge semantically, the underlying grading decision is more likely to be ambiguous or contentious—mirroring human grader uncertainty
- Evidence anchors: [abstract] "semantic entropy—a measure of variability across multiple GPT-4-generated explanation rationales—as a proxy for human grader disagreement"; [section 4.3] "Pearson correlation of r = 0.172 (p = 1.39 × 10⁻¹⁹)"
- Break condition: If LLM rationale diversity reflects sampling noise rather than genuine task ambiguity, the entropy signal becomes unreliable for triage

### Mechanism 2: Entailment-Based Clustering Captures Semantic Equivalence
- Claim: Bidirectional entailment clustering effectively groups semantically equivalent rationales, enabling meaningful entropy computation
- Mechanism: Pairwise entailment checks (at T=0) determine if two rationales express the same grading logic; semantically similar justifications cluster together regardless of surface linguistic variation
- Core assumption: Bidirectional entailment accurately captures when two rationales express equivalent grading reasoning
- Evidence anchors: [section 3] "clustered into equivalence classes {C₁, ..., Cₘ} via pairwise bidirectional entailment"; [section 4.3] "we apply pairwise bidirectional entailment checks (using GPT-4 at T = 0)"
- Break condition: If entailment model produces inconsistent cluster boundaries, entropy values become unreliable—especially for nuanced rubric interpretations

### Mechanism 3: Task Structure Modulates Entropy-Disagreement Alignment
- Claim: Source-dependent tasks with interpretive complexity produce higher semantic entropy and stronger alignment with human disagreement
- Mechanism: External grounding (passages, diagrams) increases cognitive complexity and rubric ambiguity, creating more diverse valid justification pathways that both humans and models explore
- Core assumption: Task complexity genuinely increases the space of valid interpretations rather than introducing sampling artifacts
- Evidence anchors: [section 6.3] "source-dependent responses exhibit substantially higher semantic entropy (H = 0.551) compared to non-source-dependent (H = 0.257), ∆ = +0.294, p = 1.27 × 10⁻⁴⁴"; [section 5.3] "entropy correlates more strongly with human grader disagreement in source-dependent tasks (r = 0.218) than in non-source-dependent ones (r = 0.109)"
- Break condition: If entropy differences stem from response length or rubric wording rather than interpretive complexity, the proposed task taxonomy becomes misleading

## Foundational Learning

- Concept: Bidirectional Entailment
  - Why needed here: The entire semantic entropy computation depends on correctly grouping rationales that express equivalent reasoning
  - Quick check question: If rationale A entails B but B does not entail A, should they be clustered together? Why or why not?

- Concept: Entropy over Discrete Distributions
  - Why needed here: Interpreting H(x) values requires understanding what high vs. low entropy means for cluster distributions
  - Quick check question: For K=6 rationales, what's the maximum possible entropy? What does H=0 indicate?

- Concept: Epistemic vs. Aleatoric Uncertainty
  - Why needed here: The paper claims semantic entropy captures epistemic uncertainty; distinguishing uncertainty types is critical for system design
  - Quick check question: Would increasing temperature from 0.7 to 1.2 increase epistemic or aleatoric uncertainty in this pipeline?

## Architecture Onboarding

- Component map:
  Student response + rubric + optional source material → Rationale Generator → K=6 rationales → pairwise entailment checks → clustering → m equivalence classes → Entropy Calculator → H(x) ∈ [0, log(K)]

- Critical path:
  1. Student response + rubric + optional source material → Rationale Generator → K=6 rationales
  2. K rationales → pairwise entailment checks → K×K similarity matrix
  3. Similarity matrix → transitive clustering → m equivalence classes
  4. Class sizes → Entropy Calculator → H(x) = -Σ(|Cⱼ|/K)log(|Cⱼ|/K) over cluster distribution
  5. H(x) threshold applied for triage decision (flag for human review)

- Design tradeoffs:
  - Same model for generation and entailment: Introduces potential circularity; paper acknowledges need to decouple in future work
  - K=6 samples: Balances computational cost against cluster reliability; Assumption: sufficient for stable entropy estimates
  - Temperature=1.0: Ensures rationale diversity but may amplify sampling noise; optimal temperature unvalidated
  - 30-word limit: Constrains justification depth; may truncate nuanced reasoning for complex responses

- Failure signatures:
  - High entropy + low human disagreement: Rubric permits multiple valid interpretations; review rubric clarity
  - Low entropy + high human disagreement: Model overconfidence; risk of silent errors in automated scoring
  - Near-zero entropy across dataset: Entailment model too permissive; check clustering quality
  - High variance on identical inputs: Sampling noise dominates signal; consider reducing temperature or increasing K

- First 3 experiments:
  1. Baseline comparison: Benchmark semantic entropy against simpler uncertainty measures (score variance across K samples, response length, LLM confidence) on held-out ASAP-SAS data; report which signal best predicts human disagreement
  2. Model decoupling validation: Use GPT-4 for rationale generation but DeBERTa-NLI or similar for entailment clustering; compare correlation with human disagreement to assess circularity bias magnitude
  3. Temperature sensitivity analysis: Run full pipeline at T ∈ {0.5, 0.7, 0.9, 1.0, 1.2} on 500-response subset; plot entropy-disagreement correlation vs. temperature to identify optimal operating point and quantify noise contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does decoupling rationale generation from entailment clustering (using independent models) improve semantic entropy's alignment with human disagreement?
- Basis in paper: [explicit] "Our current pipeline uses GPT-4 for both generating rationales and performing entailment-based clustering. This raises concerns about bias and methodological circularity, which we plan to address by decoupling the generation and clustering components using independent models."
- Why unresolved: A single model handling both tasks may introduce artifacts where its own entailment judgments artificially align with its generated rationales, potentially biasing entropy estimates.
- What evidence would resolve it: Experiments comparing entropy computed with unified vs. separate models (e.g., GPT-4 for generation, DeBERTa for entailment), measuring change in correlation with human disagreement.

### Open Question 2
- Question: What proportion of observed semantic entropy reflects genuine epistemic uncertainty versus stochastic sampling noise?
- Basis in paper: [explicit] "Some degree of entropy may be attributed to stochastic sampling noise rather than genuine epistemic uncertainty. Future experiments will explore robustness across temperature settings and sampling strategies."
- Why unresolved: The current study uses temperature T=1.0 without systematic ablation, leaving unclear whether entropy signal is robust across sampling configurations.
- What evidence would resolve it: Ablation studies across temperature settings (e.g., T=0.3–1.5) and sampling methods, analyzing stability of entropy–disagreement correlation.

### Open Question 3
- Question: Does semantic entropy provide incremental predictive value beyond simpler baselines such as score variance, response length, or model confidence?
- Basis in paper: [explicit] "Our approach does not yet benchmark against simpler uncertainty measures such as score variance, response length, or softmax confidence. Comparing semantic entropy with these baselines will help clarify its unique contributions."
- Why unresolved: Without comparative baselines, the computational cost of multi-sample rationale generation and clustering may not be justified.
- What evidence would resolve it: Head-to-head regression or classification experiments comparing semantic entropy against simpler proxies as predictors of human disagreement.

### Open Question 4
- Question: Do rationale clusters reflect pedagogically meaningful reasoning differences, or primarily superficial linguistic variation?
- Basis in paper: [explicit] "Validation of our rationale clusters through human annotation studies [is needed] to determine whether the observed variability reflects distinct rubric-aligned reasoning paths or superficial linguistic variation."
- Why unresolved: Entailment-based clustering captures semantic similarity but does not guarantee that different clusters represent substantively distinct grading justifications.
- What evidence would resolve it: Human expert annotation of sampled clusters, rating whether intra-cluster rationales are rubric-meaningful vs. paraphrastic.

## Limitations
- Weak correlation (r = 0.172) may limit practical utility for triage systems despite statistical significance
- Reliance on GPT-4 for both generation and entailment introduces potential circularity and methodological artifacts
- Task structure effects not definitively proven to reflect genuine interpretive complexity rather than confounding factors

## Confidence
- High confidence: Semantic entropy computation methodology is sound; pairwise entailment clustering correctly implements the proposed approach; statistical significance of correlations is valid
- Medium confidence: Correlation magnitude (r = 0.172) may be sufficient for triage applications; task structure effects reflect interpretive complexity rather than confounding factors
- Low confidence: Practical utility for real-world grading workflows; optimal temperature and K parameters; whether the weak correlation limits actionable insights

## Next Checks
1. Benchmark semantic entropy against simpler uncertainty measures (score variance, response length, LLM confidence) on held-out ASAP-SAS data to assess relative predictive power
2. Decouple generation and entailment models by using GPT-4 for rationales but DeBERTa-NLI for clustering to quantify circularity bias
3. Perform temperature sensitivity analysis (T ∈ {0.5, 0.7, 0.9, 1.0, 1.2}) to identify optimal operating point and separate epistemic from aleatoric uncertainty