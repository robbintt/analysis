---
ver: rpa2
title: 'MMRQA: Signal-Enhanced Multimodal Large Language Models for MRI Quality Assessment'
arxiv_id: '2509.24888'
source_url: https://arxiv.org/abs/2509.24888
tags:
- quality
- mmrqa
- assessment
- metrics
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MMRQA introduces a novel multimodal framework for MRI quality assessment
  that combines signal-based metrics with large language model reasoning. The approach
  extracts quantitative metrics using MRQy, transforms them into structured QA pairs
  via Qwen, and performs parameter-efficient fusion with LLaVA-OneVision through LoRA.
---

# MMRQA: Signal-Enhanced Multimodal Large Language Models for MRI Quality Assessment

## Quick Facts
- arXiv ID: 2509.24888
- Source URL: https://arxiv.org/abs/2509.24888
- Reference count: 0
- Achieves state-of-the-art performance with 88.1% accuracy and 85.7% F1-score on MR-ART dataset

## Executive Summary
MMRQA introduces a novel multimodal framework for MRI quality assessment that combines quantitative signal-based metrics with large language model reasoning capabilities. The approach addresses the critical gap between automated assessment and clinical interpretability in MRI quality control by extracting quantitative metrics using MRQy, transforming them into structured QA pairs via Qwen, and performing parameter-efficient fusion with LLaVA-OneVision through LoRA. The framework demonstrates strong zero-shot generalization and generates clinically interpretable quality descriptions that are evaluated by GPT-5 scoring.

## Method Summary
The MMRQA framework integrates signal-based MRI quality assessment with LLM-based reasoning through a three-stage pipeline. First, MRQy extracts quantitative metrics from MRI scans including signal-to-noise ratio, contrast-to-noise ratio, and other quality indicators. Second, these metrics are transformed into structured QA pairs using Qwen, which generates clinically relevant questions about image quality. Third, the framework performs parameter-efficient fusion with LLaVA-OneVision through LoRA adapters, enabling the multimodal model to reason about both the visual content and quantitative metrics. This architecture allows MMRQA to provide both accurate quality assessment scores and clinically interpretable descriptions of MRI quality issues.

## Key Results
- Achieves 88.1% accuracy and 85.7% F1-score on MR-ART dataset
- Generates clinically interpretable quality descriptions scored at 85.4/100 by GPT-5 evaluation
- Demonstrates strong zero-shot generalization across multiple datasets including FastMRI and MyConnectome

## Why This Works (Mechanism)
MMRQA works by bridging the gap between quantitative signal metrics and clinical interpretability through multimodal fusion. The signal-based metrics provide objective, reproducible measures of image quality that capture technical aspects like noise levels and contrast. The LLM component adds clinical reasoning capabilities, translating these metrics into understandable quality descriptions that align with radiologist assessment criteria. The parameter-efficient LoRA fusion allows the model to learn complex relationships between quantitative metrics and visual features without requiring full fine-tuning of the large vision-language model.

## Foundational Learning
- MRQy metric extraction: Why needed - provides objective quantitative measures of image quality; Quick check - verify MRQy outputs are consistent with known quality degradation patterns
- Structured QA pair generation: Why needed - bridges quantitative metrics with clinical reasoning; Quick check - validate generated questions cover clinically relevant quality aspects
- LoRA parameter-efficient fine-tuning: Why needed - enables adaptation of large models without full fine-tuning costs; Quick check - compare LoRA performance against full fine-tuning on validation set

## Architecture Onboarding
Component map: MRQy -> Qwen (QA pair generation) -> LLaVA-OneVision (LoRA fusion) -> Quality assessment output
Critical path: Signal extraction → Structured QA generation → Multimodal reasoning → Quality classification and description
Design tradeoffs: Parameter efficiency vs. performance (LoRA vs. full fine-tuning), LLM interpretability vs. metric precision, zero-shot generalization vs. dataset-specific optimization
Failure signatures: Metric extraction errors propagate through pipeline, LLM hallucinations in quality descriptions, domain shift in zero-shot scenarios
Three first experiments: 1) Ablation study removing signal metrics to measure LLM-only contribution, 2) Cross-dataset evaluation on unseen MRI protocols, 3) Human expert comparison of generated descriptions vs. radiologist assessments

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to specific datasets that may not capture full clinical heterogeneity
- GPT-5 based clinical interpretability scoring may not align with actual radiologist judgment
- Single point of failure through MRQy metric extraction dependency

## Confidence
- High confidence in technical implementation and quantitative results
- Medium confidence in clinical interpretability claims
- Low confidence in generalizability across diverse clinical settings

## Next Checks
1. External validation on heterogeneous clinical MRI datasets from multiple institutions with varying protocols, field strengths, and scanner manufacturers
2. Direct comparison of MMRQA-generated quality descriptions with radiologist assessments using blinded expert review rather than LLM-based scoring
3. Ablation studies to quantify the individual contributions of signal-based metrics versus LLM reasoning to overall performance, including analysis of cases where the two modalities disagree