---
ver: rpa2
title: 'OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks'
arxiv_id: '2508.05614'
source_url: https://arxiv.org/abs/2508.05614
tags:
- reasoning
- task
- tasks
- embodied
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OmniEAR, a comprehensive framework for benchmarking
  agent reasoning in embodied tasks. Unlike existing benchmarks, OmniEAR requires
  agents to dynamically acquire capabilities and autonomously determine coordination
  strategies based on task demands, using text-based environment representation to
  model continuous physical properties and complex spatial relationships across 1,500
  scenarios spanning household and industrial domains.
---

# OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks

## Quick Facts
- **arXiv ID**: 2508.05614
- **Source URL**: https://arxiv.org/abs/2508.05614
- **Reference count**: 31
- **Primary result**: Text-based embodied reasoning benchmark reveals severe performance degradation on constraint reasoning (85-96% → 56-85%) and minimal multi-agent gains from fine-tuning (1.5% → 5.5%).

## Executive Summary
OmniEAR introduces a comprehensive framework for benchmarking agent reasoning in embodied tasks, requiring dynamic capability acquisition and autonomous coordination strategies. Using text-based environment representation, the benchmark evaluates models across 1,500 scenarios spanning household and industrial domains with continuous physical properties and complex spatial relationships. Systematic evaluation reveals that while models achieve 85-96% success with explicit instructions, performance drops dramatically to 56-85% for tool reasoning and 63-85% for implicit collaboration, with compound tasks showing over 50% failure rates.

The benchmark exposes fundamental architectural limitations: fine-tuning improves single-agent tasks dramatically (0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), indicating models cannot transfer learned reasoning capabilities to collaborative settings. Surprisingly, providing complete environmental information degrades coordination performance, suggesting models cannot effectively filter task-relevant constraints from environmental noise. These findings establish OmniEAR as a rigorous benchmark for evaluating and advancing embodied AI systems beyond current capabilities.

## Method Summary
OmniEAR employs a text-based environment representation modeling continuous physical properties through directed graphs (G_t = (V_t, E_t, A_t)) where nodes store continuous attributes and edges store spatial topology. The benchmark includes 1,500 scenarios across household and industrial domains with 7 task categories: Direct Command, Attribute Reasoning, Tool Use, Compound Reasoning, Explicit Collaboration, Implicit Collaboration, and Compound Collaboration. Evaluation uses Success Rate (%), Step Count (avg actions), and Relative Step Ratio (RSR = Lexpert/Lmodel). Single-agent SFT training uses 20,346 instruction-action pairs from Qwen2.5-72B expert trajectories with full-parameter fine-tuning (Qwen2.5-3B-Instruct, batch 64, lr=1e-5, 3 epochs).

## Key Results
- Performance drops from 85-96% (explicit instructions) to 56-85% (tool reasoning) and 63-85% (implicit collaboration)
- Compound tasks show over 50% failure rates, with performance decreasing from 40.1% to 16.0% across complexity levels
- Complete environmental information degrades multi-agent coordination performance
- Fine-tuning yields dramatic single-agent improvements (0.6% → 76.3%) but minimal multi-agent gains (1.5% → 5.5%)

## Why This Works (Mechanism)

### Mechanism 1
Text-based environment representation may enable efficient scaling of continuous physical property reasoning without computational overhead of 3D simulation. The system models environments as directed graphs where nodes store continuous attributes (weight, temperature) and edges store spatial topology, with state updates modifying only affected nodes/edges rather than recomputing global physics. Core assumption: agents can ground physical constraints effectively through text descriptions alone. Evidence anchors: "text-based environment representation to model continuous physical properties... efficiently" and "eliminates expensive collision detection while preserving essential spatial constraints." Break condition: if tasks require fine-grained visual discrimination (e.g., reading labels, texture analysis), the text abstraction fails to capture necessary signal.

### Mechanism 2
Dynamic capability binding exposes whether models can autonomously infer missing abilities rather than selecting from static action sets. Tools possess `provides_abilities` attributes, and when an agent executes `grasp(v_tool)`, the system binds these capabilities to the agent's action set; `release(v_tool)` unbinds them. This forces the model to plan tool acquisition before executing tool-dependent actions. Core assumption: the model maintains an internal representation of its current capability state during planning. Evidence anchors: "dynamically acquire capabilities... based on task demands" and "action sets determined at initialization... preventing assessment of dynamic capability acquisition." Break condition: if models memorize specific tool-action pairs without reasoning about the underlying capability gap, they fail to generalize to novel tools.

### Mechanism 3
Implicit collaboration scenarios, triggered only when physical constraints exceed single-agent capacity, test autonomous recognition of coordination necessity. The system validates physical preconditions (e.g., `weight > C_max(agent)`). If an agent attempts an infeasible action, it triggers a failure that ideally prompts collaboration logic, rather than relying on explicit instructions like "cooperate with Agent B." Core assumption: models can infer comparative physical relationships (e.g., my load capacity < object weight) from text. Evidence anchors: "autonomously determine coordination strategies based on task demands" and "Implicit Collaboration falls to 63-85% compared to 88-92% with explicit coordination." Break condition: if the "World Graph" provides too much environmental noise, models fail to isolate the specific constraint triggering collaboration.

## Foundational Learning

- **Concept: Discrete vs. Continuous State Representation**
  - Why needed here: Standard benchmarks use binary states (open/closed). OmniEAR requires reasoning over continuous values (weight=50, capacity=30) to determine feasibility.
  - Quick check question: Can your model determine that `action(lift, object(weight=50))` fails if `agent(max_weight=30)` without explicit failure feedback?

- **Concept: Dynamic Action Spaces**
  - Why needed here: Unlike static APIs, the available actions change during execution based on inventory.
  - Quick check question: If an agent releases a "wrench," should the "tighten" action still be valid in the next step?

- **Concept: Information Filtering for Coordination**
  - Why needed here: The paper finds that full environmental info degrades performance. Agents must learn to ignore irrelevant objects.
  - Quick check question: Given a list of 50 room items, can the agent identify that only the "heavy table" is relevant to the "move furniture" command?

## Architecture Onboarding

- **Component map**: EAR-Sim -> Task Generator -> Evaluator
- **Critical path**: 
  1. Input: Agent receives partial observation + instruction
  2. Inference: Agent outputs Thought + Action
  3. State Update: EAR-Sim updates graph (location, tool possession, object state)
  4. Capability Check: System verifies if action is in agent's current valid set (base + tools)
  5. Loop: Until DONE or max steps

- **Design tradeoffs**:
  - Text vs. Vision: Choosing text inputs allows precise attribute specification (weight, material) but loses visual grounding.
  - Explicit vs. Implicit: Removing coordination instructions makes the benchmark harder but better isolates "reasoning" from "instruction following."

- **Failure signatures**:
  - Premature DONE: Agent claims completion without satisfying Ggoal (hallucinating success)
  - Exploration Loops: Agent repeats EXPLORE without integrating discovered objects into the plan
  - Tool Amnesia: Agent grasps tool but fails to utilize the new capability in subsequent steps
  - Coordination Paralysis: In Implicit Collab, agents attempt individual action on heavy object → fail → retry (no strategy switch)

- **First 3 experiments**:
  1. Establish Baseline: Run Qwen-7B on Direct Command (L1) vs. Compound Reasoning (L3) to replicate the 40% → 16% performance drop cited in Table 1
  2. Ablate Context: Test Implicit Collaboration with "World Graph" ON vs. OFF to verify the information overload hypothesis (expect performance drop with full info)
  3. SFT Transfer Check: Fine-tune a 3B model on single-agent trajectories and evaluate on multi-agent tasks to confirm the paper's finding that single-agent gains (0.6%→76.3%) do not transfer to multi-agent (1.5%→5.5%)

## Open Questions the Paper Calls Out
None

## Limitations
- Text-based representation may inadequately capture visual reasoning requirements critical in real-world embodied tasks
- Benchmark focus on household and industrial domains may not extend to more complex environments requiring fine-grained visual perception
- SFT methodology shows fundamental limitations in multi-agent coordination, suggesting architectural constraints beyond simple fine-tuning

## Confidence

**High Confidence**: The systematic performance degradation across reasoning levels (Direct Command 85-96% → Compound Reasoning 28.7%) and the finding that complete environmental information degrades coordination performance are well-supported by the experimental data.

**Medium Confidence**: The claim that architectural limitations prevent multi-agent gains from single-agent fine-tuning requires further investigation, as the study uses only one fine-tuning approach and dataset size may be insufficient.

**Low Confidence**: The assertion that text-based representation "efficiently" models continuous physical properties lacks direct comparison to visual alternatives in terms of reasoning quality versus computational cost.

## Next Checks

1. **Visual vs. Text Ablation Study**: Compare OmniEAR performance using text-only versus image-based environment representations on identical task sets to quantify the reasoning quality trade-off.

2. **Cross-Domain Generalization**: Evaluate the same models on OmniEAR tasks versus real-world robotics datasets (e.g., ALFRED, RoboTHOR) to assess domain transfer and identify specific reasoning gaps.

3. **Architectural Intervention Test**: Implement and evaluate attention-based constraint filtering mechanisms to determine whether the information overload hypothesis can be mitigated through architectural modifications rather than just dataset reduction.