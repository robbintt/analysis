---
ver: rpa2
title: 'XiYan-SQL: A Novel Multi-Generator Framework For Text-to-SQL'
arxiv_id: '2507.04701'
source_url: https://arxiv.org/abs/2507.04701
tags:
- multiple
- selection
- generation
- performance
- schema
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: XiYan-SQL is a multi-generator framework for text-to-SQL generation
  that addresses the challenge of improving accuracy and diversity in SQL candidate
  generation. It uses a schema filter to obtain high-quality schemas, employs multiple
  fine-tuned SQL generators with distinct generation styles, and incorporates a selection
  model with candidate reorganization to choose the optimal SQL query.
---

# XiYan-SQL: A Novel Multi-Generator Framework For Text-to-SQL

## Quick Facts
- arXiv ID: 2507.04701
- Source URL: https://arxiv.org/abs/2507.04701
- Reference count: 40
- Primary result: Achieves 75.63% execution accuracy on BIRD benchmark and 89.65% on Spider test set

## Executive Summary
XiYan-SQL introduces a multi-generator framework that addresses the challenge of improving both accuracy and diversity in Text-to-SQL generation. The framework uses a schema filter to obtain high-quality, reduced schemas, employs multiple fine-tuned SQL generators with distinct generation styles, and incorporates a selection model with candidate reorganization to choose the optimal SQL query. The approach achieves state-of-the-art performance on major benchmarks while demonstrating strong generalization capabilities.

## Method Summary
The framework follows a three-stage pipeline: (1) Schema filtering uses multi-path retrieval and iterative LLM-based column selection to produce diverse, high-quality schemas; (2) Multiple SQL generators (4 supervised fine-tuned + 1 ICL) produce candidates using multi-task learning (text-to-SQL, question inference, evidence inference, self-refinement) and varied SQL formats; (3) A selection model with candidate reorganization (clustering by execution results and sorting by generator performance) chooses the final query. The approach is trained and evaluated on BIRD and Spider benchmarks.

## Key Results
- Achieves 75.63% execution accuracy on BIRD benchmark, state-of-the-art performance
- Achieves 89.65% execution accuracy on Spider test set
- Selection model with reorganization improves from 70.21% to 73.34% compared to majority voting

## Why This Works (Mechanism)

### Mechanism 1: Schema Filtering with Precision-Recall Diversity
Filtering schemas through multi-path retrieval and iterative column selection improves SQL generation quality by reducing noise while maintaining recall above a critical threshold. The Schema Filter first retrieves relevant tables, columns, and values using keyword extraction and cosine similarity. Then, iterative LLM-based column selection creates multiple filtered schemas (S1, S2) with different precision-recall tradeoffs—early iterations favor precision, later iterations favor recall. LLMs perform better when schema size is reduced to relevant columns, and diversity in schema coverage helps multi-generator ensembles explore different reasoning paths.

### Mechanism 2: Multi-Task and Multi-Format Fine-Tuning for Generator Diversity
Fine-tuning SQL generators with auxiliary alignment tasks and varied SQL formats produces higher-quality and more diverse candidates than prompt engineering alone. Three auxiliary tasks are added: reverse question inference from SQL, evidence inference, and self-refinement based on execution results. Additionally, SQL training data is reformatted to encourage different writing patterns (complex structures vs. standardized styles), creating generators with distinct biases. Multi-task learning improves semantic alignment between NL and SQL; format diversity translates to behavioral diversity at inference.

### Mechanism 3: Candidate Reorganization and Selection Model
A fine-tuned selection model with candidate reorganization outperforms majority voting by exploiting execution consistency and model performance priors. Candidates are clustered by execution results. Clusters are sorted by size (inter-group) and generator performance ranking (intra-group). If one cluster has majority (≥50%), all candidates are presented in order; otherwise, one representative per cluster is selected. A 7B selection model fine-tuned on comparative samples makes the final choice. LLMs are sensitive to candidate order; reorganization focuses attention on likely-correct clusters.

## Foundational Learning

- **Schema Linking**: Text-to-SQL requires mapping natural language to specific database columns/tables. Large schemas overwhelm LLMs; filtering is prerequisite for quality generation. Quick check: Can you identify which columns in a database are relevant to the query "Show average salary by department"?

- **Supervised Fine-Tuning (SFT) vs. In-Context Learning (ICL)**: XiYan-SQL uses both: SFT for high-quality generators with controllable behavior, ICL (GPT-4o) for diversity. Understanding tradeoffs is essential for replication. Quick check: What are the latency and data requirements differences between SFT and ICL for a 32B model?

- **Self-Consistency and Majority Voting**: Baseline selection method that XiYan-SQL improves upon. Understanding its limitations (fails without majority, ignores minority correctness) motivates the selection model. Quick check: If 5 candidates have execution results [A, A, B, C, D], what would majority voting select and when would it fail?

## Architecture Onboarding

- **Component map**:
```
Input: Question + Evidence + Database Schema
    ↓
[Schema Filter] → Multi-path Retrieval + Iterative Column Selection → Schemas {S1, S2}
    ↓
[Multiple SQL Generation] → 5 Generators (4 SFT + 1 ICL) × 2 Schemas → 10 Candidates
    ↓
[SQL Selection] → Execution + Clustering + Reorganization + Selection Model → Final SQL
```

- **Critical path**: Schema Filter quality (P/R balance directly affects downstream generation) → Multi-task fine-tuning data quality (auxiliary tasks must be correctly constructed) → Selection model training (comparative samples must cover diverse failure modes)

- **Design tradeoffs**: Latency vs. Accuracy (10 candidates → ~40.5s inference; reducing candidates lowers latency but also upper-bound accuracy) → Quality vs. Diversity (Specialized generators sacrifice individual accuracy for candidate pool diversity) → Open vs. Closed Models (Framework uses GPT-4o for schema filter and ICL; could substitute with open models for full self-hosting)

- **Failure signatures**: Schema Filter returns empty schema → SQL generators fail with missing table errors → All candidates have same execution error → Selection model has no valid option → Selection model biased toward shorter SQL → May reject correct complex queries

- **First 3 experiments**: 1) Reproduce single-generator baseline: Fine-tune Qwen2.5-Coder-32B on standard Text-to-SQL data without multi-task; verify EX ~64.67% on BIRD dev 2) Validate Schema Filter contribution: Compare full schema vs. filtered schemas (S1, S2) on held-out subset; confirm ~1-2% EX improvement 3) Ablate selection strategy: Compare majority voting vs. selection model with/without reorganization on 10 candidates; target ~3% improvement gap

## Open Questions the Paper Calls Out

- **Open Question 1**: How can fine-tuning strategies be enhanced to overcome the inherent diversity limitations of supervised models, thereby reducing reliance on ICL-based methods? The authors note that while their method alleviates diversity issues, "inherent constraints mean that the diversity it can achieve is still limited."

- **Open Question 2**: Can an "all-in-one" SQL model integrating a broader range of tasks match the performance of the multi-stage pipeline? The authors state they are investigating "the integration of a broader range of tasks into an all-in-one SQL model... beyond SQL generation."

- **Open Question 3**: What specific additional auxiliary training tasks could further improve the trade-off between quality and diversity? The authors mention that "further investigation into additional training tasks and methods remains to be explored in future research."

## Limitations

- Training hyperparameters and prompt templates are not fully specified, making exact replication challenging
- Reliance on GPT-4o for critical components introduces cost and availability dependencies
- Limited extensive validation of generalization claims on out-of-domain datasets

## Confidence

- **High Confidence**: The core architectural design and claimed improvements over baseline majority voting are well-supported by execution accuracy numbers
- **Medium Confidence**: The Schema Filter's iterative mechanism is described but lacks complete detail on prompts and selection criteria
- **Low Confidence**: Generalization claims on out-of-domain datasets are mentioned but not extensively validated with quantitative results

## Next Checks

1. **Schema Filter Robustness Test**: Implement the schema filter and systematically evaluate its performance across different query complexities to identify the precision-recall threshold where accuracy gains plateau or degrade

2. **Selection Model Bias Analysis**: Conduct detailed error analysis of the selection model's failures, focusing on systematic biases and test performance on adversarial examples

3. **Open Model Substitution Experiment**: Replace GPT-4o with an open-source model for schema filtering and ICL components to assess performance when fully self-hosted and quantify cost-accuracy tradeoff