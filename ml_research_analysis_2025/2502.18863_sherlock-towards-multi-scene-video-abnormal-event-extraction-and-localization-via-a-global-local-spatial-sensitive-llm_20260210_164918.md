---
ver: rpa2
title: 'Sherlock: Towards Multi-scene Video Abnormal Event Extraction and Localization
  via a Global-local Spatial-sensitive LLM'
arxiv_id: '2502.18863'
source_url: https://arxiv.org/abs/2502.18863
tags:
- spatial
- video
- sherlock
- event
- abnormal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of video abnormal event extraction
  and localization (M-VAE) by proposing a new task that goes beyond traditional video
  anomaly detection to extract structured event quadruples and localize their timestamps.
  The authors identify two key challenges: global-local spatial modeling and global-local
  spatial balancing.'
---

# Sherlock: Towards Multi-scene Video Abnormal Event Extraction and Localization via a Global-local Spatial-sensitive LLM

## Quick Facts
- **arXiv ID:** 2502.18863
- **Source URL:** https://arxiv.org/abs/2502.18863
- **Reference count:** 40
- **Primary result:** Sherlock improves video abnormal event extraction (average 10.85 F1), localization (average 11.42 mAP@tIoU), and classification (average 18.38 FNRs) over advanced Video-LLMs

## Executive Summary
This paper introduces Sherlock, a Global-local Spatial-sensitive Large Language Model designed to address the challenge of Multi-scene Video Abnormal Event Extraction and Localization (M-VAE). The task extends traditional video anomaly detection by extracting structured event quadruples (subject, event type, object, scene) and localizing their timestamps. Sherlock employs a Global-local Spatial-enhanced MoE (GSM) module with four heterogeneous experts (Action, Object Relation, Background, Global) and a Spatial Imbalance Regulator (SIR) to address global-local spatial modeling and balancing challenges. The model demonstrates significant performance improvements over several advanced Video-LLMs on the M-VAE instruction dataset.

## Method Summary
Sherlock is built on Video-LLaVA with a two-stage training approach. Stage 1 involves pre-tuning on four specialized datasets (HumanML3D for actions, Ref-L4 for regions, RSI-CB for backgrounds, COCO for captions) to enhance spatial understanding capabilities. Stage 2 performs instruction tuning on the M-VAE dataset derived from CUVA surveillance videos. The core architecture features a Global-local Spatial-enhanced MoE (GSM) module that routes video inputs through four specialized experts: Action Expert (HigherHRNet for human pose), Object Relation Expert (RelTR for scene graphs), Background Expert (SAM2 + InternViT for segmentation contexts), and Global Expert (LanguageBind for holistic video features). An Expert Gate dynamically weights these experts based on input characteristics, while a Spatial Imbalance Regulator (SIR) applies composite loss regularization to prevent expert dominance.

## Key Results
- **Extraction:** Average 10.85 F1 improvement over advanced Video-LLMs
- **Localization:** Average 11.42 mAP@tIoU improvement for timestamp localization
- **Classification:** Average 18.38 FNRs reduction in false negative rates

## Why This Works (Mechanism)

### Mechanism 1: Heterogeneous Spatial Expert Decomposition
Separating global and local spatial information into specialized experts improves abnormal event detection over monolithic visual encoders. The GSM module assigns four distinct experts to different spatial granularities, with an Expert Gate dynamically weighting these based on input characteristics. Core assumption: Local spatial cues contain higher information density for anomaly discrimination than global representations alone.

### Mechanism 2: Gated Spatial Balancing Loss
Explicit regularization of expert gate weights prevents dominance by frequently-occurring spatial information types. The SIR module applies a composite loss that penalizes imbalanced expert utilization, forcing underutilized experts to receive proportional gradient signal during training. Core assumption: Data imbalance in spatial annotations translates directly to expert weight imbalance during MoE training.

### Mechanism 3: Two-Stage Spatial Pre-Tuning
Pre-training on heterogeneous spatial understanding tasks before instruction tuning improves downstream M-VAE performance. Stage 1 exposes Video-LLaVA to four specialized datasets with spatially-focused instructions, followed by Stage 2 fine-tuning on M-VAE quadruple extraction. Core assumption: Spatial understanding capabilities learned from diverse domains transfer to anomaly detection.

## Foundational Learning

- **Mixture of Experts (MoE) routing:**
  - Why needed here: Sherlock's GSM module relies on soft gating to dynamically weight four heterogeneous visual encoders
  - Quick check question: Can you explain why standard softmax gating might cause expert collapse, and how load balancing loss addresses this?

- **Scene Graph Generation:**
  - Why needed here: The Object Relation Expert uses RelTR to generate structured graphs (objects + predicate edges)
  - Quick check question: Given an image with "man near car," what would the scene graph tuple $(R_i, E_i)$ contain?

- **Video-LLaVA architecture:**
  - Why needed here: Sherlock builds on Video-LLaVA's LanguageBind encoder and projection layers
  - Quick check question: Where does Sherlock's GSM output $O$ get concatenated with textual tokens $T_t$ before LLM input?

## Architecture Onboarding

- **Component map:** Input Video → LanguageBind (Global Expert) → FFN projection; Input Video → HigherHRNet → Action Graph Attention → Action tokens; Input Video → RelTR → MaskGTN → Object relation tokens; Input Video → SAM2 → InternViT → Background tokens; Four expert outputs → Expert Gate (softmax weighting) → LayerNorm → GSM output $O$; $O$ + Text tokens → Video-LLaVA LLM → Quadruple + timestamp generation

- **Critical path:** The Expert Gate weights directly determine spatial signal strength; SIR's $L_{gate}$ modifies these weights during training. If $g_i$ values converge to uniform distribution, expert specialization is lost.

- **Design tradeoffs:**
  - Expert count vs. efficiency: Four experts increase inference time, but paper shows comparable speed to baselines
  - Pre-training diversity vs. domain gap: Stage 1 uses non-surveillance data (COCO, remote sensing), risking feature mismatch with M-VAE's surveillance videos
  - Assumption: Independent expert encoding before fusion assumes spatial modalities are separable; overlapping information may cause redundancy

- **Failure signatures:**
  - Expert collapse: Gate weights stuck on single expert → check $L_{gate}$ gradient magnitude, verify α hyperparameter
  - Quadruple hallucination: Model generates plausible but incorrect subjects/objects → inspect Object Relation Expert outputs, verify RelTR detections
  - Timestamp drift: Localization mAP drops but extraction F1 stable → check temporal alignment of spatial tokens with frame-level features

- **First 3 experiments:**
  1. Ablate single experts: Remove each expert individually and measure FNRs/mAP delta to identify most critical spatial modality
  2. Vary α hyperparameter: Sweep α ∈ {0.1, 0.2, 0.4, 0.8} and plot expert weight distributions to find balance between specialization and utilization
  3. Cross-domain validation: Test Stage 1 pre-tuning with only surveillance-relevant datasets to assess transfer learning necessity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can temporal and causal relationships between multiple abnormal events be integrated to improve extraction performance?
- Basis in paper: The Conclusion states the intent to "consider the relationships between events and enrich our tasks with event inference."
- Why unresolved: The current M-VAE task focuses on isolated event extraction and localization without explicitly modeling dependencies or causal chains between sequential events.

### Open Question 2
- Question: What mechanisms can effectively provide human-readable explanations for detected abnormal events within the Sherlock framework?
- Basis in paper: The Conclusion identifies "improve the interpretability of our model by providing explanations" as a key future direction.
- Why unresolved: While the model outputs structured quadruples, it currently lacks a module to explicitly articulate the reasoning or evidence behind a specific classification.

### Open Question 3
- Question: Does the reliance on specific pre-trained spatial experts limit the model's performance on non-surveillance video domains?
- Basis in paper: The dataset is constructed from CUVA (surveillance), and the local experts are specialized for specific spatial tasks typical of surveillance scenes.
- Why unresolved: The specific Global-local spatial balancing may overfit to the fixed angles and background characteristics of surveillance footage, potentially failing in ego-centric or dynamic camera contexts.

## Limitations
- Experimental validation relies entirely on a single M-VAE dataset derived from CUVA with no external validation on established benchmarks
- Critical architectural details remain underspecified (Action Graph Attention, MaskGTN, 6B-parameter InternViT variant unclear)
- Pre-training strategy uses non-surveillance datasets that may introduce domain mismatch
- Paper doesn't address temporal coherence between spatial tokens and video frames
- No statistical significance testing to validate reported performance improvements

## Confidence
- **High confidence:** GSM module architecture and MoE routing mechanism are well-specified and technically sound; task definition (M-VAE quadruple extraction) is novel and clearly articulated
- **Medium confidence:** Spatial imbalance problem and SIR regularization approach are reasonable, but lack ablation studies showing SIR's impact is independent of other factors
- **Low confidence:** Claims about pre-training effectiveness and overall performance superiority are not robustly validated; promising results but no statistical significance or cross-dataset generalization evidence

## Next Checks
1. Perform paired t-tests or bootstrap confidence intervals on F1 and mAP metrics across all compared methods to establish statistical significance
2. Evaluate Sherlock on at least one established video anomaly detection benchmark (UCF-Crime or ShanghaiTech) using the same quadruple extraction and localization protocols
3. Systematically remove each Stage 1 dataset individually and measure performance degradation to quantify each dataset's contribution versus potential negative transfer effects