---
ver: rpa2
title: 'Fine-tuning Pre-trained Audio Models for COVID-19 Detection: A Technical Report'
arxiv_id: '2511.14939'
source_url: https://arxiv.org/abs/2511.14939
tags:
- covid-19
- coswara
- coughvid
- performance
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This technical report evaluates pre-trained audio models (Audio-MAE
  and PANN architectures) for COVID-19 detection using the Coswara and Coughvid datasets.
  The study implements strict demographic stratification by age and gender to prevent
  models from exploiting spurious correlations between demographic characteristics
  and COVID-19 status.
---

# Fine-tuning Pre-trained Audio Models for COVID-19 Detection: A Technical Report

## Quick Facts
- arXiv ID: 2511.14939
- Source URL: https://arxiv.org/abs/2511.14939
- Reference count: 40
- Primary result: Pre-trained audio models show moderate intra-dataset COVID-19 detection performance (AUC 0.58-0.82) but severe cross-dataset generalization failure (AUC 0.43-0.68)

## Executive Summary
This technical report evaluates pre-trained audio models (Audio-MAE and PANN architectures) for COVID-19 detection using the Coswara and Coughvid datasets. The study implements strict demographic stratification by age and gender to prevent models from exploiting spurious correlations between demographic characteristics and COVID-19 status. Intra-dataset results showed moderate performance, with Audio-MAE achieving the strongest result on Coswara (0.82 AUC, 0.76 F1-score), while all models demonstrated limited performance on Coughvid (AUC 0.58-0.63). Cross-dataset evaluation revealed severe generalization failure across all models (AUC 0.43-0.68), with Audio-MAE showing strong performance degradation (F1-score 0.00-0.08). The findings highlight that demographic balancing, while reducing apparent model performance, provides more realistic assessment of COVID-19 detection capabilities by eliminating demographic leakage. Additionally, the limited dataset sizes after balancing (1,219-2,160 samples) proved insufficient for deep learning models that typically require substantially larger training sets. These results underscore fundamental challenges in developing generalizable audio-based COVID-19 detection systems and emphasize the importance of rigorous demographic controls for clinically robust model evaluation.

## Method Summary
The study fine-tunes pre-trained audio models (Audio-MAE and PANNs) for COVID-19 detection using the Coswara and Coughvid datasets. After preprocessing to filter audio samples with complete demographic metadata, the authors apply stratified undersampling by age×gender strata to balance class distributions within each demographic subgroup. This prevents models from learning spurious correlations between demographic characteristics and disease status. The Audio-MAE model uses a ViT-based encoder-decoder architecture with 2-layer MLP classification head, while PANNs use CNN backbones (6/10/14 layers) with modified classification heads. All models are fine-tuned with differential learning rates, freezing most pre-trained layers while adapting higher-level features. Performance is evaluated using standard classification metrics (AUC, F1, precision, recall) on stratified test splits, with particular attention to cross-dataset generalization capabilities.

## Key Results
- Audio-MAE achieved the strongest intra-dataset performance: 0.82 AUC and 0.76 F1-score on Coswara
- All models showed limited performance on Coughvid dataset: AUC 0.58-0.63
- Cross-dataset generalization failed severely: AUC dropped to 0.43-0.68 across all models
- Audio-MAE showed extreme performance degradation in cross-dataset testing: F1-score 0.00-0.08
- Demographic balancing reduced apparent performance but prevented demographic leakage exploitation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Demographic stratification by age and gender reveals true model capabilities by blocking spurious correlation learning.
- Mechanism: When datasets have imbalanced demographic distributions (e.g., older individuals overrepresented in COVID-positive class), models learn to associate acoustic properties of age/gender with disease status rather than learning disease-specific features. Stratified undersampling forces models to discriminate within demographic strata, preventing this shortcut.
- Core assumption: Age and gender have measurable acoustic signatures that correlate with COVID-19 status in unbalanced datasets.
- Evidence anchors:
  - [abstract] "demographic balancing, while reducing apparent model performance, provides more realistic assessment of COVID-19 detection capabilities by eliminating demographic leakage"
  - [section 5.1] "On Coswara, we went from 0.82 to 0.85 of AUC (3.7% increase change); on Coughvid, from 0.60 of AUC to 0.63 (4.5% change)" when using unbalanced data
  - [corpus] Limited direct corpus support; related work on demographic modeling (paper 17209) addresses different domain
- Break condition: If acoustic signatures of demographics are negligible or COVID status is uniformly distributed across demographics, stratification provides no benefit.

### Mechanism 2
- Claim: Pre-trained audio representations transfer moderately to COVID-19 detection but require substantially larger fine-tuning datasets than available.
- Mechanism: Audio-MAE and PANNs learn hierarchical acoustic features from large-scale pre-training (AudioSet-2M, 527 classes). Fine-tuning adapts high-level feature extractors while preserving low-level representations. However, with only 1,219-2,160 samples after balancing, models cannot fully adapt to COVID-specific patterns.
- Core assumption: Acoustic biomarkers for COVID-19 exist within the representational space learned from general audio pre-training.
- Evidence anchors:
  - [section 3.2.1] Audio-MAE "achieved state-of-the-art results on speech classification tasks with 98.3% accuracy on Speech Commands V2 (84k samples)"
  - [section 5.2] "datasets contain only 1,219 samples (Coswara) and 2,160 samples (Coughvid)—approximately 40-70 times smaller than the datasets where Audio-MAE demonstrated strong performance"
  - [corpus] Paper 8725 shows pre-trained audio models "excel at detecting acoustic patterns in auscultation sounds but often fail to grasp their clinical significance"
- Break condition: If COVID-19 acoustic features are fundamentally distinct from AudioSet classes, pre-training provides minimal transfer benefit regardless of dataset size.

### Mechanism 3
- Claim: Cross-dataset generalization fails because learned features are dataset-specific rather than disease-invariant.
- Mechanism: Models trained on one dataset learn features that correlate with COVID status within that collection context (recording conditions, population characteristics). When applied to different datasets, these correlations break down, causing performance collapse.
- Core assumption: Dataset-specific features (recording quality, population demographics, collection protocols) differ systematically between Coswara and Coughvid.
- Evidence anchors:
  - [section 4.2] "Audio-MAE completely failed to generalize... F1-score of just 0.08 when trained on Coswara and tested on Coughvid"
  - [table 10] Cross-dataset AUC ranges 0.43-0.68 across all models, vs intra-dataset AUC up to 0.82
  - [corpus] Paper 10547 reports similar cross-dataset degradation: "Coswara→Coughvid 0.53 AUC, Coughvid→Coswara 0.57 AUC"
- Break condition: If datasets share identical acoustic distributions and demographic compositions, cross-dataset transfer should succeed.

## Foundational Learning

- Concept: **Spurious Correlations / Shortcut Learning**
  - Why needed here: The paper's central contribution is demonstrating that models exploit demographic-age/gender correlations rather than learning COVID-specific features. Understanding this explains why high reported accuracies in prior work (0.92-0.93 AUC) may be misleading.
  - Quick check question: If you remove demographic information from training, does performance drop? If yes, the model was likely using demographic shortcuts.

- Concept: **Stratified Undersampling**
  - Why needed here: The methodology ensures each age×gender stratum has balanced positive/negative examples, preventing the model from inferring disease status from demographic cues. This is the key intervention that distinguishes this work from prior studies.
  - Quick check question: After stratified undersampling, are the class ratios equal within each demographic subgroup?

- Concept: **Transfer Learning Capacity vs. Data Requirements**
  - Why needed here: The paper explicitly attributes modest performance to insufficient fine-tuning data (40-70× smaller than successful applications). This frames expectations for what pre-trained models can achieve with limited domain data.
  - Quick check question: What is the ratio between your fine-tuning dataset size and the pre-training dataset size? Below 1:10 may indicate insufficient adaptation data.

## Architecture Onboarding

- Component map:
  - **Audio-MAE**: ViT-based encoder-decoder. Encoder processes 20% visible spectrogram patches; decoder with local attention reconstructs masked regions. Classification head replaced with 2-layer MLP (2048→256→2).
  - **PANNs (CNN6/10/14)**: CNN backbones with 6/10/14 convolutional layers. Pre-trained on AudioSet (527 classes). Modified with new classification head; only last 2 conv blocks unfrozen during fine-tuning.
  - **Preprocessing**: 4-second fixed windowing, label consolidation (positive/negative binary), metadata filtering for complete demographic records.

- Critical path:
  1. Filter audio samples with complete age/gender metadata
  2. Apply stratified undersampling by age×gender strata
  3. Replace classification heads with binary output
  4. Fine-tune with differential learning rates (head: 5×10⁻⁴, backbone: 5×10⁻⁶)
  5. Evaluate with demographic-stratified test splits

- Design tradeoffs:
  - **Balanced vs. unbalanced data**: Balancing reduces AUC (0.82→0.85 observed on unbalanced) but prevents demographic leakage
  - **CNN depth vs. data scarcity**: Deeper models (CNN14) showed inconsistent performance—sometimes matching Audio-MAE (0.75 accuracy on Coswara), sometimes failing (AUC 0.33 with CNN10)
  - **Freeze strategy**: Freezing early layers preserves AudioSet features but limits COVID adaptation; the paper freezes all but last 2 blocks

- Failure signatures:
  - **Cross-dataset collapse**: AUC drops from 0.82→0.51, F1 drops to 0.00-0.08 (Audio-MAE)
  - **Extreme class bias**: CNN10 on Coswara predicted all samples negative (0 true positives)
  - **Inverse correlation between depth and consistency**: CNN10 showed worst intra-dataset AUC (0.33) despite intermediate complexity

- First 3 experiments:
  1. **Baseline with unbalanced data**: Train Audio-MAE on original Coswara/Coughvid without stratification to quantify demographic leakage magnitude (expect 3-5% AUC inflation per paper's findings).
  2. **Ablation on freeze depth**: Compare freezing 0, 2, 4, and all convolutional blocks to identify optimal adaptation depth for small COVID datasets.
  3. **Cross-dataset diagnostic**: Train on Coswara balanced, test on Coughvid balanced AND unbalanced to separate generalization failure caused by distribution shift vs. demographic distribution mismatch.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can acoustic features learned for COVID-19 detection transfer to related respiratory diagnostic tasks such as respiratory insufficiency (RI) detection or blood oxygen saturation (SpO2) estimation?
- Basis in paper: [explicit] The Introduction explicitly states: "important questions remain about the relationship between different respiratory diagnostic tasks—particularly whether models trained for COVID-19 detection can inform broader respiratory conditions such as respiratory insufficiency (RI) or blood oxygen saturation (SpO2) estimation."
- Why unresolved: The authors' prior work showed 99.9% accuracy for RI detection but <70% for SpO2 estimation, suggesting acoustic features may be task-specific, but no systematic transfer learning experiments were conducted.
- What evidence would resolve it: Multi-task learning experiments evaluating whether shared representations emerge across COVID-19, RI, and SpO2 tasks, or whether task-specific fine-tuning degrades other capabilities.

### Open Question 2
- Question: What minimum dataset size, after demographic balancing, is required for deep learning models to learn generalizable COVID-19 acoustic features?
- Basis in paper: [inferred] The Discussion notes that balanced datasets (1,219–2,160 samples) proved "insufficient for deep learning models that typically require substantially larger training sets," comparing unfavorably to the 84k–138k sample datasets where Audio-MAE excels.
- Why unresolved: The study demonstrates failure at current scales but provides no systematic investigation of the relationship between balanced dataset size and generalization performance.
- What evidence would resolve it: Controlled experiments varying balanced training set sizes systematically (e.g., 5k, 10k, 25k, 50k samples) while maintaining demographic stratification, measuring both intra- and cross-dataset performance.

### Open Question 3
- Question: What acoustic representations do these models actually learn when demographic shortcuts are removed, and do they capture disease-relevant physiological features?
- Basis in paper: [explicit] The Discussion hypothesizes that "models may inadvertently learn to exploit acoustic properties associated with age and gender distributions rather than COVID-19-specific respiratory features" and presents evidence that removing this capacity reduces performance.
- Why unresolved: While the paper demonstrates that demographic stratification reduces apparent performance, it provides no analysis of what features the models attend to after balancing—whether they learn meaningful respiratory biomarkers or simply fail for lack of learnable signal.
- What evidence would resolve it: Feature attribution analysis (e.g., Grad-CAM on spectrograms, SHAP on acoustic features) on demographically balanced models to identify whether attention focuses on physiologically relevant spectral regions associated with respiratory pathology.

## Limitations

- The demographic balancing methodology significantly reduces dataset sizes (1,219-2,160 samples), potentially limiting model capacity to learn COVID-specific features.
- Cross-dataset generalization failure may reflect dataset collection protocol differences rather than fundamental detection difficulty.
- The study does not investigate whether larger balanced datasets would yield better performance, leaving open whether current limitations stem from data scarcity or fundamental acoustic detection challenges.

## Confidence

- **High confidence**: Demographic stratification prevents spurious correlation learning; this is directly demonstrated through the 3-5% AUC inflation observed in unbalanced datasets and the complete failure of demographic shortcuts in balanced evaluation.
- **Medium confidence**: Pre-trained audio models have limited COVID-19 detection capability with current dataset sizes; the transfer learning mechanism is theoretically sound but practical results show insufficient adaptation data.
- **Medium confidence**: Cross-dataset generalization failure indicates learned features are dataset-specific rather than disease-invariant; while severe performance drops are observed, the exact contribution of recording conditions versus population characteristics remains unquantified.

## Next Checks

1. **Dataset size scaling experiment**: Evaluate model performance on progressively larger balanced subsets (e.g., 2×, 5×, 10× current sizes) to determine whether performance plateaus due to fundamental detection difficulty or data scarcity.
2. **Domain adaptation assessment**: Apply domain adaptation techniques (e.g., adversarial training, DANN) to the cross-dataset setting to quantify how much performance loss stems from dataset distribution shift versus true generalization failure.
3. **Demographic subgroup analysis**: Perform within-stratum performance analysis to verify that models achieve comparable accuracy across all age×gender combinations, confirming that balancing successfully eliminated demographic shortcuts rather than simply reducing overall performance.