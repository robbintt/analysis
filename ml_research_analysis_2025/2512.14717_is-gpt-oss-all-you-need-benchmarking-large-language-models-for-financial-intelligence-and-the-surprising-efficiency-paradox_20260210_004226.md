---
ver: rpa2
title: Is GPT-OSS All You Need? Benchmarking Large Language Models for Financial Intelligence
  and the Surprising Efficiency Paradox
arxiv_id: '2512.14717'
source_url: https://arxiv.org/abs/2512.14717
tags:
- financial
- performance
- accuracy
- across
- sentiment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks five large language models on ten financial
  NLP tasks, evaluating both accuracy and computational efficiency. The study reveals
  that GPT-OSS-20B achieves 97.9% of the accuracy of its 120B parameter counterpart
  while being 2.3x faster and using 83% less memory.
---

# Is GPT-OSS All You Need? Benchmarking Large Language Models for Financial Intelligence and the Surprising Efficiency Paradox

## Quick Facts
- arXiv ID: 2512.14717
- Source URL: https://arxiv.org/abs/2512.14717
- Reference count: 40
- Primary result: GPT-OSS-20B achieves 97.9% of GPT-OSS-120B accuracy while being 2.3x faster and using 83% less memory

## Executive Summary
This paper benchmarks five large language models on ten financial NLP tasks, evaluating both accuracy and computational efficiency. The study reveals that GPT-OSS-20B achieves 97.9% of the accuracy of its 120B parameter counterpart while being 2.3x faster and using 83% less memory. GPT-OSS models outperform larger competitors like Qwen3-235B, challenging the assumption that model scale directly correlates with task performance. The paper introduces a novel Token Efficiency Score that captures the trade-off between model performance and resource utilization.

## Method Summary
The study evaluates five LLMs (GPT-OSS-20B, GPT-OSS-120B, Llama4-Scout, Llama4-Orion, Qwen3-235B) on ten financial NLP tasks using zero-shot inference. Tasks include sentiment analysis, question answering, entity recognition, and stock price prediction across seven sentiment datasets, two QA datasets, and one entity recognition dataset. Evaluation uses greedy decoding (temperature=0), batch size 32, and measures accuracy, throughput (tokens/sec), memory usage, and latency. The novel Token Efficiency Score combines these metrics to quantify the accuracy-efficiency frontier.

## Key Results
- GPT-OSS-20B achieves 97.9% of GPT-OSS-120B accuracy with 2.3x faster inference and 83% less memory usage
- GPT-OSS models outperform larger Qwen3-235B (235B parameters) on financial tasks despite smaller size
- The proposed Token Efficiency Score reveals significant efficiency gains that raw accuracy metrics miss

## Why This Works (Mechanism)

### Mechanism 1: Domain-Appropriate Training Compensates for Scale
Smaller models with domain-appropriate pre-training can match larger general-purpose models on financial NLP tasks. Financial NLP tasks emphasize constrained, precision-focused patterns (sentiment, entity recognition, numerical reasoning) that benefit more from targeted corpus exposure than from increased parameter capacity.

### Mechanism 2: Mixture-of-Experts (MoE) Sparse Activation
MoE architectures achieve higher effective capacity per active parameter through selective expert routing. During inference, only a subset of experts activates per token, reducing FLOPs while maintaining representational capacity for diverse financial text patterns.

### Mechanism 3: Token Efficiency Score (TES) Optimization Target
TES explicitly captures the accuracy-efficiency frontier, enabling quantitative trade-off decisions. TES = (Accuracy × Tokens/sec) / log(Params_B) rewards high throughput and accuracy while penalizing parameter bloat via logarithmic scaling, reflecting empirically observed diminishing returns.

## Foundational Learning

- **Concept: Zero-shot vs. Fine-tuned Evaluation** - Why needed: Paper evaluates zero-shot performance to assess inherent transfer; real deployments may involve task-specific adaptation. Quick check: Would fine-tuning narrow or widen the observed efficiency gap between GPT-OSS-20B and larger models?
- **Concept: Mixture-of-Experts Routing** - Why needed: Understanding MoE is essential to interpret how 20B/120B models achieve their efficiency profiles. Quick check: What happens to inference cost if an input activates all experts simultaneously?
- **Concept: Statistical Validation of Benchmark Claims** - Why needed: Paper uses paired t-tests, ANOVA, and bootstrap CI; understanding these determines confidence in "efficiency paradox" claims. Quick check: Why do overlapping 95% CIs between GPT-OSS-20B and GPT-OSS-120B support the efficiency conclusion?

## Architecture Onboarding

- **Component map:** GPT-OSS backbone -> MoE decoder (20B/120B params) -> Grouped-query attention -> Rotary position embeddings -> SwiGLU activation -> Token Efficiency Score computation
- **Critical path:** Model initialization with tensor parallelism for >80B models -> Unified tokenization preserving financial symbols -> Greedy decoding (temp=0) -> Batch-32 inference with memory-constrained fallback -> Metric computation: accuracy → TES → composite efficiency index
- **Design tradeoffs:** GPT-OSS-20B: 65.1% accuracy, 159.8 TPS, 40GB memory (optimal for cost-sensitive deployment) vs GPT-OSS-120B: +1.4% accuracy, -38% speed, 6× memory (justified only for high-stakes decisions)
- **Failure signatures:** Bimodal accuracy distribution (Llama4-Scout): refusal behavior + format incompatibility; Entity recognition collapse: max 43% on FLARE FINER-ORD; Reasoning edge cases: tied ranking errors despite correct individual computations
- **First 3 experiments:** 1) Replicate GPT-OSS-20B vs GPT-OSS-120B on Financial PhraseBank: expect ~98% for both, measure TPS difference 2) Profile memory breakdown across batch sizes 1/8/32 3) Test prompt sensitivity: vary instruction phrasing on FiQA-SA to assess stability scores

## Open Questions the Paper Calls Out

- Does task-specific fine-tuning alter the "efficiency paradox," potentially narrowing or widening the performance gap between small (20B) and large (120B) models?
- Can the efficiency and accuracy of GPT-OSS models be effectively transferred to non-English financial documents and cross-lingual tasks?
- Is the superior performance of smaller models driven primarily by architectural innovations (e.g., MoE, GQA) or the domain-specific composition of the training data?

## Limitations

- Model Availability and Reproducibility: GPT-OSS models appear unreleased, preventing independent validation
- Domain Transfer Generalizability: Results may not generalize to broader financial reasoning or novel financial instruments
- Metric Sensitivity: Token Efficiency Score weighting choices may artificially favor smaller models

## Confidence

- High Confidence: GPT-OSS-20B achieves 97.9% of GPT-OSS-120B accuracy while being 2.3x faster and using 83% less memory
- Medium Confidence: GPT-OSS models outperform larger competitors (Qwen3-235B) on financial tasks despite having fewer parameters
- Low Confidence: The "efficiency paradox" represents a fundamental shift in LLM scaling laws for domain-specific applications

## Next Checks

1. Independent Reproduction with Available Models - Replicate benchmark using publicly available MoE models to verify efficiency patterns without proprietary GPT-OSS weights
2. Cross-Domain Generalization Test - Apply same models to non-financial NLP benchmarks to determine if efficiency advantages are domain-specific or represent general architectural improvements
3. Fine-tuning Impact Analysis - Fine-tune both GPT-OSS-20B and GPT-OSS-120B on a subset of financial tasks to evaluate whether efficiency gap persists under adaptation