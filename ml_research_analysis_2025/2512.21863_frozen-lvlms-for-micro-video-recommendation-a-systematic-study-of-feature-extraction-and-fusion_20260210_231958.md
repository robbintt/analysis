---
ver: rpa2
title: 'Frozen LVLMs for Micro-Video Recommendation: A Systematic Study of Feature
  Extraction and Fusion'
arxiv_id: '2512.21863'
source_url: https://arxiv.org/abs/2512.21863
tags:
- recommendation
- video
- micro-video
- uni00000013
- lvlm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the first systematic empirical study of integrating
  frozen Large Video Language Models (LVLMs) into micro-video recommendation systems.
  The authors evaluate two key design dimensions: feature extraction paradigms (caption-based
  vs.'
---

# Frozen LVLMs for Micro-Video Recommendation: A Systematic Study of Feature Extraction and Fusion

## Quick Facts
- arXiv ID: 2512.21863
- Source URL: https://arxiv.org/abs/2512.21863
- Reference count: 40
- Authors: Huatuan Sun; Yunshan Ma; Changguang Wu; Yanxin Zhang; Pengfei Wang; Xiaoyu Du
- Key outcome: First systematic empirical study showing frozen LVLMs can enhance micro-video recommendation through hidden-state extraction and adaptive fusion with ID embeddings

## Executive Summary
This paper presents the first systematic empirical study of integrating frozen Large Video Language Models (LVLMs) into micro-video recommendation systems. The authors evaluate two key design dimensions: feature extraction paradigms (caption-based vs. hidden-state-based) and integration strategies (replacement vs. fusion with ID embeddings). Through extensive experiments on two real-world datasets and three representative LVLMs, they establish three key principles: intermediate decoder hidden states consistently outperform caption-based representations, fusing LVLM features with ID embeddings is strictly superior to replacement, and multi-layer feature fusion provides additional gains. Based on these insights, they propose the Dual Feature Fusion (DFF) framework, which adaptively fuses multi-layer LVLM representations with item ID embeddings.

## Method Summary
The study systematically investigates frozen LVLM integration into micro-video recommendation through two orthogonal design dimensions: feature extraction (caption-based vs. hidden-state-based) and integration strategy (replacement vs. fusion). The Dual Feature Fusion (DFF) framework is proposed as the optimal solution, extracting intermediate decoder hidden states from frozen LVLMs, projecting them to the embedding dimension, aggregating multi-layer features with learnable weights, and adaptively fusing with item ID embeddings using a gating mechanism. The framework is evaluated on two real-world micro-video datasets (MicroLens and TikHub) using three representative LVLMs (Video-ChatGPT, Video-MLLM, Video-LaVIT) across multiple recommendation backbones.

## Key Results
- Hidden-state-based representations consistently outperform caption-based approaches by 2.89%-4.64% in Hit@10 across datasets
- Fusion with ID embeddings is strictly superior to replacement, with DFF achieving 6.91% relative improvement in Hit@10 over the strongest baseline on MicroLens
- Multi-layer feature fusion provides additional gains, with middle decoder layers (approximately layers 16-22) achieving peak performance
- DFF demonstrates state-of-the-art performance while using frozen LVLMs without costly fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Bypassing text generation to extract intermediate hidden states preserves fine-grained semantics lost in natural language summarization.
- **Mechanism**: LVLMs project continuous visual data into discrete token probabilities. This "tokenization bottleneck" discards subtle visual details (e.g., specific background objects or motion styles) that might distinguish user preferences but are too granular for a general text caption. Accessing the hidden states before the output head retains this dense, continuous information.
- **Core assumption**: The information discarded during the text decoding process is relevant to predicting user preferences, not just noise.
- **Evidence anchors**:
  - [abstract] "natural-language summarization inevitably discards fine-grained visual semantics crucial for recommendation."
  - [section 4.2] "Hidden-State-Centric (HC)... bypasses text generation entirely... preserving richer recommendation-relevant semantics."
  - [corpus] Weak direct evidence; related work generally focuses on fusion methods rather than the hidden-state vs. caption comparison.
- **Break condition**: If the recommendation task relies solely on high-level taxonomic matching (e.g., "show me cat videos") rather than fine-grained style preference, the complexity of hidden states may offer diminishing returns over simpler captions.

### Mechanism 2
- **Claim**: Item ID embeddings encode irreplaceable collaborative signals that purely content-based representations cannot capture.
- **Mechanism**: ID embeddings learn vector representations based on co-occurrence patterns in user interaction histories (collaborative filtering). A frozen LVLM, trained on general video-text alignment, lacks knowledge of specific user-community trends or popularity biases present in the interaction data. Replacing IDs removes this signal; fusion retains it.
- **Core assumption**: User behavior is not fully determined by video content; interaction patterns contain independent predictive signal.
- **Evidence anchors**:
  - [abstract] "ID embeddings capture irreplaceable collaborative signals, rendering fusion strictly superior to replacement."
  - [section 4.1] Table 1 shows "Replacement" performs worse than the "ID-only" baseline.
  - [corpus] [Do Recommender Systems Really Leverage Multimodal Content?] suggests multimodal gains can be ambiguous, reinforcing that collaborative signals remain a dominant factor.
- **Break condition**: In cold-start scenarios where interaction data is missing or sparse, the ID embedding is unreliable, potentially reversing the utility of this mechanism (though the paper argues for fusion, not replacement, implying ID helps where available).

### Mechanism 3
- **Claim**: Intermediate decoder layers offer a superior balance between low-level visual grounding and high-level semantic abstraction compared to the final output layer.
- **Mechanism**: Deep Transformer layers iteratively refine representations. Shallow layers retain perceptual details but lack semantic context; deep layers approach the final text prediction (abstract), potentially "over-smoothing" or introducing linguistic priors irrelevant to the video content itself. Middle layers capture the optimal semantic "gist."
- **Core assumption**: The optimal representation for recommendation does not lie at the extreme ends of the network depth (raw pixels or final text logits).
- **Evidence anchors**:
  - [section 4.3] "middle layers (approximately layers 16–22) consistently achieve peak performance... deep layers exhibit slight performance degradation."
  - [corpus] [Benchmarking and Improving LVLMs...] acknowledges varying cross-modal capabilities, but specific layer-wise evidence is derived primarily from the paper's internal analysis.
- **Break condition**: If the LVLM architecture changes significantly (e.g., different decoder depth or normalization), the specific "middle layer" hypothesis would require re-validation.

## Foundational Learning

- **Concept**: **Collaborative Filtering vs. Content-Based Filtering**
  - **Why needed here**: The paper hinges on the synergy between ID-based embeddings (Collaborative) and LVLM features (Content). Without understanding that IDs model "who-watched-what" independently of video content, the failure of the "Replacement" strategy is confusing.
  - **Quick check question**: Can you explain why a video with perfect content features might still fail to rank high if the user has never interacted with similar items, and how ID embeddings mitigate this?

- **Concept**: **Transformer Hidden States & Residual Streams**
  - **Why needed here**: The DFF framework aggregates features from multiple layers. Understanding that different layers hold different levels of abstraction (surface features vs. deep semantics) is essential for grasping why simple averaging or weighted fusion works.
  - **Quick check question**: Why would the *last* layer of a decoder, optimized for predicting the next text token, be less suitable for video representation than a middle layer?

- **Concept**: **Gating Mechanisms (in Fusion)**
  - **Why needed here**: The DFF uses a specific gating formula ($g \cdot e_{id} + (1-g) \cdot e_{v}$) to combine embeddings. Understanding this as a "soft switch" allows you to see how the model dynamically prioritizes content vs. collaborative signals per item.
  - **Quick check question**: If the gating scalar $g$ approaches 1 for a specific video, which signal is the model ignoring?

## Architecture Onboarding

- **Component map**: Video Input -> Frozen LVLM (hidden states) -> Multi-Layer Aggregation -> Adaptive Gating with ID -> Sequence Modeling
- **Critical path**: The flow moves from Video Input → Frozen LVLM (stopping at hidden states, not text output) → Multi-Layer Aggregation → Adaptive Gating with ID → Sequence Modeling.
- **Design tradeoffs**:
  - *Caption vs. Hidden State*: Captions are interpretable and cheap to store (text); Hidden states are high-dimensional vectors requiring storage but carry more signal.
  - *Replacement vs. Fusion*: Replacement minimizes model size (no ID embeddings) but loses collaborative signal; Fusion doubles embedding parameters but maximizes performance.
- **Failure signatures**:
  - **Performance Drop vs. Baseline**: If your implementation performs worse than an ID-only baseline, verify you are using Fusion, not Replacement.
  - **Degradation in Deep Layers**: If using only the last layer, performance may dip; verify multi-layer averaging is enabled.
  - **Modality Mismatch**: If using "Title" or "Cover" instead of "Raw Video," expect significant performance drops (Table 4).
- **First 3 experiments**:
  1. **Sanity Check (Ablation)**: Run the system with "Replacement" vs. "Fusion" to reproduce the performance gap shown in Table 1. This validates the integration pipeline.
  2. **Layer Analysis**: Plot performance using only Layer 1, Layer N/2, and Layer N. Confirm the "U-shaped" or rising-then-falling curve where middle layers dominate.
  3. **Modality Input Test**: Compare feeding the LVLM raw video vs. a single cover image. This confirms the necessity of temporal processing and validates the data loading pipeline.

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- **Temporal Sensitivity**: The paper focuses exclusively on frozen LVLMs for micro-video recommendation without addressing temporal dynamics during inference. The DFF framework treats LVLM outputs as static features, potentially missing short-term temporal patterns in user behavior that evolve rapidly in short-form video contexts.
- **Generalization Constraints**: All experiments are conducted on micro-video datasets (MicroLens, TikHub) with specific temporal characteristics (typically under 60 seconds). The effectiveness of frozen LVLMs for longer-form video content or different content types remains untested.
- **Model Size Trade-offs**: While DFF achieves superior performance, the framework requires storing and processing multi-layer LVLM features alongside ID embeddings, increasing computational overhead and storage requirements compared to traditional ID-only approaches.

## Confidence
- **High Confidence (9/10)**: The superiority of hidden-state-based representations over caption-based approaches is well-supported by ablation studies across multiple LVLMs and datasets. The mechanism of preserving fine-grained semantics through bypassing text generation is theoretically sound and empirically validated.
- **Medium Confidence (7/10)**: The claim that middle decoder layers consistently outperform shallow and deep layers shows robustness across tested models, but the specific optimal layer range (16-22) may vary with different LVLM architectures or video domains. The evidence is strong within tested conditions but requires architectural adaptation for generalization.
- **Medium Confidence (7/10)**: The fusion superiority over replacement is convincingly demonstrated, but the assumption that collaborative signals are always complementary to content features may not hold in extreme cold-start scenarios where ID embeddings are unreliable or in content-dominated recommendation tasks.

## Next Checks
1. **Temporal Adaptation Study**: Implement a dynamic gating mechanism that adjusts fusion weights based on temporal proximity of interactions, testing whether real-time user preference shifts can be better captured than static feature fusion.
2. **Cross-Domain Generalization Test**: Apply DFF to longer-form video datasets (e.g., movie recommendations) and evaluate whether frozen LVLMs maintain their effectiveness when video content duration extends beyond the micro-video domain.
3. **Cold-Start Robustness Analysis**: Systematically evaluate DFF performance on cold-start items and users by comparing against ID-free content-only approaches, measuring the exact conditions where collaborative signal absence reverses the fusion advantage.