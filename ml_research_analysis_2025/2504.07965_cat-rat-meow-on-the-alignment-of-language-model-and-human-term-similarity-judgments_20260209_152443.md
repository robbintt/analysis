---
ver: rpa2
title: 'Cat, Rat, Meow: On the Alignment of Language Model and Human Term-Similarity
  Judgments'
arxiv_id: '2504.07965'
source_url: https://arxiv.org/abs/2504.07965
tags:
- choice
- accuracy
- human
- alignment
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates how well small and mid-sized language models
  align with human similarity judgments on word triplets. The authors compare both
  the internal representations and the behavioral outputs of 32 models across six
  families (Gemma 2, Llama 3, Minitron, OpenELM, Phi, and Qwen 2.5) on the 3TT dataset.
---

# Cat, Rat, Meow: On the Alignment of Language Model and Human Term-Similarity Judgments

## Quick Facts
- arXiv ID: 2504.07965
- Source URL: https://arxiv.org/abs/2504.07965
- Reference count: 40
- Key outcome: Small and mid-sized language models can achieve human-level representational alignment with term similarity judgments, with instruction tuning and model family playing larger roles than size alone.

## Executive Summary
This paper evaluates how well language models align with human similarity judgments on word triplets, comparing both internal representations and behavioral outputs across 32 models from six families. The authors find that small models can match human-level alignment in representation space, instruction tuning substantially improves alignment for some families, and layer-wise alignment patterns vary significantly by model architecture. Notably, behavioral alignment strongly depends on model size and only matches representational alignment for the largest models, revealing that small models encode more human-like similarity knowledge than they can reliably express through generation.

## Method Summary
The paper evaluates 32 language models from six families (Gemma 2, Llama 3, Minitron, OpenELM, Phi, and Qwen 2.5) on the 3TT dataset of word triplets. For representational alignment, models are prompted with each word individually, representations are extracted at multiple layers, centered, and compared via cosine similarity to determine which of two targets is more similar to an anchor. For behavioral alignment, instruction-tuned models are given a specific prompt and their generated responses are post-processed and validated. The key metric is choice accuracy - the fraction of model choices agreeing with human majority vote - evaluated both per-layer and behaviorally.

## Key Results
- Even small models can match human-level representational alignment, with model size not being the primary factor
- Instruction tuning substantially improves representational alignment for some families (Minitron, Qwen), with gains of 0.1-0.2 in choice accuracy
- Alignment patterns across layers vary significantly by model family, with some showing monotonic improvement and others showing bimodal patterns
- Behavioral alignment strongly depends on model size and only matches representational alignment for the largest models

## Why This Works (Mechanism)

### Mechanism 1: Instruction Tuning Reshapes Representational Geometry
Instruction tuning reorganizes the internal embedding space such that semantically similar concepts cluster more consistently with human intuition, improving representational alignment independent of model size.

### Mechanism 2: Layer-Specific Semantic Abstraction
Different transformer architectures develop semantic structure at different depths, with some families showing monotonic improvement and others showing bimodal patterns in representational alignment across layers.

### Mechanism 3: Representational-Behavioral Bottleneck in Small Models
Small models encode human-aligned similarity knowledge in their representations but cannot reliably express it through generation due to weaker instruction-following capacity and output format constraints.

## Foundational Learning

- **Representational Similarity Analysis (RSA)**: Understanding RSA is prerequisite to interpreting why alignment is measured via cosine similarity of representations. Quick check: Given two embedding matrices, can you compute a representational dissimilarity matrix and correlate it with human similarity ratings?

- **Triplet Task Design**: The 3TT dataset uses relative judgments rather than absolute similarity scores to eliminate scale calibration issues across raters. Quick check: Why might a triplet task produce more reliable human judgments than asking "rate similarity of A and B on a 1-10 scale"?

- **Residual Stream vs. Attention/MLP Outputs**: The paper probes three types of representations per layer, with residual streams consistently showing higher and more stable alignment. Quick check: In a transformer block, what is the relationship between the residual stream output and the attention + MLP outputs?

## Architecture Onboarding

- **Component map**: Input tokens → Embedding layer → N × Transformer blocks (each with Attention, MLP, and Residual Stream outputs) → Final layer norm → Output logits

- **Critical path**: 1) Extract embeddings for all three terms at each sub-block per layer, 2) Center embeddings per layer, 3) Compute cosine similarities: sim(anchor, B) vs. sim(anchor, C), 4) Model "choice" = whichever target has higher similarity, 5) Compare to human majority vote → choice accuracy

- **Design tradeoffs**: Probing last token vs. mean pooling (paper uses last token); centered vs. uncentered embeddings (centering generally improves choice accuracy); which layer to use (residual streams are most stable but some attention layers occasionally outperform them)

- **Failure signatures**: Behavioral alignment far below representational alignment → model struggles with instruction following; flat choice accuracy across layers → semantic structure may not be developing hierarchically; representational alignment < 0.55 → model may not have learned meaningful semantic structure

- **First 3 experiments**: 1) Reproduce choice accuracy for one pretrained model family across all layers, confirming residual streams outperform attention/MLP outputs, 2) Compare pretrained vs. instruction-tuned variant of the same model to quantify instruction-tuning gain, 3) Test behavioral alignment with simplified prompt to isolate instruction-following capability from semantic knowledge

## Open Questions the Paper Calls Out

- Can triplet-based alignment objectives be used to fine-tune language models for improved robustness and trustworthiness?
- Do humans and models fundamentally differ in their construction of similarity, specifically regarding association versus categorical type?
- Does the finding that small models achieve human-level representational alignment generalize to complex, context-dependent textual data?

## Limitations
- The 3TT dataset uses single words without surrounding context, leaving the alignment of compositional semantics untested
- The analysis of human-model differences in similarity construction is qualitative rather than systematic
- No training or fine-tuning experiments were conducted to validate the utility of triplet data for alignment

## Confidence
- High: Core finding that small models achieve human-level representational alignment
- Medium: Mechanism that instruction tuning reshapes representational geometry
- Medium: Finding that behavioral alignment lags representational alignment in small models

## Next Checks
1. Verify centering procedure is correctly applied to embeddings before computing cosine similarities
2. Confirm the exact chat template format used for each instruction-tuned model family
3. Test behavioral evaluation with simplified prompts to isolate instruction-following capability from semantic knowledge