---
ver: rpa2
title: Building and Aligning Comparable Corpora
arxiv_id: '2508.02555'
source_url: https://arxiv.org/abs/2508.02555
tags:
- documents
- arabic
- document
- comparable
- corpus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents methods for building and aligning comparable
  corpora across multiple languages. The authors extract English-French-Arabic comparable
  documents from Wikipedia and EURONEWS, aligning them at the document level.
---

# Building and Aligning Comparable Corpora

## Quick Facts
- arXiv ID: 2508.02555
- Source URL: https://arxiv.org/abs/2508.02555
- Reference count: 8
- Primary result: CL-LSI outperforms dictionary-based methods for cross-lingual document alignment, achieving 85% accuracy on BBC-JSC news alignment

## Executive Summary
This paper presents methods for building and aligning comparable corpora across English, French, and Arabic. The authors extract comparable documents from Wikipedia and EURONEWS, then propose two cross-lingual similarity measures: a dictionary-based approach and a Cross-Lingual Latent Semantic Indexing (CL-LSI) method. Experiments demonstrate that CL-LSI significantly outperforms the dictionary-based approach, achieving 85% accuracy when aligning English-Arabic news documents from BBC and ALJAZEERA. The CL-LSI method proves effective for both topic-level and event-level alignment when constrained by temporal windows.

## Method Summary
The method involves constructing comparable corpora from Wikipedia (using inter-language links) and EURONEWS (using hyperlinks), preprocessing text with morphological analysis and frequency filtering, then applying two cross-lingual similarity measures. The dictionary-based method uses bilingual dictionaries and translation lookup, while CL-LSI concatenates aligned document pairs, applies SVD to create a shared semantic space, and computes cosine similarity in the reduced k=300 dimensional space. For event-level alignment, the authors constrain similarity computation to documents from the same time window (monthly), achieving 85% accuracy on BBC-JSC news alignment.

## Key Results
- CL-LSI achieves R@1 scores of 0.87-0.94 on parallel corpora compared to 0.11-0.46 for dictionary-based methods
- 85% accuracy in aligning English-Arabic news documents from BBC and ALJAZEERA when using monthly temporal filtering
- CL-LSI successfully performs event-level alignment, not just topic-level matching
- Dictionary-based methods limited by Arabic morphological complexity and WordNet coverage gaps

## Why This Works (Mechanism)

### Mechanism 1: Cross-Lingual Latent Semantic Indexing (CL-LSI)
Concatenate aligned document pairs into single vectors, apply SVD to create shared semantic space where semantically related terms across languages map to similar dimensions. Cosine similarity in reduced k=300 space quantifies cross-lingual relatedness. Works when training and test documents share semantic overlap; degrades with domain mismatch.

### Mechanism 2: Dictionary-Based Cross-Lingual Matching
Count translation matches between document pairs using WordNet synsets, weighted by binary or tfidf schemes. Limited by dictionary coverage (14K Arabic vs 148K English entries) and morphological complexity, achieving only 0.11-0.46 R@1.

### Mechanism 3: Event-Level Alignment via Temporal Filtering
Split corpora into monthly sub-corpora and constrain alignment to same-month documents. Improves event-level matching accuracy from 305/360 pairs correctly aligned. Monthly granularity provides temporal signal but may be too coarse for rapidly evolving stories.

## Foundational Learning

- **Vector Space Model (VSM) and tfidf weighting**: Documents represented as term vectors; tfidf weights common words lower and discriminative terms higher. *Quick check*: If a word appears in 90% of documents, will its tfidf weight be high or low?

- **Singular Value Decomposition (SVD)**: Decomposes term-document matrices into latent dimensions, capturing semantic patterns while reducing noise. *Quick check*: Why might k=300 work better than k=50 or k=1000 for document retrieval?

- **Cosine similarity**: Measures angle between document vectors, invariant to document length. *Quick check*: Two documents have identical word proportions but different lengths - what will their cosine similarity be?

## Architecture Onboarding

- **Component map**: Corpus Construction -> Preprocessing -> CL-LSI Training -> Projection -> Retrieval/Alignment
- **Critical path**: Training corpus alignment quality → SVD semantic space quality → projection accuracy → retrieval performance
- **Design tradeoffs**: k dimension (100 loses nuance, 500 retains noise), temporal granularity (weekly precise but small pool, monthly broader but less precise), training corpus domain consistency
- **Failure signatures**: Low similarity scores across all pairs (check preprocessing), high-similarity misalignments (topic-level not event-level), language-pair specific degradation (morphological analyzer quality)
- **First 3 experiments**: 1) Reproduce baseline on parallel corpus (target R@1 ≥ 0.95), 2) Domain transfer test (EURONEWS to Wikipedia), 3) Ablate temporal filtering (all-vs-all alignment)

## Open Questions the Paper Calls Out

- **Sentence-level alignment**: Can CL-LSI be adapted for sentence-level rather than document-level alignment? The paper proposes this for machine translation training but hasn't evaluated it.

- **Preprocessing impact**: How do different preprocessing techniques (stemming, lemmatization, linguistic features) affect CL-LSI performance? The paper lists this as a future direction.

- **Social network adaptation**: Is CL-LSI robust enough for social network text given differences in structure and noise? The authors propose collecting social media comparable corpora for evaluation.

## Limitations

- Dictionary-based method severely limited by Arabic morphological complexity and WordNet coverage gaps (14K vs 148K entries)
- CL-LSI performance depends heavily on training corpus quality and domain consistency, with degradation rates not quantified
- Temporal filtering uses arbitrary monthly windows without exploring optimal granularity for event-level alignment

## Confidence

- **High confidence**: CL-LSI mechanism and superiority over dictionary methods, supported by multiple corpus experiments
- **Medium confidence**: Event-level alignment claims, as temporal filtering improves discrimination but monthly window choice appears arbitrary
- **Low confidence**: Dictionary-based method viability for Arabic-English matching given severe performance gaps

## Next Checks

1. **Domain Transfer Experiment**: Train CL-LSI on EURONEWS, test on Wikipedia. Measure R@1 degradation compared to same-domain training.

2. **Temporal Granularity Sweep**: Re-run BBC-JSC alignment with weekly, bi-weekly, and quarterly temporal windows. Compare accuracy to monthly baseline.

3. **Morphological Impact Analysis**: Measure Arabic-English matching rates using MorphAr+lemmatization (57%), light stemming only, and full stemming only. Quantify contribution of each preprocessing step.