---
ver: rpa2
title: 'Monet: Reasoning in Latent Visual Space Beyond Images and Language'
arxiv_id: '2511.21395'
source_url: https://arxiv.org/abs/2511.21395
tags:
- latent
- visual
- reasoning
- embeddings
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Monet, a training framework that enables
  multimodal large language models (MLLMs) to reason directly within the latent visual
  space by generating continuous embeddings as intermediate visual thoughts, rather
  than relying on external visual tools. Monet addresses two key challenges: the high
  computational cost of aligning latent embeddings with image tokens, and insufficient
  supervision over latent embeddings.'
---

# Monet: Reasoning in Latent Visual Space Beyond Images and Language

## Quick Facts
- **arXiv ID**: 2511.21395
- **Source URL**: https://arxiv.org/abs/2511.21395
- **Reference count**: 40
- **Key outcome**: Monet-7B outperforms baselines on V*, HRBench, and MME-RealWorld by 4.25%–9.75%, and achieves best open-source performance on VisualPuzzles abstract reasoning tasks.

## Executive Summary
Monet introduces a training framework enabling multimodal large language models (MLLMs) to reason directly within continuous latent visual space, eliminating dependency on external visual tools. The framework addresses two core challenges: the high computational cost of aligning latent embeddings with image tokens, and insufficient supervision over latent embeddings. Through a three-stage distillation-based supervised fine-tuning pipeline and a novel reinforcement learning method called VLPO (Visual-latent Policy Optimization), Monet trains models to generate intermediate visual thoughts as continuous embeddings. Experiments demonstrate consistent performance gains on real-world perception and reasoning benchmarks, along with strong out-of-distribution generalization on abstract visual reasoning tasks.

## Method Summary
Monet employs a three-stage supervised fine-tuning pipeline followed by reinforcement learning. Stage 1 warms up the model on interleaved image-text data, teaching it to attend to intermediate images. Stage 2 generates high-quality target latent embeddings using dual supervision: aligning observation token representations with a teacher model while controlling attention flow to enforce an image→latent→observation information path. Stage 3 trains the model to produce these embeddings without seeing auxiliary images, closing the train-inference gap. The VLPO reinforcement learning method explicitly optimizes latent embeddings by treating them as differentiable actions, addressing limitations of discrete-text-only RL approaches like GRPO. The framework is trained on Monet-SFT-125K, a high-quality text-image interleaved chain-of-thought dataset.

## Key Results
- Monet-7B consistently outperforms baselines (Deepeyes, LVR, vanilla SFT+GRPO) on V*, HRBench, and MME-RealWorld benchmarks by 4.25%–9.75%.
- Achieves best performance among open-source models on VisualPuzzles abstract visual reasoning tasks.
- Demonstrates strong out-of-distribution generalization beyond training data distribution.
- Ablation studies confirm the effectiveness of the three-stage pipeline and VLPO over standard approaches.

## Why This Works (Mechanism)

### Mechanism 1: Dual Supervision with Controlled Information Flow
The SFT Stage 2 employs teacher-student distillation where the student generates latent embeddings instead of seeing ground-truth auxiliary images. The dual supervision includes an alignment loss matching observation token representations between teacher and student, plus a controlled attention mask allowing latent embeddings to attend directly to auxiliary image embeddings. This forces latent embeddings to encode task-relevant visual information as a bottleneck for predicting subsequent observation tokens.

### Mechanism 2: Visual-latent Policy Optimization (VLPO)
VLPO treats continuous latent embeddings as differentiable actions by modeling them as samples from a Gaussian distribution, enabling direct reinforcement learning. Unlike GRPO which can only compute losses on discrete text tokens, VLPO's policy gradient updates explicitly optimize the latent visual reasoning process by maximizing the probability of "good" latent embeddings that lead to correct final answers.

### Mechanism 3: Progressive Multi-Stage Training Pipeline
The three sequential SFT stages systematically build model capability: Stage 1 teaches visual grounding, Stage 2 generates quality target embeddings via dual supervision, and Stage 3 closes the train-inference gap by training latent generation without auxiliary images. This scaffolded approach prevents trivial solutions and avoids the computational cost of direct image-token alignment during generation.

## Foundational Learning

**Policy Gradient Methods in RL**: VLPO modifies policy gradient methods (like PPO/GRPO) to handle continuous latent actions. Understanding how policy gradients use reward signals to update behavior is essential. *Quick check*: Explain the role of the probability ratio `r_i,t(θ)` in GRPO and why it's hard to compute for continuous values.

**Teacher-Student Knowledge Distillation**: The SFT pipeline uses teacher-student setup where teacher representations guide student learning via alignment loss. *Quick check*: What specific "knowledge" is distilled from teacher to student via observation tokens?

**Attention Masks in Transformers**: Controlled attention flow uses custom masks to enforce `image → latent → observation` path. *Quick check*: Why prevent post-latent text tokens from directly attending to auxiliary image embeddings in Stage 2?

## Architecture Onboarding

**Component map**: Qwen2.5-VL-7B Core MLLM -> Latent Generator (modified decoding) -> Qwen2.5-VL-7B Core MLLM -> ... -> Text Output

**Critical path**: 
1. **Inference**: Input → MLLM Core → (if `<latent>`) Latent Generator → MLLM Core → ... → Text Output
2. **Training**: SFT Stage 1 (Warm-up) → SFT Stage 2 (Generate Target Embeddings) → SFT Stage 3 (Learn Latent Generation) → VLPO (RL Refinement)

**Design tradeoffs**:
- **Fixed vs. Adaptive Latent Length (K)**: Fixed K=8 chosen for simplicity over flexibility
- **Auxiliary Image vs. Latent Embedding**: Trades inference-time external tool cost for training-time complex pipeline
- **GRPO vs. VLPO**: VLPO adds complexity but claims to fix GRPO's inability to optimize latent reasoning

**Failure signatures**:
- **Latent Mode Collapse**: Embeddings encode no useful information (performance drops when K>0)
- **Train/Inference Mismatch**: Model depends on auxiliary images seen during training
- **RL Instability**: VLPO training becomes unstable due to poor Gaussian approximation or sparse rewards

**First 3 experiments**:
1. Implement modified decoding logic to generate fixed sequence of latent embeddings and verify mode switching
2. Run warm-up SFT on small subset and confirm observation token accuracy improves with auxiliary images
3. Create unit test for Stage 2 attention mask verifying correct `image → latent → observation` flow

## Open Questions the Paper Calls Out
- How different reward designs might influence latent visual reasoning in MLLMs
- Whether the complex three-stage SFT pipeline can be condensed into a unified, end-to-end training framework
- Whether MLLMs can autonomously determine optimal number of latent embeddings (K) rather than using fixed values
- The extent to which generated latent embeddings correspond to interpretable visual concepts or discrete image features

## Limitations
- Performance highly sensitive to quality of observation token annotations in training data
- Choice of fixed latent embedding length (K=8) is somewhat arbitrary and not thoroughly validated
- VLPO's Gaussian approximation assumption for high-dimensional continuous embeddings not empirically validated
- Strong out-of-distribution generalization claims lack detailed analysis of why model generalizes to abstract reasoning tasks

## Confidence
**High Confidence**: Three-stage SFT pipeline structure and basic mechanism of generating continuous embeddings as intermediate visual thoughts; experimental results showing Monet-7B outperforming baselines
**Medium Confidence**: Specific implementation details of controlled attention flow and exact formulation of dual supervision loss; effectiveness of VLPO in improving latent reasoning
**Low Confidence**: Out-of-distribution generalization claims, particularly strong performance on VisualPuzzles dataset

## Next Checks
1. **Ablation on Latent Embedding Length**: Systematically vary K (4, 8, 12, 16) and measure performance on primary benchmarks
2. **Controlled Attention Mask Verification**: Implement unit test verifying Stage 2 attention mask correctly enforces `auxiliary image → latent → observation` flow
3. **VLPO Sensitivity Analysis**: Vary Gaussian standard deviation σ (1.0, 5.0, 10.0, 20.0) and measure training stability and final performance