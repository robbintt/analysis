---
ver: rpa2
title: Mitigating Hallucinations in Healthcare LLMs with Granular Fact-Checking and
  Domain-Specific Adaptation
arxiv_id: '2512.16189'
source_url: https://arxiv.org/abs/2512.16189
tags:
- clinical
- summaries
- fact-checking
- these
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses hallucinations in LLM-generated healthcare\
  \ summaries by proposing a two-stage pipeline: a LoRA-fine-tuned LLaMA-3.1-8B model\
  \ for generating clinically coherent discharge summaries and an independent, LLM-free\
  \ fact-checking module that uses structured proposition-level verification. The\
  \ fact-checking system applies deterministic logical rules\u2014negation, implication,\
  \ temporal, numerical, and mutual exclusivity checks\u2014without generative model\
  \ dependency."
---

# Mitigating Hallucinations in Healthcare LLMs with Granular Fact-Checking and Domain-Specific Adaptation

## Quick Facts
- **arXiv ID**: 2512.16189
- **Source URL**: https://arxiv.org/abs/2512.16189
- **Reference count**: 0
- **Primary result**: Achieves 0.8556 F1-score in hallucination detection using LLM-free fact-checking on healthcare summaries

## Executive Summary
This work addresses hallucinations in LLM-generated healthcare summaries through a two-stage pipeline that combines a LoRA-fine-tuned LLaMA-3.1-8B model for generating clinically coherent discharge summaries with an independent, LLM-free fact-checking module. The fact-checking system employs structured proposition-level verification using deterministic logical rules including negation, implication, temporal, numerical, and mutual exclusivity checks. Evaluated on 104 MIMIC-III discharge summaries containing 3,786 propositions, the system demonstrates high precision (0.8904) and recall (0.8234) while reducing hallucinations without generative model dependency.

## Method Summary
The proposed approach implements a two-stage pipeline for healthcare summary generation and verification. First, a LoRA-fine-tuned LLaMA-3.1-8B model generates clinically coherent discharge summaries. Second, an independent fact-checking module performs proposition-level verification using deterministic logical rules rather than generative models. The system processes clinical assertions through negation checks, implication verification, temporal consistency validation, numerical accuracy assessment, and mutual exclusivity detection. This LLM-free verification approach ensures that the fact-checking process itself does not introduce additional hallucinations while maintaining clinical relevance in the generated outputs.

## Key Results
- Achieves precision of 0.8904, recall of 0.8234, and F1-score of 0.8556 on MIMIC-III dataset
- Evaluates on 104 discharge summaries with 3,786 propositions total
- Outperforms existing hallucination detection methods while maintaining clinically coherent outputs
- Successfully reduces hallucinations through deterministic logical verification without generative model dependency

## Why This Works (Mechanism)
The system's effectiveness stems from separating generation from verification, using domain-specific adaptation for healthcare context, and employing deterministic logical rules that avoid the hallucination risks inherent in generative models. By fine-tuning LLaMA-3.1-8B with LoRA for clinical coherence and then applying structured fact-checking with explicit logical rules, the approach creates a reliable verification process that doesn't compound errors.

## Foundational Learning
- **LoRA fine-tuning**: Why needed - Adapts base model to healthcare domain without full fine-tuning cost; Quick check - Verify domain-specific vocabulary and clinical terminology incorporation
- **Proposition-level verification**: Why needed - Enables granular fact-checking at assertion level rather than document level; Quick check - Confirm each clinical assertion is independently validated
- **Deterministic logical rules**: Why needed - Avoids hallucination risks from generative models during verification; Quick check - Test rule coverage across different clinical assertion types
- **MIMIC-III dataset**: Why needed - Provides standardized, annotated clinical discharge summaries for evaluation; Quick check - Validate dataset completeness and annotation quality
- **Two-stage pipeline architecture**: Why needed - Separates generation quality from verification accuracy; Quick check - Ensure no cross-contamination between stages
- **Clinical coherence metrics**: Why needed - Ensures generated summaries maintain medical relevance; Quick check - Verify coherence scoring aligns with clinical standards

## Architecture Onboarding

**Component Map**: LLaMA-3.1-8B (LoRA-tuned) -> Proposition Extractor -> Fact-Checker (logical rules) -> Validation Output

**Critical Path**: Summary generation → Assertion extraction → Proposition verification → Output validation

**Design Tradeoffs**: The system trades flexibility of generative verification for reliability of deterministic rules, accepting potential rule coverage gaps for guaranteed hallucination-free verification.

**Failure Signatures**: Rule coverage gaps for complex temporal reasoning, inability to handle probabilistic statements, potential brittleness with novel clinical assertion patterns not covered by existing rules.

**Three First Experiments**:
1. Test fact-checking accuracy on single-proposition assertions before scaling to multi-proposition summaries
2. Validate logical rule coverage by systematically testing edge cases in clinical assertions
3. Compare verification speed and accuracy between deterministic rules versus LLM-based approaches on identical assertions

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Evaluation constrained to 104 MIMIC-III discharge summaries, potentially limiting generalizability to broader clinical contexts
- Fact-checking system relies on manually curated logical rules that may not adapt to evolving medical knowledge without additional engineering
- While verification avoids LLM hallucinations, initial proposition extraction may still involve LLM processing, creating ambiguity in "no hallucination" claims

## Confidence
- **High confidence**: Technical architecture specification and reproducibility of two-stage pipeline
- **Medium confidence**: Claims of outperforming existing methods require external validation due to incomplete baseline details
- **Medium confidence**: Clinical coherence assertions need independent expert verification beyond author team

## Next Checks
1. Test fact-checking pipeline on discharge summaries from other institutions to assess generalizability beyond MIMIC-III
2. Conduct blinded evaluation by independent healthcare professionals to verify clinical coherence and accuracy
3. Systematically audit logical rule set against broader sample of clinical assertions to identify coverage gaps for edge cases