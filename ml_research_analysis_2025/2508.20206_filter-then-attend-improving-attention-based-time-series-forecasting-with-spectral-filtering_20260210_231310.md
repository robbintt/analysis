---
ver: rpa2
title: 'Filter then Attend: Improving attention-based Time Series Forecasting with
  Spectral Filtering'
arxiv_id: '2508.20206'
source_url: https://arxiv.org/abs/2508.20206
tags:
- forecasting
- time
- filter
- series
- spectral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FilterFormers, a method that improves transformer-based
  time series forecasting by adding a simple learnable frequency filter before the
  transformer blocks. The filters, which add only ~1000 parameters, help transformers
  capture high-frequency patterns and counteract their inherent low-pass bias.
---

# Filter then Attend: Improving attention-based Time Series Forecasting with Spectral Filtering

## Quick Facts
- arXiv ID: 2508.20206
- Source URL: https://arxiv.org/abs/2508.20206
- Authors: Elisha Dayag; Nhat Thanh Van Tran; Jack Xin
- Reference count: 8
- Primary result: FilterFormers consistently outperform baseline transformers by 5-10% in forecasting accuracy while using smaller embedding dimensions

## Executive Summary
FilterFormers introduces a novel approach to improving transformer-based time series forecasting by incorporating a learnable frequency filter before transformer blocks. This lightweight addition (adding only ~1000 parameters) addresses the inherent low-pass bias in transformers, enabling them to better capture high-frequency patterns in time series data. The method demonstrates consistent improvements across nine diverse datasets, showing particular effectiveness in complex signal modeling.

The approach is notable for its simplicity and efficiency, working by amplifying mid-to-high frequency components through spectral filtering. This allows transformers to model complex patterns more effectively without requiring architectural overhauls. The empirical results show that FilterFormers not only improve forecasting accuracy but also enable the use of smaller embedding dimensions while maintaining or improving performance.

## Method Summary
FilterFormers is a method that enhances transformer-based time series forecasting by adding a learnable frequency filter before transformer blocks. The filters are designed to counteract transformers' inherent low-pass bias, allowing them to better capture high-frequency patterns in time series data. The approach adds approximately 1000 parameters to existing transformer architectures, making it lightweight and efficient. The filtering mechanism works by amplifying mid-to-high frequency components, enabling better modeling of complex signals while maintaining the transformer's attention-based architecture.

## Key Results
- FilterFormers consistently outperforms baseline transformer models (PatchTST, iTransformer, Leddam) by 5-10% in forecasting accuracy across nine datasets
- The approach enables use of smaller embedding dimensions while maintaining or improving performance
- Synthetic experiments confirm that filters amplify mid-to-high frequency components, enhancing modeling of complex signals

## Why This Works (Mechanism)
The mechanism behind FilterFormers' effectiveness lies in addressing the fundamental low-pass bias inherent in transformer architectures. Transformers naturally tend to focus on lower frequency patterns, missing important high-frequency details crucial for accurate forecasting. By adding a learnable frequency filter before transformer blocks, the method amplifies mid-to-high frequency components that would otherwise be attenuated. This spectral filtering allows the attention mechanism to work with enhanced high-frequency information, leading to more accurate pattern recognition and forecasting. The synthetic experiments demonstrate that this frequency amplification directly translates to better performance on complex signals.

## Foundational Learning
- **Spectral filtering in neural networks**: Why needed - to manipulate frequency domain characteristics of signals before processing. Quick check - verify that the filters can be learned and applied efficiently in the frequency domain.
- **Transformer attention mechanism**: Why needed - understanding the baseline architecture that FilterFormers modifies. Quick check - confirm that attention patterns change when filters are applied.
- **Time series frequency characteristics**: Why needed - to understand what frequency components are important for forecasting. Quick check - analyze frequency spectra of benchmark datasets.
- **Learnable frequency filters**: Why needed - to adapt filtering to specific time series characteristics. Quick check - ensure filters can be trained end-to-end with transformers.
- **Low-pass bias in transformers**: Why needed - to understand the problem FilterFormers addresses. Quick check - verify that transformers naturally attenuate high frequencies.
- **Frequency amplification effects**: Why needed - to understand how filtering improves forecasting. Quick check - measure frequency component changes after filtering.

## Architecture Onboarding

**Component Map**: Input Time Series -> Learnable Frequency Filter -> Transformer Blocks -> Output Forecasting

**Critical Path**: The learnable frequency filter is applied to the input time series before it enters the transformer blocks. This filter operates in the frequency domain, amplifying mid-to-high frequency components. The filtered output then flows through standard transformer attention mechanisms and feed-forward layers to produce forecasts.

**Design Tradeoffs**: The primary tradeoff is between model complexity and performance gain. Adding ~1000 parameters for frequency filtering is minimal compared to the overall transformer architecture, making it an efficient enhancement. However, this approach assumes that frequency domain manipulation is beneficial across all time series types, which may not hold for datasets with rapidly changing frequency characteristics.

**Failure Signatures**: The method may underperform when applied to highly non-stationary time series where frequency content changes dramatically over time. It could also be less effective on very long time series where the frequency characteristics vary significantly across different time periods. Additionally, if the learnable filters converge to trivial solutions (e.g., identity mappings), the expected performance improvements may not materialize.

**First Experiments**:
1. Apply FilterFormers to a synthetic time series with known frequency components to verify that the filters amplify the intended frequency ranges
2. Compare attention patterns with and without filtering on a benchmark dataset to confirm that high-frequency information is being utilized
3. Test the method on a dataset with rapidly changing frequency content to assess robustness to non-stationary signals

## Open Questions the Paper Calls Out
The paper acknowledges several open questions regarding the behavior of filters with non-stationary time series where frequency content changes over time. It also notes the need for further validation on datasets with very long time horizons or highly irregular sampling patterns. The universal applicability of spectral filtering to all transformer architectures remains an open question requiring additional investigation.

## Limitations
- Limited validation on datasets with highly irregular sampling rates and long-term dependencies
- Unclear behavior with non-stationary time series where frequency content changes over time
- Potential overfitting to specific frequency characteristics of benchmark datasets

## Confidence

| Claim | Confidence |
|-------|------------|
| FilterFormers consistently improves forecasting accuracy by 5-10% | Medium |
| Filters can be learned efficiently with minimal parameter overhead | High |
| Method works across diverse time series domains | Medium |
| Filters amplify mid-to-high frequency components as intended | High |
| Smaller embedding dimensions maintain performance | Medium |

## Next Checks

1. Test FilterFormers on time series datasets with highly irregular sampling rates and long-term dependencies to assess robustness beyond the nine benchmark datasets used in the paper.

2. Conduct ablation studies removing the spectral filtering component to quantify the exact contribution of the filters versus potential architectural improvements from other modifications.

3. Evaluate FilterFormers on real-world applications where frequency content is known to change over time (such as financial market data during different economic regimes) to assess performance under non-stationary conditions.