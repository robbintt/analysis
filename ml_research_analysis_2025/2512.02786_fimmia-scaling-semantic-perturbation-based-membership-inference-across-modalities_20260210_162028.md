---
ver: rpa2
title: 'FiMMIA: scaling semantic perturbation-based membership inference across modalities'
arxiv_id: '2512.02786'
source_url: https://arxiv.org/abs/2512.02786
tags:
- qwen2
- arxiv
- data
- inference
- vl-3b-instruct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of detecting whether specific
  data points were used in training multimodal large language models (MLLMs), an issue
  exacerbated by distribution shifts and instabilities in multimodal components. The
  authors propose FiMMIA, a modular framework that extends perturbation-based membership
  inference attacks to MLLMs by training a neural network to analyze differences in
  the target model's behavior on perturbed inputs.
---

# FiMMIA: scaling semantic perturbation-based membership inference across modalities

## Quick Facts
- **arXiv ID:** 2512.02786
- **Source URL:** https://arxiv.org/abs/2512.02786
- **Reference count:** 26
- **Primary result:** FiMMIA achieves AUC-ROC scores of 81%-98% and TPR@5%FPR of 60%-100% in detecting training data leakage in multimodal LLMs

## Executive Summary
FiMMIA is a novel framework for membership inference attacks on multimodal large language models (MLLMs) that extends perturbation-based approaches across image, video, and audio modalities. The method generates semantically perturbed text neighbors and analyzes the target model's behavioral responses to detect whether specific data points were used during training. Comprehensive evaluations demonstrate high effectiveness with strong transferability across different model families, addressing a critical gap in privacy auditing for multimodal systems.

## Method Summary
FiMMIA operates by generating K=24 text neighbors per sample through four perturbation techniques (masking, deletion, duplication, swapping), then analyzing the target MLLM's loss and embedding differences between original and perturbed inputs. A neural network classifier is trained on these aggregated statistics to distinguish between models that have and have not seen the data. The approach uses gray-box access to compute model losses and employs embedding space calibration to normalize for sample difficulty. Training uses Adafactor optimizer with learning rate 2×10⁻⁶ for 10 epochs, with a fixed embedding model (E5-Mistral) for feature extraction.

## Key Results
- Achieves AUC-ROC scores ranging from 81% to 98% across various fine-tuned multimodal models
- Demonstrates TPR@5%FPR of 60%-100% in detecting training data membership
- Shows strong cross-model family transferability, though with performance degradation (AUC ~78% when transferring from Qwen to Llama3 models)
- Outperforms baseline distribution shift detection methods in most evaluated scenarios

## Why This Works (Mechanism)

### Mechanism 1: Semantic Perturbation Response Divergence
Models exhibit lower and more stable loss differences on perturbed variants of training samples compared to unseen data. By generating semantically similar neighbors and measuring loss consistency, FiMMIA captures distributional differences between members and non-members. The text modality drives this signal while multimodal components remain fixed.

### Mechanism 2: Embedding Space Calibration
Difference vectors between original and perturbed text embeddings serve as a proxy for sample difficulty calibration. Large embedding shifts indicate harder perturbations, and normalizing loss differences based on this helps isolate the membership signal from sample complexity. This uses a fixed external encoder (E5-Mistral) to produce embeddings.

### Mechanism 3: Neural Network Meta-Classification of Aggregated Statistics
A neural network learns patterns in normalized loss and embedding difference statistics across multiple neighbors to robustly classify membership. The classifier takes z-score normalized loss differences and embedding differences as input, outputting membership probabilities that are averaged across all neighbors. This creates a discriminative feature space where members and non-members form separable clusters.

## Foundational Learning

**Concept: Membership Inference Attacks (MIA)**
- Why needed: This is the core problem FiMMIA addresses - detecting whether specific data points were used in training
- Quick check: What is the fundamental assumption about model behavior that makes membership inference possible? (Hint: It relates to performance on seen vs. unseen data)

**Concept: Distribution Shift in Benchmark Data**
- Why needed: The paper identifies distribution shifts between member and non-member sets as a critical flaw in existing MIA benchmarks
- Quick check: Why is a distribution shift between member and non-member sets problematic for evaluating the true efficacy of an MIA method?

**Concept: Gray-Box Model Access**
- Why needed: FiMMIA's primary experiments assume gray-box access, requiring the attacker to compute the model's loss on arbitrary inputs
- Quick check: What specific output from the target model does the FiMMIA pipeline require during its inference phase?

## Architecture Onboarding

**Component map:**
1. Target MLLM (M) → 2. Perturbation Generator → 3. Loss Calculator → 4. Embedding Encoder (E) → 5. Feature Difference Calculator → 6. Normalizer → 7. FiMMIA Classifier (f_FIMMIA) → 8. Aggregator

**Critical path:**
1. **Data Prep:** Split dataset into train/test and compute normalization statistics (µ, σ) from train split
2. **Training Phase:** For model fine-tuned on train split, generate neighbors, compute losses/embeddings, calculate differences, normalize, and train classifier
3. **Inference Phase:** For target model and suspect sample, generate neighbors, compute losses/embeddings, calculate/normalize differences, feed to trained classifier, and aggregate scores

**Design tradeoffs:**
- Perturbation count (K=24) balances robustness against computational cost
- Fixed modality (s' = s) simplifies pipeline but may miss cross-modal leakage signals
- Choice of E5-Mistral embedding model adds dependency but provides good calibration
- LoRA fine-tuning validation may not generalize to full fine-tuning scenarios

**Failure signatures:**
- Low TPR@5%FPR (< 60%) indicates insufficient discriminative power
- Poor cross-family transfer (AUC < 70%) shows limited generalizability
- High baseline AUC (> 70%) from distribution shift detection indicates dataset flaws

**First 3 experiments:**
1. **Validate on Known Split:** Fine-tune Qwen2.5-VL-3B on random 90/10 train/test split from MERA, run full pipeline, verify AUC-ROC > 90% and high TPR@5%FPR
2. **Test Cross-Model Transfer:** Train FiMMIA on Qwen2.5-VL-3B features, then test on Llama3-llava-next-8b fine-tuned on same data, measure AUC drop
3. **Run Baseline Distribution Check:** Execute distribution shift detection baseline first; expect ~50% AUC for valid datasets, >70% flags potential dataset issues

## Open Questions the Paper Calls Out

**Open Question 1:** Can FiMMIA maintain high detection efficacy when applied to pretraining and full fine-tuning regimes instead of LoRA-based fine-tuning?
- Basis: The paper explicitly states results may differ for pretraining and full fine-tuning, leaving these evaluations for future work
- Resolution needed: AUC-ROC and TPR metrics from evaluations on models trained with full parameter updates

**Open Question 2:** Is the attack methodology effective in strict black-box settings where per-sample loss access is unavailable?
- Basis: The authors acknowledge gray-box access assumption and plan to implement black-box version in future releases
- Resolution needed: Successful inference using only API-accessible outputs (top-k logprobs or generated text probabilities)

**Open Question 3:** How does generating neighbors by perturbing non-text modalities impact attack performance compared to text-only perturbations?
- Basis: The methodology fixes s=s_k' (leaving multimodal data unchanged) while acknowledging the pipeline could support different modality neighbors
- Resolution needed: Comparative ablation studies showing performance when perturbing image/video/audio versus text-only strategy

## Limitations
- Relies on gray-box access (loss computation) limiting real-world black-box applicability
- Text-centric perturbation mechanism may miss multimodal leakage signals in non-text modalities
- Significant performance degradation when transferring across different model families
- Validated primarily on LoRA fine-tuned models; effectiveness on full fine-tuning/pre-training unknown
- High computational cost requiring loading large models for inference

## Confidence

**High Confidence:** Core perturbation response divergence mechanism and experimental methodology are well-specified and reproducible
**Medium Confidence:** Embedding space calibration effectiveness is supported by ablation but relies on E5-Mistral appropriateness across datasets
**Medium Confidence:** Transferability results are promising but show significant variance across model families
**Low Confidence:** Performance in true black-box settings remains unevaluated and unknown

## Next Checks

1. **Validate Baseline Distribution Shift:** Run provided distribution shift detection baseline first; expect ~50% AUC for valid datasets, >70% indicates dataset flaws that invalidate FiMMIA results
2. **Test Black-Box Applicability:** Adapt pipeline to use only model predicted probabilities instead of losses; evaluate if AUC-ROC remains above 80% with this proxy
3. **Evaluate on Non-LoRA Fine-Tuning:** Fine-tune target MLLM using full fine-tuning instead of LoRA; compare FiMMIA performance to reported LoRA results to assess generalizability