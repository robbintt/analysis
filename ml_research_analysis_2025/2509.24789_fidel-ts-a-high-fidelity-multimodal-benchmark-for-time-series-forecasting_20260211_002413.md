---
ver: rpa2
title: 'Fidel-TS: A High-Fidelity Multimodal Benchmark for Time Series Forecasting'
arxiv_id: '2509.24789'
source_url: https://arxiv.org/abs/2509.24789
tags:
- data
- forecasting
- time
- series
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of evaluating time series forecasting
  models due to the lack of high-fidelity benchmarks, which can lead to misleading
  progress assessments. It introduces Fidel-TS, a new benchmark built from real-time
  API data to ensure data integrity, eliminate leakage, and support realistic forecasting
  scenarios.
---

# Fidel-TS: A High-Fidelity Multimodal Benchmark for Time Series Forecasting

## Quick Facts
- **arXiv ID:** 2509.24789
- **Source URL:** https://arxiv.org/abs/2509.24789
- **Reference count:** 40
- **Primary result:** Introduces Fidel-TS, a new large-scale benchmark for time series forecasting built from live APIs to prevent data leakage and contamination, revealing that no single model consistently dominates across unimodal, multimodal, and LLM approaches.

## Executive Summary
Fidel-TS addresses the critical challenge of evaluating time series forecasting models due to the lack of high-fidelity benchmarks that can lead to misleading progress assessments. The benchmark introduces a rigorous framework built from real-time API data to ensure data integrity, eliminate leakage, and support realistic forecasting scenarios. By distinguishing between subjects and channels, Fidel-TS better tests model generalization and incorporates weather data as a leak-free textual modality. Comprehensive experiments across different model types reveal that multimodal gains depend on model architecture and data relevance, while LLMs struggle with precision and reliability under realistic, long-term, context-rich settings.

## Method Summary
Fidel-TS constructs a benchmark from 6 datasets (CCP, GREG, JAP, NYTS, CAISO, BEAR) collected via APIs, ensuring recent data beyond typical LLM knowledge cutoffs. The evaluation framework uses timestamp-based splitting (e.g., Jan 1, 2021/2022) and standard metrics like Mean Squared Error (MSE) across horizons {24, 168, 336, 720} steps. Domain-specific models (PatchTST, FIATS) are trained per dataset while foundation models (Chronos, TimeMoE) are zero-shot evaluated. LLMs (Qwen, DeepSeek) are tested on `-mini` subsets with defined prompt templates. The pipeline handles missing values through linear interpolation for short gaps and marks long gaps as "Sensor Downtime" events, with weather forecasts serving as leak-free textual context.

## Key Results
- No single model consistently dominates across all datasets and conditions
- Multimodal gains depend on both model architecture and the relevance of textual data to target systems
- LLMs struggle with precision and reliability under realistic, long-term, context-rich settings
- Subject-channel distinction enables meaningful within-domain transfer evaluation
- Weather-only textual modality prevents leakage but may lack sufficient causal relevance for some domains

## Why This Works (Mechanism)

### Mechanism 1
API-sourced data reduces pre-training contamination risk for foundation model evaluation. Authentication-protected APIs provide continuously updated streams that extend beyond typical LLM knowledge cutoffs, making direct memorization detectable and preventing retrospective data access that could leak ground truth. This assumes pre-training corpora primarily contain static, indexed web content rather than gated API responses.

### Mechanism 2
Weather forecasts serve as textual modality that prevents both temporal and description leakage while maintaining causal relevance. Weather is exogenous (system-independent), verifiably available before prediction time, and describes environmental conditions rather than directly stating target values. This assumes weather has genuine causal influence on target systems rather than being noise.

### Mechanism 3
Subject-Channel distinction enables realistic transfer evaluation without cross-domain distribution shift artifacts. Separating "subjects" (system instances) from "channels" (observed variables per system) allows held-out subject testing that simulates deploying trained models to new locations within the same domain. This assumes subjects within a dataset share sufficient distributional structure that transfer is meaningful but non-trivial.

## Foundational Learning

- **Temporal leakage in multimodal forecasting**: Central to understanding why existing multimodal benchmarks produce inflated results—retrieved text may contain information from after the prediction timestamp. Quick check: If your model uses a news article dated 2024-03-15 to predict values for 2024-03-10, what leakage type is present?

- **Exogenous vs. endogenous covariates**: Weather is exogenous (independent of target system), making it safe for future-horizon conditioning; control events are partially endogenous but causally structured. Quick check: Why is a scheduled maintenance event safer as textual input than a post-hoc incident report?

- **Foundation model contamination detection**: Old datasets (ETT concluded 2018, Electricity 2019) are likely in LLM pre-training data, making zero-shot claims unreliable. Quick check: How would you distinguish between a model that "learned to forecast" vs. one that "memorized the test set"?

## Architecture Onboarding

- **Component map**: Raw API Streams → Imperfection Handling → Dynamic Event Database → [Time Series API] → [Weather API / Control Logs] → Dataset Assembly → [Subject-Channel Split] → [-h sets] → [-mini sets] → [-obs/-hid sets]

- **Critical path**: 1) Timestamp alignment between time series and textual modalities (strict indexing by start/end times in Dynamic Event Database) 2) Missing value handling: <4 consecutive gaps → linear interpolation; ≥4 gaps → mark as zero + generate "sensor downtime" textual event 3) Importance sampling for LLM evaluation: Stratify by multimodal sensitivity then sample

- **Design tradeoffs**: Weather-only text limits causal relevance for some domains (BEAR ablation shows control events more informative); importance sampling reduces computational cost but may miss edge-case failures; subject-based splits test within-domain transfer but don't evaluate cross-domain generalization

- **Failure signatures**: LLM pass@3 drops to 0% on high-dimensional multivariate inputs (context explosion exceeds 128K window); multimodal models underperform unimodal when text has weak causal link (BEAR + Weather vs. + Control); foundation models degrade sharply on long horizons despite claiming "zero-shot robustness"

- **First 3 experiments**: 1) Baseline replication: Train PatchTST and iTransformer on CCP-h, evaluate on full test set with horizons {24, 168, 336, 720} to confirm no single model dominates 2) Leakage validation: Compare GPT4MTS (history-only text) vs. FIATS (future-condition text) on JAP-h to verify architecture-dependent multimodal gains 3) LLM reliability stress test: Evaluate DeepSeek-R1 on NYTS-mini with unimodal vs. multimodal prompts, tracking both MSE and pass@3 to measure precision vs. reliability tradeoff

## Open Questions the Paper Calls Out

- **Open Question 1**: How can benchmarks incorporate diverse causally grounded textual sources beyond weather data while strictly maintaining leak-free integrity? The authors note that future work will extend the dataset with more causally grounded textual sources while maintaining real-world fidelity to avoid leakage.

- **Open Question 2**: Can robust data filtering pipelines be developed to automatically select the most relevant textual context for specific forecasting tasks? The paper concludes that robust data filtering pipelines are needed to ensure models receive the most relevant contextual information.

- **Open Question 3**: How can LLM architectures be adapted to mitigate "context explosion" and reliability failure when processing high-dimensional, dense multimodal time series? Appendix E.5 notes LLM reliability is severely compromised by context length, causing pass rates to drop to 0% in dense multimodal scenarios.

## Limitations
- Weather-only textual modality may not provide sufficient causal information for all domains, limiting multimodal gains
- Importance sampling approach for LLM evaluation may miss critical edge-case failures
- Subject-based splits test within-domain transfer but don't evaluate cross-domain generalization

## Confidence

- **High Confidence**: API-sourced data approach effectively prevents contamination from pre-training; subject-channel distinction successfully enables meaningful within-domain transfer evaluation; leakage-free weather forecasts mechanism is theoretically sound and practically implemented

- **Medium Confidence**: Claim that no single model consistently dominates requires careful interpretation; assertion that multimodal gains are architecture-dependent is well-supported but could benefit from more extensive ablation studies; finding that LLMs struggle with precision and reliability under realistic settings is robust but may be partially attributable to context window limitations

## Next Checks

1. **Cross-domain transfer validation**: Evaluate models trained on one dataset (e.g., NYTS) on completely different domains (e.g., CAISO) to test generalization beyond subject-level transfer within the same domain

2. **Weather vs. control event ablation**: Systematically compare model performance using weather forecasts versus control events as textual modality across all datasets to quantify the causal relevance tradeoff

3. **Context window stress test**: Compare model performance between standard 128K context models and 1M context variants on high-dimensional multivariate datasets to isolate whether LLM failures are due to context limits versus fundamental forecasting capability gaps