---
ver: rpa2
title: 'Format as a Prior: Quantifying and Analyzing Bias in LLMs for Heterogeneous
  Data'
arxiv_id: '2508.15793'
source_url: https://arxiv.org/abs/2508.15793
tags:
- uni00000013
- uni00000011
- uni00000010
- texts
- uni0000000d
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically investigates format bias in large language
  models (LLMs) when processing heterogeneous data such as text, tables, infoboxes,
  and knowledge graphs. The authors design a three-stage empirical study to examine
  whether such bias exists, what data-level factors drive it, and what internal mechanisms
  underlie its emergence.
---

# Format as a Prior: Quantifying and Analyzing Bias in LLMs for Heterogeneous Data

## Quick Facts
- **arXiv ID**: 2508.15793
- **Source URL**: https://arxiv.org/abs/2508.15793
- **Reference count**: 40
- **Primary result**: Format bias exists across LLM families, driven by information richness, structure quality, and format type, with attention imbalance as key mechanism

## Executive Summary
This paper systematically investigates format bias in large language models when processing heterogeneous data representations including text, tables, infoboxes, and knowledge graphs. The authors design a three-stage empirical study examining whether such bias exists, what data-level factors drive it, and what internal mechanisms underlie its emergence. Results demonstrate consistent format bias across model families, primarily driven by differences in information richness, structure quality, and format type. The study identifies attention imbalance as a key mechanism and proposes an inference-time attention-balancing intervention that improves dual coverage rate without affecting directional preference.

## Method Summary
The authors constructed a dataset of 4,000 entries from ConflictBank, converting factual evidence into four formats (text, table, infobox, knowledge graph) using GPT-4o-mini. They evaluated 10 LLMs on questions with conflicting evidence from different formats, measuring Dual Coverage Rate (DCR) and Format Preference Ratio (FPR). The study employed three stages: baseline evaluation across format pairs, causal analysis through attention gap correlation, and intervention testing with attention re-weighting. Responses were adjudicated by three LLM judges using majority voting, with results validated against human evaluation showing 95% agreement.

## Key Results
- Format bias is consistent across model families (Llama-3.1, Qwen3, Mistral, Gemma-2, GLM-4, GPT-4o-mini) with preference hierarchy: Texts ≈ KGs > Tables > Infoboxes
- Information richness drives bias - models favor evidence sources with more entries (e.g., 12-row tables over 4-row tables)
- Attention imbalance correlates negatively with DCR (-0.31 to -0.54 across models), suggesting greater attention asymmetry reduces dual coverage
- Inference-time attention-balancing intervention significantly improves DCR without affecting FPR, with downstream RAG task improvements of 6.5% and 9.5%

## Why This Works (Mechanism)

### Mechanism 1: Format Heterogeneity Induces Processing Asymmetry via Attention Imbalance
- Claim: Presenting semantically equivalent information in heterogeneous data formats creates systematic processing bias in LLMs, leading them to preferentially attend to and select information from favored formats
- Mechanism: Format bias manifests through Dual Coverage Rate (measuring acknowledgment of both sources) and Format Preference Ratio (measuring directional preference). This bias is driven by "attention imbalance" where LLMs allocate unequal attention mass to different format segments during inference. Higher attention gaps correlate with lower DCR, meaning models are less likely to recognize both sources. Interestingly, models often favor sources receiving less attention when expressing single-sided preferences, suggesting a complex, non-monotonic relationship between attention and final output preference
- Core assumption: Attention weights during inference are primary proxies for information selection and processing priority, and manipulating these weights can directly alter behavior
- Evidence anchors: "Results show that format bias is consistent across model families and is primarily driven by differences in information richness, structure quality, and format type. Attention imbalance is found to be closely associated with the presence of bias..."; "The results show coefficients of –0.31, –0.37, and –0.54 across the three LLMs, indicating a weak to moderate negative correlation."
- Break condition: If simpler confounding factors like tokenization density or prompt-order artifacts not controlled by randomization were the sole drivers of observed preference

### Mechanism 2: Attention Re-weighting Intervention Modulates Dual-Coverage but Not Directional Preference
- Claim: An inference-time intervention that normalizes attention weights to ensure equal mass for conflicting evidence segments can increase the model's ability to acknowledge both perspectives (higher DCR) but does not reliably shift its final, single-sided preference (stable FPR)
- Mechanism: The intervention operates on the attention tensor A, identifying key token indices for evidence segments, calculating total attention mass on each, and re-scaling individual attention weights so segment totals equal their average. This forces balanced attention, leading to statistically significant DCR improvements across all format pairs and models. However, FPR remains stable, indicating the "direction of bias" is more deeply entrenched, likely from pre-training data distribution, and resistant to this lightweight, inference-time manipulation
- Core assumption: Equalizing total segment attention mass is an effective and sufficient intervention strategy for mitigating the "presence" aspect of format bias
- Evidence anchors: "We apply a normalization-based reweighting to the attention distribution at each generation step... The results reveal a consistent improvement in the models' ability to attend to both conflicting sources: DCR significantly increases across all format pairs and all three models... In contrast, FPR remains largely stable after the intervention"
- Break condition: If future research demonstrates that stable FPR is an artifact of the specific, simple re-weighting formula used rather than a fundamental limitation

### Mechanism 3: Data-Level Factors (Richness, Quality, Type) as Modulators of Perceived Evidentiary Value
- Claim: LLMs apply implicit heuristics that conflate data-level attributes of a format with its semantic value, treating formats with higher information richness, better structural quality, or certain layout types as more credible or "important" evidence
- Mechanism: Behavioral preferences show LLMs favor evidence sources with more entries (suggesting "more is better" heuristic), structurally clean inputs over corrupted ones (suggesting "structural integrity as reliability signal" heuristic), and exhibit intrinsic preference hierarchy among formats (suggesting "format type as prior" heuristic). These heuristics drive attention imbalance and likely originate from patterns in pre-training corpus
- Core assumption: Behavioral preferences are not task-specific shortcuts but reflect generalizable, systematic biases learned from pre-training data
- Evidence anchors: "Results consistently indicate that LLMs favor the richer variant in each pair, irrespective of format"; "LLMs consistently favor the well-formed input, confirming that structure quality serves as a reliability signal"; "The results reveal a consistent hierarchy: tables are most competitive, followed by KGs, with infoboxes least preferred"
- Break condition: If follow-up studies find these factors are not independent or their impact is heavily mediated by specific task or domain

## Foundational Learning

**Format Bias / Heterogeneous Data Bias**
- Why needed here: This is the central phenomenon the paper investigates - understanding that LLMs may treat different data formats non-neutrally, which can undermine their performance in systems relying on integrating diverse information sources
- Quick check question: Given two evidence sources with identical semantic content, one in plain text and one in a structured table, will an LLM necessarily treat them with equal weight when making a decision? According to this paper, what factors might cause it to favor one?

**Attention as a Proxy for Information Processing**
- Why needed here: The paper uses attention mass as a key mechanism to explain and intervene on format bias - understanding how attention relates to information selection and final output generation is crucial for interpreting causal claims and intervention effects
- Quick check question: The paper finds a negative correlation between attention imbalance (absolute difference in attention mass between two evidence segments) and the Dual Coverage Rate (DCR). In simple terms, what does this correlation suggest about how the model processes two conflicting pieces of evidence?

**Format-Level Interventions (Inference-Time)**
- Why needed here: The paper proposes and tests a mitigation strategy - understanding that this is an inference-time, attention-based technique distinct from data preprocessing or model fine-tuning is essential for knowing when and how to apply it and what its limitations are
- Quick check question: The attention-reweighting intervention improves the model's ability to acknowledge both sides of a conflict (DCR) but does not change its fundamental preference (FPR). Based on the paper, what does this suggest about the "direction of bias" versus the "presence of bias"?

## Architecture Onboarding

**Component map:**
- Data Preparation (ConflictBank → GPT-4o-mini format conversion) -> Inference (LLM with randomized format pairs) -> Evaluation (LLM adjudication → DCR/FPR metrics) -> Causal Analysis (Attention extraction → Gap correlation) -> Intervention (Attention re-weighting → Modified generation)

**Critical path:**
1. **Data Preparation**: Take factual claim and evidence from source corpus, convert evidence into four target formats using specified GPT prompts
2. **Inference**: Feed model question with two conflicting evidence pieces in different formats (randomized order), collect response
3. **Evaluation & Analysis**: Use LLM adjudicator to classify response (Pref-A, Pref-B, Both) and compute DCR/FPR, extract attention maps, compute segment-level attention mass, measure attention gap
4. **Intervention (Optional)**: During inference, intercept attention tensor A at each step, apply re-weighting formula to equalize segment attention, proceed with generation using modified weights

**Design tradeoffs:**
- **Format Conversion Fidelity vs. Scale**: Using GPT-4o-mini for large-scale format conversion is efficient but introduces risk of subtle factual alteration or hallucination; authors address with manual verification on 5% sample but perfect fidelity not guaranteed
- **Intervention Simplicity vs. Efficacy**: Chosen attention-reweighting intervention is simple and lightweight, making it easy to apply; however, inability to shift FPR suggests it may be too coarse - more complex, relevance-aware attention modulation might be more effective but would add computational overhead
- **Evaluation Automation vs. Ground Truth**: Relying on LLMs as adjudicators enables large-scale analysis but introduces another potential source of bias or error; authors use multiple adjudicators and validate with human checks but small error rate persists

**Failure signatures:**
- **Low Dual Coverage Rate (DCR)**: If model consistently outputs single-sided preference (low "Both" count) when given conflicting evidence, indicates strong format bias present
- **Asymmetric Format Preference Ratio (FPR)**: If FPR deviates significantly from 0.5 across multiple random orderings, signals directional bias toward one format type
- **Stable FPR under Intervention**: If attention-balancing intervention increases DCR but FPR remains unchanged, this is expected signature of "direction of bias" being resistant to this inference-time method
- **Corrupted Input Dismissal**: If models consistently reject well-formed but corrupted structures (e.g., table with replaced brackets) in favor of plain text, signals structural quality heuristic is active

**First 3 experiments:**
1. **Replicate Core Finding on New Model**: Select one LLM (e.g., smaller open-source model not in original 10) and run Stage 1 experiment on small subset of data (e.g., 100 samples for one format pair like text vs. table), compute DCR and FPR to verify existence of format bias
2. **Implement and Test the Intervention**: Implement attention-reweighting function described in paper, run inference with and without intervention on same subset from experiment 1, compare DCR and FPR metrics to reproduce differential effect (DCR up, FPR stable)
3. **Isolate a Data-Level Factor**: Using format conversion prompts, create small dataset varying single factor (e.g., information richness: create table versions with 4 and 12 rows from same text), run inference on model and measure FPR changes to directly test "more is better" heuristic

## Open Questions the Paper Calls Out

**Open Question 1**: Can inference-time interventions be developed to alter the directional preference (Format Preference Ratio) of LLMs, or is this preference permanently fixed by pretraining?
- Basis in paper: "FPR remains largely stable after the intervention... This implies that although the LLMs process both sources more evenly, the intervention has limited effect on their final output preferences"
- Why unresolved: Authors demonstrate attention re-weighting controls whether model acknowledges both sources (presence) but did not find mechanism to control which source model ultimately prefers (direction)
- What evidence would resolve it: Study identifying specific attention heads or layers correlating with directional choice, or demonstration of training-free intervention successfully shifting FPR away from model's default preference

**Open Question 2**: To what extent does the distribution of specific formats in the pre-training corpus cause the observed format bias?
- Basis in paper: "Format preferences likely originate from pretraining data imbalance" and suggests mitigation "may involve training on format-balanced corpora"
- Why unresolved: Study evaluates off-the-shelf models with unknown training compositions, establishes correlation between model behavior and bias but does not causally link specific pretraining data mixtures to resulting bias magnitude
- What evidence would resolve it: Training series of LLMs from scratch using identical data content but controlled variations in format distribution to observe resulting changes in FPR

**Open Question 3**: Is format bias driven by fundamental structural logic of a format (e.g., graph vs. grid) or by specific syntactic conventions used to represent it (e.g., MediaWiki syntax vs. JSON)?
- Basis in paper: Study relies on GPT-4o-mini to convert data into specific Wikipedia-inspired formats (MediaWiki tables, Infobox syntax), authors acknowledge transformation process but assume bias is due to "format type" rather than specific syntax used to render it
- Why unresolved: Unclear if "infobox" bias is due to key-value structure or specific visual layout and tokenization of MediaWiki infobox syntax used in prompts
- What evidence would resolve it: Ablation study presenting same logical structures using different syntactic encodings to see if preference magnitude changes based on syntax alone

## Limitations
- Format conversion artifacts despite 98.7% accuracy claims may contaminate conflicting evidence
- Inability to fully isolate format effects from tokenization or structural differences
- Correlational rather than causal evidence for some data-level factors without direct ablation studies
- Focus on English Wikipedia-style formats may limit generalizability

## Confidence
- **High confidence**: Existence of format bias (DCR/FPR metrics consistently show preference), intervention's differential effect (DCR improvement, FPR stability) based on statistical significance across models
- **Medium confidence**: Specific mechanisms (attention imbalance, data-level factors) due to correlational rather than causal evidence for some factors
- **Medium confidence**: Attention imbalance as key mechanism due to correlational evidence without direct ablation studies

## Next Checks
1. Implement attention-based intervention on new model family (e.g., Claude) to test cross-model generalizability of DCR/FPR differential effect
2. Create synthetic format pairs with identical token counts but different structural layouts to isolate format type from information richness effects
3. Conduct ablation study where attention weights are randomized post-intervention to test whether observed DCR improvements are truly attention-driven