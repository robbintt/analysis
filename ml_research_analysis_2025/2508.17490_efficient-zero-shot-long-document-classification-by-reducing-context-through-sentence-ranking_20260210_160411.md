---
ver: rpa2
title: Efficient Zero-Shot Long Document Classification by Reducing Context Through
  Sentence Ranking
arxiv_id: '2508.17490'
source_url: https://arxiv.org/abs/2508.17490
tags:
- classification
- sentences
- long
- document
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a zero-shot approach for adapting transformer
  models trained on short texts to long document classification by using TF-IDF-based
  sentence ranking to reduce input context. The method intelligently selects the most
  informative sentences from long documents, enabling compatibility with models designed
  for short inputs without architectural changes or additional training.
---

# Efficient Zero-Shot Long Document Classification by Reducing Context Through Sentence Ranking

## Quick Facts
- arXiv ID: 2508.17490
- Source URL: https://arxiv.org/abs/2508.17490
- Reference count: 13
- Primary result: Zero-shot long document classification using TF-IDF sentence ranking preserves accuracy while reducing inference time by up to 35%.

## Executive Summary
This paper presents a zero-shot approach for adapting transformer models trained on short texts to long document classification by using TF-IDF-based sentence ranking to reduce input context. The method intelligently selects the most informative sentences from long documents, enabling compatibility with models designed for short inputs without architectural changes or additional training. Experiments on the MahaNews dataset show that selecting only the top 50% of ranked sentences achieves accuracy comparable to full-document inference (80.314% baseline) while reducing inference time by up to 35%. The normalized ranked selection strategy even slightly outperforms the full-context baseline at 75% retention, demonstrating that context reduction effectively filters noise while preserving essential semantic content.

## Method Summary
The approach fine-tunes marathi-bert-v2 on Short Headline Classification (SHC) data, then applies zero-shot transfer to Long Document Classification (LDC) by preprocessing long documents. Documents are split into sentences using the Indic NLP tokenizer, and each sentence is scored using TF-IDF where term frequency is normalized by sentence length and inverse document frequency captures term uniqueness. A composite scoring strategy combines normalized TF-IDF and sentence length with tunable weights (λ₁, λ₂). Top-ranked sentences are selected using fixed (k=1-5) or percentage-based (25%, 50%, 75%) strategies, reassembled in original order, and classified using the SHC-trained model. The method enables classification of long documents without architectural modifications or additional training data.

## Key Results
- Selecting top 50% of ranked sentences achieves 80.314% accuracy, matching the full-document baseline
- Context reduction reduces inference time by up to 35% compared to full-document processing
- Normalized ranked selection strategy slightly outperforms full-context baseline at 75% retention
- At 25% retention, ranked selection achieves 78.31% accuracy, better than all other methods at the same level
- Fixed selection of top 5 sentences achieves 79.84% accuracy, demonstrating effectiveness of position-independent ranking

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: TF-IDF-based sentence ranking identifies the most informative sentences, enabling effective classification with reduced context.
- **Mechanism**: The method treats each sentence as a document in the TF-IDF framework. Term Frequency (TF) measures term significance within a sentence, while Inverse Document Frequency (IDF) captures term uniqueness across the document. Aggregating TF-IDF scores yields a sentence informativeness score.
- **Core assumption**: Sentences containing rare, distinctive terms are more informative for classification than those with common terms.
- **Evidence anchors**:
  - [abstract]: "selecting only the top 50% of ranked sentences achieves accuracy comparable to full-document inference (80.314% baseline) while reducing inference time by up to 35%."
  - [section 4.2]: "At 25% retention, [Ranked Selection] achieves 78.31% accuracy, already close to the baseline of 80.314%, and better than all other methods at the same level."
  - [corpus]: The related paper "Improving the Efficiency of Long Document Classification using Sentence Ranking Approach" (Kokate et al., 2025) validates TF-IDF ranking in supervised settings, but corpus evidence specifically for zero-shot transfer remains limited.
- **Break condition**: If documents lack distinctive vocabulary (e.g., homogeneous topic distributions across classes), TF-IDF ranking may fail to differentiate informative sentences.

### Mechanism 2
- **Claim**: Score normalization mitigates length bias in TF-IDF aggregation, improving sentence selection fairness.
- **Mechanism**: Raw TF-IDF sums favor longer sentences. Normalization divides cumulative TF-IDF by sentence length. A composite scoring strategy balances normalized TF-IDF (λ₁) and sentence length (λ₂), allowing controlled trade-offs between term rarity and contextual richness.
- **Core assumption**: Both short, high-TF-IDF sentences and longer, context-rich sentences contribute valuable information, and an optimal balance exists between them.
- **Evidence anchors**:
  - [abstract]: "The normalized ranked selection strategy even slightly outperforms the full-context baseline at 75% retention, demonstrating that context reduction effectively filters noise."
  - [section 3.2]: "This hybrid scoring mechanism allows for controlled trade-offs between conciseness and depth."
  - [corpus]: Corpus evidence on normalization techniques for sentence ranking is weak; related work focuses on architectural solutions rather than data-based preprocessing.
- **Break condition**: If λ₁ and λ₂ are poorly tuned for a specific domain, normalization may over-emphasize short sentences with rare terms or under-represent longer, contextually important sentences.

### Mechanism 3
- **Claim**: Context reduction aligns long document inputs with the short-text training distribution, enabling zero-shot transfer.
- **Mechanism**: The model is trained on Short Headline Classification (SHC) data—concise, information-dense inputs. By extracting top-ranked sentences from long documents, the reduced input mimics the density and structure of SHC data, reducing distribution shift without architectural changes.
- **Core assumption**: Classification-relevant information is concentrated in a subset of sentences, and models trained on dense short texts generalize to dense reduced contexts from long texts.
- **Evidence anchors**:
  - [abstract]: "This method enables the adaptation of models trained on short texts, such as headlines, to long-form documents by selecting the most informative sentences."
  - [section 1]: "This zero-shot setting is inherently challenging, models optimized for short, information-dense texts often fail to generalize effectively to verbose and loosely structured long documents."
  - [corpus]: Related work on long document classification (Longformer, BigBird) uses architectural modifications; corpus lacks direct comparisons to zero-shot distribution alignment approaches.
- **Break condition**: If classification depends on document-level structure (e.g., argument flow, temporal ordering) rather than localized sentence content, reducing to top sentences may破坏 coherence.

## Foundational Learning

- **Concept: TF-IDF (Term Frequency-Inverse Document Frequency)**
  - **Why needed here**: Core mechanism for scoring sentence informativeness based on term rarity and frequency.
  - **Quick check question**: Given a sentence containing "the" (common) and "quasar" (rare), which term contributes more to the TF-IDF score?

- **Concept: Zero-Shot Transfer**
  - **Why needed here**: The approach applies a model trained on SHC to