---
ver: rpa2
title: 'Explainable AI as a Double-Edged Sword in Dermatology: The Impact on Clinicians
  versus The Public'
arxiv_id: '2512.12500'
source_url: https://arxiv.org/abs/2512.12500
tags:
- explanation
- accuracy
- skin
- other
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how explainable AI (XAI) methods, particularly
  multimodal large language models (LLMs), affect diagnostic performance across different
  expertise levels in dermatology. Two large-scale experiments were conducted: one
  with 623 general public participants performing binary melanoma vs.'
---

# Explainable AI as a Double-Edged Sword in Dermatology: The Impact on Clinicians versus The Public

## Quick Facts
- arXiv ID: 2512.12500
- Source URL: https://arxiv.org/abs/2512.12500
- Reference count: 0
- Primary result: LLM explanations improve lay user accuracy when AI is correct but reduce it when AI is wrong, while experienced PCPs remain resilient

## Executive Summary
This study investigates how explainable AI (XAI) methods affect diagnostic performance across different expertise levels in dermatology. Two large-scale experiments were conducted: one with 623 general public participants performing binary melanoma vs. nevus classification, and another with 153 primary care physicians (PCPs) performing open-ended differential diagnosis of skin diseases. The study found that AI assistance improved overall accuracy and reduced diagnostic disparities across skin tones. However, LLM explanations showed divergent effects: while lay users showed higher automation bias (accuracy increased when AI was correct but decreased when AI was wrong), experienced PCPs remained resilient and benefited from AI assistance regardless of AI accuracy. The study also found that presenting AI suggestions first led to worse outcomes when the AI was incorrect for both groups.

## Method Summary
The study used two deep learning models (ViT-B/32 and DenseNet-121) trained on 108,585 images from 16 datasets using Conditional Domain Adversarial Neural Networks to ensure fairness across skin tones. Two human studies were conducted: one with 623 lay participants classifying melanoma vs. nevus images, and another with 153 PCPs performing differential diagnosis. Participants were randomly assigned to one of eight conditions combining four XAI methods (basic, GradCAM, CBIR, LLM) with two decision paradigms (Human-First vs. AI-First). The LLM explanations were generated using GPT-4V with specific prompts designed to produce authoritative, human-like prose.

## Key Results
- LLM explanations amplified automation bias in lay users, boosting accuracy when AI was correct but reducing it when AI was wrong
- Experienced PCPs remained resilient, benefiting from AI assistance regardless of AI accuracy
- Presenting AI suggestions first (AI-First paradigm) led to worse outcomes when the AI was incorrect for both groups
- AI assistance improved overall accuracy and reduced diagnostic disparities across skin tones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM explanations amplify automation bias in lay users, boosting accuracy when AI is correct but harming it when AI errs.
- Mechanism: The fluent, narrative format of LLM explanations creates an illusion of understanding and plausible reasoning, which anchors non-expert perception and increases cognitive ease of acceptance (narrative persuasion). This lowers the threshold for overriding one's own judgment.
- Core assumption: Laypersons lack the structured clinical knowledge to independently validate AI-generated explanations.
- Evidence anchors:
  - [abstract] "lay users showed higher automation bias â€“ accuracy boosted when AI was correct, reduced when AI erred"
  - [Results - Performance improvement stems from AI deference & LLM-based explanations amplify such deference] "LLM explanations yielded divergent effects... accuracy boosted when AI was correct, reduced when AI erred"
  - [corpus] Related work (e.g., "Critical or Compliant? The Double-Edged Sword of Reasoning") confirms that reasoning-style explanations can foster confirmation bias and compliance.
- Break condition: Mechanism would not hold if lay users received training to critically evaluate AI explanations or if LLM outputs included explicit confidence and uncertainty metrics.

### Mechanism 2
- Claim: Domain expertise (PCPs) acts as a cognitive buffer, enabling resilience to misleading AI explanations and calibrating reliance.
- Mechanism: Experts use AI explanations to validate pre-existing hypotheses against structured clinical knowledge, rather than to form new beliefs. Their diagnostic mental models allow them to spot inconsistencies or low-quality explanations.
- Core assumption: Expertise is associated with both better baseline diagnostic performance and more critical engagement with AI.
- Evidence anchors:
  - [abstract] "experienced PCPs remained resilient, benefiting irrespective of AI accuracy"
  - [Results - PCPs are resilient to AI deference when AI is wrong] "incorrect AI predictions had minimal impact on PCPs' final decisions... PCPs relied on their own expertise"
  - [corpus] Weak direct evidence; corpus focuses on XAI preferences, not the buffering role of expertise.
- Break condition: Mechanism would weaken if PCP expertise is low in the specific domain (e.g., dermatology) or if AI confidence is extremely high and presented authoritatively.

### Mechanism 3
- Claim: The "AI-First" decision paradigm induces anchoring bias, increasing deference and reducing performance when AI is incorrect.
- Mechanism: Presenting AI suggestions before human judgment anchors the decision-maker's initial assessment, making them more likely to integrate the AI's (potentially wrong) prediction into their final decision, regardless of expertise.
- Core assumption: The order of information presentation significantly affects cognitive processing and decision weighting.
- Evidence anchors:
  - [abstract] "Presenting AI suggestions first also led to worse outcomes when the AI was incorrect for both groups"
  - [Results - Putting AI Before Human Decisions Amplifies Deference] "putting AI suggestions ahead increased the proportion of deferential participants... AI-First paradigm led to stronger anchoring bias"
  - [corpus] No direct corpus evidence on decision order in medical XAI.
- Break condition: Mechanism may not apply if users are explicitly trained to form independent judgments before reviewing AI, or if AI output is presented as one of multiple inputs rather than a primary anchor.

## Foundational Learning

### Automation Bias
- Why needed here: Explains the differential impact of XAI on laypersons vs. experts. It is the tendency to trust automated system outputs over other information, even when contradictory.
- Quick check question: Can you describe a scenario where a user might accept an AI's incorrect prediction over their own correct initial assessment?

### Clinical Reasoning & Mental Models
- Why needed here: PCPs' resilience is rooted in their ability to reason through cases using structured knowledge (e.g., differential diagnosis), which allows them to evaluate AI explanations critically.
- Quick check question: How might a PCP use the "ABCD" criteria to independently verify an LLM's explanation about a melanoma prediction?

### Human-AI Decision Paradigm (Sequence)
- Why needed here: The study demonstrates that the temporal order of human and AI input (Human-First vs. AI-First) causally affects diagnostic outcomes and deference rates.
- Quick check question: In designing a diagnostic support tool, what are the tradeoffs of showing AI suggestions before vs. after the clinician's initial diagnosis?

## Architecture Onboarding

### Component map
A: Training Data Aggregation -> B: Fairness-Constrained Model Training -> C: XAI Generation Module -> D: User Interface with Decision Paradigm -> E: Decision Capture and Logging

### Critical path
1. Data collection and fairness-aware model training
2. Integration of model with chosen XAI generation module
3. Development of the user interface with experimental condition assignment (between-subjects)
4. Deployment for human-subject studies and logging of decisions/demographics
5. Statistical analysis using mixed-effects models to isolate the effects of XAI type and decision paradigm

### Design tradeoffs
- **XAI Type**: LLMs offer high intelligibility but risk over-persuasion (especially for lay users). Visual methods (GradCAM, CBIR) are safer but require more cognitive effort to interpret.
- **Decision Paradigm**: Human-First promotes independent thinking and reduces anchoring bias but may slow workflow. AI-First can improve speed and leverage high AI accuracy but risks over-reliance.
- **Fairness vs. Performance**: Enforcing fairness constraints (e.g., via CDANN) may slightly reduce overall accuracy but is crucial for equitable diagnostic support across skin tones.

### Failure signatures
1. **Over-reliance in lay users**: A high deference rate coupled with performance drops when AI is incorrect, especially with LLM explanations.
2. **Under-reliance in experts**: If experts consistently ignore correct AI suggestions, the system fails to augment their capabilities.
3. **Bias amplification**: If AI assistance exacerbates skin-tone disparities (instead of reducing them), the fairness constraints have failed.

### First 3 experiments
1. **A/B Test XAI Types for Lay Users**: Deploy the system with LLM vs. CBIR explanations to a lay audience. Measure diagnostic accuracy and deference rate when AI is correct vs. incorrect. Hypothesis: LLM will show higher variance in performance based on AI correctness.
2. **Expert vs. Novice Deference Study**: Compare PCPs with medical students using the same task and AI-First paradigm. Measure deference rates and accuracy. Hypothesis: PCPs will show significantly lower deference, especially when AI is wrong.
3. **Decision Paradigm Impact Test**: Within a single user group (e.g., PCPs), test Human-First vs. AI-First workflows. Measure the proportion of diagnoses changed and final accuracy. Hypothesis: AI-First will lead to more diagnosis changes and lower accuracy when AI is incorrect.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of richer clinical context (e.g., patient history and demographics) impact the observed automation bias in laypersons using LLM-based XAI?
- Basis in paper: [explicit] The authors state in the Limitations section that "Participants did not have access to contextual data... which are crucial factors in real-world diagnosis."
- Why unresolved: The current study isolated image analysis to measure the specific impact of XAI, abstracting away the complexity of real clinical workflows.
- What evidence would resolve it: A replication of the study design where participants are provided with full clinical vignettes alongside images to see if performance changes.

### Open Question 2
- Question: To what extent does the linguistic tone (e.g., authoritative vs. tentative) of LLM explanations influence diagnostic deference rates among non-expert users?
- Basis in paper: [explicit] The authors note in the Limitations section that "the specific prompt... that may elevate a confident tone, introduces a confounder that is difficult to control."
- Why unresolved: The study used a single prompt style; it is unknown if the "double-edged sword" effect is driven by the semantic content or the perceived confidence of the text.
- What evidence would resolve it: A comparative study using identical cases but with LLM prompts engineered to vary in linguistic confidence or hedging.

### Open Question 3
- Question: Do the divergent effects of LLM-based XAI on laypersons versus primary care physicians replicate in other medical domains beyond dermatology?
- Basis in paper: [explicit] The Conclusion states, "Our findings should be replicated in other medical domains."
- Why unresolved: The study focused exclusively on skin conditions; it is unclear if the visual nature of dermatology or the specific expertise required makes this domain unique regarding AI susceptibility.
- What evidence would resolve it: Conducting similar human-AI collaboration experiments in other specialties (e.g., radiology or pathology) with comparable participant groups.

## Limitations
- Simulated clinical scenarios rather than real diagnostic workflows may limit ecological validity
- Potential selection bias in participant pools could affect generalizability
- Focus on binary classification tasks may not generalize to more complex diagnostic reasoning

## Confidence
High: LLM explanations amplify automation bias in lay users while experienced clinicians remain resilient
Medium: AI-First paradigm increases anchoring bias and reduces performance when AI is incorrect
Medium: AI assistance improves overall accuracy and reduces skin tone disparities

## Next Checks
1. Replicate the study with real clinical workflow integration to assess ecological validity
2. Test the findings with alternative LLM architectures and prompt designs to isolate the mechanism
3. Extend to multi-class diagnosis tasks to evaluate generalizability beyond binary classification