---
ver: rpa2
title: 'Representing Prompting Patterns with PDL: Compliance Agent Case Study'
arxiv_id: '2507.06396'
source_url: https://arxiv.org/abs/2507.06396
tags:
- agent
- prompt
- language
- tool
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Prompt Declaration Language (PDL), a novel
  declarative programming language for representing and customizing LLM prompting
  patterns. Unlike existing frameworks that hide prompts in APIs or provide inflexible
  canned patterns, PDL brings prompts to the forefront while abstracting away plumbing
  for composing LLM calls with rule-based code and external tools.
---

# Representing Prompting Patterns with PDL: Compliance Agent Case Study

## Quick Facts
- arXiv ID: 2507.06396
- Source URL: https://arxiv.org/abs/2507.06396
- Reference count: 4
- The paper introduces PDL (Prompt Declaration Language), a declarative language for representing and customizing LLM prompting patterns, achieving up to 4x better results with compact models compared to traditional agent architectures.

## Executive Summary
The paper introduces Prompt Declaration Language (PDL), a novel declarative programming language for representing and customizing LLM prompting patterns. Unlike existing frameworks that hide prompts in APIs or provide inflexible canned patterns, PDL brings prompts to the forefront while abstracting away plumbing for composing LLM calls with rule-based code and external tools. Through a real-world case study of a compliance agent, PDL demonstrates significant performance improvements - up to 4x better results with compact models compared to traditional agent architectures. The key innovation is PDL's ability to enable fine-grained customization of prompting patterns, which proved essential for optimizing AI agent performance, particularly with smaller language models where standard ReAct patterns often fail.

## Method Summary
The study compares CrewAI ReAct baseline against PDL-based architecture for IT compliance tasks using IT-Bench benchmark (~200 evaluation tests). PDL version splits "Think" into Think1 (natural language Thought) and Think2 (data ActionSpec) to reduce JSON corruption in smaller models. Uses LiteLLM backend, JSON Schema types with constrained decoding, custom Response Parser for ActionSpec handling.

## Key Results
- PDL-based compliance agent achieved up to 4x performance improvement on granite3.2-8b compared to baseline
- Tool call failures decreased significantly (cases with no tool called dropped from 53.5% to 35.4%)
- Two-stage Think architecture specifically addressed JSON syntax errors in smaller models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Declarative, transparent prompt representation enables precise control over LLM interactions.
- **Mechanism:** PDL uses a YAML-based declarative format to keep prompts visible and foregrounded. By defining prompts, tools, and control flow within this structure, it eliminates the hiding of prompts behind imperative code or restrictive APIs. This visibility allows for granular manual tuning and provides a structured representation amenable to automated optimization.
- **Core assumption:** Making the prompt structure explicit and manipulable is more effective for complex tasks than burying it in general-purpose code.
- **Evidence anchors:**
  - [abstract] "PDL...brings prompts to the forefront, enabling manual and automatic prompt tuning while capturing the composition of LLM calls..."
  - [section 1] "...most of these approaches bury prompts in imperative code or behind APIs...However, in practice, prompting patterns need to be customized..."
  - [corpus] The related work section (4) contrasts PDL with embedded DSLs like LMQL and frameworks like CrewAI, emphasizing PDL's unified, declarative YAML representation.
- **Break condition:** If a task is so simple that a single, non-customizable canned prompt is sufficient, the overhead of a new language may not be justified.

### Mechanism 2
- **Claim:** A two-stage "Think" process separates natural language reasoning from structured action specification, improving reliability for smaller models.
- **Mechanism:** The PDL-based CISO Agent architecture splits the traditional ReAct "Think" step into two stages: `Think1` for natural language reasoning ("Thought") and `Think2` for generating a structured `ActionSpec` (JSON). This separation prevents smaller models from failing when asked to produce both natural language and strict JSON simultaneously.
- **Core assumption:** Smaller LLMs struggle with multi-modal output (text + JSON) in a single generation step and perform better when tasks are isolated.
- **Evidence anchors:**
  - [section 3] "A typical failure example is that outputting Thought in natural language and ActionSpec as a JSON string at the same time causes syntax errors..."
  - [section 3] "...this new architecture splits Think into two stages, Think1 (natural language step) which outputs Thought, and Think2 (data step) which outputs ActionSpec."
  - [corpus] Weak or missing direct corpus support for this specific mechanism; neighboring papers focus on workflow compliance but not this two-stage cognitive split.
- **Break condition:** Highly capable models may handle the combined task well; splitting it could introduce unnecessary latency for them.

### Mechanism 3
- **Claim:** Constrained decoding and typed outputs, integrated with a custom parser, minimize tool-call failures.
- **Mechanism:** PDL integrates JSON Schema types for LLM outputs and can automatically configure constrained decoding on supporting platforms. This forces the model to generate output in the correct shape. A custom response parser can then handle residual model-specific quirks (e.g., correcting key names), creating a robust pipeline from generation to tool execution.
- **Core assumption:** Enforcing structural correctness at the generation and parsing stages is more reliable than relying on the model's unguided ability to produce perfect output.
- **Evidence anchors:**
  - [section 2] "PDL types are seamlessly integrated with constrained decoding... to ensure the shape of the output."
  - [section 3] "Our PDL-based agent addresses these issues...through a custom Response Parser..."
  - [section 4, Figure 4] Performance gains in PDL are shown to be primarily from reduced tool call failures (e.g., "cases where no tool was called decreased from 53.5% to 35.4%").
  - [corpus] Weak corpus support; related work mentions constrained decoding generally but not the specific synergy with a custom parser as presented.
- **Break condition:** If the target LLM platform does not support constrained decoding or if the tool interface is too complex for a JSON Schema, the benefits will be reduced.

## Foundational Learning

- **Concept:** Declarative vs. Imperative Programming
  - **Why needed here:** PDL's core design is declarative (specifying *what* the workflow achieves), contrasting with imperative frameworks where logic and prompts are mixed in code.
  - **Quick check question:** Why might representing a multi-step LLM workflow in a declarative YAML file make it easier to optimize than a Python script?

- **Concept:** ReAct Pattern (Reason + Act)
  - **Why needed here:** The case study builds directly upon the standard ReAct agent pattern and presents a modified version to solve its failure modes.
  - **Quick check question:** In the classic ReAct loop, what is the primary responsibility of the 'Think' step, and what does it output?

- **Concept:** Constrained Decoding / Grammar-Based Generation
  - **Why needed here:** PDL uses JSON Schema to define expected output types and leverages constrained decoding to enforce them, a key mechanism for reliability.
  - **Quick check question:** What problem does constrained decoding solve when an LLM needs to output structured data like JSON?

## Architecture Onboarding

- **Component map:** PDL Program (YAML) -> PDL Interpreter -> LiteLLM Backend -> Tool Definitions
- **Critical path:**
  1.  Define the agent's goal and available tools.
  2.  Author the PDL YAML file, structuring prompts and the two-stage `Think` process.
  3.  Specify the output `spec` (JSON Schema) for the action step (`Think2`) to enable constrained decoding.
  4.  (Optional but recommended) Implement a custom `Response Parser` to handle predictable output malformations.
  5.  Run the program via the PDL interpreter, which orchestrates the full loop.

- **Design tradeoffs:**
  - **Control vs. Complexity:** PDL offers granular control but requires designing the prompting pattern from scratch, which is more effort than using a pre-built framework.
  - **Performance vs. Latency:** The two-stage `Think` process improves success rates but introduces an extra LLM call, increasing latency and cost.
  - **Abstraction vs. Flexibility:** PDL's YAML format is a cleaner representation but may be less flexible than embedding prompts directly in a full programming language like Python.

- **Failure signatures:**
  - **High Tool Call Failure Rate:** The agent outputs text instead of calling a tool, or produces malformed JSON.
  - **Incorrect Tool Arguments:** The tool is called with parameters that don't match its signature, leading to execution errors.
  - **Reasoning Loops:** The agent gets stuck in the `Think` step without progressing to `Act` or `Observe.

- **First 3 experiments:**
  1.  **Baseline Comparison:** Implement a simple tool-using agent with a standard framework (e.g., CrewAI) and a compact model. Measure the baseline tool call success rate.
  2.  **PDL Migration:** Replicate the agent's logic in a PDL YAML file, keeping prompts and tools identical. Compare its success rate to the baseline.
  3.  **Pattern Ablation:** Modify the PDL program to use a single-stage `Think` step (producing thought and JSON together). Measure the performance drop to isolate the contribution of the two-stage mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can LLMs effectively generate PDL code to autonomously formulate and execute multi-step plans of action?
- **Basis in paper:** [Explicit] The conclusion states the authors plan to "explore PDL as a target of LLM generation... for LLMs to generate plans of action."
- **Why unresolved:** The current work focuses on human developers using PDL to manually optimize agents; LLM-driven generation of PDL remains a proposal.
- **What evidence would resolve it:** Successful execution rates of plans where an LLM generates the PDL syntax dynamically versus human-written baselines.

### Open Question 2
- **Question:** Do the performance gains from PDL's customized prompting patterns generalize to domains outside of IT compliance?
- **Basis in paper:** [Inferred] The evaluation is restricted to a single case study: the CISO Compliance Agent.
- **Why unresolved:** It is unclear if the "Think1/Think2" architectural optimization transfers to tasks requiring different reasoning modalities (e.g., creative writing or mathematical logic).
- **What evidence would resolve it:** Benchmarks across diverse agent tasks (e.g., coding, web navigation) comparing PDL to standard frameworks.

### Open Question 3
- **Question:** Is the observed 4x performance increase attributable to the PDL representation itself or the specific "Think1/Think2" architectural logic it encapsulates?
- **Basis in paper:** [Inferred] The study compares a standard ReAct pattern against a highly customized PDL pattern, confounding the benefits of the language tool with the benefits of the new logic.
- **Why unresolved:** The paper does not isolate whether PDL provides performance benefits beyond enabling the manual implementation of the specific split-step architecture.
- **What evidence would resolve it:** An ablation study implementing the same split-step architecture in a standard framework (e.g., raw Python) and comparing results to the PDL version.

## Limitations

- The specific PDL implementation and IT-Bench dataset are not available, preventing independent verification of the 4x performance claim
- Evaluation is limited to a single domain (IT compliance) using a proprietary benchmark with undisclosed criteria
- The study focuses exclusively on smaller models, leaving uncertainty about whether the two-stage approach introduces unnecessary overhead for larger models

## Confidence

**High Confidence** in the foundational claim that declarative prompt representation enables finer-grained control compared to imperative frameworks. This is supported by well-established programming language principles and the paper's clear contrast with existing approaches.

**Medium Confidence** in the two-stage Think mechanism's effectiveness for smaller models. The reasoning is logical and addresses documented failure modes of ReAct patterns, but lacks extensive ablation studies or comparisons across model families to establish generalizability.

**Medium Confidence** in the constrained decoding + custom parser combination as the primary driver of improved tool call reliability. The mechanism is technically sound, but the paper doesn't isolate the individual contributions of each component or demonstrate robustness across different LLM providers and model architectures.

## Next Checks

1. **Implement the two-stage Think ablation**: Create controlled experiments comparing single-stage vs. two-stage Think patterns across multiple model sizes (3B, 8B, 70B) to quantify the performance tradeoff and identify the threshold where the split becomes beneficial versus detrimental.

2. **Open-source the IT-Bench evaluation suite**: Release or recreate the compliance task benchmark with clear evaluation criteria to enable community replication and comparison against other agent frameworks beyond just CrewAI.

3. **Measure end-to-end latency and cost**: Document the actual latency and token usage differences between PDL's two-stage approach and traditional single-step patterns, including the impact of constrained decoding on API costs and execution time.