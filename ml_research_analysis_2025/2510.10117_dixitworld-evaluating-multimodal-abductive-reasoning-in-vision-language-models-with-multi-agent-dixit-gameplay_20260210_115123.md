---
ver: rpa2
title: 'DixitWorld: Evaluating Multimodal Abductive Reasoning in Vision-Language Models
  with Multi-Agent Dixit Gameplay'
arxiv_id: '2510.10117'
source_url: https://arxiv.org/abs/2510.10117
tags:
- reasoning
- storyteller
- listener
- abductive
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating multimodal abductive
  reasoning in vision-language models (VLMs), which involves generating and selecting
  explanatory hypotheses from partial observations. To tackle this, the authors introduce
  DixitWorld, a comprehensive evaluation suite inspired by the game Dixit.
---

# DixitWorld: Evaluating Multimodal Abductive Reasoning in Vision-Language Models with Multi-Agent Dixit Gameplay

## Quick Facts
- arXiv ID: 2510.10117
- Source URL: https://arxiv.org/abs/2510.10117
- Reference count: 16
- Primary result: Smaller VLMs excel at creative hypothesis generation but struggle with selection, while larger models perform better overall, especially as listeners.

## Executive Summary
This paper introduces DixitWorld, a novel evaluation suite for multimodal abductive reasoning in vision-language models (VLMs). Inspired by the game Dixit, the suite includes DixitArena, a dynamic multi-agent environment where models alternate between generating cryptic clues and selecting target images, and DixitBench, a static benchmark isolating the listener's task. Experiments reveal a key trade-off: smaller open-source models often excel as creative storytellers but struggle as listeners, while larger proprietary models perform better overall, especially as listeners. Performance on DixitBench strongly correlates with listener results in DixitArena, validating it as a reliable proxy. The findings highlight that current VLMs are proficient at hypothesis selection but lack the controlled creativity needed for hypothesis generation, underscoring the need for architectures that model ambiguity and communicative intent.

## Method Summary
DixitWorld evaluates multimodal abductive reasoning by decomposing it into two roles: storyteller (generating cryptic clues from images) and listener (selecting target images from decoys). The dynamic DixitArena environment uses a scoring mechanism that rewards partial correctness, incentivizing optimal balance of ambiguity and clarity. The static DixitBench benchmark isolates the listener's task as a multiple-choice QA problem with controlled distractors. The study compares various VLMs, including GPT-4o, GPT-4o-mini, and Gemma3-12B, across both environments to assess their abductive reasoning capabilities.

## Key Results
- Smaller open-source models like Gemma3-12B excel at creative storytelling (20.24% storyteller score) but perform poorly as listeners (49.44% accuracy).
- Larger proprietary models like GPT-4o show superior overall performance, particularly as listeners (53.89% accuracy) but struggle with calibrated ambiguity in storytelling (19.05% score).
- Performance on DixitBench strongly correlates with listener results in DixitArena, validating it as a reliable proxy for hypothesis selection.
- Current VLMs demonstrate proficiency in hypothesis selection but lack controlled creativity for hypothesis generation, highlighting the need for architectures that model ambiguity and communicative intent.

## Why This Works (Mechanism)

### Mechanism 1: Role-Decomposed Evaluation through Game Mechanics
- **Claim:** Decomposing multimodal abductive reasoning into two distinct, interacting roles (storyteller and listener) isolates hypothesis generation abilities from hypothesis selection abilities, revealing capability asymmetries invisible to single-task benchmarks.
- **Mechanism:** The Dixit game structure forces a "Storyteller" to generate a cryptic clue from an image (hypothesis generation) that must be ambiguous enough to not be obvious but clear enough to be solvable, while "Listeners" must select the target image from decoys (hypothesis selection). The scoring mechanism explicitly rewards partial correctness (some, but not all, listeners correct), directly incentivizing the optimal balance of ambiguity and clarity. This interaction exposes whether a model can perform *controlled* generation aligned with communicative intent.
- **Core assumption:** The performance on this game proxy generalizes to other forms of multimodal abductive reasoning where controlled communication is required.
- **Evidence anchors:** [abstract] "DixitArena... evaluates both hypothesis generation... and hypothesis selection... under imperfect information." [Page 2, Col 1] "Its core component, DixitArena, provides a near-perfect operationalization of multimodal abductive reasoning by naturally decomposing the process into two complementary roles."

### Mechanism 2: Scale-Dependent Asymmetry in Generative vs. Discriminative Tasks
- **Claim:** Smaller, open-source VLMs can exhibit high creative generation (storytelling) but low discriminative accuracy (listening), while larger, proprietary models show the inverse pattern, revealing that generative creativity and discriminative understanding scale differently and may involve distinct underlying capabilities.
- **Mechanism:** The experiments show that models like Gemma3-12B are highly creative storytellers but poor listeners. In contrast, large models like GPT-4o are excellent listeners but struggle to calibrate the ambiguity of their clues as storytellers. This suggests that discriminative ability (selecting the best explanation from options) benefits more from scale and broad pretraining, while controlled, intent-aware generation (crafting an explanation with constraints) requires a different capability not automatically conferred by scale alone.
- **Core assumption:** The temperature setting (0.7) and prompt design are sufficiently comparable across different model architectures to isolate the effect of scale.
- **Evidence anchors:** [abstract] "smaller open-source models often excel as creative storytellers... whereas larger proprietary models demonstrate superior overall performance, particularly as listeners." [Page 3, Table 1] Shows Gemma3-12B has a higher Storyteller Score (20.24%) than GPT-4o (19.05%), but a much lower Listener Accuracy (49.44% vs. 53.89%).

### Mechanism 3: Validated Static Proxy for Dynamic Evaluation
- **Claim:** A carefully constructed static benchmark (DixitBench), which isolates the listener's task as a multiple-choice QA problem with controlled distractors, serves as a reliable and efficient proxy for evaluating the hypothesis selection capability demonstrated in the dynamic multi-agent environment (DixitArena).
- **Mechanism:** The authors create DixitBench with 168 questions where the difficulty is controlled by the semantic similarity of distractor captions. The strong correlation between a model's accuracy on DixitBench and its Listener Accuracy in DixitArena validates that the static task captures the core perceptual and reasoning challenge of the dynamic game's selection phase, allowing for cheaper and faster evaluation.
- **Core assumption:** The distractor generation pipeline using semantic embeddings (`all-MiniLM-L6-v2`) and a mid-level abstraction captioning prompt creates a difficulty gradient that aligns with the abductive challenge in the game.
- **Evidence anchors:** [abstract] "Performance on DixitBench strongly correlates with listener results in DixitArena, validating it as a reliable proxy for hypothesis selection." [Page 2, Col 2] "Performance on DixitBench strongly correlates with listener results in DixitArena, validating it as a reliable and lightweight proxy for evaluating hypothesis selection."

## Foundational Learning

- **Concept: Abductive Reasoning**
  - **Why needed here:** This is the core cognitive process being evaluated. It's distinct from deduction (deriving consequences from rules) and induction (generalizing from examples). Abduction is "inference to the best explanation," a fundamental mode of reasoning for hypothesis generation under uncertainty.
  - **Quick check question:** Given the observation "The grass is wet," which of the following is an *abductive* inference? A) If it rains, the grass gets wet. (Deductive/Rule) B) The grass has been wet three times this week after rain. (Inductive/Generalization) C) It probably rained recently. (Abductive/Best Explanation)

- **Concept: Multi-Agent Systems and Game Theory**
  - **Why needed here:** DixitWorld is a multi-agent environment. Understanding concepts like roles (Storyteller/Listener), imperfect information, and how an agent's score depends on the actions of others is essential to interpreting the results and the designed scoring mechanism.
  - **Quick check question:** In a multi-agent task, what is a key difference between a static, single-agent benchmark and a dynamic, multi-agent one? (Hint: Think about the source of the challenge).

- **Concept: Vision-Language Models (VLMs)**
  - **Why needed here:** The subjects of the study. A basic understanding of how these models process both images and text to perform joint reasoning is necessary to appreciate the multimodal nature of the task and the specific challenges in integrating perception with generation.
  - **Quick check question:** What is the primary input modality that a VLM processes that a standard Large Language Model (LLM) does not?

## Architecture Onboarding

- **Component map:**
  - DixitArena (Dynamic Environment) -> Role alternation (Storyteller/Listener) -> Scoring mechanism (partial correctness)
  - DixitBench (Static Benchmark) -> Multiple-choice QA -> Controlled distractors (semantic similarity)
  - Evaluation Metrics: Storyteller Score, Listener Accuracy, Overall Score, DixitBench Accuracy

- **Critical path:**
  1. **DixitBench Construction:** The reliability of the proxy depends on the pipeline of generating mid-level abstract captions with a VLM and using `sentence-transformers` to create semantically-controlled distractors.
  2. **DixitArena Gameplay:** The game loop implementation, including the hand-swap phase to ensure fairness, is the critical dynamic element.
  3. **Interpretation:** The core finding is the *asymmetry* analysis, where you compare a model's rank on the Storyteller vs. Listener tasks, not just its overall score.

- **Design tradeoffs:**
  - **DixitArena vs. DixitBench:** DixitArena is a richer, more realistic evaluation of the full abductive loop but is computationally expensive and requires a multi-agent setup. DixitBench is a lightweight, controllable proxy but only evaluates the hypothesis *selection* half of the problem.
  - **Direct Selection vs. Entailment Scoring:** The paper shows direct selection is more reliable. Entailment scoring (giving a 0-100 score per image) introduces calibration noise and reduces separability.

- **Failure signatures:**
  - **"All-Correct" (Too Obvious):** Storyteller's clue was too literal, eliminating ambiguity. (e.g., A detailed caption). Indicator: Storyteller gets 0 points because all listeners guessed correctly.
  - **"All-Wrong" (Too Vague/Misleading):** Storyteller's clue was too abstract, unrelated, or semantically misaligned with the target. Indicator: Storyteller gets 0 points because no listener guessed correctly.
  - **High Creativity / Low Clarity:** A model generates imaginative but confusing clues, leading to a high "Creativity" score but a low "Clarity" score in human evaluation and low listener accuracy.

- **First 3 experiments:**
  1. **Establish Baseline on DixitBench:** Run your target VLM on DixitBench (Easy, Hard, Total) using the "Direct Selection" strategy. Compare accuracy to the paper's baselines. This validates the discriminative capability of your model.
  2. **Analyze Storyteller Failure Modes:** Use the prompts from Appendix C to have your model act as a Storyteller on a set of images from the paper's dataset. Classify the outputs into "Too Literal," "Too Vague," or "Balanced" and analyze the correlation with listener success rates.
  3. **Probe Listener Reasoning:** Present your model with a successful, balanced clue from the paper (e.g., "A delicate hope, reaching for something unseen...") and the corresponding image set. Ask it to generate a Chain-of-Thought (CoT) explanation for its selection to inspect its abductive reasoning process.

## Open Questions the Paper Calls Out
1. **How can VLM architectures be modified to explicitly model communicative intent and ambiguity to bridge the storyteller-listener performance gap?**
   - Basis in paper: [explicit] The Conclusion states that "advancing the field will therefore require a shift towards architectures that explicitly reason about ambiguity and model communicative intent."
   - Why unresolved: Current models lack "pragmatic control," failing to balance creativity (hypothesis generation) with discriminative understanding (hypothesis selection) in dynamic settings.
   - What evidence would resolve it: A new model architecture or fine-tuning regime that maintains high listener accuracy while significantly improving storyteller "partial correct" scores (currently ~15-30%) in DixitArena.

2. **Do hybrid or adaptive evaluation strategies capture pragmatic reasoning under uncertainty more effectively than direct selection or entailment scoring?**
   - Basis in paper: [explicit] The Limitations section suggests that "future work should explore hybrid or adaptive evaluation strategies that better capture pragmatic reasoning under uncertainty."
   - Why unresolved: The study found a significant performance gap between "direct selection" and "entailment scoring," indicating that current evaluation protocols are unstable or insufficient for measuring absolute plausibility.
   - What evidence would resolve it: Development of a new evaluation protocol that yields higher correlation with human judgments of ambiguity and intent than the current direct selection method.

3. **Does the observed role asymmetry (high listener/low storyteller performance) persist in temporal or causally complex abductive reasoning tasks?**
   - Basis in paper: [explicit] The Limitations section notes that "Real-world abductive reasoning often involves temporal, causal, or interactive elements that our static and turn-based setup does not fully capture."
   - Why unresolved: It is unclear if the creativity-discrimination trade-off is an artifact of the static image-based game format or a fundamental limitation of current VLM reasoning capabilities.
   - What evidence would resolve it: Extending the DixitWorld framework to video or sequential inputs to test if the storyteller-listener gap widens or narrows with temporal complexity.

## Limitations
- The study relies on proprietary models (GPT-4o, GPT-4o-mini) whose performance may not be fully reproducible due to API variability and lack of access to training details.
- The open-source model performance depends heavily on temperature settings (0.7) and prompt engineering, which may not generalize across different implementations or use cases.
- The game-specific constraints (cryptic clues, partial correctness scoring) may not fully represent broader abductive reasoning challenges outside the Dixit domain.

## Confidence
- **High:** The role-decomposition mechanism is well-supported by the game design and experimental results showing clear asymmetry between storyteller and listener performance.
- **Medium:** The scale-dependent asymmetry claim is supported by the presented data but requires further validation across a wider range of models and architectures.
- **Medium:** The static proxy validation is demonstrated through correlation but would benefit from testing against more diverse dynamic reasoning tasks.

## Next Checks
1. **Cross-Domain Generalization Test:** Evaluate the same models on a different abductive reasoning task (e.g., scientific explanation generation or medical diagnosis) to verify if the storyteller-listener asymmetry persists outside the Dixit framework.

2. **Controlled Scaling Experiment:** Systematically test models across different parameter scales within the same architecture family (e.g., Llama-7B, Llama-13B, Llama-70B) to isolate the effect of scale from architectural differences.

3. **Prompt Engineering Robustness:** Vary temperature settings and prompt formulations systematically to determine how sensitive the storyteller-listener performance gap is to implementation details, ensuring the observed asymmetry isn't an artifact of specific hyperparameters.