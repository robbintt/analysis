---
ver: rpa2
title: 'rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy
  Injection'
arxiv_id: '2512.08300'
source_url: https://arxiv.org/abs/2512.08300
tags:
- planner
- reasoning
- qwen2
- strategies
- strategy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enhancing reasoning capabilities
  in large language models (LLMs), particularly small ones, by introducing a reinforced
  strategy injection mechanism (rSIM). The core idea is to employ a small planner
  LLM to guide the reasoning process of another LLM by adaptively injecting predefined
  reasoning strategies (e.g., self-reflection, decomposition) at each reasoning step.
---

# rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection

## Quick Facts
- arXiv ID: 2512.08300
- Source URL: https://arxiv.org/abs/2512.08300
- Reference count: 40
- Key outcome: Small models (Qwen2.5-0.5B) achieve reasoning performance on par with much larger models (Qwen2.5-14B) on MATH dataset using reinforced strategy injection.

## Executive Summary
This paper addresses the challenge of enhancing reasoning capabilities in small language models (LLMs) by introducing a reinforced strategy injection mechanism (rSIM). The core innovation is using a small planner LLM to guide another LLM's reasoning process by adaptively injecting predefined reasoning strategies at each step. The planner and reasoner are jointly trained as two agents within a multi-agent reinforcement learning framework using a leader-follower algorithm. Results show that rSIM enables small models to achieve reasoning performance comparable to much larger models on benchmarks like MATH, and the trained planner can be used as a plug-in to improve reasoning in other LLMs without further training.

## Method Summary
rSIM employs a small planner LLM to guide a reasoner LLM's reasoning process by adaptively injecting predefined reasoning strategies at each step. The planner (leader) selects from 9 human-defined strategies and injects the corresponding strategy prompt into the reasoner's context. Both models are jointly trained as two agents using multi-agent reinforcement learning with a leader-follower framework. The training uses a two-stage optimization scheme with different emphasis on planner versus reasoner updates. The approach enables small models to achieve reasoning performance comparable to much larger models, and the trained planner can be transferred as a plug-in to other LLMs without further training.

## Key Results
- Qwen2.5-0.5B with rSIM achieves 48.8% accuracy on MATH, matching Qwen2.5-14B with pure CoT (48.6%).
- Planner can be used as a frozen plugin to improve reasoning in other LLMs (e.g., 14.5% → 21.7% on MATH when applied to Llama-3.1-8B).
- rSIM-trained models show improved cross-task generalization, with Qwen2.5-0.5B+rSIM improving from 34.5% to 46.1% on GSM8K.

## Why This Works (Mechanism)

### Mechanism 1: Externalized Strategy Injection via Planner Agent
Decoupling planning from execution allows models lacking inherent reasoning strategies to acquire them through external guidance. A small planner LLM selects from 9 human-defined strategies and injects the corresponding strategy prompt into the reasoner's context, bypassing the need for the reasoner to spontaneously discover these strategies through RL exploration alone.

### Mechanism 2: Leader-Follower Multi-Agent Reinforcement Learning
Joint optimization of planner and reasoner under a leader-follower framework enables coordinated policy improvement with stable credit assignment. The planner takes action conditioned on question and prior reasoning steps, while the reasoner generates tokens conditioned on the injected strategy. Separate reward functions provide targeted learning signals for each agent.

### Mechanism 3: Two-Stage Training with Asymmetric Optimization Emphasis
Staged optimization—prioritizing planner learning first, then reasoner adaptation—prevents destabilizing gradient conflicts during early training. Stage 1 uses higher weight for planner policy updates, allowing the planner to learn useful strategy selection before the reasoner adapts.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO) and GRPO**
  - Why needed here: rSIM builds on GRPO's rule-based reward structure and policy ratio clipping. Understanding advantage estimation and KL penalties is prerequisite to reading the objective.
  - Quick check question: Can you explain why GRPO uses group-relative advantages instead of a learned value function?

- **Concept: Multi-Agent Reinforcement Learning (MARL) with Leader-Follower Structure**
  - Why needed here: The core contribution is modeling planner-reasoner interaction as a two-agent cooperative game. Without MARL foundations, the credit assignment logic will be unclear.
  - Quick check question: In a leader-follower game, how does the follower's policy depend on the leader's action, and why does this create training challenges?

- **Concept: Chain-of-Thought (CoT) Reasoning and Strategy Decomposition**
  - Why needed here: The planner operates over CoT steps. Knowing what constitutes a reasoning step and how strategies like decomposition manifest in CoT is essential.
  - Quick check question: If a model generates "Let me break this down..." followed by sub-problems, which strategy is being invoked?

## Architecture Onboarding

- **Component map:** Question → Planner (selects strategy) → Strategy prompt injection → Reasoner (generates reasoning steps) → Reward computation → Policy update

- **Critical path:**
  1. Pre-train reasoner on 1,000 incorrectly-solved but correctly-formatted CoT samples to enforce step-wise generation.
  2. Initialize planner with same base LLM + random action head.
  3. Run two-stage training: λ=0.7 for planner-focused updates, then λ=0.3 for reasoner-focused updates.
  4. Evaluate in plugin mode: freeze planner, use with any target LLM without further training.

- **Design tradeoffs:**
  - Planner size: 0.5B planner is cheaper but learns slower; 7B planner converges faster and achieves higher final reward.
  - Fixed vs. learnable strategy space: Current design uses fixed human-defined strategies; cannot discover novel strategies during training.
  - Reward design: Rterminal encourages proper termination but may bias toward early stopping; Rpenalty discourages strategy repetition but may suppress legitimately frequent strategies.

- **Failure signatures:**
  - Training collapse on small models: Pure GRPO on Qwen2.5-0.5B shows reward collapse to 0.
  - Imbalanced strategy usage: Planner over-uses self-reflection across datasets while under-using validation and sub-planning.
  - Cross-model transfer degradation: Planner trained on Qwen shows reduced gains when transferred to Llama.

- **First 3 experiments:**
  1. Train Qwen2.5-0.5B with GRPO alone on MATH to confirm reward collapse and verify the failure mode rSIM addresses.
  2. Train rSIM with 0.5B vs. 7B planner on MATH, holding reasoner fixed at 0.5B, to compare convergence speed and final accuracy.
  3. Apply planner trained on MATH (Qwen2.5-0.5B reasoner) as frozen plugin to Qwen2.5-7B reasoner on GSM8K to verify generalization claims.

## Open Questions the Paper Calls Out

### Open Question 1
How can the framework be extended to allow the planner to dynamically expand or refine its own action space rather than relying on a static set of human-defined strategies? The current design depends on expert priors and cannot discover or inject strategies outside the predefined nine during training.

### Open Question 2
Can the multi-agent training objective be modified to mitigate the planner's over-reliance on specific strategies and encourage more balanced exploration? The current reward structure appears insufficient to enforce diverse strategy usage across datasets.

### Open Question 3
How sensitive is the reasoner's performance to the semantic ambiguity or quality of the human-crafted strategy prompts? The system relies entirely on these fixed prompts, but different phrasings of the same strategy may yield different results.

## Limitations
- Fixed strategy space limits adaptability to novel reasoning patterns that fall outside the predefined nine strategies.
- Performance depends critically on the balance and definition of reward components, with exact definitions of Rformat and Rfollow remaining underspecified.
- Cross-model transfer performance degrades when moving between different model families, suggesting the planner's strategy selection is not universally optimal.

## Confidence
- **High**: The core claim that rSIM enables small models to match larger models' reasoning performance on MATH is well-supported by controlled experiments.
- **Medium**: The assertion that the planner can be used as a plug-in for other LLMs is demonstrated but shows family-dependent performance variance.
- **Medium**: The claim that two-stage training is necessary for stable convergence is supported by training curves but lacks ablation against alternative MARL schedules.

## Next Checks
1. Apply rSIM to a reasoning task requiring strategies not in the predefined nine to measure performance degradation compared to a baseline with a more comprehensive strategy set.
2. Train rSIM variants with individual reward components removed to track planner strategy usage patterns and reasoning accuracy, quantifying reward sensitivity.
3. Systematically evaluate the trained planner across multiple LLM families (Qwen, Llama, Mistral) on held-out reasoning tasks to quantify accuracy drop per family and establish transfer limits.