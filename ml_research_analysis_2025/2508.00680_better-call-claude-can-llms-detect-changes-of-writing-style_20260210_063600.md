---
ver: rpa2
title: 'Better Call Claude: Can LLMs Detect Changes of Writing Style?'
arxiv_id: '2508.00680'
source_url: https://arxiv.org/abs/2508.00680
tags:
- style
- changes
- llms
- https
- change
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper benchmarks the zero-shot performance of large language\
  \ models (LLMs) on sentence-level style change detection, a core task in authorship\
  \ analysis. The authors evaluated four state-of-the-art LLMs\u2014Claude 3.7 Sonnet,\
  \ GPT-4o, DeepSeek-R1, and Llama-3.1-405B-Instruct\u2014using the PAN 2024 and 2025\
  \ Multi-Author Writing Style Analysis datasets."
---

# Better Call Claude: Can LLMs Detect Changes of Writing Style?

## Quick Facts
- **arXiv ID**: 2508.00680
- **Source URL**: https://arxiv.org/abs/2508.00680
- **Reference count**: 40
- **Primary result**: Claude 3.7 Sonnet achieved macro F1 scores of 0.856, 0.818, and 0.661 on easy, medium, and hard sentence-level style change detection datasets

## Executive Summary
This paper benchmarks zero-shot large language model performance on sentence-level style change detection, a core task in authorship analysis. The authors evaluated four state-of-the-art LLMs (Claude 3.7 Sonnet, GPT-4o, DeepSeek-R1, and Llama-3.1-405B-Instruct) using carefully engineered prompts that guided models to assume approximately three authors per document and focus on stylistic rather than semantic cues. Claude 3.7 Sonnet consistently outperformed competitors across all difficulty levels, achieving macro F1 scores of 0.856, 0.818, and 0.661 on easy, medium, and hard datasets respectively. The study found that Claude's errors primarily stemmed from over-segmentation rather than difficulty with the data, and that semantic similarity correlations weakened in harder datasets, suggesting stylistic changes often occur between semantically similar sentences.

## Method Summary
The authors evaluated four LLMs using the PAN 2024 and 2025 Multi-Author Writing Style Analysis datasets. They employed a carefully engineered prompt that instructed models to assume approximately three authors per document and to focus on stylistic cues rather than semantic content. The evaluation measured macro F1 scores across three difficulty levels (easy, medium, hard) and analyzed prediction accuracy using Hamming distance. The study also explored correlations between model predictions and semantic similarity, finding that while semantic similarity correlated with predictions in easier cases, this correlation weakened in harder datasets.

## Key Results
- Claude 3.7 Sonnet achieved the highest macro F1 scores: 0.856 (easy), 0.818 (medium), and 0.661 (hard)
- Performance approached or surpassed that of a fine-tuned transformer baseline
- Claude's errors were primarily due to over-segmentation rather than difficulty with the data
- Semantic similarity correlation with predictions weakened in harder datasets, indicating stylistic changes often occur between semantically similar sentences

## Why This Works (Mechanism)
The success of LLMs in style change detection stems from their ability to capture subtle linguistic patterns and stylistic nuances across sentences. The carefully engineered prompts effectively guided models to focus on stylistic rather than semantic features, which is crucial for authorship analysis. The assumption of approximately three authors per document in the prompt helped constrain the problem space and improve consistency. Claude 3.7 Sonnet's superior performance suggests it may have better fine-grained pattern recognition capabilities or more effective handling of stylistic features compared to other models tested.

## Foundational Learning
- **Style change detection**: Identifying boundaries where writing style shifts between sentences; needed to understand the core task being evaluated
- **Macro F1-score**: A metric that calculates F1-score for each class and averages them; needed to evaluate performance across different difficulty levels
- **Hamming distance**: Measures the difference between predicted and actual author boundaries; needed to analyze prediction accuracy patterns
- **Zero-shot learning**: Model performance without task-specific fine-tuning; needed to establish baseline capabilities
- **Semantic similarity**: Measuring meaning overlap between sentences; needed to analyze correlation with style change predictions
- **Prompt engineering**: Designing effective instructions for LLMs; needed to optimize model performance on style detection tasks

## Architecture Onboarding
**Component map**: Input text → Prompt template → LLM inference → Boundary prediction → F1/Hamming evaluation → Semantic similarity analysis

**Critical path**: The most critical component is the prompt engineering, as it directly shapes how the LLM interprets and processes the style detection task. The LLM inference itself is the computational bottleneck, while the evaluation metrics provide the feedback loop for analysis.

**Design tradeoffs**: The study chose zero-shot evaluation over fine-tuning to establish baseline capabilities, sacrificing potential performance gains for generalizability. The assumption of ~3 authors per document simplified the task but may not reflect real-world scenarios. The focus on sentence-level detection rather than document-level analysis provides finer granularity but increases computational complexity.

**Failure signatures**: Over-segmentation errors indicate the model is too sensitive to minor stylistic variations. Under-segmentation suggests insufficient sensitivity to genuine style changes. Weak correlation with semantic similarity in hard datasets reveals that stylistic changes often occur independently of meaning changes.

**3 first experiments**:
1. Test the same prompt on out-of-domain texts (technical, legal, social media) to assess generalizability
2. Vary the assumed author count in the prompt (2 vs 3 vs 4 authors) to determine optimal constraints
3. Compare zero-shot performance with few-shot prompting using examples of known style boundaries

## Open Questions the Paper Calls Out
None

## Limitations
- Results are limited to English text from specific domains (news, essays, novels) and may not generalize to other languages or domain-specific writing
- The study uses a single carefully engineered prompt that assumes ~3 authors per document, which may not reflect all real-world scenarios
- Evaluation relies on macro F1-score and Hamming distance, which may not capture all relevant aspects of style change detection

## Confidence
**High confidence**: Claude 3.7 Sonnet's superior performance on the PAN datasets is well-supported by reported metrics
**Medium confidence**: Claims about performance approaching or surpassing fine-tuned baselines, and semantic similarity interpretation
**Low confidence**: Broader claims about establishing LLMs as strong baselines across all authorship analysis tasks

## Next Checks
1. Test the same models and prompts on non-English texts and domain-specific writing (legal documents, scientific papers, social media posts) to assess generalizability
2. Systematically vary prompt engineering approaches (author count assumptions, stylistic vs. semantic emphasis, temperature settings) to determine robustness
3. Apply the best-performing approach to actual authorship attribution cases with known ground truth to validate practical utility beyond controlled benchmarks