---
ver: rpa2
title: From Large-scale Audio Tagging to Real-Time Explainable Emergency Vehicle Sirens
  Detection
arxiv_id: '2506.23437'
source_url: https://arxiv.org/abs/2506.23437
tags:
- audio
- siren
- dataset
- available
- ieee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces E2PANNs, a lightweight CNN architecture derived
  from PANNs and optimized for real-time emergency vehicle (EV) siren detection. The
  approach fine-tunes a pre-trained EPANNs model using a curated subset of AudioSet
  (AudioSet-EV) and evaluates performance across multiple datasets, including cross-domain
  benchmarking and deployment on embedded hardware (Raspberry Pi 5).
---

# From Large-scale Audio Tagging to Real-Time Explainable Emergency Vehicle Sirens Detection

## Quick Facts
- arXiv ID: 2506.23437
- Source URL: https://arxiv.org/abs/2506.23437
- Authors: Stefano Giacomelli; Marco Giordano; Claudia Rinaldi; Fabio Graziosi
- Reference count: 40
- Primary result: Lightweight CNN architecture achieves SoA EV siren detection with 97% accuracy and 400ms latency on Raspberry Pi 5

## Executive Summary
This paper introduces E2PANNs, a lightweight CNN architecture optimized for real-time emergency vehicle (EV) siren detection. The approach fine-tunes a pre-trained EPANNs model using a curated subset of AudioSet (AudioSet-EV) and evaluates performance across multiple datasets, including cross-domain benchmarking and deployment on embedded hardware (Raspberry Pi 5). The model employs selective fine-tuning, adaptive frame-length strategies, and eXplainable AI techniques to improve detection accuracy and interpretability. Results demonstrate SoA performance with high computational efficiency, robust false positive suppression, and suitability for edge-based safety-critical applications.

## Method Summary
E2PANNs fine-tunes a pre-trained PANNs CNN-14 backbone on AudioSet-EV, converting it from multi-label to binary classification for EV sirens. The model uses SVD-based filter pruning to reduce parameters by 70% while maintaining performance. Training employs Adam optimizer with fixed learning rate (10^-4), weight decay (10^-6), and batch sizes of 16-32. Adaptive frame-length dynamically extends analysis windows when confidence exceeds threshold (0.6), reducing false positives. The model is deployed on Raspberry Pi 5 for real-time inference at 32kHz mono audio with 310ms minimum frame length.

## Key Results
- Achieves 97% accuracy and 97.4% F1-score on AudioSet-EV binary classification
- Reduces parameters by 70% and MACs by 36% through SVD-based pruning with <1% mAP degradation
- Maintains <400ms inference latency on Raspberry Pi 5 while suppressing false positives from 4.88% to 3.90%
- Demonstrates robust cross-dataset performance with F1-scores ranging from 86% to 98% across multiple benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Transfer Learning Specialization
Fine-tuning a pre-trained audio classifier on a curated domain-specific dataset enables robust EV siren detection. E2PANNs inherits spectro-temporal feature extractors trained on AudioSet's diverse audio classes, then adapts only higher-level representations to the binary siren task, preserving low-level acoustic pattern recognition while specializing decision boundaries.

### Mechanism 2: Structured Pruning for Embedded Deployment
Operator norm-based filter pruning reduces computational footprint with minimal accuracy degradation. SVD analysis of convolutional filter banks identifies filters contributing least to input-output transformations; pruning targets the six deepest layers (99% of parameters), removing low-contribution filters while preserving discriminative feature maps.

### Mechanism 3: Adaptive Frame-Length for False Positive Suppression
Dynamically extending analysis window duration when confidence exceeds threshold reduces spurious detections without missing true events. Real-time inference begins with minimum viable input (310ms); when output probability exceeds threshold (0.6), frame length increases progressively, providing longer temporal context to disambiguate siren-like sounds from actual sirens.

## Foundational Learning

- **Concept: Transfer Learning in Audio CNNs**
  - Why needed here: Understanding how AudioSet pre-training transfers to EV sirens requires grasping why general audio features help specialized tasks.
  - Quick check question: If you fine-tuned only the final classification layer vs. the full network, which would converge faster and which would achieve higher final accuracy?

- **Concept: SVD-Based Filter Pruning**
  - Why needed here: The paper's efficiency gains rely on understanding singular value decomposition as a measure of filter information content.
  - Quick check question: A filter with 3 significant singular values out of 50 possible—is it more or less prune-worthy than one with 40 significant values, and why?

- **Concept: Frame-Based vs. Event-Based Audio Evaluation**
  - Why needed here: The paper reports both frame-wise metrics (per-timestep) and event-based metrics (temporal segment accuracy), which measure different failure modes.
  - Quick check question: A detector that fires briefly during silence then recovers—would this hurt frame-wise F1 more or event-based error rate more?

## Architecture Onboarding

- **Component map:**
  Audio Input (32kHz mono) -> STFT Layer (fixed, non-trainable) -> Log-Mel Spectrogram -> CNN Blocks 1-6 (pruned E-PANNs backbone, fine-tuned) -> Linear Embedding (2048-dim) -> Binary Classification Head -> Sigmoid Output -> Threshold (0.5 default) -> Event State Machine

- **Critical path:**
  The log-mel filterbank layer is CNN-14 pre-trained but frozen during fine-tuning. This is the primary inheritance point—if this layer's learned frequency responses don't match siren characteristics, downstream blocks cannot compensate fully.

- **Design tradeoffs:**
  - Fixed vs. adaptive frame length: Fixed provides deterministic latency; adaptive improves false positive suppression at cost of variable compute
  - Last-layer vs. full fine-tuning: Last-layer converges in ~26 minutes with 84% accuracy; full fine-tuning takes ~57 minutes with 97% accuracy
  - Batch size 16 vs. 32: Smaller batches enable memory-constrained deployment but converge ~25% slower per early stopping epoch

- **Failure signatures:**
  - High false positives on non-EV alarms: Check if log-mel filterbank centroids have drifted from standard triangular shapes
  - Inference latency spikes on embedded: Adaptive frame-length is extending window; cap maximum duration or reduce expansion rate
  - Accuracy drops on cross-domain datasets: Transfer learning may have overfit to AudioSet-EV acoustic conditions

- **First 3 experiments:**
  1. Baseline validation: Load pre-trained E-PANNs checkpoint, run inference on AudioSet-EV test split without any fine-tuning
  2. Minimum viable fine-tuning: Train only fc1 and fc_audioset layers on AudioSet-EV for 50 epochs with η=10^-3
  3. Real-time smoke test: Deploy fine-tuned model to Raspberry Pi 5 with fixed 310ms frames, stream 10 audio clips with known annotations

## Open Questions the Paper Calls Out

### Open Question 1
How effective is sequential dataset-aware training for improving cross-domain robustness?
- Basis in paper: The authors state they are developing this strategy to process datasets in isolation and dynamically adjust their influence based on validation improvements
- Why unresolved: The strategy is under evaluation; benefits remain unquantified in integrated results
- What evidence would resolve it: Benchmarks showing improved consistency and F1 scores across heterogeneous datasets

### Open Question 2
Does fine-tuning the mel-filterbank layer enhance domain-specific adaptation?
- Basis in paper: The conclusion notes this layer is not fine-tuned, potentially limiting adaptability to domain-specific acoustics
- Why unresolved: The ablation study did not include this component; its contribution is unknown
- What evidence would resolve it: Repeated ablation with fine-tuned filterbank layers, comparing accuracy and F1

### Open Question 3
How can confidence calibration be improved to reduce false positives in safety-critical deployments?
- Basis in paper: The conclusion calls for further efforts to calibrate confidence, especially to mitigate false alarms
- Why unresolved: The current model lacks explicit calibration mechanisms; confidence thresholds remain heuristic
- What evidence would resolve it: Implementation of calibration methods (e.g., temperature scaling) with reduced false positive rates and reliability diagrams

### Open Question 4
Would multi-microphone array support improve detection reliability under occlusion or high-noise conditions?
- Basis in paper: The inference pipeline is limited to single-channel mono inputs; spatial audio advantages are unexplored
- Why unresolved: Real-world embedded systems often use arrays, but this capability is not tested
- What evidence would resolve it: Benchmarks on multi-channel datasets with DOA estimation showing accuracy gains in noisy or occluded scenarios

## Limitations

- Architectural pruning strategy lacks empirical justification for 50% threshold selection
- Adaptive frame-length mechanism introduces non-deterministic latency that could violate real-time constraints
- Cross-dataset generalization claims rely heavily on curated subsets without establishing robustness to environmental acoustic variations

## Confidence

**High Confidence:** Transfer learning effectiveness (97% accuracy on AudioSet-EV), embedded deployment latency metrics (400ms on Raspberry Pi 5), and XAI visualization validity (Guided Backpropagation highlights physically meaningful spectral regions)

**Medium Confidence:** False positive suppression through adaptive frame-length (AFPSP improvements from 4.88% to 3.90%), pruning efficiency tradeoffs (70% parameter reduction with <1% mAP drop), and cross-dataset performance consistency across multiple benchmarks

**Low Confidence:** The claim that SVD-based pruning preserves "task-relevant" filters without observing data—this contradicts established pruning literature requiring data-aware methods

## Next Checks

1. Ablation on pruning thresholds: Systematically evaluate E2PANNs pruned at 25%, 50%, 75%, and 90% filter removal to establish the true accuracy-latency Pareto frontier

2. Environmental stress testing: Deploy the model in recordings with varying SNR conditions (from -10dB to +20dB) and acoustic environments (urban canyons, tunnels, open highways) to validate cross-domain robustness claims

3. Latency determinism verification: Instrument the adaptive frame-length implementation to log actual processing times across 10,000 real-time inference cycles, confirming worst-case latency remains within safety-critical bounds