---
ver: rpa2
title: Evaluating Temporal Plasticity in Foundation Time Series Models for Incremental
  Fine-tuning
arxiv_id: '2504.14677'
source_url: https://arxiv.org/abs/2504.14677
tags:
- time
- series
- data
- foundation
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates temporal plasticity in time series foundation
  models for incremental fine-tuning. The authors address the challenge of catastrophic
  forgetting and loss of plasticity in continual learning scenarios where time series
  data exhibits distribution shifts over time.
---

# Evaluating Temporal Plasticity in Foundation Time Series Models for Incremental Fine-tuning

## Quick Facts
- arXiv ID: 2504.14677
- Source URL: https://arxiv.org/abs/2504.14677
- Authors: Jia Liu; Cheng Jinguo; Xia Fang; Zhenyuan Ma; Yuankai Wu
- Reference count: 34
- Primary result: Foundation models mitigate catastrophic forgetting and plasticity loss better than smaller models during incremental fine-tuning under distribution shifts

## Executive Summary
This paper investigates temporal plasticity in time series foundation models during incremental fine-tuning scenarios where distribution shifts occur over time. The authors propose a novel evaluation framework with three metrics (Rzero, Rfull, Rfz) to assess models' ability to adapt to new data while maintaining performance on previous tasks. Through experiments on Flight and CD-Bike datasets with varying distribution shifts, they demonstrate that foundation models like Time-MoE and Chronos exhibit superior continual learning capabilities compared to smaller models. The key finding is that foundation models effectively mitigate plasticity loss and catastrophic forgetting during incremental fine-tuning, while smaller models struggle significantly with distribution shifts.

## Method Summary
The paper evaluates temporal plasticity through incremental fine-tuning on sequentially partitioned time series datasets. Models are pre-trained foundation models (Time-MoE, Chronos) and smaller models (DLinear, PatchTST, iTransformer). Data is chronologically partitioned into 10 subsets with 6:2:2 train/val/test ratios. The framework uses Algorithm 1 for incremental fine-tuning (sequential training on each partition from previous checkpoint) and Algorithm 2 for full training (upper-bound reference on accumulated data). Three metrics assess plasticity: R_zero compares incremental to zero-shot performance, R_full compares incremental to full training, and R_fz compares full to zero-shot. Experiments use 96-step context/96-step prediction windows, AdamW optimizer with lr=1e-4, and 10 epochs per subset.

## Key Results
- Foundation models maintain R_full values below 3 across all partitions while small models spike to ~10 at distribution shift points
- R_zero values for foundation models remain 0.4-0.8 across partitions, showing consistent improvement from zero-shot baseline
- Time-MoE's MoE architecture provides implicit protection against catastrophic forgetting through selective expert activation
- Raw incremental learning does not progressively enhance domain adaptation capabilities (R_zero_p does not consistently decrease with p)

## Why This Works (Mechanism)

### Mechanism 1
Foundation models preserve plasticity better than smaller models under incremental fine-tuning with distribution shifts. Large-scale pre-training on diverse temporal distributions creates redundant representational pathways that remain activatable during sequential updates. When fine-tuning on new partitions, foundation models retain underutilized capacity from pre-training rather than overwriting critical weights. This mechanism depends on pre-training corpus containing sufficient distributional diversity to create transferable representations that do not collapse during task-specific adaptation.

### Mechanism 2
Mixture-of-Experts architectures provide implicit protection against catastrophic forgetting through selective expert activation. Time-MoE's sparse expert routing means only subsets of parameters activate per input distribution. During incremental fine-tuning, new distributions primarily route to underutilized experts rather than overwriting previously-learned expert weights, preserving earlier knowledge. This mechanism assumes expert routing remains sufficiently stable across distribution shifts that new data does not systematically activate the same experts used for previous distributions.

### Mechanism 3
Zero-shot performance serves as an upper-bound indicator of incremental learning viability. Foundation models with strong zero-shot generalization have already learned transferable temporal representations. Incremental fine-tuning then refines rather than reconstructs these representations, reducing the gradient magnitude needed for adaptation and limiting interference with existing knowledge. This mechanism assumes zero-shot capability correlates with representation quality that supports stable fine-tuning dynamics.

## Foundational Learning

- **Catastrophic Forgetting**: Why needed: The paper's core problem formulation defines forgetting as performance degradation on old distributions after fine-tuning on new data. Understanding this is prerequisite to interpreting R_full and R_zero metrics. Quick check: Can you explain why Equation 6 represents a problem for time series models that might encounter recurring long-term economic cycles?

- **Loss of Plasticity**: Why needed: Distinct from forgetting, plasticity loss describes the model's inability to learn new patterns regardless of past knowledge retention. The paper demonstrates small models suffer this specifically after distribution shifts. Quick check: What does it mean when R_full spikes at partition 4 for small models but not for foundation models, and how does this relate to plasticity?

- **Temporal Distribution Shift**: Why needed: The Flight dataset contains COVID-induced shifts at partitions 5 and 9. The paper's conclusions are conditional on shift magnitude—minor shifts show different plasticity patterns than major shifts. Quick check: How would you detect whether your deployment data has undergone a distribution shift sufficient to trigger plasticity loss?

## Architecture Onboarding

- **Component map**: Pre-trained Foundation Model -> Data Partitioner -> Incremental Fine-Tuning Loop -> Full Training Baseline -> Evaluation Metrics
- **Critical path**: Load pre-trained foundation model parameters → Partition time series chronologically → For each partition p: normalize data → fine-tune → evaluate on held-out test slice → Track MSE_raw, MSE_zero, MSE_full across all partitions → Compute R ratios to diagnose plasticity
- **Design tradeoffs**: Incremental fine-tuning is computationally cheaper but may not adapt fully to large distribution shifts (R_full gap at partition 4). Foundation models require GPU memory for large parameter counts but maintain plasticity. Small models are deployable on edge devices but lose plasticity after shifts.
- **Failure signatures**: R_full > 5 for consecutive partitions indicates plasticity loss; R_zero > 1 for new partitions suggests fine-tuning is degrading zero-shot capability; sudden metric spikes at specific partitions indicate distribution shift detected.
- **First 3 experiments**: 1) Baseline Plasticity Test: Run incremental fine-tuning on CD-Bike with Time-MoE-base, verify R_zero < 0.8 and R_full < 2 across all partitions. 2) Distribution Shift Stress Test: Run on Flight dataset, plot R_full across partitions, compare Time-MoE vs. Chronos vs. DLinear. 3) Learning Rate Sensitivity: Repeat Flight experiment with learning rates [1e-5, 1e-4, 1e-3], monitor R_full trajectory.

## Open Questions the Paper Calls Out

- What fine-tuning strategies can enable time series foundation models to fully adapt to significant distribution shifts, where raw incremental learning currently struggles?
- Can foundation models achieve progressively improving domain adaptation over time, where Rzero_p consistently decreases as p increases?
- Do temporal plasticity findings generalize to other foundation models (Moirai, TimesFM, GPT4TS, Time-LLM) beyond Time-MoE and Chronos?
- What is the relative contribution of pre-training data scale versus model architecture size to temporal plasticity?

## Limitations

- Conclusions rely on specific architectural choices and dataset characteristics that may not generalize
- Proprietary CD-Bike dataset prevents full external validation
- COVID-19 distribution shift represents a single type of temporal discontinuity
- Learning rate of 1e-4 was selected without sensitivity analysis
- Paper does not investigate whether plasticity benefits persist when incremental updates occur more frequently

## Confidence

- **High Confidence**: Empirical demonstration that foundation models maintain lower R_full values than small models during distribution shifts is robust and directly supported by experimental results
- **Medium Confidence**: Claim that MoE architectures provide implicit protection against forgetting is plausible but mechanism remains incompletely explained
- **Low Confidence**: Assertion that zero-shot performance serves as a reliable indicator of incremental learning viability requires more extensive validation

## Next Checks

1. **Learning Rate Sensitivity Analysis**: Repeat Flight experiments with learning rates spanning [1e-5, 1e-4, 1e-3], track whether R_full trajectories diverge between foundation and small models, identify threshold where plasticity benefits disappear

2. **Cross-Dataset Generalizability Test**: Implement incremental fine-tuning pipeline on open time series dataset with known distribution shifts (e.g., electricity demand data), compare R_zero, R_full, R_fz across datasets to validate foundation model advantages beyond COVID-specific shifts

3. **Partition Granularity Impact Study**: Conduct experiments varying number of partitions P from 5 to 20 while maintaining constant total data volume, measure how plasticity metrics change with partition size to determine whether foundation model benefits scale with update frequency