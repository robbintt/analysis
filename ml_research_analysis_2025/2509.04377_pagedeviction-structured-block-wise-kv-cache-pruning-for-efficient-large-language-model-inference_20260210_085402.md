---
ver: rpa2
title: 'PagedEviction: Structured Block-wise KV Cache Pruning for Efficient Large
  Language Model Inference'
arxiv_id: '2509.04377'
source_url: https://arxiv.org/abs/2509.04377
tags:
- cache
- tokens
- block
- eviction
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of high memory usage in large\
  \ language model (LLM) inference due to KV cache growth, which limits sequence length\
  \ and throughput. The proposed method, PagedEviction, introduces a block-wise KV\
  \ cache eviction strategy that aligns with vLLM\u2019s paged memory layout, evicting\
  \ entire blocks based on a token importance score derived from Key and Value tensors\
  \ without modifying attention kernels."
---

# PagedEviction: Structured Block-wise KV Cache Pruning for Efficient Large Language Model Inference

## Quick Facts
- arXiv ID: 2509.04377
- Source URL: https://arxiv.org/abs/2509.04377
- Reference count: 4
- Primary result: Structured block-wise KV cache eviction improves throughput up to 3.1× and accuracy by 15-20% over baselines under tight memory budgets

## Executive Summary
PagedEviction addresses the growing memory bottleneck in LLM inference caused by KV cache expansion during long sequence processing. The method introduces a structured block-wise eviction strategy that operates on entire blocks of KV cache aligned with vLLM's paged attention layout, rather than evicting individual tokens. By using a token importance score based on L2-norm ratios of Key and Value tensors, PagedEviction achieves efficient cache management without requiring modifications to attention kernels, maintaining compatibility with existing vLLM implementations while delivering significant performance gains.

## Method Summary
PagedEviction implements block-wise KV cache eviction by dividing the KV cache into fixed-size blocks aligned with vLLM's paged attention structure. The method calculates a token importance score using the ratio of L2-norms between Value and Key tensors, allowing it to identify and evict less important blocks when memory constraints require cache reduction. This approach avoids the fragmentation and overhead associated with per-token eviction strategies while maintaining compatibility with existing attention kernels. The eviction process is integrated into vLLM's runtime through a simple patch that adds eviction logic before attention computation, making it deployable without extensive infrastructure changes.

## Key Results
- Achieved up to 3.1× throughput improvement over full cache configuration under tight memory budgets
- Outperformed StreamingLLM and Inverse Key L2-Norm baselines by 15-20% in ROUGE/F1 scores
- Reduced latency by 10-12% while maintaining better accuracy retention on LongBench tasks

## Why This Works (Mechanism)
PagedEviction's effectiveness stems from aligning cache eviction granularity with vLLM's existing paged memory layout, eliminating the mismatch overhead between block-level memory management and token-level cache operations. The L2-norm ratio score serves as an efficient proxy for token importance without requiring attention computation, enabling quick eviction decisions. By evicting entire blocks rather than individual tokens, the method avoids memory fragmentation and maintains spatial locality, which improves both cache performance and memory management efficiency.

## Foundational Learning
- **Paged Attention**: vLLM's memory layout that organizes KV cache into blocks of contiguous memory; needed for understanding the cache structure PagedEviction operates on; quick check: verify block size matches vLLM's default configuration
- **KV Cache**: The memory structure storing intermediate attention results; critical for understanding what gets evicted; quick check: confirm cache grows linearly with sequence length
- **Token Importance Scoring**: Methods to identify which tokens can be safely evicted; essential for cache pruning decisions; quick check: validate L2-norm ratio correlates with actual attention scores
- **Block-wise vs Token-wise Eviction**: Different granularities of cache removal; impacts fragmentation and overhead; quick check: compare memory fragmentation rates between approaches
- **L2-norm Ratio**: Mathematical metric for token importance using Key and Value tensor norms; serves as attention-free proxy; quick check: verify ratio distribution across different token positions

## Architecture Onboarding

**Component Map**
PagedEviction -> vLLM Runtime -> Attention Kernel -> Memory Manager

**Critical Path**
User input → PagedEviction scoring → Block eviction decision → Attention computation → Output generation

**Design Tradeoffs**
- Block size vs. granularity: larger blocks reduce overhead but may evict useful tokens; quick check: measure accuracy degradation with different block sizes
- Memory vs. accuracy: tighter budgets improve throughput but reduce retention; quick check: plot accuracy-throughput curve across cache budgets
- Compatibility vs. optimization: kernel-free design enables easy deployment but may miss attention-based optimizations; quick check: compare against modified kernel approaches

**Failure Signatures**
- Excessive accuracy drop: indicates over-aggressive eviction or poor importance scoring
- Memory fragmentation: suggests block size mismatch with underlying memory allocator
- Latency spikes: may indicate inefficient eviction decision logic or cache misses

**First Experiments**
1. Baseline throughput and accuracy with full cache on LongBench tasks
2. Memory usage profiling during sequence generation to identify cache growth patterns
3. Block eviction timing overhead measurement to verify efficiency claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can PagedEviction be enhanced by implementing layer-wise or head-wise adaptive cache budgets rather than a uniform global budget?
- Basis in paper: The Conclusion states that "techniques such as layer-wise budget allocation... and quantized KV caching... can be built on top of our approach to further enhance performance."
- Why unresolved: The current implementation applies a fixed cache budget across all layers, whereas attention density often varies hierarchically (e.g., pyramid structures), potentially leaving memory optimization gains unrealized.
- What evidence would resolve it: An evaluation of PagedEviction using variable budget profiles (e.g., allocating more cache to lower layers) on the LongBench benchmark to measure accuracy retention.

### Open Question 2
- Question: Is the proposed L2-norm ratio ($\|V\|_2 / \|K\|_2$) the optimal proxy for token importance, or do other static representations exist that correlate better with attention scores?
- Basis in paper: Section 4.1 adopts the L2-norm ratio based on a correlation observed in prior work (Devoto et al. 2024) but does not perform an ablation study comparing this against other potential attention-free metrics.
- Why unresolved: While the metric works effectively, the paper does not verify if this specific mathematical proxy is the upper bound of performance for non-attention-based methods.
- What evidence would resolve it: A comparative analysis of various static token scoring functions (e.g., Key entropy vs. L2-norm ratio) against ground-truth attention accumulation to identify the highest-accuracy proxy.

### Open Question 3
- Question: How does PagedEviction's accuracy and throughput trade-off compare to attention-based eviction methods (like H2O) if kernel modifications were permissible?
- Basis in paper: Section 5.2 explicitly excludes attention-based baselines like H2O because they require CUDA kernel modifications incompatible with the current vLLM runtime, leaving a gap in the comparative analysis.
- Why unresolved: It is unclear if PagedEviction's structural efficiency comes at a significant accuracy cost relative to methods that utilize exact cumulative attention scores for eviction.
- What evidence would resolve it: A modified implementation of vLLM that exposes attention scores to implement H2O, allowing a direct head-to-head comparison of ROUGE/F1 scores and latency against PagedEviction.

## Limitations
- Evaluated primarily on vLLM framework, limiting generalizability to other inference systems
- May discard useful information under extremely tight cache constraints in precision-critical domains
- Relies on static importance scores rather than task-specific metrics, potentially suboptimal for specialized applications

## Confidence
- **High Confidence**: The core architectural design of block-wise eviction aligned with paged memory layout is technically sound and demonstrably reduces fragmentation and overhead compared to per-token methods.
- **Medium Confidence**: Throughput and latency improvements are robust across tested models and tasks, though real-world deployment scenarios may reveal additional bottlenecks.
- **Medium Confidence**: The 15-20% accuracy improvements over baseline methods are compelling but may not generalize uniformly across all LLM architectures and task distributions.

## Next Checks
1. Evaluate PagedEviction's performance on additional inference frameworks beyond vLLM (e.g., FasterTransformer, MLC-LLM) to assess framework dependency and portability.
2. Conduct ablation studies varying block sizes and eviction thresholds to identify optimal configurations for different model scales and task types.
3. Test the method on specialized domains (e.g., code generation, medical text) where token importance patterns may differ significantly from general language tasks.