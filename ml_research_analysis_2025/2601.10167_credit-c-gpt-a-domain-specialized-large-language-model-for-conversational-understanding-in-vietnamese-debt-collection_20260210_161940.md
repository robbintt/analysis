---
ver: rpa2
title: 'Credit C-GPT: A Domain-Specialized Large Language Model for Conversational
  Understanding in Vietnamese Debt Collection'
arxiv_id: '2601.10167'
source_url: https://arxiv.org/abs/2601.10167
tags:
- conversational
- language
- credit
- c-gpt
- debt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Credit C-GPT, a 7-billion-parameter large\
  \ language model fine-tuned for Vietnamese debt collection conversations. The model\
  \ unifies multiple conversational intelligence tasks\u2014including emotion recognition,\
  \ sentiment classification, intent detection, call stage classification, and slot-value\
  \ extraction\u2014within a single reasoning framework, addressing limitations of\
  \ traditional modular NLP pipelines in handling informal spoken language, emotional\
  \ variability, and long conversational contexts."
---

# Credit C-GPT: A Domain-Specialized Large Language Model for Conversational Understanding in Vietnamese Debt Collection

## Quick Facts
- arXiv ID: 2601.10167
- Source URL: https://arxiv.org/abs/2601.10167
- Reference count: 10
- Primary result: 7B parameter model achieves 0.77 intent detection accuracy and 0.93 agent name slot extraction on Vietnamese debt collection calls

## Executive Summary
This paper introduces Credit C-GPT, a 7-billion-parameter large language model fine-tuned for Vietnamese debt collection conversations. The model unifies multiple conversational intelligence tasks—including emotion recognition, sentiment classification, intent detection, call stage classification, and slot-value extraction—within a single reasoning framework, addressing limitations of traditional modular NLP pipelines in handling informal spoken language, emotional variability, and long conversational contexts. Credit C-GPT is trained on proprietary simulated debt collection dialogues, augmented with real-world test data, using supervised instruction tuning and QLoRA for efficient adaptation. Evaluated on Vietnamese debt collection calls, Credit C-GPT consistently outperforms traditional BERT-based baselines across all tasks, achieving strong performance in classification (emotion: 0.90, sentiment: 0.89, intent: 0.77, call stage: 0.88) and slot-value extraction (e.g., agent_name: 0.93, customer_name: 0.85), while offering scalable, privacy-preserving deployment suitable for enterprise contact centers.

## Method Summary
Credit C-GPT adapts the Qwen2.5-7B Instruct base model through supervised instruction tuning on 17,000 simulated Vietnamese debt collection conversations (~336,935 turns). The fine-tuning uses QLoRA (4-bit quantization with low-rank adapters) to enable efficient training on enterprise GPUs. Training data is converted to instruction-input-output triples where outputs are structured JSON containing multiple task predictions (emotion, sentiment, intent, call stage, slot-values). The model performs turn-level inference conditioned on full conversational history in a rolling context, enabling cross-turn reasoning. Evaluation compares Credit C-GPT against BERT-based pipeline baselines on a held-out test set of 400 real-world calls (11,955 turns), measuring classification accuracy and entity-level slot extraction performance.

## Key Results
- Credit C-GPT achieves 0.77 intent detection accuracy, outperforming BERT-based pipeline (0.65) and base Qwen2.5-7B Instruct (0.59)
- Call stage classification accuracy reaches 0.88, significantly exceeding BERT baseline (0.72) and demonstrating long-context reasoning benefits
- Slot-value extraction achieves strong entity-level accuracy (agent_name: 0.93, customer_name: 0.85, total_debt: 0.90) while maintaining structured output format
- The unified model consistently outperforms task-specific BERT pipelines across all evaluated tasks, with largest gains in intent detection and call stage classification

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Unified generative modeling outperforms modular NLP pipelines on spoken Vietnamese debt collection conversations.
- **Mechanism:** By training a single model to jointly predict emotion, sentiment, intent, call stage, and slot-values in one forward pass, Credit C-GPT enables implicit cross-task reasoning. Unlike pipeline components that operate on isolated utterances with limited context windows, the unified model conditions on full conversational history, allowing predictions at turn T to inform and constrain predictions at turn T+1.
- **Core assumption:** Cross-task dependencies (e.g., emotional escalation correlating with intent shifts) are learnable patterns that benefit from shared representation rather than independent optimization.
- **Evidence anchors:** [abstract] "integrates multiple conversational intelligence tasks... within a single reasoning-based framework"; [Section 6.3] "Credit C-GPT consistently outperforms traditional BERT-based pipeline models across all tasks, with particularly strong improvements in intent detection and call stage classification"; [corpus] Weak direct corpus validation for unified-vs-pipeline comparison; neighbor paper "Debt Collection Negotiations with Large Language Models" explores LLM-based negotiation but does not isolate unified modeling as a variable.
- **Break condition:** If tasks are truly independent with negligible cross-correlation, unified modeling adds parameter overhead without accuracy gains; pipeline approaches with task-specific architectures would match or exceed performance.

### Mechanism 2
- **Claim:** Domain-specific supervised instruction tuning yields greater performance gains than model scale alone for low-resource, domain-constrained conversational tasks.
- **Mechanism:** Credit C-GPT and base Qwen2.5-7B Instruct share identical architecture and parameter count, yet Credit C-GPT outperforms across all tasks (e.g., intent: 0.77 vs. 0.59; call stage: 0.88 vs. 0.62). The performance delta isolates the effect of domain-specific instruction tuning on simulated Vietnamese debt collection dialogues, suggesting the model internalizes domain-specific negotiation patterns, colloquial expressions, and implicit intent shifts absent from general-purpose pretraining.
- **Core assumption:** Simulated conversations, when constructed by domain experts following real business logic and regulatory constraints, sufficiently approximate the distributional properties of production calls for transfer to occur.
- **Evidence anchors:** [Section 6.3] "Credit C-GPT consistently outperforms Qwen across all tasks... domain supervision, rather than model scale alone, plays a critical role"; [Section 4.1] "simulated phone calls are conducted... strictly adhering to real-world business logic, regulatory constraints, and conversational strategies"; [corpus] Neighbor paper "A Diverse and Effective Retrieval-Based Debt Collection System with Expert Knowledge" supports the value of expert knowledge injection but does not evaluate instruction-tuned LLMs, limiting direct comparison.
- **Break condition:** If simulation-to-real gap is large (distribution shift on acoustic conditions, dialect variation, or negotiation dynamics not captured in simulation), domain-specific tuning could overfit to synthetic patterns and fail to generalize.

### Mechanism 3
- **Claim:** Long-context conditioning enables robust detection of temporally evolving signals (intent shifts, emotional escalation, stage transitions) that short-context models miss.
- **Mechanism:** The model performs turn-level inference conditioned on the entire preceding conversational history in a rolling context. Tasks like call stage classification and intent detection—where labels depend on dialogue progression rather than isolated utterances—show the largest gains over baselines (call stage: 0.88 vs. 0.72 BERT). Training samples include full multi-turn transcripts, allowing the model to learn long-range dependencies inherent to debt collection negotiations.
- **Core assumption:** Rotary positional embeddings in the Qwen2.5 architecture effectively capture dependencies across 20-30 turn conversations without degradation.
- **Evidence anchors:** [Section 3.2] "turn-level inference by conditioning on the entire conversational history in a rolling context"; [Section 4.2] "dataset preserves full conversational context, enabling Credit C-GPT to learn long-range dependencies"; [corpus] No direct corpus validation for long-context mechanisms in Vietnamese conversational settings; this remains an assumption requiring further empirical scrutiny.
- **Break condition:** If context length exceeds effective attention horizon or positional encoding resolution, long-range dependencies degrade to noise, and performance converges to short-context baselines.

## Foundational Learning

- **Concept: QLoRA (Quantized Low-Rank Adaptation)**
  - **Why needed here:** Credit C-GPT uses 4-bit quantization with low-rank adapters to fine-tune a 7B model on enterprise-grade GPUs (4x NVIDIA L40S). Understanding QLoRA is prerequisite to reproducing or adapting the training pipeline.
  - **Quick check question:** Can you explain why freezing the base model weights and training only low-rank adapter matrices preserves most performance while reducing memory by ~4x?

- **Concept: Instruction Tuning with Structured JSON Outputs**
  - **Why needed here:** The model is trained on instruction–input–output triples where outputs are structured JSON containing multiple task predictions. This differs from standard classification heads and requires understanding prompt engineering and schema-constrained generation.
  - **Quick check question:** Given a sample dialogue turn, can you construct an instruction prompt that requests both emotion classification and slot extraction in a unified JSON schema?

- **Concept: Rolling Context Inference for Real-Time Systems**
  - **Why needed here:** Deployment scenarios include real-time agent assist, where the model must process each new turn without recomputing the full transcript. Understanding rolling context and incremental inference is critical for latency-sensitive applications.
  - **Quick check question:** How would you modify the inference pipeline to avoid O(n²) attention cost as conversation length grows beyond 50 turns?

## Architecture Onboarding

- **Component map:** Qwen2.5-7B Instruct (decoder-only Transformer, rotary embeddings) -> QLoRA (4-bit quantized base, low-rank adapters) -> Supervised instruction tuning on simulated dialogues -> Turn-level inference with rolling context -> Structured JSON output

- **Critical path:**
  1. Data preparation: Convert annotated dialogues to instruction–input–output format with JSON output schema
  2. QLoRA setup: Load Qwen2.5-7B in 4-bit, inject LoRA adapters (rank and target layers per hyperparameter sweep)
  3. Training: Supervised fine-tuning on mixed short/long context batches, monitor validation loss across tasks
  4. Evaluation: Run turn-level inference on held-out real-world test set (400 calls), compute per-task accuracy and entity-level slot extraction metrics

- **Design tradeoffs:**
  - Unified model vs. task-specific heads: Unified approach reduces system complexity and enables cross-task reasoning, but requires careful prompt design and may produce invalid JSON if generation is unconstrained
  - Simulated vs. real training data: Simulation ensures privacy compliance and scalability, but introduces distribution shift risk; test set uses real calls to validate generalization
  - 7B scale vs. larger models: Smaller model enables on-premise deployment with single GPU, but trades off absolute accuracy vs. GPT-5 (e.g., intent: 0.77 vs. 0.84)

- **Failure signatures:**
  - Hallucinated slot values in low-information contexts (model generates entity names not present in dialogue)
  - Inconsistent stage labels across adjacent turns (e.g., jumping from "opening" to "closure" without intermediate stages)
  - Degraded performance on conversations exceeding training max length (avg 19.5–29.9 turns/conv; longer calls may truncate or lose coherence)
  - JSON parsing failures when model generates malformed output structures

- **First 3 experiments:**
  1. **Baseline replication:** Train BERT-based pipeline (intent classifier + slot tagger + sentiment model) on identical data splits; confirm reported gap (e.g., intent: 0.65 vs. 0.77) to validate experimental setup.
  2. **Ablation on context length:** Evaluate Credit C-GPT with truncated context windows (1, 3, 5 turns) vs. full history; quantify contribution of long-context conditioning to intent and call stage accuracy.
  3. **Prompt robustness test:** Vary instruction phrasing and JSON schema ordering across 50 sample dialogues; measure output consistency and parsing failure rate to assess sensitivity to prompt formulation.

## Open Questions the Paper Calls Out

- **Question:** To what extent does the domain-specific fine-tuning of Credit C-GPT degrade its general Vietnamese language understanding capabilities?
- **Basis in paper:** [explicit] Section 7 states, "we do not assess potential catastrophic forgetting on general-domain benchmarks, which we leave for future investigation."
- **Why unresolved:** The evaluation protocol focused exclusively on domain-specific tasks (emotion, intent, slots) using proprietary data, ignoring the potential trade-off between domain specialization and general linguistic competence.
- **What evidence would resolve it:** A comparative evaluation of Credit C-GPT against the base Qwen2.5-7B model on standard Vietnamese general-purpose NLP benchmarks (e.g., reading comprehension or open-domain QA).

- **Question:** Can Reinforcement Learning from Human Feedback (RLHF) effectively mitigate hallucinations in low-information contexts without sacrificing the model's ability to perform structured slot extraction?
- **Basis in paper:** [explicit] Section 7 notes that the model "may occasionally produce hallucinated outputs" and lists "reinforcement learning from human feedback" and "tighter output validation constraints" as future work.
- **Why unresolved:** While structured output schemas help, the authors concede that eliminating errors in ambiguous contexts remains an open challenge for the current supervised fine-tuning approach.
- **What evidence would resolve it:** Experiments comparing the hallucination rates and slot extraction F1-scores of the current supervised model against a version fine-tuned with RLHF specifically targeting ambiguity and refusals.

- **Question:** Does training primarily on simulated conversational data impose a generalization ceiling when processing the high emotional variability found in authentic production calls?
- **Basis in paper:** [inferred] Section 4.1 and 4.3 reveal the model is trained on 15,300 simulated conversations but tested on "real-world" calls that are "notably longer and exhibit higher variability... and emotional dynamics."
- **Why unresolved:** The paper demonstrates that simulated data is effective for fine-tuning, but it does not quantify the performance gap that might exist if the training set consisted entirely of real, non-simulated interactions.
- **What evidence would resolve it:** An ablation study comparing model performance when trained on purely simulated data versus a dataset augmented with varying proportions of real-world, non-simulated transcriptions.

## Limitations

- The simulation-to-real generalization gap remains unquantified—training uses 15,300 simulated conversations while testing uses real calls, with no distribution shift analysis between the two.
- Unified modeling advantages over modular pipelines are demonstrated through benchmarks but lack ablation studies isolating shared representation benefits from other factors like context length or task independence.
- QLoRA implementation details (adapter rank, learning rate schedule, batch size) are unspecified, making it difficult to assess whether performance gains stem from fine-tuning quality or hyperparameter optimization.

## Confidence

- **High Confidence**: The reported performance improvements of Credit C-GPT over BERT-based baselines are well-supported by the presented metrics (e.g., intent: 0.77 vs. 0.65, call stage: 0.88 vs. 0.72). The methodology for multi-task evaluation is clear and the test set composition (400 real calls) provides reasonable validation.
- **Medium Confidence**: The claim that domain-specific instruction tuning provides greater gains than model scale alone is supported by comparative results with Qwen2.5-7B Instruct, but lacks rigorous ablation to isolate instruction tuning effects from other variables like context length or training data quality.
- **Low Confidence**: The assertion that unified generative modeling inherently enables better cross-task reasoning than modular pipelines is not empirically validated—no experiments compare Credit C-GPT against a properly optimized pipeline baseline with shared contextual embeddings or demonstrate that task dependencies actually exist in the data.

## Next Checks

1. **Distribution Shift Analysis**: Compute Wasserstein distance or KL divergence between simulated training data and real test call distributions across turn length, utterance complexity, and task label frequencies to quantify simulation-to-real generalization risk.
2. **Modular Pipeline Ablation**: Implement a BERT-based modular pipeline with shared contextual embeddings (e.g., using CLS token representations across tasks) and compare against Credit C-GPT on identical data splits to isolate the benefit of unified generative modeling versus shared context.
3. **Prompt Robustness Evaluation**: Systematically vary instruction phrasing, JSON schema ordering, and output constraints across 100 sample dialogues to measure output consistency and parsing failure rates, establishing the sensitivity of Credit C-GPT to prompt formulation variations.