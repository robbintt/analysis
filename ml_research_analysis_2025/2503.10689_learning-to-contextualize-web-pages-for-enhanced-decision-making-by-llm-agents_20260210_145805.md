---
ver: rpa2
title: Learning to Contextualize Web Pages for Enhanced Decision Making by LLM Agents
arxiv_id: '2503.10689'
source_url: https://arxiv.org/abs/2503.10689
tags:
- action
- tasks
- page
- observation
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LCoW, a framework that trains a contextualization
  module to transform complex web page observations into a comprehensible format for
  large language model (LLM) agents, improving their decision-making in web automation
  tasks. By decoupling web page understanding from decision making, LCoW trains a
  separate module to contextualize raw HTML and accessibility trees into simplified,
  task-relevant descriptions.
---

# Learning to Contextualize Web Pages for Enhanced Decision Making by LLM Agents

## Quick Facts
- arXiv ID: 2503.10689
- Source URL: https://arxiv.org/abs/2503.10689
- Authors: Dongjun Lee; Juyong Lee; Kyuyoung Kim; Jihoon Tack; Jinwoo Shin; Yee Whye Teh; Kimin Lee
- Reference count: 40
- Key outcome: LCoW improves LLM agent success rates by 15.6% (closed-source) and 23.7% (open-source) on web automation tasks

## Executive Summary
This paper introduces LCoW, a framework that trains a contextualization module to transform complex web page observations into a comprehensible format for large language model (LLM) agents, improving their decision-making in web automation tasks. By decoupling web page understanding from decision making, LCoW trains a separate module to contextualize raw HTML and accessibility trees into simplified, task-relevant descriptions. The approach significantly improves LLM agent performance across three benchmarks, achieving state-of-the-art results on WebShop and demonstrating generalization to arbitrary LLM agents and unseen task types.

## Method Summary
LCoW implements an iterative training algorithm that collects successful trajectories, samples multiple contextualized observations per raw observation, and selects those that best enable LLM agents to predict correct actions. The contextualization module (Phi-3-mini-Instruct or Llama-3.1-8B-Instruct) is fine-tuned using action-matching rewards computed by multiple LLM agents. The framework is evaluated on WorkArena, WebShop, and WebArena benchmarks, measuring success rate and average reward improvements over raw observation baselines.

## Key Results
- LCoW achieves 15.6% average improvement for closed-source LLMs (Gemini-1.5-flash, GPT-4o, Claude-3.5-Sonnet) on WorkArena
- LCoW achieves 23.7% average improvement for open-source models (Llama-3.1-8B, Llama-3.1-70B) on WorkArena
- Gemini-1.5-flash agent with LCoW achieves state-of-the-art results on WebShop, outperforming human experts
- The contextualization module generalizes to arbitrary LLM agents and unseen task types

## Why This Works (Mechanism)
The framework works by training a dedicated contextualization module that transforms raw, complex web page observations (HTML and accessibility trees) into simplified, task-relevant descriptions. This separation of concerns allows the LLM agent to focus purely on decision-making while the contextualization module handles the complexity of web page understanding. The iterative training algorithm ensures that the contextualized observations consistently improve the agent's ability to predict correct actions across multiple LLM agents, preventing overfitting to any single model's behavior.

## Foundational Learning
- **Web automation task representation**: Understanding how web automation tasks are structured as sequences of observations and actions. Why needed: Forms the basis for training data collection. Quick check: Can you identify the observation-action pairs in a simple web task?
- **Accessibility tree format**: Familiarity with the hierarchical representation of web page UI elements. Why needed: LCoW uses accessibility trees as raw observations. Quick check: Can you parse an accessibility tree and identify interactive elements?
- **Action-matching reward computation**: Understanding how to use LLM agents to evaluate whether contextualized observations lead to correct action predictions. Why needed: Core to the training algorithm's selection of best contextualizations. Quick check: Can you implement a prompt that asks an LLM to predict the next action given an observation?
- **Iterative training algorithms**: Knowledge of algorithms that alternate between data collection and model fine-tuning. Why needed: LCoW's training loop depends on this pattern. Quick check: Can you describe how to implement an iterative algorithm that refines a model based on its own outputs?
- **LLM-based reward modeling**: Understanding how to use LLMs as reward functions for training other models. Why needed: The action-matching scores serve as rewards for fine-tuning. Quick check: Can you design a reward function using an LLM to evaluate text generation quality?
- **Cross-agent generalization**: Recognizing the importance of training methods that work across different LLM architectures. Why needed: LCoW aims to improve any LLM agent's performance. Quick check: Can you test a trained model's performance across multiple different LLM architectures?

## Architecture Onboarding

**Component Map**: Raw observation -> Contextualization module -> Contextualized observation -> LLM agent -> Action prediction

**Critical Path**: The core workflow is: (1) Collect successful trajectories using current contextualization module, (2) Sample N contextualized candidates per observation, (3) Compute action-matching rewards using multiple LLM agents, (4) Select highest-reward candidate, (5) Fine-tune contextualization module on selected pairs.

**Design Tradeoffs**: 
- Separation of web page understanding from decision-making improves modularity but introduces latency
- Using multiple reward-computing agents prevents overfitting but increases computational cost
- Sampling multiple contextualizations per observation improves quality but requires more inference passes

**Failure Signatures**: 
- Contextualization module overfits to training agents (performance degrades with held-out LLMs)
- Module generates overly sparse contextualizations omitting critical UI elements
- Poor generalization to unseen task types (near-zero improvement on held-out categories)

**3 First Experiments**:
1. Implement the contextualization module with Llama-3.1-8B-Instruct and test on a single seed observation from WorkArena
2. Run the action-matching reward computation with GPT-4o on a small set of contextualized observations
3. Perform one iteration of the training loop: collect successful trajectories, sample contextualizations, select best candidates, and fine-tune the module

## Open Questions the Paper Calls Out

**Open Question 1**: Can integrating LLM-based search algorithms into the trajectory collection phase overcome the bottleneck of learning completely new tasks not covered by existing successful trajectories? The current algorithm relies exclusively on successful trajectories, creating a cold-start problem for novel task types where no demonstrations exist.

**Open Question 2**: To what extent can efficient decoding strategies (e.g., speculative decoding) mitigate the latency overhead introduced by the contextualization module? The paper does not experimentally validate whether speculative decoding or similar methods actually reduce latency while maintaining contextualization quality.

**Open Question 3**: Does scaling the training dataset with large-scale web browsing demonstrations across diverse websites enable generalization to tasks with fundamentally unseen UI elements? Current experiments show failure on unseen-category tasks where the contextualization module lacks exposure to specific UI elements during training.

## Limitations
- The method's effectiveness is weaker on tasks requiring nuanced UI understanding (Filter-List tasks in WorkArena)
- Dependency on specific reward-computing agents (GPT-4o, Gemini-1.5-flash, Claude-3.5-Sonnet) raises generalization concerns
- Lack of specification for key hyperparameters (N, M, action-matching implementation details) introduces reproducibility uncertainty

## Confidence
- Performance claims on reported benchmarks: Medium-High
- Generalization to arbitrary agents: Lower (concerns about overfitting to specific reward-computing agents)
- Generalization to unseen task types: Lower (acknowledged weakness on Filter-List tasks)

## Next Checks
1. Evaluate the contextualization module with open-source LLM agents (e.g., Llama-3.1-8B) that were not part of the Î  set used during training to verify generalization claims
2. Test the module on held-out task types/categories not present in the training data to assess whether it learned general UI understanding versus task-specific patterns
3. Compare contextualized outputs against raw observations on held-out tasks to verify that critical interactive elements are preserved and that the simplification process doesn't omit essential information