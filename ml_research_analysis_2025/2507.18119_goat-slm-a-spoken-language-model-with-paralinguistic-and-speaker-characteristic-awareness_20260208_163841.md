---
ver: rpa2
title: 'GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic
  Awareness'
arxiv_id: '2507.18119'
source_url: https://arxiv.org/abs/2507.18119
tags:
- speech
- language
- text
- training
- goat-slm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GOAT-SLM is a spoken language model that integrates paralinguistic
  and speaker characteristic awareness into spoken dialogue systems. It uses a dual-modality
  head architecture that separates linguistic modeling from acoustic generation, and
  a staged training strategy that progressively aligns speech and text while preserving
  the LLM's core language intelligence.
---

# GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness

## Quick Facts
- arXiv ID: 2507.18119
- Source URL: https://arxiv.org/abs/2507.18119
- Authors: Hongjie Chen; Zehan Li; Yaodong Song; Wenming Deng; Yitong Yao; Yuxin Zhang; Hang Lv; Xuechao Zhu; Jian Kang; Jie Lian; Jie Li; Chao Wang; Shuangyong Song; Yongxiang Li; Zhongjiang He; Xuelong Li
- Reference count: 29
- Key outcome: GOAT-SLM is a spoken language model that integrates paralinguistic and speaker characteristic awareness into spoken dialogue systems. It uses a dual-modality head architecture that separates linguistic modeling from acoustic generation, and a staged training strategy that progressively aligns speech and text while preserving the LLM's core language intelligence. The model is trained to perceive cues like dialect, age, emotion, and non-speech vocalizations, and to generate contextually appropriate spoken responses. Evaluation on the TELEV AL benchmark shows GOAT-SLM achieves strong performance in both semantic tasks (e.g., 72.33% on LlamaQA-en, 30.65% on dialect AQA) and paralinguistic-aware interaction (e.g., 45.31% on ESD-zh, 40.91% on Para_mix300-zh, 72.13% on Age-zh). It outperforms open-source baselines in handling dialect following and non-speech vocal responses, demonstrating superior adaptability in natural, socially aware spoken interactions.

## Executive Summary
GOAT-SLM is a spoken language model that perceives and generates speech with awareness of paralinguistic cues (emotion, non-speech vocalizations) and speaker characteristics (dialect, age). It uses a dual-modality head architecture that decouples linguistic modeling from acoustic generation, preserving core LLM reasoning while enabling expressive speech synthesis. The model employs a staged training strategy to progressively align speech and text modalities while maintaining language understanding capabilities. Trained on massive speech-text dialogue data, GOAT-SLM demonstrates strong performance on the TELEV AL benchmark, excelling in both semantic tasks and paralinguistic-aware interaction.

## Method Summary
GOAT-SLM uses a dual-modality head architecture with a Whisper-small encoder feeding into a 2-layer CNN and 2-layer Transformer projector that maps speech to LLM embedding space. The TeleChat2-7B backbone is split into Think (bottom 15 layers, frozen during alignment) and Write/Speak modules (top 15 layers, with Speak initialized from Write weights). Training proceeds through three stages: Stage 1 injects paralinguistic attribute awareness via instruction tuning on text dialogues; Stage 2-1 trains the Listen projector while freezing the LLM backbone on ASR data; Stage 2-2 fine-tunes the full Listen module with paralinguistic labels; Stage 3-1 trains the Speak module on <TQ/SQ, TR, SR> triplets; Stage 3-2 refines with attribute-aware speech prompts. The Speak module uses multi-token prediction for streaming synthesis, and confidence-based gradient masking filters low-confidence speech tokens.

## Key Results
- GOAT-SLM achieves 72.33% on LlamaQA-en, 30.65% on dialect AQA, and 72.13% on Age-zh
- Strong performance in paralinguistic-aware interaction: 45.31% on ESD-zh, 40.91% on Para_mix300-zh
- Outperforms baselines in dialect following (97.4% Cantonese consistency) and non-speech vocalization responses
- Successfully generates dialect-appropriate speech even when text output lacks dialect features
- Handles non-speech vocalizations like coughs and laughter with contextual appropriateness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling linguistic modeling from acoustic generation preserves core LLM reasoning while enabling expressive speech synthesis.
- Mechanism: The dual-modality head architecture shares bottom-N layers as a semantic core ("Think" module) but branches into separate "Write" (text) and "Speak" (speech token) heads using the top-K layers. The Speak head is initialized from the Write head weights, transferring linguistic knowledge to speech generation.
- Core assumption: Freezing or minimally updating the shared Think module prevents catastrophic forgetting of text-based reasoning during speech training.
- Evidence anchors:
  - [abstract] "GOAT-SLM adopts a dual-modality head architecture that decouples linguistic modeling from acoustic realization, enabling robust language understanding while supporting expressive and adaptive speech generation."
  - [section 3] "The Speak module reuses the same top K layers but adapts the output layer to predict speech tokens, enabling a Think–Speak pathway for speech generation."
  - [corpus] Weak direct validation. Related work (Kanade tokenizer) discusses disentanglement of linguistic/non-linguistic speech information, but does not validate this specific dual-head design.
- Break condition: If the Think module is insufficiently sized (N too small), semantic representations may lack depth for both modalities, degrading either text quality or speech coherence.

### Mechanism 2
- Claim: Staged progressive training aligns speech-text modalities without degrading pretrained language capabilities.
- Mechanism: Training proceeds in three ordered stages: (1) instruction tuning injects paralinguistic attribute awareness via text prompts; (2) modality alignment trains the Listen projector while freezing the LLM backbone; (3) speech generation optimization refines the Speak head. This curriculum prevents early speech training from corrupting text intelligence.
- Assumption: The model can transfer attribute awareness learned in text (Stage 1) to speech processing (Stage 2-3) without explicit speech-text paired attribute labels in early stages.
- Evidence anchors:
  - [section 4] "Stage 2-1: The parameters of the Think–Write module and the encoder of the Listen module are frozen, and only the projector in the Listen module is updated."
  - [table 3] Stage 2-1 uses 1152 batch size across 48 GPUs for 1 epoch (~2305 GPU hours), indicating substantial compute devoted to alignment before speech generation.
  - [corpus] Limited external validation. The staged approach aligns with curriculum learning principles mentioned in SpeechGPT/Moshi (Section 2.1), but no corpus paper directly tests this specific 3-stage sequence.
- Break condition: If Stage 1 instruction tuning data lacks diversity in attribute combinations, the model may fail to generalize to novel paralinguistic cues at inference.

### Mechanism 3
- Claim: Dialect characteristics can transfer from input speech representations to output speech even when generated text lacks dialect features.
- Mechanism: The Listen module encodes dialect-specific acoustic features into latent representations. These propagate through the Think module to the Speak head, which generates dialect-appropriate speech tokens even if the intermediate Write module produces standard-language text.
- Core assumption: The shared Think module preserves sufficient paralinguistic information in hidden states for the Speak head to access.
- Evidence anchors:
  - [section 5.2, discussion of Table 10] "Even though the text generated by the Write module does not exhibit dialect features, the generated speech can still effectively acquire dialect-specific cues through the representations from the Listen module."
  - [table 10] GOAT-SLM achieves 97.4% Cantonese, 99.6% Henan, 99.0% Shanghainese, 92.6% Sichuanese consistency rates in subjective dialect evaluation (northeastern Mandarin underperforms at 57.2%).
  - [corpus] Indirect support: "Residual Speech Embeddings for Tone Classification" demonstrates that disentangling linguistic content enhances paralinguistic analysis, suggesting acoustic features can be isolated from text—but does not validate bidirectional transfer for generation.
- Break condition: If the Think module's hidden dimension is too compressed, dialect features may be lost before reaching the Speak head, resulting in standard-language output regardless of input.

## Foundational Learning

- Concept: **Modality Alignment via Projector Training**
  - Why needed here: The Listen module must map speech encoder outputs to the LLM's embedding space. Engineers need to understand that freezing the LLM while training only the projector preserves text capabilities.
  - Quick check question: If you observe degraded text QA performance after speech training, which component was likely updated incorrectly?

- Concept: **Self-Distillation for Target Generation**
  - Why needed here: Stages 2-3 use LLM-generated text responses as training targets (via prompting with transcripts + attribute descriptions). This avoids manual annotation but assumes the LLM's text responses are high-quality.
  - Quick check question: What failure mode would you expect if the base LLM produces inconsistent responses to the same prompt with different attribute tags?

- Concept: **Multi-Token Prediction (MTP) for Streaming Synthesis**
  - Why needed here: The Speak module uses MTP to enable low-latency streaming speech generation with one-step delay behind text. Understanding this is critical for debugging latency issues.
  - Quick check question: If speech output lags text by more than one step, which component's cache management should you investigate?

## Architecture Onboarding

- Component map:
  - Listen: Whisper-small encoder → 2-layer CNN → 2-layer Transformer projector → LLM embedding space
  - Think: Bottom 15 layers of TeleChat2-7B (frozen during alignment stages)
  - Write: Top 15 layers of TeleChat2-7B → text token output
  - Speak: Top 15 layers (initialized from Write) → speech token output → Flow-Matching decoder → vocoder
  - Training Data: Stage 1 (115K text dialogues), Stage 2-1 (73M ASR samples, ~170K hours), Stage 2-2 (53M mixed samples), Stage 3-1 (56M triplets), Stage 3-2 (5.7M attribute-aware triplets)

- Critical path:
  1. Verify Listen projector output dimensions match LLM embedding size
  2. Confirm Speak head initialization copies Write head weights exactly
  3. Validate Stage 2-1 freezes Think-Write while updating only projector
  4. Check Stage 3 confidence-based gradient masking is applied to low-confidence speech tokens

- Design tradeoffs:
  - Smaller N (Think layers): Faster inference, but risks insufficient semantic depth for complex reasoning
  - Random vs. inherited Speak initialization: Random init allows independent optimization but loses linguistic priors; inherited init (GOAT-SLM's choice) preserves text knowledge but may constrain speech expressiveness
  - Explicit attribute prompts vs. implicit learning: Explicit prompts improve controllability but require annotated data; implicit learning is more scalable but less interpretable

- Failure signatures:
  - Text QA accuracy drops after speech training → Think module was likely unfrozen during Stage 2
  - Dialect following works for comprehension but not generation → Speak head not receiving dialect features; check Think→Speak hidden state flow
  - Non-speech vocalizations (cough, laughter) ignored in responses → Stage 1 contrastive data may lack NSV examples; verify Para_mix300 training coverage

- First 3 experiments:
  1. Ablate Speak head initialization: Train with randomly initialized Speak head vs. Write-head initialization; measure dialect following accuracy and text QA preservation to validate the transfer mechanism.
  2. Vary Think module depth: Test N=10, 15, 20 layers; evaluate tradeoff between semantic task accuracy (LlamaQA) and paralinguistic response quality (ESD-zh, Age-zh).
  3. Probe dialect feature propagation: Extract hidden states from Think module output for dialect vs. standard speech inputs; measure separability using a linear classifier to verify dialect information is preserved before reaching Speak head.

## Open Questions the Paper Calls Out

The paper identifies three key open questions: (1) Whether the trade-off between semantic intelligence and paralinguistic awareness can be mitigated through improved training strategies, as current performance shows slight declines in general AQA due to paralinguistic incorporation; (2) The cause of inconsistent performance across dialects, particularly the underperformance in northeastern Mandarin spoken response generation (57.2%) compared to other dialects exceeding 90%; (3) Whether GOAT-SLM's paralinguistic reasoning capabilities can generalize to more fine-grained and dynamic interaction scenarios beyond the evaluated categories, including blended paralinguistic cues and temporally evolving emotional states within single conversations.

## Limitations

- The staged training approach relies on an untested assumption that attribute awareness learned in text can transfer to speech processing without explicit speech-text paired attribute labels in early stages.
- The paper reports strong dialect following performance but provides no quantitative analysis of which acoustic features drive these results or whether the model simply memorizes characteristic speech patterns.
- Paralinguistic and speaker characteristic awareness claims, particularly for nuanced attributes like age and emotion, rely heavily on subjective metrics without detailed analysis of false positives/negatives or human performance baselines.

## Confidence

**High Confidence**: The dual-modality head architecture design and its separation of linguistic modeling from acoustic generation, with clear architectural specifications and training procedures that align with established principles of catastrophic forgetting prevention.

**Medium Confidence**: The staged progressive training effectiveness, as the curriculum design follows logical progression but lacks ablation studies showing what happens if stages are reordered or combined, making the claim that this approach preserves LLM capabilities better than alternatives correlative rather than experimentally validated.

**Low Confidence**: The paralinguistic and speaker characteristic awareness claims, particularly for nuanced attributes like age and emotion, due to reliance on subjective metrics without detailed analysis of false positives/negatives or comparison to human performance baselines.

## Next Checks

1. **Probe for catastrophic forgetting across stages**: Implement systematic evaluation of text-only LLM capabilities (e.g., LlamaQA-en) after each training stage to determine whether the staged approach actually prevents degradation or if text capabilities are simply masked by speech generation.

2. **Ablate projector training vs. end-to-end alignment**: Compare the proposed projector-only training (Stage 2-1) against an end-to-end approach where the full Listen module is trained from scratch to validate whether freezing the LLM backbone is truly necessary for preserving language understanding capabilities.

3. **Analyze feature attribution for dialect following**: Use integrated gradients or similar interpretability techniques to identify which acoustic features (fundamental frequency, spectral tilt, temporal patterns) the model uses to generate dialect-appropriate speech, distinguishing between genuine dialect understanding versus superficial acoustic pattern matching.