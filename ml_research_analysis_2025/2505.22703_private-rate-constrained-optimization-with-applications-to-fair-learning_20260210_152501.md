---
ver: rpa2
title: Private Rate-Constrained Optimization with Applications to Fair Learning
arxiv_id: '2505.22703'
source_url: https://arxiv.org/abs/2505.22703
tags:
- constraints
- privacy
- rate
- learning
- constraint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses differentially private optimization under\
  \ rate constraints, which encompass fairness and other performance requirements.\
  \ The core method, RaCO-DP, extends SGDA to handle these constraints by leveraging\
  \ a common structure across rate constraints\u2014partitioning data into disjoint\
  \ subgroups and computing prediction rates over their combinations."
---

# Private Rate-Constrained Optimization with Applications to Fair Learning

## Quick Facts
- arXiv ID: 2505.22703
- Source URL: https://arxiv.org/abs/2505.22703
- Authors: Mohammad Yaghini; Tudor Cebere; Michael Menart; Aurélien Bellet; Nicolas Papernot
- Reference count: 40
- One-line primary result: RaCO-DP achieves Pareto-dominance over private fairness baselines while nearly matching non-private performance through private histogram computation for rate constraints.

## Executive Summary
This paper introduces RaCO-DP, a differentially private optimization method for rate-constrained problems including fairness objectives. The key innovation is exploiting the structure of rate constraints—partitioning data into disjoint subgroups—to compute private histograms via Laplace noise, enabling constraint evaluation without additional privacy cost. RaCO-DP extends SGDA to handle these constraints while achieving (ε,δ)-DP guarantees through standard DP-SGD techniques combined with private histogram updates. The method is proven to find approximate stationary points for non-convex problems and demonstrates superior empirical performance on four datasets, nearly matching non-private baselines while satisfying constraints directly.

## Method Summary
RaCO-DP converts constrained optimization problems into Lagrangian min-max optimization and extends SGDA to handle differential privacy. The method leverages that rate constraints can be expressed as linear combinations of prediction rates over disjoint subgroups, enabling private histogram computation via Laplace noise per iteration. This histogram contains all statistics needed for both primal and dual updates, avoiding composition of multiple private queries. The algorithm combines private histogram updates with standard DP-SGD techniques (clipping and Gaussian noise) for the primal gradients. Convergence is proven for non-convex problems, showing the method finds approximate stationary points while maintaining privacy guarantees.

## Key Results
- RaCO-DP Pareto-dominates prior private fairness methods on four datasets (Adult, Credit-Card, Parkinsons, Folkstables)
- Nearly matches non-private baseline performance while satisfying constraints directly rather than through indirect tuning
- Achieves constraint satisfaction even under strict privacy budgets (ε=1) where previous methods fail
- Demonstrates versatility beyond fairness by handling false negative rate constraints on multiple datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Private histogram computation enables constraint evaluation without additional privacy cost
- Mechanism: The algorithm exploits the structure of generalized rate constraints—which partition data into disjoint subgroups—to compute a single private histogram via Laplace noise per iteration. This histogram contains all statistics needed for both primal and dual updates, avoiding composition of multiple queries.
- Core assumption: Constraints can be expressed as linear combinations of prediction rates over unions of disjoint partition elements (Eq. 6); each sample belongs to exactly one partition part with softmax predictions summing to 1.
- Evidence anchors:
  - [abstract] "the additional privacy cost of incorporating these constraints reduces to privately estimating a histogram over the mini-batch at each optimization step"
  - [Section 4.1] "ℓ1 sensitivity of H(t) is 1 because each sample belongs to exactly one element of the global partition"
  - [corpus] Weak corpus support; related papers focus on fairness-privacy tradeoffs generally, not this specific histogram mechanism
- Break condition: If constraints cannot be expressed via a small global partition (Q large), Laplace noise per histogram element degrades utility; if partition size approaches |D|, mechanism becomes impractical.

### Mechanism 2
- Claim: Dual gradient computation incurs zero additional privacy cost via post-processing
- Mechanism: Since the dual update only requires constraint values Γj(θ), and these are computable from the already-private histogram Ĥ(t), the post-processing property of DP guarantees no additional privacy budget is consumed (Eq. 12).
- Core assumption: The post-processing property holds—that any deterministic function of a private output remains private without composition penalties.
- Evidence anchors:
  - [Section 4.3] "Since Ĥ(t) is already differentially private, the post-processing property of DP ensures that this computation requires no additional privacy budget"
  - [abstract] "allowing constraint evaluation without additional privacy cost"
  - [corpus] Not explicitly addressed in corpus neighbors
- Break condition: If constraint evaluation requires statistics not derivable from the histogram structure, additional private queries would be needed, breaking the zero-cost property.

### Mechanism 3
- Claim: Biased SGDA converges to approximate stationary points despite noise from privacy mechanisms
- Mechanism: The analysis leverages the linear structure of the dual update (λ space) to achieve faster O(1/T^1/4) convergence for non-convex problems, accounting for bounded bias in gradient estimates from clipping and histogram noise.
- Core assumption: The Lagrangian L(θ, λ) is β-smooth; L(·, λ) is G-Lipschitz; dual parameter space Λ is bounded; gradient errors satisfy ||gθ - ∇θL|| ≤ τθ and ||gλ - ∇λL||∞ ≤ τλ.
- Evidence anchors:
  - [Section 5] "our analysis shows SGDA in this setting can converge as fast as 1/T^1/4 instead of 1/T^1/6"
  - [Theorem 5.2] Provides explicit α bound in terms of d, K, n, ε, δ
  - [corpus] No corpus papers address this convergence mechanism directly
- Break condition: If constraints are highly non-convex in θ or if clipping norm C is too small (introducing large bias), stationarity guarantees degrade; Figure 6 shows clipping norm below 12.5 causes constraint violation even without DP noise.

## Foundational Learning

- Concept: **Differential Privacy (ε, δ)-DP and DP-SGD**
  - Why needed here: RaCO-DP builds on DP-SGD's clipping and Gaussian noise for primal gradients while adding Laplace noise for histogram computation; understanding sensitivity, composition, and amplification by subsampling is essential.
  - Quick check question: Can you explain why Poisson sampling enables privacy amplification and why the paper avoids fixed-size mini-batches?

- Concept: **Lagrangian formulation and Gradient Descent-Ascent (GDA)**
  - Why needed here: The constrained problem is converted to min-max optimization over Lagrangian L(θ, λ); RaCO-DP extends SGDA with private gradient estimates for both primal and dual updates.
  - Quick check question: Why does the dual update use projection onto Λ = (ℝ+)J, and what happens if λ is unbounded?

- Concept: **Rate constraints and softmax relaxation**
  - Why needed here: Hard prediction rates use argmax (non-differentiable); tempered softmax στ enables differentiable optimization while approximating hard constraints as τ → ∞.
  - Quick check question: How does temperature τ affect the gap between soft and hard constraint satisfaction, and what value does the paper find sufficient?

## Architecture Onboarding

- Component map:
  - Private Histogram Module: Computes Ĥ(t) ∈ ℝ^(Q×K) via Laplace(1/ε) noise per element (Lines 4-5, Algo 1)
  - Primal Gradient Module: Per-sample gradient decomposition using Ĥ(t) for regularizer term; clipping + Gaussian noise (Lines 6-9)
  - Dual Gradient Module: Constraint evaluation via post-processing Ĥ(t) using Γpost_j (Lines 10-11)
  - Privacy Accountant: Tracks composed privacy loss across T iterations (uses numerical accountant from Doroshenko et al.)

- Critical path:
  1. Define global partition {D1,...,DQ} based on sensitive groups
  2. For each iteration: Poisson sample batch → compute private histogram → compute primal/dual gradients → update θ, λ
  3. Tune clipping norm C to balance bias vs. privacy; tune temperature τ for constraint sharpness
  4. Use validation set to select hyperparameters satisfying constraints

- Design tradeoffs:
  - **Partition size Q**: Smaller Q reduces histogram dimension and Laplace noise accumulation, but restricts expressiveness of constraints
  - **Clipping norm C**: Larger C reduces bias but requires more Gaussian noise for same privacy; C < 12.5 caused FNR constraint failure (Figure 6)
  - **Temperature τ**: Higher τ approaches hard constraints but may destabilize gradients; τ = 1 found sufficient
  - **Iterations T**: More iterations improve convergence but increase cumulative privacy cost

- Failure signatures:
  - Constraints systematically violated despite tuning → clipping norm C too small (bias dominates)
  - High variance in constraint satisfaction across runs → mini-batch size too small or Laplace noise scale too large
  - Poor accuracy-utility tradeoff → global partition Q may be too large; reconsider constraint formulation
  - Using shuffled fixed-size batches instead of Poisson sampling → privacy guarantees may be invalid (Lebeda et al. issue noted in paper)

- First 3 experiments:
  1. **Baseline replication on Adult dataset**: Implement RaCO-DP with demographic parity constraints (J = |Z|·|Y| constraints), compare to DP-FERMI across ε ∈ {1, 3, 9}; verify Pareto dominance in error-disparity tradeoff curves
  2. **Ablation on clipping norm C**: Train with varying C ∈ {1, 2.5, 5, 10, 12.5, 20} on Adult with FNR < 0 constraint (σ = 0, b = ∞ to isolate clipping bias); confirm C ≥ 12.5 needed for constraint satisfaction
  3. **Multi-group stress test**: Evaluate on ACSEmployment with 18 constraints (multiple sensitive groups); verify RaCO-DP scales linearly with constraint count and maintains near-non-private performance

## Open Questions the Paper Calls Out

- **Open Question 1**: Can differentially private optimization be extended to individual fairness constraints, which cannot be formulated as rate constraints?
  - Basis in paper: [explicit] The conclusion states: "Future work could explore private learning under individual fairness constraints, which cannot be formulated as rate constraints."
  - Why unresolved: Individual fairness requires bounding per-sample contributions differently than rate constraints, which rely on subgroup-level histogram statistics. The current RaCO-DP framework fundamentally depends on the rate constraint structure.
  - What evidence would resolve it: A DP algorithm for individual fairness with convergence guarantees, or a formal impossibility result showing fundamental incompatibility.

- **Open Question 2**: Would alternative histogram mechanisms (beyond Laplace) improve utility in high-dimensional, sparse constraint settings?
  - Basis in paper: [explicit] Remark 2 states: "Our framework can readily accommodate other differentially private histogram mechanisms that may provide better utility in some regimes, e.g., when the histogram is high-dimensional and sparse."
  - Why unresolved: The paper uses Laplace for simplicity but does not empirically compare against alternatives like Gaussian sparse histogram mechanisms.
  - What evidence would resolve it: Comparative experiments showing improved accuracy-fairness trade-offs when using alternative histogram mechanisms on datasets with many sensitive groups.

- **Open Question 3**: Can the bias introduced by gradient clipping be mitigated while maintaining both privacy guarantees and constraint satisfaction?
  - Basis in paper: [inferred] The limitations section discusses how clipping biases SGD iterates toward infeasible regions, making the clipping norm C critical. Figure 6 shows constraint violation persists even without DP noise when C is small.
  - Why unresolved: The bias from clipping is identified as orthogonal to RaCO-DP's design, stemming from general DP-SGD properties.
  - What evidence would resolve it: A modified algorithm or clipping strategy that provably reduces bias while maintaining (ε,δ)-DP guarantees and constraint satisfaction.

## Limitations
- The method assumes rate constraints must be expressible via a small global partition structure, restricting applicability to problems where constraints naturally decompose into disjoint subgroups
- Convergence analysis relies on assumptions about β-smooth Lagrangian and bounded dual space that may not hold for all real-world fairness constraints
- Scaling to high-dimensional feature spaces or many sensitive groups could be challenging due to the Q×K histogram dimension
- The method doesn't address what happens when constraints involve overlapping or continuous sensitive attributes

## Confidence
- High confidence: The DP mechanism via private histogram computation is well-established and the post-processing argument is sound
- Medium confidence: The convergence rate of 1/T^1/4 for non-convex problems under SGDA with privacy noise, as the analysis relies on specific assumptions about gradient error bounds
- Medium confidence: The empirical Pareto-dominance claims, as the hyperparameter sweeps are extensive but exact parameter values are not fully specified
- Low confidence: Generalizability to constraint types beyond the demonstrated rate constraints, as the method's structure is specifically designed for this constraint class

## Next Checks
1. Stress test the partition structure by evaluating on a dataset with continuous sensitive attributes (e.g., age) discretized into many bins—measure how histogram dimension Q affects accuracy and privacy guarantees
2. Validate the convergence claims by implementing a controlled synthetic problem where L(θ, λ) smoothness parameters are known, measuring actual convergence rate vs. the theoretical 1/T^1/4
3. Test the method's behavior when clipping norm C is systematically varied below 12.5 on the Adult dataset—confirm the predicted failure mode where constraint satisfaction degrades due to clipping bias dominating the privacy noise