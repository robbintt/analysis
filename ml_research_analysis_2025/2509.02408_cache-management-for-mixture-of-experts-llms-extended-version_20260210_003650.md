---
ver: rpa2
title: Cache Management for Mixture-of-Experts LLMs -- extended version
arxiv_id: '2509.02408'
source_url: https://arxiv.org/abs/2509.02408
tags:
- cache
- which
- paging
- page
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces and studies a new online paging problem,
  called layered paging, to model expert management in Mixture-of-Experts Large Language
  Models (MoE-LLMs). The problem captures the layered architecture of MoE-LLMs and
  the need to efficiently cache frequently used expert weights across layers.
---

# Cache Management for Mixture-of-Experts LLMs -- extended version

## Quick Facts
- **arXiv ID**: 2509.02408
- **Source URL**: https://arxiv.org/abs/2509.02408
- **Reference count**: 30
- **Primary result**: Introduces layered paging model and LLRU algorithm, achieving up to 15% fewer cache faults than standard LRU in MoE-LLM scenarios

## Executive Summary
This paper introduces layered paging, a new online paging problem that models expert weight management in Mixture-of-Experts Large Language Models. The problem captures the sequential, layer-by-layer nature of MoE inference where expert weights must be efficiently cached across multiple layers. The authors establish theoretical lower bounds showing that under mild assumptions, LRU-like policies achieve good competitive ratios. They propose LLRU (Layered LRU), a layer-aware extension of LRU that significantly outperforms standard LRU and other policies in extensive simulations using both synthetic datasets and actual MoE usage traces.

## Method Summary
The paper presents layered paging as an extension of classic paging problems, where requests arrive in rounds corresponding to model layers. They analyze deterministic and randomized algorithms, proving lower bounds on competitive ratios. The proposed LLRU algorithm modifies standard LRU by considering both the last round an expert was accessed and its relative position within the layer sequence when making eviction decisions. Extensive simulations compare LLRU against standard LRU and distributed caching approaches using both synthetic Zipfian distributions and real MoE inference traces from Mixtral models.

## Key Results
- LLRU achieves up to 15% fewer cache faults than standard LRU in practical MoE scenarios
- Fixed partition caching (distributed allocation) has unbounded competitive ratio compared to unified caching
- Under mild assumptions (constant n or ℓ), randomized algorithms have competitive ratio at least Ω(log k)
- LLRU outperforms both standard LRU and distributed caching approaches in extensive trace-based simulations

## Why This Works (Mechanism)

### Mechanism 1: Round-Aware Eviction (LLRU)
Standard LRU evicts globally least recently used pages, but in layered MoE inference, a page from an earlier layer might be evicted just before it's needed again in the next round. LLRU mitigates this by evicting pages with the largest Last-Round Index (pages not used for the most full cycles), breaking ties using Relative Layer Distance. This respects the cyclic structure of MoE inference where pages from layer i+1 are immediately needed after layer i. Evidence shows LLRU reduces cache faults by approximately 15% compared to standard LRU. This mechanism breaks if inference engines introduce non-sequential layer execution or significant asynchronous loading patterns.

### Mechanism 2: Unified vs. Distributed Cache Allocation
MoE workloads follow Zipfian distributions where some experts are "hot" and frequently accessed. Fixed partitions allocate k/ℓ space per layer, wasting capacity when one layer needs more and another needs less. A unified cache allows "memory borrowing" across layers, reducing the constraint to k total active experts. Theoretical results prove fixed-partition algorithms have unbounded competitive ratio, and empirical results show Opt-Dist makes up to 3x more cache faults than unified caching. This mechanism breaks if strict hardware memory isolation is required per layer.

### Mechanism 3: Theoretical Lower Bounds via Parallel Coupon Collector
The analysis maps layered paging to the parallel coupon collector problem. Optimal offline algorithms effectively collect "coupons" (experts) in parallel, while online algorithms suffer from uncertainty about which "collector" (layer) needs which "coupon" next. Under typical constraints (constant n or ℓ), no randomized algorithm can achieve better than Ω(log k) competitive ratio. The theoretical framework is mathematically sound but may not reflect practical performance as MoE architectures scale.

## Foundational Learning

- **Concept: Competitive Analysis (Online Algorithms)**
  - **Why needed here:** To understand the paper's theoretical claims, one must grasp that online algorithms (no future knowledge) are compared against offline optimal (full future knowledge). The competitive ratio quantifies this performance gap.
  - **Quick check question:** Why is the "unbounded competitive ratio" of fixed-partition algorithms worse than the k-competitive ratio of LRU?

- **Concept: Mixture-of-Experts (MoE) Sparsity**
  - **Why needed here:** The caching strategy relies on only a subset of experts being active per token. The paper models this as paging distinct "blocks" of memory.
  - **Quick check question:** In the paper's model, does a "cache miss" trigger recomputation of attention, or just memory load?

- **Concept: Zipfian Distribution**
  - **Why needed here:** LLRU efficiency relies on "hot" experts. Zipf's law mathematically describes this skew where few items are very popular.
  - **Quick check question:** As the Zipf parameter a increases (skew increases), should the gap between Opt and Opt-Dist widen or narrow?

## Architecture Onboarding

- **Component map:** Request sequence σ ordered by layers (1 → 2 → ... → ℓ) → Cache C (size k) storing experts → Metadata map for R(p,t) and D(p,t) → Eviction policy selecting victim p maximizing R, then D

- **Critical path:**
  1. Receive request for Expert E_i^(j)
  2. Check presence in Cache C
  3. Hit: Update timestamp/metadata
  4. Miss: Calculate R and D for all resident pages → Evict max → Load E_i^(j)

- **Design tradeoffs:**
  - LLRU vs LRU: LLRU requires per-page metadata tracking (last access time + layer ID) and modulo arithmetic for every miss, whereas LRU often uses simple counters/stacks. The 15% fault reduction must justify this logic overhead.
  - Real-world Parallelism: The paper models 1 expert/layer, but real models (Mixtral) use 2. Implementation requires deciding if these are treated as sequential rounds or parallel slots (breaking strict layer ordering).

- **Failure signatures:**
  - Thrashing: If k ≈ n (experts per layer), performance collapses
  - Partitioning: Implementing LRU-Dist will result in 1.8x~3x more faults than unified cache
  - Metadata overhead: Tracking R and D per page adds computational cost

- **First 3 experiments:**
  1. Trace Reproduction: Run LRU and Opt on Mixtral traces to replicate the "fluctuation" curve in Figure 4
  2. LLRU Validation: Implement R(p,t) and D(p,t) logic and verify 15% fault reduction against standard LRU on same traces
  3. Capacity Stress Test: Vary cache size k across range [1, nℓ] to observe crossover point where LLRU's advantage over LRU becomes negligible

## Open Questions the Paper Calls Out

- **Open Question 1:** Do there exist randomized algorithms for layered paging that achieve competitive ratio matching the lower bound of max(H_n, log(ℓ)/(6n)), or otherwise improve upon H_k?
  - **Basis in paper:** [explicit] The paper states it's still an open question whether randomized algorithms can reach this ratio or simply have better competitive ratio than H_k.
  - **Why unresolved:** While the paper establishes theoretical lower bounds, it does not propose a specific randomized algorithm proven to meet these bounds.
  - **What evidence would resolve it:** A proposed randomized algorithm with proven competitive analysis demonstrating an upper bound that matches or approximates the derived lower bounds.

- **Open Question 2:** How can the layered paging model and algorithms like LLRU be adapted to handle the general case where multiple experts are activated per layer?
  - **Basis in paper:** [explicit] The authors note their formulation assumes a single expert per layer and suggest it would be interesting to consider the more general model with several experts per layer.
  - **Why unresolved:** The current LLRU algorithm and theoretical analysis are built on the simplification of a single expert request per layer per token.
  - **What evidence would resolve it:** An extension of the theoretical model to include multiple requests per layer and a modified algorithm with corresponding competitive analysis.

- **Open Question 3:** Can learning-augmented algorithms leverage machine-learned predictions to improve competitive ratios for MoE cache management?
  - **Basis in paper:** [explicit] The conclusion identifies learning-augmented variants, where algorithms can leverage machine-learned predictions about future input items, as an interesting direction for future work.
  - **Why unresolved:** The current work focuses on standard online algorithms without predictive capabilities.
  - **What evidence would resolve it:** A learning-augmented algorithm design and analysis showing improved performance (e.g., consistency and robustness) over standard LLRU.

## Limitations

- Theoretical lower bounds assume small constant numbers of experts or layers, which may not hold as MoE architectures scale
- LLRU's advantage depends on strict autoregressive layer execution, which modern inference engines may not maintain
- The paper assumes uniform access patterns within layers, but real workloads may exhibit more complex temporal correlations

## Confidence

- **High confidence** in unified cache superiority over fixed partitions (supported by both theoretical lower bounds and empirical validation showing up to 3x improvement)
- **Medium confidence** in the 15% LLRU improvement claim (requires careful implementation of eviction logic and may vary with workload characteristics)
- **Medium confidence** in theoretical lower bounds (sound mathematical framework but dependent on assumptions about problem scale)

## Next Checks

1. **Scalability validation**: Test LLRU and theoretical bounds on MoE architectures with larger expert counts (n > 256) and more layers (ℓ > 16) to verify if asymptotic behavior matches practical performance

2. **Execution pattern robustness**: Evaluate LLRU performance under non-sequential layer execution scenarios, including parallel expert activation and dynamic layer skipping

3. **Metadata overhead measurement**: Quantify the computational and memory overhead of tracking Last-Round Index and Relative Layer Distance metadata in production inference systems