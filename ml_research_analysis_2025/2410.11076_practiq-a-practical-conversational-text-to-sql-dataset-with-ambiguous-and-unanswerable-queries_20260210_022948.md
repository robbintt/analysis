---
ver: rpa2
title: 'PRACTIQ: A Practical Conversational Text-to-SQL dataset with Ambiguous and
  Unanswerable Queries'
arxiv_id: '2410.11076'
source_url: https://arxiv.org/abs/2410.11076
tags:
- ambiguous
- column
- question
- user
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces PRACTIQ, a conversational text-to-SQL dataset
  containing ambiguous and unanswerable user queries inspired by real-world scenarios.
  The authors define eight categories of problematic queries (four ambiguous, four
  unanswerable) and construct a dataset of 2,812 conversations through systematic
  database schema modifications and LLM-based conversation generation.
---

# PRACTIQ: A Practical Conversational Text-to-SQL dataset with Ambiguous and Unanswerable Queries

## Quick Facts
- arXiv ID: 2410.11076
- Source URL: https://arxiv.org/abs/2410.11076
- Reference count: 40
- Primary result: Introduces a dataset of 2,812 conversations with ambiguous and unanswerable queries, revealing that state-of-the-art models achieve only 71.95% accuracy on such queries compared to 72.15% on standard answerable questions.

## Executive Summary
This paper introduces PRACTIQ, a conversational text-to-SQL dataset containing ambiguous and unanswerable user queries inspired by real-world scenarios. The authors define eight categories of problematic queries (four ambiguous, four unanswerable) and construct a dataset of 2,812 conversations through systematic database schema modifications and LLM-based conversation generation. Human annotation shows high quality with 93.75% accuracy on category classification and strong agreement on naturalness, factuality, and helpfulness. Evaluation using DIN-SQL and various LLMs reveals that state-of-the-art models struggle with ambiguous and unanswerable questions, achieving only 71.95% accuracy compared to 72.15% on standard answerable questions. Claude 3.5 Sonnet performs best at 77.4% classification accuracy when oracle values are provided.

## Method Summary
The authors construct PRACTIQ through a three-stage pipeline: (1) parse SQL from Spider dataset and modify database schemas to introduce ambiguity or unanswerability by removing columns, adding semantically similar columns, or removing cell values; (2) generate assistant clarification responses and reverse-generate user clarifications using LLM prompting; (3) refine conversation quality with 3-shot prompting for naturalness and validate SQL executability. The dataset includes helpful SQL responses that address multiple interpretations directly. Human annotation evaluates 100 examples for category classification accuracy (93.75%) and qualitative metrics including naturalness, factuality, and helpfulness.

## Key Results
- PRACTIQ contains 2,812 conversations covering 8 categories of ambiguous and unanswerable queries
- Human annotation achieves 93.75% accuracy on category classification and strong agreement on quality metrics
- State-of-the-art models achieve only 71.95% accuracy on ambiguous/unanswerable queries vs 72.15% on answerable questions
- Claude 3.5 Sonnet achieves 77.4% classification accuracy with oracle cell values
- Open-source models like Llama-3.1 8B perform significantly worse, showing hallucination rates up to 45% on certain categories

## Why This Works (Mechanism)

### Mechanism 1: Reverse Generation Pipeline Ensures SQL Executability
Generating the assistant's SQL response first, then using an LLM to backfill the user's clarification response, produces more accurate conversational data than forward generation. The pipeline first modifies the database schema to introduce ambiguity or unanswerability, then programmatically generates the corrected SQL, and finally prompts an LLM to generate the user clarification that would naturally lead to that SQL. This ensures the target SQL is always executable and syntactically correct before the conversation is constructed around it.

### Mechanism 2: Schema Modification Creates Controlled Ambiguity Categories
Modifying database schemas (rather than user questions) creates more natural ambiguous and unanswerable queries that better reflect real-world user experiences. Starting from valid SQL queries in Spider, the system systematically modifies the database schema by removing columns, adding semantically similar columns, or removing cell values. This creates situations where a user's original question—which would have been answerable—becomes ambiguous or unanswerable because the database no longer contains the expected information.

### Mechanism 3: Two-Task Decomposition Improves Error Diagnosis
Separating question category classification from SQL generation allows for better isolation and analysis of model failures in conversational text-to-SQL. The evaluation framework first requires models to classify questions into 9 categories (4 ambiguous + 4 unanswerable + answerable), then generates SQL based on the clarification dialogue. This separation reveals whether models fail at detecting the problem type or at generating the correct SQL after clarification.

## Foundational Learning

- **SQL Schema Linking**: Understanding how natural language terms map to database columns, tables, and relationships is essential for diagnosing why questions become ambiguous (multiple possible mappings) or unanswerable (no valid mappings).
  - *Quick check*: Given a schema with columns "Standing_Capacity" and "Seating_Capacity," why might "What is the maximum capacity?" be ambiguous?

- **Conversational Context Management**: The PRACTIQ dataset involves multi-turn conversations where clarification questions must be interpreted in context of the original query and previous responses.
  - *Quick check*: How should a system handle a clarification response like "I meant the age they entered" when the original question asked about "age"?

- **Execution Accuracy vs. Syntax Correctness**: The paper uses execution accuracy as the primary metric, which requires SQL queries to both parse correctly and return the expected results when executed against the database.
  - *Quick check*: Why might a syntactically correct SQL query still fail execution accuracy in the clarification scenario?

## Architecture Onboarding

- **Component map**: SQLGLOT parser -> Schema modification (LLM) -> Reverse generation (LLM) -> Refinement (3-shot LLM) -> Execution validation -> Human annotation
- **Critical path**: Schema modification quality determines whether ambiguity/unanswerability is realistic → SQL executability after modification ensures valid conversational targets → Classification accuracy gates whether models can detect problem types before attempting SQL generation
- **Design tradeoffs**: Template-based clarification responses vs. LLM-generated (consistency vs. naturalness); helpful SQL (all results) vs. clarification questions (user effort); oracle cell values (perfect retrieval) vs. lexical retrieval (realistic but noisier)
- **Failure signatures**: Low classification accuracy with high SQL accuracy (model generates reasonable SQL but doesn't correctly identify the problem category); hallucination in SQL generation (model assumes removed columns still exist); repeated text output (Llama-3.1 70B exhibits degenerate outputs with 2+ examples per category)
- **First 3 experiments**:
  1. Reproduce classification baseline with Claude 3.5 Sonnet using 3-shot prompting on a 100-example subset; verify that oracle cell values improve accuracy by 1-2% over lexical retrieval
  2. Run DIN-SQL framework on ambiguous SELECT Column questions only; analyze whether hallucination rate (currently 3.8%) decreases with explicit schema constraints
  3. Generate 50 new examples for a single category (e.g., Ambiguous Filter Criteria) using the Stage 1-3 pipeline; validate executability and conduct binary classification filtering to measure pass rate

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: Does fine-tuning open-source LLMs on PRACTIQ significantly enhance their capability to detect ambiguous and unanswerable questions compared to current prompt-based baselines?
**Basis in paper**: The authors state in the Limitations section: "We leave the exploration of fine-tuning open-source LLMs and the potential improvements in their capabilities as future work."
**Why unresolved**: Due to time constraints, the authors were unable to generate the volume of data required to fine-tune models and evaluate this specific intervention.
**What evidence would resolve it**: A comparative study measuring the classification accuracy and SQL execution accuracy of open-source models (e.g., Llama-3.1) before and after fine-tuning on the PRACTIQ dataset.

### Open Question 2
**Question**: Can agentic workflows improve the quality of synthetic data generation and benchmarking beyond the current LLM-based reverse generation method?
**Basis in paper**: The paper notes that "the data quality can be further improved by employing agentic workflows" and suggests experimenting with "agentic workflows to benchmark our dataset" as future work.
**Why unresolved**: The current framework relies on programmatic methods and single-turn LLM prompting; the authors did not implement a multi-agent system for generation or evaluation.
**What evidence would resolve it**: A human annotation study comparing the naturalness and factuality of conversations generated by an agentic workflow against the current PRACTIQ baseline.

### Open Question 3
**Question**: Can the "Helpful SQL" generation strategy (providing all possible results) be effectively extended to ambiguity categories other than "Ambiguous SELECT Column" and "Ambiguous WHERE Column"?
**Basis in paper**: Section 3.2.1 states: "We only generate such helpful SQL responses for the Ambiguous SELECT Column and Ambiguous WHERE Column categories, but this can be extended to other categories in the future."
**Why unresolved**: The authors restricted the implementation of helpful SQL responses to two specific categories to ensure executability, defaulting to clarification requests for others.
**What evidence would resolve it**: Designing "Helpful SQL" responses for categories like "Ambiguous Filter Criteria" and evaluating if they improve execution accuracy or user satisfaction in a conversational setting.

## Limitations

- The reverse generation pipeline may create overly structured conversations that don't fully capture the messy, exploratory nature of real user queries
- Schema modification approach assumes database administrators will maintain multiple schema versions or users will naturally encounter such structural variations
- The paper provides a benchmark for a specific problem formulation rather than solving the underlying challenges of conversational text-to-SQL

## Confidence

**High Confidence**: Dataset construction methodology and human annotation quality (93.75% accuracy, systematic validation approach)
**Medium Confidence**: Evaluation results and model performance comparisons (DIN-SQL adaptation artifacts, limited sample sizes for some categories)
**Low Confidence**: Claims about real-world applicability (schema modification may create artificial scenarios that don't generalize to actual user behavior patterns)

## Next Checks

1. **Schema Modification Realism Test**: Generate 50 additional examples using only the most common ambiguity categories and evaluate whether human annotators can distinguish these from naturally occurring ambiguous queries in production systems.

2. **DIN-SQL Adaptation Validation**: Run the DIN-SQL framework on a subset of PRACTIQ examples with explicit schema constraints and compare hallucination rates to the reported 3.8%.

3. **End-to-End Deployment Simulation**: Implement a simple conversational agent that uses PRACTIQ-style clarification logic on a real-world database schema and measure user success rates on ambiguous queries.