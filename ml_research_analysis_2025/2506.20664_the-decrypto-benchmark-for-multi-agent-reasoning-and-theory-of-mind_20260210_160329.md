---
ver: rpa2
title: The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind
arxiv_id: '2506.20664'
source_url: https://arxiv.org/abs/2506.20664
tags:
- hints
- guess
- hint
- code
- alice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Decrypto, a multi-agent reasoning and theory
  of mind benchmark based on a code-guessing board game. It aims to address limitations
  of existing benchmarks by providing an interactive, language-based platform that
  requires agents to reason about others' beliefs and information access.
---

# The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind

## Quick Facts
- arXiv ID: 2506.20664
- Source URL: https://arxiv.org/abs/2506.20664
- Reference count: 40
- Authors: Andrei Lupu; Timon Willi; Jakob Foerster
- Key outcome: Decrypto reveals a regression in newer reasoning models' theory of mind abilities, with LLMs struggling at nuanced communication and strategic reasoning compared to simple baselines and humans.

## Executive Summary
Decrypto is a multi-agent reasoning and theory of mind benchmark based on a code-guessing board game. It addresses limitations of existing benchmarks by providing an interactive, language-based platform that requires agents to reason about others' beliefs and information access. The benchmark evaluates cooperative and competitive settings, as well as three key ToM abilities through variants of classic cognitive experiments. The authors find that even state-of-the-art models struggle with the nuanced communication and strategic reasoning required in Decrypto, often underperforming simple baselines and humans in coordination and competition.

## Method Summary
Decrypto formalizes a code-guessing game where Alice (Encoder) provides hints for a 3-digit code referencing 4 secret keywords, while Bob (Decoder) and Eve (Interceptor) independently guess the code. The game ends when either team accumulates 2 miscommunications or 2 intercepts, or after 8 turns. The benchmark uses 680 predetermined keywords and evaluates LLMs and baseline agents through prompts and word-embedding similarity metrics. Key metrics include miscommunication rate, intercept rate, win rate, average game length, and ToM-specific accuracy scores. Experiments run 32 games × 3 seeds across cooperative, competitive, and ToM-focused settings.

## Key Results
- Even state-of-the-art models struggle with nuanced communication and strategic reasoning required in Decrypto
- Newer reasoning models like Claude 3.7 and o1 perform worse than older models on ToM tasks, demonstrating regression in abilities
- Simple word-embedding baselines outperform LLMs through shared representational alignment in self-play scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimal play in Decrypto requires recursive belief modeling about other agents.
- Mechanism: The game formalizes as a Rational Speech Act (RSA) pragmatic inference game where Bob must model Alice's beliefs about Eve's beliefs (second-order ToM) to decode correctly. Alice's utility function balances successful communication to Bob against interception risk by Eve.
- Core assumption: Agents have bounded rationality and cannot perfectly model each other; lexical uncertainty exists between agents with different world knowledge.

### Mechanism 2
- Claim: Current LLMs exhibit a regression in perspective-taking abilities compared to older models.
- Mechanism: Reasoning models (Claude 3.7, o1) fail to simulate agents with limited information, consistently predicting Eve will intercept correctly even on the first turn when Eve has no meaningful signal.
- Core assumption: The observed failure reflects a genuine capability gap rather than prompt sensitivity.

### Mechanism 3
- Claim: Simple word-embedding baselines outperform LLMs through shared representational alignment.
- Mechanism: When Alice and Bob share the same embedding space (GloVe or Word2Vec), Alice selects hints maximally similar to the target keyword under cosine similarity, and Bob assigns hints to keywords using the same metric.
- Core assumption: Cosine similarity in embedding space correlates with human semantic association.

## Foundational Learning

- Concept: **Theory of Mind (ToM)**
  - Why needed here: Decrypto's core challenge requires modeling others' knowledge states, beliefs, and intentions—exactly what ToM measures.
  - Quick check question: Can you explain why predicting Eve's guess requires different information than predicting Bob's guess?

- Concept: **Pragmatic Inference / Rational Speech Act (RSA)**
  - Why needed here: The paper formalizes Decrypto as an RSA game where speakers choose utterances based on inferred listener interpretation.
  - Quick check question: In RSA, why does a pragmatic listener need to model the speaker's utility function?

- Concept: **Ad-hoc Coordination**
  - Why needed here: A key evaluation setting tests whether agents can coordinate with previously unseen teammates (including humans), not just self-play.
  - Quick check question: Why might self-play performance not transfer to cross-play or human coordination?

## Architecture Onboarding

- Component map:
  Game Engine -> Agent Interface -> Evaluation Suite -> Baseline Agents
  (Game Engine manages turns, keyword sampling, code generation, win/loss conditions)

- Critical path:
  1. Initialize keywords (from 680-word pool), assign to digits 1-4
  2. Each turn: Alice receives code → generates hints → Bob and Eve guess independently
  3. Update hint history and code history (public knowledge)
  4. Check termination (2 miscommunications or 2 interceptions or 8 turns)

- Design tradeoffs:
  - Generalist vs Specialist agents: Prompt is part of environment for generalists; specialist agents can prompt-engineer
  - K parameter: Low K = easier coordination but easier interception; high K = stronger self-play but catastrophic cross-play failure
  - Token limits: Reasoning models need 5000-10000 tokens; non-reasoning ~750

- Failure signatures:
  - Alice hints too literally → high interception rate
  - Alice hints too obscurely → high miscommunication rate
  - Models assume Eve has keyword access → predict interception on turn 1 (perspective-taking failure)
  - Cross-play with unmatched embeddings → K > 128 causes >60% miscommunication

- First 3 experiments:
  1. Run cross-play matrix with 2-3 LLMs as encoder/decoder pairs against fixed interceptor to identify coordination gaps
  2. Execute perspective-taking ToM experiment: prompt Alice to predict Eve's guess on turn 1—check if model acknowledges Eve has zero signal
  3. Compare generalist (out-of-box) vs specialist (prompt-engineered) performance on human-AI cross-play using collected human games

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does reinforcement learning on verifiable tasks cause the regression in theory of mind (ToM) abilities observed in newer reasoning models compared to older counterparts?
- Basis in paper: [explicit] Section 5 and Appendix M state, "Investigating this hypothesis and better understanding the link between training methods and the resulting ToM abilities is an important direction for future work."
- Why unresolved: The paper identifies the regression but offers only a hypothesis regarding RL incentives without empirical verification.

### Open Question 2
- Question: Does fine-tuning LLMs on Decrypto using multi-agent reinforcement learning (MARL) lead to brittle, idiosyncratic strategies or generalizable coordination skills?
- Basis in paper: [explicit] Appendix A asks whether fine-tuning "produces the same kind of idiosyncrasies observed when training tabula rasa models in other cooperative environments."
- Why unresolved: The current work evaluates generalist models and simple rule-based baselines; it does not investigate if RL-based adaptation creates agents that can robustly coordinate with humans or novel AI agents.

### Open Question 3
- Question: How does lexical uncertainty or divergent world knowledge between players impact miscommunication rates in Decrypto?
- Basis in paper: [explicit] Appendix H notes, "Evaluating LLM abilities to play Decrypto and communicate under lexical differences is a promising avenue for future work."
- Why unresolved: The study assumes a shared understanding of word associations, but optimal play requires modeling a teammate's specific vocabulary.

## Limitations
- The regression finding in newer reasoning models shows Low confidence due to limited seed runs and untested prompt sensitivity
- Word-embedding baseline superiority at high K values has Medium confidence due to catastrophic cross-play failure without human semantic validation
- RSA formalism claim has Medium confidence as it's theoretically sound but lacks direct empirical validation

## Confidence
- Regression in newer reasoning models: Low confidence
- Word-embedding baseline superiority: Medium confidence
- RSA formalism claim: Medium confidence

## Next Checks
1. **Replication of the reasoning model regression**: Run the perspective-taking ToM experiment with 25+ seeds per model, systematically vary prompt phrasing around Eve's information access, and test if explicit instruction that "Eve does not know keywords" changes performance.

2. **Cross-play robustness testing**: Conduct systematic cross-play experiments between all LLM pairs (self-play vs cross-play) with the same prompt templates, measuring miscommunication rates at different K values for baseline comparison, and verify if observed coordination gaps persist across prompt variations.

3. **Human-AI coordination benchmark**: Implement human-AI cross-play using the prompt structures from Appendix J, collect at least 20 human games per AI agent, and measure whether specialist prompt-engineering improves coordination beyond generalist out-of-box performance.