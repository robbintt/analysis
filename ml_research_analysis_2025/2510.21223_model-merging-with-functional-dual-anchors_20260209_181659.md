---
ver: rpa2
title: Model Merging with Functional Dual Anchors
arxiv_id: '2510.21223'
source_url: https://arxiv.org/abs/2510.21223
tags:
- fdas
- merging
- arxiv
- task
- pretrained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Functional Dual Anchors (FDAs), a model merging
  framework that shifts from parameter-space to input-representation space. Instead
  of combining task vectors directly, FDAs construct synthetic inputs whose induced
  gradients align with task vectors, capturing task-specific functional shifts relative
  to the pretrained model.
---

# Model Merging with Functional Dual Anchors

## Quick Facts
- arXiv ID: 2510.21223
- Source URL: https://arxiv.org/abs/2510.21223
- Authors: Kexuan Shi; Yandong Wen; Weiyang Liu
- Reference count: 40
- Primary result: FDAs improve multi-task performance by up to 18% over task arithmetic

## Executive Summary
This paper introduces Functional Dual Anchors (FDAs), a novel model merging framework that shifts from parameter-space to input-representation space. Instead of combining task vectors directly, FDAs construct synthetic inputs whose induced gradients align with task vectors, capturing task-specific functional shifts relative to the pretrained model. A principled initialization scheme based on spectral analysis of linear models is introduced, and FDAs are shown to be complementary to existing parameter-space merging methods. Experiments demonstrate consistent performance gains across vision, language, and LLM architectures.

## Method Summary
The FDA framework operates in two stages: FDA construction and parameter optimization. First, synthetic inputs are constructed by minimizing the cosine distance between gradients induced on the pre-trained model and task vectors (parameter differences). This is achieved through gradient-based optimization of the synthetic inputs. Second, the merged model parameters are optimized using these FDAs as "data" to minimize representation distance across tasks. The framework uses spectral initialization derived from linear model analysis to improve convergence, and employs layer-wise strategies for Transformers. FDAs are shown to be complementary to existing parameter-space methods like TSV and WUDI.

## Key Results
- FDAs improve multi-task average accuracy by up to 18% over task arithmetic on vision benchmarks
- FDA-based merging consistently outperforms Task Arithmetic (TA) and is complementary to parameter-space methods
- Performance gains are demonstrated across ViT, RoBERTa, and LLM architectures including WizardMath-13B and LLaMA-2-13B-code-Alpaca

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Based Knowledge Projection
The framework constructs synthetic inputs by minimizing the cosine distance between gradients induced by these inputs on the pre-trained model and task vectors. This effectively inverts the fine-tuning process by creating data that would induce the same weight updates as the original task-specific fine-tuning. The core assumption is that functional shifts can be captured by first-order gradients relative to pre-trained parameters.

### Mechanism 2: Spectral Initialization for Optimization Stability
Synthetic inputs are initialized with limited energy in the "tail subspace" (low-eigenvalue directions) of the weight matrix to accelerate convergence and improve anchor quality. Analysis of linear models shows that larger initial tail energy leads to lower optimal cosine similarity and slower convergence, while smaller tail eigenvalue energy enables faster convergence.

### Mechanism 3: Loss Landscape Guidance via Input Space
Optimizing with FDAs allows for dynamic adaptation along the loss landscape, avoiding the linear drift associated with simple task vector arithmetic. Unlike Task Arithmetic which computes a static merged weight, FDA-based adaptation uses synthetic inputs to "fine-tune" the merged model, allowing the trajectory to curve towards better local minima defined by combined functional constraints.

## Foundational Learning

- **Task Arithmetic & Task Vectors**: Understanding that task vectors represent parameter differences (τ = θ_finetuned - θ_pretrained) is essential for grasping why matching gradients to τ is meaningful. Quick check: Can you explain why adding task vectors directly might cause interference in the parameter space?

- **Gradient Matching / Dataset Distillation**: The core algorithm solves a bi-level optimization problem typical in dataset distillation - finding data points that produce specific gradient behaviors. Quick check: How does matching the gradient of a loss function differ from matching the loss value itself?

- **Spectral Analysis (SVD)**: The theoretical justification for the initialization scheme relies on eigenvalue decomposition of weight difference matrices. Quick check: In the context of optimization, what is the "tail subspace" and why is it typically hard to learn?

## Architecture Onboarding

- **Component map**: Pretrained params θ₀ -> Task Vectors τᵢ -> FDA Constructor -> Parameter Optimization -> Merged model

- **Critical path**:
  1. Calculate Task Vectors τᵢ = θᵢ - θ₀
  2. Layer-wise FDA Construction: Initialize synthetic inputs and optimize them to align gradients with τᵢ
  3. Aggregation: Collect FDAs from all tasks
  4. Parameter Adaptation: Optimize target model parameters to minimize representation distance on FDAs

- **Design tradeoffs**:
  - Initialization: Linear Weight Sampling vs. Scaled Gaussian (σ=0.01)
  - Shape: Higher anchor_num and token_num improve performance but increase memory/compute quadratically
  - Distance Function: Cosine distance is preferred for semantic alignment; ℓ₁/ℓ₂ are less effective

- **Failure signatures**:
  - Slow Convergence: FDA construction loss stalls due to large initialization scale (σ) introducing too much tail energy
  - Performance Degradation: Merged model underperforms Task Arithmetic; check layer-wise strategy mismatch with model architecture

- **First 3 experiments**:
  1. Sanity Check (Linear): Reconstruct spectral initialization experiment on small ViT to validate low σ improves convergence speed
  2. Ablation (Distance): Compare Cosine vs. ℓ₂ distance in FDA construction to verify semantic sensitivity
  3. Complementarity: Apply FDAs to refine model already merged with TSV or WUDI to confirm complementarity

## Open Questions the Paper Calls Out

### Open Question 1
Can FDAs be effectively applied to all layers of large language models beyond just feed-forward networks without prohibitive computational overhead? The paper applies FDAs only to feed-forward layers in auto-regressive models without explanation of whether this is a design choice or limitation.

### Open Question 2
Does the spectral initialization strategy derived from linear models provide provable convergence guarantees for non-linear deep networks? The theoretical analysis is derived for linear encoders but experiments use deep non-linear models like ViT and RoBERTa.

### Open Question 3
What are the failure modes of FDAs when task vectors encode highly conflicting knowledge that cannot be jointly represented in the input space? The paper shows average improvements but doesn't analyze scenarios where FDAs fundamentally cannot reconcile task-specific knowledge.

### Open Question 4
Can the scaling coefficient σ be learned adaptively rather than set via heuristics, and would this improve performance across diverse architectures? Appendix C.5 mentions a practical heuristic for choosing σ but notes it's crucial for convergence, implying manual selection is a limitation.

## Limitations
- The theoretical justification via linear model spectral analysis may not fully generalize to highly non-linear Transformer architectures
- Performance improvements may depend on specific task distributions and model architectures
- Memory requirements for layer-wise gradient computation during FDA construction are not fully characterized

## Confidence
- **High**: The core gradient-matching mechanism and mathematical formulation (Eq. 1-2) are well-specified and reproducible
- **Medium**: Claims about performance improvements (up to 18% over TA) are supported by experiments but may depend on task distributions
- **Low**: Theoretical claims about spectral initialization benefits and loss landscape guidance require more rigorous validation across diverse model families

## Next Checks
1. **Architecture Generalization Test**: Apply FDAs to a ResNet-based vision model and compare performance gains against Transformer results to validate architecture-agnostic benefits
2. **Task Vector Density Analysis**: Systematically vary task vector magnitudes and overlap to determine limits of FDA effectiveness when task conflicts are severe
3. **Adversarial Robustness Check**: Evaluate whether FDAs trained on one data distribution maintain performance on out-of-distribution samples to validate representational fidelity