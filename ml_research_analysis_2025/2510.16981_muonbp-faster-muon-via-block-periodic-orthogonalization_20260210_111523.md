---
ver: rpa2
title: 'MuonBP: Faster Muon via Block-Periodic Orthogonalization'
arxiv_id: '2510.16981'
source_url: https://arxiv.org/abs/2510.16981
tags:
- block
- full
- muon
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the communication overhead of Muon optimizer
  under model parallelism, where orthogonalization requires gathering gradient matrix
  shards across devices, leading to 5%-10% throughput loss compared to AdamW. The
  authors propose MuonBP (Muon with Block-Periodic Orthogonalization), which applies
  orthogonalization independently to matrix shards on each device and periodically
  performs full orthogonalization to maintain training stability.
---

# MuonBP: Faster Muon via Block-Periodic Orthogonalization

## Quick Facts
- arXiv ID: 2510.16981
- Source URL: https://arxiv.org/abs/2510.16981
- Authors: Ahmed Khaled; Kaan Ozkara; Tao Yu; Mingyi Hong; Youngsuk Park
- Reference count: 40
- Key outcome: 8% throughput improvement on 8B model with eight-way tensor parallelism, no degradation in performance

## Executive Summary
This paper addresses the communication overhead of Muon optimizer under model parallelism, where orthogonalization requires gathering gradient matrix shards across devices, leading to 5%-10% throughput loss compared to AdamW. The authors propose MuonBP (Muon with Block-Periodic Orthogonalization), which applies orthogonalization independently to matrix shards on each device and periodically performs full orthogonalization to maintain training stability. The method requires minimal hyperparameter adjustments and uses two learning rates: one for blockwise steps and one for full orthogonalization steps.

Empirically, MuonBP achieves competitive iteration complexity compared to baseline Muon while providing per-iteration throughput comparable to coordinate-wise methods like AdamW. When training an 8B model with eight-way tensor parallelism and ZeRO optimizer state sharding, MuonBP achieves 8% throughput increase compared to Muon with no degradation in performance.

## Method Summary
MuonBP modifies the Muon optimizer by applying orthogonalization independently to gradient matrix shards on each device during most iterations, only periodically gathering all shards for full orthogonalization. The key innovation is maintaining training stability while reducing communication overhead by performing block-wise orthogonalization most of the time and full orthogonalization periodically. The method introduces a period parameter P that controls how often full orthogonalization occurs, with P=5 used in experiments. Two learning rates are employed: η for blockwise orthogonalization steps and η₀ for full orthogonalization steps, with the theoretical relation η₀ = η/P to maintain convergence guarantees.

## Key Results
- Achieves 8% throughput improvement on 8B parameter model with eight-way tensor parallelism
- Maintains convergence rates interpolating between Muon and BlockMuon through harmonic mean of smoothness constants
- Per-iteration throughput comparable to coordinate-wise methods like AdamW
- No degradation in final perplexity compared to baseline Muon

## Why This Works (Mechanism)
MuonBP reduces communication overhead by performing block-wise orthogonalization locally on each device for most iterations, only gathering gradient matrix shards for full orthogonalization periodically. This maintains the benefits of orthogonal gradient updates while dramatically reducing inter-device communication. The periodic full orthogonalization prevents the accumulation of errors from block-wise approximations and maintains training stability. The dual learning rate approach allows the optimizer to adapt to the different optimization dynamics between block-wise and full orthogonalization steps.

## Foundational Learning

**Orthogonal gradient descent**: Why needed - Prevents parameter updates from interfering with each other, crucial for stable training of large models; Quick check - Verify orthogonality of gradient updates via dot product calculations.

**Model parallelism communication patterns**: Why needed - Understanding how gradient matrices are partitioned across devices; Quick check - Map the communication topology for 8-way tensor parallelism.

**Convergence analysis with periodic operations**: Why needed - Ensures the block-periodic approach maintains theoretical guarantees; Quick check - Verify the convergence bound matches the harmonic mean of smoothness constants.

**ZeRO/FSDP optimizer state sharding**: Why needed - Understanding the interaction between MuonBP and existing parallelism techniques; Quick check - Confirm that optimizer states are correctly partitioned across devices.

## Architecture Onboarding

Component map: Gradient computation -> Block-wise orthogonalization (local) -> Periodic full orthogonalization (collective) -> Parameter update

Critical path: The periodic full orthogonalization step becomes the critical path due to collective communication requirements, though this occurs only once every P iterations.

Design tradeoffs: Reduced communication frequency vs. potential accumulation of block-wise approximation errors; two learning rates vs. simpler hyperparameter tuning.

Failure signatures: Training instability or divergence when P is too large; suboptimal throughput when P is too small; potential numerical issues with block-wise orthogonalization on ill-conditioned gradient matrices.

First experiments:
1. Vary P from 1 to 20 on a small model to find the optimal balance between communication and approximation error
2. Compare final perplexity with different η/η₀ ratios to validate the theoretical learning rate relationship
3. Measure communication volume breakdown between block-wise and full orthogonalization steps

## Open Questions the Paper Calls Out

**Open Question 1**: Can adaptive or dynamic period scheduling (varying P during training) improve upon fixed-period MuonBP?
- Basis in paper: "There are many questions still left: for example, we did not explore varying the period P over the duration of training, or how we might adaptively tune it based on observed properties."
- Why unresolved: All experiments used fixed P=5; no investigation of training-dependent scheduling.
- What evidence would resolve it: Experiments comparing fixed vs. adaptive P schedules across model scales, measuring final perplexity and wall-clock time.

**Open Question 2**: Can MuonBP be extended to expert parallelism (MoE models) while maintaining communication efficiency?
- Basis in paper: "Exploring the use of block orthogonalization with expert parallelism is also an important topic we leave to future work."
- Why unresolved: Current experiments only cover TP and ZeRO/FSDP sharding; MoE expert sharding introduces different communication patterns.
- What evidence would resolve it: Implementation and benchmarking of MuonBP on MoE architectures with expert parallelism.

**Open Question 3**: What mechanism causes MuonBP to occasionally outperform baseline Muon in final perplexity?
- Basis in paper: "Interestingly, overall, our method outperforms Muon despite doing less number of full orthogonalization, we believe this may be due to a regularization effect due to intermittency, we leave the analysis of this behavior as future work."
- Why unresolved: Empirical observation lacks theoretical explanation; unclear whether regularization, implicit optimization dynamics, or other factors dominate.
- What evidence would resolve it: Ablations isolating regularization effects, analysis of gradient subspace exploration, or theoretical bounds explaining the phenomenon.

## Limitations
- Evaluation primarily focused on transformer-based language models, not tested on CNNs or vision transformers
- Theoretical analysis relies on specific smoothness assumptions that may not hold for all loss functions
- 8% throughput improvement demonstrated on 8B parameter model, unclear if similar gains scale to larger models

## Confidence

**Major Claim Confidence:**
- Communication overhead reduction claims: High - Supported by both theoretical analysis and empirical measurements
- Convergence guarantees: Medium - Theoretical bounds exist but practical implications need more exploration
- Training stability: Medium - Periodic full orthogonalization is proposed but long-term stability is not extensively validated
- Performance equivalence to AdamW: Medium - Throughput is comparable but generalization performance needs broader validation

## Next Checks

1. Evaluate MuonBP on diverse model architectures beyond transformers, including CNNs and vision transformers, to assess generalizability
2. Conduct extensive numerical stability analysis across different gradient matrix conditions and singular value distributions
3. Test scalability to larger models (70B+ parameters) with varying degrees of tensor parallelism to validate the 8% improvement claim holds at scale