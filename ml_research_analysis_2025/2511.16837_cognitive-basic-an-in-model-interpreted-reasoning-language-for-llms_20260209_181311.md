---
ver: rpa2
title: 'Cognitive BASIC: An In-Model Interpreted Reasoning Language for LLMs'
arxiv_id: '2511.16837'
source_url: https://arxiv.org/abs/2511.16837
tags:
- cognitive
- basic
- reasoning
- declarative
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Cognitive BASIC is a minimal, BASIC-style prompting language that
  structures LLM reasoning into explicit, stepwise execution traces, enabling transparent
  cognitive control inside the model. By using a line-numbered program interpreted
  entirely by the LLM, it enforces deterministic control flow for extracting declarative
  and procedural knowledge, detecting contradictions, and resolving conflicts.
---

# Cognitive BASIC: An In-Model Interpreted Reasoning Language for LLMs

## Quick Facts
- arXiv ID: 2511.16837
- Source URL: https://arxiv.org/abs/2511.16837
- Reference count: 8
- Primary result: A minimal, line-numbered BASIC-style prompting language that structures LLM reasoning into explicit, stepwise execution traces for transparent cognitive control

## Executive Summary
Cognitive BASIC is a minimal, BASIC-style prompting language designed to structure LLM reasoning into explicit, stepwise execution traces. By interpreting line-numbered programs entirely within the model, it enforces deterministic control flow for extracting declarative and procedural knowledge, detecting contradictions, and resolving conflicts. Evaluated on 25 scenarios requiring all three cognitive stages, the approach revealed strong but uneven performance across models: declarative extraction was highly reliable (>0.96), but conflict detection and resolution showed variability, with full-chain accuracy ranging from 0.60 (gpt-oss:20b) to 0.88 (granite3.3) and 0.80 (mistral:7b).

## Method Summary
The method presents a BASIC-style in-model program that extracts declarative knowledge, detects contradictions, and resolves conflicts. The process uses a natural-language interpreter file defining command semantics and memory schema (working, declarative, procedural, conflicts, resolution), then prompts the LLM to execute the line-numbered program. Execution traces are logged line-by-line with state snapshots, and FINAL MEMORY provides a structured summary of complete reasoning state.

## Key Results
- Declarative knowledge extraction achieved >0.96 accuracy across models
- Conflict detection and resolution showed model-dependent variability
- Full-chain accuracy (D→C→R) ranged from 0.60 (gpt-oss:20b) to 0.88 (granite3.3) and 0.80 (mistral:7b)
- Temporal and numeric inconsistencies were systematically missed by some models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Line-numbered syntax enforces sequential, auditable reasoning traces
- Mechanism: Programs execute in ascending order with explicit branching (IF...THEN, GOTO), converting unstructured text generation into traceable cognitive operations with logged state after each instruction
- Core assumption: LLMs can reliably simulate deterministic execution semantics when given explicit line structure
- Evidence anchors:
  - [abstract] "line-numbered program interpreted entirely by the LLM, it enforces deterministic control flow"
  - [section 2.1] "Lines execute in ascending order, with conditional branching through IF ... THEN <line> or direct jumps using GOTO <line>"
  - [corpus] PLSemanticsBench (arXiv:2510.03415) studies LLMs as language interpreters based on formal semantics, supporting in-model execution feasibility
- Break condition: Models lose line-number tracking during extended execution; complex nested branching exceeds context management

### Mechanism 2
- Claim: Factored memory schema exposes cognitive state transitions explicitly
- Mechanism: Each command operates on five discrete memory variables (working, declarative, procedural, conflicts, resolution), producing transparent state updates rather than implicit reasoning
- Core assumption: Decomposing cognitive state into typed components improves reliability over monolithic reasoning
- Evidence anchors:
  - [abstract] "enabling transparent multi-step reasoning inside the model"
  - [section 2.2] "Each instruction updates this memory state explicitly, producing a transparent, auditable reasoning trace"
  - [corpus] Decoupling Knowledge and Reasoning (arXiv:2507.18178) finds separating knowledge from reasoning aids analysis, providing weak supporting signal
- Break condition: Memory corruption via incorrect state updates; state consistency degrades across many instructions

### Mechanism 3
- Claim: Natural-language interpreter definitions enable in-model program simulation
- Mechanism: An interpreter file written in natural language specifies command semantics and memory rules; the LLM applies these definitions to simulate execution without external engines
- Core assumption: In-context learning sufficiently conveys operational semantics for reliable simulation
- Evidence anchors:
  - [abstract] "natural-language interpreter file specifies command semantics, memory updates, and logging behavior"
  - [section 2.3] "This design turns text generation into a transparent sequence of cognitive operations"
  - [corpus] PLSemanticsBench directly addresses whether LLMs can execute programs from formal semantics, relevant but not directly cited
- Break condition: Ambiguous command definitions cause interpretation drift; models below ~7B parameters show unreliable program following (per section 3.2)

## Foundational Learning

- Concept: **State Machine Execution Models**
  - Why needed here: Understanding program counters and sequential execution is essential for tracing how line-numbered BASIC creates deterministic paths
  - Quick check question: Given lines 10, 20, 30 with a GOTO 15 at line 20, what is the execution order?

- Concept: **Declarative vs Procedural Knowledge Distinction**
  - Why needed here: The memory schema explicitly separates facts (declarative) from action rules (procedural); this distinction drives extraction commands
  - Quick check question: In "The store opens at 9am" vs "Arrive early to avoid crowds," which is declarative and which is procedural?

- Concept: **Conflict Taxonomy (Negation, Qualification, Numeric)**
  - Why needed here: DETECT CONFLICTS checks for specific contradiction types; understanding these categories is necessary to debug failure modes
  - Quick check question: What type of conflict exists between "The system is always available" and "Maintenance occurs Sundays 2-4am"?

## Architecture Onboarding

- Component map:
  Interpreter File -> Memory Schema (working, declarative, procedural, conflicts, resolution) -> Instruction Set (EXTRACT, ADD, DETECT, RESOLVE, control flow) -> Execution Logger -> Final Memory

- Critical path:
  Load scenario → EXTRACT declarative/procedural → ADD to memory → DETECT conflicts → IF conflicts exist, RESOLVE → END with final memory state

- Design tradeoffs:
  - **Simplicity vs. Expressiveness**: BASIC syntax is readable but lacks loops, functions, or complex data structures
  - **Transparency vs. Verbosity**: Full logging creates detailed traces but increases token usage
  - **In-Model vs. External Tools**: All computation occurs inside model; no external retrieval or calculation currently supported

- Failure signatures:
  - **Control flow deviation**: Skipping lines, executing out of order, incorrect branch targets
  - **Conflict blindness**: Temporal and numeric inconsistencies missed (observed in granite3.3 and gpt-oss:20b per section 3.3)
  - **Incomplete resolution**: Summary generated but conflicts list not cleared
  - **Scale threshold**: Models <7B show unreliable program following (section 3.2)

- First 3 experiments:
  1. Run the conflict-resolution program (lines 10-100) on 5 scenarios with clear contradictions; verify each stage (D, C, R) outputs correct state
  2. Probe temporal/numeric edge cases specifically (e.g., "opens at 9" vs "opens at 10") to characterize model-specific blind spots
  3. Compare execution traces across granite3.3, gpt-oss:20b, and mistral:7b to identify where control flow or memory updates diverge

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Cognitive BASIC be extended to allow models to issue and incorporate external tool calls autonomously during execution?
- Basis in paper: [explicit] The conclusion states future work will extend the framework with "tool-use capabilities that can be invoked directly during in-model execution."
- Why unresolved: The current design requires an outside controller to handle external retrieval or computation before the program can resume.
- What evidence would resolve it: A modified interpreter allowing successful autonomous API calls or database queries within the execution trace.

### Open Question 2
- Question: Does oversight by a higher-level executive agent improve the stability of the interpreter's control flow?
- Basis in paper: [explicit] The authors propose "hierarchical control" where "each Cognitive BASIC step is overseen by a higher-level executive agent" as a research direction.
- Why unresolved: The current system relies on a single-level execution loop, which may contribute to the observed control flow instability in smaller models.
- What evidence would resolve it: Comparative benchmarks showing improved full-chain accuracy (D→C→R) in hierarchical architectures versus the standard interpreter.

### Open Question 3
- Question: How do alternative syntactic designs compare to BASIC regarding model interpretability and reasoning performance?
- Basis in paper: [explicit] The paper notes that "Alternative syntactic designs may also be explored," questioning if the retro BASIC style is the optimal format.
- Why unresolved: BASIC was chosen for its simplicity and fit with stepwise programs, but it is unknown if other syntaxes (e.g., Pythonic or functional) would reduce errors.
- What evidence would resolve it: Ablation studies testing different syntactic styles on the same 25-scenario benchmark to measure variance in conflict detection accuracy.

## Limitations

- The exact natural-language interpreter file content is not provided, only its purpose and schema are described
- The 25 evaluation scenarios are not included; only their properties are stated (contradictory statements, various types including temporal/numeric)
- No external computation or retrieval capability constrains the language to purely symbolic reasoning within model context windows

## Confidence

- **High confidence**: Declarative knowledge extraction performance (>0.96 accuracy across models) is empirically robust and well-supported by the trace data
- **Medium confidence**: Conflict detection and resolution show model-dependent variability, with strong performers (granite3.3, mistral:7b) but notable blind spots in temporal/numeric reasoning
- **Low confidence**: The general scalability claim—that BASIC-style syntax can structure complex reasoning—lacks evidence beyond the 25 curated scenarios; no stress tests on larger programs or multi-hop reasoning chains are presented

## Next Checks

1. Publish the complete interpreter file and 25 test scenarios to enable independent replication of the execution traces and accuracy measurements
2. Systematically test model blind spots by constructing targeted scenarios emphasizing temporal and numeric contradictions, measuring per-category detection accuracy
3. Evaluate program execution beyond the conflict-resolution template—test multi-branch logic, nested conditions, and state persistence across longer instruction sequences to probe scalability limits