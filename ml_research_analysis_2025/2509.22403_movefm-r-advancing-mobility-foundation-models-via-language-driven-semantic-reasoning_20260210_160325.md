---
ver: rpa2
title: 'MoveFM-R: Advancing Mobility Foundation Models via Language-driven Semantic
  Reasoning'
arxiv_id: '2509.22403'
source_url: https://arxiv.org/abs/2509.22403
tags:
- trajectory
- mobility
- location
- data
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MoveFM-R integrates large language models (LLMs) with mobility
  foundation models (MFMs) to address the semantic understanding gap in human mobility
  modeling. The framework introduces semantically enhanced location encoding to discretize
  geographic coordinates into interpretable tokens, a progressive curriculum that
  aligns LLM reasoning with mobility patterns, and an interactive self-reflection
  mechanism for conditional trajectory generation.
---

# MoveFM-R: Advancing Mobility Foundation Models via Language-driven Semantic Reasoning

## Quick Facts
- arXiv ID: 2509.22403
- Source URL: https://arxiv.org/abs/2509.22403
- Reference count: 32
- Key outcome: Integrates LLMs with MFMs to improve semantic understanding in mobility modeling, achieving up to 16.8% HR@1 improvement and better trajectory generation across four U.S. cities.

## Executive Summary
MoveFM-R addresses the semantic understanding gap in human mobility modeling by integrating large language models with mobility foundation models. The framework introduces semantically enhanced location encoding that discretizes geographic coordinates into interpretable tokens, a progressive curriculum that aligns LLM reasoning with mobility patterns, and an interactive self-reflection mechanism for conditional trajectory generation. Experiments demonstrate significant improvements in next location prediction and trajectory generation quality, with strong generalization capabilities in zero-shot and few-shot settings across four real-world datasets.

## Method Summary
MoveFM-R employs a three-stage framework: (1) RQ-VAE discretizes semantic vectors into hierarchical tokens using 4 codebooks of 512 embeddings each; (2) Two-stage LLM alignment fine-tunes Qwen2.5-7B via bidirectional instruction tuning and progressive curriculum from trajectory description to pattern summarization; (3) GRPO-based self-reflection enables conditional generation with distributional rewards. The approach combines semantic profile building from OSM POIs and addresses, MFM encoding via TrajMoE, and MLP projection to LLM input embeddings.

## Key Results
- Achieves up to 16.8% improvement in HR@1 for next location prediction compared to strongest MFM baseline
- Improves BLEU scores by up to 2.3 points and reduces TVD by up to 0.021 for trajectory generation
- Demonstrates superior generalization in zero-shot and few-shot settings across four U.S. cities
- Conditional generation improves location metrics across scenarios while maintaining temporal distributions

## Why This Works (Mechanism)

### Mechanism 1
Discretizing geographic semantic space into tokens enables LLMs to process location information while preserving transferability across cities. RQ-VAE decomposes high-dimensional semantic vectors into hierarchical codeword sequences, creating a universal vocabulary because geographic language concepts are largely city-agnostic. The core assumption is that semantic similarity between locations is more transferable across cities than spatial proximity alone.

### Mechanism 2
Progressive curriculum (description→summarization→prediction) installs mobility "grammar" by forcing the model to articulate spatiotemporal patterns before outputting predictions. Two-stage supervised fine-tuning translates MFM latent sequences into factual trajectory text and infers abstract patterns. The core assumption is that forcing explicit pattern articulation improves prediction accuracy by reducing reliance on superficial sequence correlations.

### Mechanism 3
Self-reflective iterative refinement with distributional rewards generates trajectories satisfying both user-specific patterns and scenario constraints. GRPO-based RL loop iteratively detects constraint violations and applies minimal edits until constraints are satisfied. The core assumption is that minimal-edit principle preserves user behavioral consistency while satisfying new constraints, and distributional matching suffices for realism.

## Foundational Learning

- **Concept**: Residual Quantized VAE (RQ-VAE)
  - **Why needed here**: Converts continuous semantic vectors into discrete token sequences hierarchically—each layer captures residual error from previous layers.
  - **Quick check question**: Can you explain why RQ-VAE produces a sequence of codewords rather than a single code, and how the reconstruction loss ensures fidelity?

- **Concept**: Foundation Models vs. Task-Specific Models
  - **Why needed here**: MoveFM-R synthesizes a mobility foundation model (statistical patterns) with an LLM (semantic reasoning)—understanding their complementary failures is essential.
  - **Quick check question**: Why can't LLMs alone generate physically plausible trajectories, and what statistical properties do MFMs capture that LLMs miss?

- **Concept**: Group Relative Policy Optimization (GRPO)
  - **Why needed here**: The self-reflection mechanism uses GRPO to avoid training a separate value function.
  - **Quick check question**: How does GRPO compute advantages using group-relative rewards, and why does this reduce memory overhead compared to PPO?

## Architecture Onboarding

- **Component map**: Raw trajectories (GPS + timestamps) → Semantic Profile Builder (OSM POIs + addresses) → Text Encoder → Semantic Vectors → RQ-VAE → Location Tokens → MFM Encoder (TrajMoE) → Latent Trajectory Sequence → MLP Projection → LLM Input Embeddings → Progressive Curriculum SFT (Description → Summarization) → GRPO Self-Reflection Module (for conditional generation)

- **Critical path**: 1) Build multi-city semantic codebook (requires OSM data + millions of locations); 2) Align codebook with LLM via bidirectional instruction tuning; 3) Train MFM→LLM projection with curriculum SFT; 4) (Optional) Apply GRPO for conditional generation capabilities

- **Design tradeoffs**: Codebook size vs. precision (larger codebook captures finer distinctions but increases vocabulary explosion risk); Semantic vs. spatial encoding (semantic-first enables cross-city transfer but may lose fine-grained spatial relationships); SFT vs. GRPO (GRPO adds constraint satisfaction but requires significant compute)

- **Failure signatures**: Cross-city transfer fails (zero-shot HR@1 drops close to random); Conditional generation ignores constraints (generated trajectories match unconditional baseline); Location tokenization collapses (many locations map to identical tokens)

- **First 3 experiments**: 1) Codebook transfer test (train on 3 cities, evaluate reconstruction quality on held-out city; target reconstruction loss < 0.1); 2) Ablation: Curriculum impact (train with/without summarization stage; expect HR@1 gap of 3-5%); 3) Conditional generation sanity check (generate "late-night commuter" scenario; verify temporal distribution shift toward 10PM-6AM window using TVD metric)

## Open Questions the Paper Calls Out

### Open Question 1
How can the self-reflection mechanism be improved to handle temporal generation in scenarios characterized by high stochasticity or distinct behavioral pattern shifts (e.g., weekends vs. weekdays)? The paper notes a "slight temporal decline" in conditional scenarios involving "high stochasticity and pattern shift," such as predicting weekend trajectories from weekday data. The current "minimal edits" heuristic breaks down when future intent deviates significantly from past patterns.

### Open Question 2
Does reliance on statistical feature matching in the reward function result in a loss of fine-grained, individual-level trajectory uniqueness? Section 3.3.2 states the reward targets "distributional consistency" rather than "exact-match," aiming to match statistical properties rather than specific ground-truth sequences. Optimizing for aggregate statistics might lead to "average" trajectories that statistically resemble the ground truth distribution but fail to capture specific intent or outliers of individual users.

### Open Question 3
To what extent does the "Universal Codebook" generalize to cities with drastically different urban structures or linguistic contexts not present in the pre-training mix? Section 3.1.1 claims the codebook is "universal" and "transferable," yet experiments are restricted to four US cities. The semantic encoding relies on POIs and addresses which may have vastly different distributions, densities, or naming conventions in non-Western or developing urban environments.

## Limitations
- Semantic transferability across cities with fundamentally different urban structures remains untested beyond similar U.S. cities
- Self-reflection mechanism's computational overhead and convergence guarantees for incompatible constraints are not characterized
- Distributional reward matching may optimize for statistical similarity while missing fine-grained behavioral patterns

## Confidence
- **High confidence**: Semantic tokenization approach works for cross-city transfer within similar urban contexts
- **Medium confidence**: Progressive curriculum meaningfully improves prediction accuracy
- **Low confidence**: Self-reflection mechanism robustly handles all constraint combinations in real-world scenarios

## Next Checks
1. **Cross-city robustness test**: Train semantic codebook on three cities with diverse urban morphologies, then evaluate reconstruction quality and downstream prediction accuracy on a fourth city with significantly different structure.
2. **Curriculum ablation with behavioral analysis**: Compare trajectory prediction accuracy and generated trajectory realism between models trained with full curriculum, description-only, and summarization-only stages.
3. **Constraint satisfaction stress test**: Systematically generate impossible constraint combinations and measure self-reflection convergence rates, edit distances, and whether the mechanism degrades to random generation when constraints cannot be satisfied.