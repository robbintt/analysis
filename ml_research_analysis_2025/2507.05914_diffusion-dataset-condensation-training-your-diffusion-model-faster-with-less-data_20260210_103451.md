---
ver: rpa2
title: 'Diffusion Dataset Condensation: Training Your Diffusion Model Faster with
  Less Data'
arxiv_id: '2507.05914'
source_url: https://arxiv.org/abs/2507.05914
tags:
- diffusion
- training
- dataset
- data
- interval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces D2C, the first framework for diffusion dataset\
  \ condensation\u2014condensing large image datasets into compact, synthetic subsets\
  \ that enable high-quality diffusion model training with dramatically less data\
  \ and compute. The core idea is a two-stage process: Select, which identifies a\
  \ compact and diverse subset using a diffusion difficulty score and interval sampling,\
  \ and Attach, which enriches selected samples with semantic (text embeddings) and\
  \ visual (DINOv2 features) information to strengthen conditional signals."
---

# Diffusion Dataset Condensation: Training Your Diffusion Model Faster with Less Data

## Quick Facts
- arXiv ID: 2507.05914
- Source URL: https://arxiv.org/abs/2507.05914
- Reference count: 40
- Primary result: First framework for diffusion dataset condensation, achieving 100× faster training and FID 4.3 using only 0.8% of data

## Executive Summary
D2C introduces the first framework for diffusion dataset condensation, enabling high-quality diffusion model training with dramatically less data and compute. The method condenses large image datasets into compact synthetic subsets through a two-stage process: Select (identifying diverse samples using diffusion difficulty scores) and Attach (enriching samples with semantic and visual information). Experiments on ImageNet demonstrate up to 100× faster training than baselines while maintaining high generation quality, achieving FID 4.3 in just 40k steps using only 0.8% of the original dataset.

## Method Summary
The D2C framework consists of two core stages: Select and Attach. The Select stage identifies a compact and diverse subset of training data using a diffusion difficulty score that measures how challenging each sample is for the diffusion model to generate, combined with interval sampling to ensure coverage across difficulty levels. The Attach stage enriches the selected samples by incorporating both semantic information (text embeddings from CLIP) and visual features (DINOv2 features), creating stronger conditional signals for training. This process transforms a large dataset into a small set of synthetic, information-rich samples that preserve the original data distribution while enabling much faster training.

## Key Results
- Achieves 100× faster training than REPA and 233× faster than vanilla SiT-XL/2 baselines
- Reaches FID 4.3 on ImageNet in just 40k training steps using only 0.8% of the original dataset
- Outperforms random sampling, K-Center, Herding, and SRe2L across multiple dataset sizes and resolutions (256×256 and 512×512)

## Why This Works (Mechanism)
D2C works by strategically selecting the most informative samples and enriching them with complementary information to maximize learning efficiency. The Select stage uses diffusion difficulty scores to identify samples that are both diverse and challenging, ensuring the condensed dataset captures the full complexity of the original distribution. The Attach stage adds semantic and visual context that strengthens the conditional guidance during training, allowing the model to learn more effectively from fewer examples. This dual approach addresses the key challenge in diffusion model training: the need for both diversity (to cover the data distribution) and informativeness (to drive learning progress).

## Foundational Learning

**Diffusion Models**: Generative models that learn to reverse a noising process, typically requiring large datasets and compute. Why needed: Understanding the training requirements and challenges that D2C addresses. Quick check: Can you explain the forward and reverse processes in diffusion models?

**Dataset Distillation**: The process of creating small synthetic datasets that preserve the learning capacity of much larger datasets. Why needed: Provides the conceptual foundation for why dataset condensation is valuable. Quick check: What are the key differences between dataset distillation and dataset condensation?

**Diffusion Difficulty Score**: A metric measuring how challenging each sample is for a diffusion model to generate, used to identify informative training examples. Why needed: Core mechanism for selecting the most valuable samples in the Select stage. Quick check: How does diffusion difficulty relate to sample informativeness?

**CLIP Embeddings**: Text representations that capture semantic meaning, used to enrich selected samples with semantic information. Why needed: Provides the semantic context that enhances the learning signal. Quick check: What role do text embeddings play in conditional diffusion model training?

**DINOv2 Features**: Visual features extracted from self-supervised vision models, used to add visual context to condensed samples. Why needed: Complements semantic information with visual features for richer training signals. Quick check: How do visual features enhance the quality of condensed datasets?

## Architecture Onboarding

**Component Map**: Large Dataset -> Diffusion Difficulty Score Calculation -> Interval Sampling -> Selected Subset -> Semantic Enrichment (CLIP) + Visual Enrichment (DINOv2) -> Condensed Synthetic Dataset -> Diffusion Model Training

**Critical Path**: The bottleneck in traditional diffusion training is the need to process massive amounts of data to capture distribution diversity and informativeness. D2C's critical path is the Select-Attach pipeline that creates maximally informative condensed datasets, reducing the number of training steps needed while maintaining quality.

**Design Tradeoffs**: The framework trades computational overhead in the condensation phase for dramatic reductions in training time and data requirements. This upfront cost is justified by the 100× speedup in training, though the condensation process itself requires careful tuning of difficulty thresholds and enrichment strategies.

**Failure Signatures**: Poor selection quality (too easy or too hard samples) leads to incomplete distribution coverage. Insufficient enrichment results in weak conditional signals and slower convergence. Overly aggressive condensation causes loss of important data modes and artifacts in generated samples.

**First Experiments**:
1. Test D2C on a small subset of ImageNet (e.g., 10 classes) to verify the Select-Attach pipeline works as expected
2. Compare training curves of D2C vs random sampling on the same condensed dataset size
3. Evaluate the impact of semantic vs visual enrichment separately to understand their individual contributions

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on ImageNet and limited synthetic datasets, with less evidence for diverse real-world domains like medical imaging or satellite data
- Does not extensively evaluate downstream task performance or robustness to distribution shifts
- Computational cost of the condensation process itself is not fully characterized

## Confidence

**High confidence**: The core technical contribution (Select-Attach framework), the significant speedup claims (100× over REPA, 233× over vanilla), and the consistent outperformance of established baselines across multiple dataset sizes and resolutions are well-supported by experimental results.

**Medium confidence**: The generalization claims to other datasets and domains are plausible but not extensively validated; the robustness to different diffusion model architectures beyond the tested SiT variants is suggested but not comprehensively explored.

**Medium confidence**: The impact of semantic and visual enrichment (Attach stage) is demonstrated, but ablation studies isolating their individual contributions are limited.

## Next Checks
1. **Downstream Transfer Evaluation**: Test the distilled datasets on fine-tuning tasks and measure performance on out-of-distribution data to assess generalization beyond the original training objective.

2. **Computational Overhead Analysis**: Quantify the condensation process's runtime, memory footprint, and storage requirements to provide a complete cost-benefit assessment.

3. **Cross-Domain Robustness**: Validate D2C on non-ImageNet datasets (e.g., medical, satellite, or specialized scientific datasets) to confirm its effectiveness across diverse data modalities and distributions.