---
ver: rpa2
title: 'Harmony: A Unified Framework for Modality Incremental Learning'
arxiv_id: '2504.13218'
source_url: https://arxiv.org/abs/2504.13218
tags:
- modal
- learning
- knowledge
- modality
- modalities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Modality Incremental Learning (MIL), a new
  paradigm where models must continuously learn from sequentially arriving data with
  distinct modalities, unlike traditional incremental learning that assumes consistent
  modalities. The proposed Harmony framework addresses MIL challenges through two
  key components: (1) Adaptive Compatible Feature Modulation, which generates augmented
  historical modality features by perturbing current modality inputs to simulate diverse
  historical distributions, and (2) Cumulative Modal Bridging, which injects historical
  modal knowledge into current learning through feature fusion and parameter adaptation,
  combined with hybrid alignment strategies including direct feature alignment, contrastive
  alignment, and distribution-level alignment.'
---

# Harmony: A Unified Framework for Modality Incremental Learning

## Quick Facts
- **arXiv ID**: 2504.13218
- **Source URL**: https://arxiv.org/abs/2504.13218
- **Reference count**: 40
- **Key outcome**: Harmony achieves 37.48% and 55.07% average accuracy on EPIC-MIL and Drive&Act-MIL datasets, outperforming state-of-the-art methods by 3.7% and 4.5% respectively.

## Executive Summary
This paper introduces Modality Incremental Learning (MIL), a new paradigm where models must continuously learn from sequentially arriving data with distinct modalities, unlike traditional incremental learning that assumes consistent modalities. The proposed Harmony framework addresses MIL challenges through two key components: (1) Adaptive Compatible Feature Modulation, which generates augmented historical modality features by perturbing current modality inputs to simulate diverse historical distributions, and (2) Cumulative Modal Bridging, which injects historical modal knowledge into current learning through feature fusion and parameter adaptation, combined with hybrid alignment strategies including direct feature alignment, contrastive alignment, and distribution-level alignment. Experiments on EPIC-MIL and Drive&Act-MIL datasets show Harmony achieves 37.48% and 55.07% average accuracy respectively, outperforming state-of-the-art methods by 3.7% and 4.5%. The framework demonstrates superior multimodal classification accuracy (50.30% and 75.85%) while effectively preserving knowledge across modality transitions.

## Method Summary
Harmony addresses MIL through sequential training phases where each phase learns a new modality. The framework uses a Transformer backbone with Modality Knowledge Aggregation modules, Adaptive Compatible Feature Modulation for generating historical feature proxies, and Cumulative Modal Bridging via gated low-rank adapters. The training alternates between current modality classification and hybrid alignment objectives (direct MSE, contrastive, and distribution-level alignment) to bridge modal gaps. The method generates synthetic historical features by combining current inputs with adaptively perturbed historical classifier weights, then fuses these with current features through cross-attention and parameter-level adapter integration.

## Key Results
- Harmony achieves 37.48% average accuracy on EPIC-MIL dataset
- Harmony achieves 55.07% average accuracy on Drive&Act-MIL dataset
- Outperforms state-of-the-art methods by 3.7% and 4.5% respectively
- Demonstrates superior multimodal classification accuracy (50.30% and 75.85%)

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Compatible Feature Modulation
- **Claim:** By generating perturbation-augmented historical features from current inputs, the model creates synthetic historical representations that are compatible with both the historical model and current data.
- **Mechanism:** Historical classifier weights serve as semantic prototypes. These are combined with current features plus adaptive Gaussian noise (mixture of K distributions with learned coefficients) to simulate diverse historical feature distributions that bridge modal gaps.
- **Core assumption:** Historical classifier weights encode sufficient semantic information to reconstruct meaningful historical feature distributions; current features provide necessary structural context.
- **Evidence anchors:**
  - [abstract] "generates augmented historical modality features by perturbing current modality inputs to simulate diverse historical distributions"
  - [section 3.4] Details adaptive mixture augmentation with K=3 components and λg=0.6 perturbation intensity
  - [corpus] Weak direct evidence; neighbor papers focus on multimodal incremental learning with paired data access, not synthetic feature generation
- **Break condition:** If historical prototypes are too sparse (few classes) or current features lack semantic overlap with historical modalities, generated features may not activate relevant historical knowledge.

### Mechanism 2: Cumulative Knowledge Aggregation
- **Claim:** Low-rank gated adapters enable knowledge injection at both token-level (feature fusion via cross-attention) and parameter-level (adapter weight merging) while preserving model structure.
- **Mechanism:** Gated adapter filters historical features → cross-attention merges filtered historical tokens with current features → learned adapter weights merge into aggregation module. This dual-level accumulation enables progressive knowledge retention without architectural expansion.
- **Core assumption:** Low-rank decomposition (rank=128) captures sufficient historical knowledge; gating prevents harmful interference from mismatched historical features.
- **Evidence anchors:**
  - [abstract] "injects historical modal knowledge into current learning through feature fusion and parameter adaptation"
  - [section 3.5] Describes W_adapter = ω·B^t·A^t low-rank implementation and cross-attention token merging
  - [corpus] Neighbor paper "Multi-Modal Continual Learning via Cross-Modality Adapters" validates adapter-based approaches for multimodal continual learning
- **Break condition:** If adapter rank is insufficient for complex modalities, or gate learns to suppress all historical knowledge (ω→0), knowledge transfer fails.

### Mechanism 3: Hybrid Alignment (Three-Granularity Strategy)
- **Claim:** Combining direct (MSE), contrastive, and distribution-level alignment creates robust modal bridging by enforcing consistency at sample-pair, discriminative, and population levels.
- **Mechanism:** Direct alignment reduces local feature gaps; contrastive alignment enhances discriminative boundaries; distribution-level alignment (weighted batch aggregation) captures underlying population structure. Loss weights (λ_con=0.8, λ_dis=0.6) balance contributions.
- **Core assumption:** The three alignment objectives are complementary and don't conflict; distribution proxies (weighted batch samples) adequately represent modality distributions.
- **Evidence anchors:**
  - [abstract] "hybrid alignment strategies including direct feature alignment, contrastive alignment, and distribution-level alignment"
  - [section 3.5] Equations 7-11 define the three alignment losses and their combination
  - [table 2] Ablation shows 2.87% AA3 drop without L_align; removing individual alignment types degrades performance
  - [corpus] Neighbor papers on contrastive learning and representation alignment support multi-granularity approaches
- **Break condition:** If modalities are fundamentally incompatible (e.g., spatial vs. temporal dominant), alignment may force harmful feature distortion rather than useful bridging.

## Foundational Learning

- **Concept: Catastrophic Forgetting in Incremental Learning**
  - Why needed here: MIL faces severe forgetting because modal disparities prevent effective distillation—current data doesn't activate historical model knowledge.
  - Quick check question: Can you explain why standard LwF distillation fails when modality t inputs are fed to modality t-1 models?

- **Concept: Transformer Cross-Attention and Token Fusion**
  - Why needed here: Cumulative knowledge aggregation uses cross-attention to merge historical and current tokens; understanding attention mechanisms is essential for debugging feature fusion.
  - Quick check question: In Equation 6, which features attend to which, and what does the softmax normalize over?

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: The gated knowledge adapter uses low-rank decomposition (A^t, B^t matrices) to enable efficient parameter-level knowledge accumulation.
  - Quick check question: Why does low-rank constraint help with knowledge retention versus full-rank adaptation?

## Architecture Onboarding

- **Component map:** Preprocessing → Raw features F^t_i (L×d) → Modality Knowledge Aggregation modules (E^t_m, E^(t-1)_m) → Adaptive Compatible Feature Modulation → Gated Knowledge Adapter → Cross-Attention Fusion → Transformer Backbone (E^t_b) → Hybrid Alignment + Classification loss → Classifier Head (C^t)

- **Critical path:** Current features → Aggregation module → Adapter-gated historical fusion → Backbone → Hybrid alignment + Classification loss. The adapter weights then merge back into aggregation module for next phase.

- **Design tradeoffs:**
  - Perturbation intensity λg (0.6): Too low → insufficient diversity; too high → noise dominates semantics
  - Alignment weight λ (1.5): Too low → modal gap persists; too high → current modality learning degrades
  - Adapter rank (128): Higher captures more knowledge but increases interference risk

- **Failure signatures:**
  - AA3 high but Amulti low: Features aligned per-modality but not unified (check distribution alignment)
  - Later modalities perform poorly: Gating collapsed (ω→0), check adapter gradient flow
  - High variance across runs: Perturbation too aggressive, reduce λg or increase K

- **First 3 experiments:**
  1. **Ablation sanity check:** Train with single alignment type only (direct/contrastive/distribution) to verify each contributes; compare to full hybrid.
  2. **Modality order sensitivity:** Run RGB→Flow→Audio vs. Audio→RGB→Flow; if performance differs significantly, investigate aggregation module initialization.
  3. **Adapter rank sweep:** Test rank ∈ {32, 64, 128, 256} on EPIC-MIL; plot AA3 vs. rank to find saturation point before overfitting.

## Open Questions the Paper Calls Out

- **Question:** How does Harmony perform in open-world scenarios where the label space is not fixed or unified across modalities?
  - **Basis in paper:** [explicit] The conclusion states the intent to "extend our work to... explore modality incremental learning challenges in open-world scenarios."
  - **Why unresolved:** The current problem definition (Section 3.1) explicitly assumes all sequential tasks "share the same data label space," ignoring the complexity of evolving categories.
  - **What evidence would resolve it:** Evaluation on datasets where distinct modalities introduce novel classes not present in previous phases.

- **Question:** Can the framework maintain effectiveness when trained end-to-end on raw sensor data rather than pre-extracted features?
  - **Basis in paper:** [inferred] Section 4.3 notes that "raw features are extracted and fed into models," implying the feature encoders are not updated incrementally.
  - **Why unresolved:** Decoupling feature extraction from the incremental learning process may hide optimization challenges involved in learning raw signal representations sequentially.
  - **What evidence would resolve it:** Experiments integrating modality-specific encoders into the sequential training loop without using pre-computed features.

- **Question:** Does the sequential order of modalities significantly impact the theoretical limits of the Cumulative Modal Bridging component?
  - **Basis in paper:** [inferred] While Section 4.7 analyzes modality order, the paper acknowledges varying distributional gaps, suggesting the "bridging" difficulty might depend on specific modality pairings.
  - **Why unresolved:** The study is limited to three specific modalities per dataset, leaving the scalability of the bridging mechanism across diverse modality sequences undetermined.
  - **What evidence would resolve it:** Systematic evaluation across a wider variety of modal sequences with varying degrees of information overlap.

## Limitations

- The synthetic feature generation mechanism lacks theoretical justification for its specific perturbation configuration (K=3, λg=0.6) and depends critically on feature distribution overlap between modalities.
- The low-rank gated adapter mechanism faces potential gating collapse risks with no guarantee that gates won't suppress historical knowledge entirely when modal disparities are large.
- The framework assumes fixed label spaces across modalities, ignoring the complexity of evolving categories in open-world scenarios.

## Confidence

- **High Confidence:** Hybrid alignment effectiveness - The three-granularity approach is well-supported by ablation studies showing 2.87% AA3 degradation when removed.
- **Medium Confidence:** Adaptive Compatible Feature Modulation - While clearly described, the assumption that classifier weights can meaningfully reconstruct historical feature distributions lacks strong theoretical foundation.
- **Low Confidence:** Cumulative Knowledge Aggregation robustness - Limited analysis of adapter dynamics, gating behavior, and failure modes suggests potential brittleness in challenging scenarios.

## Next Checks

1. **Perturbation Sensitivity Analysis:** Systematically vary λg from 0.1 to 1.0 and K from 1 to 5 on both datasets, measuring AA3 and adapter gate dynamics to identify optimal configurations and stability boundaries.

2. **Gate Behavior Monitoring:** Instrument the gated adapter during training to track ω distributions across modalities and phases, verifying that gates remain dynamic rather than collapsing to fixed values.

3. **Historical Prototype Sufficiency Test:** Remove top N classes from historical prototypes (where N varies from 10% to 50%) and measure degradation in synthetic feature quality and final performance, establishing the minimum semantic coverage required for effective reconstruction.