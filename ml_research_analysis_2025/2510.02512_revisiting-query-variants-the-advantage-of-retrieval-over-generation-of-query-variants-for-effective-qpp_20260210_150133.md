---
ver: rpa2
title: 'Revisiting Query Variants: The Advantage of Retrieval Over Generation of Query
  Variants for Effective QPP'
arxiv_id: '2510.02512'
source_url: https://arxiv.org/abs/2510.02512
tags:
- query
- retrieval
- target
- which
- retrieved
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses query performance prediction (QPP) for neural
  rankers by leveraging query variants (QVs) that share similar information needs
  with the target query. Unlike existing QV-based QPP methods that generate QVs using
  query expansion or non-contextual embeddings (which can introduce topical drifts
  and hallucinations), the authors propose retrieving QVs directly from a training
  set like MS MARCO.
---

# Revisiting Query Variants: The Advantage of Retrieval Over Generation of Query Variants for Effective QPP

## Quick Facts
- arXiv ID: 2510.02512
- Source URL: https://arxiv.org/abs/2510.02512
- Authors: Fangzheng Tian; Debasis Ganguly; Craig Macdonald
- Reference count: 30
- One-line primary result: Retrieved query variants from MS MARCO significantly outperform generated variants for QPP, achieving up to 20% improvement in Kendall's τ correlation for neural rankers.

## Executive Summary
This paper addresses the challenge of Query Performance Prediction (QPP) for neural rankers by proposing a novel approach to retrieve query variants (QVs) from a training corpus rather than generating them synthetically. The key innovation is extending beyond direct (1-hop) QV retrieval by using the relevant documents of these QVs to retrieve additional (2-hop) QVs, improving recall of similar queries. Experiments on TREC DL'19 and DL'20 show that QPP methods using the proposed retrieved QVs significantly outperform existing generated-QV-based approaches, achieving up to 20% improvement in Kendall's τ correlation for neural rankers like MonoT5.

## Method Summary
The method retrieves query variants from the MS MARCO training set using a two-stage approach. First, it retrieves 1-hop QVs by matching the target query against the training query index using BM25 or SBERT. Then, for each 1-hop QV, it uses their relevant documents as pseudo-queries to retrieve 2-hop QVs. The combined set of QVs is re-ranked using RBO (Rank-Biased Overlap) similarity between their retrieval results and the target query's results. These re-ranked QVs are incorporated into a QV-based QPP framework that combines the target query's base predictor score with the weighted average of selected QVs' scores using a smoothing formula.

## Key Results
- QV-based QPP using retrieved QVs (QV-R) significantly outperforms generated QV approaches (QV-RLM, QV-W2V) on TREC DL'19 and DL'20
- 2-hop retrieval improves QPP effectiveness by capturing more semantically similar queries than 1-hop alone
- QV-R achieves up to 20% improvement in Kendall's τ correlation for neural rankers like MonoT5
- Optimal performance achieved with k=1 retrieved QV, indicating that a single highly similar QV provides the strongest signal

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Retrieved query variants (QVs) from a training set yield more reliable QPP signals than generated QVs.
- **Mechanism:** Generated QVs via query expansion (RM3) or non-contextual embeddings (Word2Vec) can introduce terms unrelated to the original information need. Retrieved QVs are actual user-formulated queries from MS MARCO, which inherently represent real information needs and avoid synthetic hallucinations.
- **Core assumption:** Real user queries in the training set adequately cover the information need space of test queries.
- **Evidence anchors:** [abstract] "Existing QV-based QPP methods generate QVs... which may introduce topical drifts and hallucinations." [Section 1] "prone to contain hallucinations and information drifts"

### Mechanism 2
- **Claim:** 2-hop QV retrieval via relevant documents improves recall of semantically similar queries.
- **Mechanism:** Short queries ambiguously represent information needs. The relevant documents associated with 1-hop QVs serve as richer representations of those queries' intents. Using these documents as pseudo-queries to retrieve 2-hop QVs captures queries that share underlying relevance patterns even without lexical overlap with the target query.
- **Core assumption:** The relevant documents in the training set (MS MARCO) accurately reflect the information need of their associated queries.
- **Evidence anchors:** [abstract] "we extend the directly retrieved QVs (1-hop QVs) by a second retrieval using their denoted relevant documents" [Section 3.2] "the root of this low recall in retrieving high-quality QVs is the ambiguity of representing an information need by a short query"

### Mechanism 3
- **Claim:** RBO-based re-ranking selects QVs whose retrieval behavior (not just text) aligns with the target query.
- **Mechanism:** RBO (Rank-Biased Overlap) measures similarity between ranked document lists. Re-ranking QVs by RBO between their top-k documents and the target query's retrieval result prioritizes QVs that would retrieve similar documents—an operational proxy for shared information need.
- **Core assumption:** Queries with similar retrieval results share similar information needs, and this similarity transfers to QPP utility.
- **Evidence anchors:** [Section 1] "re-ranking... is a more effective indicator of the similarity between information needs than simply measuring similarity between queries" [Section 3.1] "we use rank-biased overlap (RBO) similarity [7] as σ"

## Foundational Learning

- **Concept:** Query Performance Prediction (QPP)
  - **Why needed here:** The entire paper is an improvement on QV-based QPP; understanding the base task (estimating retrieval quality without relevance judgments) is essential.
  - **Quick check question:** Can you explain why NQC (a score distribution predictor) performs poorly on neural rankers?

- **Concept:** Rank-Biased Overlap (RBO)
  - **Why needed here:** RBO is the similarity measure for both re-ranking QVs and weighting their contributions in the final QPP estimate.
  - **Quick check question:** Why would RBO be preferred over Jaccard similarity for comparing ranked retrieval results?

- **Concept:** Neural vs. Lexical Retrieval Score Distributions
  - **Why needed here:** The paper explicitly separates the target retriever (neural, e.g., MonoT5) from the internal retriever (BM25) because traditional QPP predictors fail on neural score distributions.
  - **Quick check question:** What property of neural ranker scores makes variance-based predictors like NQC unreliable?

## Architecture Onboarding

- **Component map:** Target query -> Target retriever (MonoT5/BERT/TCT-ColBERT) -> Internal retriever (BM25) for QV retrieval -> Query index (MS MARCO training queries) -> 1-hop QV retrieval -> Relevant documents extraction -> 2-hop QV retrieval -> RBO re-ranking -> QV-based QPP smoothing

- **Critical path:** The 2-hop retrieval depends on having labeled relevant documents for 1-hop QVs. Without relevance labels in the training set, the system degrades to 1-hop only.

- **Design tradeoffs:**
  - **k (number of QVs):** Paper finds k=1 optimal; higher k dilutes signal with lower-RBO QVs.
  - **λ (QV weight):** Optimal λ varies significantly across test sets (DL'19 vs DL'20), suggesting sensitivity to query set characteristics.
  - **Retriever choice for QV retrieval:** SBERT outperforms BM25 in some configurations, but adds embedding overhead.

- **Failure signatures:**
  - Low Kendall's τ despite high k: QV retrieval is returning dissimilar queries; check RBO threshold or training set coverage.
  - Large variance in optimal λ across folds: Target query set may have heterogeneous difficulty; consider query-stratified tuning.

- **First 3 experiments:**
  1. **Reproduce 1-hop vs. 2-hop comparison** on TREC DL'19/20 with MonoT5 as target retriever; verify ~20% τ improvement claim.
  2. **Ablate RBO re-ranking** by using raw retrieval scores for QV selection; expect degradation per Section 3.1's rationale.
  3. **Test on out-of-domain training set** (e.g., retrieve QVs from a different corpus than MS MARCO) to probe the coverage assumption in Mechanism 1.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but identifies key limitations including the dependence on labeled relevance judgments for 2-hop retrieval and the assumption that MS MARCO training queries adequately cover test query information needs.

## Limitations
- The approach depends on labeled relevance judgments in the training set for 2-hop retrieval; without them, the system cannot expand to 2-hop QVs.
- The method assumes the MS MARCO training queries adequately cover the information need space of test queries, which may not hold for specialized domains.
- Optimal hyperparameters (k, λ) show significant variation across test sets, requiring dataset-specific tuning and suggesting sensitivity to query set characteristics.

## Confidence
- **High Confidence**: Retrieved QVs outperform generated QVs for QPP (empirical results with 20% Kendall's τ improvement are clearly demonstrated)
- **Medium Confidence**: 2-hop retrieval improves recall of semantically similar queries (mechanism is sound but lacks corpus validation)
- **Medium Confidence**: RBO similarity effectively identifies QVs with shared retrieval behavior (operational but no direct evidence for QPP utility)

## Next Checks
1. **Cross-domain generalization test**: Apply the QV-QPP framework to a test set from a different domain than MS MARCO (e.g., news queries) to evaluate the coverage assumption.
2. **Ablation of 2-hop retrieval**: Run experiments with only 1-hop QVs to quantify the contribution of the 2-hop expansion mechanism.
3. **RBO threshold sensitivity**: Systematically vary the RBO cutoff for QV selection to determine the minimum similarity threshold required for QPP improvement.