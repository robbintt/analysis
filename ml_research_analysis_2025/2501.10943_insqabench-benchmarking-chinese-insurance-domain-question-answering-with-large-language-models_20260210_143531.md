---
ver: rpa2
title: 'InsQABench: Benchmarking Chinese Insurance Domain Question Answering with
  Large Language Models'
arxiv_id: '2501.10943'
source_url: https://arxiv.org/abs/2501.10943
tags:
- insurance
- database
- answer
- clause
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces InsQABench, a comprehensive Chinese insurance
  domain QA benchmark dataset that covers three distinct tasks: Insurance Commonsense
  Knowledge, Insurance Structured Database, and Insurance Unstructured Documents.
  To tackle the challenges in these tasks, the authors propose SQL-ReAct for structured
  database queries and RAG-ReAct for unstructured document processing.'
---

# InsQABench: Benchmarking Chinese Insurance Domain Question Answering with Large Language Models

## Quick Facts
- arXiv ID: 2501.10943
- Source URL: https://arxiv.org/abs/2501.10943
- Reference count: 40
- Key outcome: Introduces InsQABench, a Chinese insurance domain QA benchmark with three tasks (Commonsense, Database, Clause), and proposes SQL-ReAct and RAG-ReAct methods that outperform standard approaches.

## Executive Summary
This paper introduces InsQABench, a comprehensive Chinese insurance domain QA benchmark dataset that covers three distinct tasks: Insurance Commonsense Knowledge, Insurance Structured Database, and Insurance Unstructured Documents. To tackle the challenges in these tasks, the authors propose SQL-ReAct for structured database queries and RAG-ReAct for unstructured document processing. Experiments show that fine-tuning large language models on InsQABench significantly improves performance on domain-specific terminology and complex clause texts. The proposed methods outperform standard approaches and achieve state-of-the-art results in the insurance domain, demonstrating the effectiveness of task-specific adaptations for specialized QA applications.

## Method Summary
The InsQABench dataset contains 95k training and 2.4k test samples across three tasks: Commonsense QA (basic insurance concepts), Database QA (structured SQL queries on insurance data), and Clause QA (retrieval from unstructured PDF documents). The authors propose SQL-ReAct, an iterative agent that refines SQL queries based on database execution feedback, and RAG-ReAct, which iteratively refines retrieval queries for document processing. All models are fine-tuned using LoRA on Baichuan2-13B, GLM4-9B, and Qwen1.5-14B with task-specific hyperparameters. The evaluation uses both rule-based metrics (ROUGE-L) and model-based scoring (GPT-4o) across accuracy, professionalism, and completeness dimensions.

## Key Results
- Fine-tuned models consistently outperform base models on all three tasks, with GLM4-9B improving from 64.40 to 70.26 in model-based accuracy on Commonsense QA
- SQL-ReAct achieves superior performance on Database QA compared to single-shot SQL generation methods
- RAG-ReAct demonstrates better answer quality for complex clause questions requiring multi-step retrieval compared to standard RAG approaches

## Why This Works (Mechanism)

### Mechanism 1: Iterative SQL Refinement with Feedback (SQL-ReAct)
- Claim: An iterative loop of SQL generation, execution, and error-based feedback improves accuracy on structured database question answering by allowing models to recover from syntax and semantic errors.
- Mechanism: The model generates a <Thought> and an initial <SQL> query. The query is executed against the database, and the result (row count, sample rows) is returned as structured feedback <exe>. The model uses this feedback to diagnose issues (e.g., empty results from a cell mismatch) and generates a refined query in the next turn, repeating until an <Answer> is ready.
- Core assumption: The model can correctly interpret database execution results and error states to perform corrective reasoning and query modification.
- Evidence anchors:
  - [section] Section 3.2 states, "SQL-ReAct employs an iterative approach to continuously refine SQL statements based on feedback from the database until the desired query result is ready."
  - [section] The paper describes the process: "The execution results undergo post-processing... providing the necessary feedback for further refinement."
  - [corpus] Related benchmarks (e.g., INSEva) focus on evaluation datasets, not on this specific iterative SQL mechanism; corpus evidence is weak for iterative refinement in this domain.
- Break condition: The mechanism fails if the database feedback is too ambiguous for the model to diagnose the error, or if the model enters a reasoning loop, repeatedly generating incorrect queries without converging on a solution.

### Mechanism 2: Query-Refining Retrieval Augmentation (RAG-ReAct)
- Claim: Iteratively refining the retrieval query based on retrieved document chunks enhances answer quality for complex, unstructured documents by compensating for initial retrieval imperfections.
- Mechanism: An initial user query retrieves a set of document chunks from a vector database. The LLM analyzes these chunks and generates a refined query to retrieve additional, potentially more relevant chunks in the next iteration. This cycle continues until the model determines it has sufficient context (signaled by `<AnswerReady>`) to synthesize a final answer with evidence.
- Core assumption: The model can reliably judge information sufficiency and effectively reformulate a query to close knowledge gaps identified in initial retrieval results.
- Evidence anchors:
  - [section] Section 3.3 details: "The LLM then uses these chunks to refine the query and retrieve additional chunks in subsequent iterations."
  - [section] The method is triggered by an `<AnswerReady>` signal, indicating a conditional, model-driven stopping criterion.
  - [corpus] Related work on RAG (MORQA) discusses evaluation metrics but does not validate this specific iterative query refinement loop.
- Break condition: The process degrades if the model prematurely signals `<AnswerReady>` based on incomplete or misleading initial chunks, or if query reformulation drifts away from the original user intent.

### Mechanism 3: Domain-Specific Supervised Fine-Tuning (SFT)
- Claim: Supervised fine-tuning on a curated, domain-specific dataset adapts general-purpose LLMs to the specialized terminology and reasoning patterns of the insurance domain.
- Mechanism: A general LLM is trained on the InsQABench dataset, which contains QA pairs for commonsense, structured data, and unstructured text. This process, performed with LoRA, adjusts model weights to increase the probability of generating correct domain-specific language and reasoning, establishing a stronger baseline before applying agentic methods.
- Core assumption: The training dataset's distribution is representative of real-world queries and contains high-quality, accurate answers that the model can generalize from.
- Evidence anchors:
  - [abstract] The paper concludes that "fine-tuning large language models on InsQABench significantly improves performance on domain-specific terminology and complex clause texts."
  - [section] Results in Section 5.1 show fine-tuned models consistently outperform base counterparts (e.g., GLM4-9B-Chat improving from 64.40 to 70.26 in model-based accuracy).
  - [corpus] Similar benchmarks (e.g., DentalBench) also report performance gains from domain-specific fine-tuning, supporting the general efficacy of this mechanism.
- Break condition: Performance gains will not appear if the fine-tuning data is of poor quality, too narrow in scope (leading to overfitting), or if the learning rate causes catastrophic forgetting of general reasoning abilities.

## Foundational Learning

**Concept: ReAct (Reasoning + Acting)**
- Why needed here: This paradigm is the core of the proposed SQL-ReAct and RAG-ReAct agents, enabling them to interleave reasoning traces with external tool use (database execution, retrieval).
- Quick check question: In a ReAct loop, what is the model's primary output after receiving an observation from an external tool?

**Concept: Low-Rank Adaptation (LoRA)**
- Why needed here: The paper uses LoRA for all fine-tuning experiments. Understanding it is crucial for replicating the efficient training process described.
- Quick check question: What are the primary practical benefits of LoRA compared to full-parameter fine-tuning?

**Concept: Dense Retrieval**
- Why needed here: The RAG-ReAct agent depends on a dual-encoder dense retriever (BGE-M3) and a vector database (Faiss) to find relevant information in unstructured text.
- Quick check question: How does a dense retriever determine the relevance of a document chunk to a user query?

## Architecture Onboarding

**Component map:**
InsQABench Dataset -> Fine-tuning Module (LoRA) -> SQL-ReAct Agent (MySQL database) + RAG-ReAct Agent (Faiss vector DB) -> Evaluation Suite (ROUGE-L + GPT-4o)

**Critical path:**
1. Parse PDFs and build the vector database for Clause QA.
2. Set up the MySQL database for Database QA.
3. Apply SFT to a base LLM using the full InsQABench dataset.
4. For inference, route queries to the appropriate agent (SQL-ReAct or RAG-ReAct) or the SFT model.
5. Collect outputs and score them using the evaluation suite.

**Design tradeoffs:**
- Iteration vs. Latency: The iterative nature of SQL-ReAct and RAG-ReAct improves accuracy but significantly increases inference time and API/database load compared to single-pass methods.
- Fine-tuning Compute vs. Performance: SFT requires upfront GPU resources (3 L40s in the paper) but yields a model that outperforms larger, non-fine-tuned proprietary models on domain tasks.
- Complexity vs. Interpretability: The agents generate explicit reasoning traces (<Thought>, <Retrieve>), which improves debuggability at the cost of a more complex architecture.

**Failure signatures:**
- SQL-ReAct: Repeatedly generating syntactically invalid SQL or queries with empty result sets, failing to converge on an answer. The model should eventually generate an answer forcibly if max iterations are reached.
- RAG-ReAct: Getting stuck in a retrieval loop, failing to emit the `<AnswerReady>` token, or generating an answer that cites evidence IDs not present in the final retrieved set.
- SFT: The model generates fluent but factually incorrect answers (hallucinations) about insurance terms not well-represented in the training data.

**First 3 experiments:**
1. **Reproduce SFT Gains:** Select one base model (e.g., Qwen1.5-14B-Chat), fine-tune it on the InsQABench dataset, and compare its zero-shot performance on the Commonsense QA test set against the non-fine-tuned baseline.
2. **Ablate SQL Iterations:** On the Database QA task, compare the fine-tuned model's performance using the SQL-ReAct agent against a "two-round" baseline (where all SQL is generated at once). Measure accuracy and average number of iterations.
3. **Validate RAG-ReAct:** On the Clause QA task, compare the RAG-ReAct agent against a standard RAG implementation (single retrieval step) using the fine-tuned model. Evaluate the accuracy and completeness of answers for questions requiring multi-step reasoning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well do SQL-ReAct and RAG-ReAct transfer to insurance domains in other languages and regulatory frameworks beyond Chinese?
- Basis in paper: [inferred] The benchmark is exclusively Chinese ("Chinese insurance sector"), with no evaluation of cross-lingual or cross-jurisdictional applicability.
- Why unresolved: Insurance terminology, regulatory structures, and clause formats differ significantly across countries; model performance may not generalize.
- What evidence would resolve it: Evaluation of the fine-tuned models on translated test sets or comparable insurance QA benchmarks in other languages (e.g., English, Japanese).

### Open Question 2
- Question: How does the quality of LLM-generated training data (via GPT-3.5 and Gemini) affect downstream model performance and potential error propagation?
- Basis in paper: [explicit] Training data for Commonsense QA is refined by GPT-3.5, and Database QA uses Gemini 1.5 Pro for question generation and answer synthesis.
- Why unresolved: LLM-generated data may contain hallucinations or subtle errors that propagate into fine-tuned models, but no analysis of noise impact is provided.
- What evidence would resolve it: Ablation studies comparing models trained on LLM-generated vs. human-verified-only data, with error categorization analysis.

### Open Question 3
- Question: How robust is the system to temporal changes in insurance products, regulations, and company data?
- Basis in paper: [inferred] The database contains information from 192 insurance companies and 25k products crawled at a fixed time; the paper notes "ever-changing regulations" as a challenge but does not address temporal updating.
- Why unresolved: Insurance policies and regulations frequently change, potentially rendering both static knowledge and database entries outdated.
- What evidence would resolve it: Longitudinal evaluation measuring performance degradation over time, or experiments with incremental database/knowledge updates.

### Open Question 4
- Question: Can a unified model effectively handle all three QA tasks simultaneously, rather than requiring task-specific fine-tuning and methods?
- Basis in paper: [inferred] The three tasks (Commonsense, Database, Clause) use different approaches (SFT, SQL-ReAct, RAG-ReAct); no unified multi-task model is explored.
- Why unresolved: A unified system would better reflect real-world scenarios where users may ask questions spanning multiple knowledge types.
- What evidence would resolve it: Experiments training a single model on mixed-task data and evaluating on all three test sets, with comparison to specialized models.

## Limitations
- Key implementation details (LoRA learning rate, Faiss index parameters, PDF parsing heuristics) are underspecified, limiting reproducibility.
- The "state-of-the-art" claim is domain-specific and not benchmarked against broader Chinese QA models or cost-efficiency metrics.
- No analysis of error propagation from LLM-generated training data or evaluation of cross-lingual/cross-jurisdictional applicability.

## Confidence

**High Confidence:** The general framework of InsQABench (three-task structure), the core design of SQL-ReAct and RAG-ReAct agents, and the observed performance gains from supervised fine-tuning are well-supported by the presented results and are consistent with established LLM training and agentic paradigms.

**Medium Confidence:** The specific numerical performance metrics (e.g., exact accuracy scores, improvements) and the relative effectiveness of LoRA hyperparameters (r=8, alpha=16) are less certain due to the unspecified learning rate and the lack of ablation studies on these critical values.

**Low Confidence:** Claims about the model's ability to handle "complex clause texts" and the generalizability of the iterative agent methods to other specialized domains are not empirically validated beyond the insurance context provided.

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Re-run the LoRA fine-tuning experiments on one base model (e.g., Qwen1.5-14B) across a range of learning rates (e.g., 1e-5, 5e-5, 1e-4) and rank values (e.g., r=4, r=8, r=16) to determine the sensitivity of performance to these settings.

2. **Cross-Domain Transfer:** Apply the SQL-ReAct and RAG-ReAct agent architectures, without modification, to a different structured database and document retrieval task (e.g., a legal or financial domain dataset) to test the generality of the iterative refinement approach.

3. **Error Analysis on Iterative Agents:** For a subset of failed or low-scoring examples from the Database and Clause QA tasks, manually analyze the agent's reasoning traces (<Thought>, <SQL>, <Retrieve>) to identify systematic failure modes (e.g., misinterpretation of feedback, poor query reformulation) and quantify their frequency.