---
ver: rpa2
title: 'Enhancing Privacy in Decentralized Min-Max Optimization: A Differentially
  Private Approach'
arxiv_id: '2508.07505'
source_url: https://arxiv.org/abs/2508.07505
tags:
- privacy
- decentralized
- min-max
- optimization
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DPMixSGD, a differentially private algorithm
  for decentralized non-convex strongly-concave min-max optimization. The method builds
  on STORM-based DM-HSGD and adds Gaussian noise to local gradients during communication
  to ensure differential privacy.
---

# Enhancing Privacy in Decentralized Min-Max Optimization: A Differentially Private Approach

## Quick Facts
- **arXiv ID**: 2508.07505
- **Source URL**: https://arxiv.org/abs/2508.07505
- **Reference count**: 40
- **Primary result**: DPMixSGD adds Gaussian noise to gradients for differential privacy while maintaining convergence in decentralized min-max optimization.

## Executive Summary
This paper introduces DPMixSGD, a novel algorithm for differentially private decentralized non-convex strongly-concave min-max optimization. The method extends STORM-based DM-HSGD by adding Gaussian noise to local gradient estimators during communication to ensure differential privacy. Theoretical analysis proves convergence with added noise while providing privacy guarantees. Experiments on logistic regression and image classification tasks demonstrate that DPMixSGD achieves competitive or better performance compared to DM-HSGD, SGDA, and DP-SGDA baselines while maintaining privacy and robustness to Deep Leakage from Gradients attacks.

## Method Summary
DPMixSGD builds on STORM-based decentralized momentum with gradient tracking to solve non-convex strongly-concave min-max problems. The key innovation is adding calibrated Gaussian noise to local gradient estimators before communication to ensure differential privacy. Agents maintain momentum estimators and gradient tracking variables to reduce variance and achieve consensus. The algorithm uses a doubly stochastic mixing matrix for communication and requires bounded gradient assumptions for privacy analysis.

## Key Results
- Achieves competitive or better AUROC scores compared to DM-HSGD, SGDA, and DP-SGDA baselines on robust logistic regression tasks
- Demonstrates robustness to Deep Leakage from Gradients (DLG) attacks through noise injection
- Provides theoretical convergence guarantees with added noise while maintaining differential privacy
- Shows privacy-utility tradeoff can be managed through careful noise calibration

## Why This Works (Mechanism)

### Mechanism 1
Injecting Gaussian noise into local gradients before communication establishes Differential Privacy by masking individual data point contributions. Each agent perturbs its local gradient estimator with noise drawn from a normal distribution, preventing reconstruction attacks like Deep Leakage from Gradients.

### Mechanism 2
The STORM framework maintains convergence rates despite noise introduction and non-convex problem structure. Recursive gradient estimators combine current stochastic gradients with correction terms from previous steps, reducing variance and counteracting noise effects.

### Mechanism 3
Gradient tracking allows agents to reach consensus on the global min-max solution despite local perturbations and data heterogeneity. Auxiliary variables track average gradients across the network, correcting drift caused by local data distributions and injected noise.

## Foundational Learning

- **Min-Max Optimization (Saddle Points)**: The algorithm solves problems of the form min_x max_y f(x,y), common in adversarial training. Standard Gradient Descent without the "Ascent" part for y fails to find saddle points.
- **Decentralized Consensus & Mixing Matrices**: Without a central server, agents must agree using only neighbor communication. The mixing matrix W determines trust between agents' updates.
- **Differential Privacy (DP)**: DP provides mathematical guarantees that an algorithm's output is not significantly affected by single data point inclusion/exclusion.

## Architecture Onboarding

- **Component map**: Agent Local State (x,y,g,h) -> Privacy Module (Gaussian noise) -> Communication Module (v,u) -> Optimizer (parameter updates)
- **Critical path**: Local Gradient Computation -> Momentum Update -> Privacy Injection -> Tracking & Consensus -> Model Update
- **Design tradeoffs**: Noise Scale (privacy vs utility), Network Topology (convergence speed vs bandwidth), Batch Size (variance reduction vs startup cost)
- **Failure signatures**: Oscillation with high learning rates, privacy breach with incorrect noise calibration, consensus failure with poor network connectivity
- **First 3 experiments**: 
  1. Vary number of agents (m) on a8a/a9a datasets
  2. Test DLG attack robustness on CIFAR-10
  3. Sweep privacy parameter θ to observe utility degradation

## Open Questions the Paper Calls Out

- Can theoretical guarantees extend to non-strongly-convex or purely non-convex-non-concave scenarios?
- Does the algorithm maintain guarantees with directed or time-varying communication graphs?
- Can bounded gradient assumptions be replaced by adaptive gradient clipping in theoretical analysis?

## Limitations

- Noise accumulation over many communication rounds may degrade convergence in practice
- Algorithm performance is sensitive to hyperparameter choices not fully specified
- Decentralized communication overhead may be bottleneck in resource-constrained environments
- Theoretical analysis limited to non-convex strongly-concave min-max problems

## Confidence

- **High**: Privacy mechanism and theoretical guarantees are mathematically sound
- **Medium**: Complex convergence proof with standard assumptions, empirical DLG robustness
- **Low**: Practical impact on large-scale real-world problems not fully explored

## Next Checks

1. Reproduce privacy budget calibration experiment varying θ and verify utility degradation matches theoretical bound
2. Monitor consensus error variance across agents to verify mixing matrix implementation
3. Derive and report explicit gradient sensitivity bound used for noise calibration in experiments