---
ver: rpa2
title: 'From Formal Language Theory to Statistical Learning: Finite Observability
  of Subregular Languages'
arxiv_id: '2509.22598'
source_url: https://arxiv.org/abs/2509.22598
tags:
- languages
- subregular
- finite
- classes
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes that all standard subregular language classes
  are linearly separable, providing a geometric foundation for their learnability.
  The authors define "finite observability," showing that membership in these classes
  depends only on finitely many primitive predicates.
---

# From Formal Language Theory to Statistical Learning: Finite Observability of Subregular Languages

## Quick Facts
- arXiv ID: 2509.22598
- Source URL: https://arxiv.org/abs/2509.22598
- Authors: Katsuhiko Hayashi; Hidetaka Kamigaito
- Reference count: 16
- All standard subregular language classes are linearly separable when represented by their deciding predicates, establishing finite observability.

## Executive Summary
This paper bridges formal language theory and statistical learning by proving that all standard subregular language classes (SL, SP, LT, PT, LTT, TSL) are linearly separable when represented by their deciding predicates. The authors introduce the concept of "finite observability," showing that membership in these classes depends only on a finite number of primitive boolean predicates. This geometric foundation enables direct application of statistical learning theory, demonstrating that simple linear models can achieve perfect classification on noise-free data while maintaining strong theoretical guarantees.

## Method Summary
The paper establishes finite observability through a theoretical framework where subregular languages are characterized by finite sets of boolean predicates (k-grams, subsequences, threshold counts, tier projections). Minterm linearization provides explicit hyperplane constructions that guarantee linear separability with margin bounds. Synthetic experiments confirm theoretical predictions, while real-data experiments on English morphology show learned features align with linguistic constraints. The work demonstrates that subregular classes can be modeled with interpretable linear classifiers while preserving the mathematical elegance of formal language theory.

## Key Results
- All standard subregular language classes (SL, SP, LT, PT, LTT, TSL) are linearly separable in Boolean feature spaces.
- Membership in subregular classes depends only on finitely many primitive predicates, establishing "finite observability."
- Synthetic experiments show 100% accuracy under noise-free conditions; real morphology experiments achieve >85% accuracy with linguistically meaningful feature interpretations.
- Margin bounds guarantee robustness, with theoretical margin ≥ 1/2 for noise-free classification.

## Why This Works (Mechanism)

### Mechanism 1: Finite Observability via Predicates
Subregular languages can be decided by a finite set of primitive predicates (k-grams, subsequences, threshold counts, tier projections), enabling linear separability. Membership is determined by boolean predicates producing truth vectors r(x) ∈ {0,1}^n. Minterm linearization constructs hyperplanes separating truth-vector profiles with margin ≥ 1/2. Core assumptions: finite alphabet Σ and fixed class parameters. Evidence: synthetic experiments show perfect separability under noise-free conditions. Break condition: unbounded alphabet or non-fixed parameters cause predicate sets to become infinite, breaking Theorem 3.2 guarantees.

### Mechanism 2: Linear Separability via Minterm Embedding
Any finitely observable class is linearly separable in Boolean feature space via minterm embedding. The construction uses explicit weights w_a = 1 if a ∈ S, else 0, and bias b = -1/2, achieving margin ≥ 1/2. This enables perceptron or linear SVMs to achieve exact classification on noise-free data. Core assumption: truth vectors faithfully represent language membership. Evidence: 100% accuracy on synthetic SL3, SP2, LTT2 experiments. Break condition: label noise degrades margins; underspecified features lose separability.

### Mechanism 3: Linguistic Interpretability via Predicate Alignment
Learned linear weights on morphological data recover linguistically meaningful constraints (affix ordering, boundary restrictions). Logistic regression with PT+LTT features achieves >85% accuracy on English morphology. Top-weighted predicates correspond to known constraints like -ly, -ness at right edge, prefix-suffix co-occurrence restrictions. Core assumption: morphological data can be approximated by subregular constraints. Evidence: F1 scores 86% and Figure 4 showing linguistically meaningful affix constraints. Break condition: constraints exceeding subregular complexity cause accuracy drops and misaligned features.

## Foundational Learning

- Concept: Finite Observability and Predicate Decidability
  - Why needed here: This is the core property distinguishing subregular classes from regular/star-free; it guarantees finite observations suffice for membership decisions.
  - Quick check question: Given a new language family, can you list a finite set of properties (predicates) that completely determine membership? If not, the family is not finitely observable.

- Concept: Minterm Embedding and One-Hot Representations
  - Why needed here: The minterm construction translates finite observability into an explicit feature space for linear classification, proving separability is achievable.
  - Quick check question: For a predicate set of size n=3, how many minterm features are there? (Answer: 2^3 = 8 one-hot indicators).

- Concept: Linear Separability and Margin
  - Why needed here: Margin provides robustness guarantees and controls mistake bounds in perceptron learning (Novikoff bound: M ≤ (1/γ)²).
  - Quick check question: If the functional margin is 1/2 and ||w||_2 = sqrt(4|S|), what is the geometric margin? (Answer: γ = 1/(2||w||_2) = 1/(4√|S|)).

## Architecture Onboarding

- Component map:
  Predicate Extractor -> Linear Classifier -> Constraint Interpreter

- Critical path:
  1. Choose subregular class (SL_k, SP_k, LTT_{k,τ}, TSL_k) and fix parameters.
  2. Instantiate corresponding predicate set from Table 1.
  3. Extract predicate features for each string in training data.
  4. Train linear classifier on labeled positive/negative examples.
  5. Evaluate accuracy and analyze top-weighted predicates for interpretability.

- Design tradeoffs:
  - Minterm embedding (exponential 2^n) vs. direct predicate features (polynomial in |Σ| and parameters). Paper recommends direct features for practical use.
  - Noise sensitivity: SP languages degrade faster under label noise than SL; consider regularization.
  - Choice of k and τ: Larger k captures longer dependencies but increases feature dimensionality exponentially in worst case.

- Failure signatures:
  - Accuracy < 100% on noise-free synthetic data: Missing deciding predicates in feature set or incorrect boundary handling.
  - Top-weighted predicates not linguistically interpretable: Data quality issues, negative example generation flaws, or constraints exceeding subregular complexity.
  - Negative margins on test set: Indicates ambiguous or irregular cases; may require data cleaning or tier projection (TSL).

- First 3 experiments:
  1. Replicate synthetic SL3/SP2/LTT2 experiments to verify 100% noise-free accuracy and observe margin degradation under label noise.
  2. Train logistic regression on English morphology data with PT+LTT features; confirm >85% accuracy and inspect top-weighted predicates against known linguistic constraints.
  3. Conduct ablation study removing boundary predicates from LTT features; quantify accuracy drop to validate theoretical importance of prefix/suffix indicators.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do modern neural networks, such as RNNs and LSTMs, effectively learn subregular structures in practice, despite their theoretical capacity for higher complexity?
- Basis in paper: The conclusion states that future work should examine "whether the effective capacity of neural models in practice aligns more closely with subregular structure than with their theoretical upper bounds."
- Why unresolved: While neural models are theoretically Turing-complete or capable of simulating pushdown automata, it remains unclear if they naturally converge on the linearly separable, subregular representations proposed here when trained on natural language.
- What evidence would resolve it: Probing studies demonstrating that neural internal representations correspond to the finite predicate feature maps defined for SL, SP, or TSL languages.

### Open Question 2
- Question: Can the concept of finite observability be extended to language classes beyond the standard subregular hierarchy?
- Basis in paper: The conclusion proposes examining "whether larger families admit analogous notions of observability."
- Why unresolved: The paper proves standard subregular classes are finitely observable, but Section 4 explicitly provides a counterexample showing the full class of regular languages (and star-free languages) cannot be captured by a fixed finite set of predicates.
- What evidence would resolve it: Identification of intermediate classes between subregular and regular that retain finite observability, or a formal proof delineating the exact boundary where the property is lost.

### Open Question 3
- Question: Can the necessary predicate sets (such as the specific tier projection T or k-value) be efficiently learned from raw data rather than fixed a priori?
- Basis in paper: The "Scope and limitations" section notes that results require fixing class parameters and that unions over unbounded parameters should be viewed as directed unions.
- Why unresolved: The theoretical guarantee holds for fixed features, but the paper does not propose a method for automatically discovering the optimal predicate basis (e.g., finding the relevant tier T for TSL languages) directly from a corpus.
- What evidence would resolve it: An algorithm that simultaneously learns the predicate parameters and the linear separator with sample complexity guarantees consistent with the theoretical margins.

## Limitations
- Theoretical guarantees rely critically on finite alphabet and fixed class parameters; violations cause finite observability to fail.
- Exponential growth of minterm features (2^n) remains a practical concern despite recommendation to use direct predicate features.
- Linguistic interpretability results rely on expert interpretation not fully validated against linguistic literature.

## Confidence

- **High confidence**: The finite observability property for standard subregular classes and its implications for linear separability in Boolean feature spaces. Synthetic experiments demonstrating perfect separability under noise-free conditions are straightforward and convincing.
- **Medium confidence**: The linguistic interpretability results from real morphological data. While accuracy is consistently above 85%, the connection between top-weighted predicates and actual linguistic constraints relies on expert interpretation.
- **Medium confidence**: The robustness claims regarding margin degradation under noise. Experimental evidence is suggestive but limited in scope, and theoretical bounds assume specific noise models not fully explored.

## Next Checks

1. **Boundary Predicate Ablation**: Systematically remove prefix/suffix boundary predicates from LTT features and quantify the exact accuracy degradation on the morphological dataset. This would validate the theoretical claim about the importance of boundary markers.

2. **Noise Model Expansion**: Extend the noise experiments beyond uniform label flips to include feature noise (character substitutions, insertions, deletions) and measure how margin distributions change across subregular classes. This would better characterize real-world robustness.

3. **Cross-linguistic Morphology Test**: Apply the PT+LTT framework to morphologically rich languages from different families (e.g., Turkish agglutination, Arabic templatic morphology) to assess whether the linguistic interpretability findings generalize beyond English affix patterns.