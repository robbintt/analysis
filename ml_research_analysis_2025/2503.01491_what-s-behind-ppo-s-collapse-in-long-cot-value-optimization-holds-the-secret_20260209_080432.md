---
ver: rpa2
title: What's Behind PPO's Collapse in Long-CoT? Value Optimization Holds the Secret
arxiv_id: '2503.01491'
source_url: https://arxiv.org/abs/2503.01491
tags:
- value
- policy
- reward
- tasks
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies the key challenges that cause PPO to fail
  in long CoT tasks: value initialization bias and reward signal decay. To address
  these issues, the authors propose Value-Calibrated PPO (VC-PPO), which incorporates
  value pretraining to align the value model with the initial policy and decoupled
  GAE computation to better balance the bias-variance trade-off for the policy and
  value models.'
---

# What's Behind PPO's Collapse in Long-CoT? Value Optimization Holds the Secret

## Quick Facts
- arXiv ID: 2503.01491
- Source URL: https://arxiv.org/abs/2503.01491
- Authors: Yufeng Yuan; Yu Yue; Ruofei Zhu; Tiantian Fan; Lin Yan
- Reference count: 24
- Primary result: VC-PPO addresses PPO failure in long CoT by fixing value initialization bias and reward decay, achieving higher AIME pass@1 than prior work.

## Executive Summary
This paper identifies the key challenges that cause PPO to fail in long CoT tasks: value initialization bias and reward signal decay. To address these issues, the authors propose Value-Calibrated PPO (VC-PPO), which incorporates value pretraining to align the value model with the initial policy and decoupled GAE computation to better balance the bias-variance trade-off for the policy and value models. Experimental results on the AIME dataset show that VC-PPO significantly improves PPO performance, achieving a higher score than previously reported. Ablation studies demonstrate that both value pretraining and decoupled GAE are essential for enhancing PPO in long CoT tasks. The study also reveals differences in variance-bias preferences between value and policy models, providing insights for future RL and RLHF research.

## Method Summary
VC-PPO addresses PPO failure in long CoT by aligning the value model with the initial policy through offline value pretraining, and by using decoupled GAE computation with different λ values for policy and value updates. The method involves a cold-start SFT phase with CoT-formatted data, followed by value pretraining using Monte-Carlo returns, and then RL training with decoupled GAE (λ_actor ∈ [0.95, 1.0), λ_critic = 1.0). The approach is tested on AIME math problems using Qwen2.5-32B with sparse terminal rewards.

## Key Results
- VC-PPO significantly improves PPO performance on AIME, achieving higher pass@1 than previously reported results.
- Ablation studies show that both value pretraining and decoupled GAE are essential for enhancing PPO in long CoT tasks.
- The study reveals differences in variance-bias preferences between value and policy models, with value models favoring high-variance targets and policy models preferring lower variance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Initializing the value model from a reward model creates position-correlated advantage bias that causes output length collapse in long CoT tasks.
- Mechanism: Reward models are trained to score only at the `<EOS>` token. When reused as value initializations, earlier tokens receive lower implicit scores due to perceived "incompleteness." GAE then accumulates this as positive advantage bias toward preceding tokens, incentivizing the policy to terminate early.
- Core assumption: The correlation between token position and advantage magnitude directly drives length collapse, rather than other optimization dynamics.
- Evidence anchors:
  - [abstract]: "This paper identifies that value initialization bias and reward signal decay are the root causes of PPO's failure."
  - [Section 3.2]: "The root cause of the positive bias lies in the objective mismatch between the reward model and the value model... This explains why tokens that are preceding tend to exhibit a greater positive bias in advantages."
  - [corpus]: Weak direct support; related papers focus on CoT distillation and merging, not PPO failure analysis.
- Break condition: If length collapse persists after value pretraining, this mechanism may not be the primary driver.

### Mechanism 2
- Claim: Using GAE with λ < 1.0 causes exponential decay of terminal reward signals, preventing early tokens in long trajectories from receiving meaningful learning signal.
- Mechanism: In sparse-reward RLHF (reward only at trajectory end), the signal propagates as λ^(T-t) × r_terminal. For long CoT where T-t is large, λ^large → 0, leaving early reasoning steps untrained.
- Core assumption: The primary bottleneck is signal propagation rather than exploration or credit assignment complexity.
- Evidence anchors:
  - [abstract]: "...decoupled GAE computation to better balance the bias-variance trade-off for the policy and value models."
  - [Section 3.3]: "When T − t is large, the resulting reward signal is essentially zero... Therefore, optimizing the value in an unbiased manner outweighs learning it in a variance-reduced way."
  - [corpus]: No direct corpus support for GAE decay mechanism specifically.
- Break condition: If switching to λ=1.0 for critic does not improve performance, reward decay may not be the bottleneck.

### Mechanism 3
- Claim: Policy and value models have fundamentally different variance-bias tolerance profiles, justifying separate λ configurations.
- Mechanism: Value optimization (MSE regression) is robust to variance but sensitive to bias; policy optimization (policy gradient) is variance-sensitive. Decoupled GAE allows λ_critic=1.0 (unbiased, high-variance) while keeping λ_actor≈0.95 (lower variance).
- Core assumption: The mathematical justification (Equation 8) that arbitrary value functions don't bias policy gradients holds in practice for LLM scales.
- Evidence anchors:
  - [Section 3.3]: "Using a value function obtained with a different λ from the policy is mathematically justifiable without introducing additional bias."
  - [Table 4]: λ_actor values < 1.0 significantly outperform λ_actor=1.0, supporting variance-sensitivity for policy.
  - [corpus]: VAPO paper references similar value-based optimization for reasoning, suggesting independent convergence on related ideas.
- Break condition: If unified λ produces comparable results with lower complexity, the decoupling benefit may be marginal.

## Foundational Learning

- Concept: Generalized Advantage Estimation (GAE) and the λ parameter
  - Why needed here: The entire VC-PPO contribution centers on manipulating λ differently for actor and critic.
  - Quick check question: Can you explain why λ=1.0 gives unbiased returns but higher variance, while λ<1.0 introduces bias?

- Concept: Value function vs. reward model distinction in RLHF
  - Why needed here: The failure mode stems from conflating these two objectives during initialization.
  - Quick check question: What does a reward model predict vs. what does a value function predict at non-terminal tokens?

- Concept: PPO clipped surrogate objective
  - Why needed here: VC-PPO modifies PPO internals without changing the core objective; understanding PPO is prerequisite.
  - Quick check question: Why does PPO clip the probability ratio, and what happens if advantages are systematically biased?

## Architecture Onboarding

- Component map:
  - Value Pretraining Module -> Decoupled GAE Calculator -> Standard PPO Loop

- Critical path:
  1. Complete SFT cold-start with CoT format (e.g., `...` tags).
  2. Run Value Pretraining until explained variance stabilizes (paper suggests ~100 steps).
  3. Initialize RL training with pretrained value checkpoint.
  4. Configure λ_actor ∈ [0.95, 1.0), λ_critic = 1.0.

- Design tradeoffs:
  - Value pretraining steps: Too few → residual bias; too many → overfitting (Table 3 shows peak at 100 steps).
  - λ_actor selection: Lower variance (0.9) vs. stability (0.95-0.99); AIME benefits from 0.99, other tasks vary.
  - Computational cost: Value pretraining adds upfront offline phase; decoupled GAE adds minimal runtime overhead.

- Failure signatures:
  - Output length drops rapidly in first 10-20 RL steps → check value initialization alignment.
  - Early-token advantages systematically higher than late-token → value model not aligned with policy prior.
  - Value loss oscillates without converging → consider increasing λ_critic or reducing learning rate.

- First 3 experiments:
  1. Reproduce baseline PPO failure on AIME subset: confirm length collapse correlates with position-biased advantages (visualize Figure 2 equivalent).
  2. Ablate value pretraining alone: measure advantage distribution shift and output length stability.
  3. Ablate decoupled GAE: compare λ_critic=1.0 vs. λ_critic=0.95 while holding λ_actor fixed; monitor value loss convergence and final pass@1.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the variance-bias preference fundamentally differ between regression-style (value) and policy-gradient (policy) objectives, and can this asymmetry be theoretically formalized?
- Basis in paper: [explicit] The discussion states that value optimization dynamics are more tolerant to variance than policy optimization and suggests this difference "could serve as a promising avenue for further research in RL or RLHF."
- Why unresolved: The paper empirically observes that value models favor high-variance ($\lambda=1.0$) targets while policies prefer lower variance, but does not provide a theoretical derivation for this difference in tolerance.
- What evidence would resolve it: A theoretical analysis quantifying the variance sensitivity of MSE value loss versus surrogate policy loss, supported by experiments across diverse RL tasks.

### Open Question 2
- Question: Does the high-variance, unbiased value target ($\lambda_{critic}=1.0$) in VC-PPO remain effective when using dense, neural reward models that are susceptible to reward hacking?
- Basis in paper: [inferred] The authors utilize sparse, rule-based rewards specifically because they "cannot be hacked," and set the KL penalty to 0. It is unclear if the high-variance gradient updates from $\lambda=1.0$ would exacerbate instability or over-optimization on dense neural rewards.
- Why unresolved: The experimental scope is limited to verifiable reasoning tasks (math/code) with sparse rewards, avoiding the complexities of standard RLHF with neural preference models.
- What evidence would resolve it: Experiments applying VC-PPO to standard RLHF benchmarks (e.g., summarization or chat) using neural reward models, measuring performance against reward hacking metrics.

### Open Question 3
- Question: Can the value initialization bias be prevented through architectural modifications, such as initializing the value head from the SFT policy rather than the reward model, thereby removing the need for offline value pretraining?
- Basis in paper: [inferred] The paper identifies the "objective mismatch" between the reward model (trained on EOS) and value model as the root cause of bias, but addresses it only via procedural pretraining.
- Why unresolved: The study confirms that standard initialization causes failure and pretraining fixes it, but does not explore if alternative weight initializations could align the value and policy models from the start.
- What evidence would resolve it: An ablation study comparing VC-PPO against a PPO variant initialized with a random or policy-derived value head, analyzing the resulting initial advantage distributions.

## Limitations
- Evidence granularity gap: Most claims rest on indirect comparisons rather than quantitative breakdowns of each mechanism's contribution.
- Architecture ambiguity: Critical implementation details like value model initialization and exact GAE parameter tuning remain underspecified.
- Generalization uncertainty: The method's efficacy on other long CoT domains beyond mathematical reasoning with sparse terminal rewards remains untested.

## Confidence
**High confidence**: The existence of PPO failure modes in long CoT tasks. The mathematical justification for decoupled GAE. The overall improvement trend in AIME results.
**Medium confidence**: The specific attribution of failure to value initialization bias versus reward signal decay. The quantitative importance of each VC-PPO component relative to others. The optimal λ_actor value.
**Low confidence**: The mechanism by which reward models create position-correlated advantage bias in practice. Whether the 100-step pretraining duration is universally optimal. The claimed fundamental difference in variance-bias tolerances between value and policy models.

## Next Checks
1. **Mechanism isolation ablation**: Run controlled experiments where value pretraining and decoupled GAE are applied independently. Measure: (a) output length distribution, (b) advantage position correlation, (c) value loss convergence curves.
2. **Cross-domain robustness test**: Apply VC-PPO to GPQA and Codeforces as mentioned but not reported. Compare performance against AIME to determine if the value initialization bias mechanism is task-agnostic.
3. **Variance sensitivity analysis**: Systematically vary λ_actor across [0.90, 0.95, 0.98, 0.99, 1.0] while holding all else constant. Plot pass@1 vs. advantage variance magnitude to empirically validate the claimed variance-bias trade-off preference differences.