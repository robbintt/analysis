---
ver: rpa2
title: Privacy-Preserving Inference for Quantized BERT Models
arxiv_id: '2508.01636'
source_url: https://arxiv.org/abs/2508.01636
tags:
- lookup
- table
- bits
- input
- quantized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a privacy-preserving inference framework for
  quantized BERT models using secure multi-party computation (MPC). The authors address
  the challenge of efficient secure inference for large language models, where traditional
  MPC suffers from high communication and computation overhead due to floating-point
  operations.
---

# Privacy-Preserving Inference for Quantized BERT Models

## Quick Facts
- arXiv ID: 2508.01636
- Source URL: https://arxiv.org/abs/2508.01636
- Authors: Tianpei Lu; Bingsheng Zhang; Lekun Peng; Bowen Zheng; Lichun Li; Kui Ren
- Reference count: 6
- Primary result: Up to 8× speedup over prior MPC BERT inference methods with 82.9% GLUE accuracy using 1-bit weights and 4-bit activations

## Executive Summary
This paper presents a privacy-preserving inference framework for quantized BERT models using secure multi-party computation (MPC). The authors address the challenge of efficient secure inference for large language models, where traditional MPC suffers from high communication and computation overhead due to floating-point operations. They propose a fine-grained, layer-wise quantization scheme with 1-bit weights and 4-bit activations, combined with a multi-input lookup table protocol for efficient softmax evaluation. The approach uses dual secret sharing schemes with lookup table-based precision conversion to eliminate truncation overhead entirely. Experimental evaluation demonstrates significant performance improvements: up to 8× speedup compared to Lu et al. (NDSS 25), 9× speedup compared to Gupta et al. (PETS 24), and 22× speedup compared to Knott et al. (NeurIPS 21). The method achieves 82.9 average accuracy on GLUE benchmark tasks with 1-4 bit precision, while reducing online communication costs by 885-1778× compared to CrypTen and 9.8-11.8× compared to Sigma.

## Method Summary
The framework implements 3-party MPC inference for BERT-base using replicated secret sharing (RSS) for linear layers and 2-party additive sharing with lookup tables for nonlinear operations. The approach employs fine-grained layer-wise quantization with 1-bit weights (binary {-1, 1}) and 4-bit signed activations, reducing the arithmetic domain and enabling efficient computation. Knowledge distillation from a full-precision teacher model recovers accuracy lost to quantization. The core innovation is eliminating truncation overhead through lookup table-based precision conversion: instead of probabilistic truncation with high-bit margin errors, the framework directly extracts significant bits and uses LUTs to expand back to target precision. A novel multi-input lookup table protocol with dual offsets enables efficient softmax evaluation without expensive share concatenation. The offline phase generates shifted lookup tables for all nonlinear functions, while the online phase performs secret-shared computation through RSS for matrix multiplications and additive sharing for LUT-based nonlinear operations.

## Key Results
- Achieves 82.9% average accuracy on GLUE benchmark with 1-bit weights and 4-bit activations (4.3% drop from 87.2% full precision)
- Reduces online communication costs by 885-1778× compared to CrypTen and 9.8-11.8× compared to Sigma
- Provides up to 8× speedup over Lu et al. (NDSS 25), 9× over Gupta et al. (PETS 24), and 22× over Knott et al. (NeurIPS 21)
- Maintains practical latency: 16.5ms online inference time for sequence length 16 on LAN

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-grained layer-wise quantization (1-bit weights, 4-bit activations) enables efficient MPC inference with acceptable accuracy loss.
- Mechanism: Binary weights (W ∈ {−1, 1}) and 4-bit signed activations (x ∈ {−8, ..., 7}) reduce the arithmetic domain. Inner products over N elements require only 4 + log₂N bits (e.g., 16 bits for N=768). Knowledge distillation recovers accuracy lost to quantization.
- Core assumption: The accuracy-efficiency trade-off at 4-bit activation is acceptable for target applications.
- Evidence anchors:
  - [abstract] "fine-grained, layer-wise quantization scheme with 1-bit weights and 4-bit activations"
  - [Page 3] "we found that 4-bit activation offers the best trade-off between model performance and efficiency"
  - [Page 6, Table 1] Accuracy drops from 87.2% (full precision) to 82.9% (1-4 bit)
- Break condition: If target accuracy requires >85% on GLUE, 4-bit may be insufficient; higher precision or architectural changes needed.

### Mechanism 2
- Claim: Multi-input lookup table protocol with dual offsets enables efficient nonlinear function evaluation (e.g., Softmax) without expensive share concatenation.
- Mechanism: Instead of converting two ℓ′/2-bit shares to ℓ′-bit shares and concatenating, the protocol uses two offsets (∆, ∆′): a global offset for outer blocks and a local offset within each block. Parties reveal δ = x − ∆ and δ′ = y − ∆′ separately, then index T(δ·2^(ℓ′/2) + δ′). Communication cost matches single-input LUT.
- Core assumption: Opening δ and δ′ separately does not leak information about x, y individually (masked by random offsets).
- Evidence anchors:
  - [abstract] "multi-input lookup table protocol to evaluate softmax efficiently and securely"
  - [Page 4, Algorithm 2] Full protocol specification for Π^(ℓ′/2,ℓ′/2)_look
  - [corpus] No directly comparable multi-input LUT mechanisms found in neighbor papers; this appears novel to this work.
- Break condition: If security analysis requires stricter indistinguishability guarantees, the dual-offset approach may need formal verification.

### Mechanism 3
- Claim: Lookup table-based precision conversion eliminates truncation overhead entirely by avoiding probabilistic truncation errors.
- Mechanism: After multiplication, instead of truncating via division (which requires high-order margin bits), the framework (1) extracts significant bits directly (e.g., top 4 bits from 16), then (2) uses LUT to expand back to target precision. For RSS ↔ additive sharing conversion, reshare step reconstructs RSS shares from additive shares.
- Core assumption: Direct bit extraction followed by LUT expansion preserves sufficient numerical fidelity for quantized inference.
- Evidence anchors:
  - [abstract] "eliminating truncation overhead entirely"
  - [Page 2] "we truncate by discarding the lower bits... and then use a lookup table to expand the 4-bit value back to 16 bits. This avoids high-bit errors caused by truncation"
  - [Page 5] Π^(ℓ′,ℓ)_convert procedure for JxK_ℓ′ → ⟨x⟩_ℓ conversion
- Break condition: If intermediate values exceed designed bit-width (e.g., matrix summation overflow), additional margin or clamping required.

## Foundational Learning

- **Replicated Secret Sharing (RSS)**:
  - Why needed here: Enables constant-communication inner products for matrix multiplication; each party holds two of three shares.
  - Quick check question: In 3PC-RSS, if ⟨x⟩₀ + ⟨x⟩₁ + ⟨x⟩₂ = x mod 2^ℓ, which parties hold ⟨x⟩₁?

- **Additive Secret Sharing over Rings**:
  - Why needed here: Used for lookup table evaluation; two-party shares support efficient LUT indexing after offset reveal.
  - Quick check question: Given JxK₁ + JxK₂ = x mod 2^16, what is the communication cost to reveal x?

- **Quantization-Aware Training with Distillation**:
  - Why needed here: 1-bit weights and 4-bit activations require specialized training to recover accuracy.
  - Quick check question: Why does knowledge distillation help quantized models maintain accuracy compared to direct quantization?

## Architecture Onboarding

- **Component map**:
  - P₀ (Model Owner) -> P₁ (Data Owner) -> P₂ (Computing Assistant)
  - P₀: Holds quantized weights (1-bit), generates lookup tables offline, reveals embedding parameters publicly
  - P₁: Holds input data, performs local embedding + 4-bit quantization, holds RSS and additive shares
  - P₂: Holds auxiliary shares, participates in RSS multiplication and LUT evaluation

- **Critical path**:
  1. Offline: P₀ generates and distributes offset-shifted lookup tables for all nonlinear functions
  2. Online: Input quantized to 4-bit → additive sharing → convert to RSS → linear computation → extract significant bits → LUT for nonlinear → repeat per layer
  3. Softmax: Π_max → local subtraction → exponential LUT → sum → division LUT

- **Design tradeoffs**:
  - Accuracy vs. efficiency: 4-bit activation chosen empirically; 8-bit would double communication for linear layers
  - RSS vs. additive: RSS for multiplication (output-dimension-dependent communication), additive for LUT (simple reveal-and-index)
  - Offline vs. online: LUTs shifted offline; online phase only reveals masked indices

- **Failure signatures**:
  - Accuracy degradation: If GLUE score <80%, check quantization scaling factors or increase distillation epochs
  - Overflow in inner products: If N > 4096, 4 + log₂N may exceed 16 bits; increase ring size
  - LUT miss: If δ or δ′ exceeds 2^(ℓ′/2), check offset generation correctness

- **First 3 experiments**:
  1. Reproduce Table 1 GLUE accuracy: Train 1-4 bit quantized BERT with distillation; compare against full-precision baseline
  2. Benchmark communication cost: Measure online/offline bytes for sequence lengths 8, 16, 32, 64 against reported values in Table 4
  3. Ablate LUT-based precision conversion: Replace Π^(4,16)_convert with probabilistic truncation; measure accuracy loss and latency change

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework be extended to provide security against malicious adversaries while maintaining the 8× speedup over semi-honest baselines?
- Basis in paper: [explicit] The paper states: "Our BERT inference framework ensures privacy in the semi-honest adversarial setting, meaning that as long as all parties follow the protocol, no party can obtain any information about the model weights or the input tokens."
- Why unresolved: Malicious security requires authenticated secret sharing and zero-knowledge proofs of correct computation, which would add substantial overhead to the lookup table protocols and inner product computations that currently achieve efficiency through simpler semi-honest assumptions.
- What evidence would resolve it: An implementation demonstrating malicious security with benchmarked latency and communication costs comparable to current semi-honest performance.

### Open Question 2
- Question: How does the framework's efficiency and accuracy scale to larger transformer models (e.g., BERT-large, GPT-style architectures) beyond the 110M parameter BERT-base tested?
- Basis in paper: [inferred] The paper evaluates only on "BERT-base with 12 transformer layers and over 110 million parameters" and notes that "MPC-based inference can generate several gigabytes of communication, creating barriers for real-world deployment."
- Why unresolved: Larger models have different layer dimension ratios, attention patterns, and may exhibit different quantization sensitivity, particularly for the 1-bit weight constraint that showed task-specific variance (RTE accuracy dropped from 87.2% to 58.8%).
- What evidence would resolve it: Benchmarks on larger model architectures showing GLUE or equivalent benchmark accuracy and end-to-end inference latency at various sequence lengths.

### Open Question 3
- Question: What is the privacy-utility trade-off when keeping embedding parameters private versus the current design that "publicly reveals the embedding parameters"?
- Basis in paper: [explicit] The paper states: "In this framework, the model owner publicly reveals the embedding parameters."
- Why unresolved: Embedding parameters encode semantic relationships in the input vocabulary and may leak information about training data characteristics or model behavior, yet privatizing them would require additional MPC operations at the embedding lookup stage.
- What evidence would resolve it: A formal privacy analysis quantifying information leakage from public embeddings, combined with performance benchmarks of a modified protocol with private embeddings.

## Limitations

- Knowledge distillation hyperparameters (learning rate, epochs, temperature, loss weighting) are referenced but not fully specified, limiting exact reproduction
- The 4-bit activation precision represents a significant accuracy-efficiency trade-off that may not generalize across all NLP tasks or domains
- The RSS-to-additive sharing conversion and dual-offset multi-input LUT protocol require formal security analysis to verify privacy guarantees

## Confidence

**High Confidence**: Performance improvements (8× vs Lu et al., 9× vs Gupta et al., 22× vs Knott et al.) - These are empirical measurements with clear baselines and metrics, directly comparable across works.

**Medium Confidence**: Accuracy preservation with 1-4 bit quantization (82.9% on GLUE) - Based on distillation training, but sensitive to hyperparameter choices not fully specified.

**Medium Confidence**: Communication reduction (885-1778× vs CrypTen, 9.8-11.8× vs Sigma) - Relative improvements are clear, but absolute communication costs depend on implementation details.

**Low Confidence**: Security guarantees of multi-input LUT protocol - Novel approach without comparative analysis or formal proof of security properties.

## Next Checks

1. **Security Formalization**: Conduct formal security analysis of the dual-offset multi-input LUT protocol to verify that revealing δ and δ′ separately does not leak information about individual inputs x and y, particularly under active adversary models.

2. **Hyperparameter Sensitivity Analysis**: Systematically vary knowledge distillation hyperparameters (temperature, learning rate, epochs) to determine the sensitivity of the 82.9% GLUE accuracy result and establish robustness margins for the quantization scheme.

3. **Generalization Testing**: Evaluate the framework on diverse NLP tasks beyond GLUE (e.g., SQuAD, SuperGLUE) and different model architectures (BERT-large, RoBERTa) to assess whether the 4-bit activation precision remains optimal across domains and whether accuracy degradation becomes prohibitive.