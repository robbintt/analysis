---
ver: rpa2
title: 'Capabilities and Evaluation Biases of Large Language Models in Classical Chinese
  Poetry Generation: A Case Study on Tang Poetry'
arxiv_id: '2510.15313'
source_url: https://arxiv.org/abs/2510.15313
tags:
- uni0000004a
- uni00000005
- uni00000054
- uni00000053
- uni00000057
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates large language models (LLMs)
  for generating and judging classical Chinese Tang poetry. A three-step framework
  combining computational feature extraction, LLM-as-a-judge cross-evaluation, and
  human expert validation was applied to six state-of-the-art models.
---

# Capabilities and Evaluation Biases of Large Language Models in Classical Chinese Poetry Generation: A Case Study on Tang Poetry

## Quick Facts
- arXiv ID: 2510.15313
- Source URL: https://arxiv.org/abs/2510.15313
- Reference count: 40
- Key outcome: LLM judges converge on flawed standards, missing prosodic violations that human experts catch

## Executive Summary
This study systematically evaluates six state-of-the-art LLMs for generating classical Chinese Tang poetry using a three-step framework: computational metrics, LLM-as-a-judge cross-evaluation, and human expert validation. The results reveal a critical "echo chamber" effect: automated judges converge on flawed evaluation standards that diverge sharply from human assessments. While models produce fluent poetry, they systematically overlook structural and prosodic violations. Human experts found that models like Gemma suffered severe prosodic violations unnoticed by LLM judges, while Qwen performed best technically but received low automated scores. This highlights the need for hybrid human-AI validation in culturally and technically complex creative tasks.

## Method Summary
The study evaluated six instruction-tuned LLMs (DeepSeek-V2-Lite, Qwen2.5-7B, GLM-4-9B, Mistral-7B, Baichuan2-7B, Gemma-2-9B) on Tang poetry generation across five dimensions. A three-step evaluation framework was employed: (1) computational metrics (Shannon entropy, TF-IDF, semantic similarity), (2) LLM-as-judge cross-evaluation in a 6x6 matrix, and (3) human expert validation. Generation used prompts with temperature T=0.4, followed by a two-stage cleaning pipeline (Qwen2.5 extractor then regex fallback). LLM evaluation used T=0.2 with JSON output requirements. The study generated approximately 15,000 poems total across all models and conditions.

## Key Results
- LLM judges exhibit an "echo chamber" effect, converging on flawed standards that miss prosodic violations
- Qwen performed best technically but received low automated scores due to evaluator bias
- Gemma showed severe prosodic violations unnoticed by LLM judges but caught by human experts
- Poetic Form prompts yielded highest Information Gain (entropy reduction), while Theme prompts showed minimal constraint
- Cross-model semantic similarity scores exceeded 0.94, indicating shared representation rooted in overlapping training data

## Why This Works (Mechanism)

### Mechanism 1: The Evaluation "Echo Chamber"
If LLMs are used as sole evaluators for creative generation, they likely converge on "shared yet inaccurate standards" rather than ground truth, inflating scores for models that share their training biases. Models evaluate based on semantic plausibility and training distribution rather than strict structural rules. Because models share overlapping training data (semantic similarity >0.94), they reinforce each other's errors—specifically overlooking prosodic violations in favor of fluent content. This mechanism fails if human evaluators also prioritize semantic fluency over structural correctness, or if the evaluating LLMs are specifically fine-tuned on the constraint rules (e.g., tonal prosody).

### Mechanism 2: Dimensional Disentanglement via Information Gain
Explicit structural prompts (Form) significantly reduce lexical uncertainty (entropy) and improve adherence, whereas semantic prompts (Theme/Emotion) fail to effectively constrain the model's vocabulary. The study observes that "Poetic Form" yields the highest Information Gain (largest entropy drop), forcing the model into a narrower, rule-bound search space. In contrast, "Theme" prompts show minimal Information Gain, leading models to rely on generic, high-frequency vocabulary rather than nuanced stylistic markers. This breaks if the model has been specifically fine-tuned on thematic style-transfer, allowing it to map abstract themes to specific low-entropy vocabularies.

### Mechanism 3: Hybrid Calibration of Creative Quality
Reliable evaluation of culturally complex creative tasks requires a three-step pipeline because single metrics capture orthogonal quality dimensions that do not correlate. Computational metrics detect template copying or "template-driven" strategies (low entropy). LLM-as-judge detects semantic incoherence but is blind to cultural/prosodic "blind spots." Human expert validation is required to catch the specific cultural deficits (e.g., the missing "willow-sadness" association). This framework is inefficient if a unified "Expert-Level" LLM evaluator can be trained to detect prosodic errors as accurately as humans.

## Foundational Learning

- **Concept: Shannon Entropy (in NLP)**
  - **Why needed here:** Used to distinguish "diverse" generation (high entropy) from "template-driven" or repetitive generation (low entropy). The paper relies on this to classify model strategies (e.g., Baichuan vs. GLM).
  - **Quick check question:** If a model generates the same word 100 times out of 100 tokens, is the entropy high or low? (Answer: Low).

- **Concept: TF-IDF (Term Frequency-Inverse Document Frequency)**
  - **Why needed here:** Used to create "lexical fingerprints" to prove that while models share a baseline lexicon, they have distinct stylistic preferences (e.g., Baichuan favors "ten thousand miles").
  - **Quick check question:** Does a high TF-IDF score mean a word is common in the whole corpus or just specific to one document/model? (Answer: Specific to one document/model).

- **Concept: Prosodic Constraints (Ping-Ze/Tonal Patterns)**
  - **Why needed here:** The core failure mode identified is the violation of strict tonal/rhyme rules in classical poetry. Understanding that "fluency ≠ prosody" is essential to interpreting why LLMs failed to penalize Gemma.
  - **Quick check question:** Can a poem be semantically meaningful but prosodically wrong? (Answer: Yes, this is the primary error mode observed).

## Architecture Onboarding

- **Component map:** Generation Layer (6 LLMs) -> Cleaning Module (Qwen2.5 + Regex) -> Evaluation Pipeline (Computational Metrics -> LLM-as-Judge -> Human Expert)
- **Critical path:** The Prompt Engineering -> Cleaning interface is fragile. Raw outputs contain prefixes that break evaluation scripts. The paper uses a two-stage cleaner (Model-based then Regex fallback) which is the likely bottleneck for data quality.
- **Design tradeoffs:** TF-IDF vs. Embeddings: The paper uses TF-IDF for stylistic fingerprinting (interpretable) but Sentence Embeddings for semantic coherence (robust). Do not swap these; embeddings obscure specific keyword usage. Temperature: Generation uses T=0.4 (creative but constrained), while Evaluation uses T=0.2 (deterministic). Raising generation temperature >0.6 likely triggers the "entropy paradox" (diversity without quality).
- **Failure signatures:** Cross-lingual Contamination: Gemma outputting English words (e.g., "blown") in Chinese verse. Hallucinated Commentary: DeepSeek generating *analysis* of a poem instead of the poem itself. The "Baichuan" Pattern: Extremely low entropy combined with high information gain on "Historical" themes indicates a hardcoded template, not generative AI.
- **First 3 experiments:** Run the Entropy/Theme check: Generate 50 poems with "Historical" vs. "Landscape" themes. Plot entropy. If the lines are flat (no entropy drop), the model ignores theme constraints (likely failure). Semantic Similarity Matrix: Generate poems for 5 distinct poets. Compute cosine similarity. If similarity > 0.96, the model is stylistically blind (cannot distinguish Li Bai from Du Fu). Prosodic Stress Test: Explicitly prompt for a "Five-character Regulated Verse" and check line lengths. If the model outputs 7 characters or irregular lengths (like Gemma), it fails structural adherence immediately.

## Open Questions the Paper Calls Out

- **Can evaluation metrics specifically designed for poetic and metaphorical language—beyond embeddings trained on prose—reliably capture structural fidelity and cultural nuance in classical poetry generation?**
  - **Basis in paper:** The limitations section states: "The computational metrics we employed... were limited by their reliance on models trained primarily on prosaic text. Future work should develop evaluation metrics specifically designed for poetic and metaphorical language..."
  - **Why unresolved:** Current embedding-based metrics (e.g., semantic similarity) fail to assess prosodic rules and cultural associations (e.g., the willow-sadness motif), limiting automated evaluation reliability.
  - **What evidence would resolve it:** Development and validation of a metric trained on classical poetic corpora that shows significant correlation with human expert judgments on prosodic and cultural dimensions.

- **Does the "echo chamber" effect in LLM-as-a-judge evaluation generalize across other culturally complex creative domains and languages?**
  - **Basis in paper:** The limitations section notes: "The focus on classical Chinese poetry... could have the limits the generalizability of findings to other creative domains in different languages. Future research should extend this methodological approach to other languages in digital literacy domains..."
  - **Why unresolved:** The study only examines Tang poetry; it is unknown if cross-model consensus similarly diverges from human judgment in other creative traditions (e.g., Persian ghazal, English sonnet).
  - **What evidence would resolve it:** Applying the three-step framework to another classical poetry form in a different language, measuring the agreement between LLM judges and human experts.

- **What training or prompting interventions can mitigate the systematic "echo chamber" biases and shared blind spots (e.g., overlooking prosodic violations) in cross-model LLM evaluation?**
  - **Basis in paper:** While the paper identifies the echo chamber effect and suggests "diversify training and evaluation data to reduce shared blind spots" (Discussion), no specific intervention is tested.
  - **Why unresolved:** The paper demonstrates the problem but does not evaluate methods to break the self-reinforcing evaluation loop among LLMs.
  - **What evidence would resolve it:** An experiment comparing standard LLM-as-a-judge evaluation against a debiased or adversarially-prompted version, with human evaluation as the ground truth.

## Limitations

- The evaluation framework relies heavily on specific Chinese language models, limiting generalizability to other linguistic contexts
- The "echo chamber" effect is specific to the 6 evaluated models sharing overlapping training data and may not hold for models with diverse training corpora
- The human expert validation sample size is not explicitly reported, raising questions about statistical power and potential cultural bias

## Confidence

- **High Confidence**: The observed "echo chamber" effect where LLM judges systematically overlook prosodic violations that human experts catch (directly evidenced by the Gemma case)
- **Medium Confidence**: The claim that specifying "Poetic Form" yields higher Information Gain than "Theme" prompts (entropy calculations are sound but interpretation assumes structural constraints are superior)
- **Low Confidence**: The generalizability of the three-step evaluation framework beyond classical Chinese poetry (specific cultural and technical constraints may make this framework more necessary here than in other creative domains)

## Next Checks

1. **Cross-Lingual Generalization Test**: Evaluate the same 6 models on English-language poetry (Shakespearean sonnets) using identical metrics to test whether the "echo chamber" effect is language-specific or a general LLM evaluation bias.

2. **Expert Sample Size Validation**: Calculate the statistical power of the human expert validation by determining the minimum number of poems needed to detect the observed effect sizes with 95% confidence, quantifying the robustness of the human-AI alignment findings.

3. **Blind Re-evaluation**: Have human experts re-evaluate a subset of poems without knowing which model generated them, and compare their consistency scores to the original LLM-as-judge consistency, isolating whether the "echo chamber" is due to shared training biases or human evaluators' own consistency challenges.