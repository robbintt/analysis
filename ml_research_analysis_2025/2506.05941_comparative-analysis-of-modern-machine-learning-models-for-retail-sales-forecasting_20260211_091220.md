---
ver: rpa2
title: Comparative Analysis of Modern Machine Learning Models for Retail Sales Forecasting
arxiv_id: '2506.05941'
source_url: https://arxiv.org/abs/2506.05941
tags:
- wmape
- group
- series
- demand
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates modern machine learning models for long-horizon
  retail sales forecasting in a brick-and-mortar setting with highly intermittent,
  sparse data. Models tested include tree-based ensembles (XGBoost, LightGBM) and
  neural architectures (N-BEATS, N-HiTS, TFT), across configurations with and without
  data imputation.
---

# Comparative Analysis of Modern Machine Learning Models for Retail Sales Forecasting

## Quick Facts
- **arXiv ID**: 2506.05941
- **Source URL**: https://arxiv.org/abs/2506.05941
- **Reference count**: 40
- **Primary result**: Tree-based models (LightGBM) outperform neural architectures on long-horizon retail sales forecasting with highly intermittent, sparse data

## Executive Summary
This study evaluates modern machine learning models for retail sales forecasting in a brick-and-mortar setting with highly intermittent, sparse data. The authors compare tree-based ensembles (XGBoost, LightGBM) against neural architectures (N-BEATS, N-HiTS, TFT) across different training configurations with and without data imputation. The key finding is that tree-based models, especially LightGBM, consistently deliver superior forecasting accuracy and computational efficiency when trained per product group on raw, non-imputed data, achieving lower WMAPE for revenue and profit than neural models.

## Method Summary
The study uses daily retail sales data from a major South-East Europe retailer (75K+ series, 50% missingness, 70% intermittent) organized into product groups and units-of-need. Models tested include XGBoost, LightGBM, N-BEATS, N-HiTS, and TFT across four configurations: individual groups with raw data, whole category with raw data, individual groups with imputed data, and whole category with imputed data. Feature engineering includes lag features, rolling statistics, and enrichment with competitor prices and macroeconomic indicators. Feature selection uses Boruta with LightGBM as the background model, while categorical encoding uses CatBoost encoder. Hyperparameter tuning employs HEBO on a 10% subset, and neural models require minimum 730 observations.

## Key Results
- LightGBM on individual groups with raw data achieves Group Revenue WMAPE of 0.069 and Group Profit WMAPE of 0.117, outperforming all neural models
- Neural models show marked improvement with SAITS imputation but still lag behind tree-based ensembles
- Training time for tree-based models is 6-11 minutes versus 170-14,678 minutes for neural models
- Localized modeling per product group consistently outperforms global training across all model families

## Why This Works (Mechanism)

### Mechanism 1: Tree-Based Ensemble Robustness to Intermittent, Sparse Data
- **Claim:** Gradient-boosted decision trees may outperform neural architectures when forecasting highly intermittent, sparse retail data because their piecewise partitioning handles irregular demand patterns without requiring smooth temporal continuity.
- **Mechanism:** Tree-based models make axis-aligned splits that directly isolate sparse, non-contiguous demand patterns. Unlike neural networks that learn distributed representations requiring dense training signals, GBDTs can model zero-inflated series by splitting directly on feature thresholds that capture intermittent behavior.
- **Core assumption:** The advantage holds when data exhibits high intermittency (>70% of series) and significant missingness (~50%), as in this brick-and-mortar dataset. Assumption: dense, continuous series may show different dynamics.
- **Evidence anchors:**
  - [abstract]: "Our results show that localized modeling strategies especially those using tree-based models on individual groups with nonimputed data, consistently deliver superior forecasting accuracy and computational efficiency."
  - [section 4, Table 3]: Case A (individual groups, non-imputed): LightGBM achieves Group Revenue WMAPE of 0.069 and Group Profit WMAPE of 0.117; XGBoost achieves 0.072 and 0.096 respectively, substantially outperforming N-BEATS (0.221, 0.226), N-HiTS (0.192, 0.201), and TFT (0.194, 0.214).
  - [corpus]: AttnBoost (arXiv:2509.10506) similarly notes GBDTs offer strong predictive performance on structured data but lack adaptive mechanisms, suggesting the tradeoff depends on data heterogeneity.

### Mechanism 2: Localized Modeling Preserves Group-Specific Signal
- **Claim:** Training separate models per product group (rather than pooling all products) appears beneficial when inter-group heterogeneity exceeds intra-group variance, preventing cross-group noise from obscuring category-specific patterns.
- **Mechanism:** When product groups have distinct demand dynamics (e.g., seasonal decorative items vs. daily consumables), joint training forces the model to learn compromises that fit all groups poorly. Localized models specialize on each group's unique temporal patterns, promotional responses, and intermittency profiles.
- **Core assumption:** Product groups have limited shared information. If groups share strong common patterns, joint training could enable transfer learning benefits. Assumption: the dataset has sufficient samples per group to support individual models.
- **Evidence anchors:**
  - [section 5]: "When data is segmented into individual groups and they do not share too much information between them, localized modeling can capture unique patterns more effectively. In contrast, training on the whole category does not introduce enough inter-group information to bring improvements."
  - [section 4, Table 3]: Comparing Case A (individual groups) to Case B (whole category) for tree-based models shows competitive or better performance with localization: LightGBM Case A RMSSE 0.758 vs Case B 0.773.
  - [corpus]: FreshRetailNet-50K (arXiv:2505.16319) addresses censored demand in fresh retail, suggesting data characteristics vary significantly across retail sub-domains, supporting localized approaches.

### Mechanism 3: Neural Model Dependence on Temporal Continuity
- **Claim:** Neural forecasting architectures show marked improvement with deep learning-based imputation, suggesting their performance degrades when facing extended missing sequences rather than isolated gaps.
- **Mechanism:** Neural sequence models learn temporal dependencies through recurrent connections or attention mechanisms that expect reasonably continuous input. Extended missing periods break the temporal chain, forcing models to extrapolate from incomplete patterns. Imputation (SAITS in this study) restores temporal consistency, though potentially introducing estimation uncertainty.
- **Core assumption:** Imputation quality matters—SAITS may work better than simple forward-fill, but extensive contiguous missing sections still introduce uncertainty that limits neural model gains. Assumption: the neural architectures evaluated have fixed inductive biases that don't handle missingness natively.
- **Evidence anchors:**
  - [section 4]: "For neural network-based models (including NBEATS, NHiTS, and the Temporal Fusion Transformer), the use of imputed data (Cases C and D) makes a significant difference. These models exhibit marked improvements, with reduced error rates compared to their nonimputed counterparts."
  - [section 6]: "This can be attributed to the extensive amount of missing data in our setting, rather than a few isolated gaps, many series contain large contiguous sections of missing values. As a result, the imputed values introduce uncertainty that can overwhelm the underlying signal."
  - [corpus]: TFT for Multi-Horizon Probabilistic Forecasting (arXiv:2511.00552) shows strong TFT performance on weekly Walmart data with exogenous features, suggesting performance depends on data density and feature richness.

## Foundational Learning

### Concept: Intermittent Demand Classification (Syntetos-Boylan)
- **Why needed here:** The paper classifies 70%+ of series as intermittent, 23% as lumpy, and only 2.4% as smooth. Understanding these categories is essential for selecting appropriate forecasting approaches and interpreting why GBDTs excel.
- **Quick check question:** Given a series with demand occurring in only 30% of time periods with highly variable quantities when demand occurs, which classification applies and what forecasting challenges does it present?

### Concept: Gradient Boosting Decision Trees vs. Neural Networks for Tabular Data
- **Why needed here:** The paper's central finding concerns GBDT superiority on sparse tabular retail data. Understanding why GBDTs handle heterogeneous, irregular features well while neural networks often struggle is critical for model selection.
- **Quick check question:** Why might a tree ensemble handle a binary promotional flag combined with a highly skewed price distribution more naturally than a feedforward neural network without extensive preprocessing?

### Concept: Forecast Evaluation Metrics for Business Impact (WMAPE, RMSSE, Financial Metrics)
- **Why needed here:** The paper emphasizes revenue and profit WMAPE as primary metrics alongside standard error measures. Business-aligned metrics connect model selection to operational outcomes like inventory costs and lost sales.
- **Quick check question:** If a model achieves low MAE but has significant negative demand bias, what business risks does this create in a brick-and-mortar retail context, and how would this appear in the profit WMAPE metric?

## Architecture Onboarding

### Component Map:
Raw Data (product-store-day level) -> Data Preprocessing Pipeline -> Model Training Configurations -> Model Zoo -> Evaluation

### Critical Path:
1. **Data quality assessment first:** Classify series intermittency and quantify missingness patterns before selecting model families. The 70% intermittent + 50% missingness profile drives the GBDT advantage.
2. **Localized vs. global decision:** Start with individual group training; only pursue global training if groups show strong shared patterns or some groups lack sufficient data.
3. **Imputation strategy:** If using neural models, invest in sophisticated imputation (SAITS or similar); tree-based models can proceed with raw data per paper findings.

### Design Tradeoffs:
- **Accuracy vs. Efficiency:** Tree-based models achieve 6-11 minute training vs. 170-14,678 minutes for neural models (Table 4), with better accuracy. Neural models only competitive with imputation at substantial computational cost.
- **Simplicity vs. Potential Scaling:** GBDTs with raw data are simpler and performant now. If data volume increases 10-100x, transformer scaling laws (Zalando reference) may favor neural architectures—current dataset appears below that threshold.
- **Feature Engineering vs. Representation Learning:** Tree-based approach requires explicit feature engineering (lag features, rolling windows). Neural approaches learn representations but need denser data to do so effectively.

### Failure Signatures:
- **Neural models on raw intermittent data:** WMAPE 2-5x higher than tree-based (Table 3, Cases A/B). Characterized by poor handling of extended zero-demand periods.
- **Global training on heterogeneous groups:** Neural model degradation is dramatic (Case B neural RMSSE 1.3-1.5 vs. Case A 0.9-1.0). Tree models more robust but still slightly worse than localized.
- **Imputation for tree-based models:** LightGBM performance degrades with imputation (Group Revenue WMAPE 0.069 raw → 0.928 imputed in Case C), suggesting imputation noise harms models that handle missingness natively.

### First 3 Experiments:
1. **Baseline establishment:** Train LightGBM per product group on raw (non-imputed) data using Boruta-selected features. Measure Group Revenue WMAPE and training time. This establishes your production-ready baseline per the paper's best configuration.
2. **Imputation impact assessment:** Train the same LightGBM configuration on SAITS-imputed data. Expect degradation based on paper findings (Table 3, Case C). Document whether your dataset shows similar patterns.
3. **Neural comparison with proper controls:** Train TFT on the same product groups with two conditions: (a) raw data with 730+ observation filter, (b) SAITS-imputed data. Measure accuracy gap relative to LightGBM and computational cost ratio. This quantifies whether neural models justify their overhead for your specific data characteristics.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can hybrid architectures that combine tree-based feature learning with neural sequence modeling outperform pure ensembles on intermittent brick-and-mortar retail data?
- **Basis in paper:** [explicit] "Future research should investigate hybrid approaches that integrate the interpretability and efficiency of tree-based models with the representation learning capabilities of neural networks."
- **Why unresolved:** No hybrid models were tested; the study only compared standalone tree ensembles against standalone neural architectures.
- **What evidence would resolve it:** Benchmark LightGBM feature embeddings fed into N-BEATS/TFT on the same dataset, comparing WMAPE and training time against pure LightGBM.

### Open Question 2
- **Question:** At what data scale (number of series and historical depth) do transformer-based models begin to exhibit scaling-law advantages over ensembles in fragmented retail settings?
- **Basis in paper:** [explicit] Zalando's scaling laws for transformers were not observed; the authors attribute this to "comparatively smaller scale of our dataset" and absence of centralized, high-density demand signals.
- **Why unresolved:** The dataset covers one retailer with ~75K series and 50% missingness; the crossover point for neural scaling remains unknown.
- **What evidence would resolve it:** Systematic experiments varying training series count (e.g., 10K, 100K, 1M) and horizon length, measuring where TFT/N-HiTS error begins decreasing predictably with data volume.

### Open Question 3
- **Question:** Would more sophisticated imputation methods (e.g., diffusion-based or retrieval-augmented imputation) narrow the performance gap between neural networks and tree ensembles without introducing the noise that SAITS imputation introduced?
- **Basis in paper:** [explicit] The authors suggest "further exploration into advanced data preprocessing techniques, such as more sophisticated imputation methods" could bridge the gap.
- **Why unresolved:** SAITS improved neural models but ensembles still dominated; the study notes imputed values may have introduced noise that neural models overfit.
- **What evidence would resolve it:** Compare multiple imputation strategies (SAITS, CSDI, retrieval-based) on held-out validation, tracking downstream forecast WMAPE per model class.

### Open Question 4
- **Question:** How does model performance vary across product groups with different demand intermittency patterns (intermittent vs. lumpy vs. erratic)?
- **Basis in paper:** [inferred] The authors acknowledge "reliance on aggregated error metrics... may obscure variability in model performance across different retail segments or temporal conditions" and note Syntetos-Boylan classification shows diverse demand patterns.
- **Why unresolved:** Results are reported at group and series level but not stratified by intermittency type; it remains unclear whether neural models perform relatively better on smoother subseries.
- **What evidence would resolve it:** Stratify evaluation by Syntetos-Boylan category, reporting WMAPE and RMSSE per model for intermittent, lumpy, erratic, and smooth series separately.

## Limitations
- Findings are highly dataset-specific to brick-and-mortar retail with 70% intermittent series and 50% missingness
- Neural model results depend critically on SAITS imputation quality, which is not publicly validated for retail data
- Study excludes multivariate forecasting (single-target per series), limiting generalizability to scenarios requiring cross-series dependencies

## Confidence
- **High confidence:** Tree-based models (LightGBM, XGBoost) outperform neural architectures on raw intermittent data with superior computational efficiency
- **Medium confidence:** Localized per-group modeling provides consistent advantage over global training when inter-group heterogeneity is high
- **Medium confidence:** Neural models require sophisticated imputation (SAITS) to achieve competitive accuracy, at substantial computational cost
- **Low confidence:** Generalization to other retail contexts with different data densities, missingness patterns, or product categorization schemes

## Next Checks
1. **Data density sensitivity test:** Repeat experiments with varying thresholds of series continuity (>50% vs >70% non-missing periods) to quantify the break point where neural models begin outperforming GBDTs
2. **Cross-domain robustness check:** Apply the same methodology to a dense, high-frequency retail dataset (e.g., daily e-commerce with <10% missingness) to test if findings hold when intermittent demand is less prevalent
3. **Imputation quality ablation:** Systematically compare SAITS vs. simpler imputation methods (forward-fill, KNN) on the same neural architectures to isolate the contribution of imputation sophistication vs. neural architecture choice