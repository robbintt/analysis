---
ver: rpa2
title: 'InvertiTune: High-Quality Data Synthesis for Cost-Effective Single-Shot Text-to-Knowledge
  Graph Generation'
arxiv_id: '2512.03197'
source_url: https://arxiv.org/abs/2512.03197
tags:
- knowledge
- triples
- text
- graph
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of automatic knowledge graph
  construction from text (Text2KG), where existing methods rely on computationally
  expensive iterative LLM prompting and lack high-quality, realistically sized datasets
  for supervised fine-tuning. The proposed InvertiTune framework introduces a data
  generation pipeline that extracts noise-minimized, semantically coherent subgraphs
  from a large knowledge base and uses an LLM to generate corresponding textual descriptions,
  a task more aligned with LLM capabilities than direct KG generation from text.
---

# InvertiTune: High-Quality Data Synthesis for Cost-Effective Single-Shot Text-to-Knowledge Graph Generation

## Quick Facts
- **arXiv ID:** 2512.03197
- **Source URL:** https://arxiv.org/abs/2512.03197
- **Reference count:** 21
- **Primary result:** Lightweight 1.5B model fine-tuned on synthetic data outperforms larger non-fine-tuned LLMs on Text2KG task

## Executive Summary
InvertiTune addresses the challenge of automatic knowledge graph construction from text by introducing a novel data generation pipeline that extracts noise-minimized subgraphs from Wikidata and uses LLMs to generate corresponding textual descriptions. This inverted approach aligns better with LLM capabilities than direct KG generation from text. The framework enables creation of high-quality datasets with longer texts paired with larger, more realistic knowledge graphs. Experimental results show that a fine-tuned 1.5B model outperforms significantly larger non-fine-tuned baselines on established metrics.

## Method Summary
The InvertiTune framework uses a data generation pipeline that extracts k-hop subgraphs from Wikidata using controlled traversal with inline noise filtering (entity blacklist, rule-based filters, and uniqueness constraints), then employs an LLM (DeepSeek-V3) to generate natural text from these triples. This produces the CE12k dataset (12k samples). A lightweight Qwen2.5-1.5B Instruct model is then fine-tuned on this (text, KG) dataset for single-shot Text2KG generation, eliminating the need for iterative prompting. The model learns to directly map input text to triple sets during supervised fine-tuning.

## Key Results
- InvertiTune achieves 82.02 G-BLEU, 82.67 G-ROUGE, and 92.58 G-BERTScore on CE12k test set
- 1.5B fine-tuned model outperforms Qwen2.5-32B non-fine-tuned baseline (95.73 vs 83.57 F1 BERTScore)
- Strong cross-dataset generalization on CrossEval-1200 combining three benchmarks with CE12k
- Comparable performance achievable with fewer than 12k samples, indicating high data quality over quantity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Inverting the Text2KG pipeline (extracting subgraphs from knowledge bases first, then generating text) produces higher-quality training data than traditional text-to-KG extraction.
- **Mechanism:** The pipeline extracts denoised, semantically coherent subgraphs from Wikidata using controlled k-hop traversal with inline filters, then uses an LLM (DeepSeek-V3) to generate natural text from those triples. This aligns the harder task (KG generation) with the easier task (text generation) for LLMs.
- **Core assumption:** LLMs are more reliable at generating coherent text from structured triples than extracting accurate triples from unstructured text.
- **Evidence anchors:**
  - [abstract] "the data-generation pipeline systematically extracts subgraphs from large knowledge bases, applies noise filtering, and leverages LLMs to generate corresponding natural text descriptions, a task more aligned with LLM capabilities than direct KG generation from text"
  - [section 4.1.1] Formal definition of recursive goal-oriented graph traversal with Fvalid(e0) filtering operators
  - [corpus] Weak direct evidence; neighbor papers address data synthesis broadly (e.g., FASTGEN, Unicorn) but do not validate inverted pipelines specifically
- **Break condition:** If source KB has systematic biases or if LLM text generation introduces hallucinations that contradict extracted triples, the (text, KG) pairs become unreliable for SFT.

### Mechanism 2
- **Claim:** Inline noise filtering during subgraph extraction improves training data quality more than post-hoc filtering.
- **Mechanism:** Three sequential filters applied during traversal: Φ_noexpand (entity expansion blacklist from LLM-assisted preprocessing), Φ_rule (seven deterministic rules removing IDs, URLs, non-Latin scripts, self-references), and Φ_sp-uniq (subject-predicate uniqueness). These prevent low-quality paths from entering the subgraph.
- **Core assumption:** Noise patterns in Wikidata are predictable and can be captured by static rules and a one-time curated blacklist.
- **Evidence anchors:**
  - [abstract] "applies noise filtering"
  - [section 4.1.1.1–4.1.1.3] Detailed specification of three filtering operators with examples in Appendix B and C
  - [corpus] No direct validation; corpus papers do not address KB noise filtering strategies
- **Break condition:** If target domain has different noise characteristics than Wikidata, or if rules over-filter semantically useful triples (e.g., multilingual entities), quality degrades.

### Mechanism 3
- **Claim:** A lightweight LLM (1.5B parameters) fine-tuned on high-quality synthetic data outperforms larger non-fine-tuned models on Text2KG.
- **Mechanism:** Supervised fine-tuning of Qwen2.5-1.5B-Instruct on CE12k enables single-shot KG generation, eliminating iterative prompting overhead. The model learns to map text → triple set directly.
- **Core assumption:** The synthetic (text, KG) distribution is sufficiently similar to real-world Text2KG tasks for transfer.
- **Evidence anchors:**
  - [abstract] "fine-tuning a lightweight model (Qwen 2.5-1.5B Instruct) on this dataset. Results show that InvertiTune outperforms larger non-fine-tuned LLMs"
  - [section 5.2.2] Table 3 shows InvertiTune (1.5B) achieves 95.73 F1 BERTScore vs. 83.57 for Qwen2.5-32B
  - [corpus] Indirect support from FASTGEN and Cequel for cost-effective LLM strategies, but no direct SFT-vs-prompting comparison in Text2KG
- **Break condition:** If downstream domains have very different entity/relation schemas or text styles not covered in CE12k, the single-shot model may underfit or produce invalid triples.

## Foundational Learning

- **Concept:** Knowledge graph triples (subject, predicate, object)
  - **Why needed here:** Core data structure; the entire pipeline builds around extracting, filtering, and generating triples.
  - **Quick check question:** Given the triple ["Marie Curie", "field of work", "physics"], identify subject, predicate, object.

- **Concept:** Supervised fine-tuning (SFT) vs. prompting
  - **Why needed here:** InvertiTune uses SFT to enable single-shot inference; understanding the trade-off against iterative prompting is essential.
  - **Quick check question:** What is the inference-time cost difference between single-shot SFT output and 5-step iterative LLM prompting?

- **Concept:** k-hop neighborhood traversal
  - **Why needed here:** Subgraph extraction uses controlled k-hop expansion; understanding graph traversal depth vs. semantic coherence is key.
  - **Quick check question:** If k=2 and m=3 (neighbors per hop), what is the maximum number of triples extracted (assuming all passes filters)?

## Architecture Onboarding

- **Component map:** Preprocessing -> Subgraph Extraction -> Text Generation -> Dataset Assembly -> Training -> Inference
- **Critical path:** Subgraph extraction quality → text coherence → SFT effectiveness. If any filter is misconfigured, noise propagates through the entire pipeline.
- **Design tradeoffs:**
  - Higher k/m → larger, more complex KGs but longer texts and potential semantic drift
  - Stricter rules → cleaner data but risk of over-filtering domain-specific triples
  - Lightweight model (1.5B) → faster inference but may struggle with very long contexts
- **Failure signatures:**
  - Model outputs malformed triples (missing subject/predicate/object)
  - Generated KG includes entities not mentioned in input text
  - Cross-dataset performance drops significantly on non-Human categories
- **First 3 experiments:**
  1. **Baseline filter ablation:** Train with each filter (Φ_noexpand, Φ_rule, Φ_sp-uniq) disabled independently; measure G-BLEU/G-ROUGE degradation on CE12k test set.
  2. **Scale sensitivity:** Fine-tune on subsets (2k, 4k, 6k, 8k, 10k samples) and plot performance saturation curves (as shown in Figures 2–5).
  3. **Cross-domain probe:** Evaluate fine-tuned model on a held-out category (e.g., Disease, City) to assess generalization beyond Human entities.

## Open Questions the Paper Calls Out

- **Question:** How does InvertiTune's performance scale when fine-tuning larger model architectures (e.g., 7B, 13B, 70B parameters) or alternative model families (Llama, Mistral, Gemma)?
- **Basis in paper:** [explicit] The authors explicitly state in the Limitations section: "The evaluation was restricted to the Qwen2.5-1.5B Instruct model, and we did not examine how performance may vary with larger or different architectures."
- **Why unresolved:** Only one lightweight model was tested, leaving unclear whether the data quality benefits transfer across model scales and architectures.
- **What evidence would resolve it:** Experiments fine-tuning models of varying sizes and architectures on CE12k, comparing against their respective non-fine-tuned baselines.

## Limitations
- The inverted pipeline's superiority over traditional text-to-KG extraction lacks direct empirical comparison
- CE12k dataset focuses exclusively on Human entities, limiting domain generalization assessment
- KB noise filters are optimized for Wikidata but effectiveness on other knowledge bases is untested
- No systematic evaluation of filter parameter sensitivity or over-filtering risks

## Confidence

**High confidence:** Experimental results showing InvertiTune outperforming larger non-fine-tuned models on CE12k metrics (G-BLEU 82.02, G-ROUGE 82.67, G-BERTScore 92.58) are well-documented with statistical significance testing via Wilcoxon signed-rank tests.

**Medium confidence:** The claim that fewer than 12k samples can achieve comparable performance is supported by training curve analysis, but the exact saturation point varies with dataset composition and task complexity.

**Low confidence:** The assertion that the inverted pipeline fundamentally produces higher-quality training data than traditional approaches lacks direct empirical validation through head-to-head comparisons.

## Next Checks
1. **Filter ablation study:** Systematically disable each filtering operator (Φ_noexpand, Φ_rule, Φ_sp-uniq) independently and measure performance degradation on CE12k test set to quantify each filter's contribution to data quality.

2. **Domain transfer experiment:** Evaluate the fine-tuned model on a held-out entity category (e.g., Disease, City) not represented in the Human-focused CE12k training data to assess cross-domain generalization limits.

3. **Traditional vs. inverted pipeline comparison:** Generate an equivalent dataset using traditional text-to-KG extraction methods and conduct head-to-head comparison on downstream Text2KG performance metrics.