---
ver: rpa2
title: 'TV2TV: A Unified Framework for Interleaved Language and Video Generation'
arxiv_id: '2512.05103'
source_url: https://arxiv.org/abs/2512.05103
tags:
- video
- text
- tv2tv
- generation
- interleaved
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TV2TV introduces a unified generative modeling framework that decomposes
  video generation into an interleaved text and video generation process. The approach
  jointly learns language modeling and video flow matching using a Mixture-of-Transformers
  architecture, dynamically alternating between generating text plans and video frames
  during inference.
---

# TV2TV: A Unified Framework for Interleaved Language and Video Generation

## Quick Facts
- arXiv ID: 2512.05103
- Source URL: https://arxiv.org/abs/2512.05103
- Authors: Xiaochuang Han; Youssef Emad; Melissa Hall; John Nguyen; Karthik Padthe; Liam Robbins; Amir Bar; Delong Chen; Michal Drozdzal; Maha Elbayad; Yushi Hu; Shang-Wen Li; Sreya Dutta Roy; Jakob Verbeek; XuDong Wang; Marjan Ghazvininejad; Luke Zettlemoyer; Emily Dinan
- Reference count: 26
- Primary result: 91% preference win over text-to-video baseline in visual quality on gaming data; 19-point improvement in instruction-following accuracy

## Executive Summary
TV2TV introduces a unified generative modeling framework that decomposes video generation into an interleaved text and video generation process. The approach jointly learns language modeling and video flow matching using a Mixture-of-Transformers architecture, dynamically alternating between generating text plans and video frames during inference. This allows the model to "think in words" about subsequent content before "acting in pixels" to produce frames, offloading semantic decision-making to the language component and enabling fine-grained controllability through text interventions. On video game data, TV2TV achieved 91% preference over text-to-video baseline in visual quality and showed a 19-point improvement in instruction-following accuracy compared to think-then-act approaches.

## Method Summary
TV2TV trains a unified generative model to produce interleaved sequences of text and video chunks. The architecture uses a Mixture-of-Transformers with separate modality-specific towers for text and video, combined through global self-attention. Text is generated autoregressively while video chunks are generated via flow matching conditioned on preceding text. The model maintains dual clean/noisy latent representations for video frames to resolve the conflict between autoregressive teacher-forcing and flow-matching noise requirements. During inference, the model generates text until a BOF token is sampled, then generates a video chunk using an ODE solver before resuming text generation. This interleaved approach enables fine-grained controllability through text interventions at any point in the generation process.

## Key Results
- 91% human preference win over text-to-video baseline in visual quality on CS:GO gaming data
- 19-point improvement in instruction-following accuracy compared to think-then-act approaches
- 54% holistic preference win on real-world sports videos against comparable baselines
- Demonstrated strong visual quality and prompt alignment when scaled to sports data with synthetically augmented captions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interleaved text-video generation improves visual quality and prompt alignment by offloading semantic planning to the language component.
- Mechanism: The model generates text "plans" before producing video frame chunks, reducing the entropy of video generation by pre-committing to semantic decisions in text space. Text is generated autoregressively; video chunks are generated via flow matching conditioned on preceding text.
- Core assumption: Language modeling has stronger reasoning capabilities than direct pixel prediction, and decomposing generation reduces overall task complexity.
- Evidence anchors:
  - [abstract] "This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment."
  - [section 3.4] "TV2TV achieves substantial improvements in overall visual quality compared to both T2V and Think2V...91% vs. 1% with 8% ties" in human evaluations.
  - [corpus] ViTCoT (2507.09876) supports interleaved video-text chain-of-thought for boosting understanding, suggesting the interleaved reasoning paradigm has broader applicability.

### Mechanism 2
- Claim: The Mixture-of-Transformers (MoT) architecture enables modality-specific processing while maintaining cross-modal attention for unified generation.
- Mechanism: Separate transformer towers for text and video with modality-specific Q/K/V projections and FFNs, combined through global self-attention over the entire interleaved sequence. This allows text-to-video conditioning while preserving modality-specific inductive biases.
- Core assumption: Modality-specific parameterization improves over shared parameters when modalities have different optimal processing patterns (discrete tokens vs. continuous latents).
- Evidence anchors:
  - [section 2.2] "TV2TV adopts a Mixture-of-Transformers (MoT) architecture with dedicated towers for the video and text modalities, enabling modality-specific processing while maintaining a global self-attention."
  - [section 2.2] Equations 5-10 specify modality-specific QKV and FFN transformations.

### Mechanism 3
- Claim: Dual clean/noisy latent representations resolve the conflict between autoregressive teacher-forcing and flow-matching noise requirements.
- Mechanism: Each video frame chunk appears twice in the sequence: noisy version (for flow matching prediction target) followed by clean version (for conditioning subsequent tokens). This preserves clean historical context for autoregressive generation while enabling standard flow matching training.
- Core assumption: The model can learn to use clean latents for conditioning without interference from noisy counterparts.
- Evidence anchors:
  - [section 2.1] "This design allows the model to condition on clean historical context while learning to denoise current frames. This contrasts with MAGI-1...which enforces monotonicity in noise."
  - [section 2.1] Equation 1 shows the sequence structure with interleaved clean and noisy latents.

## Foundational Learning

- Concept: Flow matching / Rectified flow
  - Why needed here: Video chunks are generated via flow matching (ODE-based denoising), not autoregressive token prediction. Understanding noise interpolation schedules and ODE solvers is essential.
  - Quick check question: Can you explain why flow matching uses interpolated noisy latents `x_noisy = t * x_clean + (1-t) * ε` rather than pure noise-to-clean diffusion?

- Concept: Autoregressive language modeling with teacher forcing
  - Why needed here: Text generation follows standard next-token prediction; the BOF token triggers mode switching. Exposure bias mitigation (clean-vid flipping) is borrowed from AR training practices.
  - Quick check question: Why does the paper apply "soft dropout" to clean video latents, and how might this help with exposure bias?

- Concept: Hybrid attention masking (causal + bidirectional)
  - Why needed here: Text tokens use causal masking; video frame chunks use bidirectional attention internally; noisy tokens cannot be attended by future tokens. Understanding mask construction is critical for correct implementation.
  - Quick check question: If a text token at position 100 attends to a noisy video token at position 50, what constraint must hold about the relationship between those tokens' timestamps?

## Architecture Onboarding

- Component map:
  - BPE tokenizer for text → `h_txt_in` via linear embedding
  - VAE tokenizer (Cosmos-Tokenize) for video → `h_vid_in` via U-Net downsampler with timestep embedding
  - MoT layers (×L): Modality-specific LayerNorm → QKV projection → global self-attention with hybrid mask → modality-specific FFN → residual connection
  - `h_txt` → logits via LM head; `h_vid` → flow prediction via U-Net upsampler
  - Special tokens: BOF (beginning-of-frame) triggers video generation mode; EOF (end-of-frame) may delimit chunks; EOS terminates generation

- Critical path:
  1. Text tokens generated autoregressively until BOF sampled
  2. On BOF: initialize noisy latents from N(0,I), run ODE solver for m steps using KV cache
  3. After ODE: run forward pass with final denoised latents, update KV cache, resume text generation
  4. For intervention: user inserts text at any point; generation continues from that context

- Design tradeoffs:
  - Dual vs. single latent representation: Dual requires ~2× memory for video tokens but preserves clean context; MAGI-1 uses single representation with monotonic noise but doesn't work well with interleaved text
  - Interleaved vs. think-then-act: Interleaved allows mid-sequence interventions; Think2V cannot react to context changes after initial plan
  - VAE vs. discrete tokenization: Continuous latents enable flow matching; discrete tokens would require pure AR

- Failure signatures:
  - Video frames ignore text interventions: Check CFG is enabled (λ_CFG ≈ 7.5 in sports experiments) and text dropout is small during training
  - Temporal incoherence across chunks: Verify causal masking is correct and clean latents are properly conditioned
  - Mode collapse (text-only generation): Check BOF token distribution during training; model may be undertrained on video prediction

- First 3 experiments:
  1. Ablate dual latent representation: Compare TV2TV with single-representation variant (MAGI-1 style monotonic noise) on interleaved gaming data. Expect degraded performance if clean context is critical.
  2. Vary text intervention timing: Test controllability with interventions at t=0.5s, 1.5s, 3.0s on sports data. Expect later interventions to have weaker effect due to committed generation.
  3. Scale interleaved caption density: Train on sports data with captions every 1.0s vs. 2.0s vs. 4.0s. Expect higher density (closer to gaming's per-frame actions) to improve alignment but require more synthetic caption quality filtering.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal granularity and quality of interleaved text annotations for video generation?
- Basis in paper: [explicit] "Two factors potentially contribute to this gap. First, the density of interleaved text: CS:GO provides frame-level textual signals, while sports rely on synthetic captions inserted every 1.9 seconds on average. Second, the quality of the interleaved text: CS:GO uses ground-truth actions, whereas sports captions are generated by VLMs and often contain hallucinations."
- Why unresolved: The paper demonstrates a correlation between text density/quality and performance gains but does not systematically vary these factors to identify optimal settings or the trade-off curve between them.
- What evidence would resolve it: Controlled experiments training TV2TV on the same video data with systematically varied text annotation frequencies (e.g., 0.25s, 0.5s, 1s, 2s intervals) and quality levels (ground-truth vs. VLM-generated with varying accuracy), measuring downstream visual quality and controllability.

### Open Question 2
- Question: How can interleaved text-video training data be obtained at scale for diverse real-world video domains beyond sports and gaming?
- Basis in paper: [explicit] "A central question is how to obtain such interleaved text and video data for training."
- Why unresolved: The paper demonstrates one approach (VLM-based augmentation for sports) and leverages existing controller data for games, but the pipeline may not transfer well to domains lacking clear action boundaries or where VLMs struggle to describe fine-grained changes accurately.
- What evidence would resolve it: Application of the TV2TV framework to additional domains (e.g., cooking, instructional videos, dialogue-heavy scenes) with adapted data pipelines, comparing performance against domain-specific baselines to assess generalization.

### Open Question 3
- Question: To what extent do VLM-generated caption hallucinations in the training data degrade TV2TV's video generation quality and controllability?
- Basis in paper: [inferred] The paper acknowledges that sports captions "often contain hallucinations" but does not quantify their impact or explore mitigation strategies beyond the filtering pipeline described.
- Why unresolved: While noise filters and quality classifiers are applied, the direct causal relationship between caption hallucination types (temporal errors, object misidentification, fabricated actions) and generation failures remains uncharacterized.
- What evidence would resolve it: Ablation studies injecting controlled types and rates of hallucinations into ground-truth CS:GO action captions, or human evaluation comparing videos generated from high- vs. low-hallucination caption subsets within the sports dataset.

## Limitations
- Data quality dependency: Performance relies heavily on the accuracy and density of interleaved text annotations, with sports data showing smaller gains due to VLM hallucinations in synthetic captions
- Evaluation scope: Experiments focus on gaming data with dense action annotations and sports data with synthetic captions, leaving uncertainty about performance on real-world video with sparse or no annotations
- Caption hallucination impact: The paper acknowledges hallucinations in VLM-generated sports captions but does not quantify their specific impact on generation quality

## Confidence

- **High confidence**: The MoT architecture design with modality-specific processing and global attention is well-specified and theoretically sound. The dual latent representation approach for resolving autoregressive vs. flow matching conflicts is clearly described and implementable.
- **Medium confidence**: The interleaved generation mechanism's effectiveness relies heavily on data quality. While the gaming results are strong, the sports data improvements are more modest and potentially sensitive to caption accuracy.
- **Low confidence**: The generalizability of the framework to real-world video generation tasks with sparse annotations remains uncertain. The paper does not test on open-domain video datasets or demonstrate robustness to caption errors.

## Next Checks

1. **Caption quality ablation**: Train TV2TV on sports data with varying levels of caption accuracy (e.g., ground truth captions where available, corrupted captions with controlled noise levels) to quantify sensitivity to caption quality and identify failure thresholds.

2. **Open-domain evaluation**: Test TV2TV on publicly available video datasets (e.g., Kinetics, Something-Something) with standard text-to-video generation benchmarks to assess generalization beyond gaming and sports domains.

3. **Intervention timing analysis**: Systematically vary the timing and content of text interventions during generation on sports data to measure the persistence of semantic control and identify temporal windows where interventions are most/least effective.