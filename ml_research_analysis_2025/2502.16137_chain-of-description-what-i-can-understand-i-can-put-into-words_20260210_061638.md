---
ver: rpa2
title: 'Chain-of-Description: What I can understand, I can put into words'
arxiv_id: '2502.16137'
source_url: https://arxiv.org/abs/2502.16137
tags:
- prompting
- standard
- experiments
- qwen2
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Chain-of-Description (CoD) Prompting, a novel
  inference strategy for multi-modal large language models (MLLMs) that first generates
  a detailed description of the multi-modal input before answering questions. The
  approach was evaluated on Qwen2-Audio, Qwen2-VL, and Qwen2.5-VL models across audio
  and vision benchmarks.
---

# Chain-of-Description: What I can understand, I can put into words

## Quick Facts
- arXiv ID: 2502.16137
- Source URL: https://arxiv.org/abs/2502.16137
- Reference count: 4
- Introduces Chain-of-Description prompting that improves multi-modal LLM performance by 4-5.3% on benchmark tasks

## Executive Summary
This paper introduces Chain-of-Description (CoD), a novel inference strategy for multi-modal large language models (MLLMs) that first generates detailed descriptions of multi-modal inputs before answering questions. The approach was evaluated on Qwen2-Audio, Qwen2-VL, and Qwen2.5-VL models across audio and vision benchmarks, showing consistent performance improvements. The method demonstrates particular effectiveness for complex inputs with rich information density, where generating intermediate descriptions helps the model better process and understand the input data.

## Method Summary
CoD Prompting works by first prompting the MLLM to generate a comprehensive description of the multi-modal input, then using this description as context for the actual question-answering task. The approach treats description generation as an intermediate reasoning step that helps the model better understand and contextualize the input before attempting to answer questions. The method was implemented across Qwen2-Audio for speech comprehension tasks and Qwen-VL models for visual reasoning tasks, with specific prompt templates designed to elicit detailed, structured descriptions of the input data.

## Key Results
- CoD improved Qwen2-Audio's speech comprehension by 4% on AIR-Bench-Chat
- CoD improved Qwen-VL's performance by 5.3% on hard-level MMMU_Pro questions
- Ablation studies confirmed that better-generated descriptions lead to improved model performance

## Why This Works (Mechanism)
The paper posits that multi-modal inputs often contain rich, complex information that can overwhelm models when presented directly with questions. By first generating a detailed description, the model effectively performs self-explanation and context-building, which helps organize the information before reasoning about it. This intermediate step allows the model to better understand the input structure, identify key elements, and establish relationships between different modalities before attempting to answer questions, particularly benefiting complex inputs where information density is high.

## Foundational Learning
- Multi-modal large language models (MLLMs): Models that process and reason across multiple input types like text, audio, and vision. Why needed: Core subject of the paper's evaluation and contribution.
- Prompt engineering: The practice of designing input prompts to elicit desired model behaviors. Quick check: Verify that CoD prompts are structurally different from standard prompts.
- Chain-of-Thought reasoning: Intermediate reasoning steps that improve model performance on complex tasks. Quick check: Compare CoD to standard CoT approaches to understand the distinction.

## Architecture Onboarding
- Component map: Input -> CoD Description Generation -> Question Answering
- Critical path: Multi-modal input → Description generation module → Enhanced understanding → Improved answer generation
- Design tradeoffs: The approach adds computational overhead through an additional generation step, but this is justified by performance gains on complex tasks
- Failure signatures: May underperform on simple inputs where direct question-answering is sufficient, or when description generation is poor quality
- First experiments: 1) Compare CoD vs direct prompting on simple vs complex inputs, 2) Measure description quality correlation with final performance, 3) Test different description prompt templates for optimal results

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation scope is narrow, focusing primarily on Qwen-family models without extensive comparison to alternative multi-modal models or prompting strategies
- Lack of discussion of potential failure modes or edge cases where CoD might degrade performance
- Analysis of what constitutes "better" descriptions and how quality varies across input types is not thoroughly explored

## Confidence
- Core CoD methodology and implementation: High
- Performance improvement claims on Qwen models: Medium
- Generalizability to other MLLMs and domains: Low
- Explanation of why CoD works (mechanism understanding): Medium

## Next Checks
1. Test CoD prompting on diverse multi-modal models (GPT-4V, Gemini, Claude) to assess cross-model generalizability
2. Conduct systematic failure case analysis to identify input types where CoD degrades performance
3. Implement automated description quality metrics and correlate them with downstream task performance across varying input complexity levels