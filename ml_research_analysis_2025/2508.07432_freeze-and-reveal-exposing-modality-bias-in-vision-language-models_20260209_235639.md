---
ver: rpa2
title: 'Freeze and Reveal: Exposing Modality Bias in Vision-Language Models'
arxiv_id: '2508.07432'
source_url: https://arxiv.org/abs/2508.07432
tags:
- bias
- gender
- text
- vision
- only
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates gender bias in Vision-Language Models (VLMs)
  by dissecting the contributions of vision and text backbones using targeted debiasing
  techniques: Counterfactual Data Augmentation (CDA) and Task Vector methods. A novel
  metric, Degree of Stereotypicality (DoS), and a corresponding debiasing method,
  Data Augmentation Using DoS (DAUDoS), are introduced to reduce bias with minimal
  computational cost.'
---

# Freeze and Reveal: Exposing Modality Bias in Vision-Language Models

## Quick Facts
- **arXiv ID:** 2508.07432
- **Source URL:** https://arxiv.org/abs/2508.07432
- **Reference count:** 4
- **Primary result:** Gender bias in VLMs can be effectively reduced by targeting either the vision or text encoder, with CLIP's vision encoder and PaliGemma2's text encoder being the dominant bias sources in each model respectively.

## Executive Summary
This paper investigates gender bias in Vision-Language Models (VLMs) by dissecting the contributions of vision and text backbones using targeted debiasing techniques: Counterfactual Data Augmentation (CDA) and Task Vector methods. A novel metric, Degree of Stereotypicality (DoS), and a corresponding debiasing method, Data Augmentation Using DoS (DAUDoS), are introduced to reduce bias with minimal computational cost. Experiments on the VisoGender benchmark show that CDA reduces the gender gap by 6% and DAUDoS by 3% using only one-third of the training data. Both methods improve gender identification accuracy by 3%. Key findings indicate that CLIP's vision encoder is more biased, while PaliGemma2's text encoder is more biased. By identifying the dominant source of bias, this work enables more targeted and effective bias mitigation strategies in future multi-modal systems.

## Method Summary
The paper introduces two debiasing approaches: Counterfactual Data Augmentation (CDA) and Data Augmentation Using DoS (DAUDoS). CDA generates synthetic data by swapping gender-associated attributes in images and captions to balance the dataset. DAUDoS uses a learned DoS metric to identify and augment the most stereotypical samples, reducing bias efficiently with minimal data. Both methods are applied selectively to either the vision or text encoder, and their impact is evaluated on the VisoGender benchmark. The study also employs a task vector method to isolate and measure the contribution of each modality to overall model bias, enabling targeted interventions.

## Key Results
- CDA reduces gender bias gap by 6% and DAUDoS by 3% using only one-third of the training data.
- Both debiasing methods improve gender identification accuracy by 3%.
- CLIP's vision encoder is identified as the primary source of bias, while PaliGemma2's text encoder is more biased.

## Why This Works (Mechanism)
The paper's effectiveness stems from its dual approach: first, isolating the modality-specific sources of bias using task vectors, and second, applying targeted data augmentation methods to the dominant bias source. By freezing one modality and fine-tuning the other, the researchers can attribute bias contributions accurately. The DoS metric enables efficient identification of stereotypical patterns, allowing DAUDoS to focus augmentation on the most problematic samples rather than uniformly expanding the dataset.

## Foundational Learning
- **Modality-specific bias attribution** (why needed: to identify whether vision or text encoder contributes more to bias; quick check: compare bias scores when freezing each modality)
- **Counterfactual Data Augmentation** (why needed: to balance gender representations in training data; quick check: measure gender gap before and after CDA)
- **Degree of Stereotypicality (DoS) metric** (why needed: to quantify and target the most biased samples; quick check: correlate DoS scores with human-labeled stereotypicality)
- **Task vector fine-tuning** (why needed: to isolate and measure modality-specific contributions; quick check: validate task vectors by comparing with ablated models)
- **Multi-modal bias propagation** (why needed: to understand how bias transfers between vision and text; quick check: track bias scores across model layers)
- **Efficient data augmentation** (why needed: to reduce computational cost while maintaining effectiveness; quick check: compare performance with full vs. one-third training data)

## Architecture Onboarding
- **Component map:** Input Image/Caption -> Vision Encoder (CLIP/PaliGemma2) + Text Encoder -> Cross-Encoder Fusion -> Output Classification
- **Critical path:** Data augmentation (CDA/DAUDoS) -> Modality-specific fine-tuning -> Bias evaluation on VisoGender
- **Design tradeoffs:** Targeted modality fine-tuning vs. full model retraining; efficient DoS-based augmentation vs. comprehensive data balancing
- **Failure signatures:** Increased gender gap after fine-tuning; degraded gender identification accuracy; bias persisting in one modality despite intervention
- **First experiments:** 1) Run bias attribution task vectors on baseline models, 2) Apply CDA to dominant bias source and measure gender gap reduction, 3) Test DAUDoS with one-third training data and compare to full CDA results

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses exclusively on gender bias, which may not generalize to other bias types (racial, age-related, etc.).
- The DoS metric, while novel, requires further validation across diverse datasets and model architectures.
- The claim that CLIP's vision encoder is more biased while PaliGemma2's text encoder is more biased appears model-specific and may not extend to other VLMs.

## Confidence
- **High confidence:** Methodology and experimental design are rigorous and well-documented
- **Medium confidence:** Generalizability of findings across different VLMs and bias types
- **Low confidence:** Universal applicability of DoS as a bias mitigation metric across all domains

## Next Checks
1. Test DoS and DAUDoS on non-gender bias types (racial, age-related) across multiple VLMs
2. Evaluate long-term stability and robustness of debiased models under distribution shifts
3. Conduct ablation studies to isolate individual contributions of vision vs. text encoders in bias propagation across wider range of architectures