---
ver: rpa2
title: 'On Generalization across Measurement Systems: LLMs Entail More Test-Time Compute
  for Underrepresented Cultures'
arxiv_id: '2506.02591'
source_url: https://arxiv.org/abs/2506.02591
tags:
- measurement
- system
- systems
- llms
- default
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models often struggle to accurately translate and
  reason across measurement systems, leading to decreased performance and increased
  inference costs for users relying on underrepresented cultural contexts. This work
  shows that LLMs default to the measurement system most common in their training
  data and exhibit significant instability when asked to convert between systems.
---

# On Generalization across Measurement Systems: LLMs Entail More Test-Time Compute for Underrepresented Cultures

## Quick Facts
- **arXiv ID:** 2506.02591
- **Source URL:** https://arxiv.org/abs/2506.02591
- **Reference count:** 12
- **Primary result:** LLMs default to dominant cultural measurement systems, requiring more inference compute for underrepresented users.

## Executive Summary
Large language models exhibit significant performance disparities when handling measurement systems from underrepresented cultures, defaulting to Western-centric units and incurring higher computational costs for accurate responses. The paper demonstrates that while chain-of-thought reasoning improves accuracy for non-default systems, it increases inference costs by up to 300%. This creates a fundamental equity issue where users from marginalized cultural backgrounds face both lower reliability and higher expenses when interacting with these models.

## Method Summary
The authors conducted a comprehensive evaluation across three measurement domains (currency, weight, length) using multiple model sizes (7B, 32B, 70B parameters). They designed synthetic queries targeting both default (USD, imperial/metric) and underrepresented (Eritrean Nakfa, Sudanese Pound, etc.) measurement systems. The study compared baseline performance against chain-of-thought reasoning and sequential single-hop prompting strategies, measuring both accuracy (MAPD) and computational costs (token generation).

## Key Results
- LLMs default to USD in 80% of currency queries and imperial/metric systems for weights, aligning with training data distribution
- Chain-of-thought reasoning improves accuracy for non-default systems but increases test-time compute by 180-300%
- Small models (7B) show significant instability across all measurement systems, while large models (≥70B) benefit from explicit reasoning
- Performance gaps are most pronounced for low-income country currencies and rare measurement units

## Why This Works (Mechanism)

### Mechanism 1: Multi-hop Reasoning Decomposition
The paper shows that queries involving non-default measurement systems implicitly require multi-hop reasoning: (1) Source Fact Retrieval, (2) Conversion Retrieval, and (3) Arithmetic Reasoning. Without explicit prompting, models fail to reliably chain these steps, especially for underrepresented systems where conversion knowledge is sparsely encoded.

### Mechanism 2: Training Data Bias Towards Dominant Cultural Contexts
LLMs default to measurement systems most prevalent in their training data (e.g., USD for currency). This statistical prevalence causes parametric knowledge to be more robustly encoded for "default" systems, leading to instability when users request alternative systems where conversion knowledge is less robustly encoded or absent.

### Mechanism 3: Test-Time Compute Scaling for Cultural Adaptation
Large LLMs can benefit from explicit reasoning via Chain-of-Thought, which stabilizes performance across measurement systems at the cost of significantly increased test-time compute (up to 300%). This reasoning process activates correct knowledge and arithmetic procedures but generates longer responses, directly increasing inference costs.

## Foundational Learning

- **Multi-hop Question Answering**: Understanding that non-default measurement queries decompose into source retrieval + conversion retrieval + arithmetic is essential for designing mitigation strategies.
  - *Quick check*: Can you explain why asking "What is Japan's GDP in Vietnamese Dong?" requires at least two separate knowledge retrievals?

- **Test-Time Compute / Inference Cost**: The paper's core equity argument hinges on how reasoning strategies increase token generation and therefore API costs, creating disproportionate burden for underrepresented users.
  - *Quick check*: If an API charges $0.10 per 1K input tokens and $0.40 per 1K output tokens, and CoT triples output token count, what is the cost multiplier for a query with 500 input tokens and baseline 100 output tokens?

- **Cultural Bias in Training Data Distribution**: The mechanism assumes performance gaps stem from training corpus overrepresentation of Western/dominant measurement systems, not from architectural limitations.
  - *Quick check*: Why might a model trained predominantly on English web text know USD-EUR exchange rates better than USD-ERN rates?

## Architecture Onboarding

- **Component map:** Fact Retrieval Subsystem -> Conversion Knowledge Subsystem -> Reasoning Controller -> Output Generator

- **Critical path:**
  1. Query received with target measurement system
  2. Model identifies default system for the fact
  3. **If target ≠ default:** Model must retrieve conversion factor and apply arithmetic (multi-hop)
  4. **If CoT enabled:** Model generates explicit intermediate steps before final answer
  5. Output generated; cost proportional to token count

- **Design tradeoffs:**
  - **Accuracy vs. Cost:** CoT improves accuracy for non-default systems but increases cost 180-300%
  - **Model Size vs. Stability:** Large models (≥70B) can benefit from reasoning; small models do not
  - **Explicit vs. Implicit Reasoning:** Sequential single-hop prompts offer controlled cost but require multi-turn interaction

- **Failure signatures:**
  - Hallucinated conversions: Model invents exchange rates for rare systems
  - Silent defaulting: Model ignores requested system and returns value in default system
  - Arithmetic errors: Correct retrievals but incorrect multiplication/division
  - Verbose non-answers: CoT prompts generate long reasoning traces without numerical answers

- **First 3 experiments:**
  1. **Baseline Mapping:** For each dataset type, query models with no measurement system specified to calculate default system frequency
  2. **Cost-Accuracy Tradeoff Quantification:** Run identical queries in default vs. non-default systems with CoT disabled/enabled, measuring MAPD accuracy, output token count, and cost per query
  3. **Failure Mode Analysis:** Inspect model outputs for lowest-performing currencies/units to classify failure types

## Open Questions the Paper Calls Out

### Open Question 1
Does aligning the prompt language with the target measurement system (e.g., asking for North Korean Won in Korean) mitigate the performance gap compared to English prompts?

### Open Question 2
Can smaller models (<70B parameters) be optimized to perform stable measurement conversions without relying on computationally expensive Chain-of-Thought reasoning?

### Open Question 3
Is it possible to induce LLMs to "natively" reason in underrepresented measurement systems without first triangulating through a default Western system?

## Limitations
- Synthetic dataset validity: The evaluation relies on manually constructed query templates
- Language effects: The interplay between language and measurement systems remains unexplored
- Economic focus: Currency conversion is emphasized, potentially biasing conclusions about other measurement domains

## Confidence
- Multi-hop reasoning mechanism: High
- Training data bias as primary cause: Medium
- Cost-disparity quantification: High
- Small model limitations: High

## Next Checks
1. Replicate baseline mapping experiment with different model families to verify default system selection patterns
2. Conduct cost-accuracy tradeoff analysis using real API pricing data to validate 300% cost increase claim
3. Perform failure mode analysis on at least 10 additional underrepresented measurement systems to confirm generalization of findings