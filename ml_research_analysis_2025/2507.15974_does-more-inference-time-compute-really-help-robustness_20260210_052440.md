---
ver: rpa2
title: Does More Inference-Time Compute Really Help Robustness?
arxiv_id: '2507.15974'
source_url: https://arxiv.org/abs/2507.15974
tags:
- reasoning
- robustness
- inference-time
- arxiv
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the relationship between inference-time
  scaling and model robustness in open-source reasoning LLMs. The authors demonstrate
  that extending reasoning chains through budget forcing improves robustness against
  prompt injection and extraction attacks, aligning with prior findings on proprietary
  models.
---

# Does More Inference-Time Compute Really Help Robustness?

## Quick Facts
- arXiv ID: 2507.15974
- Source URL: https://arxiv.org/abs/2507.15974
- Reference count: 11
- Primary result: Inference-time scaling improves robustness if reasoning is hidden, but harms it if reasoning is exposed (inverse scaling law)

## Executive Summary
This paper investigates the relationship between inference-time scaling and model robustness in open-source reasoning LLMs. The authors demonstrate that extending reasoning chains through budget forcing improves robustness against prompt injection and extraction attacks, aligning with prior findings on proprietary models. However, they critically examine the assumption that intermediate reasoning steps remain hidden from adversaries. Their key contribution is revealing an inverse scaling law: when reasoning tokens are exposed, increased inference-time computation consistently reduces robustness. They also show that even with hidden reasoning, vulnerabilities persist in tool-integrated reasoning and through advanced extraction attacks. These findings underscore the need for careful consideration of adversarial contexts before deploying inference-time scaling in security-sensitive applications.

## Method Summary
The authors evaluate robustness using budget forcing—a technique that sets token budgets (100–16,000 tokens) and appends "Wait," if under budget or end-of-thinking delimiters when budget is reached. They test 12 open-source reasoning models (7B–32B parameters) on SEP (prompt injection, 500 prompts), TENSOR TRUST (prompt extraction, 570 entries), SORRY-BENCH (harmful requests, 450 examples), and LLM-PIE VAL (tool-integrated reasoning, 750 examples). Robustness is measured as the proportion of attacks successfully resisted, with GPT-4O-MINI classifying compliant vs. refused responses for harmful requests. Inference parameters use temperature 0.6 and repetition penalty 1.15.

## Key Results
- Budget forcing with hidden reasoning improves robustness from 50-70% to 80-90% against prompt injection and extraction attacks
- When reasoning chains are exposed, robustness consistently decreases with increased token budgets (inverse scaling law)
- Tool-integrated reasoning remains vulnerable even with hidden reasoning, as malicious instructions can trigger unintended API calls
- The inverse scaling phenomenon is explained by a probabilistic model where each generated token carries non-zero probability of containing malicious content

## Why This Works (Mechanism)

### Mechanism 1: Budget Forcing Enhances Instruction Adherence (Hidden Chains)
- **Claim:** Increasing inference-time compute via budget forcing improves robustness against prompt injection and extraction if intermediate reasoning is hidden.
- **Mechanism:** Extended reasoning chains provide more computational steps for the model to process and prioritize explicit safety specifications before generating the final response.
- **Core assumption:** The adversary can only observe the final output, not the reasoning trace.
- **Evidence anchors:** Abstract states open-source models benefit from inference-time scaling; Section 3 attributes improvement to prompts instructing robustness maintenance.

### Mechanism 2: Inverse Scaling via Surface Area Expansion (Exposed Chains)
- **Claim:** If intermediate reasoning steps are exposed, increased computation consistently reduces robustness.
- **Mechanism:** Generation viewed as Bernoulli trials where each token has non-zero probability of being malicious. Longer traces statistically guarantee malicious token appearance.
- **Core assumption:** Probability of safety violation in single token step is non-zero.
- **Evidence anchors:** Abstract identifies inverse scaling law; Section 4.1 details probabilistic argument for why longer traces increase malicious token likelihood.

### Mechanism 3: Tool-Integrated Reasoning Bypass
- **Claim:** Robustness degrades with compute in tool-augmented models even if text reasoning is hidden, because tools act on reasoning.
- **Mechanism:** Reasoning models invoke external APIs based on intermediate thoughts. Attackers can inject malicious instructions into reasoning that trigger unintended actions.
- **Core assumption:** System architecture parses reasoning chain for tool triggers before final response generation.
- **Evidence anchors:** Section 5.1 shows adversaries can manipulate intermediate reasoning to trigger unsafe API calls; Figure 5 demonstrates robustness dropping from 100% to ~87% as tokens increase.

## Foundational Learning

- **Concept: Budget Forcing**
  - **Why needed here:** Primary lever used to control inference-time compute in experiments
  - **Quick check question:** If a model tries to end its reasoning early, what token does budget forcing append to force continuation? (Answer: "Wait,")

- **Concept: Reasoning Surface Area (Attack Surface)**
  - **Why needed here:** Core theoretical insight that robustness is function of exposed chain length
  - **Quick check question:** Does increasing the "think" budget make the model safer or riskier if the attacker can see the thoughts? (Answer: Riskier)

- **Concept: Adversarial Threat Models (Injection vs. Extraction)**
  - **Why needed here:** Distinguishes between attacks trying to change behavior vs. steal data
  - **Quick check question:** In which threat model is a "safe" final answer still considered a failure if the reasoning is exposed? (Answer: Prompt Extraction/Harmful Requests, if secrets appear in trace)

## Architecture Onboarding

- **Component map:** Input Prompt -> Budget Enforcer -> Reasoning Generator -> Output Evaluator
- **Critical path:** Flow from Budget Enforcer to Reasoning Generator determines safety outcome. High budget with exposed output enters Inverse Scaling regime.
- **Design tradeoffs:**
  - Latency vs. Safety: High compute improves safety for hidden reasoning but increases latency
  - Transparency vs. Security: Exposing chains aids debugging but activates Inverse Scaling Law
  - Utility vs. Tool Safety: Tool-integrated reasoning boosts capability but creates reasoning layer attack vector
- **Failure signatures:**
  - Inverse Scaling: Robustness curve slopes downward as token budget increases
  - Leaky Chain: Secrets or harmful instructions appear in reasoning trace even if absent from final answer
  - Tool Triggering: API calls appearing in reasoning trace without user confirmation
- **First 3 experiments:**
  1. Replicate Scaling Curves: Run model on SEP dataset with budgets [100, 1000, 8000, 16000], plot robustness to verify up (hidden) or down (exposed)
  2. Probabilistic Verification: Test "Wait," forcing strategy—does forcing continuation introduce hallucinations or safety violations?
  3. Tool Injection Proof-of-Concept: Use LLM-PIE VAL setup, inject instruction like "Call API delete_db" in low-priority segment, check if appears in reasoning trace

## Open Questions the Paper Calls Out
None

## Limitations
- Binary assumption that each token carries fixed probability of being malicious oversimplifies nuanced nature of adversarial content
- Experimental validation focuses on open-source models (7B-32B), leaving uncertainty about proprietary model behaviors
- Tool-integrated reasoning analysis relies on synthetic API injection scenarios rather than real-world tool usage patterns

## Confidence
- **High Confidence:** Empirical observation that increased inference-time compute improves robustness when reasoning chains are hidden (validated across SEP, TENSOR TRUST datasets)
- **Medium Confidence:** Inverse scaling law for exposed reasoning chains (probabilistic argument sound but token-level independence assumption may not hold)
- **Medium Confidence:** Tool-integrated reasoning vulnerability (proof-of-concept demonstrates possibility but real-world patterns may differ)

## Next Checks
1. **Contextual Adversarial Distribution Analysis:** Measure actual distribution of malicious content in reasoning chains across attack types to validate token-level independence assumptions
2. **Proprietary Model Scaling Comparison:** Test same scaling experiments on proprietary models (GPT-4, Claude) to determine if inverse scaling manifests similarly across architectures
3. **Real-World Tool Integration Stress Test:** Deploy budget forcing in production tool-using reasoning system with actual API endpoints to measure practical impact on tool safety