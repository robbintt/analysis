---
ver: rpa2
title: Subjective functions
arxiv_id: '2512.15948'
source_url: https://arxiv.org/abs/2512.15948
tags:
- learning
- function
- reward
- agent
- goal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework for open-ended learning based on
  expected prediction error (EPE) as a subjective function that agents optimize to
  select both goals and policies. The core idea is that agents should select goals
  and actions that maximize prediction error - seeking states where their value estimates
  are wrong - rather than simply maximizing expected reward.
---

# Subjective functions

## Quick Facts
- arXiv ID: 2512.15948
- Source URL: https://arxiv.org/abs/2512.15948
- Reference count: 10
- Primary result: Proposes Expected Prediction Error (EPE) as a subjective function for open-ended learning where agents select goals and policies that maximize prediction error rather than expected reward

## Executive Summary
This paper proposes a framework for open-ended learning based on Expected Prediction Error (EPE) as a subjective function that agents optimize to select both goals and policies. The core idea is that agents should select goals and actions that maximize prediction error - seeking states where their value estimates are wrong - rather than simply maximizing expected reward. This addresses the problem of how agents can generate their own objectives and explore effectively without external supervision. The approach builds on reinforcement learning theory by defining a goal-based reward function and using EPE as the objective.

## Method Summary
The method defines a goal-conditioned MDP where agents select both goals and policies to maximize Expected Prediction Error. The EPE utility is defined as U^π_g(s) = V^π_g(s) - V̂^π_g(s), where V̂ is a frozen or slowly-updating value estimate (similar to target networks in deep RL). Principle 1 states that agents should select policies maximizing EPE, while Principle 2 states they should select goals maximizing EPE under the optimal policy. The framework can be combined with standard value maximization using a parameter α to weight EPE versus value in the objective.

## Key Results
- Theoretical framework connecting EPE to open-ended learning and goal-directed behavior
- Mathematical derivation showing EPE telescopes to the difference between true and estimated value functions
- Connections to psychological phenomena including hedonic adaptation and preference for increasing reward sequences
- Links to neuroscience findings about dopamine responses to prediction errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Agents select goals that maximize expected prediction error (EPE), which operationalizes "positive surprise" or learning progress.
- Mechanism: EPE is defined as U^π_g(s) = Σ γ^t δ_t, where δ_t is the TD error. This telescopes to U^π_g(s) = V^π_g(s) - V̂^π_g(s) (true value minus estimated value). Agents pursue goals where they expect to learn—not too easy (zero error) or too hard (inaccessible error).
- Core assumption: Value estimates are frozen or slowly changing during EPE computation (similar to target networks in deep RL).
- Evidence anchors:
  - [abstract] "Expected prediction error is studied as a concrete example of a subjective function."
  - [section 3, Principle 2] "g* = argmax_g U*_g(s_0)... agent will select goals that it expects to yield positive surprise."
  - [corpus] Weak direct evidence; corpus contains multi-objective RL and teaching papers but no direct EPE implementations.
- Break condition: If value estimates update too rapidly relative to policy evaluation, EPE signals become noisy and goal selection destabilizes.

### Mechanism 2
- Claim: Goal attainment quenches incentive because EPE drops to zero once a state is fully learned—explaining hedonic adaptation.
- Mechanism: When V̂^π_g = V^π_g (perfect value estimate), EPE = 0. The goal loses appeal regardless of its absolute reward value. This generates open-ended learning without requiring external reward shaping.
- Core assumption: Agents cannot sustain interest in fully predictable outcomes, even if instrumentally valuable.
- Evidence anchors:
  - [abstract] "Once a goal is achieved and no longer surprising, it loses its appeal."
  - [section 4, Hedonic adaptation] Cites Brickman et al. (1978) lottery winners, Rolls et al. (1981) food satiety, Rutledge et al. (2014) well-being prediction by recent prediction errors.
  - [corpus] No corpus papers test this mechanism directly.
- Break condition: If α (value-EPE weighting parameter) is set too high, agents revert to value-maximizing behavior and fail to seek new goals.

### Mechanism 3
- Claim: Information avoidance occurs when value estimates are optimistic; information demand occurs when estimates are pessimistic.
- Mechanism: When V̂ > V, EPE is negative, and agents avoid information that might reduce the optimism gap. When V̂ < V, EPE is positive, and agents seek information. This asymmetry predicts apparently suboptimal choices (e.g., pigeons preferring informative-but-less-rewarding options).
- Core assumption: Pessimistic value estimates arise from noisy magnitude estimation with Bayesian regularization toward priors.
- Evidence anchors:
  - [section 4, Information avoidance and demand] "EPE model predicts information avoidance when values are overestimated and information demand when values are underestimated."
  - [section 4] Cites Kendall (1974) pigeon experiments, Bromberg-Martin & Hikosaka (2009) dopamine neuron responses to informative cues.
  - [corpus] Weak evidence; corpus contains no direct tests of information avoidance via prediction error.
- Break condition: If delay-induced noise is misestimated, the model mispredicts when agents will seek vs. avoid information.

## Foundational Learning

- Concept: Temporal Difference (TD) Error
  - Why needed here: EPE is built entirely from TD errors; understanding δ_t = R + γV̂(s') - V̂(s) is prerequisite to understanding the telescoping property.
  - Quick check question: Can you explain why TD error is an unbiased estimate of the advantage function?

- Concept: Goal-Conditioned Value Functions
  - Why needed here: The framework requires V^π_g where g is an explicit goal state; standard RL assumes fixed reward functions.
  - Quick check question: How does V^π_g change when the goal state g changes in a sparse reward setting?

- Concept: Actor-Critic Architecture with Target Networks
  - Why needed here: EPE computation requires frozen value estimates during error calculation—directly analogous to target network practice in DQN/actor-critic methods.
  - Quick check question: Why does using the same network for both value estimation and TD error computation create instability?

## Architecture Onboarding

- Component map:
  Inner loop: Policy optimizer (π_g) that maximizes EPE for a fixed goal g using TD error as intrinsic reward
  Outer loop: Goal selector that chooses g* = argmax_g U*_g(s_0) based on expected EPE under optimal policy
  Value estimator: V̂^π_g with frozen/slow-updating target for EPE computation
  Optional: α-weighted hybrid objective combining EPE and value for sustained motivation

- Critical path:
  1. Implement goal-conditioned value function V̂^π_g
  2. Add target network mechanism to freeze V̂ during EPE computation
  3. Implement EPE-based policy gradient (inner loop)
  4. Implement goal selection via EPE maximization (outer loop)
  5. Tune α parameter for EPE-value tradeoff

- Design tradeoffs:
  - λ in generalized advantage estimation: λ=1 gives unbiased EPE estimate but high variance; λ=0 gives low-variance but biased estimate
  - α weighting: Low α favors exploration/novelty; high α favors exploitation of known rewards
  - Goal space discretization: More goals increase outer-loop compute; fewer goals reduce open-endedness

- Failure signatures:
  - Agent cycles through same goals repeatedly → α too high or goal space too sparse
  - Agent selects impossible goals and makes no progress → no "too hard" filtering mechanism
  - Agent avoids all information seeking → optimistic value initialization causing systematic overestimation
  - EPE signal collapses to zero everywhere → value network learning too fast relative to policy evaluation

- First 3 experiments:
  1. Sparse-reward maze: Verify agent learns to reach goal, then abandons it for new goals (vs. value-maximizing baseline that loops).
  2. Information demand task: Replicate Kendall (1974) paradigm with optimistic vs. pessimistic value initialization to confirm information avoidance/demand asymmetry.
  3. Increasing-reward preference: Test whether EPE agent prefers sequences of increasing reward over decreasing reward with equal total, as predicted by "EPE ≈ dV/dt" prior to goal attainment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Expected Prediction Error (EPE) framework be operationalized into a practical algorithm for open-ended learning?
- Basis in paper: [explicit] The author states the proposal "is not, however, fully worked out as a practical algorithm" and lists "practical implementation questions" as a primary area for future work.
- Why unresolved: The paper provides a conceptual formulation and mathematical derivation but does not validate the approach with algorithmic implementations in complex environments.
- What evidence would resolve it: A scalable deep reinforcement learning agent demonstrating continuous, open-ended goal acquisition driven by EPE optimization.

### Open Question 2
- Question: Is Expected Prediction Error sufficient as a complete theory of human goal selection, or must it be integrated with other subjective functions?
- Basis in paper: [explicit] The conclusion explicitly calls for future work to address "questions about the adequacy of expected prediction error as a theory of human goal selection."
- Why unresolved: While the paper shows EPE aligns with phenomena like hedonic adaptation, it does not prove EPE is the sole driver of goal synthesis over alternative intrinsic motivations.
- What evidence would resolve it: Behavioral experiments distinguishing EPE-driven choices from those driven by other intrinsic rewards (e.g., novelty or power) in controlled settings.

### Open Question 3
- Question: How does the stability of learning depend on the assumption that the value estimate is "frozen" during prediction error calculation?
- Basis in paper: [inferred] Page 3 notes that deriving the EPE utility function requires assuming the value estimate $\hat{V}$ is "frozen (or slowly changing)" similar to target networks in deep RL.
- Why unresolved: If the value estimate updates too rapidly, the telescoping series derivation (Eq. 6) may fail to hold, potentially destabilizing the agent's goal pursuit.
- What evidence would resolve it: Theoretical analysis or simulations demonstrating the robustness of the EPE objective under varying rates of value function updates.

## Limitations
- No concrete algorithmic implementation or empirical validation provided
- EPE maximization may not be sufficient alone as a complete theory of human goal selection
- Stability depends on maintaining frozen value estimates, which may be difficult in practice

## Confidence
- High confidence: The mathematical formulation of EPE as V^π_g(s) - V̂^π_g(s) is well-defined and internally consistent
- Medium confidence: The psychological connections to hedonic adaptation and information demand/avoidance are theoretically plausible based on cited literature
- Low confidence: The claim that EPE maximization naturally leads to open-ended learning without external reward shaping remains speculative without experimental validation

## Next Checks
1. Implement EPE-based learning in a simple gridworld with sparse rewards to test whether agents abandon fully learned goals and seek new ones, compared to standard reward-maximizing agents
2. Test the information avoidance/demand prediction by manipulating value estimate initialization (optimistic vs pessimistic) in a bandit task where information comes at a cost
3. Verify the preference for increasing reward sequences predicted by the dV/dt ≈ EPE relationship using a multi-step task with controllable reward trajectories