---
ver: rpa2
title: 'OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code Generation'
arxiv_id: '2511.04495'
source_url: https://arxiv.org/abs/2511.04495
tags:
- level
- cefr
- simplification
- program
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes two multi-round text simplification methods
  to address the challenge of large CEFR gaps between source and target readability
  levels. The methods use rule-based rewriting combined with automatic CEFR level
  and semantic similarity verification.
---

# OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code Generation

## Quick Facts
- arXiv ID: 2511.04495
- Source URL: https://arxiv.org/abs/2511.04495
- Authors: Cuong Huynh; Jie Cao
- Reference count: 6
- Primary result: Multi-round approaches outperform single-step simplification for large CEFR gaps

## Executive Summary
This paper addresses the challenge of large CEFR gaps between source and target readability levels in text simplification by proposing two multi-round simplification methods. The approaches combine rule-based rewriting with automatic CEFR level and semantic similarity verification. MRS-Rule uses iterative rule-based simplification without LLM calls, while MRS-Joint starts with an LLM-generated simplification and refines it through multiple rule-based rounds. Experiments demonstrate that multi-round approaches are more effective than single-step simplification for handling large CEFR gaps, with MRS-Joint achieving the best performance by leveraging LLM-initialization before rule-based refinement.

## Method Summary
The system employs a multi-round text simplification pipeline that uses iterative rule-based transformations combined with verification against target CEFR levels and semantic similarity thresholds. Two variants are proposed: MRS-Rule performs purely rule-based simplification across multiple rounds, while MRS-Joint starts with an LLM-generated candidate (GPT-4o-mini) before applying iterative rule-based refinement. Both methods use a 3-model ModernBERT ensemble for CEFR prediction and SBERT cosine similarity for meaning preservation verification. The pipeline generates multiple candidates per round, selects the best based on weighted scoring (CEFR match weight 10, reference weight 2.5, original weight 0.5), and relaxes thresholds across retries when target levels aren't reached.

## Key Results
- MRS-Joint achieves test RMSE of 0.552, outperforming MRS-Rule (0.714) and baseline prompt-only (0.755)
- RMSE increases from 0.624 (1-level gap) to 1.581 (3-level gap) in single-step simplification, demonstrating the challenge of large CEFR gaps
- MRS-Joint improves CEFR level matching while maintaining meaning preservation, with MB-Orig score increasing from 0.855 (baseline) to 0.866 (test)
- For A2 targets, 66% hit target, 32% overshot to B1, and 2% remained at B2, showing systematic difficulty with lower-level targets

## Why This Works (Mechanism)

### Mechanism 1
Multi-round simplification outperforms single-step approaches when the CEFR gap between source and target levels is large. Instead of attempting a single large complexity jump, the system applies incremental transformations across multiple rounds, each verified against CEFR level and semantic similarity thresholds. Each round takes the previous simplification as input, progressively closing the gap. The core assumption is that breaking a large complexity reduction into smaller steps reduces error accumulation and improves controllability. Evidence shows RMSE rises from 0.624 (1-level gap) to 1.581 (3-level gap) in single-step approaches, while multi-round methods maintain lower error rates.

### Mechanism 2
Starting multi-round simplification from an LLM-generated candidate (MRS-Joint) improves final CEFR accuracy compared to starting from raw text (MRS-Rule). The LLM provides a semantically coherent initial simplification that is closer to the target level than the source. Rule-based refinement then handles fine-grained adjustments without additional LLM calls, reducing semantic drift while improving level precision. The core assumption is that LLMs capture global semantic coherence better than rule-based methods while rules are better at precise local adjustments. Evidence shows MRS-Joint test RMSE = 0.552 vs. MRS-Rule = 0.714, with MRS-Joint reaching more target-level sentences per retry than MRS-Rule.

### Mechanism 3
Verification-guided candidate selection (CEFR prediction + semantic similarity) enables controllable simplification with explicit tradeoffs. After each round, candidates are scored by predicted CEFR level via 3-Model ModernBERT ensemble voting and cosine similarity to original text via SBERT. The best candidate is selected using weighted scoring; thresholds relax across retries if no valid candidate emerges. The core assumption is that CEFR classifiers and semantic similarity metrics are sufficiently correlated with human judgments of readability and meaning preservation. The system uses dynamic threshold relaxation with floor_step=0.03 and max_retries=6 to balance precision with convergence.

## Foundational Learning

- **CEFR Levels (Common European Framework of Reference)**: The entire task is defined by mapping source text to target CEFR levels (A1–C2). Understanding that A1/A2 = basic, B1/B2 = intermediate, C1/C2 = advanced is essential for interpreting RMSE and the "CEFR gap" problem. Quick check: If a source sentence is C1 and the target is A2, what is the CEFR gap? (Answer: 4)

- **Multi-Round/Iterative Refinement with Verification**: The core insight is that large complexity jumps fail in one step. Understanding iterative pipelines with acceptance criteria (hit target + preserve meaning) is foundational to replicating this architecture. Quick check: Why does the system relax similarity thresholds across retries rather than keeping them fixed? (Answer: To allow stronger edits when conservative attempts fail to reach target CEFR)

- **Semantic Similarity Metrics (SBERT/Cosine Similarity)**: The system uses cosine similarity from sentence-transformers/all-MiniLM-L6-v2 to enforce meaning preservation. Understanding that this is a continuous score (not binary) and that thresholds control the tradeoff between simplification aggressiveness and semantic drift is critical. Quick check: If similarity floor is 0.88 and all candidates score 0.85, what happens? (Answer: System enters another retry round with relaxed thresholds, or eventually applies nearest-level fill)

## Architecture Onboarding

- **Component map**: Input text + target CEFR → (MRS-Joint: LLM generates initial candidate) → Rule-based candidates generated → All candidates scored (CEFR + similarity) → Best valid candidate selected → If target hit → output; else → next retry with relaxed thresholds → After max_retries → nearest-level fill → output

- **Critical path**: Input text → (MRS-Joint: LLM generates initial candidate) → Rule-based candidates generated → All candidates scored (CEFR + similarity) → Best valid candidate selected → If target hit → output; else → next retry with relaxed thresholds → After max_retries → nearest-level fill → output

- **Design tradeoffs**: CEFR accuracy vs. meaning preservation (lower similarity thresholds improve CEFR matching but risk semantic drift); LLM cost vs. rule-based efficiency (MRS-Joint calls LLM once per sentence; MRS-Rule uses zero LLM calls but may require more retries); Code generation via GPT-4o enables rapid prototyping but introduces reproducibility concerns.

- **Failure signatures**: Overshoot (A2→B1) where vocabulary simplified but abstract ideas/relative clauses remain, pushing output above target; Lexical imitation where output copies formal phrases from source without rephrasing, retaining higher complexity despite shortening; Under-generation where incomplete outputs drop conceptual content, appearing easier than intended and causing CEFR predictor to underestimate.

- **First 3 experiments**: 1) Reproduce baseline CEFR-gap analysis on trial data, grouping by gap size (1, 2, 3) and computing RMSE and MB scores to verify larger gaps correlate with higher RMSE and lower meaning preservation. 2) Ablate LLM initialization in MRS-Joint by comparing MRS-Joint vs. MRS-Rule on same test set with identical hyperparameters, measuring RMSE difference and number of retries needed to reach target. 3) Stress-test verification thresholds by running MRS-Joint with similarity_floor ∈ {0.95, 0.88, 0.75} on held-out subset, plotting CEFR accuracy vs. meaning preservation to characterize tradeoff curve.

## Open Questions the Paper Calls Out

### Open Question 1
Can curriculum-based domain knowledge explicitly integrated into multi-round simplification pipelines improve performance on large CEFR gaps (3+ levels) compared to purely rule-based or LLM-prompting approaches? The current MRS-Joint method uses general rules without pedagogically-structured simplification progressions that might better handle radical complexity reduction. Evidence would require A/B comparison of MRS-Joint against a curriculum-enhanced variant on the same TSAR test set, measuring RMSE and MeaningBERT scores specifically for CEFR gaps ≥3.

### Open Question 2
Would self-evolving algorithm discovery approaches (e.g., AlphaEvolve-style coding agents) generate more effective simplification rules than the manually-prompted GPT-4o code generation used in this work? Current rules were suggested by GPT-4o in a single-pass prompt interaction; evolutionary search might discover more optimal rule combinations. Evidence would require head-to-head comparison of rule sets discovered through evolutionary optimization vs. the current MRS-Rule implementation, with statistical significance testing on held-out simplification data.

### Open Question 3
Can explicit completeness and lexical-adaptation constraints prevent the three failure modes (overshoot, lexical imitation, under-generation) identified in the case studies? Current semantic similarity checks do not explicitly enforce lexical simplification or content completeness, allowing these failure modes. Evidence would require implementing and evaluating additional constraint modules (e.g., content coverage metrics, formal-register detectors) integrated into the multi-round verification loop, with error classification on outputs.

## Limitations
- Evaluation relies on automated CEFR prediction and semantic similarity metrics rather than human judgments, introducing uncertainty about actual readability and meaning preservation quality
- Performance varies significantly across CEFR levels, with A2 targets showing higher failure rates (overshooting to B1 in 32% of cases)
- Rule-based component uses unspecified synonym dictionaries and exact tie-breaking logic for CEFR voting, creating reproducibility gaps
- Reliance on GPT-4o-mini for MRS-Joint initialization raises concerns about replicability and cost-effectiveness

## Confidence
- **Multi-round simplification effectiveness (High confidence)**: Strong empirical support from RMSE comparisons across trial and test sets
- **LLM-initialization advantage (Medium confidence)**: Performance improvement demonstrated but exact contribution of LLM vs. verification mechanism is confounded
- **Verification-guided controllability (Medium confidence)**: Framework is sound but correlation between automated metrics and human judgments remains unproven

## Next Checks
1. Ablation of verification components: Run MRS-Joint without semantic similarity thresholds (accept any CEFR hit) and without CEFR verification (accept based on similarity only) to isolate contribution of each verification mechanism to final RMSE improvement.

2. Human evaluation correlation study: Select 50 sentences spanning different CEFR gaps and have human raters assess both readability level and meaning preservation, computing correlation between human judgments and automated metrics.

3. Cross-lingual transferability test: Apply the MRS-Joint architecture to a non-English simplification dataset with CEFR-level annotations (e.g., French or Spanish texts) to test generalization beyond English-specific components.