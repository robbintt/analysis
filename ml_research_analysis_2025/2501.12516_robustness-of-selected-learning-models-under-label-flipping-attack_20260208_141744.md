---
ver: rpa2
title: Robustness of Selected Learning Models under Label-Flipping Attack
arxiv_id: '2501.12516'
source_url: https://arxiv.org/abs/2501.12516
tags:
- learning
- label-flipping
- attacks
- machine
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the robustness of 10 machine learning and
  deep learning models under label-flipping adversarial attacks using a malware classification
  dataset. The models tested include traditional techniques (SVM, Random Forest, Gaussian
  Naive Bayes), boosting methods (Gradient Boosting Machine, LightGBM, XGBoost), and
  deep learning architectures (Multilayer Perceptron, Convolutional Neural Network,
  MobileNet, DenseNet).
---

# Robustness of Selected Learning Models under Label-Flipping Attack

## Quick Facts
- arXiv ID: 2501.12516
- Source URL: https://arxiv.org/abs/2501.12516
- Reference count: 21
- Key outcome: MLP achieves best combination of initial accuracy (~98%) and robustness under label-flipping attacks on malware dataset

## Executive Summary
This paper evaluates the robustness of 10 machine learning and deep learning models under label-flipping adversarial attacks using a malware classification dataset. Models tested include traditional techniques (SVM, Random Forest, Gaussian Naive Bayes), boosting methods (Gradient Boosting Machine, LightGBM, XGBoost), and deep learning architectures (Multilayer Perceptron, Convolutional Neural Network, MobileNet, DenseNet). The Multilayer Perceptron achieved the best combination of initial accuracy (~98%) and robustness, maintaining high performance even under severe label-flipping attacks. Support Vector Machines also showed strong robustness, while Random Forest was the least resilient. The results highlight the importance of selecting inherently robust models when facing adversarial attacks on training data.

## Method Summary
The study uses the Malicia malware dataset (8,054 samples from 7 families) and tests 10 models under label-flipping attacks at rates from 10% to 100%. Two feature extraction methods are employed: TF-IDF on opcode sequences for classical and boosting models, and 64×64 opcode images for CNN-based models. Models are trained on corrupted training data while being evaluated on clean test data. The label-flipping is performed per-class to maintain class balance, with a random selection of labels in each class being flipped to other classes.

## Key Results
- Multilayer Perceptron achieved the best combination of initial accuracy (~98%) and robustness under label-flipping attacks
- Support Vector Machines showed strong robustness with accuracy virtually unchanged until more than 60% of labels were flipped
- Random Forest was the least resilient model, showing linear degradation in accuracy with increasing label-flipping rates
- Gradient Boosting Machine demonstrated reasonable robustness while being derived from the less robust Random Forest

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MLP architectures exhibit superior robustness to label-flipping attacks compared to other tested models.
- Mechanism: MLP learns an equivalent kernel function during training rather than having it specified as a hyperparameter (as in SVM). This learned representation may create decision boundaries that are less sensitive to individual mislabeled samples, as the network can distribute error signals across multiple layers and neurons.
- Core assumption: The learned kernel representation provides implicit regularization against label noise.
- Evidence anchors:
  - [abstract] "our MLP model achieving the best combination of initial accuracy and robustness"
  - [section 4.5.1] "Roughly speaking, an MLP can be viewed as a generalization of an SVM, where the equivalent of the kernel function is learned"
  - [corpus] Weak direct evidence—neighbor papers focus on defenses and attack methods, not MLP-specific robustness mechanisms
- Break condition: If label-flipping is targeted rather than random (e.g., flipping samples near decision boundaries), MLP robustness may degrade significantly.

### Mechanism 2
- Claim: SVM margin maximization provides inherent resilience to random label corruption up to a critical threshold.
- Mechanism: SVM optimization seeks the maximum-margin hyperplane, which depends on support vectors. Random label-flipping may not consistently corrupt support vectors until a high percentage of labels are flipped, preserving classification accuracy until the corruption overwhelms the margin structure.
- Core assumption: Random flipping does not preferentially target support vectors.
- Evidence anchors:
  - [section 4.3.1] "accuracy was virtually unchanged until more than 60% of the labels were flipped"
  - [section 4.6] "The SVM model yields slightly worse results than MLP, while also providing robustness"
  - [corpus] Xiao et al. [20] examined SVM resilience to adversarial label noise, supporting this mechanism
- Break condition: If an attacker can identify and flip labels of support vectors specifically, robustness would collapse earlier.

### Mechanism 3
- Claim: Boosting methods' sequential error-correction creates vulnerability to label-flipping, but GBM's specific architecture mitigates this.
- Mechanism: Boosting iteratively fits new models to correct previous errors. Flipped labels cause the algorithm to "correct" toward wrong targets, amplifying damage. However, GBM's use of shallow trees and residual-fitting may limit the cascade effect compared to XGBoost and LightGBM's more aggressive optimization.
- Core assumption: The depth and structure of base learners influence vulnerability propagation.
- Evidence anchors:
  - [section 3.2] "mislabelled training data is considered a weakness of boosting"
  - [section 4.6] "GBM is reasonably robust in this regard"
  - [section 4.4.3] "LightGBM is—in the sense of robustness—much weaker than the GBM model from which it is derived"
  - [corpus] Weak direct comparison of GBM vs. LightGBM robustness in neighbor papers
- Break condition: Targeted flipping that corrupts early boosting iterations would disproportionately damage all boosting variants.

## Foundational Learning

- Concept: **Label-flipping attacks (training data poisoning)**
  - Why needed here: This is the attack vector being studied; understanding that corruption occurs during training (not inference) is essential.
  - Quick check question: If you flip 50% of test labels instead of training labels, would you expect the same accuracy degradation? (Answer: No—this measures prediction on corrupted targets, not model robustness.)

- Concept: **Margin-based classification**
  - Why needed here: Explains why SVM maintains accuracy despite substantial label noise.
  - Quick check question: Why would a maximum-margin classifier be more robust than a probabilistic classifier like Naive Bayes to label noise?

- Concept: **Ensemble error amplification**
  - Why needed here: Explains why boosting methods can degrade faster than single models.
  - Quick check question: If each boosting iteration corrects 10% of errors, and 30% of labels are flipped, what happens to the correction direction?

## Architecture Onboarding

- Component map:
  - Malicia dataset -> Filter families (>=50 samples) -> Feature extraction (TF-IDF or Image) -> Model training -> Label-flipping attack injection -> Evaluation on clean test data

- Critical path:
  1. Filter dataset to classes with >=50 samples
  2. Extract features appropriate to model type
  3. Inject label flips into training data only
  4. Train model on corrupted data
  5. Evaluate on clean test data

- Design tradeoffs:
  - MLP: High robustness, requires feature engineering (TF-IDF)
  - SVM: Strong robustness, may scale poorly with very large datasets
  - GBM: Moderate robustness, interpretable feature importance
  - CNN-based: Good accuracy, poor robustness, requires image conversion
  - Random Forest: Highest clean accuracy, poorest robustness

- Failure signatures:
  - Linear accuracy decay (Random Forest, XGBoost) -> model directly fitting noise
  - Sudden collapse after threshold (SVM, MLP) -> margin structure overwhelmed
  - Erratic behavior (DenseNet) -> insufficient data for architecture capacity

- First 3 experiments:
  1. Replicate baseline: Train all 10 models on clean data to verify ~96-98% accuracy range for top performers.
  2. Targeted attack test: Instead of random flipping, flip labels only for samples closest to decision boundaries (test if SVM/MLP robustness persists).
  3. Cross-dataset validation: Apply same methodology to a non-malware dataset (e.g., MNIST) to determine if robustness rankings generalize.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the robustness of these models change when subjected to targeted or optimized label-flipping attacks instead of the random flipping strategy employed in this study?
- Basis in paper: [explicit] The conclusion states that future work could consider "more advanced and targeted label-flipping attacks," as the current research relied on random label flipping.
- Why unresolved: Random flipping may not represent the most efficient adversarial strategy; targeted attacks could maximize classifier degradation with fewer flipped labels, potentially altering the observed robustness hierarchy.
- What evidence would resolve it: Experiments utilizing gradient-based or optimization-based label-flipping algorithms on the same model set to compare degradation curves against the random baseline.

### Open Question 2
- Question: Do the robustness rankings—specifically the superior performance of Multilayer Perceptron (MLP) and Support Vector Machines (SVM)—persist across different datasets or problem domains?
- Basis in paper: [explicit] The authors list "additional datasets and learning problems" as a specific avenue for future work to extend the results found using the Malicia malware dataset.
- Why unresolved: Model robustness can be data-dependent; the structural advantages of MLPs observed on opcode frequency data might not generalize to image-based tasks or non-malware tabular data.
- What evidence would resolve it: Replicating the experimental methodology on diverse datasets (e.g., image classification or network intrusion detection) to verify if MLP remains more robust than boosting methods.

### Open Question 3
- Question: What defense mechanisms can effectively protect vulnerable models like Random Forest or XGBoost without negating their baseline accuracy?
- Basis in paper: [explicit] The paper explicitly identifies "defenses against attacks, and countermeasures to those defenses" as a key area for future research.
- Why unresolved: While the paper identifies which models are fragile, it does not evaluate sanitization techniques or robust training modifications that could mitigate the accuracy drop in susceptible models.
- What evidence would resolve it: Testing defense strategies, such as label sanitization or robust loss functions, on the less robust models to measure the trade-off between defense efficacy and clean-data accuracy.

## Limitations
- The evaluation is restricted to a single malware dataset (Malicia), which may not generalize to other domains or larger-scale problems.
- The paper does not specify critical implementation details such as train-test split ratios, exact hyperparameter settings, or the precise mapping from opcodes to image pixels.
- The analysis focuses only on random label-flipping without exploring targeted attacks that could exploit model-specific vulnerabilities.

## Confidence
- **High confidence**: MLP superiority claim and general robustness rankings (MLP > SVM > GBM > others) are well-supported by empirical results across multiple attack levels.
- **Medium confidence**: The mechanism explanations (MLP learning kernel vs. SVM having fixed kernel, boosting error amplification) are plausible but not rigorously proven through ablation studies or theoretical analysis.
- **Medium confidence**: Cross-dataset generalizability is suggested but not demonstrated; the Malicia dataset may have unique characteristics that influence robustness patterns.

## Next Checks
1. **Targeted attack vulnerability**: Instead of random flipping, systematically flip labels for samples closest to decision boundaries to test whether SVM and MLP robustness is due to margin structure or random noise distribution.
2. **Cross-domain validation**: Apply the same methodology to a non-malware dataset (e.g., image classification on MNIST or text classification on 20 Newsgroups) to verify if robustness rankings generalize beyond cybersecurity.
3. **Hyperparameter sensitivity**: Systematically vary key hyperparameters (kernel type for SVM, network depth for MLP, tree depth for GBM) to determine if robustness rankings persist across the hyperparameter space.