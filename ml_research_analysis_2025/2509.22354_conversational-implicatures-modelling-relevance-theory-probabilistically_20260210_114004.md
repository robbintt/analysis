---
ver: rpa2
title: 'Conversational Implicatures: Modelling Relevance Theory Probabilistically'
arxiv_id: '2509.22354'
source_url: https://arxiv.org/abs/2509.22354
tags:
- mary
- relevance
- drink
- peter
- utterance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores how Bayesian probabilistic modeling can be
  applied to relevance-theoretic pragmatics, specifically to model conversational
  implicature comprehension. The authors develop a computational model in ProbLog
  that captures the parallel adjustment of explicatures and implicatures during utterance
  interpretation.
---

# Conversational Implicatures: Modelling Relevance Theory Probabilistically

## Quick Facts
- arXiv ID: 2509.22354
- Source URL: https://arxiv.org/abs/2509.22354
- Reference count: 40
- Primary result: Develops a ProbLog computational model that uses KL divergence between prior and posterior probability distributions to quantify utterance relevance and demonstrate how different interpretive hypotheses vary in relevance during conversational implicature comprehension.

## Executive Summary
This paper presents a probabilistic computational framework for modeling conversational implicature comprehension within Relevance Theory. The authors implement a ProbLog model that captures the parallel adjustment of explicatures and implicatures during utterance interpretation, using KL divergence between prior and posterior probability distributions to measure utterance utility. The approach demonstrates how relevance expectations guide interpretation through backward inference from questions under discussion to candidate explicatures and implicated premises. The model successfully illustrates how interpretive hypotheses vary in relevance based on their ability to satisfy expectations while minimizing processing effort.

## Method Summary
The authors develop a probabilistic computational model in ProbLog that captures the Relevance-theoretic comprehension procedure. The method involves constructing knowledge graphs with probabilistic edges encoding encyclopedic knowledge, defining inference rules that bridge words to concepts, modeling utterances as evidence, and using queries to represent relevance expectations. The model evaluates interpretive hypotheses by computing KL divergence between prior and posterior probability distributions over queries, ranking hypotheses based on their ability to satisfy expectations while minimizing processing effort (approximated by Herbrand base size).

## Key Results
- The model successfully demonstrates parallel adjustment of explicatures and implicatures through bidirectional inference from logical form to implicatures and from expected cognitive effects to explicatures.
- KL divergence between prior and posterior probability distributions provides a quantitative measure of utterance utility that aligns with Relevance Theory's notion of cognitive effects.
- The computational framework shows how different interpretive hypotheses vary in relevance according to their ability to satisfy relevance expectations while minimizing processing effort.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parallel adjustment of explicatures and implicatures enables coherent interpretation under uncertainty.
- Mechanism: Interpretive hypotheses about explicit content, contextual assumptions, and implicatures co-evolve through bidirectional inference—"bottom-up" from logical form to implicatures and "top-down" from expected cognitive effects to explicatures—rather than sequential stages.
- Core assumption: Audiences maintain fine-tuned relevance expectations (e.g., expecting an answer to a specific question) that guide backward inference to select among candidate explicature-implicature pairings.
- Evidence anchors:
  - [abstract] "parallel adjustment of explicatures and implicatures during utterance interpretation"
  - [section 4.2] "hypotheses about explicatures, implicated premises and implicated conclusions are developed in parallel against a background of expectations"
  - [corpus] Related work on pragmatic reasoning in LLMs (paper 2509.06952) supports parallel processing frameworks but lacks direct comparison to this model.
- Break condition: If relevance expectations are weak or non-specific (e.g., no QUD), the mechanism degrades to explicature-only interpretation without implicature derivation.

### Mechanism 2
- Claim: KL divergence between prior and posterior probability distributions quantifies utterance utility/relevance.
- Mechanism: An utterance shifts the audience's probability distribution over relevant propositions; the KL divergence between prior P and posterior P' measures information gain, serving as a proxy for "cognitive effects achieved" without requiring explicit effort quantification.
- Core assumption: Manifestness (probability a proposition influences cognition) maps to probability distributions; higher KL divergence indicates more relevant interpretations.
- Evidence anchors:
  - [abstract] "uses KL divergence between prior and posterior probability distributions to measure utterance utility"
  - [section 4.3.2] "Mary changes Peter's probability distribution over these propositions from a prior value to a posterior value. The utility...can then be measured in terms of the divergence"
  - [corpus] No direct corpus evidence on KL divergence for implicature modeling; this is a novel contribution.
- Break condition: If queries cannot be answered (probability = 0), KL divergence becomes undefined/infinite, indicating failed relevance expectation satisfaction.

### Mechanism 3
- Claim: Relevance-theoretic comprehension operates as a "Take the Best" heuristic with accessibility-ordered hypothesis sampling.
- Mechanism: Hypotheses are accessed in order of accessibility (least effort); the first hypothesis satisfying relevance expectations is accepted without exhaustive search—a fast-and-frugal procedure justified by the presumption of optimal relevance.
- Core assumption: Communicators produce optimally relevant stimuli; audiences are licensed to stop at the first satisficing interpretation.
- Evidence anchors:
  - [section 3.4] "access the most easily accessible interpretive hypothesis...Check if the stimulus...yields enough cognitive effects"
  - [section 4.1.2] "the procedure appears to be an instance of what Gigerenzer (2004, p. 74) calls a Take the Best heuristic"
  - [corpus] Related work (2508.06167) discusses pragmatics as dynamic interface but doesn't address heuristic stopping criteria.
- Break condition: If no hypothesis satisfies expectations within tolerable effort, search is abandoned—model predicts communication failure.

## Foundational Learning

- Concept: **Explicature vs. Implicature**
  - Why needed here: The model requires distinguishing linguistically-derived meaning (explicature, via logical form development) from contextually-supplied meaning (implicature, via premises/conclusions).
  - Quick check question: Given "I've eaten" as a refusal, which part is the explicature and which is the implicature?

- Concept: **Manifestness (Relevance Theory)**
  - Why needed here: Probabilities in the model represent manifestness—likelihood a proposition is entertained and accepted—not truth or certainty.
  - Quick check question: How does manifestness differ from standard probability in Bayesian models?

- Concept: **Probabilistic Logic Programming (ProbLog basics)**
  - Why needed here: The implementation uses ProbLog's syntax for probabilistic facts, rules, evidence, and queries.
  - Quick check question: What does `0.9:: edge(coffee, tirednessBlockingDrink)` mean in ProbLog?

## Architecture Onboarding

- Component map:
  - Knowledge graphs with probabilistic edges -> Inference rules linking words to concepts -> Utterance model (say predicates) -> Queries representing relevance expectations -> KL divergence calculation -> Hypothesis ranking

- Critical path:
  1. Define knowledge graph with probabilistic edges
  2. Add inference rules linking words to concepts
  3. Model utterance as evidence: `evidence(say(mary, sentence(idontDrinkED)))`
  4. Run queries representing relevance expectations
  5. Extract posterior probabilities; compute KL divergence from priors
  6. Rank hypotheses: eliminate unsatisfied, prefer smaller Herbrand base if KL equal, select highest KL

- Design tradeoffs:
  - **Manual hypothesis construction**: Model requires experimenter to pre-define candidate interpretations (Int 1–5); hypothesis generation is not automated (explicit limitation, section 6).
  - **Fixed knowledge graphs**: Encyclopedia knowledge is hand-crafted; doesn't learn or adapt from data.
  - **Effort proxy via Herbrand base**: Processing effort approximated by atom count—coarse but tractable.
  - **Audience-only focus**: Speaker's perspective (utterance choice) not modeled; comprehension side only.

- Failure signatures:
  - Query probability = 0: indicates missing knowledge graph paths; KL divergence becomes infinite.
  - All hypotheses have equal KL: suggests relevance expectations underspecified or knowledge graph insufficiently differentiated.
  - Excessive Herbrand base without KL gain: hypothesis includes superfluous context (like Int 3) but no additional effects.

- First 3 experiments:
  1. **Replicate Int 1 vs Int 3**: Run the provided ProbLog code for both hypotheses; verify that Int 1 has higher KL divergence despite identical inferences, confirming effort penalty from larger Herbrand base.
  2. **Add new beverage**: Extend knowledge graph with `decafCoffee`; test whether `query(wantsNotDrink(mary, decafCoffee))` returns lower probability than regular coffee, probing the "tiredness-blocking" concept boundary.
  3. **Test relevance expectation variation**: Modify queries to exclude secondary beverages (keep only `coffee`); confirm Int 2 no longer produces infinite KL—demonstrates expectation-dependence of interpretation quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the probabilistic framework be expanded to model the speaker's production process, specifically the metacognitive estimation of an audience's relevance expectations?
- Basis in paper: [explicit] The conclusion states that extending the model to the speaker's perspective requires understanding how communicators estimate relevance for others.
- Why unresolved: The current model focuses exclusively on the comprehension (listener) phase, abstracting away from the asymmetric cognitive responsibilities of the speaker.
- What evidence would resolve it: A computational model capable of generating optimal utterances by successfully predicting audience relevance expectations in novel contexts.

### Open Question 2
- Question: Do the quantitative predictions regarding the selection of interpretive hypotheses derived from the ProbLog model correspond to human behavioral data in implicature resolution tasks?
- Basis in paper: [explicit] Section 5.4.6 suggests the optimization model produces quantitative predictions that "could be used in experiments investigating the heuristic nature of relevance-guided comprehension."
- Why unresolved: The paper provides a computational proof-of-concept but does not conduct empirical validation to confirm if the model's output matches actual human processing constraints.
- What evidence would resolve it: Correlation data between the model's KL-divergence rankings and human response times or interpretation choices in controlled experiments.

### Open Question 3
- Question: Can the manual construction of interpretive hypotheses and accessibility orders be automated to handle the open-ended nature of encyclopedic knowledge?
- Basis in paper: [inferred] The authors note in Section 4.2 that "hypothesis generation" is distinct from evaluation and currently requires the experimenter to manually supply and order hypothesis triplets.
- Why unresolved: The model relies on a manually curated knowledge graph and hypothesis set, limiting its scalability and ability to process naturalistic, unrestricted text.
- What evidence would resolve it: An implementation that dynamically retrieves and ranks contextual assumptions from a large-scale knowledge base without manual intervention.

## Limitations
- Manual hypothesis construction and knowledge graph creation limit scalability and automation.
- Edge probabilities and prior distributions are not empirically grounded, raising questions about generalizability.
- The "effort" proxy (Herbrand base size) is coarse and may not accurately capture cognitive processing costs.

## Confidence
- **High confidence**: The core mechanism of parallel explicature-implicature adjustment and KL divergence as relevance measure is well-supported by theoretical argumentation.
- **Medium confidence**: The computational implementation demonstrates the approach but depends on hand-crafted inputs whose optimality is unproven.
- **Low confidence**: Generalizability to complex, real-world conversations remains untested; the coffee example is a controlled case study.

## Next Checks
1. **Scalability test**: Apply the model to a corpus of annotated conversational exchanges to assess whether manual hypothesis construction remains tractable.
2. **Probability calibration**: Compare model-predicted probabilities with empirical measures of interpretation likelihood from psycholinguistic experiments.
3. **Effort refinement**: Test alternative effort proxies (e.g., parse tree complexity, dependency distance) against the Herbrand base measure to validate the approximation.