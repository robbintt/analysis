---
ver: rpa2
title: 'Understand, Think, and Answer: Advancing Visual Reasoning with Large Multimodal
  Models'
arxiv_id: '2505.20753'
source_url: https://arxiv.org/abs/2505.20753
tags:
- visual
- reasoning
- data
- arxiv
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Griffon-R, a Large Multimodal Model (LMM)
  enhanced with a unified visual reasoning mechanism. The key innovation is a human-like
  "understand-think-answer" process that enables the model to solve complex visual
  reasoning tasks in a single forward pass without external tools or multiple inferences.
---

# Understand, Think, and Answer: Advancing Visual Reasoning with Large Multimodal Models

## Quick Facts
- arXiv ID: 2505.20753
- Source URL: https://arxiv.org/abs/2505.20753
- Reference count: 40
- Primary result: State-of-the-art visual reasoning with 70.9% on VSR, 63.7% on CLEVR

## Executive Summary
This paper introduces Griffon-R, a Large Multimodal Model (LMM) enhanced with a unified visual reasoning mechanism. The key innovation is a human-like "understand-think-answer" process that enables the model to solve complex visual reasoning tasks in a single forward pass without external tools or multiple inferences. This approach leverages the model's intrinsic capabilities such as grounding and visual understanding to analyze questions, gather relevant information, and reason step-by-step. To train this mechanism, the authors curate 334K high-quality visual reasoning samples across general and text-rich scenes using a semi-automatic expert-supervised data engine. Experimental results show Griffon-R achieves state-of-the-art performance on compositional visual reasoning benchmarks like VSR (70.9%) and CLEVR (63.7%), while also excelling on multimodal tasks including MMBench (79.0%) and ScienceQA (87.0%). The model demonstrates superior understanding quality in referring expression comprehension tasks and offers significant efficiency advantages over toolkit-based approaches.

## Method Summary
Griffon-R is built on the Griffon v2 architecture with a CLIP-ViT-L/14-336 visual encoder (interpolated to 1022 resolution), Gemma-9B LLM, and a 3×3 conv projector. The model is trained through a three-stage process: (I) projector pretraining on 1.2M captions, (II) whole-model pretraining on 3.0M perception data, and (III) fine-tuning on 3.9M mixed data including 334K curated visual reasoning samples. The visual reasoning data is collected using a semi-automatic expert-supervised data engine that combines AI-generated annotations (via Qwen2-VL-72B) with human expert review and completion. The core innovation is the unified "understand-think-answer" mechanism that enables single forward pass reasoning by generating intermediate understanding and thinking context before producing the final answer.

## Key Results
- State-of-the-art performance on VSR (70.9%) and CLEVR (63.7%) compositional visual reasoning benchmarks
- Strong multimodal task performance: MMBench (79.0%), ScienceQA (87.0%), TextVQA (72.4%), POPE (89.3%)
- Superior understanding quality with 93.2/95.1/90.2 accuracy on RefCOCO/RefCOCO+/RefCOCOg benchmarks
- 13× efficiency advantage over toolkit-based approaches (0.336s vs 4.586s per sample on V-Star_Spat.)

## Why This Works (Mechanism)

### Mechanism 1: Sequential Context Generation for Self-Prompting
The model generates intermediate "understanding" and "thinking" context that self-prompts more accurate final answers. Instead of direct question-to-answer mapping, the model autoregressively generates problem analysis and information-gathering instructions with visual cues, then self-prompted reasoning, then the final answer. This is formalized as p([X_U, X_T, X_ans]) = ∏p(x_i | I, Q, X_<i). The core assumption is that the model can effectively use its own generated visual cues (coordinate-format references) as contextual grounding without additional forward passes.

### Mechanism 2: Intrinsic Capability Unification Without Tool Calling
Leveraging built-in visual capabilities (grounding, captioning, text recognition) in a unified process outperforms toolkit-based approaches that call external specialist models. The model is trained to generate structured instructions that trigger its intrinsic capabilities, producing visual cues within the same generation stream. The core assumption is that the model's intrinsic grounding and perception capabilities are sufficiently accurate that external tool augmentation is unnecessary.

### Mechanism 3: Semi-Automatic Data Curation with Expert Supervision
A hybrid AI-human annotation pipeline produces higher-quality visual reasoning training data than fully automatic or fully manual approaches. The process involves AI Expert (Qwen2-VL-72B) generating problem analysis and planning annotations, followed by human experts completing tasks AI struggles with (multi-object grounding) and reviewing/correcting AI outputs. The core assumption is that initial high-quality annotations from human experts enable better scaling than purely synthetic data.

## Foundational Learning

- **Concept: Autoregressive Language Modeling**
  - Why needed: Griffon-R's entire mechanism relies on next-token prediction generating the understand→think→answer sequence in one pass
  - Quick check: Can you explain why p(X) = ∏p(x_i | context) enables the model to generate intermediate reasoning before the final answer?

- **Concept: Visual Grounding / Referring Expression Comprehension (REC)**
  - Why needed: The "understand" phase depends on the model's intrinsic ability to localize objects given textual descriptions
  - Quick check: Given an image and the phrase "the red balloon on the left," can you describe what output format a grounding-capable model would produce?

- **Concept: Shortcut Learning in Neural Networks**
  - Why needed: The paper explicitly contrasts its approach against "shortcut learning" where models directly predict answers without compositional reasoning
  - Quick check: Why might a model that achieves high accuracy on simple VQA still fail on compositional questions like "Is the white balloon above or below the red balloon?"

## Architecture Onboarding

- **Component map:** Image (up to 1022×1022) → High-resolution Visual Encoder (CLIP-ViT-L/14-336, interpolated) → Vision-Language Connector (3×3 conv, stride 2, compresses tokens) → LLM (Gemma-9B) → Autoregressive Output [Understand context | Think context | Answer]

- **Critical path:** Data curation produces 334K samples with problem analysis, grounding hints, and reasoning chains → Three-stage training with 1.2M (Stage I), 3.0M (Stage II), and 3.9M (Stage III) samples → Inference: Single forward pass generates complete reasoning sequence

- **Design tradeoffs:** Single-pass inference (0.336s/sample) vs toolkit-based (4.586s/sample), but unified approach may struggle with tasks requiring capabilities the model lacks; 334K human-curated samples vs potential for larger synthetic datasets with lower quality; Strong visual reasoning while maintaining general multimodal performance

- **Failure signatures:** Hallucination with confidence (model outputs "none" when relevant objects not found but may still produce confident wrong answers); Sequence length explosion in complex scenes with many question-relevant objects; Inherited AI Expert biases from Qwen2-VL-72B

- **First 3 experiments:** Verify grounding quality baseline by evaluating on RefCOCO/RefCOCO+/RefCOCOg benchmarks; Ablate mechanism components by comparing full understand-think-answer vs direct answering vs understand-answer without think; Efficiency stress test by measuring inference time scaling as number of relevant objects increases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inference latency of Griffon-R scale relative to toolkit-based methods in scenarios requiring the association of a large number of objects?
- Basis: Section C (Limitations) notes that output sequence growth can lead to increased response time when question-related objects are associated
- Why unresolved: While demonstrating 13× speedup, the paper does not quantify performance degradation in dense, complex association scenarios
- What evidence would resolve it: Comparative latency analysis on datasets with high object density showing the relationship between reasoning steps/objects and inference time

### Open Question 2
- Question: Can the proposed mechanism effectively scale to specialized domains such as mathematics or accounting by incorporating specific new capabilities?
- Basis: Appendix A.4 states it is "possible to include more capabilities into our set and curated related data to support scenes like math, accounting, etc."
- Why unresolved: The paper does not demonstrate the mechanism's adaptability to domains requiring abstract symbolic reasoning or specific domain knowledge
- What evidence would resolve it: Experimental results on domain-specific benchmarks after training with the proposed data engine on mathematical or symbolic datasets

### Open Question 3
- Question: What distinct "additional reasoning paradigms" beyond the current "understand-think-answer" structure could further enhance generalizability?
- Basis: The Conclusion states "we plan to explore additional reasoning paradigms and more diverse data to extend the applicability of our approach"
- Why unresolved: The authors leave exploration of alternative cognitive architectures or reasoning frameworks as an open next step
- What evidence would resolve it: A study integrating alternative reasoning structures (e.g., tree-of-thoughts) into the single-pass framework and evaluating performance on novel, out-of-distribution tasks

## Limitations
- The semi-automatic data curation pipeline's scalability and quality maintenance beyond 334K samples remains unproven
- Efficiency advantage claim may not hold when scaling to more complex scenes with many relevant objects
- The unified mechanism's performance advantage may be dataset-specific and could degrade on tasks requiring capabilities the model lacks entirely

## Confidence
- **High Confidence:** Empirical results showing state-of-the-art performance on compositional visual reasoning benchmarks (VSR 70.9%, CLEVR 63.7%)
- **Medium Confidence:** Efficiency advantage claim (13× speedup) supported by runtime measurements but may not scale to complex scenes
- **Low Confidence:** Semi-automatic data curation methodology's scalability and generalizability to entirely new task categories

## Next Checks
1. **Generalization Stress Test:** Evaluate Griffon-R on novel compositional reasoning tasks not represented in the training data to assess whether the understand-think-answer mechanism transfers beyond its training distribution

2. **Toolkit Capability Gap Analysis:** Systematically identify and quantify specific visual reasoning capabilities that external tool-based approaches can perform but Griffon-R's unified mechanism cannot

3. **Human Evaluation of Reasoning Quality:** Conduct blinded human studies comparing Griffon-R's intermediate "understanding" and "thinking" outputs against ground truth annotations to verify that the self-prompting mechanism produces genuinely useful intermediate representations