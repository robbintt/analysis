---
ver: rpa2
title: Solving General-Utility Markov Decision Processes in the Single-Trial Regime
  with Online Planning
arxiv_id: '2505.15782'
source_url: https://arxiv.org/abs/2505.15782
tags:
- policy
- objective
- single-trial
- state
- occupancy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of solving infinite-horizon discounted
  general-utility Markov decision processes (GUMDPs) in the single-trial regime, where
  an agent's performance is evaluated based on a single trajectory. The key contribution
  is showing that non-Markovian policies are required for optimality in this setting,
  unlike standard MDPs.
---

# Solving General-Utility Markov Decision Processes in the Single-Trial Regime with Online Planning

## Quick Facts
- arXiv ID: 2505.15782
- Source URL: https://arxiv.org/abs/2505.15782
- Reference count: 40
- This paper addresses solving infinite-horizon discounted GUMDPs in the single-trial regime where non-Markovian policies are required for optimality, and proposes an MCTS-based online planning approach that outperforms baselines.

## Executive Summary
This paper tackles the challenge of solving infinite-horizon discounted General-Utility Markov Decision Processes (GUMDPs) in the single-trial regime, where an agent's performance is evaluated based on a single trajectory rather than expected returns over multiple trials. The key insight is that non-Markovian policies are required for optimality in this setting, unlike standard MDPs where stationary policies suffice. The authors introduce an "occupancy MDP" that compresses the full history into a running occupancy vector, proving this formulation preserves optimality. They then propose a Monte Carlo Tree Search (MCTS) algorithm to solve this occupancy MDP through online planning, demonstrating superior performance across multiple tasks and environments.

## Method Summary
The approach constructs an occupancy MDP where states are augmented with a running occupancy vector tracking discounted visitation frequencies of state-action pairs. This transforms the history-dependent GUMDP into a solvable MDP while preserving optimality. The method uses MCTS with 4000 iterations per timestep to solve this occupancy MDP online, handling the sparse final-reward structure where costs are zero until the terminal horizon H. The algorithm evaluates the final utility f of the simulated occupancies at leaf nodes and backpropagates these values through the search tree.

## Key Results
- Non-Markovian policies are strictly required for optimality in single-trial GUMDPs, unlike standard infinite-trial MDPs
- The occupancy MDP formulation preserves optimality while compressing history into a tractable state representation
- MCTS-based online planning outperforms relevant baselines including policies optimized for infinite-trials setting
- Superior performance demonstrated across maximum entropy exploration, imitation learning, and adversarial MDP tasks

## Why This Works (Mechanism)

### Mechanism 1: Non-Markovianity for Empirical Utility
In single-trial regime, optimal policies must be non-Markovian because the objective f is a non-linear function of occupancy measure. Since f(E[d]) ≠ E[f(d)] for non-linear f, the agent must condition actions on history to influence the specific trajectory's occupancy.

### Mechanism 2: Occupancy MDP (State Augmentation)
The history-dependent GUMDP is cast as standard MDP using running occupancy vector as state. This compresses full history into sufficient statistic that preserves all information needed for optimality, transforming POMDP-like problem into solvable MDP.

### Mechanism 3: MCTS for Sparse, Final-Reward Optimization
MCTS handles sparse final-reward structure by building look-ahead tree where value is backpropagated from terminal nodes (evaluated by f(d)) to current decision node, optimizing single-trajectory utility online despite computational hardness.

## Foundational Learning

- **Concept: General-Utility MDPs (GUMDPs)**
  - Why needed: Generalizes RL beyond additive rewards to optimize function f over distribution of state visits
  - Quick check: If I visit state A twice and state B once, does my utility depend on the order I visited them? (In GUMDPs, generally no, it depends on final frequencies)

- **Concept: Discounted Occupancy Measures**
  - Why needed: Core state representation of Occupancy MDP tracks this measure
  - Quick check: If I take action now vs. at step t=100, how does discount factor γ change contribution to final occupancy vector? (Answer: γ^t weight)

- **Concept: Sample Average vs. Expectation (Jensen's Inequality)**
  - Why needed: Mathematical root of why single-trial and infinite-trial optimization differ
  - Quick check: For convex function f, is f(average(x)) larger or smaller than average(f(x))? (Answer: Smaller. This mismatch drives need for new algorithm)

## Architecture Onboarding

- **Component map:** Environment -> State Wrapper (augments with occupancy vector) -> MCTS Planner -> Controller
- **Critical path:** Update logic for vector o_t is most sensitive component. The paper defines update as o_{t+1}(s,a) = γ^t + o_t(s,a) if current state-action is (s,a), else o_t(s,a).
- **Design tradeoffs:** Horizon Truncation (H) - larger H improves optimality but exponentially increases search space. Iterations - more MCTS iterations improve action quality but linearly increase compute time.
- **Failure signatures:** "Flat" MCTS performance indicates utility function f not applied correctly at leaf nodes only. State Space Explosion occurs when |S||A| is large, causing memory issues.
- **First 3 experiments:** 1) Verify non-Markovianity by implementing PPO/DQN agent and confirming it fails to match MCTS performance. 2) Ablation on horizon by running MCTS with varying H and plotting regret against H. 3) Test robustness to stochasticity on FrozenLake environment.

## Open Questions the Paper Calls Out

- Can running occupancy vector be compressed (e.g., via state aggregation) to handle large state-action spaces without sacrificing optimality?
- Can occupancy MDP formulation and MCTS approach be extended to environments with continuous state spaces?
- Do optimality guarantees hold in finite-horizon undiscounted setting?

## Limitations
- Computational complexity scales with |S||A|, making approach infeasible for large state-action spaces
- Limited empirical evaluation to small-scale environments, leaving scalability questions unanswered
- Does not address practical implementation challenges like MCTS hyperparameter sensitivity

## Confidence

**High Confidence**: Theoretical framework is rigorous with mathematically proven theorems establishing equivalence between original GUMDP and occupancy MDP.

**Medium Confidence**: Experimental results demonstrate effectiveness in controlled environments with statistically significant improvements over baselines.

**Low Confidence**: Practical implementation challenges not addressed, including computational requirements for large state spaces and robustness to partial observability.

## Next Checks

1. **Scalability Test**: Implement algorithm on larger state space (e.g., 50x50 grid) and measure growth in computational time and memory usage, quantifying performance degradation.

2. **Hyperparameter Sensitivity**: Conduct systematic study of MCTS hyperparameters on representative task to determine sensitivity of algorithm's performance.

3. **Robustness to Partial Observability**: Modify environment to include partial observability and evaluate algorithm's performance compared to history-based baseline.