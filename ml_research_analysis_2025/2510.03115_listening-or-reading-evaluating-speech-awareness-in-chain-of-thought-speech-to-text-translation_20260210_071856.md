---
ver: rpa2
title: Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text
  Translation
arxiv_id: '2510.03115'
source_url: https://arxiv.org/abs/2510.03115
tags:
- speech
- translation
- training
- s2tt
- transcription
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether Chain-of-Thought (CoT) speech-to-text
  translation (S2TT) models can effectively leverage speech information beyond transcripts,
  overcoming limitations of cascade systems like error propagation and ignoring prosody.
  The authors evaluate CoT models using interpretability methods (Value Zeroing) to
  quantify speech contribution, robustness tests with corrupted transcripts, and prosody-awareness
  benchmarks (CONTRAPROST).
---

# Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation

## Quick Facts
- **arXiv ID**: 2510.03115
- **Source URL**: https://arxiv.org/abs/2510.03115
- **Reference count**: 0
- **Primary result**: Chain-of-Thought S2TT models rely mainly on transcripts and minimally on speech, but training interventions can improve speech utilization and robustness.

## Executive Summary
This paper investigates whether Chain-of-Thought (CoT) speech-to-text translation (S2TT) models can effectively leverage speech information beyond transcripts, overcoming limitations of cascade systems like error propagation and ignoring prosody. The authors evaluate CoT models using interpretability methods (Value Zeroing) to quantify speech contribution, robustness tests with corrupted transcripts, and prosody-awareness benchmarks (CONTRAPROST). They find that standard CoT models largely mimic cascade behavior, relying mostly on transcripts and minimally on speech. Simple training interventions—adding Direct S2TT data (DUAL) or injecting noisy transcripts (NOISY)—improve speech attribution and robustness, with NOISY showing the largest gains. However, prosody-awareness remains low across models, suggesting that acoustic information is not well integrated. Overall, CoT does not inherently provide the hypothesized benefits unless explicitly trained to use speech, highlighting the need for architectures that better integrate acoustic cues.

## Method Summary
The authors evaluate Chain-of-Thought speech-to-text translation models through three interconnected analyses. First, they quantify speech contribution using Value Zeroing attribution, which measures how much each input modality (speech DSUs, transcript, previous translation tokens) influences the model's output. Second, they test robustness by corrupting transcripts with increasing percentages of unrelated words (2.5-30%) to see if models relying on speech show smaller performance drops. Third, they assess prosody awareness using CONTRAPROST, a benchmark with minimal lexical differences but varying prosodic patterns. The study compares three model variants: BASE (standard CoT training), DUAL (adds Direct S2TT data to training), and NOISY (injects noisy transcripts during training). Speech is discretized into 500 discrete speech units (DSUs) using mHuBERT and k-means clustering, enabling unified token-level attribution analysis alongside text.

## Key Results
- Standard CoT models (BASE) show minimal speech contribution (0.0228) and behave like cascades, relying mainly on transcripts.
- Training interventions improve speech attribution: DUAL increases contribution to 0.035 (1.54× BASE) and NOISY to 0.051 (2.24× BASE).
- NOISY training provides the most robust performance, showing near-flat degradation curves even with 30% transcript corruption.
- All models perform poorly on CONTRAPROST (directional scores <40%), indicating low prosody awareness regardless of training approach.

## Why This Works (Mechanism)

### Mechanism 1: Training Data Format Shapes Modality Reliance
When transcripts are always available and reliable during training, gradient descent learns to condition primarily on text. Adding Direct S2TT samples creates pressure to extract meaning from DSUs alone, while noisy transcript injection reduces transcript reliability as a signal, forcing the model to attend more to speech to minimize loss. Core assumption: Model behavior reflects statistical regularities in training data; models take shortcuts when available. Evidence: BASE shows 0.0228 speech contribution vs NOISY's 0.051 (2.24× higher), and NOISY displays near-flat robustness curves even with 30% transcript corruption.

### Mechanism 2: Discrete Speech Units Enable Token-Level Attribution Analysis
mHuBERT encodes speech → k-means clusters representations into 500 DSUs → DSUs mapped to private-use Unicode characters → LLM processes them alongside text tokens. This unified token space permits Value Zeroing to compute contribution scores per-modality by aggregating over token spans. Core assumption: DSUs preserve sufficient semantic information for translation; attribution on tokens reflects meaningful information flow. Evidence: The unified token space enables attribution analysis that would be impossible with continuous speech features, though DSUs may lose prosodic detail.

### Mechanism 3: Robustness Tests Reveal Implicit Speech Usage
Corrupted transcripts create a controlled mismatch. If speech representations are integrated, the model can cross-verify and compensate. NOISY training explicitly prepares models for this scenario, resulting in near-flat degradation curves. Core assumption: Speech contains redundant semantic information that can compensate for transcript corruption. Evidence: NOISY shows minimal degradation (near-flat curve) even with 30% transcript corruption, while BASE degrades sharply, indicating genuine speech integration.

## Foundational Learning

- **Discrete Speech Units (DSUs)**
  - Why needed here: Core architectural choice enabling speech-as-tokens; understanding this is essential for how attribution works or why prosody is lost.
  - Quick check: Can you explain why DSUs enable Value Zeroing attribution but may lose prosodic detail?

- **Value Zeroing Attribution**
  - Why needed here: Primary interpretability method used; results depend on understanding what it measures (token contribution) vs. doesn't (information theoretic causality).
  - Quick check: How does Value Zeroing differ from inspecting attention weights, and why is it more reliable?

- **Cascade vs. CoT Inference Paradigms**
  - Why needed here: Paper's central comparison; the "self-cascade" design where same model does both steps is non-obvious.
  - Quick check: In CASCADE mode, why is the transcript generated first then translation conditioned only on that transcript, not speech?

## Architecture Onboarding

- **Component map**:
  Speech Audio → mHuBERT (frozen encoder, layer 11) → k-means (500 centroids) → DSUs
  DSUs + Text → SALAMANDRATA-7B (expanded vocab) → Translation Output

- **Critical path**: Embedding adaptation quality → DSU representation alignment → whether model can learn to use speech. Paper uses next-token prediction on speech-only data with frozen LLM, updating only embeddings and output projection.

- **Design tradeoffs**:
  - DSU vocabulary size (500 centroids): Smaller = coarser granularity, may lose acoustic detail; larger = more parameters, harder to train.
  - Training data mixture: Too much Direct → worse ASR quality; too much CoT → speech ignored. Paper uses 25%/75% for DUAL.
  - Speech encoder freezing: Reduces compute but locks in representations; may limit adaptation to translation task.

- **Failure signatures**:
  - Speech attribution near zero across all layers → model learned cascade shortcut (BASE pattern).
  - Sharp performance drop with transcript corruption → no speech integration.
  - Near-random CONTRAPROST scores → prosody not captured in DSUs or not learned by model.

- **First 3 experiments**:
  1. Replicate attribution analysis: Train BASE model, run Value Zeroing on FLEURS translations, verify speech contribution ~0.02.
  2. Ablate NOISY corruption ratio: Test 10%, 25%, 50% corruption rates to find optimal sweet spot.
  3. Pilot prosody probe before full training: Translate CONTRAPROST pairs using a checkpoint; if Directional score <35%, prosody integration requires explicit architectural changes.

## Open Questions the Paper Calls Out

### Open Question 1
Why does layer 4 consistently show a peak in speech attribution scores across all model variants? The authors observe this pattern but do not investigate the underlying mechanism or significance of this specific layer's behavior.

### Open Question 2
What architectural modifications would enable effective integration of prosodic information in CoT S2TT models? Current discrete speech units may lose prosodic detail, and neither training intervention substantially improved prosody awareness scores.

### Open Question 3
What scale and diversity of training data are required for models to implicitly learn prosody-aware translation? The study uses a constrained dataset, leaving unclear whether scaling alone would address prosody limitations.

### Open Question 4
How does source language affect speech awareness and prosody integration in CoT S2TT? The study restricts source speech to English, acknowledging potential cross-language effects but not exploring them.

## Limitations
- Attribution methods measure token contribution rather than true causal information flow, potentially conflating correlated inputs with actual usage.
- Discrete Speech Units (DSUs) may systematically discard prosodic information critical for translation quality due to coarse 500-centroid discretization.
- Findings may not generalize beyond read speech datasets (FLEURS, CONTRAPROST) to spontaneous speech with richer prosody.

## Confidence
- **High confidence**: Standard CoT training leads to cascade-like behavior ignoring speech (consistent attribution scores and robustness test degradation curves).
- **Medium confidence**: NOISY training improves speech attribution and robustness (measurable increases but unclear mechanism).
- **Medium confidence**: CoT does not inherently provide better speech utilization than cascades without explicit training interventions (uniformly low CONTRAPROST scores across all models).

## Next Checks
1. Validate attribution reliability with controlled information ablation: Systematically remove specific acoustic features from speech input before DSU discretization and measure attribution score changes.
2. Test cross-linguistic generalization: Evaluate same model architectures on a language pair with rich prosody-dependent grammatical distinctions (e.g., Mandarin tone sandhi).
3. Benchmark against end-to-end cascade degradation: Compare BASE model's robustness to natural ASR errors versus controlled corruption to test real-world applicability.