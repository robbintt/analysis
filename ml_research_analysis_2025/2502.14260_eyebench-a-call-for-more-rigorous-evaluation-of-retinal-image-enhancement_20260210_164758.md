---
ver: rpa2
title: 'EyeBench: A Call for More Rigorous Evaluation of Retinal Image Enhancement'
arxiv_id: '2502.14260'
source_url: https://arxiv.org/abs/2502.14260
tags:
- images
- uni00000013
- uni00000011
- evaluation
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces EyeBench, a comprehensive evaluation benchmark
  for retinal image enhancement methods. The key contributions include: (1) Multi-dimensional
  clinical alignment downstream evaluation - incorporating clinically significant
  tasks like vessel segmentation, DR grading, denoising generalization, and lesion
  segmentation to assess enhancement quality.'
---

# EyeBench: A Call for More Rigorous Evaluation of Retinal Image Enhancement

## Quick Facts
- **arXiv ID**: 2502.14260
- **Source URL**: https://arxiv.org/abs/2502.14260
- **Reference count**: 40
- **Primary result**: Introduces EyeBench, a comprehensive evaluation benchmark for retinal image enhancement methods incorporating clinically significant downstream tasks and expert-guided protocols.

## Executive Summary
EyeBench addresses the critical need for rigorous evaluation of retinal image enhancement methods by introducing a multi-dimensional benchmark that goes beyond traditional pixel-level metrics. The framework evaluates enhancement quality through clinically relevant downstream tasks including vessel segmentation, diabetic retinopathy grading, denoising generalization, and lesion segmentation, combined with expert manual assessments. The benchmark demonstrates that comprehensive evaluation better aligns with clinical preferences compared to single-dimension approaches, revealing important insights about method performance across different scenarios.

## Method Summary
The EyeBench framework consists of a comprehensive evaluation suite for retinal image enhancement methods. It uses the EyePACS dataset with EyeQ quality annotations, creating separate training and testing sets for paired and unpaired enhancement approaches. The benchmark employs synthetic degradation algorithms to create paired data for full-reference evaluation and real-world noisy images for no-reference evaluation. Methods are assessed through multiple dimensions: traditional denoising metrics (PSNR, SSIM), downstream clinical task performance (vessel segmentation, DR grading, lesion segmentation), representation quality metrics (FID scores), and expert manual evaluation protocols. The framework includes both paired methods (SCR-Net, RFormer) and unpaired methods (CycleGAN, OTEGAN, CUNSB-RFIE) to provide comprehensive coverage of enhancement approaches.

## Key Results
- Paired methods generally outperform unpaired methods in full-reference settings, while unpaired methods show strong performance in no-reference scenarios
- Multi-dimensional evaluation closely aligns with clinical preferences compared to single-dimension evaluations
- SDE-based generative models tend to smooth high-frequency details like lesions during iterative denoising, potentially degrading diagnostic utility
- Unpaired GAN-based methods face a critical trade-off between removing noise and preserving anatomical structure, controlled by regularization weights

## Why This Works (Mechanism)

### Mechanism 1
Evaluating retinal image enhancement through multi-dimensional downstream clinical tasks provides more rigorous alignment with clinical utility than traditional pixel-level metrics. Enhancement models are optimized on their ability to preserve or improve features critical for clinical diagnosis, measured through downstream network performance on enhanced images. High PSNR does not guarantee structural preservation of lesions.

### Mechanism 2
SDE-based generative models smooth high-frequency details (like lesions) during iterative denoising. The probabilistic transport process progressively deactivates or smooths high-frequency regions to model the bridge between low and high-quality distributions, focusing on smooth modal-distribution modeling at the expense of fine lesion details.

### Mechanism 3
In unpaired GAN-based enhancement, there is a critical trade-off controlled by regularization weights between removing noise and preserving anatomical structure. If regularization is too strong, the model cannot alter the image enough to remove noise; if too weak, it may "hallucinate" or alter lesions.

## Foundational Learning

- **Full-Reference vs. No-Reference Evaluation**: Why needed - The paper splits experiments based on this distinction. Full-reference allows pixel-comparison (PSNR), while no-reference requires task-based or feature-based metrics. Quick check - Can you explain why PSNR is calculable in the Full-Reference dataset but impossible in the No-Reference dataset?

- **Unpaired Image-to-Image Translation (GANs & Optimal Transport)**: Why needed - The benchmark evaluates unpaired methods which treat enhancement as a domain adaptation problem. Understanding how generators map domains without paired data is crucial for interpreting trade-off results. Quick check - How does CycleGAN enforce consistency when it lacks paired training data?

- **FrÃ©chet Inception Distance (FID) in Medical Context**: Why needed - The paper utilizes FID-Retfound and FID-Clip to measure distance between enhanced and real image distributions. Quick check - Why does the paper argue that standard Inception-v3 FID is insufficient, necessitating FID-Retfound?

## Architecture Onboarding

- **Component map**: EyePACS/EyeQ dataset -> Degradation algorithms (illumination, blur) -> Paired/Unpaired enhancement models -> Downstream evaluations (U-Net, MobileNet) -> Expert protocols (LPR/BPR/SPR)

- **Critical path**: 1) Prepare Data (Resample EyeQ -> Create disjoint A/B sets -> Synthesize A*) 2) Train Enhancement Models (Paired on A/A*, Unpaired on A*/B) 3) Run Inference (Generate enhanced images) 4) Train/Infer Downstream Tasks 5) Correlate results with Expert Protocols

- **Design tradeoffs**: Synthetic vs. Real Noise - Using synthetic degradation allows objective PSNR/SSIM measurement but may not capture real clinical noise complexity. Paired vs. Unpaired Fairness - Dataset splits A and B strictly to ensure unpaired methods don't "see" the target distribution during training.

- **Failure signatures**: Over-smoothing (SDEs) - High PSNR but low lesion segmentation accuracy; Hallucination (GANs) - High visual quality but incorrect DR grading; Low Correlation - High PSNR but low Expert Preference indicates pixel-accurate but clinically irrelevant enhancement.

- **First 3 experiments**: 1) Train SCR-Net and CycleGAN, calculate PSNR/SSIM on Full-Reference test set 2) Take enhanced images, train U-Net on DRIVE, evaluate on enhanced DRIVE images 3) Run OTEGAN on No-Reference data, calculate FID-Retfound, inspect LPR correlation

## Open Questions the Paper Calls Out

### Open Question 1
Can the EyeBench evaluation framework be effectively extended to include unsupervised traditional algorithms and applied to MRI enhancement tasks? The authors state in "Limitations and Future Work" that current evaluations are limited to deep learning and they plan to expand to traditional algorithms and MRI tasks.

### Open Question 2
How can SDE-based generative models be modified to prevent smoothing of high-frequency clinical details, such as retinal lesions, during iterative denoising? The paper identifies this progressive degradation but does not propose a structural solution to retain high-frequency information.

### Open Question 3
What adaptive weighting strategies can effectively balance the trade-off between structural preservation and noise removal in Optimal Transport (OT) based GANs? Current methods rely on fixed weights, risking either incomplete noise modeling or misclassification of lesions/vessels as noise.

## Limitations
- Dataset representativeness - EyePACS data with EyeQ annotations may not capture full spectrum of clinical imaging conditions and disease presentations
- Downstream task proxy validity - Correlation between automated task performance and actual clinical diagnostic accuracy remains unproven
- Expert evaluation protocol - Manual evaluation protocol details (number of annotators, experience levels, inter-rater reliability) are not fully specified

## Confidence

- **High Confidence**: Paired methods outperform unpaired methods in full-reference settings
- **Medium Confidence**: Unpaired methods show strong performance in no-reference scenarios
- **Medium Confidence**: Multi-dimensional evaluation better aligns with clinical preferences than single-dimension evaluations

## Next Checks

1. **Clinical validation study**: Conduct blinded study with multiple retinal specialists comparing diagnostic accuracy on real patient cases using enhanced images versus original low-quality images.

2. **Cross-dataset generalization test**: Evaluate top enhancement methods on completely independent retinal datasets to assess performance beyond EyePACS distribution.

3. **Ablation study on downstream task selection**: Systematically remove each downstream task to measure impact on overall method rankings and quantify each task's contribution.