---
ver: rpa2
title: 'ReToP: Learning to Rewrite Electronic Health Records for Clinical Prediction'
arxiv_id: '2601.19286'
source_url: https://arxiv.org/abs/2601.19286
tags:
- clinical
- prediction
- rewriter
- patient
- retop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of clinical prediction using
  Electronic Health Records (EHRs), which are high-dimensional, heterogeneous, and
  sparse. The authors propose ReToP, a framework that uses a Large Language Model
  (LLM)-based EHR rewriter trained to generate clinically relevant rewrites aligned
  with specific prediction tasks.
---

# ReToP: Learning to Rewrite Electronic Health Records for Clinical Prediction

## Quick Facts
- **arXiv ID:** 2601.19286
- **Source URL:** https://arxiv.org/abs/2601.19286
- **Reference count:** 40
- **Primary result:** Achieves up to 23% improvement in AUC-ROC for clinical prediction tasks using task-aligned EHR rewrites.

## Executive Summary
ReToP addresses the challenge of clinical prediction using high-dimensional, heterogeneous, and sparse Electronic Health Records (EHRs) by learning to rewrite EHRs in a task-aligned manner. The framework uses an LLM-based rewriter trained on synthetic data generated through feature-based paraphrasing and aligned with prediction objectives via a Classifier Supervised Contribution (CSC) score. By optimizing the rewriter to generate clinically relevant text that emphasizes task-specific predictive features, ReToP significantly outperforms strong baselines across mortality, readmission, and length-of-stay prediction tasks on MIMIC-IV. The framework demonstrates strong generalizability to unseen datasets and tasks while preserving faithful rewrites that maintain clinical validity.

## Method Summary
ReToP generates synthetic training data by applying 8 paraphrasing operators to EHR feature sets, creating diverse patient rewrites. A task-specific scorer selects the top 25% of rewrites as pseudo-labels to fine-tune the EHR rewriter using LoRA. The framework then aligns the rewriter with prediction objectives using CSC scores that quantify each rewrite's contribution to correct classification. This alignment is achieved by minimizing KL divergence between the rewriter's output distribution and the CSC-weighted distribution. During inference, predictions are interpolated between original EHRs and rewritten versions using a task-specific parameter α, allowing flexible trade-offs between information preservation and noise reduction.

## Key Results
- Achieves up to 23% improvement in AUC-ROC score compared to strong baselines across three clinical tasks.
- Outperforms task-agnostic EHR completion methods by filtering noisy features aligned with specific prediction objectives.
- Demonstrates generalizability to unseen datasets and tasks with minimal fine-tuning while preserving clinically faithful rewrites.
- Shows task-dependent optimal interpolation (α) with mortality benefiting from rewrite preservation (α→1) while readmission/length-of-stay benefit from original preservation (α→0).

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Task-agnostic EHR completion methods can introduce noisy features that hinder prediction; feature filtering aligned with task objectives improves signal-to-noise ratio.
- **Mechanism:** Paraphrasing operators (πh, πd, πr) create candidate rewrites by selecting subsets of clinical features. A scorer trained on task-specific data evaluates each rewrite's predictive utility, retaining only top-k% as pseudo-labels. This filters features that contribute little to the target outcome while preserving task-relevant signals.
- **Core assumption:** Features with higher predictive scores from the scorer generalize to improving downstream classifier performance.
- **Evidence anchors:**
  - [abstract] "To cope with the lack of EHR rewrite training data, we generate synthetic pseudo-labels using clinical-driven feature selection strategies to create diverse patient rewrites for fine-tuning the EHR rewriter."
  - [section 4.1.1] Describes 8 paraphrasing operators grouped by heuristic, data-driven, and random strategies.
  - [corpus] Weak direct evidence; related work (CKD-EHR, EHR-RAG) focuses on retrieval/knowledge distillation rather than rewrite-based feature filtering.
- **Break condition:** If the scorer's feature importance scores correlate poorly with the final predictor's learned importance (e.g., due to distribution shift), pseudo-label quality degrades, reducing rewriter effectiveness.

### Mechanism 2
- **Claim:** Aligning the EHR rewriter's output distribution with the predictor's confidence distribution via KL divergence produces rewrites that directly optimize prediction likelihood.
- **Mechanism:** The CSC score quantifies each rewrite's contribution to correct classification relative to all candidate rewrites. Minimizing KL divergence between the LM's rewrite distribution (pLM) and the CSC-weighted distribution (pCSC) trains the rewriter to generate higher-utility rewrites.
- **Core assumption:** The predictor's confidence on a rewrite is a valid proxy for rewrite quality for the clinical task.
- **Evidence anchors:**
  - [abstract] "ReToP aligns the rewriter with prediction objectives using a novel Classifier Supervised Contribution (CSC) score."
  - [section 4.2.2] Equations 7-10 define CSC score and KL loss minimization procedure.
  - [corpus] No direct corpus evidence for CSC-specific alignment; LSR (LM-Supervised Retrieval) in related work applies similar KL alignment for retrieval, not generation.
- **Break condition:** If τ (temperature) is poorly calibrated, CSC scores become uninformative (too flat or too peaked), causing gradient signals to vanish or overfit to spurious rewrites.

### Mechanism 3
- **Claim:** Interpolating between original EHR and rewritten EHR at inference allows task-specific tradeoffs between information preservation and noise reduction.
- **Mechanism:** Final prediction uses α-weighted combination: p(y|P̃) × α + p(y|P) × (1-α). Tasks with sparse/noisy signals (e.g., mortality) benefit from higher α; tasks with rich signals (e.g., readmission) benefit from lower α.
- **Core assumption:** Optimal α is task-dependent but can be determined via validation.
- **Evidence anchors:**
  - [section 4.2.2, Eq. 2] Defines the α-weighted linear combination.
  - [section 6.4, Figure 3] Shows ROC scores across α values stratified by EHR length; MOR benefits from α → 1, RA/LOS from α → 0.
  - [corpus] No corpus evidence for inference-time interpolation in clinical prediction.
- **Break condition:** If rewrites introduce systematic bias (e.g., consistently omitting rare but critical features), increasing α will degrade performance regardless of task characteristics.

## Foundational Learning

- **Concept: EHR Verbalization (Tabular-to-Text Conversion)**
  - **Why needed here:** ReToP uses LLMs as rewriters; EHR feature-value tuples must be converted to natural language via a verbalizer function v before processing.
  - **Quick check question:** Given an EHR tuple `(blood_pressure, 140/90)`, what markdown-formatted string would the verbalizer produce?

- **Concept: KL Divergence for Distribution Alignment**
  - **Why needed here:** The core training loop minimizes KL divergence between the rewriter's output distribution and the CSC-weighted target distribution, requiring understanding of how this loss shapes generation behavior.
  - **Quick check question:** If pLM assigns uniform probability over all rewrites and pCSC is sharply peaked on one rewrite, what happens to the KL gradient?

- **Concept: LoRA (Low-Rank Adaptation) Fine-Tuning**
  - **Why needed here:** The EHR rewriter is fine-tuned using LoRA with specific hyperparameters (rank=8, α=16), not full fine-tuning. Understanding this is critical for implementation and computational budgeting.
  - **Quick check question:** Why does LoRA reduce trainable parameters compared to full fine-tuning, and what tradeoff does the rank hyperparameter control?

## Architecture Onboarding

- **Component map:**
  1. **Verbalizer**: Converts EHR tuples → markdown text.
  2. **Paraphrasing operators**: 8 operators (πh^t, πh^v, πd^mi, πd^mrmr, πd^rfe, πr^f, πr^v, πr^I) generate candidate rewrites.
  3. **Scorer**: Task-specific classifier trained on Dsub^s to score rewrites.
  4. **EHR Rewriter**: LLM (Llama3-8B or Qwen2.5-7B) fine-tuned on pseudo-labels DRw, then aligned via KL loss.
  5. **Clinical Predictor**: Encoder classifier (ModernBERT-base) trained with BCE loss on augmented data Das.

- **Critical path:**
  1. Generate candidate rewrites using all 8 paraphrasing operators for each patient.
  2. Train Scorer on subset of task data, score candidates, select top-25% as pseudo-labels.
  3. Fine-tune rewriter on DRw using causal LM loss.
  4. Generate rewrites from fine-tuned rewriter, compute CSC scores, apply KL alignment.
  5. Inoculate predictor on small sample of KL-aligned rewrites.
  6. Inference: interpolate original EHR and rewrite using tuned α.

- **Design tradeoffs:**
  - **Synthetic data size vs. quality**: Higher top-k% threshold retains more diverse rewrites but may include lower-quality ones. Paper uses 25%.
  - **λ in loss combination**: Higher λ prioritizes LM fluency over prediction alignment. Paper finds λ ≥ 0.5 works best for MOR (severe imbalance); λ ≤ 0.25 for RA/LOS.
  - **Rewrites per patient (ni)**: More rewrites increase CSC score reliability but raise compute. Paper uses 8.

- **Failure signatures:**
  - **High KL loss with flat CSC distribution**: τ too high, predictor confidence uninformative.
  - **Rewrites lose faithfulness**: Over-alignment (low λ) causes hallucinated features not in original EHR.
  - **Performance degrades with long EHRs**: Figure 3 shows rewrite quality drops for EHRs >4096 tokens; consider chunking or truncation.

- **First 3 experiments:**
  1. **Sanity check**: Train scorer on Dsub^s and verify it assigns higher scores to data-driven paraphrases (πd^mrmr) vs. random (πr^f) on held-out data.
  2. **Ablation**: Run ReToP without KL alignment (w/o KL in Table 3) to quantify alignment contribution; expect 20-40% PRC drop on imbalanced tasks.
  3. **α sweep**: For each task, evaluate ROC/PRC across α ∈ [0, 1] to identify task-specific optimal interpolation; verify MOR peaks near α=1, RA/LOS near α=0.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the ReToP framework be effectively adapted to support multi-label clinical classification tasks where label correlations are significant?
- **Basis in paper:** [explicit] The conclusion states: "Future work could explore extending this framework to a wider range of clinical tasks, including multi-label classification tasks."
- **Why unresolved:** The current mathematical formulation (Eq. 1-3) defines the clinical prediction task as mapping an EHR to a single binary label y_s^i ∈ {0, 1}, and the Classifier Supervised Contribution (CSC) score is designed for single-outcome optimization.
- **What evidence would resolve it:** An extension of the CSC score to account for inter-label dependencies and experimental results on a multi-label dataset (e.g., simultaneous prediction of multiple comorbidities) showing performance retention compared to the binary case.

### Open Question 2
- **Question:** How can the trade-off between prediction accuracy and expert actionability be optimized during the KL alignment phase?
- **Basis in paper:** [explicit] The Case Study (Section 7) reveals that while KL alignment improves prediction, it increases "serendipitous" features (46% labeled 'N' by experts), lowering actionability. The authors conclude: "Future work could [investigate] the right compromise between model performance and model explainability."
- **Why unresolved:** The current CSC score maximizes predictive contribution without explicitly constraining the output to clinical feature sets that are semantically acceptable or "actionable" to human experts.
- **What evidence would resolve it:** A modified loss function incorporating an expert-feedback loop or a plausibility constraint, resulting in rewrites that maintain high ROC scores while improving the "Actionability" scores defined in Table 5.

### Open Question 3
- **Question:** Does the reliance on heuristic-based feature selection for generating synthetic pseudo-labels impose an upper bound on the quality of the final rewriter?
- **Basis in paper:** [inferred] Section 4.1 notes the EHR rewriter is fine-tuned on pseudo-labels derived from operators like mutual information (πd). The ablation study (Table 3) shows that removing this synthetic dataset (DRw) causes performance degradation.
- **Why unresolved:** It is unclear if the rewriter simply learns to mimic the distribution of the static feature selectors or if it generalizes beyond them to identify novel predictive patterns that the initial heuristics missed.
- **What evidence would resolve it:** An analysis comparing the overlap of features selected by the trained LLM rewriter versus the initial heuristic operators on a held-out test set to determine if the model has surpassed its synthetic supervision.

## Limitations
- The effectiveness of the CSC score alignment mechanism depends heavily on predictor confidence being a valid proxy for rewrite quality, which may not hold under domain shift.
- The synthetic data generation approach introduces potential distribution mismatch between pseudo-labels and real clinical scenarios.
- Clinical interpretability of rewrites remains unclear - while the framework claims to emphasize task-relevant features, there's no systematic analysis of which clinical features are prioritized or whether these align with medical best practices.

## Confidence

- **High Confidence:** The core architectural design (EHR verbalization → paraphrasing → scorer-based selection → LLM fine-tuning → KL alignment) is well-specified and technically sound. The implementation details for LoRA fine-tuning and ModernBERT predictors are standard and reproducible.
- **Medium Confidence:** The 23% AUC-ROC improvement claim is significant but context-dependent on MIMIC-IV's specific data distribution and task definitions. The generalizability to unseen datasets requires further validation beyond the single external dataset tested.
- **Low Confidence:** The clinical interpretability of rewrites remains unclear - while the framework claims to "emphasize task-relevant predictive features," there's no systematic analysis of which clinical features are prioritized or whether these align with medical best practices.

## Next Checks

1. **Distribution Shift Analysis:** Test ReToP on a clinically distinct EHR dataset (e.g., eICU or clinical notes from another institution) to quantify performance degradation and identify failure patterns.
2. **Feature Attribution Audit:** Use SHAP or integrated gradients to compare feature importance distributions between original and rewritten EHRs, verifying that task-relevant features are preserved while irrelevant ones are filtered.
3. **CSC Score Calibration:** Systematically vary predictor architectures and training data sizes to test whether CSC score reliability breaks down under resource constraints, and develop fallback mechanisms for low-confidence scenarios.