---
ver: rpa2
title: 'MathSE: Improving Multimodal Mathematical Reasoning via Self-Evolving Iterative
  Reflection and Reward-Guided Fine-Tuning'
arxiv_id: '2511.06805'
source_url: https://arxiv.org/abs/2511.06805
tags:
- reasoning
- error
- arxiv
- correct
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MathSE improves multimodal mathematical reasoning by introducing
  a self-evolving framework that iteratively refines models through inference, reflection,
  and reward-guided feedback. Unlike static teacher-derived datasets, MathSE leverages
  an Outcome Reward Model to identify and correct reasoning errors, enabling continuous
  improvement.
---

# MathSE: Improving Multimodal Mathematical Reasoning via Self-Evolving Iterative Reflection and Reward-Guided Fine-Tuning

## Quick Facts
- arXiv ID: 2511.06805
- Source URL: https://arxiv.org/abs/2511.06805
- Reference count: 40
- Key outcome: MathSE achieves 64.70% accuracy on MathVL-test, surpassing leading open-source models like QVQ

## Executive Summary
MathSE introduces a self-evolving framework for multimodal mathematical reasoning that iteratively refines models through inference, reflection, and reward-guided feedback. Unlike static teacher-derived datasets, MathSE leverages an Outcome Reward Model (ORM) to identify and correct reasoning errors, enabling continuous improvement. The framework achieves significant performance gains on challenging benchmarks including MathVista, MathVerse, MathVision, and MathVL-test, demonstrating its effectiveness in enhancing reasoning capabilities.

## Method Summary
MathSE operates through three stages: initial supervised fine-tuning on GPT-4o-distilled Chain-of-Thought data, iterative self-evolving fine-tuning where correct reasoning paths are identified by ORM and added to training data, and reflection learning where incorrect paths are corrected using ORM feedback. The ORM provides diagnostic error analysis beyond binary correctness, identifying specific error steps and detailed error analysis. This self-evolving approach addresses the limitations of static teacher-distilled datasets by creating training data that aligns with the student model's own reasoning distribution.

## Key Results
- Achieves 64.70% accuracy on MathVL-test, outperforming QVQ and other open-source models
- Self-evolving data (62.35%) outperforms full GPT-4o distilled data (58.00%) with identical data size (~240K)
- ORM with error analysis achieves 97.10% overall accuracy vs. 92.65% for binary-only baseline on ORM-2K test set
- Reflection stage improves performance across all backbones: CogVLM2 (+2.35%), Qwen2-VL (+1.85%), InternVL2.5 (+0.68%)

## Why This Works (Mechanism)

### Mechanism 1: In-Distribution Self-Generated Training Data
- Claim: Training on model-generated correct reasoning paths improves generalization more than exclusive reliance on teacher-distilled data.
- Mechanism: The model generates reasoning paths on held-out data (D_remain). Correct paths (validated by ORM) are incorporated into D_SFT for subsequent fine-tuning rounds. This creates training data that matches the model's own reasoning distribution rather than the static patterns of a teacher model.
- Core assumption: Self-generated correct reasoning paths provide distributional alignment between training data and model capabilities that teacher-distilled data cannot achieve.
- Evidence anchors:
  - [abstract]: "These datasets are typically distilled directly from teacher models, which capture only static reasoning patterns and leaving substantial gaps compared to student models."
  - [Table 4]: Self-evolving data (62.35%) outperforms full GPT-4o distilled data (58.00%) with identical data size (~240K).
  - [corpus]: Related work on self-evolved preference optimization (arxiv:2503.04813) supports the benefit of self-generated training signals for mathematical reasoning.
- Break condition: If the model's initial reasoning capability is too weak to generate sufficient correct paths, the self-evolving loop fails to accumulate high-quality training data.

### Mechanism 2: Diagnostic Error Analysis Over Binary Supervision
- Claim: An Outcome Reward Model that identifies specific error steps and provides detailed analysis enables more effective reflection than binary correct/incorrect labels.
- Mechanism: The ORM outputs (1) correctness classification, and if incorrect, (2) the specific error step and (3) detailed error analysis. This diagnostic feedback is passed to the reflection stage where GPT-4o generates corrected reasoning paths.
- Core assumption: Fine-grained error localization provides actionable signal for correction that binary labels cannot convey.
- Evidence anchors:
  - [Table 5]: ORM with error analysis achieves 97.10% overall accuracy vs. 92.65% for binary-only baseline on ORM-2K test set, even when measuring only binary judgment capability.
  - [Table 3]: ORM feedback (64.70%) outperforms both GPT-4o feedback (64.25%) and no reflection (62.35%).
  - [corpus]: Process-based supervision research (Lightman et al. 2023 cited in paper) suggests step-level verification benefits reasoning, though MathSE's ORM focuses on diagnostic analysis rather than exhaustive step verification.
- Break condition: If ORM error localization accuracy is low, incorrect feedback propagates to reflection stage, generating flawed corrected paths.

### Mechanism 3: Reflection-Driven Knowledge Consolidation
- Claim: External reflection using ORM feedback on incorrect paths produces training data that reinforces error correction patterns.
- Mechanism: Incorrect reasoning paths are collected across all iterative stages. ORM feedback (error step + analysis) is provided to GPT-4o, which generates corrected reasoning paths. These reflected paths are filtered by ORM and added to the final training set.
- Core assumption: Learning from corrected mistakes creates representations that improve robustness against similar error patterns.
- Evidence anchors:
  - [Figure 2]: Consistently correct samples increase from 402 to 1018 across stages; persistently incorrect and correct→incorrect transitions both decrease.
  - [Table 1]: Adding reflection (280K) improves over self-evolving alone (240K) across all backbones: CogVLM2 (+2.35%), Qwen2-VL (+1.85%), InternVL2.5 (+0.68%).
  - [corpus]: ReflectEvo (arxiv:2505.16475) demonstrates similar benefits of reflection learning for small LLMs' meta-introspection.
- Break condition: If reflection model (GPT-4o) cannot reliably correct errors based on ORM feedback, the reflection dataset introduces noise rather than signal.

## Foundational Learning

- Concept: **Multimodal Large Language Models (MLLMs)**
  - Why needed here: MathSE operates on MLLMs (CogVLM2, Qwen2-VL, InternVL2.5) that process both visual and textual inputs for mathematical problem-solving.
  - Quick check question: Can you explain how a vision encoder connects to a language model backbone?

- Concept: **Knowledge Distillation**
  - Why needed here: Stage 1 uses GPT-4o-distilled CoT data for initial SFT; understanding teacher-student distribution gaps is central to the paper's motivation.
  - Quick check question: Why might student model performance plateau when trained exclusively on teacher-generated reasoning paths?

- Concept: **Outcome vs. Process Reward Models**
  - Why needed here: The paper positions its ORM as a hybrid—evaluating overall reasoning while providing step-level diagnostic feedback, distinct from traditional binary ORMs and exhaustive process-supervised verifiers.
  - Quick check question: What information does a process reward model require that an outcome reward model does not?

## Architecture Onboarding

- Component map:
  ```
  [MathVL Dataset] → [GPT-4o Distillation] → [D_SFT (initial)]
                                              ↓
  [Base MLLM] ← SFT ← [D_SFT] → [Evolving Model]
       ↓                                    ↓
  [ORM] ←── evaluates ── [Generated Reasoning Paths on D_remain]
       ↓                                    ↓
  [Error Step + Analysis] → [GPT-4o Reflection] → [D_Reflection]
       ↓                                    ↓
  [R_correct] ──────────────────────→ [Updated D_SFT] → [Final Model]
  ```

- Critical path:
  1. Train ORM on 60K samples (30K correct CoT + 30K incorrect with GPT-4o-annotated error steps/analysis)
  2. Stage 1: SFT base model on initial D_SFT (~100K GPT-4o distilled paths)
  3. Stage 2: For K rounds, generate paths on D_remain, ORM-evaluate, add correct paths to D_SFT, retrain
  4. Stage 3: Collect all incorrect paths, use ORM feedback + GPT-4o reflection, add corrected paths to D_SFT, final SFT

- Design tradeoffs:
  - **ORM complexity vs. annotation cost**: Diagnostic ORM requires detailed error annotations (GPT-4o-generated) but outperforms binary-only models (Table 5). Corpus evidence on this specific tradeoff is limited.
  - **Iteration count vs. compute budget**: More rounds increase consistently correct samples (Figure 2), but each round requires full inference on D_remain. The paper uses K=4 stages without explicit ablation on optimal K.
  - **GPT-4o vs. ORM for feedback**: ORM feedback (64.70%) slightly outperforms GPT-4o feedback (64.25%), suggesting specialized reward models can match or exceed general LLM feedback for domain-specific error analysis.

- Failure signatures:
  - **Stagnant accuracy across iterations**: Indicates ORM is not identifying correct paths OR model has reached capacity ceiling on D_remain
  - **Reflection accuracy below baseline**: Suggests ORM error localization is unreliable, corrupting D_Reflection
  - **Large gap between self-evolving and full GPT-4o baselines**: Indicates model is not generating sufficient correct paths; may need to expand D_SFT initialization or adjust sampling temperature

- First 3 experiments:
  1. **ORM validation before training**: Test trained ORM on held-out reasoning paths; verify error step localization accuracy exceeds 90% on known-error samples before deploying in pipeline.
  2. **Stage 1 ablation**: Compare 50K vs. 100K vs. 200K initial distilled data to find minimum viable D_SFT for self-evolving to bootstrap.
  3. **Single-iteration baseline**: Run only Stage 1 + Stage 3 (skip iterative self-evolving) to isolate reflection contribution from iterative accumulation effect.

## Open Questions the Paper Calls Out
None

## Limitations
- Relies heavily on GPT-4o for initial distillation and reflection generation, creating dependency on closed-source model capabilities
- Iterative self-evolving mechanism's effectiveness depends critically on model's ability to generate sufficient correct reasoning paths early in training
- ORM's error localization accuracy and generalization to different mathematical domains or problem types is not extensively validated

## Confidence

- **High confidence**: The comparative results showing MathSE's improvement over baseline models (64.70% vs 62.35% self-evolving, 58.00% GPT-4o baseline) on MathVL-test, and the ORM performance advantage (97.10% vs 92.65%) on the binary judgment task are well-supported by presented evidence.
- **Medium confidence**: The mechanism explanations for why self-generated data outperforms teacher-distilled data assumes distributional alignment benefits without extensive ablation on training data composition. The reflection contribution is validated through controlled comparisons but lacks analysis of optimal reflection model selection or error correction reliability.
- **Low confidence**: Claims about ORM's superiority over binary-only models for the mathematical reasoning domain specifically are supported by internal comparisons but lack external validation across different mathematical domains or problem types.

## Next Checks

1. **Ablation on training data composition**: Train models using only GPT-4o-distilled data, only self-evolving data, only reflection data, and combinations thereof to quantify each component's marginal contribution and identify potential redundancy.

2. **ORM generalization stress test**: Evaluate the trained ORM on mathematical reasoning paths from different domains (physics problems, code generation) and with models of varying capabilities to assess error localization robustness beyond the curated MathVL-test distribution.

3. **Reflection quality audit**: Manually examine 100 corrected reasoning paths from the reflection stage to determine the accuracy rate of GPT-4o corrections based on ORM feedback, and identify failure modes where ORM error localization leads to incorrect corrections.