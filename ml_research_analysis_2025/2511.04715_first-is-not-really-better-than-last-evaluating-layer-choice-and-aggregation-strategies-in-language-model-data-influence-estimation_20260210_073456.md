---
ver: rpa2
title: 'First is Not Really Better Than Last: Evaluating Layer Choice and Aggregation
  Strategies in Language Model Data Influence Estimation'
arxiv_id: '2511.04715'
source_url: https://arxiv.org/abs/2511.04715
tags:
- influence
- layers
- tracin
- datainf
- cosine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of identifying the most informative
  model layers for computing data influence in large language models (LLMs), a critical
  step for effective data attribution and model interpretability. The authors challenge
  the widely accepted assumption that embedding layers are optimal for influence estimation,
  instead proposing that middle attention layers often provide more reliable signals.
---

# First is Not Really Better Than Last: Evaluating Layer Choice and Aggregation Strategies in Language Model Data Influence Estimation

## Quick Facts
- arXiv ID: 2511.04715
- Source URL: https://arxiv.org/abs/2511.04715
- Reference count: 40
- Primary result: Middle attention layers and Vote aggregation outperform embedding layers and Mean aggregation for influence estimation

## Executive Summary
This paper challenges the widely held belief that embedding layers are optimal for influence estimation in LLMs. Through systematic experiments across multiple model architectures and tasks, the authors demonstrate that middle attention layers consistently provide more reliable influence signals than embedding or classification head layers. They introduce novel aggregation strategies (Rank and Vote) that improve influence estimation by mitigating score dominance effects, and propose Noise Detection Rate (NDR) as a computationally efficient proxy metric for influence quality evaluation. The findings have practical implications for model interpretability and data attribution in large language models.

## Method Summary
The method involves fine-tuning LLMs with LoRA on GLUE tasks with injected noise, then computing influence scores at different layer groups (embeddings, attention splits, classification head) using TracIn, Cosine, or DataInf methods. Influence scores are aggregated using Mean, Rank, or Vote strategies, with the Vote method assigning positional votes that decay with rank. The best k% of samples by influence score are filtered out, the model is retrained, and performance is evaluated. NDR is proposed as a proxy metric measuring the fraction of known mislabeled samples captured in the bottom k% of influence scores.

## Key Results
- Middle attention layers (particularly LoRA value projection modules) consistently outperform embedding and classification head layers for influence estimation
- Vote aggregation significantly improves filtering accuracy, with win rates increasing from 0.71 to 0.84 on Mistral 7B
- NDR correlates strongly with downstream performance (Spearman ρ up to 0.9), especially under Vote aggregation
- The gradient cancellation effect is an unreliable predictor of influence performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Middle attention layers may provide more discriminative signals for influence estimation than embedding or classification head layers.
- Mechanism: Gradients at middle attention layers (particularly LoRA value projection modules) show higher separation between noisy and clean training samples, with noise influence rank decreasing while benign samples maintain stable influence.
- Core assumption: Layer-specific gradient patterns reflect differential sensitivity to detrimental samples, and this sensitivity correlates with downstream filtering performance.
- Evidence anchors:
  - [abstract] "middle attention layers consistently outperform embedding and classification head layers"
  - [section 5.2] Figure 2 shows accuracy distributions; Table 3 ranks layers with attention groups 1-2 frequently top-ranked across models
  - [section K] Figures 27-28 visualize noise rank variation across layers, showing minimum average noise rank at middle layers for Qwen-2.5
- Break condition: If noise distribution becomes uniform across influence scores (high entropy), middle layer advantage diminishes (observed partially for Llama-3.2 1B in Figure 21).

### Mechanism 2
- Claim: Rank and Vote aggregation strategies can improve influence estimation by mitigating score dominance and compensation effects inherent in mean averaging.
- Mechanism: Rank converts influence scores to ordinal rankings before summing, eliminating magnitude dominance. Vote assigns positional votes that decay with rank, reducing sensitivity to outliers while preserving relative ordering. Both methods optionally filter incorrectly predicted validation samples.
- Core assumption: Individual high-magnitude scores across layers/validation samples should not dominate the aggregate, and consensus across multiple signals improves reliability.
- Evidence anchors:
  - [abstract] "Vote aggregation significantly improves filtering accuracy, with win rates increasing from 0.71 to 0.84 on Mistral 7B"
  - [section 5.3] Figure 3 shows accuracy improvements; Table 6 demonstrates Vote improves TracIn CL rank from 12 (Mean) to 1 (Vote) on Mistral
  - [corpus] Weak direct support; neighbor papers on layer optimization (Intermediate Layer Classifiers, First Attentions Last) address different problems
- Break condition: When influence signals are highly correlated across layers (weak complementarity), aggregation benefits are marginal (observed for Cosine method in Figure 3).

### Mechanism 3
- Claim: Noise Detection Rate (NDR) at 30% threshold correlates with downstream performance and can serve as a proxy metric to avoid expensive retraining.
- Mechanism: NDR measures the fraction of known mislabeled samples captured in the top-k% least influential. Higher NDR indicates the influence function correctly ranks detrimental samples low. Correlation with post-filtering accuracy suggests NDR captures filtering efficacy.
- Core assumption: The relationship between NDR and downstream accuracy is sufficiently stable across model architectures, tasks, and influence methods to guide hyperparameter selection.
- Evidence anchors:
  - [abstract] "NDR correlates strongly with downstream performance, particularly under Vote aggregation (Spearman ρ up to 0.9)"
  - [section 5.4] Table 2 shows correlations; Figure 5 visualizes layer-wise NDR with Value B modules achieving highest rates
  - [corpus] No direct corpus support; this is a novel proxy metric proposal
- Break condition: When noise is distributed bimodally across influence range (both high and low), NDR may not reflect true filtering capability (observed for TracIn in Figure 20, last quantile noise spikes).

## Foundational Learning

- **Influence Functions**
  - Why needed here: Core method for estimating how individual training samples affect model predictions via gradient-based analysis.
  - Quick check question: Can you explain the computational bottleneck that motivates layer-restricted influence computation in LLMs?

- **Gradient Cancellation Effect**
  - Why needed here: Prior work's hypothesis that high cancellation (opposing gradients) reduces layer discriminability; this paper challenges this assumption.
  - Quick check question: Given Theorem 5.1, why might weights with high cancellation actually improve influence estimation?

- **LoRA Fine-Tuning**
  - Why needed here: Practical parameter-efficient method used in experiments; only LoRA weights are tuned, but gradients computed on frozen components for influence.
  - Quick check question: Why does the paper compute influence on frozen embeddings (WE) despite not training them?

## Architecture Onboarding

- **Component map:**
  - WE (embeddings) -> Attention groups 1-4 (equal splits) -> CL (classification head)
  - LoRA modules: Query A/B, Value A/B attached to attention projections
  - Influence functions: TracIn (dot product), Cosine (normalized dot product), DataInf (second-order approximation)
  - Aggregation operators: Mean (baseline), Rank (sum of ordinal positions), Vote (positional voting with decay)

- **Critical path:**
  1. Inject noise → Fine-tune with LoRA → Select checkpoint (lowest validation loss)
  2. Compute layer-wise influence scores for training/validation pairs
  3. Aggregate across layers and validation samples (choose method)
  4. Remove bottom k% by influence → Retrain → Evaluate test accuracy

- **Design tradeoffs:**
  - Layer choice: Early/middle attention vs. embeddings—paper suggests attention groups 1-2; middle layers for larger models, later layers for smaller models (RoBERTa)
  - Aggregation: Mean is simple but susceptible to dominance; Vote requires hyperparameter k (paper uses k=30)
  - Evaluation: NDR is cheap proxy but correlation varies (0.5-0.9); full retraining is gold standard but expensive

- **Failure signatures:**
  - TracIn on CL layer consistently underperforms (win rates ~0.17-0.21 across models)
  - Llama-3.2 1B shows poor influence-based filtering vs. Random baseline—noise distributed across influence range (Figure 21)
  - Cosine method degrades with Vote aggregation on some tasks (CoLA, QQP)—suggests method-specific aggregation sensitivity

- **First 3 experiments:**
  1. **Single-layer baseline:** Compute influence using only LoRA Value B module on middle attention layer (e.g., layer 8-15 for Mistral) with TracIn and mean aggregation; measure NDR at 30% and post-filtering accuracy.
  2. **Aggregation comparison:** On the same layer setup, compare Mean vs. Rank vs. Vote (k=30) across TracIn, DataInf, and Cosine; track win rates and statistical significance.
  3. **Layer sweep:** For one model (e.g., Qwen-2.5 1.5B) and one method (DataInf with Vote), compute influence on all layer groups separately; plot NDR vs. layer number to validate middle-layer peak hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal value of k in Vote aggregation scale with the level of injected noise in training data?
- Basis in paper: [explicit] Appendix L states: "Determining how the optimal k scales with the level of injected noise remains an open question."
- Why unresolved: The authors tested fixed k=30 across experiments but observed performance sensitivity to k values, with optimal ranges varying by influence method (k∈[10,50] for DataInf/TracIn, larger k for Cosine).
- What evidence would resolve it: Systematic experiments varying noise injection rates (e.g., 5%, 10%, 20%, 40%) alongside k values, measuring NDR and downstream accuracy to establish scaling relationships.

### Open Question 2
- Question: Can unsupervised or multi-objective aggregation methods outperform Vote aggregation for influence score combination?
- Basis in paper: [explicit] Section 5.3 states: "future work could explore efficient unsupervised or multi-objective aggregation methods to exploit the influence signals and further enhance filtering performance."
- Why unresolved: Vote aggregation requires the hyperparameter k and still shows degradation on some datasets (CoLA, QQP); unsupervised approaches could eliminate manual tuning.
- What evidence would resolve it: Developing and benchmarking unsupervised aggregation schemes (e.g., clustering-based, learned weighting) against Vote across GLUE tasks, measuring win rates and computational cost.

### Open Question 3
- Question: What mechanistic properties of middle attention layers make them encode representations most informative for detrimental sample detection?
- Basis in paper: [inferred] The authors demonstrate empirically that middle attention layers consistently outperform embedding and classification layers, but provide limited theoretical explanation for why this occurs.
- Why unresolved: The paper establishes the "where" (middle layers) but not the "why" of influence signal localization; the correlation analysis in Appendix D shows weak between-group correlations but doesn't explain representational differences.
- What evidence would resolve it: Probing studies analyzing what semantic or syntactic properties middle-layer representations capture that distinguish noisy from clean samples; causal interventions on layer representations.

### Open Question 4
- Question: How can proxy metrics achieve consistent reliability (ρ > 0.8) across all model-task-configuration combinations without requiring Vote aggregation?
- Basis in paper: [inferred] Table 2 shows NDR correlations of only 0.5–0.7 under Mean aggregation, improving to 0.8–0.9 with Vote, suggesting the metric itself depends on aggregation quality.
- Why unresolved: The authors note "values in the 0.5–0.7 range indicate that these metrics are not consistently reliable across all configurations."
- What evidence would resolve it: Developing composite metrics that integrate NDR with layer-wise agreement scores or gradient distribution properties, validated against exhaustive fine-tuning experiments.

## Limitations
- Results are derived from GLUE benchmark tasks with uniform synthetic noise injection, which may not generalize to more complex, real-world noisy datasets
- The claim about middle attention layers being universally optimal is based on relatively small models (up to 7B parameters) and may not hold for frontier-scale LLMs
- The Noise Detection Rate correlation, while strong (ρ up to 0.9), shows significant variance across different aggregation methods and influence functions

## Confidence

- **High confidence:** The empirical demonstration that Vote aggregation consistently improves filtering accuracy across multiple models and tasks, with quantified win rate improvements (0.71 to 0.84 for Mistral 7B).
- **Medium confidence:** The claim that middle attention layers outperform embedding/classification head layers, as this shows model-dependent variation (attention group 1-2 for larger models, later layers for smaller models like RoBERTa).
- **Low confidence:** The universal applicability of NDR as a proxy metric, given its variable correlation strength (0.5-0.9) and the observed failure cases where noise distribution violates the method's assumptions.

## Next Checks

1. Test the middle layer hypothesis on multilingual datasets and non-GLUE benchmarks to assess cross-domain generalizability.
2. Evaluate NDR correlation stability across different noise injection patterns (class-specific vs. uniform) and real-world mislabeled data.
3. Scale experiments to larger model families (Llama-3 70B, GPT-4 class) to verify if middle layer advantage persists at frontier scale.