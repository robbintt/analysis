---
ver: rpa2
title: 'Supercm: Revisiting Clustering for Semi-Supervised Learning'
arxiv_id: '2506.23824'
source_url: https://arxiv.org/abs/2506.23824
tags:
- supercm
- learning
- clustering
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SuperCM, a semi-supervised learning method
  that extends a differentiable clustering module (CM) by using labeled data to guide
  cluster centroids via moving averages, avoiding complex training schemes typical
  of consistency regularization or entropy minimization approaches. The method is
  simple, end-to-end trainable, and incorporates the clustering assumption explicitly.
---

# Supercm: Revisiting Clustering for Semi-Supervised Learning

## Quick Facts
- **arXiv ID:** 2506.23824
- **Source URL:** https://arxiv.org/abs/2506.23824
- **Reference count:** 0
- **Primary result:** Up to 10% improvement when SuperCM is used as a regularizer for SSL base models in low supervision regimes.

## Executive Summary
This paper introduces SuperCM, a semi-supervised learning method that extends a differentiable clustering module (CM) by using labeled data to guide cluster centroids via moving averages, avoiding complex training schemes typical of consistency regularization or entropy minimization approaches. The method is simple, end-to-end trainable, and incorporates the clustering assumption explicitly. Experiments on CIFAR-10 show SuperCM improves supervised-only baseline accuracy by up to 5.2% with 600 labels and up to 3.6% with 4000 labels. When used as a regularizer for SSL base models like Pseudo-Label and VAT, it achieves significant gains of up to 10% in low supervision regimes. Feature visualizations reveal better class separation compared to cross-entropy training.

## Method Summary
SuperCM extends the differentiable Clustering Module (CM) by updating cluster centroids as class-wise moving averages of labeled data features, preventing gradient-based training pitfalls. The model combines supervised cross-entropy loss with a clustering loss derived from a Gaussian mixture model. During training, centroids are updated non-differentiably using labeled data only, while the backbone and CM encoder are trained end-to-end. The approach can be used standalone or as a regularizer for existing SSL methods, requiring minimal additional hyperparameters.

## Key Results
- Standalone SuperCM improves CE-only baseline by 5.2% (600 labels) and 3.6% (4000 labels) on CIFAR-10
- As regularizer for VAT and Pseudo-Label, achieves up to 10% improvement in low supervision regimes
- UMAP visualizations show SuperCM yields more separated and compact classes than CE training alone

## Why This Works (Mechanism)

### Mechanism 1: Centroid Guidance via Moving Averages
- Claim: Updating centroids as class-wise moving averages of labeled data prevents trivial solutions and centroid collapse during training.
- Mechanism: Instead of learning centroids through gradient descent (which causes rapid backbone training leading to trivial solutions), centroids μ_k are updated at iteration t using the exponential moving average of labeled features belonging to class k. This provides stable, class-representative anchors.
- Core assumption: Labeled samples are reasonably representative of their class distributions in feature space.
- Evidence anchors:
  - [abstract]: "Leveraging annotated data to guide the cluster centroids results in a simple end-to-end trainable deep SSL approach."
  - [section 3.2]: "We overcome this obstacle by learning the centroids as a class-wise average of the labeled data instead of gradient descent... Computing the centroids this way also prevents them from collapsing and makes the Dirichlet prior along with its hyper-parameter α unnecessary."
  - [corpus]: Weak direct corpus evidence for this specific centroid update strategy; related clustering-SSL methods use different approaches.
- Break condition: If labeled data is severely unrepresentative or contains labeling errors, centroids will misguide the clustering, degrading performance.

### Mechanism 2: Differentiable GMM-based Clustering Loss
- Claim: The CM loss explicitly enforces the clustering assumption by maximizing a differentiable GMM Q-function.
- Mechanism: The loss combines four terms: (1) reconstruction error, (2) cluster assignment entropy regularization, (3) inter-centroid separation penalty, and (4) Dirichlet prior on cluster probabilities. This enables joint representation learning and clustering through gradient descent.
- Core assumption: Data in feature space approximately follows a mixture model structure with separable components.
- Evidence anchors:
  - [section 3.1]: "The model aims to maximize a differentiable, rephrased version of the Q-function of a Gaussian mixture model."
  - [section 1]: "explicitly incorporates the underlying clustering assumption in SSL through extending a recently proposed differentiable clustering module."
  - [corpus]: Limited corpus validation for this specific loss formulation in SSL contexts.
- Break condition: If feature space lacks cluster structure (e.g., uniformly distributed data), the clustering assumption provides no useful inductive bias.

### Mechanism 3: Complementary Regularization for SSL Base Models
- Claim: SuperCM improves existing SSL methods (VAT, Pseudo-Label) in low-label regimes by providing well-separated feature representations.
- Mechanism: When labeled data is scarce, CE loss provides unreliable supervision. SuperCM's clustering loss creates better class separation (shown in UMAP visualizations), which improves pseudo-label quality for entropy minimization methods and consistency targets for regularization methods.
- Core assumption: Better cluster separation in feature space improves the quality of pseudo-labels or consistency targets.
- Evidence anchors:
  - [section 5.2]: "the SSL base models benefit from the well-separated clustering obtained by SuperCM, when it is difficult to obtain a reliable supervisory signal with the CE loss in the training."
  - [table 1]: VAT baseline improves from 68.43% to 75.23% (+6.8%) with 600 labels; improvement diminishes at 4000 labels.
  - [section 5.1]: UMAP visualizations show "SuperCM yields more separated and compact classes."
  - [corpus]: Bridged Clustering (arxiv:2510.07182) similarly exploits clustering structure for semi-supervised tasks, providing weak corroborating evidence.
- Break condition: At higher label counts (≥2000-4000), the regularization benefit diminishes as base models already receive sufficient supervision.

## Foundational Learning

- Concept: **Gaussian Mixture Models and Soft Cluster Assignments (γ_ik)**
  - Why needed here: SuperCM's loss derives from the GMM Q-function; understanding responsibilities (posterior probabilities of cluster membership) is essential for interpreting the CM encoder's softmax outputs.
  - Quick check question: Given a data point and K cluster centroids, can you compute the soft assignment probabilities assuming Gaussian clusters with equal covariance?

- Concept: **The Clustering Assumption in SSL**
  - Why needed here: SuperCM explicitly operationalizes this core SSL assumption: "points within the same cluster likely share the same label." Understanding this distinguishes it from consistency-based approaches.
  - Quick check question: How does the clustering assumption differ from the smoothness assumption, and when might one be more appropriate than the other?

- Concept: **Moving/Exponential Averages for Online Updates**
  - Why needed here: Centroid updates use the formula μ_k = (t-1)/t · μ_k + 1/t · (class mean). Understanding how this balances historical stability with new information is critical for debugging centroid drift.
  - Quick check question: After 100 iterations, what is the effective weight given to the initial centroid value versus information from iteration 100?

## Architecture Onboarding

- Component map:
  - **Feature Extractor F** (Wide-ResNet-28-2, 1.5M params) → 128-dim feature vectors
  - **CM Encoder** (single linear layer + softmax) → cluster responsibilities Γ = {γ_ik}
  - **CM Decoder** (centroids μ_k as weight matrix) → reconstructions X̄
  - **Centroid Update Module** (non-differentiable, moving average) → maintains μ_k using labeled data only

- Critical path:
  1. Forward pass: Images I → F → features X → CM encoder → Γ; CM decoder → X̄
  2. Centroid update (after forward, before loss): Extract labeled features, compute class-wise means, apply Equation 2
  3. Loss computation: L = CE(Γ^(l), Y^(l)) + β·L_CM(X^(l+u), Γ^(l+u), X̄^(l+u)) + δ·L_SSL_base

- Design tradeoffs:
  - **β weight**: Controls clustering loss contribution. Figure 5 shows β > 0 improves over CE-only, but performance degrades if β is too high (CE signal diluted). Paper recommends validation-based tuning.
  - **Assumption**: Dirichlet prior α is made unnecessary by moving average centroid updates; simplifies hyperparameter search.
  - **Decoder as centroids**: Using centroids directly as decoder weights ties representation quality to cluster structure.

- Failure signatures:
  - **Trivial solutions with gradient-based centroids**: Paper explicitly notes this failure mode; centroids must use moving average updates.
  - **Diminishing returns at very low labels**: 250-label setting shows reduced improvement—"supervision is extremely scarce and not sufficient to learn a cluster-friendly embedding."
  - **No improvement at high labels with base models**: 4000-label setting shows no significant gain—saturation effect.

- First 3 experiments:
  1. **Baseline replication**: Train CE-only vs SuperCM (β=0.5) on CIFAR-10 with 600 labels using Wide-ResNet-28-2; verify ~5% improvement and check UMAP feature separation.
  2. **β sensitivity sweep**: Train SuperCM with β ∈ {0.1, 0.25, 0.5, 1.0, 2.0} on validation set; identify optimal β and confirm Figure 5 curve shape.
  3. **Integration test with VAT**: Add SuperCM as regularizer (δ=1.0, tune β) to VAT baseline with 600 labels; target ~6-7% improvement over VAT alone.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why does the performance gain from SuperCM regularization diminish for base models in higher supervision regimes (4000 labels)?
- **Basis in paper:** [Explicit] Section 4.2 states the authors "do not observe significant improvement for the 4000 labels setting" when combining SuperCM with VAT or Pseudo-Label.
- **Why unresolved:** The paper hypothesizes benefits arise from separation in low supervision but lacks analysis on why this regularization becomes redundant or detrimental with more labels.
- **What evidence would resolve it:** An analysis of the feature space overlap and gradient conflicts between the CE loss and CM loss as label density increases.

### Open Question 2
- **Question:** Can SuperCM effectively integrate with modern SSL frameworks that utilize confidence thresholding?
- **Basis in paper:** [Inferred] The conclusion claims the method "can be integrated into any gradient based SSL method," yet the experiments are limited to VAT and Pseudo-Label.
- **Why unresolved:** It is unclear if the clustering assumption conflicts with the hard pseudo-labeling or thresholding strategies used in more recent state-of-the-art methods.
- **What evidence would resolve it:** Empirical evaluation of SuperCM combined with methods like FixMatch or FlexMatch.

### Open Question 3
- **Question:** Does the centroid update strategy remain stable and effective on high-dimensional datasets with large numbers of classes?
- **Basis in paper:** [Inferred] The experimental validation is restricted to the CIFAR-10 dataset, which has relatively low dimensionality and only 10 classes.
- **Why unresolved:** The moving average centroid update relies on scarce labeled data to guide high-dimensional centroids; this may suffer from high variance in more complex settings.
- **What evidence would resolve it:** Benchmarks on datasets with higher complexity, such as ImageNet or CIFAR-100.

## Limitations
- Key hyperparameters (Adam learning rate, β, δ) were tuned but exact values not reported, requiring validation set sweeps
- CM architecture details (layer sizes, reconstruction dimensions) are underspecified
- SWA implementation details are missing
- Gaussian noise augmentation parameters are undefined

## Confidence

- **High:** Centroid update mechanism prevents trivial solutions (explicitly stated and experimentally validated)
- **Medium:** Clustering assumption improves base SSL models (supported by Table 1 results but lacking ablation of mechanism)
- **Medium:** Feature visualizations show improved separation (qualitative claim with limited quantitative backing)

## Next Checks

1. Implement SWA with systematic sweeps of starting iteration and frequency to verify reported improvements
2. Create ablation study comparing moving-average centroids vs. gradient-based centroids on CIFAR-10 with 600 labels
3. Test SuperCM integration with VAT and Pseudo-Label across different label counts (250, 600, 2000, 4000) to verify diminishing returns pattern