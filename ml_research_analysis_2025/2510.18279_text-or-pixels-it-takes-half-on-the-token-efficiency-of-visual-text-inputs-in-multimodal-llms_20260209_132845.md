---
ver: rpa2
title: 'Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text Inputs
  in Multimodal LLMs'
arxiv_id: '2510.18279'
source_url: https://arxiv.org/abs/2510.18279
tags:
- text
- image
- tokens
- token
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models can achieve nearly 50% token savings by rendering
  long text inputs as images for multimodal models, without sacrificing task performance
  on retrieval and summarization benchmarks.
---

# Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text Inputs in Multimodal LLMs

## Quick Facts
- **arXiv ID**: 2510.18279
- **Source URL**: https://arxiv.org/abs/2510.18279
- **Reference count**: 35
- **Primary result**: Large language models can achieve nearly 50% token savings by rendering long text inputs as images for multimodal models, without sacrificing task performance on retrieval and summarization benchmarks.

## Executive Summary
This paper introduces ConTexImage, a method for compressing long text inputs by rendering them as images and processing them through a multimodal model's vision encoder. The approach achieves approximately 50% token savings while maintaining comparable accuracy on retrieval and summarization tasks. The technique exploits the fact that vision encoders can process text rendered as images into fixed-length visual tokens, effectively decoupling input length from the decoder's attention complexity. Experiments demonstrate that this method is particularly effective for large models (72B+ parameters) and long contexts, providing both computational efficiency and practical deployment benefits.

## Method Summary
The method renders long text contexts as images using LaTeX compilation and rasterization, then processes these images through a multimodal model's vision encoder to produce visual tokens. The visual tokens are concatenated with the query and fed to the LLM decoder. The compression ratio is controlled by adjusting image resolution to achieve approximately half the token count of the original text. The approach is evaluated on the RULER S-NIAH benchmark for retrieval and CNN/DailyMail for summarization, comparing against text-only baselines and other compression methods like LLMLingua.

## Key Results
- Achieved ~50% token savings by rendering long text as images for multimodal models
- Maintained 97-99% accuracy on retrieval tasks compared to text-only inputs
- Provided 25-45% faster inference on large models (72B parameters) due to reduced attention computation
- Generated summarization outputs with ROUGE scores matching or exceeding text pruning baselines

## Why This Works (Mechanism)

### Mechanism 1: Vision Encoder as a Fixed-Budget Compressor
The vision encoder component of a Multimodal LLM (MLLM) can function as a spatial compressor, converting a variable-length text sequence (m tokens) into a fixed-length visual sequence (k tokens) where k ≈ m/2. Instead of tokenizing text directly (where length m grows linearly with content), the text is rendered as a visual patch (an image). The vision encoder (e.g., ViT) processes this image into a sequence of visual tokens determined by image resolution, not text length. This decouples the decoder's input length from the raw text volume.

### Mechanism 2: Shifting Compute from Quadratic Decoding to Linear Encoding
Reducing the text token count passed to the LLM decoder yields disproportionate efficiency gains because it alleviates the quadratic self-attention bottleneck, shifting the load to the typically more efficient vision encoder. Transformer decoders scale quadratically O(m²). By rendering text to an image, the input to the decoder becomes k tokens where k < m. Even though encoding the image adds overhead, the reduction in the decoder's attention matrix computation results in a net speedup, particularly for large models.

### Mechanism 3: Implicit Semantic Preservation via Large-Scale Pre-training
Large-scale MLLMs (specifically ≥ 72B parameters) possess sufficient "pixel literacy" to perform retrieval and reasoning on visual text at accuracy levels comparable to raw text, up to a specific density threshold. Large models are trained on vast amounts of documents and screenshots. They learn to map visual patterns of characters (pixels) to semantic concepts effectively. Unlike explicit text pruning which risks discarding keywords, the "image-of-text" method preserves all spatial information implicitly, relying on the model's ability to "read" the image.

## Foundational Learning

- **Concept: Self-Attention Complexity (O(n²))**
  - Why needed here: The primary motivation for this technique is that standard text processing becomes prohibitively expensive for long contexts due to the quadratic nature of attention mechanisms.
  - Quick check question: If you halve the input sequence length of a Transformer, does the attention computation cost halve, quarter, or stay the same?

- **Concept: Vision Tokenization (Patching)**
  - Why needed here: To understand why rendering text saves tokens, one must grasp that vision models turn an image into a grid of patches (tokens) based on resolution, independent of the semantic complexity of the text inside the image.
  - Quick check question: Does a 600x800 image of the word "Hello" result in more, fewer, or the same number of visual tokens as a 600x800 image of a full news article?

- **Concept: Needle-in-a-Haystack (NIAH)**
  - Why needed here: This is the primary evaluation benchmark used in the paper to prove the method works. It tests if the model can find a specific fact hidden in a long context.
  - Quick check question: In the RULER benchmark, what constitutes the "needle" and what constitutes the "haystack"?

## Architecture Onboarding

- **Component map:**
  Text Renderer (ConTexImage) -> Vision Encoder (Φ) -> Projector (ψ) -> LLM Decoder

- **Critical path:**
  The ratio ρ = m/k (Compression Ratio). The system relies on tuning the image resolution so that k (visual tokens) is roughly half of m (text tokens). If resolution is too low, k drops but OCR fails. If too high, k approaches m and savings vanish.

- **Design tradeoffs:**
  - Resolution vs. Accuracy: Increasing image size increases k (cost) but improves OCR accuracy (performance). The paper identifies a "sweet spot" around k ≈ m/2.
  - Model Size vs. Robustness: Smaller models (7B) struggle with high-density visual text; larger models (72B) are required for robust compression.
  - Overhead vs. Sequence Length: For short texts, the vision encoding overhead is higher than the text processing cost. This method is only efficient for long contexts.

- **Failure signatures:**
  - The "Accuracy Cliff": As observed in Figure 2, accuracy remains stable up to a threshold m*, then drops vertically. This indicates the visual token budget k was insufficient to resolve the text density.
  - Latency Inversion: On smaller models or API-based models with high network overhead, the total time Time_img may exceed Time_text despite lower decoder token counts.

- **First 3 experiments:**
  1. Calibrate the Ratio: Take a 2000-token document. Render it at varying resolutions (e.g., 600x800, 750x1000) and plot the visual token count (k) against RULER retrieval accuracy to find the local m*.
  2. Cross-Modality Baseline: Run the same long-context prompt in "text-only" mode vs. "text-as-image" mode on a 7B vs. 72B model to quantify the performance gap in OCR capabilities.
  3. Summarization Quality Check: Compare the ROUGE scores of summaries generated from (a) full text, (b) text pruned by 50% (LLMLingua), and (c) the text-as-image method to verify semantic preservation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can token-level pruning be applied before visual rendering to compound compression gains without signal loss?
- Basis in paper: The conclusion suggests "applying token-level pruning before visual rendering to further compound compression gains."
- Why unresolved: The current study only compared text-as-image inputs against pruning baselines separately; it did not evaluate a stacked pipeline where pruning occurs before rendering.
- Evidence: Experiments combining methods like LLMLingua with the visual rendering pipeline on the RULER benchmark.

### Open Question 2
- Question: Does the text-as-image approach maintain performance in domains where every token is critical, such as mathematical reasoning?
- Basis in paper: The conclusion recommends expanding to "other domains (e.g. math) where most prompt tokens are critical and thus difficult to prune."
- Why unresolved: The paper only evaluated natural language tasks (retrieval and summarization), leaving the handling of dense symbolic information in visual form untested.
- Evidence: Evaluation on mathematical benchmarks (e.g., GSM8K) or coding tasks where whitespace and syntax are rendered visually.

### Open Question 3
- Question: How does the visual text method scale to extremely long contexts spanning tens of thousands of tokens?
- Basis in paper: The Limitations section states the work "has not yet fully evaluated the impact... on extremely large contexts that span tens of thousands of tokens."
- Why unresolved: Experiments were limited to shorter contexts (mostly under 3,500 tokens); it is unknown if image resolution or model vision encoders become bottlenecks at scale.
- Evidence: Testing on long-context benchmarks like LongBench or specialized retrieval tasks with 50k+ token contexts.

## Limitations

- **Input Density Constraints**: The method exhibits a sharp "accuracy cliff" when text density exceeds the model's visual token budget, with performance degrading rapidly beyond specific density thresholds (m*).
- **Model Size Dependency**: The approach requires large models (72B+ parameters) to maintain performance, limiting practical deployment scenarios where computational resources are constrained.
- **Rendering Pipeline Specificity**: The ConTexImage method relies on LaTeX-based rendering with specific parameters, and performance may degrade with different rendering approaches, languages with complex scripts, or non-standard document layouts.

## Confidence

- **High Confidence (9/10)**: The fundamental mechanism of achieving ~50% token savings through image-based compression is well-supported by the experimental results.
- **Medium Confidence (6/10)**: The generalization of performance across different document types, languages, and rendering approaches remains uncertain.
- **Low Confidence (4/10)**: Claims about optimal rendering parameters (font size, fill ratio, resolution) are based on limited empirical exploration.

## Next Checks

- **Check 1: Cross-Linguistic Robustness Test**
  Validate the method across multiple languages (e.g., English, Chinese, Arabic, Devanagari scripts) using the same rendering pipeline. Measure accuracy degradation patterns and identify whether the "text-token tolerance" (m*) varies systematically by language complexity or script density.

- **Check 2: Multi-Modal Document Evaluation**
  Test the approach on documents containing mixed content (text + tables + figures + mathematical equations) rather than pure text passages. Evaluate whether the visual encoder's compression maintains semantic integrity when processing heterogeneous document layouts.

- **Check 3: Edge Case Density Analysis**
  Systematically explore the "accuracy cliff" by generating test cases that incrementally increase text density beyond the reported m* threshold. Document the precise failure modes (character recognition errors, semantic confusion, or complete breakdown) and measure the relationship between density, resolution, and accuracy recovery.