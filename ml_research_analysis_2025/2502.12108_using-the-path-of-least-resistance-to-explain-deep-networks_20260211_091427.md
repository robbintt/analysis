---
ver: rpa2
title: Using the Path of Least Resistance to Explain Deep Networks
arxiv_id: '2502.12108'
source_url: https://arxiv.org/abs/2502.12108
tags:
- geodesic
- path
- gradients
- points
- attribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper identifies a key limitation of Integrated Gradients\
  \ (IG), where straight-line paths can lead to flawed attributions due to high gradients\
  \ in irrelevant regions. To address this, the authors propose Geodesic Integrated\
  \ Gradients (GIG), which computes attributions by integrating gradients along geodesic\
  \ paths on a Riemannian manifold defined by the model\u2019s gradients."
---

# Using the Path of Least Resistance to Explain Deep Networks

## Quick Facts
- arXiv ID: 2502.12108
- Source URL: https://arxiv.org/abs/2502.12108
- Reference count: 13
- One-line primary result: Geodesic Integrated Gradients (GIG) outperforms standard Integrated Gradients on both synthetic and real-world data by avoiding attribution errors from irrelevant high-gradient regions.

## Executive Summary
This paper identifies a fundamental limitation in Integrated Gradients (IG) where straight-line paths between input and baseline can lead to flawed attributions when passing through irrelevant regions of high gradient sensitivity. To address this, the authors propose Geodesic Integrated Gradients (GIG), which computes attributions by integrating gradients along geodesic paths on a Riemannian manifold defined by the model's gradients. Two approximation methods are introduced: a k-NN-based approach for simpler models and a Stochastic Variational Inference-based method for complex ones. The paper also introduces a new axiom called Strong Completeness and proves GIG is uniquely satisfied by this method.

## Method Summary
GIG replaces the straight-line path of standard IG with a geodesic path on a Riemannian manifold where the metric tensor is defined by the model's Jacobian. The manifold's geometry causes paths to naturally avoid high-gradient regions, finding the "path of least resistance." For simple models, geodesics are approximated using a k-NN graph with Dijkstra's algorithm. For complex models like ConvNext, an energy-based formulation with Stochastic Variational Inference finds the geodesic. The method introduces Strong Completeness, requiring the sum of absolute attributions to equal the output difference, and proves GIG uniquely satisfies this property.

## Key Results
- On Pascal VOC 2012, GIG achieves AUC of 0.27 for Comprehensiveness and 1.44 for Log-Odds, outperforming IG (0.21 and 1.25 respectively)
- On synthetic half-moons data, GIG (kNN) achieves Purity AUC of 0.531 vs IG's 0.487
- Strong Completeness is proven to be uniquely satisfied by GIG among path-based methods

## Why This Works (Mechanism)

### Mechanism 1
Standard straight-line integration misattributes importance when the path crosses irrelevant regions of high gradient sensitivity. The method replaces the Euclidean straight line with a geodesic path on a Riemannian manifold defined by the model's Jacobian ($G_x = J_x^T J_x$), causing the path's "length" to depend on gradient magnitude. By minimizing this length, the path automatically avoids high-gradient regions unless strictly necessary, finding the "path of least resistance."

### Mechanism 2
Standard "Completeness" allows positive and negative attributions to cancel out, hiding noise. The paper introduces Strong Completeness ($\sum |A_i| = |f(x)-f(x')|$), forcing the sum of absolute attributions to match the output difference. The paper proves GIG uniquely satisfies this, preventing artificial cancellation of noisy attributions.

### Mechanism 3
In high-dimensional spaces like images, discrete graph search fails, requiring continuous optimization. For complex models, the method employs Stochastic Variational Inference (SVI) with an energy function that balances distance to the straight line against curvature penalties that repel the path from high-gradient regions.

## Foundational Learning

- **Integrated Gradients (IG) & Baselines**: Understanding how IG integrates gradients from a baseline to input is essential to see why straight lines are problematic. *Quick check: If baseline is black and object is black, why does standard IG struggle?*
- **Riemannian Manifolds & Metric Tensors**: The core innovation redefines "distance" based on model sensitivity. The metric tensor $G$ distorts space so high-gradient areas become harder to traverse. *Quick check: According to Eq. 6, does a geodesic path get longer or shorter as it passes through high gradient magnitude?*
- **Variational Inference (SVI)**: The paper uses SVI to approximate geodesics for large models. Understanding SVI as optimizing a distribution to approximate a complex posterior is necessary. *Quick check: In SVI context, what does minimizing "energy" $E(\gamma)$ represent for the attribution path?*

## Architecture Onboarding

- **Component map**: Input/Baseline -> Manifold Definition -> Path Solver -> Integrator
- **Critical path**: The Path Solver is the bottleneck - k-NN costs $O(N^2)$ for Dijkstra, while SVI requires iterative optimization (23 hours for 100 images on L4 GPU)
- **Design tradeoffs**: k-NN is exact but discrete for low-D data; SVI is continuous but noisy/heavy for images/high-D. Speed vs. accuracy trade-off exists with sampling steps
- **Failure signatures**: Disconnected graphs (k-NN) if density is too low; endpoint deviation (SVI) where path drifts from input/baseline; sensitivity to $\beta$ parameter
- **First 3 experiments**: 1) Visual sanity check on half-moons comparing IG vs GIG heatmaps; 2) Ablation on k-NN connectivity with varying $k$; 3) Real-world stress test on Pascal VOC measuring Comprehensiveness/Log-Odds

## Open Questions the Paper Calls Out

### Open Question 1
Can directly solving the geodesic equation, rather than relying on Stochastic Variational Inference (SVI), reduce noise and computational cost while maintaining attribution accuracy? The authors note this could offer greater computational efficiency but left it unimplemented.

### Open Question 2
To what extent does systematic hyperparameter tuning (specifically of $\beta$ trade-off and SVI learning rate) improve performance metrics like Comprehensiveness and Log-Odds? The authors admit limited computational resources prevented tuning that could "significantly improve results."

### Open Question 3
Can the k-NN approximation method be modified to ensure graph connectivity in high-dimensional spaces without resorting to the "not optimal" bridge heuristic? The current bridge solution is a heuristic patch that doesn't guarantee following the true geodesic curvature.

## Limitations
- SVI hyperparameter sensitivity (β, learning rate) is not specified, raising reproducibility concerns
- Computational cost is extremely high (23 hours for 100 images on L4 GPU), limiting practical adoption
- Theoretical uniqueness of Strong Completeness is difficult to validate without deeper literature review

## Confidence
- **High**: Synthetic experiment results and core geodesic path integration mechanism are well-specified
- **Medium**: Real-world results depend on unclear SVI hyperparameters and computational resources
- **Low**: Theoretical uniqueness of Strong Completeness and practical implications are difficult to validate

## Next Checks
1. **SVI Ablation Study**: Systematically vary β and sampling steps to quantify impact on attribution quality and runtime
2. **Scaling Test**: Measure GIG (SVI) runtime and attribution quality on progressively larger datasets/models to assess scalability
3. **Alternative Baselines**: Compare GIG against newer path-based attribution methods to contextualize performance gains