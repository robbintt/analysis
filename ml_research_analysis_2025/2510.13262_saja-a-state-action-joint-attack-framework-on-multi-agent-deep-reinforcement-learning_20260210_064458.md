---
ver: rpa2
title: 'SAJA: A State-Action Joint Attack Framework on Multi-Agent Deep Reinforcement
  Learning'
arxiv_id: '2510.13262'
source_url: https://arxiv.org/abs/2510.13262
tags:
- action
- attack
- saja
- state
- joint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of joint state-action adversarial
  attacks in Multi-Agent Deep Reinforcement Learning (MADRL), where existing methods
  focus only on state or action attacks in isolation. The authors propose the State-Action
  Joint Attack (SAJA) framework, which exploits synergistic vulnerabilities between
  state and action spaces through a two-phase gradient-based approach.
---

# SAJA: A State-Action Joint Attack Framework on Multi-Agent Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.13262
- Source URL: https://arxiv.org/abs/2510.13262
- Authors: Weiqi Guo; Guanjun Liu; Ziyuan Zhou
- Reference count: 40
- One-line primary result: State-Action Joint Attack (SAJA) framework outperforms isolated state or action attacks, achieving up to 9.90% reward drop in Multi-Agent Particle Environment

## Executive Summary
This paper introduces the State-Action Joint Attack (SAJA) framework, addressing a critical vulnerability in Multi-Agent Deep Reinforcement Learning (MADRL) systems. Unlike existing adversarial attack methods that target either states or actions in isolation, SAJA exploits synergistic vulnerabilities across both spaces through a two-phase gradient-based approach. The framework demonstrates that coordinated attacks on state and action representations can be significantly more effective than traditional single-space attacks.

The proposed method operates by first crafting adversarial states using both actor and critic networks, then leveraging these perturbed states to generate adversarial actions. Through experiments in the Multi-Agent Particle Environment (MPE), SAJA shows superior attack performance compared to state-only or action-only baselines, achieving greater reward degradation while maintaining stealthier perturbations through smaller per-dimension budgets.

## Method Summary
SAJA employs a two-phase gradient-based attack framework that first crafts adversarial states and then uses these states to generate adversarial actions. The first phase optimizes a perturbed state representation by leveraging gradients from both actor and critic networks, maximizing the difference between Q-values of original and adversarial actions while constraining the perturbation magnitude. The second phase uses this adversarial state as input to the actor network to generate an adversarial action that minimizes the agent's expected return. A heuristic loss function combining Q-value maximization and action distance minimization guides the optimization process, balancing attack effectiveness with stealth requirements.

## Key Results
- SAJA achieves up to 9.90% reward drop compared to state-only or action-only attacks in MPE
- The joint attack demonstrates superior effectiveness in degrading agent performance across various cooperative and competitive scenarios
- SAJA perturbations remain stealthier with smaller per-dimension budgets while maintaining attack efficacy

## Why This Works (Mechanism)
SAJA exploits the fundamental vulnerability that arises when both state and action representations in MADRL systems can be manipulated in a coordinated manner. By crafting adversarial states that maximally impact the critic's Q-value estimation and then using these states to generate adversarial actions, the framework creates a compounding effect that amplifies attack impact beyond what either space alone could achieve. The synergistic exploitation of actor-critic vulnerabilities represents a novel attack surface that existing defenses against single-space attacks fail to address.

## Foundational Learning
- **Multi-Agent Deep Reinforcement Learning**: MADRL involves multiple agents learning simultaneously in shared environments, where each agent's policy depends on both individual observations and joint actions (needed to understand attack targets; quick check: verify understanding of actor-critic architecture in multi-agent settings)
- **Adversarial Attacks in RL**: These attacks manipulate inputs or policies to degrade agent performance, typically targeting either observations or actions in isolation (needed to contextualize SAJA's novelty; quick check: compare gradient-based attack mechanisms)
- **Actor-Critic Architecture**: Combines policy-based actor networks with value-based critic networks, where both components are vulnerable to adversarial manipulation (needed to understand SAJA's dual-phase approach; quick check: trace how perturbations flow through both networks)
- **Gradient-based Optimization**: SAJA uses gradient ascent/descent to find adversarial examples that maximize reward degradation while minimizing perturbation visibility (needed to understand attack methodology; quick check: verify gradient flow through actor and critic)
- **Perturbation Stealth**: The constraint on perturbation magnitude ensures attacks remain imperceptible while maintaining effectiveness (needed to evaluate practical attack viability; quick check: compare perturbation norms across attack variants)

## Architecture Onboarding

**Component Map:**
State Space -> Adversarial State Crafter (actor+critic gradients) -> Perturbed State -> Actor Network -> Adversarial Action Generator -> Action Space

**Critical Path:**
Observation → State Perturbation (Q-value maximization) → Adversarial State → Action Generation (policy optimization) → Adversarial Action → Reward Degradation

**Design Tradeoffs:**
- Joint attack vs. isolated attacks: SAJA sacrifices computational efficiency for significantly improved attack effectiveness
- Stealth constraint vs. attack power: The L2 regularization balances imperceptibility against degradation capability
- White-box access assumption: Provides powerful attacks but limits applicability to scenarios with full network visibility

**Failure Signatures:**
- Ineffective attacks when actor and critic gradients point in opposing directions
- Reduced stealth when Q-value maximization dominates action distance minimization
- Diminished effectiveness in highly stochastic environments where small perturbations are naturally absorbed

**First Experiments:**
1. Compare reward degradation between SAJA and state-only attacks across varying perturbation budgets
2. Measure action distance between original and adversarial actions to quantify stealth
3. Evaluate attack persistence across multiple time steps to assess long-term effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation restricted to MPE with 2-3 agents, limiting generalization to complex scenarios
- Attack success measured only through reward degradation without examining agent adaptation over time
- White-box attack assumption requires full network access, not representative of all threat models

## Confidence
- **Scalability to complex environments**: Medium confidence - limited agent configurations tested
- **Attack persistence and adaptation**: Medium confidence - no long-horizon evaluation performed
- **Universal applicability of heuristic loss**: Low confidence - hyperparameter sensitivity unexplored
- **Practical stealth assessment**: Medium confidence - stealth claims lack task-relevant evaluation
- **Black-box attack applicability**: Low confidence - methodology assumes white-box access

## Next Checks
1. Test SAJA on more complex MADRL environments with larger agent populations and heterogeneous reward structures
2. Evaluate attack effectiveness against agent adaptation mechanisms and over extended episodes to assess persistence
3. Perform ablation studies on the heuristic loss function components to quantify their individual contributions to attack success