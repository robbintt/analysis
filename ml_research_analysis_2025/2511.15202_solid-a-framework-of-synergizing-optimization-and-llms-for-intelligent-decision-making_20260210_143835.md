---
ver: rpa2
title: 'SOLID: a Framework of Synergizing Optimization and LLMs for Intelligent Decision-Making'
arxiv_id: '2511.15202'
source_url: https://arxiv.org/abs/2511.15202
tags:
- optimization
- llms
- arxiv
- solid
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SOLID introduces a framework that combines mathematical optimization
  with large language models (LLMs) for improved decision-making. The approach enables
  iterative collaboration between optimization and LLM agents through dual prices
  and deviation penalties, inspired by the alternating direction method of multipliers
  (ADMM).
---

# SOLID: a Framework of Synergizing Optimization and LLMs for Intelligent Decision-Making

## Quick Facts
- arXiv ID: 2511.15202
- Source URL: https://arxiv.org/abs/2511.15202
- Reference count: 31
- Combines mathematical optimization with LLMs for decision-making through dual pricing and deviation penalties

## Executive Summary
SOLID introduces a framework that combines mathematical optimization with large language models (LLMs) for improved decision-making. The approach enables iterative collaboration between optimization and LLM agents through dual prices and deviation penalties, inspired by the alternating direction method of multipliers (ADMM). The framework demonstrates convergence under convexity assumptions while maintaining modularity and data privacy. In a portfolio optimization case study using historical prices and financial news, SOLID variants achieved higher annualized returns compared to optimization-only methods while maintaining lower volatility.

## Method Summary
SOLID coordinates optimization and LLM agents using an ADMM-inspired iterative process. The coordinator maintains a public decision and dual prices that serve as economic signals. At each iteration, the optimization agent solves a constrained optimization problem with penalty-adjusted objectives, while the LLM agent processes contextual information and outputs semantic confidence levels mapped to numerical weights. The coordinator then updates dual prices based on deviations from consensus and computes a new public decision by minimizing combined penalties. This process repeats until primal/dual residuals fall below thresholds, with the framework demonstrating convergence under convexity assumptions.

## Key Results
- SOLID variants achieved higher annualized returns than optimization-only methods in portfolio optimization
- The framework maintained lower volatility compared to pure optimization approaches
- Convergence was demonstrated through progressive alignment of agent proposals across iterations

## Why This Works (Mechanism)

### Mechanism 1: Dual Pricing as Inter-Agent Coordination Signals
- Claim: Dual prices function as economic signals that coordinate optimization and LLM agents toward consensus decisions.
- Mechanism: At each iteration, the coordinator updates dual prices λ based on how much each agent's preferred decision deviates from the current public plan. Agents receive these prices, which effectively compensate or charge them for their activity levels, creating aligned incentives. This mirrors ADMM's decomposition-coordination approach where distributed subproblems converge through price adjustments.
- Core assumption: Agents respond rationally to price signals by incorporating them into their utility maximization; the optimization agent's objective remains sufficiently convex.
- Evidence anchors:
  - [abstract] "SOLID facilitates iterative collaboration between optimization and LLMs agents through dual prices and deviation penalties."
  - [section Algorithm 1, Eq. 3] "Coordinator updates activity prices: λ(k) = λ(k-1) - ρ(x̃(k) - x(k-1))"
  - [corpus] Weak direct corpus support; related work on multi-agent coordination (LiveTradeBench) emphasizes real-world decision dynamics but lacks dual pricing mechanisms.
- Break condition: Non-convex agent objectives (e.g., LLM discrete outputs) may prevent convergence guarantees; empirical convergence observed but not theoretically guaranteed.

### Mechanism 2: Quadratic Deviation Penalties for Consensus Enforcement
- Claim: Quadratic penalties on decision deviation from the public plan create soft constraints that pull agent proposals toward agreement without hard enforcement.
- Mechanism: Each agent's augmented utility includes a term -(ρ/2)||x - x^(k-1)||² that penalizes distance from the current consensus. The coordinator's reconciliation step (Eq. 4) minimizes combined penalties from both agents. This soft coupling allows flexibility while progressively reducing disagreement.
- Core assumption: Penalty parameter ρ is appropriately tuned; agents have overlapping feasible regions so consensus is achievable.
- Evidence anchors:
  - [abstract] "interaction improves the quality of the decisions while maintaining modularity"
  - [section 2] "This information creates augmented utility functions that pay agents for activities while penalizing deviations from consensus."
  - [corpus] No direct corpus evidence for this specific penalty mechanism in LLM-optimization hybrids.
- Break condition: If agent preferences are fundamentally opposed (non-overlapping high-utility regions), penalties alone may not achieve meaningful consensus; solutions may converge to unsatisfactory compromises.

### Mechanism 3: Semantic Abstraction for LLM Numerical Limitations
- Claim: Mapping continuous portfolio weights to discrete semantic levels ("Very High" to "Very Low") aligns LLM tasks with their strengths in semantic reasoning rather than numerical optimization.
- Mechanism: The LLM agent outputs confidence levels on a 7-point semantic scale (0.0 to 0.6 in 0.1 increments), which are then mapped to numerical weights. This abstraction avoids requiring precise numerical calculations from the LLM while preserving directional signals.
- Core assumption: Semantic discretization provides sufficient resolution for decision quality; mapping preserves ordinal relationships.
- Evidence anchors:
  - [section 3.1] "We discretize LLM decisions into semantic levels: 'Very High', 'High', 'Somewhat High', 'Neutral', 'Somewhat Low', 'Low', and 'Very Low', mapping to numerical values from 0.6 to 0"
  - [section C] "LLMs struggle with numerical calculations but excel at interpreting semantic descriptions such as 'high', 'medium', and 'low'"
  - [corpus] Weak corpus support; related work (LB-MCTS) combines LLMs with Bayesian optimization but uses different abstraction strategies.
- Break condition: Coarse discretization may lose critical precision for marginal decisions; 7 levels may be insufficient for high-dimensional problems.

## Foundational Learning

- Concept: **ADMM (Alternating Direction Method of Multipliers)**
  - Why needed here: SOLID's coordination mechanism is directly derived from ADMM's decomposition-coordination approach. Understanding how ADMM uses dual variables and quadratic penalties to achieve consensus is essential for debugging convergence behavior.
  - Quick check question: In standard ADMM with minimize f(x) + g(z) subject to x = z, what role does the penalty parameter ρ play in the augmented Lagrangian?

- Concept: **Dual Variables as Shadow Prices**
  - Why needed here: The framework interprets λ as "economic costs of preferred decisions." This economic lens explains how price signals coordinate agents—understanding Lagrange multipliers as marginal costs is critical for interpreting coordinator outputs.
  - Quick check question: If the dual price λ_llm for a stock weight increases, what does this imply about the LLM agent's recent proposals relative to the consensus?

- Concept: **Markowitz Mean-Variance Portfolio Optimization**
  - Why needed here: The optimization agent uses this classic formulation. Understanding the tradeoff between expected return (μ) and risk (Σ) explains why pure optimization can exhibit "pronounced volatility spikes" and why LLM contextual input provides diversification benefits.
  - Quick check question: In min_w (1/2)w'Σw subject to w'μ = p, w'1 = 1, what happens to optimal weights when the covariance matrix Σ has high off-diagonal correlations?

## Architecture Onboarding

- Component map:
  Optimization Agent -> Coordinator -> LLM Agent -> Coordinator -> Optimization Agent

- Critical path:
  1. Initialize x^(0), λ^(0) (typically uniform weights, zero dual prices)
  2. Optimization agent solves Markowitz problem with penalty-adjusted objective
  3. LLM agent processes news and prices, outputs semantic levels → numerical weights
  4. Coordinator updates λ and reconciles to new x^(k)
  5. Repeat until residuals below threshold or max iterations reached
  6. Output final portfolio weights

- Design tradeoffs:
  - **Step size ρ**: Larger ρ accelerates convergence but may overshoot; smaller ρ is stable but slower. Paper does not specify tuning method.
  - **Semantic granularity**: 7 levels balances LLM capability with decision resolution; increasing levels raises LLM error risk.
  - **LLM temperature**: Set to 0 for determinism; reduces exploration around current solutions. Higher temperature could aid non-convex landscapes but introduces noise.
  - **Prompt design**: System prompt biases LLM toward optimizer's proposals ("optimization model has consistently outperformed by 10-20%"); this accelerates consensus but may suppress valuable LLM dissent.

- Failure signatures:
  - **Non-convergence**: Primal residuals (||x̃_opt - x̃_llm||) do not approach zero; check for fundamentally opposed agent preferences or inappropriate ρ.
  - **Extreme allocations from optimizer**: Pure optimization shows concentrated weights (e.g., MDT in Panel C); indicates covariance matrix singularities or extreme expected returns—should trigger LLM override or regularization.
  - **LLM semantic drift**: Confidence levels inconsistent across iterations; check prompt stability and context window limits.
  - **Divergent dual prices**: λ values grow unbounded; indicates infeasible consensus region.

- First 3 experiments:
  1. **Replicate single-stock convergence trace**: Reproduce Figure 3 (AMZN coordination process) with same hyperparameters (ρ, K, prompts) to validate implementation. Check exact convergence at months 1 and 7, progressive alignment elsewhere.
  2. **Ablate penalty parameter**: Run SOLID on 5-stock subset with ρ ∈ {0.1, 0.5, 1.0, 2.0}. Measure convergence speed (iterations to primal residual < 0.01) and final portfolio variance. Identify ρ that balances speed and stability.
  3. **Test semantic discretization sensitivity**: Compare 7-level vs. 11-level vs. continuous (forced numerical output) LLM decisions on same time period. Measure return/risk tradeoff and LLM output consistency (variance across repeated runs with temperature 0).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions can theoretical convergence guarantees be extended to SOLID frameworks with non-convex LLM agents?
- Basis in paper: [explicit] The authors state that "real-world systems often violate convexity assumptions due to discrete variables or non-convex objectives" and "if one or more agents exhibit non-convex behavior, convergence cannot always be guaranteed."
- Why unresolved: The theoretical convergence proof (Theorem 1) requires Assumption 1 (proper, closed, convex functions), but LLM agents using discrete semantic outputs inherently violate convexity. The paper acknowledges this but only offers heuristics (higher-level abstractions) without formal guarantees.
- What evidence would resolve it: Formal analysis characterizing convergence rates or fixed-point conditions when one agent is non-convex, or empirical studies establishing conditions (e.g., LLM output stability bounds) under which convergence is probable.

### Open Question 2
- Question: How does the choice of semantic discretization granularity (number of confidence levels) affect both convergence behavior and decision quality?
- Basis in paper: [inferred] The paper discretizes LLM decisions into seven semantic levels ("Very High" to "Very Low," mapped 0–0.6), but provides no justification for this granularity or analysis of alternatives.
- Why unresolved: Coarser discretization may improve convergence by reducing solution space complexity but could limit expressiveness; finer granularity approaches continuous decisions but may exacerbate LLM numerical weaknesses. The trade-off remains unexplored.
- What evidence would resolve it: Systematic experiments varying the number of discrete levels (e.g., 3, 5, 7, 10) and measuring convergence iterations, portfolio returns, and volatility to identify optimal granularity.

### Open Question 3
- Question: How does SOLID's performance scale with problem dimension (number of decision variables) and what are the computational bottlenecks?
- Basis in paper: [inferred] The case study uses 60 stocks across 10 industries, but the paper makes broad claims about applicability to "diverse domains" without testing larger-scale problems.
- Why unresolved: LLM context windows and output consistency may degrade with larger action spaces, and the iterative coordinator updates (equations 3–4) face computational costs that scale with variable count. Whether SOLID remains tractable for problems with hundreds or thousands of variables is unknown.
- What evidence would resolve it: Experiments with progressively larger stock universes (e.g., S&P 500 constituents) or different domains with higher-dimensional decisions (supply chain networks, healthcare resource allocation), reporting iteration counts and runtime.

## Limitations
- Theoretical convergence guarantees do not extend to non-convex LLM agents with discrete outputs
- Framework performance critically depends on hand-tuned hyperparameters without systematic sensitivity analysis
- Computational overhead and quadratic penalty scaling raise scalability concerns for larger problems

## Confidence
- **High confidence**: The ADMM-inspired coordination mechanism (dual pricing + quadratic penalties) is mathematically sound for convex problems. The semantic abstraction approach for LLM numerical limitations is well-motivated by documented LLM weaknesses in precise calculations.
- **Medium confidence**: The portfolio optimization case study results showing improved risk-adjusted returns versus optimization-only baselines. The convergence behavior observed in the single-stock analysis appears reproducible under similar conditions.
- **Low confidence**: Generalizability to non-financial domains, performance under market stress conditions, and the framework's behavior with more sophisticated optimization objectives (beyond mean-variance).

## Next Checks
1. **Convergence sensitivity analysis**: Systematically vary ρ ∈ {0.1, 0.5, 1.0, 2.0} and maximum iterations K ∈ {50, 100, 200} across multiple stocks. Measure convergence speed, final portfolio variance, and sensitivity of optimal allocations to these parameters.

2. **Semantic discretization robustness**: Compare 7-level semantic outputs against 11-level and continuous (forced numerical) LLM decisions on the same market period. Quantify degradation in decision quality and LLM output consistency (variance across repeated runs).

3. **Out-of-sample stress testing**: Apply SOLID to historically volatile periods (e.g., 2008 financial crisis, 2020 COVID crash) using pre-2024 training data. Compare against optimization-only baselines in terms of maximum drawdown, recovery time, and tail risk metrics.