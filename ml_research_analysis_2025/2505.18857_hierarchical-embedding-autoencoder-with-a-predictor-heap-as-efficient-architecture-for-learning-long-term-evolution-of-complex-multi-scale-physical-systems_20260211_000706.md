---
ver: rpa2
title: Hierarchical-embedding autoencoder with a predictor (HEAP) as efficient architecture
  for learning long-term evolution of complex multi-scale physical systems
arxiv_id: '2505.18857'
source_url: https://arxiv.org/abs/2505.18857
tags:
- learning
- layers
- neural
- embedding
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a new neural network architecture, HEAP, for
  predicting the long-term evolution of multi-scale physical systems. HEAP is based
  on the idea that structures of various scales interact only locally and that smaller
  structures can interact indirectly through larger ones.
---

# Hierarchical-embedding autoencoder with a predictor (HEAP) as efficient architecture for learning long-term evolution of complex multi-scale physical systems

## Quick Facts
- **arXiv ID**: 2505.18857
- **Source URL**: https://arxiv.org/abs/2505.18857
- **Reference count**: 40
- **Primary result**: HEAP architecture achieves multifold improvement in long-term prediction accuracy for Hasegawa-Wakatani turbulence compared to conventional ResNet-based approaches

## Executive Summary
The authors propose HEAP, a neural network architecture for predicting long-term evolution of multi-scale physical systems. The key insight is that structures of different scales interact locally and indirectly, with smaller structures communicating through larger ones. HEAP uses a hierarchical fully-convolutional autoencoder to encode the system state into embedding layers at multiple resolutions, then predicts dynamics by advancing all embeddings synchronously while accounting for both lateral and vertical interactions.

## Method Summary
HEAP consists of three components: a hierarchical encoder that outputs embedding layers at progressively coarser spatial resolutions, a predictor that advances all embedding layers in sync using lateral and vertical convolutions, and a hierarchical decoder that reconstructs physical fields. The architecture is trained by first learning an autoencoder, then training the predictor on consecutive embedding pairs. For Hasegawa-Wakatani turbulence, the authors use a 3-layer hierarchy with 8 channels per layer, predicting 2D density and potential fields on 128×128 grids.

## Key Results
- HEAP outperforms flat ResNet architectures by 3× or more in long-term prediction accuracy
- Hierarchical models with at least 2 layers consistently outperform fully-convolutional models
- Interactions between embedding layers in the predictor are crucial for accurate predictions
- HEAP correctly predicts key statistical characteristics including Fourier spectra and temporal autocorrelations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-scale physical systems can be efficiently modeled by enforcing local interactions only, where distant small-scale structures communicate indirectly through larger encompassing structures.
- Mechanism: The hierarchical embedding creates a cascade where each layer encodes structures at a corresponding spatial scale. Small structures in the same neighborhood interact directly (same layer convolutions), while distant small structures interact via the deeper layers that encode larger structures encompassing both.
- Core assumption: Physical systems exhibit scale separation where structures interact primarily locally, with indirect long-range coupling mediated through larger-scale features.
- Evidence anchors:
  - [abstract] "Structures of various scales that dynamically emerge in the system interact with each other only locally. Structures of similar scale can interact directly when they are in contact and indirectly when they are parts of larger structures that interact directly."
  - [section 3] "Interactions between the embeddings only occur locally, between neighbors within each layer (laterally) and between the layers (vertically)."
- Break condition: Systems where small-scale features interact strongly at long range without intermediate structure mediation may violate this assumption.

### Mechanism 2
- Claim: Hierarchical embeddings preserve spatial information better than single-layer compressed representations for multi-scale prediction tasks.
- Mechanism: Unlike conventional autoencoders that collapse to a single bottleneck, HEAP's encoder outputs embeddings at multiple depths simultaneously. Each strided convolution creates both a "pass-down" pathway and an "output" to an embedding layer.
- Core assumption: Preserving explicit spatial representations at multiple resolutions is more important for dynamics prediction than achieving maximum compression.
- Evidence anchors:
  - [section 3.1] "An additional strided (stride=2) 'output' convolution is applied at each layer of the encoder to divert and compress information from this layer into a corresponding embedding layer."
  - [section 4.2] "Hierarchical models with at least 2 layers (H2-H5) outperform fully-convolutional models (C1=H1, C2, C3)."
- Break condition: Systems dominated by a single spatial scale may not benefit from hierarchical representation.

### Mechanism 3
- Claim: Synchronized advancement of all embedding layers with explicit cross-layer convolutions is necessary for accurate long-term dynamics.
- Mechanism: The predictor operates on all hierarchical layers simultaneously using a ResNet-like structure enhanced with vertical convolutional operators between layers.
- Core assumption: Different scales co-evolve with mutual influence; updating them independently or sequentially introduces systematic drift.
- Evidence anchors:
  - [abstract] "The predictor advances all embedding layers in sync, accounting for local interactions within each layer and between layers."
  - [section 4.4] "We show that interactions between embedding layers in the predictor are crucial."
  - [section 3.2] "When the number of embedding layers in our model reduces to one, the predictor converts into a ResNet."
- Break condition: Systems with truly separable timescales might be better served by adiabatic elimination rather than synchronized prediction.

## Foundational Learning

- **Convolutional Autoencoder Basics**: HEAP's encoder/decoder are fully-convolutional; understanding strided convolutions (downsampling) and transposed convolutions (upsampling) is prerequisite.
  - Quick check: Can you sketch how a 4×4 stride-2 convolution reduces a 128×128 input to 64×64, and how a transposed convolution inverts this?

- **ResNet Skip Connections**: The hierarchical predictor extends ResNet's residual structure; vanishing/exploding gradient mitigation via skip connections is assumed knowledge.
  - Quick check: Why does adding the input to a convolution block's output (F(x) + x) ease training of deep networks compared to learning F(x) alone?

- **Autoregressive Rollout and Error Accumulation**: HEAP predicts in embedding space autoregressively; small systematic errors compound over long rollouts.
  - Quick check: If a predictor has 1% error per timestep, what approximate error magnitude do you expect after 100 autoregressive steps?

## Architecture Onboarding

- **Component map**: Input -> Hierarchical Encoder -> Embedding Layers (E1, E2, E3) -> Hierarchical Predictor -> Hierarchical Decoder -> Output

- **Critical path**:
  1. Input: 2D fields (n, φ) at 128×128×2
  2. Encoder produces N embedding layers (H3 model: 32×32×8, 16×16×8, 8×8×8)
  3. Predictor takes embedding state at time t, outputs embedding state at t+1
  4. Autoregressive loop: predictor output feeds directly back as next input
  5. Decoder applied only when physical-space output needed

- **Design tradeoffs**:
  - Number of layers (N): H3 (3 layers) optimal for HW turbulence; H4/H5 showed no further improvement
  - Channels per layer: Paper uses 8 channels uniformly; deeper single-layer models increase channels but perform worse
  - Training time: Hierarchical predictors ~2× slower than flat ResNet, but accuracy gains justify cost

- **Failure signatures**:
  - Training instability: 10-20% of random initializations fail to converge
  - Spectral drift in rollout: Shallow models underpredict high-frequency content
  - Autocorrelation decay errors: Flat models fail to capture long-term temporal correlations

- **First 3 experiments**:
  1. Baseline autoencoder reconstruction: Train H1 and H3 autoencoders; compare spatial FFT spectra against ground truth
  2. Ablation: remove cross-layer interactions: Train predictor with vertical convolutions disabled; expect significant accuracy degradation
  3. Layer count sweep: Train H1 through H5 models; plot spatial FFT error, temporal FFT error, and autocorrelation error vs. layer count

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the HEAP architecture be effectively scaled to 3D physical systems using 3D convolutions without incurring prohibitive computational costs or loss of accuracy?
- Basis in paper: [explicit] The authors state, "We study a 2D system, but the model can be extended to 3D systems using 3D convolutions."
- Why unresolved: While theoretically possible, 3D convolutions significantly increase memory and compute requirements compared to 2D.
- What evidence would resolve it: Successful application of HEAP to a 3D turbulence dataset demonstrating comparable statistical accuracy and computational efficiency.

### Open Question 2
- Question: How can the hierarchical embedding approach be adapted for unstructured grids using Graph Neural Networks (GNNs) while preserving the local interaction constraints?
- Basis in paper: [explicit] The paper notes, "this approach can be adapted to unstructured data using graph networks (GNNs)... by utilizing graph convolutions."
- Why unresolved: Translating the specific hierarchical strided convolution mechanism to an irregular graph topology without losing the physical inductive bias requires a non-trivial architectural redesign.
- What evidence would resolve it: Implementation of a "Graph-HEAP" model applied to an unstructured mesh simulation showing performance metrics.

### Open Question 3
- Question: Does HEAP generalize to multi-scale systems where the assumption of strictly local interactions is violated or where energy cascades differ from the Hasegawa-Wakatani model?
- Basis in paper: [explicit] The authors apply the model only to Hasegawa-Wakatani turbulence but "expect the results to be applicable to other continuous multi-scale physical systems."
- Why unresolved: The architecture is predicated on the intuition that structures interact locally and via encompassing larger structures.
- What evidence would resolve it: Benchmarking HEAP against baselines on diverse physical systems to verify if the multifold improvement holds.

## Limitations
- Hierarchical predictor is approximately 2× slower than flat ResNet, though accuracy gains justify the computational cost
- 10-20% of random initializations fail to converge, requiring multiple training attempts
- Architecture tuned specifically for Hasegawa-Wakatani turbulence; generalization to other multi-scale systems untested

## Confidence
- **High confidence**: Hierarchical embeddings outperform flat models in long-term prediction accuracy for HW turbulence
- **Medium confidence**: Cross-layer interactions are necessary for accuracy
- **Medium confidence**: 3-layer hierarchy is optimal for this specific system

## Next Checks
1. **Cross-layer interaction ablation**: Train a variant of HEAP with vertical convolutions removed (lateral only) and compare rollout statistics to the full model.

2. **Single-scale system test**: Apply HEAP to a system dominated by one spatial scale (e.g., 2D Navier-Stokes at high Reynolds number) and compare performance to conventional ResNet.

3. **Multi-seed stability analysis**: Train 20 random initializations of H3 on HW turbulence and report the distribution of rollout errors.