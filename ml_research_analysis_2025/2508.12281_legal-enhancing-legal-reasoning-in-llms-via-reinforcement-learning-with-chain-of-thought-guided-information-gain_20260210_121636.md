---
ver: rpa2
title: "Legal$\u0394$: Enhancing Legal Reasoning in LLMs via Reinforcement Learning\
  \ with Chain-of-Thought Guided Information Gain"
arxiv_id: '2508.12281'
source_url: https://arxiv.org/abs/2508.12281
tags:
- legal
- reasoning
- reward
- learning
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Legal$Δ$: Enhancing Legal Reasoning in LLMs via Reinforcement Learning with Chain-of-Thought Guided Information Gain

## Quick Facts
- arXiv ID: 2508.12281
- Source URL: https://arxiv.org/abs/2508.12281
- Reference count: 0
- Primary result: Two-stage framework combining SFT distillation from DeepSeek-R1 with GRPO-based refinement improves legal reasoning performance across multiple Chinese legal datasets

## Executive Summary
LegalΔ introduces a reinforcement learning framework that enhances legal reasoning in LLMs by leveraging information-theoretic rewards based on chain-of-thought (CoT) augmented predictions. The method employs a dual-mode input setup comparing direct answers with reasoning-augmented outputs, rewarding models when CoT improves prediction quality. Through two-stage training—first distilling reasoning capabilities from DeepSeek-R1 via supervised fine-tuning, then refining with group-wise proximal policy optimization—the approach achieves state-of-the-art results on multiple legal reasoning tasks while demonstrating improved confidence and legal token prominence.

## Method Summary
LegalΔ employs a two-stage approach to enhance legal reasoning in LLMs. Stage 1 involves distilling 450 high-quality reasoning samples from DeepSeek-R1 through supervised fine-tuning (SFT) on Qwen2.5-7B/14B-Instruct models. Stage 2 applies group-wise proximal policy optimization (GRPO) with a novel information gain reward mechanism. The framework computes ∆Q(r) = logits(a|q,r) − logits(a|q), representing the information gain between reasoning-augmented and direct predictions. This differential is used multiplicatively with legal rewards: R_info = R_legal × σ(∆Q · T). Training uses 6,981 samples with 563 validation samples, generating G=6 trajectories per query and optimizing with KL-regularized policy gradients.

## Key Results
- Achieves state-of-the-art performance on Chinese legal reasoning benchmarks (CAIL2018, JEC-QA)
- Demonstrates significant improvements over R1-distilled baseline through GRPO refinement (73.45 vs 71.95 average scores)
- Shows progressive development of higher confidence and legal token prominence during training

## Why This Works (Mechanism)

### Mechanism 1: Information-Theoretic Reward via Q-Value Differential
The framework rewards models based on information gain between direct and reasoning-augmented predictions using ∆Q(r) = logits(a|q,r) − logits(a|q). This differential mathematically decomposes into pointwise mutual information between reasoning and answer plus a global confidence regularizer. The reward is applied multiplicatively: R_info(a) = R_legal(a) × σ(∆Q · T). The core assumption is that logits approximate Q-functions in offline inverse RL, and confidence shifts induced by reasoning correlate with reasoning quality.

### Mechanism 2: Two-Stage Distillation + GRPO Training
Sequential SFT distillation from DeepSeek-R1 followed by GRPO-based refinement yields greater gains than either stage alone. Stage 1 imitates expert reasoning via cross-entropy SFT on DeepSeek-R1 outputs. Stage 2 applies GRPO, which estimates advantages via intra-group comparison and optimizes with KL-regularized policy gradient. The core assumption is that reasoning patterns from DeepSeek-R1 transfer to legal domain, and GRPO's group-wise advantage estimation effectively exploits quality variance.

### Mechanism 3: Confidence-Enhanced Legal Token Attention
Training with information-gain rewards progressively increases model confidence (logits) and legal token prominence. The ∆Q-based reward produces higher logits in CoT scenarios over training, correlating with increased attention to legally relevant tokens. The core assumption is that higher logits reflect more stable internal representations and genuine understanding rather than overconfidence.

## Foundational Learning

**Concept: Q-Learning and Policy Gradient Methods**
- Why needed here: GRPO builds on policy gradient foundations using log-probability weighted advantages; understanding the relationship between Q-values, value functions, and policy optimization is essential for the ∆Q formulation.
- Quick check question: Given a policy π(a|s) and Q-function Q(s,a), how would you compute the advantage A(s,a)? Why does GRPO avoid needing a separate critic?

**Concept: Information Theory (Mutual Information and PMI)**
- Why needed here: The reward mechanism decomposes ∆Q into PMI and a normalization term; understanding why PMI measures reasoning-answer correlation—and when it fails—is critical for debugging.
- Quick check question: If PMI(reasoning, answer) is negative, what does that imply about the reasoning chain's contribution? Could this ever indicate a useful reasoning pattern?

**Concept: Chain-of-Thought Prompting**
- Why needed here: The dual-mode setup (direct vs. CoT) is the core signal source; understanding how CoT changes model behavior and when it fails to help is foundational.
- Quick check question: In ∆Q(r) = logits(a|q,r) − logits(a|q), what does a strongly positive value indicate? What if ∆Q is large but the answer is wrong?

## Architecture Onboarding

**Component map:**
Base LLM (Qwen2.5-7B/14B-Instruct) -> SFT Module (cross-entropy loss on DeepSeek-R1 distilled pairs) -> GRPO Engine (group sampling, intra-group advantage computation) -> Reward Calculator (format + info-gain rewards) -> Dual-Mode Evaluator (computes logits for ∆Q calculation)

**Critical path:**
1. Distill 450 high-quality reasoning samples from DeepSeek-R1 → SFT on Qwen2.5 backbone
2. Collect 6,981 training samples for GRPO phase (563 validation)
3. For each query: generate G=6 trajectories, compute format + info-gain rewards
4. Compute group-wise advantages and update policy via GRPO with β KL penalty

**Design tradeoffs:**
- Temperature T=0.2: Lower = conservative reward amplification; higher = more sensitive but potentially unstable training
- G=6 samples per prompt: More samples improve advantage estimates but increase compute; fewer may introduce noise
- Multiplicative reward structure: Preserves correctness primacy but may underweight high-quality reasoning when baseline confidence is already high

**Failure signatures:**
- Logits diverging without accuracy improvement → potential reward hacking
- ∆Q consistently positive/negative → dual-mode comparison not capturing meaningful signal
- GRPO advantage estimates near zero → insufficient quality variance in sampled trajectories
- Out-of-domain performance collapse → distilled patterns not generalizing

**First 3 experiments:**
1. **Sanity check:** Verify ∆Q correlates with reasoning quality—sample 20 queries, compute ∆Q for good vs. bad reasoning chains (expert-labeled), confirm separation.
2. **Hyperparameter grid:** Test T ∈ {0.1, 0.2, 0.5, 1.0} and G ∈ {3, 6, 12} on validation set to identify stable training regime.
3. **Generalization probe:** Train on CAIL2018, evaluate on all out-of-domain benchmarks; compare against R1-Distill baseline to isolate GRPO contribution from SFT transfer.

## Open Questions the Paper Calls Out

### Open Question 1
How effectively does LegalΔ generalize across different legal jurisdictions and legal systems beyond Chinese law? All experiments are conducted exclusively on Chinese legal datasets (CAIL2018, JEC-QA) and benchmarks. No validation on other legal systems is mentioned.

### Open Question 2
How sensitive is LegalΔ's performance to the choice and quality of the teacher model used for distillation? The framework relies on DeepSeek-R1 for reasoning distillation, but no ablation or comparison with alternative teacher models is provided.

### Open Question 3
How robust is the information-gain reward mechanism to hyperparameter choices, particularly the temperature parameter T? The paper sets T=0.2 without systematic analysis of how this value affects reward modulation, training dynamics, or final performance.

### Open Question 4
Can the information-gain mechanism effectively scale to significantly larger legal corpora and more diverse task distributions? Training uses only 6,981 samples; performance on larger-scale, real-world legal corpora with greater domain diversity remains untested.

## Limitations
- Empirical generalization scope limited to Chinese legal datasets with minimal out-of-domain testing
- Reward signal validity depends on unverified assumptions about logits differences reflecting reasoning quality
- Data efficiency concerns with requirement for 450 distilled samples plus 6,981 training samples

## Confidence
- **High Confidence:** The two-stage distillation architecture (SFT + GRPO) is technically sound and produces measurable improvements on reported benchmarks
- **Medium Confidence:** The information gain reward mechanism's theoretical foundation is reasonable, but practical effectiveness depends on unverified assumptions
- **Low Confidence:** Claims about legal token prominence and confidence development as indicators of reasoning quality lack external validation

## Next Checks
1. **Cross-Domain Transfer Test** - Train LegalΔ on CAIL2018, then evaluate on a completely different legal reasoning task (e.g., legal contract analysis or statutory interpretation) to measure generalization beyond training domain.

2. **Human Evaluation Correlation** - Collect expert ratings for reasoning quality on 100 samples from validation set, then compute correlation between human scores and ∆Q values to validate reward signal's alignment with actual reasoning quality.

3. **Ablation on Information Gain** - Train a variant that optimizes only R_legal (removing the σ(∆Q·T) multiplier) while keeping all other components identical, to isolate contribution of information gain mechanism versus standard legal reasoning rewards.