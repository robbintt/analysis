---
ver: rpa2
title: Federated Learning for Heterogeneous Electronic Health Record Systems with
  Cost Effective Participant Selection
arxiv_id: '2404.13318'
source_url: https://arxiv.org/abs/2404.13318
tags:
- learning
- federated
- host
- patient
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce EHRFL, a federated learning framework for
  building institution-specific clinical prediction models in healthcare settings
  where EHR systems are heterogeneous and cost constraints exist. The framework comprises
  text-based EHR modeling to enable cross-institution compatibility without costly
  data standardization, and a differentially private participant selection strategy
  based on averaged patient embedding similarity to reduce the number of federated
  learning participants.
---

# Federated Learning for Heterogeneous Electronic Health Record Systems with Cost Effective Participant Selection

## Quick Facts
- arXiv ID: 2404.13318
- Source URL: https://arxiv.org/abs/2404.13318
- Reference count: 40
- Authors: Jiyoun Kim; Junu Kim; Kyunghoon Hur; Edward Choi
- Primary result: EHRFL framework achieves performance improvements over local-only training while effectively reducing the number of participating subjects without sacrificing model performance.

## Executive Summary
The authors introduce EHRFL, a federated learning framework for building institution-specific clinical prediction models in healthcare settings where EHR systems are heterogeneous and cost constraints exist. The framework comprises text-based EHR modeling to enable cross-institution compatibility without costly data standardization, and a differentially private participant selection strategy based on averaged patient embedding similarity to reduce the number of federated learning participants. Experiments on MIMIC-III, MIMIC-IV, and eICU datasets demonstrate that EHRFL achieves performance improvements over local-only training while effectively reducing the number of participating subjects without sacrificing model performance.

## Method Summary
EHRFL enables federated learning across heterogeneous EHR systems through a two-component approach. First, it linearizes raw EHR events into text sequences by concatenating event types and feature name-value pairs, replacing medical codes with textual descriptions using institution-specific dictionaries. These text sequences are processed by a two-step Transformer architecture (EventEncoder and EventAggregator) to produce patient embeddings. Second, it implements a differentially private participant selection mechanism where the host institution trains an initial model, extracts and averages patient embeddings from all candidate subjects, adds Gaussian noise for privacy, and selects participants based on embedding similarity (cosine, Euclidean, or KL divergence). This approach reduces the number of participants in FL training while maintaining or improving model performance.

## Key Results
- Text-based linearization enables cross-institution compatibility without data standardization, with FL models outperforming local-only training across multiple FL algorithms and host institutions.
- Averaged patient embedding similarity between host and candidate subjects correlates with performance gain (Spearman correlations ~0.42-0.45 for cosine similarity), enabling effective participant selection.
- Participant selection based on embedding similarity reduces the number of participants (K=2-4 of 5) while maintaining or exceeding the performance of using all 5 participants in 13/15 cases.

## Why This Works (Mechanism)

### Mechanism 1: Text-based EHR Linearization for Heterogeneous Systems
- Claim: Text-based linearization enables compatible modeling across institutions with different EHR schemas and coding systems.
- Mechanism: Raw EHR events (event types and feature pairs) are converted into text sequences by concatenating event type and feature name-value pairs, replacing medical codes with their textual descriptions using institution-specific dictionaries. These text sequences are then tokenized and processed by Transformer-based encoders (EventEncoder and EventAggregator).
- Core assumption: The text-based representation captures sufficient clinical semantics across heterogeneous EHR systems, and the code-to-text dictionaries within each institution's EHR database are accurate and complete enough for meaningful cross-institutional learning.
- Evidence anchors:
  - [abstract] "text-based EHR modeling, which facilitates cross-institution compatibility without costly data standardization"
  - [section 2.1.2] Formula: ri = (ei ⊕ n_i1 ⊕ v'_i1 ⊕ n_i2 ⊕ v'_i2 ···)
  - [section 4.1] Table 4: FL with text-based modeling improves over Single training across 4 FL algorithms and 5 hosts.
  - [corpus] "Representation Learning to Advance Multi-institutional Studies with EHR Data" (FMR 0.506) addresses heterogeneity, supporting general relevance of this challenge.
- Break condition: If code-to-text mappings are incomplete or semantically misaligned across institutions (e.g., same concept mapped to different text terms), text representations will not be compatible, degrading FL performance.

### Mechanism 2: Differentially Private Averaged Patient Embedding Similarity for Participant Selection
- Claim: Averaged patient embedding similarity between host and candidate subjects correlates with performance gain, enabling cost-effective participant exclusion.
- Mechanism: Host trains a model and shares weights. Both host and subjects extract patient embeddings, clip them (ℓ2-norm), and average them. Subjects add Gaussian noise (DP) before sharing. Host computes similarity (cosine, Euclidean, or KL divergence) and excludes low-similarity subjects from FL.
- Core assumption: Averaged patient embeddings provide a sufficient statistic of data distribution similarity relevant to the host's predictive tasks.
- Evidence anchors:
  - [abstract] "participant selection strategy based on averaged patient embedding similarity... differentially private"
  - [section 4.2] Table 5: Spearman correlations ~0.42-0.45 (cosine), ~-0.42 to -0.49 (Euclidean), ~-0.41 to -0.51 (KL) between similarity and host performance change.
  - [section 4.3] Table 6: In 13/15 cases, selection via embedding similarity achieves performance ≥ using all 5 clients; in 8/15 matches Best; in 14/15 exceeds Average.
  - [corpus] Weak direct evidence for this specific DP-protected averaged embedding selection method in FL.
- Break condition: If embedding averaging loses critical distributional information (e.g., multi-modality, rare subpopulations), similarity scores may not reflect actual utility, leading to exclusion of beneficial subjects.

### Mechanism 3: Cost Reduction Through Selective Participation
- Claim: Excluding low-similarity subjects reduces overall FL cost (data usage fees, computation, communication) without sacrificing host model performance.
- Mechanism: By selecting K subjects from N candidates based on embedding similarity, the host avoids (N-K)×(data usage fee + training cost) and (N-K)×R×(communication cost), offset by one-time selection costs.
- Core assumption: The one-time selection overhead (training the embedding model, extracting and sharing embeddings) is significantly lower than the recurring per-round costs of including all N participants.
- Evidence anchors:
  - [abstract] "reducing the number of participating subjects without sacrificing model performance... reduced costs and time"
  - [section 2.3] Table 3: Total Net Savings = (N-K)·X + ((N-K)·R·L - E)·C_train + ... - N·(C_extract + C_average + C_embedding + C_sim)
  - [section 4.3] K=2-4 often matches or exceeds K=5 performance.
  - [corpus] No direct corpus evidence for this specific cost model.
- Break condition: If selection costs approach or exceed savings from excluding subjects (especially when N-K is small), or if excluded subjects provide unique data for generalization, net benefits diminish.

## Foundational Learning

- Concept: Federated Learning (FL) Fundamentals (FedAvg, communication rounds, local epochs, aggregation)
  - Why needed here: The entire framework operates within an FL paradigm. Understanding weight aggregation, communication rounds, and local training is essential.
  - Quick check question: Can you explain the FedAvg algorithm, including how client updates are aggregated and the role of communication rounds vs. local epochs?

- Concept: Differential Privacy (DP) - Gaussian Mechanism, (ε, δ)-DP
  - Why needed here: Participant selection relies on DP to share averaged patient embeddings while protecting individual patient privacy.
  - Quick check question: How does the Gaussian mechanism for differential privacy work, and what do the parameters ε, δ, and the clipping bound C control?

- Concept: Transformer-based Sequence Modeling (Tokenization, Encoding, Aggregation)
  - Why needed here: EHR modeling uses a two-step Transformer architecture (EventEncoder, EventAggregator) to convert text events into patient embeddings.
  - Quick check question: How does a Transformer encoder process a sequence of tokens, and how is the output pooled to produce a single embedding?

## Architecture Onboarding

- Component map:
  1. **Data Preprocessing:** Raw EHR → Text Linearization (event type + feature pairs → text string)
  2. **Embedding Model (Host-trained):** Tokenized text → EventEncoder (2-layer Transformer) → Event vectors → EventAggregator (2-layer Transformer + average) → Patient embedding
  3. **Participant Selection (Pre-FL):** Host clips/averages embeddings; Subjects clip/average/add DP noise → share; Host computes similarity → select top K
  4. **Federated Learning Loop (K selected):** Broadcast → Local training on text-linearized EHR → Aggregation → Repeat
  5. **Host Inference:** Host uses final aggregated model for institution-specific predictions

- Critical path:
  1. Host trains initial embedding model on local text-linearized EHR (most time-consuming, 1-2 days GPU)
  2. Distribute model weights to all N candidates
  3. Extract, clip, average (and add DP noise for subjects) embeddings
  4. Collect averaged embeddings, compute similarities, select K subjects
  5. Execute FL with K subjects (up to 300 rounds or early stopping)
  6. Validate host performance; iterate if necessary

- Design tradeoffs:
  - **Similarity Metric:** Cosine (positive-correlated) vs. Euclidean/KL (negative-correlated); all showed similar effectiveness.
  - **DP Parameters (ε=1.0, δ=10⁻⁵, C=1.0):** Tighter privacy (lower ε) increases noise, potentially degrading similarity accuracy.
  - **K Selection:** Too few risks missing beneficial subjects; too many reduces savings. Experiments suggest K=2-4 (of 5) often suffices.
  - **FL Algorithm:** Framework is algorithm-agnostic; FedBN performed best in some cases, suggesting local batch normalization helps with feature heterogeneity.

- Failure signatures:
  - **Text linearization failure:** Missing/misaligned code-to-text mappings degrade embedding quality and FL performance.
  - **DP noise overwhelming signal:** Small patient counts (m) relative to C make noise (σ = C/m × √(...)) large, rendering embeddings uninformative.
  - **Correlation breakdown:** If host-subject similarity doesn't correlate with performance gain (Spearman < 0.2), selection may be random or harmful.
  - **Selection cost dominance:** For few candidates (small N) or expensive embedding extraction, selection overhead may exceed savings.

- First 3 experiments:
  1. **Baseline reproduction:** Replicate Table 4. Train Single (local-only) vs. FL (2-5 clients) using text-based linearization and one FL algorithm (e.g., FedAvg) to validate FL improves over Single.
  2. **Correlation validation:** Replicate Table 5. Compute averaged patient embeddings (without DP for debugging), measure cosine similarity, and correlate with performance change. Confirm Spearman > 0.3.
  3. **Selection pilot:** Replicate one row of Table 6. Select K=2 subjects using cosine similarity on DP-protected embeddings (ε=1.0, δ=10⁻⁵, C=1.0), run FL, and compare to K=5. Verify K=2 matches or exceeds K=5 for at least one host configuration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the EHRFL framework be effectively generalized to clinical settings outside of Intensive Care Units (ICUs), such as outpatient or emergency department data?
- Basis in paper: [explicit] The authors state in the Limitations section: "In future work, we plan to extend our framework to other clinical settings (e.g., outpatient data, emergency department data)" to assess generalizability, as current experiments were restricted to ICU datasets due to resource constraints.
- Why unresolved: The distinct characteristics of outpatient and emergency data (e.g., varying visit frequencies, different event types) may impact the efficacy of the text-based linearization and participant selection methods optimized for ICU data.
- What evidence would resolve it: Experimental results demonstrating the framework's performance (AUROC) and cost-effectiveness when applied to outpatient and emergency department EHR datasets.

### Open Question 2
- Question: Does the integration of explicit data quality control and cleaning strategies prior to text-based linearization improve the robustness and reliability of the EHRFL framework?
- Basis in paper: [explicit] The authors note: "Our framework does not explicitly incorporate data quality checks... Integrating data quality control and data cleaning strategies... may help remove erroneous data... We leave such applications to future work."
- Why unresolved: The current implementation assumes a baseline level of data quality, and it is unknown how noisy or erroneous raw EHR data affects the text-based modeling and subsequent participant selection.
- What evidence would resolve it: A comparative study evaluating model performance and text representation quality on datasets with and without the integration of automated data cleaning pipelines.

### Open Question 3
- Question: How does the proposed averaged patient embedding similarity criterion for participant selection compare to alternative criteria, such as label distributions or feature statistics, in terms of selection robustness?
- Basis in paper: [explicit] The authors mention in the Discussion: "A systematic comparison of different selection criteria may further improve robustness, which we leave for future work as this is the first work for participating client selection."
- Why unresolved: While embedding similarity showed positive correlations, it remains untested whether simpler statistical metrics or label-based metrics could achieve similar or better performance with lower computational overhead.
- What evidence would resolve it: A benchmark study comparing the host model performance and selection accuracy using embedding similarity versus label distribution divergence and feature statistics.

## Limitations

- **Text Linearization Completeness:** The framework's reliance on code-to-text dictionaries assumes perfect semantic alignment across institutions. Missing or misaligned mappings could degrade embedding quality and FL performance, though this is not directly evaluated.
- **DP Parameter Sensitivity:** The study uses fixed DP parameters (ε=1.0, δ=10⁻⁵, C=1.0). The impact of varying these parameters on selection accuracy and privacy-utility tradeoffs is not explored.
- **Cost Model Assumptions:** The cost reduction claims depend on specific cost assumptions (data usage fees, training costs, communication costs) that may not generalize across institutions. The one-time selection overhead is assumed to be low, but this depends on implementation details not fully specified.

## Confidence

- **High Confidence:** The FL framework with text-based linearization improves performance over local-only training (Table 4 results are robust).
- **Medium Confidence:** The participant selection strategy based on averaged patient embedding similarity effectively reduces participants without sacrificing performance (Table 6 results are promising but depend on specific FL algorithms and tasks).
- **Medium Confidence:** The cost reduction mechanism is theoretically sound, but the magnitude of savings depends on institution-specific cost parameters not fully disclosed.

## Next Checks

1. **Cross-Institution Code Mapping Validation:** Manually inspect code-to-text mappings for a sample of medical concepts across two institutions to verify semantic consistency and completeness.
2. **DP Parameter Sweep:** Systematically vary ε (e.g., 0.1, 0.5, 1.0, 5.0) and δ (e.g., 10⁻⁷, 10⁻⁵, 10⁻³) to assess their impact on selection accuracy (correlation with performance) and model utility.
3. **Cost Model Sensitivity Analysis:** Vary assumed costs (data usage fees, training costs, communication costs) within plausible ranges to determine how sensitive the net savings are to these parameters.