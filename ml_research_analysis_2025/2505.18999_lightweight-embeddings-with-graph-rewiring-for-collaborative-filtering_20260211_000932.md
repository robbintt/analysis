---
ver: rpa2
title: Lightweight Embeddings with Graph Rewiring for Collaborative Filtering
arxiv_id: '2505.18999'
source_url: https://arxiv.org/abs/2505.18999
tags:
- graph
- embedding
- embeddings
- entities
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of deploying GNN-based recommender
  systems on resource-constrained edge devices by proposing LERG, a framework that
  significantly reduces embedding storage and runtime computation costs. LERG extends
  the prior work LEGCF by introducing quantization techniques to compress meta-embeddings
  and a graph rewiring method to sparsify the propagation graph.
---

# Lightweight Embeddings with Graph Rewiring for Collaborative Filtering

## Quick Facts
- arXiv ID: 2505.18999
- Source URL: https://arxiv.org/abs/2505.18999
- Authors: Xurong Liang; Tong Chen; Wei Yuan; Hongzhi Yin
- Reference count: 40
- Key outcome: LERG achieves NDCG@20 of 0.0294 on Yelp2020 while reducing storage by 97.7% and runtime memory by 27.1% compared to baselines.

## Executive Summary
This paper presents LERG, a framework for deploying GNN-based recommender systems on resource-constrained edge devices. LERG extends prior work by introducing quantization to compress meta-embeddings and graph rewiring to sparsify the propagation graph. The quantized compositional embedding table is pretrained on resource-rich servers, then fine-tuned on a rewired graph that excludes low-contribution entities, further reducing computational complexity. Experiments on three datasets, including an industry-scale one, demonstrate superior recommendation performance while dramatically reducing storage and computation costs.

## Method Summary
LERG combines quantization and graph rewiring to create lightweight embeddings for collaborative filtering. The framework pretrains a quantized compositional embedding table on the full graph, then identifies and prunes low-contribution entities through a binary integer programming approach. The resulting sparse rewired graph and quantized embeddings are deployed to edge devices, where fine-tuning updates only retained entity embeddings. Pruned entities are represented by placeholder meta-embeddings generated from clustering their pretrained embeddings, minimizing runtime memory and computational load.

## Key Results
- Achieves NDCG@20 of 0.0294 on Yelp2020 dataset
- Reduces storage by 97.7% compared to unified dimensionality approaches
- Cuts runtime memory by 27.1% during fine-tuning
- Maintains competitive recommendation performance while significantly reducing computational complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantizing the meta-embedding codebook increases the number of unique embeddings representable under a fixed storage budget, reducing embedding collision and maintaining model expressiveness.
- Mechanism: Full-precision meta-embeddings are converted to low-bit integers (e.g., INT16) via learned step size quantization (LSQ). This reduces per-element storage, allowing more meta-embeddings within the same space. A sparse assignment matrix maps entities to combinations of these quantized meta-embeddings.
- Core assumption: The recommendation performance is more sensitive to embedding uniqueness and diversity than to per-element numerical precision, especially under severe storage constraints.
- Evidence anchors:
  - [abstract] "...introduces quantization techniques to compress meta-embeddings..."
  - [section 3.3] "This implies that under a fixed storage budget, replacing the full-precision codebook... allows more meta-embeddings to be included... reducing the risk of entity embedding collisions."
  - [corpus] Related work (Node-Aware Dynamic Quantization) confirms quantization as a viable path for GNN-based CF on resource-constrained devices.

### Mechanism 2
- Claim: Rewiring the user-item interaction graph by pruning low-contribution entities reduces computational complexity (MACs) and runtime memory during graph propagation without proportionally harming recommendation quality.
- Mechanism: A binary integer programming (BIP) problem is formulated using entity-entity similarity scores from pretrained graph-propagated embeddings. The solution identifies a subset of high-impact entities to retain. Edges from pruned entities are removed, and dangling connections are re-routed to multi-hop neighbors to ensure connectivity.
- Core assumption: Entities with high similarity to many others in the learned latent space are critical for preserving collaborative semantics during message passing, while low-similarity entities contribute less to the overall graph's signal.
- Evidence anchors:
  - [abstract] "...a fine-tuning stage is engaged to identify and prune low-contribution entities via a gradient-free binary integer programming approach..."
  - [section 3.5] "Thus, retaining the set of such entities in the rewired graph maximally preserves the overall collaborative semantics in the user-item interaction graph."
  - [corpus] Corpus provides weak direct evidence for this specific BIP-based rewiring; it is a novel contribution of the paper.

### Mechanism 3
- Claim: Decoupling the training of pruned and retained entity embeddings via a placeholder codebook minimizes peak runtime memory and computational load during on-device fine-tuning.
- Mechanism: During fine-tuning, only the embeddings of retained entities are recomputed by propagating signals through the rewired graph. Embeddings for pruned entities are not recomputed but are instead assigned from a small, fixed set of placeholder meta-embeddings generated by clustering their pretrained embeddings.
- Core assumption: Pruned entities require only a representative embedding for inference and do not need to participate in gradient-based updates, which would require storing their full intermediate embeddings during backpropagation.
- Evidence anchors:
  - [abstract] "The quantized compositional embedding table with selective embedding participation and sparse rewired graph are transferred to edge devices which significantly reduce computation memory..."
  - [section 3.6] "In the fine-tuning step, only the embeddings of retained entities will be updated using the rewired graph... effectively controls the peak runtime memory... at O(md)."
  - [corpus] The concept of using compressed representations for pruned entities is common in lightweight recommendation, but the specific placeholder clustering method is a design choice here.

## Foundational Learning

- Concept: **Compositional Embeddings & Codebooks**
  - Why needed here: LERG's core storage savings come from representing many user/item embeddings as combinations of a smaller set of shared "meta-embeddings" (the codebooks).
  - Quick check question: Can you explain how using a codebook of 2,000 meta-embeddings to represent 116,000 users/items reduces storage?

- Concept: **Quantization-Aware Training (QAT) & Straight-Through Estimator (STE)**
  - Why needed here: To train the codebook in low-bit precision (e.g., INT16) while still using gradient descent. The STE allows gradients to pass through the non-differentiable rounding operation.
  - Quick check question: During QAT, how are gradients computed for the full-precision weights when the forward pass uses quantized values?

- Concept: **Message Passing in Graph Neural Networks (GNNs)**
  - Why needed here: LERG optimizes the expensive matrix multiplication (MACs) inherent in GNN-based recommenders' graph propagation steps.
  - Quick check question: What is the primary computational operation during the graph propagation phase of a GNN-based recommender like LightGCN?

## Architecture Onboarding

- **Component map:**
  1. Quantized Compositional Embedding Table (E_meta, Î”) -> Sparse Assignment Matrix (S) -> Rewired Propagation Graph (A') -> Placeholder Codebook (C_prune) -> Training Pipeline (Pretrain -> Rewire -> Fine-tune)

- **Critical path:**
  1. Initialize assignment matrix S using METIS graph partitioning.
  2. Pretrain the quantized codebook on the server using the full graph.
  3. Compute entity similarity, solve BIP to identify entities to prune, and generate the rewired graph A'.
  4. Generate placeholder codebook C_prune for pruned entities.
  5. Deploy codebook, A', and C_prune. Fine-tune using only retained entities on A'.

- **Design tradeoffs:**
  - **Quantization bit-length (b)**: Lower bits (e.g., INT4) save more storage but risk performance on large datasets. INT8/16 is a safer default.
  - **Retention ratio (m/N)**: A key knob trading off computational cost (MACs) vs. recommendation accuracy. 0.7 is a common starting point.
  - **Assignment matrix update**: Fixed (as in LERG) for efficiency vs. learnable (as in prior LEGCF) for potential (but costly) accuracy gains.

- **Failure signatures:**
  - **Performance collapse on large dataset with low retention**: Check if retention ratio is too aggressive (e.g., <0.5). Increase or verify BIP entity selection.
  - **Poor performance after fine-tuning**: The placeholder embeddings for pruned entities may be too generic. Increase size of placeholder codebook (r).
  - **Storage budget exceeded**: Check if placeholder codebook (C_prune) size (r) is too large relative to the main codebook (c).

- **First 3 experiments:**
  1. **Baseline Comparison**: Implement LERG on a small dataset (Yelp2020) and compare its NDCG@20 and storage against a full-precision LightGCN (UD-128) and a compositional baseline (LEGCF). Verify storage reduction (~96%) with competitive performance.
  2. **Ablation Study on Rewiring**: Run LERG on the same dataset with retention ratios {1.0, 0.7, 0.5, 0.1}. Plot NDCG@20 and training epoch time to demonstrate the efficiency-accuracy tradeoff curve.
  3. **Quantization Precision Test**: On a medium dataset (Amazon-book), train LERG with quantization precisions of INT4, INT8, and INT16. Compare final NDCG@20 and storage to validate the impact of precision choice.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the interference between graph perturbation methods (e.g., xSimGCL's augmentation and LERG's rewiring) generalize across other contrastive learning recommenders, and can this interaction be theoretically characterized?
- Basis in paper: [explicit] Section 4.5 notes xSimGCL exhibits higher sensitivity to retention ratios, hypothesizing that "the two techniques together may have caused slight interference" but this remains unverified.
- Why unresolved: Only three base recommenders were tested; the interference mechanism was hypothesized but not systematically investigated across different graph augmentation strategies.
- What evidence would resolve it: Ablation studies across multiple contrastive GNN recommenders with controlled graph augmentation settings, measuring performance degradation patterns at varying retention ratios.

### Open Question 2
- Question: What factors determine the optimal quantization precision for different dataset scales, and why does INT16 significantly outperform INT4/INT8 specifically on large-scale datasets like iFashion?
- Basis in paper: [explicit] Section 4.6 shows INT4 settings are "more sensitive to retention ratios" on small datasets, while INT16 shows a "great leap forward" on iFashion, but the underlying cause remains hypothesized as expressiveness limitations.
- Why unresolved: The paper demonstrates the phenomenon but does not investigate whether it stems from embedding collision rates, gradient quantization noise, or representation capacity relative to entity count.
- What evidence would resolve it: Controlled experiments varying entity count while holding other factors constant, analyzing embedding uniqueness metrics and gradient flow across quantization levels.

### Open Question 3
- Question: How does LERG handle cold-start entities (new users/items) that appear after pretraining, and what is the degradation trajectory as the entity distribution shifts from the pretrained graph?
- Basis in paper: [inferred] The framework pretrains on a fixed user-item graph (Section 3.4) and relies on pretrained embeddings for both graph rewiring and placeholder assignment, but real-world edge deployment requires handling newly arriving entities.
- Why unresolved: The paper evaluates on static benchmark splits without simulating temporal entity addition; placeholder meta-embeddings are derived from pruned pretrained entities only.
- What evidence would resolve it: Streaming evaluation protocol where new entities arrive incrementally, measuring recommendation accuracy for cold-start entities and existing entities over time.

### Open Question 4
- Question: How sensitive is the BIP-to-LP relaxation and subsequent rounding to the threshold boundary parameter (o=0.5), and does optimal threshold vary with graph density or retention ratio?
- Basis in paper: [inferred] Section 3.5 introduces a fixed rounding boundary o=0.5 without sensitivity analysis; the entity selection depends critically on this threshold, yet no ablation is provided.
- Why unresolved: The binary approximation quality after LP relaxation may vary with the similarity score distribution, which differs across datasets and retention ratios.
- What evidence would resolve it: Grid search over threshold values [0.1, 0.9] across datasets with different densities, comparing resulting graph structures and downstream recommendation performance.

## Limitations
- Performance gains from graph rewiring are dataset-dependent and may not consistently identify the most critical entities across diverse recommendation scenarios.
- The fixed placeholder codebook size (r=500) may not be optimal for all datasets and recommendation settings.
- The framework's assumption that pruned entities can be adequately represented by clustered embeddings may break down in highly sparse or niche recommendation scenarios.

## Confidence
- **High Confidence**: The effectiveness of quantized compositional embeddings (Mechanism 1) is well-supported by prior work and experimental results showing substantial storage savings with maintained performance.
- **Medium Confidence**: The graph rewiring approach (Mechanism 2) is a novel contribution with promising results, but the corpus lacks strong evidence for the specific BIP formulation used. Its success is sensitive to the retention ratio and dataset characteristics.
- **Medium Confidence**: The runtime memory reduction from partial embedding updates (Mechanism 3) is theoretically sound and supported by the abstract and section 3.6, but the exact memory savings are not fully detailed in the results.

## Next Checks
1. **Robustness to Retention Ratio**: Conduct a sensitivity analysis on a large dataset (e.g., iFashion) by varying the retention ratio from 0.1 to 1.0 and plotting the NDCG@20 vs. MACs curve to identify the optimal tradeoff point.
2. **Placeholder Codebook Ablation**: Run an ablation study where the size of the placeholder codebook (r) is varied (e.g., 100, 500, 1000) on a medium dataset (e.g., Amazon-book) to measure its impact on recommendation accuracy and runtime memory.
3. **Cross-Dataset Pruning Efficacy**: Apply LERG's rewiring strategy to a dataset with a different interaction pattern (e.g., sequential recommendation) and compare the identified high-contribution entities to those from the original datasets to test the generalizability of the BIP-based pruning method.