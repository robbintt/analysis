---
ver: rpa2
title: 'Holmes: Automated Fact Check with Large Language Models'
arxiv_id: '2505.03135'
source_url: https://arxiv.org/abs/2505.03135
tags:
- evidence
- claim
- verification
- aletheia
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Aletheia is an end-to-end framework that improves multimodal fact-checking
  by enhancing evidence quality. It addresses the problem that existing retrieval-augmented
  methods struggle with incomplete coverage and noisy evidence.
---

# Holmes: Automated Fact Check with Large Language Models

## Quick Facts
- arXiv ID: 2505.03135
- Source URL: https://arxiv.org/abs/2505.03135
- Reference count: 40
- Key outcome: Aletheia achieves 88.3% accuracy on multimodal disinformation datasets and 90.2% on real-time verification tasks by improving evidence retrieval and evaluation

## Executive Summary
Aletheia addresses the challenge of automated multimodal fact-checking by introducing a novel evidence retrieval and evaluation framework. Traditional retrieval-augmented methods struggle with incomplete coverage and noisy evidence, leading to verification failures. Aletheia decomposes claims into structured sub-claims for targeted search and evaluates evidence using credibility, relevance, and integrity metrics. The framework demonstrates significant improvements over state-of-the-art methods, with up to 30.8% higher accuracy in fact-checking tasks.

## Method Summary
Aletheia is a three-stage framework that combines LLM-based claim interpretation, enhanced evidence retrieval, and structured verification. The system first decomposes multimodal claims into retrieval-oriented sub-claims, then retrieves and evaluates evidence using a three-metric pipeline (credibility filtering, relevance scoring via BLIP-2 embeddings, and integrity assessment via structured event extraction). Finally, an LLM verifies the claim using the high-quality evidence, producing binary verdicts with justifications. The framework was tested on multimodal disinformation datasets with commercial and open-source LLM backbones.

## Key Results
- 88.3% accuracy on public multimodal disinformation datasets
- 90.2% accuracy on real-time verification tasks
- Up to 30.8% improvement in fact-checking accuracy compared to state-of-the-art methods
- 0.11 USD/claim cost and ~24.6s latency with GPT-4o backbone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing multimodal claims into retrieval-oriented sub-claims improves evidence coverage and reduces verification failures from insufficient information.
- Mechanism: An LLM interprets both text and visual content to generate structured sub-claims, each targeting a specific factual aspect. These sub-claims are reformulated into targeted search queries rather than using the original claim directly.
- Core assumption: Search engines return better results for specific, decomposed queries than for complex multimodal claims as a whole.
- Evidence anchors:
  - [abstract] "It introduces a novel evidence retrieval strategy that (1) decomposes claims into structured sub-claims for targeted search"
  - [section 4.1] "Compared to directly using the original claim for search, these sub-claims provide more specific and diverse search queries. As a result, this enables the retrieval of broader and more relevant information."
  - [corpus] Weak direct support; neighbor papers (e.g., MCP-Orchestrated Multi-Agent System) focus on disinformation detection pipelines but do not specifically validate claim decomposition strategies.
- Break condition: Claims that are inherently simple or contain a single verifiable fact may not benefit from decomposition; overly granular decomposition could dilute semantic coherence.

### Mechanism 2
- Claim: A three-metric evidence evaluation pipeline (credibility, relevance, integrity) filters noisy evidence and selects high-quality sources, reducing misleading verification outcomes.
- Mechanism: (1) Credibility filters sources using blacklists and automated reliability models; (2) Relevance computes semantic similarity between claim and evidence using BLIP-2 embeddings; (3) Integrity measures completeness of factual information via structured event argument extraction. A weighted score ranks candidates.
- Core assumption: Evidence quality can be approximated through these three orthogonal dimensions, and their linear combination yields actionable ranking.
- Evidence anchors:
  - [abstract] "evaluates evidence using credibility, relevance, and integrity metrics"
  - [section 4.2.3, Algorithm 1] Formalizes the three-stage pipeline with credibility filtering followed by joint relevance-integrity scoring (q_i = α·r_i + (1−α)·m_i)
  - [corpus] Weak direct support; InFi-Check (arXiv:2601.06666) addresses fine-grained fact-checking interpretability but does not evaluate this specific three-metric evidence ranking.
- Break condition: Highly specialized claims where relevance and integrity trade off non-linearly; sources on blacklists that nonetheless contain unique primary evidence.

### Mechanism 3
- Claim: Providing LLMs with high-quality retrieved evidence enables reliable verification, whereas LLMs alone cannot assess truthfulness due to knowledge cutoffs and hallucination risks.
- Mechanism: LLMs are prompted in a stage-wise protocol (task initialization → evidence incorporation → verification) to produce binary verdicts with grounded justifications. Evidence quality gates prevent garbage-in-garbage-out failures.
- Core assumption: LLM reasoning is sound when supplied with complete, relevant, and credible evidence; the bottleneck is evidence quality, not reasoning capacity.
- Evidence anchors:
  - [abstract] "LLMs alone cannot reliably assess the truthfulness of claims; providing relevant evidence significantly improves their performance"
  - [section 3, Table 1] Human-written evidence yields 90.3% correctness vs. 63.7% for DEFAME's automated retrieval; 48.8% of failures stem from insufficient evidence, 46.3% from noisy evidence
  - [corpus] Multimodal Fact-Checking Agent-based Approach (arXiv:2512.22933) similarly argues that LVLMs suffer from shallow evidence utilization, supporting the evidence-quality bottleneck hypothesis.
- Break condition: Claims requiring specialized domain expertise not captured in available web sources; adversarial evidence that appears credible but is subtly misleading.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: Aletheia is fundamentally a RAG system optimized for fact-checking; understanding evidence retrieval vs. generation tradeoffs is prerequisite.
  - Quick check question: Can you explain why retrieving evidence before LLM generation mitigates hallucination for claims outside training data?

- Concept: **Multimodal Embeddings (BLIP-2 style)**
  - Why needed here: Relevance scoring requires joint text-image representation; BLIP-2 encodes multimodal claims for cosine similarity computation.
  - Quick check question: How does a vision-language model produce comparable embeddings for a text claim and an image-based evidence snippet?

- Concept: **Evidence Quality Dimensions (Credibility, Relevance, Integrity)**
  - Why needed here: The framework's core innovation is explicit evidence evaluation; each dimension uses different tools (blacklists, semantic similarity, structured extraction).
  - Quick check question: Why might a source score high on relevance but low on integrity, and how does the weighted combination handle this?

## Architecture Onboarding

- Component map:
  Claim Interpreter -> Evidence Retriever -> Content Extractor -> Evidence Evaluator -> Verifier

- Critical path: Evidence quality evaluation (Section 4.2.3) is the bottleneck. If this component fails, downstream verification degrades substantially (ablation shows accuracy drops from 90.2% to 64.1% with random evidence).

- Design tradeoffs:
  - Binary verdicts vs. fine-grained labels: Reduces ambiguity but loses nuance (Section 4.3)
  - Commercial vs. open-source LLMs: GPT-4o achieves 90.2% accuracy; Llama-3.2-Vision achieves 73.4% on open-world tasks (Table 5)
  - α parameter (relevance vs. integrity weight): Paper finds α=0.5 optimal (Appendix C.2), but this may not generalize across domains

- Failure signatures:
  - **Insufficient evidence (NEI)**: LLM outputs "not enough information" → indicates query formulation or search coverage failed
  - **Noisy evidence (NSY)**: Verdict flips relative to ground truth → indicates evidence evaluator passed weakly relevant or misleading sources
  - **Reasoning-justification mismatch**: Verdict contradicts generated justification → LLM reasoning instability, not evidence issue

- First 3 experiments:
  1. **Ablate evidence evaluation**: Replace structured evaluation with random sampling on a held-out subset of MMDV; confirm accuracy drop replicates (~26 points per Table 6).
  2. **Vary α parameter**: Sweep α ∈ {0.3, 0.4, 0.5, 0.6, 0.7} on domain-specific claims (e.g., political vs. health) to test if optimal balance shifts.
  3. **Stress test retrieval coverage**: Evaluate on claims where authoritative sources are paywalled or non-English; measure NEI rate to identify search engine coverage limitations.

## Open Questions the Paper Calls Out

- **How can the framework be extended to effectively detect disinformation in audio and video formats, given the challenges of temporal signals and synchronized multimodal reasoning?**
  - **Basis in paper:** [explicit] The "Limitations" section identifies a "critical blind spot" regarding audio and video content and suggests "directions for integrating audio and video LLMs into Aletheia."
  - **Why unresolved:** The current system only processes text and images; detecting disinformation in audio/video requires handling complex temporal and visual-temporal signals that current tools and the framework do not support.
  - **What evidence would resolve it:** A modified version of the framework that integrates video/audio LLMs and demonstrates high verification accuracy on a benchmark of video and audio disinformation datasets.

- **How can automated fact-checking systems mitigate verification failures when authoritative evidence is limited or delayed in public search engine results?**
  - **Basis in paper:** [explicit] The "Limitations" section notes that for newly emerging events, "relevant evidence may be limited or delayed in public search results," which constrains the framework's effectiveness.
  - **Why unresolved:** The framework relies on third-party search tools; if the public web has not yet indexed authoritative sources (e.g., institutional reports), the retrieval component returns insufficient evidence.
  - **What evidence would resolve it:** A retrieval strategy that successfully leverages alternative information channels or real-time indexing to verify claims where standard search engine coverage is sparse.

- **How can the evidence evaluation pipeline be optimized to reduce the significant performance gap between commercial and open-source LLM backbones?**
  - **Basis in paper:** [inferred] Tables 3 and 5 show a substantial accuracy drop (e.g., 90.2% to 73.4% on the MMDV dataset) when switching the backbone from GPT-4o to Llama-3.2-Vision.
  - **Why unresolved:** The framework's complex tasks (e.g., claim interpretation, evidence extraction) appear to demand the advanced reasoning capabilities of commercial models, which open-source alternatives currently struggle to match.
  - **What evidence would resolve it:** Techniques such as distillation or fine-tuning that enable open-source models to achieve performance parity with commercial models within the Aletheia pipeline.

## Limitations

- Limited to text and image inputs, unable to handle audio