---
ver: rpa2
title: 'QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory'
arxiv_id: '2512.05049'
source_url: https://arxiv.org/abs/2512.05049
tags:
- quantum
- lstm
- classical
- learning
- qkan-lstm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces QKAN-LSTM, a quantum-inspired long short-term
  memory architecture that replaces classical affine transformations with Data Re-Uploading
  Activation (DARUAN) modules based on single-qubit quantum circuits. These quantum
  variational activation functions (QVAFs) provide exponentially enriched spectral
  representations while remaining fully executable on classical hardware.
---

# QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory

## Quick Facts
- arXiv ID: 2512.05049
- Source URL: https://arxiv.org/abs/2512.05049
- Reference count: 40
- Primary result: 79% reduction in trainable parameters with superior predictive accuracy on time-series tasks

## Executive Summary
QKAN-LSTM introduces a quantum-inspired long short-term memory architecture that replaces classical affine transformations with Data Re-Uploading Activation (DARUAN) modules based on single-qubit quantum circuits. These Quantum Variational Activation Functions (QVAFs) provide exponentially enriched spectral representations while remaining fully executable on classical hardware. The model achieves superior performance on three datasets: damped simple harmonic motion, Bessel function regression, and urban telecommunication forecasting, while maintaining parameter efficiency across varying sequence lengths.

## Method Summary
The architecture substitutes standard LSTM weight matrix multiplication with summation of Quantum Variational Activation Functions (QVAFs). Instead of learning dense weight matrices, the model learns parameters of single-qubit rotation circuits acting on input dimensions individually, which are then aggregated. The model operates in "exact solver mode" using classical simulation of quantum circuits with PyTorch autograd for end-to-end differentiability. Training uses Adam optimizer with learning rates of 10⁻² for synthetic datasets and 10⁻³ for telecom data.

## Key Results
- Achieved testing losses as low as 3.21×10⁻⁴ on Bessel functions
- R² scores exceeding 0.986 on synthetic datasets
- 79% reduction in trainable parameters compared to classical LSTMs
- Maintained parameter efficiency across varying sequence lengths in real-world telecommunication data

## Why This Works (Mechanism)

### Mechanism 1
Replacing static affine transformations with additive quantum-inspired univariate functions reduces parameter redundancy while maintaining representational capacity. The architecture substitutes the standard LSTM weight matrix multiplication with a summation of QVAFs, learning parameters of single-qubit rotation circuits acting on input dimensions individually.

**Core assumption:** The Kolmogorov-Arnold representation theorem holds that complex high-dimensional functions can be decomposed into sums of simpler one-dimensional functions.

**Evidence anchors:** 79% reduction in trainable parameters, Eq. (2) showing QKAN layer mapping, context from KAN vs LSTM performance literature.

**Break condition:** If target function requires significant interaction terms between input dimensions that cannot be captured by additive summation of univariate QVAFs.

### Mechanism 2
Data re-uploading in single-qubit circuits generates exponentially enriched spectral representations, improving modeling of oscillatory dynamics. The QVAFs utilize Data Re-Uploading where input is repeatedly encoded into quantum state via parameterized rotation gates, theoretically allowing single-qubit system to access deeper Fourier frequencies.

**Core assumption:** The depth L of re-uploading blocks is sufficient to span frequency spectrum required by target data distribution.

**Evidence anchors:** Mentions "exponentially enriched spectral representation," Eq. (4) defining re-uploading circuit, superior performance on Bessel functions.

**Break condition:** If quantum circuit depth L is too shallow, spectral capacity may be insufficient for high-frequency signals.

### Mechanism 3
The hybrid quantum-classical optimization loop enables end-to-end differentiability on classical hardware. While activations are "quantum-inspired," implementation uses "exact solver" mode where quantum circuits are simulated as analytic, differentiable functions, allowing standard classical backpropagation to update variational angles and encoding weights simultaneously.

**Core assumption:** Classical simulation of quantum circuit remains computationally tractable and provides stable gradients.

**Evidence anchors:** Section III.A.3 states model operates in "exact solver mode," context from neural quantum states literature.

**Break condition:** If deployed on actual NISQ hardware, "exact solver" assumption fails, requiring parameter-shift rule and potentially introducing noise that destabilizes LSTM gating dynamics.

## Foundational Learning

**Concept: Kolmogorov-Arnold Networks (KAN)**
- **Why needed here:** This paper replaces core Linear layer of LSTM with KAN structure. Understanding that KANs learn univariate functions on edges rather than weights on nodes is critical to understanding parameter reduction.
- **Quick check question:** How does "edge-wise" function application in QKAN differ from standard Linear layer's matrix multiplication?

**Concept: Data Re-Uploading (DARUAN)**
- **Why needed here:** This is specific technique used to give KAN its "quantum" expressivity. Explains how single qubit can encode complex, non-linear data by repeatedly encoding input into rotation of quantum state.
- **Quick check question:** Why does repeating encoding circuit L times enrich spectral representation (Fourier spectrum) of activation function?

**Concept: LSTM Gating Mechanisms**
- **Why needed here:** QKAN is not standalone model; it is embedded inside LSTM gates. Must understand flow of h_t and c_t to know where QKAN modules plug in.
- **Quick check question:** In Eq. 7a, does QKAN module replace sigmoid activation σ or affine transformation inside it?

## Architecture Onboarding

**Component map:**
Input -> Concatenated vector v_t = [h_{t-1}; x_t] -> DARUAN layers (single-qubit simulations with parameterized rotations) -> Summation of QVAF outputs (KAN style) -> Standard LSTM Cell logic (Cell state C_t, Hidden state h_t) -> Prediction ŷ_t

**Critical path:**
1. Input x_t and previous hidden state h_{t-1} are concatenated
2. This vector passes through DARUAN layers (single-qubit simulations) which apply parameterized rotations
3. Results are aggregated via summation (KAN style)
4. Resulting values flow into standard LSTM sigmoid/tanh gates

**Design tradeoffs:**
- **Hidden Size vs. Spectral Depth:** Paper uses tiny hidden sizes (e.g., 1 or 2) for QKAN-LSTM compared to LSTM (e.g., 4 or 5). Assumption: expressivity comes from depth of quantum circuit L rather than width of network
- **Exact Solver vs. Real Hardware:** Current results rely on classical simulation (exact gradients). Moving to real hardware would require switching from autograd to parameter-shift rule

**Failure signatures:**
- **Slow Early Convergence:** QKAN-LSTM starts with higher loss than LSTM (Epoch 1: 0.219 vs 0.112) before eventually winning, suggesting quantum parameters require "warm-up" to align spectral phases
- **Underfitting on Simple Dynamics:** For simple tasks (Damped SHM), using hidden size of 1 was insufficient; had to increase to 2

**First 3 experiments:**
1. **Sanity Check (Damped SHM):** Replicate Damped Simple Harmonic Motion test. Verify model captures oscillatory decay better than baseline LSTM, but watch for "slow start" in first 5-10 epochs
2. **Spectral Stress Test (Bessel Function):** Run Bessel function regression. This stresses "exponentially enriched spectral representation" claim. If loss plateaus early, check re-uploading depth L
3. **Real-world Noise (Telecom):** Test on Milan Telecommunication dataset with varying sequence lengths (4 vs 64). Check if parameter reduction holds (should be ~50-70% fewer parameters) without MAE degradation

## Open Questions the Paper Calls Out
None explicitly identified in the paper.

## Limitations
- Performance on real quantum hardware remains untested; all results rely on classical simulation
- Parameter efficiency gains may not generalize to all sequence modeling tasks beyond the three tested datasets
- The relationship between re-uploading depth and generalization versus overfitting is not empirically validated

## Confidence
- **High confidence:** Parameter efficiency results (79% reduction), testing loss improvements on Bessel functions, R² scores >0.986 on synthetic datasets
- **Medium confidence:** Generalization claims to real-world telecom data
- **Low confidence:** Claims about quantum-inspired expressivity translating to actual quantum hardware performance

## Next Checks
1. **Hardware Validation:** Implement parameter-shift rule and evaluate QKAN-LSTM on actual quantum simulators with noise models to test gap between theoretical and practical quantum advantage
2. **Spectral Analysis:** Conduct Fourier analysis of QKAN-LSTM outputs versus classical LSTM to empirically verify "exponentially enriched spectral representation" claim on Bessel function dataset
3. **Architecture Robustness:** Test QKAN-LSTM on additional sequence tasks (e.g., Mackey-Glass, chaotic Lorenz system) with varying sequence lengths to validate generalization beyond three presented datasets