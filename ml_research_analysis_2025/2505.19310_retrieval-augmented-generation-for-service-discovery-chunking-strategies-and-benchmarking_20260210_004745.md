---
ver: rpa2
title: 'Retrieval-Augmented Generation for Service Discovery: Chunking Strategies
  and Benchmarking'
arxiv_id: '2505.19310'
source_url: https://arxiv.org/abs/2505.19310
tags:
- endpoint
- chunking
- endpoints
- discovery
- service
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of service discovery in automated
  service composition, where Large Language Models (LLMs) face input token limitations
  when processing comprehensive API documentation. The authors propose using Retrieval-Augmented
  Generation (RAG) with various chunking strategies to preprocess OpenAPI specifications,
  reducing token count while preserving relevant information.
---

# Retrieval-Augmented Generation for Service Discovery: Chunking Strategies and Benchmarking

## Quick Facts
- **arXiv ID:** 2505.19310
- **Source URL:** https://arxiv.org/abs/2505.19310
- **Reference count:** 40
- **Primary result:** Endpoint-based chunking strategies outperform naive methods for service discovery using RAG on OpenAPI specs

## Executive Summary
This paper addresses the challenge of service discovery in automated service composition where LLMs face token limitations when processing comprehensive API documentation. The authors propose using Retrieval-Augmented Generation (RAG) with various chunking strategies to preprocess OpenAPI specifications, reducing token count while preserving relevant information. They introduce SOCBench-D, a novel benchmark spanning multiple domains, and evaluate their approach using both SOCBench-D and the real-world RestBench benchmark. Results show that endpoint-based chunking strategies outperform naive methods, with the summary approach achieving the best performance.

## Method Summary
The authors propose a RAG-based approach for service discovery that preprocesses OpenAPI specifications using different chunking strategies to overcome LLM token limitations. They introduce four chunking methods: naive (splitting at character boundaries), section-based (splitting at major sections), endpoint-based (splitting at individual endpoints), and summary (condensing each endpoint). The system uses a Discovery Agent that retrieves endpoint details on demand to improve precision. They evaluate their approach using two benchmarks: SOCBench-D (a novel benchmark spanning multiple domains) and RestBench (a real-world benchmark), measuring recall and precision across different embedding models.

## Key Results
- Endpoint-based chunking strategies achieve 40-90% recall depending on domain and embedding model
- Summary approach outperforms other chunking strategies in overall performance
- Discovery Agent improves precision by retrieving endpoint details on demand
- The approach effectively reduces token count while maintaining service discovery capabilities

## Why This Works (Mechanism)
The mechanism works by preprocessing comprehensive API documentation into manageable chunks that can be processed by LLMs within token limits. By splitting OpenAPI specifications at logical boundaries (endpoints or sections), the system preserves semantic relationships while reducing overall token count. The RAG framework allows the LLM to retrieve relevant information on-demand rather than processing everything at once. The Discovery Agent further enhances this by selectively retrieving endpoint details when needed, improving precision without overwhelming the context window.

## Foundational Learning

### Retrieval-Augmented Generation (RAG)
- **Why needed:** Combines information retrieval with LLM generation to overcome context window limitations
- **Quick check:** Can retrieve relevant documents/documents chunks before generating responses

### OpenAPI Specification
- **Why needed:** Standardized format for describing RESTful APIs that enables systematic preprocessing
- **Quick check:** Supports automated chunking and extraction of service metadata

### Chunking Strategies
- **Why needed:** Different ways to split large documents into manageable pieces while preserving semantic coherence
- **Quick check:** Balance between token reduction and information preservation

### Service Discovery
- **Why needed:** Automated identification of relevant services/APIs for given tasks or queries
- **Quick check:** Matching user queries to appropriate API endpoints or services

## Architecture Onboarding

### Component Map
OpenAPI Spec -> Chunker -> Vector Store -> Retrieval Engine -> LLM -> Discovery Agent

### Critical Path
User Query -> Embedding -> Vector Store Search -> Chunk Retrieval -> Context Construction -> LLM Response

### Design Tradeoffs
- Chunk size vs. information loss: smaller chunks reduce tokens but may lose context
- Retrieval frequency vs. latency: more frequent retrieval improves accuracy but increases response time
- Embedding model choice affects retrieval quality and computational cost

### Failure Signatures
- Poor recall indicates ineffective chunking or embedding model mismatch
- Low precision suggests retrieval noise or insufficient context filtering
- Token overflow errors indicate inadequate preprocessing

### First Experiments
1. Compare recall/precision across different chunking strategies on a small API set
2. Test embedding model variations to identify optimal retrieval performance
3. Measure token reduction achieved by each chunking strategy

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation relies on two benchmarks that may not represent full API ecosystem diversity
- Performance metrics depend heavily on chosen embedding models and thresholds
- Focus on RESTful APIs limits generalizability to other API types
- Scalability to very large API collections not thoroughly examined

## Confidence
- **High confidence:** Chunking strategies effectively reduce token count while preserving service discovery capabilities
- **Medium confidence:** Endpoint-based strategies show consistent gains across benchmarks, but results depend on benchmark choice
- **Low confidence:** Generalizability to non-RESTful APIs and scalability to large collections

## Next Checks
1. Test the chunking strategies on a broader range of API documentation formats (e.g., GraphQL, gRPC) to assess generalizability.
2. Conduct ablation studies on embedding model choice and threshold tuning to quantify their impact on retrieval performance.
3. Evaluate the system's performance on streaming or rapidly evolving API specifications to measure robustness to documentation changes.