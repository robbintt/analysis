---
ver: rpa2
title: Cross-Lingual Transfer for Low-Resource Natural Language Processing
arxiv_id: '2502.02722'
source_url: https://arxiv.org/abs/2502.02722
tags:
- language
- data
- languages
- transfer
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Cross-Lingual Transfer for Low-Resource Natural Language Processing

## Quick Facts
- **arXiv ID**: 2502.02722
- **Source URL**: https://arxiv.org/abs/2502.02722
- **Authors**: Iker GarcÃ­a-Ferrero
- **Reference count**: 40
- **Primary result**: T-Projection outperforms traditional annotation projection methods by treating projection as a constrained generation and ranking problem.

## Executive Summary
This thesis addresses the challenge of cross-lingual transfer for sequence labeling tasks in low-resource languages. The work introduces three key mechanisms: T-Projection, which generates higher quality silver data through constrained generation and translation probability scoring; constrained decoding for text-to-text LLMs to prevent structural hallucinations and language mixing; and domain-continued pre-training to adapt general multilingual models to specialized domains like medicine. These approaches collectively improve zero-shot cross-lingual transfer performance, particularly for morphologically complex languages.

## Method Summary
The thesis proposes T-Projection, a novel annotation projection method that generates target-language candidate spans using a fine-tuned mT5-xl model, then selects the best candidates via translation probability scoring with M2M100 or NLLB200. It also introduces constrained decoding using Finite State Automata to restrict text-to-text LLM outputs to valid structures and exact input token copies. Additionally, the work presents Medical mT5, created by continuing pre-training mT5 on a 3B word multilingual medical corpus. These methods are evaluated across multiple sequence labeling tasks including Named Entity Recognition, Opinion Target Extraction, Argument Mining, and Question Answering in biomedical domains.

## Key Results
- T-Projection significantly outperforms traditional word alignment methods like AWESOME on sequence labeling tasks
- Constrained decoding enables text-to-text LLMs to match or exceed encoder-only models in zero-shot cross-lingual transfer
- Medical mT5 achieves state-of-the-art performance on biomedical benchmarks while maintaining competitive English performance
- The constrained decoding approach effectively prevents structural hallucinations and language mixing in morphologically complex languages

## Why This Works (Mechanism)

### Mechanism 1: Candidate Generation and Selection (T-Projection)
T-Projection treats annotation projection as a constrained generation and ranking problem. An mT5 model generates top-K candidate spans in the target language conditioned on source labels, then an MT model scores candidates based on translation probability to select the best fit. This approach is more robust than static word alignment because translation probability better captures semantic equivalence.

### Mechanism 2: Finite State Automata Constrained Decoding
Text-to-text LLMs can outperform encoder-only models on low-resource sequence labeling when output is constrained to prevent hallucinations and language mixing. A Finite State Automaton restricts vocabulary during beam search to valid tokens: HTML tags matching the taxonomy or exact input sentence tokens, preventing word invention or language switching.

### Mechanism 3: Domain-Continued Pre-training
Continuing pre-training of a general multilingual model on a moderate-sized domain-specific corpus (3B words) improves domain-specific task performance more effectively than increasing model size alone. Medical mT5, created by continuing mT5 pre-training on a multilingual medical corpus, outperforms similarly-sized text-to-text models on biomedical tasks.

## Foundational Learning

- **Concept: Annotation Projection (Data Transfer)**
  - Why needed: This is the baseline T-Projection improves upon
  - Quick check: Can you explain why word alignment fails on "many-to-one" translations compared to T-Projection?

- **Concept: Zero-Shot Cross-Lingual Transfer**
  - Why needed: This is the paradigm for Model Transfer
  - Quick check: Why does a "high-capacity" model generally outperform a "low-capacity" model in zero-shot settings?

- **Concept: Sequence Labeling Formulations**
  - Why needed: The thesis shifts from Token Classification to Text-to-Text generation with HTML tags
  - Quick check: How does the constrained decoding algorithm modify standard Text-to-Text beam search?

## Architecture Onboarding

- **Component map**: Source Text + Labels -> mT5-xl (Generator) -> M2M100/NLLB (Scorer) -> Silver Dataset
- **Critical path**: The Constrained Decoding Logic - a finite state machine that filters LLM logits at every generation step
- **Design tradeoffs**: Compute vs. Quality (T-Projection requires expensive models but yields better data) and Model Transfer vs. Data Transfer (faster but less reliable for low-capacity models)
- **Failure signatures**: Word Splitting (fixes: constrained decoding forces exact input token copying) and Language Mixing/Hallucination (fixes: constrained decoding restricts vocabulary)
- **First 3 experiments**:
  1. Intrinsic Evaluation: Reproduce T-Projection vs. AWESOME comparison on OTE/NER datasets
  2. Constrained vs. Unconstrained: Fine-tune mT0-xl on MasakhaNER and evaluate with/without constrained decoding
  3. Domain Ablation: Fine-tune standard mT5 vs. Medical-mT5 on new Medical benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
Can current machine translation systems be effectively adapted to handle long contexts and complex structures required for instruction-tuning data in low-resource languages? Current sentence-level models fail while document-level translation LLMs are only available for high-resource languages.

### Open Question 2
Can model-based cross-lingual transfer reliably generate high-quality synthetic instruction-tuning data for low-resource languages without hallucinations or quality degradation? This remains uncertain for languages with minimal target-language examples.

### Open Question 3
How can cross-lingual transfer methods be improved to effectively transfer or encode cultural knowledge about specific communities, rather than just linguistic competence? Current methods often rely on translating data from dominant languages that may not reflect cultural nuances.

### Open Question 4
What are the limitations of current evaluation metrics for generative tasks in specialized domains like medicine? Current metrics like ROUGE are inadequate for medical tasks where factuality and safety are critical.

## Limitations

- **Scaling and Compute Requirements**: T-Projection relies on very large models (3B mT5 + 12B NMT) making it computationally expensive
- **Generalizability Beyond Tested Domains**: Medical mT5's effectiveness for other specialized domains remains unproven
- **Robustness to Typological Diversity**: Constrained decoding effectiveness on languages with different typological features is not fully evaluated
- **Dependency on Parallel Data Quality**: T-Projection's effectiveness depends heavily on the quality of translated parallel corpora

## Confidence

- **High Confidence**: Constrained decoding mechanism's effectiveness in preventing structural hallucinations and language mixing
- **Medium Confidence**: T-Projection's superiority over traditional word alignment methods
- **Low Confidence**: Scalability claims for T-Projection with smaller models

## Next Checks

1. **Scaling Validation**: Reproduce T-Projection with progressively smaller mT5 variants (base, small) on OTE/NER benchmarks to establish minimum viable model size
2. **Domain Transfer Robustness**: Apply Medical mT5 approach to non-biomedical domains (legal, financial) with 3B word domain corpus to validate general applicability
3. **Typological Robustness Testing**: Evaluate constrained decoding on typologically diverse languages (tonal, polysynthetic) to identify failure modes and necessary adaptations