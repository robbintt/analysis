---
ver: rpa2
title: 'AI for Statutory Simplification: A Comprehensive State Legal Corpus and Labor
  Benchmark'
arxiv_id: '2508.19365'
source_url: https://arxiv.org/abs/2508.19365
tags:
- state
- retrieval
- legal
- https
- statutory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LaborBench, a benchmark dataset for evaluating
  AI capabilities in statutory simplification, specifically focusing on unemployment
  insurance laws across U.S. states.
---

# AI for Statutory Simplification: A Comprehensive State Legal Corpus and Labor Benchmark

## Quick Facts
- **arXiv ID:** 2508.19365
- **Source URL:** https://arxiv.org/abs/2508.19365
- **Reference count:** 40
- **Primary result:** Introduces LaborBench (3,700 Q&A pairs) and StateCodes (8.7 GB) to benchmark AI on unemployment insurance statutory simplification; RAG improves performance but F1 remains at 0.67.

## Executive Summary
This paper introduces LaborBench, a benchmark dataset for evaluating AI capabilities in statutory simplification, specifically focusing on unemployment insurance laws across U.S. states. The dataset is derived from a comprehensive annual report by the U.S. Department of Labor, containing 3,700 questions and answers about complex legal provisions. The authors also compile StateCodes, a large corpus of state statutes and regulations totaling 8.7 GB, to support retrieval-augmented generation (RAG) approaches. Experiments with five state-of-the-art models show that while RAG improves performance, the overall accuracy (F1 of 0.67) is far below the promises of AI for end-to-end regulatory simplification. The study highlights the need for further advances in legal AI to handle complex statutory language effectively.

## Method Summary
The authors construct StateCodes by scraping statutes and regulations from Justia for all 50 U.S. states, then filter for UI-relevant text using regular expressions. The corpus is chunked into 1,000-token windows with 200-token overlap, ensuring chunks never span multiple sections. For retrieval, they use a hybrid of dense (OpenAI, Gemini, E5) and sparse (Okapi BM25) embeddings to find top-5 chunks per query, mapping back to full sections for context. The inference pipeline uses a two-step LLM prompting approach: first generating reasoning and citations, then formatting outputs as JSON. The benchmark LaborBench consists of 3,700 boolean QA pairs derived from DOL comparison tables.

## Key Results
- RAG improves F1 from 0.49 to 0.67 on LaborBench.
- Retrieval recall reaches 0.806 (vs 0.515 without RAG), but answer accuracy lags at 0.77 even when correct citations are retrieved.
- State-specific performance varies widely (e.g., Virginia F1 0.52 vs Arkansas 0.81), indicating jurisdictional complexity affects model capability.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Converting expert-compiled legal tables into a question-answering format creates a high-fidelity ground truth for testing statutory interpretation.
- **Mechanism:** The authors leverage the "Comparison of State Unemployment Insurance Laws" (CSUIL), a resource requiring six months of lawyer effort, to generate 3,700 Q&A pairs. This process transforms implicit legal expertise into explicit evaluation tasks (LaborBench), allowing models to be tested on realistic legal differentiation rather than synthetic queries.
- **Core assumption:** The DOLâ€™s annual compilation accurately captures the relevant statutory distinctions and serves as a valid proxy for the broader task of "code simplification."
- **Evidence anchors:**
  - [abstract] Mentions the dataset updated annually by teams of lawyers at the U.S. Department of Labor.
  - [section 3.1.1] Describes the CSUIL as a 200-page compilation resulting from a six-month process.
  - [corpus] *Chinese Labor Law Large Language Model Benchmark* validates the trend of using domain-specific expert data to ground legal benchmarks.
- **Break condition:** If the DOL tables contain latent errors or fail to capture nuances required for simplification (beyond simple comparison), the benchmark validity collapses.

### Mechanism 2
- **Claim:** Retrieval-Augmented Generation (RAG) improves performance by restricting the model's reasoning scope to relevant jurisdictional text, though it fails to resolve semantic ambiguity.
- **Mechanism:** By compiling StateCodes (8.7 GB of statutes/regulations) and filtering for UI laws, the system retrieves specific context chunks (top-k) to feed the generator. This reduces false negatives by providing the necessary raw text that general pre-training likely missed or hallucinated.
- **Core assumption:** The correct statutory section is retrievable within the top-k results, and the model is capable of correctly interpreting the "byzantine" language once provided.
- **Evidence anchors:**
  - [abstract] Notes that RAG improves performance but overall F1 remains at 0.67.
  - [section 5.2.1] Table 6 shows RAG boosting recall from 0.515 to 0.806.
  - [corpus] *The Massive Legal Embedding Benchmark (MLEB)* suggests that retrieval quality is a primary driver of performance in legal RAG systems.
- **Break condition:** If statutes are semantically similar but legally distinct (e.g., "20 days" vs. "20 weeks"), retrieval alone cannot solve the reasoning gap, limiting F1 gains.

### Mechanism 3
- **Claim:** Chain-of-Thought (CoT) prompting with citation elicitation serves primarily as an interpretability layer rather than an accuracy booster.
- **Mechanism:** The paper employs a two-step inference where the model explains reasoning and extracts a citation before outputting a final JSON answer. While this forces the model to ground its output, the paper notes the performance gain is marginal; the value lies in verifying *where* the model thinks the answer resides.
- **Core assumption:** The model can faithfully map its internal reasoning to the provided text context without post-hoc rationalization of incorrect answers.
- **Evidence anchors:**
  - [section 5.2.2] States citation elicitation with CoT "appears to have little effect on performance" but provides valuable insight.
  - [section 5.2.3] Reveals that even when the correct citation is in the top-5, accuracy is only 0.77, indicating the generator struggles to read the retrieved text correctly.
  - [corpus] *Pat-DEVAL* references "Chain-of-Legal-Thought" for evaluation, supporting the utility of structured reasoning in legal domains.
- **Break condition:** If the model hallucinates citations or misattributes weight to irrelevant retrieved chunks, the reasoning trace becomes misleading.

## Foundational Learning

- **Concept:** **Statutory vs. Regulatory Hierarchy**
  - **Why needed here:** StateCodes combines both statutes (legislative) and regulations (administrative). You must distinguish these because UI laws often rely on a mix of enabling statutes and detailed administrative codes, which may conflict or duplicate.
  - **Quick check question:** Can you identify if a specific UI provision in the dataset likely originates from the legislative code or the administrative code?

- **Concept:** **RAG Metrics (Recall@k vs. F1)**
  - **Why needed here:** The paper highlights a gap between retrieval success (Recall) and answer correctness (F1). High recall means the text was found; low F1 means the model failed to use it.
  - **Quick check question:** If a system has a Recall@5 of 0.93 but an F1 of 0.67, where is the bottleneck: the retriever or the generator?

- **Concept:** **Cooperative Federalism (UI Domain)**
  - **Why needed here:** The benchmark relies on state variations of a federal model (FUTA). Understanding that states must comply with federal minimums but can diverge (e.g., definition of "employer") is key to understanding the "diff" questions in LaborBench.
  - **Quick check question:** Why would a state statute define "employer" differently than the federal FUTA, and how does that complexity challenge a "simplification" AI?

## Architecture Onboarding

- **Component map:** lawscraper (Justia) -> OCR/Cleaning -> StateCodes (8.7GB) -> Filter by UI keywords -> Chunking (1k tokens, 200 overlap) -> Embedding (E5/OpenAI/Gemini) -> MIPS retrieval -> Retrieve full parent sections -> System Prompt (Context + CoT) -> User Query -> JSON Output (Answer + Citation)

- **Critical path:**
  1.  **Corpus Construction:** Scraping and structuring StateCodes is the prerequisite.
  2.  **UI Filtering:** Regular expressions identify relevant UI sections to reduce noise.
  3.  **Chunking:** 1k token chunks with 200 overlap (sections never span chunks).
  4.  **Context Expansion:** Retrieved *chunks* are mapped back to full *sections* for the prompt to ensure complete context.

- **Design tradeoffs:**
  - **Chunking Strategy:** The authors use 1k chunks for retrieval but serve *full sections* to the generator. This trades off token efficiency (longer prompts) for context completeness (reducing fragmentation of legal logic).
  - **Boolean Simplification:** Experiments focus on boolean questions to make evaluation tractable, potentially sacrificing the ability to measure nuance in categorical/string answers.

- **Failure signatures:**
  - **Semantic Conflation:** Model confusing "days" vs. "weeks" in similar statutes (e.g., Colorado vs. FUTA).
  - **Citation Hallucination:** Model selecting a citation from the top-k list that looks relevant but doesn't actually support the boolean answer.
  - **State Capacity Gap:** Performance varies wildly by state (Virginia F1 0.52 vs Arkansas 0.81), suggesting "one-size-fits-all" models fail on specific jurisdictions.

- **First 3 experiments:**
  1.  **Retrieval Ablation:** Run the baseline (no RAG) vs. RAG on a single state to verify the reported 0.18 F1 lift.
  2.  **Citation Verification:** Manually inspect 20 "False" answers where the correct citation was retrieved to classify if the error is in legal reasoning or text extraction.
  3.  **Chunk Sensitivity:** Test smaller chunks (512 tokens) to see if retrieval precision improves for specific definition queries.

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark's validity depends on the assumption that the DOL's comparison tables fully capture the relevant statutory distinctions needed for simplification.
- Experiments focus only on boolean questions, potentially missing the complexity of categorical or string-based legal queries.
- The 8.7 GB corpus, while comprehensive, may still miss historical statutes or cross-referenced provisions that affect interpretation.

## Confidence
- **High Confidence:** The retrieval mechanism improves performance by providing relevant context (supported by recall improvements from 0.515 to 0.806).
- **Medium Confidence:** RAG and CoT together achieve F1 of 0.67, but the gap between retrieval success (0.93) and answer correctness suggests the generator remains the bottleneck.
- **Low Confidence:** The claim that this benchmark will drive progress in "end-to-end regulatory simplification" is premature given the modest performance gains and narrow boolean focus.

## Next Checks
1. **Statute Coverage Validation:** Manually verify 50 random QA pairs to ensure the DOL tables accurately represent the statutory text in StateCodes, checking for missing cross-references or historical provisions.
2. **Generator Capability Test:** Run the same RAG pipeline on a subset of non-boolean questions (e.g., categorical "what is the minimum wage?") to measure if performance degrades significantly.
3. **State-Specific Analysis:** Compare model performance on populous vs. smaller states to identify if the 0.67 F1 is driven by easy states (e.g., Arkansas 0.81) masking poor performance on complex jurisdictions.