---
ver: rpa2
title: 'QU-NLP at QIAS 2025 Shared Task: A Two-Phase LLM Fine-Tuning and Retrieval-Augmented
  Generation Approach for Islamic Inheritance Reasoning'
arxiv_id: '2508.15854'
source_url: https://arxiv.org/abs/2508.15854
tags:
- alefisolated
- laminitial
- wawisolated
- behinitial
- noonfinal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents QU-NLP, a system for SubTask 1: Islamic Inheritance
  Reasoning at QIAS 2025, addressing the complex challenge of reasoning within Islamic
  inheritance law. The core method combines a LoRA fine-tuned Fanar-1-9B causal language
  model with a Retrieval-Augmented Generation (RAG) pipeline to enhance reasoning
  accuracy and domain-specific knowledge grounding.'
---

# QU-NLP at QIAS 2025 Shared Task: A Two-Phase LLM Fine-Tuning and Retrieval-Augmented Generation Approach for Islamic Inheritance Reasoning

## Quick Facts
- arXiv ID: 2508.15854
- Source URL: https://arxiv.org/abs/2508.15854
- Reference count: 15
- Achieved 85.8% overall accuracy on QIAS 2025 SubTask 1 test set

## Executive Summary
This paper presents QU-NLP, a system for SubTask 1: Islamic Inheritance Reasoning at QIAS 2025, addressing the complex challenge of reasoning within Islamic inheritance law. The core method combines a LoRA fine-tuned Fanar-1-9B causal language model with a Retrieval-Augmented Generation (RAG) pipeline to enhance reasoning accuracy and domain-specific knowledge grounding. The system achieved an overall accuracy of 0.858 on the final test set, outperforming zero-shot baselines such as GPT-4.5, LLaMA 3, Fanar, Mistral, and ALLaM. Notably, QU-NLP excelled on advanced reasoning questions with an accuracy of 97.6%, surpassing Gemini 2.5 and OpenAI's o3, demonstrating that domain-specific fine-tuning combined with retrieval grounding enables mid-scale Arabic LLMs to surpass frontier models in Islamic inheritance reasoning.

## Method Summary
The QU-NLP system employs a two-phase approach: first, LoRA fine-tuning of the Fanar-1-9B Arabic causal language model on 20K MCQs from the QIAS 2025 dataset; second, RAG-enhanced inference using a FAISS-indexed knowledge corpus and all-MiniLM-L6-v2 retriever. The fine-tuning uses 4-bit NF4 quantization, gradient checkpointing, and targets specific projection and MLP modules with LoRA adapters (r=32, α=64). During inference, top-5 passages are retrieved and integrated into an Arabic chat prompt, with greedy decoding at temperature 0.05 to produce answers A–F.

## Key Results
- Achieved 85.8% overall accuracy on the final test set
- Scored 74.0% accuracy on Beginner-level questions and 97.6% on Advanced-level questions
- Outperformed zero-shot baselines including GPT-4.5, LLaMA 3, Fanar, Mistral, and ALLaM
- Surpassed frontier models Gemini 2.5 and OpenAI o3 on advanced reasoning questions (97.6% vs lower scores)

## Why This Works (Mechanism)
The system's success stems from combining domain-specific fine-tuning with retrieval-augmented generation. The LoRA fine-tuned Fanar-1-9B model learns the complex hierarchical rules of Islamic inheritance (Ilm al-Mawārīth) through exposure to 20K MCQs, while the RAG component grounds reasoning in authoritative jurisprudential texts. This dual approach addresses both the memorization of inheritance formulas and the application of nuanced exceptions and blocking rules (hijb), enabling the system to handle both straightforward calculations and complex edge cases involving negation, exceptions, and near-duplicate answer options.

## Foundational Learning
- **LoRA Fine-Tuning**: Lightweight adapter-based parameter-efficient fine-tuning; needed to adapt large models to domain-specific tasks without full fine-tuning overhead; quick check: verify adapter modules are correctly attached to projection/MLP layers
- **RAG Pipeline**: Retrieval-augmented generation using FAISS and sentence transformers; needed to ground LLM reasoning in authoritative knowledge sources; quick check: confirm retrieval top-k=5 returns relevant passages
- **Islamic Inheritance Hierarchies**: Complex family relationship rules and blocking (hijb) mechanisms; needed to understand why certain heirs prevent others from inheriting; quick check: verify model correctly handles blocked heir scenarios
- **Arabic Orthography Normalization**: Handling diacritic and letter-form variations in MCQs; needed to prevent spurious mismatches in near-duplicate options; quick check: ensure preprocessing normalizes يقبا vs ؤقبا variants
- **Negation and Exception Handling**: Processing Arabic negation cues (لانيخ/مابطلا) and conditional exceptions; needed for accurate reasoning on edge cases; quick check: test accuracy on items containing explicit negation
- **Quantization and Memory Efficiency**: 4-bit NF4 quantization with gradient checkpointing; needed to fit 9B parameter model on practical hardware; quick check: confirm model loads and trains without OOM errors

## Architecture Onboarding
**Component Map**: Dataset -> LoRA Fine-tuning (Fanar-1-9B) -> RAG Knowledge Corpus (FAISS) -> Inference Pipeline -> MCQ Classification

**Critical Path**: QIAS dataset → LoRA fine-tuning → RAG indexing → inference with retrieval → answer extraction

**Design Tradeoffs**: The system trades model size (9B parameters) for accuracy, using quantization and LoRA to maintain efficiency while achieving superior performance to larger frontier models on this specialized task.

**Failure Signatures**: 
- Poor performance on blocked-heir cases (محجوب) where model assigns shares despite higher-priority heirs
- Polarity flips on negation/exception cues (e.g., "لانيخ/مابطلا")
- Near-duplicate option mismatches due to Arabic orthography variations

**First Experiments**:
1. Test LoRA fine-tuning on a small subset of the QIAS dataset to verify adapter attachment and training stability
2. Validate RAG retrieval quality by checking top-5 passages for relevance to sample questions
3. Verify answer extraction accuracy by running inference on validation set and comparing against gold labels

## Open Questions the Paper Calls Out
None

## Limitations
- Exact composition and source of the RAG knowledge corpus remains unspecified
- Specific prompt template format for integrating retrieved passages is not detailed
- Dataset access pathway for QIAS 2025 SubTask 1 data is not provided
- Generalization beyond the QIAS dataset has not been evaluated

## Confidence
- Confidence in core methodology and training approach: High
- Confidence in RAG pipeline's contribution: Medium
- Confidence in system's generalization: Low

## Next Checks
1. Obtain and publish the complete RAG knowledge corpus composition, including specific Islamic jurisprudential sources indexed in FAISS
2. Release or specify the exact prompt template format used for system-user-assistant turns and retrieved passage integration
3. Conduct ablation studies removing the RAG component to quantify its contribution, and test on at least one external Islamic inheritance reasoning dataset to assess generalization