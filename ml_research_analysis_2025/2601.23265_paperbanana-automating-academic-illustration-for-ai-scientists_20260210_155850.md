---
ver: rpa2
title: 'PaperBanana: Automating Academic Illustration for AI Scientists'
arxiv_id: '2601.23265'
source_url: https://arxiv.org/abs/2601.23265
tags:
- diagram
- academic
- visual
- data
- plot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PaperBanana automates academic illustration generation for AI scientists
  by orchestrating specialized agents (Retriever, Planner, Stylist, Visualizer, Critic)
  to produce publication-ready methodology diagrams and statistical plots. The system
  retrieves relevant references, synthesizes detailed visual descriptions, applies
  academic-style guidelines, and iteratively refines outputs through self-critique.
---

# PaperBanana: Automating Academic Illustration for AI Scientists

## Quick Facts
- **arXiv ID:** 2601.23265
- **Source URL:** https://arxiv.org/abs/2601.23265
- **Reference count:** 40
- **Primary result:** PaperBanana improves overall academic illustration quality by 17.0% across faithfulness, conciseness, readability, and aesthetics dimensions

## Executive Summary
PaperBanana is an automated system for generating publication-ready academic illustrations, specifically methodology diagrams and statistical plots. The system orchestrates specialized agents including a Retriever that finds relevant reference examples, a Planner that synthesizes visual descriptions, a Stylist that applies aesthetic guidelines, a Visualizer that generates images, and a Critic that iteratively refines outputs. Using a new benchmark dataset of 292 methodology diagrams from NeurIPS 2025 papers, PaperBanana consistently outperforms baselines across four evaluation dimensions with particular improvements in conciseness (+37.2%) and readability (+12.9%).

## Method Summary
PaperBanana processes input methodology text and captions through a five-agent pipeline: Retriever selects top 10 relevant examples using VLM-based matching; Planner generates detailed visual descriptions using retrieved examples; Stylist refines descriptions with auto-synthesized aesthetic guidelines from the reference set; Visualizer generates images (or code for statistical plots); Critic iteratively examines outputs against source context for 3 refinement rounds. The system uses Gemini-3-Pro for agents and Nano-Banana-Pro for image generation, though these can be substituted with equivalent models. Evaluation uses VLM-as-a-Judge protocol comparing generated outputs against human references on faithfulness, conciseness, readability, and aesthetics dimensions.

## Key Results
- **Overall performance:** 17.0% improvement in overall score compared to baselines
- **Individual dimensions:** Faithfulness (+2.8%), Conciseness (+37.2%), Readability (+12.9%), Aesthetics (+6.6%)
- **Ablation insights:** Each component contributes significantly; Retriever improves conciseness/readability, Stylist boosts aesthetics, and Critic recovers faithfulness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieving stylistically similar reference examples improves both structural coherence and aesthetic alignment of generated diagrams.
- Mechanism: The Retriever Agent uses a VLM to rank candidates by matching research domain AND diagram type, with visual structure prioritized over topic similarity. Retrieved examples provide concrete patterns for the Planner to learn appropriate logical compositions and visual layouts.
- Core assumption: Reference examples from similar diagram types generalize to new contexts; stylistic patterns are transferable across topics.
- Evidence anchors:
  - [abstract]: "orchestrates specialized agents to retrieve references, plan content and style, render images, and iteratively refine via self-critique"
  - [section]: "Without reference examples as guidance, the no-retriever setting significantly underperforms in Conciseness, Readability, and Aesthetics, as the Planner defaults to verbose, exhaustive descriptions."
  - [corpus]: Related work (AutoFigure, SciFig, SridBench) addresses scientific figure generation but PaperBananaBench is specifically curated from NeurIPS 2025, providing domain-specific references.
- Break condition: If reference set lacks diversity or contains low-quality examples, retrieval may propagate poor patterns.

### Mechanism 2
- Claim: Automatic style guide synthesis from reference collections enables consistent adherence to publication aesthetics without manual rule authoring.
- Mechanism: The Stylist Agent traverses the entire reference collection R to synthesize an Aesthetic Guideline G covering color palette, shapes, lines, layout, and typography. This is then applied to refine the Planner's output description before visualization.
- Core assumption: Aggregating visual patterns across many examples produces a coherent style guide; VLMs can reliably extract and synthesize design patterns from images.
- Evidence anchors:
  - [section]: "The Stylist Agent traverses the entire reference collection R to automatically synthesize an Aesthetic Guideline G covering key dimensions such as color palette, shapes and containers, lines and arrows, layout and composition, and typography and icons"
  - [section]: "Comparing rows ③ and ④ shows that the Stylist boosts Conciseness (+17.5%) and Aesthetics (+4.7%)"
  - [corpus]: No direct corpus comparison for this specific auto-summarization technique; related work does not emphasize automatic style extraction.
- Break condition: If reference collection has inconsistent styles or domain-specific conventions conflict, the synthesized guideline may be incoherent.

### Mechanism 3
- Claim: Iterative Visualizer-Critic refinement closes the faithfulness gap that style-focused modifications can introduce.
- Mechanism: The Critic inspects generated images against source context (S, C) to identify misalignments, then produces refined descriptions. This loop runs for T=3 rounds, recovering technical accuracy while maintaining aesthetic improvements.
- Core assumption: VLMs can reliably detect factual misalignments in generated diagrams; iterative correction converges rather than oscillating.
- Evidence anchors:
  - [section]: "However, the Critic Agent (row ① vs. ③) effectively bridges this gap, substantially recovering Faithfulness"
  - [section]: "T=3 rounds, with the final output being I=I_T"
  - [corpus]: Related agentic frameworks (CODA, PlotGen, MatplotAgent) use self-reflection but primarily for code generation, not visual diagram refinement.
- Break condition: If Critic fails to detect fine-grained connectivity errors (noted as primary failure mode in Figure 10), iterations provide limited benefit.

## Foundational Learning

- **Vision-Language Models (VLMs) for multi-modal reasoning**
  - Why needed here: All agents (Retriever, Planner, Stylist, Critic) are VLM-based; understanding how VLMs process images alongside text is essential for debugging agent outputs and prompt engineering.
  - Quick check question: Can you explain why a VLM might struggle to detect subtle connection errors in a diagram compared to obvious visual artifacts?

- **In-context learning with retrieved examples**
  - Why needed here: The Planner uses retrieved reference triplets (S_i, C_i, I_i) as demonstrations to learn structural and stylistic patterns. Understanding few-shot learning dynamics helps diagnose retrieval quality issues.
  - Quick check question: What happens to output quality if retrieved examples match the topic but have different diagram types?

- **Hierarchical evaluation with veto rules**
  - Why needed here: The benchmark uses faithfulness/readability as primary dimensions with veto rules, and conciseness/aesthetics as secondary. Understanding this prevents misinterpreting overall scores.
  - Quick check question: If a diagram has perfect aesthetics but violates a faithfulness veto rule, what is its overall score?

## Architecture Onboarding

- **Component map:**
  Retriever Agent -> Planner Agent -> Stylist Agent -> Visualizer Agent -> Critic Agent (loop T=3 times)

- **Critical path:**
  1. Input (S, C) → Retriever → Examples E
  2. (S, C, E) → Planner → Description P
  3. (P, G) → Stylist → Optimized Description P*
  4. P* → Visualizer → Image I_0
  5. I_t + (S, C) → Critic → P_{t+1} → Visualizer → I_{t+1} (repeat T=3 times)
  6. Final output: I_T

- **Design tradeoffs:**
  - **Style standardization vs. diversity**: Unified style guide ensures academic compliance but reduces stylistic variety (explicitly noted as limitation)
  - **Raster output vs. editability**: Current output is raster; vector graphics would require different approach (reconstruction pipeline or GUI agent)
  - **Code vs. image generation for plots**: Code provides numerical fidelity; image generation offers better aesthetics for sparse data but hallucinates dense data

- **Failure signatures:**
  - **Connection errors**: Mismatched source-target nodes, redundant connections, incorrect arrow directions (primary failure mode from Figure 10)
  - **Numerical hallucination**: Image generation for statistical plots produces incorrect values for dense data
  - **Style-content tension**: Stylist boosts aesthetics but can reduce faithfulness; requires Critic to recover

- **First 3 experiments:**
  1. **Ablate retrieval strategy**: Compare semantic retrieval vs. random retrieval vs. no retrieval on Conciseness, Readability, and Aesthetics metrics to validate reference importance.
  2. **Vary iteration count**: Test T=1, T=2, T=3, T=5 refinement rounds to find point of diminishing returns for Faithfulness recovery.
  3. **Cross-domain generalization**: Train Retriever on one diagram category (e.g., Agent & Reasoning), test on another (e.g., Vision & Perception) to assess style guide robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can automated academic illustration systems be extended to produce fully editable vector graphics rather than raster outputs?
- Basis in paper: [explicit] The authors state: "The most prominent limitation of PaperBanana lies in the raster nature of its output... unlike vector graphics—which are preferred in academic contexts for their infinite scalability and precise detail preservation—raster images are inherently difficult to edit."
- Why unresolved: Current image generation models output raster images; generating editable vectors requires either reconstruction pipelines or GUI agents operating professional software, both of which face robustness challenges with complex visual elements.
- What evidence would resolve it: Development and evaluation of a vector-output variant on PaperBananaBench, measuring editability and preservation of detail.

### Open Question 2
- Question: How can the trade-off between style standardization (academic norms) and stylistic diversity be optimally balanced?
- Basis in paper: [explicit] The authors note: "While our unified style guide ensures rigid compliance with academic standards, it inevitably reduces the stylistic diversity of the output."
- Why unresolved: The current Stylist Agent applies a single synthesized aesthetic guideline; dynamic adaptation mechanisms that preserve personalization while maintaining professionalism are unexplored.
- What evidence would resolve it: A user study comparing satisfaction with outputs from adaptive vs. fixed style systems across diverse researcher preferences.

### Open Question 3
- Question: Can hybrid approaches combining code-based visualization (for dense/complex data) and image generation (for sparse visualizations) achieve optimal balance for statistical plots?
- Basis in paper: [explicit] From Section 6.2: "Image generation excels in presentation... but underperforms in content fidelity... hybridly using image generation for sparse visualizations and code for dense plots may offer the best balance."
- Why unresolved: The trade-off is observed but not systematically evaluated; criteria for automatically selecting the appropriate modality are undefined.
- What evidence would resolve it: Experiments on PaperBananaBench statistical plots comparing hybrid, code-only, and image-only approaches across data density levels.

### Open Question 4
- Question: How can fine-grained faithfulness (e.g., accurate connectivity, arrow directions) be improved when current VLM critics fail to detect such errors?
- Basis in paper: [explicit] The authors identify that "the most prevalent errors involve fine-grained connectivity, such as misaligned start/end points or incorrect arrow directions. These subtleties often escape the detection of current critic models."
- Why unresolved: The iterative refinement loop depends on the Critic Agent's perception; fine-grained structural errors require capabilities beyond current foundation VLMs.
- What evidence would resolve it: Comparison of diagram faithfulness using enhanced critics (e.g., structure-based verification or specialized connectivity models) on error-prone cases from the failure analysis.

## Limitations
- **Raster output format**: Prevents post-hoc editing and limits practical utility for iterative research adjustments
- **Subtle connectivity errors**: Current VLMs struggle to detect fine-grained visual alignment verification failures
- **Statistical plot hallucination**: Image synthesis can produce incorrect values in dense datasets, requiring code-based generation for accuracy

## Confidence
- **High Confidence**: The 17.0% overall improvement and ablation results are well-supported by experimental data
- **Medium Confidence**: Stylistic pattern transferability across topics assumes visual composition principles generalize, which may not hold for specialized domains
- **Medium Confidence**: Automatic style guide synthesis effectiveness depends on VLM capabilities not directly benchmarked against manual extraction

## Next Checks
1. **Cross-Domain Generalization Test**: Evaluate the system on methodology diagrams from conferences outside NeurIPS (ICML, CVPR, etc.) to assess whether the NeurIPS-trained style guide transfers effectively to other venues' visual conventions.

2. **Fine-Grained Error Detection Benchmark**: Create a targeted test set with deliberately introduced subtle connectivity errors to measure whether increasing VLM model size or using specialized visual reasoning models improves Critic detection rates.

3. **Vector Output Feasibility Study**: Implement a reconstruction pipeline that converts raster outputs to SVG format, then evaluate whether this approach maintains aesthetic quality while enabling post-hoc editing, comparing user satisfaction scores against pure raster output.