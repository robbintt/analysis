---
ver: rpa2
title: 'InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient,
  Unlimited-Input Vision-Language Models'
arxiv_id: '2512.08829'
source_url: https://arxiv.org/abs/2512.08829
tags:
- infinitevl
- memory
- attention
- arxiv
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InfiniteVL addresses the limitations of existing vision-language
  models by combining sliding window attention for local context with Gated DeltaNet
  for efficient long-term memory, creating a linear-complexity architecture. It achieves
  this through a three-stage training strategy involving distillation pretraining,
  instruction fine-tuning, and long-sequence fine-tuning.
---

# InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models

## Quick Facts
- **arXiv ID**: 2512.08829
- **Source URL**: https://arxiv.org/abs/2512.08829
- **Reference count**: 40
- **Primary result**: Achieves 3.6× inference speedup and constant memory usage while matching leading VLMs on benchmarks using <2% of training data

## Executive Summary
InfiniteVL addresses the limitations of existing vision-language models by combining sliding window attention for local context with Gated DeltaNet for efficient long-term memory, creating a linear-complexity architecture. It achieves this through a three-stage training strategy involving distillation pretraining, instruction fine-tuning, and long-sequence fine-tuning. The model matches leading Transformer-based VLMs' performance on multimodal benchmarks while delivering over 3.6× inference speedup and maintaining constant memory usage. In streaming video scenarios, InfiniteVL sustains real-time 24 FPS processing with effective long-term memory retention, all while using less than 2% of the training data required by comparable models.

## Method Summary
InfiniteVL is a vision-language model that hybridizes sliding window attention (SWA) and Gated DeltaNet to achieve linear complexity with unlimited input length. The architecture replaces Qwen2.5-VL's attention layers with 9 hybrid blocks (25% SWA, 75% Gated DeltaNet), trained through a three-stage process: distillation pretraining from a Transformer teacher, instruction fine-tuning, and long-sequence fine-tuning. This approach enables constant memory usage and 3.6× faster inference while maintaining competitive performance on multimodal benchmarks.

## Key Results
- Matches leading Transformer-based VLMs on multimodal benchmarks while using <2% of training data
- Achieves 3.6× inference speedup and constant memory usage regardless of input length
- Sustains 24 FPS real-time processing in streaming video scenarios with effective long-term memory retention

## Why This Works (Mechanism)

### Mechanism 1
The hybridization of Sliding Window Attention (SWA) and Gated DeltaNet allows the model to process fine-grained local details (OCR) and global long-term context (video) simultaneously, mitigating the limitations of using either in isolation. SWA layers (25% of total) restrict attention to a fixed local window (e.g., 8192 tokens) using RoPE, preserving high-frequency local feature alignment. Gated DeltaNet layers (75%) maintain a constant-size recurrent memory state $S_t$ updated via a delta rule with Householder rotations, compressing global history into a fixed matrix. Visual-language tasks decouple into local alignment tasks (requiring precise positional indexing) and global reasoning tasks (tolerating compressed state representations).

### Mechanism 2
Gated DeltaNet enables constant-inference-complexity long-term memory by preventing the "low-rank collapse" common in standard linear attention. It replaces the standard cumulative sum in linear attention with a gated update rule ($S_t = S_{t-1}(\dots) + \dots$) that includes a Householder-like rotation ($I - \beta k k^\top$). This actively rotates the memory state to "make room" for new key-value pairs, reducing interference and information entanglement over long sequences. The delta rule provides sufficient capacity to disentangle historical visual features better than scalar-gated linear attention (e.g., Mamba/GLA).

### Mechanism 3
A three-stage training strategy, specifically distillation from a Transformer teacher, is required to align the linear student's latent space with strong pre-trained representations using minimal data. Instead of pre-training from scratch, InfiniteVL replaces Qwen2.5-VL's attention layers with Gated DeltaNet. It uses "same-input" layer-wise MSE distillation to align intermediate features, followed by end-to-end KL divergence on logits. This transfers "teacher reasoning" into the "student architecture" before SFT. The linear attention layers can approximate the input-output mapping of softmax attention layers without requiring web-scale data discovery.

## Foundational Learning

- **Concept: Sliding Window Attention (SWA)**
  - **Why needed here:** Essential for understanding how InfiniteVL maintains high performance on OCR and local grounding. Without SWA, linear attention struggles with fine-grained text extraction.
  - **Quick check question:** If you process a 100-page document, can a pure SWA model recall information from page 1 when processing page 100? (Answer: No, unless combined with a global memory mechanism like Gated DeltaNet).

- **Concept: Linear Attention & State Compression**
  - **Why needed here:** Explains the core efficiency gain. Standard Transformer attention grows $O(L^2)$; linear attention creates a fixed-size "memory state" (like a summary) that grows $O(1)$ in memory.
  - **Quick check question:** Why does standard linear attention often fail at "needle in a haystack" retrieval? (Answer: Over-compression/interference of history in the fixed state).

- **Concept: Knowledge Distillation**
  - **Why needed here:** Explains how the model achieves competitive performance with <2% data. It leverages an existing "smart" teacher to guide the "efficient" student.
  - **Quick check question:** Why use MSE on intermediate layers rather than just matching final outputs? (Answer: To force the student to learn similar internal feature representations, not just mimic outputs).

## Architecture Onboarding

- **Component map:** Input: Qwen2.5-VL Vision Encoder → Projection MLP → LLM Backbone (9 Hybrid Blocks) → Output
- **Critical path:** The recurrent state update in Gated DeltaNet is the bottleneck. Ensure custom CUDA kernels (or optimized Torch.compile) are used for the `state = state @ rotation + update` step to actually hit the 24 FPS claim. Stage 1 distillation requires strict "same-input" feeding to both Teacher and Student; misaligned batches will break the layer-wise MSE loss.
- **Design tradeoffs:** Ratio 1:4 (SWA : Gated DeltaNet) trades a small amount of maximum theoretical context retention for significantly better OCR/short-context performance. Constant Memory gains unlimited input length capability at the cost of "lossy" compression (unlike exact KV cache retrieval in Transformers).
- **Failure signatures:** "NaN" losses during distillation if learning rates are too high for the linear layers. FPS dropping over time suggests a memory leak (accidental accumulation of KV cache in SWA layers rather than discarding old tokens) or state explosion in DeltaNet. High performance on general QA but failure on DocVQA/ChartQA indicates the SWA window is too small or SWA ratio is too low.
- **First 3 experiments:**
  1. Length Generalization Test: Run Video-MME or LongVideoBench, scaling frames from 8 to 1024. Plot performance vs. tokens. Expect Transformer baselines to crash (OOM) or drop, while InfiniteVL remains stable.
  2. Distillation Ablation: Train a variant skipping Stage 1 (direct SFT from random init or base weights) to prove the distillation is necessary for sample efficiency.
  3. Throughput Benchmark: Measure per-token latency and GPU memory allocation (VRAM) at 4k, 32k, and 100k context lengths. Verify "flat" memory curve.

## Open Questions the Paper Calls Out
None

## Limitations
- Distribution shift in long-context tasks may cause gradual performance degradation beyond 64k tokens
- Data efficiency verification requires scrutiny of dataset composition and absolute vs relative training data claims
- Memory state saturation risk exists for extreme sequence lengths (>500k tokens) without periodic reset mechanisms
- Hardware specificity may affect the 24 FPS streaming claim depending on CUDA kernel optimization

## Confidence
- **Hybrid Architecture Effectiveness**: High - Well-supported by architectural description and ablation studies
- **Training Efficiency**: Medium - Three-stage training strategy is sound but data efficiency claims need validation
- **Streaming Performance**: Medium - 24 FPS claim is specific but lacks detailed hardware specifications
- **Memory Complexity**: High - O(1) memory claim follows directly from fixed-size state design

## Next Checks
1. **State Saturation Benchmark**: Run InfiniteVL on sequences exceeding 500k tokens and measure retrieval accuracy degradation over time. Plot state "freshness" metrics to identify saturation points.
2. **Hardware-Independent Throughput Test**: Implement and benchmark both naive PyTorch and optimized CUDA kernels for the Gated DeltaNet state update operation. Measure FPS across different GPU generations.
3. **Dataset Efficiency Ablation**: Train two variants - one using the claimed <2% dataset with three-stage training, and another using 100% of a standard large-scale dataset but skipping distillation. Compare final performance.