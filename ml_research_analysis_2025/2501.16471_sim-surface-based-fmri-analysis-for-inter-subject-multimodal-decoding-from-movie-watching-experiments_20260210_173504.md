---
ver: rpa2
title: 'SIM: Surface-based fMRI Analysis for Inter-Subject Multimodal Decoding from
  Movie-Watching Experiments'
arxiv_id: '2501.16471'
source_url: https://arxiv.org/abs/2501.16471
tags:
- fmri
- movie
- training
- figure
- brain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces SIM, a surface-based fMRI analysis framework
  that enables inter-subject multimodal decoding from movie-watching experiments.
  It leverages surface vision transformers to encode cortical functional dynamics
  as a moving image across the cortical surface, capturing spatial autocorrelation
  of brain activity.
---

# SIM: Surface-based fMRI Analysis for Inter-Subject Multimodal Decoding from Movie-Watching Experiments

## Quick Facts
- **arXiv ID:** 2501.16471
- **Source URL:** https://arxiv.org/abs/2501.16471
- **Reference count:** 40
- **Primary result:** Introduces SIM, a surface-based fMRI analysis framework that enables inter-subject multimodal decoding from movie-watching experiments, achieving high retrieval accuracy (76.8% top-1, 94.2% top-10 for fMRI→video decoding).

## Executive Summary
The study introduces SIM, a surface-based fMRI analysis framework that enables inter-subject multimodal decoding from movie-watching experiments. It leverages surface vision transformers to encode cortical functional dynamics as a moving image across the cortical surface, capturing spatial autocorrelation of brain activity. This is combined with tri-modal self-supervised contrastive learning (CLIP) alignment of audio, video, and fMRI modalities. The framework achieves high retrieval accuracy and generalizes to new subjects and movie clips not seen during training. Visualization of attention maps reveals the model captures semantic and visual brain systems. The method enables personalized simulations of brain function and opens doors for brain-computer interfaces and neurofeedback.

## Method Summary
The method processes 7T fMRI data by mapping it onto a spherical cortical surface (I6 icosahedral mesh) and patching it into triangular tokens for a Surface Vision Transformer (SiT). The SiT is pre-trained as a video surface Masked Autoencoder (vsMAE) to reconstruct masked spatio-temporal tubes of brain activity. The pre-trained encoder is then fine-tuned with a tri-modal CLIP objective that aligns fMRI embeddings with corresponding video and audio embeddings. This alignment enables retrieval of video clips from fMRI patterns and vice versa, with strong inter-subject generalization.

## Key Results
- Achieves 76.8% top-1 and 94.2% top-10 retrieval accuracy for fMRI→video decoding.
- Model generalizes to new subjects and movie clips not seen during training.
- Visualization of attention maps reveals the model captures semantic and visual brain systems.
- Ablation studies show pre-training and tri-modal alignment are critical for performance.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mapping fMRI data onto a spherical cortical surface may improve inter-subject generalization compared to volumetric approaches.
- **Mechanism:** By projecting 3D fMRI volumes onto a 2D tessellated icosahedral mesh (I6) and patching it into triangular tokens, the model preserves the topological neighborhood structure of the cortex. This allows the Surface Vision Transformer (SiT) to model spatial autocorrelation and long-range interactions on the manifold rather than in Euclidean space.
- **Core assumption:** Inter-subject alignment (via MSMAll) sufficiently normalizes anatomical and functional variability so that shared activation patterns appear in corresponding surface locations.
- **Evidence anchors:**
  - [abstract] "leverages surface vision transformers... capturing spatial autocorrelation of brain activity."
  - [Page 2] "surface modelling more faithfully represents the true geometry of the convoluted cortical surface."
  - [corpus] Related work (arXiv:2507.16389) suggests surface-based methods redefine decoding by respecting cortex structure.
- **Break condition:** If the cortical surface reconstruction or registration (MSMAll) fails, the topological correspondence required for patching and attention mechanisms is lost, degrading cross-subject performance.

### Mechanism 2
- **Claim:** Self-supervised pre-training via masked autoencoding (vsMAE) creates a robust initialization for encoding spatio-temporal brain dynamics.
- **Mechanism:** The model masks ~50% of spatio-temporal "tubes" (patches across time) and attempts to reconstruct the raw signal. This forces the transformer to learn dependencies between distant cortical regions and across time steps (hemodynamic delays), providing a dense prior before the contrastive alignment phase.
- **Core assumption:** The haemodynamic response and functional connectivity patterns contain learnable redundancies that can be compressed and reconstructed by a transformer.
- **Evidence anchors:**
  - [Page 4] "The pre-trained vsMAE encoder... forms the basis of the proposed decoding framework."
  - [Page 20] Ablation studies show performance "drastically improves when we pre-train the SiT fMRI encoder on the vsMAE self-supervision task."
  - [corpus] Weak direct evidence for "tube masking" in fMRI specifically, though related to general MAE principles.
- **Break condition:** If the masking ratio is too high (e.g., >90%) or temporal resolution is too low, the context becomes insufficient for reconstruction, leading to trivial or failed feature learning.

### Mechanism 3
- **Claim:** Tri-modal contrastive alignment (CLIP) disentangles semantic content from noise by anchoring fMRI embeddings to both visual and auditory stimuli simultaneously.
- **Mechanism:** The model maximizes similarity between the [CLS] token of an fMRI clip and the embeddings of the corresponding video and audio clips, while pushing apart non-matching pairs. This triangulation (fMRI-Video-Audio) appears to filter out modality-specific noise and isolate the shared semantic "concept" of the movie scene.
- **Core assumption:** The semantic content of the movie (visual + audio) is linearly or non-linearly decodable from the aggregate cortical activity over a 3-second window.
- **Evidence anchors:**
  - [Page 8] "best retrieval performance is achieved when training on all three modalities, demonstrating the complementary contributions."
  - [Page 3] "tri-modal self-supervised contrastive (CLIP) alignment... enables the retrieval."
  - [corpus] TRIBE (arXiv:2507.22229) supports the efficacy of tri-modal encoding for fMRI response prediction.
- **Break condition:** If the subject is not attending to the movie, or if the audio/visual tracks are semantically discordant, the alignment signal weakens, potentially confusing the embedding space.

## Foundational Learning

- **Concept:** **Geometric Deep Learning / Surface Patching**
  - **Why needed here:** The input is not a standard 3D image or 2D grid, but a spherical mesh. Understanding how the icosahedron is discretized into non-overlapping triangular patches is essential to grasp how the Transformer processes "space."
  - **Quick check question:** How does the model handle the non-Euclidean nature of the cortical surface when generating input tokens?

- **Concept:** **Contrastive Learning (CLIP)**
  - **Why needed here:** This is the training objective that bridges the gap between brain activity and digital media.
  - **Quick check question:** In the tri-modal setup, what constitutes a "positive" pair versus a "negative" pair during the loss calculation?

- **Concept:** **Haemodynamic Response Function (HRF) & Temporal Lag**
  - **Why needed here:** fMRI measures blood flow, not direct firing. The paper uses a 6-second lag.
  - **Quick check question:** Why is the 3-second fMRI window aligned with video clips from 6 seconds prior?

## Architecture Onboarding

- **Component map:** fMRI (Native Mesh -> I6 Sphere -> I3 Patches) + 3s Video Clip + 3s Audio Clip -> SiT Encoder (vsMAE pre-trained) -> Frozen Encoders (VideoMAE, Wav2Vec) -> Projection Heads (Multimodal Mappers) -> Tri-modal CLIP Loss.

- **Critical path:**
  1. Pre-process volumes to native surface -> Register to I6 sphere.
  2. Run vsMAE pre-training (Tube Masking + Reconstruction) to initialize SiT weights.
  3. Run CLIP Fine-tuning (Freeze/Finetune SiT, Train Mappers) to align embeddings.
  4. Inference: Calculate cosine similarity between query fMRI embedding and candidate video embeddings.

- **Design tradeoffs:**
  - **Resolution vs. Compute:** Increasing mesh resolution (I3 -> I4) increases complexity 16x (Page 10). The authors stick to I3 patching.
  - **Tube Masking Ratio:** 50% found optimal (Page 26). Higher ratios destroy too much context for noisy fMRI; lower ratios leave too much redundancy.
  - **Temporal Window:** 3 seconds chosen to match average movie "shot" duration (Page 16).

- **Failure signatures:**
  - **Random Retrieval Performance:** If Top-1 accuracy is near 3.7% (random chance), the alignment failed (likely HRF misalignment or data shuffling error).
  - **Overfitting:** High performance on train subjects but <20% on new subjects suggests the model memorized anatomical idiosyncrasies rather than functional dynamics (indicates insufficient surface alignment or overly complex mapper).

- **First 3 experiments:**
  1. **Sanity Check (Bio-Physics):** Verify the 6-second temporal lag. Plot correlation between video embeddings and fMRI embeddings across various lags (as in Appendix C.8) to ensure the signal aligns.
  2. **Ablation (Modality):** Train three separate models: fMRI-Video only, fMRI-Audio only, and the full Tri-modal. Compare Top-1 retrieval on held-out subjects to confirm the "complementary" contribution claim.
  3. **Pre-training Validation:** Compare a "from scratch" SiT vs. a vsMAE-pre-trained SiT on the retrieval task. If the pre-trained model does not significantly outperform the scratch model, the masking strategy or reconstruction task may be flawed.

## Open Questions the Paper Calls Out

- **Open Question 1:** Do the learned self-attention maps vary meaningfully across individuals, and can they predict specific behavioral or cognitive traits? The Discussion states, "We are yet to explore whether self-attention varies meaningfully across individuals, including whether maps are predictive of behavioural or cognitive traits." This remains unresolved due to current analysis being limited to global comparisons between average attention patterns and standard functional organization maps.

- **Open Question 2:** Can the Surface Vision Transformer (SiT) scale to higher cortical resolutions (e.g., I4) without incurring prohibitive computational costs? The Discussion notes that increasing the resolution by one level increases self-attention complexity 16×, suggesting the need to "explor[e] lighter forms of attention computation." This is unresolved due to the quadratic complexity of standard transformer attention making the current architecture computationally infeasible at higher resolutions.

- **Open Question 3:** Does the SIM framework generalize to audio-visual stimuli that differ significantly in style or content from the HCP movie dataset? The Discussion highlights the inability to test if the model "would generalise to completely different movies" due to the restricted variety of stimuli in the HCP dataset. This is unresolved because the model was trained and tested exclusively on a specific set of Hollywood movie clips.

## Limitations
- **Cross-dataset generalization:** High retrieval accuracies are evaluated within the HCP 7T dataset; performance on different fMRI acquisition protocols or entirely different movie datasets remains untested.
- **Temporal resolution constraints:** The 3-second temporal window and 6-second hemodynamic lag are critical design choices, but the paper doesn't extensively validate whether these are optimal or capture fine-grained temporal dynamics versus coarse semantic content.
- **Computational and data intensity:** The I6→I3 patching scheme generates 1,280 tokens, requiring significant GPU memory for the transformer attention mechanism, and depends on large-scale pre-training data.

## Confidence
- **High confidence:** The core mechanism of using surface-based representation (I6→I3 patching) to preserve cortical topology and enable spatial autocorrelation modeling through SiT.
- **Medium confidence:** The specific retrieval accuracies (76.8% top-1) for inter-subject generalization, which depend on the particular HCP dataset characteristics and preprocessing pipeline.
- **Low confidence:** The broader claims about "personalized simulations of brain function" and enabling "brain-computer interfaces and neurofeedback," which are speculative and not demonstrated within the paper.

## Next Checks
1. **Cross-dataset validation:** Test the pre-trained model on a completely different movie-watching fMRI dataset (e.g., from a different scanner, different subjects, different movies).
2. **Temporal alignment optimization:** Systematically vary the temporal lag (e.g., test 4s, 6s, 8s) and the temporal window length (e.g., 2s, 3s, 4s) to determine if the current choices are optimal.
3. **Attention interpretability analysis:** Perform a more detailed analysis of the attention maps beyond the qualitative visualizations, quantifying the spatial and semantic selectivity of attention heads.