---
ver: rpa2
title: Labels or Input? Rethinking Augmentation in Multimodal Hate Detection
arxiv_id: '2508.11808'
source_url: https://arxiv.org/abs/2508.11808
tags:
- multimodal
- hateful
- hate
- prompt
- augmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of detecting hate speech in multimodal
  memes, where subtle text-image interactions make classification difficult. The authors
  propose two complementary strategies: prompt optimization to enrich supervision
  through structured prompts and fine-grained labels, and multimodal data augmentation
  to generate counterfactually neutral memes using a multi-agent LLM-VLM pipeline.'
---

# Labels or Input? Rethinking Augmentation in Multimodal Hate Detection

## Quick Facts
- **arXiv ID:** 2508.11808
- **Source URL:** https://arxiv.org/abs/2508.11808
- **Reference count:** 10
- **Primary result:** Prompt design and scaled labels improve small model performance; multimodal augmentation reduces spurious correlations and boosts robustness.

## Executive Summary
This paper tackles the challenge of detecting hate speech in multimodal memes, where subtle text-image interactions make classification difficult. The authors propose two complementary strategies: prompt optimization to enrich supervision through structured prompts and fine-grained labels, and multimodal data augmentation to generate counterfactually neutral memes using a multi-agent LLM-VLM pipeline. Experiments on the Facebook Hateful Memes dataset show that prompt structure and scaled labels improve small model performance, with category prompts boosting InternVL2 F1-score from 60.00 to 64.74. Multimodal augmentation adds 2,479 non-hateful memes, raising Qwen2-VL F1-score from 69.59 to 71.31 and improving robustness across modalities. Human evaluation confirms high quality and label reliability (92.5% agreement). The findings demonstrate that prompt structure and targeted augmentation are as critical as model size for building robust and fair hate detection systems.

## Method Summary
The authors develop a two-pronged approach to multimodal hate detection. First, they optimize prompt design by comparing simple versus category-based prompts and binary versus scaled (0–9) labels, using GPT-4o-mini as a teacher to generate nuanced supervision. Second, they create a multimodal augmentation pipeline that identifies memes where hate resides primarily in the text and generates neutral text-image pairs to decorrelate visual features from hateful labels. The pipeline employs a multi-agent system: a Qwen2.5-14B model for hatefulness attribution, InternLM-XComposer for background description, GPT-4o-mini for caption neutralization, and Gemini 2.0 Flash for meme rendering. Experiments fine-tune both unimodal (BERT, RoBERTa) and multimodal (InternVL2, Qwen2-VL, SmolVLM) models under different prompt and label configurations, with additional training on the augmented dataset.

## Key Results
- Category prompts with scaled labels boost InternVL2 F1-score from 60.00 to 64.74
- Multimodal augmentation adds 2,479 non-hateful memes, improving Qwen2-VL F1-score from 69.59 to 71.31
- Zero-shot prompting with large VLMs rivals fine-tuning on scaled labels, but fine-tuning remains superior for binary classification
- Human evaluation confirms high quality and label reliability (92.5% agreement)

## Why This Works (Mechanism)

### Mechanism 1
Structured prompts with scaled labels (0–9) improve performance on nuanced hate detection compared to binary labels, provided the model has sufficient capacity to utilize the semantic context. Category-based prompts define subtypes of hate (e.g., misogyny, xenophobia), guiding the model's attention to specific cues. The scaled label format forces the model to predict a continuous score rather than a hard boundary, acting as a form of soft supervision that reduces gradient noise during fine-tuning. Performance degrades if the category definitions in the prompt confuse smaller models with limited reasoning capacity (e.g., RoBERTa F1 dropped when using category prompts on binary labels).

### Mechanism 2
Counterfactual multimodal augmentation reduces spurious correlations between visual features and toxic labels. The pipeline identifies "NH" (Neutral Image, Hateful Text) memes and generates a synthetic twin where the text is neutralized (̃Ti) while the visual features (Ii) are preserved. By training on these pairs, the model learns that the visual features in that context are not inherently predictive of hate, effectively decorrelating the image from the hateful label. This mechanism breaks down if the regeneration model fails to preserve the visual context (semantic drift) or produces incoherent text, introducing label noise and degrading model calibration.

### Mechanism 3
Zero-shot prompting with large VLMs can rival fine-tuning on scaled labels, but fine-tuning remains superior for standard binary classification. Large VLMs (e.g., InternVL2) possess pre-trained semantic knowledge that structured prompts can activate directly (In-Context Learning) for complex tasks. However, for simpler binary tasks, the specificity gained through weight updates (fine-tuning) outweighs the general knowledge accessed via prompting. This equivalence breaks down for smaller models (SmolVLM 2B) which rely heavily on fine-tuning to adapt to specific data distributions.

## Foundational Learning

- **Concept:** **Multimodal Hatefulness Attribution**
  - **Why needed here:** To implement the augmentation pipeline, one must determine which modality (text, image, or both) carries the hateful signal. Without this, you cannot generate valid counterfactuals (e.g., neutralizing text is useless if the image itself is hateful).
  - **Quick check question:** Given a meme featuring a benign stock photo of a cat and a caption promoting violence, which class (Hi) does this belong to, and which component should be rewritten?

- **Concept:** **Label Scaling vs. Binarization**
  - **Why needed here:** The paper demonstrates that binary labels lose information about the "degree" of hate. Understanding how to map continuous "hatefulness" (0-9) to discrete classes during inference (thresholding at 5) is critical for reproducing the prompt optimization results.
  - **Quick check question:** If a model predicts a hate score of 4/9, should this be classified as hateful or non-hateful under the paper's mapping rules?

- **Concept:** **Spurious Correlation (Shortcut Learning)**
  - **Why needed here:** The core motivation for the data augmentation is preventing models from learning "harmful" associations (e.g., "religious clothing = hate") that exist in training data but not in reality. Understanding this bias is necessary to diagnose why a model might be overfitting.
  - **Quick check question:** If a model flags all memes containing watermelons as hateful because of training set artifacts, is this a failure of feature extraction or spurious correlation?

## Architecture Onboarding

- **Component map:** Input (Ii, Ti) + Ground Truth Label → Label Augmentation (GPT-4o-mini) → Scaled Label (0-9) → Data Augmentation Pipeline → Target Models (BERT/ViT/InternVL2/Qwen2-VL)
- **Critical path:** The Hatefulness Attribution step. If the Qwen2.5-14B model incorrectly labels an image as "neutral" when it is implicitly hateful, the augmentation pipeline will generate a "neutral" meme that retains hateful visual features, poisoning the dataset with false negatives.
- **Design tradeoffs:** Using GPT-4o-mini for scaling ensures high label quality but introduces API dependency and cost. Using Gemini for regeneration preserves layout but occasionally fails to render text (7/200 failure rate in paper). The pipeline explicitly generates neutral variants only to avoid synthesizing new hate speech, potentially limiting the diversity of "hard negative" examples.
- **Failure signatures:** Semantic Drift (generated image loses key visual elements), Format Collapse (renderer outputs image with no text overlay), Label Misalignment (teacher model scores sarcasm as "0" due to lack of cultural context).
- **First 3 experiments:** (1) Prompt Variance Test: Fine-tune BERT using Simple vs. Category prompts on binary labels to verify the "noise" effect. (2) Augmentation Validity Check: Run full pipeline on 100 random samples and manually verify Hatefulness Attribution accuracy. (3) Scaling Threshold Ablation: Test different mapping thresholds (e.g., 0-9 scores to binary at threshold 4 vs. 5 vs. 6) to find optimal cutoff.

## Open Questions the Paper Calls Out

- **Question:** Does the proposed framework generalize to culturally diverse settings and platforms beyond Facebook?
  - **Basis in paper:** The authors explicitly call for future work to "explore scaling the augmentation pipeline to culturally diverse settings" and assess generalization across platforms.
  - **Why unresolved:** The experiments rely solely on the Facebook Hateful Memes dataset, limiting the applicability of findings to other content distributions or cultural idioms.
  - **What evidence would resolve it:** Evaluating the pipeline on multi-cultural benchmarks (e.g., Multi3Hate) or cross-platform datasets (e.g., Twitter/X) to verify robustness.

- **Question:** How can the augmentation pipeline be improved to eliminate semantic drift and context-preservation errors?
  - **Basis in paper:** The paper identifies "remaining context-preservation challenges" as a limitation, citing generated examples where specific visual details were lost or formatting failed.
  - **Why unresolved:** The current VLM generator occasionally hallucinates or drops semantic details during the rendering process, necessitating manual validation.
  - **What evidence would resolve it:** A modified generation architecture that achieves near-perfect scores in "Background Alignment" and "Caption Alignment" during human evaluation.

- **Question:** Can prompt structures be further optimized to capture implicit, context-dependent hate speech that currently causes labeling disagreement?
  - **Basis in paper:** The authors note that label disagreements frequently stem from "indirect or context-dependent hate speech" which even advanced models (GPT-4o-mini) fail to identify correctly.
  - **Why unresolved:** Standard category prompts may lack the sociocultural grounding necessary to disambiguate satire from subtle harm, leading to the 7.5% label error rate observed.
  - **What evidence would resolve it:** Introducing "context-aware" prompts or knowledge-enhanced labeling that improves human-model agreement on implicit hate instances.

## Limitations

- Reliance on generative models (GPT-4o-mini, Gemini 2.0 Flash) for data augmentation introduces quality variability and potential bias propagation
- 7.5% of generated content potentially introduces noise due to imperfect human evaluation agreement
- Focus on binary hate detection may not capture the full spectrum of nuanced harmful content
- Prompt effectiveness is highly model-dependent and may not transfer across architectures

## Confidence

- **High Confidence:** The core finding that structured prompts with scaled labels improve performance for larger VLMs (InternVL2 results from 60.00 to 64.74). The experimental methodology and results are clearly presented with appropriate statistical comparisons.
- **Medium Confidence:** The multimodal augmentation pipeline's effectiveness (Qwen2-VL improvement from 69.59 to 71.31). While results are promising, the reliance on multiple generative models and the potential for semantic drift during meme regeneration introduce uncertainty.
- **Low Confidence:** The generalizability of prompt optimization benefits across all model sizes. The paper notes that category prompts hurt smaller models (RoBERTa F1 dropped), suggesting that prompt effectiveness is highly model-dependent and may not transfer across architectures.

## Next Checks

1. **Robustness Across Datasets:** Test the prompt optimization and augmentation strategies on alternative multimodal hate detection datasets (e.g., MMHS150K, BreakingNews) to verify that improvements are not dataset-specific artifacts of the Facebook Hateful Memes corpus.

2. **Cross-Generational Model Transfer:** Evaluate whether VLMs fine-tuned on the augmented dataset maintain performance when tested against memes generated by different VLM families (e.g., test a model trained on Gemini-augmented data against memes rendered by DALL-E 3).

3. **Long-Tail Distribution Analysis:** Conduct a detailed error analysis focusing on the long-tail categories of hate (e.g., intersectional hate, cultural-specific slurs) to determine whether prompt optimization and augmentation disproportionately benefit certain hate types or leave specific categories under-detected.