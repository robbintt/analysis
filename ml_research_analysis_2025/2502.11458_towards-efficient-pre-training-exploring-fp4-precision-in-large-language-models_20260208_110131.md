---
ver: rpa2
title: 'Towards Efficient Pre-training: Exploring FP4 Precision in Large Language
  Models'
arxiv_id: '2502.11458'
source_url: https://arxiv.org/abs/2502.11458
tags:
- training
- arxiv
- quantization
- language
- precision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an FP4 precision training scheme for large
  language models, addressing the challenge of quantization errors and limited representation
  capability in ultra-low precision training. The core method employs mixed-precision
  quantization strategies tailored for different modules and training stages, with
  FP8 protection for attention mechanisms and gradient-sensitive linear layers, combined
  with a target precision training schedule.
---

# Towards Efficient Pre-training: Exploring FP4 Precision in Large Language Models

## Quick Facts
- **arXiv ID:** 2502.11458
- **Source URL:** https://arxiv.org/abs/2502.11458
- **Reference count:** 15
- **Primary result:** FP4 training with mixed-precision strategies achieves BF16/FP8-level accuracy with 30% theoretical computational cost reduction.

## Executive Summary
This paper addresses the challenge of training large language models in ultra-low precision (FP4) by proposing a mixed-precision quantization strategy that maintains accuracy while significantly reducing computational costs. The authors identify that standard FP4 training causes attention mechanism collapse and gradient underflow, leading to poor model performance. Their solution employs FP8 precision for attention mechanisms and gradient-sensitive layers while using FP4 for the bulk of computations, combined with a target precision training schedule that recovers performance loss in the final training stages.

## Method Summary
The method employs a mixed-precision quantization strategy for large language model pretraining, using FP4 for forward pass computations in feed-forward networks while protecting attention mechanisms with FP8 precision. Key components include per-block quantization (block size 128) with straight-through estimator for gradient propagation, FP8 protection for attention QKV projections and output layers to prevent attention score flattening, and FP8 computation for weight and activation gradients to prevent underflow. A critical innovation is the "Target Precision Training Schedule" that switches to FP16 for the final 5-10% of training steps to recover any performance gaps caused by low-precision training. The approach is implemented in Megatron-LM framework with peak learning rates of 6e-4 for GPT and 1e-4 for LLaMA architectures, using Adam optimizer and cosine decay schedules.

## Key Results
- Achieves validation loss and downstream task performance comparable to BF16 and FP8 training
- Reduces theoretical computational costs by approximately 30% compared to higher precision training
- Maintains WikiText perplexity scores within 0.13 points of FP16 baselines (5.26 vs 5.39)
- Successfully trains GPT-2 and LLaMA models up to 1B parameters without significant accuracy degradation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mixed-precision assignment prevents attention mechanism collapse caused by ultra-low bit quantization noise.
- **Mechanism:** FP4 quantization errors flatten attention score distributions, making models unable to distinguish token importance. By maintaining FP8 precision for QKV projections and output projection, the model retains dynamic range for accurate attention score calculation while allowing FFN to operate in FP4.
- **Core assumption:** Computational load of FFN is distinct from numerical sensitivity of attention mechanism, enabling module-wise precision decoupling.
- **Evidence anchors:** Mixed-precision quantization strategies tailored for different modules; FP8 precision for QKV computation prevents uniform attention distribution; related work Quartet validates difficulty of native FP4 without specific handling.

### Mechanism 2
- **Claim:** FP8 protection for backward pass gradients prevents weight update underflow and accumulation errors.
- **Mechanism:** Gradient magnitudes decrease during training, often centering near 0.02. FP4's limited representational capacity causes significant underflow and quantization bias in these small values. Computing weight gradients in FP8 ensures parameter updates remain numerically valid, stabilizing convergence.
- **Core assumption:** Chain rule propagation of errors through backward pass is more sensitive to quantization noise than forward pass activations in specific layers.
- **Evidence anchors:** Fine-grained quantization methods ensure backpropagation stability; FP8 adopted for weight gradients to prevent underflow; related work Metis identifies gradient spectrum sensitivity requiring higher precision handling.

### Mechanism 3
- **Claim:** Target Precision Training Schedule recovers performance loss by allowing models to escape noise floor of low-precision basins.
- **Mechanism:** Continuous low-precision training creates persistent gap compared to FP16 as model adapts weights to quantization noise. Switching to FP16 for final 5-10% of training allows optimizer to refine weights without this noise, closing generalization gap.
- **Core assumption:** Weights learned during FP4 phase reside in basin sufficiently close to optimal FP16 solution that short fine-tuning period can bridge gap.
- **Evidence anchors:** Target precision training schedule achieves accuracy comparable to BF16; final 5-10% of steps allows model to return to ideal state; related work Oscillation-Reduced MXFP4 highlights necessity of recovery mechanisms.

## Foundational Learning

- **Concept:** Quantization Noise vs. Underflow
  - **Why needed here:** Paper distinguishes between rounding errors (noise) and values falling below minimum representable number (underflow), driving split between FP4 (forward) and FP8 (backward) strategies.
  - **Quick check question:** If gradient value is 0.015, is it more likely to suffer from quantization noise or underflow in FP4 container with minimum positive normal value potentially higher than 0.015?

- **Concept:** Module Sensitivity Analysis
  - **Why needed here:** Method relies on observation that not all Transformer layers are equal; Attention is sensitive to precision while FFN dominates compute cost.
  - **Quick check question:** Which component (Attention or FFN) creates bottleneck for computational cost, and which creates bottleneck for numerical stability?

- **Concept:** Straight-Through Estimator (STE)
  - **Why needed here:** Referenced in Appendix and standard for quantization; required to backpropagate gradients through non-differentiable quantization function.
  - **Quick check question:** How does gradient pass through round() operation during backward pass in this framework? (Answer: passed through unchanged/identity)

## Architecture Onboarding

- **Component map:** Input -> FP8 GEMM (Attention QKV) -> FP4 GEMM (FFN) -> FP8 GEMM (Attention output) -> Output
- **Critical path:**
  1. Casting: FP32 Master Weights cast to FP4 (FFN) or FP8 (Attn) per block
  2. GEMM: Inputs quantized, MatMul performed in target precision
  3. Gradient Sync: Gradients computed in FP8 to prevent underflow, accumulated in FP32 master weights
  4. Switch: Monitor step count; at 90-95% of total steps, disable quantization (switch to FP16) for "Target Precision" recovery phase

- **Design tradeoffs:**
  - Efficiency vs. Complexity: Gains ~30% theoretical compute reduction but increases software complexity with module-wise casting logic
  - Memory vs. Precision: Uses FP32 master weights (standard) but low-bit activations, trading memory bandwidth for numerical risk

- **Failure signatures:**
  - Attention Collapse: Attention heatmaps become uniform; validation loss plateaus early (Mitigation: Verify FP8 enforcement on QKV)
  - Gradient Underflow: Loss spikes or NaNs appear mid-training (Mitigation: Verify FP8 enforcement on backward pass)
  - Convergence Gap: Final loss lower than FP16 baseline but PPL/downstream tasks perform poorly (Mitigation: Ensure "Target Precision" schedule activated)

- **First 3 experiments:**
  1. Baseline Sanity Check: Train small LLaMA-125M using pure FP4 (no protection) to replicate "uniform attention" failure mode
  2. Module Ablation: Enable FP8 only for Attention, then only for Backward, to isolate contribution of Mechanism 1 vs. Mechanism 2
  3. Schedule Validation: Train for 10B tokens with and without final FP16 switch to quantify exact PPL gap recovered by "Target Precision" schedule

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can proposed FP4 training scheme maintain convergence stability and accuracy when scaling to models significantly larger than 1B parameters or datasets exceeding 25B tokens?
- **Basis in paper:** Section 6 states method "has not been validated on larger models and larger datasets," and Appendix B notes precision requirements become more stringent as data volume grows, identifying this as "critical direction for future research."
- **Why unresolved:** Experiments limited to GPT-2 (up to 774M) and LLaMA-1B, leaving behavior of ultra-large models (e.g., 70B+) unknown.
- **What evidence would resolve it:** Successful pretraining runs on models with 7B+ parameters or trillion-token datasets exhibiting validation loss curves comparable to BF16 baselines.

### Open Question 2
- **Question:** What is realized throughput and efficiency gain when deploying method on hardware with native FP4 tensor cores compared to theoretical costs?
- **Basis in paper:** Section 6 highlights that because method uses "simulated FP4 approach," it is "unable to obtain accurate increase in training efficiency" in terms of wall-clock time.
- **Why unresolved:** 30% cost reduction cited is theoretical; actual speedups depend on memory bandwidth and arithmetic unit utilization on next-generation hardware.
- **What evidence would resolve it:** Benchmarks on native FP4 hardware (e.g., NVIDIA Blackwell) showing actual training steps per second and memory usage.

### Open Question 3
- **Question:** Can more granular, customized quantization strategies replace current conservative protection for sensitive components to further increase computational efficiency?
- **Basis in paper:** Section 6 notes authors employed "simple strategy" for sensitive components and plan to "explore more customized approaches to enable broader range of computations to utilize FP4."
- **Why unresolved:** Current method relies on higher precision (FP8/FP16) for attention and gradients; moving these to FP4 without destabilizing training is unexplored optimization.
- **What evidence would resolve it:** Ablation study showing advanced quantization techniques (e.g., differentiable estimators) allow sensitive modules to operate in FP4 without performance degradation.

## Limitations

- **FP4 Format Specification:** Paper does not specify exact FP4 format (E2M1, E3M1, etc.) used, creating ambiguity for exact reproduction and affecting underflow/overflow behavior
- **Computational Overhead Accounting:** Claimed 30% reduction is theoretical; paper does not quantify actual hardware overhead from mixed-precision casting logic or frequency of precision switches
- **Scalability Validation:** Method validated only on models up to 1B parameters and datasets up to 25B tokens, leaving performance on larger models and datasets uncertain

## Confidence

- **High Confidence:** Attention collapse mechanism (FP4 causing uniform attention scores) well-supported by experimental evidence and aligns with established quantization literature
- **Medium Confidence:** Gradient underflow mitigation (FP8 backward pass) logically sound and referenced, but exact distribution of gradients and underflow threshold depend on unspecified FP4 format details
- **Low Confidence:** Target Precision Training Schedule recovery mechanism demonstrated but claim of closing FP16 baseline gap lacks direct comparison data and may be dataset-dependent

## Next Checks

1. **Format Fidelity Check:** Replicate Fig 1(c) with pure FP4 to confirm uniform attention distribution, then verify enforcing FP8 on QKV recovers score diversity
2. **Gradient Underflow Quantification:** Monitor gradient histograms during training; measure percentage of values falling below FP4's minimum positive normal value to validate need for FP8 backward pass
3. **Schedule Recovery Validation:** Conduct ablation study training with and without final 5-10% FP16 switch on WikiText, directly measuring PPL gap to isolate schedule's contribution