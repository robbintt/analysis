---
ver: rpa2
title: 'T3C: Test-Time Tensor Compression with Consistency Guarantees'
arxiv_id: '2601.01299'
source_url: https://arxiv.org/abs/2601.01299
tags:
- arxiv
- latency
- budget
- https
- rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: T3C introduces a train-once, test-time compression framework that
  enables dynamic rank and precision adjustment via a lightweight controller and fast
  consistency certificate. It maintains elastic tensor factorizations up to a maximal
  rank, couples rank with rank-tied mixed-precision quantization, and snaps per-layer
  assignments to hardware-aligned profiles for monotone, device-aware trade-offs.
---

# T3C: Test-Time Tensor Compression with Consistency Guarantees

## Quick Facts
- **arXiv ID:** 2601.01299
- **Source URL:** https://arxiv.org/abs/2601.01299
- **Reference count:** 40
- **Primary result:** Single checkpoint delivers predictable, certificate-backed accuracy-latency-size curves across devices with zero budget violations.

## Executive Summary
T3C introduces a train-once, test-time compression framework that enables dynamic rank and precision adjustment via a lightweight controller and fast consistency certificate. It maintains elastic tensor factorizations up to a maximal rank, couples rank with rank-tied mixed-precision quantization, and snaps per-layer assignments to hardware-aligned profiles for monotone, device-aware trade-offs. The certificate upper-bounds logit drift using spectral proxies and activation statistics, allowing regularization during training and risk-aware deployment reporting. On ImageNet-1k, T3C achieves matched accuracy with lower latency and smaller model size than strong PTQ/QAT baselines (e.g., ResNet-50: 1.18ms p50, 38MB, <0.5% drop vs. PTQ-8b 1.44ms, 88MB). Across vision and language models, a single checkpoint delivers predictable, certificate-backed accuracy-latency-size curves across devices with zero budget violations.

## Method Summary
T3C is a train-once, test-time compression framework that maintains weight tensors in elastic factorized form (SVD/Tucker/CP) up to a maximal rank, with differentiable rank selection via Gumbel-Top-k masks. A lightweight controller maps budget tokens (latency/energy/size) to per-layer rank/bit assignments, snapping outputs to hardware-aligned discrete profiles to ensure monotonicity and device compatibility. Mixed-precision quantization is rank-tied, with per-tensor/channel uniform affine quantization and STE for gradients. A layerwise consistency certificate, computed from spectral proxies and activation statistics, upper-bounds logit drift and serves as both a training regularizer and a deployment risk report. The total loss combines task cross-entropy, self-distillation, augmentation consistency, certificate penalty, and budget proxy terms.

## Key Results
- T3C achieves matched accuracy with lower latency and smaller model size than strong PTQ/QAT baselines on ImageNet-1k (ResNet-50: 1.18ms p50, 38MB, <0.5% drop vs. PTQ-8b 1.44ms, 88MB).
- Across vision and language models, a single checkpoint delivers predictable, certificate-backed accuracy-latency-size curves across devices with zero budget violations.
- The framework enables zero-shot deployment on diverse hardware (A100, Jetson, Android, NPU) with device-aware, monotone trade-offs.

## Why This Works (Mechanism)

### Mechanism 1: Elastic Factorization with Differentiable Rank Selection
Maintaining weight tensors in factorized form (SVD/Tucker/CP) up to a maximal rank, with Gumbel-Top-k soft masks, enables a single checkpoint to be sampled at any rank during training and snapped to discrete profiles at deployment. Factor weights are stored as $U, \Sigma, V$ (or Tucker/CP equivalents) with a temperature-controlled logistic mask over singular values. During forward passes, diag($\hat{m}$) selects active spectral components. This creates a shared-parameter space where lower ranks are contiguous subsets of higher-rank reconstructions, avoiding re-training per budget. Core assumption: spectral decay is sufficiently steep that truncated singular vectors at rank $k$ remain valid for ranks $k' > k$ without catastrophic error propagation; Gumbel relaxation approximates hard top-k well enough after annealing. Evidence anchors: [abstract] "elastic tensor factorization (maintained up to a maximal rank) with rank-tied mixed-precision quantization"; [Section 3.1–3.2] Eq. (3)–(4) define quantized forward with soft mask $\hat{m}_{1:k_{\max}}$; "differentiable top-k mask selects active singulars/factors"; [corpus] Minima (2602.01613) and FlexRank (2602.02680) similarly use low-rank structural compression for adaptive deployment, corroborating shared-parameter benefits, though without certificates. Break condition: If spectral energy is flat (no decay), truncated factors provide poor approximations at any $k < k_{\max}$; if activation distributions are heavy-tailed, truncation error may not correlate with spectral residual.

### Mechanism 2: Budget-Conditioned Controller with Hardware-Aligned Snapping
A lightweight MLP controller, conditioned on a budget token (latency/energy/size) and optional input summary, can learn to map to per-layer $(k_\ell, q_\ell)$ that respect monotone, hardware-compatible profiles. Controller $\pi_\phi(b, s(x))$ outputs layerwise proposals, projected onto a discrete profile lattice $P_\ell$ matched to kernel tiles. Snapping enforces monotonicity ($b' \succ b \Rightarrow k(b') \geq k(b)$, $q(b') \geq q(b)$). Controller gradients flow via Gumbel-Softmax + straight-through, or REINFORCE with a learned baseline. Core assumption: the profile lattice $P_\ell$ covers the relevant operating points; latency/energy are well-approximated by a calibrated hybrid proxy (FLOPs + bytes) that transfers across devices. Evidence anchors: [abstract] "lightweight controller that maps a latency/energy/size budget token to per-layer rank/bit assignments; the policy snaps to hardware-aligned profiles and is monotone in the budget"; [Section 3.3] Eq. (5) defines $\{k_\ell, q_\ell\} = \pi_\phi(b, s(x))$ with snapping; Appendix D.2 specifies profile selection with certificate safety gate; [corpus] Seer Self-Consistency (2511.09345) addresses budget-aware test-time scaling for LLMs, but focuses on token budgets rather than hardware profiles; corpus does not provide direct evidence for profile snapping. Break condition: If the profile lattice is misaligned with actual kernel performance (e.g., vendor NPU rejects certain $(k,q)$ combinations), snapping may produce invalid or slow profiles; certificate rejection loops can fail if no neighbor satisfies constraints.

### Mechanism 3: Consistency Certificate via Spectral Proxies and Activation Statistics
A layerwise certificate $\hat{\Delta}(k) = \sum_\ell \hat{L}_\ell \|\Delta W_\ell(k)\|_2 \alpha_\ell$ upper-bounds logit drift for compressed profiles, providing both a training regularizer and a deployment risk report. Per-layer Lipschitz proxy $\hat{L}_\ell$ is estimated via power iteration on the post-layer Jacobian; residual spectral norm $\|\Delta W_\ell(k)\|_2$ is cheaply bounded from discarded singulars; activation RMS $\alpha_\ell$ is calibrated on a held-out set. The certificate penalty $\lambda_{\text{CERT}} \max(0, \hat{\Delta}(k) - \epsilon)$ penalizes profiles predicted to exceed tolerance $\epsilon$. Core assumption: network blocks are locally Lipschitz (ReLU/GELU with 1-Lipschitz activations, eval-mode normalization); activation statistics on calibration set generalize to in-distribution inputs; power-iteration estimates are conservative. Evidence anchors: [abstract] "fast, layerwise consistency certificate, computed from spectral proxies and activation statistics, upper-bounds logit drift and regularizes training"; [Section 4.1, Proposition 4.1] Formal bound $\|\delta_z(x;k)\|_2 \leq \sum_\ell \hat{L}_\ell \|\Delta W_\ell(k)\|_2 \|a_{\ell-1}(x)\|_2$; expected form with $\alpha_\ell$; [corpus] No direct corpus evidence for logit-drift certificates tied to compression; Weng et al. (2018, cited in paper) provide Lipschitz-based robustness bounds, but not hardware-aware. Break condition: Heavy-tailed activations or highly non-Lipschitz blocks (e.g., train-mode BatchNorm, large-gain residual branches) cause $\hat{L}_\ell$ to underestimate gain; distribution shift makes $\alpha_\ell$ unrepresentative; certificate becomes loose or violated.

## Foundational Learning

- **Concept: Truncated SVD / Tucker-2 Tensor Factorization**
  - **Why needed here:** T3C represents dense and conv layers in factorized form; understanding singular value decay and residual norms is essential for interpreting certificate tightness and rank allocation.
  - **Quick check question:** Given a $512 \times 512$ weight matrix with singular values $\sigma_1=10, \sigma_2=3, \sigma_{3:512} \approx 0.1$, what is the spectral norm of the residual after rank-2 truncation?

- **Concept: Straight-Through Estimator (STE) for Quantization**
  - **Why needed here:** Mixed-precision quantization uses STE to backprop through hard rounding; understanding gradient approximations is critical for debugging training instability at low bit-widths.
  - **Quick check question:** For STE rounding $Q(x) = s \cdot \lfloor x/s \rceil$, what is the assumed gradient $\partial Q / \partial x$ within the clipping range? What happens outside?

- **Concept: Gumbel-Softmax Relaxation for Discrete Selection**
  - **Why needed here:** Rank selection is discrete; Gumbel-Top-k provides a differentiable proxy that anneals to hard decisions. Understanding temperature schedules is key to avoiding train-deploy mismatch.
  - **Quick check question:** As temperature $\tau \to 0$, what distribution does Gumbel-Softmax approach? How does high $\tau$ affect gradient variance?

## Architecture Onboarding

- **Component map:** Elastic factorization module -> Mixed-precision quantizer -> Budget controller -> Certificate module -> Loss combiner
- **Critical path:** 1. During training: sample budget $b$ -> controller outputs $(k_\ell, q_\ell)$ -> snapping -> factorized + quantized forward -> compute loss + certificate penalty -> backprop through STE and Gumbel relaxation. 2. At export: for each discrete profile $s_j \in S$, compute and store $\hat{\Delta}(s_j)$, latency/energy proxy, packed factor weights, and manifest. 3. At inference: runtime receives $b$, snaps to nearest profile $s_j$, loads prepacked factors, dispatches fused kernels.
- **Design tradeoffs:** Certificate tightness vs. compute: More power-iteration steps tighten $\hat{L}_\ell$ but increase training overhead; paper uses 3–5 steps intermittently. Profile lattice granularity: Finer lattices improve Pareto coverage but increase export size and kernel tuning; paper uses 3 profiles (Tiny/Med/Max) per model. Rank-tied vs. per-factor bits: Per-factor offsets ($q_U = q+1$) improve accuracy on sensitive layers but complicate kernel selection.
- **Failure signatures:** Monotonicity violation: Accuracy decreases when budget increases -> usually due to profile snapping selecting a slower kernel or certificate penalty over-regularizing high-$k$ profiles. Certificate violation at runtime: Observed drift exceeds $\epsilon$ -> check for distribution shift, eval-mode normalization mismatch, or underestimated $\hat{L}_\ell$ (increase power-iteration steps). Kernel mismatch at load-time: NPU rejects $(k,q)$ -> profile lattice was not pruned for vendor constraints; fallback to smaller profile or CPU.
- **First 3 experiments:** 1. **Single-layer certificate sanity check:** On a small MLP (2–3 layers), manually inject known perturbations $\Delta W$ and verify that $\hat{\Delta}$ bounds observed logit drift; plot bound vs. actual as rank varies. 2. **Controller ablation on ResNet-50:** Compare T3C (joint rank+bit) vs. rank-only vs. bit-only vs. uniform allocation at fixed latency target; report accuracy drop, violation %, and $\epsilon$. 3. **Cross-device proxy transfer:** Fit latency proxy on A100, evaluate MAPE on Jetson/Android/NPU; compare FLOPs-only, bytes-only, and hybrid proxies to validate Appendix F.5 findings.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the consistency certificate be tightened for architectures with non-Lipschitz components or under distribution shift without relying on conservative running maxima?
- **Basis in paper:** [explicit] Section 7 states future work will focus on "tightening and stress-testing the certificate under distribution shift and highly non-Lipschitz components."
- **Why unresolved:** The current spectral-norm proxy relies on assumptions that loosen bounds for heavy-tailed activations or aggressive normalization, potentially over-penalizing tight budgets.
- **What evidence would resolve it:** A refined bound demonstrating a <10% gap between predicted and observed logit drift under these stressed conditions.

### Open Question 2
- **Question:** How can controller robustness be maintained in low-data regimes or under extreme budget constraints where current regularization strategies fail?
- **Basis in paper:** [explicit] Section 7 calls for "improving controller robustness in extreme-budget and low-data regimes."
- **Why unresolved:** The lightweight controller exhibits instability on tiny datasets or when budgets are too tight to allow valid profile snapping, leading to constraint violations.
- **What evidence would resolve it:** Stable policy convergence and zero budget violations on datasets with <10k samples or at budgets <50% of the "Tiny" profile.

### Open Question 3
- **Question:** Can the T3C control plane be extended to incorporate structured sparsity while preserving monotonic budget guarantees?
- **Basis in paper:** [explicit] Section 7 lists "extending the control plane to incorporate structured sparsity" as a key future direction.
- **Why unresolved:** Integrating structured sparsity (e.g., N:M patterns) introduces discrete kernel constraints and vendor-specific interactions that may conflict with the current rank-precision snapping logic.
- **What evidence would resolve it:** A unified framework optimizing rank, bits, and sparsity that maintains monotone latency-accuracy trade-offs and valid certification.

## Limitations

- **Hardware Profile Dependency:** The paper's claimed efficiency gains are tightly coupled to specific hardware profiles (A100, Jetson, Android, NPU) that are not fully specified in the main text, creating uncertainty about generalizability to other accelerators or future GPU architectures.
- **Certificate Conservatism and Distribution Shift:** The spectral certificate provides a theoretically grounded upper bound, but its practical tightness depends heavily on the local Lipschitzness of the network and the representativeness of calibration data; no empirical analysis of certificate calibration is provided.
- **Controller Architecture Specificity:** While the controller is described as a "lightweight MLP," the exact dimensions, embedding size for budget tokens, and profile lattice resolution are not numerically specified, creating ambiguity in reproducing the exact controller behavior and Pareto coverage.

## Confidence

- **High Confidence:** The core mechanism of elastic factorization with Gumbel-Top-k rank selection is well-grounded in prior work (Minima, FlexRank) and the spectral certificate derivation follows established Lipschitz-based robustness bounds. The basic training pipeline (elastic SVD/Tucker + controller + loss terms) is clearly specified.
- **Medium Confidence:** The practical effectiveness of the framework—achieving matched accuracy with lower latency/size than strong PTQ/QAT baselines—relies on the interplay of all components (controller snapping, certificate regularization, profile lattice design). While ImageNet-1k results are presented, the paper does not provide extensive ablation studies isolating the impact of each mechanism.
- **Low Confidence:** The generalizability of the framework to other model families (e.g., Vision Transformers, large language models) and to diverse hardware ecosystems (cloud GPUs, edge NPUs, FPGAs) is asserted but not empirically demonstrated beyond the reported ResNet-50 and small BERT/LLM experiments.

## Next Checks

1. **Certificate Calibration Study:** On ResNet-50, measure the empirical coverage rate of the certificate (fraction of test samples where observed logit drift ≤ predicted bound) across different ranks and profiles. Vary the certificate slack $\eta$ and report how often the controller's profile selection aligns with the true Pareto-optimal trade-off.

2. **Controller Ablation on ResNet-50:** Compare T3C's full joint rank+bit controller against ablations: (a) rank-only allocation (fixed 8-bit), (b) bit-only allocation (full rank), (c) uniform allocation (equal rank reduction across layers). For each, report accuracy drop at a fixed latency target (e.g., 1.2ms), certificate violation rate, and controller entropy to assess exploration vs. exploitation.

3. **Cross-Device Proxy Transfer Validation:** Using the hybrid FLOPs+bytes latency proxy, fit the model on A100 and evaluate its MAPE on Jetson Orin, Pixel 8 Pro, and a representative NPU (e.g., Google TPU or Qualcomm Snapdragon). Compare against pure FLOPs-only and pure memory-only proxies to quantify the benefit of the hybrid approach and identify the worst-case transfer error.