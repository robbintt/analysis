---
ver: rpa2
title: Semi-Supervised Multi-Task Learning for Interpretable Quality As- sessment
  of Fundus Images
arxiv_id: '2511.13353'
source_url: https://arxiv.org/abs/2511.13353
tags:
- quality
- image
- multi-task
- good
- overall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a semi-supervised multi-task learning approach
  to improve interpretability in retinal image quality assessment (RIQA) while reducing
  annotation costs. The method uses a small manually labeled dataset to train a Teacher
  model that generates pseudo-labels for quality details (illumination, clarity, contrast),
  which are then combined with ground-truth overall quality labels to fine-tune a
  multi-task model.
---

# Semi-Supervised Multi-Task Learning for Interpretable Quality As- sessment of Fundus Images

## Quick Facts
- **arXiv ID:** 2511.13353
- **Source URL:** https://arxiv.org/abs/2511.13353
- **Reference count:** 40
- **Primary result:** Semi-supervised multi-task learning with pseudo-labels improves retinal image quality assessment F1 scores to 0.875 (EyeQ) and 0.778 (DeepDRiD), outperforming single-task baselines.

## Executive Summary
This paper introduces a semi-supervised multi-task learning approach for interpretable quality assessment of fundus images. The method trains a Teacher model on a small labeled dataset to generate pseudo-labels for quality details (illumination, clarity, contrast), which are then used to fine-tune a multi-task model alongside ground-truth overall quality labels. Using ResNet-18, the approach achieves state-of-the-art F1 scores while providing interpretable feedback through auxiliary outputs and improved GradCAM visualizations. The authors publicly release expert annotations for EyeQ quality details to support future research.

## Method Summary
The method employs a Teacher-student framework where a small manually labeled dataset (MSHF) trains a Teacher model to predict quality details. This Teacher generates pseudo-labels for larger datasets (EyeQ, DeepDRiD) that lack detailed annotations. A pre-trained single-task model on overall quality is then fine-tuned in a multi-task setting using the pseudo-labels for quality details alongside ground-truth overall quality labels. The combined loss function balances primary and auxiliary task objectives, with the auxiliary task acting as a regularizer that improves feature learning and interpretability.

## Key Results
- Multi-task model achieves F1 scores of 0.875 on EyeQ and 0.778 on DeepDRiD for overall quality assessment
- Performance is statistically comparable to Teacher model for most quality detail predictions (p > 0.05)
- Multi-task model outperforms single-task baselines (F1: 0.875 vs. 0.863 on EyeQ)
- Generated GradCAMs are more informative and highlight problematic areas relevant to capture conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Auxiliary task supervision via pseudo-labels improves primary task discrimination.
- Mechanism: The multi-task learning objective forces a shared feature representation to encode information relevant to both overall quality and specific quality details, acting as a regularizer that prevents overfitting to spurious correlations.
- Core assumption: Features learned for predicting quality details are a subset of features needed for overall quality.
- Evidence anchors: Abstract shows F1 improvement over single-task baselines; Discussion explains how auxiliary tasks push model to focus on actual acquisition defects.
- Break condition: Correlation between auxiliary task (quality details) and primary task (overall quality) breaks down if overall quality is driven by factors not captured in detail labels.

### Mechanism 2
- Claim: Pseudo-label noise from the Teacher model is functionally comparable to inter-expert label noise.
- Mechanism: Teacher model predictions on unlabeled data are treated as pseudo-labels, and the student can learn effectively because this "machine noise" is statistically similar to "human noise" (inter-observer variability).
- Core assumption: Teacher's error distribution on unlabeled data is not systematically biased.
- Evidence anchors: Abstract states multi-task performance is statistically comparable to Teacher for detail prediction tasks; Results show no statistically significant differences (p > 0.05).
- Break condition: Teacher model is not generalizable to larger dataset, leading to systematically wrong pseudo-labels that degrade student performance.

### Mechanism 3
- Claim: Multi-task learning produces more clinically interpretable and faithful explanations.
- Mechanism: Training with explicit labels for quality details forces GradCAM activations to align with specific image regions corresponding to defects, making heatmaps more actionable for technicians.
- Core assumption: Class activation maps from more accurate models are inherently more faithful and interpretable to humans.
- Evidence anchors: Abstract mentions interpretable feedback and more informative GradCAMs; Results show multi-task model correctly predicted 'rejectable' and highlighted problematic areas.
- Break condition: GradCAM visualizations do not correspond to image features a human expert would identify as problematic, leading to user confusion.

## Foundational Learning
- **Semi-Supervised Learning (Teacher-Student/Noisy Student):** Provides method to generate quality detail labels automatically, enabling multi-task approach without prohibitive manual labeling costs. *Quick check: What is the risk if the Teacher model makes systematically biased predictions on the larger, unlabeled dataset?*
- **Multi-Task Learning (Hard Parameter Sharing):** Enables primary task to benefit from auxiliary task by forcing shared representation useful for both tasks. *Quick check: If the auxiliary task was completely unrelated to overall quality, what would likely happen to primary task performance?*
- **Interpretability via Auxiliary Outputs and GradCAM:** Provides actionable feedback beyond "good/bad" scores through explicit outputs and visual cues. *Quick check: Why is a high-confidence GradCAM on the optic disc potentially misleading for assessing image quality caused by peripheral artifacts?*

## Architecture Onboarding
- **Component map:** Input Image → ResNet-18 Backbone → Shared Features → Task A Head (Quality Details) + Task B Head (Overall Quality)
- **Critical path:** 1) Train Teacher on MSHF for Task A 2) Generate pseudo-labels for EyeQ/DeepDRiD 3) Pre-train single-task model on EyeQ/DeepDRiD for Task B 4) Add Task A head and fine-tune using combined loss
- **Design tradeoffs:** Teacher quality vs annotation cost (small MSHF sufficient), Model capacity vs overfitting (ResNet-18 lightweight), Loss weights balance (λ_A, λ_B critical hyperparameter)
- **Failure signatures:** Task interference (primary task drops), Teacher overfitting (poor pseudo-labels), Meaningless GradCAMs (non-salient regions highlighted)
- **First 3 experiments:** 1) Train single-task baseline on EyeQ/DeepDRiD 2) Train Teacher on MSHF and generate pseudo-labels 3) Ablation comparing multi-task with ground-truth vs pseudo-labels for Task A

## Open Questions the Paper Calls Out
- Can self-supervised pretraining enable effective use of larger backbone architectures for the Teacher model without overfitting?
- Would reformulating the overall quality task as a regression objective improve performance?
- Does integrating coarse anatomical pseudo-labels as an additional auxiliary task enhance the quality assessment framework?

## Limitations
- Pseudo-label quality depends on Teacher model generalization to larger datasets
- RandAugment parameters and loss weight hyperparameters not fully specified
- Interpretability improvements lack rigorous quantitative validation against human expert interpretation

## Confidence
- **High Confidence:** Core experimental methodology, dataset usage, and F1 score metrics
- **Medium Confidence:** Equivalence of pseudo-label noise to inter-expert label noise
- **Medium Confidence:** Improved interpretability through GradCAMs

## Next Checks
1. Validate Teacher model performance on held-out EyeQ-D subset before generating pseudo-labels
2. Conduct ablation study comparing multi-task performance with ground-truth vs pseudo-labels for Task A
3. Perform human study evaluating whether multi-task GradCAMs provide more actionable guidance than single-task baselines