---
ver: rpa2
title: A Transformer-based Autoregressive Decoder Architecture for Hierarchical Text
  Classification
arxiv_id: '2501.13598'
source_url: https://arxiv.org/abs/2501.13598
tags:
- label
- labels
- text
- encoder
- hierarchy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RADAr, a hierarchical text classification
  model that uses a RoBERTa encoder and a custom autoregressive decoder. Unlike existing
  approaches, it avoids explicit graph encoding of the label hierarchy and does not
  use label semantics.
---

# A Transformer-based Autoregressive Decoder Architecture for Hierarchical Text Classification

## Quick Facts
- arXiv ID: 2501.13598
- Source URL: https://arxiv.org/abs/2501.13598
- Authors: Younes Yousef; Lukas Galke; Ansgar Scherp
- Reference count: 40
- One-line primary result: RADAr achieves SOTA-competitive results while halving inference time and reducing training time

## Executive Summary
This paper introduces RADAr, a hierarchical text classification model that uses a RoBERTa encoder and a custom autoregressive decoder. Unlike existing approaches, it avoids explicit graph encoding of the label hierarchy and does not use label semantics. Instead, it learns hierarchical relationships directly from label sequences during training. The decoder is trained from scratch and generates labels in a child-to-parent order, which consistently outperforms parent-to-child ordering. Experiments on three benchmark datasets show that RADAr achieves results competitive with the state of the art while halving inference time and reducing training time. Ablation studies confirm that the model's performance is robust without label semantics or a hierarchy encoder. The approach provides a simpler, faster, and more flexible architecture for hierarchical text classification, with implications for future research and practical deployment.

## Method Summary
RADAr uses a RoBERTa-base encoder and a 2-layer Transformer decoder trained from scratch. Labels are converted to symbolic identifiers and organized in child-to-parent sequences with `<unk>` separators. The model predicts labels autoregressively, with focal loss applied at the batch level to handle label imbalance. Training uses dual learning rates (5e-5 for encoder, 3e-4 for decoder) and includes label smoothing. The architecture avoids explicit graph encoding of the hierarchy, instead learning structural relationships from the sequence order during training.

## Key Results
- Child-to-parent label generation consistently outperforms parent-to-child ordering across all datasets
- Model achieves SOTA-competitive performance while reducing inference time by 50%
- Ablation studies confirm effectiveness without label semantics or hierarchy encoders
- Model is robust to label imbalance through batch-level focal loss

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Generating labels in a child-to-parent order yields better performance than the standard parent-to-child approach.
- **Mechanism:** The model predicts specific leaf nodes first, then predicts broader parents. This likely functions as a form of logical entailment, where the specific label implies the general one, reducing the search space for the parent.
- **Core assumption:** The autoregressive factorization benefits from predicting high-information tokens (children) before dependent tokens (parents).
- **Evidence anchors:**
  - [abstract] "Our model consistently performs better when organizing the label sequences from children to parents..."
  - [section 6.1] "This effect relates to the depth of the hierarchy, i. e., the deeper the hierarchy is, the more remarkable the effect is."
  - [corpus] Weak support; standard approaches in neighbors (e.g., Seq2Tree) typically use parent-to-child or level-wise traversal.
- **Break condition:** Performance gains diminish on shallow hierarchies (e.g., WOS with depth 2) where the sequence length is short.

### Mechanism 2
- **Claim:** Implicit hierarchy encoding via sequence ordering is sufficient, rendering explicit Graph Neural Networks (GNNs) unnecessary.
- **Mechanism:** The model replaces complex graph encoders with standard transformer attention over a linearized label sequence. It uses position embeddings and special separator tokens (`<unk>`) to demarcate hierarchy levels, allowing the decoder to learn structural relationships via self-attention.
- **Core assumption:** The structural information is fully captured by the token order and level separators during training.
- **Evidence anchors:**
  - [abstract] "...avoids explicit graph encoding of the label hierarchy... learns hierarchical relationships directly from label sequences..."
  - [section 6.1] "RADAr captures hierarchical information from the organization of the label sequences it is trained on."
  - [corpus] Supported by neighbors exploring alternatives to GNNs, though RADAr is unique in removing graph encoding entirely while remaining competitive.
- **Break condition:** If the hierarchy is extremely wide or deep, the fixed sequence length or tokenization strategy may fail to capture distant sibling relationships compared to dedicated graph convolutions.

### Mechanism 3
- **Claim:** Decoupling the encoder fine-tuning from decoder training using batch-level focal loss mitigates label imbalance.
- **Mechanism:** The system uses a separate learning rate for the pre-trained encoder (slow) and the scratch decoder (fast). Focal loss scales the cross-entropy to prioritize "hard" batches (those with low confidence/high loss), which often contain rare labels.
- **Core assumption:** Batches with high loss correlate with under-represented hierarchical paths that require more optimization signal.
- **Evidence anchors:**
  - [section 3] "...model aims to optimize batches that are hard to classify. Those usually include samples with low-frequent labels..."
  - [algorithm 1] Loss is calculated as `(1 - e^-loss)^gamma * loss`.
  - [corpus] General support in literature for focal loss in classification, but specific batch-level application is distinct here.
- **Break condition:** If the batch size is too small, the variance in batch difficulty may cause unstable gradients.

## Foundational Learning

- **Concept:** **Autoregressive Decoding**
  - **Why needed here:** Unlike classification heads that output probabilities independently, this model generates labels sequentially ($P(y_t | y_{t-1}, x)$). You must understand how the decoder conditions on previous tokens to generate the hierarchy.
  - **Quick check question:** How does the model determine when to stop generating labels for a specific sample?

- **Concept:** **Teacher Forcing & Exposure Bias**
  - **Why needed here:** The model is trained with ground-truth labels (teacher forcing) but tested on its own predictions. The error analysis highlights "exposure bias" (incorrect child predictions leading to incorrect parents) as a primary failure mode.
  - **Quick check question:** What happens during inference if the model predicts the wrong child label at the start of the sequence?

- **Concept:** **Label Semantics vs. Symbolic Labels**
  - **Why needed here:** RADAr explicitly removes "label semantics" (textual descriptions of labels like "Job Market") and uses symbolic IDs (e.g., `[a_14]`). Understanding this distinction is key to seeing why the model is faster and simpler but relies entirely on the training data distribution.
  - **Quick check question:** Does the model know that `[a_14]` corresponds to "Job Market" during training, or does it treat it as an arbitrary token?

## Architecture Onboarding

- **Component map:** RoBERTa encoder -> Context tensor -> 2-layer Transformer decoder -> Label sequence
- **Critical path:**
  1. **Data Prep:** Convert multi-label sets into Child $\to$ Parent sequences with `<unk>` separators.
  2. **Forward Pass:** RoBERTa encodes text $\to$ Context Tensor. Decoder attends to Context Tensor + shifted Label Sequence.
  3. **Loss:** Compute Focal Loss on batch perplexity.
- **Design tradeoffs:**
  - **Inference Speed vs. Global Context:** By removing the graph encoder, inference is 2x faster (SOTA competitive), but the model loses explicit message-passing capabilities between sibling nodes found in GNN-based approaches.
  - **Symbolic vs. Semantic:** Removing label semantics simplifies the pipeline but prevents the model from leveraging zero-shot capabilities based on label descriptions.
- **Failure signatures:**
  - **Short/Long Sequences:** On complex datasets (NYT), the model struggles to predict the `<\s>` token correctly, generating sequences that are too short (missing roots) or too long (hallucinating deep paths).
  - **Error Propagation:** A mispredicted child almost always guarantees a mispredicted parent (exposure bias).
- **First 3 experiments:**
  1. **Directionality Ablation:** Validate the core claim by training on Parent $\to$ Child vs. Child $\to$ Parent ordering on the WOS dataset.
  2. **Decoder Depth:** Test 1 vs. 2 vs. 4 decoder layers to confirm the paper's finding that 2 layers are optimal for balancing speed and hierarchy depth.
  3. **Generalization Check:** Evaluate the "children only + hierarchy" setting where the model only predicts leaf nodes and infers parents structurally post-hoc (if the hierarchy is known at test time).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the exposure bias inherent in RADAr's autoregressive decoder be effectively mitigated?
- Basis in paper: [explicit] The conclusion states, "In future work, we plan to further investigate the exposure bias problem."
- Why unresolved: The error analysis reveals that the model frequently mispredicts parent labels when a child label is generated incorrectly, propagating the error through the autoregressive process.
- What evidence would resolve it: Demonstrating reduced error propagation in parent labels using techniques like scheduled sampling or reinforcement learning.

### Open Question 2
- Question: Can the RADAr architecture be effectively generalized to non-hierarchical multi-label classification tasks?
- Basis in paper: [explicit] The conclusion notes a plan to "evaluate the model on non-hierarchical multi-label classification tasks."
- Why unresolved: The model is currently optimized to exploit label hierarchies via sequence ordering; its applicability and efficiency on flat label structures remain untested.
- What evidence would resolve it: Benchmarking RADAr's performance on standard multi-label datasets against established flat-classification baselines.

### Open Question 3
- Question: Does the fixed two-layer decoder architecture limit performance on datasets with significantly deeper label hierarchies?
- Basis in paper: [inferred] The paper notes the two-layer limit was based on pre-experiments, yet the error analysis shows increased difficulties in sequence length prediction on the NYT dataset (8 levels).
- Why unresolved: It is unclear if a shallow decoder (2 layers) possesses sufficient capacity to model the complex dependencies in deeper taxonomies without bottlenecking.
- What evidence would resolve it: Ablation studies varying decoder depth specifically on datasets with hierarchies deeper than 8 levels.

## Limitations

- **Scalability to Extremely Deep Hierarchies:** Performance on hierarchies deeper than 8 levels is untested, and the fixed 2-layer decoder may struggle with longer-range dependencies.
- **Generalization to Unseen Subtrees:** The symbolic label approach without semantics prevents zero-shot learning capabilities for dynamically added branches.
- **Decoder Architecture Sensitivity:** Limited exploration of alternative architectures and hyperparameters may affect performance on different hierarchical structures.

## Confidence

**High Confidence (8/10):** The core claim that child-to-parent generation outperforms parent-to-child ordering is well-supported by consistent experimental results across three datasets. The ablation studies clearly demonstrate this advantage, with mechanistic explanations (reduced search space through entailment) that align with the observed performance patterns.

**Medium Confidence (6/10):** The claim that removing explicit graph encoding maintains SOTA performance while halving inference time is supported but requires caution. The comparison baselines are from 2020-2022, and recent approaches using larger models or different architectures may have surpassed these results. The inference speedup is demonstrated but depends heavily on the specific graph encoder implementations used for comparison.

**Low Confidence (4/10):** The effectiveness of batch-level focal loss for mitigating label imbalance is theoretically sound but lacks comprehensive ablation. The paper shows it works but doesn't compare against standard focal loss, class-weighted cross-entropy, or other imbalance mitigation techniques. The correlation between high-loss batches and rare labels is asserted but not empirically validated.

## Next Checks

1. **Depth Scaling Experiment:** Test RADAr on a dataset with hierarchies of depth 12-15 (e.g., biomedical taxonomies or product ontologies) to evaluate whether the 2-layer decoder architecture maintains performance or requires architectural modifications. Measure both accuracy degradation and inference time scaling.

2. **Dynamic Hierarchy Evaluation:** Simulate real-world conditions by training on an older version of a hierarchy, then testing on a newer version with added branches. Compare RADAr's performance against GNN-based approaches that can leverage label semantics for zero-shot generalization to unseen paths.

3. **Focal Loss Ablation:** Implement a controlled ablation comparing batch-level focal loss against (a) standard focal loss, (b) class-weighted cross-entropy, and (c) oversampling rare labels. Quantify the specific contribution of the batch-level mechanism to overall performance, particularly on datasets with severe label imbalance.