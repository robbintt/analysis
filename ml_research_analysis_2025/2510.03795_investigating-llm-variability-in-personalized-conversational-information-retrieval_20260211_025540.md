---
ver: rpa2
title: Investigating LLM Variability in Personalized Conversational Information Retrieval
arxiv_id: '2510.03795'
source_url: https://arxiv.org/abs/2510.03795
tags:
- retrieval
- ptkb
- information
- query
- ikat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the variability of Large Language Models
  (LLMs) in personalized Conversational Information Retrieval (CIR). We rigorously
  reproduce and extend prior work on PTKB integration, applying methods to both TREC
  iKAT 2023 and 2024 datasets and evaluating diverse models including Llama, Qwen,
  and GPT variants.
---

# Investigating LLM Variability in Personalized Conversational Information Retrieval

## Quick Facts
- **arXiv ID**: 2510.03795
- **Source URL**: https://arxiv.org/abs/2510.03795
- **Reference count**: 40
- **Key outcome**: This study investigates the variability of Large Language Models (LLMs) in personalized Conversational Information Retrieval (CIR). We rigorously reproduce and extend prior work on PTKB integration, applying methods to both TREC iKAT 2023 and 2024 datasets and evaluating diverse models including Llama, Qwen, and GPT variants. Our findings reveal that human-selected PTKBs consistently improve retrieval performance, while LLM-based PTKB selection does not reliably outperform manual choices. We observe substantially higher variability in personalized CIR compared to standard CIR, with recall-oriented metrics exhibiting lower variance than precision-oriented ones. These results underscore the need for multi-run evaluations and variance reporting when assessing LLM-based CIR systems, particularly in dense and sparse retrieval settings.

## Executive Summary
This paper rigorously investigates the variability of Large Language Models in personalized Conversational Information Retrieval, specifically focusing on PTKB (Personal Textual Knowledge Base) integration. Through comprehensive experiments on TREC iKAT 2023 and 2024 datasets, the authors demonstrate that human-selected PTKBs consistently enhance retrieval performance, particularly for recall-oriented metrics. The study reveals that personalized CIR exhibits substantially higher variability compared to standard CIR, with LLM-based PTKB selection methods failing to reliably outperform manual human choices. These findings highlight the critical need for multi-run evaluations and variance reporting in assessing LLM-based CIR systems, especially when using both dense and sparse retrieval methods.

## Method Summary
The study reproduces and extends prior work on PTKB integration for personalized conversational search. The pipeline involves PTKB selection (human, automatic oracle, or various LLM-based strategies) followed by LLM query reformulation using conversation history and selected PTKB statements. The rewritten queries are then processed by either BM25 (Pyserini) or dense ANCE retrieval models. The evaluation uses TREC iKAT 2023 (176 turns) and 2024 (116 turns) datasets with ClueWeb22-B corpus, measuring MRR, NDCG@k, MAP, and Recall@1000. The authors conduct 5-10 independent runs per configuration to quantify variance, using paired T-tests for statistical significance. Six PTKB selection methods are compared: None, Use All, Human, STR, SAR, and Oracle.

## Key Results
- Human-selected PTKBs consistently improve retrieval performance, particularly for recall-oriented metrics like Recall@1000
- LLM-based PTKB selection methods do not reliably outperform human choices and show high instability across runs
- Personalized CIR exhibits substantially higher variability than standard CIR, with precision-oriented metrics showing greater variance than recall-oriented ones
- Automatic oracle selection achieves high scores on specific metrics but underperforms on others, suggesting it exploits random variance rather than learning a robust strategy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human-selected Personal Textual Knowledge Bases (PTKBs) improve retrieval performance, particularly for recall-oriented metrics.
- Mechanism: A human annotator identifies which user-specific PTKB statements (e.g., dietary constraints) are relevant to a query. This curated context is integrated into the query, typically via LLM-based rewriting, producing a more semantically specific search signal that reduces ambiguity for the retrieval model.
- Core assumption: Human annotators can reliably identify which pieces of user-specific information will be useful for disambiguating a query and retrieving relevant passages.
- Evidence anchors:
  - [abstract] "...human-selected PTKBs consistently enhance retrieval performance..."
  - [section 4.1, Table 2] Shows "Human" PTKB selection achieving statistically significant gains on Recall@1000 compared to other methods.
  - [corpus] Weak or missing direct corpus evidence for the human-selection mechanism.
- Break condition: The mechanism fails if human annotations are too noisy, if the PTKB contains no relevant information for a given query, or if the retrieval model cannot effectively use the personalized context.

### Mechanism 2
- Claim: The inherent stochasticity of Large Language Models (LLMs) in query reformulation is a primary driver of high performance variance in downstream retrieval.
- Mechanism: An LLM, prompted to rewrite a conversational query, produces different output texts even under controlled conditions (e.g., fixed seeds for some models). These variations in lexical choice and query structure cause the downstream retrieval model to produce different document rankings, leading to fluctuating metric scores.
- Core assumption: Variability in LLM output is the dominant source of performance instability in the CIR pipeline.
- Evidence anchors:
  - [abstract] "...substantially higher variability in personalized CIR... underscoring the need for multi-run evaluations..."
  - [section 1, Figure 1] Visual comparison shows higher standard deviation in MRR for personalized CIR (iKAT) versus standard CIR (CAsT).
  - [corpus] Weak or missing corpus evidence, as related papers do not analyze this variability.
- Break condition: The mechanism is irrelevant if the retrieval system is robust to minor phrasing changes or if the LLM is perfectly deterministic.

### Mechanism 3
- Claim: Automatic "oracle" PTKB selection methods, which optimize a metric (e.g., NDCG@3) by post-hoc selection from multiple random rewrites, exploit LLM variability rather than demonstrating a robust selection strategy.
- Mechanism: The automatic method generates multiple query rewrites, each incorporating a different PTKB statement, and selects the one that maximizes a target metric. This process benefits from the chance that one random reformulation will be highly effective for that specific metric, even without consistently identifying the correct PTKB.
- Core assumption: A method's ability to achieve high scores on a metric via post-hoc selection indicates a robust and generalizable strategy.
- Evidence anchors:
  - [section 4.1, Table 2] The "Automatic/Oracle" method achieves the highest scores on NDCG@3 and MRR but underperforms significantly on Recall@1000.
  - [section 5.1, Figure 5] Shows the automatic method rarely selects the same PTKB across multiple runs, indicating high instability.
  - [corpus] Weak or missing corpus evidence, as related papers do not analyze this selection method.
- Break condition: The mechanism is flawed as a practical selection strategy if it fails to converge on a consistent choice and its strong performance does not generalize to other metrics or datasets.

## Foundational Learning

- Concept: **LLM Stochasticity & Reproducibility**
  - Why needed here: The paper's core thesis is that LLM variability undermines the reproducibility of findings in personalized CIR. Understanding that LLMs are non-deterministic—even with controls—is critical for designing valid experiments.
  - Quick check question: If you run the same prompt through an LLM twice with temperature=0, will you always get the exact same response? (Answer: Often no, especially for closed-source APIs or complex models).

- Concept: **Retrieval Evaluation Metrics (Precision vs. Recall)**
  - Why needed here: The paper reveals a key difference: precision-oriented metrics (e.g., NDCG@3) are highly sensitive to LLM variability, while recall-oriented metrics (e.g., Recall@1000) are more stable and crucial for evaluating first-stage retrieval.
  - Quick check question: A system returns 10 documents, 5 of which are relevant. The total number of relevant documents in the entire corpus is 20. What are the system's Precision@10 and Recall@10? (Answer: Precision@10 = 0.5, Recall@10 = 0.25).

- Concept: **Query Rewriting in Conversational Search**
  - Why needed here: This is the technique central to the paper's pipeline. It transforms a context-dependent conversational query into a self-contained, standalone query suitable for a standard retriever, often by incorporating history and, in this case, personalization context.
  - Quick check question: The user asks "How about its capital?" after discussing France. What would a query rewriter produce? (Answer: A self-contained query like "What is the capital of France?").

## Architecture Onboarding

- Component map:
  1. PTKB Source: Stores user-specific free-text statements (e.g., "User is a vegetarian")
  2. Selection Module: A component (Human, STR, SAR) that chooses a subset of PTKB statements relevant to the current turn
  3. Reformulation LLM: An LLM (GPT, Llama, Qwen) that takes the conversation history, current query, and selected PTKB to generate a self-contained, rewritten query
  4. First-Stage Retriever: A search model (e.g., sparse BM25 or dense ANCE) that takes the rewritten query and returns a ranked list of passages from the corpus
  5. Evaluator: Computes retrieval metrics (MRR, NDCG@k, Recall@k) by comparing retrieved passages against ground-truth relevance judgments

- Critical path:
  1. Receive conversational query `q_n` and PTKB `U`
  2. The Selection Module identifies relevant PTKB statements `s_t` based on `q_n` and conversation history `H`
  3. The Reformulation LLM generates a standalone query `q'_n` using `q_n`, `H`, and selected `s_t`
  4. The First-Stage Retriever executes a search with `q'_n` to produce a ranked list of documents `D`
  5. The Evaluator scores `D` against ground-truth relevance judgments

- Design tradeoffs:
  - STR vs. SAR: STR (Select-Then-Reformulate) decomposes the task into two LLM calls, improving interpretability. SAR (Select-And-Reformulate) uses a single call, which is more efficient but less transparent
  - BM25 vs. Dense Retrieval: The paper finds sparse BM25 outperforms dense ANCE, partly due to the dataset's construction (judgment pool bias). BM25 is also more robust to the longer, concatenated queries generated by the pipeline
  - Open-source vs. Closed-source LLMs: Open-source models (Llama) allow for full determinism with fixed seeds, aiding reproducibility. Closed-source models (GPT) are more powerful but are inherently stochastic, complicating evaluation

- Failure signatures:
  - High Metric Variance: NDCG@3 fluctuates by >2% across repeated runs with identical settings. Signature of uncontrolled LLM stochasticity
  - Recall/Precision Discrepancy: A method achieves high NDCG@3 but low Recall@1000. Signature of overfitting to a specific metric or "lucky" top-ranked results, not robust retrieval
  - Non-Convergent Automatic Selection: The automatic/oracle method selects different PTKB statements for the same query on different runs. Signature of exploiting random chance rather than learning a consistent strategy

- First 3 experiments:
  1. Quantify Baseline Variability: Run the full pipeline (with 'None' and 'Human' PTKB selection) 10 times using a single LLM (e.g., GPT-4o-mini). Report mean and standard deviation for all metrics to establish the system's inherent noise floor
  2. Ablate Selection Strategies: Implement and compare the performance and variance of the 'Use All', 'STR', and 'SAR' selection methods. Determine if any LLM-based method can consistently outperform the 'None' baseline when variance is accounted for
  3. Model Scaling Analysis: Substitute the primary LLM with smaller open-source models (e.g., Llama-8B, Qwen-7B). Use few-shot in-context learning (ICL) and measure if performance improves and how variance compares to the larger closed-source models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed LLM variability in query reformulation propagate to or get mitigated by downstream neural re-ranking stages?
- Basis in paper: [explicit] The authors state in the Limitations section that "future work could explore reranking and other paradigms for retrieval," noting the current study is limited to first-stage retrieval.
- Why unresolved: The study establishes that variance is high in first-stage retrieval (BM25, ANCE), but it is unknown if subsequent, computationally intensive re-rankers amplify this noise or smooth it out through deeper semantic analysis.
- What evidence would resolve it: A multi-stage pipeline evaluation measuring metric variance (e.g., NDCG@10, MAP) after applying a cross-encoder or LLM-based re-ranker to the variable first-stage outputs.

### Open Question 2
- Question: Can prompt engineering specifically reduce output variance and improve the reproducibility of personalized query rewriting?
- Basis in paper: [explicit] The Limitations section suggests that "refining [prompt] inputs could further enhance performance and potentially reduce variance across runs," as the study retained original prompts for reproducibility.
- Why unresolved: The study identifies high sensitivity to prompt phrasing but does not test if optimized or more explicit prompt structures can stabilize the stochastic behavior of the LLMs.
- What evidence would resolve it: An ablation study comparing the standard deviation of retrieval metrics across multiple runs using various prompt styles (e.g., chain-of-thought, constrained decoding) versus the original prompt.

### Open Question 3
- Question: Do these findings regarding PTKB integration and variability generalize to real-world scenarios involving culturally nuanced or under-represented user profiles?
- Basis in paper: [explicit] The authors explicitly note that they "only experimented on iKAT," and that "real-world scenarios where user personalization is culturally nuanced or under-represented remain an open challenge."
- Why unresolved: The TREC iKAT datasets may not capture the full complexity of cross-cultural personalization or the data sparsity often found in production systems, limiting the ecological validity of the results.
- What evidence would resolve it: Evaluation of the proposed PTKB integration methods on diverse, non-Western-centric conversational datasets or datasets specifically designed with low-resource user attributes.

## Limitations
- The study relies on unspecified prompt templates for LLM query reformulation, which may affect reproducibility
- The iKAT datasets contain only 176 and 116 conversational turns, limiting statistical power for variance analysis
- Evaluation methodology uses single-run comparisons despite identifying variance as a critical issue
- Closed-source models (GPT) cannot be fully controlled, making precise variance attribution challenging

## Confidence
**High Confidence** (Level 3):
- Human-selected PTKBs consistently improve recall-oriented metrics across multiple runs and models
- Personalized CIR exhibits substantially higher variability than standard CIR
- Recall-oriented metrics show lower variance than precision-oriented metrics

**Medium Confidence** (Level 2):
- Automatic oracle selection does not provide robust strategy beyond exploiting random variance
- BM25 retrieval outperforms dense ANCE for concatenated query formats used in the pipeline
- Performance differences between STR and SAR selection strategies

**Low Confidence** (Level 1):
- Specific variance magnitude estimates across all model configurations
- Generalization of findings to larger conversational datasets
- Exact impact of prompt template variations on performance

## Next Checks
1. **Prompt Template Replication**: Implement and test multiple prompt template variants for STR/SAR strategies to assess sensitivity of results to prompt formulation choices.
2. **Dataset Expansion Analysis**: Apply the methodology to larger conversational search datasets (e.g., CAsT) to validate whether observed variance patterns hold with increased sample sizes.
3. **Deterministic Model Comparison**: Conduct head-to-head comparisons between controlled open-source models and uncontrolled GPT models across identical tasks to isolate the impact of model determinism on variance measurements.