---
ver: rpa2
title: 'AdapSNE: Adaptive Fireworks-Optimized and Entropy-Guided Dataset Sampling
  for Edge DNN Training'
arxiv_id: '2508.16647'
source_url: https://arxiv.org/abs/2508.16647
tags:
- dataset
- training
- adapsne
- accuracy
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of training deep neural networks
  (DNNs) on resource-constrained edge devices, where conventional training methods
  impose prohibitive computational overhead due to the large scale of datasets. The
  authors propose AdapSNE, a DNN-free dataset sampling framework that leverages Fireworks
  Algorithm (FWA)-optimized t-SNE for dimensionality reduction and entropy-guided
  optimization to ensure uniform sampling, thereby addressing issues of outliers and
  representative bias in the state-of-the-art NMS method.
---

# AdapSNE: Adaptive Fireworks-Optimized and Entropy-Guided Dataset Sampling for Edge DNN Training

## Quick Facts
- **arXiv ID**: 2508.16647
- **Source URL**: https://arxiv.org/abs/2508.16647
- **Reference count**: 40
- **Primary result**: 4.4%-14.4% accuracy improvement over DNN-free NMS on image datasets

## Executive Summary
This paper tackles the challenge of training deep neural networks (DNNs) on resource-constrained edge devices, where conventional training methods impose prohibitive computational overhead due to the large scale of datasets. The authors propose AdapSNE, a DNN-free dataset sampling framework that leverages Fireworks Algorithm (FWA)-optimized t-SNE for dimensionality reduction and entropy-guided optimization to ensure uniform sampling, thereby addressing issues of outliers and representative bias in the state-of-the-art NMS method. A custom hardware accelerator with dataflow optimization and time-multiplexing is designed to reduce on-device training energy and silicon area. Experimental results demonstrate that AdapSNE significantly outperforms both DNN-based (DQAS) and DNN-free (NMS) methods, achieving accuracy improvements of 4.4%-14.4% on image datasets and 2.1%-4.1% on LLM benchmarks across varying dataset scales and keeping ratios.

## Method Summary
AdapSNE is a DNN-free dataset sampling framework that replaces binary search in t-SNE with the Fireworks Algorithm (FWA) to handle non-monotonicity in perplexity error, and adds an entropy-guided feedback loop to adaptively tune the target perplexity for uniform sampling. The FWA optimizes the bandwidth parameter σi in the t-SNE similarity function, while grid entropy is used as a feedback signal to iteratively adjust the target perplexity until a uniform distribution is achieved. A custom hardware accelerator with time-multiplexed entropy calculation and FWA-specific dataflow optimization is proposed to make the iterative process feasible on edge devices. The final sampled subset is used to train downstream models such as ResNet-18 or LLaMA-7B.

## Key Results
- 4.4%-14.4% accuracy improvement over DNN-free NMS on image datasets (CIFAR-10/100, CINIC-10, SVHN, ImageNet-100/1K)
- 2.1%-4.1% improvement over DQAS on LLM fine-tuning benchmarks (Alpaca)
- Hardware accelerator achieves 2.7% power overhead and reduces energy consumption through time-multiplexed entropy calculation

## Why This Works (Mechanism)

### Mechanism 1: Non-Monotonic Error Suppression via FWA
- **Claim**: Replacing binary search or differential evolution with the Fireworks Algorithm (FWA) during t-SNE dimensionality reduction appears to reduce the emergence of outliers in the low-dimensional embedding space.
- **Mechanism**: The authors mathematically demonstrate that the perplexity error function |Ri(σi) - Πt| is non-monotonic (derivative analysis in Section III.B). Traditional search methods assume monotonicity and fail, leading to incorrect σi values which manifest as outliers. FWA uses an "explosion" strategy to explore the non-monotonic landscape globally before narrowing down, ensuring the derived σi actually minimizes the perplexity error.
- **Core assumption**: The search space for σi is sufficiently bounded or regular that FWA sparks do not diverge, and that FWA convergence is faster/cheaper than the differential evolution used in prior work (NMS).
- **Evidence anchors**:
  - [Section III.B]: "This non-monotonicity leads to failures in locating the function’s optimal value... resulting in outliers."
  - [Fig. 1]: Visual comparison showing "Search Failure" leading to outliers vs "Optim. Success."
  - [corpus]: Related work "NMS" (arXiv:2508.02313) is cited as the specific baseline suffering from this mismatch.
- **Break condition**: If the dataset X is extremely high-dimensional such that the search space for σi becomes excessively large, FWA may require too many iterations (sparks) to converge, negating energy benefits.

### Mechanism 2: Closed-Loop Perplexity Tuning via Entropy
- **Claim**: Using grid entropy as a feedback signal to adaptively tune the target perplexity (Πt) correlates with improved sampling uniformity and training accuracy compared to static, empirically set values.
- **Mechanism**: Instead of fixing Πt (which dictates the "scale" of the manifold view), the system calculates the entropy H of the grid distribution of points in the reduced space Y. If H < H0 (indicating uneven distribution/clustering), the system updates Πt using the ratio ΔH / ΔΠt and re-runs the reduction.
- **Core assumption**: Maximizing grid entropy (uniformity) in the low-dimensional space is a valid proxy for minimizing representative bias in the high-dimensional space for the downstream task.
- **Evidence anchors**:
  - [Fig. 2]: Contrast between "Open-Loop" (arbitrary setting) and "Closed-Loop Adaptive Sampling."
  - [Algorithm 1]: Defines the update rule Πk+1t = Πkt + ... based on entropy change.
  - [Section III.E]: "Entropy is highly sensitive... in detecting outliers and distribution abnormalities."
- **Break condition**: If the data naturally has a highly imbalanced class distribution, enforcing uniform grid entropy might force the sampling to select "noisy" or unrepresentative outliers to fill grid cells, degrading accuracy.

### Mechanism 3: Iterative Compute Reduction via Time-Multiplexing
- **Claim**: The proposed hardware accelerator seems to mitigate the energy overhead of the iterative FWA and entropy loops, making the "closed-loop" approach feasible on edge devices.
- **Mechanism**: FWA requires evaluating fitness for many "sparks." The accelerator uses a custom dataflow (SPK/POP/MUT RAMs) to parallelize this without instruction-fetch overhead. The entropy module uses time-multiplexing to reuse adder/logic circuits across grid cells, reducing silicon area relative to a fully parallel implementation.
- **Core assumption**: The latency added by time-multiplexing the entropy calculation is less critical than the area/power savings gained, fitting the "resource-constrained" edge constraint.
- **Evidence anchors**:
  - [Section III.D.3]: "Avoiding the instruction-fetching and decoding processes... improving computational efficiency."
  - [Section V.A]: "Added FWA Search and entropy-guided optimization engines incur only modest overhead—2.7% of total power."
  - [corpus]: General edge constraints (memory/battery) are standard context; this specific architectural breakdown is unique to the paper.
- **Break condition**: If the required grid resolution (g × g) increases drastically, the time-multiplexed entropy calculation could become a bottleneck, stalling the FWA update loop.

## Foundational Learning

- **Concept: t-SNE (t-Distributed Stochastic Neighbor Embedding)**
  - **Why needed here**: The entire sampling pipeline relies on projecting high-dimensional data to low dimensions while preserving local structure (neighbors). The paper modifies how t-SNE parameters (σi, Πt) are found.
  - **Quick check question**: How does the choice of perplexity (Πt) affect the "view" of the data (i.e., clustering vs. sparse)?

- **Concept: Fireworks Algorithm (FWA)**
  - **Why needed here**: This is the heuristic search engine replacing binary search. Understanding "explosion" (local search around good solutions) vs. "mutation" (random global search) is key to debugging convergence.
  - **Quick check question**: In FWA, does a "good" firework generate more sparks or fewer sparks with a smaller amplitude?

- **Concept: Information Entropy (Shannon Entropy)**
  - **Why needed here**: This is the metric for "quality" of the sampling distribution. You must understand that high entropy ≈ uniform distribution (maximum uncertainty/randomness).
  - **Quick check question**: If all data points fall into a single grid cell, what is the entropy? (Answer: 0 / Minimum).

## Architecture Onboarding

- **Component map**: Dataset X in DRAM -> FWA Module (RAND generator -> SPK RAM -> Evaluate Module -> POP/MUT RAM) -> Entropy Module (Reconfigurable Unit -> Shared RAM -> Grid logic) -> Controller (manages loop between FWA and Entropy)
- **Critical path**: The Iterative Feedback Loop: Compute Y (t-SNE) -> Calculate Entropy H -> Check Threshold H0 -> Update Πt -> Re-compute Y. The system stalls until H > H0.
- **Design tradeoffs**:
  - **Accuracy vs. Latency**: The closed-loop entropy optimization improves accuracy (abstract: +4.4% to +14.4%) but requires re-running t-SNE multiple times until convergence of H.
  - **Area vs. Parallelism**: The entropy module uses time-multiplexing (sequential processing of grid cells) to save area, trading off raw throughput for silicon efficiency.
- **Failure signatures**:
  - **"Empty Sampling" (Outliers)**: If the FWA module gets stuck in a local minimum of the perplexity error function, the reduced space Y will have sparse outliers, causing grid cells to be empty.
  - **Oscillation**: If the ΔH / ΔΠt update step is too aggressive, the target perplexity Πt might oscillate around H0 without ever triggering the final grid sampling.
- **First 3 experiments**:
  1. **Convergence Validation**: Run AdapSNE on a small subset (e.g., CIFAR-10, 10% ratio) and plot the "Entropy vs. Iteration" curve to verify the feedback loop actually converges to H > H0.
  2. **Ablation on FWA**: Compare the standard t-SNE binary search vs. the FWA search on the *same* dataset; specifically, measure the number of "outliers" (points with > avg distance to nearest neighbor) to validate Section III.B's claim.
  3. **Hardware Stress Test**: Increase the dataset size N and grid resolution g to observe the memory usage in the "Shared RAM" and latency in the "Time-Multiplexed" entropy unit to verify the edge-device feasibility claims in Section V.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Is the empirically set entropy threshold (H0 using α=80%) robust against datasets with severe class imbalance or non-uniform intrinsic distributions?
- **Basis in paper**: [Explicit] The description of Algorithm 1 states: "The threshold H0 is set to... We set α to 80% based on [31], which indicates that the distribution is sufficiently even."
- **Why unresolved**: A fixed threshold assumes a universal target for "evenness." In long-tailed datasets, forcing uniform entropy might cause the algorithm to iterate unnecessarily or select outliers to fill "empty" grids, potentially misrepresenting the true data density.
- **What evidence would resolve it**: A sensitivity analysis on datasets with known skew (e.g., ImageNet-LT) showing accuracy and convergence speed as α is varied.

### Open Question 2
- **Question**: Does the reliance on Euclidean distance in the high-dimensional probability distribution (Eq. 1) limit performance when sampling for LLM fine-tuning compared to semantic metrics?
- **Basis in paper**: [Inferred] Section III-A defines similarity pj|i using Euclidean distance ||xi - xj||2, while Section IV-D applies this to LLM embeddings.
- **Why unresolved**: The paper demonstrates accuracy improvements but does not ablate the distance metric. Text embeddings often reside in high-dimensional spaces where cosine similarity correlates better with semantic meaning than Euclidean magnitude.
- **What evidence would resolve it**: Comparative experiments on the MMLU benchmark substituting the Euclidean kernel in Eq. 1 with a cosine-based similarity kernel.

### Open Question 3
- **Question**: Does the strict enforcement of uniform sampling risk destroying meaningful local topological structures (clusters) present in the original data?
- **Basis in paper**: [Inferred] Section III-E states the method employs entropy-guided optimization to "enforce uniform sampling," and Fig. 2 contrasts "Unevenly Distributed" with "Evenly Distributed" as the ideal outcome.
- **Why unresolved**: While uniformity prevents empty sampling grids, natural datasets often form distinct clusters. Artificially dispersing these clusters to maximize entropy could degrade the local manifold structure required for fine-grained classification tasks.
- **What evidence would resolve it**: Evaluating "Trustworthiness" and "Continuity" metrics (mentioned in Section III-E-1) for AdapSNE to verify if local neighborhood preservation is sacrificed for global uniformity.

## Limitations

- **Hyperparameter Dependency**: The effectiveness of AdapSNE critically depends on the specific FWA parameters (number of sparks, explosion amplitude) and entropy threshold H0, which are not fully specified in the paper, making direct replication challenging.
- **Data Distribution Assumption**: The entropy-guided sampling assumes that maximizing grid uniformity correlates with representative sampling. This may not hold for naturally imbalanced datasets where forcing uniformity could introduce noisy samples.
- **Hardware Scalability**: While the time-multiplexed entropy module saves area, it may become a bottleneck for very large grid resolutions or datasets, potentially limiting the approach's scalability on edge devices.

## Confidence

- **High**: The core mechanism of using FWA to handle non-monotonicity in t-SNE perplexity error is well-supported by mathematical analysis and visual evidence. The entropy-guided loop structure is clearly defined.
- **Medium**: The claimed accuracy improvements (4.4%-14.4%) and hardware efficiency gains are supported by experiments, but the specific hyperparameter settings and their sensitivity are not fully disclosed.
- **Low**: The generalizability of the approach to datasets with highly imbalanced class distributions or extremely high dimensionality is not thoroughly validated.

## Next Checks

1. **Convergence Validation**: Run AdapSNE on a small subset (e.g., CIFAR-10, 10% ratio) and plot the "Entropy vs. Iteration" curve to verify the feedback loop actually converges to H > H0.
2. **Ablation on FWA**: Compare the standard t-SNE binary search vs. the FWA search on the *same* dataset; specifically, measure the number of "outliers" (points with > avg distance to nearest neighbor) to validate Section III.B's claim.
3. **Hardware Stress Test**: Increase the dataset size N and grid resolution g to observe the memory usage in the "Shared RAM" and latency in the "Time-Multiplexed" entropy unit to verify the edge-device feasibility claims in Section V.