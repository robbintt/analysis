---
ver: rpa2
title: 'MODS: Moderating a Mixture of Document Speakers to Summarize Debatable Queries
  in Document Collections'
arxiv_id: '2502.00322'
source_url: https://arxiv.org/abs/2502.00322
tags:
- topic
- mods
- summary
- arxiv
- perspectives
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MODS (Mixture of Document Speakers) is a multi-LLM system designed
  to address Debatable Query-Focused Summarization (DQFS), where summaries must comprehensively
  cover and balance opposing perspectives in documents responding to yes/no queries.
  MODS treats each document as an individual "Speaker" LLM and uses a "Moderator"
  LLM to select speakers and tailor queries for planned topics, creating a structured
  outline of perspectives that guides the final summary.
---

# MODS: Moderating a Mixture of Document Speakers to Summarize Debatable Queries in Document Collections

## Quick Facts
- arXiv ID: 2502.00322
- Source URL: https://arxiv.org/abs/2502.00322
- Reference count: 40
- MODS outperforms state-of-the-art baselines by 38-59% in topic paragraph coverage and balance while maintaining readability and summary quality.

## Executive Summary
MODS (Mixture of Document Speakers) addresses Debatable Query-Focused Summarization (DQFS) by treating each document as an individual "Speaker" LLM and using a "Moderator" LLM to select speakers and tailor queries for planned topics. This multi-LLM architecture creates a structured outline of perspectives that guides the final summary, achieving 38-59% improvement in topic paragraph coverage and balance metrics compared to baselines. Users find MODS's outputs more balanced and informative while maintaining summary quality.

## Method Summary
MODS is a multi-LLM system that generates balanced summaries for yes/no queries by partitioning the task across document-specific speakers. The system retrieves document biographies using ColBERT, plans topics via an agenda planner, selects relevant speakers through a moderator LLM, and generates tailored queries for each speaker. Each speaker retrieves contexts and extracts yes/no perspectives, which are aggregated into a structured outline. A final summarizer converts this outline into the summary, either all at once or per topic.

## Key Results
- MODS outperforms baselines by 38-59% in topic paragraph coverage (DC) and fairness metrics
- Human evaluations show MODS produces more balanced and informative summaries
- MODS-Topic variant achieves higher coverage than MODS-All through per-topic summarization
- The system maintains readability while improving comprehensive perspective coverage

## Why This Works (Mechanism)

### Mechanism 1: Document Speaker Partitioning
Treating each document as an individual LLM speaker prevents any single source from dominating the summary and improves coverage of minority perspectives. Each document gets its own dedicated inference pass where a Speaker LLM retrieves relevant contexts and extracts perspectives. This avoids the "lost in the middle" problem where LLMs ignore documents positioned mid-prompt and prevents parametric memory from overriding documents that oppose the LLM's prior beliefs.

### Mechanism 2: Tailored Query Generation via Moderator
Generating document-specific queries improves retrieval of relevant contexts compared to using the original debatable query for all documents. A Moderator LLM reviews each document's biography and generates a tailored query that aligns with that document's unique content. This compensates for vocabulary mismatch between the broad debatable query and document-specific terminology.

### Mechanism 3: Structured Outline as Content Plan
Building a structured outline before summarization improves balance and coverage compared to directly generating free-form text. Speaker perspectives are organized into an outline with explicit topic-document-query-perspective tuples. This structure forces the final summarizer to acknowledge all extracted perspectives rather than collapsing them into a biased narrative. The outline also tracks yes/no stance labels for each perspective.

## Foundational Learning

- **Query-Focused Summarization (QFS)**: MODS extends QFS to debatable queries with opposing perspectives. Understanding baseline QFS assumptions (single correct answer) clarifies why MODS's innovations matter.
  - Quick check: Can you explain why standard QFS systems fail when documents contain equally-valid opposing answers?

- **Late-Interaction Retrieval (ColBERT)**: MODS relies on ColBERT for creating document biographies and context retrieval. ColBERT's contextualized late interaction differs from dense retrieval by preserving token-level representations.
  - Quick check: Why might ColBERT outperform BM25 for creating document biographies in this architecture?

- **Multi-Agent LLM Systems**: MODS implements a multi-turn single-speaker interaction pattern. Understanding coordination patterns (sequential vs. parallel) helps debug latency and ordering effects.
  - Quick check: If you parallelized all Speaker LLM calls, what information would each speaker lose?

## Architecture Onboarding

- **Component map**: Agenda Planner -> Moderator LLM -> Speaker LLMs (N instances) -> Outline Store -> Summarizer
- **Critical path**: 1) Retrieve biographies (ColBERT) → Plan topics (Agenda Planner) 2) For each topic: Select speakers + tailor queries (Moderator) 3) For each selected speaker: Retrieve contexts + extract perspectives (Speaker LLM) 4) Aggregate into outline → Generate final summary (Summarizer)
- **Design tradeoffs**: MODS-All vs. MODS-Topic (all-at-once vs. per-topic summarization); Speaker selection vs. all speakers (efficiency vs. completeness); Outline as intermediate output (cost vs. user-explorability)
- **Failure signatures**: Low coverage but high balance (agenda planning excludes document themes); High coverage but low balance (speaker stance detection failing); Readability degradation (too many citations per sentence); Stance label noise (80% accuracy reported)
- **First 3 experiments**: 1) Ablate query tailoring (set tailored_query = original debatable query for all speakers) 2) Vary k (retrieval contexts) test k=1, 3, 5 3) Test with smaller LLM (GPT-4-mini) to identify which component fails most

## Open Questions the Paper Calls Out

### Open Question 1
Would implementing multi-turn debates between Speaker LLMs in the MODS framework improve summary quality? The current implementation limits speaker interaction to a single turn, preventing iterative refinement or rebuttal of perspectives before the final outline is fixed.

### Open Question 2
How can the MODS framework be adapted to handle input documents containing factual misinformation? The system currently assumes input documents are factual and written in good faith, lacking a mechanism to filter or correct hallucinations in the source text or outline.

### Open Question 3
Can the MODS system be modified to generate summaries that align with or challenge a specific user's perspective? The current DQFS task definition strictly mandates that summaries must be balanced and favor no side, ignoring user-specific biases or preferences.

## Limitations
- Evaluation relies on automatic citation metrics without human validation of citation accuracy
- Stance detection accuracy (~80%) introduces potential bias in balance metrics
- High cost (~50% overhead vs. baselines) and sequential LLM calls may limit practical deployment

## Confidence
- **High Confidence**: Document partitioning mechanism (improved coverage and balance metrics)
- **Medium Confidence**: Query tailoring benefit (modest improvement in coverage metrics)
- **Medium Confidence**: Structured outline advantage (balance metrics require further validation given stance detection accuracy concerns)

## Next Checks
1. **Citation Accuracy Audit**: Manually verify 50 random citations across MODS outputs to assess whether cited documents actually support the perspectives claimed.
2. **Cost-Benefit Analysis**: Profile MODS runtime vs. Hierarchical Merging on a representative sample to quantify the claimed 50% overhead and assess whether coverage gains justify the cost.
3. **Stance Detection Impact**: Re-run the ablation study while injecting controlled stance label errors to measure how sensitive balance metrics are to stance detection accuracy.