---
ver: rpa2
title: 'Natural Language Satisfiability: Exploring the Problem Distribution and Evaluating
  Transformer-based Language Models'
arxiv_id: '2508.17153'
source_url: https://arxiv.org/abs/2508.17153
tags:
- language
- satisfiability
- problem
- tlms
- fragments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates transformer-based language models' ability
  to solve natural language satisfiability problems across fragments of English with
  varying computational complexity. Five language fragments are defined, corresponding
  to fragments of first-order logic with complexity ranging from NLOGSPACE-complete
  to NEXPTIME-complete.
---

# Natural Language Satisfiability: Exploring the Problem Distribution and Evaluating Transformer-based Language Models

## Quick Facts
- arXiv ID: 2508.17153
- Source URL: https://arxiv.org/abs/2508.17153
- Reference count: 21
- Key outcome: Transformer-based language models show performance degradation correlating with computational complexity class of natural language satisfiability problems, from 88.3% accuracy for simplest fragment to 70.1% for most complex, failing to generalize to problems with more predicates than seen during training

## Executive Summary
This paper investigates transformer-based language models' ability to solve natural language satisfiability problems across five English fragments with varying computational complexity. The authors define fragments corresponding to first-order logic complexity classes from NLOGSPACE-complete to NEXPTIME-complete, empirically determine phase-change regions where problems are most challenging, and construct datasets accordingly. After fine-tuning T5-large and DeBERTa-v3-large models on these datasets, they find that model accuracy decreases as computational complexity increases, with models failing to generalize to problems with more predicates than seen during training. In zero-shot settings, even large models like GPT-4 achieve only modest performance, suggesting transformers are far from reliably learning logical inference rules despite their impressive capabilities.

## Method Summary
The study defines five English language fragments (S, W, V, Z, A) corresponding to first-order logic complexity classes from NLOGSPACE to NEXPTIME. For each fragment, the authors empirically determine phase-change regions where satisfiability probability is near 0.5, then generate datasets using a Z3 theorem prover to label truth values. They fine-tune T5-large (700M parameters) and DeBERTa-v3-large (304M parameters) models on these datasets with binary cross-entropy loss, Adam optimizer, 5 epochs, and batch size 24. Models are evaluated on same-fragment, cross-fragment, and scale generalization tasks, with zero-shot evaluation on LLaMA-2 and GPT-4. The vocabulary consists of 156 nouns and 70 transitive verbs, with dataset sizes of 120K train, 10K eval, and 10K test instances per fragment.

## Key Results
- Model accuracy decreases systematically as computational complexity increases: 88.3% (Fragment S, NLOGSPACE) to 70.1% (Fragment A, NEXPTIME)
- Models fail to generalize to problems with more predicates than seen during training, suggesting they learn surface patterns rather than general inference rules
- Zero-shot performance of even large models (GPT-4) remains modest, with accuracy declining as the number of variables increases
- Transfer learning shows some benefit: training on simpler fragments improves performance on complex fragments, but requires large joint datasets (600K instances)

## Why This Works (Mechanism)

### Mechanism 1: Complexity-Class Dependency
Transformer-based Language Models (TLMs) exhibit performance degradation correlating with the computational complexity class of satisfiability problems. Fragments in lower complexity classes (like Fragment S, NLOGSPACE) often require detecting simple, local "forbidden configurations" to determine unsatisfiability, which TLMs can approximate via statistical pattern matching. Higher-complexity fragments (like Fragment A, NEXPTIME) involve intricate relational structures that cannot be solved by simple local checks, pushing TLMs beyond their effective capacity for "algorithm emulation."

### Mechanism 2: Phase-Change Data Sampling
Evaluating logical reasoning requires sampling problem instances from the "phase-change" region where satisfiability probability is near 0.5, rather than random sampling. Randomly generated satisfiability problems are often trivial (easily satisfiable or trivially unsatisfiable) due to degrees of freedom. Constraining data to the phase-change region forces the model to engage in genuine reasoning rather than learning simple heuristics.

### Mechanism 3: Inference Pattern Transfer
Training on computationally simpler fragments can improve performance on more complex fragments when sufficient data volume is provided. Complex natural language fragments are supersets of simpler ones, and sentences in complex fragments often embed syllogistic structures found in simpler fragments. When trained on a joint dataset, the model learns inference rules of simpler subsets that act as "subroutines" for solving harder problems.

## Foundational Learning

- **Concept: Phase-Change Region (Critical Threshold)**
  - **Why needed here:** To distinguish between a model "guessing based on constraints" vs. actually "reasoning." In SAT problems, difficulty doesn't scale linearly; it peaks where constraints tighten.
  - **Quick check question:** If a model has 95% accuracy on a SAT dataset, does that mean it reasons well? (Not necessarily; check if the dataset excludes the phase-change region).

- **Concept: Computational Complexity Classes (NL vs NP vs NEXPTIME)**
  - **Why needed here:** To interpret the performance ceilings. A problem in NEXPTIME (Fragment A) is fundamentally harder than NL (Fragment S).
  - **Quick check question:** Why is the drop from S (NLOGSPACE) to W (NPTIME) significant? (It marks the transition from deterministic log-space verification to non-deterministic polynomial time, often correlated with the need for search/backtracking).

- **Concept: Systematic Generalization**
  - **Why needed here:** The paper explicitly tests if models can generalize to $n > n_{train}$.
  - **Quick check question:** Can a model trained on equations with 3 variables solve one with 10 variables? (This paper suggests TLMs generally cannot, failing scale-invariance).

## Architecture Onboarding

- **Component map:** Data Engine (Z3 Theorem Prover + NL Generator) -> Encoder (T5-large or DeBERTa-v3) -> Binary Classification (Satisfiable vs. Unsatisfiable)

- **Critical path:**
  1. Define Language Fragment (e.g., Sentence Templates)
  2. Empirically determine Phase-Change Region λL for the fragment
  3. Generate/Label Data using Z3 (bottleneck is solver time in λL)
  4. Fine-tune TLM

- **Design tradeoffs:**
  - **Fragment Complexity vs. Data Efficiency:** High-complexity fragments (A) require significantly more training data (600k) to see benefits from transfer learning compared to simple fragments
  - **Accuracy vs. Generalization:** High test accuracy within training distribution ($n \le 16$) does not guarantee out-of-distribution generalization ($n \ge 20$)

- **Failure signatures:**
  - **Scale Failure:** Accuracy drops precipitously when $n_{variables}$ increases slightly beyond training max (Table 3)
  - **Instruction Following Failure:** LLaMA-2 fails to output specific tokens ("satisfiable"/"unsatisfiable") in zero-shot, defaulting to generic continuations

- **First 3 experiments:**
  1. **Probe Phase Sensitivity:** Train T5 on "easy" regions (probability of sat ≈ 0.1 or 0.9) vs. "hard" regions (0.5). Verify if "easy" model relies on shortcuts
  2. **Cross-Fragment Transfer:** Train on Fragment S (Syllogistic) only, then test on Fragment W (Relative Clauses). Quantify performance gap to isolate cost of grammatical generalization
  3. **Scale Extrapolation:** Train on $n \in [6, 12]$ and test on $n=13, 14, \dots, 20$. Plot decay curve to determine if failure is sudden (catastrophic) or gradual

## Open Questions the Paper Calls Out

- What specific architectural modifications or training regimes enable TLMs to systematically generalize inference rules to problem instances with larger vocabularies than encountered during training? (The authors acknowledge that generalization aspects of TLMs may need deeper analysis)

- How do neuro-symbolic or recurrence-augmented architectures perform on natural language satisfiability tasks compared to standard T5 and DeBERTa models? (The study is restricted to standard transformer architectures; it remains unknown if architectures designed for algorithmic reasoning can bridge the performance gap)

- Does the empirically identified phase-change region for natural language fragments correlate strictly with computational hardness (e.g., theorem prover runtime), or does it admit "trivial" instances? (The sampling method does not guarantee all instances are non-trivial, potentially inflating performance metrics)

## Limitations

- The phase-change region parameters (λL values) for each fragment are empirically determined but not explicitly provided, requiring re-estimation for reproduction
- Zero-shot results are reported for GPT-4 but not for other large language models, limiting generalizability of instruction-following findings
- The study focuses on a specific vocabulary (156 nouns, 70 transitive verbs) which may not capture full complexity of natural language reasoning

## Confidence

- **High Confidence:** The correlation between computational complexity class and model performance degradation is well-supported by experimental results
- **Medium Confidence:** The conclusion that TLMs fail to learn general inference rules is supported but could benefit from ablation studies on transfer learning mechanisms
- **Low Confidence:** The specific failure modes for zero-shot inference (particularly for models other than GPT-4) are not thoroughly explored

## Next Checks

1. **Cross-Model Zero-Shot Validation:** Test additional zero-shot models (Claude, Gemini) with modified prompt formats to verify instruction-following failure pattern is not model-specific
2. **Transfer Learning Ablation:** Train models on subsets of simpler fragments (S, W) only, then evaluate transfer performance on complex fragments to isolate benefit of specific inference rule learning
3. **Phase-Change Region Sensitivity:** Systematically vary phase-change region boundaries (e.g., [0.3, 0.7] vs [0.35, 0.65]) to test robustness of difficulty sampling methodology and its impact on model performance metrics