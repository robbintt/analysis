---
ver: rpa2
title: Vehicle-to-Infrastructure Collaborative Spatial Perception via Multimodal Large
  Language Models
arxiv_id: '2509.03837'
source_url: https://arxiv.org/abs/2509.03837
tags:
- spatial
- prediction
- multimodal
- link
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multimodal large language model (MLLM)-based
  framework for vehicle-to-infrastructure (V2I) link quality prediction. The key idea
  is to address the lack of spatial understanding in MLLMs by fusing sensor data from
  neighboring vehicles into a unified bird's-eye view (BEV) representation.
---

# Vehicle-to-Infrastructure Collaborative Spatial Perception via Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2509.03837
- Source URL: https://arxiv.org/abs/2509.03837
- Authors: Kimia Ehsani; Walid Saad
- Reference count: 23
- Primary result: BEV-injection improves macro-average accuracy by up to 13.9% over ego-only baselines, with gains up to 32.7% under adverse conditions

## Executive Summary
This paper proposes a multimodal large language model (MLLM)-based framework for vehicle-to-infrastructure (V2I) link quality prediction. The key innovation addresses the lack of spatial understanding in MLLMs by fusing sensor data from neighboring vehicles into a unified bird's-eye view (BEV) representation. This BEV is then injected into a pre-trained LLM via a lightweight, plug-and-play connector, enabling spatial reasoning without retraining the entire model. A custom co-simulation environment combining CARLA and MATLAB ray tracing generates multimodal data and ground-truth link quality labels.

## Method Summary
The framework uses BEVFusion to convert ego RGB+LiDAR to BEV, BEVFormer TSA for temporal self-attention, GPS-based warping to align helper BEVs to ego coordinates, and 3×3 convolution to merge warped BEVs into a unified map. A Q-Former distills instruction-relevant spatial cues from the aggregated BEV, which are then cross-attended with ego features before being fed to a frozen LLM. The entire pipeline is trained end-to-end with only the connector parameters, while the LLM and encoders remain frozen.

## Key Results
- BEV-injection improves macro-average accuracy by up to 13.9% compared to ego-only baselines
- Zero-shot robustness gains up to 32.7% under challenging rainy and nighttime conditions
- Performance gains plateau after 2-3 helper agents, with diminishing returns beyond this point
- Ablation studies show each component (warp, temporal fusion, Q-Former) contributes 3-6% macro-average accuracy

## Why This Works (Mechanism)

### Mechanism 1: Multi-Agent BEV Aggregation Fills Ego Blind Spots
- Claim: Aggregating BEV representations from neighboring vehicles extends spatial awareness beyond the ego vehicle's field of view.
- Mechanism: Each vehicle fuses RGB and LiDAR into a local BEV via BEVFusion; temporal self-attention (BEVFormer TSA) adds motion context; GPS-based warping aligns helper BEVs to ego coordinates; a 3×3 convolution merges warped BEVs into a unified map.
- Core assumption: Neighboring vehicles have complementary viewpoints revealing occluded regions critical for link quality prediction.
- Evidence anchors: [abstract] and [Page 4] confirm multi-agent aggregation fills blind spots; [Page 5, Figure 3] shows accuracy improvements with 1-2 neighbors; corpus evidence from coBEVMoE and Beyond BEV validates multi-agent collaborative perception gains.
- Break condition: If helper vehicles are distant, occluded themselves, or share redundant viewpoints, fusion gains diminish sharply.

### Mechanism 2: Q-Former Distills Instruction-Relevant Spatial Cues
- Claim: The instruction-aware Q-Former extracts a compact set of task-relevant geometric tokens from the aggregated BEV, reducing LLM input overhead.
- Mechanism: Learnable queries [Q_bev; L_inst] attend to B_agg via the Q-Former φ_QF, producing F_bev tokens; cross-attention fuses F_bev with ego features; only connector parameters θ are trained.
- Core assumption: Natural language instructions can guide extraction of a sufficiently expressive spatial token set for downstream reasoning.
- Evidence anchors: [abstract] and [Page 3] describe Q-Former's role in distilling instruction-relevant cues; [Page 4, Eq. 11-13] formalize the distillation and cross-attention; limited direct corpus evidence exists for Q-Former in V2I collaborative perception.
- Break condition: Poorly initialized queries or ambiguous instructions may cause the Q-Former to discard geometric cues critical for link prediction.

### Mechanism 3: Geometry-Focused Tokens Enable Zero-Shot Robustness
- Claim: Injecting BEV-derived spatial tokens into a frozen LLM supports zero-shot generalization to unseen environmental conditions.
- Mechanism: BEV representations abstract away raw pixel intensities; spatial relationships (occlusions, distances) persist across lighting/weather changes; frozen LLM applies pre-trained reasoning to consistent geometric inputs.
- Core assumption: Environmental variations affect appearance more than geometry, so BEV tokens remain informative.
- Evidence anchors: [abstract] reports 32.7% performance gain under adverse conditions; [Page 5, Figure 4] shows rain accuracy 80.0% vs 50.0% and night 77.7% vs 45.0% for BEV-injection vs ego-only; [Page 5] explains reasoning over geometry-focused BEV tokens rather than raw pixel intensities.
- Break condition: Extreme weather (e.g., heavy snow altering road geometry) may invalidate the assumption that BEV structure remains stable.

## Foundational Learning

**Concept: Bird's Eye View (BEV) Representation**
- Why needed here: Provides a unified top-down spatial grid for multi-modal (RGB + LiDAR) and multi-agent fusion with consistent coordinate alignment.
- Quick check question: Why is BEV preferable to raw perspective images for coordinate-frame transformation between agents?

**Concept: Q-Former (Instruction-Aware Query Transformer)**
- Why needed here: Bridges high-dimensional BEV features and compact LLM tokens while respecting task instructions.
- Quick check question: How does conditioning on instruction tokens differ from unconditional pooling?

**Concept: Frozen Backbone with Trainable Connector**
- Why needed here: Enables parameter-efficient adaptation while preserving pre-trained language/vision knowledge.
- Quick check question: What is the risk of fine-tuning the entire LLM vs. only the connector?

## Architecture Onboarding

**Component map:**
Multi-view RGB + LiDAR + GPS (ego + helpers) → BEVFusion (frozen) → BEVFormer TSA → GPS-based warp → 3×3 conv aggregation → Q-Former distillation → cross-attention fusion → frozen LLM → response

**Critical path:** GPS accuracy → warp quality → multi-agent fusion → Q-Former distillation → LLM reasoning

**Design tradeoffs:**
- Helper count: 1-2 agents yield largest gains; diminishing returns beyond 3 (Figure 3)
- Token budget: Fewer tokens reduce overhead but may lose fine-grained spatial cues
- Temporal window: Larger Δ improves motion modeling but increases latency

**Failure signatures:**
- Misaligned BEVs (no warp): Macro-avg drops 6.5% (Table II)
- No temporal fusion: Blockage prediction degrades; macro-avg drops ~5%
- No Q-Former: Token overload or instruction mismatch; macro-avg drops ~3%

**First 3 experiments:**
1. Reproduce Table II ablation on a data subset to confirm each module's contribution.
2. Sweep helper agent count (1-5) and plot per-task accuracy to validate diminishing-returns claim.
3. Train on daytime-clear data only; evaluate zero-shot on held-out rain/night episodes to verify robustness.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the methodology and results suggest several areas for future investigation, particularly regarding real-world deployment, GPS localization accuracy, communication overhead, and cross-LLM compatibility.

## Limitations
- Q-Former architecture specifics (query count, dimensions, conditioning mechanism) are underspecified, creating a critical barrier to exact reproduction
- The paper assumes GPS-based warping is sufficiently accurate for BEV alignment without validating against GPS error distributions
- Zero-shot robustness claims rely on the assumption that geometric relationships remain stable across environmental conditions, but extreme weather scenarios are not tested

## Confidence
- **High Confidence**: The core ablation results showing BEV-injection improves macro-average accuracy over ego-only baselines (87.2% vs 80.7%) are well-supported by the data and methodology
- **Medium Confidence**: The 32.7% zero-shot robustness gain under adverse conditions is credible given the controlled simulation environment, but real-world GPS inaccuracies could reduce this advantage
- **Low Confidence**: The claim that the approach is "plug-and-play" with any pre-trained LLM requires validation across different backbone architectures

## Next Checks
1. **GPS Error Sensitivity**: Inject realistic GPS noise (σ = 1-5m) into the warp stage and measure degradation in macro-average accuracy. This validates the critical assumption that GPS accuracy bounds the entire pipeline's performance.
2. **Cross-LLM Generalization**: Replace Llama-3.2-11B-Vision with a different frozen backbone (e.g., ViT-Adapter) and retrain only the connector. Compare macro-average accuracy to establish whether the "plug-and-play" claim holds across architectures.
3. **Extreme Weather Testing**: Generate synthetic extreme weather scenarios (dense fog, heavy snow accumulation) in the simulation environment and evaluate whether the BEV structure remains stable enough for the Q-Former to extract meaningful geometric tokens.