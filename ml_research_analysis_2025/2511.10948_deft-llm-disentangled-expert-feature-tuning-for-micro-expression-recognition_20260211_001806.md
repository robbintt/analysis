---
ver: rpa2
title: 'DEFT-LLM: Disentangled Expert Feature Tuning for Micro-Expression Recognition'
arxiv_id: '2511.10948'
source_url: https://arxiv.org/abs/2511.10948
tags:
- motion
- facial
- flow
- recognition
- emotion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DEFT-LLM addresses the challenge of recognizing micro-expressions
  by disentangling static and dynamic facial cues using three specialized expert encoders.
  The method introduces Uni-MER, a motion-grounded instruction dataset linking Action
  Unit labels to quantified optical flow patterns.
---

# DEFT-LLM: Disentangled Expert Feature Tuning for Micro-Expression Recognition

## Quick Facts
- arXiv ID: 2511.10948
- Source URL: https://arxiv.org/abs/2511.10948
- Reference count: 40
- State-of-the-art micro-expression recognition with UF1 scores of 43.72% on DFME TestA

## Executive Summary
DEFT-LLM addresses the challenge of recognizing micro-expressions by disentangling static and dynamic facial cues using three specialized expert encoders. The method introduces Uni-MER, a motion-grounded instruction dataset linking Action Unit labels to quantified optical flow patterns. DEFT-LLM employs parallel experts for facial structure, dynamic textures, and motion semantics, and uses a hybrid training objective to prioritize discriminative accuracy. Experiments show state-of-the-art performance on multiple benchmarks, with UF1 scores of 43.72% and 42.81% on DFME test sets, demonstrating both superior accuracy and interpretability in modeling subtle facial motions.

## Method Summary
DEFT-LLM uses three frozen expert encoders (Structure, Temporal, Motion-Semantics) to extract disentangled visual features from RGB frames, video sequences, and optical flow respectively. These features are projected as prefix tokens into a LoRA-tuned LLaMA-3.1-8B model. The approach introduces Uni-MER, a motion-grounded instruction dataset with 8,041 samples, constructed using dual constraints from optical flow and Action Unit labels. A hybrid training objective combines generative loss with discriminative calibration modules (DCM) for AU detection and emotion classification, using token-weighted loss to prioritize label accuracy over fluency.

## Key Results
- Achieves state-of-the-art UF1 scores of 43.72% on DFME TestA and 42.81% on DFME TestB
- Outperforms generalist MLLMs (Qwen-VL-MAX, Gemini) that hallucinate or generalize AU mappings incorrectly
- Demonstrates superior AU detection accuracy through the discriminative calibration module, improving UF1 from 41.38% to 43.72% on TestA

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Disentangling facial features through specialized expert encoders improves micro-expression recognition by preventing static appearance cues from overwhelming subtle motion signals.
- Mechanism: Three parallel frozen encoders (Structure, Temporal, Motion-Semantics) extract independent representations that are projected as prefix tokens into the LLM. This forces the model to reason from separated signals rather than entangled representations.
- Core assumption: The performance gains stem from architectural separation rather than merely increased model capacity.
- Evidence anchors: [abstract] "disentangling static and dynamic facial cues using three specialized expert encoders"; [section III-C] "To overcome the feature entanglement common in single-encoder MLLMs, we propose DEFT-LLM"; [corpus] MER-CLIP uses AU-guided alignment but single-encoder approach; corpus average FMR=0.40
- Break condition: If a single well-tuned encoder with equivalent total parameters matches performance, disentanglement is not the causal factor.

### Mechanism 2
- Claim: Motion-grounded instruction data bridges the semantic gap between textual AU labels and physical facial movements, improving grounding and interpretability.
- Mechanism: Uni-MER dataset construction extracts region-wise optical flow, quantifies motion direction/intensity, and generates rationales via bidirectional verification (GT-to-motion and motion-to-GT), ensuring textual supervision corresponds to observable facial evidence.
- Core assumption: The dual-verification process produces higher-quality supervision than LLM-generated Chain-of-Thought descriptions.
- Evidence anchors: [abstract] "Uni-MER, a motion-grounded instruction dataset linking Action Unit labels to quantified optical flow patterns"; [section III-B] "Its construction leverages dual constraints from optical flow and Action Unit (AU) labels to ensure spatio-temporal consistency"; [corpus] Prior MER datasets lack physical grounding; MER-CLIP mentions "semantic drift" risk
- Break condition: If models trained on LLM-generated CoT rationales (without motion quantification) achieve equivalent performance, physical grounding is not the key factor.

### Mechanism 3
- Claim: Hybrid discriminative-generative training with token-weighted loss prioritizes classification accuracy over textual fluency.
- Mechanism: DCM (Discriminative Calibration Module) adds parallel AU/emotion classification heads that operate on pooled expert token representations. Generative loss uses hierarchical weighting (label tokens weighted 2.0x vs. 0.3x for rationale tokens).
- Core assumption: Masking answer tokens during DCM training forces discriminators to rely on visual features, not textual shortcuts.
- Evidence anchors: [section III-D] "L_total = L_gen + w_emo · L_emo + w_au · L_au"; [section IV-E] Ablation shows DCM boosts UF1 from 41.38% to 43.72% on TestA (Table IV); [corpus] No direct corpus comparison
- Break condition: If equivalent performance is achieved using standard generative loss with higher label weight but no DCM, the classification heads are not necessary.

## Foundational Learning

- Concept: **Optical Flow and Motion Compensation**
  - Why needed here: The Motion-Semantics Expert uses compensated flow (nose-tip referenced) to isolate local muscle motion from head movement; understanding HSV visualization convention is essential for interpreting model inputs.
  - Quick check question: Can you explain why subtracting the nose-tip flow vector helps isolate facial muscle movements?

- Concept: **Action Units (FACS) and AU-to-ROI Mapping**
  - Why needed here: Uni-MER links AU labels to quantified motion in 17 anatomically-defined ROIs; the Structural Expert uses landmark-defined regions for cross-attention.
  - Quick check question: Why might AU4 (Brow Lowerer) map to the "full eyebrow" ROI while AU1 (Inner Brow Raiser) maps only to the inner sub-region?

- Concept: **Prefix Tuning and Projection Layers**
  - Why needed here: Expert features are projected via lightweight linear layers (1024→4096) into LLM embedding space as prefix tokens; understanding this interface is critical for debugging expert contributions.
  - Quick check question: If expert prefix tokens were corrupted but text tokens remained, would the model still generate grammatically correct rationales?

## Architecture Onboarding

- Component map: Video Input → RGB+Landmarks → Structural Expert → u_struc → σ_struc; Video Sequence → Temporal Expert → u_temp → σ_temp; Optical Flow (HSV) → Motion-Semantics Expert → u_sem → σ_sem; Expert Prefix Tokens + Text Prompt → LLaMA-3.1-8B → DCM + Generative output
- Critical path: Video → Optical Flow extraction → Motion compensation → HSV visualization → SigLIP encoder → σ_sem projection → Expert prefix token concatenation → LLaMA attention → Final hidden states → DCM loss + Generative loss
- Design tradeoffs: Frozen vs. fine-tuned experts (reduces overfitting vs. limits adaptation); Discriminative vs. generative focus (enforces accuracy vs. simpler training); ROI granularity (provides precision vs. sparse coverage for AU9)
- Failure signatures: AU9 underperformance (motion compensation suppresses nasal wing signals); Generalist MLLM comparison failures (hallucination of AU12/AU14, generic "Anger" instead of context-specific "Disgust"); Class imbalance effects (AU4 in 33.3%, AU15 in 2.28% of samples)
- First 3 experiments: 1) Expert ablation on held-out dataset (train with only Motion-Semantics, add Temporal, add Structural; measure UF1 on DFME TestA); 2) Loss weight sensitivity (vary w_au/w_emo and w_label to test DCM gains across hyperparameters); 3) Motion compensation alternative (replace nose-tip reference with global affine motion estimation; compare AU detection scores)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the integration of acoustic and physiological signals into the DEFT-LLM framework improve the holistic perception of emotional states compared to visual-only inputs?
- **Basis in paper:** [explicit] The conclusion states, "Future research will be directed towards integrating more modalities, such as acoustic and physiological signals. We aim to design more reasonable interaction mechanisms to achieve a more precise and holistic perception of emotional states."
- **Why unresolved:** The current DEFT-LLM architecture relies exclusively on visual experts and does not account for non-visual cues like voice pitch or heart rate, which are often correlates of genuine emotion.
- **What evidence would resolve it:** An experimental evaluation comparing the current visual-only DEFT-LLM against a multimodal variant that includes dedicated encoders for acoustic and physiological data, showing a statistically significant increase in UF1 scores on multimodal MER benchmarks.

### Open Question 2
- **Question:** How can the motion compensation strategy be modified to prevent the suppression of nasal wing movement signals required for accurate AU9 (Nose Wrinkler) detection?
- **Basis in paper:** [explicit] Section IV-D notes the model's underperformance on AU9 and attributes it to the fact that "Our optical flow normalization, which subtracts the nose-tip flow vector, may inadvertently suppress the motion signals in the nasal wing region."
- **Why unresolved:** The current normalization technique uses the nose tip as a stable anchor to remove global head motion, but this creates a blind spot for local movements occurring at the anchor point itself, leading to low recall for AU9.
- **What evidence would resolve it:** A comparative study of alternative reference points (e.g., forehead or ears) or localized compensation algorithms that demonstrate improved AU9 detection accuracy without degrading the suppression of global head motion artifacts.

### Open Question 3
- **Question:** To what extent does the severe class imbalance in the Uni-MER dataset limit the model's generalization, and can optimized reasoning logic mitigate this?
- **Basis in paper:** [explicit] Section IV-D states, "the improvement margin is modest. We hypothesize this stems from label biases in the training data... Future work will focus on optimizing reasoning logic and incorporating diverse cues to mitigate this."
- **Why unresolved:** The paper acknowledges that the Uni-MER dataset has a long-tail distribution where 'Contempt' constitutes only 2.24% of samples and some AUs appear in <2.5%, potentially causing the model to learn superficial correlations rather than robust features.
- **What evidence would resolve it:** Ablation studies using re-weighted loss functions or data augmentation techniques to balance the dataset, resulting in significantly higher UF1 scores for minority classes (e.g., Contempt, AU9) without sacrificing performance on majority classes.

## Limitations

- **Dataset Dependence**: The Uni-MER dataset construction relies on accurate optical flow estimation and AU annotation quality, with limited systematic error analysis for cases where normalization fails or AU-ROI mappings are ambiguous.
- **Computational Overhead**: The three-encoder architecture increases computational requirements significantly compared to single-encoder approaches, introducing non-trivial costs that may not be justified for all applications.
- **Architectural Generalization**: The specific ROI definitions and optical flow compensation strategy are tuned for controlled laboratory micro-expression datasets and may not transfer directly to in-the-wild settings.

## Confidence

**High Confidence**: The disentangled expert architecture improves feature separation compared to single-encoder approaches; Motion-grounded instruction data provides more physically consistent supervision than LLM-generated rationales; Hybrid discriminative-generative training with token-weighted loss improves classification accuracy.

**Medium Confidence**: The specific choice of three expert types is optimal for MER (could other encoder combinations work equally well?); The observed performance gains are primarily due to architectural disentanglement rather than increased model capacity; The DCM module is necessary for achieving state-of-the-art results (could simpler modifications achieve similar gains?).

**Low Confidence**: The generalizability of DEFT-LLM to other fine-grained expression tasks beyond micro-expressions; The robustness of the approach to optical flow estimation errors in real-world scenarios; The necessity of the specific ROI definitions and cross-attention architecture for the Structural Expert.

## Next Checks

1. **Expert Contribution Quantification**: Conduct systematic ablation studies removing each expert encoder individually on held-out datasets to quantify their marginal contributions. Specifically, measure the performance drop when removing the Motion-Semantics Expert to validate whether optical flow features are truly essential beyond what the Temporal Expert provides.

2. **Optical Flow Sensitivity Analysis**: Compare DEFT-LLM performance using different optical flow algorithms (Farneback, RAFT, TV-L1) and without motion compensation to determine the robustness of the approach to optical flow estimation quality. This would validate whether the specific motion compensation strategy is critical or if the architecture can work with approximate motion features.

3. **Generalization to Spontaneous Expressions**: Evaluate DEFT-LLM on datasets containing spontaneous micro-expressions in uncontrolled environments (e.g., using webcam recordings or mobile device data) to test whether the architectural advantages persist outside laboratory conditions. This would validate the practical applicability beyond curated datasets.