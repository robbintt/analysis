---
ver: rpa2
title: 'The truth is no diaper: Human and AI-generated associations to emotional words'
arxiv_id: '2511.04077'
source_url: https://arxiv.org/abs/2511.04077
tags:
- human
- associations
- responses
- words
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares human and large language model (LLM) word associations,
  focusing on emotionally loaded words. The authors analyze 40 emotionally charged
  Slovenian words, collecting human associations from the SWOW-SL dataset and generating
  associations using three LLMs (Llama-3.3, GaMS-9B, and Claude 3.7 Sonnet).
---

# The truth is no diaper: Human and AI-generated associations to emotional words

## Quick Facts
- arXiv ID: 2511.04077
- Source URL: https://arxiv.org/abs/2511.04077
- Reference count: 23
- Human and LLM word associations overlap moderately, with LLMs showing less creativity and more emotional amplification

## Executive Summary
This study compares human and large language model (LLM) word associations for emotionally loaded Slovenian words, analyzing 40 emotional words from the SWOW-SL dataset. The authors find that LLM-generated associations overlap moderately with human ones, with Claude 3.7 Sonnet achieving the highest overlap. LLM associations tend to amplify emotional content and follow paradigmatic patterns more than human responses, which show greater creativity and variability including unexpected wordplay and cultural references.

## Method Summary
The study analyzes 40 emotionally charged Slovenian words selected from SloEmoLex, comparing human associations from the SWOW-SL dataset against LLM-generated associations from three models (Llama-3.3, GaMS-9B, and Claude 3.7 Sonnet). Using zero-shot prompts ("What does the word X remind you of?"), each model generates three associations per cue. Overlap is computed via set intersection, and sentiment is analyzed using SloEmoLex Valence and binary scores. Qualitative analysis categorizes responses into meaning-based, position-based, form-based, and erratic types.

## Key Results
- LLM associations overlap moderately with human ones (Claude: 23%, Llama: 17%, GaMS: 14%)
- LLM associations amplify emotional load of stimulus words compared to human responses
- Human responses show greater creativity and variability than LLM outputs
- LLMs follow paradigmatic patterns (synonyms, same POS) more than syntagmatic ones

## Why This Works (Mechanism)

### Mechanism 1: Paradigmatic Association Bias
LLMs disproportionately generate associations within the same syntactic and semantic category as the cue due to distributional co-occurrence patterns in training data. This results in frequent synonyms and near-synonyms rather than cross-category links. Break condition: Prompts explicitly requesting cross-category associations may weaken this dominance.

### Mechanism 2: Sentiment Amplification via Antonym Suppression
LLMs reinforce cue sentiment because they rarely generate antonyms, which humans use to reverse emotional polarity. This occurs because instruction-tuned models optimize for coherence and relevance, filtering antonyms as discourse-discontinuing responses. Break condition: Antonym generation explicitly rewarded or sentiment-neutral prompts used.

### Mechanism 3: Diversity Collapse Under Sampling
LLMs generate fewer unique associations than humans even with identical prompts due to mode-seeking behavior in decoding. Standard sampling concentrates probability mass on high-likelihood continuations, suppressing low-frequency but valid human-like associations. Break condition: Temperature substantially increased or ensemble prompting used.

## Foundational Learning

- Concept: Paradigmatic vs. Syntagmatic Associations
  - Why needed here: Essential for understanding why LLMs prefer same-category responses over cross-category links
  - Quick check question: Given cue "dangerous," would "risky" (paradigmatic) or "cliff" (syntagmatic) be more likely from an LLM?

- Concept: Valence and Sentiment Lexicons
  - Why needed here: The paper quantifies emotional amplification using Valence scores; understanding continuous valence (0–1) vs binary labels is crucial
  - Quick check question: If a cue has valence 0.2 and LLM responses average 0.15 while humans average 0.35, what does this suggest about LLM behavior?

- Concept: Human–Human Overlap Baselines
  - Why needed here: Raw overlap percentages appear low without context; human–human overlap typically doesn't exceed 40%
  - Quick check question: Why is 23% overlap between Claude and humans not necessarily "poor" performance?

## Architecture Onboarding

- Component map: Cue selection (40 emotional words) -> LLM prompting (zero-shot, 3 associations) -> Response normalization -> Overlap calculation -> Sentiment annotation -> Comparative analysis
- Critical path: Cue selection → LLM prompting → Response normalization → Overlap calculation → Sentiment annotation → Comparative analysis
- Design tradeoffs: Zero-shot prompts preserve comparability but limit creativity; Claude vs Llama vs GaMS trade-off between size and language-specific pretraining; 40 cues enable manual analysis but limit statistical generalization
- Failure signatures: Low overlap (<15%) suggests language mismatch or prompt misalignment; excessive paradigmatic responses indicate model isn't accessing associative knowledge; sentiment amplification signals antonym suppression
- First 3 experiments: 1) Replicate with temperature variation (0.3, 0.7, 1.0) to test diversity and paradigmatic bias; 2) Add explicit antonym prompt condition to measure antonym generation rates; 3) Extend cue set to neutral words to test amplification specificity

## Open Questions the Paper Calls Out

### Open Question 1
Would prompt engineering strategies (e.g., explicitly requesting creative responses or providing examples of human associations) significantly increase LLM association variability to match human levels? The study used zero-shot prompts without exploring alternative strategies. What evidence would resolve it: Running the same task with varied prompts requesting creativity and comparing response diversity metrics against human baselines.

### Open Question 2
How do different LLMs systematically differ in their fine-grained association patterns across Fitzpatrick's response categories (meaning-based, position-based, form-based, erratic)? The study performed manual inspection but not systematic quantitative categorization. What evidence would resolve it: Annotation of all responses using Fitzpatrick's categories and statistical comparison of category distributions across models and humans.

### Open Question 3
Why do LLMs rarely generate antonyms as associations, and would mechanisms encouraging antonym generation reduce the observed sentiment amplification effect? The mechanism behind antonym avoidance was identified but not tested. What evidence would resolve it: Analysis of antonym generation rates across cues, and experiments with prompts or decoding strategies that encourage antonym generation to test effects on sentiment scores.

### Open Question 4
Do the findings on emotional amplification and reduced creativity generalize across larger cue samples and typologically diverse languages? The study tested only 40 Slovenian words. What evidence would resolve it: Replication with larger cue sets (hundreds of words) and across languages with different morphological complexity and cultural contexts.

## Limitations
- Small stimulus set (40 words) constrains statistical generalizability
- Reliance on SloEmoLex sentiment lexicon coverage and accuracy
- Zero-shot prompting may underutilize LLM capabilities
- Cultural and linguistic factors beyond emotional valence not addressed

## Confidence
- High Confidence: LLM associations show less creativity and greater predictability than human responses
- Medium Confidence: LLMs follow paradigmatic patterns more than syntagmatic ones; antonym suppression mechanism needs stronger evidence
- Low Confidence: "Departures from expected associations remain distinctive traits of human responses" extrapolates beyond current dataset scope

## Next Checks
1. **Prompt Engineering Test**: Systematically vary temperature (0.3, 0.7, 1.0) and prompt specificity to quantify impact on paradigmatic bias and diversity collapse
2. **Antonym Generation Benchmark**: Design controlled test where LLMs are explicitly prompted to generate opposites for emotional words, measuring antonym production rates compared to human norms
3. **Extended Vocabulary Coverage**: Expand analysis to 200+ emotionally charged words and evaluate sentiment consistency using more comprehensive sentiment lexicon or fine-tuned classifier