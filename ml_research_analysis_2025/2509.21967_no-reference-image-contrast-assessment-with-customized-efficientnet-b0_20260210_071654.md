---
ver: rpa2
title: No-Reference Image Contrast Assessment with Customized EfficientNet-B0
arxiv_id: '2509.21967'
source_url: https://arxiv.org/abs/2509.21967
tags:
- contrast
- image
- quality
- assessment
- efficientnet-b0
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of no-reference image quality
  assessment for contrast distortions, which are common in real-world imaging but
  often overlooked in existing methods. The authors propose a deep learning framework
  that customizes and fine-tunes pre-trained models, particularly EfficientNet-B0,
  for perceptual contrast quality estimation using Mean Opinion Score regression.
---

# No-Reference Image Contrast Assessment with Customized EfficientNet-B0

## Quick Facts
- arXiv ID: 2509.21967
- Source URL: https://arxiv.org/abs/2509.21967
- Reference count: 40
- Primary result: Customized EfficientNet-B0 achieves state-of-the-art PLCC of 0.9581 on CID2013 and 0.9286 on CCID2014 for no-reference contrast quality assessment

## Executive Summary
This study addresses no-reference image quality assessment for contrast distortions using deep learning. The authors customize pre-trained EfficientNet-B0, ResNet-18, and MobileNetV2 architectures with regression heads to predict Mean Opinion Scores (MOS) for image contrast quality. Among the evaluated models, the customized EfficientNet-B0 achieves the highest performance with PLCC of 0.9581 on CID2013 and 0.9286 on CCID2014, outperforming traditional handcrafted methods and other deep learning baselines. The approach demonstrates that lightweight pre-trained networks with targeted augmentation can effectively capture perceptual contrast distortions for scalable, efficient quality assessment.

## Method Summary
The method employs a pre-trained EfficientNet-B0 backbone with frozen ImageNet weights, connected to a custom regression head (1280→512→256→1) with ReLU activations and 0.5 dropout. The model predicts continuous MOS values for contrast quality assessment. Training uses MSE loss with Adam optimizer (lr=1e-4, weight_decay=1e-5), batch size 32, and 50 epochs with ReduceLROnPlateau scheduler. Extensive data augmentation includes random horizontal flip (50%), rotation (±10°), and color jitter (±20% brightness/contrast/saturation). Two benchmark datasets (CID2013: 400 images, CCID2014: 655 images) are used with 80/20 train-validation splits.

## Key Results
- Customized EfficientNet-B0 achieves PLCC of 0.9581 on CID2013 and 0.9286 on CCID2014
- SRCC scores reach 0.9369 (CID2013) and 0.9178 (CCID2014), indicating strong rank correlation
- EfficientNet-B0 outperforms ResNet-18 and MobileNetV2 in both correlation metrics
- Siamese network approach fails to capture contrast distortions effectively (PLCC ~0.45-0.51)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained EfficientNet-B0 backbone with frozen weights provides effective feature representations for contrast quality estimation.
- Mechanism: ImageNet pre-training encodes low-level visual features (edges, textures) that transfer to contrast distortion detection. The compound scaling strategy (α=1.2, β=1.1, γ=1.15) jointly optimizes depth, width, and resolution, yielding better accuracy-per-parameter than arbitrary scaling. MBConv blocks with inverted residuals capture hierarchical features efficiently.
- Core assumption: Contrast distortion manifests in feature representations learned from natural image classification.
- Evidence anchors:
  - [abstract] "customizing and fine-tuning three pre-trained architectures, EfficientNet-B0, ResNet18, and MobileNetV2"
  - [Section 3.3] "EfficientNet-B0 provided superior performance. The key to its success lies in two innovations: compound scaling and MBConv blocks."
  - [corpus] Weak direct evidence for contrast-specific transfer; neighboring papers focus on CLIP-based NR-IQA and perceptual optimization, not EfficientNet contrast transfer.
- Break condition: If contrast distortions require fundamentally different features than those learned from ImageNet classification, transfer learning gains would diminish significantly.

### Mechanism 2
- Claim: A customized regression head with dropout regularization enables accurate MOS prediction from backbone features.
- Mechanism: The regression head (1280 → 512 → 256 → 1) transforms global pooled features into scalar quality scores. Dropout (0.5) prevents overfitting to limited training data. MSE loss directly optimizes for perceptual alignment with human ratings. Z-score normalization of MOS values stabilizes training.
- Core assumption: The mapping from CNN features to perceptual quality scores is learnable via supervised regression.
- Evidence anchors:
  - [Section 3.1] "associated MOS values were normalized using a Z-score transformation"
  - [Section 3.4] "Fully Connected Layers 1280 → 512 → 256 → 1" with "Loss Function Mean Squared Error (MSE)"
  - [corpus] No direct corpus validation for this specific regression head design.
- Break condition: If MOS variance across datasets is too high or subjective ratings are inconsistent, regression targets become noisy and correlation degrades.

### Mechanism 3
- Claim: Targeted data augmentation improves generalization across contrast distortion types and camera sources.
- Mechanism: Online augmentation applies random horizontal flip (50%), rotation (±10°), color jitter (brightness/contrast/saturation ±20%), and shifts during training. This exposes the model to diverse orientations, lighting conditions, and tonal variations without expanding dataset size.
- Core assumption: Augmentation-induced variability approximates real-world contrast distortion diversity.
- Evidence anchors:
  - [abstract] "Extensive data augmentation is applied to enhance generalization across two benchmark datasets"
  - [Section 3.2] "random horizontal flip, applied with a 50% probability... random color jitter was applied with adjustments to brightness (±20%), contrast (±20%)"
  - [corpus] Neighboring papers on perceptual quality (DP²O-SR, CLIP-based NR-IQA) do not specifically validate augmentation for contrast assessment.
- Break condition: If augmentation transformations alter perceptual quality in ways inconsistent with actual contrast distortions, the model may learn spurious correlations.

## Foundational Learning

- Concept: **Transfer Learning with Frozen Backbones**
  - Why needed here: The approach freezes pre-trained EfficientNet weights and only trains the regression head initially, relying on pre-learned features. Understanding why freezing preserves feature quality while reducing overfitting is essential.
  - Quick check question: Can you explain why freezing backbone layers might help when training data is limited (CCID2014: 655 images, CID2013: 400 images)?

- Concept: **Regression vs. Classification for Quality Assessment**
  - Why needed here: Unlike standard image classification, this task outputs continuous MOS scores. The loss function (MSE) and evaluation metrics (PLCC, SRCC) differ fundamentally from classification accuracy.
  - Quick check question: What would happen if you framed contrast assessment as binary classification (high/low quality) instead of regression?

- Concept: **Correlation Metrics for Perceptual Alignment**
  - Why needed here: PLCC measures linear correlation; SRCC measures rank correlation. High PLCC (0.9581) indicates accurate score prediction; high SRCC (0.9369) indicates correct relative ranking—both matter for perceptual tasks.
  - Quick check question: Why might a model have high PLCC but low SRCC, and which is more important for a quality ranking system?

## Architecture Onboarding

- Component map:
  Input Image (224×224×3) -> Data Augmentation (flip, rotation, color jitter, normalize) -> EfficientNet-B0 Backbone (frozen, ImageNet weights) -> Global Average Pooling → 1280-dim vector -> Regression Head: FC(512) + ReLU + Dropout(0.5) -> FC(256) + ReLU -> FC(1) -> MOS Score (denormalized)

- Critical path: Backbone feature extraction quality directly determines regression accuracy. If backbone features don't encode contrast-relevant information, the regression head cannot compensate.

- Design tradeoffs:
  - EfficientNet-B0: Best PLCC/SRCC (0.9286/0.9178 on CCID2014), balanced accuracy-efficiency, slower than MobileNetV2
  - ResNet-18: Highest validation accuracy (87.64%), lower correlation (0.8669 PLCC), potential overfitting
  - MobileNetV2: Fastest training, lowest accuracy, moderate correlation (0.8746 PLCC)
  - Siamese networks: Failed approach (PLCC ~0.45-0.51), suggesting pairwise ranking alone insufficient for contrast assessment

- Failure signatures:
  - Training-validation accuracy gap >20% indicates overfitting (seen in ResNet-18)
  - PLCC <0.7 on validation suggests backbone features not transferable
  - Siamese network PLCC/SRCC ~0.5 indicates architectural mismatch with task

- First 3 experiments:
  1. **Baseline sanity check**: Train with frozen EfficientNet-B0 backbone on CCID2014 without augmentation. Expect lower PLCC (~0.85-0.88) and visible overfitting. Confirms augmentation contribution.
  2. **Backbone comparison**: Train identical regression heads on EfficientNet-B0, ResNet-18, and MobileNetV2 with same augmentation. Verify EfficientNet-B0 achieves highest PLCC/SRCC per paper claims.
  3. **Ablation on regression head depth**: Test 1280→256→1 vs. 1280→512→256→1 vs. 1280→1024→512→256→1. Paper uses 3-layer head; deeper may overfit, shallower may underfit on 655-image dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the customized EfficientNet-B0 model maintain state-of-the-art performance when evaluated on broader IQA datasets containing diverse contrast characteristics and co-occurring distortions beyond CID2013 and CCID2014?
- Basis in paper: [explicit] The authors state: "Testing this framework on a broader range of IQA datasets, especially those with diverse contrast characteristics or other co-occurring distortions, would be a crucial next step" and note that cross-dataset generalization "was not explicitly examined."
- Why unresolved: The evaluation was limited to two benchmark datasets with primarily synthetic contrast distortions; no experiments were conducted on datasets with authentic distortions or mixed distortion types.
- What evidence would resolve it: Performance metrics (PLCC, SRCC) on diverse datasets such as TID2013, LIVE, KonIQ-10k, or domain-specific datasets (medical, low-light, autonomous driving) with mixed distortions.

### Open Question 2
- Question: Would scaling to larger EfficientNet variants (B1–B7) yield meaningful accuracy gains for contrast quality assessment without prohibitive computational costs?
- Basis in paper: [explicit] "It would also be interesting to experiment with other members of the EfficientNet-B0 family (e.g., B0–B7) to see if a slightly larger model offered a worthwhile accuracy boost without too much of a speed penalty."
- Why unresolved: Only EfficientNet-B0 was evaluated; the trade-off between model scale and perceptual contrast prediction accuracy remains unexplored.
- What evidence would resolve it: Comparative PLCC/SRCC results and inference time measurements across B0–B7 variants on the same benchmark datasets.

### Open Question 3
- Question: What architectural and training modifications would enable Siamese networks to effectively capture fine-grained perceptual contrast distortions?
- Basis in paper: [explicit] The authors report that their Siamese implementation achieved only moderate correlations (PLCC ≈ 0.45–0.51) and state: "these correlation levels fall within the low-to-moderate predictive regime... the current Siamese training configuration was not extracting or leveraging sufficiently discriminative features for contrast degradation."
- Why unresolved: The paper tested Siamese architectures but did not investigate alternative loss functions, attention mechanisms, or backbone configurations that could improve contrast sensitivity.
- What evidence would resolve it: Systematic ablation studies varying Siamese loss functions (e.g., ranking loss, contrast-aware loss), attention modules, and feature fusion strategies, with resulting PLCC/SRCC improvements.

### Open Question 4
- Question: How does the model perform on authentic, real-world contrast distortions in specialized domains such as medical imaging, low-light photography, or autonomous navigation?
- Basis in paper: [inferred] The paper acknowledges that "the reliance on synthetic and benchmark datasets... does not fully represent the diversity of real-world distortions" and mentions medical and autonomous driving contexts in the introduction, but provides no empirical validation in these domains.
- Why unresolved: All experiments used controlled datasets with synthetic contrast alterations; no evaluation on authentic distortions from clinical imaging, consumer photography, or safety-critical systems.
- What evidence would resolve it: Domain-specific evaluation with PLCC/SRCC scores on medical image datasets (e.g., contrast-varied CT/MRI), low-light image collections, or autonomous vehicle camera feeds with ground-truth quality ratings.

## Limitations
- The paper lacks ablation studies for key architectural decisions, particularly the arbitrary 3-layer regression head depth
- Validation procedure remains underspecified with no concrete threshold for backbone unfreezing when validation "plateaus"
- Augmentation strategy is not individually validated for contribution, leaving uncertainty about which transformations drive performance gains

## Confidence
- **High Confidence**: PLCC/SRCC correlation results on benchmark datasets (0.9581/0.9369 CID2013, 0.9286/0.9178 CCID2014) - these are well-established metrics with clear computational procedures
- **Medium Confidence**: Claims about EfficientNet-B0 superiority over ResNet-18 and MobileNetV2 - while stated, the paper lacks detailed ablation showing relative contributions of architecture vs. augmentation
- **Low Confidence**: Mechanism claims about ImageNet feature transferability to contrast assessment - no direct evidence provided beyond correlation performance

## Next Checks
1. **Ablation of regression head architecture**: Systematically test 1-layer (1280→1), 2-layer (1280→512→1), and 4-layer (1280→1024→512→256→1) variants to determine optimal capacity for the 655-image dataset
2. **Augmentation impact analysis**: Train identical models with: (a) no augmentation, (b) only geometric transforms (flip/rotation), (c) only photometric transforms (color jitter), and (d) full augmentation to isolate contribution of each transformation type
3. **Cross-dataset generalization**: Evaluate the trained model on an unseen contrast-distorted dataset (if available) or perform leave-one-dataset-out validation to assess true generalization beyond the two benchmark datasets used for training