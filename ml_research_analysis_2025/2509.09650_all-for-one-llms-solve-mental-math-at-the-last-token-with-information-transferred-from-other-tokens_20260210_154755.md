---
ver: rpa2
title: 'All for One: LLMs Solve Mental Math at the Last Token With Information Transferred
  From Other Tokens'
arxiv_id: '2509.09650'
source_url: https://arxiv.org/abs/2509.09650
tags:
- token
- layers
- information
- layer
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how large language models (LLMs) perform
  mental math tasks, focusing on minimal computation needed. The authors propose two
  techniques: Context-Aware Mean Ablation (CAMA) and Attention-Based Peeking (ABP)
  to progressively simplify the model architecture.'
---

# All for One: LLMs Solve Mental Math at the Last Token With Information Transferred From Other Tokens

## Quick Facts
- arXiv ID: 2509.09650
- Source URL: https://arxiv.org/abs/2509.09650
- Reference count: 23
- Primary result: LLMs perform mental math with computation deferred to the last token, requiring only brief information transfer periods

## Executive Summary
This paper investigates how large language models perform mental math tasks with minimal computation. The authors discover an "All-for-One" (AF1) subgraph where computation is concentrated at the last token, which receives information from other tokens only during specific middle layers. Using two novel techniques—Context-Aware Mean Ablation (CAMA) and Attention-Based Peeking (ABP)—they progressively simplify the model architecture while maintaining high accuracy on arithmetic tasks. The study reveals that LLMs prioritize last-token prediction over compositional reasoning across token positions, with meaningful computation occurring very late and only at the last token position.

## Method Summary
The paper proposes a three-stage AF1 subgraph for mental math tasks. First, Context-Aware Mean Ablation (CAMA) replaces intermediate representations with conditional expectations during early layers, allowing tokens to compute independently while preserving task-relevant features. Second, Attention-Based Peeking (ABP) enables the last token to gather operand information from other tokens during a brief transfer period using selective attention masks. Third, ABP enforces self-peeking (attending only to self and BOS) for remaining layers. The authors identify critical layers (Lwait=15, Ltransfer=2) for Llama-3-8B and demonstrate the AF1 structure maintains high faithfulness across various arithmetic tasks while significantly reducing computational requirements.

## Key Results
- AF1 subgraph preserves high accuracy on arithmetic tasks with minimal cross-token computation
- Computation is deferred to the last token, which receives information from other tokens only during specific middle layers
- Last-token concentration is critical—non-last tokens can be restricted to self-attention without performance loss
- AF1 completely fails on tasks requiring semantic understanding (word problems, Python code)
- Smaller models (Pythia, GPT-J) require earlier transfer and longer information transfer periods

## Why This Works (Mechanism)

### Mechanism 1: Context-Aware Mean Ablation (CAMA) — Deferred Computation
- **Claim:** Tokens can perform task-general computation without input-specific cross-token information for many layers before information transfer becomes necessary.
- **Mechanism:** CAMA replaces the true intermediate representation at layer $L_{wait}$ with the conditional expectation $\tilde{x}_t^{(L_{wait})} = \mathbb{E}_{x' \sim P(x|x_t)}[m(x', t, L_{wait})]$, preserving task-relevant features while erasing input-specific values from other tokens.
- **Evidence:** Faithfulness remains high for $L_{wait} \leq 15$ then collapses at $L_{wait} = 16$ for Llama-3-8B on A+B+C. Alternative waiting mechanisms (DEC, RTMA, SPAW, IFP) all fail, suggesting CAMA's distributional awareness is critical.

### Mechanism 2: Attention-Based Peeking (ABP) — Minimal Cross-Token Transfer
- **Claim:** A brief burst of information transfer layers (as few as 2) is sufficient for the last token to gather all operand information.
- **Mechanism:** ABP enforces selective attention via masking: the last token attends to all tokens during $L_{transfer}$ layers, then only to itself and BOS; non-last tokens attend only to themselves and BOS throughout.
- **Evidence:** Performance stays high as long as $L_{transfer} \geq 2$ (layers 15-16) for Llama-3-8B. Critical heads (e.g., L15H13, L15H3, L16H21) attend to specific operands, transferring information to the last token.

### Mechanism 3: Last-Token Computational Centrality
- **Claim:** Input-specific computation is concentrated at the last token position; non-last tokens serve primarily as information sources, not independent processors.
- **Mechanism:** Due to next-token prediction training, the model allocates computational bandwidth to the position directly responsible for output; non-last tokens can be restricted to self-attention without performance loss.
- **Evidence:** Non-last tokens restricted to self-attention show no performance degradation. Hypothesizes that token-level training signal prevents compositional reasoning across positions.

## Foundational Learning

- **Concept: Residual Stream and Layer-wise Representations**
  - **Why needed here:** CAMA operates on intermediate representations; understanding how activations evolve across layers is essential to grasp what is being ablated and preserved.
  - **Quick check question:** Can you explain why replacing $x_t^{(L_{wait})}$ with its conditional expectation preserves task-general features while erasing input-specific information?

- **Concept: Causal Self-Attention and Attention Masking**
  - **Why needed here:** ABP directly manipulates attention masks; you must understand causal attention constraints and how masking controls information flow.
  - **Quick check question:** Given a 5-token sequence, what is the effective attention pattern if token 5 (last) can only attend to tokens 1 and 5?

- **Concept: Mean Ablation and Intervention Methods**
  - **Why needed here:** CAMA is a variant of mean ablation; distinguishing it from naive approaches (e.g., zero ablation, random token ablation) clarifies why it succeeds where others fail.
  - **Quick check question:** Why might ablating with random token contexts destroy arithmetic reasoning even if the target token is preserved?

## Architecture Onboarding

- **Component map:** Input Embeddings → CAMA Layers (Lwait): tokens compute independently, context-averaged → ABP Transfer Layers (Ltransfer): last token full-peek, others self-peek → ABP Self-Compute Layers (remaining): all tokens self-peek only → Unembed → Output Logits

- **Critical path:**
  1. Identify $L_{wait}$ by incrementally replacing early layers with CAMA until performance drops sharply
  2. Identify $L_{transfer}$ by testing ABP with varying transfer durations after $L_{wait}$
  3. Validate necessity by removing last-token attention at individual layers and head-level ablation

- **Design tradeoffs:**
  - CAMA preserves in-distribution representations but requires computing conditional expectations over task samples
  - ABP allows precise control of information flow but may not generalize to tasks requiring richer inter-token communication
  - Smaller models require earlier transfer and longer $L_{transfer}$, suggesting less efficient information routing

- **Failure signatures:**
  - Performance collapse when CAMA applied past critical transfer layer (e.g., $L_{wait} = 16$ for Llama-3-8B)
  - Near-zero faithfulness when using alternative waiting methods (DEC, RTMA, SPAW, IFP) instead of CAMA
  - Failure on semantically complex tasks (word problems, Python code) indicates AF1 does not capture all arithmetic-relevant circuits

- **First 3 experiments:**
  1. **Replicate AF1 discovery on a new model:** Apply CAMA + ABP grid search on a model not in the paper (e.g., Mistral-7B) for A+B+C to test cross-model transfer of the AF1 pattern
  2. **Head-level importance mapping:** For the identified transfer layers, perform iterative head removal to find the minimal sufficient head set; compare with Llama-3-8B's critical heads (L15H13, L15H3, L16H21)
  3. **Semantic generalization test:** Apply the AF1 subgraph to verbal math and word problem templates to quantify the boundary between direct arithmetic and context-dependent reasoning

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can custom training signals encourage compositional reasoning across intermediate tokens, preventing the deferred computation observed in the AF1 subgraph?
- **Basis in paper:** The authors hypothesize that standard token-level prediction signals cause models to focus solely on the last token, and suggest exploring training signals that place heavier weight on "important" tokens to foster intermediate computation.
- **Why unresolved:** The study analyzes static, pre-trained models and does not conduct training experiments to validate if this intervention would change the computational structure.
- **Evidence:** Training a model with the proposed custom loss functions and observing whether intermediate tokens begin to compute partial results (e.g., calculating $A+B$ before $C$ is added) rather than deferring all work to the last token.

### Open Question 2
- **Question:** How can the CAMA and ABP techniques be extended to handle models that tokenize numbers into multiple digits (e.g., Qwen, Gemma) rather than single tokens?
- **Basis in paper:** The authors identify this as the "biggest limitation," noting that the current method depends on "cooperative" tokenizers where numbers are single tokens, and leave multi-token extensions to future work.
- **Why unresolved:** The current CAMA implementation relies on marginalizing over specific token positions; this logic breaks when numerical operands are split across variable-length token sequences.
- **Evidence:** A modified version of the AF1 subgraph that successfully maintains high faithfulness on arithmetic tasks for models with digit-level tokenization.

### Open Question 3
- **Question:** What additional subgraph components are required to support tasks involving semantic understanding, such as math word problems or Python code interpretation?
- **Basis in paper:** The authors note that AF1 "completely fails" on Python and Math Word Problem templates and suggest "exploring the additional components to AF1 that endow models with such capabilities could be useful."
- **Why unresolved:** The paper demonstrates that the current sparse subgraph is insufficient for semantic tasks but does not isolate the specific circuits (heads or layers) responsible for this missing capability.
- **Evidence:** Identifying specific attention heads or layers that, when restored to the AF1 subgraph, recover high accuracy on the semantic templates listed in Table 5.

## Limitations

- The AF1 mechanism is task-specific and completely fails on semantically complex tasks like word problems and Python code
- The CAMA technique requires non-trivial computational overhead for sampling from conditional distributions
- The critical layer numbers (Lwait=15, Ltransfer=2) are model-specific and may not generalize to other architectures
- The method depends on "cooperative" tokenizers where numbers are single tokens, limiting applicability to models with digit-level tokenization

## Confidence

**High Confidence:** The existence of an AF1 subgraph that preserves high accuracy with minimal cross-token computation. The ablation results showing last-token centrality. The task-specific failure on semantic reasoning.

**Medium Confidence:** The exact layer numbers for AF1 discovery are model-specific and may not transfer directly to other architectures. The claim that training objective creates last-token inductive bias is supported by evidence but not definitively proven.

**Low Confidence:** The generalizability of AF1 to models outside the tested set. The precise sampling requirements and computational complexity of CAMA for practical implementation. The boundary between tasks that can and cannot be solved by AF1.

## Next Checks

1. **Cross-Model AF1 Pattern Validation:** Apply the CAMA+ABP methodology to a new transformer family (e.g., Mistral-7B, Gemma-7B) on A+B+C arithmetic tasks. Measure whether similar critical layer patterns emerge and whether the AF1 subgraph preserves faithfulness. This tests the generality of the mechanism across architectures.

2. **Critical Head Set Identification:** For the identified transfer layers in Llama-3-8B, perform iterative head removal experiments to find the minimal sufficient head set for AF1 performance. Compare the identified heads with those in Table 4 (L15H13, L15H3, L16H21) to determine if there are common attention patterns across models.

3. **Semantic Boundary Characterization:** Systematically test AF1 on a spectrum of tasks from pure arithmetic (A+B+C) to semantically rich word problems with increasing contextual complexity. Quantify the exact point where AF1 fails and characterize what additional information or computation becomes necessary beyond direct arithmetic.