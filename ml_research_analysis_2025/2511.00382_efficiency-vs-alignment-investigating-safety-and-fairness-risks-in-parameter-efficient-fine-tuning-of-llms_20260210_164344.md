---
ver: rpa2
title: 'Efficiency vs. Alignment: Investigating Safety and Fairness Risks in Parameter-Efficient
  Fine-Tuning of LLMs'
arxiv_id: '2511.00382'
source_url: https://arxiv.org/abs/2511.00382
tags:
- base
- safety
- fine-tuning
- bias
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically evaluates how parameter-efficient fine-tuning\
  \ (PEFT) methods affect the safety and fairness of instruction-tuned LLMs. Four\
  \ widely used PEFT techniques\u2014LoRA, IA3, Prompt-Tuning, and P-Tuning\u2014\
  are applied to four instruction-tuned models (LLaMA-3-8B, Qwen2.5-7B, Mistral-7B,\
  \ and Gemma-7B) and assessed across eleven safety hazard categories and nine demographic\
  \ fairness dimensions."
---

# Efficiency vs. Alignment: Investigating Safety and Fairness Risks in Parameter-Efficient Fine-Tuning of LLMs

## Quick Facts
- **arXiv ID:** 2511.00382
- **Source URL:** https://arxiv.org/abs/2511.00382
- **Reference count:** 40
- **Primary result:** Adapter-based PEFT methods preserve or improve safety/fairness; prompt-based methods degrade them, with outcomes highly model-dependent.

## Executive Summary
This study systematically evaluates how parameter-efficient fine-tuning (PEFT) methods affect the safety and fairness of instruction-tuned LLMs. Four widely used PEFT techniques—LoRA, IA3, Prompt-Tuning, and P-Tuning—are applied to four instruction-tuned models (LLaMA-3-8B, Qwen2.5-7B, Mistral-7B, and Gemma-7B) and assessed across eleven safety hazard categories and nine demographic fairness dimensions. Adapter-based methods (LoRA, IA3) generally improve or preserve safety and fairness, while prompt-based methods (Prompt-Tuning, P-Tuning) consistently degrade them. Safety outcomes are strongly model-dependent: LLaMA remains stable, Qwen shows modest gains, Mistral exhibits high variance, and Gemma experiences the steepest safety decline. No single configuration optimizes all fairness metrics simultaneously, revealing inherent trade-offs between safety and fairness objectives. The findings highlight the need for category-specific audits and careful PEFT selection, especially for safety-critical applications.

## Method Summary
The study fine-tunes four instruction-tuned LLMs (LLaMA-3-8B, Mistral-7B, Qwen2.5-7B, Gemma-7B) using four PEFT methods (LoRA, IA3, Prompt-Tuning, P-Tuning) on UltraFeedback Binarized and Ultrachat 200k datasets (sub-sampled to ~20.8k examples each). Safety is evaluated using HEx-PHI benchmark with LLaMA-Guard 2 as judge; fairness is assessed via a modified BBQ-Lite dataset measuring accuracy and bias across nine demographic dimensions. Training uses SFT and DPO with learning rates of 2e-5 and 1e-3, epochs of 1 and 5, and three repetitions per configuration.

## Key Results
- Adapter-based PEFT methods (LoRA, IA3) tend to improve safety scores and are least disruptive to fairness.
- Prompt-based methods (Prompt-Tuning, P-Tuning) generally reduce safety and cause larger fairness regressions.
- No single PEFT configuration optimizes all fairness metrics simultaneously, revealing inherent trade-offs.
- Model dependency is pronounced: LLaMA remains stable, Qwen shows gains, Mistral exhibits high variance, and Gemma experiences the steepest decline in safety.

## Why This Works (Mechanism)

### Mechanism 1: Weight Preservation via Adapters
Adapter-based methods (LoRA, IA3) are more likely to preserve safety alignment than prompt-based methods because they isolate updates to small, additive modules rather than altering the core model's activation paths. Adapters introduce low-rank matrices that learn additive updates to frozen weights, limiting the capacity to "unlearn" or bypass the safety alignment embedded in the pre-trained base model. This assumes safety alignment in the base model is distributed across frozen transformer layers and is robust to low-rank perturbations. Break condition: If the base model has weak initial alignment (e.g., Mistral), the preservation effect of adapters diminishes.

### Mechanism 2: Input Representation Drift
Prompt-based methods (Prompt-Tuning, P-Tuning) may degrade safety by shifting the input embedding space, effectively moving the model's operational context away from the distribution where safety training was applied. These methods optimize continuous prompt embeddings at the input layer, altering how subsequent layers process inputs and potentially bypassing safety constraints that rely on specific input features. This assumes safety mechanisms rely on feature detection in the original input embedding space. Break condition: If safety alignment is robustly encoded in deeper layers (like LLaMA) rather than input sensitivity, prompt-based methods might show less degradation.

### Mechanism 3: Layer-wise Sensitivity (Architecture Specifics)
The impact of PEFT on safety is moderated by where a model encodes its safety behaviors; models with safety features in initial/final layers (Gemma) are more vulnerable to fine-tuning disruption than those with features in deeper layers. Fine-tuning updates parameters, and if safety "neurons" or features are concentrated in the layers most affected by the PEFT configuration, alignment degrades faster. This assumes the location of safety features is static and determines vulnerability to specific PEFT strategies. Break condition: If PEFT methods specifically target deeper layers, the vulnerability profile of models like Gemma might change.

## Foundational Learning

- **Concept: PEFT Method Families (Adapter vs. Prompt-based)**
  - **Why needed here:** The paper's core finding is that these two families have drastically different risk profiles for safety and fairness.
  - **Quick check question:** Does LoRA modify the input embeddings or inject trainable matrices into specific layers?

- **Concept: Safety vs. Fairness Trade-offs**
  - **Why needed here:** The study shows improvements in one do not guarantee improvements in the other; aggregate scores hide category-specific failures.
  - **Quick check question:** If a model improves its refusal of harmful prompts, does it necessarily reduce bias in ambiguous contexts (Acc. AMB)?

- **Concept: Base Model Alignment Robustness**
  - **Why needed here:** The "Alignment Tax" of fine-tuning is heavily dependent on the starting model (e.g., Mistral's lack of internal moderation vs. LLaMA's stability).
  - **Quick check question:** Which model in the study lacked an internal moderation layer, leading to high variance?

## Architecture Onboarding

- **Component map:** Base Model (Llama, Mistral, Gemma, Qwen) -> PEFT Layer (LoRA/IA3 vs. P-Tuning/Prompt-Tuning) -> Evaluation Suite (HEx-PHI + BBQ-Lite) -> Guardrail (LLaMA-Guard 2)

- **Critical path:**
  1. **Audit Base Model:** Check initial safety/fairness scores (is it robust like LLaMA or volatile like Mistral?).
  2. **Select PEFT Method:** Choose Adapters (LoRA/IA3) for safety-critical apps; strictly avoid P-Tuning for fragile models like Gemma.
  3. **Configure Training:** Prioritize conservative learning rates (2e-5) and DPO over SFT if possible (DPO showed lower variance).
  4. **Category Audit:** Monitor "Child Abuse Content" and "Sexual Orientation" categories specifically, as they are the most sensitive to regression.

- **Design tradeoffs:**
  - **Safety vs. Fairness:** No single configuration optimizes both simultaneously.
  - **Efficiency vs. Stability:** Prompt-Tuning is highly efficient but causes the steepest alignment decline; LoRA is slightly less efficient but far more stable.
  - **Accuracy vs. Bias:** Fine-tuning often reduces bias amplitude (Bias AMB) but at the cost of accuracy, meaning the model makes fewer mistakes but might be less helpful.

- **Failure signatures:**
  - **Gemma + Prompt-Tuning:** Steep safety decline and fairness regression.
  - **Mistral + Any PEFT:** High variance; relative safety gains but absolute safety scores remain low.
  - **SFT (Supervised Fine-Tuning):** statistically significant degradation in fairness accuracy compared to DPO.

- **First 3 experiments:**
  1. **Baseline Profiling:** Run HEx-PHI and BBQ-Lite on the chosen base model (e.g., LLaMA vs. Gemma) to establish category-specific vulnerabilities.
  2. **Adapter A/B Test:** Compare LoRA vs. IA3 on the target base model using a held-out dataset to verify which adapter preserves the "core positive group" (accuracy, utility, safety) better.
  3. **Learning Rate Stress Test:** Compare 2e-5 vs. 1e-3 on a fairness-sensitive category (e.g., Sexual Orientation) to confirm if higher LRs degrade accuracy as suggested.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can multi-objective optimization techniques (e.g., constrained optimization, Pareto-front methods) be adapted to the PEFT setting to jointly improve safety and fairness while maintaining task performance?
- **Basis in paper:** [explicit] The authors state: "Future work could design PEFT strategies with explicit alignment guarantees, by investigate multi-objective optimization for adding safety and fairness to PEFT, especially with prompt-based methods."
- **Why unresolved:** Current PEFT methods do not explicitly optimize for alignment objectives; the study reveals inherent trade-offs where no single configuration optimizes all fairness metrics simultaneously.
- **What evidence would resolve it:** Development and empirical validation of a multi-objective PEFT method that demonstrates simultaneous improvements in safety scores, fairness accuracy, and task utility across multiple base models.

### Open Question 2
- **Question:** What architectural factors in base models determine their vulnerability or resilience to alignment degradation during PEFT?
- **Basis in paper:** [explicit] The authors call for "characterizing architectural factors that prioritizes alignment and robustness," noting that Gemma concentrates safety neurons in initial and final layers while LLaMA's lie in deeper layers.
- **Why unresolved:** The study documents that LLaMA remains stable, Gemma degrades sharply, and Mistral shows high variance, but the underlying architectural causes remain hypothesized rather than proven.
- **What evidence would resolve it:** Mechanistic interpretability analysis mapping how different PEFT methods affect safety-critical neurons across model architectures, coupled with controlled experiments varying architectural properties.

### Open Question 3
- **Question:** How do different prompt initialization strategies for prompt-based PEFT methods affect safety and fairness outcomes?
- **Basis in paper:** [explicit] The authors note: "it would interesting to analyze the effect of different types of initialization on the alignment, as what is appended to the prompt in terms of prompt templates has been proved to have an impact on the safety."
- **Why unresolved:** This study used only one configuration profile (random initialization) for prompt-based methods, yet prompt-based methods consistently degraded alignment more than adapter-based methods.
- **What evidence would resolve it:** Systematic comparison of prompt-based PEFT across varied initialization strategies (random, task-specific embeddings, safety-aware prompts) measuring alignment outcomes on the same benchmarks.

## Limitations
- The findings are based on a specific set of models and PEFT methods, limiting generalizability to other popular models or emerging techniques.
- Safety and fairness assessments rely on specific benchmarks (HEx-PHI, BBQ-Lite) and a safety judge (LLaMA-Guard 2) that may not capture all real-world edge cases or domain-specific hazards.
- The study explores different training configurations, but optimal settings for balancing safety, fairness, and utility likely depend on specific applications and base models.

## Confidence
- **High Confidence:** Adapter-based PEFT methods (LoRA, IA3) tend to preserve or improve safety and fairness compared to prompt-based methods (Prompt-Tuning, P-Tuning) across multiple models and evaluation metrics.
- **Medium Confidence:** Specific safety and fairness scores are sensitive to the choice of base model, PEFT method, and training configuration, though the trade-offs are real.
- **Low Confidence:** Proposed causal mechanisms (weight preservation, input representation drift) are plausible but not definitively proven, as the evidence is correlational.

## Next Checks
1. **Causal Mechanism Isolation:** Conduct ablation studies to isolate the effects of weight preservation (adapters) vs. input representation drift (prompts) on safety and fairness.
2. **Dataset and Evaluation Generalization:** Validate the safety and fairness impacts of PEFT on additional datasets (e.g., RealToxicityPrompts, StereoSet) and with alternative evaluation methods.
3. **PEFT Method and Model Expansion:** Extend the study to include other popular models (e.g., GPT-3.5, Claude) and PEFT techniques (e.g., QLoRA, BitFit) to assess generalizability across a broader configuration space.