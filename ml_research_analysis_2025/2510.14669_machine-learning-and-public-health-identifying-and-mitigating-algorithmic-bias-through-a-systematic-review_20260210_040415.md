---
ver: rpa2
title: 'Machine Learning and Public Health: Identifying and Mitigating Algorithmic
  Bias through a Systematic Review'
arxiv_id: '2510.14669'
source_url: https://arxiv.org/abs/2510.14669
tags:
- bias
- fairness
- data
- studies
- reporting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study conducts a systematic review of algorithmic bias (AB)\
  \ reporting in Dutch public health (PH) machine learning (ML) research from 2021\u2013\
  2025. It introduces the Risk of Algorithmic Bias Assessment Tool (RABAT), which\
  \ integrates elements from the Cochrane Risk of Bias Tool, PROBAST, and the Microsoft\
  \ Responsible AI checklist, and applies it to 35 peer-reviewed studies."
---

# Machine Learning and Public Health: Identifying and Mitigating Algorithmic Bias through a Systematic Review

## Quick Facts
- arXiv ID: 2510.14669
- Source URL: https://arxiv.org/abs/2510.14669
- Reference count: 40
- Primary result: Systematic review reveals significant gaps in algorithmic bias reporting in Dutch public health ML research; introduces RABAT tool and ACAR framework to address these gaps.

## Executive Summary
This study conducts a systematic review of algorithmic bias (AB) reporting in Dutch public health (PH) machine learning (ML) research from 2021–2025. It introduces the Risk of Algorithmic Bias Assessment Tool (RABAT), which integrates elements from the Cochrane Risk of Bias Tool, PROBAST, and the Microsoft Responsible AI checklist, and applies it to 35 peer-reviewed studies. The analysis reveals significant gaps: while data sampling and missing data practices are well documented, most studies omit explicit fairness framing, subgroup analyses, and transparent discussion of potential harms. In response, the authors propose the ACAR framework—Awareness, Conceptualization, Application, Reporting—to guide PH+ML researchers in addressing AB across the research lifecycle. The findings highlight the need for systematic attention to fairness to ensure that ML innovations advance health equity rather than reinforce disparities.

## Method Summary
The study performs a systematic literature review using Google Scholar with a specific query targeting Dutch public health ML research. Two search phases (2021–2023 and 2024–2025) yielded 1,940 initial results, reduced to 35 included studies after screening. The authors developed RABAT, a 10-item assessment tool scoring studies on algorithmic bias reporting using a 0–3 scale. Two independent reviewers applied RABAT to all studies, resolving discrepancies through discussion. The ACAR framework was then developed to address identified gaps, providing guiding questions for researchers at each stage of the ML lifecycle.

## Key Results
- Data sampling and missing data practices are well reported, but fairness-specific items (sensitive attributes, subgroup analyses, harm transparency) score consistently low.
- Only 3 of 35 studies (8.6%) framed their work in terms of fairness or equity.
- RABAT items on sensitive attributes (Q7) and harm transparency (Q9) had mean scores below 0.3, classified as "High Risk."
- The ACAR framework was developed to operationalize fairness across the ML lifecycle but remains untested in practice.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If a systematic assessment tool (RABAT) integrates clinical risk-of-bias standards with AI ethics checklists, it exposes the specific disconnect between methodological rigor (data handling) and ethical rigor (fairness framing) in Public Health ML (PH+ML).
- **Mechanism:** The RABAT tool scores studies on a 0–3 scale across 10 items. By distinguishing between technical reporting (e.g., "Sampling and Missing Data") and fairness-specific reporting (e.g., "Sensitive Attributes," "ML Fairness"), the mechanism quantifies the "High Risk" gap in the latter. The score distribution visualizes this asymmetry (Figure 2), revealing that while researchers follow data protocols, they fail to frame bias in terms of equity.
- **Core assumption:** The integration of Cochrane (clinical validity) and Microsoft Responsible AI (societal impact) covers the necessary breadth of PH+ML risks.
- **Evidence anchors:** [Abstract] "...developed the Risk of Algorithmic Bias Assessment Tool (RABAT) by integrating elements from established frameworks... and applied it to 35 peer-reviewed studies." [Results 4.5] "Only Q6 [Sampling] was classified as Low Risk... fairness-specific items (e.g., ML fairness, sensitive attributes, harm transparency) score consistently low."

### Mechanism 2
- **Claim:** If a lifecycle framework (ACAR) forces researchers to define fairness and subgroups *before* modeling (Conceptualization stage), it may prevent the "High Risk" reporting gaps observed in retrospective studies.
- **Mechanism:** ACAR (Awareness, Conceptualization, Application, Reporting) operationalizes fairness by mapping RABAT gaps to actionable guiding questions. By requiring "Fairness Framing" (Q3) and "At-Risk Subgroups" (Q5) to be defined in the early stages, the mechanism aims to embed equity considerations into the research design, rather than treating them as post-hoc reporting obligations.
- **Core assumption:** Researchers are willing and able to define "fairness" relevant to their specific population context early in the workflow, despite the current low prevalence of this practice.
- **Evidence anchors:** [Abstract] "...introduce a four-stage fairness-oriented framework called ACAR... to help researchers address fairness across the ML lifecycle." [Discussion 5.6] "These guiding questions are intentionally designed for interdisciplinary teams... allowing integration into existing projects regardless of ML maturity."

### Mechanism 3
- **Claim:** In high-resource health settings (like the Netherlands), strict adherence to data privacy and procedural consent (Q10) may paradoxically obscure algorithmic harms (Q9) because technical teams assume compliance equals safety.
- **Mechanism:** The study identifies that while "Informed Consent" (Q10) is reported moderately well ("Some Concerns"), "Harm Transparency" (Q9) is near absent ("High Risk," mean 0.13). The mechanism suggests that existing governance (e.g., GDPR, ethics approvals) creates a "compliance ceiling" where researchers document *process* (consent obtained) but fail to analyze *outcome* (differential harm), leaving the actual algorithmic risk unaudited.
- **Core assumption:** The observed gap in reporting harms is due to a lack of tools/awareness rather than deliberate concealment of negative findings.
- **Evidence anchors:** [Results 4.4] "Informed Consent... classified as Some Concerns... [however] Harm Transparency... classified as High Risk." [Discussion 5.1] "...fairness-specific items... were clustered near zero... underscoring the persistent absence of structural AB risk assessment in routine Dutch PH+ML reporting."

## Foundational Learning

- **Concept: Algorithmic Bias (AB) vs. Model Error**
  - **Why needed here:** The paper distinguishes between technical model errors (overfitting, missing data) and *algorithmic bias*—systematic patterns that yield unfair outcomes for specific groups. Understanding this is prerequisite to using RABAT, which penalizes studies that discuss "model bias" technically but ignore "ML fairness" socially.
  - **Quick check question:** Does your study report "accuracy" differently for majority vs. minority subgroups?

- **Concept: Subgroup Calibration**
  - **Why needed here:** The review found that while datasets are often described (Q6), *sensitive attributes* (Q7) are rarely analyzed for performance disparities. A learner must understand that a model can be accurate globally (AUC 0.90) but miscalibrated for a specific ethnic or socioeconomic group.
  - **Quick check question:** Have you defined which sensitive attributes (e.g., sex, age, origin) influence predictions, and have you tested for it?

- **Concept: The Fairness-Aware Lifecycle**
  - **Why needed here:** The ACAR framework proposes that fairness cannot be "added at the end." Learners must grasp the iterative nature: Awareness -> Conceptualization -> Application -> Reporting. This counters the observed trend where "fairness" is an afterthought or omitted entirely.
  - **Quick check question:** Can you map your current research stage to one of the four ACAR stages, and do you have fairness-related artifacts for that stage?

## Architecture Onboarding

- **Component map:** RABAT (Audit Tool) -> ACAR (Development Guide) -> 35 Dutch PH+ML studies (Evidence Base)

- **Critical path:**
  1. **Audit:** Apply RABAT to existing or proposed research to establish a baseline score (likely "High Risk" on Q3, Q7, Q9).
  2. **Conceptualize:** Use ACAR Stage 2 to define fairness and subgroups explicitly *before* model training.
  3. **Apply & Report:** Execute ACAR Stages 3 & 4, ensuring subgroup testing (Q5) and harm transparency (Q9) are documented in the final manuscript.

- **Design tradeoffs:**
  - **Breadth vs. Depth:** RABAT covers 10 items but may lack depth for complex intersectional harms.
  - **Generalizability vs. Context:** The framework is tailored to Dutch PH (universal care, specific disparities). Adapting to low-resource settings may require redefining "Sensitive Attributes" (Q7).
  - **Rigidity vs. Adoption:** Strict scoring (0–3) ensures objectivity but may discourage adoption by teams lacking fairness expertise; ACAR's "guiding questions" offer a softer entry point.

- **Failure signatures:**
  - **The "Checklist" Trap:** High RABAT scores achieved by rote compliance (e.g., mentioning subgroups but not analyzing them), known as "fairwashing."
  - **The "Data Paradox":** Excluding sensitive attributes (Q7) to avoid bias, which prevents any subgroup analysis (Q5), resulting in an un-auditable model.
  - **Isolated Reporting:** High scores on "Informed Consent" (Q10) but zero on "Harm Transparency" (Q9), indicating a focus on process over impact.

- **First 3 experiments:**
  1. **Retrospective RABAT Audit:** Score 3–5 recent PH+ML papers from your own lab using the condensed RABAT questionnaire (Table 1) to identify the "High Risk" blind spots in current documentation.
  2. **ACAR Simulation:** Take a project currently in the "Application" phase and retroactively apply ACAR Stages 1 & 2 (Awareness, Conceptualization). Document what changes would be required if those stages had been completed initially.
  3. **Subgroup Feature Analysis:** For an existing model, select one "Sensitive Attribute" (e.g., sex or age) and run a disaggregated performance analysis (Q5) to see if the global performance metric holds for that subgroup.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the application of the ACAR framework effectively improve fairness-aware reporting and practice in active public health machine learning projects?
- Basis in paper: [explicit] The authors state that the ACAR framework "remains theoretical, as it has not yet been applied in real-world PH+ML settings" and recommend "empirically validating ACAR in active PH+ML projects."
- Why unresolved: While ACAR was derived from observed gaps, its utility as an intervention has not been tested in a live research setting.
- What evidence would resolve it: Results from pilot studies where interdisciplinary teams apply ACAR throughout the research lifecycle, measured by improvements in RABAT scores or qualitative assessments of workflow integration.

### Open Question 2
- Question: Do the identified gaps in fairness reporting result from a lack of researcher awareness or a failure to document considerations that were actually made?
- Basis in paper: [explicit] The authors call for "exploring researchers’ decision-making around AB (e.g., interviews or surveys) to understand whether and why fairness considerations are addressed but not documented."
- Why unresolved: The systematic review could only assess reported text, not the unreported internal deliberations or motivations of the researchers.
- What evidence would resolve it: Qualitative data from interviews or surveys with authors of the reviewed studies, contrasting their reported workflows with their published papers.

### Open Question 3
- Question: Are the RABAT assessment tool and ACAR framework applicable to public health machine learning research in non-Dutch or low-resource governance contexts?
- Basis in paper: [explicit] The authors note that findings "reflect the Dutch context and may, for example, not generalize to low-resource or alternative governance settings" and suggest "applying RABAT and ACAR in non-Dutch contexts."
- Why unresolved: The study is limited to the specific regulatory and infrastructural environment of the Netherlands.
- What evidence would resolve it: A replication of the systematic review using RABAT on PH+ML literature from other countries, or validation studies testing ACAR in international research teams.

## Limitations
- The RABAT tool was developed and validated on a single national corpus (n=35 studies), limiting generalizability to other health systems.
- The ACAR framework remains theoretical without empirical validation of its impact on research practices.
- The Google Scholar search methodology may have missed relevant studies due to query specificity and database limitations.

## Confidence
- **High confidence**: The systematic gap in fairness-specific reporting (RABAT Q3, Q7, Q9) across the 35-study corpus, given the transparent scoring methodology and inter-rater reliability.
- **Medium confidence**: The generalizability of these gaps to other national contexts or broader ML research, due to the study's Dutch-specific focus and single-database search.
- **Low confidence**: The practical effectiveness of the ACAR framework in changing research practices, as it lacks implementation evidence or user testing.

## Next Checks
1. Replicate the RABAT assessment on a similar corpus from another high-resource health system (e.g., UK, Germany) to test cross-national generalizability.
2. Conduct a pilot implementation of the ACAR framework with 2-3 research teams to evaluate usability and impact on fairness reporting.
3. Perform sensitivity analysis by varying the RABAT risk thresholds (0.75 and 1.5) to assess the stability of the "High Risk" classifications.