---
ver: rpa2
title: 'VIBE: Video-Input Brain Encoder for fMRI Response Modeling'
arxiv_id: '2507.17958'
source_url: https://arxiv.org/abs/2507.17958
tags:
- features
- brain
- vibe
- transformer
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VIBE introduces a two-stage Transformer that fuses multi-modal
  video, audio, and text features to predict fMRI brain activity. Features from open-source
  models (Qwen2.5, BEATs, Whisper, SlowFast, V-JEPA) are integrated via a modality-fusion
  transformer and temporally decoded by a prediction transformer with rotary embeddings.
---

# VIBE: Video-Input Brain Encoder for fMRI Response Modeling

## Quick Facts
- arXiv ID: 2507.17958
- Source URL: https://arxiv.org/abs/2507.17958
- Reference count: 31
- Primary result: State-of-the-art fMRI encoding model achieving mean parcel-wise Pearson correlation of 0.3225 on in-distribution Friends S07 and 0.2125 on six out-of-distribution films

## Executive Summary
VIBE introduces a two-stage Transformer architecture that fuses multi-modal video, audio, and text features to predict fMRI brain activity. The model extracts features from specialized foundation models (Qwen2.5 for text, BEATs/Whisper for audio, SlowFast/V-JEPA for video) and integrates them through a modality-fusion transformer before temporally decoding with a prediction transformer using rotary embeddings. Trained on 65 hours of movie data from the CNeuroMod dataset and ensembled across 20 seeds, VIBE achieves state-of-the-art performance on both in-distribution and out-of-distribution cinematic stimuli, significantly outperforming baseline ridge regression models.

## Method Summary
VIBE uses a two-stage Transformer architecture: a 1-layer modality fusion transformer that cross-attends over modalities per time-point, followed by a 2-layer prediction transformer with rotary positional embeddings that processes the fused features across time. Features are extracted from specialized models including Qwen2.5 14B (text), BEATs/Whisper (audio), SlowFast/V-JEPA (video), and projected to 256 dimensions before concatenation with subject embeddings. The model is trained with a compound loss (Pearson correlation + 0.03×MSE) and ensembled across 20 seeds. Three specialized models are trained for visual, default mode, and all brain networks, with predictions combined based on parcel networks.

## Key Results
- Achieves mean parcel-wise Pearson correlation of 0.3225 on in-distribution Friends S07
- Maintains performance of 0.2125 on six out-of-distribution films
- Significantly outperforms baseline ridge regression models on both in-distribution and out-of-distribution data
- Demonstrates robust generalization across diverse cinematic styles and languages

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Feature Alignment via Specialized Encoders
The architecture maps features from modality-specific foundation models to anatomically distinct functional brain networks. Shapley value analysis confirms that text features selectively activate the default mode network while visual features drive the occipital cortex, leveraging statistical structures shared between AI representations and human sensory processing pathways.

### Mechanism 2: Bidirectional Temporal Integration (Predictive Coding)
Removing the causal mask in the Prediction Transformer allows attention to past and future time points simultaneously, better approximating brain's predictive processing dynamics. This reflects the hypothesis that current neural activity is partially a function of expected future inputs rather than purely reactive processing.

### Mechanism 3: Variance Reduction via Ensembling and Stabilized Loss
State-of-the-art performance is achieved through massive ensembling (20 seeds) and a compound loss function that combines Pearson correlation with a small MSE term (λ=0.03) to fix output scale. This mitigates high variance inherent in fMRI modeling and reduces idiosyncratic initialization noise.

## Foundational Learning

- **Rotary Positional Embeddings (RoPE)**: Needed to encode relative temporal distance between fMRI time-points without interfering with semantic content. Quick check: How does RoPE encode relative position through rotation matrices differently than additive sinusoidal embeddings?

- **Hemodynamic Response Function (HRF)**: fMRI signals are delayed relative to neural firing. VIBE learns temporal delay dynamics implicitly rather than using explicit HRF convolution. Quick check: Why might fixed HRF convolution hurt performance on diverse naturalistic movie data?

- **Cross-Attention in Multimodal Transformers**: VIBE fuses features "independently for each time point" via cross-attention rather than treating modalities as a single flat sequence. Quick check: How does TR-wise fusion differ from standard multimodal attention across time and modality?

## Architecture Onboarding

- **Component map**: Raw Stimuli -> Feature Extraction -> Dimension Projection -> Concatenation -> Modality Fusion (Per TR) -> Prediction Transformer (Across TRs w/ RoPE) -> Parcel Prediction

- **Critical path**: The model processes each TR independently through modality fusion before temporal modeling, allowing specialized attention to cross-modal interactions at each time point.

- **Design tradeoffs**: 
  - Explicit vs. Learned HRF: Transformer learns temporal dynamics implicitly rather than using canonical HRF convolution
  - General vs. Specialized Models: Three separate models (Visual, Default, All) rather than universal model
  - Omni vs. Specific Encoders: Qwen2.5 Omni added negligible value compared to specific encoders

- **Failure signatures**: Poor prediction in limbic/ventral attention regions (low SNR or internal state dependence); absolute positional embeddings degraded performance due to interference.

- **First 3 experiments**:
  1. Feature ablation: Test Qwen2.5 text vs SlowFast visual features on visual cortex parcels
  2. Temporal mechanism: Compare causal vs bidirectional mask performance
  3. Loss function: Train with pure Pearson loss (λ=0) to test MSE anchor necessity

## Open Questions the Paper Calls Out

1. **Shared vs subject-specific models**: Whether a single shared model with subject embeddings provides better generalization than training separate models for each subject. The authors hypothesized shared models would generalize better but didn't test due to time constraints.

2. **Predictive coding mechanism**: The extent to which performance gains from "backwards causality" reflect true neurophysiological predictive coding versus statistical artifacts. The limited magnitude (0.002 improvement) suggests limits to predictive capabilities.

3. **Poor region prediction**: Whether poor accuracy in limbic/intraparietal regions stems from low signal-to-noise ratio or stimuli-independent internal states. The authors note both factors but lack analysis to differentiate contributions.

## Limitations

- Architectural details of Modality Fusion Transformer underspecified (TR-wise independent fusion vs full multimodal attention across time)
- Exact integration point and initialization of subject embeddings not specified
- Limited analysis of whether AI model representations truly correspond to human sensory processing pathways versus coincidental correlations

## Confidence

- **High confidence**: Core empirical results outperforming baselines and achieving state-of-the-art performance on both in-distribution and out-of-distribution data
- **Medium confidence**: Hierarchical feature alignment mechanism via specialized encoders (Shapley analysis supports but pathway correspondence unproven)
- **Medium confidence**: Bidirectional temporal integration claim (modest 0.002 improvement reported but mechanism not fully validated)

## Next Checks

1. **Cross-attention architecture validation**: Implement both interpretations of Modality Fusion Transformer (TR-wise independent vs full multimodal attention) and compare performance to determine which matches reported results.

2. **Subject embedding verification**: Conduct ablation studies systematically removing subject embeddings to confirm their contribution and validate initialization scheme.

3. **Predictive coding mechanism test**: Implement versions with learned attention masks modeling hemodynamic delays to distinguish whether bidirectional improvement stems from predictive coding or better handling of fMRI temporal properties.