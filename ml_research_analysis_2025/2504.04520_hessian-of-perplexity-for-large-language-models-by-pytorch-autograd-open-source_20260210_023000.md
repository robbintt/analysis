---
ver: rpa2
title: Hessian of Perplexity for Large Language Models by PyTorch autograd (Open Source)
arxiv_id: '2504.04520'
source_url: https://arxiv.org/abs/2504.04520
tags:
- hessian
- matrix
- function
- diagonal
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper provides a comprehensive guide on computing the Hessian
  matrix for large language models (LLMs) using PyTorch's autograd library, addressing
  the infeasibility of computing the full Hessian due to memory constraints. The core
  method involves computing exact Hessian for small parameter subsets using automatic
  differentiation and estimating the full diagonal using Hutchinson's trick with multiple
  vector-Hessian products.
---

# Hessian of Perplexity for Large Language Models by PyTorch autograd (Open Source)

## Quick Facts
- arXiv ID: 2504.04520
- Source URL: https://arxiv.org/abs/2504.04520
- Authors: Ivan Ilin
- Reference count: 4
- Primary result: PyTorch autograd can compute exact Hessian for small parameter subsets and estimate full diagonal via Hutchinson's trick for LLMs

## Executive Summary
This paper presents a practical guide for computing Hessian matrices of perplexity for large language models using PyTorch's automatic differentiation capabilities. The work addresses the fundamental challenge that full Hessian computation is infeasible for models with millions of parameters due to memory constraints. The authors demonstrate that while exact Hessians can be computed for small parameter subsets, the full diagonal can be estimated efficiently using Hutchinson's trick with vector-Hessian products. Experiments with OPT-125M show that the Hessian structure is approximately diagonal, validating common approximations used in second-order optimization methods.

## Method Summary
The method combines exact Hessian computation for small parameter subsets using PyTorch's `torch.autograd.functional.hessian` with diagonal estimation via Hutchinson's trick. For exact computation, the perplexity function is re-parametrized to accept only the subset of parameters being analyzed. For diagonal estimation, random probe vectors are used to compute Hessian-vector products (HVPs) via `torch.autograd.functional.vhp`, with the expectation over probe vectors yielding the diagonal. The perplexity function is modified to have an additive property, allowing Hessians to be computed over smaller batches and summed to handle large batch sizes without memory overflow. The implementation is available at https://github.com/vectozavr/llm-hessian.

## Key Results
- Exact Hessian computation is feasible for parameter subsets up to ~1800 parameters using PyTorch autograd
- Full Hessian diagonal can be estimated with relative ℓ2 loss of 0.25 using 3000 Hutchinson iterations
- Diagonal structure holds empirically for OPT-125M, validating common approximations in second-order optimization
- Batch size around 60 provides stable perplexity estimates while remaining memory-efficient

## Why This Works (Mechanism)

### Mechanism 1: Automatic Differentiation for Exact Hessian Computation
PyTorch's reverse-mode AD systematically applies the chain rule to the computational graph of the perplexity function. The `torch.autograd.functional.hessian` function traces all second-order partial derivatives by differentiating the gradient computation, yielding exact derivatives to machine precision. This works because the perplexity function is implemented using PyTorch operations that support second-order gradients.

### Mechanism 2: Hutchinson's Trick for Diagonal Estimation via HVPs
For symmetric Hessian H and zero-mean random vector v with independent components, E[v ⊙ (Hv)] = H_diag. Each Hessian-vector product is computed as Hv = ∇(∇f(x)ᵀv) using a single backward pass through the gradient. Averaging over K samples converges to the true diagonal. Rademacher random vectors are preferred for lower variance than Gaussian distributions.

### Mechanism 3: Additive Perplexity Property for Memory-Efficient Batching
By removing the 1/b averaging and exponential from perplexity computation, the modified loss becomes additive across samples. Since differentiation is linear, ∇²P'(X) = Σᵢ ∇²P'([X]ᵢ). This allows computing Hessians over smaller batches and summing them, enabling analysis of large batches without memory overflow.

## Foundational Learning

- **Hessian matrix and second-order optimization**: Why needed: The entire paper centers on computing and interpreting curvature information; understanding that diagonal dominance implies certain optimization properties. Quick check: Explain why a nearly-diagonal Hessian justifies diagonal approximations in optimizers like Adam.

- **Automatic differentiation modes (forward vs. reverse)**: Why needed: PyTorch's reverse-mode AD is the core enabling technology; understanding computational graph retention is essential for debugging memory issues. Quick check: Why does computing a Hessian require retaining the gradient's computational graph?

- **Perplexity as exponentiated cross-entropy**: Why needed: The paper re-parametrizes perplexity; you must understand the standard definition to grasp what's being modified. Quick check: How does perplexity relate to cross-entropy loss, and why does batch size affect its stability?

## Architecture Onboarding

- **Component map**: Tokenized sequences X ∈ R^(b×s) -> LLM f: R^(b×s) → R^(b×s×n) -> CrossEntropyLoss g: R^(b×s×n) → R^b -> Perplexity P = exp(mean(CE)) -> Hessian module: `torch.autograd.functional.hessian` or `vhp` + Hutchinson -> Memory management: Additive decomposition for batch splitting

- **Critical path**: 1) Define subset of parameters to analyze 2) Create re-parametrized perplexity function accepting only that subset 3) Call `hessian()` for small subsets or iterate `vhp()` for diagonal estimation 4) Accumulate results across mini-batches using additive property

- **Design tradeoffs**: Exact Hessian (small subset) vs. diagonal estimation (full model); batch size b: larger b improves perplexity accuracy but increases memory; Hutchinson iterations K: more samples reduce variance but cost 25 hours on 4×A100

- **Failure signatures**: CUDA OOM when computing Hessian for >~1800 parameters directly; noisy diagonal estimates with insufficient K; numerical instability if using finite differences instead of autograd

- **First 3 experiments**: 1) Replicate Figure 1a: Compute exact 768×768 Hessian for first row of Q1 with b=140, verify diagonal dominance visually 2) Batch size sweep: Plot relative ℓ2 loss vs. b to find minimal stable batch size 3) Diagonal estimation convergence: Implement Hutchinson's trick for Q1, plot relative error vs. K to validate 3000-iteration requirement

## Open Questions the Paper Calls Out

### Open Question 1
Does the approximately diagonal structure of the Hessian persist for LLMs with significantly larger parameter counts (e.g., 7B+ parameters)? The authors state this report is part of a broader study on quantization but all experiments are conducted exclusively on OPT-125M.

### Open Question 2
Can the convergence of Hutchinson's diagonal estimation be accelerated beyond 3000 iterations to achieve the same relative error of 0.25? The paper reports requiring 3000 iterations and 25 hours on 4× A100 GPUs to reach relative ℓ₂ loss of 0.25.

### Open Question 3
Do the Hessian structure observations generalize to other loss functions beyond perplexity? The methodology is specific to the perplexity function, and the additive property derivation relies on the particular form of cross-entropy loss.

## Limitations

- Memory constraints limit exact Hessian computation to small parameter subsets (t ≤ 1800 parameters)
- Additive perplexity modification assumes the modified Hessian captures the same optimization-relevant information as the true Hessian
- Diagonal estimation requires substantial computational resources (3000 Hutchinson iterations × 25 hours on 4×A100)

## Confidence

- **High confidence**: Core mechanism of using PyTorch autograd for exact Hessian computation on small parameter subsets
- **Medium confidence**: Hutchinson trick diagonal estimation approach
- **Low confidence**: Additive perplexity property and its implications for curvature analysis

## Next Checks

1. **Verification of additive property**: Compute both the standard perplexity Hessian and the modified additive perplexity Hessian for a small parameter subset, then compare their eigenvalues and condition numbers to assess whether the modification preserves the essential curvature characteristics.

2. **Convergence analysis extension**: Perform a systematic study of diagonal estimation accuracy versus Hutchinson iterations for different batch sizes and model depths to identify optimal parameter settings and potential bottlenecks.

3. **Comparison with K-FAC approximation**: Implement a Kronecker-factored curvature approximation for the same parameter subsets and compare the estimated curvature patterns, computational costs, and memory requirements to assess relative advantages.