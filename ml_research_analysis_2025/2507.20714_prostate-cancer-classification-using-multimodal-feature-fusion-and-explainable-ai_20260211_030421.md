---
ver: rpa2
title: Prostate Cancer Classification Using Multimodal Feature Fusion and Explainable
  AI
arxiv_id: '2507.20714'
source_url: https://arxiv.org/abs/2507.20714
tags:
- cancer
- data
- class
- prostate
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces an explainable AI system for prostate cancer
  classification that fuses BERT embeddings (for textual clinical notes) with Random
  Forest (for numerical lab data). The multimodal approach leverages both structured
  data and unstructured text to improve diagnostic accuracy, particularly for intermediate
  cancer stages.
---

# Prostate Cancer Classification Using Multimodal Feature Fusion and Explainable AI

## Quick Facts
- arXiv ID: 2507.20714
- Source URL: https://arxiv.org/abs/2507.20714
- Reference count: 37
- Primary result: BERT+RF multimodal fusion achieves 98% accuracy, 99% AUC for prostate cancer staging

## Executive Summary
This study presents an explainable AI system for prostate cancer classification that combines BERT embeddings from clinical notes with Random Forest classification of numerical lab data. The multimodal approach significantly improves diagnostic accuracy, particularly for intermediate cancer stages (Classes 2 and 3), by capturing both structured and unstructured diagnostic information. Using the PLCO-NIH dataset, the model achieves 98% accuracy and 99% AUC while providing interpretable feature importance rankings through SHAP analysis. The method demonstrates strong performance (F1=89%) while maintaining computational efficiency suitable for resource-constrained healthcare settings.

## Method Summary
The method processes the PLCO-NIH prostate cancer dataset through a two-branch feature extraction pipeline. Textual clinical notes are converted to BERT embeddings (768-dimensions) using 'bert-base-uncased', then compressed to 39 principal components via PCA (retaining 98% variance). Numerical lab data (PSA levels, age, BMI) is median-imputed and concatenated with the PCA-reduced text features. The combined feature set undergoes SMOTE oversampling on training data only, followed by Random Forest classification with balanced class weights. The model employs 5-fold stratified cross-validation and SHAP analysis for interpretability.

## Key Results
- Multimodal fusion achieves 98% accuracy and 99% AUC on the PLCO-NIH dataset
- Combined features improve recall for Classes 2 and 3 to 0.900 versus 0.824 (numerical only) and 0.725 (text only)
- SHAP analysis identifies PSA levels and clinical text as top predictive features
- Model maintains F1-score of 89% while providing interpretable feature importance rankings

## Why This Works (Mechanism)

### Mechanism 1: Complementary Information Fusion
Integrating textual clinical notes with numerical lab data improves classification recall for intermediate cancer stages by providing complementary diagnostic signals. Textual data captures qualitative descriptors that numerical PSA levels miss, allowing the Random Forest to distinguish nuanced stage progression better than sparse numerical metrics alone.

### Mechanism 2: Semantic Compression via PCA
Principal Component Analysis effectively condenses high-dimensional BERT embeddings (768D to 39D) while retaining 98% of diagnostic variance. This prevents overfitting and allows the Random Forest to find meaningful splits based on dominant semantic themes in clinical notes without requiring massive training data.

### Mechanism 3: Feature Contribution Transparency (SHAP)
SHAP analysis identifies specific features driving individual stage predictions, mitigating the "black box" problem. The method calculates marginal contributions of each feature to prediction probability, allowing clinicians to understand whether Stage III predictions are driven by PSA levels or clinical text descriptions.

## Foundational Learning

- **BERT Embeddings (NLP)**: Converts unstructured clinical text into numerical vectors for machine learning processing. *Why needed*: To make clinical notes computationally usable. *Quick check*: How does the `[CLS]` token embedding represent the entire clinical note?

- **Random Forest Classifier**: Serves as the final decision engine, chosen for handling tabular data and providing feature importance. *Why needed*: Balances performance with interpretability unlike deep neural networks. *Quick check*: Why is Random Forest preferred over Gradient Boosting for resource-constrained settings?

- **SMOTE (Synthetic Minority Over-sampling Technique)**: Generates synthetic samples for minority classes to prevent model bias. *Why needed*: Addresses severe class imbalance (Stage I: 37 samples vs Stage II: 7,624). *Quick check*: Why is SMOTE applied only to training fold, not test set?

## Architecture Onboarding

- **Component map**: Input CSV → Median Imputation + BERT Tokenization → BERT `[CLS]` vector → PCA (39 components) → Concatenate Num+Text → SMOTE (train only) → StandardScaler → Random Forest → SHAP Explanation

- **Critical path**: The PCA-on-BERT transformation is most fragile. If `n_components` (set to 0.98 variance) results in too many/few dimensions, it directly impacts Random Forest's generalization ability.

- **Design tradeoffs**: Uses Random Forest + BERT (inference mode) instead of full Multimodal Deep Neural Network, trading potential accuracy gains from fine-tuning BERT for computational efficiency and easier deployment.

- **Failure signatures**: Class 0 collapse shows ~0.50 recall, indicating features lack signal to distinguish very early stages. Overfitting on text may occur if PCA components are too few.

- **First 3 experiments**:
  1. Train Random Forest using only numerical columns to establish baseline before adding text branch
  2. Vary PCA `n_components` (90%, 95%, 99% variance) to test if dimensionality bottlenecks signal
  3. Retrain excluding Class 0 to see if performance on Classes 1-3 stabilizes

## Open Questions the Paper Calls Out

1. Does integration of radiological (MRI) and histological data improve classification accuracy for intermediate stages compared to current text-lab fusion? The authors state integrating diverse data sources like radiological and pathological data is a necessary direction for comprehensive view.

2. Can the model maintain 98% accuracy and high recall when validated on external datasets from different demographic populations? The conclusion notes validating performance across diverse datasets from various clinical settings is essential but not yet performed.

3. Can advanced oversampling or few-shot learning techniques improve the low recall (0.50) observed for Stage I (Class 0) cancer? While SMOTE is applied, the ablation study reveals Class 0 recall remains significantly lower and the text acknowledges this remains a challenge.

## Limitations

- Relies on single dataset (PLCO-NIH) with severe class imbalance (Stage I: 37 samples), raising external validity concerns
- BERT embeddings extracted without domain fine-tuning, potentially limiting diagnostic specificity
- Random Forest hyperparameters remain unspecified beyond class_weight='balanced', leaving critical architectural decisions unclear
- SHAP interpretability claims depend on assumption that feature importance rankings translate meaningfully to clinical decision-making, which wasn't validated with domain experts

## Confidence

- **High Confidence**: Ablation study demonstrating combined features improve recall for Classes 2 and 3 (0.900 vs 0.824 numerical/0.725 textual)
- **Medium Confidence**: 98% accuracy and 99% AUC claims may reflect dataset-specific patterns rather than generalizable performance
- **Low Confidence**: Clinical interpretability claims from SHAP analysis lack validation that clinicians can effectively use these feature rankings in practice

## Next Checks

1. Test the pretrained model on an independent prostate cancer dataset to assess generalizability beyond the PLCO cohort

2. Have urologists evaluate whether SHAP-identified features (PSA levels, textual descriptors) align with actual clinical reasoning for staging decisions

3. Systematically vary Random Forest parameters (n_estimators, max_depth) and PCA variance thresholds to determine if reported performance is robust or optimized to specific dataset configuration