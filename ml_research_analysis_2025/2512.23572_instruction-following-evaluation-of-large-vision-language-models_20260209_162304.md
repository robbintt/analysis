---
ver: rpa2
title: Instruction-Following Evaluation of Large Vision-Language Models
arxiv_id: '2512.23572'
source_url: https://arxiv.org/abs/2512.23572
tags:
- instructions
- instruction-following
- ability
- output
- format
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study quantitatively demonstrates that large vision-language
  models (LVLMs) exhibit a decline in instruction-following ability after fine-tuning,
  even when the underlying large language models (LLMs) possess strong instruction-following
  capabilities. To investigate this phenomenon, the researchers created new training
  datasets that explicitly specify output formats, including FOVIT (Format Oriented
  Visual Instruction Tuning) and FOIT (Format Oriented Instruction Tuning) datasets.
---

# Instruction-Following Evaluation of Large Vision-Language Models

## Quick Facts
- **arXiv ID:** 2512.23572
- **Source URL:** https://arxiv.org/abs/2512.23572
- **Reference count:** 40
- **Primary result:** LVLMs exhibit a decline in instruction-following ability after visual instruction tuning, which can be mitigated by training with datasets containing explicit output format instructions.

## Executive Summary
This study demonstrates that large vision-language models (LVLMs) lose their ability to follow specific output format instructions after standard visual instruction tuning, even when their underlying large language models (LLMs) have strong instruction-following capabilities. To address this, the researchers created FOVIT and FOIT datasets that explicitly specify output formats during training. Their evaluation using verbalizer manipulation on nine binary classification tasks shows that models trained with format instructions significantly outperform those without. Critically, adding even a small fraction of FOVIT examples to existing datasets substantially improves instruction-following without degrading visual understanding performance.

## Method Summary
The researchers created four 5,000-example training datasets using COCO 2014 validation images: FOVIT (images with format instructions), NoFOVIT (images without format instructions), FOIT (captions with format instructions), and NoFOIT (captions without format instructions). GPT-4V generated captions and GPT-4 synthesized QA pairs with varying format constraints. They fine-tuned LLaVA-v1.5-7B architecture (CLIP-ViT-L-14 vision encoder frozen, 2-layer MLP projector, Llama 2-Chat 7B or Llama 3.1 8B Instruct LLM) for one epoch using AdamW optimization. Evaluation used nine binary classification datasets with verbalizer manipulation (Natural/Neutral/Unnatural labels) and LLaVA-Bench-in-the-Wild for visual understanding assessment.

## Key Results
- LVLMs trained with explicit format instructions (FOVIT/FOIT) significantly outperformed those without (NoFOVIT/NoFOIT) on instruction-following tasks
- Adding just 3-5% of FOVIT examples to standard datasets substantially improved instruction-following ability
- This improvement did not negatively impact general visual understanding performance on LLaVA-Bench-in-the-Wild
- Models trained without format instructions often produced semantically correct but instructionally wrong outputs

## Why This Works (Mechanism)

### Mechanism 1: Prior Knowledge Override via Verbalizer Manipulation
Explicit output format instructions force the model to prioritize task directives over semantic priors learned during pre-training. Verbalizer manipulation (mapping labels to arbitrary tokens) tests whether the model follows the instruction or relies on semantic meaning. Success indicates the model treats the instruction as ground truth for behavior.

### Mechanism 2: Catastrophic Forgetting Mitigation in the LLM Backbone
Standard visual instruction tuning may implicitly deprioritize strict structural adherence, degrading the LLM's original ability to follow complex constraints. Explicit format instructions act as a regularizer to preserve this capability by maintaining gradient updates for structural constraint satisfaction.

### Mechanism 3: Low-Data Interpolation for Format Adherence
The mechanism relies on the presence of explicit format signals to shift the output distribution rather than requiring massive datasets. Adding even a small fraction (3-5%) of FOVIT data to standard datasets triggers a mode shift where the model learns to attend to constraint tokens in the prompt.

## Foundational Learning

- **Concept: Verbalizer Manipulation**
  - *Why needed here:* To distinguish whether a model is actually "following instructions" or just outputting the most probable semantic completion
  - *Quick check question:* If I tell the model to output "banana" for a sad movie review, does it comply (instruction following) or output "sad" (semantic prior)?

- **Concept: Visual Instruction Tuning (VIT)**
  - *Why needed here:* The standard process for training LVLMs, which the paper argues is flawed because standard VIT datasets lack strict format constraints
  - *Quick check question:* Does the training data teach the model *what* to see (grounding) or *how* to speak (formatting)?

- **Concept: Projection Layer (Adapter)**
  - *Why needed here:* The interface between the Vision Encoder and the LLM; understanding that the LLM remains the "text generator" is crucial to understanding why the LLM's inherent instruction-following ability is the asset being lost or preserved
  - *Quick check question:* Is the error occurring because image features are wrong (Vision Encoder/Projector failure) or because text generation is unstructured (LLM failure)?

## Architecture Onboarding

- **Component map:** CLIP-ViT-L-14 (frozen) -> 2-layer MLP projector (trainable) -> Llama 2-Chat 7B/Llama 3.1 8B Instruct (trainable)

- **Critical path:** Image + Text (Question + Format Instruction) -> Vision Encoder -> Visual Tokens -> Projector -> LLM Embedding Space -> LLM processes [Visual Tokens + Text Tokens] -> Output String -> Validate against format instruction

- **Design tradeoffs:** Synthetic data (GPT-4V) vs. human data; freezing vision encoder stabilizes visual grounding but limits adaptation to new instruction formats

- **Failure signatures:** Verbose refusal (ignores "output only 'yes' or 'no'" constraint), semantic override (outputs semantically correct label despite unnatural instruction)

- **First 3 experiments:**
  1. **Sanity Check:** Run evaluation suite on base LLM vs. integrated LVLM to quantify immediate "decline" post-visual-tuning
  2. **Data Ablation:** Train two LVLMs (standard LLaVA vs. LLaVA + 5% FOVIT) and compare F1 scores on unnatural label set
  3. **Qualitative Audit:** Inspect LVLM_NoFOVIT outputs on binary tasks to confirm if failures are due to lack of understanding or instruction adherence

## Open Questions the Paper Calls Out

1. **How can hallucinations and artifacts in GPT-4V synthetic data be mitigated?** The study relies on synthetic data with 5-17% inconsistency rates but focuses on format inclusion rather than cleaning the generation pipeline.

2. **Does FOVIT improvement transfer to open-ended generation and multi-turn dialogues?** Verbalizer manipulation focuses on binary classification, leaving complex instruction adherence and open-ended tasks untested.

3. **What are the underlying internal mechanisms causing instruction-following decline?** The study demonstrates data composition effects but doesn't analyze specific weight changes or attention shifts responsible for the initial forgetting.

## Limitations

- **Dataset Generalization Gap:** FOVIT/FOIT datasets use synthetic GPT-4V/GPT-4 data which may not capture full diversity of real-world visual instruction-following scenarios
- **Evaluation Scope Constraint:** Verbalizer manipulation focuses on binary classification tasks, potentially not capturing performance on complex multi-step instructions
- **Architecture Specificity:** Results primarily demonstrated on LLaVA-v1.5-7B architecture, uncertain if findings generalize to other LVLM architectures

## Confidence

- **High Confidence:** LVLMs with explicit format instructions significantly outperform those without on verbalizer manipulation tasks
- **Medium Confidence:** Adding 3-5% of FOVIT examples improves instruction-following without harming visual understanding
- **Low Confidence:** Degradation is primarily due to catastrophic forgetting rather than capacity limitations or architectural constraints

## Next Checks

1. **Cross-Domain Robustness Test:** Evaluate models on visual instruction-following tasks from domains not represented in COCO (medical imaging, satellite imagery, etc.)

2. **Multi-Step Instruction Evaluation:** Design evaluation tasks requiring complex, multi-step instructions to test if format instruction benefits extend to sophisticated reasoning chains

3. **Ablation on Synthetic Data Quality:** Create FOVIT using human-annotated data for a subset and compare performance against fully synthetic version to quantify hallucination impact