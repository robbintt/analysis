---
ver: rpa2
title: Efficient Adaptation of Reinforcement Learning Agents to Sudden Environmental
  Change
arxiv_id: '2505.10330'
source_url: https://arxiv.org/abs/2505.10330
tags:
- learning
- world
- novelty
- policy
- adaptation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This dissertation addresses the challenge of enabling reinforcement
  learning (RL) agents to efficiently adapt to sudden environmental changes during
  deployment without catastrophic forgetting. The core contribution is demonstrating
  that efficient online adaptation requires two key capabilities: prioritized exploration
  and sampling strategies that reduce distribution shift, and selective preservation
  of prior knowledge through structured representations.'
---

# Efficient Adaptation of Reinforcement Learning Agents to Sudden Environmental Change

## Quick Facts
- **arXiv ID:** 2505.10330
- **Source URL:** https://arxiv.org/abs/2505.10330
- **Reference count:** 40
- **Primary result:** Online adaptation to environmental changes requires prioritized exploration and selective preservation of prior knowledge.

## Executive Summary
This dissertation addresses the challenge of enabling reinforcement learning agents to efficiently adapt to sudden environmental changes during deployment without catastrophic forgetting. The core contribution is demonstrating that efficient online adaptation requires two key capabilities: prioritized exploration and sampling strategies that reduce distribution shift, and selective preservation of prior knowledge through structured representations. The work introduces a formal framework for studying online test-time adaptation (OTTA) to novelty in sequential decision making, along with the Novelty Minigrid (NovGrid) test environment and evaluation metrics.

## Method Summary
The dissertation develops multiple complementary approaches to online adaptation. DOPS (Dual Objective Priority Sampling) extends DreamerV3 with dual-buffer sampling, prioritizing high-loss data for world model updates and low-TD-error data for policy updates. WorldCloner combines neural policy learning with symbolic rule representation for rapid adaptation in discrete environments. Concept Bottleneck World Models (CBWMs) introduce an interpretable bottleneck layer that enforces concept disentanglement, allowing selective updating of changed environmental factors while preserving stable knowledge. These methods are evaluated on NovGrid for discrete control and RWRL/DMControl for continuous control tasks.

## Key Results
- Exploration strategies emphasizing stochasticity (NoisyNets) and explicit diversity (RE3) are most effective for adaptation across different novelty types.
- DOPS improves adaptation efficiency by decoupling sampling distributions for world model and policy/critic updates, reducing objective mismatch.
- CBWMs enable selective preservation of prior knowledge through concept disentanglement, preventing catastrophic forgetting during adaptation.

## Why This Works (Mechanism)

### Mechanism 1: Task-Agnostic Stochasticity and Diversity
- **Claim:** Exploration strategies with stochasticity or explicit diversity improve adaptation by preventing overfitting to source domain state distributions.
- **Mechanism:** Broader state-action space coverage increases likelihood of sampling relevant experiences when environments shift, reducing required distribution shift.
- **Core assumption:** Optimal target policies overlap with diverse behaviors from training, or nearby actions remain valid.
- **Evidence anchors:** Section 4.4 shows stochastic/diversity methods adapt most efficiently despite slower source convergence. Section 4.4 notes explicit diversity prevents stale transitions.

### Mechanism 2: Objective-Aligned Data Sampling (DOPS)
- **Claim:** Decoupling sampling distributions for world model and policy/critic mitigates objective mismatch in model-based RL.
- **Mechanism:** High-loss data benefits world model; low-TD-error data prevents critic gradient overshooting. DOPS uses separate batches for each component.
- **Core assumption:** Latent space dynamics are distinct enough that separate sampling improves signal-to-noise ratio.
- **Evidence anchors:** Section 5.2.3 shows limiting L2 data reduces critic gradient overshoot. Section 5.2.2 identifies uniform sampling as suboptimal.

### Mechanism 3: Concept Disentanglement for Selective Preservation
- **Claim:** Bottleneck-enforced concept disentanglement prevents catastrophic forgetting by isolating task-relevant concepts.
- **Mechanism:** Orthogonality loss and supervision ensure gradients from novel domains only modify weights associated with changed environmental factors.
- **Core assumption:** Ground-truth concept labels are available during pre-training and concepts are adequately represented.
- **Evidence anchors:** Section 7.5 shows CWBM avoids overwriting reusable concept knowledge. Section 7.2.2 describes orthogonality loss pushing concept embeddings apart.

## Foundational Learning

- **Concept: Online Test-Time Adaptation (OTTA)**
  - **Why needed here:** Specific problem framing requiring adaptation during deployment without dedicated safety net, unlike offline fine-tuning.
  - **Quick check question:** Can you distinguish between "stationary MDP," "non-stationary process," and "novelty" constraint?

- **Concept: Catastrophic Interference (Forgetting)**
  - **Why needed here:** Primary failure mode addressed - explains why continuing training on new data destroys previously learned valid knowledge.
  - **Quick check question:** Why does standard gradient descent fail when data distribution shifts suddenly?

- **Concept: Objective Mismatch in Model-Based RL**
  - **Why needed here:** Crucial for understanding DOPS contribution - explains why optimizing world model and policy simultaneously with same data is suboptimal.
  - **Quick check question:** In Dreamer, why might data good for world model be bad for critic?

## Architecture Onboarding

- **Component map:** NovGrid -> DOPS (DreamerV3 extension) -> WorldCloner (PPO + Symbolic Rule Learner) -> CBWM (DreamerV3 with CEM layer)

- **Critical path:**
  1. Pre-training: Train agent on Source Task using Dreamer+DOPS or CBWM
  2. Novelty Injection: Environment shifts (NovGrid)
  3. Detection/Observation: Agent encounters new state/reward dynamics
  4. Selective Update: DOPS samples specific batches → updates World Model & Critic separately; CBWM backpropagates error → updates specific concept embeddings

- **Design tradeoffs:**
  - Symbolic (WorldCloner) vs. Neural (CBWM): WorldCloner offers instant symbolic updates but struggles with complex continuous dynamics; CBWM handles complex dynamics but requires concept supervision
  - Exploration Speed vs. Adaptation: Stochastic methods adapt faster but converge slower initially

- **Failure signatures:**
  - Shortcut Blindness: Agents fail to adapt to easier tasks if relying on separate objective exploration or greedy policies
  - Gradient Overshooting: High TD-error data from novelty causes value function collapse in standard Dreamer
  - World Model Overfitting: World model predicts negative consequences for novel safe states, preventing policy updates

- **First 3 experiments:**
  1. Implement PPO with NoisyNets vs. ICM on NovGrid "DoorKey" task to validate stochasticity > separate objective claim
  2. Modify standard DreamerV3 with DOPS (split TD-error priorities) on Walker2d "ThighLengthChange" novelty
  3. Train CBWM on LIBERO, fine-tune on target task, measure cosine similarity of concept embeddings before/after adaptation

## Open Questions the Paper Calls Out

- **Open Question 1:** Can CBWMs be extended to discover concepts without supervision?
  - Basis: Page 130 asks about extending CBWMs to "discover" concepts without prior knowledge
  - Why unresolved: Current CBWMs rely on researcher-provided concept labels and supervision
  - What evidence would resolve it: Integrating unsupervised object-centric modeling techniques like slot attention or CLIP

- **Open Question 2:** Can symbolic world models like WorldCloner handle complex continuous dynamics?
  - Basis: Page 99 notes rule learning for complex continuous dynamics is challenging without prior knowledge
  - Why unresolved: AABI-based rule learner designed for discrete, deterministic environments
  - What evidence would resolve it: Extending rule learner to utilize dynamic movement primitives

- **Open Question 3:** How can the difference between source and target MDPs be quantitatively measured?
  - Basis: Page 128 states need to "more precisely quantify how and how much environments change"
  - Why unresolved: Current ontology characterizes novelty qualitatively without quantitative MDP similarity measures
  - What evidence would resolve it: Development of metrics quantifying transformation between source and target MDPs

## Limitations
- Effectiveness depends heavily on novelty type, with continued struggles on "Shortcut Novelties" where tasks become easier
- Trade-off between exploration efficiency and initial convergence speed remains empirically driven
- Neurosymbolic approaches like WorldCloner don't scale well to complex continuous environments

## Confidence
- **High Confidence:** Experimental validation of DOPS in DreamerV3 with measurable improvements in adaptive efficiency across NovGrid and RWRL tasks
- **Medium Confidence:** Theoretical framework for OTTA and distinction from traditional transfer learning
- **Low Confidence:** Scalability of neurosymbolic approaches to complex continuous environments

## Next Checks
1. **Exploration Ablation Study:** Controlled comparison between stochastic exploration methods (NoisyNets, RE3) and deterministic alternatives on simplified NovGrid environment to isolate exploration diversity's contribution

2. **DOPS Sensitivity Analysis:** Systematically vary overlap parameter W and sampling ratio between Curious Replay and uniform sampling across multiple novelty types to determine optimal configurations

3. **Concept Bottleneck Generalization:** Test CBWMs on domain with partially corrupted or missing concept labels to evaluate robustness when supervision is imperfect