---
ver: rpa2
title: Do Chains-of-Thoughts of Large Language Models Suffer from Hallucinations,
  Cognitive Biases, or Phobias in Bayesian Reasoning?
arxiv_id: '2503.15268'
source_url: https://arxiv.org/abs/2503.15268
tags:
- reasoning
- blocks
- they
- natural
- card
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated how Large Language Models (LLMs) with Chain-of-Thought
  (CoT) reasoning handle Bayesian reasoning problems, specifically examining their
  use of ecologically valid strategies like natural frequencies, whole objects, and
  embodied heuristics. Using four stages of prompts, the research tested three LLMs
  (GEMINI Flash 2.0, ChatGPT o3-mini, and DeepSeek R1) on a Bayesian lie detection
  problem designed for elementary students.
---

# Do Chains-of-Thoughts of Large Language Models Suffer from Hallucinations, Cognitive Biases, or Phobias in Bayesian Reasoning?

## Quick Facts
- arXiv ID: 2503.15268
- Source URL: https://arxiv.org/abs/2503.15268
- Reference count: 40
- Key outcome: LLMs consistently revert to symbolic probability reasoning despite explicit prompts to use natural frequencies and embodied heuristics, displaying a "phobia" toward ecologically valid strategies.

## Executive Summary
This study investigates how Large Language Models (LLMs) with Chain-of-Thought (CoT) reasoning handle Bayesian reasoning problems, specifically examining their use of ecologically valid strategies like natural frequencies, whole objects, and embodied heuristics. Using four progressive prompts, the research tested three LLMs (GEMINI Flash 2.0, ChatGPT o3-mini, and DeepSeek R1) on a Bayesian lie detection problem. The results show that LLMs autonomously rely on symbolic reasoning with probabilities rather than intuitive strategies, displaying a persistent cognitive bias or "phobia" toward natural frequencies and embodied heuristics. Even with explicit prompts, their use of these strategies was inconsistent and superficial.

## Method Summary
The study used a Bayesian reasoning "lie detection" problem where students determine which book contains a hidden card based on probabilistic clues from animal characters. Four progressive prompts were tested: (1) zero-shot baseline, (2) with natural frequencies instruction, (3) with plastic blocks instruction, and (4) with colored blocks and pedagogical framing. Three LLMs were evaluated: GEMINI Flash 2.0, ChatGPT o3-mini, and DeepSeek R1 (February 2025). The methodology assessed correctness, strategy adoption, consistency of strategy use, and reasoning quality.

## Key Results
- LLMs consistently defaulted to symbolic probability reasoning despite explicit instructions to use natural frequencies and embodied heuristics
- Even with forced heuristic prompts, models showed superficial adherence while reverting to probabilistic calculations
- The Einstellung effect was observed, where learned formal strategies inhibited the application of simpler requested strategies

## Why This Works (Mechanism)

### Mechanism 1: Training Distribution Bias Toward Symbolic Formalism
LLMs default to symbolic, abstract probability reasoning likely due to over-representation of formal mathematical expert data in training corpora. The model's weights encode a strong prior for "expert-like" reasoning patterns (formulas, percentages) found in academic papers and textbooks. When faced with uncertainty, the activation of these dominant neural pathways overrides less frequent "intuitive" or "heuristic" patterns.

### Mechanism 2: The Einstellung Effect in Reasoning Paths
Chain-of-Thought reasoning exhibits rigidity, where a familiar problem-solving method (Bayes' theorem) inhibits the exploration or maintenance of a requested alternative strategy (counting blocks). CoT is path-dependent, and once the model initiates a "standard" mathematical solution plan, the sequential token prediction locks into that logic stream.

### Mechanism 3: Superficial Adherence vs. Deep Simulation
LLMs can simulate the *language* of a requested heuristic (e.g., mentioning blocks) without simulating the *logic* of that heuristic. The model satisfies the prompt's semantic constraint by generating relevant tokens, but the underlying computational logic remains driven by the dominant symbolic reasoning module.

## Foundational Learning

- **Natural Frequencies vs. Probabilities**
  - Why needed here: To understand the target "ecologically valid" strategy the models are failing to use. Humans reason better with "90 out of 100" (frequencies) than "0.9" (probabilities).
  - Quick check question: Convert the probability "20% prevalence" into a natural frequency format for a population of 50.

- **The Einstellung Effect**
  - Why needed here: To diagnose *why* the model fails to switch strategies. It explains the phenomenon where a learned solution (formulas) blocks the discovery or application of a simpler, requested solution (counting).
  - Quick check question: If an expert uses a complex formula to solve 2+2 because they are "prone to complexity," is this an efficient strategy or a cognitive trap?

- **Chain-of-Thought (CoT) Fidelity**
  - Why needed here: To evaluate the reliability of the "reasoning" shown in the paper. The paper demonstrates that a long CoT can be a mask for incorrect sub-reasoning or inconsistency.
  - Quick check question: Does a long, detailed explanation guarantee a correct conclusion if the intermediate steps contradict each other?

## Architecture Onboarding

- **Component map:** Input Layer (User Prompt) -> Reasoning Core (Pre-trained LLM) -> Process (Chain-of-Thought Generation) -> Output (Textual Explanation) -> Evaluator (Human/model checker)
- **Critical path:** The transition from Prompt 1 (Zero-shot) to Prompt 4 (Forced Heuristic). The critical failure point is the *fidelity of the reasoning path* to the requested strategy.
- **Design tradeoffs:** Symbolic Robustness vs. Ecological Validity - models are optimized for math accuracy at the expense of human-like intuition. Prompt Complexity - increasing specificity sometimes confuses the model.
- **Failure signatures:** Hybrid Reasoning (calculates with percentages but describes with blocks), Strategy Reversion (starts with "blocks" then switches to Bayes), Correct Answer Wrong Logic (ignores contradictory evidence).
- **First 3 experiments:**
  1. Ablation of Notation: Run the problem forbidding "%", "probability", and "formula" tokens to force frequency-based reasoning.
  2. Strategy Consistency Check: Run Prompt 4 (Blocks/Colors) 10 times and measure frequency of "symbolic reversion" instances.
  3. Comparative Logic Trace: Manually trace DeepSeek vs. ChatGPT logic on Prompt 3 to identify if "blocks" are used as variables or merely labels.

## Open Questions the Paper Calls Out

- Can fine-tuning LLMs on "synchronized thinking datasets" (human verbalized thoughts during action) eliminate the cognitive bias against ecologically valid strategies?
- Do advanced prompting techniques exist that can reliably override the "probability phobia" and Einstellung effect observed in LLMs?
- Is the observed cognitive bias towards symbolic reasoning consistent across diverse Bayesian problem domains, such as medical diagnosis or legal reasoning?

## Limitations

- The study does not provide empirical evidence about the actual composition of LLM training corpora or quantify the prevalence of "ecologically valid" reasoning patterns.
- Only one Bayesian reasoning problem was tested, limiting generalization to other domains.
- The mechanisms proposed (training bias, Einstellung effect) are theoretically plausible but require validation through controlled experiments that isolate these factors.

## Confidence

- **High Confidence:** LLMs consistently revert to symbolic probability reasoning despite explicit instructions to use natural frequencies and embodied heuristics.
- **Medium Confidence:** The interpretation that this represents a "phobia" or cognitive bias analogous to human biases like the Einstellung effect.
- **Low Confidence:** The claim that this bias stems specifically from training data distribution favoring expert-edited formal texts.

## Next Checks

1. Conduct a systematic analysis of LLM training corpora to quantify the relative prevalence of formal mathematical language versus naturalistic, frequency-based reasoning patterns.
2. Design an experiment where models are explicitly forbidden from using probability notation to test whether they can successfully reason using only natural frequencies and embodied heuristics.
3. Test whether the observed bias toward symbolic reasoning extends to other domains where intuitive strategies might be more effective, such as spatial reasoning or everyday decision-making problems.