---
ver: rpa2
title: 'SoM-1K: A Thousand-Problem Benchmark Dataset for Strength of Materials'
arxiv_id: '2509.21079'
source_url: https://arxiv.org/abs/2509.21079
tags:
- uni00000030
- uni00000014
- uni00000013
- engineering
- uni00000052
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SoM-1K, the first large-scale multimodal
  benchmark dataset for evaluating foundation models on strength of materials problems.
  The dataset contains 1,065 annotated problems that require reasoning across both
  textual problem statements and engineering diagrams.
---

# SoM-1K: A Thousand-Problem Benchmark Dataset for Strength of Materials

## Quick Facts
- **arXiv ID:** 2509.21079
- **Source URL:** https://arxiv.org/abs/2509.21079
- **Reference count:** 33
- **Key outcome:** First large-scale multimodal benchmark for evaluating foundation models on strength of materials problems, revealing significant performance gaps and proposing DoI prompting strategy

## Executive Summary
SoM-1K introduces a thousand-problem benchmark dataset for evaluating foundation models on strength of materials engineering problems. The dataset contains 1,065 annotated problems requiring reasoning across textual problem statements and engineering diagrams. Due to current models' limitations in interpreting complex visual information, the authors propose a novel Descriptions of Images (DoI) prompting strategy where expert-generated text descriptions replace visual diagrams. Evaluation of eight representative foundation models reveals that current models struggle significantly with these engineering problems, with the best-performing model achieving only 56.6% accuracy. Interestingly, LLMs provided with DoI often outperform VLMs provided with visual diagrams, suggesting that accurate text-based descriptions can be more effective than direct image input for current foundation models.

## Method Summary
The study constructs a multimodal benchmark dataset containing 1,065 annotated strength of materials problems sourced from textbooks and competitions. Each problem includes four components: Problem Statement (PS), Schematic Diagram (Image), Description of Image (DoI), and Ground Truth (GT). Three prompting strategies are evaluated: PS+I (VLMs), PS+I+DoI (VLMs), and PS+DoI (LLMs and VLMs). Five independent responses per problem are generated using respective model APIs, with majority voting via DeepSeek-V3-0324 API for final answer selection. DoI is generated by Doubao VLM then human-refined, and LaTeX is converted to natural language via DeepSeek-V3-0324. Human experts evaluate all responses using binary scoring (1 = valid reasoning AND correct answer, 0 = otherwise).

## Key Results
- Best-performing model (Qwen-plus with DoI) achieves 56.6% accuracy, while worst-performing (GPT-3.5) reaches only 8.3%
- VLMs with images perform worse (48.5% average) than LLMs with DoI descriptions (56.6% average)
- DoI strategy significantly reduces visual extraction errors, with Doubao's Type E error rate dropping from 19% under PS+I to 3% under PS+DoI
- Performance correlates positively with model scale, with DeepSeek-R1 (671B) achieving 52.4% accuracy versus Llama (70B) at 9.1%

## Why This Works (Mechanism)

### Mechanism 1: Semantic Decoupling via Description of Images (DoI)
- **Claim:** Providing expert-generated textual descriptions of engineering diagrams improves reasoning performance more effectively than direct visual ingestion
- **Mechanism:** Bypasses visual encoding bottleneck by converting high-context visual schematics into explicit natural language, allowing stronger text-reasoning pathways to operate without visual misinterpretation noise
- **Core assumption:** Expert-generated description accurately captures all relevant visual information required for physical reasoning
- **Evidence anchors:** LLMs with DoI often outperform VLMs with visual diagrams; DoI crucial for mitigating visual misinterpretation errors; VLMs struggle with visual extraction in scientific domains

### Mechanism 2: Visual Extraction as the Primary Failure Mode
- **Claim:** Performance gap between VLMs and LLMs+DoI driven primarily by "Extraction Errors" rather than calculation or logic failures
- **Mechanism:** VLMs lack specialized embeddings to distinguish subtle engineering markers, leading to fundamentally wrong force analyses when visual extraction fails
- **Core assumption:** Models possess underlying mathematical and physical knowledge to solve problems if initial state is correctly identified
- **Evidence anchors:** Type E defined as failing to interpret or extract information from prompts; VLMs show significantly higher Type E errors with images compared to text

### Mechanism 3: Parameter Scaling in Low-Data Regimes
- **Claim:** Performance correlates positively with model scale, particularly for text-only reasoning
- **Mechanism:** Larger models possess broader latent knowledge bases covering physics and mechanics, allowing better retrieval of relevant formulas and principles
- **Core assumption:** Model's reasoning capacity is not saturated by language task complexity, leaving capacity for domain knowledge retrieval
- **Evidence anchors:** Larger LLMs generally perform better; DeepSeek-R1 (671B) achieves 52.4% accuracy versus Llama (70B) at 9.1%

## Foundational Learning

- **Concept: Boundary Conditions (Supports)**
  - **Why needed here:** Distinguishing between "hinge supports" and "roller supports" is critical visual extraction task; misidentifying leads to fundamentally wrong force analyses
  - **Quick check question:** Does a roller support resist a moment (rotation)?

- **Concept: Static Determinacy**
  - **Why needed here:** Dataset split into "statically determinate" (easy) and "statically indeterminate" (hard); understanding difference required to interpret why certain problems are harder for AI
  - **Quick check question:** Can you solve for all internal forces in a statically determinate beam using only equilibrium equations?

- **Concept: Multimodal Grounding**
  - **Why needed here:** Core challenge is mapping textual symbols to visual elements in diagrams; understanding this mapping necessary to evaluate why DoI helps
  - **Quick check question:** If text says "downward load P" but image shows upward arrow, which modality should model trust?

## Architecture Onboarding

- **Component map:** Problem Statement (Text) + Schematic (Image) or Description of Image (Text) -> Model Layer (VLM or LLM) -> Majority Voting mechanism (5 responses) -> Expert Human Review (Binary scoring)

- **Critical path:** The transition from Image -> Text (DoI). If this human-in-the-loop translation is inaccurate, the LLM will fail. If the VLM attempts this directly (Image -> Internal Rep), it currently shows high error rates.

- **Design tradeoffs:**
  - **DoI Strategy:** High accuracy (mitigates Type E errors) but low scalability (requires human experts to write descriptions)
  - **VLM Direct Strategy:** Low accuracy (~48% vs ~56%) but high scalability (no human preprocessing required)

- **Failure signatures:**
  - **Type E (Extraction):** Model invents dimensions or swaps support types (e.g., "fixed" vs "pinned")
  - **Type K (Knowledge):** Model sets up correct geometry but applies wrong formula (e.g., wrong moment of inertia)
  - **Type C (Calculation):** Correct logic, but arithmetic error in final step

- **First 3 experiments:**
  1. **VLM vs. LLM+DoI Baseline:** Run full test set on VLM with raw images vs. strong LLM with human-written DoI to establish "visual tax" gap
  2. **Error Taxonomy Validation:** Take 100 failures and classify into K, C, E, O to determine if bottleneck is visual (E) or reasoning (K/C)
  3. **Majority Vote Sensitivity:** Test if n=5 majority voting significantly reduces Type C errors compared to n=1, distinguishing random from systematic errors

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does foundation model performance on strength of materials generalize to advanced engineering domains like structural dynamics, plasticity, and nonlinear mechanics?
- **Basis in paper:** Conclusion states future research should expand beyond SoM-1K to include "more advanced engineering domains like structural dynamics, plasticity, and nonlinear mechanics"
- **Why unresolved:** Current dataset limited to quasi-static problems and does not contain time-dependent or non-linear physical phenomena
- **What evidence would resolve it:** Evaluation results from new multimodal benchmark for structural dynamics and nonlinear mechanics problems

### Open Question 2
- **Question:** Can foundation models be enhanced to reliably generate accurate scientific diagrams, such as internal force diagrams or deformation shapes?
- **Basis in paper:** Conclusion notes ability to "reliably generate accurate scientific diagrams... remains a significant hurdle for current foundation models"
- **Why unresolved:** Current study focused on interpreting existing images rather than generating new ones, and models still struggle with basic visual extraction
- **What evidence would resolve it:** Study evaluating geometric and numerical accuracy of model-generated engineering diagrams against ground truth solutions

### Open Question 3
- **Question:** To what extent does integrating specialized external computational tools mitigate the calculation (Type C) and knowledge-based (Type K) errors identified in error analysis?
- **Basis in paper:** Discussion suggests Type K and C errors "could potentially be mitigated through external tools," and Conclusion calls for models to "integrate more effectively with specialized tools"
- **Why unresolved:** Current evaluation isolated model reasoning capabilities without systematically testing agentic workflows utilizing external solvers
- **What evidence would resolve it:** Comparative performance metrics between standard foundation models and tool-augmented agents on SoM-1K dataset

## Limitations

- **Scalability constraint:** Expert-generated DoI descriptions require human-in-the-loop preprocessing, making the approach unsustainable for real-world deployment
- **Evaluation granularity:** Binary accuracy scoring without examining quality of intermediate reasoning steps or partial credit scenarios limits understanding of model capabilities
- **Domain specificity:** Results may not generalize to domains requiring different types of multimodal reasoning or more complex physical phenomena

## Confidence

- **High Confidence:** Dataset construction methodology and problem categorization are well-documented and reproducible; error taxonomy provides useful framework for understanding model failures
- **Medium Confidence:** Core finding that LLMs+DoI outperform VLMs on these tasks is supported by data, but specific performance gaps may shift as models improve visual reasoning capabilities
- **Medium Confidence:** Claim that visual extraction is primary failure mode is plausible given error analysis, but alternative explanations cannot be ruled out with current experimental design

## Next Checks

1. **Automated DoI Generation:** Test whether current VLMs can generate accurate DoI descriptions without human refinement, potentially enabling scalable deployment of the approach

2. **Intermediate Reasoning Quality:** Implement partial credit evaluation system examining correctness of individual reasoning steps, not just final answers, to better understand where models fail

3. **Cross-Domain Generalization:** Evaluate whether models that perform well on SoM-1K can transfer multimodal reasoning capability to other engineering domains like thermodynamics or fluid mechanics