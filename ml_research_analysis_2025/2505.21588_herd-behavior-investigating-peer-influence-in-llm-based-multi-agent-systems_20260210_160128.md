---
ver: rpa2
title: 'Herd Behavior: Investigating Peer Influence in LLM-based Multi-Agent Systems'
arxiv_id: '2505.21588'
source_url: https://arxiv.org/abs/2505.21588
tags:
- agents
- herd
- peer
- behavior
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates herd behavior in LLM-based multi-agent
  systems, where agents align their outputs with peers' responses. The authors conduct
  controlled experiments using six benchmarks (factual and opinionated) to explore
  how self-confidence, perceived peer confidence, and peer information presentation
  format influence conformity.
---

# Herd Behavior: Investigating Peer Influence in LLM-based Multi-Agent Systems

## Quick Facts
- **arXiv ID**: 2505.21588
- **Source URL**: https://arxiv.org/abs/2505.21588
- **Reference count**: 14
- **Primary result**: Flip rates peak when agents have low self-confidence and perceive peers as confident, with the most persuasive peer response (2nd) driving strongest herding (avg. 0.48 flip rate) but reducing accuracy on factual tasks.

## Executive Summary
This study investigates herd behavior in LLM-based multi-agent systems, where agents align their outputs with peers' responses. The authors conduct controlled experiments using six benchmarks (factual and opinionated) to explore how self-confidence, perceived peer confidence, and peer information presentation format influence conformity. Key findings show that flip rates peak when agents have low self-confidence and perceive peers as confident, with the most persuasive peer response (2nd) driving strongest herding (avg. 0.48 flip rate) but reducing accuracy on factual tasks. Presentation format significantly impacts behavior, with reason-based formats yielding highest flip rates (0.63) and group accuracy (0.29). The study demonstrates that herd behavior can be systematically tuned, and appropriately calibrated conformity enhances collaborative outcomes, offering design principles for adaptive multi-agent systems.

## Method Summary
The authors use gpt-4o-mini-2024-07-18 with τ=0 (τ=1 for 5-agent diversity) to measure herd behavior through flip rate - the fraction of agents changing answers after seeing peer inputs. For each question, they extract top-k token probabilities for responses, construct peer prompts per condition (1st/2nd/rnd/last), and format peer information using five methods (Count/Ratio/List/Disc/Reason) in specified orders (Agree First/Disagree First). They compute flip rate, entropy, consensus rate, and accuracy across six benchmarks (factual: MMLU-Pro, GPQA-Diamond, ARC-Challenge; opinionated: OpinionQA, GlobalOpinionQA, SOCIAL IQA) with 200 questions per benchmark (500 for MMLU-Pro and GlobalOpinionQA).

## Key Results
- Flip rates peak at 0.48 when agents have low self-confidence and perceive peers as highly confident, with the 2nd peer response driving strongest herding
- Reason-based presentation format yields highest flip rates (0.63) and group accuracy (0.29) on factual tasks
- Leading with disagreement triggers stronger herding than leading with agreement, analogous to first-impression bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Herd behavior intensity is primarily driven by the gap between an agent's self-confidence and its perceived confidence in peers.
- Mechanism: When self-confidence is low and perceived peer confidence is high, agents are more likely to flip their initial answer. The 2nd most probable peer response triggers the strongest conformity.
- Core assumption: Agents process confidence signals as Bayesian-style evidence, weighting peer input inversely to their own certainty.
- Evidence anchors:
  - [abstract]: "flip rates peak when agents have low self-confidence and perceive peers as confident, with the most persuasive peer response (2nd) driving strongest herding (avg. 0.48 flip rate)"
  - [section 3.3]: "flip rates are highest when self-confidence is low and perceived peer confidence is high"
- Break condition: When self-confidence is high (probability > 0.7), flip rates drop sharply regardless of peer confidence.

### Mechanism 2
- Claim: Presentation format of peer information modulates conformity strength, with reason-based formats yielding highest flip rates.
- Mechanism: Numerical summaries (Count, Ratio) enable clear comparative reasoning with sharp decision boundaries, while reason-based formats (Reason) provide justifications that increase persuasive power even when agreement outnumbers disagreement.
- Core assumption: Agents weight informational content over mere social signals when justifications are provided.
- Evidence anchors:
  - [abstract]: "Presentation format significantly impacts behavior, with reason-based formats yielding highest flip rates (0.63) and group accuracy (0.29)"
  - [section 4.3]: "the Reason format results in the highest overall flip rates, even when agreement is greater"
- Break condition: On opinionated benchmarks, reason-based effects are negligible; format primarily influences objective tasks.

### Mechanism 3
- Claim: Presentation order creates framing effects that systematically influence conformity direction and magnitude.
- Mechanism: Leading with disagreement triggers stronger herding than leading with agreement, analogous to first-impression bias - early negative signals shift priors more aggressively.
- Core assumption: Sequential information processing creates recency/primacy weighting despite identical information content.
- Evidence anchors:
  - [section 4.3]: "When disagreement is presented first, herding behavior is generally stronger"
  - [section 6.1]: "leading with disagreement encourages greater conformity than leading with agreement"
- Break condition: When agreeing agents outnumber disagreeing by 3+, order effects diminish substantially.

## Foundational Learning

- Concept: **Confidence as token probability**
  - Why needed here: Flip rates depend on comparing self-confidence P(r|q) to perceived confidence P(r_peer|q) extracted from logit distributions.
  - Quick check question: Given logit scores [2.1, 1.5, 0.3] for choices A, B, C, which represents the "2nd" response and why?

- Concept: **Flip rate as behavioral metric**
  - Why needed here: The paper quantifies herd behavior through flip rate - the fraction of agents changing answers after peer exposure.
  - Quick check question: If 5 agents see peer responses and 3 change their answers, what is the flip rate?

- Concept: **Presentation format taxonomy**
  - Why needed here: Five formats (Count, Ratio, List, Disc, Reason) have distinct conformity profiles that require understanding to design interventions.
  - Quick check question: Which format would you choose to maximize consensus on a factual task - Count or Reason?

## Architecture Onboarding

- Component map: Agent initialization -> Context builder -> Confidence extractor -> Peer persona module -> Response aggregator
- Critical path: 1. Generate initial responses r' = argmax P(r|q) at τ=0 for all agents 2. Select peer condition (1st/2nd/rnd/last) and persona 3. Format peer information per presentation method 4. Present in specified order (agree-first or disagree-first) 5. Collect revised responses r^h = argmax P(r|q, R_{-i}) 6. Compute Iflip and aggregate flip rate
- Design tradeoffs:
  - High flip rate ↔ Low diversity: Strong herding improves consensus but may suppress exploration
  - Reason format ↔ Accuracy risk: Highest flip rate but 2nd peer condition reduces accuracy on factual tasks
  - Prompt control ↔ Structural control: Prompt-based interventions ("be agreeable") show minimal effect vs. format/persona manipulation
- Failure signatures:
  - Accuracy collapse on factual tasks when 2nd peer condition dominates (MMLU-Pro drops from 0.45 to 0.41)
  - Opinionated benchmarks show weak format effects - reason-based amplification fails
  - High-temperature initialization (τ=1) required for diversity; low-temperature (τ=0) for reproducibility
- First 3 experiments:
  1. Replicate confidence gap: Run 2-agent setup on 100 MMLU-Pro questions with 2nd peer condition; verify flip rate ≈0.51 and accuracy drop
  2. Format sweep: Compare all 5 presentation formats on 200-question subset; expect Reason ≈0.30 flip rate on factual benchmarks
  3. Order effect validation: Test agree-first vs. disagree-first with 3 agree/2 disagree configuration; confirm disagreement-first yields higher flip rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does herd behavior manifest in systems composed of heterogeneous agents with different model sizes or training paradigms?
- Basis in paper: [explicit] The authors identify the use of agents from the "same underlying language model architecture" as a limitation and explicitly suggest "exploring heterogeneous agent configurations" as future work.
- Why unresolved: Homogeneous agents limit behavioral diversity, potentially obscuring asymmetric influence dynamics where stronger models might disproportionately sway weaker ones.
- What evidence would resolve it: Experiments measuring flip rates and accuracy in mixed-model groups (e.g., GPT-4 paired with smaller open-source models).

### Open Question 2
- Question: Do the observed confidence-driven herding mechanisms transfer to open-ended generation tasks and multi-turn reasoning?
- Basis in paper: [explicit] The authors note the multiple-choice setup "may not fully capture the complexity... of real-world collaborative tasks, such as open-ended discussions."
- Why unresolved: Discrete response spaces differ structurally from nuanced text generation, potentially altering how peer reasoning is perceived and adopted.
- What evidence would resolve it: Extending the methodology to generative benchmarks and analyzing semantic convergence rather than discrete answer flips.

### Open Question 3
- Question: How do long-term memory and adaptive learning mechanisms modulate herd behavior over repeated interactions?
- Basis in paper: [explicit] The paper lists the "absence of memory or learning mechanisms" as a limitation, noting it prevents agents from adapting behavior over time.
- Why unresolved: The current static snapshot approach cannot determine if agents learn to resist social pressure or if cascades stabilize/destabilize over longitudinal interactions.
- What evidence would resolve it: Multi-round simulations where agents update policies based on historical consensus outcomes.

## Limitations
- Findings constrained to GPT-4o-mini with fixed temperature settings, leaving open whether other LLM architectures exhibit similar conformity patterns
- Blind conformity can degrade accuracy on factual tasks (accuracy drops from 0.45 to 0.41 under the most persuasive peer condition)
- The study doesn't address potential adversarial exploitation of herding mechanisms or long-term convergence dynamics in iterative peer interactions

## Confidence

- **High Confidence**: The core finding that herd behavior intensity correlates with the gap between self-confidence and perceived peer confidence is strongly supported by controlled experiments across multiple benchmarks
- **Medium Confidence**: The differential effects of presentation formats (reason-based vs. numerical) and order effects are well-documented but may be benchmark-dependent
- **Low Confidence**: The scalability of these findings to real-world multi-agent deployments remains uncertain

## Next Checks

1. **Cross-architecture validation**: Replicate the 2nd peer condition experiments with Claude 3.5 Sonnet and Llama 3.1 to verify whether herd behavior patterns generalize beyond GPT-4o-mini's confidence calibration
2. **Dynamic accuracy preservation**: Design a multi-round peer interaction protocol where agents alternate between conformity and dissent modes, measuring whether accuracy on factual tasks can be maintained above 0.40 while achieving consensus rates >0.70
3. **Adversarial robustness test**: Introduce systematically deceptive peer responses (high-confidence incorrect answers) and measure agent susceptibility across formats, identifying which presentation methods best preserve epistemic vigilance