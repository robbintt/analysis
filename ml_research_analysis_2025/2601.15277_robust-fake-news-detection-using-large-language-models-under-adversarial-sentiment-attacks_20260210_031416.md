---
ver: rpa2
title: Robust Fake News Detection using Large Language Models under Adversarial Sentiment
  Attacks
arxiv_id: '2601.15277'
source_url: https://arxiv.org/abs/2601.15277
tags:
- news
- fake
- sentiment
- detection
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the vulnerability of fake news detectors
  to sentiment manipulation using large language models (LLMs). The authors propose
  AdSent, a framework that detects fake news in a sentiment-agnostic manner by training
  models on sentiment-neutralized variants of news articles.
---

# Robust Fake News Detection using Large Language Models under Adversarial Sentiment Attacks

## Quick Facts
- **arXiv ID:** 2601.15277
- **Source URL:** https://arxiv.org/abs/2601.15277
- **Reference count:** 40
- **Primary result:** AdSent achieves 87.76% and 78.56% macro-F1 on two datasets, robust to sentiment manipulation

## Executive Summary
This paper addresses the vulnerability of fake news detectors to adversarial sentiment manipulation. The authors propose AdSent, a framework that trains detectors on sentiment-neutralized articles to achieve robust classification. By using an LLM to rewrite news articles with neutral sentiment, AdSent reduces the spurious correlation between sentiment and veracity that attackers exploit. Experiments show state-of-the-art models drop by up to 21.51% F1 under sentiment attacks, while AdSent maintains strong performance across datasets and generalizes to style-based attacks.

## Method Summary
The proposed AdSent framework uses a two-stage pipeline: first, an LLM "counterfeiter" rewrites news articles to neutral sentiment while preserving factual content; second, a detector LLM is fine-tuned on this neutralized training set. During inference, incoming articles are also neutralized before classification. The approach targets the spurious correlation between sentiment and veracity that makes detectors vulnerable to adversarial attacks. The framework is evaluated on PolitiFact and GossipCop datasets, with comparisons against competitive baselines including RoBERTa and other LLM-based detectors.

## Key Results
- State-of-the-art fake news detectors experience F1-score drops of up to 21.51% under sentiment manipulation attacks
- AdSent achieves macro-F1 scores of 87.76% and 78.56% on PolitiFact and GossipCop datasets respectively
- The framework demonstrates generalization to unseen datasets and adversarial scenarios beyond sentiment manipulation

## Why This Works (Mechanism)
AdSent works by breaking the spurious correlation between sentiment and news veracity. Traditional detectors learn that fake news often has negative sentiment and real news is more neutral or positive, creating a vulnerability that attackers can exploit through sentiment manipulation. By training on sentiment-neutralized data, AdSent learns to classify based on content and context rather than sentiment patterns. The counterfeiter LLM rewrites articles to neutral tone while preserving facts, creating a training distribution that focuses on the actual indicators of fake news rather than sentiment-based shortcuts.

## Foundational Learning
- **Concept: Adversarial Attacks and Robustness in NLP**
  - **Why needed here:** The entire paper is framed around identifying and defending against a specific type of adversarial attackâ€”sentiment manipulation. Understanding that models learn spurious correlations that can be exploited is fundamental.
  - **Quick check question:** Can you explain why adding a single pixel to an image or changing the sentiment of a sentence without altering its core meaning can cause a deep learning model to misclassify it?

- **Concept: Large Language Models (LLMs) for Text Rewriting**
  - **Why needed here:** The proposed framework, AdSent, uses an LLM ("counterfeiter") to rewrite articles. Understanding that LLMs can follow instructions like "rewrite this with a neutral tone while keeping all facts" is key to grasping the proposed defense mechanism.
  - **Quick check question:** How would you prompt a model to rephrase "The movie was a total disaster" with a neutral sentiment, without changing the underlying event?

- **Concept: Spurious Correlations in Machine Learning**
  - **Why needed here:** The vulnerability of the detectors is caused by a spurious correlation between sentiment and veracity (e.g., fake news is often negative). The solution is to train a model that does not rely on this correlation.
  - **Quick check question:** If a model trained to distinguish wolves from huskies learns to detect snow in the background, what problem will it have when tested on images of wolves in a zoo?

## Architecture Onboarding
- **Component map:**
  1. Counterfeiter (LLM): Rewrites articles to target sentiment (positive, negative, neutral)
  2. Detector (Fine-tuned LLM): Classifies articles as "fake" or "real"
  3. Adversarial Attack Pipeline: Tests detector robustness using sentiment manipulation
  4. Robust Training Pipeline (AdSent): Creates sentiment-neutralized training corpus
  5. Inference Pipeline: Neutralizes incoming articles before classification

- **Critical path:**
  1. Vulnerability Identification: Discover detectors are biased by sentiment through adversarial testing
  2. Robust Data Creation: Generate neutralized training corpus using counterfeiter
  3. Model Fine-tuning: Train detector on neutralized data and validate against original and adversarial test sets

- **Design tradeoffs:**
  - Compute & Latency: Two-stage inference pipeline is slower due to LLM rewriting
  - Fact Preservation: Defense quality depends on counterfeiter's ability to preserve facts
  - Generalization: Shows promise for style attacks but unknown for other manipulation types

- **Failure signatures:**
  - Biased Training Data: If sentiment-veracity correlation is weak, defense shows marginal gains
  - Poor Counterfeiter: Fact alteration during rewriting degrades model performance
  - Unseen Attack Types: Other semantic manipulations not tested could remain vulnerabilities

- **First 3 experiments:**
  1. Reproduce the Vulnerability: Apply sentiment manipulation to a pre-trained detector and measure F1 drop
  2. Implement the Counterfeiter: Test LLM's ability to rewrite articles to neutral tone while preserving facts
  3. Validate AdSent: Fine-tune detector on neutralized data and compare performance against baseline on both normal and adversarial test sets

## Open Questions the Paper Calls Out
- **Question:** How do visual sentiments interact with textual sentiments to influence multimodal fake news detection predictions?
  - **Basis in paper:** [explicit] The conclusion states the authors will "examine how sentiment is conveyed in the visual modality... and analyze its interaction with textual sentiment."
  - **Why unresolved:** The current AdSent framework and experiments are restricted to textual analysis; visual features are outside the current scope.
  - **What evidence would resolve it:** An extension of the AdSent framework that processes image inputs, evaluated on a multimodal fake news dataset.

- **Question:** To what extent does adversarial manipulation of news values other than sentiment, such as proximity or novelty, degrade detector performance?
  - **Basis in paper:** [explicit] The conclusion proposes exploring "adversarial perturbations targeting other news values, such as proximity, novelty, or prominence."
  - **Why unresolved:** The study focused specifically on sentiment-based attacks; the impact of manipulating other journalistic dimensions remains untested.
  - **What evidence would resolve it:** Experiments applying the AdSent framework to datasets where articles are adversarially altered for non-sentiment news values.

- **Question:** Can fine-tuning the LLM counterfeiter improve factual consistency preservation beyond the observed 0.66 Cohen's Kappa agreement with human judges?
  - **Basis in paper:** [inferred] Section 4.2.3 notes that while the LLM-as-a-judge shows potential, "they still need further improvement (e.g., via fine-tuning)" to replace human annotation reliably.
  - **Why unresolved:** The current approach relies on zero-shot instruction tuning for the counterfeiter, which occasionally alters facts, particularly in negative sentiment variants.
  - **What evidence would resolve it:** A study comparing the factual preservation rates of zero-shot versus fine-tuned counterfeiter models on the manipulated test sets.

## Limitations
- The two-stage inference pipeline creates computational overhead that may be impractical for high-throughput applications
- Defense effectiveness depends entirely on the counterfeiter's ability to preserve factual content during sentiment neutralization
- While robust to sentiment and style attacks, generalization to other forms of semantic manipulation remains untested

## Confidence
- **High Confidence:** Experimental demonstration of detector vulnerability and AdSent's superior performance on benchmark datasets
- **Medium Confidence:** Claims of generalization to unseen datasets and style attacks are supported but limited to tested scenarios
- **Low Confidence:** Practical feasibility for production deployment is questionable due to computational requirements

## Next Checks
1. **Fact Preservation Validation:** Implement automated fact-checking pipeline using ROUGE scores, NLI models, or entity matching to quantify information loss during sentiment neutralization

2. **Latency and Resource Benchmarking:** Measure end-to-end inference time and memory usage, comparing AdSent against baselines to evaluate computational overhead in real-world scenarios

3. **Generalization to Unseen Attacks:** Test AdSent against entity substitution, context truncation, and semantic paraphrasing to identify potential blind spots in the defense mechanism