---
ver: rpa2
title: Effects of Introducing Synaptic Scaling on Spiking Neural Network Learning
arxiv_id: '2601.11261'
source_url: https://arxiv.org/abs/2601.11261
tags:
- neurons
- synaptic
- learning
- neuron
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the impact of synaptic scaling on the learning
  performance of a spiking neural network (SNN) based on the winner-take-all (WTA)
  principle. The network was trained using spike-timing-dependent plasticity (STDP)
  combined with homeostatic plasticity (HP), which includes synaptic scaling and excitability
  regulation.
---

# Effects of Introducing Synaptic Scaling on Spiking Neural Network Learning

## Quick Facts
- **arXiv ID:** 2601.11261
- **Source URL:** https://arxiv.org/abs/2601.11261
- **Reference count:** 21
- **One-line primary result:** L2-norm-based synaptic scaling achieves 88.84% accuracy on MNIST, outperforming L1-norm methods.

## Executive Summary
This study investigates how different synaptic scaling methods impact learning in a spiking neural network (SNN) using the winner-take-all (WTA) principle. The network combines spike-timing-dependent plasticity (STDP) with homeostatic plasticity, including both synaptic scaling and excitability regulation. Through experiments on MNIST and Fashion-MNIST datasets, the research demonstrates that L2-norm-based synaptic scaling consistently outperforms L1-norm methods, achieving higher classification accuracy while producing more selective weight patterns.

## Method Summary
The method employs a three-layer network architecture: 784 input neurons (Poisson spike generators), 400 excitatory LIF neurons, and 400 inhibitory LIF neurons. The network uses STDP for unsupervised learning combined with homeostatic plasticity mechanisms including L2-norm synaptic scaling and dynamic threshold adaptation. Input images are converted to Poisson spike trains and presented for 150ms during training. Classification is performed by assigning each neuron to the class it responds to most strongly, then using the highest-firing neuron's label as the prediction. The network is trained on 40,000 images and tested on 10,000 images.

## Key Results
- L2-norm synaptic scaling achieves 88.84% accuracy on MNIST versus 87% for L1-norm
- L2-norm scaling shows consistent superiority across different target norm values
- L2-norm method produces lower average weight values and higher selectivity
- Fashion-MNIST results show similar pattern: L2 (68.01%) outperforms L1 (66%)

## Why This Works (Mechanism)

### Mechanism 1
L2-norm-based synaptic scaling provides superior classification accuracy compared to L1-norm methods in WTA networks. The L2 norm constrains the overall energy of weights differently than the L1 norm, resulting in lower average weight values and higher selectivity. The superiority of L2 is empirically observed but the exact underlying cause regarding weight statistics requires further investigation.

### Mechanism 2
Dynamic thresholding prevents single neurons from monopolizing firing activity, enabling distributed feature learning. The firing threshold increases by 0.11mV every time a neuron fires and decays slowly, creating negative feedback that reduces excitability of highly active neurons. Without this homeostatic counter-force, the network would destabilize or converge to a trivial solution where one neuron fires for everything.

### Mechanism 3
WTA architecture combined with STDP self-organizes synaptic weights to resemble input patterns (prototypes). Lateral inhibition ensures only the most strongly activated neurons fire, while STDP potentiates synapses that contributed to the spike. Over time, the weight vector of a "winner" neuron converges to the average of the input patterns it won for.

## Foundational Learning
- **Spike-Timing-Dependent Plasticity (STDP):** Core unsupervised learning rule updating weights based on temporal correlation. Why needed: Updates weights based on temporal correlation. Quick check: Can you explain why a presynaptic spike immediately preceding a postsynaptic spike strengthens the synapse (LTP), while the reverse weakens it (LTD)?
- **Leaky Integrate-and-Fire (LIF) Neurons:** Model used for excitatory and inhibitory layers to simulate membrane potential dynamics and generate spikes. Why needed: Simulates membrane potential dynamics and generates spikes. Quick check: What happens to the membrane potential after a spike is fired (Reset/Refractory period), and how does the "leak" term affect temporal integration?
- **Homeostatic Plasticity (HP):** Regulatory mechanism keeping the network stable and efficient. Why needed: HP is the regulatory mechanism that keeps the network stable and efficient. Quick check: If STDP is a positive feedback loop, why would a network eventually fail without a mechanism to regulate total synaptic strength or firing rates?

## Architecture Onboarding
- **Component map:** 784 input → 400 excitatory LIF neurons → 400 inhibitory LIF neurons
- **Critical path:** Image converted to Poisson spikes → Currents charge Excitatory LIF neurons → First neuron to threshold fires → Triggers corresponding Inhibitory neuron → Inhibition suppresses other Excitatory neurons (WTA) → STDP updates weights for firing neuron → Synaptic Scaling normalizes winner's weight vector → Winner's threshold increases slightly
- **Design tradeoffs:** L1 vs L2 Norm (L2 is proven better for accuracy but L1 might promote sparser weights); Neuron Count (400 improves accuracy but increases cost); Target Norm (too low results in no learning, too high risks instability)
- **Failure signatures:** Silent Network (no spikes occur - threshold too high or Wt too low); Seizure/Monopoly (one neuron fires for every input - inhibition broken or Excitability regulation too slow); Random Weights (weights remain noise after training - learning rate too low or STDP time constant mismatched)
- **First 3 experiments:** 1) Baseline Reproduction: Implement LIF + STDP on MNIST with 100 neurons, verify weights form digit-like patterns; 2) Scaling Ablation: Run with No Scaling vs. L1 Scaling vs. L2 Scaling to reproduce accuracy delta; 3) Parameter Sweep: Vary Wt on Fashion-MNIST to find "cliff" where accuracy drops

## Open Questions the Paper Calls Out
- What are the underlying mechanistic reasons that L2-norm-based synaptic scaling yields higher classification accuracy than L1-norm-based methods? The study observed the performance gap empirically but did not provide a theoretical or mathematical proof explaining why the L2 norm is more effective in this specific WTA context.
- Can implementing a learning rule that dynamically adjusts the target norm (Wt) improve classification robustness? The current study treated Wt as a static hyperparameter that required manual tuning for different datasets and configurations.
- How do the statistical properties of input spike trains determine the optimal time constant (τp) for STDP? The authors identified the correlation between τp and datasets but did not isolate which specific statistical features of input data drove the need for different time constants.

## Limitations
- The underlying reasons for L2-norm superiority over L1-norm are not mechanistically explained
- Optimal target norm (Wt) appears dataset-dependent and requires manual tuning
- Study does not explore impact of different neuron counts beyond 400 neurons

## Confidence
- **High Confidence:** Empirical finding that L2-norm scaling achieves higher accuracy than L1-norm (88.84% vs 87% on MNIST) is well-supported by direct experimental comparisons
- **Medium Confidence:** Mechanism that L2 scaling produces "lower average weight values and higher selectivity" is observed but not rigorously proven to be the causal factor for improved accuracy
- **Medium Confidence:** Claim that homeostatic plasticity is essential for stability is supported by literature but not directly tested by ablation in this specific network configuration

## Next Checks
1. **Parameter Sensitivity Analysis:** Systematically vary Wt across a wider range (e.g., 1 to 20) for both MNIST and Fashion-MNIST to identify precise thresholds where accuracy collapses and validate the "too low" and "too high" failure modes described.
2. **Architecture Scaling Study:** Reproduce the experiment with different neuron counts (e.g., 100, 200, 400, 800) to quantify accuracy vs. computational cost tradeoff and determine if the L2 advantage persists at different scales.
3. **Ablation of Homeostatic Components:** Run controlled experiments disabling either synaptic scaling or excitability regulation to directly measure their individual contributions to accuracy and network stability, testing the claim that HP is "essential."