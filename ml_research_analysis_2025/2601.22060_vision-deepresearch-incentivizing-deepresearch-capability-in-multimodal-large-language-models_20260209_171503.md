---
ver: rpa2
title: 'Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large
  Language Models'
arxiv_id: '2601.22060'
source_url: https://arxiv.org/abs/2601.22060
tags:
- search
- visual
- multimodal
- reasoning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes Vision-DeepResearch, a new multimodal deep-research
  paradigm that performs multi-turn, multi-entity, and multi-scale visual and textual
  search to robustly hit real-world search engines under heavy noise. It addresses
  the limitations of existing multimodal deep-research MLLMs by overcoming the hit-rate
  problem in image retrieval and increasing reasoning depth and search breadth through
  long-horizon trajectories.
---

# Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2601.22060
- Source URL: https://arxiv.org/abs/2601.22060
- Reference count: 12
- Multimodal deep-research model achieves state-of-the-art performance across 6 factual VQA benchmarks

## Executive Summary
Vision-DeepResearch addresses the limitations of existing multimodal deep-research models by introducing a new paradigm that performs multi-turn, multi-entity, and multi-scale visual and textual search. The approach overcomes the hit-rate problem in image retrieval by generating multiple bounding boxes at varying scales and positions, and increases reasoning depth through long-horizon trajectories. By synthesizing high-quality VQA instances and integrating deep-research capabilities via cold-start supervision and reinforcement learning, the model significantly outperforms existing multimodal deep-research MLLMs and workflows built on strong closed-source foundation models like GPT-5, Gemini-2.5-pro, and Claude-4-Sonnet.

## Method Summary
The method involves synthesizing high-quality VQA instances through an interleaved answer-entity obfuscation strategy that forces genuine search behavior. Multi-scale visual cropping generates multiple bounding boxes per image, which are submitted to visual search tools in parallel to improve hit-rate under noisy real-world conditions. Visual trajectories are converted to text-only format through detailed image descriptions, allowing text-based deep research LLMs to continue the reasoning process. The model is trained using supervised fine-tuning on 30K trajectories followed by reinforcement learning with GRPO optimization, achieving state-of-the-art performance across multiple multimodal factual benchmarks.

## Key Results
- Achieves state-of-the-art performance across 6 multimodal factual benchmarks
- Improves visual search hit-rate from 4.8% to 37.8% using multi-scale multi-entity cropping
- Outperforms workflows built on closed-source models like GPT-5 and Gemini-2.5-pro
- RL training provides 3-5% improvement over SFT-only approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-entity, multi-scale visual cropping improves retrieval hit-rate under noisy real-world search conditions.
- Mechanism: Instead of single full-image queries, the model generates multiple bounding boxes at varying scales and positions. Each crop is submitted to visual search tools in parallel. Evidence accumulates across successful hits, with an external judge model evaluating whether sufficient information has been gathered before terminating visual search.
- Core assumption: Target entities exist at multiple scales and positions within the image; search engines return inconsistent results for semantically similar but visually different queries.
- Evidence anchors: [abstract]: "performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise"; [Section 2.1]: "Even when querying the same visual or textual entity, retrieval results can differ markedly across query scales"; [Table 2]: CIS (cropped-image search) improves VDR from 4.8% to 15.4% vs. Direct Answer; full pipeline (CIS+TS) achieves 37.8%; [corpus]: Related work on visual representation in MLLMs confirms visual modality is often underutilized (FMR: 0.64), supporting the need for explicit multi-scale probing.

### Mechanism 2
- Claim: Text bridging transfers long-horizon reasoning capabilities from text-only deep research LLMs to multimodal settings.
- Mechanism: Visual trajectories (image + actions + observations) are converted to text-only format by replacing images with detailed descriptions. A text-based deep research LLM continues the trajectory using its pre-trained long-horizon planning capabilities. The resulting text trajectory is merged back with the original visual trajectory, yielding complete multimodal deep-research sequences with 50+ turns and 64K context.
- Core assumption: Text-based deep research LLMs have transferable planning behaviors; image descriptions sufficiently preserve visual reasoning context.
- Evidence anchors: [abstract]: "internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training"; [Section 2.2.1]: "We first generate a detailed textual description D for the input image I, while replacing I with D... yields the subsequent text-based trajectory"; [Section 2.2.1]: "The textual tool trajectory of text-based deep-research foundation LLM can denote as C_text"; [corpus]: Yunque DeepResearch (FMR: 0.51) and Dingtalk DeepResearch (FMR: 0.56) demonstrate strong text-based long-horizon reasoning, supporting transfer hypothesis.

### Mechanism 3
- Claim: Interleaved answer-entity obfuscation creates challenging multi-hop VQA that forces genuine search behavior rather than shortcut exploitation.
- Mechanism: Starting from entity-level questions, complexity increases through two strategies applied alternately: (1) answer obfuscation chains relations around the answer (e.g., "teacher of the cat owner's daughter"); (2) entity obfuscation replaces original entities via random walks over webpages. An automated pipeline validates solvability, removes shortcuts, and filters questions answerable without external evidence.
- Core assumption: Interleaved obfuscation prevents both rigid templated reasoning and cross-source consistency shortcuts.
- Evidence anchors: [Section 2.2.2]: "we therefore adopt an interleaved obfuscation strategy that alternates between answer and entity obfuscation"; [Section 2.2.2]: "We directly feed the VQA instance to an MLLM. If it can answer correctly without external evidence, we discard the sample"; [Table 3]: Adding 6K fuzzy VQA trajectories improves Avg from 33.5% to 36.9% (+3.4%); [corpus]: WebSailor and related works (not directly in corpus, but referenced in paper) use entity obfuscation; corpus lacks direct validation of interleaved strategy.

## Foundational Learning

- Concept: ReAct (Reasoning + Acting) paradigm
  - Why needed here: The entire framework builds on interleaved reasoning steps and tool calls. Understanding how thoughts and actions alternate is prerequisite to grasping trajectory construction.
  - Quick check question: Given a question "What is the date of this lecture?", can you sketch a 3-step ReAct trajectory with reasoning, action, and observation?

- Concept: Visual grounding and region proposals
  - Why needed here: Multi-scale cropping requires understanding how MLLMs localize entities and generate bounding boxes. Without this, the visual search strategy appears arbitrary.
  - Quick check question: How would an MLLM generate multiple bounding boxes for an image containing a person holding a sign? What scales would you expect?

- Concept: Group Relative Policy Optimization (GRPO) for tool-calling agents
  - Why needed here: RL training refines SFT behavior. Understanding reward signals, advantage computation, and policy updates explains why trajectories improve beyond cold-start supervision.
  - Quick check question: In a multi-turn search task, how would you compute advantage for a trajectory that took 25 turns to find the correct answer versus one that took 10 turns?

## Architecture Onboarding

- Component map: Image filtering (size >224×224, multi-entity preference) → Entity extraction + multi-scale cropping → Visual search + judge verification → Text bridging → Text-based trajectory continuation → Rejection sampling
- Critical path: Image filtering quality → visual search hit-rate → trajectory completeness → SFT coverage → RL exploration efficiency. Weak filtering (trivial images) propagates to low-quality trajectories and unstable RL.
- Design tradeoffs:
  - BF16 vs. FP16: Paper chose BF16 after FP16 caused numerical overflow with 64K context—trade stability for precision
  - Trajectory masking vs. zero reward: Masking anomalous trajectories prevents negative gradient injection but reduces effective batch size
  - Judge leniency: Lenient visual evidence assessment allows deeper exploration but may include weak evidence in training
- Failure signatures:
  - Repetitive loops: Model generates near-duplicate responses until context limit (detect via n-gram repetition detector)
  - Cascading tool failures: Consecutive format/tool-call errors persist until turn budget (terminate after 3 consecutive errors)
  - Premature termination: Model settles for partial evidence—indicates insufficient long-horizon training
  - Low hit-rate: Visual search returns no matches—suggests cropping strategy mismatched to entity scales
- First 3 experiments:
  1. Pipeline ablation (Direct Answer → WIS → WIS+TS → CIS → CIS+TS) on held-out benchmark to isolate contribution of each search component. Expected: CIS+TS > CIS > WIS+TS > WIS > Direct Answer
  2. Data ablation (base → +VQA traj → +QA traj → +fuzzy VQA → +RL) to measure incremental gains. Expected: RL provides 3-5% improvement over SFT-only
  3. Trajectory length vs. reward curve during RL training to verify model learns efficiency (shorter trajectories, higher rewards over time)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What performance gains would larger-scale RL optimization yield, and at what compute cost?
- Basis in paper: [explicit] "Moreover, due to API cost and wall-clock constraints, we do not exhaustively scale RL training. We expect the model to further benefit from larger-scale RL optimization."
- Why unresolved: The authors explicitly state they did not exhaustively scale RL training due to practical constraints, leaving the scaling properties of their approach unexplored.
- What evidence would resolve it: Training runs with varying RL budgets and trajectory counts, reporting performance vs. compute tradeoffs.

### Open Question 2
- Question: How robust is the system to errors or biases in the judge model used for trajectory termination and reward signals?
- Basis in paper: [inferred] The pipeline relies heavily on judge models for two critical decisions: (1) determining when visual evidence is sufficient to terminate search (Eq. 4), and (2) providing reward signals during RL training. The accuracy and potential biases of these judge models are not analyzed.
- Why unresolved: If the judge model makes systematic errors (e.g., prematurely terminating promising trajectories or rewarding incorrect answers), this could propagate failures without detection.
- What evidence would resolve it: Ablation studies varying judge model quality/size, and analysis of failure cases attributable to judge errors.

### Open Question 3
- Question: Does the text-bridging approach lose fine-grained visual information that could improve retrieval or reasoning?
- Basis in paper: [inferred] The method bridges visual trajectories to text-only deep-research LLMs by replacing images with textual descriptions (Sec. 2.2.1). This assumes descriptions preserve all relevant visual information, which may not hold for subtle visual details.
- Why unresolved: Textual descriptions are lossy compressions; fine-grained visual attributes (texture, spatial relations, partial occlusions) may be inadequately captured.
- What evidence would resolve it: Compare performance when using ground-truth descriptions vs. MLLM-generated descriptions, or when retaining image tokens during text-based reasoning.

### Open Question 4
- Question: How does multi-scale multi-entity search scale computationally with image complexity and the number of entities?
- Basis in paper: [inferred] The approach generates multiple bounding boxes per image and searches at multiple scales (Sec. 2.2.1). While effective for hit-rate, computational cost and latency grow with entity count, potentially limiting real-world deployment.
- Why unresolved: The paper reports performance gains but does not analyze the computational overhead or propose mechanisms for adaptive search budget allocation.
- What evidence would resolve it: Latency and API call statistics across images with varying entity counts, plus comparison to adaptive early-termination strategies.

## Limitations

- Computational overhead of multi-scale visual search requiring multiple concurrent API calls per image, with unknown real-world deployment costs
- Synthetic nature of obfuscated training data raises questions about generalization to truly open-ended real-world questions
- Heavy reliance on judge models for trajectory termination and reward signals without analysis of their accuracy or potential biases

## Confidence

- **High confidence**: Multi-scale cropping improves visual search hit-rates (supported by 15.4%→37.8% accuracy gains in Table 2)
- **Medium confidence**: Text bridging transfer claim (lacks ablation showing what text-based LLM is used or direct comparison to visual-only reasoning)
- **Medium confidence**: Interleaved obfuscation effectiveness (Table 3 shows gains but lacks comparison to simpler obfuscation strategies)
- **Low confidence**: Claimed state-of-the-art performance against closed-source models (benchmarks may favor synthetic data strengths)

## Next Checks

1. **Real-world deployment test**: Measure actual API costs and latency for multi-scale visual search across 100 diverse images from different domains, comparing against single-query approaches
2. **Generalization probe**: Evaluate model on truly open-ended questions without synthetic obfuscation (e.g., user-generated queries) to test if training patterns transfer
3. **Ablation of text bridging**: Replace the text-based deep-research continuation with direct visual reasoning continuation to quantify the actual contribution of the transfer mechanism