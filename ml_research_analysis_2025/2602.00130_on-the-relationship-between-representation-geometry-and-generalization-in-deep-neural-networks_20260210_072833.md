---
ver: rpa2
title: On the Relationship Between Representation Geometry and Generalization in Deep
  Neural Networks
arxiv_id: '2602.00130'
source_url: https://arxiv.org/abs/2602.00130
tags:
- accuracy
- dimension
- effective
- compression
- geometric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the relationship between representation\
  \ geometry and neural network performance through systematic empirical studies across\
  \ vision and language domains. The authors analyze 52 pretrained ImageNet models\
  \ across 13 architecture families and demonstrate that effective dimension\u2014\
  an unsupervised geometric metric requiring no labels\u2014strongly predicts classification\
  \ accuracy."
---

# On the Relationship Between Representation Geometry and Generalization in Deep Neural Networks

## Quick Facts
- arXiv ID: 2602.00130
- Source URL: https://arxiv.org/abs/2602.00130
- Reference count: 0
- Key outcome: Effective dimension predicts accuracy (r=0.75 partial correlation) and is causally linked to performance across vision and language domains

## Executive Summary
This paper establishes that representation geometry, specifically effective dimension computed from activation covariance eigenvalues, strongly predicts neural network accuracy across diverse architectures and domains. The authors demonstrate this through empirical studies on 52 ImageNet models showing output effective dimension achieves partial correlation r=0.75 with accuracy after controlling for model capacity, and causal interventions showing that degrading geometry causes accuracy loss (r=-0.94) while improving geometry maintains accuracy (-0.03pp at 95% variance). The relationship generalizes across vision, NLP encoders, and decoder-only LLMs, where model size shows no predictive power for LLMs (r=0.07).

## Method Summary
The study extracts representation geometry by sampling 2,000 inputs, computing penultimate-layer activations, and calculating effective dimension via the participation ratio formula (tr(Σ))²/tr(Σ²). Total compression measures log-ratio of effective dimensions between first and last layers. Causality is tested through controlled interventions: noise injection (Gaussian, Uniform, Dropout, Salt-and-pepper) degrades geometry and reduces accuracy, while PCA projection improves geometry and preserves accuracy. The framework is applied to 52 pretrained ImageNet models across 13 architecture families, 8 NLP encoders on SST-2/MNLI, and 15 decoder-only LLMs on AG News.

## Key Results
- Output effective dimension achieves partial correlation r=0.75 (p < 10^-10) with accuracy across 52 vision models, controlling for model capacity
- Bidirectional causality established: noise degrades geometry and causes accuracy loss (r=-0.94, p < 10^-9), while PCA improves geometry and maintains accuracy (-0.03pp at 95% variance)
- Geometric signatures generalize across domains, predicting NLP performance (r=0.69 on AG News) where model size does not (r=0.07)

## Why This Works (Mechanism)

### Mechanism 1
Effective dimension predicts accuracy because it captures how well a network's learned representation concentrates task-relevant information into a structured, low-dimensional subspace. The participation ratio formula quantifies meaningful variance dimensions, and well-structured representations concentrate signal into optimal effective dimensions. For discriminative models, higher accuracy correlates with richer but compressed final-layer representations, while generative decoders show opposite compression signs but maintain predictive magnitude.

### Mechanism 2
Causal intervention on representation geometry directly alters model accuracy, demonstrating mechanistic linkage rather than mere correlation. Perturbing penultimate-layer activations with various noise types increases effective dimension and proportionally reduces accuracy, while PCA projection preserves accuracy. This bidirectional relationship (r=-0.94) shows geometry is not just correlated but causally determines performance.

### Mechanism 3
The relationship between geometric metrics and performance is domain-agnostic, generalizing across vision, NLP encoders, and decoder-only LLMs. Architectural differences lead to opposite compression signs, but the magnitude of geometric transformation remains predictive. Effective dimension consistently predicts performance across domains where model size fails to capture accuracy patterns.

## Foundational Learning

- **Concept: Covariance Matrix Eigenvalues**
  - Why needed here: Essential for understanding how effective dimension metric (EffDim = (tr(Σ))²/tr(Σ²)) quantifies variance distribution in representations
  - Quick check question: Can you explain how eigenvalues of a covariance matrix represent variance along principal components and how EffDim summarizes this distribution?

- **Concept: Partial Correlation**
  - Why needed here: Core to establishing that geometric metrics predict accuracy beyond model capacity by measuring relationship while controlling for model size
  - Quick check question: What does a partial correlation of r=0.75 between output EffDim and accuracy, controlling for model size, indicate about the relationship?

- **Concept: Causal Intervention**
  - Why needed here: The paper moves beyond correlation by intervening on geometry and observing effects, providing stronger evidence for "bidirectional causality"
  - Quick check question: Why is intervening on an intermediate variable (representation geometry) and observing the outcome a stronger test of causal mechanism than observational correlation?

## Architecture Onboarding

- **Component map**: Model + Dataset -> Sample 2000 inputs -> Extract layer activations -> Center representations -> Compute SVD -> Calculate EffDim per layer -> Return summary statistics (C, d₁, d_L, d_min)
- **Critical path**: 1) Extract penultimate-layer activations for representative sample (m=2000) 2) Compute centered covariance and eigenvalues 3) Calculate EffDim via participation ratio 4) For causality tests, inject noise or apply PCA to activations and re-evaluate accuracy
- **Design tradeoffs**: Sample size (m=2000) balances stability vs compute; layer choice affects predictive power (output EffDim strongest); PCA threshold determines preservation vs degradation (90-95% safe range)
- **Failure signatures**: Erratic EffDim suggests insufficient sample size; PCA >1pp accuracy loss indicates distributed information reliance; inconsistent noise-type correlations challenge noise-agnostic claim
- **First 3 experiments**: 1) Replicate correlation: 5+ vision models, extract output EffDim, correlate with accuracy (expect r>0.5) 2) Causal degradation: Train ResNet18 on CIFAR-10, inject Gaussian noise at inference, plot ΔEffDim vs ΔAccuracy (expect r<-0.9) 3) Causal improvement: Apply PCA at 95% variance to same model, verify accuracy change <0.1pp

## Open Questions the Paper Calls Out

- **Question**: Does correlation between geometric signatures and accuracy stem from optimization dynamics, architectural inductive biases, or data structure?
  - Basis in paper: Section 6.4 states this remains unclear and future work should investigate
  - Why unresolved: Paper establishes correlations but doesn't isolate fundamental driver
  - What evidence would resolve it: Ablation studies controlling for data manifold geometry and optimization algorithms

- **Question**: Can relationship between effective dimension and generalization be formalized into rigorous theoretical bound?
  - Basis in paper: Section 3.5 notes precise relationship remains open theoretical question
  - Why unresolved: Authors provide informal connections to Rademacher complexity but no formal proofs
  - What evidence would resolve it: Formal derivation showing generalization error scales with effective dimension

- **Question**: Do geometric metrics retain predictive power in domains outside vision and NLP, specifically reinforcement learning or speech?
  - Basis in paper: Section 6.4 lists extension to RL, speech, and other domains as necessary step
  - Why unresolved: Study restricted to supervised vision and language modeling
  - What evidence would resolve it: Demonstrate effective dimension correlates with RL cumulative reward or speech WER

## Limitations
- Potential confounding from batch effects in pre-trained models where effective dimension may capture training artifacts
- Sensitivity of effective dimension to activation distribution shifts beyond noise injection
- Assumption that geometric signatures are task-relevant rather than architecture-specific artifacts
- Sample size of 2,000 per model may not fully capture layer-wise variance stability

## Confidence
- High confidence: Empirical correlation between effective dimension and accuracy (r=0.75 partial correlation)
- Medium confidence: Causal intervention results due to strong correlation (r=-0.94) but limited noise-type analysis
- Medium confidence: Cross-domain generalization given consistent pattern but limited decoder-only LLM experiments

## Next Checks
1. Verify correlation direction and magnitude for total compression vs accuracy using exact log-ratio formula across all 52 ImageNet models
2. Replicate causal intervention on ResNet18 across all four noise types, measuring ΔEffDim vs ΔAccuracy correlation to confirm noise-agnosticism
3. Test PCA preservation thresholds (80%, 90%, 95%) on DenseNet121 and ResNet18 trained on CIFAR-10 to confirm 90-95% range maintains accuracy