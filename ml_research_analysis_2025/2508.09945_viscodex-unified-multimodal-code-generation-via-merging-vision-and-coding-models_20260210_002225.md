---
ver: rpa2
title: 'VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding
  Models'
arxiv_id: '2508.09945'
source_url: https://arxiv.org/abs/2508.09945
tags:
- code
- multimodal
- arxiv
- language
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces VisCodex, a framework that unifies vision
  and coding models to enable multimodal code generation. It merges a vision-language
  model with a coding LLM using task vectors, preserving visual comprehension and
  coding skills.
---

# VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models

## Quick Facts
- arXiv ID: 2508.09945
- Source URL: https://arxiv.org/abs/2508.09945
- Reference count: 40
- Key outcome: VisCodex achieves state-of-the-art performance among open-source models on multimodal code generation benchmarks, with an average score of 72.3, approaching proprietary models like GPT-4o.

## Executive Summary
This work introduces VisCodex, a framework that unifies vision and coding models to enable multimodal code generation. It merges a vision-language model with a coding LLM using task vectors, preserving visual comprehension and coding skills. The authors also present MCD, a large-scale dataset with 598k multimodal coding samples, and InfiBench-V, a challenging benchmark for evaluating real-world multimodal coding tasks. Experiments show VisCodex achieves state-of-the-art performance among open-source models, with an average score of 72.3 on key benchmarks, approaching proprietary models like GPT-4o.

## Method Summary
VisCodex employs a task vector-based model merging technique to integrate a coding LLM into a vision-language backbone. Task vectors (τ = θ_ft − θ_base) encode parameter shifts from domain fine-tuning and are linearly combined to merge capabilities. Only the LLM backbone is modified; the vision encoder and cross-modal projection modules remain frozen to preserve visual grounding. The model is fine-tuned on MCD, a large-scale dataset of 598k multimodal coding samples spanning HTML, chart, QA, and algorithm domains. Training uses 2 epochs with AdamW, LR 1e-5, batch 128, and seq len 8K.

## Key Results
- VisCodex achieves state-of-the-art performance among open-source models on multimodal coding benchmarks.
- On MMCode, VisCodex reaches 65.7 pass@1 (8B) and 68.1 pass@1 (33B), approaching GPT-4o's 70.2 pass@1.
- Ablation studies confirm the effectiveness of task vector merging and the importance of MCD dataset diversity.

## Why This Works (Mechanism)

### Mechanism 1
Task vector arithmetic enables capability transfer from code-specialized models to vision-language models without full retraining. Task vectors (τ = θ_ft − θ_base) encode parameter shifts from domain fine-tuning. By computing separate vectors for vision-language (τ_vlm) and coding (τ_code) tasks, then linearly combining them (θ_final = θ_base + λτ_vlm + (1−λ)τ_code), the merged model inherits both visual grounding and code generation. The λ hyperparameter (set to 0.7 for 8B model) controls the trade-off. Core assumption: Code expertise resides primarily in the LLM backbone parameters, and task vectors compose additively without destructive interference in shared semantic subspaces.

### Mechanism 2
Selective component preservation maintains visual comprehension while enabling code capability injection. Only the LLM backbone undergoes merging; the vision encoder and cross-modal projection modules remain frozen. This preserves learned visual-semantic alignment (how visual tokens map to language embeddings) while allowing code knowledge to modify reasoning and generation in the language model. Core assumption: The projection module has already learned sufficient visual-to-language mapping during pretraining, and code capabilities do not require relearning visual representations.

### Mechanism 3
Multi-domain instruction tuning with quality-filtered data aligns merged model to multimodal coding tasks. The MCD dataset (598k samples) spans four domains—HTML (200k), Chart (210k), QA (59k), Algorithm (129k)—each with quality filtering pipelines. GPT-4o assists in rewriting, scoring aesthetics, and filtering non-executable code. This breadth prevents capability collapse while instruction-tuning aligns the merged model to task formats. Core assumption: Diverse task coverage maintains both visual grounding and code reasoning, and synthetic/augmented data from GPT-4o transfers effectively to smaller models.

## Foundational Learning

- Concept: **Task Vectors and Model Merging**
  - Why needed here: Understanding how parameter arithmetic enables knowledge transfer without retraining is essential for diagnosing merge failures and tuning λ.
  - Quick check question: Given θ_base, a fine-tuned code model θ_code, and a VLM's LLM backbone θ_vlm, write the merged parameter formula with λ=0.7.

- Concept: **MLLM Architecture (Encoder-Projector-LLM)**
  - Why needed here: Knowing which components are frozen vs. merged determines where to debug visual grounding loss vs. code generation failures.
  - Quick check question: In VisCodex's pipeline, which three components comprise the MLLM, and which one undergoes parameter modification?

- Concept: **Instruction Tuning for Multimodal Code**
  - Why needed here: Post-merge fine-tuning aligns the model to task formats; understanding data composition helps diagnose domain-specific failures.
  - Quick check question: Name the four domains in MCD and explain why algorithmic data is included despite the focus on visual tasks.

## Architecture Onboarding

- Component map:
  - Vision Encoder (Qwen2.5-VL ViT) -> Projection Module -> LLM Backbone (Merged)

- Critical path:
  1. Obtain base model (Qwen2.5-VL-7B), code model (Nemotron-1.1-7B), and shared pretrained base
  2. Compute τ_vlm = θ_vlm − θ_base and τ_code = θ_code − θ_base
  3. Merge: θ_merged = θ_base + λτ_vlm + (1−λ)τ_code (λ=0.7 for 7B)
  4. Replace LLM backbone with merged parameters, keep ViT/projector frozen
  5. Fine-tune on MCD for 2 epochs, lr=1e-5, batch_size=128

- Design tradeoffs:
  - Higher λ → stronger visual grounding but weaker code; lower λ → inverse
  - Freezing projector → preserves alignment but limits adaptation; full fine-tuning → risks catastrophic forgetting
  - Code model choice: Nemotron improves MMCode (+4.2 pass@1 over Qwen2.5-Coder), but Qwen2.5-Coder may excel in other domains

- Failure signatures:
  - Visual grounding loss: Model describes images incorrectly, generates HTML/layout that doesn't match visual input (suggests λ too low or projector issues)
  - Code syntax errors: Generates plausible-looking but non-executable code (suggests insufficient code task vector weight or data quality issues)
  - Catastrophic forgetting: Model loses conversation ability or general reasoning (suggests overfitting to MCD, need to check algorithm domain data)

- First 3 experiments:
  1. Lambda sweep on MMCode: Test λ ∈ {0.5, 0.6, 0.7, 0.8, 0.9} to validate optimal trade-off; expect ~0.7 to balance visual and code per paper.
  2. Ablation on code model source: Merge with Nemotron vs. Qwen2.5-Coder vs. OpenThinker2, compare on ChartMimic and MMCode to reproduce Table 3 patterns.
  3. Data domain ablation: Train with MCD minus one domain at a time to confirm Algorithm data prevents reasoning degradation (check MMCode pass@1).

## Open Questions the Paper Calls Out

### Open Question 1
How does the optimal merge coefficient (λ) scale with model size and domain-specific data ratios? The authors note that for the 33B variant, they "directly set λ = 0.85" without additional tuning due to resource constraints, leaving the relationship between model scale and the optimal trade-off between vision and code vectors unexplored.

### Open Question 2
Is the success of this merging strategy dependent on the underlying Vision Transformer's ability to handle dynamic resolutions? The authors explicitly adopt Qwen2.5-VL because it introduces "2D Rotary Position Embedding (RoPE)... [allowing] flexible processing of images with arbitrary resolutions," implying this feature may be critical for the merging success.

### Open Question 3
To what extent does enhancing code generation capabilities via merging degrade the model's general visual reasoning proficiency? While the paper claims the method "preserves visual comprehension," the experimental results focus exclusively on coding benchmarks and omit evaluation on standard general-purpose vision benchmarks.

## Limitations
- The choice of λ=0.7 (7B) and λ=0.85 (33B) was empirically determined on MMCode validation data, but sensitivity analysis across the full task vector space is not reported.
- MCD dataset curation relies on GPT-4o for quality filtering and rewriting, but exact prompt templates and filtering thresholds are unspecified, limiting reproducibility.
- While the model demonstrates strong average performance (72.3 across benchmarks), detailed ablation on individual domain contributions is limited.

## Confidence

- **High confidence**: VisCodex achieves state-of-the-art performance among open-source models on multimodal coding benchmarks (validated by direct comparisons to competing models on Design2Code, ChartMimic, and MMCode).
- **Medium confidence**: Task vector merging effectively preserves visual grounding while injecting code capabilities (supported by architectural ablation studies, but dependent on freezing assumptions that may not generalize).
- **Medium confidence**: MCD dataset quality and diversity drive performance gains (supported by ablation showing MCD outperforms other datasets, but dependent on GPT-4o curation without full methodological transparency).

## Next Checks

1. Lambda sensitivity sweep: Systematically test λ ∈ {0.5, 0.6, 0.7, 0.8, 0.9} on the 7B model across all three primary benchmarks (Design2Code, ChartMimic, MMCode) to confirm the claimed optimal λ=0.7 and identify domain-specific trade-offs.

2. Data domain ablation: Train VisCodex with MCD minus one domain at a time (e.g., remove Algorithm data) and evaluate on MMCode and Design2Code to quantify each domain's contribution and validate the claim that Algorithm data prevents reasoning degradation.

3. Architecture compatibility test: Verify that all three models (base, VLM, code) share identical layer naming conventions and hidden dimensions before computing task vectors. Attempt merging with mismatched architectures to confirm failure modes and validate the assumption that shared architecture is required.