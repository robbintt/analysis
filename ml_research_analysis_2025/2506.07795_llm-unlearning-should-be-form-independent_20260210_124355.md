---
ver: rpa2
title: LLM Unlearning Should Be Form-Independent
arxiv_id: '2506.07795'
source_url: https://arxiv.org/abs/2506.07795
tags:
- unlearning
- knowledge
- rocr
- tasks
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a critical vulnerability in LLM unlearning
  called Form-Dependent Bias, where unlearning effectiveness heavily depends on the
  training sample format and fails to generalize to alternate expressions of the same
  knowledge. The authors characterize two failure patterns: Cross-Task Transfer Failure
  (methods trained on QA pairs fail on fill-in-the-blank tasks) and Unseen Token Generalization
  Failure (methods fail when answers involve tokens unseen during unlearning).'
---

# LLM Unlearning Should Be Form-Independent

## Quick Facts
- arXiv ID: 2506.07795
- Source URL: https://arxiv.org/abs/2506.07795
- Authors: Xiaotian Ye; Mengqi Zhang; Shu Wu
- Reference count: 40
- Primary result: Form-dependent bias causes unlearning methods to fail on unseen formats and tokens; ROCR achieves 71.47% probability reduction on forget set vs 58.12% (RT) and 46.81% (GA).

## Executive Summary
This paper identifies a critical vulnerability in LLM unlearning called Form-Dependent Bias, where unlearning effectiveness heavily depends on the training sample format and fails to generalize to alternate expressions of the same knowledge. The authors characterize two failure patterns: Cross-Task Transfer Failure (methods trained on QA pairs fail on fill-in-the-blank tasks) and Unseen Token Generalization Failure (methods fail when answers involve tokens unseen during unlearning). To address this, they propose Rank-one Concept Redirection (ROCR), a training-free method that redirects the model's perception of a dangerous concept to a harmless one by modifying MLP parameters. Experiments on ORT benchmark show ROCR achieves 71.47% probability reduction on forgetting targets (vs 58.12% for RT and 46.81% for GA) while maintaining 56.82% on retain set (vs 30.54% for RT and 55.12% for GA), demonstrating superior generalization across task formats.

## Method Summary
ROCR addresses form-dependent bias in LLM unlearning through a training-free three-step approach: (1) collect characteristic activation vectors from N=5 chat-templated sentences containing the forget target, (2) compute a redirection vector using the target concept representation and a harmless anchor entity (default: "Donald Trump"), and (3) apply a rank-one update to MLP down-projection matrices using null-space constrained optimization. The method modifies MLPs at layers [4,5,6] by computing closed-form weight updates that preserve semantic coherence while reducing the model's association with dangerous concepts. Unlike fine-tuning-based approaches, ROCR maintains retention performance on unrelated concepts while achieving stronger cross-format generalization.

## Key Results
- ROCR achieves 71.47% probability reduction on forget set (vs 58.12% for RT and 46.81% for GA)
- ROCR maintains 56.82% retention on retain set (vs 30.54% for RT and 55.12% for GA)
- Cross-task transfer failure: Baseline methods degrade from 96.72% MCP performance to 33.60% on forget set
- Unseen token generalization: ROCR maintains 28.72% reduction on SQA tasks where answers contain unseen tokens

## Why This Works (Mechanism)
ROCR works by redirecting the model's internal representation of dangerous concepts through rank-one MLP updates that preserve semantic structure while breaking the association with harmful knowledge. The method exploits the observation that dangerous concepts occupy similar activation spaces in MLP layers, allowing targeted modification without affecting general capabilities. By using null-space projection, ROCR ensures that the redirection preserves the model's ability to process unrelated concepts while specifically attenuating the dangerous concept's influence. The training-free nature avoids catastrophic forgetting of benign knowledge that typically occurs with fine-tuning approaches.

## Foundational Learning
- **Null-space projection**: A mathematical technique for modifying matrix parameters while preserving constraints; needed to ensure MLP updates don't disrupt general model capabilities. Quick check: Verify SVD correctly computes null space of K0 matrix.
- **Characteristic activation vectors**: Low-rank representations of concept embeddings extracted from model activations; needed to capture the semantic footprint of dangerous concepts. Quick check: Confirm 5 sentences adequately represent concept diversity.
- **Rank-one updates**: Matrix modifications that preserve most singular values while changing specific directions; needed for efficient, targeted parameter changes. Quick check: Validate rank-one assumption holds across different concept types.
- **Concept redirection**: The process of mapping one concept's representation to another's; needed to substitute dangerous knowledge with harmless alternatives. Quick check: Test redirection works across different anchor entities.
- **Cross-format generalization**: Ability to forget concepts regardless of how they're queried; needed to address real-world unlearning scenarios. Quick check: Evaluate performance across all 4 task formats.

## Architecture Onboarding

**Component Map**: Characteristic activations -> Null-space projection -> Rank-one update -> MLP modification -> Unlearning

**Critical Path**: Concept extraction (5 sentences) → K0 null-space computation → MLP parameter update → Evaluation across 4 formats

**Design Tradeoffs**: Training-free approach trades computational efficiency (21s vs hours) for potential precision loss compared to fine-tuning; uses existing model knowledge vs creating new representations

**Failure Signatures**: 
- Excessive retain set degradation (>10%) indicates incorrect null-space projection
- Poor cross-task generalization suggests wrong MLP layers modified
- Computational overhead >21s suggests K0 matrix not properly cached

**3 First Experiments**:
1. Verify characteristic activation collection by checking activation similarity across the 5 chat-templated sentences
2. Test null-space projection by ensuring retain set performance stays above 50% baseline
3. Validate MLP modification by confirming weight updates follow rank-one structure

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can pre-training a dedicated "fictional anchor entity" serve as a more controllable and effective redirection target for ROCR than existing real-world entities or non-semantic vectors?
- Basis in paper: [explicit] Appendix D states future research could explore "pre-training a fictional anchor entity... specifically designed to serve as a redirection target... [which] could lead to more controllable model outputs."
- Why unresolved: Current experiments only utilize existing entities (e.g., Trump) or simple vectors (e.g., Gaussian noise), which may inherit unwanted associations or lack semantic coherence, limiting the precision of the redirection.
- Evidence: Comparison of unlearning efficacy and output controllability when redirecting to a specifically pre-trained fictional entity versus existing entities on the ORT benchmark.

### Open Question 2
- Question: Does ROCR maintain robustness against stronger adversarial strategies, such as gradient-based optimization or prompt perturbations?
- Basis in paper: [explicit] Section 2.2 restricts the threat model to "simple natural language queries," explicitly excluding "stronger adversarial strategies such as using optimizers."
- Why unresolved: The paper evaluates only weak black-box queries; it remains unknown if parameter redirection persists against white-box attacks that optimize tokens to maximize the probability of the original forgotten knowledge.
- Evidence: Evaluation of ROCR against white-box gradient-based attacks (e.g., GCG) to test if the concept redirection can be bypassed by optimized adversarial suffixes.

### Open Question 3
- Question: How does ROCR perform in multi-target unlearning scenarios where numerous concepts must be forgotten simultaneously?
- Basis in paper: [inferred] Appendix B states experiments were "evaluated in a single-target unlearning setting," leaving the interaction between multiple rank-one updates unexplored.
- Why unresolved: Applying multiple rank-one updates to MLP parameters could lead to interference or superposition effects, potentially degrading the model's general capabilities or the efficacy of individual unlearning operations.
- Evidence: Experiments applying ROCR to forget sets of increasing size (e.g., 50 or 100 targets) simultaneously, measuring the degradation of retain-set performance and average unlearning success.

## Limitations
- Relies on chat-templated sentences without specifying exact templates, creating reproducibility challenges
- Uses only 5 sentences to characterize dangerous concepts, which may not capture full semantic breadth
- Default redirection target "Donald Trump" is arbitrary without justification for why this entity serves as an effective knowledge sink
- Single-target focus leaves multi-target scenarios and potential interference effects unexplored

## Confidence

**High Confidence**: The experimental demonstration of Form-Dependent Bias (cross-task transfer failure and unseen token generalization failure) is robust and well-documented. The comparative performance metrics against baseline methods are reliable given the systematic evaluation protocol.

**Medium Confidence**: The theoretical foundation of rank-one concept redirection is sound, but the practical effectiveness depends heavily on implementation details not fully specified (chat templates, layer selection strategy).

**Low Confidence**: The generalizability of ROCR beyond the specific ORT benchmark setup and LLaMA-based models tested remains unproven.

## Next Checks

1. **Template Specification**: Obtain or reconstruct the exact chat templates used for characteristic activation collection to ensure reproducibility of the N=5 sentence generation process.

2. **Layer Selection Sensitivity**: Systematically evaluate ROCR's performance when modifying different MLP layer combinations (not just [4,5,6]) to determine if current layer selection is optimal or arbitrary.

3. **Cross-Model Generalization**: Test ROCR on models substantially different from LLaMA (different architectures, scales, or pretraining objectives) to validate whether the null-space projection approach transfers across model families.