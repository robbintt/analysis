---
ver: rpa2
title: Simple-Sampling and Hard-Mixup with Prototypes to Rebalance Contrastive Learning
  for Text Classification
arxiv_id: '2405.11524'
source_url: https://arxiv.org/abs/2405.11524
tags:
- learning
- class
- classification
- text
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of imbalanced text classification,
  where supervised contrastive learning (SCL) is sensitive to class imbalance and
  amplifies the imbalance ratio quadratically. To solve this, the authors propose
  SharpReCL, a novel model that combines simple-sampling and hard-mixup techniques
  with prototype-based contrastive learning.
---

# Simple-Sampling and Hard-Mixup with Prototypes to Rebalance Contrastive Learning for Text Classification

## Quick Facts
- **arXiv ID:** 2405.11524
- **Source URL:** https://arxiv.org/abs/2405.11524
- **Reference count:** 40
- **Primary result:** SharpReCL outperforms existing methods on imbalanced text classification, achieving state-of-the-art accuracy and macro-F1 scores across six benchmark datasets with imbalance ratios up to 50.

## Executive Summary
This paper addresses the problem of imbalanced text classification where supervised contrastive learning (SCL) amplifies class imbalance quadratically. The authors propose SharpReCL, a novel model that combines simple-sampling and hard-mixup techniques with prototype-based contrastive learning. The model uses class prototypes as anchors to generate hard positive and negative samples via mixup, creating a balanced dataset for SCL. Additionally, the classification and SCL branches interact through prototype vectors for mutual guidance. Experiments on six benchmark datasets show that SharpReCL outperforms existing methods, including popular large language models, achieving high accuracy and macro-F1 scores across various imbalanced settings.

## Method Summary
SharpReCL addresses imbalanced text classification by rebalancing contrastive learning through a two-branch architecture. The model uses BERT-base as encoder, with a balanced classification branch employing logit compensation and a balanced contrastive learning branch using supervised contrastive loss. Class prototypes are derived from classification branch weights and projected through an MLP head. These prototypes serve as anchors in every training batch, ensuring minority class presence. The model constructs balanced positive/negative sets by identifying hard samples (top-k samples with extreme similarity to prototypes) and generating synthetic samples via mixup. The overall loss combines classification loss and weighted contrastive loss, with interaction between branches through prototype vectors.

## Key Results
- SharpReCL achieves state-of-the-art performance on six benchmark datasets (R52, Ohsumed, TREC, DBLP, Biomedical, CR) across imbalance ratios of 10, 20, and 50.
- The model significantly outperforms popular large language models including Llama and Qwen on both accuracy and macro-F1 metrics.
- Ablation studies confirm that both simple-sampling and hard-mixup components are critical for performance, with hard-mixup being particularly important for minority class performance.

## Why This Works (Mechanism)

### Mechanism 1: Prototype Vectors Bridge Classification and Contrastive Learning Branches
The linear classification layer weights are projected through an MLP head to produce prototype vectors for each class. These prototypes are added to every training batch and serve as anchors in the target sample set during contrastive learning, guaranteeing that even classes with few samples contribute to the learning objective. This ensures minority class presence in every batch and guides contrastive learning through class-specific representatives.

### Mechanism 2: Quadratic Imbalance Amplification is Countered by Balanced Target Sample Construction
Standard supervised contrastive learning amplifies dataset imbalance by the square of the original ratio because SCL operates on pairs. SharpReCL constructs balanced positive/negative sets with equal cardinality across all classes, so each class contributes equally to contrastive gradients regardless of original frequency. This rebalancing mitigates the quadratic amplification effect.

### Mechanism 3: Hard Samples Generated via Mixup Prevent Gradient Vanishing
Hard positives are selected as top-k samples with low similarity to their class prototype, while hard negatives are top-k samples with high similarity to a different class prototype. These are mixed using Beta-distributed coefficients to create synthetic samples. This prevents gradient contributions from easy pairs from diminishing during training, maintaining gradient magnitude for minority classes.

## Foundational Learning

- **Concept: Supervised Contrastive Learning (SCL) Loss**
  - **Why needed here:** The entire SharpReCL architecture modifies SCL for imbalance. Understanding how SCL treats same-class samples as positives and different-class as negatives is essential for grasping the rebalancing motivation.
  - **Quick check question:** Given a batch with 3 classes (sizes 50, 30, 20), how many positive pairs does a minority class sample form under SCL, and how does this compare to a majority class sample?

- **Concept: Logit Compensation for Imbalanced Classification**
  - **Why needed here:** The classification branch uses logit compensation instead of standard cross-entropy. This prior-based adjustment is critical for why the classification branch produces useful prototypes.
  - **Quick check question:** If class A has 1000 samples and class B has 100 samples, what logit adjustment would be applied to each during training?

- **Concept: Mixup Data Augmentation**
  - **Why needed here:** Hard-mixup is the core synthetic data generation technique. Understanding that mixup creates convex combinations of embeddings with interpolated labels is necessary to interpret the synthetic sample generation.
  - **Quick check question:** If you mix embeddings $z_i$ and $z_j$ with $\alpha = 0.3$, what is the resulting synthetic embedding, and how does the Beta distribution for $\alpha$ differ from uniform sampling?

## Architecture Onboarding

- **Component map:** Input text -> BERT encoder -> Classification branch (logits + prototypes) + Contrastive branch (projections) -> Hard sample mining -> Mixup generation -> Balanced contrastive loss computation -> Combined loss backpropagation

- **Critical path:**
  1. BERT encodes batch → features
  2. Classification branch: features → logits (with compensation) AND weights → prototypes via projection
  3. CL branch: features → projections; augment batch with prototypes
  4. Hard sample identification using prototype similarities
  5. Mixup hard samples; combine with simple-sampled set
  6. Compute overall loss on rebalanced set; backprop through both branches

- **Design tradeoffs:**
  - Computational overhead: Each batch requires prototype computation, similarity scoring, and mixup operations, adding ~15-25% training time overhead compared to vanilla SCLCE
  - Memory footprint: Storing hard positive/negative sets for all classes requires O(C × k) additional memory per batch, manageable but scales with class count
  - Hard-mixup ratio schedule: Progressive increase assumes models benefit from easier samples early; tuning may be needed for specific datasets

- **Failure signatures:**
  - Prototype drift: Validation accuracy plateaus while training loss decreases; monitor prototype-to-mean-class-embedding cosine similarity
  - Hard sample collapse: Synthetic samples cluster near prototypes; check mixup coefficient distribution for diversity
  - Minority class overfitting: Macro-F1 significantly trails accuracy; reduce synthetic sample count or increase simple-sampling ratio

- **First 3 experiments:**
  1. **Baseline validation:** Run SharpReCL on TREC (small, 6 classes, relatively balanced) with ir=10 to verify implementation matches paper (target: ~97.2% Acc, 97.5% F1)
  2. **Ablation on hard-mixup:** On Ohsumed (imbalanced medical, 23 classes), compare full model vs. w/o HM to isolate hard-mixup contribution
  3. **Imbalance ratio scaling:** Test DBLP at ir=10, 20, 50 to verify robustness and check for non-monotonic performance degradation

## Open Questions the Paper Calls Out

- **Open Question 1:** How does SharpReCL performance compare against domain-adapted or instruction-tuned Large Language Models (LLMs) rather than general-purpose baselines?
  - The paper attributes lower performance of baselines like Llama to lack of "domain-specific training data," but only evaluates general versions without exploring advanced prompting or domain fine-tuning.

- **Open Question 2:** Can an adaptive or learnable data augmentation strategy be developed to universally optimize performance across different text classification datasets?
  - The authors note that "no universal method [augmentation] can perform optimally on all datasets," yet the model defaults to word substitution, limiting its ability to handle unique linguistic characteristics.

- **Open Question 3:** How does the model behave under extreme class imbalance ratios (e.g., ir > 100) compared to the tested scenarios (ir ≤ 50)?
  - While the paper proves that SCL squares the imbalance ratio, the empirical evaluation primarily tests artificial imbalances up to 50 and naturally imbalanced datasets, leaving extreme sparse data regimes unverified.

## Limitations
- The paper assumes prototypes derived from classification weights remain reliable class representatives under severe imbalance but does not validate prototype-to-class-sample alignment during training
- The exact hard-mixup ratio schedule sensitivity is only shown via ablation on two datasets; generalization across diverse imbalance patterns is not established
- Computational overhead (15-25% estimated) is not benchmarked against memory or wall-clock constraints for larger-scale applications

## Confidence
- **High Confidence:** Prototype-based rebalancing effectively counters quadratic imbalance amplification in SCL (supported by ablation and mathematical analysis)
- **Medium Confidence:** Hard-mixup improves minority class gradients without overfitting (limited by lack of noise robustness testing)
- **Low Confidence:** The progressive hard-mixup ratio schedule is optimal for all imbalance scenarios (only validated on two datasets with specific settings)

## Next Checks
1. Monitor prototype-to-mean-class-embedding cosine similarity during training to detect and quantify prototype drift under severe imbalance
2. Test hard-mixup performance with noisy labels (e.g., 5-10% corruption) to assess robustness to mislabeled samples
3. Benchmark SharpReCL against larger-scale imbalanced datasets (e.g., Amazon reviews, EURLex) to validate scalability and generalization