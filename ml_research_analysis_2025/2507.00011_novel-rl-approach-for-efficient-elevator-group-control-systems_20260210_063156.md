---
ver: rpa2
title: Novel RL approach for efficient Elevator Group Control Systems
arxiv_id: '2507.00011'
source_url: https://arxiv.org/abs/2507.00011
tags:
- agent
- elevator
- environment
- passenger
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of efficient elevator dispatching
  in large buildings, where traditional heuristic controllers struggle with the stochastic
  and combinatorial nature of passenger arrivals. The authors model a 6-elevator,
  15-floor system as a Markov Decision Process and train a Reinforcement Learning
  agent using Dueling Double Deep Q-learning.
---

# Novel RL approach for efficient Elevator Group Control Systems

## Quick Facts
- arXiv ID: 2507.00011
- Source URL: https://arxiv.org/abs/2507.00011
- Reference count: 28
- Primary result: RL-based EGCS outperforms ETD rule-based algorithm by ~10% in passenger travel time

## Executive Summary
This paper addresses the challenge of efficient elevator dispatching in large buildings using reinforcement learning. Traditional heuristic controllers struggle with the stochastic and combinatorial nature of passenger arrivals. The authors propose modeling a 6-elevator, 15-floor system as a Markov Decision Process and training a Dueling Double Deep Q-learning agent. A key innovation is the use of "infra-steps" to model continuous passenger arrivals, along with a novel action space encoding that avoids combinatorial explosion by dispatching elevators only when new hall calls arrive. The RL-based Elevator Group Control System (EGCS) achieves approximately 10% improvement in passenger travel time compared to a modern rule-based ETD algorithm while maintaining comparable energy consumption.

## Method Summary
The method models elevator dispatching as an MDP with a custom discrete-event simulator advancing in 0.1s "infra-steps." The agent receives 28-dimensional state vectors including hall call information, time, day of week, and per-elevator features (position, ETD score, speed, weight). A novel action space encoding dispatches elevators only when new hall calls arrive, reducing combinatorial complexity. The agent uses Dueling Double DQN with either combinatorial (41 outputs) or branching (12 outputs) architectures. Fixed discounting (γ=0.95) is applied to rewards accumulated between agent actions. Training uses 10M steps with AdamW optimizer, Huber loss, and epsilon-greedy exploration decaying from 1 to 0.1.

## Key Results
- RL-based EGCS achieves ~10% lower passenger wait time (~2 seconds) compared to ETD baseline
- Combinatorial action space architecture outperforms branching architecture by better coordinating elevator dispatch decisions
- Fixed discounting strategy provides more stable learning than variable discounting in the discrete-event formulation
- Energy consumption remains comparable between RL agent and rule-based ETD algorithm

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The specific encoding of the action space significantly reduces the combinatorial complexity that typically makes elevator dispatching intractable for standard RL methods.
- Mechanism: Instead of providing a control action at every discrete time step, the agent only provides a dispatch action when a new passenger registers a hall call. This decision assigns one or more elevators to that specific call. All lower-level movement is handled automatically by the elevator's internal logic.
- Core assumption: The lower-level logic of an elevator is efficient enough that the primary optimization lever is the high-level assignment of elevators to new calls.
- Evidence anchors:
  - [abstract] "...a novel action space encoding that avoids combinatorial explosion by dispatching elevators only when new hall calls arrive."
  - [section 2.2.1] "We only prompt the EGCS for an action when a new passenger or passenger group enters the system... The hall call floor is then added to the selected elevator(s)'s destination queue..."
  - [corpus] Corpus papers on dispatching (e.g., CoDriveVLM, LLM-ODDR) focus on matching optimization but do not explore this specific hierarchical action space decomposition for elevator control.
- Break condition: This mechanism breaks if the optimal policy requires fine-grained control between hall calls, such as proactively moving an empty elevator to a different floor for parking without a specific call.

### Mechanism 2
- Claim: The use of a fixed discount factor (γ) with the infra-step formulation provides a more stable and effective learning signal compared to a variable, time-aware discount factor.
- Mechanism: The environment simulates at fine-grained 0.1s intervals (infra-steps). The agent only acts at decision points (new hall calls), which can be separated by a variable number of infra-steps. The fixed discounting scheme sums all rewards received during the infra-steps between two agent actions and applies the discount factor (γ) only once to the next state's value, regardless of how much time passed.
- Core assumption: The reward accumulated between decision points is roughly proportional to the "importance" of that interval, and the variability in time between decisions is not the dominant factor in the value function.
- Evidence anchors:
  - [abstract] "They also explore discounting strategies adapted to the infra-step formulation."
  - [section 3.2] "...the fixed discounting scheme works best for two main reasons. First, the number of infra-steps contained in a step is highly variable... making for a very unstable learning problem [with variable discounting]. ... By disregarding the inter-step length, the average rewards remain similar in transitions...".
  - [corpus] The related paper (Wan et al., 2024) employs a variable discounting approach. This paper directly contrasts its fixed approach against that baseline.
- Break condition: This mechanism may break if the cost of waiting is highly non-linear or if the value of a future state is fundamentally time-dependent in a way not captured by the accumulated reward.

### Mechanism 3
- Claim: The combinatorial action space architecture, which treats each possible set of elevator assignments as a distinct output, enables better coordination than an action-branching architecture where elevators make independent binary decisions.
- Mechanism: In the combinatorial architecture, the single neural network outputs a Q-value for every legal combination of sending 1, 2, or 3 elevators to a new call. This allows the final decision to implicitly consider the interdependencies between elevators. The branching architecture forces a decomposition where each elevator makes a go/no-go decision independently.
- Core assumption: The number of possible action combinations is small enough (41) to be handled by a single output layer without making training intractable.
- Evidence anchors:
  - [section 2.3.2] "Combinatorial Action Space: The RL agent computes the Q-values for all combinations... This results in an action space of... 41 outputs." vs "Action Branching: ...each elevator makes an independent binary decision...".
  - [section 3.1] "The Combinatorial agent is largely superior to the Branching agent... It is unable to coordinate its branches well enough to avoid this situation.".
- Break condition: This mechanism breaks if the number of elevators (n) or the number of possible assignments per call (k) increases significantly.

## Foundational Learning

### Concept: Dueling Network Architecture
- Why needed here: The paper uses "Dueling Double Deep Q-learning." The dueling architecture separates the estimation of the state-value function V(s) from the state-dependent action advantage function A(s,a). This is useful in EGCS because many states may have similar values regardless of the specific elevator dispatched.
- Quick check question: In a state where all elevators are identical and idle, would the advantage values for dispatching any one of them be high or low, and what would the state value represent?

### Concept: Markov Decision Process (MDP) Formulation
- Why needed here: The core of the paper's contribution is modeling the elevator system as an MDP (S, A, P, R, γ). Understanding how to define the state space, action space, and reward signal is the prerequisite for applying any RL algorithm.
- Quick check question: Based on the paper, what are the key elements included in the state vector s provided to the agent at each decision point?

### Concept: Exploration vs. Exploitation (Epsilon-Greedy)
- Why needed here: Training the RL agent requires it to discover good dispatching policies through trial and error. The training details mention an epsilon decay strategy, starting with high exploration and ending with exploitation.
- Quick check question: Why might it be problematic if the epsilon (exploration rate) remained high (e.g., 0.5) during the final evaluation of the trained agent's performance?

## Architecture Onboarding

### Component map:
Environment Simulator -> State Builder -> Reward Calculator -> RL Agent (Dueling Double DQN) -> Training Loop

### Critical path:
The agent's performance hinges on the State Builder -> Reward Calculator -> RL Agent path. If the state features don't capture necessary information or the reward signal is misaligned, the agent will fail.

### Design tradeoffs:
- Action Space Size vs. Coordination: Combinatorial architecture enables better coordination but its output layer size scales factorially with the number of elevators.
- Simulation Fidelity vs. Computational Cost: Modeling with 0.1s infra-steps adds fidelity but computational cost.
- Reward Engineering vs. Emergent Behavior: Manually balancing sub-rewards is an art. A poorly weighted reward could lead to undesirable emergent behavior.

### Failure signatures:
1. Action Space Explosion: If you scale the system to 10 elevators, the combinatorial action space will be far too large for a simple output layer.
2. Reward Hacking: The agent might discover a way to accumulate reward without solving the core problem (e.g., cycling elevators).
3. Variable Discounting Instability: If you revert to variable discounting, training might become unstable with high variance in loss.

### First 3 experiments:
1. Baseline Reproduction: Implement the described environment and the ETD rule-based agent. Train the combinatorial Dueling DQN agent and compare its performance on the test set against the provided baseline metrics.
2. Ablation on Action Space: Train both the "Combinatorial" and the "Branching" agents on the same problem. Confirm that the branching agent fails to coordinate and has lower total reward.
3. Sensitivity to Reward Weights: Re-train the combinatorial agent after doubling the "Movement penalty". Measure the trade-off between energy consumption and passenger wait time.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework be extended to allow the agent to revisit and revise dispatching decisions that become suboptimal over time?
- Basis in paper: [explicit] Section 4 identifies the need to develop "the ability to revisit decisions once they become suboptimal."
- Why unresolved: The current architecture enforces an immediate, irrevocable commitment of an elevator to a hall call, even if subsequent events render the choice inefficient.
- What evidence would resolve it: An implementation allowing reassignment without computational explosion, validated by showing reduced passenger wait times compared to the current committed strategy.

### Open Question 2
- Question: Can the trained agent successfully bridge the "reality gap" when deployed in the physical VU Amsterdam building?
- Basis in paper: [explicit] Section 4 concludes that future work must verify "its ability to bridge the reality gap" in a real-life setting.
- Why unresolved: All reported results are derived from a simulation environment using recorded data, not physical hardware.
- What evidence would resolve it: Successful deployment and performance metrics (wait time, energy) collected from the live system under varying real-world traffic conditions.

### Open Question 3
- Question: Is there a theoretical justification for the empirically observed superiority of fixed discounting over variable discounting in this discrete-event formulation?
- Basis in paper: [explicit] Section 2.2.3 notes "limited theoretical foundations" for the variable discounting approach, which performed poorly in Section 3.2 due to highly variable step lengths.
- Why unresolved: The paper empirically determined fixed discounting was more stable but did not provide a theoretical proof for why variable discounting failed to handle the variance in infra-steps.
- What evidence would resolve it: A theoretical analysis of Bellman equation stability given the high variance in infra-steps between decision points.

## Limitations
- Validation focuses on a single building configuration (6 elevators, 15 floors) and traffic dataset
- No comparison against other RL baselines from literature, only against one rule-based ETD algorithm
- Action space design assumes optimal lower-level elevator logic, potentially missing strategic empty vehicle repositioning opportunities
- Combinatorial action space scalability may become problematic for significantly larger elevator systems

## Confidence

- **High Confidence**: The core mechanism of action space encoding (dispatching only on new hall calls) and its impact on reducing combinatorial complexity is well-supported by the results showing the combinatorial agent outperforming the branching architecture. The fixed discounting approach and its stability advantages over variable discounting are clearly demonstrated.

- **Medium Confidence**: The claimed 10% improvement in passenger travel time versus the ETD baseline is credible given the experimental setup, but the absence of additional RL baselines makes it difficult to assess absolute performance. The reward function engineering and its balance between wait time and energy consumption appear reasonable but could benefit from sensitivity analysis.

- **Low Confidence**: The scalability of the approach to significantly larger elevator systems or different building configurations has not been demonstrated. The assumption that the lower-level elevator logic is optimal enough that high-level dispatch decisions are sufficient may not hold in all scenarios.

## Next Checks

1. **Baseline Comparison**: Implement and compare against at least one other published RL approach for elevator dispatching (e.g., from Wan et al. 2024 or similar) to establish relative performance.

2. **Scalability Test**: Evaluate the approach on a larger configuration (e.g., 10 elevators, 20 floors) to assess the practical limits of the combinatorial action space design.

3. **Reward Function Sensitivity**: Conduct an ablation study varying the relative weights of the sub-rewards (movement penalty, wait penalty, etc.) to quantify their impact on the trade-off between passenger service quality and energy consumption.