---
ver: rpa2
title: An Open-Source Dual-Loss Embedding Model for Semantic Retrieval in Higher Education
arxiv_id: '2505.04916'
source_url: https://arxiv.org/abs/2505.04916
tags:
- semantic
- educational
- retrieval
- embedding
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study addresses the limitations of general-purpose embedding
  models in educational semantic retrieval by developing domain-specific fine-tuned
  models for academic QA tasks. Two training strategies were evaluated: a contrastive
  learning approach using MultipleNegativesRankingLoss (MNRL) and a dual-loss model
  combining MNRL with CosineSimilarityLoss.'
---

# An Open-Source Dual-Loss Embedding Model for Semantic Retrieval in Higher Education

## Quick Facts
- arXiv ID: 2505.04916
- Source URL: https://arxiv.org/abs/2505.04916
- Reference count: 5
- Primary result: Dual-loss model achieves 88.10% accuracy on faculty information retrieval and 87.50% on teaching assistant information retrieval

## Executive Summary
This paper addresses the limitations of general-purpose embedding models in educational semantic retrieval by developing domain-specific fine-tuned models for academic QA tasks. The authors evaluate two training strategies: a contrastive learning approach using MultipleNegativesRankingLoss (MNRL) and a dual-loss model combining MNRL with CosineSimilarityLoss. Using a synthetic dataset of 3,197 sentence pairs constructed through manual curation and LLM-assisted generation, the dual-loss model outperforms open-source baselines and approaches the performance of proprietary OpenAI embeddings. This work provides open-source, transparent, and domain-aligned embedding models for educational applications such as syllabus-based question answering and intelligent academic chatbots.

## Method Summary
The study fine-tunes the all-MiniLM-L6-v2 sentence transformer on a synthetic dataset of 3,197 sentence pairs (2,710 positive, 487 negative) to create domain-specific embedding models for educational semantic retrieval. Two training strategies are evaluated: MNRL-only and a dual-loss approach combining MNRL with CosineSimilarityLoss. The models are trained on 25 epochs with batch size 64, using AdamW optimizer and WarmupCosine scheduler. Evaluation uses 28 university syllabi chunked into ~300-word segments, with top-3 cosine similarity retrieval passed to GPT-4o-mini for answer generation. The dual-loss model demonstrates superior performance in distinguishing between closely related academic concepts like office hours versus lecture times.

## Key Results
- Dual-loss model achieves 88.10% accuracy in faculty information retrieval
- Dual-loss model achieves 87.50% accuracy in teaching assistant information retrieval
- Outperforms open-source baselines while approaching proprietary OpenAI embedding performance

## Why This Works (Mechanism)

### Mechanism 1: Dual-Objective Geometric Alignment
Combining ranking-based and distance-based loss functions improves semantic fidelity by enforcing both distinct clusters (via MNRL) and accurate internal density (via CosineSimilarityLoss). This addresses the challenge of distinguishing fine-grained academic concepts that appear in similar textual forms.

### Mechanism 2: Implicit-to-Explicit Query Mapping
Training on synthetic pairs that map conversational questions to formal document statements bridges the linguistic gap between student queries and administrative text. The model learns to associate implicit phrasing with explicit, structured attributes in syllabi.

### Mechanism 3: Hard Negative Discrimination
Inclusion of semantically close but incorrect pairs forces the model to learn fine-grained feature distinctions rather than relying on superficial keyword matching. This reduces false positives when retrieving information about different academic roles.

## Foundational Learning

- **Concept: Contrastive Learning (MNRL)**
  - Why needed here: Core training dynamic that treats other batch elements as implicit negatives without explicit negative labels
  - Quick check question: If you increase batch size from 64 to 128 in MNRL, how does the number of implicit negatives per anchor change?

- **Concept: Semantic Calibration vs. Ranking**
  - Why needed here: Understanding why ranking (A > B) differs from similarity scoring (A is 0.9 similar to B) is crucial for the dual-loss architecture
  - Quick check question: Why would a model that perfectly ranks relevant documents above irrelevant ones still fail in threshold-based retrieval?

- **Concept: Chunking Strategies**
  - Why needed here: Fixed-size chunks (~300 words) affect retrieval fidelity and must be understood for reproducing results
  - Quick check question: What is the risk of fixed-size chunking on a syllabus document that contains large tables or lists?

## Architecture Onboarding

- **Component map:** Base Model (all-MiniLM-L6-v2) -> Two DataLoaders (Positives-only for MNRL, Labeled Pos/Neg for Cosine) -> Alternating optimization between two loss functions -> Chunked Syllabus -> Embedding -> Cosine Sim -> Top-3 Chunks -> LLM (GPT-4o-mini)

- **Critical path:** 1) Data Curation (3,197 pairs generation bottleneck), 2) Dual-Loss Loop (coordinated multi-objective training), 3) Retrieval Eval (manual validation of generated answers)

- **Design tradeoffs:** Open vs. Proprietary (sacrificing raw performance peak for transparency and cost control), Synthetic Data (faster but risks model collapse), Architecture Size (L6 ensures low latency but may limit semantic depth)

- **Failure signatures:** High "I don't know" rate (domain shift in syllabus formatting), Confusion of Roles (failure in hard negative discrimination), Chunk Boundary Errors (answers split across chunks missed)

- **First 3 experiments:** 1) Baseline Validation (run on new syllabus not in 28-file test set), 2) Loss Ablation (train models with only CosineSimilarityLoss and only MNRL), 3) Chunking Stress Test (compare fixed-size vs semantic chunking impact)

## Open Questions the Paper Calls Out

- **Generalization to other educational artifacts:** Can the models generalize to lecture slides or academic policies beyond syllabi? (Current study restricted to syllabi; future research suggested)

- **Structure-aware segmentation:** Does replacing fixed-size chunking with structure-aware segmentation improve retrieval precision? (Current method may split related content; semantic boundaries suggested)

- **Harder negative examples:** To what extent does training with harder negative examples improve discriminative capabilities? (Current relies on in-batch negatives; curriculum learning suggested)

## Limitations

- **Synthetic data generalization:** Model performance relies heavily on synthetic training pairs; no testing on institutional variations in syllabus formatting or terminology

- **Hard negative quality:** Effectiveness depends on quality of 487 hard negative pairs; paper doesn't detail negative pair selection or difficulty calibration

- **Loss function interaction:** Exact alternation strategy between MNRL and CosineSimilarityLoss batches is unspecified; reproducing exact performance is uncertain

## Confidence

- **High Confidence:** Baseline architecture (all-MiniLM-L6-v2 with MNRL training) and general improvement over open-source baselines are well-supported by experimental results

- **Medium Confidence:** Specific contribution of dual-loss approach is supported by accuracy gains, but lack of ablation studies introduces uncertainty about optimal configuration

- **Low Confidence:** Generalization claims to new institutions or syllabus formats are not empirically validated beyond 28-syllabi test set

## Next Checks

1. **Out-of-Domain Syllabus Test:** Evaluate dual-loss model on syllabi from institutions not in original 28-file test set to measure generalization to different formatting conventions

2. **Loss Ablation Experiment:** Train models with only CosineSimilarityLoss and only MNRL using identical hyperparameters to quantify specific performance contribution of each loss component

3. **Chunking Strategy Comparison:** Replace fixed-size chunking with semantic chunking (e.g., splitting by markdown headers) and measure impact on retrieval accuracy, particularly for questions requiring cross-chunk context