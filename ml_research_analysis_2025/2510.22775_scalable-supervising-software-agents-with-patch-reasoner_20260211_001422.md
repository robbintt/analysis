---
ver: rpa2
title: Scalable Supervising Software Agents with Patch Reasoner
arxiv_id: '2510.22775'
source_url: https://arxiv.org/abs/2510.22775
tags:
- patch
- patches
- training
- arxiv
- issue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces R4P, a reasoning-based patch verifier for
  software engineering agents that addresses the scalability bottleneck of test-based
  supervision. R4P uses group-wise verification where multiple patches are compared
  against each other to provide dense rewards, eliminating the need for test sandboxes.
---

# Scalable Supervising Software Agents with Patch Reasoner

## Quick Facts
- arXiv ID: 2510.22775
- Source URL: https://arxiv.org/abs/2510.22775
- Reference count: 21
- Primary result: R4P verifier achieves 72.2% accuracy on SWE-bench-verified, 50× faster than testing, and trains Mini-SE agent to 26.2% Pass@1 (32.8% with TTS)

## Executive Summary
This paper introduces R4P, a reasoning-based patch verifier that addresses the scalability bottleneck of test-based supervision in software engineering agents. R4P uses group-wise verification where multiple patches are compared against each other to provide dense rewards, eliminating the need for test sandboxes. Trained via rule-based reinforcement learning on real-world issues, R4P achieves 72.2% accuracy on SWE-bench-verified, surpassing OpenAI o3. The paper also presents Mini-SE, a lightweight agent trained purely with R4P rewards, which achieves 26.2% Pass@1 and 32.8% with test-time scaling. R4P verifies patches within one second—50× faster than testing—and provides stable rewards for scalable data utilization.

## Method Summary
R4P is a Qwen-2.5-Coder-Instruct-32B model trained via GRPO with a group-wise objective, where it verifies multiple patches against each other for the same issue to provide dense rewards. The training dataset consists of 2,438 SWE-Gym issues with 6 patches each, creating 14,628 total patches sampled via Claude-3.7-sonnet/OpenHands. Mini-SE is a Qwen-3-32B agent trained purely with RL using R4P rewards, featuring a simple "entity → code → patch" workflow with static code search and syntax-validated edit tools. The system achieves verification in under one second per patch while maintaining 72.2% accuracy on SWE-bench-verified.

## Key Results
- R4P achieves 72.2% accuracy on SWE-bench-verified (F1 63.3%, EM 41.8%)
- Mini-SE agent reaches 26.2% Pass@1 and 32.8% Best@16 with test-time scaling
- Verification speed: <1 second per patch, 50× faster than traditional testing
- Group-wise approach transforms binary outcome space into denser reward signal

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Group-wise verification provides more stable supervision than binary classification.
- Mechanism: By presenting multiple patches together, the model cross-references edits to identify subtle bugs, transforming sparse binary rewards into denser multi-class problems.
- Core assumption: Incorrect patches provide useful negative context for detecting errors when compared against other patches.
- Evidence anchors: [abstract] group-wise objective enables dense rewards; [Section 3] group-wise transforms binary space into denser one for stable RL training.

### Mechanism 2
- Claim: Reasoning-based verification is significantly faster and more scalable than sandbox-based test execution.
- Mechanism: R4P performs static analysis via model forward passes instead of heavy Docker builds and container execution.
- Core assumption: The model can learn to reason about code correctness from static text without dynamic execution.
- Evidence anchors: [abstract] verifies patches within one second—50× faster than testing; [Section 5.2] average time per instance does not exceed 1 second.

### Mechanism 3
- Claim: A lightweight agent scaffold can be trained effectively using R4P's rewards for both RL and test-time scaling.
- Mechanism: Mini-SE uses focused workflow with two tools (static code search, syntax-validated edit) and R4P provides dense reward signal during RL.
- Core assumption: Decoupling patch generation from verification simplifies the agent's learning objective.
- Evidence anchors: [abstract] Mini-SE achieves 26.2% Pass@1 improved to 32.8% with R4P for test-time scaling; [Section 4] Mini-SE focuses solely on issue resolution without self-test-self-fix.

## Foundational Learning

- **Reinforcement Learning with Verifiable Rewards (RLVR)**: Why needed? R4P's training uses rule-based RL where reward is verifiable accuracy, not human preference. Quick check: How does RLVR reward signal differ from standard RLHF?
- **Test-Time Scaling (TTS)**: Why needed? The paper frames patch verification as core component of TTS for selecting best patch from multiple candidates. Quick check: How does a verifier enable TTS for a language model?
- **Reward Hacking**: Why needed? Central problem the paper tries to solve—binary rewards are easy to hack. Quick check: What is "reward hacking" in context of training a reward model?

## Architecture Onboarding

- **Component map**: Issue + 4 Patches -> R4P -> Reasoning + Correct Patch IDs; Issue -> Mini-SE -> search_tool + edit_tool -> Patch
- **Critical path**: Correctness hinges on R4P's ability to reliably discriminate between correct and incorrect patches. If R4P is inaccurate, it provides noisy reward signal that hampers agent training and leads to poor patch selection.
- **Design tradeoffs**: Scaffold simplicity vs capability (Mini-SE uses simple toolset to speed training); Static verification vs execution (R4P avoids sandbox overhead but may miss runtime-only bugs); Group size fixed at N=4 (larger groups offer denser rewards but increase complexity).
- **Failure signatures**: Reward collapse (RL training becomes unstable); Over-estimation bias (verifier consistently rates incorrect patches as correct); Timeouts/infinite loops (legacy issue in test-based systems, R4P is designed to be immune).
- **First 3 experiments**: 1) Baseline ablation comparing R4P accuracy against strong baseline LLMs on same patch verification task; 2) Reward analysis monitoring training curve for Mini-SE to ensure stability; 3) Scaffold validation comparing Mini-SE's performance with and without R4P-guided TTS.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does a static patch verifier maintain alignment with true answer quality as the supervised agent's policy improves? [explicit] Section 7 states static reward model may become misaligned with true answer quality as agent policy improves. R4P weights remain fixed after training without mechanism to adapt to shifting distribution of generated patches.

- **Open Question 2**: Can reasoning-based rewards effectively supervise agents that perform complex in-loop self-verification? [explicit] Section 7 notes R4P cannot solely provide end-to-end supervision for complex agents that perform in-loop self-verification via test generation. R4P is designed to supervise patch generation outcome, not intermediate test execution process.

- **Open Question 3**: Does the group-wise verification approach transfer effectively to non-Python software repositories? [explicit] Section 7 identifies R4P and Mini-SE are Python-centric, requiring data collection and static analyzer adaptation for other languages. Current implementation relies on Python-specific training data and code search tooling.

## Limitations

- R4P's accuracy on SWE-bench-verified hasn't been directly compared against sandbox-based testing on same dataset
- Group-wise verification lacks ablation studies showing optimal group sizes and compositions
- Mini-SE agent performance lags behind state-of-the-art agents using more complex scaffolds and hybrid verification approaches

## Confidence

- **High Confidence**: R4P's speed advantage over testing (50× faster) and general principle that group-wise verification provides denser rewards than binary classification
- **Medium Confidence**: R4P's accuracy on SWE-bench-verified (72.2%) and ability to surpass o3
- **Low Confidence**: Claim that R4P is "more accurate" than testing without direct empirical evidence

## Next Checks

1. **Head-to-Head Accuracy Comparison**: Run R4P and traditional sandbox-based verifier on shared held-out set of issues from SWE-bench to measure accuracy and false positive/negative rates.

2. **Group Size Ablation Study**: Train and evaluate R4P with different group sizes (N=2, 3, 4, 6) to measure impact on accuracy, training stability, and computational cost.

3. **Runtime Bug Detection Test**: Create synthetic dataset where correct/incorrect patches differ only in runtime behavior (memory leaks, infinite loops, external dependencies) to quantify limitations of static reasoning-based verification.