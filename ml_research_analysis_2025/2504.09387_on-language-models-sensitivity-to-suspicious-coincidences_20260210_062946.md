---
ver: rpa2
title: On Language Models' Sensitivity to Suspicious Coincidences
arxiv_id: '2504.09387'
source_url: https://arxiv.org/abs/2504.09387
tags:
- hypothesis
- cities
- hypotheses
- input
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) exhibit
  human-like sensitivity to suspicious coincidences during inductive reasoning. Humans
  tend to favor specific hypotheses over general ones when generalizing from data,
  assuming the data was sampled in a specific way.
---

# On Language Models' Sensitivity to Suspicious Coincidences

## Quick Facts
- arXiv ID: 2504.09387
- Source URL: https://arxiv.org/abs/2504.09387
- Reference count: 29
- This paper investigates whether large language models (LLMs) exhibit human-like sensitivity to suspicious coincidences during inductive reasoning.

## Executive Summary
This paper investigates whether large language models (LLMs) exhibit human-like sensitivity to suspicious coincidences during inductive reasoning. Humans tend to favor specific hypotheses over general ones when generalizing from data, assuming the data was sampled in a specific way. The authors tested this by analyzing LLMs' performance on the number game (predicting if a number belongs to a set) and a newly created city game. Five models (Llama3-8b, GPT-4o, etc.) were evaluated using three prompting methods: zero-shot, chain-of-thought, and knowledge-rich (explicitly providing hypothesis information). Results showed that LLMs do not naturally exhibit suspicious coincidence effects in zero-shot settings, often defaulting to "yes" predictions. However, when provided with knowledge-rich prompts that explicitly guide reasoning over specific hypotheses, models like GPT-4o showed near-human-level sensitivity to suspicious coincidences, especially in the number domain. This suggests that LLMs' inductive reasoning can be enhanced by providing access to relevant hypothesis spaces, aligning their behavior more closely with human reasoning patterns.

## Method Summary
The authors evaluated five LLMs (Llama3-8b, Mistral-7b, Gemma-2-9b, GPT-3.5, GPT-4o) on two tasks: the Number Game (integers 1-100) and a new City Game (world cities). They used three prompting strategies: zero-shot, chain-of-thought (CoT), and knowledge-rich (explicitly providing entity attributes). Performance was measured using F1 score against the ground-truth category membership of the smallest compatible hypothesis. The knowledge-rich prompts required pre-filling entity attributes, while CoT encouraged the model to verbalize features. All models used greedy decoding, and outputs were post-processed via keyword matching for "yes"/"no" responses.

## Key Results
- LLMs do not naturally exhibit suspicious coincidence effects in zero-shot settings, often defaulting to "yes" predictions.
- Knowledge-rich prompting significantly improves suspicious coincidence sensitivity, especially in GPT-4o, which showed near-human-level performance on the number game.
- Chain-of-thought prompting only works when the model possesses sufficient parametric knowledge of the domain (e.g., GPT-4o vs. Mistral-7B).

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explicit enumeration of entity features (knowledge-rich prompting) enables LLMs to mimic Bayesian inference by constraining the hypothesis space.
- **Mechanism:** LLMs often fail to retrieve relevant hypothesis constraints (e.g., "powers of 2" vs. "even numbers") from parametric memory during zero-shot reasoning. By explicitly injecting feature data (e.g., "64 is... a power of 2") into the context, the model is forced to align its predictions with the smallest compatible hypothesis, thereby approximating the "size principle" where specific hypotheses are preferred over general ones.
- **Core assumption:** The model possesses the logical capacity to prioritize specific hypotheses over general ones if the relevant features are available in the immediate context, but fails to summon this context internally.
- **Evidence anchors:**
  - [abstract] "inductive reasoning behavior in LMs can be enhanced with explicit access to the hypothesis landscape."
  - [section 4] "When prompted using the Knowledge prompt, most LMs start to show a similar trend as humans... GPT-4o exhibits a behavior consistent with humans on the number game."
  - [corpus] Related work (*Shoot First, Ask Questions Later?*) suggests LMs struggle to act rationally in information-seeking without scaffolding, supporting the need for explicit hypothesis context.
- **Break condition:** If the model is presented with entities outside its training distribution where feature extraction is impossible, knowledge-rich prompting will fail to induce the effect.

### Mechanism 2
- **Claim:** Chain-of-Thought (CoT) prompting elicits suspicious coincidence sensitivity only when the model possesses high-fidelity parametric knowledge of the domain.
- **Mechanism:** CoT encourages the model to verbalize latent features of the input (e.g., identifying "16, 32, 2" as powers of 2). However, this acts as a retrieval mechanism. If the underlying parametric knowledge is weak (e.g., smaller models confusing mathematical properties), CoT cannot fabricate the necessary features, and the model defaults to generic (non-specific) predictions.
- **Core assumption:** CoT serves primarily to surface existing knowledge rather than to execute a purely symbolic logic procedure.
- **Evidence anchors:**
  - [section 4] "CoT prompt enables LMs to favor the smallest hypothesis, when LMs have sufficient knowledge... GPT-4o, where parametric knowledge is sufficient, the CoT prompt yields a trend similar to the Knowledge prompt."
  - [section 3] Table 3 shows Mistral-7B has low knowledge F1 (0.38) for numbers, which correlates with its failure to show sensitivity in CoT compared to GPT-4o (0.98 F1).
  - [corpus] *Language Agents Mirror Human Causal Reasoning Biases* corroborates that reasoning biases in models are contingent on underlying capabilities.
- **Break condition:** If a model has strong parametric knowledge but lacks the instruction-following capability to structure a CoT derivation, the mechanism fails.

### Mechanism 3
- **Claim:** Zero-shot LLMs default to an "AlwaysYes" bias because they lack a simulation of the data sampling process assumed in human pragmatics.
- **Mechanism:** Humans infer that a set like {2, 16} was *sampled* from a specific rule, making a general rule (even numbers) unlikely (a "suspicious coincidence"). Zero-shot LLMs appear to judge strictly on set membership compatibility (Is 4 compatible with {2, 16}? Yes) without reasoning about the probability of the sampling event itself, leading to a generic positive bias.
- **Core assumption:** The failure is not a lack of logic, but a lack of pragmatic context regarding *how* the data was generated (random sampling vs. exhaustive listing).
- **Evidence anchors:**
  - [abstract] "Humans... make assumptions as to how the data was sampled... We do not find strong evidence for suspicious coincidences in LMs' zero-shot behavior."
  - [section 4] "LMs produce 'Yes' most of the time... while Humans only do this 24% of the time."
  - [corpus] Evidence regarding sampling sensitivity is weak in the provided corpus; related papers focus on causal/planning biases rather than sampling heuristics.
- **Break condition:** If the prompt explicitly frames the input as a "random sample from a program," the "AlwaysYes" bias *might* reduce, though the paper suggests this alone is insufficient without hypothesis scaffolding.

## Foundational Learning

- **Concept: The Size Principle (Bayesian Inference)**
  - **Why needed here:** This is the mathematical formalization of the "suspicious coincidence" effect. It states that smaller, more specific hypotheses (e.g., *powers of 2*) predict the observed data with higher likelihood than larger hypotheses (e.g., *even numbers*) under random sampling.
  - **Quick check question:** Given the set {2, 4, 8}, why does a Bayesian model prefer "powers of 2" (size 7) over "even numbers" (size 50)?

- **Concept: Parametric vs. Contextual Knowledge**
  - **Why needed here:** The paper distinguishes between what a model "knows" (weights) and what it "uses" (context). Understanding this gap is crucial for diagnosing why CoT works for GPT-4o but not Mistral-7B.
  - **Quick check question:** Why does explicit prompting (Knowledge-rich) succeed where zero-shot fails even if the model's weights haven't changed?

- **Concept: Inductive Reasoning & Pragmatics**
  - **Why needed here:** The task is not just logic; it is about inferring intent (pragmatics). The "suspicious coincidence" is a communicative cue.
  - **Quick check question:** How does the assumption that "the speaker is informative" change the hypothesis selection from general to specific?

## Architecture Onboarding

- **Component map:**
  - Input Layer: Datasets (Number Game, City Game) -> formatted as "Computer program produced [X, Y, Z]"
  - Prompting Module: Three branches (Zero-shot, CoT, Knowledge-Rich). The Knowledge-Rich branch requires a pre-processing step to fetch entity attributes.
  - Inference Engine: Target LLMs (GPT-4o, Llama3, etc.)
  - Evaluation Suite: Calculates F1 score between model predictions (Yes/No) and the ground-truth category membership of the *smallest* compatible hypothesis

- **Critical path:**
  1. **Knowledge Verification:** Before testing reasoning, verify the model knows the facts (Table 3). If F1 < 0.6, the model cannot be tested for reasoning failure (it is a knowledge failure).
  2. **Knowledge-Rich Construction:** Map inputs (e.g., "64") to explicit feature lists ("even, power of 2") to create the intervention prompt.
  3. **Compatibility Scoring:** Compare model "Yes/No" against the ground-truth smallest hypothesis to measure sensitivity.

- **Design tradeoffs:**
  - **Domain choice:** Numbers allow precise hypothesis sizes; Cities are "fuzzier" but test generalization to non-math entities.
  - **Prompting:** Knowledge-rich prompts prove the mechanism but are impractical for real-world deployment (requires an oracle). CoT is practical but unreliable.

- **Failure signatures:**
  - **"AlwaysYes" Mode:** Model outputs "Yes" > 80% of the time (e.g., Llama-3-8b zero-shot). Indicates failure to utilize the input distribution for constraint.
  - **Hallucinated CoT:** Model claims a number has a property it doesn't have (e.g., "4 is odd"), leading to wrong inferences. Common in smaller models (Mistral-7B).

- **First 3 experiments:**
  1. **Parametric Baseline:** Run the "Hypothesis Knowledge" test (Table 3) on your target model. If it fails to identify "power of 2" for "64", stop—fix the knowledge base or use a larger model.
  2. **Zero-Shot vs. Random:** Run the zero-shot prompt. Verify if the "% Yes" is significantly higher than the Bayesian ideal (12%) and closer to Random (57%). This confirms the lack of sensitivity.
  3. **Scaffolded Intervention:** Implement the "Knowledge-Rich" prompt generator. Compare the F1 score with the "Smallest Hypothesis" against the Zero-Shot baseline. An increase in F1 as input size grows confirms the mechanism is recoverable.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can language models be trained to exhibit sensitivity to suspicious coincidences in zero-shot settings without external hypothesis guidance?
- Basis in paper: [explicit] The discussion states that findings "motivate methods that can make LMs sensitive to human-like suspicious coincidence effects in a zero-shot manner—e.g., by using distillation."
- Why unresolved: The study demonstrates that zero-shot performance is poor, and sensitivity currently relies on explicit prompting. It is unknown if this behavior can be internalized as a default reasoning capability through training.
- What evidence would resolve it: Fine-tuning experiments (e.g., knowledge distillation) that result in models naturally preferring specific hypotheses in zero-shot settings, matching human baseline performance.

### Open Question 2
- Question: What are the mechanistic differences in model states between architectures that are sensitive to communicative goals versus those that are not?
- Basis in paper: [explicit] The conclusion suggests the results "provide rich empirical evidence that could be used to further study mechanistic differences between LM states... which differ in their sensitivity to communicative goals."
- Why unresolved: While the paper identifies that knowledge-rich prompts change behavior, it does not explain the internal mechanistic shifts that occur when a model switches from a general "yes" bias to specific hypothesis selection.
- What evidence would resolve it: Mechanistic interpretability analysis (e.g., probing or causal tracing) comparing activations in zero-shot versus knowledge-rich conditions to identify the circuits responsible for hypothesis selection.

### Open Question 3
- Question: Why does Chain-of-Thought (CoT) prompting fail to elicit suspicious coincidence sensitivity in smaller models despite them possessing the requisite parametric knowledge?
- Basis in paper: [inferred] Results show GPT-4o (with high knowledge) succeeds with CoT, whereas Llama-3-8B and Mistral-7B (with lower knowledge) show no improvement over baselines, suggesting a specific failure to connect knowledge to reasoning in smaller architectures.
- Why unresolved: The paper establishes that smaller models have some knowledge (Table 3) but fail to apply it during CoT. It is unclear if this is a failure of retrieval, instruction following, or insufficient reasoning complexity in smaller weights.
- What evidence would resolve it: Ablation studies varying the amount of explicit knowledge retrieval required in CoT steps for models of different scales to determine the specific bottleneck.

## Limitations
- The study's reliance on constructed datasets and keyword matching for output parsing may not generalize to real-world tasks.
- Knowledge-rich prompting requires an external oracle to supply entity features, raising scalability concerns.
- The paper does not explore the impact of sampling heuristics or explicit context about data generation on model performance.

## Confidence
- **High Confidence:** The finding that Knowledge-Rich prompting significantly improves suspicious coincidence sensitivity, especially in GPT-4o, is well-supported by the experimental results and aligns with the mechanism that explicit context enables hypothesis prioritization.
- **Medium Confidence:** The claim that CoT prompting only works when parametric knowledge is sufficient (e.g., GPT-4o vs. Mistral-7B) is supported by the correlation between knowledge F1 and reasoning performance, but the underlying mechanism (CoT as retrieval vs. symbolic reasoning) is not definitively proven.
- **Low Confidence:** The assertion that zero-shot LLMs default to "AlwaysYes" due to a lack of pragmatic reasoning about data sampling is plausible but weakly supported by the corpus, which lacks direct evidence on sampling heuristics in LLMs.

## Next Checks
1. **Test Sampling Context Effects:** Re-run the Number Game with prompts that explicitly state the input set was "randomly sampled" from a program, to see if this reduces the "AlwaysYes" bias without requiring Knowledge-Rich features.
2. **Cross-Domain Generalization:** Apply the Knowledge-Rich prompting to a non-math domain (e.g., music genres or animal traits) to test if the mechanism generalizes beyond numbers and cities, and to identify if feature extraction remains reliable.
3. **Ablation of Knowledge Sources:** Compare model performance when Knowledge-Rich features are provided vs. when the model must retrieve them internally (e.g., via CoT alone), to isolate the effect of context vs. retrieval ability.