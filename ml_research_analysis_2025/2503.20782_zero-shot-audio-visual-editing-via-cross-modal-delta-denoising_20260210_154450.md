---
ver: rpa2
title: Zero-Shot Audio-Visual Editing via Cross-Modal Delta Denoising
arxiv_id: '2503.20782'
source_url: https://arxiv.org/abs/2503.20782
tags:
- video
- editing
- audio
- diffusion
- audio-video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces zero-shot audio-video editing, a task requiring
  synchronized transformation of audio and video content to match a textual prompt
  without additional model training. The authors curate AVED-Bench, a new benchmark
  dataset containing 110 videos spanning 11 categories from VGGSound, each paired
  with human-annotated source and target prompts.
---

# Zero-Shot Audio-Visual Editing via Cross-Modal Delta Denoising

## Quick Facts
- **arXiv ID:** 2503.20782
- **Source URL:** https://arxiv.org/abs/2503.20782
- **Reference count:** 40
- **Primary result:** AVED achieves AV-Align score of 0.42 vs. 0.33 baseline, outperforming state-of-the-art zero-shot audio-visual editing models.

## Executive Summary
This paper introduces zero-shot audio-video editing, a task requiring synchronized transformation of audio and video content to match a textual prompt without additional model training. The authors curate AVED-Bench, a new benchmark dataset containing 110 videos spanning 11 categories from VGGSound, each paired with human-annotated source and target prompts. They identify limitations in existing methods, which often suffer from synchronization and coherence issues between modalities. To address these challenges, the authors propose AVED, a cross-modal delta denoising framework that leverages audio-video interactions to achieve synchronized edits. AVED uses prompt-relevant patch identification and a cross-modal contrastive loss to ensure coherent and high-fidelity audio-video transformations. Experiments on AVED-Bench and the OAVE dataset demonstrate that AVED outperforms state-of-the-art zero-shot video and audio editing models, achieving significant improvements in audio-visual alignment (A V-Align score of 0.42 vs. 0.33 for the baseline) and overall coherence. Human evaluation also shows a strong preference for AVED over competing methods.

## Method Summary
AVED extends Delta Denoising Score (DDS) by adding a cross-modal contrastive loss ($L_{cmds}$) that jointly optimizes audio and video latents to maintain synchronization during editing. The framework identifies prompt-relevant patches using cross-attention maps from pretrained diffusion models, then applies a contrastive loss across audio-video pairs while preserving background content through branch-consistency sampling. The method uses Stable Diffusion 2.1 for video (reshaped into 2Ã—2 grids) and AudioLDM2-Large for audio, optimizing for 200 steps with an SGD optimizer (lr=1, decay 0.99/step).

## Key Results
- AVED achieves AV-Align score of 0.42, significantly outperforming baseline of 0.33
- Outperforms state-of-the-art zero-shot video and audio editing models on AVED-Bench and OAVE datasets
- Human evaluation shows strong preference for AVED over competing methods
- Maintains high structural similarity preservation (DINO score 0.956) while achieving prompt alignment

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Delta Denoising with Contrastive Alignment
- **Claim:** Synchronized audio-video editing requires joint gradient updates rather than independent sequential processing.
- **Mechanism:** AVED extends Delta Denoising Score (DDS) by adding a cross-modal contrastive loss ($L_{cmds}$). During the denoising step, the system computes gradients not just to align the latent with the text prompt (via DDS), but simultaneously to align audio and video latent features with each other. This penalizes edits where the visual change (e.g., a dog appearing) does not accompany the corresponding auditory change (e.g., barking).
- **Core assumption:** The latent spaces of the pretrained text-to-video (Stable Diffusion) and text-to-audio (AudioLDM) models share semantic geometries that can be bridged via contrastive learning without retraining the backbones.
- **Evidence anchors:**
  - [Section 3.2] Describes the final objective as $L_{cmds} + L_{DDS}$, integrating cross-modal supervision into the denoising process.
  - [Page 7, Table 4] Shows that the combined "AVED" method significantly outperforms "Audio-Only" and "Video-Only" delta denoising variants on AV-Align (0.42 vs 0.36/0.38).
- **Break condition:** If the feature encoders for audio and video produce orthogonal representations (no shared semantics), the contrastive loss will likely destabilize the optimization rather than align it.

### Mechanism 2: Prompt-Relevant Patch Identification
- **Claim:** High-fidelity editing requires distinguishing content to be changed from background content to be preserved.
- **Mechanism:** The framework extracts intermediate cross-attention maps (specifically Queries and Keys) from the diffusion U-Net. It computes similarity scores between visual/audio patches and the text prompt. Patches scoring above a threshold $\tau$ are classified as "relevant" (to be edited), while others are treated as "irrelevant" (to be preserved). This allows the delta denoising to selectively update specific regions (e.g., the lion) without distorting the entire scene.
- **Core assumption:** The cross-attention maps in pretrained diffusion models reliably localize the objects described in the text prompt within the spatial/spectral dimensions.
- **Evidence anchors:**
  - [Section 3.2, Eq. 3] Formally defines the relevance scores $S_a$ and $S_v$ derived from attention maps.
  - [Page 8, Table 5] Demonstrates that "Random A+V" sampling (assuming all patches correspond) degrades performance (DINO 0.902), while prompt-relevant selection restores it (DINO 0.956).
- **Break condition:** If the cross-attention mechanism attends broadly to irrelevant image regions (attention bleeding), the preservation mechanism fails, leading to artifacts or unintended edits in static backgrounds.

### Mechanism 3: Branch-Consistency Contrastive Pairing
- **Claim:** Effective zero-shot editing requires maintaining structural consistency between the source content and the edited output.
- **Mechanism:** The system constructs positive pairs not only across modalities (audio-to-video) but also across the source and target branches for irrelevant patches. By treating "irrelevant" patches (background) in the edited result as positive pairs with the source input, the contrastive loss explicitly forces the model to preserve background details.
- **Core assumption:** There exist "irrelevant" patches in the latent space that perfectly correspond between the noisy source and the denoising target.
- **Evidence anchors:**
  - [Page 4, Section 3.2] Details the sampling of $H^-_{a,src}$ and $H^-_{a,trg}$ as positive pairs to preserve unaltered content.
  - [Page 7, Table 2] Reports a high DINO score (0.956), indicating strong structural similarity preservation compared to baselines like TokenFlow (0.924).
- **Break condition:** If the noise inversion process shifts the latent distribution significantly, "corresponding" spatial indices in source and target may no longer represent the same semantic content, making the preservation constraint ineffective.

## Foundational Learning

- **Concept:** Delta Denoising Score (DDS)
  - **Why needed here:** AVED builds its core optimization loop on top of DDS. Understanding how DDS uses a "source branch" and "target branch" to isolate edit deltas (rather than generating from scratch) is required to interpret the loss functions.
  - **Quick check question:** Can you explain why DDS uses a reference branch with the source prompt instead of just standard classifier-free guidance with the target prompt?

- **Concept:** Cross-Attention Control in Diffusion
  - **Why needed here:** The method relies on manipulating cross-attention maps to identify "prompt-relevant patches."
  - **Quick check question:** In a standard diffusion U-Net (like Stable Diffusion), where do the Queries ($Q$) come from (the image or the text) and where do the Keys ($K$) come from?

- **Concept:** InfoNCE / Contrastive Loss
  - **Why needed here:** The synchronization mechanism is enforced via $L_c$, a contrastive objective.
  - **Quick check question:** What happens to the gradient signal if the temperature parameter $\alpha$ in the contrastive loss is set too high versus too low?

## Architecture Onboarding

- **Component map:** Stable Diffusion 2.1 (Video grids) -> AudioLDM2-Large (Audio) -> Attention Extractor -> Pair Sampler -> Optimizer
- **Critical path:**
  1. Source latent inversion
  2. Grid shuffling & noise addition
  3. Forward pass through frozen diffusion models (Video & Audio)
  4. Extraction of attention maps & computation of relevance scores
  5. Sampling of pairs and calculation of Contrastive Loss ($L_{cmds}$)
  6. Calculation of DDS Loss ($L_{DDS}$)
  7. Backprop to latent $z(\theta)$

- **Design tradeoffs:**
  - **Grid Size:** A $2 \times 2$ grid preserves fidelity and audio-video sync better, while $4 \times 4$ improves temporal consistency (DINO) but hurts object accuracy (Obj.) [Page 11, Table 6]
  - **Threshold $\tau$:** A threshold of 0.8 is optimal; lower values include too much noise in "relevant" patches, while higher values miss edit regions [Page 11, Figure 8]

- **Failure signatures:**
  - **Audio Artifacts:** "Purple squares" in the paper refer to temporal desynchronization or noise, typically caused by running audio editing independently of video (Sequential baseline) [Page 1, Figure 1]
  - **Background Bleed:** If the contrastive loss weight on irrelevant pairs is too low, the background may morph unexpectedly
  - **Over-smoothing:** If DDS weight is too high relative to the contrastive loss, the output may lose details (a known limitation of SDS/DDS approaches)

- **First 3 experiments:**
  1. **Attention Sanity Check:** Visualize the heatmap of $S_v$ (relevance scores) for a sample prompt (e.g., "dog"). Verify that high scores actually localize on the dog and not the grass.
  2. **Loss Ablation:** Run the optimization with *only* DDS (set $\lambda_{cmds}=0$) vs. *only* Contrastive. Confirm that DDS alone changes the content but desynchronizes audio/video, while Contrastive alone preserves structure but fails to follow the edit prompt.
  3. **Threshold Sweep:** Run a batch edit on 5 videos varying $\tau$ from 0.5 to 0.9. Plot AV-Align scores to confirm the paper's finding that 0.8 is the robust operational point.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the grid-based video representation effectively scale to videos longer than 10 seconds or higher resolutions without compromising temporal coherence?
- **Basis in paper:** [inferred] Appendix C specifies the method structures 10-second videos (4 fps) into a $2 \times 2$ grid. Table 6 shows that increasing grid size (e.g., to $4 \times 4$) slightly improves structural consistency (DINO) but significantly degrades alignment metrics (CLIP-T, AV-Align).
- **Why unresolved:** The current performance peaks at small grid sizes, suggesting the cross-modal delta denoising scheme struggles to maintain synchronization when frame density increases.
- **What evidence would resolve it:** Evaluation results on minute-long videos or higher frame rates where the grid structure is expanded, showing stable or improved AV-Align scores compared to the $2 \times 2$ baseline.

### Open Question 2
- **Question:** Can the iterative optimization process be accelerated to enable interactive editing speeds?
- **Basis in paper:** [inferred] Appendix C notes that the full optimization process takes approximately 20 minutes on an NVIDIA A6000 GPU for a single 10-second video clip.
- **Why unresolved:** The method relies on 200 steps of gradient descent via the Delta Denoising Score, which is computationally expensive and limits practical application in real-time creative workflows.
- **What evidence would resolve it:** A modified training-free or few-step approach that reduces generation time to under one minute while maintaining a DINO score above 0.95 and AV-Align score above 0.40.

### Open Question 3
- **Question:** Is it possible to replace fixed thresholds with an adaptive mechanism for identifying prompt-relevant patches?
- **Basis in paper:** [inferred] Figure 8 demonstrates that performance metrics (DINO, LPAPS, AV-Align) are highly sensitive to the threshold setting ($\tau$), with distinct peaks at 0.8. The method currently requires manually setting $\tau_v$ and $\tau_a$.
- **Why unresolved:** A fixed threshold may fail to generalize across different audio-visual categories or prompt complexities where the distribution of attention scores varies.
- **What evidence would resolve it:** An adaptive selection strategy that automatically determines relevant patches per sample and outperforms fixed-threshold baselines across all 11 categories in the AVED-Bench.

## Limitations
- Cross-modal alignment relies on untested assumption that audio and video latent spaces share common semantic geometry
- Method requires 200 optimization steps, taking approximately 20 minutes per video clip on NVIDIA A6000
- Performance sensitive to fixed threshold parameters ($\tau$) for patch relevance identification
- Limited evaluation to VGGSound and OAVE datasets, inheriting biases from pretrained models

## Confidence
- **High:** Experimental framework and reported results (AV-Align 0.42 vs 0.33 baseline) are well-documented and supported by human evaluation
- **Medium:** Cross-modal alignment mechanism depends on untested assumption about shared semantic geometry between audio and video latent spaces
- **Medium:** Patch identification relies on cross-attention maps in pretrained diffusion models, which may not reliably localize objects across all categories

## Next Checks
1. **Attention Mechanism Validation:** Visualize the heatmap of relevance scores ($S_v$) for a sample prompt (e.g., "dog") to verify that high scores localize on the intended objects and not on background regions.
2. **Loss Ablation Study:** Run the optimization with only DDS (setting $\lambda_{cmds}=0$) versus only the contrastive loss to confirm that DDS alone changes content but desynchronizes audio-video, while the contrastive loss alone preserves structure but fails to follow the edit prompt.
3. **Threshold Sensitivity Analysis:** Perform a batch edit on 5 videos, varying the relevance threshold $\tau$ from 0.5 to 0.9, and plot AV-Align scores to confirm the paper's finding that 0.8 is the robust operational point.