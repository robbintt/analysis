---
ver: rpa2
title: Fine-grained Spatiotemporal Grounding on Egocentric Videos
arxiv_id: '2508.00518'
source_url: https://arxiv.org/abs/2508.00518
tags:
- object
- video
- egocentric
- videos
- grounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces EgoMask, the first pixel-level benchmark for
  fine-grained spatiotemporal grounding in egocentric videos. The dataset is constructed
  using an automatic annotation pipeline that generates object masks and referring
  expressions across short-, medium-, and long-term videos, resulting in a large-scale
  training set (EgoMask-Train) and a high-quality evaluation benchmark (EgoMask).
---

# Fine-grained Spatiotemporal Grounding on Egocentric Videos

## Quick Facts
- **arXiv ID**: 2508.00518
- **Source URL**: https://arxiv.org/abs/2508.00518
- **Reference count**: 40
- **Key outcome**: EgoMask dataset and fine-tuning improves spatiotemporal grounding on egocentric videos by 41.30% relative

## Executive Summary
This work introduces EgoMask, the first pixel-level benchmark for fine-grained spatiotemporal grounding in egocentric videos. The dataset addresses unique challenges of egocentric video including shorter object durations, sparser trajectories, smaller object sizes, and larger positional shifts compared to exocentric videos. An automatic annotation pipeline generates object masks and referring expressions across short-, medium-, and long-term videos, resulting in a large-scale training set (EgoMask-Train) and a high-quality evaluation benchmark (EgoMask). Experiments demonstrate that state-of-the-art spatiotemporal grounding models perform poorly on EgoMask, but fine-tuning on EgoMask-Train yields significant improvements while maintaining performance on exocentric datasets.

## Method Summary
The EgoMask benchmark is constructed using an automatic annotation pipeline that combines EgoTracks bounding boxes with RefEgo referring expressions. GPT-4o generates referring expressions for objects in sampled video frames. The dataset includes 2,624 videos with 9,592 objects and 47,968 expressions for training (EgoMask-Train), plus 315 videos with 700 queries across different durations for evaluation (EgoMask benchmark). Models are fine-tuned on an 80/20 mix of EgoMask-Train and existing segmentation datasets (Ref-DAVIS, MeVis, Ref-YT-VOS). Two VideoLLM architectures are evaluated: Sa2VA-4B/26B and VideoLISA-3.8B, with specific training hyperparameters and hardware configurations detailed for each.

## Key Results
- State-of-the-art spatiotemporal grounding models perform poorly on EgoMask benchmark
- Fine-tuning on EgoMask-Train yields average relative improvement of 41.30% on EgoMask
- Models maintain performance on exocentric datasets (Ref-DAVIS, MeVis, ReasonVOS) after egocentric fine-tuning
- Performance drops to near 0% if target objects are absent from initial keyframes (5-frame window for Sa2VA, initial detection for Grounded-SAM2)

## Why This Works (Mechanism)
The approach works by providing egocentric-specific training data that addresses the unique characteristics of egocentric videos. The automatic annotation pipeline leverages existing tracking and referring expression datasets to create pixel-level masks and text queries. VideoLLMs learn to ground referring expressions in spatiotemporal contexts by processing masked video clips and generating mask tubes across frames. The 80/20 training mix balances egocentric adaptation with maintaining general segmentation capabilities.

## Foundational Learning

**Video Grounding**: Locating objects or regions in video given text queries across temporal dimensions. Needed because egocentric videos require understanding both spatial appearance and temporal continuity of objects as viewed from first-person perspective. Quick check: Model can track object identity across discontinuous appearances.

**Pixel-level Spatiotemporal Localization**: Generating precise masks for objects across multiple video frames rather than bounding boxes or single-frame segmentation. Needed because egocentric videos contain small, rapidly moving objects requiring fine-grained localization. Quick check: Generated masks align with object boundaries at 1 FPS resolution.

**Referring Expression Grounding**: Mapping natural language descriptions to specific visual entities. Needed because egocentric queries often describe objects in context of wearer's actions and intentions. Quick check: Model understands contextual cues like "the cup I'm about to pick up."

## Architecture Onboarding

**Component Map**: GPT-4o -> Annotation Pipeline -> EgoMask-Train/EgoMask -> VideoLLM (Sa2VA/VideoLISA) -> Mask Tube Generation

**Critical Path**: Video frames → SAM2 mask generation → GPT-4o expression generation → VideoLLM fine-tuning → Mask tube prediction

**Design Tradeoffs**: Automatic annotation enables large-scale dataset creation but may introduce systematic biases from SAM2-based ground truth generation. 80/20 training mix balances domain adaptation with generalization but optimal ratio is unexplored.

**Failure Signatures**: 
- T recall near 0% indicates initialization failure when target absent from initial keyframes
- Large gap between IoU all and IoU gold suggests background-dominated predictions
- Performance drops on long videos due to sparser trajectories and smaller objects

**First Experiments**:
1. Zero-shot evaluation of Sa2VA-4B and VideoLISA-3.8B on EgoMask benchmark using provided metrics
2. Fine-tuning VideoLISA-3.8B on EgoMask-Train with specified hyperparameters and 80/20 mix
3. Ablation study removing exocentric dataset portion from training mix

## Open Questions the Paper Calls Out

**Open Question 1**: How can video grounding architectures overcome the dependency on early frame initialization to track targets absent from the initial keyframes? The authors observe performance drops to nearly 0% if targets aren't in first five frames and suggest optimizing frame selection as future work.

**Open Question 2**: Does reliance on SAM2 for automatic ground truth generation introduce systematic bias against non-SAM2-based segmentation models? The evaluation tool and ground truth generator being the same may unfairly penalize models with different segmentation priors.

**Open Question 3**: To what extent do observed failures stem from training data scale versus inherent architectural limitations of current VideoLLMs? While improvements are seen, absolute performance remains low, leaving the bottleneck unclear between data quantity and model architecture.

## Limitations
- Automatic annotation pipeline may introduce biases from SAM2-based ground truth generation
- Evaluation metrics like IoU all can be misleading on longer videos as background frames dominate scores
- Comparative validation is limited as EgoMask has no established baselines from prior work
- Performance differences between egocentric and exocentric domains may reflect data characteristics rather than fundamental model capability differences

## Confidence

**High Confidence**: 
- Core observation that egocentric videos present unique challenges (shorter durations, sparser trajectories, smaller objects) is well-supported by data analysis
- Zero-shot transfer from exocentric datasets performs poorly on EgoMask is robust given controlled experimental setup

**Medium Confidence**: 
- 41.30% improvement from fine-tuning on EgoMask-Train is significant but depends on specific model architectures and training procedures
- Claim that egocentric-specific training data is necessary is reasonable but would benefit from more extensive ablation studies

**Medium Confidence**: 
- VideoLISA maintaining exocentric performance while improving on EgoMask suggests good generalization, though experimental design could be strengthened with more exocentric test sets

## Next Checks
1. Conduct manual annotation validation on a subset of EgoMask to verify quality and consistency of automatic annotation pipeline, particularly GPT-4o referring expressions
2. Test models trained on EgoMask-Train on additional exocentric datasets beyond three currently evaluated to understand generalization boundaries
3. Analyze model performance on long-duration videos (>60 seconds) separately to distinguish between better target localization versus better background handling