---
ver: rpa2
title: 'Safety-Efficacy Trade Off: Robustness against Data-Poisoning'
arxiv_id: '2602.00822'
source_url: https://arxiv.org/abs/2602.00822
tags:
- poison
- self
- poisoning
- input
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We establish a mathematical framework linking data-poisoning efficacy
  to the induced curvature of the loss landscape in input space, showing that clustered
  dirty-label poisons generate a rank-one spike in the input Hessian whose magnitude
  scales quadratically with attack efficacy. For nonlinear kernels we identify a near-clone
  regime where poisoning remains effective yet induces vanishing curvature, making
  attacks spectrally undetectable.
---

# Safety-Efficacy Trade Off: Robustness against Data-Poisoning

## Quick Facts
- arXiv ID: 2602.00822
- Source URL: https://arxiv.org/abs/2602.00822
- Reference count: 40
- We establish a mathematical framework linking data-poisoning efficacy to the induced curvature of the loss landscape in input space, showing that clustered dirty-label poisons generate a rank-one spike in the input Hessian whose magnitude scales quadratically with attack efficacy.

## Executive Summary
This paper presents a comprehensive mathematical and empirical analysis of data poisoning attacks through the lens of input-space curvature. The key insight is that poisoning efficacy and spectral detectability are fundamentally decoupled: clustered poisons create a quadratic relationship between attack success and Hessian eigenvalue magnitude, while near-clone attacks in nonlinear kernels can be both highly effective and spectrally invisible. The work introduces input-gradient regularization as a provable defense that contracts poison-aligned eigenmodes at the cost of reduced model capacity, establishing a fundamental safety-efficacy trade-off. Extensive experiments across linear models and deep CNNs confirm these theoretical predictions.

## Method Summary
The core methodology combines theoretical analysis of kernel ridge regression with empirical validation on deep neural networks. Poisoning is implemented using L-shaped triggers at controlled fractions, with input-gradient regularization added as κ||∇xL||² to the loss. The input-space Hessian is computed via Hessian-vector products using Lanczos iteration (max_iter=10) to extract top eigenvalues and eigenvectors. Attack success rate is measured as the fraction of poisoned samples misclassified to the target class, while spectral detectability is quantified through cosine overlap between the top Hessian eigenvector and the poison pattern. Experiments span MNIST, CIFAR-10, and CIFAR-100 using Pre-ResNet-110 architectures.

## Key Results
- Clustered dirty-label poisons induce a rank-one spike in the input Hessian whose magnitude scales quadratically with attack efficacy (Λ_GN ∝ (ASR)²)
- Near-clone attacks in nonlinear kernels achieve high efficacy while inducing vanishing input curvature, making them spectrally undetectable
- Input-gradient regularization provably contracts poison-aligned Fisher and Hessian eigenmodes under gradient flow, reducing poisoning efficacy
- Data augmentation combined with regularization provides synergistic defense beyond either method alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clustered dirty-label poisons induce a rank-one spike in the input Hessian whose magnitude scales quadratically with attack efficacy.
- Mechanism: When poisoned samples cluster at location ζ with target label y_t, the Gauss-Newton term ∇xf∇xf⊤ dominates the input Hessian at the trigger point x₀. The eigenvalue Λ_GN(x₀) = ∥∇xk(x₀,ζ)∥² S(m;λ)², where S is the scalar gain function of poison count m. Since efficacy ∆f grows linearly in m while curvature grows as m², the spike-efficacy law emerges: Λ_GN = R_k(x₀,ζ)(∆f)².
- Core assumption: Assumption 3.1—poison block satisfies K_PP ≈ k_ζ 11⊤ and cross-block effects are negligible at the trigger point.
- Evidence anchors:
  - [abstract] "clustered dirty label poisons induce a rank one spike in the input Hessian whose magnitude scales quadratically with attack efficacy"
  - [Section 3.2, Theorem 3.4] Derives the spike-efficacy law with explicit formulas
  - [corpus] Corpus papers address poisoning robustness but do not derive this quadratic curvature scaling; direct evidence is weak for this specific mechanism.
- Break condition: If poisoned samples do not cluster tightly or if cross-block kernel effects dominate at the trigger, the rank-one approximation fails.

### Mechanism 2
- Claim: For nonlinear kernels in the near-clone regime (r ≪ ℓ), poisoning remains effective while inducing vanishing input curvature, making attacks spectrally undetectable.
- Mechanism: For exponential kernels k(x,x') = exp(-∥x-x'∥²/2ℓ²), when the poison cluster ζ lies near the trigger x₀ (r = ∥x₀ - ζ∥ ≪ ℓ), the kernel value k₀ ≈ 1 but ∥∇xk∥² = O(r²/ℓ⁴) vanishes quadratically. Efficacy ∆f ≈ y_t S(m;λ) remains order-one, yet Λ_GN = S² r²/ℓ⁴ → 0. Neural collapse during training drives triggered features toward class means, naturally producing this near-clone geometry.
- Core assumption: The trigger point and poison cluster are close in feature space relative to the kernel length scale; neural collapse concentrates class features.
- Evidence anchors:
  - [abstract] "near clone regime in which poison efficacy remains order one while the induced input curvature vanishes, making the attack provably spectrally undetectable"
  - [Section 3.3, Corollary 3.7] Derives the near-clone regime mathematically
  - [Section 3.4, Figures 1-3] PCA visualization shows poisoned features collapse toward target class mean
  - [corpus] No corpus papers identify this near-clone detectability gap; mechanism is novel to this work.
- Break condition: If r/ℓ is not small (poisons are far from trigger in feature space), curvature becomes detectable.

### Mechanism 3
- Claim: Input-gradient regularisation provably contracts poison-aligned Fisher and Hessian eigenmodes under gradient flow, reducing poisoning at the cost of data-fitting capacity.
- Mechanism: Adding penalty (κ/2)E∥∇xL∥² modifies the loss to J(w) = E[L] + (κ/2)E∥∇xL∥². For KRR, this yields (K + λI + κG)α = y where G ⪰ 0, reducing effective degrees of freedom monotonically. Under gradient flow, Fisher eigenmodes with large variance decay as exp(-2καt). For exponential kernels, this acts as anisotropic high-pass filtering, increasing effective length scale ℓ²_eff = ℓ² + cκ, shrinking the near-clone regime.
- Core assumption: Gradient flow dynamics; positive semi-definite structure of the gradient Gram matrix G.
- Evidence anchors:
  - [abstract] "input gradient regularisation contracts poison aligned Fisher and Hessian eigenmodes... unavoidable safety efficacy trade off"
  - [Section 3.5, Theorem 3.9] Proves df(κ) strictly decreasing, residual increasing
  - [Section 3.5.2, Theorem 3.12] Derives exponential compression of large Fisher eigenmodes
  - [Section 5, Figures 6-7] Experiments show κ reduces ASR while lowering clean accuracy
  - [corpus] Related work on adversarial training (Geiping et al., 2021; Bal et al., 2025) uses AT empirically but does not prove Fisher/Hessian contraction.
- Break condition: If κ is too large, capacity loss dominates and model underfits; if κ is too small, defense is ineffective.

## Foundational Learning

- **Kernel Ridge Regression (KRR) and the Neural Tangent Kernel connection**
  - Why needed here: The paper uses KRR as an exact model of wide neural networks via the NTK correspondence. Understanding how kernel choice (linear vs. exponential) affects gradient behavior and length scales is essential.
  - Quick check question: For an RBF kernel with length scale ℓ, how does ∥∇xk(x,x')∥ scale with the distance ∥x-x'∥?

- **Input-Space Hessian vs. Parameter-Space Hessian**
  - Why needed here: Standard Hessian analysis examines curvature in weight space. This paper analyzes curvature in input space—where the poison pattern lives—revealing why spectral detection can fail.
  - Quick check question: What is the rank of the Gauss-Newton term ∇xf∇xf⊤, and what does its top eigenvalue represent?

- **Neural Collapse and Feature Concentration**
  - Why needed here: Neural collapse explains why triggered poisoned samples become near-clones of target class features in deep networks, creating the low-curvature regime. Without this, the theoretical near-clone mechanism lacks empirical grounding.
  - Quick check question: At terminal training with near-zero error, what structure do class feature means and classifier weights form?

## Architecture Onboarding

- **Component map:**
  - Poisoned dataset creation -> Gradient regularization training loop -> Input-Hessian spectral analysis -> Overlap computation -> ASR/accuracy tracking

- **Critical path:**
  1. Implement gradient regularization with `create_graph=True` for second-order derivatives
  2. Build HVP operator that computes H_x v without materializing full Hessian
  3. Run Lanczos (10 iterations sufficient per paper) to get top spectrum
  4. Track ASR, clean accuracy, and eigenvector-poison overlap across θ and κ

- **Design tradeoffs:**
  - Higher κ → better poisoning defense but lower clean accuracy (safety-efficacy tradeoff)
  - Data augmentation + regularization synergize; augmentation alone is insufficient
  - Longer training (450 vs 90 epochs) improves Pareto frontier but may recover poison efficacy at high κ

- **Failure signatures:**
  - Poison rotates among Hessian eigenvectors (check argmax_k overlap, not just top-1)
  - Near-clone regime: high ASR but low spectral overlap (expected for nonlinear kernels)
  - Gradient regularization becomes equivalent to ridge for linear kernels—no mode-dependent suppression

- **First 3 experiments:**
  1. **Baseline poisoning sweep:** Train ResNet-110 on CIFAR-10 with θ ∈ {0.001, 0.005, 0.01}, κ=0; measure ASR and top-eigenvector overlap to establish detectability lag
  2. **Gradient regularization ablation:** Fix θ=0.01, vary κ ∈ {0, 10³, 10⁴, 10⁵}; plot safety-efficacy frontier (ASR vs. clean accuracy)
  3. **Augmentation + regularization synergy:** Compare ASR/accuracy with augmentation enabled vs. disabled at κ=10⁴; verify joint suppression effect claimed in Section 5

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the deviation in intra-class variance between poisoned and clean features quantitatively impact the theoretical curvature-efficacy relationship?
- Basis in paper: [explicit] The authors state they "do not quantify to what extent the deviation impacts our theoretical analysis, which could be interesting future work" regarding the observed smaller intra-class variance of poisoned classes.
- Why unresolved: While the paper observes the variance difference (feature collapse), the theoretical analysis assumes "tight clusters" without formally incorporating the variance ratio into the error bounds of the spike-efficacy law.
- What evidence would resolve it: A theoretical derivation linking the variance ratio to correction terms in the rank-one spike law (Theorem 3.4), or empirical bounds on prediction error as a function of class variance.

### Open Question 2
- Question: Are there viable detection mechanisms for the "near-clone" regime where attacks are effective yet spectrally undetectable?
- Basis in paper: [inferred] The paper identifies a "twilight zone" where "poisoning remains effective yet induces vanishing curvature, making attacks spectrally undetectable," leaving this specific attack window unaddressed by defenses.
- Why unresolved: The paper proves the fundamental invisibility of these attacks to spectral methods based on input Hessian analysis, but does not explore non-spectral heuristics or alternative metrics for this regime.
- What evidence would resolve it: Identification of a non-spectral statistic (e.g., related to feature density or activation dynamics) that remains sensitive to poisons when the input Hessian eigenvalue spike vanishes.

### Open Question 3
- Question: Can the interpretation of gradient regularisation as an anisotropic high-pass filter be generalized to kernels beyond the exponential kernel?
- Basis in paper: [inferred] The authors show that "for exponential kernels, this defence admits a precise interpretation as an anisotropic high-pass filter," implying this precise geometric intuition may not hold for other kernel types used in theory or practice.
- Why unresolved: The frequency-domain analysis relies on the specific spectral decay of the exponential kernel; the behavior of the defense for polynomial or ReLU kernels remains qualitative.
- What evidence would resolve it: A derivation of the effective frequency response or length-scale modification induced by gradient regularisation for non-exponential kernels (e.g., polynomial or Matérn).

## Limitations

- The theoretical analysis relies on kernel approximations that may not hold for small networks or at early training stages
- The near-clone detectability gap requires specific geometric conditions (r ≪ ℓ) that may not always arise naturally
- The anisotropic high-pass interpretation of gradient regularization is specific to exponential kernels and may not generalize to learned deep network kernels

## Confidence

- Quadratic curvature-scaling law: **High** (well-supported by theory and experiments)
- Near-clone detectability gap: **Medium-High** (novel theoretical insight with strong derivation, Medium for empirical prevalence)
- Gradient regularization defense: **High** (theoretically sound and experimentally validated, Medium for high-pass interpretation in deep networks)

## Next Checks

1. **Cross-architecture generalization**: Test the near-clone detectability gap on Vision Transformers and MLPs to verify neural collapse is not architecture-specific.

2. **Dynamic poison geometry**: During training, track the evolution of poison-trigger distance r(t) in feature space to confirm the shift into near-clone regime over epochs.

3. **Defense robustness**: Evaluate gradient regularization against adaptive attacks that maximize curvature in input space rather than just attack efficacy.