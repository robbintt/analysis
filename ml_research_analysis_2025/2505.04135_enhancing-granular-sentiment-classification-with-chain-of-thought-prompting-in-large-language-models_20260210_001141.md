---
ver: rpa2
title: Enhancing Granular Sentiment Classification with Chain-of-Thought Prompting
  in Large Language Models
arxiv_id: '2505.04135'
source_url: https://arxiv.org/abs/2505.04135
tags:
- prompting
- sentiment
- reviews
- simple
- review
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that Chain-of-Thought (CoT) prompting significantly
  improves granular sentiment classification accuracy in app store reviews, raising
  performance from 84% to 93% compared to simple prompting. By guiding large language
  models through structured reasoning steps and incorporating keyword-based sentiment
  cues, CoT enables more accurate interpretation of nuanced, context-dependent user
  feedback.
---

# Enhancing Granular Sentiment Classification with Chain-of-Thought Prompting in Large Language Models

## Quick Facts
- arXiv ID: 2505.04135
- Source URL: https://arxiv.org/abs/2505.04135
- Authors: Vihaan Miriyala; Smrithi Bukkapatnam; Lavanya Prahallad
- Reference count: 12
- One-line primary result: CoT prompting improves granular sentiment classification accuracy from 84% to 93% on app store reviews

## Executive Summary
This study demonstrates that Chain-of-Thought (CoT) prompting significantly improves granular sentiment classification accuracy in app store reviews, raising performance from 84% to 93% compared to simple prompting. By guiding large language models through structured reasoning steps and incorporating keyword-based sentiment cues, CoT enables more accurate interpretation of nuanced, context-dependent user feedback. The approach resolves 80% of errors made by basic prompting, particularly in reviews with mixed or layered sentiments. Remaining challenges include handling vague expressions and very short reviews.

## Method Summary
The study employed a 5-class sentiment classification task on 2,000 Amazon app reviews, using GPT-4 via OpenAI API with temperature 0.3. Two prompting strategies were compared: a simple prompt requesting a 1-5 rating, and a Chain-of-Thought prompt with five structured reasoning steps (read review, identify expressions, evaluate tone, assign rating, justify) plus keyword hints. Human annotators established ground truth through majority voting. The CoT approach demonstrated significant accuracy improvements over simple prompting while providing interpretable justifications for predictions.

## Key Results
- CoT prompting improved classification accuracy from 84% to 93% on the dataset
- Approximately 80% of misclassified reviews by simple prompting were correctly predicted by CoT
- CoT errors primarily involved vague expressions, sarcasm, or very short reviews lacking context

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured reasoning decomposition improves sentiment classification accuracy on mixed-affect text.
- Mechanism: CoT prompting forces the model to process reviews through discrete stages rather than directly mapping text to a label, reducing premature commitment to a polarity class.
- Core assumption: The model possesses sufficient latent reasoning capacity to follow multi-step instructions reliably at temperature 0.3.
- Evidence anchors:
  - [abstract] "CoT prompting improved classification accuracy from 84% to 93% highlighting the benefit of explicit reasoning."
  - [section 5.4] "approximately 80% of the misclassified reviews (about 64 out of 80) were correctly predicted by the CoT method. These cases often involved conflicting or layered sentiments."
- Break condition: Reviews with extremely short or vague content may not provide sufficient signal for reasoning steps to operate on.

### Mechanism 2
- Claim: Keyword-aware prompting guides attention toward sentiment-bearing expressions and reduces reliance on surface-level word counts.
- Mechanism: The CoT prompt explicitly lists positive/negative/neutral keyword examples and instructs the model to identify specific expressions before evaluating dominant tone.
- Core assumption: Keyword hints derived from manual annotation generalize to unseen reviews in the same domain.
- Evidence anchors:
  - [section 4.2] "We incorporated keyword hints into the CoT prompt design to guide the model in identifying sentiment-bearing expressions."
  - [section 5.2] Simple prompting "often defaulted to the dominant polarity of isolated words, disregarding the reviewer's overall experience."
- Break condition: If keyword distributions shift significantly across domains, hint effectiveness may degrade without re-annotation.

### Mechanism 3
- Claim: Requiring explicit justification creates a verification loop that catches misclassifications from skipped reasoning.
- Mechanism: Step 5 mandates a brief explanation for the rating, causing the model to self-correct if the justification contradicts the assigned label.
- Core assumption: The model's justification generation correlates with its classification decision and is not post-hoc rationalization.
- Evidence anchors:
  - [section 4.2] Example output shows structured "Main Points Identified" and "Rating Justification" before final rating.
  - [section 5.3] CoT errors included cases where "the CoT approach occasionally overemphasized one aspect."
- Break condition: Justification quality depends on model scale; smaller models may produce plausible-sounding but unfaithful explanations.

## Foundational Learning

- Concept: **Chain-of-Thought Prompting**
  - Why needed here: The entire method depends on understanding how to structure multi-step prompts that elicit intermediate reasoning.
  - Quick check question: Can you explain why "Let's think step by step" often improves performance on reasoning tasks, and when it might not help?

- Concept: **Granular vs. Polarity Sentiment Scales**
  - Why needed here: The paper argues that binary/ternary sentiment is insufficient for app reviews; understanding why 5-class granularity matters is essential for evaluating the results.
  - Quick check question: What information is lost when mapping "The app works fine most of the time, but crashes occasionally" to positive/negative/neutral?

- Concept: **Human Annotation Agreement and Majority Voting**
  - Why needed here: Ground truth was established via 3 annotators with majority vote; understanding inter-annotator variability contextualizes the 93% accuracy ceiling.
  - Quick check question: If two of three annotators disagree on a review's sentiment, what does that imply about the inherent difficulty of the classification task?

## Architecture Onboarding

- Component map: Data Layer -> Prompt Layer -> Model Layer -> Evaluation Layer
- Critical path:
  1. Load and preprocess review text (remove redundancy)
  2. Construct prompt (simple or CoT with keyword hints)
  3. Call GPT-4 API with temperature 0.3
  4. Parse model output (rating + justification for CoT)
  5. Compare prediction to human label
  6. Aggregate accuracy and conduct error categorization

- Design tradeoffs:
  - Accuracy vs. Cost/Latency: CoT prompts are longer and generate more tokens, increasing API cost and response time
  - Explainability vs. Complexity: CoT provides justifications but requires more prompt engineering and maintenance
  - Keyword Hints vs. Domain Specificity: Hints improve performance on the target domain but may require re-derivation for new app categories

- Failure signatures:
  - Sarcasm/Irony: Both methods struggle; paper notes persistent errors in "vague expressions or sarcasm"
  - Very short reviews: "It's okay" lacks sufficient content for reasoning steps
  - Competing sentiments with unclear weighting: CoT may overweight one aspect
  - Domain shift: Keyword hints derived from app reviews may not transfer to other review types

- First 3 experiments:
  1. Baseline replication: Run simple prompt on a held-out subset to verify 84% accuracy; document error categories.
  2. Ablation on keyword hints: Test CoT prompt with and without the keyword hint section to isolate contribution of explicit lexical guidance.
  3. Short-review analysis: Filter reviews under 20 words; compare simple vs. CoT accuracy on this subset to quantify reasoning breakdown conditions.

## Open Questions the Paper Calls Out
- Can Chain-of-Thought (CoT) prompting effectiveness be extended to multilingual and multimodal sentiment classification tasks?
- Does integrating CoT prompting with model fine-tuning offer significant gains over prompt engineering alone?
- How can CoT prompt structures be optimized to reduce the 7% error rate involving sarcasm and vague expressions?

## Limitations
- Study relies on a single, internally constructed dataset of 2,000 Amazon app reviews without public availability
- Mechanism by which keyword hints improve reasoning (vs. simply biasing output) is not directly tested through ablation
- Temperature setting of 0.3 may mask model uncertainty that could be valuable for real-world deployment

## Confidence
- **High confidence**: CoT prompting improves accuracy over simple prompting for this dataset (84% â†’ 93%)
- **Medium confidence**: Structured reasoning specifically resolves mixed-sentiment classification errors
- **Medium confidence**: Keyword hints contribute to performance gains, though relative contribution untested
- **Low confidence**: Generalization to other domains without keyword re-derivation

## Next Checks
1. **Ablation study**: Test CoT prompting with and without keyword hints on the same dataset to quantify their individual contribution to the 9% accuracy gain.
2. **Domain transfer test**: Apply the method to a different review corpus using the same keyword hints, measuring performance drop to assess domain dependence.
3. **Uncertainty quantification**: Run experiments at multiple temperature settings (0.1, 0.5, 1.0) to evaluate whether the 0.3 setting suppresses meaningful uncertainty signals in ambiguous reviews.