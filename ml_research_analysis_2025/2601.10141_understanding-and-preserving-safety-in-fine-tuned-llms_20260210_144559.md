---
ver: rpa2
title: Understanding and Preserving Safety in Fine-Tuned LLMs
arxiv_id: '2601.10141'
source_url: https://arxiv.org/abs/2601.10141
tags:
- safety
- fine-tuning
- utility
- gradient
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper identifies a fundamental challenge in fine-tuning large
  language models (LLMs): even benign fine-tuning tasks can degrade pre-trained safety
  alignment, leading to increased vulnerability to jailbreak attacks. Existing fine-tuning-stage
  defenses suffer from a safety-utility trade-off instability and fail to maintain
  safety under deep fine-tuning.'
---

# Understanding and Preserving Safety in Fine-Tuned LLMs

## Quick Facts
- **arXiv ID**: 2601.10141
- **Source URL**: https://arxiv.org/abs/2601.10141
- **Reference count**: 40
- **Primary result**: SPF reduces ASR from 0.955 to 0.019 on harmful data while maintaining task accuracy at 0.847

## Executive Summary
The paper identifies that fine-tuning large language models, even on benign tasks, can severely degrade pre-trained safety alignment, making models vulnerable to jailbreak attacks. Existing fine-tuning-stage defenses suffer from safety-utility trade-offs and fail under deep fine-tuning. The authors systematically analyze gradient structures and discover that safety gradients occupy a compact, low-rank subspace, while utility gradients span a broader high-dimensional space with often negative correlation. Based on these insights, they propose Safety-Preserving Fine-tuning (SPF), which projects utility gradients orthogonal to the safety subspace when conflicts are detected. SPF achieves near-perfect safety recovery without sacrificing utility and demonstrates robust resistance to adaptive jailbreak attacks across multiple model architectures.

## Method Summary
SPF modifies the fine-tuning process by computing both utility and safety gradients per step. The safety gradient is estimated from a single representative harmful prompt, and its dominant subspace is extracted via truncated SVD. If the inner product between safety and utility gradients is negative (indicating directional conflict), the utility gradient is projected orthogonal to the safety subspace before parameter updates. This projection removes conflicting components while preserving most task-relevant information. The method is implemented as a hook in the training loop, operating on parameter blocks with block-wise SVD for efficiency.

## Key Results
- SPF reduces ASR from 0.955 to 0.019 on harmful data while maintaining math task accuracy at 0.847
- Safety gradients occupy a compact low-rank subspace (CER reaches 0.85 at k=10) versus broader utility gradients
- SPF demonstrates robustness to deep fine-tuning (up to 20 epochs) and adaptive jailbreak attacks
- Single-sample safety gradient estimation achieves subspace overlap >0.8 with batch estimation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Safety gradients occupy a compact, low-rank subspace, whereas utility gradients span a broader high-dimensional space.
- **Mechanism**: Safety-relevant information is concentrated in a small number of orthogonal gradient directions (low effective rank). Utility optimization distributes across many dimensions, increasing interference probability with safety directions. SPF removes conflicting components by projecting utility updates onto the orthogonal complement of the estimated safety subspace.
- **Core assumption**: The safety-critical gradient manifold is intrinsically low-dimensional and remains approximately stable across samples and steps.
- **Evidence anchors**: Figure 3 shows layer-wise singular value spectra; Table 4 reports cumulative explained ratio (CER) of top-k singular values: safety CER reaches 0.85 at k=10 vs. 0.45 (math) and 0.35 (code) for Llama.

### Mechanism 2
- **Claim**: Safety and utility gradients are often negatively correlated, causing directional conflicts during fine-tuning.
- **Mechanism**: When cosine similarity between safety and utility gradients is negative, standard gradient descent on utility moves parameters away from the safety-aligned region. SPF detects this conflict via inner product sign and projects away the conflicting component, preventing the "push" out of the safety basin.
- **Core assumption**: Negative correlation is systematic enough to be detected per-step; positive or neutral samples exist but are less safety-critical.
- **Evidence anchors**: Figure 5 shows step-wise cosine similarity between safety and utility gradients across Harm/Math/Code tasks; strong negative correlation with harmful gradients, mixed for benign tasks.

### Mechanism 3
- **Claim**: The dominant safety direction can be efficiently estimated from a single sample.
- **Mechanism**: Because safety gradients are low-rank, the principal directions are shared across diverse harmful prompts. A single generic refusal example yields a gradient whose top-k singular vectors overlap (φ > 0.8) with those from full-batch estimation, enabling runtime-efficient safety subspace tracking.
- **Core assumption**: The safety subspace is sufficiently invariant across harmful prompts that one representative sample captures the dominant directions.
- **Evidence anchors**: Figure 4 reports subspace similarity φ(g_s_single, g_s_batch) > 0.8 across layers; ablation (Table 8, Figure 8) shows generic single-sample achieves similar ASR reduction to batch-averaged.

## Foundational Learning

- **Concept**: Singular Value Decomposition (SVD) for gradient structure analysis
  - **Why needed here**: SPF uses truncated SVD to extract the low-rank safety subspace; understanding spectral decay explains why low k works.
  - **Quick check question**: Given a gradient matrix G, what does a rapid decay in singular values imply about the effective dimensionality of the update space?

- **Concept**: Orthogonal projection onto subspace complements
  - **Why needed here**: SPF projects utility gradients onto the orthogonal complement of the safety subspace (Equation 7).
  - **Quick check question**: If P = I - UU^T where U is orthonormal, what does Px compute, and how does it differ from projecting onto U itself?

- **Concept**: Gradient conflict detection via inner product sign
  - **Why needed here**: SPF conditionally applies projection only when ⟨g_s, g_u⟩ < 0.
  - **Quick check question**: Why use the sign rather than magnitude of the inner product as the conflict criterion?

## Architecture Onboarding

- **Component map**: Utility gradient computation -> Safety gradient computation -> Conflict detection -> Subspace extraction -> Orthogonal projection -> Parameter update

- **Critical path**: Lines 5-9 in Algorithm 1; the conflict check and conditional projection determine whether SPF acts or passes through unchanged.

- **Design tradeoffs**:
  - **k (safety subspace dimension)**: Higher k → better safety but potential utility loss; Figure 9 shows k∈[10,20] is optimal.
  - **Safety sample design**: Generic vs. category-specific; ablation shows generic works well but category-specific may generalize less.
  - **Block-wise vs. global projection**: Block-wise preserves more gradient structure but complicates implementation.

- **Failure signatures**:
  - Safety degradation under very deep fine-tuning (≥20 epochs) if k is too small or safety gradient estimation drifts.
  - Utility collapse if k is too large (over-constraining the update).
  - Adaptive jailbreaks (e.g., DRA, AutoDAN-Turbo) still achieve non-zero ASR (Table 7), indicating residual vulnerability.

- **First 3 experiments**:
  1. Reproduce the spectral analysis (Figure 3, Table 4) on your target model to confirm low-rank safety structure before deploying SPF.
  2. Validate single-sample vs. batch safety gradient overlap (Figure 4) with your own safety data to ensure estimation reliability.
  3. Run a controlled fine-tuning experiment (e.g., Math+Harm for 5 epochs) comparing ASR/utility trade-off curves for k∈{5, 10, 20, 50} to calibrate the safety-utility-efficiency frontier for your use case.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the low-rank structure of safety gradients (Insight I) persist as model scale increases significantly (e.g., to 70B+ parameters), or does the safety subspace dimension k increase proportionally with model width?
- **Basis in paper**: [inferred] The paper validates SPF on 7B/8B models (Llama, Mistral, Qwen) with a fixed k=10, but does not test if the cumulative explained ratio (CER) for safety gradients changes with model dimensionality.
- **Why unresolved**: It is unclear if safety alignment resides in a fixed-size compact subspace across all scales or if the dimensionality of the "safety manifold" scales with the parameter count.
- **What evidence would resolve it**: SVD analysis of safety gradients on 70B+ models to determine if the spectral decay rate and optimal k remain consistent with smaller models.

### Open Question 2
- **Question**: Can an adversary construct a specific utility dataset that creates a higher-order optimization path to bypass the SPF projection mechanism?
- **Basis in paper**: [inferred] The paper demonstrates robustness to standard adaptive jailbreaks, but the theoretical bounds rely on first-order approximations (Lemma 2).
- **Why unresolved**: While SPF projects conflicting gradients, it is theoretically possible for a sequence of projected updates to accumulate curvature effects that eventually shift the safety subspace itself, which the current analysis does not account for.
- **What evidence would resolve it**: A white-box attack specifically optimizing utility data to maximize safety loss drift while strictly adhering to SPF constraints over a long time horizon.

### Open Question 3
- **Question**: Does the theoretical "random-subspace" assumption used in the convergence proof accurately reflect the geometry of real-world RLHF-aligned models?
- **Basis in paper**: [explicit] Appendix A.1 states: "We adopt a standard random-subspace approximation... This assumption serves as a geometric baseline and captures the absence of systematic alignment."
- **Why unresolved**: The convergence proof relies on the assumption that the safety subspace is isotropically distributed relative to utility gradients, which may not hold if fine-tuning data is semantically related to safety concepts.
- **What evidence would resolve it**: Empirical measurement of the angle distribution between safety and utility subspaces across diverse fine-tuning tasks to verify isotropy.

## Limitations

- Safety gradient estimation reliability depends on a single generic safety sample, which may not capture safety directions across diverse harmful categories if they are more fragmented than shown.
- SPF remains vulnerable to advanced adaptive jailbreak attacks, achieving residual ASR of 0.19-0.25 despite significant improvement.
- Deep fine-tuning scalability is unproven beyond 20 epochs, with potential degradation if safety gradients drift or become more heterogeneous.

## Confidence

- **High confidence**: The core empirical findings about safety-utility trade-off instability (ASR rising from 0.04 to 0.95 under standard fine-tuning) and SPF's effectiveness in recovering safety without utility loss are well-supported by controlled experiments with clear baselines.
- **Medium confidence**: The mechanistic claims about low-rank safety gradients and negative correlation are supported by spectral analysis and cosine similarity metrics, but the causal link to SPF's effectiveness relies on the assumption that these properties remain stable during fine-tuning.
- **Low confidence**: The single-sample safety gradient estimation's generalizability across diverse harmful categories is asserted but not rigorously tested across multiple prompt families or safety domains.

## Next Checks

1. **Cross-category safety subspace consistency** - Test whether single-sample safety gradient estimation from one harmful category (e.g., weapons) generalizes to others (e.g., illegal activities) by measuring subspace overlap φ across categories and validating ASR recovery per category.

2. **Multi-step adaptive attack evaluation** - Beyond the reported advanced jailbreak methods, design a progressive attack protocol where an adversary observes SPF's responses and iteratively crafts prompts to maximize ASR, measuring whether the residual 0.19-0.25 ASR can be systematically reduced.

3. **Deep fine-tuning stability analysis** - Extend SPF evaluation to 50+ epochs on challenging utility tasks, monitoring safety gradient rank stability, subspace drift rates, and ASR/utility trade-off evolution to identify failure thresholds and potential mitigation strategies.