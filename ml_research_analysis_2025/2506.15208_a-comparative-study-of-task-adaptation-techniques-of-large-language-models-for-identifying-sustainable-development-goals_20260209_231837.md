---
ver: rpa2
title: A Comparative Study of Task Adaptation Techniques of Large Language Models
  for Identifying Sustainable Development Goals
arxiv_id: '2506.15208'
source_url: https://arxiv.org/abs/2506.15208
tags:
- text
- https
- classification
- language
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluated 18 large language models for classifying
  text according to the United Nations Sustainable Development Goals (SDGs), using
  three task adaptation techniques: Zero-Shot Learning (ZSL), Few-Shot Learning (FSL),
  and Fine-Tuning (FT). Models ranged from small (220 million parameters) to very
  large (1.5 trillion parameters), including proprietary and open-source options.'
---

# A Comparative Study of Task Adaptation Techniques of Large Language Models for Identifying Sustainable Development Goals

## Quick Facts
- **arXiv ID:** 2506.15208
- **Source URL:** https://arxiv.org/abs/2506.15208
- **Reference count:** 40
- **Primary result:** Fine-tuned smaller models (e.g., LLaMa-2 13B) achieved F1 scores of 92.4%, outperforming or matching much larger proprietary models.

## Executive Summary
This study benchmarks 18 large language models (LLMs) for classifying text into the 17 United Nations Sustainable Development Goals (SDGs). Three task adaptation techniques are compared: Zero-Shot Learning (ZSL), Few-Shot Learning (FSL) with semantic similarity sampling, and Fine-Tuning (FT). The results demonstrate that fine-tuned smaller models, such as LLaMa-2 13B, achieve top performance (F1 92.4%), surpassing or matching much larger models like GPT-4. FSL with semantic similarity sampling significantly outperforms random sampling. Quantized versions of models also deliver competitive results, offering a resource-efficient alternative for SDG text classification.

## Method Summary
The study evaluates 18 LLMs ranging from 220M to 1.5T parameters, including open-source and proprietary models. Three adaptation techniques are tested: ZSL (no examples), FSL (3 examples per class, selected randomly or via semantic similarity), and FT (on curated datasets). Performance is measured using F1 score on a held-out test set. The experiments assess how model size, architecture, and adaptation method affect classification accuracy for SDG text.

## Key Results
- Fine-tuned LLaMa-2 13B achieved the highest F1 score (92.4%), outperforming GPT-4 (91.4%) and other large models.
- FSL with semantic similarity sampling significantly outperformed random sampling, improving F1 scores by leveraging relevant examples.
- Quantized models (e.g., LLaMa-2 13B 4-bit) achieved competitive results (85.7% F1), demonstrating resource efficiency.
- FT improved performance by an average of 25.7 F1 points over ZSL across all models.

## Why This Works (Mechanism)

### Mechanism 1: Fine-Tuning for Specialized Domain Adaptation
Fine-tuning adapts a pre-trained model to a specific domain (SDGs) by adjusting its weights on a labeled dataset, improving sensitivity to domain-specific terminology and patterns. This requires a high-quality, representative training dataset and general pre-trained knowledge. Performance gains may diminish with noisy or unrepresentative data.

### Mechanism 2: In-Context Learning via Semantic Similarity
In FSL, providing semantically similar examples in the prompt guides the model's attention to relevant features, leveraging its pre-trained semantic representation. This assumes the model can understand relationships between examples and inputs, and is bounded by context window and analogical reasoning ability.

### Mechanism 3: Performance-Resource Efficiency Trade-off
Smaller, fine-tuned models (7B-13B) and quantized models achieve performance comparable to larger models by specializing efficiently. This trade-off holds for constrained tasks like single-label classification but may not generalize to tasks requiring broad knowledge or complex reasoning.

## Foundational Learning

- **Concept: Transfer Learning & Fine-Tuning**
  - **Why needed here:** Explains how pre-trained models adapt to specific tasks with small datasets, enabling smaller models to compete with GPT-4.
  - **Quick check question:** Can you explain why we don't need to train a model from scratch for every new NLP task, and what the primary risk of fine-tuning on a very small, biased dataset is?

- **Concept: In-Context Learning (Zero-Shot & Few-Shot)**
  - **Why needed here:** Essential for interpreting ZSL/FSL results and understanding why FT outperformed them.
  - **Quick check question:** If you provide a model with three labeled examples in a prompt before asking it to classify a fourth text, what is the fundamental difference in how it "learns" compared to a model that is fine-tuned on thousands of examples?

- **Concept: Model Quantization**
  - **Why needed here:** Explains how quantization reduces memory and compute requirements with minimal accuracy loss, enabling resource-efficient deployment.
  - **Quick check question:** How does converting a model's weights from 32-bit floating-point numbers to 4-bit integers primarily affect the hardware resources required to run it, and what is the potential downside?

## Architecture Onboarding

- **Component map:** [Input Text] -> [Preprocessing & Prompt Engineering] -> [LLM Core (ZSL/FSL)] or [Fine-Tuning Pipeline (FT)] -> [Inference Engine] -> [Post-Processing & Classification Label]

- **Critical path:** Fine-Tuning (FT): [Input Text] -> [Fine-Tuning Pipeline] -> [Fine-Tuned Model] -> [Inference Engine] -> [Label]. This path yielded the top F1 score (92.4%).

- **Design tradeoffs:**
  - **Performance vs. Resource Cost:** Large models (e.g., GPT-4) offer excellent ZSL performance but high cost; fine-tuned smaller models require upfront training but lower per-inference cost.
  - **Adaptability vs. Simplicity:** FT is complex (dataset prep, training) but more performant; ZSL/FSL are simpler but less robust and prompt-sensitive.
  - **Accuracy vs. Latency:** Larger or non-quantized models may be more accurate but slower; quantized models are faster with a small accuracy drop.

- **Failure signatures:**
  - **Overfitting in FT:** High validation but low test performance, indicating learning of noise or specific patterns.
  - **Prompt Brittleness in FSL:** Performance varies with prompt rephrasing or example selection, indicating reliance on superficial cues.
  - **Context Truncation:** Input text truncated due to small context window, leading to lost information and failed classification.

- **First 3 experiments:**
  1. **Establish a ZSL Baseline:** Run 2-3 models (e.g., GPT-3.5-turbo, LLaMa-2-7B) on test set with simple zero-shot prompt; record F1 scores.
  2. **Ablate Example Selection in FSL:** Compare random vs. semantic similarity vs. diversity-constrained example selection for a single model (e.g., LLaMa-2-7B) with 3 examples; validate FSL optimization.
  3. **Fine-Tune a Small Model:** Fine-tune Flan-T5-base on training dataset; compare test F1 against best FSL result to assess cost-benefit of FT.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the superior performance of fine-tuned smaller models over proprietary models (GPT-3.5/4) generalize to text classification tasks in unrelated domains?
- **Basis in paper:** Section VI states future research will evaluate performance on a broader range of classification tasks in different domains.
- **Why unresolved:** The study is restricted to SDG domain; it is unclear if efficiency of smaller models is specific to SDG semantic structure.
- **What evidence would resolve it:** Comparative analysis using same models and techniques on standardized benchmarks from distinct fields (e.g., legal, biomedical text classification).

### Open Question 2
- **Question:** Why did the newer LLaMa-3 8B model underperform the older LLaMa-2 13B and Mistral 7B models in Fine-Tuning experiments?
- **Basis in paper:** Section V notes LLaMa-3 8B (86.7%) was outperformed by Mistral 7B (88.1%) and BERT (87.2%), unexpected given recency and architecture improvements.
- **Why unresolved:** Paper does not investigate whether drop is due to training data, parameter count, or hyperparameters.
- **What evidence would resolve it:** Ablation study varying epoch counts and learning rates for LLaMa-3, or comparison against LLaMa-3 13B to isolate parameter count.

### Open Question 3
- **Question:** Can a hybrid prompt sampling strategy improve Few-Shot Learning performance beyond simple semantic similarity?
- **Basis in paper:** Section V shows "SEM. SIMILARITY" outperformed "SS WITH DC" (diversity constraint), but suggests inferior performance of latter was due to excluding semantically close texts.
- **Why unresolved:** Unclear if strategy exists that balances semantic relevance with class diversity to prevent overfitting to specific examples.
- **What evidence would resolve it:** Experiments using weighted sampling algorithm prioritizing high semantic similarity while enforcing minimum distance threshold between examples.

## Limitations
- Results are based on a single, specialized SDG text classification dataset; generalizability to other domains is uncertain.
- Performance gap between models may narrow or widen on different datasets or tasks requiring broader reasoning or generation.
- While quantized models are "competitive," the exact performance degradation (e.g., 85.7% vs. 92.4% F1) may be unacceptable in high-stakes applications.

## Confidence
- **High Confidence:** Fine-tuning provides substantial performance boost over zero-shot learning (average F1 increase 25.7 points), well-supported by results and established in transfer learning.
- **Medium Confidence:** Semantic similarity sampling in FSL outperforms random sampling, but mechanism (deepening understanding) is plausible but not definitively proven; relative performance of specific models is task-specific.
- **Medium Confidence:** Quantized models offer good performance-resource efficiency trade-off, but exact threshold for "competitive" performance is context-dependent.

## Next Checks
1. **Dataset Diversity Audit:** Re-run benchmark (ZSL, FSL, FT) on held-out test set from completely different source (e.g., social media vs. formal reports); test generalizability and reveal overfitting.
2. **Cost-Performance Analysis:** Calculate total cost of ownership for top-performing fine-tuned model (LLaMa-2 13B), including training, hosting, and per-inference cost; compare to API cost of GPT-4 over a year to validate "cost-effective" claim.
3. **Error Analysis by Class:** Perform detailed error analysis to identify which SDG classes are most frequently misclassified by different adaptation techniques; reveal if performance differences are uniform or if certain goals (e.g., abstract concepts) are consistently harder for smaller, fine-tuned models.