---
ver: rpa2
title: Graph-Enhanced Retrieval-Augmented Question Answering for E-Commerce Customer
  Support
arxiv_id: '2509.14267'
source_url: https://arxiv.org/abs/2509.14267
tags:
- support
- knowledge
- retrieval
- customer
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel knowledge graph-enhanced retrieval-augmented
  generation framework for e-commerce customer support. The system combines structured
  knowledge graphs with traditional document retrieval to produce more accurate and
  factually grounded responses to customer queries.
---

# Graph-Enhanced Retrieval-Augmented Question Answering for E-Commerce Customer Support

## Quick Facts
- arXiv ID: 2509.14267
- Source URL: https://arxiv.org/abs/2509.14267
- Authors: Piyushkumar Patel
- Reference count: 31
- Key outcome: 23% improvement in factual accuracy over document-only RAG approaches for e-commerce customer support

## Executive Summary
This paper presents a novel knowledge graph-enhanced retrieval-augmented generation framework for e-commerce customer support that combines structured knowledge graphs with traditional document retrieval to produce more accurate and factually grounded responses. The system processes both KG subgraphs and retrieved documents in parallel, then uses a large language model to generate coherent responses that incorporate structured facts while maintaining natural language flow. Experimental evaluation demonstrates significant improvements in factual accuracy while maintaining sub-second response times suitable for real-time customer support applications.

## Method Summary
The framework combines structured knowledge graphs with document retrieval in a hybrid RAG architecture. During the offline phase, product catalogs and support tickets are processed to extract entities and relations, building a domain-specific KG in Neo4j. Online, customer queries undergo entity extraction using spaCy and BERT-based models, then parallel retrieval fetches relevant KG subgraphs (via Cypher queries up to depth=2) and documents (via hybrid BM25+dense search). The LLM synthesizes answers by linearizing KG facts into structured text and combining them with document context, generating responses that maintain factual accuracy while preserving natural language flow.

## Key Results
- 23% improvement in factual accuracy compared to document-only RAG approaches
- 89% user satisfaction rate in human evaluations
- Sub-second response times (1,340ms average) suitable for real-time deployment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parallel retrieval from structured knowledge graphs and unstructured documents produces more factually grounded responses than either source alone.
- Mechanism: Entity extraction from the query drives two simultaneous retrieval paths—graph traversal retrieves structured subgraphs containing relational facts (e.g., "WidgetX compatible-with PhoneY"), while hybrid BM25+dense search retrieves contextual documents. Both streams feed the LLM, with KG facts constraining hallucination and documents providing natural language context.
- Core assumption: Query entities are extractable with sufficient accuracy, and the KG contains relevant product/issue relationships.
- Evidence anchors:
  - [abstract] "combines structured subgraphs from a domain-specific KG with text documents retrieved from support archives, producing more coherent and grounded responses"
  - [Section 3.2] "Using entities E, we retrieve relevant subgraphs S through graph traversal... Parallel text retrieval uses hybrid search combining BM25 and dense retrieval"
  - [corpus] DO-RAG paper confirms similar architecture benefits domain-specific QA with heterogeneous data integration
- Break condition: If entity extraction fails or KG coverage is sparse for query domain, subgraph retrieval returns empty/noisy results, degrading synthesis quality.

### Mechanism 2
- Claim: Linearizing KG subgraphs into textual fact statements allows LLMs to reason over structured knowledge without specialized graph neural networks.
- Mechanism: Algorithm 1 converts retrieved subgraphs into linearized text (e.g., "AcmeWidgetPro compatible_with Acme-Phone 12"). This flattened representation is concatenated with document context and passed to the LLM. The structured format acts as a soft constraint—the LLM generates fluent responses while preserving factual accuracy.
- Core assumption: LLMs can correctly interpret linearized triples and maintain factual consistency during generation.
- Evidence anchors:
  - [Section 3.3] "The algorithm linearizes subgraphs into structured fact statements, combines them with retrieved document context"
  - [Section 3.3] "The LLM cannot readily alter structured triples it sees in text format, reducing hallucination"
  - [corpus] mKG-RAG paper similarly shows multimodal KG linearization improves VQA performance
- Break condition: If linearization produces ambiguous or contradictory statements, or if the LLM's temperature is too high (current: 0.7), factual drift may occur.

### Mechanism 3
- Claim: Maintaining retrieval depth limits (depth=2) and sub-second latency enables real-time deployment without sacrificing accuracy.
- Mechanism: Graph traversal is bounded at depth 2 to limit subgraph size. Neo4j Cypher queries are optimized for real-time performance. Document retrieval runs in parallel, not sequentially. Total pipeline achieves 1,340ms average—higher than document-only RAG (1,230ms) but lower than hybrid retrieval baselines (1,850ms).
- Core assumption: Two-hop graph traversal captures most relevant product relationships for customer queries.
- Evidence anchors:
  - [Table 1] Shows 1,340ms latency vs. 1,230ms (Standard RAG) and 1,850ms (Hybrid Retrieval)
  - [Section 3.2] "graph traversal with configurable depth limits... optimized for real-time performance"
  - [corpus] Corpus signals show related KG2QA paper achieves similar tradeoffs in standards QA domain
- Break condition: Complex multi-hop queries requiring deeper traversal may miss relevant relationships; latency may spike with dense subgraphs on large-scale KGs.

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**
  - Why needed here: The entire architecture extends standard RAG with graph-structured retrieval. Without understanding baseline RAG (document retrieval → LLM generation), the hybrid contribution is unclear.
  - Quick check question: Can you explain why standard RAG struggles with "multi-hop or schema-rich queries" (Section 2.1)?

- **Knowledge Graph Construction & Representation**
  - Why needed here: The offline phase builds a domain-specific KG with entities (products, features, issues) and relations ("compatible-with", "resolves"). Understanding schema design is critical for implementation.
  - Quick check question: Given the KG schema in Section 3.1, what entities and relations would you extract from a support ticket about "WidgetX battery draining fast"?

- **Entity Recognition & Linking**
  - Why needed here: Query understanding depends on extracting entities E={e₁, e₂, ...} using spaCy and BERT-based models. Retrieval quality hinges on this step.
  - Quick check question: If entity extraction misses a product mention, which retrieval paths fail and what's the downstream impact on answer synthesis?

## Architecture Onboarding

- **Component map:**
  - **Offline Phase:** Product Catalogs + Support Tickets → Entity/Relation Extraction → KG Construction (Neo4j) → Graph Indexing
  - **Online Phase:** Customer Query → Query Understanding (spaCy + BERT) → Parallel Retrieval (KG Subgraph via Cypher + Documents via BM25/dense) → Answer Synthesis (GPT-3.5-turbo) → Final Response

- **Critical path:** Entity extraction accuracy → Subgraph retrieval relevance → Linearization quality → LLM grounding. The 23% accuracy gain is most sensitive to entity extraction failures and KG coverage gaps.

- **Design tradeoffs:**
  - Depth=2 traversal: Faster but may miss multi-hop relationships (e.g., product→accessory→compatibility chain)
  - Temperature=0.7: Balanced fluency vs. factual consistency; higher values risk hallucination
  - Parallel vs. sequential retrieval: Adds complexity but keeps latency sub-second
  - KG maintenance cost: 500K tickets require continuous entity/relation extraction updates

- **Failure signatures:**
  - Empty subgraph returned: Query entities not in KG → falls back to document-only, reducing accuracy
  - High latency (>2s): Complex queries generating large subgraphs; check Cypher query performance
  - Contradictory outputs: KG facts conflict with retrieved documents; synthesis may select wrong source
  - Generic responses: Retrieval returns irrelevant documents; check entity extraction or embedding quality

- **First 3 experiments:**
  1. **Entity extraction baseline:** Run spaCy+BERT on 100 sample queries, measure precision/recall of extracted entities against manual labels. Target: >90% precision before retrieval.
  2. **Retrieval ablation:** Compare three configurations on held-out test set—(a) KG-only, (b) Document-only, (c) Hybrid. Measure accuracy and latency to validate the 23% improvement claim.
  3. **Depth sensitivity:** Test depth=1, 2, 3 traversal on multi-hop queries (e.g., "What accessories work with WidgetX's compatible phones?"). Measure accuracy vs. latency tradeoff to justify depth=2 default.

## Open Questions the Paper Calls Out
- **Open Question 1:** How can the framework be adapted to support dynamic, real-time updates to the knowledge graph from new support cases rather than relying on the current static, offline construction process?
- **Open Question 2:** To what extent does the answer synthesis algorithm degrade in performance when the knowledge graph contains automated extraction errors or inconsistent relations?
- **Open Question 3:** Does the joint synthesis of structured subgraphs and unstructured documents generalize effectively to multilingual customer support scenarios?

## Limitations
- The 23% accuracy improvement claim depends on ground truth evaluation methodology that is not fully specified, particularly how factual correctness was adjudicated when KG facts conflicted with document claims.
- The depth-2 graph traversal assumption may not hold for complex multi-hop queries like compatibility chains, potentially missing relevant relationships.
- The 89% user satisfaction metric depends on subjective human evaluation criteria that are unspecified, making independent verification difficult.

## Confidence
- **High Confidence:** The parallel retrieval architecture (KG + documents) is technically sound and supported by evidence; the linearization mechanism for KG facts is straightforward and well-explained.
- **Medium Confidence:** The 23% accuracy improvement and 89% user satisfaction metrics—these require access to ground truth labels and evaluation protocols for independent verification.
- **Low Confidence:** The depth-2 traversal assumption for capturing relevant relationships, and the scalability of the system to larger KGs or more complex query patterns.

## Next Checks
1. **Ground Truth Verification:** Request access to the evaluation dataset and ground truth annotations to independently verify the 23% accuracy improvement claim, particularly focusing on cases where KG facts and document claims conflict.
2. **Query Complexity Analysis:** Test the system on a benchmark of multi-hop queries requiring depth>2 traversal (e.g., "Which accessories are compatible with WidgetX's compatible phones?") to validate the depth-2 assumption and identify failure modes.
3. **Latency Profiling:** Measure per-component latency (entity extraction, subgraph retrieval, document retrieval, LLM synthesis) on queries of varying complexity to identify bottlenecks and confirm the 1,340ms average remains sub-second under load.