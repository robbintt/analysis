---
ver: rpa2
title: Generics and Default Reasoning in Large Language Models
arxiv_id: '2508.13718'
source_url: https://arxiv.org/abs/2508.13718
tags:
- llama
- instruct
- claude
- chat
- urbo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates 28 large language models on 20 benchmark problems
  for non-monotonic reasoning involving generic generalizations. The authors find
  that while several frontier models perform well on default reasoning patterns, performance
  varies widely across models and prompting strategies.
---

# Generics and Default Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2508.13718
- Source URL: https://arxiv.org/abs/2508.13718
- Authors: James Ravi Kirkpatrick; Rachel Katharine Sterken
- Reference count: 40
- Key outcome: Frontier models show mixed performance on default reasoning with CoT prompting often degrading accuracy

## Executive Summary
This paper evaluates 28 large language models on 20 benchmark problems for non-monotonic reasoning involving generic generalizations. While several frontier models perform well on default reasoning patterns, performance varies widely across models and prompting strategies. Few-shot prompting modestly improves performance for some models, but chain-of-thought prompting often leads to significant performance degradation (mean accuracy drop -11.14%, SD 15.74% in high-performing models). Most models struggle to distinguish between defeasible and deductive inference or misinterpret generics as universal statements.

## Method Summary
The study tests 28 LLMs (closed-weight via API, open-weight via LM Studio with quantization) on 20 defeasible reasoning patterns with 20 instances each. Three prompting conditions are evaluated: zero-shot, few-shot (4 examples), and zero-shot chain-of-thought. Temperature 0 (greedy) and temperature 1 (sampling) are tested. Binary accuracy is measured against expert judgments, with Pearson correlations computed between nonsense and real variants to diagnose conceptual confusion.

## Key Results
- CoT prompting significantly degrades performance on high-performing models (mean -11.14% accuracy drop)
- Models show systematic confusion between generic and universal quantification (negative correlations r < -0.6 between base and v-variants in 6/8 patterns)
- Few-shot prompting helps weaker models but has minimal effect on high-performers
- Error patterns include Generic→Universal conversion, Defeasible vs. Deductive Confusion, Overly Statistical reasoning, and Step-Consistent Bad Conclusions

## Why This Works (Mechanism)

### Mechanism 1
- Chain-of-thought prompting may induce "overthinking" that degrades defeasible reasoning performance
- CoT encourages analytic processing that misrepresents the exception-permitting character of generics, pushing models toward treating generic statements as universals or applying deductive reasoning where defeasible reasoning is appropriate
- Core assumption: Models have implicit competence with default reasoning patterns that explicit deliberation disrupts
- Evidence: CoT degrades performance by -11.14% mean; four error patterns identified showing generic-universal confusion
- Break condition: If models given CoT with explicit meta-commentary about defeasibility maintain performance, the overthinking hypothesis is weakened

### Mechanism 2
- Models conflate generic generalizations with universal quantification due to insufficient conceptual differentiation in training distributions
- Generic statements and universal statements appear in similar contexts without explicit logical-semantic distinctions, leading to systematic misapplication of deductive reasoning patterns
- Core assumption: Negative correlations between base and v-variant conditions reflect conceptual confusion rather than task misunderstanding
- Evidence: Strong negative correlations (r < -0.6) between base and v-variants in 6/8 patterns; models treat generics as universals
- Break condition: If fine-tuning on explicit generic-vs-universal distinction tasks transfers to improved default reasoning, training distribution insufficiency is supported

### Mechanism 3
- Few-shot prompting improves weaker models by providing contextual anchors that calibrate task understanding
- Weaker models lack robust internal representations of the defeasible reasoning task format; few-shot examples provide scaffolding that clarifies expected reasoning pattern
- Core assumption: Performance improvement reflects task calibration rather than improved logical reasoning capability
- Evidence: Weaker models substantially benefit from few-shot examples while high-performing models show minimal effects
- Break condition: If few-shot examples with incorrect reasoning patterns still improve performance for weak models, benefit stems from format calibration

## Foundational Learning

- **Non-monotonic reasoning**
  - Why needed here: The paper evaluates defeasible inferences where conclusions can be withdrawn upon learning new information—fundamentally different from classical deductive logic
  - Quick check question: Given "Birds fly" and "Tweety is a bird," does adding "Tweety is a penguin" change whether you can defeasibly infer "Tweety flies"?

- **Generic generalizations vs. universal quantification**
  - Why needed here: The core test distinguishes whether models interpret "Birds fly" (generic, exception-tolerant) as equivalent to "All birds fly" (universal, exception-intolerant)
  - Quick check question: Does the truth of "Birds fly" require that >50% of birds fly, that all birds fly, or something else?

- **Defeasible Modus Ponens/Tollens**
  - Why needed here: The paper tests 20 inference patterns; DMP and DMT are foundational for understanding when these patterns apply (and when they're defeated)
  - Quick check question: From "Adults have driving licenses" and "Mary doesn't have a driving license," can you defeasibly conclude "Mary isn't an adult"?

## Architecture Onboarding

- **Component map**: 20 inference patterns × 20 instances × 3 prompting conditions × nonsense/real variants × order variants → 28 LLMs → Binary accuracy vs expert judgments → Correlation analysis

- **Critical path**: Select inference patterns → Generate 20 instances per pattern → Create variant sets → Run at temperature 0 and 1 → Analyze correlations between variants

- **Design tradeoffs**: Temperature 0 gives deterministic outputs but hides uncertainty; constrained single-token outputs reduce parsing complexity but prevent nuanced examination

- **Failure signatures**: Generic→Universal (treating exceptions as counterexamples), Defeasible vs. Deductive Confusion, Overly Statistical (rejecting generics lacking frequency support), Step-Consistent Bad Conclusion (CoT coherent but final answer contradicts steps)

- **First 3 experiments**: 1) Replicate with o1/o3-style reasoning models to test whether explicit reasoning training mitigates CoT degradation, 2) Fine-tune Llama 3.1 8B on explicit generic-vs-universal distinction task, 3) Test premise order effects systematically across all two-premise patterns

## Open Questions the Paper Calls Out

- Does chain-of-thought (CoT) prompting degrade performance on default reasoning tasks because it encourages "overthinking" or because it exposes underlying conceptual competence gaps? The authors cannot definitively distinguish between processing interference and fundamental lack of capability.

- How does LLM performance on these generic reasoning benchmarks align with actual human experimental subjects compared to expert theoretical judgments? The study evaluated models against normative logic standards rather than descriptive cognitive data.

- Are the failure modes in distinguishing generics from universal generalizations robust across languages with different morphological markers for genericity? Experiments were limited to English and bare plural generics.

## Limitations
- The study cannot definitively establish whether models' failures reflect fundamental conceptual limitations or superficial pattern-matching failures
- Corpus support for proposed mechanisms is weak, with related work focusing on bias propagation rather than explaining why CoT degrades performance
- The paper doesn't address whether limitations reflect training data deficiencies, architectural constraints, or inherent challenges in representing defeasible reasoning

## Confidence
- **High confidence**: CoT prompting degrades performance on defeasible reasoning tasks (empirically demonstrated across multiple models)
- **Medium confidence**: Models confuse generic generalizations with universal quantification (supported by negative correlations between variants)
- **Low confidence**: Specific mechanisms for why CoT degrades performance lack strong empirical or corpus support

## Next Checks
1. Test o1/o3-style reasoning models on the same benchmark to determine whether explicit reasoning training mitigates or exacerbates CoT degradation
2. Fine-tune a smaller model (Llama 3.1 8B) on explicit generic-vs-universal distinction tasks, then re-evaluate to test whether conceptual clarity transfers
3. Systematically test premise order effects across all two-premise patterns to map sensitivity to surface structure versus logical structure