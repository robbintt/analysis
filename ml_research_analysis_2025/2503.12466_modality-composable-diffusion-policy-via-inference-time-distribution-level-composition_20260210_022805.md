---
ver: rpa2
title: Modality-Composable Diffusion Policy via Inference-Time Distribution-level
  Composition
arxiv_id: '2503.12466'
source_url: https://arxiv.org/abs/2503.12466
tags:
- diffusion
- policy
- mcdp
- learning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Modality-Composable Diffusion Policy (MCDP),
  a method to enhance diffusion-based robotic policies by combining multiple pre-trained
  single-modality diffusion policies at inference time without additional training.
  MCDP leverages the compositional properties of diffusion models to combine distributional
  scores from policies trained on different visual modalities (e.g., RGB images and
  point clouds), creating a more expressive and adaptable policy.
---

# Modality-Composable Diffusion Policy via Inference-Time Distribution-level Composition

## Quick Facts
- **arXiv ID**: 2503.12466
- **Source URL**: https://arxiv.org/abs/2503.12466
- **Reference count**: 9
- **Primary result**: Combines pre-trained RGB and point cloud diffusion policies at inference time, achieving up to 0.86 success rate on RoboTwin tasks, outperforming individual policies (0.42 and 0.62).

## Executive Summary
This paper introduces Modality-Composable Diffusion Policy (MCDP), a method to enhance diffusion-based robotic policies by combining multiple pre-trained single-modality diffusion policies at inference time without additional training. MCDP leverages the compositional properties of diffusion models to combine distributional scores from policies trained on different visual modalities (e.g., RGB images and point clouds), creating a more expressive and adaptable policy. The authors demonstrate that MCDP achieves superior performance compared to unimodal diffusion policies on the RoboTwin dataset, with success rates reaching up to 0.86 in certain tasks, surpassing individual policies that achieve 0.42 and 0.62. The paper also provides empirical guidance on optimal weight configurations for combining policies based on their relative performance, showing that MCDP is most effective when the better-performing unimodal policy is given higher weight in the composition.

## Method Summary
MCDP combines pre-trained single-modality diffusion policies at inference time by composing their noise estimates (score functions) through weighted linear combination. The approach requires no additional training - policies remain frozen while their outputs are combined during the denoising process. The method specifically addresses scenarios where different visual modalities provide complementary information for robotic manipulation tasks. Implementation involves aligning diffusion schedulers across policies and empirically tuning composition weights based on individual policy performance.

## Key Results
- MCDP achieves 0.86 success rate on Dual Bottles Pick (Easy), outperforming individual policies (0.77 and 0.36).
- On Block Hammer Beat, MCDP achieves 0.76 when DP_pcd performs well, but drops to 0.07 when over-weighting the weaker RGB policy.
- Finding 3 confirms optimal composition weights favor the stronger unimodal policy, with peak performance at w_1 ≈ 0.7-0.8 for asymmetric performance gaps.

## Why This Works (Mechanism)

### Mechanism 1: Score Function Composition at Inference Time
- Claim: Combining noise estimates from multiple modality-specific policies creates a more expressive joint distribution that can outperform any single-modality policy.
- Mechanism: At each denoising timestep t, MCDP computes a weighted linear combination of score functions (noise estimates) from separately trained policies: ē_M* = Σ w_i · ε_θ(τ_t, t, c_i). This composed score guides the diffusion sampling toward trajectories that satisfy multiple modality constraints simultaneously.
- Core assumption: Score functions from different modalities can be meaningfully combined linearly, and modalities provide complementary information rather than redundant or conflicting signals.
- Evidence anchors:
  - [abstract] "combine their distributional scores to form a more expressive Modality-Composable Diffusion Policy"
  - [section 3.2, Eq. 10] ē_M*(τ_t, t, c) = Σ w_i(ε_θ(τ_t, t, c_i)), with Σ w_i = 1
  - [corpus] Paper 60746 ("Compose Your Policies!") validates similar test-time distribution-level composition for diffusion-based robot policies, achieving comparable improvements.

### Mechanism 2: Distribution Product via Energy Summation
- Claim: The linear composition of diffusion scores is mathematically grounded in compositional energy-based models, where the product of probability distributions corresponds to summation of energy functions.
- Mechanism: From EBM theory, p_product(τ) ∝ e^(-Σ E_i(τ)) = Π p_i(τ). The score function ∇_τ log p(τ) is proportional to the diffusion noise estimate, so summing scores approximates sampling from the product distribution—a tighter, more constrained distribution that lies in the intersection of high-probability regions from each modality.
- Core assumption: Individual modality-conditional distributions p(τ|c_i) are approximately independent given the trajectory, and each policy has learned meaningful energy landscapes.
- Evidence anchors:
  - [section 2, Eq. 3] p_product(τ) ∝ p_θ^1(τ) · p_θ^2(τ) ··· p_θ^n(τ) ∝ e^(-Σ E_θ^i(τ))
  - [section 3.1, Eq. 7] Derivation showing ∇_τ log p(τ|c_1,...,c_n) = ∇_τ log p(τ) + Σ α_i(∇_τ log p(τ|c_i) - ∇_τ log p(τ))

### Mechanism 3: Weighted Dominance Favoring Stronger Policies
- Claim: Optimal composition weights should assign higher values to better-performing unimodal policies; equal weighting is suboptimal when performance gaps exist.
- Mechanism: Higher weights amplify the score contribution from more reliable policies while diluting contamination from weaker policies' noisy estimates. This creates an asymmetric composition that preserves the stronger policy's high-probability regions while incorporating complementary corrections from the weaker policy.
- Core assumption: Better-performing unimodal policies produce more accurate score estimates (lower noise-to-signal ratio in their gradient guidance).
- Evidence anchors:
  - [section 4.1, Finding 3] "improvement of MCDP is maximized when the better-performing unimodal DP holds a larger weight in MCDP"
  - [table 1] Dual Bottles Pick (Easy): DP_img=0.77, DP_pcd=0.36 → MCDP achieves 0.85 with w_1=0.8 (favoring stronger RGB policy)

## Foundational Learning

### Energy-Based Models (EBMs) and Compositionality
- Why needed here: MCDP's theoretical justification derives from compositional EBMs where probability distributions combine via energy summation. Understanding this connection explains why score composition works mathematically.
- Quick check question: If p(x) ∝ e^(-E(x)) and you have two distributions p_1 and p_2, what is the energy of their product distribution p_1 · p_2?

### Diffusion Sampling and Score Functions
- Why needed here: The noise predictor ε_θ(τ_t, t) in diffusion models is functionally equivalent (up to scaling) to the score function ∇_τ log p(τ_t). This equivalence enables score composition without modifying model architecture.
- Quick check question: In Langevin dynamics, what does the term -γ∇_x log p(x) represent, and how does it relate to diffusion denoising?

### Classifier-Free Guidance (CFG) as Composition
- Why needed here: MCDP's formulation (Eq. 8) generalizes CFG, which composes unconditional and conditional scores. Understanding CFG provides intuition for why the paper opts for CFG-free composition (Eq. 10) instead.
- Quick check question: Why does CFG require computing both ε_θ(τ_t, t, c) and ε_θ(τ_t, t) (unconditional), and what computational cost does this incur?

## Architecture Onboarding

### Component Map:
```
Visual Input M_1 (e.g., RGB)     Visual Input M_2 (e.g., Point Cloud)
         ↓                                ↓
Pre-trained Policy π_1 (DP_img)   Pre-trained Policy π_2 (DP_pcd)
         ↓                                ↓
Noise Estimate ε_1(τ_t, t)        Noise Estimate ε_2(τ_t, t)
         └──────────┬─────────────────────┘
                    ↓
         Weighted Composition: ē = w_1·ε_1 + w_2·ε_2
                    ↓
         DDPM Denoising Step: τ_{t-1} = α_t(τ_t - γ_t·ē + ξ)
                    ↓
         Final Action Trajectory τ_0 (after N steps)
```

### Critical Path:
1. **Scheduler Alignment**: Ensure both policies use compatible diffusion schedulers (paper aligned DP_pcd's DDIM-10 to DDPM-100 to match DP_img). Mismatched schedulers produce incoherent composed scores.
2. **Timestep Synchronization**: At each step t, query both policies for noise estimates before composition.
3. **Weight Calibration**: Before deployment, run unimodal evaluations to determine relative performance, then set weights to favor stronger policy (e.g., w_strong ≈ 0.6-0.8).

### Design Tradeoffs:
- **CFG-free vs. CFG-based Composition**: CFG-free (Eq. 10) chosen for (a) flexibility—works with any pre-trained DP without CFG training, and (b) efficiency—avoids double computation per step. Tradeoff: CFG-free may be less theoretically grounded when policies have different unconditional priors.
- **Manual vs. Learned Weights**: Paper uses fixed weights tuned empirically. Learned/adaptive weights could improve but require validation data and add complexity.
- **Modality Choice**: RGB provides texture/appearance; point clouds provide geometry. Paper tests only this pair—extending to depth, tactile, or language modalities requires validation.

### Failure Signatures:
- **Cascading degradation**: When one policy fails catastrophically (SR < 0.1), composition may still succeed if weight on failing policy is low, but high weights on failing policy cause collapse (Block Hammer Beat: w_1=0.7 → SR=0.07).
- **No improvement despite composition**: Both policies weak (SR < 0.3) → composition offers marginal gains at best (Pick Apple Messy: 0.26 → 0.25).
- **Scheduler mismatch symptoms**: Generated trajectories appear chaotic or fail to converge smoothly; action magnitudes inconsistent across timesteps.

### First 3 Experiments:
1. **Unimodal Baseline Reproduction**: Replicate DP_img (RGB-based) and DP_pcd (point-cloud-based) performance on 2-3 RoboTwin tasks to establish baseline SRs and verify policy implementations match paper's setup.
2. **Equal-Weight Composition Test**: Run MCDP with w_1=w_2=0.5 on tasks where both baselines have moderate performance (e.g., Empty Cup Place: 0.42/0.62). Verify composition yields improvement (target: SR > max(unimodal)).
3. **Weight Sensitivity Sweep**: On one task with asymmetric baselines (e.g., Dual Bottles Pick Easy: 0.77 vs 0.36), sweep w_1 ∈ {0.1, 0.3, 0.5, 0.7, 0.9}. Confirm peak performance occurs at w_1 ≈ 0.7-0.8 (favoring stronger policy) and document degradation pattern when weak policy is over-weighted.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the MCDP framework be effectively extended to cross-domain or cross-embodiment scenarios where policies operate on different action spaces or robot dynamics?
- Basis in paper: [explicit] The abstract and conclusion explicitly state the goal of facilitating "generalizable cross-modality, cross-domain, and even cross-embodiment policies," though the current work only validates visual modality composition.
- Why unresolved: The experiments are restricted to composing policies based on different visual inputs (RGB vs. point cloud) within the same simulation environment (RoboTwin).
- What evidence would resolve it: Demonstration of successful policy composition between distinct robot morphologies or simulation-to-real transfer without additional training.

### Open Question 2
- Question: Can the weighting coefficients ($w_i$) be determined dynamically during inference rather than requiring manual tuning?
- Basis in paper: [inferred] The paper notes that weights are "manually tuned" and Finding 3 highlights that optimal performance relies heavily on assigning higher weights to the stronger policy.
- Why unresolved: Static weights may not be optimal across all task states or time steps, requiring extensive ablation studies for every new policy pair.
- What evidence would resolve it: An adaptive weighting mechanism that estimates modality reliability in real-time, outperforming fixed-weight baselines.

### Open Question 3
- Question: How can the composition mechanism be improved to prevent performance degradation when one component policy is significantly inaccurate?
- Basis in paper: [inferred] Finding 2 reports that MCDP often fails to surpass the best unimodal policy when the other is weak, as "low-accuracy scores... significantly impact the joint distribution."
- Why unresolved: The linear summation of scores inherently incorporates noise from the weaker modality, dragging the composed trajectory away from the optimal path.
- What evidence would resolve it: A robust composition technique (e.g., outlier rejection or non-linear gating) that allows the composed policy to ignore a failing modality and match the performance of the stronger unimodal expert.

## Limitations
- **Modality Compatibility Assumption**: The paper assumes that score functions from RGB and point cloud modalities can be meaningfully combined through linear weighting. This assumption lacks rigorous validation across different modality pairs.
- **Score Function Independence**: The theoretical foundation relies on the independence of conditional distributions p(τ|c_i), enabling product distribution composition. The paper provides limited empirical evidence for this independence assumption.
- **Scheduler Alignment**: MCDP requires both policies to use identical diffusion schedulers. The paper aligns DP_pcd from DDIM-10 to DDPM-100, but doesn't analyze the impact of this conversion or validate performance when policies have different native schedulers.

## Confidence
- **High Confidence**: The empirical results showing improved performance over unimodal baselines are well-supported by the experimental data. The finding that optimal weights favor stronger policies is consistently validated across multiple tasks.
- **Medium Confidence**: The theoretical connection to compositional EBMs provides a plausible explanation for why score composition works, but the application to robotic policy composition represents a novel extension that lacks comprehensive theoretical validation.
- **Low Confidence**: Generalization claims to other modality pairs and domains are speculative. The paper focuses exclusively on RGB + point cloud and RoboTwin tasks, limiting confidence in broader applicability.

## Next Checks
1. **Modality Pair Ablation**: Test MCDP with alternative modality combinations (RGB+depth, point cloud+language) on RoboTwin tasks to validate whether compositional benefits extend beyond the specific RGB+point cloud pairing.

2. **Scheduler Mismatch Analysis**: Intentionally use mismatched schedulers (e.g., DDPM-100 + DDIM-50) in composition and measure performance degradation to quantify the sensitivity to scheduler alignment.

3. **Independent Error Analysis**: Design tasks where RGB and point cloud policies have systematically different failure modes (e.g., RGB fails on textureless objects, point cloud fails on transparent objects) to empirically validate whether independent errors produce multiplicative performance gains.