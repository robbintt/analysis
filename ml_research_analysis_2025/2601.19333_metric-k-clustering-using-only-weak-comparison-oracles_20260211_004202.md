---
ver: rpa2
title: Metric $k$-clustering using only Weak Comparison Oracles
arxiv_id: '2601.19333'
source_url: https://arxiv.org/abs/2601.19333
tags:
- clustering
- oracle
- vertices
- lemma
- quadruplet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We study $k$-clustering in the Rank-model, where pairwise distances
  are replaced by a noisy quadruplet oracle that only provides relative distance comparisons.
  Our main contribution is the design of randomized algorithms that, using only such
  an oracle, compute $O(k \cdot \text{polylog}(n))$ centers and a mapping from input
  items to centers such that the clustering cost is at most a constant times the optimal
  cost.
---

# Metric $k$-clustering using only Weak Comparison Oracles

## Quick Facts
- arXiv ID: 2601.19333
- Source URL: https://arxiv.org/abs/2601.19333
- Reference count: 40
- Primary result: Randomized algorithms compute O(k·polylog n) centers and mapping achieving constant-factor approximation using only noisy quadruplet oracle queries

## Executive Summary
This paper addresses $k$-clustering in the Rank-model where only a noisy quadruplet oracle is available, comparing relative distances between edge pairs rather than providing absolute distances. The authors develop algorithms that compute O(k·polylog n) centers and a mapping achieving constant-factor approximation to optimal clustering cost, using only quadruplet queries. They show that this relaxation from exactly k centers is necessary, as no o(n)-approximation is possible with only k centers in this model. For metrics with bounded doubling dimension, they further improve to (1+ε)-approximation while maintaining the same query complexity.

## Method Summary
The framework uses recursive sampling with kernel-guard structures to enable approximate nearest neighbor discovery under noisy comparisons. The main algorithm (Alg-G) samples pairs of sets, uses ProbSort to obtain approximately ordered edges, extracts kernel and guard vertices for filtering, and applies AdvSort with Alg-Tester to emulate adversarial comparisons. For bounded doubling dimension, Alg-D adds partitioning and lazy filtering to reduce query complexity. Alg-DI refines the solution to (1+ε)-approximation through multi-scale bucketing. The algorithms achieve O(n·k·polylog n) query complexity generally, improving to O((n+k²)·polylog n) for bounded doubling dimension metrics.

## Key Results
- Algorithms compute O(k·polylog n) centers achieving O(1)-approximation in general metrics
- For bounded doubling dimension, algorithms achieve (1+ε)-approximation with same query complexity
- Query complexity improves from O(n·k·polylog n) to O((n+k²)·polylog n) for bounded doubling dimension
- Theoretical lower bound shows O(k) centers cannot achieve o(n)-approximation in R-model

## Why This Works (Mechanism)

### Mechanism 1: Recursive Sampling with Kernel-Guard Structure
A noisy quadruplet oracle can support approximate nearest neighbor discovery when sample sets are structured with kernel (close) and guard (boundary) vertices. In each round, the algorithm samples two sets S^(1) and S^(2), extracts kernel_i(s) (top-ranked edges) and Guard_i(s) (next-ranked edges) from the approximately ordered edges, and uses proximity scores to filter vertices. The guard set filters vertices too close to sample points while the kernel set enables distance comparisons by majority voting over comparisons with known-close vertices. The oracle's error rate φ < 1/4 and ProbSort's maximum dislocation D = O(log n) are bounded relative to kernel/guard sizes.

### Mechanism 2: Emulating Adversarial Oracle from Probabilistic Oracle
A probabilistic quadruplet oracle can emulate an adversarial oracle with multiplicative error μ=1 when comparing edges to filtered vertices. Alg-Tester selects the smaller kernel, discards ambiguous comparisons near the boundary, and uses majority voting over comparisons with the selected kernel. When distances differ by more than 2×, the majority test succeeds with high probability because all kernel-to-vertex distances are bounded relative to the smaller distance. The comparison threshold τ = ⌊m/2⌋ is large enough for Chernoff bounds to ensure correctness.

### Mechanism 3: Multi-Scale Bucket Sampling for (1+ε)-Approximation
In bounded doubling dimension, a Coreset+ can be refined to ε-Coreset+ by sampling at multiple distance scales from each center's assigned vertices. Alg-DI partitions assigned vertices into buckets by approximate distance, then samples recursively from progressively smaller suffixes. This hits all relevant distance scales; the doubling dimension ensures each bucket can be covered by few small balls, so samples provide good coverage. The initial Coreset+ mapping M satisfies d(v,M(v)) = O(1)·d(v,C*), and the refined mapping satisfies Σ d(v,M+(v)) ≤ δ·Σ d(s,v).

## Foundational Learning

- **Concept: Noisy comparison oracles and dislocation bounds**
  - Why needed here: The entire framework depends on ProbSort producing orderings with O(log n) maximum dislocation
  - Quick check question: Given a probabilistic comparison oracle with error φ=0.2, what is the expected maximum dislocation when sorting 1000 items using ProbSort?

- **Concept: Coreset+ vs. coreset distinction**
  - Why needed here: The paper outputs O(k·polylog n) centers (Coreset+) rather than exactly k centers
  - Quick check question: Why does an α-Coreset+ directly yield an O(α)-coreset? What is the weight function construction?

- **Concept: Doubling dimension and covering numbers**
  - Why needed here: Alg-D's improved query complexity and Alg-DI's (1+ε)-approximation both rely on bounded doubling dimension
  - Quick check question: A metric has doubling dimension δ if every ball of radius r can be covered by 2^δ balls of radius r/2. What is the doubling dimension of d-dimensional Euclidean space?

## Architecture Onboarding

- **Component map:**
  ProbSort -> AdvSort -> Alg-Tester -> Alg-G (Alg-D -> Alg-DI for doubling dimension)

- **Critical path:**
  1. Sample S^(1), S^(2) and run ProbSort on E(S^(1), S^(2))
  2. Extract kernel/guard sets from sorted order
  3. Filter vertices using proximity scores against guards
  4. Run AdvSort with Alg-Tester on E(S^(1), V') to find approximate nearest neighbors
  5. Select safe subset V'' from prefix of sorted nearest-neighbor edges
  6. Map V'' to their approximate nearest neighbors in S^(1), recurse on remainder
  7. (For doubling metrics) Apply Alg-D's partitioning and lazy filtering to reduce queries
  8. (Optional refinement) Run Alg-DI to improve approximation quality

- **Design tradeoffs:**
  - Query complexity vs. center count: Using O(k·polylog n) centers instead of k enables constant-factor approximation in R-model
  - Pre-filtering (Alg-G) vs. lazy filtering (Alg-D): Alg-G's global filtering costs O(nk) queries per round but guarantees Alg-Tester preconditions
  - Approximation vs. queries: ε-approximation (Alg-DI) requires additional O(n·polylog n) queries beyond the O(1)-approximation base

- **Failure signatures:**
  - Too many eliminations in filtering: If |V'| < (3/5)|V_i|, check that kernel/guard sizes are O(log n)
  - Alg-Tester incorrect on close distances: Expected behavior; only guaranteed when distances differ by >2×
  - Coreset+ cost exceeds O(1)·OPT: Check that injection ψ from bad to good vertices is constructed correctly across rounds

- **First 3 experiments:**
  1. **Noise sensitivity:** Run Alg-G on synthetic clustered data with oracle error φ ∈ {0.05, 0.15, 0.20, 0.24}, measure clustering cost ratio vs. optimal
  2. **Query complexity validation:** Count actual quadruplet queries on datasets of varying n (1K to 100K) with fixed k=10
  3. **Doubling dimension benefit:** Compare Alg-G vs. Alg-D query counts on high-dimensional synthetic data (d=2 vs. d=50)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the algorithmic techniques be extended to other clustering objectives, such as hierarchical clustering or sum-of-radii clustering, within the R-model?
- Basis in paper: Section 5 explicitly lists "studying whether the techniques can be extended to other clustering objectives" as a primary direction for future research
- Why unresolved: The current theoretical analysis and algorithmic construction are specifically tailored for $(k, p)$-clustering objectives
- What evidence would resolve it: Derivation of approximation algorithms for hierarchical or sum-of-radii clustering that operate using only the noisy quadruplet oracle

### Open Question 2
- Question: Can the proposed framework be adapted to solve related graph problems, specifically the Minimum Spanning Tree (MST) problem, using only a weak comparison oracle?
- Basis in paper: Section 5 states the authors plan to extend the framework "to related graph problems (such as the Minimum Spanning Tree problem)"
- Why unresolved: The paper focuses exclusively on clustering primitives and does not provide methods for computing global graph structures like MSTs
- What evidence would resolve it: An algorithm that constructs an approximate MST with provable query complexity bounds using only the quadruplet oracle

### Open Question 3
- Question: Is it possible to generalize the theoretical guarantees to non-metric spaces or adapt the algorithms to alternative oracle models, such as triplet oracles?
- Basis in paper: Section 5 outlines plans to "generalize our framework to non-metric graphs, to alternative oracle models (such as triplet oracles)"
- Why unresolved: The analysis relies heavily on metric properties and the specific definition of the quadruplet oracle
- What evidence would resolve it: Theoretical results showing whether constant-factor approximations are achievable in non-metric settings or providing a reduction/analysis for triplet oracles

## Limitations

- The framework's success critically depends on the quadruplet oracle having error rate φ < 1/4, a non-trivial assumption about oracle implementation
- The kernel/guard structure assumes sufficient separation between close and boundary vertices, which may fail on datasets with high metric density
- The relaxation to O(k·polylog n) centers is necessary but increases output size, potentially limiting practical applicability

## Confidence

- **High confidence:** The core algorithmic structure (recursive sampling, kernel/guard filtering, multi-scale bucketing) is well-specified
- **Medium confidence:** The claim that O(k·polylog n) centers suffice for O(1)-approximation in the R-model is supported by lower bound arguments
- **Medium confidence:** The emulation of adversarial oracles from probabilistic ones via Alg-Tester is theoretically sound when preconditions hold

## Next Checks

1. **Oracle sensitivity analysis:** Systematically vary φ from 0.05 to 0.24 and measure clustering cost degradation to validate the claimed independence from oracle error

2. **Doubling dimension verification:** Test Alg-D on datasets with varying intrinsic dimensionality to verify the claimed query complexity advantage appears only when the metric's doubling dimension is bounded

3. **Kernel/guard separation robustness:** Introduce adversarial distance configurations where kernel and guard sets have minimal separation to test whether filtering fails catastrophically or degrades gracefully