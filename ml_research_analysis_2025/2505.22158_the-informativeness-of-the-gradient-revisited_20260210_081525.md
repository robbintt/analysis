---
ver: rpa2
title: The informativeness of the gradient revisited
arxiv_id: '2505.22158'
source_url: https://arxiv.org/abs/2505.22158
tags:
- function
- gradient
- distribution
- learning
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies when gradient-based methods struggle to learn
  functions due to low gradient informativeness, measured by the variance of the gradient
  with respect to a randomly chosen target function from a hypothesis class. The author
  introduces a general upper bound on this variance in terms of a parameter related
  to the pairwise independence of the target function class and the collision entropy
  of the input distribution.
---

# The informativeness of the gradient revisited

## Quick Facts
- arXiv ID: 2505.22158
- Source URL: https://arxiv.org/abs/2505.22158
- Reference count: 40
- Primary result: Gradient informativeness variance bounded by O(ε + e^(-1/2 Ec)) where ε measures pairwise independence and Ec is collision entropy

## Executive Summary
This paper revisits the problem of gradient informativeness in learning, focusing on when gradient-based methods struggle due to low gradient variance with respect to randomly chosen target functions. The author introduces a general upper bound on gradient informativeness variance that depends on the pairwise independence of the target function class and the collision entropy of the input distribution. The key insight is that when target functions are almost pairwise independent, gradient-based learning becomes difficult unless the input distribution is carefully tuned. The paper applies this framework to Learning with Errors (LWE) mappings and high-frequency functions, showing that these classes present challenges for gradient-based methods due to their near-pairwise independence.

## Method Summary
The paper develops a theoretical framework measuring gradient informativeness through the variance of gradients when the target function is randomly selected from a hypothesis class. The main contribution is an upper bound on this variance expressed as O(ε + e^(-1/2 Ec)), where ε quantifies the pairwise independence of the target function class and Ec represents the collision entropy of the input distribution. The author applies this bound to analyze Learning with Errors (LWE) mappings and high-frequency functions, demonstrating that both classes are almost pairwise independent and thus challenging for gradient-based learning. Experiments validate that manipulating the collision entropy of input distributions significantly impacts gradient informativeness, with preprocessing techniques that reduce collision entropy improving gradient-based attacks on LWE.

## Key Results
- Theoretical upper bound O(ε + e^(-1/2 Ec)) characterizes gradient informativeness variance
- LWE mappings and high-frequency functions identified as almost pairwise independent classes
- Input distribution collision entropy significantly impacts gradient informativeness in experiments
- Preprocessing to reduce input collision entropy improves gradient-based attacks on LWE

## Why This Works (Mechanism)
The mechanism relies on the relationship between gradient variance and the statistical properties of the target function class. When functions are pairwise independent, the gradient of the loss function with respect to different target functions provides little information about the true target, making gradient-based learning inefficient. The collision entropy of the input distribution determines how much information the gradients carry about the underlying function, with lower collision entropy leading to more informative gradients. The O(ε + e^(-1/2 Ec)) bound captures this relationship, showing that gradient informativeness decreases when either the pairwise independence ε is large or the collision entropy Ec is small.

## Foundational Learning

### Pairwise Independence
Why needed: Fundamental concept for characterizing the statistical independence of target functions
Quick check: Functions f and g are pairwise independent if P(f(x)=a, g(x)=b) = P(f(x)=a)P(g(x)=b) for all a,b

### Collision Entropy
Why needed: Measures the concentration of the input distribution, affecting gradient informativeness
Quick check: Ec = -log(∑_x P(x)^2), where lower values indicate more concentrated distributions

### Gradient Variance Analysis
Why needed: Core metric for quantifying how informative gradients are for learning
Quick check: Var[∇L] = E[||∇L||^2] - ||E[∇L]||^2, measuring gradient dispersion

### Cryptographic Function Properties
Why needed: Understanding how cryptographic hardness relates to learning difficulty
Quick check: LWE mappings are designed to be hard to distinguish from random, affecting gradient informativeness

## Architecture Onboarding

### Component Map
Target Function Class -> Pairwise Independence Parameter ε -> Gradient Informativeness Bound -> Input Distribution Collision Entropy Ec

### Critical Path
1. Define target function class and measure pairwise independence
2. Characterize input distribution collision entropy
3. Apply theoretical bound O(ε + e^(-1/2 Ec))
4. Validate through experiments on gradient-based learning

### Design Tradeoffs
- Higher pairwise independence (larger ε) → Lower gradient informativeness
- Lower collision entropy (smaller Ec) → Lower gradient informativeness
- Cryptographic security often requires both properties, making learning difficult

### Failure Signatures
- Gradient norms remain consistently small across training iterations
- Loss landscape shows minimal variation with respect to different target functions
- Learning curves plateau early without meaningful progress

### 3 First Experiments
1. Measure gradient informativeness variance for LWE mappings with different input distributions
2. Compare learning performance on high-frequency functions with varying collision entropy
3. Test gradient-based attacks on LWE with preprocessing to reduce input collision entropy

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical bounds assume idealized conditions for pairwise independence that may not hold empirically
- Experimental validation limited to specific cases without broader function class exploration
- Practical implications for cryptographic security discussed but not fully validated
- Connection between theoretical bounds and finite-sample learning performance needs investigation

## Confidence

### Theoretical framework and bounds: High
The mathematical derivations and theoretical framework appear sound and well-established

### Application to LWE and high-frequency functions: Medium
While theoretically justified, the practical implications require more empirical validation

### Experimental validation of claims: Medium
Limited scope of experiments and lack of broader function class testing reduces confidence

### Practical implications for cryptography: Low
Theoretical connections made but not extensively validated through real-world cryptographic attacks

## Next Checks

1. Test the gradient informativeness bounds across diverse function classes beyond LWE and high-frequency examples, including practical deep learning architectures and real-world datasets.

2. Conduct extensive empirical studies varying the collision entropy of input distributions systematically to validate the theoretical relationship between collision entropy and gradient informativeness.

3. Evaluate the impact of finite sample sizes and optimization hyperparameters on the practical relevance of the theoretical bounds, particularly for common deep learning scenarios.