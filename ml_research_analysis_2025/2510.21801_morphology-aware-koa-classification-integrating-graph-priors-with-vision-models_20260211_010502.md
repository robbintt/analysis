---
ver: rpa2
title: 'Morphology-Aware KOA Classification: Integrating Graph Priors with Vision
  Models'
arxiv_id: '2510.21801'
source_url: https://arxiv.org/abs/2510.21801
tags:
- graph
- vision
- features
- severity
- radiographic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accurately diagnosing knee
  osteoarthritis (KOA) severity from radiographs by integrating morphological graph
  representations with vision models. The core method constructs anatomical graphs
  from Segment Anything Model (SAM) segmentations and combines them with radiographic
  features using mutual information maximization.
---

# Morphology-Aware KOA Classification: Integrating Graph Priors with Vision Models

## Quick Facts
- arXiv ID: 2510.21801
- Source URL: https://arxiv.org/abs/2510.21801
- Reference count: 0
- Surpasses single-modality baselines by up to 10% in accuracy, reaching nearly 80%

## Executive Summary
This paper addresses the challenge of accurately diagnosing knee osteoarthritis (KOA) severity from radiographs by integrating morphological graph representations with vision models. The core method constructs anatomical graphs from Segment Anything Model (SAM) segmentations and combines them with radiographic features using mutual information maximization. This approach explicitly encodes structural relationships between bones, enriching the feature space with clinically validated biomarkers. Experiments on the Osteoarthritis Initiative dataset show that the method surpasses single-modality baselines by up to 10% in accuracy, reaching nearly 80%. It also outperforms existing state-of-the-art methods by 8% in accuracy and 11% in F1 score, demonstrating the critical importance of incorporating anatomical structure into radiographic analysis for accurate KOA severity grading.

## Method Summary
The method constructs anatomical graphs from bone contours detected via SAM segmentation, then fuses these geometry-aware features with traditional radiographic features through mutual information maximization. The approach uses a two-stage training strategy where a graph encoder is pre-trained on graph-only classification and then frozen, while the vision encoder and fusion components are trained jointly. The mutual information maximization aligns the vision embeddings with the morphology-rich graph features through an adaptive translation mechanism that gradually shifts from fixed graph embeddings to translated vision embeddings during training.

## Key Results
- Achieves 80.80% accuracy and 81.38% F1 with MM-F (MIM) ViT-Large architecture
- Outperforms existing state-of-the-art methods by 8% in accuracy and 11% in F1 score
- Graph-only classifier achieves 74.94% accuracy, materially outperforming most vision-only models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit graph encoding of bone morphology captures diagnostic information that texture-based vision models miss
- Mechanism: SAM-generated bone masks → contour sampling → graph construction (nodes=contour points, edges=k-NN) → EdgeConv layers aggregate local geometry → global pooling → morphology embedding. This forces the model to attend to joint space width, bone curvature, and structural relationships rather than local texture artifacts.
- Core assumption: Clinical KL-grading criteria depend more on structural biomarkers (osteophytes, joint space narrowing) than texture patterns
- Evidence anchors:
  - [abstract] "morphological graph representation—derived from Segment Anything Model (SAM) segmentations—with a vision encoder"
  - [section 2.1] "This graph, which reflects key morphological attributes (e.g., joint space width, bone curvature)"
  - [section 4.2] "MorphoGraph classifier... achieves 74.94% accuracy... materially outperforming most vision-only models"
  - [corpus] Weak direct corpus support for graph-based KOA; ClinNet uses ordinal regression without explicit graphs
- Break condition: If KL grades were primarily determined by texture features rather than bone geometry, graph encoding would underperform vision baselines (contradicted by Table 2)

### Mechanism 2
- Claim: Mutual Information Maximization (MIM) aligns vision embeddings with geometry-aware features, improving cross-modal coherence
- Mechanism: Translation head maps vision embedding → graph embedding space. MIM combines: (1) adaptive MSE loss that gradually shifts from fixed graph embedding to translated embedding via decay rate r(e), and (2) InfoNCE contrastive loss pulling positive pairs closer while pushing negatives apart. This creates shared geometry-aware latent space.
- Core assumption: Vision features contain relevant diagnostic information but require explicit structural guidance to focus on clinically relevant patterns
- Evidence anchors:
  - [abstract] "enforces alignment between geometry-informed graph embeddings and radiographic features through mutual information maximization"
  - [section 3.1] "This strategy ensures that the learned representation from image modality is progressively adapts toward the fixed, morphology-rich graph features space"
  - [Table 1] MM-F (MIM) consistently outperforms MM-F Classic across all architectures (+1-2% accuracy)
  - [corpus] No direct corpus evidence for MIM in KOA; limited cross-modal alignment examples in related medical imaging papers
- Break condition: If vision and graph features were already well-aligned or if MIM degraded texture information needed for classification

### Mechanism 3
- Claim: Two-stage training (frozen graph encoder → trainable vision+fusion) preserves morphological priors while adapting to radiographic features
- Mechanism: Pre-train MorphoGraph on graph-only classification → freeze graph encoder → train vision encoder, translation head, and fusion network jointly with L_total = λ_CE * L_CE + λ_Info * (L_InfoNCE + L_MSE) where λ_CE=0.8, λ_Info=0.2. Classification loss captures radiographic cues; MIM losses enforce geometry-aware alignment.
- Core assumption: Graph encoder learns transferable morphological features that should not be corrupted by joint training
- Evidence anchors:
  - [section 3] "integrat[e] this morphological prior with a learnable vision encoder" from "pre-trained (and thus frozen) graph encoder"
  - [section 3.2] "classification term prompts the vision encoder to capture radiographic details essential for accurate grading, while the alignment term drives the image features into a geometry-aware domain"
  - [Table 1] ViT-Large baseline (66.3%) → MM-F MIM (80.8%) shows +14.5% absolute gain
  - [corpus] Corpus papers typically train end-to-end without frozen encoders; no direct comparison available
- Break condition: If graph encoder overfits to graph-only task or if frozen weights prevent adaptation to vision feature space

## Foundational Learning

- Concept: Graph Neural Networks (EdgeConv/Dynamic Graph CNN)
  - Why needed here: Core mechanism for processing bone contour graphs. EdgeConv aggregates local neighborhoods via edge features (relative position + node features) with max pooling. Understanding this is essential for debugging graph construction and feature propagation.
  - Quick check question: Given a bone contour graph with N nodes, what happens to a node's embedding if its k-nearest neighbors all have similar features?

- Concept: Mutual Information Maximization (InfoNCE)
  - Why needed here: Alignment mechanism between vision and graph modalities. InfoNCE treats (z_graph, z_trans) as positive pairs and all other combinations as negatives. Critical for understanding why MIM improves over simple concatenation fusion.
  - Quick check question: In equation (3), what happens to the loss when the temperature τ → 0? When τ → ∞?

- Concept: Cross-Modal Translation with Adaptive Masking
  - Why needed here: Novel technique for gradually transferring learning from fixed graph embeddings to translated vision embeddings. Decay rate r(e) = max(0, 1-e/E) controls this transition. Essential for implementation and hyperparameter tuning.
  - Quick check question: At training step e=E/2, what proportion of z_graph vs z_trans contributes to z_combined?

## Architecture Onboarding

- Component map:
  1. SAM Mask Selection: Input radiograph X → SAM with point grids → candidate masks M → IoU matching with templates T_U, T_L → optimal masks m*U, m*L
  2. Graph Construction: Contour sampling (N points) → k-NN edges with threshold τ → undirected graph G_joint
  3. MorphoGraph Encoder (frozen after pre-training): 3× EdgeConv blocks (dimensionality doubles each layer) → GraphNorm → global mean+max pooling → linear classifier
  4. Vision Encoder: ImageNet-pretrained backbone (ResNet/ConvNeXt/Swin/ViT) → feature embedding z_vision
  5. Translation Head: Linear MLP mapping z_vision → z_trans (same dimension as z_graph)
  6. Fusion Network: Concatenate [z_graph, z_trans] → MLP → z_rep → linear classifier

- Critical path: Radiograph → SAM → mask selection → graph construction → MorphoGraph → z_graph (frozen) || radiograph → vision encoder → z_vision → translation → z_trans → fusion → classification. MIM losses connect z_graph and z_trans during training.

- Design tradeoffs:
  - Graph encoder frozen vs. fine-tuned: Freezing preserves morphological priors but limits adaptation; paper shows frozen works well
  - Template matching vs. learning-based segmentation: Templates provide interpretability but may fail on anatomical outliers
  - EdgeConv depth (3 layers): Deeper could capture higher-order geometry but risks over-smoothing; 3 layers chosen empirically
  - λ_CE=0.8, λ_Info=0.2: Classification prioritized over alignment; different weighting might help if vision features were weaker

- Failure signatures:
  - Low IoU with templates (Section 2.1): SAM generates poor masks → wrong bone regions graphed → noisy morphology features. Check: visualize m* overlap with radiograph.
  - Disconnected graph (Section 2.1): τ too small → isolated nodes → EdgeConv receives empty neighborhoods. Check: verify graph connectivity before training.
  - MIM degrades performance: Translation head overfits to graph space, losing texture information. Check: compare MM-F Classic vs. MM-F MIM; if MIM worse, reduce λ_Info.
  - Large accuracy gap between architectures (Table 1): Vision encoder capacity mismatch with graph features. Check: ensure vision encoder is adequately pretrained.

- First 3 experiments:
  1. Ablation on fusion strategy: Compare (a) vision-only, (b) graph-only, (c) MM-F Classic (concatenation), (d) MM-F MIM. Expect: (d) > (c) > (a) ≈ (b). If not, debug translation head or MIM implementation.
  2. Template matching validation: Manually inspect 50 random samples to verify SAM mask quality and IoU-based selection. If >20% fail, refine templates or try alternative SAM prompts (bounding boxes).
  3. Graph hyperparameter sweep: Vary N (contour points: 32, 64, 128), k (neighbors: 3, 5, 10), τ (distance threshold). Report accuracy on validation set. Expect: moderate sensitivity to N, high sensitivity to τ (affects connectivity).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can dynamic graph embeddings effectively model the temporal progression of KOA severity using longitudinal data?
- Basis in paper: [explicit] The authors state in the Discussion: "Future work will explore dynamic graph embeddings for modeling disease progression."
- Why unresolved: The current study utilizes a static graph representation derived from single time-point radiographs to classify severity, lacking a temporal component to track how morphological changes evolve over time.
- What evidence would resolve it: Experiments on longitudinal cohorts (e.g., follow-up visits in OAI) demonstrating that incorporating temporal dependencies into the graph structure improves the prediction of future KOA grade transitions compared to static baselines.

### Open Question 2
- Question: To what extent can self-supervised pretraining strengthen the cross-modal alignment between vision and graph embeddings compared to the current supervised mutual information maximization?
- Basis in paper: [explicit] The Discussion lists "self-supervised pretraining to strengthen cross-modal alignment" as a specific avenue for future work.
- Why unresolved: The current alignment mechanism relies on supervised mutual information maximization (InfoNCE) and translation modules trained on labeled data, which may not fully exploit the latent structural correspondences present in the vast amount of unlabeled radiographic data.
- What evidence would resolve it: A comparative study showing that a self-supervised pretraining phase (e.g., contrastive learning on unlabeled X-rays and graphs) yields superior embedding alignment and higher classification accuracy than the current supervised-from-scratch approach.

### Open Question 3
- Question: How robust is the anatomical template matching mechanism when presented with severe bone deformations or atypical radiographic views?
- Basis in paper: [inferred] The method relies on selecting an optimal SAM mask by maximizing IoU with "predefined anatomical templates" ($T_U$ and $T_L$), but the paper does not analyze performance on cases where severe pathology or imaging variance might cause low IoU with these fixed templates.
- Why unresolved: If severe osteophytes or joint space narrowing significantly alter the bone contour compared to the generic templates, the initial mask selection step might fail, leading to the construction of a non-representative or disconnected graph.
- What evidence would resolve it: A sensitivity analysis reporting classification performance specifically on "out-of-distribution" samples (e.g., extreme KL grades or rotated images) or cases where the initial template-matching IoU is low, compared to a baseline using ground-truth masks.

### Open Question 4
- Question: Does the graph modality provide orthogonal diagnostic information to the vision encoder, or does it primarily serve as a regularization prior?
- Basis in paper: [inferred] The vision-only baselines show lower performance than the multimodal approach, but the paper does not deeply analyze whether the graph captures unique structural biomarkers that the Vision Transformer (ViT) fails to see, or if it simply forces the model to focus on shape rather than texture.
- Why unresolved: While performance improves, the specific nature of the "complementarity" is not fully quantified; it is unclear if the graph adds new information or merely suppresses noise (texture artifacts) present in the pixel space.
- What evidence would resolve it: An ablation study measuring the mutual information between the graph embeddings and the raw image features, or a "stitch-and-cut" experiment (e.g., *SinGAN* style analysis) to determine if the performance gain persists when texture information is systematically removed or altered.

## Limitations
- The frozen graph encoder assumption prevents adaptation to vision-specific feature spaces, which may be suboptimal for certain anatomical configurations
- Computational overhead of SAM-based segmentation and graph construction may limit real-time clinical deployment
- Template-based SAM mask selection strategy effectiveness remains untested on diverse anatomical variations beyond the OAI dataset

## Confidence
- High: Graph encoding captures clinically relevant morphological features (supported by ablation showing graph-only performance of 74.94%)
- Medium: MIM alignment consistently improves fusion performance across architectures (shown by +1-2% gains, but limited cross-modal alignment literature)
- Low: Frozen graph encoder is optimal for this task (assumption-driven choice without ablation; may depend on dataset size and diversity)

## Next Checks
1. Test template matching on external KOA datasets with different demographic distributions to assess generalizability
2. Perform ablation comparing frozen vs. fine-tuned graph encoders on validation accuracy and alignment quality
3. Evaluate computational efficiency on clinical hardware and test real-time inference constraints