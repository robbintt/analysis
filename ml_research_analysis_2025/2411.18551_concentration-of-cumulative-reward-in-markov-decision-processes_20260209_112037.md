---
ver: rpa2
title: Concentration of Cumulative Reward in Markov Decision Processes
arxiv_id: '2411.18551'
source_url: https://arxiv.org/abs/2411.18551
tags:
- have
- reward
- proof
- policy
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the concentration properties of cumulative
  rewards in Markov Decision Processes (MDPs), establishing both asymptotic and non-asymptotic
  results across three key frameworks: average reward, discounted reward, and finite-horizon
  settings. The authors introduce a unified approach based on martingale decomposition
  to analyze reward concentration, deriving law of large numbers, central limit theorem,
  and law of iterated logarithms for asymptotic behavior, along with Azuma-Hoeffding-type
  inequalities and non-asymptotic law of iterated logarithms.'
---

# Concentration of Cumulative Reward in Markov Decision Processes

## Quick Facts
- **arXiv ID**: 2411.18551
- **Source URL**: https://arxiv.org/abs/2411.18551
- **Reference count**: 40
- **Primary result**: Establishes both asymptotic and non-asymptotic concentration bounds for cumulative rewards across average-reward, discounted-reward, and finite-horizon MDP frameworks

## Executive Summary
This paper provides a comprehensive analysis of concentration properties for cumulative rewards in Markov Decision Processes. The authors develop a unified martingale decomposition approach that transforms reward concentration into martingale concentration problems, yielding both asymptotic results (law of large numbers, central limit theorem, law of iterated logarithms) and non-asymptotic bounds (Azuma-Hoeffding-type inequalities). The analysis covers three key MDP frameworks: average reward, discounted reward, and finite-horizon settings, with results applicable to recurrent, unichain, communicating, and weakly communicating MDPs. The derived bounds depend on the span and maximum absolute deviation of value functions, providing both policy-dependent and policy-independent guarantees.

## Method Summary
The core methodology relies on decomposing cumulative reward into a martingale difference sequence plus boundary terms involving the value function. For average-reward MDPs, this decomposition yields a bounded martingale sequence that can be analyzed using standard concentration inequalities. The authors establish both asymptotic properties (LLN, CLT, LIL) and non-asymptotic bounds (Azuma-Hoeffding-type inequalities) for this martingale. They further extend the analysis to discounted and finite-horizon settings through appropriate policy evaluation equations. The bounds are characterized in terms of the span H^π and maximum absolute deviation K^π of the value function, with additional policy-independent bounds derived using the model diameter for communicating MDPs.

## Key Results
- Sample path concentration bounds that depend on the span and maximum absolute deviation of value functions
- Policy-independent bounds for communicating MDPs using model diameter
- Characterization of performance differences between any two stationary policies showing the difference is upper-bounded by O(√T) with high probability
- Demonstration that two alternative definitions of regret (cumulative regret and interim cumulative regret) are rate-equivalent, both growing at most as Õ(√T)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The cumulative reward process can be decomposed into a martingale difference sequence (MDS) plus value function boundary terms, enabling direct application of concentration inequalities for bounded martingales.
- **Mechanism:** For any policy π ∈ Π^AR, the cumulative reward R_T^π decomposes as: R_T^π = TJ^π + Σ_t M_t^π + V^π(S_0) - V^π(S_T), where M_t^π is an MDS bounded by K^π (maximum absolute deviation of V^π). This transforms reward concentration into martingale concentration, yielding explicit non-asymptotic bounds via Azuma-Hoeffding and finite-time LIL.
- **Core assumption:** The policy π satisfies the Average Reward Policy Evaluation equation (ARPE), guaranteeing existence of a well-defined value function V^π. For the bounds to be computable, K^π must be finite (implied by AROE solvability per Assumption 1).
- **Evidence anchors:**
  - [Section 3, Definition 65, Lemma 66-67]: Martingale decomposition explicitly defined and proven
  - [Section 2, Definition 3]: ARPE equation defines the class Π^AR for which results hold
  - [corpus]: Weak — no corpus papers directly replicate this decomposition, though paper 6883 ("Efficient Q-Learning...for Robust Average Reward RL") addresses average-reward MDPs with non-asymptotic analysis but uses different techniques (robust Q-operators)
- **Break condition:** If the underlying Markov chain induced by π is not ergodic (e.g., multiple recurrent classes without unichain assumption), V^π may not exist uniquely, and K^π could be unbounded, invalidating the Azuma-Hoeffding bounds

### Mechanism 2
- **Claim:** The span H^π and maximum absolute deviation K^π of the differential value function directly control the tightness of concentration bounds, and for communicating MDPs, these quantities are bounded by policy diameter times max reward.
- **Mechanism:** By Lemma 13, K^π ≤ H^π ≤ D^π R_max for communicating MDPs, where D^π is the policy diameter. This provides policy-independent bounds using the model diameter D when considering optimal policies in weakly communicating MDPs (H^* ≤ DR_max). The bound scales linearly with diameter, making wide/slow-mixing MDPs have looser concentration.
- **Core assumption:** The MDP is communicating or weakly communicating (Definition 7). For general MDPs, K^π must be computed numerically from the ARPE solution.
- **Evidence anchors:**
  - [Section 3.1, Lemma 13]: Explicit inequality chain σ^π ≤ K^π ≤ H^π ≤ D^π R_max for communicating case
  - [Corollary 22-23]: Policy-dependent bounds using D^π R_max; optimal policy bounds using D R_max
  - [corpus]: Weak — corpus papers don't explicitly connect span/diameter to concentration; paper 87561 ("Achieving ε^{-2} Dependence for Average-Reward Q-Learning") mentions span-norm contractions but in a different context (Q-learning convergence rates)
- **Break condition:** For non-communicating MDPs with large transient regions or multiple closed classes, the diameter-based bound may be arbitrarily loose or undefined, requiring direct computation of H^π from ARPE solution

### Mechanism 3
- **Claim:** The difference between cumulative regret and interim cumulative regret (D_T(ω)) grows at most Õ(√T), making the two regret definitions rate-equivalent for algorithms with Õ(√T) regret.
- **Mechanism:** Since D_T = R_T^{π^*} - TJ^* depends only on the optimal policy's concentration around its mean, Theorem 29-30 apply martingale concentration to show |D_T| ≤ K^*√(2T log(2/δ)) + H^* with probability ≥ 1-δ. This bridges sample-path regret (comparing learning algorithm to optimal on same trajectory) with expected-reward-based regret definitions.
- **Core assumption:** The optimal policy π^* exists and belongs to Π^AR (guaranteed for recurrent, unichain, communicating, or weakly communicating MDPs per Proposition 9). The learning algorithm's regret under one definition is Õ(√T).
- **Evidence anchors:**
  - [Section 3.4, Theorem 33]: Formal equivalence proof with asymptotic and non-asymptotic statements
  - [Section 2.4, Equations 1-3]: Definitions of interim cumulative regret and cumulative regret
  - [corpus]: Moderate — paper 84065 ("Online MDPs with Terminal Law Constraints") addresses non-asymptotic performance without resets, touching on similar infinite-horizon concerns, but doesn't analyze regret definitions
- **Break condition:** If the optimal policy has large H^* (e.g., in highly disconnected state spaces), the Õ(√T) equivalence may have impractically large constants, making the equivalence theoretically valid but operationally weak

## Foundational Learning

- **Concept: Martingale Difference Sequences and Azuma-Hoeffding Inequality**
  - **Why needed here:** The entire proof strategy relies on representing cumulative reward as a sum of bounded martingale differences and applying concentration inequalities. Appendix B provides background.
  - **Quick check question:** Given a sequence Y_t with |Y_t| ≤ c_t and E[Y_t | F_{t-1}] = 0, what is the Azuma-Hoeffding bound for P(|Σ Y_t| ≥ ε)?

- **Concept: Differential/Bias Value Functions in Average-Reward MDPs**
  - **Why needed here:** The key parameters K^π and H^π are derived from V^π, which satisfies λ^π + V^π(s) = r(s, π(s)) + E[V^π(S^+) | s, π(s)]. Understanding this fixed-point equation is essential.
  - **Quick check question:** Why is V^π unique only up to an additive constant in the average-reward setting?

- **Concept: MDP Diameter and Communicating Structure**
  - **Why needed here:** The policy-independent bounds rely on diameter D (minimum expected steps between any two states under some policy). This connects structural MDP properties to concentration rates.
  - **Quick check question:** For a 5-state chain MDP with deterministic transitions 1→2→3→4→5, what is the diameter D?

## Architecture Onboarding

- **Component map:**
  [Policy π] → Induces Markov chain → [ARPE/DRPE/FHPE solver] → Computes V^π
      ↓
  [Trajectory S_0:T, A_0:T-1] → Martingale decomposition (Eq. 58) → [MDS sequence M_t^π]
      ↓
  [Concentration module] ← K^π, H^π from V^π ← [Statistical properties calculator]
      ↓
  [Output: Concentration bounds (Theorem 17/18/37/48)]

- **Critical path:**
  1. Verify MDP class (recurrent/unichain/communicating) to determine applicable theorems
  2. Solve policy evaluation equation (ARPE for average-reward, DRPE for discounted, FHPE for finite-horizon) to obtain V^π
  3. Compute K^π = max_{s,s^+} |V^π(s^+) - E[V^π(S^+)|s,π(s)]| and H^π = sp(V^π)
  4. Apply appropriate theorem (Theorem 18 for average-reward, Theorem 37 for discounted, Theorem 48 for finite-horizon)

- **Design tradeoffs:**
  - **Exact K^π/H^π computation vs. diameter bounds:** Exact computation requires solving ARPE and evaluating max over state pairs; diameter bounds (Corollary 22-23) are looser but require only model diameter D
  - **Policy-dependent vs. policy-independent:** Policy-dependent bounds (D^π R_max) are tighter for specific policies; policy-independent bounds (D R_max) apply to all optimal policies but may be significantly looser
  - **Sample-path vs. expected bounds:** Sample-path bounds (Theorem 17) include trajectory-dependent V^π(S_T) term; expected bounds (Theorem 18) add H^π to handle worst-case terminal state

- **Failure signatures:**
  - **K^π computation returns NaN/Inf:** MDP likely not unichain; V^π may not exist or be unbounded
  - **Bounds exceeding empirical variance by orders of magnitude:** Diameter D or H^π vastly overestimates actual mixing; consider tighter policy-specific computation
  - **Regret equivalence claim fails empirically:** Learning algorithm may not satisfy Õ(√T) regret under either definition; verify regret rate first

- **First 3 experiments:**
  1. **Validate martingale decomposition on simple MDP:** Implement the decomposition for a 3-state ergodic MDP with known V^π; verify that Σ M_t^π + boundary terms exactly equals R_T^π - TJ^π
  2. **Compare K^π bounds vs. diameter bounds:** For a 10-state communicating MDP, compute exact K^π and compare concentration bounds from Theorem 18 vs. Corollary 22; quantify looseness introduced by diameter approximation
  3. **Empirical regret equivalence test:** Run UCRL2 or similar algorithm on a weakly communicating MDP; compute both R_T^μ and R̄_T^μ; verify |R_T^μ - R̄_T^μ| ≤ Õ(√T) empirically matches the theoretical D_T bounds

## Open Questions the Paper Calls Out

- **Question:** Can the derived concentration bounds for cumulative reward with stochastic (random) rewards be formally extended to the infinite-horizon discounted and finite-horizon frameworks?
- **Basis:** The conclusion states that while the proof technique for random rewards was applied to the average reward setup, these extensions to discounted and finite-horizon frameworks "are omitted for brevity."
- **Why unresolved:** The paper provides the result for the average-reward setting but leaves the derivation for the other two key frameworks as an unproven implication of the proof technique.
- **What evidence would resolve it:** Formal proofs of concentration bounds for R̃^π_T in discounted and finite-horizon settings, analogous to Theorem 54.

## Limitations

- **Assumption sensitivity:** The concentration bounds rely heavily on the ergodicity of the policy-induced Markov chain, breaking down for non-unichain MDPs with multiple recurrent classes.
- **Bound looseness:** Policy-independent bounds using diameter D can be extremely loose for MDPs with high diameter, as K^π and H^π scale linearly with D.
- **Regret rate assumption:** The equivalence between regret definitions assumes the learning algorithm achieves Õ(√T) regret, which may not hold for algorithms with different regret rates.

## Confidence

- **High confidence**: The martingale decomposition approach and its application to derive concentration bounds (Theorem 17-18) - the mathematical framework is rigorous and follows standard martingale concentration techniques
- **Medium confidence**: The policy-independent bounds using diameter D (Corollary 22-23) - while the theoretical derivation is sound, the looseness for large-diameter MDPs makes practical utility uncertain
- **Low confidence**: The regret equivalence claim for algorithms with non-√T regret rates - the proof assumes specific regret scaling that may not generalize

## Next Checks

1. **Validate martingale decomposition numerically**: Implement the decomposition for a 3-state ergodic MDP with known V^π; verify that Σ M_t^π + boundary terms exactly equals R_T^π - TJ^π across 10,000 simulated trajectories.

2. **Bound tightness analysis**: For a 10-state communicating MDP, compute exact K^π and compare concentration bounds from Theorem 18 vs. Corollary 22; measure the empirical 95th percentile deviation and quantify how much the diameter-based bound exceeds it.

3. **Regret equivalence empirical test**: Run UCRL2 on a weakly communicating MDP; compute both R_T^μ and R̄_T^μ for T ∈ {10^2, 10^3, 10^4}; verify |R_T^μ - R̄_T^μ| ≤ K^*√(2T log(2/δ)) empirically matches theoretical bounds and check if the growth rate remains Õ(√T).