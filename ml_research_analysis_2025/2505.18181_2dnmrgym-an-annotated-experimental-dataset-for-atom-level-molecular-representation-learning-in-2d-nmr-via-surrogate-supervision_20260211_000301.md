---
ver: rpa2
title: '2DNMRGym: An Annotated Experimental Dataset for Atom-Level Molecular Representation
  Learning in 2D NMR via Surrogate Supervision'
arxiv_id: '2505.18181'
source_url: https://arxiv.org/abs/2505.18181
tags:
- molecular
- chemical
- dataset
- hsqc
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 2DNMRGym is the first annotated experimental dataset for atom-level
  molecular representation learning in 2D NMR spectroscopy. It contains over 22,000
  HSQC spectra with algorithm-generated silver labels and 479 expert-annotated spectra
  serving as gold-standard evaluation data.
---

# 2DNMRGym: An Annotated Experimental Dataset for Atom-Level Molecular Representation Learning in 2D NMR via Surrogate Supervision

## Quick Facts
- arXiv ID: 2505.18181
- Source URL: https://arxiv.org/abs/2505.18181
- Reference count: 28
- Primary result: First annotated experimental dataset enabling atom-level representation learning for 2D NMR via surrogate supervision

## Executive Summary
2DNMRGym is the first annotated experimental dataset enabling atom-level molecular representation learning for 2D NMR spectroscopy. It contains over 22,000 HSQC spectra with algorithm-generated "silver" labels and 479 expert-annotated spectra serving as gold-standard evaluation data. The dataset uniquely enables surrogate supervision, where models are trained on imperfect algorithmic annotations and evaluated on expert-labeled ground truth. Benchmark experiments with GNN and GNN-Transformer models achieve chemical shift prediction errors of ~2-3 ppm for 13C and ~0.2-0.3 ppm for 1H. GIN-based architectures with transformer components perform best, demonstrating the dataset's utility for advancing atom-level representation learning in NMR-guided molecular structure analysis.

## Method Summary
2DNMRGym provides a novel framework for training molecular representation models on 2D NMR data by leveraging surrogate supervision. The dataset contains 21,869 HSQC spectra with algorithm-generated silver labels for training and 479 expert-annotated spectra for evaluation. Molecules are represented as graphs with node features (atomic type, chirality, hybridization) and edge features (bond type, direction) derived from SMILES via RDKit. Models predict 13C and 1H chemical shifts for C-H pairs, with chemical shifts normalized to [0,1] by dividing 13C by 200 and 1H by 10. Training uses MAE loss with equal weights for both shift types, following an 80/20 split between training and validation on silver data, with evaluation on gold-labeled test sets.

## Key Results
- GNN-Transformer models achieve 13C MAE of 2.370 ppm and 1H MAE of 0.2315 ppm on gold-standard test data
- GIN-based architectures outperform GCN and GAT variants across all-shot settings
- Surrogate supervision enables scalable ML training despite annotation bottlenecks, with silver labels achieving 95.21% full-molecule accuracy
- 2D molecular topology captures sufficient information for HSQC cross-peak prediction, outperforming 3D geometric models

## Why This Works (Mechanism)

### Mechanism 1: Surrogate Supervision for Scalable ML Training
Surrogate supervision enables scalable ML training for 2D NMR despite annotation bottlenecks. Algorithm-generated silver labels provide abundant supervision for training, while a smaller expert-annotated gold set enables rigorous evaluation of generalization from imperfect to expert-level interpretation. The core assumption is that silver labels capture sufficient signal about atom-to-peak mappings to enable meaningful representation learning despite being imperfect. Evidence shows silver labels achieve 95.21% full-molecule accuracy on test sets, with partial-correct molecules having 81.56% peak accuracy.

### Mechanism 2: 2D Molecular Topology Sufficiency
2D molecular topology captures sufficient information for HSQC cross-peak prediction, outperforming 3D geometric models. HSQC correlations are governed by short-range electronic environments (connectivity, atom types, hybridization, chirality) directly encoded in molecular graphs. Single RDKit-embedded conformers may introduce spurious 3D patterns that 3D GNNs overfit. Evidence shows 2D GIN (2.370 MAE 13C) outperforms 3D ComENet (3.143) and SchNet (3.156), supporting the claim that HSQC spectra primarily reflect short-range correlations governed by 2D molecular structure.

### Mechanism 3: GIN Injective Aggregation
GIN's injective aggregation preserves atom-level discriminative power needed for NMR shift prediction. GIN uses injective aggregation functions (via MLP with learnable Îµ) that better preserve node uniqueness compared to GCN's normalized summation or GAT's attention-weighted averaging. This is critical for distinguishing atoms in subtly different chemical environments. Evidence shows GIN variants consistently outperform GCN and GAT across all-shot settings, supporting the claim that atom-level distinctions in NMR-relevant environments require maximum graph isomorphism expressiveness.

## Foundational Learning

- **HSQC Spectroscopy Basics**: Understanding heteronuclear single quantum coherence (HSQC) as a 2D NMR technique that correlates 1H and 13C nuclei directly bonded to each other. Why needed: Forms the basis for understanding the correlation patterns the models must learn to predict. Quick check: Can identify direct C-H correlations in HSQC spectra.

- **Chemical Shift Fundamentals**: Chemical shifts reflect the electronic environment around nuclei and are measured in ppm relative to reference compounds. Why needed: Models must learn to predict these shifts based on molecular structure. Quick check: Understand how electronegativity, hybridization, and neighboring groups affect chemical shifts.

- **Graph Neural Networks (GNNs)**: Deep learning architectures that operate on graph-structured data using message passing between nodes. Why needed: The primary modeling framework for learning atom-level representations from molecular graphs. Quick check: Can explain how node features are updated through message passing.

- **Surrogate Supervision**: Training machine learning models using imperfect algorithmic annotations when expert labels are scarce. Why needed: Enables large-scale training despite limited expert annotations. Quick check: Understand the trade-offs between silver label quantity and gold label quality.

## Architecture Onboarding

**Component Map**: SMILES -> RDKit Graph -> GNN/GNN-Transformer -> Chemical Shift Predictions

**Critical Path**: Molecular Graph Construction -> Message Passing (GIN/GCN/GAT) -> Transformer Integration (optional) -> Shift Prediction

**Design Tradeoffs**: 
- 2D topology vs. 3D geometry: 2D GIN performs better than 3D models for HSQC prediction
- Aggregation function choice: GIN's injective aggregation outperforms GCN/GAT for NMR shifts
- Silver vs. gold labels: Large silver dataset enables training, small gold set enables evaluation

**Failure Signatures**:
- High 13C MAE (>5 ppm) suggests insufficient learning of carbon environments
- Large gap between silver and gold performance indicates poor generalization
- Similar performance across architectures suggests dataset limitations rather than model capacity

**First Experiments**:
1. Train GIN baseline on silver labels and evaluate on held-out silver validation set
2. Compare 2D GIN vs 3D ComENet performance on the same dataset split
3. Vary the ratio of silver to gold labels in training to quantify learning curve

## Open Questions the Paper Calls Out
None

## Limitations
- Silver label quality may be insufficient for certain chemical classes, potentially biasing learned representations
- Dataset focuses exclusively on HSQC spectra, limiting generalizability to other 2D NMR experiments
- Chemical space coverage may be limited due to pharmaceutical source bias

## Confidence
**Confidence Assessment**: High confidence in dataset creation and benchmark results, Medium confidence in surrogate supervision generalization claims, Low confidence in mechanism-specific architectural claims without broader ablation studies.

## Next Checks
1. Perform detailed error analysis by molecular scaffold to identify systematic error patterns in silver labels
2. Test model generalization to other 2D NMR experiments (HMBC, COSY, NOESY) without additional fine-tuning
3. Systematically vary the ratio of silver to gold labels in training to quantify the learning curve and identify diminishing returns