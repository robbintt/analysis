---
ver: rpa2
title: On Integrating Large Language Models and Scenario-Based Programming for Improving
  Software Reliability
arxiv_id: '2509.09194'
source_url: https://arxiv.org/abs/2509.09194
tags:
- scenario
- agent
- llms
- scenario-based
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the unreliability of large language models
  (LLMs) in generating correct software code. It proposes combining LLMs with Scenario-Based
  Programming (SBP) to improve software reliability.
---

# On Integrating Large Language Models and Scenario-Based Programming for Improving Software Reliability

## Quick Facts
- **arXiv ID:** 2509.09194
- **Source URL:** https://arxiv.org/abs/2509.09194
- **Reference count:** 40
- **Primary result:** Combining LLMs with Scenario-Based Programming (SBP) produced a Connect4 agent that won 100% of matches against three strong AI opponents and enabled formal verification of its correctness in certain game scenarios.

## Executive Summary
This paper addresses the fundamental unreliability of large language models (LLMs) in generating correct software code by proposing a hybrid methodology that combines LLMs with Scenario-Based Programming (SBP). The approach involves decomposing system requirements into discrete, modular scenarios that human developers define, then using LLMs to generate code incrementally for each scenario. The methodology was evaluated by creating a Connect4 game-playing agent that outperformed three strong AI opponents and achieved formal verification of correctness in specific game scenarios. The hybrid approach demonstrates that LLM-generated code can be made both reliable and formally verifiable when structured through SBP semantics.

## Method Summary
The methodology follows a structured four-step process: First, developers decompose system requirements into discrete, modular scenarios (Step 1). Second, they provide the LLM with background knowledge about SBP semantics and domain context through carefully crafted prompts (Step 2). Third, the LLM generates code incrementally for one scenario object at a time, which is then reviewed and verified before integration (Step 3). Finally, the process iterates, using formal verification results to identify missing strategies and prompt the LLM for additional scenarios (Step 4). The approach was implemented using the BPpy library and evaluated through creating a Connect4 agent, with performance measured against benchmark AI opponents and correctness verified through model checking.

## Key Results
- The Connect4 agent won 100% of matches against three strong AI opponents: "Pro Player," "Unbeatable AI," and "Keith Galli"
- Formal verification proved the agent's correctness in specific game scenarios using fixed opening sequences
- The hybrid approach successfully generated reliable and verifiable software components, demonstrating the effectiveness of combining LLMs with SBP semantics

## Why This Works (Mechanism)

### Mechanism 1
Decomposing system requirements into discrete, modular "scenario objects" aligns LLM generation with localized logic, reducing hallucination frequency common in holistic code generation. The methodology mandates that developers decompose goals into narrow scenarios (Step 1). The LLM is then prompted to generate code for only one scenario object at a time (Step 3). This constrains the context window, preventing the LLM from attempting complex global planning which it often fails, instead focusing probabilistic generation on isolated behavioral units.

### Mechanism 2
Utilizing Scenario-Based Programming (SBP) semantics creates an architecture amenable to formal verification, allowing developers to mathematically prove correctness properties of LLM-generated code that would otherwise be intractable to test. SBP structures code as concurrent scenario threads synchronizing on events. This explicit state-transition structure allows off-the-shelf model checkers (like BPPy's DFSBProgramVerifier) to explore state spaces exhaustively. The paper uses this to verify winning strategies in Connect4, transforming LLM "black box" code into a verifiable "white box" model.

### Mechanism 3
Extending standard SBP semantics with "event-specific priorities" allows the system to resolve conflicts between aggressive strategies and safety rules, a nuance identified via LLM interaction. Standard SBP allows scenarios to request or block events. The authors (prompted by LLM suggestions) introduced a priority system where a high-priority request (e.g., "Win Immediately") can override a low-priority block (e.g., "Avoid this move because it might lead to a fork later"). This "soft blocking" ensures the agent doesn't play safely when it can win.

## Foundational Learning

- **Concept: Scenario-Based Programming (SBP)**
  - Why needed here: This is the architectural substrate. Unlike standard imperative coding, you must think in terms of concurrent "scenarios" (threads) that request, wait for, or block events. You cannot simply write a `main()` loop; you must define the protocol of interaction.
  - Quick check question: Can you explain the difference between a "hard block" and a "soft block" (priority-based block) in the context of the paper's Connect4 agent?

- **Concept: Model Checking / Formal Verification**
  - Why needed here: The paper claims reliability not just through testing, but through mathematical proofs of correctness for specific game traces. You need to understand that "verification" here means exhaustively checking all possible future states against a specification (e.g., "Yellow always wins").
  - Quick check question: Why did the authors have to restrict the verification to "fixed opening sequences" instead of verifying the entire game from the start?

- **Concept: Iterative Prompt Engineering**
  - Why needed here: The methodology relies on a "query-review-refine" cycle. You must learn how to provide "Background Knowledge" (the preamble) and how to correct the LLM when it hallucinates functionality without introducing new errors.
  - Quick check question: According to the paper, why is it critical to provide the LLM with a preamble about SBP specifically?

## Architecture Onboarding

- **Component map:**
  - Scenario Objects (independent Python generators using @bp.thread) -> BPPy Runtime (engine that collects requests/blocks and selects final event) -> Verifier (DFSBProgramVerifier for model checking) -> Event Bus (implicit communication layer)

- **Critical path:**
  1. **Define Scenarios:** Map requirements to specific scenario objects (e.g., "Block Horizontal Win")
  2. **Context Setup:** Prompt LLM with BPPy syntax and domain rules (the "Preamble")
  3. **Generate & Verify:** Generate code for one scenario → Run unit tests → Run formal verifier (if applicable) → Integrate
  4. **Refine:** If verifier finds a counterexample (Red wins), explain the trace to LLM to generate a blocking scenario

- **Design tradeoffs:**
  - **Modularity vs. State Visibility:** SBP makes scenarios modular, but individual scenarios have limited visibility of the global board state. The paper relies on a global list of placement events to mitigate this.
  - **Verifiability vs. Scalability:** The system is highly verifiable, but as noted in the paper, the state space grows rapidly, requiring "fixed opening sequences" to make verification tractable.

- **Failure signatures:**
  - **The "Suicidal Block":** A strategy scenario blocks a move because it looks dangerous (e.g., sets up a fork for the opponent), but that move was actually an immediate winning move. (Resolved by the Priority extension)
  - **Hallucinated Methods:** LLM generates BPPy code using methods that do not exist in the library. (Resolved by explicit "Background Knowledge" in the prompt)

- **First 3 experiments:**
  1. **Hello World in BPPy:** Implement the "Hot/Cold" water example from Page 3 to understand `request`, `waitFor`, and `block` semantics.
  2. **Simple Strategy Isolation:** Create a single scenario object for Connect4 (e.g., "Center Column Priority") and verify it runs without errors in isolation.
  3. **Adversarial Test:** Run the agent against one of the "Unbeatable" AIs mentioned (or a simple random agent) and observe if the priority system correctly allows a "Win" event to override a "Block" event.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can model checking techniques be optimized to handle the state space explosion inherent in complex scenario-based programs?
- Basis in paper: [explicit] The authors explicitly state the need for "more scalable and efficient model checking techniques tailored to SBP" in the Future Work section.
- Why unresolved: In the evaluation, the state space of Connect4 was too large for current verification tools, forcing the authors to restrict verification to fixed opening sequences rather than the full game tree.
- What evidence would resolve it: The development of verification algorithms capable of fully analyzing the Connect4 agent's strategy or similarly complex systems without requiring manual state space restrictions.

### Open Question 2
- Question: To what extent does the proposed LLM-SBP methodology transfer to domains outside of game playing, such as robotics or smart environments?
- Basis in paper: [explicit] The Future Work section lists "robotics, smart environments, and human-computer interaction" as target domains for future application.
- Why unresolved: The paper's evaluation is limited to a single case study involving a Connect4 game agent, leaving the efficacy of the method in other domains unproven.
- What evidence would resolve it: Successful case studies in the specified domains (e.g., a robotics controller) demonstrating that the method retains its reliability and verifiability advantages.

### Open Question 3
- Question: Would creating LLMs specifically adapted (fine-tuned) for scenario-based reasoning improve the code generation process?
- Basis in paper: [explicit] The authors envision the "creation of LLM agents specifically adapted for scenario-based reasoning... by fine-tuning on SBP artifacts."
- Why unresolved: The study relied on general-purpose models (GPT-4 and Claude), which the authors noted sometimes struggled due to limited SBP-specific knowledge in public training data.
- What evidence would resolve it: A comparative study measuring the reduction in hallucinations and the improvement in code correctness when using an SBP-specialized LLM versus a general-purpose model.

## Limitations

- **Unverified generalization:** The SBP + LLM methodology has been validated on only one domain (Connect4). Claims about improved reliability across diverse software domains remain speculative without broader empirical testing.
- **Manual bottleneck:** Despite automation in code generation, the methodology requires significant human involvement in scenario decomposition, code review, and priority assignment. The paper does not quantify the reduction in manual effort compared to traditional development.
- **State space constraints:** Formal verification was only possible by restricting games to fixed opening sequences, suggesting the approach may not scale to fully verify complex systems without similar constraints.

## Confidence

- **High confidence:** The core mechanism of decomposing requirements into scenarios and generating code incrementally is clearly described and demonstrated. The Connect4 performance results (100% win rate) are specific and reproducible.
- **Medium confidence:** The theoretical benefits of combining SBP with LLMs for reliability are plausible but not extensively validated. The claim that SBP semantics enable formal verification of LLM-generated code is novel but tested only in narrow scenarios.
- **Low confidence:** Claims about broader applicability to general software development lack empirical support beyond the single case study.

## Next Checks

1. **Cross-domain replication:** Apply the methodology to a different domain (e.g., a text-based game or simple web application) to test generalizability of the SBP + LLM approach.
2. **Effort quantification:** Measure and compare the total human effort (scenario definition, code review, testing) against traditional development for equivalent functionality.
3. **Verification scalability test:** Systematically vary game complexity (board size, number of pieces needed to win) to empirically determine the limits of formal verification feasibility.