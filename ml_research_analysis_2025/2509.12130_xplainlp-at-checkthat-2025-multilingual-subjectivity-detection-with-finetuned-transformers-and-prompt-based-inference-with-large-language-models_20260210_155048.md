---
ver: rpa2
title: 'XplaiNLP at CheckThat! 2025: Multilingual Subjectivity Detection with Finetuned
  Transformers and Prompt-Based Inference with Large Language Models'
arxiv_id: '2509.12130'
source_url: https://arxiv.org/abs/2509.12130
tags:
- subjectivity
- language
- detection
- training
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes the XplaiNLP submission to the CheckThat!
  2025 shared task on multilingual subjectivity detection.
---

# XplaiNLP at CheckThat! 2025: Multilingual Subjectivity Detection with Finetuned Transformers and Prompt-Based Inference with Large Language Models

## Quick Facts
- arXiv ID: 2509.12130
- Source URL: https://arxiv.org/abs/2509.12130
- Reference count: 40
- Primary result: 1st place in Italian monolingual subjectivity detection (F1 = 0.8104) using LLM prompting

## Executive Summary
This paper presents XplaiNLP's participation in the CheckThat! 2025 shared task on multilingual subjectivity detection. The authors evaluate two distinct approaches: fine-tuning transformer encoders (EuroBERT, XLM-RoBERTa, German-BERT) on monolingual and machine-translated data, and zero-shot prompting using large language models (o3-mini and gpt-4.1-mini) with rule-based instructions. The LLM-based Annotation approach achieved top performance in Italian (F1 = 0.8104), while XLM-RoBERTa fine-tuned on Romanian data ranked 3rd (F1 = 0.7917). The work demonstrates that explicit linguistic rule prompting can outperform supervised fine-tuning in high-resource settings, while multilingual encoders provide reliable cross-lingual transfer in zero-shot scenarios.

## Method Summary
The authors employed two complementary approaches: (1) supervised fine-tuning of transformer encoders (EuroBERT, XLM-RoBERTa, German-BERT) using Focal Loss and class weighting to handle label imbalance, with a reduced classification threshold of 0.45 for the minority Subjective class; (2) zero-shot prompting with o3-mini and gpt-4.1-mini using explicit linguistic decision rules (14 rules from Antici et al. [22]) to guide subjectivity classification. German-BERT was fine-tuned on translated data from related languages (English, Italian, Bulgarian), with experiments showing performance degradation when distant languages (Arabic) were added. For LLM inference, the Annotation strategy applied all rules to the input, while DoubleDown and Perspective strategies used multi-step reasoning, though the simpler Annotation approach proved most effective.

## Key Results
- Annotation approach with o3-mini achieved 1st place in Italian monolingual subtask (F1 = 0.8104), outperforming baseline of 0.6941
- XLM-RoBERTa fine-tuned on Romanian data obtained F1 = 0.7917, ranking 3rd and exceeding baseline of 0.6461
- Same XLM-RoBERTa model improved over baseline in Greek zero-shot setting
- German-BERT fine-tuned on translated data from related languages yielded competitive performance
- Ukrainian and Polish zero-shot performance fell slightly below baselines

## Why This Works (Mechanism)

### Mechanism 1: Rule-Guided Instruction Following
The LLM (o3-mini) processes input text against 14 predefined decision rules (e.g., identifying irony, evaluative framing) within the prompt context. Instead of relying on implicit patterns, the model uses its reasoning capacity to apply these explicit definitions, generating a structured verdict. The model must possess sufficient intrinsic language understanding to interpret the rules and input text accurately, with subjectivity cues aligning with provided definitions. Performance degrades if the model fails to retrieve or apply complex rules consistently, or if subjectivity relies on sarcasm and pragmatic devices difficult to capture via static rules.

### Mechanism 2: Cross-Lingual Representation Transfer
Fine-tuning multilingual transformers (XLM-RoBERTa) on available language data allows generalization to unseen languages in zero-shot settings. The model aligns subjectivity features into a shared multilingual embedding space during pre-training and fine-tuning. When presented with a zero-shot language (e.g., Romanian), the model maps input to this shared space where the decision boundary learned from other languages remains effective. Generalization fails when the target language has low coverage in pre-training data or diverges significantly in subjectivity expression.

### Mechanism 3: Typology-Aware Data Augmentation
Fine-tuning monolingual models on machine-translated data from typologically related languages improves performance, but including distant languages introduces noise that degrades the model. Translating training data from related languages expands training examples, helping the model generalize. For distant languages, translation artifacts or semantic shifts likely introduce label noise or conflicting syntactic patterns, disrupting the learned decision boundary. The mechanism relies on accurate translation; if translation quality is low or source dataset has different class distributions, augmentation creates negative transfer effects.

## Foundational Learning

- **Concept: Zero-Shot vs. Fine-Tuning Trade-offs**
  - Why needed here: The paper explicitly compares these two paradigms, showing that prompting can win in high-resource scenarios (Italian) while fine-tuning wins in cross-lingual transfer (Romanian).
  - Quick check question: Does the target language have sufficient training data to update model weights, or does it rely entirely on the model's pre-existing knowledge and reasoning capabilities?

- **Concept: Class Imbalance and Calibration**
  - Why needed here: The paper addresses significant label skew (e.g., Italian data is >73% Objective) using Focal Loss and a reduced classification threshold (0.45).
  - Quick check question: If a dataset is 80% Objective, why might a standard classifier with a 0.5 threshold still underperform on the minority Subjective class?

- **Concept: Context Windows in Classification**
  - Why needed here: The authors identify "Ambiguities" where isolated sentences lose context needed to detect irony or sarcasm, limiting model performance.
  - Quick check question: How does the "context window" of the input (a single sentence vs. paragraph) constrain the ability to detect pragmatic devices like sarcasm?

## Architecture Onboarding

- **Component map:** Raw sentence + Language ID -> Router (Annotation vs Encoder) -> Annotation Path: Prompt Constructor (Rules + Sentence) -> API (o3-mini/gpt-4.1-mini) -> JSON Parser -> Label; Encoder Path: XLM-RoBERTa/German-BERT -> Linear Classifier -> Threshold (0.45) -> Label

- **Critical path:** The fine-tuning pipeline for XLM-RoBERTa is the most robust generalist, but the Annotation (LLM) pipeline is the critical path for maximum accuracy in supported languages like Italian.

- **Design tradeoffs:**
  - Complexity vs. Accuracy: The DoubleDown and Perspective strategies use multi-step reasoning but did not outperform the simpler Annotation strategy.
  - Data Volume vs. Quality: Adding Arabic data (6418 sentences total) lowered performance compared to the smaller, related-language dataset (3972 sentences), prioritizing typological similarity over data volume.

- **Failure signatures:**
  - Quotation Artifacts: Unmatched quotation marks caused excessive token sequences (>500); requires preprocessing.
  - API Fragility: Need for fallback to keyword matching if LLM fails to return valid JSON.
  - Zero-Shot Drop: Ukrainian and Polish performance fell below baseline; model signals low confidence or high domain shift.

- **First 3 experiments:**
  1. Threshold Sweep: Validate the 0.45 threshold choice. Run inference on Dev set with thresholds [0.3, 0.4, 0.45, 0.5] to confirm optimal bias adjustment for SUBJ class.
  2. Translation Ablation: Train German-BERT on only German data vs. German + English vs. German + Arabic to reproduce data augmentation curve and verify negative impact of distant languages.
  3. Prompt Ablation: Run Annotation strategy on sample of English sentences with and without the 14 decision rules in the prompt to quantify performance gain of explicit instructions versus base model's intuition.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Which linguistic features or task characteristics determine whether instruction-based LLM inference outperforms supervised fine-tuning for subjectivity detection?
- Basis in paper: [explicit] "In particular, we aim to better understand which types of tasks or linguistic features favor instruction-based inference over supervised training."
- Why unresolved: Submission constraints allowed only one system per language, preventing systematic comparison under controlled conditions.
- What evidence would resolve it: Controlled experiments testing both approaches across all languages with feature-level analysis correlating linguistic patterns (e.g., sarcasm, framing) with approach performance.

### Open Question 2
- Question: Can hybrid strategies—such as model ensembling or confidence-based switching between fine-tuned models and LLM prompting—improve performance in low-resource languages like Ukrainian and Polish?
- Basis in paper: [explicit] "Exploring more flexible combinations of prompting and fine-tuning, e.g., via model ensembling or fallback strategies such as few-shot prompting or confidence-based model switching, could help improve performance, especially in low-resource or zero-shot settings."
- Why unresolved: Only single approaches were evaluated per language; zero-shot performance in Ukrainian and Polish fell below baselines.
- What evidence would resolve it: Ablation studies combining XLM-RoBERTa predictions with LLM prompting, measuring gains against standalone systems.

### Open Question 3
- Question: Why does adding Arabic translated training data degrade German fine-tuning performance, and does this negative transfer effect generalize to other typologically distant language pairs?
- Basis in paper: [inferred] Table 3 shows F1 dropping from 0.7692 to 0.7275 when Arabic data is added, despite consistent gains from English, Italian, and Bulgarian additions.
- Why unresolved: Authors ordered languages by similarity but did not investigate mechanism behind Arabic's interference.
- What evidence would resolve it: Controlled experiments analyzing translation quality, semantic drift, and feature overlap between German and various source languages.

### Open Question 4
- Question: Would incorporating discourse context (surrounding sentences) improve detection of context-dependent subjectivity cues such as sarcasm and implicit framing?
- Basis in paper: [inferred] Authors identify "lack of broader context for each sentence" as key limitation; Section 3.1 notes sarcasm as major source of annotation disagreement.
- Why unresolved: Task protocol required classifying isolated sentences without article-level context.
- What evidence would resolve it: Experiments comparing sentence-level versus sliding-window classification on annotated hard cases identified in Section 3.1.

## Limitations
- Exact prompt templates for the LLM-based Annotation approach are not provided, making precise replication difficult.
- Machine translation process for German data augmentation lacks details on translation engine and quality control measures.
- Focal Loss hyperparameters (alpha and gamma values) are not specified, leaving uncertainty in the exact optimization procedure.

## Confidence

- **High Confidence**: The core finding that rule-guided prompting with o3-mini achieved top performance in Italian (F1 = 0.8104) is well-supported by the abstract and results section.
- **Medium Confidence**: The claim that XLM-RoBERTa fine-tuned on Romanian data (F1 = 0.7917) demonstrates effective cross-lingual transfer is supported, but the specific mechanism of multilingual embedding alignment is assumed based on general transformer literature.
- **Low Confidence**: The assertion that adding Arabic data degraded German-BERT performance relies on the assumption that translation artifacts or semantic shifts from distant languages are the primary cause, without direct experimental evidence isolating this factor.

## Next Checks
1. **Threshold Validation**: Conduct systematic sweep of classification thresholds (0.3, 0.4, 0.45, 0.5) on Dev set to empirically confirm that 0.45 is optimal for SUBJ class in presence of label imbalance.
2. **Data Augmentation Ablation**: Replicate German-BERT training with varying datasets: only German data, German + English/Italian, and German + Arabic, to directly observe impact of adding distant language data on performance.
3. **Prompt Structure Ablation**: Test LLM Annotation strategy on subset of English sentences with and without the explicit "14 decision rules" in the prompt to quantify contribution of rule-based reasoning versus base model's intrinsic capabilities.