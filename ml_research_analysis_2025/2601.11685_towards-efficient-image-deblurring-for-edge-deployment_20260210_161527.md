---
ver: rpa2
title: Towards Efficient Image Deblurring for Edge Deployment
arxiv_id: '2601.11685'
source_url: https://arxiv.org/abs/2601.11685
tags:
- deblurring
- image
- latency
- accuracy
- block
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a hardware-aware optimization framework for
  adapting image deblurring networks to mobile NPUs. The approach combines sensitivity-guided
  block substitution, surrogate distillation, and training-free multi-objective Bayesian
  optimization using direct device profiling.
---

# Towards Efficient Image Deblurring for Edge Deployment

## Quick Facts
- arXiv ID: 2601.11685
- Source URL: https://arxiv.org/abs/2601.11685
- Authors: Srinivas Miriyala; Sowmya Vajrala; Sravanth Kodavanti
- Reference count: 28
- One-line primary result: Achieves 1.25× latency improvement and up to 55% reduction in GMACs on Samsung Galaxy S24 Ultra while maintaining competitive PSNR/SSIM accuracy for image deblurring

## Executive Summary
This paper introduces a hardware-aware optimization framework for adapting image deblurring networks to mobile NPUs, combining sensitivity-guided block substitution, surrogate distillation, and training-free multi-objective Bayesian optimization. Applied to NAFNet for motion and defocus deblurring, the approach achieves up to 55% reduction in GMACs and 1.25× latency improvement on Samsung Galaxy S24 Ultra while maintaining competitive PSNR and SSIM accuracy. The method demonstrates generality across GoPro, RealBlur, HIDE, and DPDD datasets.

## Method Summary
The framework employs a three-stage optimization: first, network surgery using saliency analysis identifies critical blocks to preserve; second, feature-wise distillation trains hardware-friendly surrogate blocks using 20-25% of training data; third, training-free multi-objective Bayesian optimization searches the architecture space using accuracy-loss and latency penalties derived from device profiling. The approach enables rapid adaptation of NAFNet to edge hardware while balancing accuracy and efficiency trade-offs.

## Key Results
- Achieves 1.25× latency improvement and up to 55% reduction in GMACs on Samsung Galaxy S24 Ultra compared to baseline NAFNet
- Maintains competitive PSNR (33.76 vs 35.80 baseline on GoPro) and SSIM accuracy while reducing computational complexity
- Demonstrates effectiveness across multiple datasets including GoPro, RealBlur, HIDE, and DPDD for both motion and defocus deblurring tasks

## Why This Works (Mechanism)

### Mechanism 1: Sensitivity-Guided Block Substitution
Replaces low-saliency blocks with hardware-friendly alternatives while preserving accuracy-critical components. Blocks are evaluated using six saliency metrics (gradient norm, SNIP, GraSP, Fisher information, plain gradient, Synflow) to identify candidates for substitution. The 4th encoder block showed highest saliency and was preserved while other blocks were eligible for replacement.

### Mechanism 2: Surrogate Distillation at Feature Level
Trains hardware-efficient "digital twins" to approximate original block behavior through feature-wise knowledge distillation. Each candidate replacement block is trained independently in parallel using only 20-25% of training data, with feature-level matching sufficient to preserve functional behavior when surrogates are composed into full networks.

### Mechanism 3: Training-Free Multi-Objective Search via Device Profiling
Decouples accuracy estimation from latency measurement by evaluating surrogate accuracy through single forward passes while directly profiling block latencies on target hardware. Bayesian optimization with expected hypervolume improvement iteratively samples the Pareto front, enabling tractable search over combinatorial architecture spaces.

## Foundational Learning

- **Concept: Saliency/Importance Metrics for Neural Network Pruning**
  - Why needed: Framework relies on identifying which NAFNet blocks can be safely replaced. Understanding gradient-based (SNIP, GraSP) vs. information-based (Fisher) saliency helps interpret Table 1 and select appropriate metrics for new architectures.
  - Quick check: Given a block with low gradient norm but high Fisher information, would you consider it safe to replace? Why or why not?

- **Concept: Knowledge Distillation (Feature-Level vs. Output-Level)**
  - Why needed: Surrogate blocks are trained via feature-wise distillation, not output matching. Understanding what intermediate features to match—and how to align them—determines surrogate quality.
  - Quick check: If distilling a convolutional block's features to a simpler variant, would you match activations before or after the nonlinearity? What tradeoff does this introduce?

- **Concept: Multi-Objective Optimization and Pareto Fronts**
  - Why needed: MOBO stage produces a Pareto front trading accuracy vs. latency. Selecting the "knee region" requires understanding what points represent good compromises.
  - Quick check: On a Pareto front with accuracy (y-axis, higher is better) and latency (x-axis, lower is better), where is the knee region typically located, and why might it not be the optimal choice for all deployment scenarios?

## Architecture Onboarding

- **Component map:** Baseline Model -> Saliency Analyzer -> Hardware-Friendly Alternatives -> Surrogate Trainer -> Device Profiler -> MOBO Search Engine -> Fine-Tuner

- **Critical path:**
  1. Profile baseline model on device → identify latency bottlenecks
  2. Compute saliency scores → determine protected blocks (Enc3 in NAFNet)
  3. Design hardware-friendly alternatives for non-critical blocks
  4. Train surrogates via feature-level distillation (parallelizable)
  5. Profile all alternatives on device → build latency lookup table
  6. Run MOBO → obtain Pareto front
  7. Select knee-point architecture → fine-tune end-to-end
  8. Validate on held-out benchmarks (RealBlur, HIDE, DPDD)

- **Design tradeoffs:**
  - More alternatives per block → finer-grained search but higher surrogate training cost
  - Stricter saliency threshold → fewer substitutions → higher accuracy but smaller latency gains
  - Fine-tuning budget → longer fine-tuning may recover accuracy but increases adaptation time
  - Device-specific profiling → optimal for target hardware but requires re-profiling for new devices

- **Failure signatures:**
  - Large accuracy gap after substitution (>3dB): Saliency metrics may not capture functional importance
  - Latency improvement <10%: Bottleneck may be memory-bound or dominated by protected blocks
  - Pareto front collapses to single point: Objectives may be too correlated; verify latency measurements
  - Fine-tuning fails to recover accuracy: Surrogate features may be misaligned; inspect distillation loss curves

- **First 3 experiments:**
  1. Saliency validation: Replace one low-saliency block with random alternative, measure accuracy drop vs. saliency-predicted importance
  2. Surrogate fidelity test: Train surrogates with varying distillation data fractions (10%, 25%, 50%), measure feature MSE and end-to-end PSNR
  3. Latency additivity check: Profile 5 random full-network configurations, compare measured latency to summed block latencies

## Open Questions the Paper Calls Out

### Open Question 1
Can the hardware-aware adaptation framework maintain its effectiveness when transferred across different NPU architectures (e.g., Qualcomm vs. Apple Neural Engine vs. MediaTek APU) without re-profiling all block alternatives? The discussion states "feedback-driven adaptation offers a practical mechanism for rapidly reconfiguring networks across devices," but validation is limited to a single device (Samsung Galaxy S24 Ultra with Qualcomm NPU).

### Open Question 2
Does the linear approximation of global latency as the sum of block-level latencies hold for complex architectures with non-trivial block interactions (e.g., skip connections, attention mechanisms)? Section II.C states "Global latency is approximated as the sum of block-level latencies," but this assumes block latencies are independent.

### Open Question 3
Can the accuracy gap between optimized and baseline models (e.g., 2.04 dB PSNR drop on GoPro) be systematically reduced while preserving latency gains through improved surrogate design or search strategies? Table 3 shows the optimized model achieves 33.76 PSNR vs. 35.80 for NAFNet baseline, a notable accuracy trade-off.

### Open Question 4
How does the choice of hardware-friendly block alternatives affect the Pareto-optimal solutions, and is there a principled method for constructing this candidate pool? Section II.A states alternatives are "drawn from prior literature and device-specific profiling," but the paper selects 6 alternatives ad hoc.

## Limitations
- Architectural details of hardware-friendly alternatives are not fully specified, relying on visual Figure 1 rather than text descriptions
- Device profiling methodology lacks transparency—specific NPU deployment framework and measurement protocol are not detailed
- Additive latency assumption for MOBO search is untested; while claimed to hold, no validation is provided against actual end-to-end measurements

## Confidence
- **High Confidence**: General framework structure (saliency-guided surgery + distillation + MOBO) is well-described and theoretically sound
- **Medium Confidence**: Experimental results show meaningful improvements (1.25× latency, 55% GMACs reduction), but lack statistical significance testing and ablation studies
- **Low Confidence**: Claims about additive latency approximation and surrogate fidelity are stated without empirical validation on target hardware

## Next Checks
1. **Latency Additivity Validation**: Profile 10 random full-network configurations on target NPU and compare measured latencies to summed block latencies. If error exceeds 15%, investigate memory bandwidth or kernel fusion effects.
2. **Surrogate Fidelity Assessment**: For 3 critical block types, measure feature reconstruction MSE during distillation and correlate with post-stitching accuracy degradation. Determine minimum viable distillation data fraction.
3. **Saliency Metric Reliability**: Replace 5 low-saliency blocks with random alternatives and measure actual accuracy impact. Compare observed drops to saliency-predicted importance to validate metric selection criteria.