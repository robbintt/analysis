---
ver: rpa2
title: 'Autocorrelated Optimize-via-Estimate: Predict-then-Optimize versus Finite-sample
  Optimal'
arxiv_id: '2602.01877'
source_url: https://arxiv.org/abs/2602.01877
tags:
- data
- varma
- optimization
- learning
- a-ove
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares data-driven optimization methods in portfolio
  optimization with trading costs under autocorrelated uncertainty. It proposes an
  autocorrelated Optimize-via-Estimate (A-OVE) model that optimizes out-of-sample
  performance as a function of VARMA process sufficient statistics.
---

# Autocorrelated Optimize-via-Estimate: Predict-then-Optimize versus Finite-sample Optimal

## Quick Facts
- **arXiv ID:** 2602.01877
- **Source URL:** https://arxiv.org/abs/2602.01877
- **Reference count:** 40
- **Primary result:** A-OVE achieves significantly lower regret than PTO, ETO, and FPtP baselines in portfolio optimization with autocorrelated returns.

## Executive Summary
This paper introduces an autocorrelated Optimize-via-Estimate (A-OVE) model for portfolio optimization under VARMA uncertainty with trading costs. Unlike standard Predict-then-Optimize (PTO) methods that first predict returns and then optimize, A-OVE directly optimizes out-of-sample performance as a function of VARMA sufficient statistics, treating parameter uncertainty explicitly. Experiments show A-OVE consistently outperforms PTO (RNN, LSTM, RF, XGB), ETO, and FPtP on both synthetic and real-world data, particularly when the model is well-specified. Crucially, higher prediction accuracy does not guarantee better decision quality, highlighting the importance of aligning learning objectives with downstream optimization tasks.

## Method Summary
A-OVE optimizes the expected portfolio utility over a finite sample of candidate VARMA parameters drawn from a prior distribution. The key innovation is computing the exact likelihood of future returns conditioned on past observations using sufficient statistics derived from the VARMA process. The model uses Monte Carlo integration over the parameter space with likelihood weighting, solved via a closed-form portfolio solution that incorporates both mean returns and covariance forecasts. Synthetic data is generated from VARMA(1,1) processes, while real-world data uses four stocks (GOOGL, HMC, AXP, BAMXF) from 2010-2019. Performance is measured by relative regret compared to an oracle.

## Key Results
- A-OVE achieves significantly lower regret than PTO, ETO, and FPtP methods on both synthetic and real-world data.
- Higher prediction accuracy does not guarantee better decision quality; A-OVE's regret is lowest despite not having the best prediction MSE.
- Performance remains robust under small model mis-specification but degrades when commutativity assumptions are violated.
- The closed-form solution enables efficient computation even with 500-1000 parameter candidates.

## Why This Works (Mechanism)
A-OVE works by directly optimizing the decision objective with respect to parameter uncertainty, rather than treating prediction as a separate pre-processing step. By leveraging the VARMA sufficient statistics and exact likelihood computation, the model maintains uncertainty quantification throughout the optimization process. This alignment between the learning objective (minimizing regret) and the optimization task (maximizing utility) explains why higher prediction accuracy doesn't always translate to better decisions.

## Foundational Learning
- **VARMA process modeling:** Understanding the Vector Autoregressive Moving Average structure is essential for capturing autocorrelated returns in portfolio optimization.
- **Sufficient statistics computation:** The recursive formulas for mean and covariance sufficient statistics enable efficient likelihood calculation without storing full data history.
- **Monte Carlo integration over parameter space:** Approximating the expected utility by sampling from a prior distribution with likelihood weighting is the core computational technique.
- **Closed-form portfolio optimization:** The quadratic programming solution that incorporates both mean and covariance forecasts is critical for efficient decision-making.
- **Regret-based evaluation:** Measuring performance relative to an oracle rather than absolute prediction accuracy aligns with the optimization objective.
- **Commutativity assumptions:** The mathematical tractability of the likelihood computation relies on specific parameter structure assumptions.

## Architecture Onboarding

### Component Map
VARMA simulator -> Sufficient statistics computation -> Likelihood weighting -> Monte Carlo integration -> Closed-form optimization -> Regret evaluation

### Critical Path
The most critical computational path is the Monte Carlo integration loop: sampling parameters from prior → computing g₁(Y,ξ) via innovation algorithm → approximating c₁ and C₂ → solving for optimal portfolio weights. Bottlenecks occur in the innovation algorithm for high-dimensional covariance matrices.

### Design Tradeoffs
- **Parameter space sampling:** Larger N_ove improves accuracy but increases computation time quadratically.
- **Prior specification:** Symmetric priors centered at MLE estimates work well but require careful bandwidth selection in real data.
- **Model complexity:** Higher VARMA orders capture more complex dynamics but increase estimation variance and computational cost.

### Failure Signatures
- Non-positive definite covariance matrices during innovation algorithm indicate numerical instability.
- Regret increasing with prediction accuracy suggests misalignment between prediction and optimization objectives.
- Extremely long computation times with N_ove > 1000 suggest scalability limits for larger portfolios.

### First 3 Experiments
1. Implement VARMA(1,1) simulator with symmetric parameters and verify stationarity conditions.
2. Test sufficient statistics computation on synthetic data with known parameters.
3. Compare regret performance of A-OVE against PTO-RNN baseline on synthetic data.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can OVE be extended to contextual settings where side information is explicitly utilized rather than relying on time-series structure?
- **Open Question 2:** Can a non-parametric version of OVE be developed to mitigate performance degradation under model mis-specification?
- **Open Question 3:** Do the comparative findings generalize to other optimization domains like inventory management or energy scheduling?
- **Open Question 4:** Can the theoretical derivation of sufficient statistics be generalized to cases without symmetric covariance assumptions?

## Limitations
- The theoretical guarantees rely on commutativity and symmetry assumptions that may be violated in real-world datasets.
- Closed-form solution depends on accurate estimation of high-dimensional covariance matrices, which can be unstable for n ≥ 10.
- Computational burden of Monte Carlo integration (N_ove = 500-1000) may limit scalability for high-frequency trading.
- Real-world experiments are limited to only four assets, reducing generalizability.

## Confidence
- **High:** Theoretical guarantees (Theorem 3.2, Corollary 3.3) and synthetic experiment validation
- **Medium:** Comparative performance claims based on limited real-world dataset
- **Low:** Practical applicability under severe model misspecification not thoroughly tested

## Next Checks
1. Test A-OVE under severe parameter misspecification by generating synthetic data violating commutativity assumptions.
2. Scale synthetic experiments to n=50 assets and measure regret performance and computation time.
3. Implement cross-validation for prior construction in real-world experiments to optimize bandwidth selection.