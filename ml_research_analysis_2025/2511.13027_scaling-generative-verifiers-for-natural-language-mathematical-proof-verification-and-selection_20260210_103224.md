---
ver: rpa2
title: Scaling Generative Verifiers For Natural Language Mathematical Proof Verification
  And Selection
arxiv_id: '2511.13027'
source_url: https://arxiv.org/abs/2511.13027
tags:
- proof
- verification
- mathematical
- judgement
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the challenge of verifying and selecting
  natural-language mathematical proofs using large language models (LLMs). The authors
  develop a unified test-time scaling approach that combines GenSelect tournaments
  with LLM-as-a-Judge evaluation, demonstrating that this hybrid method outperforms
  individual techniques in proof selection tasks.
---

# Scaling Generative Verifiers For Natural Language Mathematical Proof Verification And Selection

## Quick Facts
- **arXiv ID**: 2511.13027
- **Source URL**: https://arxiv.org/abs/2511.13027
- **Reference count**: 40
- **Primary result**: Hybrid GenSelect + LLM-as-a-Judge approach outperforms individual methods for proof selection, while RL fine-tuning reduces prompt sensitivity but not final-answer precision

## Executive Summary
This paper addresses the challenge of verifying and selecting natural-language mathematical proofs using large language models. The authors develop a unified test-time scaling approach that combines knockout GenSelect tournaments with LLM-as-a-Judge evaluation, demonstrating that this hybrid method outperforms individual techniques in proof selection tasks. Through extensive experiments, they show that reinforcement learning can improve proof-level metrics and reduce prompt sensitivity in LLM-as-a-Judge methods, though it does not enhance final-answer precision, suggesting current approaches rely more on recognizing stylistic features than deep mathematical understanding. The study reveals significant dataset imbalances in existing proof evaluation benchmarks and establishes practical guidelines for designing reliable proof-verification systems, while also highlighting that current LLMs, despite high accuracy on proof judgment tasks, cannot yet be fully trusted as reliable mathematical judges in high-stakes scenarios.

## Method Summary
The paper proposes a hybrid approach combining two proof verification paradigms: GenSelect (pairwise comparison tournaments) and LLM-as-a-Judge (absolute scoring). The method works by first generating n_p candidate proofs, running a knockout tournament to reduce to n_s semifinalists (requiring n_p - 1 LLM calls), then having the LLM-as-a-Judge evaluate n_j samples per remaining proof and averaging scores for final selection. The authors also explore reinforcement learning with GRPO to reduce prompt sensitivity in judge models, finding it improves proof-level metrics but not final-answer precision. The approach is evaluated across six datasets spanning competition mathematics problems, with comprehensive analysis of accuracy, precision, recall, F1 scores, and final-answer correctness as auxiliary metrics.

## Key Results
- Hybrid GenSelect + LLM-as-a-Judge achieves 100% accuracy on AIME-2025 across all 8 runs, outperforming individual methods
- Reinforcement learning reduces prompt sensitivity between OPC and GIMO prompts but fails to improve final-answer precision
- Dataset imbalance is significant, with trivial heuristics achieving 65% accuracy by exploiting problem-type correlations
- Current LLMs cannot be fully trusted as mathematical judges, with GPT-5 incorrectly validating 7/48 erroneous USAMO proofs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: A unified test-time scaling approach combining knockout GenSelect tournaments with LLM-as-a-Judge evaluation can achieve superior proof selection accuracy than either method alone, while being more computationally efficient.
- **Mechanism**: GenSelect efficiently reduces n_p candidate proofs to n_s semifinalists through pairwise comparisons (n_p - 1 calls for knockout format). LLM-as-a-Judge then samples n_j independent judgments per remaining proof and averages scores for fine-grained ranking. The hybrid exploits GenSelect's strength in coarse filtering and LLM-as-a-Judge's ability to discriminate among similar-quality proofs.
- **Core assumption**: Relative comparison and absolute scoring capture complementary signals; correct proofs exist in the candidate pool with non-trivial probability.
- **Evidence anchors**:
  - [abstract]: "identify their combination as the most effective framework for solution verification and selection"
  - [section 7]: "achieves 100% accuracy on AIME-2025 across all 8 runs... significantly outperforms majority voting, and consistently matches or outperforms GenSelect"
  - [corpus]: Related work on budget-aware test-time scaling (arXiv:2510.14913, FMR=0.54) explores cost tradeoffs but does not directly compare hybrid architectures.
- **Break condition**: When all candidate proofs are fundamentally flawed (no correct proof in pool), or when candidate quality is uniformly low, selection methods return the "least wrong" option without validity guarantees.

### Mechanism 2
- **Claim**: Reinforcement learning fine-tuning of judge models can reduce sensitivity to prompt design and improve proof-level metrics, but this improvement primarily reflects calibration to stylistic correctness rather than enhanced mathematical reasoning.
- **Mechanism**: GRPO training with binary reward (matching ground-truth correctness labels) teaches the model to weight features that correlate with labeled correctness. Different prompts (OPC, GIMO, General Summary) converge to similar performance post-RL because the model learns to extract invariant signals from varied prompt formats. However, if training labels correlate with stylistic features (e.g., "––-" separators associated with correct proofs), the model may learn superficial patterns.
- **Core assumption**: Training labels are sufficiently accurate and balanced; feature-label correlations in training data generalize to evaluation data.
- **Evidence anchors**:
  - [abstract]: "reinforcement learning can reduce this sensitivity... despite improving proof-level metrics, reinforcement learning does not enhance final-answer precision"
  - [section 5, Figure 1]: Shows convergence of OPC and GIMO prompts after RL training; table shows RL improves proof F1 but not final-answer precision
  - [section 4]: "OpenAI reasoning models... consistently use '––-' to separate proof sections—creating an unintended marker for correctness"
  - [corpus]: Shi and Jin (Heimdall, arXiv:2504.10337) show training set imbalances lead to exploitable correlations, though in final-answer rather than proof context.
- **Break condition**: If training data contains systematic label noise or strong spurious correlations (e.g., model-specific formatting markers), RL may amplify shortcuts rather than improve genuine reasoning assessment.

### Mechanism 3
- **Claim**: Evaluating both proof-level correctness and final-answer precision provides a more reliable measure of judge capability than either metric alone, revealing whether models assess mathematical validity versus superficial features.
- **Mechanism**: Final-answer precision serves as an orthogonal validation signal—conditional on a judge predicting "correct," what fraction have the correct answer? Low precision despite high proof accuracy indicates the judge rewards stylistic correctness (e.g., well-formatted but mathematically flawed proofs). Balanced datasets prevent models from exploiting problem-type or generator-source correlations.
- **Core assumption**: Correct final answers require correct reasoning (for problems chosen); evaluation sets are balanced across problem types and generators.
- **Evidence anchors**:
  - [abstract]: "focusing on a single benchmark can lead to brittle or misleading conclusions... evaluate both proof-based and final-answer reasoning"
  - [section 4]: Case studies show trivial heuristics (predict "incorrect" for geometry problems) achieve 65% accuracy on imbalanced datasets; balanced final-answer evaluation exposes such shortcuts
  - [corpus]: ProcessBench (arXiv:2510.13744) and PRM800K focus on step-level error detection but do not combine proof and final-answer metrics systematically.
- **Break condition**: If problems permit correct answers through guessing or pattern-matching without valid reasoning, final-answer precision becomes an unreliable proxy for proof validity.

## Foundational Learning

- **Concept: Generative Verification Paradigms (LLM-as-a-Judge vs. GenSelect)**
  - Why needed here: The hybrid architecture requires understanding when each paradigm excels—absolute scoring for fine-grained ranking, relative comparison for efficient filtering.
  - Quick check question: Given 64 candidate proofs, how many LLM calls does a knockout tournament require? How many for a full pairwise tournament?

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed here: The paper uses GRPO with binary rewards; understanding reward design and potential reward hacking is critical for interpreting why RL improves proof metrics but not final-answer precision.
  - Quick check question: Why might a model learn to predict "correct" for proofs with "---" separators even if that marker is incidental?

- **Concept: Test-Time Compute Scaling Tradeoffs**
  - Why needed here: The hybrid method explicitly trades off accuracy vs. computational cost; practitioners must set n_p, n_s, n_j based on budget constraints.
  - Quick check question: If you have budget for 1000 LLM calls and want to select from 128 candidates, which configuration (n_p, n_s, n_j) would you choose and why?

## Architecture Onboarding

- **Component map**: Problem statement -> Generator (n_p proofs) -> GenSelect knockout (n_p - 1 calls, output n_s proofs) -> Judge scoring (n_j × n_s calls) -> Final selection

- **Critical path**: Problem statement → Generator (n_p proofs) → GenSelect knockout (n_p - 1 calls, output n_s proofs) → Judge scoring (n_j × n_s calls) → Final selection

- **Design tradeoffs**:
  - Higher n_p: Better coverage of solution space, linear increase in GenSelect cost
  - Higher n_s: More candidates reach expensive judge phase, diminishing returns beyond ~16
  - Higher n_j: More reliable judge scores, linear cost increase; saturates around 20-32
  - Rubric inclusion: May improve precision for some models but can hurt on out-of-distribution data

- **Failure signatures**:
  - High proof accuracy + low final-answer precision → judge exploiting stylistic features
  - Large variance across random seeds → insufficient n_j or n_s
  - Strong in-distribution performance, weak OOD performance → overfitting to training set correlations
  - Verifier (e.g., GPT-5) accepts proofs with fabricated theorems → verifier weaker than generator on frontier problems

- **First 3 experiments**:
  1. **Baseline characterization**: Run pure GenSelect (knockout, n_p=128) and pure LLM-as-a-Judge (n_j=32) on SelProofBench and SelOPC to establish single-method performance and variance.
  2. **Hybrid sweep**: Fix n_p=128, sweep n_s ∈ {4, 8, 16, 32} with n_j=32 on Challenge-19 to identify compute-optimal configuration (accuracy vs. total LLM calls).
  3. **RL generalization test**: Train Qwen3-8B on VerOPC with and without rubric; evaluate on both VerOPC validation (in-distribution) and VerProofArena (OOD) to assess whether rubric helps or hurts generalization.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can improved aggregation strategies or RL fine-tuning enable step-based judgement to outperform whole-proof verification? The paper notes that exploring better aggregation or step decomposition is a "promising direction" to address the low recall of current step-based methods.

- **Open Question 2**: Does scaling reinforcement learning training data and steps shift model rewards from stylistic correctness to genuine mathematical validity? The paper hypothesizes that scaling training data and RL steps is required to realize gains in "mathematical content judgement" beyond style.

- **Open Question 3**: Do the advantages of 7-point grading prompts for proof selection generalize across diverse datasets? The paper states "more research is needed" to determine if 7-point grading benefits observed on SelProofBench transfer to other datasets like SelOPC.

## Limitations

- Current approaches may rely more on recognizing stylistic features than deep mathematical understanding, as evidenced by RL improving proof metrics but not final-answer precision
- Dataset imbalances allow trivial heuristics to achieve high accuracy, potentially misleading evaluation of model capabilities
- GPT-5 incorrectly validates 7/48 erroneous USAMO proofs, demonstrating that current LLMs cannot be fully trusted as mathematical judges in high-stakes scenarios

## Confidence

- **High confidence**: The empirical observation that LLM-as-a-Judge methods are prompt-sensitive and that reinforcement learning can reduce this sensitivity is well-supported by the data.
- **Medium confidence**: The interpretation that RL improvements reflect calibration to stylistic features rather than enhanced reasoning is plausible but requires additional validation.
- **Low confidence**: The assertion that RL cannot improve final-answer precision despite improving proof metrics may be dataset-dependent.

## Next Checks

1. **Spurious correlation validation**: Manually examine proofs where the model predicts "correct" but contains mathematical errors to identify specific stylistic features being exploited. Create a controlled dataset where these features are systematically varied while mathematical content remains constant.

2. **Generalization stress test**: Evaluate the hybrid method on mathematical domains outside competition mathematics (e.g., research-level proofs, applied mathematics problems) to assess whether the approach transfers beyond its training distribution.

3. **Human-expert comparison**: Have mathematics PhDs independently verify a sample of proofs from VerProofArena, particularly those where GPT-5 accepts erroneous proofs. Compare human consistency and error rates with LLM-as-a-Judge performance to establish baseline reliability expectations.