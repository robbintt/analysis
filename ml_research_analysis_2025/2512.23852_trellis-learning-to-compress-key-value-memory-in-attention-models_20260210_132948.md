---
ver: rpa2
title: 'Trellis: Learning to Compress Key-Value Memory in Attention Models'
arxiv_id: '2512.23852'
source_url: https://arxiv.org/abs/2512.23852
tags:
- memory
- arxiv
- trellis
- learning
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Trellis, a novel Transformer architecture
  with bounded memory that dynamically compresses its key-value memory at test time.
  Trellis replaces the standard KV cache with a fixed-size memory and trains a two-pass
  recurrent compression mechanism to store new keys and values into memory.
---

# Trellis: Learning to Compress Key-Value Memory in Attention Models

## Quick Facts
- **arXiv ID**: 2512.23852
- **Source URL**: https://arxiv.org/abs/2512.23852
- **Reference count**: 18
- **Primary result**: Introduces Trellis, a Transformer architecture with bounded memory that dynamically compresses KV cache at test time using two-pass recurrent compression and online gradient descent.

## Executive Summary
Trellis addresses the challenge of unbounded KV cache growth in Transformers by replacing it with a fixed-size memory and training a two-pass recurrent compression mechanism. The architecture learns to compress keys and values into a bounded memory using online gradient descent with a forget gate, enabling efficient long-context processing. Extensive experiments show Trellis outperforms strong baselines across language modeling, commonsense reasoning, and needle-in-haystack tasks, with performance gains increasing as sequence length grows.

## Method Summary
Trellis implements a two-pass recurrent compression mechanism where keys and values are compressed into a fixed-size memory using online gradient descent. The model treats compression as a regression problem, reconstructing latent representations from compressed memory. A forget gate (β_t) controls memory decay through ℓ₂ regularization, allowing selective retention of important context. The architecture parallelizes computation using stale gradients within chunks, enabling hardware-efficient matmul operations while preserving nonlinear recurrence benefits.

## Key Results
- Outperforms strong baselines (Mamba2, Gated-DeltaNet) on language modeling with perplexity improvements up to 0.3 points
- Shows increasing performance gains over baselines as sequence length grows (2K→32K context)
- Achieves competitive results on RULER benchmark for needle-in-haystack tasks while maintaining efficiency
- Ablation studies confirm importance of forget gate, intermediate activation choice, and memory size

## Why This Works (Mechanism)

### Mechanism 1: Two-Pass Recurrent Compression via Online Gradient Descent
Trellis replaces unbounded KV cache with fixed-size memory (m slots) and learns to compress via gradient descent, treating compression as reconstruction problem. Memory updates via one gradient descent step: M_t = β_t M_{t-1} - γ_t ∇_M L(M_{t-1}, v_t, α_t). This two-pass process first compresses keys, then values, using same latent target α_t for both passes. Core assumption: KV matrices exhibit low-rank structure compressible without critical information loss; online gradient descent tracks non-stationary sequential data efficiently.

### Mechanism 2: State Decay (Forget Gate) for Memory Management
Applies ℓ₂ regularization within inner-loop optimization to induce forget gate that selectively decays prior memory, preventing overfitting to early tokens. Regularized objective yields update M_t = (1-λ_t)M_{t-1} - γ_t ∇_M L, where β_t = 1-λ_t ∈ [0,1] acts as forget gate. β_t → 1 retains memory; β_t → 0 erases it (context shift detection). Core assumption: Token-level adaptive decay is learnable and improves memory management over fixed or purely additive update rules.

### Mechanism 3: Chunkwise Parallelization via Stale Gradients
Approximates gradients within each chunk using state at chunk start, linearizing recurrence locally for parallel scan and hardware-efficient matmul while preserving nonlinear recurrence benefits. Divide sequence into chunks of size C; within chunk b, approximate ∇_M L_t ≈ G_t(M_{t'}, α_t, v_t) where M_{t'} is state at chunk start. This yields locally linear recurrence satisfying associativity, parallelizable via prefix scan or chunkwise matrix form.

## Foundational Learning

- **Concept: Fast Weight Programmers (FWPs) and Meta-Learning** - Why needed: Trellis treats memory M_t as "fast weights" updated in inner loop per sequence, while model parameters W are "slow weights" updated in outer loop. This bi-level optimization clarifies why gradient updates to M_t occur during forward pass.
- **Concept: Online Gradient Descent for Sequential Data** - Why needed: Compression layer uses one gradient step per token to minimize reconstruction loss, treating sequence as streaming data. This differs from batch optimization and requires understanding non-stationary loss landscapes.
- **Concept: Linear Attention and Kernel Trick** - Why needed: Trellis builds on ABC and GSA, which decompose attention into two linear attention passes. Understanding how linear attention replaces softmax with kernel feature maps clarifies what Trellis preserves (softmax nonlinearity) vs. modifies (nonlinear recurrence with forget gate).

## Architecture Onboarding

- **Component map**: Input embedding → Conv1D → Linear projections → Trellis block (two-pass compression) → Intermediate activation → Value compression → Output → RMSNorm → SwiGLU gate → Residual add → Next layer
- **Critical path**: 1) Key compression pass: compress({q_t, k_t, α_t}) → intermediate ŷ_t ∈ R^m; 2) Value compression pass: compress({f(ŷ_t), v_t, α_t}) → output y_t ∈ R^d; 3) Memory update: M_t = β_t M_{t-1} - γ_t u_t k_t^T; 4) Readout: y_t = M_t^T q_t
- **Design tradeoffs**: Memory size m: Larger m improves recall but increases compute; ablation shows m=128 outperforms m=32 (10.87 vs 11.14 ppl). Intermediate activation f: Normalized SiLU outperforms softmax (10.98 vs 11.29 ppl). Chunk size C: Smaller C improves gradient accuracy but reduces parallelism; chunk size 1 achieves 10.75 ppl but is slower. Forget gate β_t: Essential for long-context; removal increases ppl from 10.87 to 11.28.
- **Failure signatures**: Perplexity increasing with context length may indicate memory overflow or insufficient decay; check β_t distribution and memory slot utilization. Poor NIAH performance at long sequences may indicate forget gate decaying critical information; consider reducing λ_t or using hybrid retention strategies. Training instability with large chunks may indicate stale gradient divergence; reduce chunk size C or increase gradient clipping.
- **First 3 experiments**: 1) Ablation on memory size: Train 125M model with m ∈ {16, 32, 64, 128} on Pile (2K context); measure ppl and NIAH accuracy to identify saturation point. 2) Forget gate analysis: Visualize β_t distribution across layers and timesteps on long sequences (e.g., Books 16K); correlate with retrieval accuracy on held-out tokens. 3) Chunk size sensitivity: Train with C ∈ {64, 128, 256, 512}; measure training throughput (tokens/sec) vs. ppl to find Pareto-optimal point for target hardware.

## Open Questions the Paper Calls Out

1. **Can Trellis effectively serve as a target architecture for fine-tuning large, pre-trained Transformers?** The paper suggests T2R (finetuning pretrained Transformers into RNNs) as a promising direction but provides no experimental results on converting existing Transformer checkpoints.

2. **What are the actual wall-clock inference throughput and latency metrics for Trellis compared to optimized baselines?** While chunk-wise formulation enables hardware-efficient matmul operations, the paper omits system-level speed or memory benchmarks, making practical efficiency unclear.

3. **How can the forget gate mechanism be refined to prevent performance degradation on Needle-in-a-Haystack tasks?** The current gate design shows tension between compressing irrelevant context and preserving rare, high-value tokens, with S-NIAH-PK tasks showing degradation at longer sequences.

4. **Does integrating architectural advancements from recent non-linear recurrent models improve Trellis?** The paper acknowledges Titans and Lattice as promising directions but does not test hybrid architectures combining their memory management strategies.

## Limitations

- Core compression mechanism underspecified: exact parameterization of learning rates (γ_t), forget gates (β_t), and inner-loop optimization procedure not fully specified
- Performance comparisons qualified: primarily against specialized architectures (Mamba2, Gated-DeltaNet) rather than standard Transformers with KV cache optimization
- Claims of superior performance rely on models of different scales and training regimes
- Chunkwise parallelization stale gradient quality not empirically validated across different chunk sizes

## Confidence

- **High Confidence**: Two-pass recurrent compression mechanism and its basic implementation are well-specified through Equations 13-14. Ablation showing 10.87→11.28 PPL when removing forget gate provides strong evidence for this component's importance.
- **Medium Confidence**: State decay mechanism and its role in long-context retention is supported by ablation but lacks comparison to alternative memory management strategies. Chunkwise parallelization is theoretically sound but lacks empirical validation of stale gradient approximation quality.
- **Low Confidence**: Exact implementation details for inner-loop learning rates, forget gate parameterization, and training hyperparameters insufficient for faithful reproduction. Claims about superior performance at long contexts rely on comparisons with models of different scales and training regimes.

## Next Checks

1. **Memory Stability Analysis**: Monitor memory norm trajectories ||M_t|| across layers and timesteps during training. Verify state decay effectively bounds memory and that β_t distributions correlate with task-specific context requirements. Compare memory utilization patterns between reasoning tasks (requiring longer retention) and language modeling.

2. **Stale Gradient Quality Assessment**: Systematically vary chunk size C ∈ {64, 128, 256, 512} and measure reconstruction error between true gradients and stale approximations. Correlate gradient approximation quality with both training stability (gradient norm variance) and final performance metrics.

3. **Forget Gate Behavior Investigation**: Visualize β_t distributions across layers and context positions on long sequences (16K+). Conduct controlled experiments removing forget gate on NIAH tasks with varying context lengths to quantify tradeoff between retention and adaptability. Test whether hybrid strategies (layer-specific or context-adaptive decay) outperform uniform forget gate.