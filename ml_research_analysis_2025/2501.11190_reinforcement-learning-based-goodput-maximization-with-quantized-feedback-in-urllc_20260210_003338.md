---
ver: rpa2
title: Reinforcement Learning Based Goodput Maximization with Quantized Feedback in
  URLLC
arxiv_id: '2501.11190'
source_url: https://arxiv.org/abs/2501.11190
tags:
- channel
- feedback
- where
- goodput
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses goodput maximization in Ultra-Reliable Low-Latency
  Communication (URLLC) systems with quantized feedback under dynamic channel conditions.
  The authors propose a two-part system: (1) a novel learning-based Rician-K factor
  estimator using XGBoost regression on empirical moments of channel magnitude, and
  (2) a reinforcement learning-based quantized feedback scheme selection algorithm
  using Q-learning to optimize quantization thresholds and transmission rates.'
---

# Reinforcement Learning Based Goodput Maximization with Quantized Feedback in URLLC

## Quick Facts
- **arXiv ID**: 2501.11190
- **Source URL**: https://arxiv.org/abs/2501.11190
- **Reference count**: 23
- **Primary result**: Novel RL-based quantized feedback scheme with XGBoost K-factor estimator achieves superior goodput maximization in dynamic URLLC environments

## Executive Summary
This paper proposes a two-part system for goodput maximization in URLLC systems with quantized feedback under dynamic channel conditions. The first component is a learning-based Rician-K factor estimator using XGBoost regression on empirical moments of channel magnitude, which outperforms traditional moment-based estimators. The second component is a reinforcement learning-based quantized feedback scheme selection algorithm using Q-learning to optimize quantization thresholds and transmission rates, enabling the system to track changing Rician-K factors and approach maximum achievable goodput values.

## Method Summary
The method combines XGBoost regression with Q-learning to create an adaptive URLLC system. The K-factor estimator extracts the first ten empirical moments from N channel samples as features, then uses XGBoost to predict the Rician-K factor. The RL component treats each quantization threshold as an independent agent in a multi-agent MDP, using ε-greedy exploration with decaying exploration rate to update threshold positions based on empirical goodput rewards. The two-phase architecture executes all learning algorithms at the receiver, minimizing transmitter complexity and feedback overhead.

## Key Results
- XGBoost-based K-factor estimator outperforms traditional moment-based estimators in accuracy
- RL-based feedback scheme effectively adapts to channel variations, tracking optimal quantization thresholds
- Two-phase architecture enables computational efficiency while maintaining goodput performance
- System demonstrates superior goodput maximization in dynamic URLLC environments with varying Rician-K factors

## Why This Works (Mechanism)

### Mechanism 1
XGBoost regression using empirical moments as features provides superior Rician-K factor estimation compared to traditional moment-based estimators. The estimator extracts the first ten empirical raw moments (including mean, variance, skewness, kurtosis) from N channel magnitude samples as input features. These moments capture the statistical characteristics of the Rician distribution more comprehensively than traditional estimators that rely on only 2-3 moments. XGBoost's gradient boosting framework then learns the nonlinear mapping from moment features to K-factor values. Core assumption: channel magnitude follows Rician distribution with slowly varying K-factor (assumed constant for ~200 channel realizations). Evidence anchors: [section III] "we extract the first ten empirical moments of the Rice-distributed random variable as input features... This approach offers a more detailed and informative representation of the channel compared to conventional moment-based estimators" [section III.A] "the best performance is obtained when the training dataset is comprised of N = 100. However, similar performance results can be achieved with selections of N > 50" Break condition: If channel distribution deviates significantly from Rician (e.g., Nakagami-m, Weibull), the trained model's mapping will be invalid. Non-stationary K-factor changes faster than ~200 realizations may prevent sufficient sample collection.

### Mechanism 2
Q-learning with ε-greedy exploration enables adaptive quantized feedback scheme selection that tracks varying channel statistics. The algorithm models quantization threshold selection (λ_l values) as a multi-agent Markov decision process. Each λ_l agent maintains a Q-table indexed by current K-factor estimate and threshold position. The ε-greedy strategy balances exploitation (selecting best-known action with probability 1-ε_t) and exploration (random action with probability ε_t), where ε_t decays as ε_t/√t over iterations. Actions modify threshold positions incrementally (+1, 0, -1 index steps), and rewards are computed as empirical goodput over M transmissions. Core assumption: The optimization problem can be decomposed into independent agents per threshold without significant suboptimality; K-factor changes slowly enough for Q-table convergence. Evidence anchors: [section IV] "we design the RL algorithm in such a way that in every subsequent iteration only one agent, e.g. λ_l, can change its status... at ∈ {−1, 0, +1}" [section IV.A] "the algorithm treats each agent independently, thus it only needs the creation of distinct Q-tables for each agent" [section V, Figure 3] "algorithm can reach the optimum value in both cases but faster, in terms of t, when M = 10^3" Break condition: If the number of quantization levels Λ grows large, the combinatorial state space may exceed practical Q-table storage even with independent agent decomposition. Rapid K-factor changes (faster than Q-learning convergence time) will cause persistent lag behind optimal configuration.

### Mechanism 3
Two-phase architecture separating K-estimation from RL-based scheme selection enables computational efficiency and reduced feedback overhead. All learning algorithms execute at the receiver side. The K-estimator runs on collected channel samples and provides scalar K-factor estimates to the RL module. The RL agent selects optimal quantization thresholds and transmission rates, then sends updated rate information to transmitter only when changes are needed (via secondary feedback channel). This separation minimizes transmitter complexity and feedback bandwidth during steady-state operation. Core assumption: Receiver has perfect knowledge of instantaneous channel coefficient h for each codeword; feedback channels are error-free. Evidence anchors: [section II.C, Figure 1] "all learning algorithms are implemented at the receiver. This also contributes to reducing the overall computational load on transceivers" [section II.C] "this secondary feedback channel is not employed in every subsequent transmission; rather, its usage is triggered by the decision made by the RL-based feedback scheme" Break condition: If the K-estimator produces biased estimates, the RL agent will converge to suboptimal policies for the actual channel conditions. Feedback channel errors (violating error-free assumption) could corrupt rate updates.

## Foundational Learning

- Concept: **Rician Distribution and K-Factor**
  - Why needed here: The K-factor (ratio of line-of-sight to scattered power) determines the shape of the channel magnitude distribution, which directly affects optimal quantization boundaries. Without understanding K's physical meaning, the estimator's output is just a number without context.
  - Quick check question: Can you explain why K=0 dB corresponds to Rayleigh fading and how increasing K shifts the optimal quantization thresholds?

- Concept: **Q-Learning and Temporal Difference Learning**
  - Why needed here: The paper uses tabular Q-learning with specific design choices (ε-greedy decay, independent agent decomposition). Understanding the Bellman update equation and exploration-exploitation tradeoff is essential to diagnose convergence issues.
  - Quick check question: Why does the paper decay ε_t as ε_t/√t rather than a fixed schedule, and what would happen if ε decayed too slowly or too quickly?

- Concept: **Goodput and Outage Probability in Quasi-Static Fading**
  - Why needed here: The reward function is defined as empirical goodput, which combines rate and reliability. Understanding Equation (8)'s outage formulation explains why the optimization problem in Equation (9) has its specific structure.
  - Quick check question: Given Equation (3) G = r(1-ε), why is maximizing goodput different from maximizing either rate alone or reliability alone?

## Architecture Onboarding

- Component map: Channel samples → K-Estimator (XGBoost with 10 moments) → RL Selector (Q-learning with ε-greedy) → Quantized feedback index L(γ) → Transmitter (rate r_l) → Feedback (secondary channel for rate updates)

- Critical path: 1) Collect N=100 channel samples for K-estimation 2) Run XGBoost inference to obtain K-hat 3) For each RL iteration: select agent → ε-greedy action → observe M transmissions → compute reward → update Q-table 4) If rate change warranted: send updated r_l to transmitter via secondary channel

- Design tradeoffs:
  - **M (transmissions per iteration)**: Higher M (1000 vs 100) yields faster convergence in iteration count but more channel uses per iteration. Total samples to convergence may be similar.
  - **N (estimator samples)**: N=50-100 provides good estimation; N=1000 achieves near-perfect estimation but increases latency before K-hat is available.
  - **Q-table granularity**: Larger state set S improves threshold resolution but increases table size per Equation (23) |K||S| per agent.
  - **Independent agent assumption**: Reduces complexity but sacrifices global optimality; paper acknowledges this yields "close approximation" not exact optimum.

- Failure signatures:
  - **Estimator bias**: K-hat systematically off by >2 dB → check training data distribution matches deployment K-range; verify moment computation handles edge cases.
  - **RL non-convergence**: Goodput oscillates without settling → ε_t may be decaying too slowly; increase initial ε or adjust decay schedule.
  - **Tracking lag**: Goodput consistently below optimum after K changes → Q-table may lack coverage for new K values; consider online Q-table updates or expanded K discretization.
  - **Constraint violation**: λ_l ≥ λ_{l+1} occurs → action selection does not properly enforce Equation (17b); add constraint check before accepting actions.

- First 3 experiments:
  1. **K-estimator validation in isolation**: Generate Rician samples with known K values (0-20 dB in 2 dB steps), N ∈ {25, 50, 100, 500}. Measure estimation error vs ground truth. Compare against moment-based estimator from Equation (13). Target: replicate Figure 2 performance curves.
  2. **RL convergence with fixed K**: Set K=10 dB, Λ=4, P=20 dB, M ∈ {100, 500, 1000}. Track goodput per iteration against theoretical maximum (compute via Equation (10)). Verify convergence within paper's reported iteration ranges. Measure variance across multiple random seeds.
  3. **End-to-end tracking with K variation**: Implement full system with K switching: 0 dB → 10 dB → 20 dB (as in Figure 4). Measure adaptation latency (iterations to reach within 5% of new optimum). Test whether Q-table retention enables instant re-adaptation when revisiting previously-seen K values.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the proposed framework be extended to non-Rician fading environments or alternative wireless communication scenarios?
  - Basis in paper: [explicit] The Conclusion states: "Future research could extend the proposed framework to various other wireless communication scenarios."
  - Why unresolved: The current system relies specifically on a Rician-K factor estimator and assumes a specific quasi-static fading model.
  - What evidence would resolve it: Simulation results demonstrating the RL agent's ability to adapt feedback schemes in Nakagami-m or Rayleigh fading channels.

- **Open Question 2**: How does the presence of errors in the feedback channel impact the convergence and stability of the Q-learning algorithm?
  - Basis in paper: [inferred] Section I.B explicitly assumes the receiver transmits "via an error-free quantized feedback channel."
  - Why unresolved: In practical URLLC systems, feedback channels are noisy; errors in the index mapping $L(\gamma)$ could mislead the transmitter's rate selection and distort the reward signal $\omega_t$.
  - What evidence would resolve it: Performance evaluation of the RL algorithm under non-zero feedback error rates, measuring goodput degradation and convergence time.

- **Open Question 3**: What is the impact on goodput when channel statistics vary more rapidly than the assumed 200-realization coherence period?
  - Basis in paper: [inferred] Footnote 1 states: "In our analyses, we assumed that $\mu$ remains constant for 200 channel realizations."
  - Why unresolved: If the Rician-K factor changes before the XGBoost estimator collects sufficient samples ($N=100$) or before the Q-learning agent converges, the system may operate on outdated policies.
  - What evidence would resolve it: Analysis of tracking performance and goodput when the K-factor transition intervals are shorter than the learning convergence time.

## Limitations

- **Q-learning parameter values** (α, η) are unspecified, making exact reproduction difficult
- **State space discretization** details are absent, which critically affects Q-table size and learning performance
- **XGBoost hyperparameters** are not provided, potentially affecting estimator accuracy

## Confidence

- **High**: K-factor estimator using XGBoost with empirical moments outperforms traditional methods (supported by clear comparative results)
- **Medium**: RL-based quantized feedback scheme adaptation achieves near-optimal goodput (converges to optimum in simulations but independent agent assumption introduces suboptimality)
- **Medium**: Two-phase architecture provides computational efficiency (conceptually sound but not empirically validated against alternatives)

## Next Checks

1. **Reproduce K-estimator error curves**: Generate synthetic Rician data with known K values (0-20 dB) and validate estimation accuracy versus traditional moment-based estimators
2. **Isolate RL convergence behavior**: Fix K=10 dB and track goodput per iteration for M=100 vs M=1000 to verify convergence patterns and iteration efficiency
3. **Test tracking capability under K variations**: Implement full system with K switching between 0, 10, and 20 dB to measure adaptation latency and tracking performance