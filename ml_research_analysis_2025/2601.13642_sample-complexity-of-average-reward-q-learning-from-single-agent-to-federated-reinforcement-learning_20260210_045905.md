---
ver: rpa2
title: 'Sample Complexity of Average-Reward Q-Learning: From Single-agent to Federated
  Reinforcement Learning'
arxiv_id: '2601.13642'
source_url: https://arxiv.org/abs/2601.13642
tags:
- learning
- q-learning
- sample
- federated
- average-reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies Q-learning algorithms for average-reward reinforcement\
  \ learning (RL) in both single-agent and federated settings. The key contribution\
  \ is establishing sample complexity guarantees that improve upon existing Q-learning\
  \ analyses by a factor of eO(\u2225h\u22C6\u2225\xB2/\u03B5\xB2) in the single-agent\
  \ case, achieving eO(SA\u2225h\u22C6\u2225\xB3/\u03B5\xB3), and demonstrating linear\
  \ speedup in the federated setting where M agents achieve eO(SA\u2225h\u22C6\u2225\
  \xB3/M\u03B5\xB3) per-agent sample complexity."
---

# Sample Complexity of Average-Reward Q-Learning: From Single-agent to Federated Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.13642
- Source URL: https://arxiv.org/abs/2601.13642
- Authors: Yuchen Jiao; Jiin Woo; Gen Li; Gauri Joshi; Yuejie Chi
- Reference count: 40
- Primary result: Establishes sample complexity guarantees for average-reward Q-learning in both single-agent and federated settings, achieving eO(SA∥h⋆∥³/ε³) for single-agent and eO(SA∥h⋆∥³/Mε³) per-agent for federated learning with only eO(∥h⋆∥sp/ε) communication rounds.

## Executive Summary
This paper studies Q-learning algorithms for average-reward reinforcement learning (RL) in both single-agent and federated settings. The key contribution is establishing sample complexity guarantees that improve upon existing Q-learning analyses by a factor of eO(∥h⋆∥²/ε²) in the single-agent case, achieving eO(SA∥h⋆∥³/ε³), and demonstrating linear speedup in the federated setting where M agents achieve eO(SA∥h⋆∥³/Mε³) per-agent sample complexity. The federated algorithm requires only eO(∥h⋆∥sp/ε) communication rounds, independent of the number of agents, enabling scalable collaboration. The analysis covers weakly communicating MDPs and leverages carefully designed learning rates, discount factors, and communication schedules to balance bias and variance while maintaining communication efficiency.

## Method Summary
The method employs epoch-based Q-learning with carefully scheduled discount factors γk and learning rates ηk,t. Two parameter regimes are proposed: exponential decay of (1−γk) with rescaled learning rates, and polynomial decay with constant learning rates per epoch. Both balance the bias from horizon mismatch (∥Q⋆γk − J⋆∥∞) against the stochastic convergence error (∥Qk,t − Q⋆γk∥∞). In the federated setting, each agent maintains local Q-estimates with periodic averaging at a central server. The federated parameter schedules accelerate discount factor decay by M^(1/3) and slow learning rate decay proportionally, achieving linear speedup through variance reduction from M independent samples.

## Key Results
- Single-agent Q-learning achieves eO(SA∥h⋆∥³/ε³) sample complexity, improving previous results by eO(∥h⋆∥²/ε²)
- Federated learning with M agents achieves eO(SA∥h⋆∥³/Mε³) per-agent sample complexity with linear speedup
- Communication complexity of eO(∥h⋆∥sp/ε) rounds is independent of the number of agents M

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Carefully scheduled discount factors and learning rates improve sample complexity in average-reward Q-learning by a factor of ∥h⋆∥²sp/ε² compared to prior work.
- Mechanism: The algorithm operates in epochs with epoch-dependent discount factors γk that decay toward 1. Two parameter regimes are proposed: (1) exponential decay of (1−γk) with rescaled learning rates, and (2) polynomial decay with constant learning rates per epoch. Both balance the bias from horizon mismatch (∥Q⋆γk − J⋆∥∞) against the stochastic convergence error (∥Qk,t − Q⋆γk∥∞), ensuring both errors decrease at comparable rates.
- Core assumption: Weakly communicating MDP structure with finite span norm ∥h⋆∥sp of the bias function.
- Evidence anchors:
  - [abstract] "Q-learning with carefully chosen parameters achieves sample complexity Õ(|S||A|∥h⋆∥³sp/ε³)...improving previous results by at least a factor of ∥h⋆∥²sp/ε²"
  - [section 3.2] Defines two parameter groups (8a) and (8b) with specific formulas for ηk,t, γk, Nk, and ι(k,t)
  - [corpus] Limited direct corpus support for this specific improvement claim; related work focuses on robust/constrained settings rather than this parameter scheduling approach
- Break condition: If the MDP is not weakly communicating or ∥h⋆∥sp is unbounded, the convergence bounds do not apply.

### Mechanism 2
- Claim: Federated aggregation across M agents achieves linear speedup in per-agent sample complexity.
- Mechanism: Each agent maintains local Q-estimates via independent sampling, with periodic averaging at a central server. The federated parameter schedules (12a)-(12b) accelerate discount factor decay by M^(1/3) and slow learning rate decay proportionally. This reflects variance reduction from M independent samples, allowing the same error tolerance with M× fewer samples per agent.
- Core assumption: All agents interact with the same underlying MDP; synchronous sampling with generative model access.
- Evidence anchors:
  - [abstract] "collaboration reduces the per-agent sample complexity to Õ(|S||A|∥h⋆∥³sp/(Mε³))"
  - [section 4.2] Theorem 2 states the per-agent sample bound and provides proof outline showing variance reduction factor of 1/M
  - [corpus] Federated RL for average-reward is novel; neighbor paper 100930 addresses robust average-reward RL but not federated settings
- Break condition: If agents face heterogeneous MDPs or asynchronous sampling, the linear speedup guarantee may not hold (analysis assumes homogeneous MDP).

### Mechanism 3
- Claim: Communication complexity Õ(∥h⋆∥sp/ε) is achievable and independent of the number of agents M.
- Mechanism: Two scheduling strategies—periodic communication within epochs (12a) vs. epoch-end-only communication (12b)—achieve identical communication complexity bounds. The interval gk scales with convergence requirements, not agent count. Aggregation reduces variance proportionally to M, so more agents don't require more synchronization.
- Core assumption: Central server can aggregate all agent updates; no communication failures or stragglers.
- Evidence anchors:
  - [abstract] "only Õ(∥h⋆∥sp/ε) communication rounds required...independent of the number of agents M"
  - [section 4.2] Equations (82)-(83) derive the communication bound from gk and Nk schedules
  - [corpus] Neighbor paper 6870 (robust average-reward) discusses sample complexity but not communication; no direct corpus comparison available
- Break condition: If communication bandwidth is limited or delays are unbounded, the theoretical communication schedule may not be realizable in practice.

## Foundational Learning

- Concept: **Average-reward MDPs and bias functions**
  - Why needed here: The paper's bounds depend on ∥h⋆∥sp; understanding why this replaces the discount factor is essential for interpreting results.
  - Quick check question: Can you explain why weakly communicating MDPs guarantee a state-independent optimal average reward J⋆?

- Concept: **Stochastic approximation with time-varying parameters**
  - Why needed here: The algorithm's epoch-dependent γk and ηk,t schedules require understanding how learning rate decay interacts with contraction properties.
  - Quick check question: Why must the discount factor γk approach 1 as epochs progress for average-reward estimation?

- Concept: **Federated aggregation and variance reduction**
  - Why needed here: Linear speedup depends on understanding how averaging M independent samples reduces variance and how communication intervals affect convergence.
  - Quick check question: What would happen to the speedup if agents only communicate every Nk/2 iterations instead of at the scheduled gk intervals?

## Architecture Onboarding

- Component map: Local Q-updater (per agent) -> Communication scheduler -> Global aggregator (central server) -> Parameter controller
- Critical path: Parameter initialization → Epoch loop → Local updates → Check communication schedule → Aggregate if scheduled → Update global Q → Next epoch until ||Q_K,NK − J⋆|| ≤ ε
- Design tradeoffs:
  - Parameter group 1 (8a/12a): Faster initial convergence with more frequent communication; preferred when communication cost is low
  - Parameter group 2 (8b/12b): Simpler implementation with epoch-end communication; preferred when communication is expensive or Nk is large
  - Tradeoff: Group 1 requires O(Nk/gk) communications per epoch; Group 2 requires exactly 1
- Failure signatures:
  - If ∥Q_{k,t} − J⋆∥∞ plateaus above ε, check that γk is increasing correctly (should approach 1 as k grows)
  - If federated speedup is sublinear, verify agents sample independently and aggregate correctly (bugs in averaging logic)
  - If communication rounds exceed Õ(∥h⋆∥sp/ε), verify gk schedule matches (12a) or that epoch sizes grow as specified
- First 3 experiments:
  1. **Single-agent baseline on a simple weakly communicating MDP** (e.g., 5 states, 3 actions, known h⋆): Verify sample complexity Õ(SA∥h⋆∥³sp/ε³) by measuring iterations to achieve ||Q − J⋆|| ≤ ε across multiple ε values.
  2. **Federated scaling test with M ∈ {1, 2, 4, 8, 16}**: Plot per-agent samples vs. M; confirm linear speedup (samples per agent should decrease as ~1/M).
  3. **Communication frequency ablation**: Compare parameter groups 1 and 2; measure actual communication rounds against theoretical bound Õ(∥h⋆∥sp/ε) for fixed ε.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed sample complexity guarantees for single-agent and federated average-reward Q-learning be extended to the asynchronous online setting with Markovian noise?
- Basis in paper: [explicit] Section 3.1 states, "In this paper, we focus on the synchronous sampling... assuming access to a generative model," and Section 1.2 explicitly contrasts this with prior work (e.g., Chen, 2025) that considers asynchronous settings.
- Why unresolved: The current analysis relies on the independence of samples generated by the simulator; asynchronous settings introduce temporal correlations (mixing) that fundamentally change the variance analysis of the stochastic approximation process.
- What evidence would resolve it: A finite-time analysis of the algorithm under a Markovian trajectory assumption, establishing bounds that account for mixing time or span norm dependencies in the absence of a generative model.

### Open Question 2
- Question: Can variance reduction techniques be integrated into this federated framework to achieve the minimax optimal eO(ε^{-2}) sample complexity while maintaining communication efficiency?
- Basis in paper: [explicit] Section 3.3 notes that the performance is "constrained by the behavior of Q-learning... known to be suboptimal," and contrasts the results with Lee et al. (2025), who achieved near-optimal complexity using variance reduction.
- Why unresolved: While variance reduction improves sample complexity, it typically requires maintaining additional global state variables (e.g., reference Q-functions), which may increase the per-round communication overhead or break the current analysis structure.
- What evidence would resolve it: A modified federated algorithm incorporating a variance reduction mechanism (like reference-advantage decomposition) with a proof showing eO(ε^{-2}) sample complexity and a communication complexity that remains independent of the number of agents.

### Open Question 3
- Question: How does environment heterogeneity impact the linear speedup and convergence guarantees of federated average-reward Q-learning?
- Basis in paper: [inferred] The related work section (1.2) mentions that other works (Woo et al., 2023; Yang et al., 2024) have explored heterogeneous environments, and Naskar et al. (2025) studied federated TD learning for average-reward in heterogeneous settings, whereas this paper assumes agents operate on the same MDP.
- Why unresolved: The theoretical guarantees rely on agents estimating the same optimal Q-function Q^*; heterogeneity introduces a distribution shift between local and global objectives that the current convergence analysis does not address.
- What evidence would resolve it: An analysis defining a measure of heterogeneity (e.g., gradient dissimilarity) and providing convergence bounds that degrade gracefully as heterogeneity increases, or a modified aggregation rule that specifically corrects for local environment differences.

### Open Question 4
- Question: Can the sample complexity for finding an ε-optimal policy be tightened from eO(ε^{-5}) to match the eO(ε^{-3}) complexity of value estimation?
- Basis in paper: [inferred] Table 1 and Theorem 3 show a gap between the complexity for policy learning (eO(ε^{-5})) and value estimation (eO(ε^{-3})), suggesting the policy extraction step or the parameter schedules used for policy learning may be suboptimal.
- Why unresolved: The analysis for policy learning (Theorem 3) utilizes a specific discount factor schedule that differs from the value estimation setting, potentially introducing a bottleneck where the Q-function must be estimated with excessively high precision to ensure the greedy policy is optimal.
- What evidence would resolve it: A refined analysis demonstrating that an ε-optimal policy can be extracted from the Q-estimates with the same ε^{-3} scaling, or an algorithm modification that decouples the accuracy requirements of value estimation and policy identification.

## Limitations

- The federated speedup analysis assumes homogeneous MDPs and perfect synchronization, which may not hold in practical deployments with heterogeneous agents or communication delays.
- The paper's improvement claim relies on specific parameter scheduling that has not been widely validated in prior literature.
- The analysis assumes access to a generative model with synchronous sampling, which is not realistic in many real-world RL applications.

## Confidence

- **High confidence**: Sample complexity bounds for single-agent Q-learning (Theorem 1) - the analysis follows established stochastic approximation techniques with clear parameter tuning.
- **Medium confidence**: Federated linear speedup guarantee - while the variance reduction argument is sound, real-world factors like agent heterogeneity could affect practical speedup.
- **Medium confidence**: Communication complexity bound - the analysis is rigorous but assumes idealized communication conditions.

## Next Checks

1. **Robustness test**: Implement the algorithm on MDPs with varying span norms ∥h⋆∥sp to empirically verify the claimed sample complexity improvement factor ∥h⋆∥²sp/ε².
2. **Heterogeneous agent simulation**: Test the federated algorithm with agents sampling from slightly different MDPs to quantify speedup degradation when homogeneity assumption breaks.
3. **Communication delay sensitivity**: Introduce artificial communication delays and measure impact on both convergence speed and whether the theoretical communication complexity bound remains achievable.