---
ver: rpa2
title: 'PCS-UQ: Uncertainty Quantification via the Predictability-Computability-Stability
  Framework'
arxiv_id: '2505.08784'
source_url: https://arxiv.org/abs/2505.08784
tags:
- conformal
- coverage
- prediction
- methods
- width
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PCS-UQ, a predictability-computability-stability
  framework for uncertainty quantification in machine learning models. The method
  addresses the limitations of traditional statistical approaches that rely on generative
  models and conformal inference methods that ignore model selection.
---

# PCS-UQ: Uncertainty Quantification via the Predictability-Computability-Stability Framework

## Quick Facts
- arXiv ID: 2505.08784
- Source URL: https://arxiv.org/abs/2505.08784
- Reference count: 0
- Key outcome: PCS-UQ achieves 90% coverage with 20% narrower prediction intervals than conformal methods across 17 regression and 6 classification datasets.

## Executive Summary
PCS-UQ introduces a framework for uncertainty quantification that addresses limitations of traditional statistical approaches and conformal inference. The method screens models based on predictive performance, uses bootstrap resampling to assess uncertainty, and applies multiplicative calibration for local adaptivity. Experiments show PCS-UQ achieves target coverage while reducing prediction interval width by approximately 20% compared to conformal baselines, with better subgroup coverage. For deep learning models, dropout and weight noise approximations maintain efficiency while achieving similar improvements. The framework is theoretically grounded, with a modified procedure shown to be equivalent to split conformal inference under exchangeability assumptions.

## Method Summary
PCS-UQ screens candidate models by predictive performance on a validation set, then fits the top-k models across B bootstrap samples to assess inter-sample variability. Out-of-bag predictions form the basis for uncalibrated prediction intervals, which are then multiplicatively calibrated to achieve target coverage. The framework supports both regression (prediction intervals) and multi-class classification (prediction sets). For deep learning models, dropout or weight noise perturbations approximate the bootstrap ensemble. The method assumes at least one candidate model achieves acceptable predictive accuracy and that bootstrap variance reliably estimates true uncertainty.

## Key Results
- Achieves 90% target coverage across 17 regression and 6 classification datasets
- Reduces prediction interval width by approximately 20% compared to conformal baselines
- Demonstrates better subgroup coverage, particularly for heterogeneous populations
- Deep learning approximations (dropout/noise) maintain 20% width reduction vs. conformal while using 10× fewer perturbations

## Why This Works (Mechanism)

### Mechanism 1
Screening models by predictive performance reduces prediction interval width without sacrificing coverage. PCS-UQ fits multiple candidate algorithms and retains only top-k performers on a validation set. By excluding poorly-fitting models that generate dispersed predictions, the ensemble's inter-sample variance decreases, leading to tighter intervals while maintaining calibration. Core assumption: Models with poor predictive accuracy on held-out data produce unreliable uncertainty estimates that should be excluded. Evidence: Figure 4 shows interval width increases as more (worse-performing) models are included; R² of selected models decreases while width grows.

### Mechanism 2
Bootstrap resampling creates a pseudo-population that captures finite-sample uncertainty and algorithmic instability. By fitting screened models on B bootstrapped datasets, PCS-UQ simulates multiple plausible training sets. For each test point, predictions across bootstrapped models form a distribution whose quantiles define the uncalibrated interval. Out-of-bag samples avoid data-splitting inefficiency. Core assumption: The bootstrap approximates the sampling distribution of predictions; ensemble variability reflects true uncertainty. Evidence: Section 3 states these discrete sets of bootstraps create a pseudo-population that allows assessment of finite-sample uncertainty.

### Mechanism 3
Multiplicative calibration improves local adaptivity and subgroup coverage. Rather than expanding all intervals by a fixed constant, multiplicative calibration scales interval width proportionally to its uncalibrated size. Samples with higher bootstrap prediction variance receive larger expansions, adapting to heterogeneous uncertainty across feature space. Core assumption: Local prediction variance correlates with local coverage difficulty; scaling proportionally addresses subgroup imbalance. Evidence: Figure 6 shows additive calibration fails to achieve target coverage for large-house subgroup (Miami dataset), while multiplicative succeeds.

## Foundational Learning

- Concept: **Conformal Prediction (Split Conformal)**
  - Why needed here: PCS-UQ is positioned as an alternative/complement to conformal inference. Understanding split conformal—its data split, score function, and quantile-based calibration—clarifies what PCS-UQ modifies.
  - Quick check question: Given a trained model and calibration set, how does split conformal construct a prediction interval with 90% coverage?

- Concept: **Bootstrap Resampling and Out-of-Bag (OOB) Estimates**
  - Why needed here: PCS-UQ's core uncertainty assessment uses B bootstrap samples; OOB samples are used for calibration to avoid additional data splitting.
  - Quick check question: Why might OOB estimates be preferable to holdout calibration sets in small-data regimes?

- Concept: **Prediction Intervals vs. Confidence Intervals**
  - Why needed here: PCS-UQ targets prediction intervals (covering future observations), not parameter confidence intervals. The distinction matters for interpretation and evaluation.
  - Quick check question: A 90% prediction interval for Y|X should contain what proportion of future observations?

## Architecture Onboarding

- Component map: Data splitter -> Model zoo -> Prediction screener -> Bootstrap ensemble -> OOB calibrator -> Inference module
- Critical path: Train all candidates → screen to top-k → bootstrap refit screened models → compute OOB predictions → calibrate γ → deploy. Bottleneck: Bootstrap refitting is expensive for deep learning; use dropout/noise perturbations as approximation.
- Design tradeoffs:
  - k (number of screened models): Larger k includes more diversity but increases interval width. Default: start with 3, refine.
  - B (number of bootstraps): More bootstraps stabilize estimates; stabilization after ~100. Trade compute for stability.
  - Multiplicative vs. additive calibration: Multiplicative improves subgroup coverage but requires that uncalibrated width meaningfully signals uncertainty.
  - Deep learning approximation: Dropout/noise perturbation avoids B full retrainings but achieves smaller width reductions.
- Failure signatures:
  - Under-coverage: If no model achieves strong predictive accuracy, or if bootstrap variance severely underestimates true uncertainty.
  - Over-wide intervals: If k is too large (poor models included) or if bootstrap predictions are highly dispersed.
  - Subgroup coverage failure: If using additive calibration instead of multiplicative; or if subgroups are defined by features not captured in training.
  - Computational blowup: Full bootstrap refitting for large models without using approximation schemes.
- First 3 experiments:
  1. Reproduce ablation on k: On CA Housing, vary k from 1 to 8; plot interval width and R². Confirm width increases with k while predictive quality decreases.
  2. Compare calibration schemes: On Miami Housing split by square footage, run PCS-UQ with multiplicative vs. additive calibration; verify subgroup coverage differences match Figure 6.
  3. Test deep learning approximation: On CIFAR-100, compare full PCS-UQ (B=100), dropout approximation, and noise perturbation; measure coverage, set size, and runtime.

## Open Questions the Paper Calls Out

- How can PCS-UQ be extended to produce calibrated probability intervals for binary classification rather than prediction sets? The authors note binary labels provide no direct observation of class probabilities, making evaluation of probability intervals challenging.

- How can PCS principles be applied to quantify uncertainty in large language models and generative models? The connection to hallucinations and factuality remains unexplored, requiring definitions of appropriate prediction sets and coverage notions.

- How can uncertainty from data-cleaning choices and human judgment calls throughout the data science life cycle be systematically incorporated into PCS-UQ? Quantifying uncertainty from subjective choices requires enumerating a "garden of forking paths" that is difficult to formalize.

- Can approximation schemes for deep learning be improved to match the ~26% reduction of full PCS-UQ rather than the observed ~20%? The dropout and weight noise perturbations may not fully capture the inter-sample variability that bootstrap resampling provides.

## Limitations

- Mechanism 3 confidence is Low: While ablation studies show multiplicative calibration improves subgroup coverage, the underlying assumption—that local bootstrap variance reliably signals coverage difficulty—is not directly tested.
- Deep learning approximation is underspecified: No ablation shows how B, dropout rate, or noise magnitude affect coverage-width tradeoff.
- Binary classification limitation: The framework only produces prediction sets for classification, which are unsuitable for binary classification where probability intervals are more relevant.

## Confidence

- **High**: Coverage claims on tabular regression/classification datasets; ablation showing model screening reduces interval width; equivalence proof to split conformal under exchangeability.
- **Medium**: Claim of ~20% width reduction over conformal methods (depends on baseline choices); subgroup coverage improvements (Miami house size split is only example).
- **Low**: Mechanism 3 (local adaptivity via multiplicative calibration); deep learning approximation schemes; claim that screening guarantees valid coverage when "at least one model fits well."

## Next Checks

1. **Mechanism 3 validation**: Construct a synthetic dataset where uncertainty is heteroscedastic (e.g., variance increases with feature magnitude). Run PCS-UQ with both calibration schemes; verify multiplicative calibration maintains coverage across the full feature range while additive fails in high-variance regions.

2. **Deep learning approximation sensitivity**: On CIFAR-100, systematically vary B (10, 50, 100), dropout rate (0.1, 0.3, 0.5), and noise scale. Measure coverage, interval width, and runtime; determine if approximation quality plateaus and at what cost.

3. **Screening failure mode**: Create a dataset where all candidate models have poor validation performance (e.g., highly nonlinear with discontinuities). Run PCS-UQ; check if coverage degrades and whether warning mechanisms detect screening inadequacy.