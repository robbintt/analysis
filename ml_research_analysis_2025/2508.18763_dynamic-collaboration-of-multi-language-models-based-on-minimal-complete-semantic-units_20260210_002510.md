---
ver: rpa2
title: Dynamic Collaboration of Multi-Language Models based on Minimal Complete Semantic
  Units
arxiv_id: '2508.18763'
source_url: https://arxiv.org/abs/2508.18763
tags:
- answer
- arxiv
- llms
- which
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a dynamic selection strategy for multi-language
  model collaboration that selects optimal tokens from multiple models' next-token
  distributions to enhance reasoning performance. The method introduces minimal complete
  semantic units (MCSU) to address vocabulary misalignment issues between different
  models, allowing natural alignment within linguistic space.
---

# Dynamic Collaboration of Multi-Language Models based on Minimal Complete Semantic Units

## Quick Facts
- arXiv ID: 2508.18763
- Source URL: https://arxiv.org/abs/2508.18763
- Reference count: 37
- Primary result: Proposed DDS achieves 83.4% average accuracy on reasoning benchmarks, outperforming single models (up to 82.1%) and other ensemble methods (81.9-82.1%)

## Executive Summary
This paper introduces a dynamic selection strategy for multi-language model collaboration that enhances reasoning performance by selecting optimal tokens from multiple models' next-token distributions. The method addresses vocabulary misalignment issues between different models through minimal complete semantic units (MCSU), enabling natural alignment within linguistic space without explicit projection matrices. Experiments demonstrate significant improvements across mathematical reasoning, commonsense reasoning, and symbolic reasoning benchmarks, with the approach achieving emergent capabilities where the ensemble provides correct answers even when all individual models fail.

## Method Summary
The approach uses training-free multi-model collaboration that operates at the token level during autoregressive generation. It implements minimal complete semantic units (MCSU) to handle vocabulary misalignment by treating words, punctuation, and numbers as integration units rather than individual tokens. At each generation step, the method collects top-k MCSU distributions from participating models, computes pairwise KL divergence between these distributions, and filters out divergent distributions using a threshold (ε=0.1). The retained distributions are averaged and sampled to select the next MCSU, which is then fed back as context for subsequent steps. The method requires loading all models locally simultaneously and uses greedy decoding with CoT prompting.

## Key Results
- DDS achieves 83.4% average accuracy across multiple reasoning benchmarks
- Outperforms single models (up to 82.1%) and other ensemble methods (81.9-82.1%)
- Demonstrates emergent capabilities where DDS provides correct answers even when all individual models fail
- Maintains effectiveness across cross-task and cross-lingual scenarios including code generation and Chinese evaluations
- Shows optimal KL threshold of ε=0.1 on GSM8K (85.1%) and CSQA (76.0%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Minimal Complete Semantic Units (MCSU) enable natural vocabulary alignment across heterogeneous LLMs without explicit projection matrices.
- Mechanism: Replace tokens with semantically complete units (words/punctuation/numbers) as the base integration granularity. When a model's tokenizer splits a word into subword tokens, the model continues generating until tokens form a complete MCSU. The joint probability of constituent tokens represents the MCSU's probability score.
- Core assumption: ~90% of common English words are already single tokens in most vocabularies, so MCSU introduces minimal overhead. Whitespace/punctuation reliably delimit MCSUs in alphabetic languages.
- Evidence anchors: Abstract mentions MCSU enables natural alignment; Section 3.1 defines MCSU probability via product of joint probabilities; Appendix D shows 88.9-92.1% of common words encode as single tokens across Qwen/Llama/GLM.

### Mechanism 2
- Claim: Distribution distance-based dynamic selection filters out harmful probability distributions, improving ensemble quality beyond naive averaging.
- Mechanism: At each autoregressive step, compute pairwise KL divergence between models' top-k MCSU distributions. Distributions with divergence < threshold ε (set to 0.1) are retained and averaged; outliers are discarded. This implements the principle "correct answers converge; incorrect answers diverge."
- Core assumption: Models near the correct answer will have similar distributions; erroneous models will produce diverse, distant distributions.
- Evidence anchors: Abstract mentions distribution distance-based dynamic selection; Section 3.2 defines KL filtering with ε=0.1 shown optimal in Table 7; "The Law of Multi-Model Collaboration" discusses scaling limits of ensembling.

### Mechanism 3
- Claim: Token-level knowledge combination creates an expanded answer space that can yield correct outputs even when no individual model succeeds.
- Mechanism: Combine next-MCSU distributions from multiple LLMs into a unified distribution. The combined answer space is the union of individual answer spaces, assigning higher probability to answers that multiple models support. This enables "emergent" corrections during autoregressive generation.
- Core assumption: Knowledge is encoded in next-token distributions; combining distributions combines knowledge without requiring any single model to be correct.
- Evidence anchors: Abstract mentions emergent capabilities where DDS provides correct answers even when all models fail; Section 4.6, Table 8 shows concrete examples with all three models wrong but DDS correct; "CURE" shows confidence-driven multi-model ensemble benefits for medical QA.

## Foundational Learning

- Concept: **KL Divergence as Distribution Similarity**
  - Why needed here: Core metric for DDS filtering; understanding what KL measures (asymmetric, 0=identical, >1=divergent) is essential for threshold tuning.
  - Quick check question: If P=[0.9, 0.1] and Q=[0.1, 0.9], is D_KL(P||Q) small or large?

- Concept: **Autoregressive Generation and Next-Token Distributions**
  - Why needed here: DDS operates at each generation step; understanding how LLMs sample from softmax distributions is prerequisite for debugging integration logic.
  - Quick check question: Why does greedy decoding differ from sampling with temperature?

- Concept: **Vocabulary/tokenizer heterogeneity across LLMs**
  - Why needed here: The core problem MCSU solves; different tokenizers split text differently, making naive probability averaging impossible.
  - Quick check question: Why can't you directly average probability vectors from Llama-3 and Qwen-2 for the same input?

## Architecture Onboarding

- Component map: MCSU Detector -> Distribution Collector -> KL Filter -> Aggregator -> Autoregressive Loop

- Critical path:
  1. Load all participating models (Qwen-2-7B, Llama-3-8B, GLM-4-9B) into memory simultaneously
  2. At each step, query each model's next-token distribution given shared context
  3. Accumulate tokens until MCSU boundary detected per model
  4. Compute joint probability per MCSU, extract top-k candidates
  5. Compute KL matrix, filter, average, sample
  6. Append sampled MCSU to context, repeat until EOS

- Design tradeoffs:
  - Memory vs. throughput: Must load all models concurrently (unlike API-based voting); scales linearly with model count
  - ε threshold sensitivity: Too low = DDS inactive (keep all); too high = harmful distributions included (Table 7 shows sharp performance drop outside 0.08-0.15)
  - Top-k size: Paper uses k=5; larger k increases computation but may miss rare-correct answers with smaller k

- Failure signatures:
  - All models retained at every step (ε too low or distributions highly divergent) → reverts to naive averaging
  - Single high-confidence wrong model dominates filtered average (Appendix F scenario)
  - MCSU accumulation loops indefinitely (malformed input without delimiters)

- First 3 experiments:
  1. Reproduce GSM8K baseline: Run DDS with Qwen+Llama+GLM, ε=0.1, k=5; verify ~85.1% accuracy matches paper
  2. Ablate ε threshold: Sweep ε ∈ {0.05, 0.08, 0.1, 0.15, 0.2} on held-out subset; plot accuracy curve to validate 0.1 optimal
  3. Test emergent capability: Construct adversarial set where all models fail individually; measure DDS success rate (paper shows non-zero but doesn't quantify)

## Open Questions the Paper Calls Out

- **Inference efficiency**: How can the inference efficiency of DDS be optimized to mitigate the linear scaling of computational costs associated with loading and running multiple models simultaneously? The authors explicitly state in the Limitations section that reasoning time and calculation will increase compared to single model reasoning, and improving efficiency is an issue that needs attention.

- **Safety alignment bypass**: Does token-level multi-model collaboration bypass the safety alignment of individual models, increasing the risk of generating harmful content? The Limitations section notes we must pay attention to whether this approach might bypass some of the restrictions of single-model generated answers, leading to the production of unethical or harmful content.

- **Dynamic selection refinement**: How can the dynamic selection strategy be refined to prevent failures when individual models produce incorrect answers with high confidence? Analysis of failure cases in Appendix F reveals that DDS often fails when a model provides an incorrect MCSU with excessively high confidence, which dominates the integration process despite being wrong.

## Limitations

- **Language dependency**: The MCSU approach relies on whitespace and punctuation to delimit semantic units in alphabetic languages, breaking down for logographic languages like Chinese and potentially agglutinative languages without clear word boundaries.
- **Memory and scalability**: The approach requires loading all models simultaneously into GPU memory, making it infeasible for large models (70B+ parameters) and imposing significant hardware requirements.
- **Threshold calibration**: The KL threshold ε=0.1 is determined via global mean statistics on the test set itself, creating potential overfitting and not exploring whether the optimal threshold is task-dependent.

## Confidence

- **High confidence**: The mathematical framework for MCSU probability computation (product of conditional probabilities) is sound and directly implementable. The empirical improvements over single models and other ensemble methods are well-documented across multiple benchmarks.
- **Medium confidence**: The distribution filtering mechanism works as described, but the optimal threshold determination methodology is questionable due to test-set-based calibration. The emergent capability claims are supported by examples but lack systematic quantification.
- **Low confidence**: Cross-lingual effectiveness claims are supported by single-number improvements on Chinese benchmarks without ablation studies or analysis of linguistic factors affecting performance. The scalability analysis is absent despite explicit claims about the method being "memory-hungry."

## Next Checks

1. **Threshold generalization test**: Implement ε calibration on a validation split separate from test data. Run DDS across ε ∈ {0.05, 0.08, 0.1, 0.15, 0.2} on held-out validation sets for each task, then report final test performance using validation-selected thresholds. This validates whether the 0.1 optimal threshold generalizes or is overfit.

2. **Linguistic boundary analysis**: Systematically evaluate MCSU performance across language families with varying tokenization characteristics: (a) logographic (Chinese/Japanese), (b) agglutinative (Turkish/Finnish), (c) scripts without clear boundaries (Thai). Compare accuracy degradation against English baseline to quantify linguistic robustness.

3. **Distribution similarity validation**: Construct adversarial datasets where all models share common misconceptions (e.g., "A bat is a bird" questions). Measure whether DDS successfully identifies and corrects these shared errors, or whether distribution similarity leads to collective failure. Quantify the frequency of this failure mode in real benchmarks.