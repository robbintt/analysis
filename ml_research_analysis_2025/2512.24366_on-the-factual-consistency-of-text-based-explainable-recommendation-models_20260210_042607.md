---
ver: rpa2
title: On the Factual Consistency of Text-based Explainable Recommendation Models
arxiv_id: '2512.24366'
source_url: https://arxiv.org/abs/2512.24366
tags:
- metrics
- factual
- explainable
- recommendation
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for evaluating the factual consistency
  of text-based explainable recommendation models. The authors design a prompting-based
  pipeline that extracts atomic explanatory statements from user reviews, creating
  ground-truth explanations that focus on factual content.
---

# On the Factual Consistency of Text-based Explainable Recommendation Models

## Quick Facts
- **arXiv ID:** 2512.24366
- **Source URL:** https://arxiv.org/abs/2512.24366
- **Reference count:** 35
- **Primary result:** While models achieve high semantic similarity scores (BERTScore F1: 0.81-0.90), factuality metrics reveal alarmingly low performance (LLM-based precision: 4.38%-32.88%)

## Executive Summary
This paper introduces a framework for evaluating the factual consistency of text-based explainable recommendation models. The authors design a prompting-based pipeline that extracts atomic explanatory statements from user reviews, creating ground-truth explanations that focus on factual content. Applying this pipeline to five Amazon Reviews categories, they construct augmented benchmarks for fine-grained evaluation. They propose statement-level alignment metrics combining LLM- and NLI-based approaches to assess factual consistency and relevance. Across experiments on six state-of-the-art models, they find that while models achieve high semantic similarity scores, factuality metrics reveal critically low performance, highlighting the gap between surface-level fluency and factual accuracy in explainable recommendation systems.

## Method Summary
The authors develop a prompting-based pipeline using LLMs to extract atomic explanatory statements from reviews, creating ground-truth explanations focused on factual content. The framework constructs triplet sets of (statement, topic, polarity) for each review, then aggregates these into ground-truth paragraphs. They propose two families of statement-level alignment metrics: LLM-based St2Exp-P/R/F1 that evaluate each generated statement against full reference using an LLM scoring function, and NLI-based StEnt/StCoh using DeBERTa entailment/contradiction probabilities over statement pairs. The evaluation is applied to six state-of-the-art models across five Amazon categories, revealing that high semantic similarity scores systematically mask low factual consistency.

## Key Results
- Models achieve high semantic similarity scores (BERTScore F1: 0.81-0.90) but factuality metrics show alarmingly low performance (LLM-based statement-level precision: 4.38%-32.88%)
- Negative coherence scores (StCoh-R as low as -0.317) indicate models generate statements contradicting references
- Pearson correlation between BERTScore and St2Exp metrics is only r=0.476-0.480, showing they measure fundamentally different qualities
- Factuality metrics consistently reveal critical gaps across all six tested models and five Amazon categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing reviews into atomic statement-topic-sentiment triplets isolates factual content from noise for evaluation.
- Mechanism: LLM prompting extracts atomic explanatory statements with domain-specific topic labels and polarity (positive/negative/neutral), then rule-based aggregation constructs ground-truth paragraphs preserving all extracted content without additional LLM cost.
- Core assumption: Atomic statements faithfully capture the factual content users care about, and LLM extraction preserves rather than distorts ground truth.
- Evidence anchors:
  - [abstract] "We design a prompting-based pipeline that uses LLMs to extract atomic explanatory statements from reviews, thereby constructing a ground truth that isolates and focuses on their factual content."
  - [Section 3.1] Eq. (1) formalizes triplet extraction: S_ui = LLM(t_ui | ·, T, P) = {(s_1, t_1, p_1), ···, (s_n, t_n, p_n)}
  - [corpus] Limited direct corpus support for this specific extraction approach; G-Refer uses graph retrieval rather than statement decomposition.
- Break condition: If extraction misses key explanatory content or introduces systematic bias (authors acknowledge this limitation in Section 5).

### Mechanism 2
- Claim: Statement-level alignment metrics reveal factual inconsistencies that semantic similarity scores (BERTScore, BLEURT) systematically miss.
- Mechanism: Two parallel metric families—(1) LLM-based St2Exp-P/R/F1 using LLM scoring function f_LLM to evaluate each generated statement against full reference, and (2) NLI-based StEnt/StCoh using DeBERTa entailment/contradiction probabilities over statement pairs.
- Core assumption: Models can achieve high surface similarity while generating factually unsupported claims—fluency and factuality are orthogonal.
- Evidence anchors:
  - [abstract] "while models achieve high semantic similarity scores (BERTScore F1: 0.81-0.90), all our factuality metrics reveal alarmingly low performance (LLM-based statement-level precision: 4.38%-32.88%)"
  - [Section 4.3] Figure 1 shows Pearson correlation r=0.476-0.480 between BERTScore and St2Exp metrics—moderate but not determinative.
  - [corpus] Corpus evidence is limited; related work focuses on generation methods rather than factuality evaluation specifically.
- Break condition: If metrics don't correlate with human judgment of factual accuracy (not validated in this work).

### Mechanism 3
- Claim: Current training objectives optimize lexical/syntactic patterns without grounding outputs in verifiable user preferences.
- Mechanism: Standard cross-entropy loss on text generation rewards fluent continuation patterns; no explicit signal penalizes claims contradicted by user review history or unsupported by item attributes.
- Core assumption: The observed factuality gap stems from training objective misalignment, not model capacity limitations alone.
- Evidence anchors:
  - [Section 5] "models have learned to generate fluent, contextually appropriate text that appears explanatory but frequently fails to ground its claims in verifiable evidence"
  - [Section 4.3] Negative coherence scores (StCoh-R as low as -0.317) indicate models generate statements contradicting references.
  - [corpus] arXiv:2504.05315 notes similar coherence issues between predicted ratings and explanations in multi-task setups.
- Break condition: If architectural modifications (e.g., retrieval-augmented grounding) could close the gap without changing training objectives.

## Foundational Learning

- Concept: **Natural Language Inference (NLI)**
  - Why needed here: Core to NLI-based metrics (StEnt, StCoh) that compute entailment/contradiction probabilities between statement pairs using DeBERTa-large-mnli.
  - Quick check question: Given two statements—"The fabric is soft" and "The material feels comfortable"—what NLI label (entailment/neutral/contradiction) should dominate, and why?

- Concept: **Semantic Similarity vs. Factual Consistency**
  - Why needed here: Paper's central finding is that BERTScore F1 (0.81-0.90) and St2Exp-P (4.38%-32.88%) measure fundamentally different qualities; conflation leads to misleading conclusions.
  - Quick check question: Why might paraphrasing cause high semantic similarity while factual contradictions still exist?

- Concept: **Atomic Fact Decomposition**
  - Why needed here: Enables fine-grained evaluation by breaking composite explanations into verifiable units; allows precision/recall at statement level rather than holistic scoring.
  - Quick check question: What information is lost when evaluating "The phone has great battery life and the camera is terrible" as a single unit vs. two atomic statements?

## Architecture Onboarding

- **Component map:**
  - Extraction Pipeline: Llama-3-8B-Instruct with domain-specific prompts → statement-topic-sentiment triplets → rule-based aggregation → ground-truth explanation paragraph
  - Evaluation Suite: Llama-3.1-8B-Instruct for St2Exp metrics (statement-to-explanation scoring) + DeBERTa-large-mnli for StEnt/StCoh metrics (pairwise entailment/contradiction)
  - Benchmark Data: Five Amazon categories (Toys, Clothes, Beauty, Sports, Cellphones) with ~4-5 statements per interaction, ~140K-220K training samples each

- **Critical path:**
  1. Define domain-specific topic set T (10 topics per category in this work)
  2. Design extraction prompt with illustrative examples
  3. Run LLM extraction on all reviews → triplet sets S_ui
  4. Aggregate triplets → ground-truth explanations via rule-based procedure
  5. Generate explanations with target model
  6. Extract statements from generated explanations
  7. Apply St2Exp (LLM) and StEnt/StCoh (NLI) metrics

- **Design tradeoffs:**
  - LLM-based extraction: flexible but may introduce errors; authors used manual verification to mitigate
  - Rule-based vs. LLM aggregation: rule-based preserves all content at cost of unnatural structure
  - LLM vs. NLI metrics: LLM preserves full context (more expensive), NLI enables pairwise scaling (may lose context)

- **Failure signatures:**
  - High BERTScore + low St2Exp-P: model generates fluent but hallucinated content
  - Negative StCoh scores: model produces statements contradicting reference
  - Low St2Exp-R: model fails to cover ground-truth explanatory passages
  - Large variance in NLI-based scores (std ~0.2-0.4): unstable pairwise comparison

- **First 3 experiments:**
  1. **Baseline verification:** Run provided evaluation code on one Amazon category with Att2Seq and XRec to reproduce the BERTScore vs. St2Exp gap—confirm your pipeline matches reported ranges.
  2. **Ablation on statement extraction:** Replace Llama-3-8B-Instruct with a smaller model (e.g., Llama-3.1-3B) for extraction; measure impact on ground-truth quality and downstream metric consistency.
  3. **Domain transfer test:** Apply extraction pipeline to a new domain not in the five categories (e.g., Books); manually verify triplet quality and identify topic set requirements for reliable extraction.

## Open Questions the Paper Calls Out
- How to mitigate factual hallucinations in generated explanations (current methods show poor factuality despite fluency)
- Whether the extraction pipeline faithfully captures the factual content users care about (potential systematic bias)
- How to develop training objectives that explicitly ground generated explanations in verifiable user preferences rather than optimizing for fluency alone

## Limitations
- The LLM-based extraction of atomic statements may miss key explanatory content or introduce systematic bias, unquantified despite manual verification
- No human validation studies confirm that proposed metrics align with human judgments of factual accuracy and relevance
- Framework was developed and validated on five specific Amazon categories; generalizability to other domains remains untested
- Ground-truth construction relies entirely on review quality, inheriting potential biases from user-generated content

## Confidence

**High Confidence**: The core finding that semantic similarity metrics (BERTScore 0.81-0.90) and factuality metrics (LLM-based precision 4.38%-32.88%) reveal fundamentally different aspects of model performance. This is supported by consistent experimental results across six models and five categories.

**Medium Confidence**: The claim that current training objectives create the factuality gap. While the experimental evidence shows models generate fluent but factually inconsistent explanations, the paper doesn't establish causal link between training objectives and observed behavior.

**Low Confidence**: Claims about the extraction pipeline faithfully capturing factual content users care about. Without systematic evaluation of extraction quality or bias analysis, the reliability of ground-truth construction remains uncertain.

## Next Checks

1. **Human Validation Study**: Conduct user studies where participants rate generated explanations for factual accuracy and relevance. Compare human judgments with St2Exp, StEnt, and StCoh scores to establish metric validity and identify potential misalignments.

2. **Cross-Domain Transfer Test**: Apply the complete pipeline (extraction + evaluation) to a new recommendation domain (e.g., movie recommendations) with different linguistic patterns and user expectations. Measure extraction quality, metric stability, and whether the factuality gap persists.

3. **Ablation on Extraction Quality**: Systematically vary extraction quality by using different LLM models (from Llama-3.1-3B to Llama-3.1-70B) for ground-truth construction. Measure impact on downstream factuality metrics to quantify sensitivity to extraction errors and establish error bounds for the framework.