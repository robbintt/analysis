---
ver: rpa2
title: 'ArVoice: A Multi-Speaker Dataset for Arabic Speech Synthesis'
arxiv_id: '2505.20506'
source_url: https://arxiv.org/abs/2505.20506
tags:
- speech
- arabic
- corpus
- were
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ArVoice, a multi-speaker Modern Standard
  Arabic (MSA) speech corpus with diacritized transcriptions designed for multi-speaker
  speech synthesis. The dataset combines professionally recorded speech from six voice
  talents, modified subsets of existing corpora, and high-quality synthetic speech
  from commercial systems, totaling 83.52 hours across 11 voices.
---

# ArVoice: A Multi-Speaker Dataset for Arabic Speech Synthesis

## Quick Facts
- arXiv ID: 2505.20506
- Source URL: https://arxiv.org/abs/2505.20506
- Reference count: 0
- Multi-speaker Arabic speech corpus with diacritized transcriptions for TTS and voice conversion

## Executive Summary
ArVoice is a comprehensive multi-speaker Modern Standard Arabic speech corpus designed for speech synthesis and voice conversion tasks. The dataset contains 83.52 hours of speech across 11 voices, combining professionally recorded human speech, modified existing corpora, and high-quality synthetic speech. The corpus includes diacritized transcriptions, which are crucial for resolving pronunciation ambiguity in Arabic. Experiments with three TTS models (ArTST-tts, VITS, Fish-Speech) and two voice conversion systems (AAS-VC, KNN-VC) demonstrate that diacritized transcripts significantly improve TTS quality, with VITS achieving the best performance. Voice conversion models achieved speaker similarity scores of 0.69-0.72 with false acceptance rates of 0.81-0.95.

## Method Summary
The ArVoice corpus combines four data sources: professionally recorded speech from six voice talents, modified subsets of existing corpora (Khaleej, Arabic Speech Corpus), and high-quality synthetic speech from commercial systems. The dataset totals 83.52 hours across 11 voices with diacritized transcriptions for TTS and VC tasks. Three open-source TTS models (ArTST-tts, VITS, Fish-Speech) were fine-tuned on the corpus, along with two voice conversion systems (AAS-VC for parallel data, KNN-VC for non-parallel). Evaluation used ASR-based WER metrics and crowdsourced subjective preference tests, with speaker similarity measured using ECAPA-TDNN models.

## Key Results
- Diacritized transcripts improved VITS WER from 35.69% to 20.67%
- Multi-speaker training preferred even for single-target speaker synthesis
- Synthetic augmentation reduced WER by 8.2% absolute with 44% subjective preference
- VITS achieved best TTS performance among tested models
- Voice conversion achieved SS scores of 0.69-0.72 with FAR of 0.81-0.95

## Why This Works (Mechanism)

### Mechanism 1
Diacritized transcripts improve TTS intelligibility by explicitly specifying vowel information that resolves pronunciation ambiguity. Arabic orthography without diacritics can map to multiple phonetic realizations (e.g., /suk:ar/ 'sugar' vs. /sak:ara/ 'he closed'). Diacritized inputs reduce the search space for acoustic models by providing explicit phonetic targets. This mechanism works best for end-to-end models like VITS that can leverage character-level phonetic distinctions, though models pre-trained on undiacritized text (e.g., ArTST) may not benefit consistently.

### Mechanism 2
Multi-speaker training improves synthesis quality through implicit regularization and exposure to broader acoustic-linguistic patterns. Joint training across speakers allows models to learn more robust phoneme-to-speech mappings that transfer to individual speakers. Speaker embeddings (x-vectors) effectively disentangle speaker identity from linguistic content, enabling cross-speaker data utilization. This approach works when speaker embeddings capture identity independently of linguistic content and the speaker space isn't too heterogeneous.

### Mechanism 3
High-quality synthetic parallel speech improves TTS through data augmentation by expanding training data coverage. Synthetic speech from commercial TTS systems provides additional aligned text-audio pairs with varied phonetic contexts and speaking patterns. The model learns from these additional examples even if synthetic quality isn't perfect. This mechanism requires synthetic speech quality to be high enough that artifacts don't dominate the learning signal, and quality filtering may be necessary to avoid transferring systematic pronunciation errors.

## Foundational Learning

- **Arabic Diacritization (Tashkīl)**: Diacritics are optional in Arabic orthography but essential for unambiguous pronunciation. Without them, TTS models must guess vowels, leading to intelligibility errors. Quick check: Given "كتب" (k-t-b), name at least three possible pronunciations with different diacritics and their meanings?
- **Speaker Embeddings (x-vectors, ECAPA-TDNN)**: Multi-speaker TTS requires disentangling speaker identity from content. X-vectors encode speaker characteristics into fixed-dimensional vectors for conditioning. Quick check: How would you verify that a speaker embedding model captures identity independent of linguistic content?
- **Voice Conversion vs. TTS**: ArVoice supports both tasks. VC transforms source speech to target voice without text; TTS generates speech from text. Different data requirements (parallel vs. non-parallel). Quick check: For parallel VC model like AAS-VC, what data format is required, and how does KNN-VC differ in its data needs?

## Architecture Onboarding

- **Component map**: Text Input → Text Cleaner (Arabic + diacritics) → Phoneme/Character Encoder → Speaker Embedding (x-vector from SpeechBrain) → Acoustic Model (VITS/ArTST/Fish-Speech) → Vocoder (HiFi-GAN/FF-GAN) → Waveform Output

- **Critical path**: 
  1. Text preprocessing: Remove markdown, segment sentences (20-50 chars), handle digits via PyArabic with manual verification
  2. Diacritization verification: Manually inspect transcripts against recordings (speakers may deviate from written diacritics)
  3. Speaker embedding extraction: Extract x-vectors using SpeechBrain toolkit for all training speakers
  4. Model training: Fine-tune from pre-trained checkpoints with default hyperparameters
  5. Evaluation: ASR-based WER for intelligibility + crowdsourced preference tests (minimum 10 evaluators per sample)

- **Design tradeoffs**:
  | Decision | Option A | Option B | Guidance |
  |----------|----------|----------|----------|
  | Text input | Diacritized | Undiacritized | Diacritized preferred for VITS; no significant difference for ArTST (pre-training dependent) |
  | Training scope | Multi-speaker | Single-speaker | Multi-speaker preferred even for single target voice |
  | Data source | Human only | Human + Synthetic | Synthetic augmentation reduces WER but verify quality |
  | VC architecture | Parallel (AAS-VC) | Non-parallel (KNN-VC) | AAS-VC achieves higher speaker similarity (0.72 vs 0.69); KNN-VC more flexible |

- **Failure signatures**:
  - WER > 100% on generated speech: Model architecture incompatible with language (e.g., Fish-Speech without Arabic support)
  - High diacritic mismatch errors: Speakers deviated from transcripts during recording; requires manual post-hoc correction
  - Poor speaker similarity (SS < 0.5): Insufficient speaker data or embedding quality issues
  - ASC transcript errors (WER ~45%): Original ASC uses non-standard spelling; requires GPT-4 + manual correction

- **First 3 experiments**:
  1. Baseline VITS training: Train VITS on Part 1 (diacritized human speech, 4 speakers) with default hyperparameters. Evaluate on held-out test set using ASR WER and 10+ native speaker preference ratings. Target: WER < 25%.
  2. Diacritization ablation: Compare VITS trained on diacritized vs. undiacritized transcripts using same data. Run pairwise preference test (25 samples, 10+ raters each). Expect significant preference for diacritized version.
  3. Synthetic augmentation test: Add Part 4 (synthetic speech) to training for half the epochs. Measure WER reduction (target: ~8% absolute improvement) and preference shift. Verify synthetic data doesn't introduce artifacts in subjective evaluation.

## Open Questions the Paper Calls Out

- **Can speech-based diacritic restoration models trained on ArVoice achieve lower error rates on MSA transcripts compared to text-based models trained primarily on Classical Arabic?** The paper states existing text-based diacritic restoration systems trained on Classical Arabic result in high diacritic error rates when applied to Modern Standard Arabic speech transcripts, and describes ArVoice as useful for speech-based diacritic restoration, but does not train or evaluate such models.

- **What architectural or training modifications are needed to make Fish-Speech perform competently on Arabic TTS?** Fish-Speech produced extremely low quality speech with WER above 100%, leading to its exclusion from further analysis, while VITS and ArTST performed adequately on the same data. The paper does not investigate root causes or potential modifications.

- **Can voice conversion systems trained on ArVoice achieve speaker similarity and false acceptance rates indistinguishable from same-speaker comparisons?** Voice conversion systems achieved SS of 0.69-0.72 and FAR of 0.81-0.95, with a footnote noting the ground truth SS score was 0.81, indicating a gap between converted and real speech in speaker verification metrics.

## Limitations

- The study shows clear benefits of diacritization for VITS but not for ArTST-tts, suggesting model architecture dependence that requires further investigation.
- Voice conversion results (SS 0.69-0.72, FAR 0.81-0.95) show moderate speaker similarity but may not meet production quality thresholds.
- Dataset composition varies across parts with different recording conditions, diacritization levels, and synthetic components, potentially introducing confounding factors in evaluation.

## Confidence

- **High Confidence**: Diacritized transcripts improve VITS intelligibility (WER reduction 35.69%→20.67%), multi-speaker training benefits single-speaker synthesis (subjective preference), and VITS outperforms other TTS models on Arabic.
- **Medium Confidence**: Synthetic augmentation provides consistent WER improvements (8.2% absolute), and parallel voice conversion (AAS-VC) achieves higher speaker similarity than non-parallel (KNN-VC).
- **Low Confidence**: Cross-model generalizability of diacritization benefits, synthetic data quality impact across all speakers, and voice conversion quality meeting production standards.

## Next Checks

1. Conduct cross-validation across all three TTS models (VITS, ArTST, Fish-Speech) with identical diacritized datasets to isolate architecture effects from language modeling capacity.

2. Perform systematic synthetic data quality assessment by comparing generated speech against professional recordings using both objective metrics (MUSHRA, CMOS) and subjective preference tests across multiple Arabic dialects.

3. Extend voice conversion evaluation with larger test sets (n>100 samples) and diverse speaker pairs to validate the 0.69-0.72 speaker similarity scores represent consistent performance across the corpus.