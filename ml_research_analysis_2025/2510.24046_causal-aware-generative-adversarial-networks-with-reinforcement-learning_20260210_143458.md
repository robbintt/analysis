---
ver: rpa2
title: Causal-Aware Generative Adversarial Networks with Reinforcement Learning
arxiv_id: '2510.24046'
source_url: https://arxiv.org/abs/2510.24046
tags:
- data
- causal
- real
- tabular
- ca-gan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CA-GAN, a GAN-based method for generating
  synthetic tabular data that explicitly preserves causal relationships. The method
  uses a two-step approach: causal graph extraction to learn the data''s underlying
  causal structure, followed by a custom Conditional WGAN-GP that generates variables
  in topological order based on the causal graph.'
---

# Causal-Aware Generative Adversarial Networks with Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.24046
- Source URL: https://arxiv.org/abs/2510.24046
- Reference count: 40
- This paper introduces CA-GAN, a GAN-based method for generating synthetic tabular data that explicitly preserves causal relationships, outperforming six state-of-the-art methods across key metrics.

## Executive Summary
This paper introduces CA-GAN, a GAN-based method for generating synthetic tabular data that explicitly preserves causal relationships. The method uses a two-step approach: causal graph extraction to learn the data's underlying causal structure, followed by a custom Conditional WGAN-GP that generates variables in topological order based on the causal graph. A novel reinforcement learning-based objective aligns causal graphs from real and fake data using non-differentiable SHD scores as rewards. Experiments on 14 datasets show CA-GAN outperforms six state-of-the-art methods across key metrics: causal preservation (average SHD score of 3.78), utility preservation (F1-score of 0.659), and privacy preservation (re-identification risk of 0.155). The method achieves this while maintaining computational efficiency and balancing realism with privacy protection.

## Method Summary
CA-GAN uses a two-step approach: first, it extracts a causal graph from real data using the PC algorithm to learn the data's underlying causal structure and topological ordering. Second, it employs a custom Conditional WGAN-GP architecture that generates variables sequentially according to this topological order, with each sub-generator conditioned on its parent variables. A reinforcement learning objective aligns the causal graphs from real and fake data by using non-differentiable SHD scores as rewards, optimized through the REINFORCE trick. The generator loss combines adversarial loss with causal loss weighted by λ=0.01, while the discriminator is trained with standard WGAN-GP objective. The method is trained using Adam optimizer with learning rate 2e-4 and β₁=0.5.

## Key Results
- Achieves average SHD score of 3.78 for causal preservation across 14 datasets
- Maintains utility with F1-score of 0.659 on downstream classification tasks
- Provides strong privacy protection with re-identification risk of 0.155
- Outperforms six state-of-the-art methods across all three metrics
- Demonstrates computational efficiency despite additional causal constraints

## Why This Works (Mechanism)

### Mechanism 1: Topological Causal Conditioning
Generating variables sequentially based on a causal graph's topological order preserves dependencies better than simultaneous generation. The architecture employs M sub-generators, one per variable. Instead of generating all columns from noise, a sub-generator Gⱼ takes the specific values of its parent nodes Pa(Xⱼ) (already generated due to topological order) as conditioning inputs. This enforces the causal skeleton Xⱼ = f(Pa(Xⱼ), z).

### Mechanism 2: Reinforcement Learning for Non-Differentiable Graph Alignment
Optimizing for structural graph similarity (SHD) is feasible within a neural generator despite the metric being non-differentiable, achieved by treating generation as a stochastic policy. The generator acts as a policy network. It produces a sample x̂, and the Structural Hamming Distance (SHD) between the inferred fake graph and real graph serves as the reward signal R(x̂). The system uses the REINFORCE trick (log-probability scaling) to backpropagate gradients that maximize this reward (minimize SHD).

### Mechanism 3: Wasserstein Distance with Gradient Penalty (WGAN-GP)
Combining causal constraints with the WGAN-GP loss maintains training stability and prevents mode collapse. The discriminator (critic) is trained to estimate the Wasserstein distance between real and generated distributions, regularized by a gradient penalty. This provides a stable "adversarial loss" (L_adv) that runs alongside the causal loss (L_causal), preventing the generator from producing causally valid but statistically unrealistic data.

## Foundational Learning

- **Concept: Causal Discovery (PC Algorithm & DAGs)**
  - Why needed here: You cannot generate data "causally" without first extracting the causal skeleton. The PC algorithm uses conditional independence tests to orient edges.
  - Quick check question: If variable A causes B, and B causes C, why must A be generated before C?

- **Concept: Policy Gradient (REINFORCE)**
  - Why needed here: Standard backpropagation cannot flow through a discrete metric like SHD (which involves counting edge differences). You need to understand how "log-prob × reward" moves the probability mass.
  - Quick check question: Why do we multiply the log-probability of the generated sample by the reward score when calculating the gradient?

- **Concept: Gumbel-Softmax Relaxation**
  - Why needed here: To apply backpropagation to categorical variables within the generator, discrete one-hot vectors must be approximated by continuous differentiable tensors during training.
  - Quick check question: How does the temperature parameter τ in Gumbel-Softmax trade off between discrete accuracy and gradient flow?

## Architecture Onboarding

- **Component map:** Pre-processor (PC Algorithm) → Generator (M sub-generators) → Discriminator (MLP critic) → RL Evaluator (PC Algorithm)
- **Critical path:** The Causal Discovery Phase. If the PC algorithm infers a wrong graph (e.g., incorrect edge orientation), the sub-generators will be conditioned on the wrong parents, and the RL reward will reinforce incorrect structures.
- **Design tradeoffs:** Compute vs. Fidelity - Computing SHD inside the training loop requires running the PC algorithm repeatedly on generated batches, significantly increasing training time. Lambda (λ) tuning - If the causal weight λ is too high, the model ignores data realism; if too low, it acts like a standard GAN with no causal guarantee.
- **Failure signatures:** Cyclic Gradients - If the topological sort fails or the graph has cycles, the generation pipeline will crash or stall. Reward Hacking - The generator might learn to output simple, repetitive patterns that are easy for the PC algorithm to classify as the target graph, failing to capture data diversity.
- **First 3 experiments:**
  1. **Sanity Check (Oracle Graph):** Manually define a simple 4-node linear graph. Train CA-GAN and verify if the SHD drops to 0.
  2. **Ablation (RL component):** Run CA-GAN with λ=0 (removing RL loss). Compare SHD scores to see the marginal benefit of the reinforcement learning loop.
  3. **Lambda Sensitivity:** Sweep λ (e.g., 0.001, 0.01, 0.1) on a dataset like `adult`/`bank` to find the "cliff" where adversarial training destabilizes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CA-GAN scale to high-dimensional datasets (e.g., >100 features) given the computational bottleneck of running the PC algorithm and calculating SHD rewards at every training epoch?
- Basis: The paper notes that CA-GAN is slower than CTGAN due to repeated SHD computations, yet experiments were limited to datasets with only 4 to 30 features.
- Why unresolved: The complexity of constraint-based causal discovery (PC) typically scales poorly with dimension, potentially making the per-epoch reward calculation prohibitively expensive for wide datasets.
- What evidence would resolve it: Runtime and performance benchmarks on high-dimensional tabular datasets.

### Open Question 2
- Question: How robust is the framework to errors in the initial causal graph extraction (G_real)?
- Basis: The method relies on a fixed "base causal graph" extracted initially, assuming the PC algorithm provides a reliable ground truth.
- Why unresolved: If the initial graph contains spurious edges or incorrect orientations, the topological ordering and reinforcement rewards might enforce an incorrect causal structure on the synthetic data.
- What evidence would resolve it: Sensitivity analysis measuring performance degradation when varying levels of noise are injected into the initial causal graph.

### Open Question 3
- Question: Does minimizing Structural Hamming Distance (SHD) guarantee accurate interventional causal inference on the synthetic data?
- Basis: The introduction emphasizes the risk of misleading inferences under intervention, but the evaluation relies on observational metrics (SHD) and downstream classification (F1).
- Why unresolved: Structural similarity (SHD) does not necessarily guarantee that the generated data preserves the quantitative causal effects (e.g., treatment efficacy) required for valid policy decisions.
- What evidence would resolve it: Evaluation using interventional metrics, such as the Average Treatment Effect (ATE) error between real and synthetic data.

## Limitations
- Heavy reliance on PC algorithm output introduces critical fragility - structural errors in the initial causal graph will cascade through the generation pipeline
- Computational overhead of running PC repeatedly during training (to compute SHD rewards) is significant and not fully quantified
- The method's effectiveness on high-dimensional datasets (>100 features) remains unproven due to PC algorithm scalability constraints

## Confidence

- **High confidence:** The technical feasibility of combining WGAN-GP with topological conditioning and the basic formulation of the REINFORCE objective
- **Medium confidence:** The empirical claim that this approach achieves state-of-the-art balance across all three metrics (SHD, F1, re-identification risk) due to potential dataset-specific performance variations
- **Low confidence:** The assumption that the RL reward signal is stable and effective across all dataset sizes and types, given the non-differentiable nature of SHD and the small batch sizes used for reward computation

## Next Checks

1. **Robustness to PC Variability:** Systematically vary the PC algorithm's significance level (alpha) and independence test type, then measure the resulting SHD scores and utility metrics to identify the stability range of CA-GAN's performance.

2. **Reward Signal Analysis:** Monitor the variance of the SHD reward during training across multiple runs. If variance is high, implement a baseline subtraction or increase batch size for reward computation and compare the impact on convergence speed and final SHD.

3. **Complexity Benchmarking:** Quantify the wall-clock time per epoch for CA-GAN versus a non-causal baseline (like CTGAN) on a medium-sized dataset (e.g., `bank`), isolating the time spent on PC computation versus neural network training to assess the practical cost of causal preservation.