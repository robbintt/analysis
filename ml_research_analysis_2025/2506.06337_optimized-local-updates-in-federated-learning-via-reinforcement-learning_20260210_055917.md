---
ver: rpa2
title: Optimized Local Updates in Federated Learning via Reinforcement Learning
arxiv_id: '2506.06337'
source_url: https://arxiv.org/abs/2506.06337
tags:
- client
- optimized
- learning
- data
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses performance degradation in Federated Learning
  (FL) due to non-IID data distributions across clients. The authors propose using
  Deep Reinforcement Learning (DRL) to dynamically select optimized subsets of local
  training data for each client during FL rounds.
---

# Optimized Local Updates in Federated Learning via Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2506.06337
- **Source URL**: https://arxiv.org/abs/2506.06337
- **Reference count**: 40
- **Key outcome**: DRL-guided data partitioning improves post-FL fine-tuning accuracy by up to 10 percentage points across CIFAR-10, CIFAR-100, and FashionMNIST datasets using FedAvg, FedAvgM, FedMedian, FedProx, and FedCDA frameworks

## Executive Summary
This paper addresses performance degradation in Federated Learning caused by non-IID data distributions across clients. The authors propose using Deep Reinforcement Learning to dynamically select optimized subsets of local training data for each client during FL rounds. The DRL agent learns to partition the dataset by class, maximizing training loss reduction while minimizing data usage. After FL rounds, the optimized client fine-tunes on its complete local dataset, mitigating non-IID effects. Experiments show the optimized client consistently outperforms naive clients in post-FL fine-tuning across multiple FL frameworks with accuracy gains of up to 10 percentage points and faster convergence.

## Method Summary
The framework uses DDPG (Deep Deterministic Policy Gradient) to train an agent that outputs class-wise sample weights for local data partitioning during FL rounds. Each client maintains an actor-critic network that observes local F1-score as state and outputs actions determining data subset composition. The reward function balances loss reduction against data usage, encouraging efficient training. During FL rounds, clients train on the optimized subset and send updates to the server. After FL completion, each optimized client fine-tunes the aggregated model on its complete local dataset. The approach is evaluated across CIFAR-10, CIFAR-100, and FashionMNIST datasets using FedAvg, FedAvgM, FedMedian, FedProx, and FedCDA frameworks with 8 clients over 100 rounds.

## Key Results
- Post-FL fine-tuning accuracy improves by up to 10 percentage points compared to naive clients
- Optimized clients converge faster during fine-tuning phase across all tested datasets and FL frameworks
- The approach maintains competitive performance while reducing data exposure during FL rounds
- Theoretical analysis provides an upper bound on performance improvement

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Data Sub-setting for Non-IID Alignment
Restricting local training data via DRL-guided selection mitigates conflicting gradient updates caused by non-IID data distributions. The DRL agent outputs a vector of weights for each class, creating an "Action Partitioned Dataset" rather than using the full local dataset. By training on this subset, the client minimizes the risk that its local updates diverge significantly from the global objective or "cancel out" the updates of other clients.

### Mechanism 2: Loss-Guided Reward Shaping
A reward signal based on the reduction of local training loss effectively guides the DRL agent to select data partitions that maximize local performance improvement per round. The reward function compares the loss of the aggregated global model against the loss of the locally updated model, incentivizing actions that result in the steepest loss descent.

### Mechanism 3: Sequential Knowledge Transfer via Fresh Data
Withholding a portion of local data during FL rounds allows the client to use "fresh" gradients for post-FL personalization, enhancing final performance. The framework operates in two phases: FL rounds train on the optimized subset, and a final local training phase uses the entire local dataset to fine-tune the aggregated model. The data excluded during FL serves as a "reservoir" of fresh information to adapt the global model to the local distribution.

## Foundational Learning

- **Concept: Federated Learning (FL) & Non-IID Data**
  - **Why needed here:** The paper attempts to solve the "client drift" or performance drop caused by non-IID data (data that is not independent and identically distributed across clients). Understanding FedAvg is required to see why training on less data might actually help alignment.
  - **Quick check question:** Can you explain why averaging model updates from clients with vastly different data distributions might hurt the global model's accuracy on any single client?

- **Concept: Deep Deterministic Policy Gradient (DDPG)**
  - **Why needed here:** The paper uses DDPG, an actor-critic algorithm, to handle the continuous action space (the ratio of data to select for each class). Unlike discrete RL (e.g., Q-learning), DDPG can output specific float values (e.g., "use 42.5% of class 0").
  - **Quick check question:** Why would a continuous action space algorithm be preferred over a discrete one for determining the "amount" of data to use?

- **Concept: Overfitting vs. Generalization in FL**
  - **Why needed here:** The paper argues that "oversharing" local data leads to overfitting to the local distribution, which degrades the global model (and subsequently the local model after aggregation). The mechanism relies on the tension between local optimization and global contribution.
  - **Quick check question:** How does limiting local training epochs or data size traditionally help prevent a local model from diverging too far from the global initialization?

## Architecture Onboarding

- **Component map:** Server -> Client (Trainer) -> Client (DRL Agent) -> Data Partitioner -> Local Model Training
- **Critical path:** 1. Server broadcasts global model w_t. 2. Client computes local F1-Score (State s_t) on w_t using local validation data. 3. DRL Agent observes s_t and outputs action a_t (class-wise weights). 4. Data Partitioner constructs subset D'_k using a_t. 5. Client trains on D'_k and sends update to Server. 6. Post-FL: Client takes final global model and trains on full D_k.
- **Design tradeoffs:** Performance vs. Privacy (trades maximum local performance during FL for reduced data exposure and better post-FL performance), Compute Overhead (introduces DRL inference overhead on client side), Temporal Lag (optimized client performs worse than naive clients during FL rounds, only surpassing them after post-FL fine-tuning phase).
- **Failure signatures:** Agent Collapse (DRL agent learns to output near-zero weights to minimize penalty term λ), Metric Saturation (if local F1-score is 0 or 1, State signal provides no gradient for agent to learn from), Post-FL Divergence (if FL phase converges to too generic global model, fine-tuning phase may fail to converge quickly).
- **First 3 experiments:** 1. Sanity Check (Static Partition): Run FL framework with fixed, random partition (e.g., 50% of each class) to verify dynamic DRL is necessary. 2. Reward Sensitivity: Vary normalization parameter λ to observe trade-off between data usage amount and loss reduction reward. 3. Post-FL Ablation: Run full pipeline but skip final "train on full dataset" step to isolate whether performance gain comes from DRL partitioning or final fine-tuning on fresh data.

## Open Questions the Paper Calls Out
1. How can multi-agent reinforcement learning be integrated into this framework to support simultaneous optimization across multiple clients?
2. Can the framework be modified to maintain competitive accuracy during FL rounds without sacrificing final post-FL performance?
3. Does the "optimized" data selection strategy remain effective if all clients in the federation adopt it simultaneously?

## Limitations
- The computational overhead of running a DDPG agent on each client (requiring local F1-score calculation and DRL inference) is not thoroughly discussed in terms of its impact on training time or resource constraints.
- The claim that DRL-guided data sub-setting "consistently outperforms" naive clients across all tested FL frameworks rests on ablation studies that compare against FedAvg baselines without testing against more recent personalized FL methods.
- The assertion that "oversharing" local data causes performance degradation is supported by intuition but lacks rigorous validation - it would be valuable to quantify exactly how much information is "wasted" during standard FL training.

## Confidence
- **High confidence**: The mechanism that withholding data during FL provides "fresh" information for post-FL fine-tuning - empirically demonstrated across multiple datasets and frameworks in Tables I and II
- **Medium confidence**: The claim that DRL-guided partitioning mitigates non-IID effects better than fixed partitioning - while supported by results, ablation studies could be more comprehensive
- **Low confidence**: The theoretical upper bound on performance improvement - analysis provides bounds but doesn't rigorously connect them to practical gains observed

## Next Checks
1. **Baseline expansion**: Compare against state-of-the-art personalized FL methods (FedPer, LG-FedAvg) to establish whether the DRL approach provides unique advantages beyond what's achievable with existing personalization techniques
2. **Resource overhead quantification**: Measure the wall-clock time and computational resources required for DRL inference and local F1-score calculation, comparing against performance gains to assess practical viability
3. **Cross-architecture generalization**: Test the framework with different backbone architectures (e.g., MobileNet, Vision Transformer) to verify that performance gains aren't specific to ResNet50 and that the DRL agent generalizes across model families