---
ver: rpa2
title: 'Protect: Towards Robust Guardrailing Stack for Trustworthy Enterprise LLM
  Systems'
arxiv_id: '2510.13351'
source_url: https://arxiv.org/abs/2510.13351
tags:
- safety
- text
- label
- explanation
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Protect addresses the critical need for robust, multi-modal guardrailing
  in enterprise LLM deployments by integrating text, image, and audio safety checks
  into a unified framework. It uses fine-tuned LoRA adapters trained on a diverse,
  multi-modal dataset with teacher-assisted relabeling for improved accuracy and explainability.
---

# Protect: Towards Robust Guardrailing Stack for Trustworthy Enterprise LLM Systems

## Quick Facts
- arXiv ID: 2510.13351
- Source URL: https://arxiv.org/abs/2510.13351
- Reference count: 40
- Primary result: Achieves state-of-the-art multi-modal safety classification across text, image, and audio with low inference latency

## Executive Summary
Protect introduces a robust guardrailing framework for enterprise LLM systems by integrating fine-tuned, category-specific LoRA adapters across text, image, and audio modalities. The system leverages teacher-assisted relabeling with reasoning traces to improve label fidelity and model explainability. By addressing four critical safety dimensions—toxicity, sexism, data privacy, and prompt injection—Protect demonstrates superior performance over leading baselines while maintaining real-time deployment viability through latency-decoupled inference.

## Method Summary
Protect uses category-specific LoRA adapters (rank 8) trained on a multi-modal dataset covering text, image, and audio safety dimensions. The framework employs teacher-assisted relabeling with reasoning traces to improve label quality, using Gemini-2.5-Pro to generate explanations and proposed relabels verified by human auditors. Synthetic audio is generated via CosyVoice 2.0 with systematic variation across emotion, accent, speaking rate, and style. Four LoRA variants per category (Vanilla, Thinking, Explanation, Comprehensive) are trained using Axolotl with AdamW optimizer, achieving efficient specialization while preserving the base Gemma-3n-E4B-it model's multi-modal representations.

## Key Results
- Achieves state-of-the-art performance on multi-modal safety classification across all four categories
- Outperforms leading baselines in Failed-class F1 scores, demonstrating superior minority-class detection
- Maintains low inference latency suitable for real-time applications with TTL of ~65ms for text

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Category-specific LoRA adapters enable efficient, specialized safety classification across modalities.
- Mechanism: Low-rank matrices (rank 8) are injected into attention and MLP layers of a frozen Gemma-3n-E4B-it backbone. Separate adapters are trained for toxicity, sexism, privacy, and prompt injection, allowing the base model's multi-modal representations to be steered toward distinct safety tasks without full fine-tuning.
- Core assumption: Specialization per safety domain yields better detection than a single unified classifier, and the base model already encodes transferable cross-modal features.
- Evidence anchors:
  - [abstract]: "Protect integrates fine-tuned, category-specific adapters trained via Low-Rank Adaptation (LoRA) on an extensive, multi-modal dataset covering four safety dimensions."
  - [section]: Section 3.1 specifies LoRA configuration (r=8, targeting attention & MLP layers) and that adapters were trained for each of the four safety categories.
  - [corpus]: Neighbor papers discuss multi-modal safety and guardrailing (e.g., Phare, SafeCoop), but direct evidence for category-specific LoRA adapters is limited; corpus signals are weak here.
- Break condition: If the base model lacks robust cross-modal grounding, adapter specialization may not rescue performance—especially for nuanced image/audio safety.

### Mechanism 2
- Claim: Teacher-assisted relabeling with reasoning traces improves label fidelity and model interpretability.
- Mechanism: A teacher model (Gemini-2.5-Pro) generates chain-of-thought reasoning and explanation traces for each sample, proposing relabels that are human-verified. Text permits bidirectional relabeling; images allow only conservative Passed→Failed changes; audio inherits text labels.
- Core assumption: Context-aware reasoning reduces false positives/negatives from keyword-based or decontextualized annotations.
- Evidence anchors:
  - [abstract]: "Our teacher-assisted annotation pipeline leverages reasoning and explanation traces to generate high-fidelity, context-aware labels across modalities."
  - [section]: Section 2.2 reports a 20.90% (text) and 20.33% (image) disagreement rate between original and teacher-proposed labels, with human-in-the-loop verification.
  - [corpus]: Corpus references context-dependent toxicity evaluation (e.g., Ruddit study cited in Section 2.2), supporting the premise that context matters, though direct replication of this specific relabeling pipeline is not shown.
- Break condition: If the teacher model encodes systematic biases or produces reasoning that diverges from human auditor judgments, relabeling could propagate errors at scale.

### Mechanism 3
- Claim: Systematically varied synthetic audio augments training for native audio guardrailing without relying on lossy transcription.
- Mechanism: Text samples are synthesized via CosyVoice 2.0 with a full-factorial grid over emotion, accent, speaking rate, and style; ambient noise is overlaid at SNRs 5–20 dB. This preserves acoustic cues (tone, affect, background sounds) lost in cascaded approaches.
- Core assumption: Acoustic diversity in synthetic data generalizes to real-world audio safety scenarios.
- Evidence anchors:
  - [abstract]: Mentions native multi-modal operation across text, image, and audio, and the need to avoid cascaded approaches.
  - [section]: Section 2.1 describes the TTS pipeline, 200 reference speakers, instruction grid, and noise augmentation to cover varied acoustic conditions.
  - [corpus]: Corpus lacks strong evidence directly validating synthetic audio for guardrailing; this is a gap.
- Break condition: If synthetic artifacts or domain shift cause the model to overfit to synthetic cues, real audio performance may degrade.

## Foundational Learning

### Concept: Low-Rank Adaptation (LoRA)
- Why needed here: Protect uses LoRA to train lightweight, task-specific adapters atop a frozen multi-modal backbone. Understanding rank, target modules, and gradient accumulation is necessary for reproducibility.
- Quick check question: What layers are typically targeted for LoRA injection, and why might attention and MLP layers be chosen for safety classification?

### Concept: Chain-of-Thought Prompting
- Why needed here: The teacher-assisted relabeling relies on reasoning traces; understanding CoT helps diagnose why certain variants (Thinking/Explanation) perform differently.
- Quick check question: How does generating intermediate reasoning steps affect label consistency and model explainability?

### Concept: Multi-modal Fusion and Latency-Decoupled Inference
- Why needed here: Protect must handle text, image, and audio inputs and decouple label emission from explanation generation for real-time use.
- Quick check question: Why is Time-to-Label (TTL) a critical metric separate from total response latency in production guardrails?

## Architecture Onboarding

### Component map:
Base model (Gemma-3n-E4B-it) -> LoRA adapters (4 categories, 4 variants each) -> vLLM serving engine with token streaming

### Critical path:
1. Curate and harmonize public/private datasets; map to binary Pass/Fail schema
2. Run teacher-assisted relabeling with reasoning traces; human-verify changes
3. Synthesize audio with acoustic variation; augment with noise
4. Train category-specific LoRA adapters using Axolotl (3 epochs, lr 1e-4)
5. Deploy with vLLM; stream tokens to emit label early (TTL ~65ms for text)

### Design tradeoffs:
- Vanilla vs Explanation Assistant: Vanilla prioritizes speed; Explanation adds auditability but increases latency (~65ms TTL vs ~653ms total for text)
- Conservative image relabeling (only Passed→Failed) vs bidirectional text relabeling reduces false negative risk but may retain some false positives
- LoRA rank 8 keeps adapter size small; higher rank could improve nuance detection at cost of memory/throughput

### Failure signatures:
- Complex memes: Over-sensitive to satire (false positives) or misses culturally embedded harmful tropes (false negatives)
- Audio: If synthetic training doesn't cover real accents/noise profiles, detection may degrade
- Prompt injection: Evolving attack patterns may outpace dataset coverage

### First 3 experiments:
1. Baseline comparison: Evaluate all four adapters against Gemma-3n-E4B-it baseline on held-out text/image/audio splits; record per-class F1 and accuracy
2. Variant ablation: Compare Vanilla, Thinking, Explanation, and Comprehensive assistants for each safety category; analyze tradeoffs between Failed-class F1 and explainability
3. Latency profiling under load: Measure TTL and total response latency across modalities with vLLM; quantify tail latencies (p95, p99) and compare to WildGuard/LlamaGuard-4 on text

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the absence of image-modality data for prompt injection attacks create an exploitable vulnerability in multi-modal deployments?
- Basis in paper: [explicit] Page 3 states "the prompt injection category contains no image-modality data, a consequence of the scarcity of public examples for this attack vector."
- Why unresolved: No image-based prompt injection training data exists in Protect, leaving this attack surface untested and potentially unprotected.
- What evidence would resolve it: Curating image-based prompt injection examples and benchmarking Protect's detection performance against text-based attacks.

### Open Question 2
- Question: Does training on synthetic TTS-generated audio generalize to real-world audio containing natural artifacts, overlapping speech, and acoustic conditions not captured in synthetic data?
- Basis in paper: [inferred] Section 2.1 creates all audio via CosyVoice 2.0 TTS from text; no evaluation on natural/recorded audio safety data is reported.
- Why unresolved: Synthetic audio may lack the acoustic diversity, recording artifacts, and spontaneous speech patterns present in real enterprise voice interactions.
- What evidence would resolve it: Evaluating Protect on a benchmark of human-recorded audio with safety violations.

### Open Question 3
- Question: Does the 21% relabeling rate from the teacher model (Gemini-2.5-Pro) introduce systematic annotation biases that affect Protect's safety judgments?
- Basis in paper: [inferred] Table 3 shows ~21% of labels were changed by the teacher model; Section 2.2 relies entirely on Gemini-2.5-Pro for reasoning and relabeling.
- Why unresolved: Teacher model biases could propagate into training labels, systematically affecting what Protect learns to flag or ignore.
- What evidence would resolve it: Comparing Protect trained on teacher-relabeled vs. original labels against human-annotated gold standards.

### Open Question 4
- Question: Can cultural and contextual understanding for complex meme-based content be improved without sacrificing latency for real-time deployment?
- Basis in paper: [explicit] Page 10 identifies limitations with "oversensitive interpretations of satire" and "failure to grasp culturally-embedded harmful tropes."
- Why unresolved: Current errors suggest the model lacks robust cross-cultural commonsense reasoning, but proposed solutions (richer datasets) may conflict with latency constraints.
- What evidence would resolve it: Benchmarking on culturally-diverse meme datasets and measuring latency impact of expanded training data.

### Open Question 5
- Question: What mechanisms can enable continuous dataset expansion for rapidly-evolving attack vectors like prompt injection while maintaining annotation quality?
- Basis in paper: [explicit] Page 3 identifies "continuous expansion of our dataset, particularly for dynamic categories like prompt injection and toxicity, as a key priority for future work."
- Why unresolved: Novel attack patterns emerge faster than manual or teacher-assisted annotation pipelines can capture them.
- What evidence would resolve it: Developing and evaluating automated or crowdsourced annotation pipelines that can track emerging threats with low latency.

## Limitations
- Private dataset dependence limits reproducibility and external validation of generalization claims
- Synthetic audio fidelity uncertainty introduces potential domain shift for real-world audio safety detection
- Teacher model bias in relabeling may propagate systematic errors into training labels

## Confidence

### High confidence
- LoRA adapter specialization mechanism and latency-decoupled inference design

### Medium confidence
- Teacher-assisted relabeling benefits and multi-modal training pipeline

### Low confidence
- Long-term robustness against evolving threats and generalization to unseen private data

## Next Checks
1. Independent relabeling audit: Use a different strong model (e.g., Claude 3.5) to regenerate reasoning and labels on a held-out subset. Compare label agreement with original Gemini-2.5-Pro outputs and human ground truth to quantify relabeling bias.

2. Synthetic-to-real audio transfer: Curate a small real-world audio safety dataset (e.g., from podcasts or call transcripts). Evaluate Protect's audio adapter performance on this held-out set and compare to synthetic-only benchmarks to measure domain shift impact.

3. Dynamic prompt injection resilience test: Construct an adversarial prompt injection benchmark with novel patterns not present in the training corpora. Measure adapter detection rates and track false negative trends over time to assess adaptability limits.