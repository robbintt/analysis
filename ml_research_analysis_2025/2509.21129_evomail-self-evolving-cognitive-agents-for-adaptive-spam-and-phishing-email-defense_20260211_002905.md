---
ver: rpa2
title: 'EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing Email
  Defense'
arxiv_id: '2509.21129'
source_url: https://arxiv.org/abs/2509.21129
tags:
- evomail
- graph
- wang
- arxiv
- spam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EvoMail addresses the challenge of adaptive spam and phishing email
  defense in the face of evolving multi-modal attacks. It introduces a self-evolving
  cognitive agent framework that constructs a unified heterogeneous email graph integrating
  textual content, metadata, URLs, and attachments, then applies an LLM-enhanced cognitive
  graph neural network for context-aware reasoning.
---

# EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing Email Defense

## Quick Facts
- **arXiv ID:** 2509.21129
- **Source URL:** https://arxiv.org/abs/2509.21129
- **Reference count:** 12
- **Primary result:** Achieves 92.8% accuracy and 89.6% F1-score on spam/phishing detection, outperforming traditional and neural baselines through self-evolving adversarial learning.

## Executive Summary
EvoMail introduces a self-evolving cognitive agent framework for adaptive spam and phishing email defense. The system constructs a unified heterogeneous email graph integrating textual content, metadata, URLs, and attachments, then applies an LLM-enhanced cognitive graph neural network for context-aware reasoning. A key innovation is an adversarial self-evolution loop where a "red-team" generates evasion tactics while a "blue-team" detector learns from failures through experience compression. Experiments demonstrate state-of-the-art performance with significant improvements over traditional and neural baselines while providing interpretable reasoning traces.

## Method Summary
EvoMail addresses adaptive spam/phishing defense by constructing a heterogeneous email graph from multi-modal features (text, metadata, URLs, attachments) and applying a Cognitive Graph Neural Network (COGGNN) enhanced with LLM-guided attention. The system operates through an adversarial self-evolution loop: a red-team generates evasion tactics via gradient-based perturbation and semantic mutation, while a blue-team detector learns from failures by compressing experience traces into a memory module. The framework achieves real-time performance through adaptive neighbor sampling and memory-efficient experience compression, validated on real-world datasets (Enron-Spam, Ling-Spam, SpamAssassin, TREC) and synthetic adversarial variants.

## Key Results
- Achieves 92.8% accuracy and 89.6% F1-score on real-world email datasets, significantly outperforming traditional and neural baselines.
- Self-evolution loop improves detection performance from ~0.78 to ~0.89 F1 across 10 iterations, with smallest AUC drop (0.047) from P1→P3 among tested methods.
- Ablation studies show LLM-guided attention contributes 14.2 F1 points, self-evolution adds 8.7 points, and memory compression provides 4.8 points of performance.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLM-guided attention improves detection by fusing semantic reasoning with graph structure, outperforming pure structural GNNs.
- **Mechanism:** For each node pair (u,v), a structured prompt encodes node descriptions, relation types, and task context via an LLM encoder. The resulting embedding P(u,v) feeds into an MLP attention scorer that supplements traditional structural attention (degree, path length, relation weights). This allows the model to attend to neighbors based on semantic relevance, not just topology.
- **Core assumption:** The LLM can capture contextual relationships between email entities that pure structural features miss; the prompt template sufficiently encodes task-relevant context.
- **Evidence anchors:** [abstract] "A Cognitive Graph Neural Network (COGGNN) enhanced by a Large Language Model (LLM) performs context-aware reasoning across these sources"; [section: CogGNN] Equations 20-24 define llm_score(u,v) = MLP_attn(P(u,v)) combined with struct_score and representation similarity via attention weights; [corpus] Weak direct evidence—neighbor papers use LLMs for classification (e.g., "Phishing Email Detection Using Large Language Models") but not for graph attention specifically.

### Mechanism 2
- **Claim:** Red-blue adversarial self-evolution sustains detection performance under distribution shift by surfacing novel evasion tactics and compressing failures into reusable memory.
- **Mechanism:** The red team generates adversarial emails via gradient-based perturbation (Eq. 30), semantic mutation (Eq. 31), and hybrid combination (Eq. 32), optimizing a multi-objective reward balancing novelty, evasion success, and complexity. The blue team identifies failures (f_θ(E) < δ_fail where ground truth = spam), extracts attention traces, clusters failures via k-medoids, and stores compressed exemplars in LRU memory. The adversarial loss (Eq. 45) trains the detector to correctly classify both adversarial and benign examples, while the consistency loss (Eq. 44) ensures memory retention.
- **Core assumption:** The red team's adversarial generation sufficiently approximates real attacker behavior; compressed traces preserve decision-relevant features; memory budget M_max is adequate for tactic diversity.
- **Evidence anchors:** [abstract] "EvoMail engages in an adversarial self-evolution loop: a 'red-team' agent generates novel evasion tactics... while the 'blue-team' detector learns from failures, compresses experiences into a memory module"; [section: Self-Evolution] Figure 3 shows F1 improving from ~0.78 to ~0.89 across 10 iterations; Table 4 shows smallest AUC drop (0.047) from P1→P3 vs. baselines (0.061-0.070); [corpus] "A Comprehensive Analysis of Adversarial Attacks against Spam Filters" documents adversarial vulnerability of deep learning filters but does not validate self-evolution specifically.

### Mechanism 3
- **Claim:** Heterogeneous graph construction enables cross-modal correlation detection that single-modality models miss.
- **Mechanism:** Emails are decomposed into nodes of multiple types (emails, senders, domains, URLs, attachments) with edges capturing relations (sent-to, contains, hosted-on, linked-to, replied-to). Features span text (TF-IDF), metadata (hour, weekday, length, attach_count), and network signals (sender_reputation, domain_age, url_count). Relation weights (Eq. 6-9) combine domain match, temporal decay, semantic similarity (BERT), and sender identity, enabling the GNN to propagate suspicion across modalities (e.g., forged header + suspicious domain + obfuscated URL).
- **Core assumption:** The selected features and relation types capture the signals attackers leave; threshold ε_r for edge creation appropriately balances graph density vs. noise.
- **Evidence anchors:** [abstract] "constructs a unified heterogeneous email graph that fuses textual content, metadata (headers, senders, domains), and embedded resources (URLs, attachments)"; [section: Heterogeneous Graph Construction] Equations 2-5 define multi-modal feature extraction; Equations 6-10 define relation modeling and edge construction; [corpus] Moderate support—"MultiPhishGuard" and "Debate-Driven Multi-Agent LLMs" both emphasize multi-modal/heterogeneous signals but use different fusion approaches.

## Foundational Learning

- **Graph Neural Networks (GNNs) and Attention Mechanisms**
  - **Why needed here:** The core architecture is a multi-layer GNN with LLM-guided attention. Understanding message passing, neighbor aggregation, and attention weight computation is essential to debug the COGGNN module.
  - **Quick check question:** Given a node v with neighbors {u1, u2, u3} and attention scores [0.5, 0.3, 0.2], what is the aggregated representation if W_neigh · h = [1,0], [0,1], [1,1] respectively?

- **Adversarial Machine Learning (Gradient-based Attacks, Evasion)**
  - **Why needed here:** The red team uses FGSM-style gradient perturbation (Eq. 30) and semantic mutation. Understanding the attack objective (maximize detector loss while maintaining semantic validity) is critical to interpret red-team outputs.
  - **Quick check question:** In Eq. 30, if ε = 0.1 and ∇_E log f_θ(E_seed) = [0.5, -0.3, 0.8], what perturbation is added to E_seed?

- **Experience Replay and Memory Compression for Continual Learning**
  - **Why needed here:** The blue team compresses failure traces via k-medoids and stores them in LRU memory to avoid catastrophic forgetting. Understanding clustering objectives and memory budgets is essential to tune M_max and k_t.
  - **Quick check question:** If memory budget M_max = 100 and current memory |M_t| = 80 with 30 new failures, how many clusters does k-medoids create (per Eq. 40)?

## Architecture Onboarding

- **Component map:** Multi-modal feature extraction → Heterogeneous graph construction → COGGNN with LLM-guided attention → Prediction layer → (if self-evolution enabled) Red-team adversarial generation → Blue-team failure detection → Experience compression → Memory update → Combined loss computation
- **Critical path:** Graph construction → COGGNN forward pass → prediction → (if self-evolution enabled) red-team generation → failure detection → memory update → backward pass with combined loss. The LLM attention call (Eq. 13, 20) is the inference bottleneck.
- **Design tradeoffs:**
  - **K (neighbors per node):** Higher K captures more context but increases O(K · C_LLM) cost; ablation suggests K ∈ {8,16,24,32} with validation tuning
  - **M_max (memory budget):** Larger memory retains more failure patterns but increases L_cons computation; default not specified, monitor compression efficiency
  - **LLM capacity:** Table 5 shows 15M → 76M improves Acc (91.6→92.8) and CIM (0.63→0.70) with diminishing returns; balance against inference latency
- **Failure signatures:**
  - **Low CIM with high accuracy:** LLM attention not contributing meaningfully; check prompt template quality and LLM encoder output
  - **Rapid performance degradation across phases:** Memory not retaining useful traces; check k-medoids clustering quality and LRU eviction
  - **High F1 on static but low on P3 Novel:** Red-team adversarial samples not representative; inspect Novelty/Evasion/Complexity tradeoff (Eq. 36)
  - **Attention weights uniform (all ~1/K):** Struct score and LLM score may be canceling; check β, γ weights and temperature τ
- **First 3 experiments:**
  1. **Reproduce ablation:** Run Full, w/o Context, w/o Query Opt, w/o Memory on Enron-Spam. Verify F1 drops match Table 3 (14.2, 3.3, 4.8 points). If Context drop is smaller, inspect LLM encoder integration.
  2. **Self-evolution trajectory:** Train on P1, iteratively update on P2-P3 with 10% novel attacks. Plot F1 per iteration; should match Figure 3 (0.78→0.89). If plateau early, check red-team reward weights (w_n, w_e, w_c).
  3. **Interpretability validation:** Run explanation module on 20 manually labeled phishing emails. Have a security analyst rate CIM dimensions (coherence, relevance, auditability). Target: CIM > 0.65. If lower, inspect evidence path extraction thresholds (α_min, D_max).

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the text provided.

## Limitations
- Relies on proprietary datasets (Ling-Spam, SpamAssassin) and synthetic adversarial corpus without public availability, limiting reproducibility.
- Key components including the LLM attention head architecture (15M vs 76M parameters), LoRA configurations, prompt template, and SUMMARIZE function are underspecified.
- Self-evolution loop assumes red-team generated samples approximate real attacker behavior without empirical validation of this assumption.

## Confidence
- **High:** Heterogeneous graph construction enables cross-modal correlation detection (strong evidence from ablation: w/o Context drops F1 by 14.2 points).
- **Medium:** Red-blue adversarial self-evolution sustains detection under distribution shift (moderate evidence: AUC drop P1→P3 is smallest among baselines, but no real-world adversarial validation).
- **Medium:** LLM-guided attention improves detection by fusing semantic reasoning with graph structure (weak direct evidence; relies on neighbor papers using LLMs for classification, not graph attention).

## Next Checks
1. **Prompt template validation:** Implement a BERT-base encoder with a structured prompt (node descriptions, relation types, task context) and measure attention score quality vs. structural attention alone.
2. **Red-team sample authenticity:** Compare novelty scores and evasion rates of generated samples to real-world phishing campaigns; if divergence exceeds 15%, adjust reward weights or add human-in-the-loop filtering.
3. **Memory compression efficiency:** Track k-medoids clustering quality (silhouette score) and LRU eviction impact on F1; if memory budget M_max < 50, monitor catastrophic forgetting.