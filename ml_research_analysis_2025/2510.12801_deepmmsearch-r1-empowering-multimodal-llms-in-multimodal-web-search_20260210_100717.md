---
ver: rpa2
title: 'DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search'
arxiv_id: '2510.12801'
source_url: https://arxiv.org/abs/2510.12801
tags:
- search
- image
- information
- answer
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeepMMSearch-R1 is a multimodal LLM that performs on-demand, multi-turn
  web searches for knowledge-intensive visual question answering. It dynamically crafts
  queries for both image and text search tools, using cropped image search to focus
  on relevant regions and iterative text search refinement for self-correction.
---

# DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search

## Quick Facts
- arXiv ID: 2510.12801
- Source URL: https://arxiv.org/abs/2510.12801
- Reference count: 29
- DeepMMSearch-R1 achieves state-of-the-art performance on multimodal web search, surpassing RAG workflows and prompt-based agents by +21.13% and +8.89% respectively while maintaining general VQA capabilities.

## Executive Summary
DeepMMSearch-R1 is a multimodal large language model designed to perform on-demand, multi-turn web searches for knowledge-intensive visual question answering. The model dynamically crafts queries for both image and text search tools, using cropped image search to focus on relevant regions and iterative text search refinement for self-correction. Trained on a novel DeepMMSearchVQA dataset with structured tool annotations, it demonstrates significant improvements over existing approaches while maintaining strong performance on general visual question answering tasks.

## Method Summary
DeepMMSearch-R1 employs a two-stage training pipeline: supervised finetuning (SFT) on a carefully curated DeepMMSearchVQA dataset, followed by online reinforcement learning optimization using Group-Relative Policy Optimization (GRPO). The model uses a LoRA-based adaptation of Qwen2.5-VL-7B-Instruct, maintaining frozen vision encoders while adapting the LLM for tool use. A key innovation is the use of cropped image search via referring expression grounding, which improves retrieval accuracy by focusing on query-relevant image regions rather than whole images.

## Key Results
- Achieves state-of-the-art performance on multimodal web search, surpassing RAG workflows by +21.13% and prompt-based agents by +8.89%
- Maintains strong general VQA capabilities while excelling at knowledge-intensive visual question answering
- Demonstrates effective self-correction through iterative query refinement in multi-turn search scenarios

## Why This Works (Mechanism)

### Mechanism 1: Cropped Image Search via Referring Expression Grounding
The model generates referring expressions for visual entities most pertinent to the question, which are then used by a grounding tool (Grounding DINO) to dynamically identify and crop the corresponding image region. This focused crop is used for image search, retrieving more contextually relevant results than whole-image search. The core assumption is that grounding models accurately localize referring expressions and that web image search returns higher-quality results for focused crops.

### Mechanism 2: Iterative Query Refinement via Multi-Turn Search with Self-Reflection
The model issues multiple refined text searches, capturing self-reflection and self-correction capabilities. After an initial search, it inspects retrieved information and, if insufficient, outputs revised queries in subsequent turns. This iterative refinement based on actual retrieved content improves final answer accuracy. The approach assumes training data includes multi-turn reasoning traces with query corrections.

### Mechanism 3: Two-Stage Training with SFT + Online GRPO RL
A cold-start supervised finetuning phase teaches the model tag schema and when to search, followed by online reinforcement learning that optimizes a composite reward (correctness + format adherence). This approach yields better tool-use efficiency and higher accuracy than either stage alone. The reward model provides reliable binary correctness and format scores, while online rollouts enable efficient learning.

## Foundational Learning

- **Group-Relative Policy Optimization (GRPO)**: Needed because standard PPO can be unstable with noisy reward signals from web search rollouts. GRPO stabilizes training by comparing rollouts within a group and centering advantages, making learning more robust to reward scale. Quick check: Can you explain how GRPO differs from PPO in how advantages are computed and why that helps with noisy rewards?

- **Grounding as an Intermediate Tool**: Critical for the cropped image search mechanism, which depends on accurately localizing referring expressions to produce bounding boxes. Understanding how open-vocabulary grounding models work (e.g., Grounding DINO) is essential for debugging crop quality. Quick check: Given a referring expression, how does a grounding model output a bounding box, and what failure modes would lead to poor crops?

- **Structured Tag-Based Tool Invocation**: DeepMMSearch-R1 uses special tokens (`<text_search>`, `<img_search>`, `<answer>`, etc.) to interface with external tools. Training must enforce correct formatting so that tools can parse model outputs reliably. Quick check: If the model outputs malformed tags (e.g., missing closing tag), how should the system handle it, and how does the format reward component mitigate this?

## Architecture Onboarding

- **Component map**: Base MLLM (Qwen2.5-VL-7B-Instruct) with LoRA adapters on LLM layers → External tools (text search API → LLM summarizer → `<information>` context; image search API → LLM summarizer → `<information>` context; Grounding tool for crop bounding boxes) → Training pipeline (SFT stage → Online RL stage using GRPO in veRL framework with GPT-based reward model)

- **Critical path**: Input image + question → model generates `<reason>` + tool tag → If `<img_search>` with referring expression → grounding tool → crop → image search API → summarized results → fed back as `<information>` → If `<text_search>` → text search API → summarized results → fed back as `<information>` → Model iterates until `<answer>` or max turns → During RL, rollouts collected, rewards computed, GRPO updates applied

- **Design tradeoffs**: LoRA rank (r=8) balances expressivity vs. preserving pretrained capabilities; KL penalty (0.001) and clip ratio (0.2) control policy deviation; summarization trades off detail vs. context length

- **Failure signatures**: Excessive tool calls may indicate weak format reward or poor SFT initialization; malformed or missing tags suggest format reward should penalize; degraded general VQA performance may indicate KL penalty too weak or LoRA updates too aggressive

- **First 3 experiments**: (1) Ablate cropped vs. whole-image search to measure accuracy drop on datasets with prominent background noise; (2) Vary search-required/search-free ratio in SFT data to observe tool-call frequency and accuracy on knowledge-intensive vs. search-free benchmarks; (3) Compare GRPO vs. PPO under identical conditions to assess stability, tool-use efficiency, and final accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on in-house text and image search APIs creates significant barriers to direct replication
- Cropped image search mechanism depends heavily on Grounding DINO performance and may struggle with occluded objects or complex scenes
- LLM-as-Judge evaluation framework introduces variability as different reward models may produce different judgments

## Confidence

**High Confidence**: Core mechanism of combining cropped image search with iterative text search refinement is well-articulated and supported by reported performance improvements; two-stage training pipeline (SFT + GRPO RL) is clearly described with specific hyperparameters

**Medium Confidence**: Claims of state-of-the-art performance are supported by reported numbers, but exact evaluation methodology and dataset composition for comparison models are not fully specified; generalization to non-search benchmarks is demonstrated but extent of performance preservation needs further validation

**Low Confidence**: Scalability and robustness of cropped image search mechanism across diverse visual domains remains uncertain; model's behavior on extremely knowledge-intensive queries requiring multiple search iterations has not been thoroughly characterized

## Next Checks
1. **Ablation of Cropped vs. Whole-Image Search**: Disable the grounding tool and force whole-image search across all datasets, then measure the accuracy degradation specifically on images with prominent background clutter or multiple objects.

2. **Multi-Turn Search Efficiency Analysis**: Track the average number of search turns required per query type and measure the marginal benefit of each additional search turn to reveal whether the self-correction mechanism is being optimally triggered.

3. **Cross-Domain Generalization Test**: Evaluate DeepMMSearch-R1 on visual domains significantly different from training data (e.g., medical imaging, satellite imagery, or highly abstract artwork) to assess whether the grounding and search mechanisms generalize beyond knowledge-intensive visual question answering tasks.