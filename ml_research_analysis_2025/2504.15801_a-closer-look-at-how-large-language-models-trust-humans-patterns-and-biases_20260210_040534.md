---
ver: rpa2
title: 'A closer look at how large language models trust humans: patterns and biases'
arxiv_id: '2504.15801'
source_url: https://arxiv.org/abs/2504.15801
tags:
- trust
- integrity
- scenario
- benevolence
- trustworthiness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how large language models (LLMs) develop
  implicit trust in humans across five decision-making scenarios, using established
  psychological theories of trustworthiness (competence, benevolence, and integrity)
  and demographic factors (gender, age, and religion). Across 43,200 simulations using
  five popular LLM models, the research finds that LLM trust in humans generally aligns
  with human trust mechanisms, with strong correlations between trustworthiness and
  trust in most scenarios.
---

# A closer look at how large language models trust humans: patterns and biases

## Quick Facts
- arXiv ID: 2504.15801
- Source URL: https://arxiv.org/abs/2504.15801
- Reference count: 40
- Key outcome: LLMs exhibit implicit trust patterns aligned with human trust mechanisms but show context-dependent biases in demographic attributes, especially in financial scenarios.

## Executive Summary
This study investigates how large language models (LLMs) develop implicit trust in humans across five decision-making scenarios, using established psychological theories of trustworthiness (competence, benevolence, and integrity) and demographic factors (gender, age, and religion). Across 43,200 simulations using five popular LLM models, the research finds that LLM trust in humans generally aligns with human trust mechanisms, with strong correlations between trustworthiness and trust in most scenarios. However, trust expression varies significantly across models and contexts, and demographic biases—especially in financial scenarios—were observed, with religion-based bias being particularly consistent. Notably, newer and larger models tended to better mimic human-like trust behavior. These findings highlight the need to monitor and mitigate AI-to-human trust biases, as unchecked, such patterns could embed harmful biases into high-stakes decision-making systems.

## Method Summary
The study employs a three-stage prompting pipeline across five LLM models (ChatGPT 3.5 Turbo, GPT-4o Mini, o3-mini, Gemini Pro 1.5, Gemini Flash 2) and five scenarios (senior manager, loan request, donation request, trip instructor, babysitter). Each simulation combines trustee descriptions with manipulated trustworthiness (competence, benevolence, integrity) and demographics (gender, age, religion) across 144 unique prompt combinations, repeated 12 times for robustness (43,200 total simulations). Trust is quantified through scenario-specific behavioral outputs (loan amounts, babysitting hours, etc.) and validated via Mayer & Davis (1999) trust questionnaire items. Statistical analysis includes correlation matrices and OLS regression per scenario-model pair to identify independent effects and biases.

## Key Results
- LLM trust correlates strongly with trustworthiness dimensions (competence, benevolence, integrity) across most scenarios, mirroring human trust patterns.
- Trust sensitivity varies substantially across models, with newer/larger models showing stronger trustworthiness-trust correlations.
- Demographic biases emerge in trust-related outputs, particularly in financial scenarios, with systematic favor toward certain religious groups and older individuals.
- The donation request scenario showed notably weak correlations for some models (e.g., ChatGPT 3.5), suggesting context-dependent trust sensitivity.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit implicit effective trust that correlates systematically with the three trustworthiness dimensions (competence, benevolence, integrity), mirroring human trust patterns documented in psychological literature.
- Mechanism: When presented with trustee descriptions containing trustworthiness cues, LLMs translate these into quantifiable trust outputs (loan amounts, babysitting hours, etc.) through implicit associations learned during training—potentially reflecting human trust patterns embedded in training data.
- Core assumption: The correlation between trustworthiness manipulation and trust outputs reflects meaningful trust-like processing rather than surface-level pattern matching.
- Evidence anchors:
  - [abstract] "LLM trust is strongly predicted by trustworthiness... in some cases also biased by age, religion and gender"
  - [Results] Table 1 shows integrity-trust correlations of 0.62–0.77 across models; Table 3 shows similar patterns in loan, trip, and babysitter scenarios
  - [corpus] Related paper "Trust in AI emerges from distrust in humans" examines complementary trust dynamics but does not directly validate the competence-benevolence-integrity mechanism
- Break condition: If trustworthiness-trust correlations drop to near-zero or become inconsistent across semantically equivalent prompt reformulations, the mechanism may be prompt-artifact rather than implicit trust.

### Mechanism 2
- Claim: Trust sensitivity varies substantially across models and scenarios, with newer/larger models showing stronger trustworthiness-trust correlations.
- Mechanism: Model architecture and training data differences create heterogeneous sensitivity to trustworthiness cues—some models (e.g., ChatGPT 3.5 Turbo, Gemini Pro 1) show weak or absent correlations in certain scenarios (donation request), while others (o3-mini, Flash 2) show stronger, more consistent patterns.
- Core assumption: Assumption: Larger or more recent models have learned richer representations of human trust dynamics through training data exposure.
- Evidence anchors:
  - [Results] "for each scenario and each trustworthiness dimension the two models with the strongest correlations... are mostly exhibited for the newer better models (4o mini, 3o mini and Flash 2)"
  - [Table 3] Donation scenario: ChatGPT 3.5 shows near-zero correlations (0.02–0.06) while o3-mini shows 0.34–0.45
  - [corpus] No direct corpus evidence for the model-scale mechanism; this remains an exploratory observation
- Break condition: If scaling does not consistently improve trustworthiness sensitivity across broader model sets, the mechanism may be architecture-specific rather than scale-dependent.

### Mechanism 3
- Claim: Demographic biases emerge in trust-related outputs, particularly in financial scenarios, with systematic favor toward certain religious groups and older individuals.
- Mechanism: When making trust-adjacent decisions (loan amounts, donations), LLMs assign differential trust based on demographic attributes—e.g., Jewish subjects receive $2,234–$14,471 higher loan amounts; age 40/60 subjects receive $14,657–$49,480 more than age 20 subjects.
- Core assumption: These biases reflect training data patterns rather than random noise or prompt artifacts.
- Evidence anchors:
  - [Results] Table 4: "a person of Jewish religion gets a boost to trust across models. Older age also seems to increase trust"
  - [Table 2] Senior manager scenario shows minimal demographic bias—context matters
  - [corpus] Corpus contains "Divine LLaMAs: Bias, Stereotypes... of Religion in Large Language Models" (reference 33), supporting religious bias as a documented phenomenon
- Break condition: If demographic effects disappear when controlling for scenario framing or disappear in counterbalancing experiments, the mechanism may be prompt-contextual rather than systematic.

## Foundational Learning

- Concept: Mayer et al. (1995) Trust Framework—trust as "willingness to be vulnerable," with trustworthiness decomposed into competence, benevolence, and integrity
  - Why needed here: This is the theoretical backbone of the entire experimental design; understanding why these three dimensions matter is essential to interpreting results
  - Quick check question: Can you explain why integrity might predict trust differently than competence in a loan vs. babysitter scenario?

- Concept: Implicit vs. Explicit Trust Measurement
  - Why needed here: The paper infers trust from behavioral outputs (loan amounts, hours) rather than asking models directly—understanding this distinction is critical for methodology
  - Quick check question: Why might a loan amount be a better trust proxy than asking "How much do you trust this person?"

- Concept: OLS Regression for Causal Attribution in Observational LLM Data
  - Why needed here: The paper uses regression coefficients to attribute trust to trustworthiness dimensions while controlling for demographics; understanding collinearity risks is essential
  - Quick check question: If competence and integrity are highly correlated (r=0.68 in Table S.2), how might this affect coefficient interpretation?

## Architecture Onboarding

- Component map:
  - Stage I (Trustee Description): 144 unique prompts combining 3 trustworthiness dimensions × 2 levels × 2 genders × 3 ages × 3 religions
  - Stage II (Trust Quantification): Scenario-specific behavioral output (loan amount, donation, days, hours)
  - Stage III (Trust Questionnaire): Mayer & Davis (1999) 4-item scale administered to LLM for validation
  - Analysis Pipeline: Correlation matrices → OLS regression per scenario-model pair → t-tests for manipulation checks

- Critical path:
  1. Verify manipulation check (t-tests confirm high vs. low trustworthiness is "perceived" correctly)
  2. Examine correlation patterns for each dimension-trust pair
  3. Run OLS with all predictors to identify independent effects and biases
  4. Compare across models/scenarios to identify heterogeneity

- Design tradeoffs:
  - Using 12 repetitions per condition increases robustness but cannot eliminate prompt-dependence
  - OLS assumes linear additive effects; trust formation may be non-linear
  - Limited to 5 models (3 OpenAI, 2 Gemini)—findings may not generalize to other architectures
  - Scenarios mix literature-aligned (senior manager) and novel contexts (donation, trip), trading internal validity for generalizability

- Failure signatures:
  - Near-zero correlations in Table 3 (donation scenario for ChatGPT 3.5/Gemini Pro 1): indicates trustworthiness insensitivity in certain model-context combinations
  - Negative benevolence coefficient in loan scenario (Table 4): controlling for integrity/competence, benevolence may reduce trust—potentially counterintuitive
  - Adjusted R² of 0.01–0.04 in donation scenario (Table S.7): predictors explain almost no variance

- First 3 experiments:
  1. Replicate one scenario with prompt paraphrasing to test robustness of trustworthiness-trust correlations against wording artifacts
  2. Add a neutral baseline (no demographic info) to quantify absolute bias magnitude rather than relative effects
  3. Test a new scenario with inverted trust logic (e.g., "How much risk should this person be exposed to?") to validate whether the mechanism generalizes beyond prosocial contexts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent are the observed trust patterns sensitive to the specific phrasing and structure of the prompts used?
- Basis in paper: [explicit] The authors state in the Discussion that findings "could be influenced by the specific prompt formulations" and suggest future research should "systematically examine the impact of changing prompt formulation."
- Why unresolved: The study relies on a specific three-step prompting procedure, and LLM outputs are known to be highly unstable across different phrasings.
- What evidence would resolve it: Ablation studies varying prompt syntax and semantics while holding trustee characteristics constant to see if trust correlations persist.

### Open Question 2
- Question: Do non-linear interaction models predict LLM trust decisions more accurately than the OLS regressions utilized in this study?
- Basis in paper: [explicit] The Discussion notes that "one can argue that the effective trust calculation can be equivalent to a non-linear and more complex model," suggesting this as a avenue for future work.
- Why unresolved: Linear regression assumes additive effects of trustworthiness dimensions, potentially missing complex interactions (e.g., high competence amplifying the effect of high integrity).
- What evidence would resolve it: Comparative analysis fitting non-linear models (e.g., neural networks or interaction terms) to the dataset to test for higher predictive accuracy.

### Open Question 3
- Question: Why do financial scenarios elicit significantly stronger demographic biases (e.g., religion, age) compared to non-financial scenarios like childcare or tourism?
- Basis in paper: [inferred] The Results section notes that "most biases occur for the money-related scenarios" whereas there is "little demographic bias" in others, but the paper does not test the mechanism driving this context-dependence.
- Why unresolved: The paper identifies the disparity but does not determine if this is due to specific training data correlations (finance vs. demographics) or contextual priming.
- What evidence would resolve it: Isolating the "financial" context variable in new scenarios or probing model embeddings for stereotypical associations specific to financial decision-making.

## Limitations

- The study is limited to five LLM models from two providers (OpenAI and Google), restricting generalizability across the broader LLM ecosystem.
- The correlational nature of findings cannot establish whether observed trust patterns reflect genuine human-like reasoning or sophisticated pattern matching.
- The prompt-dependent methodology raises concerns about robustness across linguistic variations, particularly given substantial heterogeneity in model responses across scenarios.

## Confidence

**High Confidence**: The finding that LLMs exhibit varying sensitivity to trustworthiness cues across models and scenarios, with newer/larger models generally showing stronger correlations. The consistency of demographic biases, particularly religion-based effects in financial scenarios. The general alignment of LLM trust patterns with human trust mechanisms across most scenarios.

**Medium Confidence**: The specific magnitude of trust differences (e.g., "$2,234–$14,471 higher loan amounts" for Jewish subjects) given potential prompt sensitivity. The claim that larger models better mimic human-like trust behavior, as this observation is based on limited model comparisons. The interpretation that correlations reflect meaningful trust-like processing rather than surface-level pattern matching.

**Low Confidence**: The claim that trust formation follows the exact three-factor structure (competence, benevolence, integrity) in all contexts, given observed scenario-specific variations. The mechanism explaining why benevolence shows negative coefficients in some models/scenarios. The generalizability of findings beyond the specific scenarios and demographic attributes tested.

## Next Checks

1. **Prompt Robustness Test**: Replicate the most sensitive scenario (e.g., loan request) using semantically equivalent but linguistically distinct prompts to verify whether trustworthiness-trust correlations persist across wording variations, distinguishing genuine trust processing from prompt artifacts.

2. **Neutral Baseline Comparison**: Add a control condition with no demographic information across all scenarios to quantify absolute bias magnitude rather than relative effects, clarifying whether observed demographic effects represent genuine preference or baseline response patterns.

3. **Inverted Logic Validation**: Test a scenario with reversed trust logic (e.g., "How much risk should this person be exposed to?") to determine whether the observed trustworthiness-trust correlations generalize beyond prosocial contexts, validating the mechanism's scope and limitations.