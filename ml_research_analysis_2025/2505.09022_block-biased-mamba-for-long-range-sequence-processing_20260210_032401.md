---
ver: rpa2
title: Block-Biased Mamba for Long-Range Sequence Processing
arxiv_id: '2505.09022'
source_url: https://arxiv.org/abs/2505.09022
tags:
- mamba
- learning
- b2s6
- neural
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Mamba models underperform on long-range sequential tasks despite
  their design for long-range dependencies. This paper identifies three limitations:
  expressiveness, inductive bias, and training stability.'
---

# Block-Biased Mamba for Long-Range Sequence Processing

## Quick Facts
- **arXiv ID**: 2505.09022
- **Source URL**: https://arxiv.org/abs/2505.09022
- **Reference count**: 40
- **Primary result**: B2S6 achieves state-of-the-art performance on Long-Range Arena benchmark while maintaining Mamba's language modeling capabilities

## Executive Summary
Mamba models underperform on long-range sequential tasks despite their design for long-range dependencies. This paper identifies three limitations: expressiveness, inductive bias, and training stability. The authors propose B2S6, a block-biased extension of Mamba's S6 unit that incorporates block-wise selective dynamics and channel-specific bias. B2S6 addresses Mamba's shortcomings by improving expressiveness, providing a more suitable inductive bias for long-range tasks, and enhancing training stability. On the Long-Range Arena benchmark, B2S6 achieves state-of-the-art performance, outperforming both S4 and S4D. Additionally, B2S6 maintains Mamba's performance on language modeling tasks, demonstrating its versatility across domains.

## Method Summary
B2S6 extends the Mamba S6 unit by partitioning the input dimension into blocks and adding a channel-specific, input-independent bias term. The method operates by splitting the d-dimensional input into h blocks of size p, computing block-wise selective dynamics independently, and adding a trainable bias that is independent of the current input. This addresses three core limitations: lack of universal approximation due to parameter sharing, aggressive input-dependent forgetting in long-range tasks, and training instability on extremely long sequences. The architecture maintains Mamba's efficiency while providing improved expressiveness and stability for long-range sequence modeling tasks.

## Key Results
- B2S6 achieves state-of-the-art performance on the Long-Range Arena benchmark, outperforming both S4 and S4D
- The model maintains Mamba's language modeling performance while improving long-range task capabilities
- B2S6 demonstrates improved training stability on extremely long sequences (length 16384) compared to standard Mamba

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Partitioning the S6 unit into blocks recovers universal approximation capabilities lost by standard Mamba's parameter sharing.
- **Mechanism:** Standard S6 shares state matrices across all channels, limiting expressiveness. B2S6 partitions the input dimension into blocks with independent parameters per block, increasing effective width and theoretical approximation capability.
- **Core assumption:** Universal approximation property serves as a valid proxy for practical expressiveness of a single layer.
- **Evidence anchors:** Theorem 1 proves single-layer S6 is not a universal approximator; Theorem 4 proves block unit restores this property; Figure 3 shows error reduction with increasing width.
- **Break condition:** If tasks require only short-range dependencies where parameter sharing provides sufficient capacity, added complexity may not yield ROI.

### Mechanism 2
- **Claim:** Adding channel-specific, input-independent bias mitigates aggressive, input-dependent forgetting in long-range tasks.
- **Mechanism:** Input-dependent sampling interval and matrix B in S6 create exponential bias causing instant memory discard. B2S6 introduces fixed bias term per channel that counteracts selective mechanisms' tendency to zero out state information.
- **Core assumption:** Long-range tasks require preserving information across uninformative or noisy input steps.
- **Evidence anchors:** Theorem 2 proves S6 imposes exponentially large bias; Theorem 5 shows B2S6 reduces this to polynomial decay; Figure 4 visualizes stability vs. input variance.
- **Break condition:** In tasks requiring rapid context switching, this bias might retain noise.

### Mechanism 3
- **Claim:** Reducing learning rate for selection parameters stabilizes training on extremely long sequences.
- **Mechanism:** Gradients with respect to sampling interval grow polynomially as input magnitude or sequence length increases. B2S6 applies reduced learning rate to selection parameters, smoothing optimization trajectory.
- **Core assumption:** Instability primarily driven by first-order gradient magnitude of selection parameters.
- **Evidence anchors:** Theorem 3 derives Ω(c³) and √L gradient growth in S6; Figure 6 shows training stability comparison.
- **Break condition:** If adaptive optimizers with very low global learning rates are used, specific sensitivity of Δ parameters might be dampened.

## Foundational Learning

- **Concept**: **Zero-Order Hold (ZOH) Discretization**
  - **Why needed here**: Paper analyzes continuous-to-discrete conversion (A = exp(ΔA)). Understanding how step size Δ controls hidden state decay is essential to grasp why input-dependent Δ causes forgetting.
  - **Quick check question**: If Δ → 0, does the system change faster or slower? (Answer: Slower; preserves memory longer).

- **Concept**: **Universal Approximation Theory (UAT)**
  - **Why needed here**: Authors use UAT to formally prove expressiveness gap. Must understand that "lack of universal approximation" means architecture theoretically cannot approximate certain continuous functions, regardless of parameter count.
  - **Quick check question**: Does non-UAT result for S6 (Theorem 1) mean multi-layer Mamba cannot solve task? (Answer: No, paper notes depth enhances expressiveness; theorem highlights limitation of single unit).

- **Concept**: **Inductive Bias in Sequence Modeling**
  - **Why needed here**: Core conflict between position-based bias (S4D, good for generic long-range) and content-based bias (S6, good for language). B2S6 acts as hybrid; understanding distinction key to knowing when to apply it.
  - **Quick check question**: Why is purely input-dependent bias bad for classifying flattened images? (Answer: Pixel intensity doesn't necessarily correlate with "importance" in way that should trigger fast memory decay).

## Architecture Onboarding

- **Component map**: Input -> Block Partition (h blocks) -> Bias Injection (per block) -> Selective Dynamics -> Recurrent Scan (per block) -> Aggregation (concatenate)
- **Critical path**: Implementation of Algorithm 2 (B2S6 Forward Pass). Specifically, ensuring line 4 (Broadcast_p(jB) + Broadcast_L(jBbias)) is handled correctly to add input-independent bias across sequence length and channels.
- **Design tradeoffs**:
  - Width (p) vs. Blocks (h): Increasing h improves selectivity but reduces receptive field size per block. Increasing p increases capacity per block but shares parameters more.
  - Complex Parameters: Paper uses complex-valued parameters for state matrix A, which is critical for performance per ablation study.
- **Failure signatures**:
  - Gradient Explosion: If training loss spikes erratically on long sequences (L > 4096), learning rate for Δ parameters (w, b) is likely too high.
  - Capacity Stagnation: If increasing model dimension d does not improve performance on synthetic recovery tasks, "effective width" is bottlenecked (likely h is too small or bias is disabled).
- **First 3 experiments**:
  1. Synthetic Validation: Implement "coefficient recovery" task (Figure 3) to verify B2S6 loss decreases as d increases, confirming UAT mechanism.
  2. Ablation on Bias: Train on sCIFAR-10 with and without B_bias term to quantify contribution of "gentle inductive bias" (Table 1).
  3. Stability Test: Train on PathX (L=16384) task with standard S6 learning rates vs. reduced Δ learning rates to reproduce stability improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does B2S6 perform when scaled to large language model sizes (billion+ parameters) and longer training durations?
- Basis in paper: [explicit] Conclusion states "Future work includes... scaling up B2S6 as a language or foundation model." Language experiments are "preliminary" with only ~250M parameters trained for one epoch.
- Why unresolved: Preliminary evaluation demonstrates parity with Mamba at small scale; scaling behavior remains unknown.
- What evidence would resolve it: Training B2S6 at 1B+ parameters with full training runs on standard benchmarks, comparing against Mamba baselines.

### Open Question 2
- Question: Can training stability analysis be extended to general case where w ≠ 0 in S6 parameterization?
- Basis in paper: [explicit] Section 5 states "We also fixed w = 0, leaving case when w ≠ 0 for future work" when proving Theorem 3 on training stability.
- Why unresolved: Theoretical stability guarantees only cover restricted parameterization; practical Mamba models use nonzero w.
- What evidence would resolve it: Extending Theorem 3's analysis to nonzero w, or empirical stability studies with w ≠ 0 across varying sequence lengths.

### Open Question 3
- Question: What is optimal selection strategy for number of blocks h, and how does it interact with sequence length and task type?
- Basis in paper: [inferred] Authors note hyperparameter h was "not carefully fine-tuned but rather picked randomly" (Table 4). Table 1 shows non-monotonic performance with different h values.
- Why unresolved: Trade-off between block selectivity quality and receptive field per block lacks principled guidance.
- What evidence would resolve it: Systematic ablations across tasks with varying sequence lengths, or theoretical bounds on optimal h given sequence properties.

## Limitations
- Theoretical analysis focuses on single-layer units while practical models use deep stacks where expressiveness limitations may be mitigated
- Reduced learning rate for selection parameters is mentioned but exact magnitude is not specified, potentially affecting reproducibility
- Limited evaluation scope - only tested on LRA benchmark and SlimPajama language modeling, with no evaluation on other domains like speech or multimodal tasks

## Confidence
- **High Confidence**: B2S6 improves LRA benchmark performance compared to Mamba and matches S4D (Table 1, Table 2)
- **Medium Confidence**: Three identified limitations (expressiveness, inductive bias, training stability) are valid for Mamba and B2S6 addresses them theoretically
- **Medium Confidence**: B2S6 maintains language modeling performance while improving long-range tasks, though this requires careful hyperparameter tuning

## Next Checks
1. **Generalization Test**: Evaluate B2S6 on additional long-range tasks beyond LRA (e.g., LongBench, PG-19, or speech recognition datasets) to verify broader applicability
2. **Scaling Analysis**: Systematically vary the number of blocks (h) and block width (p) to identify optimal configurations and verify expressiveness claims hold across different scales
3. **Architecture Comparison**: Conduct controlled ablation studies comparing B2S6 with other Mamba variants (D-SSM, Mamba-2) using identical hyperparameters to isolate impact of each proposed modification