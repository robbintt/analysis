---
ver: rpa2
title: Quality-Controlled Multimodal Emotion Recognition in Conversations with Identity-Based
  Transfer Learning and MAMBA Fusion
arxiv_id: '2511.14969'
source_url: https://arxiv.org/abs/2511.14969
tags:
- emotion
- speaker
- recognition
- meld
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles quality issues in multimodal emotion recognition
  in conversations by combining systematic data curation with identity-based transfer
  learning. The authors introduce a pipeline that validates speaker identity, aligns
  audio and text, and verifies face detection in MELD and IEMOCAP datasets.
---

# Quality-Controlled Multimodal Emotion Recognition in Conversations with Identity-Based Transfer Learning and MAMBA Fusion

## Quick Facts
- arXiv ID: 2511.14969
- Source URL: https://arxiv.org/abs/2511.14969
- Reference count: 33
- Primary result: Quality-controlled identity-based transfer learning + MAMBA fusion achieves 64.8% accuracy on MELD and 74.3% on IEMOCAP

## Executive Summary
This paper addresses quality issues in multimodal emotion recognition in conversations by introducing a systematic pipeline that combines data curation with identity-based transfer learning. The authors implement quality control for MELD and IEMOCAP datasets, validating speaker identity, audio-text alignment, and face detection. They leverage pre-trained speaker and face recognition models to extract identity-discriminative embeddings, assuming these capture both stable traits and person-specific emotional expression patterns. A MAMBA-based fusion model achieves competitive results while providing theoretical efficiency advantages over attention-based approaches.

## Method Summary
The method consists of three stages: (1) Quality control filtering using Whisper for audio-text alignment (cosine>0.25, Levenshtein>0.3), YOLOv8+Facenet-512 for face detection, reducing MELD from 13,708 to 10,244 utterances; (2) Extracting 512-dim speaker/face embeddings from RecoMadeEasy engines and 768-dim text embeddings from fine-tuned MPNet-v2, then adapting through MLPs trained on unimodal emotion datasets to produce 128-dim emotion-aware representations; (3) Concatenating modalities in shared 1024-dim space and processing through single MAMBA block (d_state=64, expand=2, d_conv=4) with mean pooling and linear classification. The approach assumes identity-discriminative embeddings inherently encode emotion-relevant patterns from speaker and face recognition.

## Key Results
- MELD: 64.8% accuracy and 61.5% weighted F1 with trimodal fusion (vs. 58.1% text-only)
- IEMOCAP: 74.3% accuracy and 74.1% weighted F1 with trimodal fusion (vs. 68.7% text-only)
- Visual modality achieves only 42.3% on MELD, suggesting temporal misalignment issues
- Fear (16.7%) and disgust (29.0%) remain near-random on MELD despite quality control

## Why This Works (Mechanism)

### Mechanism 1: Identity-Based Transfer Learning
Identity-discriminative embeddings from speaker and face recognition transfer to emotion recognition because they encode fine-grained acoustic and facial dynamics. Speaker recognition models capture prosodic variations, voice quality, and temporal dynamics; face recognition models learn subtle facial muscle movements. These fine-grained patterns overlap with emotion-relevant cues. The authors extract 512-dimensional embeddings from RecoMadeEasy engines, then adapt them through MLPs trained on unimodal emotion datasets to produce 128-dimensional emotion-aware representations. The core assumption is that identity-preserving representations inherently encode emotion-relevant patterns rather than discarding them as noise.

### Mechanism 2: Systematic Quality Control
Systematic quality control improves multimodal alignment and reduces spurious cross-modal correlations. The pipeline re-labels conflated speaker identities, verifies audio-text alignment by comparing original transcripts with Whisper transcriptions using cosine and Levenshtein similarity thresholds, and validates face detection using Facenet-512 embeddings matched to speaker references. This filtered MELD from 13,708 to 10,244 utterances. The core assumption is that misaligned multimodal samples teach models incorrect cross-modal associations that harm generalization.

### Mechanism 3: Efficient MAMBA Fusion
A single MAMBA block provides competitive fusion with linear complexity by processing concatenated multimodal token sequences. Modalities are projected to a shared 1024-dimensional space (768 text + 128 speaker + 128 face), concatenated per-token, and processed through one MAMBA block (d_state=64, expand=2, d_conv=4) followed by mean pooling and a linear classifier. The core assumption is that sequential state-space modeling captures cross-modal dependencies without quadratic attention costs.

## Foundational Learning

- **State Space Models (SSMs) / MAMBA**: Why needed here: The fusion architecture uses MAMBA instead of transformers; understanding selective state spaces is required to debug and modify the fusion layer. Quick check question: Can you explain how MAMBA's selective state mechanism differs from standard attention's query-key-value computation?

- **Transfer Learning from Speaker/Face Recognition**: Why needed here: The core hypothesis is that identity embeddings contain emotion information; understanding what these embeddings encode is critical for evaluating whether this transfer is appropriate. Quick check question: What acoustic features do x-vector speaker embeddings capture, and which of these are shared with emotional speech?

- **Multimodal Alignment in Conversational Data**: Why needed here: The quality control pipeline addresses temporal and identity misalignment; understanding why this matters informs decisions about when to apply similar filtering. Quick check question: If audio and text are misaligned by 500ms, what spurious patterns might a multimodal model learn?

## Architecture Onboarding

- Component map: RecoMadeEasy extractors (512-dim) → MLP adaptation (512→256→128) → Token concatenation → MAMBA block (d_state=64) → Mean pooling → Linear classifier
- Critical path: Raw utterance → Quality control filtering → Foundation embeddings → MLP adaptation → Token concatenation → MAMBA processing → Classification
- Design tradeoffs: Static speaker embeddings vs. temporal (paper acknowledges static embeddings miss within-utterance emotional dynamics; future work suggests wav2vec 2.0); Quality filtering vs. data quantity (removing ~25% of MELD may introduce bias; no analysis of removed sample distribution); Single MAMBA block vs. deeper architecture (paper uses one block; tradeoff between efficiency and representational capacity not ablated)
- Failure signatures: Visual modality achieves only 42.3% on MELD (vs. 58.1% text), suggesting temporal misalignment between extracted frames and emotional peaks; Fear (16.7%) and disgust (29.0%) remain near-random on MELD, indicating dataset quality issues persist for sparse classes; Trimodal fusion improves only 6.7% over text-only, questioning multimodal benefit
- First 3 experiments: (1) Reproduce quality control pipeline on held-out MELD subset; verify alignment metrics correlate with human judgment; (2) Ablate MLP adaptation stage: compare 512-dim identity embeddings directly vs. 128-dim adapted embeddings; (3) Compare MAMBA fusion against standard cross-modal transformer baseline on quality-controlled data

## Open Questions the Paper Calls Out
None

## Limitations
- Proprietary RecoMadeEasy engines create fundamental barriers to exact reproduction and raise questions about generalizability to open-source alternatives
- Quality control filtering criteria could introduce systematic bias without analysis of removed sample distribution
- Sparse classes (fear, disgust) remain near-random even after quality control, suggesting fundamental dataset limitations

## Confidence
- High Confidence: Quality control methodology, text embedding pipeline, and MAMBA fusion architecture are clearly specified and reproducible
- Medium Confidence: Identity-based transfer hypothesis relies on related works but specific combination for ERC needs independent validation
- Low Confidence: MAMBA efficiency claims not directly validated against attention baselines on same quality-controlled data

## Next Checks
1. Reproduce quality control filtering pipeline on held-out MELD subset; measure alignment quality metrics and verify correlation with human judgments
2. Ablate transfer learning adaptation: compare 512-dim identity embeddings directly against 128-dim adapted embeddings to quantify transfer benefit
3. Compare MAMBA fusion against standard cross-modal transformer baseline on same quality-controlled data to validate efficiency-performance tradeoff