---
ver: rpa2
title: 'EduGuardBench: A Holistic Benchmark for Evaluating the Pedagogical Fidelity
  and Adversarial Safety of LLMs as Simulated Teachers'
arxiv_id: '2511.06890'
source_url: https://arxiv.org/abs/2511.06890
tags:
- safety
- educational
- scenarios
- language
- academic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EduGuardBench is a novel benchmark for evaluating the pedagogical
  fidelity and adversarial safety of large language models as simulated teachers.
  It combines a Select All That Apply (SATA) question set to assess teaching competence
  and a curated set of adversarial prompts to probe safety vulnerabilities.
---

# EduGuardBench: A Holistic Benchmark for Evaluating the Pedagogical Fidelity and Adversarial Safety of LLMs as Simulated Teachers

## Quick Facts
- arXiv ID: 2511.06890
- Source URL: https://arxiv.org/abs/2511.06890
- Reference count: 40
- Key outcome: EduGuardBench reveals significant performance polarization across 14 models, with reasoning models excelling in pedagogical fidelity and an Educational Transformation Effect correlating with enhanced safety

## Executive Summary
EduGuardBench introduces a comprehensive framework for evaluating large language models as simulated teachers, assessing both pedagogical competence and adversarial safety through two distinct components. The benchmark employs 2,636 SATA questions across five teaching scenarios and 801 adversarial prompts targeting harmful behaviors, using HITL-calibrated LLM-as-Judge evaluation. Experiments reveal that reasoning-oriented models demonstrate superior teaching fidelity in cognitively demanding scenarios, while safety performance exhibits a non-monotonic relationship with model scale, peaking at medium sizes. Critically, the safest models show a strong negative correlation between Educational Transformation Effect and attack success rate, converting harmful requests into teachable moments rather than simple refusals.

## Method Summary
The benchmark evaluates LLMs through Component I (Teaching Harm Assessment) using 2,636 SATA questions across five scenarios, scored via Role-playing Fidelity Score (RFS) and classified by error types (Incompetence, Offensiveness, Indolence). Component II (Adversarial Safety) uses 801 persona-based jailbreak prompts across five attack categories, classified by response quality (Flimsy/Standard/Educational). Evaluation employs zero-shot inference with temperature=0 and Best-of-N voting (N=9) using DeepSeek-V3 as HITL-calibrated LLM-as-Judge, achieving Cohen's Kappa of 0.882 for harmfulness classification. The benchmark supports bilingual evaluation (English/Chinese) and is publicly available on GitHub.

## Key Results
- Reasoning models outperform non-reasoning models in pedagogical fidelity, particularly for cognitively intensive scenarios (Idea Provision: 20.3% improvement, p=0.004)
- Medium-scale models (32B) show highest adversarial vulnerability, with ASR peaking at 75.2% before decreasing to 70.0% at 235B parameters
- Educational Transformation Effect shows strong negative correlation with attack success rate (r = -0.948, p = 0.000)
- Safety performance varies widely across architectures, with Claude-3.7 achieving 64.5% transformation versus Deepseek-V3's 14.5%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Educational Transformation Effect—converting harmful requests into teachable moments—correlates strongly with reduced adversarial vulnerability.
- **Mechanism:** Models that refuse harmful requests while explaining underlying ethical principles (Educational Refusal) appear to develop transferable safety representations that generalize across attack vectors, rather than relying on surface-level pattern matching.
- **Core assumption:** The correlation (r = -0.948, p = 0.000) reflects a causal relationship where deeper safety reasoning enables both better refusals and broader robustness.
- **Evidence anchors:** [abstract]: "the safest models excel at converting harmful requests into teachable moments by providing ideal Educational Refusals. This capacity is strongly negatively correlated with ASR"
- **Break condition:** If the correlation weakens when controlling for model family or training data composition, the effect may reflect alignment strategy rather than architecture.

### Mechanism 2
- **Claim:** Reasoning-oriented architectures improve teaching fidelity selectively—most strongly for cognitively intensive, open-ended scenarios.
- **Mechanism:** Enhanced chain-of-thought capabilities enable more coherent persona maintenance and pedagogical reasoning when responses require creative synthesis or emotional attunement, but provide less advantage for structured, verifiable tasks.
- **Core assumption:** The scenario-specific advantage pattern reflects differential cognitive demands rather than training data artifacts.
- **Evidence anchors:** [Results/Teaching Harm Assessment]: "reasoning capabilities interact significantly only with Idea Provision scenarios (β = -11.471, p = 0.0350), with no advantages in other contexts"
- **Break condition:** If paired model comparisons (reasoning vs. non-reasoning versions of same base) show inconsistent patterns across architectures, the effect may be training-specific.

### Mechanism 3
- **Claim:** Model scale exhibits a non-monotonic relationship with adversarial safety—medium-scale models (32B parameter range) can be most vulnerable.
- **Mechanism:** Intermediate-scale models may occupy an unstable equilibrium: sufficient capability to engage with complex adversarial prompts but insufficient representational capacity for robust safety generalization, while larger models develop more stable safety boundaries.
- **Core assumption:** The inverted U-shaped pattern (ASR peaks at 32B, decreases at 235B) reflects genuine capability-safety dynamics rather than training differences.
- **Evidence anchors:** [Results/Adversarial Safety Assessment]: "ASR increases 14.8 percentage points from 8B (60.4%) to 32B (75.2%), yet decreases 5.2 percentage points to 235B (70.0%)" for Qwen3 series
- **Break condition:** If the pattern inverts or disappears in other model families (e.g., LLaMA, Gemma), this may be Qwen-specific training artifact.

## Foundational Learning

- **Concept: Role-playing Fidelity Score (RFS)**
  - **Why needed here:** Core metric for Component I; measures how well models select ideal pedagogical responses vs. harmful distractors across SATA questions.
  - **Quick check question:** Can you explain why RFS penalizes both omission (missing ideal answers) and incorrect inclusion (selecting harmful distractors) differently?

- **Concept: Refusal Quality Taxonomy**
  - **Why needed here:** Distinguishes safety mechanisms by pedagogical value; Educational > Standard > Flimsy refusals.
  - **Quick check question:** What differentiates an Educational Refusal from a Standard Refusal in this framework?

- **Concept: Persona-based Jailbreak Methodology**
  - **Why needed here:** Component II uses teacher personas with flawed core beliefs to test whether role-play fidelity overrides safety training.
  - **Quick check question:** How does the Teacher Prompt / Student Request structure create ethical tension for the model?

## Architecture Onboarding

- **Component map:** Component I (Teaching Harm): 2,636 SATA questions across 5 scenarios (PS, EC, IP, PLS, ES) with S1/S2/S3 error classification → Component II (Adversarial Safety): 801 open-ended prompts across 5 categories (AP, HR, TS, PI, AM) with 4-tier response classification → Evaluation: HITL-calibrated LLM-as-Judge (DeepSeek-V3) with N=9 Best-of-N voting

- **Critical path:** Seed prompt creation → LLM expansion → Automated pre-screening → Manual cross-review → Expert verification → Bilingual verification

- **Design tradeoffs:** SATA format enables automated scoring but limits nuance in assessing open-ended pedagogical reasoning; LLM-as-Judge scales evaluation but requires gold-standard calibration; zero-shot, temperature=0 ensures reproducibility but may underestimate model capability under optimized prompting

- **Failure signatures:** High inclusion rate (selecting harmful distractors) indicates pedagogical alignment failure; high flimsy-refusal rate suggests fragile safety boundaries exploitable via follow-up prompts; binary polarization (only attack-success or educational-refusal) indicates under-developed intermediate safety responses

- **First 3 experiments:** 1) Replicate the Educational Transformation correlation on your target model using the 801 adversarial prompts; compute ASR vs. Educational Refusal Rate; 2) Run paired comparison on a model family with reasoning and non-reasoning variants (e.g., Qwen3-32B-R vs. Qwen3-32B) across the five teaching scenarios; 3) Probe scale sensitivity by testing 3+ parameter sizes from the same model family on the Academic Misconduct (AM) category—the highest-vulnerability domain

## Open Questions the Paper Calls Out
None

## Limitations
- The Educational Transformation Effect interpretation assumes causation from correlation; alternative explanations include dataset bias or evaluation prompt artifacts
- The scaling paradox conclusion extrapolates from Qwen3 family data to universal claims about model size and safety
- Findings hinge on a specific LLM-as-Judge pipeline, raising questions about generalizability across different evaluation frameworks

## Confidence
- **High Confidence**: The benchmark construction methodology (SATA format, bilingual curation, HITL calibration) is well-documented and reproducible
- **Medium Confidence**: The Educational Transformation Effect interpretation assumes causation from correlation
- **Low Confidence**: The scaling paradox conclusion extrapolates from Qwen3 family data to universal claims about model size and safety

## Next Checks
1. **Cross-Architecture Scale Testing**: Evaluate the same parameter sizes (8B, 32B, 235B) from multiple model families (e.g., LLaMA, Gemma) on the AM category to test whether the inverted U-shaped safety pattern holds beyond Qwen3

2. **Judge Model Ablation**: Replicate key findings using GPT-4o as judge instead of DeepSeek-V3 to assess sensitivity to evaluator choice, particularly for the Educational Transformation Effect correlation

3. **Paired Architecture Comparison**: Test reasoning vs. non-reasoning versions of the same base model (e.g., Qwen3-32B-R vs Qwen3-32B) across all five teaching scenarios to isolate architecture effects from training differences