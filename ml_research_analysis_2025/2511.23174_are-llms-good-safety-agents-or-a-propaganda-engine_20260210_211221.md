---
ver: rpa2
title: Are LLMs Good Safety Agents or a Propaganda Engine?
arxiv_id: '2511.23174'
source_url: https://arxiv.org/abs/2511.23174
tags:
- refusal
- censorship
- arxiv
- political
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether LLM refusals reflect genuine safety
  policies or political censorship by introducing PSP, a dataset of politically sensitive
  prompts derived from censored tweets and Chinese context prompts. Using data-driven
  and representation-level de-politicization methods, the authors find that some models
  (e.g., Llama 3.1, DeepSeek R1) exhibit censorship-like behavior, with significantly
  higher refusal rates when political content is present.
---

# Are LLMs Good Safety Agents or a Propaganda Engine?

## Quick Facts
- arXiv ID: 2511.23174
- Source URL: https://arxiv.org/abs/2511.23174
- Reference count: 27
- Key outcome: Study finds certain LLMs exhibit censorship-like behavior in politically sensitive contexts, with refusal rates significantly influenced by political content presence.

## Executive Summary
This paper investigates whether LLM refusals stem from genuine safety concerns or political censorship by introducing a novel dataset of politically sensitive prompts (PSP). The authors employ data-driven and representation-level de-politicization methods to demonstrate that certain models show significantly higher refusal rates when political content is present. Their experiments reveal that representation erasure techniques can reduce these refusal rates, confirming political bias in model guardrails. The study also shows that guardrail models like PromptGuard maintain resilience to political de-politicization, while other models collapse entirely.

## Method Summary
The authors introduce PSP, a dataset derived from censored tweets and Chinese context prompts, to systematically evaluate politically sensitive content. They employ two de-politicization approaches: data-driven fine-tuning using adversarial prompts and representation-level erasure via LEACE (Learning to Erase Activation Channel Effects). The study measures refusal rates across different models and contexts, comparing baseline performance against de-politicized versions. Cognitive hacking attacks are used to probe model behavior under ethical dilemmas, revealing increased partial refusals when models face conflicting directives.

## Key Results
- Some models (Llama 3.1, DeepSeek R1) show significantly higher refusal rates for politically sensitive content
- Representation erasure via LEACE reduces refusal rates, confirming political bias in guardrails
- Guardrail models like PromptGuard show resilience to political de-politicization while others collapse entirely
- Refusal distributions are consistent across countries but vary by model architecture, not parameter count

## Why This Works (Mechanism)
The mechanism operates through learned associations between political content and refusal behavior in model guardrails. When models encounter politically sensitive prompts, internal representations activate safety-related pathways that trigger refusals. The de-politicization methods work by either retraining these associations through adversarial examples or directly modifying the activation patterns that encode political content. Cognitive hacking exploits the model's ethical decision-making processes, creating internal conflicts that manifest as increased partial refusals.

## Foundational Learning

**Representation erasure via LEACE** - Why needed: To systematically remove political content from model representations without retraining. Quick check: Verify activation patterns are modified while preserving task performance.

**Adversarial prompt generation** - Why needed: To create challenging examples that expose censorship tendencies. Quick check: Confirm generated prompts successfully trigger refusals in baseline models.

**Cognitive hacking attack methodology** - Why needed: To probe model behavior under ethical dilemmas and conflicting directives. Quick check: Measure changes in refusal patterns when models face contradictory instructions.

## Architecture Onboarding

**Component map**: PSP dataset generation -> De-politicization methods (LEACE + fine-tuning) -> Refusal rate measurement -> Cognitive hacking attacks -> Model comparison

**Critical path**: Prompt generation → Model processing → Refusal classification → Statistical analysis → Interpretation

**Design tradeoffs**: The study balances between creating realistic politically sensitive prompts versus maintaining experimental control. Using representation erasure offers speed but may miss nuanced political content compared to comprehensive fine-tuning.

**Failure signatures**: Models showing uniform refusal rates regardless of content type, de-politicization methods failing to reduce refusals, or cognitive hacking attacks producing no change in partial refusal rates.

**3 first experiments**:
1. Measure baseline refusal rates for PSP prompts across multiple models
2. Apply LEACE representation erasure and measure refusal rate changes
3. Conduct cognitive hacking attacks and analyze partial refusal patterns

## Open Questions the Paper Calls Out
None

## Limitations
- PSP dataset may not fully represent censorship dynamics or capture all forms of nuanced political content
- Study focuses primarily on English-language models, limiting multilingual and cultural generalizability
- De-politicization methods may not account for model adaptation over time or evolving censorship patterns

## Confidence

**Censorship behavior finding**: Medium confidence - Compelling experimental evidence but interpretation remains somewhat subjective and context-dependent

**Model-specific refusal patterns**: High confidence - Supported by consistent experimental data across multiple evaluation scenarios

**Guardrail model resilience**: Medium confidence - Experimental results support findings but generalizability to all deployment scenarios requires further validation

## Next Checks

1. Conduct multilingual experiments using PSP-derived prompts in multiple languages to assess cross-cultural consistency of censorship patterns
2. Implement longitudinal studies tracking refusal rates over time as models are updated to distinguish between persistent censorship and temporal policy shifts
3. Design more sophisticated adversarial attack scenarios that combine political and safety-related content to better understand model decision boundaries in complex contexts