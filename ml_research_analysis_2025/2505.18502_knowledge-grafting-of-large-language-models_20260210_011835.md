---
ver: rpa2
title: Knowledge Grafting of Large Language Models
arxiv_id: '2505.18502'
source_url: https://arxiv.org/abs/2505.18502
tags:
- arxiv
- knowledge
- learning
- fusion
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GraftLLM, a novel framework for cross-capability
  transfer in large language models (LLMs). The core idea is to store specialized
  knowledge from source models as compact, modular SkillPacks, which can be efficiently
  integrated into a target model.
---

# Knowledge Grafting of Large Language Models

## Quick Facts
- arXiv ID: 2505.18502
- Source URL: https://arxiv.org/abs/2505.18502
- Reference count: 40
- Introduces GraftLLM, a framework for modular knowledge transfer between LLMs using compressed SkillPacks

## Executive Summary
GraftLLM presents a novel framework for cross-capability transfer in large language models by extracting and compressing specialized knowledge from source models into modular SkillPacks. These SkillPacks can be efficiently integrated into target models, addressing key limitations of existing methods like parameter conflicts and capability degradation. The approach enables forget-free continual learning and model fusion while maintaining parameter efficiency and preserving general capabilities of the target model.

## Method Summary
The GraftLLM framework works by first distilling knowledge from source models into SkillPacks through supervised fine-tuning (SFT) and direct preference optimization (DPO). These SkillPacks are then compressed using low-rank matrix decomposition and mixed-precision quantization to create compact representations. The compressed SkillPacks are integrated into target models through low-rank matrix addition, enabling the target model to acquire specialized capabilities without overwriting its general knowledge. This modular approach allows for efficient knowledge transfer, continual learning, and model fusion while avoiding the catastrophic forgetting issues common in traditional fine-tuning methods.

## Key Results
- Outperforms existing techniques in knowledge transfer, fusion, and continual learning on benchmarks like MT-Bench and AlpacaEval 2.0
- Achieves significant performance gains while maintaining parameter efficiency through compact SkillPack representation
- Demonstrates successful cross-capability transfer between diverse model pairs including LLaMA, Qwen, and DeepSeek architectures

## Why This Works (Mechanism)
GraftLLM works by extracting specialized knowledge from source models and compressing it into SkillPacks that can be efficiently integrated into target models. The method uses low-rank matrix decomposition and mixed-precision quantization to create compact representations that avoid parameter conflicts. By performing knowledge distillation before compression, the framework ensures that only the most relevant capabilities are transferred, while the target model's general knowledge remains intact through the low-rank addition process.

## Foundational Learning
- **Knowledge Distillation**: Transfer learning technique that compresses knowledge from larger models to smaller ones. Needed to extract relevant capabilities from source models without transferring unnecessary parameters. Quick check: Verify distilled outputs match source model behavior on target tasks.
- **Low-Rank Matrix Decomposition**: Technique that approximates large matrices with products of smaller matrices. Needed to compress SkillPacks while preserving essential information. Quick check: Measure reconstruction error after decomposition.
- **Mixed-Precision Quantization**: Method of representing weights using different bit-widths. Needed to further compress SkillPacks for efficient storage and transfer. Quick check: Compare model performance across different precision levels.
- **Parameter-Efficient Fine-Tuning**: Training techniques that modify only a small subset of model parameters. Needed to integrate SkillPacks without disrupting the target model's general capabilities. Quick check: Monitor performance on general vs. specialized tasks.
- **Continual Learning**: Training approach that enables models to learn new tasks without forgetting previous ones. Needed to support sequential knowledge grafting. Quick check: Test performance degradation after multiple grafting operations.
- **Cross-Architecture Transfer**: Ability to transfer knowledge between different model architectures. Needed for practical deployment across diverse LLM families. Quick check: Verify successful transfer between LLaMA, Qwen, and DeepSeek models.

## Architecture Onboarding

**Component Map**: Source Model -> Knowledge Distillation -> SkillPack Extraction -> Compression (Low-rank + Quantization) -> Integration (Low-rank Addition) -> Target Model

**Critical Path**: The most time-consuming steps are the initial knowledge distillation and compression phases, which require multiple training epochs. The integration phase is computationally efficient, involving only matrix operations that scale linearly with model size.

**Design Tradeoffs**: The framework trades some inference efficiency (requires full parameter reconstruction) for superior parameter efficiency and capability preservation. This design choice prioritizes storage and transfer efficiency over deployment speed.

**Failure Signatures**: Poor distillation quality leads to degraded SkillPack effectiveness. Excessive compression causes information loss and reduced capability transfer. Parameter conflicts may occur if SkillPacks are incompatible with target model architecture.

**3 First Experiments**:
1. Single knowledge transfer: Move a specialized capability from one model to another and measure performance improvement
2. Capability preservation test: Verify the target model maintains its original capabilities after grafting
3. Parameter efficiency validation: Compare total parameters before and after grafting operations

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can the selection of compression hyperparameters, such as rank and mixed-precision ratios, be automated to remove the reliance on empirical tuning?
- Basis in paper: The authors state in Section 6.3 that the approach currently "relies on empirically chosen compression hyperparameters" and that "automating this process remains an important direction for future work."
- Why unresolved: The current implementation requires manual configuration for different modules (e.g., Embedding, MLP, Attention), which limits scalability across diverse tasks.
- What evidence would resolve it: An adaptive algorithm that dynamically determines optimal compression settings based on parameter sensitivity without human intervention.

### Open Question 2
- Question: Can the method be adapted to achieve inference efficiency comparable to PEFT techniques like LoRA by avoiding full parameter reconstruction?
- Basis in paper: Section 6.3 notes the method "does not achieve the inference efficiency of techniques like LoRA... as it requires reconstructing the full model parameters through de-quantization."
- Why unresolved: The necessity to de-quantize and merge SkillPacks into the base model creates computational overhead compared to lightweight adapter methods.
- What evidence would resolve it: A modified deployment strategy allowing quantized skill updates to be applied during the forward pass without full weight reconstruction.

### Open Question 3
- Question: How can the framework be made robust against suboptimal or noisy data during the initial SFT and DPO distillation stages?
- Basis in paper: The authors acknowledge in Section 6.3 that the method "relies on the quality of prior SFT and DPO; poor distillation hampers its ability to capture source model capabilities."
- Why unresolved: The SkillPack extraction process assumes high-quality distillation; the paper does not address how to filter or correct for poor source knowledge transfer.
- What evidence would resolve it: Integration of quality estimation metrics or noise-filtering mechanisms within the compression pipeline that maintain retention even with imperfect source data.

## Limitations
- Inference efficiency is lower than adapter-based methods due to full parameter reconstruction requirements
- Performance heavily depends on the quality of initial knowledge distillation, with poor distillation leading to ineffective SkillPacks
- Limited experimental validation of long-term capability retention beyond 3 successive grafting operations

## Confidence
- Performance claims: Medium (based on MT-Bench and AlpacaEval 2.0 results)
- Long-term retention claims: Low (only tested up to 3 grafting operations)
- Parameter efficiency claims: Medium (comparisons to full fine-tuning but limited adapter comparisons)

## Next Checks
1. Test SkillPack retention after 10+ successive grafting operations to verify true forget-free learning claims
2. Compare GraftLLM against adapter-based and LoRA approaches on both performance and parameter efficiency metrics
3. Analyze internal model representations pre/post-grafting using probing tasks to verify capability preservation beyond surface-level metrics