---
ver: rpa2
title: 'ORACLE: Explaining Feature Interactions in Neural Networks with ANOVA'
arxiv_id: '2509.10825'
source_url: https://arxiv.org/abs/2509.10825
tags:
- interaction
- oracle
- anov
- interactions
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ORACLE introduces an ANOVA-based framework for explaining pairwise
  interactions in neural networks on tabular data. It treats the trained network as
  a black-box response, discretizes inputs onto a grid, and fits an orthogonal factorial
  (ANOVA-style) surrogate.
---

# ORACLE: Explaining Feature Interactions in Neural Networks with ANOVA

## Quick Facts
- **arXiv ID:** 2509.10825
- **Source URL:** https://arxiv.org/abs/2509.10825
- **Authors:** Dongseok Kim; Hyoungsun Choi; Mohamed Jismy Aashik Rasool; Gisung Oh
- **Reference count:** 40
- **Primary result:** ANOVA-based surrogate explanations outperform SHAP-family interaction methods on tabular regression benchmarks in ranking, localization, and stability.

## Executive Summary
ORACLE explains pairwise interactions in neural networks by fitting an orthogonal factorial (ANOVA-style) surrogate to the trained model's predictions. The method discretizes inputs into a grid, fits a contrast-coded factorial basis via least squares, and applies recentering and µ-rebalancing to produce interpretable main-effect and interaction tables. It is designed for tabular data with interpretable factorial structure and produces grid-based interaction maps that are easy to visualize and align with classical design-of-experiments practice.

## Method Summary
ORACLE treats the trained neural network as a black-box function and discretizes each input feature into L levels. It constructs a contrast-coded factorial basis for main effects and pairwise interactions, then fits this basis via least squares to the model's predictions. After fitting, recentering and µ-rebalancing enforce ANOVA identifiability constraints (zero-mean main effects, zero row/column sums for interactions). Interaction strengths are computed as RMS values over the grid, and the resulting tables and heatmaps summarize pairwise interactions faithful to the original model in the L² sense.

## Key Results
- ORACLE outperforms Monte Carlo SHAP-family interaction methods on NDCG@5 and Peak-IoU@0.10 metrics for synthetic factorial and UCI tabular benchmarks
- Cross-backbone stability (Xfer-NDCG@5) shows ORACLE explanations are more consistent across different trained models than baseline methods
- Grid resolution L=5 provides optimal trade-off between discretization bias and estimation variance on 5D problems

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Discretizing inputs and fitting an orthogonal factorial surrogate approximates the neural network's interaction structure in an L² sense.
- **Mechanism:** The network f is treated as a black-box function. Inputs are binned into L levels per feature, creating an L^d implicit grid. A contrast-coded factorial basis φ(ℓ) is fitted via least squares: β̂ = argmin_β (1/n) Σ(f(x_i) - ⟨β, φ(z_i)⟩)². This yields the L²(P_Z)-orthogonal projection of f onto the discrete factorial subspace.
- **Core assumption:** Features can be meaningfully discretized; the product-measure reference (independent coordinates) aligns with the evaluation goal.
- **Evidence anchors:**
  - [abstract]: "L² orthogonal projection of the model response onto a finite-dimensional factorial subspace"
  - [Section 3.3]: "ORACLE solves β̂ = argmin_β (1/n) Σ(f(x_i) - ⟨β, φ(z_i)⟩)²"
  - [corpus]: Related work on functional ANOVA models (Tensor Product Neural Networks for Functional ANOVA, Bayesian Neural Networks for Functional ANOVA) supports the decomposition approach but does not directly validate this specific surrogate.
- **Break condition:** When features have high cardinality, strong dependence, or latent entangled structure (e.g., image/text embeddings), binning loses fidelity and the surrogate becomes biased.

### Mechanism 2
- **Claim:** Recentering and µ-rebalancing enforces identifiability constraints, producing main and interaction tables that satisfy classical ANOVA sum-to-zero conventions.
- **Mechanism:** Post-fitting, main effects are centered to zero mean; interaction tables have row and column sums set to zero. Any induced constant shift is absorbed into the global mean μ̂. This preserves the fitted surrogate function while fixing a unique representation.
- **Core assumption:** Identifiability conventions (zero-mean main effects, zero row/column sums for interactions) are the desired semantic interpretation.
- **Evidence anchors:**
  - [Section 3.4]: "A naive least-squares fit... does not necessarily satisfy the standard identifiability constraints... we apply a lightweight post-processing step"
  - [Appendix B, Algorithm 1]: Detailed recentering and µ-rebalancing procedure
  - [corpus]: No direct corpus evidence on this specific step; classical ANOVA literature provides foundational support.
- **Break condition:** If the original least-squares fit is severely underdetermined (sparse bins, high L), post-hoc centering cannot recover stable estimates.

### Mechanism 3
- **Claim:** Discrete interaction strengths (RMS over grids) provide consistent rankings of pairwise interactions when grid resolution and sample size are sufficient.
- **Mechanism:** For each pair (j,k), strength is computed as Ŝ_jk = (Σ_{ℓ_j,ℓ_k} p̂_jk(ℓ_j,ℓ_k) ĝ_jk(ℓ_j,ℓ_k)²)^{1/2}. Under margin conditions separating top-K oracle strengths, empirical strengths converge and recover true top-K pairs.
- **Core assumption:** There exists a margin Δ > 0 between the K-th and (K+1)-th oracle interaction strengths.
- **Evidence anchors:**
  - [Section 4.3, Theorem 4.7-4.8]: Consistency and Top-K selection recovery under stated assumptions
  - [Section 5, Figure 1]: Empirical NDCG@K and Xfer-NDCG@K show ORACLE outperforming SHAP-family on tabular benchmarks
  - [corpus]: Corpus does not provide independent validation of consistency claims.
- **Break condition:** When oracle strengths are near-tied (Δ ≈ 0) or sample size is insufficient for the chosen grid resolution, ranking becomes unstable.

## Foundational Learning

- **Concept: Functional ANOVA decomposition**
  - **Why needed here:** ORACLE's surrogate is a discrete approximation to the functional ANOVA decomposition of the predictor into main effects, pairwise interactions, and residuals.
  - **Quick check question:** Can you explain why the decomposition f(X) = μ + Σ m_j(X_j) + Σ g_jk(X_j,X_k) + r(X) is orthogonal under a product measure?

- **Concept: Contrast coding in factorial designs**
  - **Why needed here:** The factorial basis φ uses contrast-coded indicators; understanding how these encode effects is necessary to interpret the coefficient blocks.
  - **Quick check question:** For a 2-level factor, what constraint makes a contrast "centered" under uniform distribution?

- **Concept: L² projection in Hilbert spaces**
  - **Why needed here:** The surrogate is defined as the L²(P_X)-orthogonal projection; the consistency results rely on projection properties.
  - **Quick check question:** If you project f onto subspace V_L, what equation characterizes the residual f - f_L?

## Architecture Onboarding

- **Component map:** Input discretization -> Factorial basis construction -> Least-squares fitting -> Recentering + μ-rebalancing -> Interaction strength extraction -> Visualization
- **Critical path:** (1) -> (2) -> (3) -> (4) -> (5). Step (4) is lightweight but required for semantic correctness; skipping it yields misaligned tables.
- **Design tradeoffs:**
  - Grid resolution L: Higher L reduces discretization bias but increases bin sparsity and coefficient count. Ablation (Table 2) suggests L=5 is a practical trade-off for 5D problems.
  - Pairwise-only vs. full surrogate: Pairwise-only (independent 2D averaging) can match or exceed full surrogate on simple problems (Table 1) but may miss cross-pair regularities.
  - Product-measure reference: Independent-coordinate assumption simplifies orthogonality but may not reflect true feature dependence.
- **Failure signatures:**
  - NDCG drops sharply when L is too high (sparse bins) or too low (excessive smoothing).
  - Peak-IoU near zero indicates hotspot localization failure (e.g., synthetic benchmark where ground truth is concentrated).
  - Large cross-backbone variance (low Xfer-NDCG) suggests the surrogate is fitting backbone-specific artifacts.
  - On latent image/text features: ORACLE underperforms SHAP-family on NDCG and CCC (Tables 9-12), indicating the grid-based projection is biased for entangled representations.
- **First 3 experiments:**
  1. **Sanity check on synthetic 2^5 factorial:** Train a small MLP to interpolate a known factorial response (Eq. 2). Verify ORACLE with L=2 recovers true coefficients (Proposition 4.10). NDCG@5 should be 1.0.
  2. **Grid resolution sweep on Airfoil:** Run ORACLE with L ∈ {3,5,7,9}. Plot NDCG@5, Peak-IoU@0.10, and Xfer-NDCG@5 vs. L. Confirm moderate L (5) optimizes the trade-off.
  3. **Cross-backbone stability test:** Train two independent MLPs on kin8nm. Compute ORACLE explanations on each. Measure Xfer-NDCG@5 and compare against SHAP-family baselines. ORACLE should show higher transfer.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the framework be extended to handle strong input feature dependence without losing orthogonality or mixing effects?
- **Basis in paper:** [explicit] The authors note that the theoretical analysis assumes independent coordinates, and "under strong feature dependence, pairwise projection-based effects can mix with higher-order structure."
- **Why unresolved:** The current method relies on a product measure for its orthogonal decomposition, which fails if features are correlated, leading to potential misinterpretation of effects.
- **What evidence would resolve it:** A modified ORACLE method using dependence-aware bases that accurately recovers ground-truth interaction structure on synthetic benchmarks with known correlated features.

### Open Question 2
- **Question:** Can adaptive or sparse grid strategies successfully scale the discrete ANOVA surrogate to high-dimensional tabular data?
- **Basis in paper:** [explicit] The authors identify performance degradation in high dimensions as a limitation and state that future work "will likely require adaptive/sparse grids."
- **Why unresolved:** Fixed grid discretization suffers from the curse of dimensionality, causing sparse cells and poor estimation, limiting the current method to low- to medium-dimensional tasks.
- **What evidence would resolve it:** Empirical results demonstrating that an adaptive-grid variant maintains high NDCG and Peak-IoU scores on datasets with significantly more features ($d > 50$) compared to the fixed-grid baseline.

### Open Question 3
- **Question:** Can ORACLE's interaction maps be integrated with Shapley-based methods to create a hybrid estimator with superior stability and semantics?
- **Basis in paper:** [explicit] Future work suggests "hybrid approaches that use interaction maps to guide or regularize SHAP-style estimators may combine structured DoE semantics with the flexibility of Shapley-based indices."
- **Why unresolved:** The paper establishes ANOVA surrogates and SHAP as distinct approaches with different theoretical foundations; a mechanism to merge their strengths is currently undefined.
- **What evidence would resolve it:** A specific hybrid algorithm that demonstrates improved cross-backbone stability (Xfer-NDCG) and scale alignment (CCC) over both standard ORACLE and SHAP baselines on standard benchmarks.

## Limitations

- ORACLE is not designed for latent, entangled representations (e.g., image embeddings or text tokens), where binning introduces significant bias
- Consistency and oracle recovery rely on a margin condition between top-K interaction strengths; near-tied oracle strengths may cause instability
- The product-measure reference (independent-coordinate assumption) may not reflect true feature dependence, limiting applicability to strongly correlated tabular data

## Confidence

- **High:** ORACLE's ability to fit a discrete factorial surrogate and enforce ANOVA identifiability via recentering; empirical NDCG and localization performance on synthetic factorial and UCI benchmarks
- **Medium:** Claims about cross-backbone stability (Xfer-NDCG), scale alignment (CCC), and intervention gain (IG), as these depend on hyperparameter choices for baselines and training reproducibility
- **Low:** Generalization to latent high-cardinality or entangled features (e.g., image or text embeddings), where discretization bias dominates

## Next Checks

1. Run ORACLE with L ∈ {3,5,7,9} on kin8nm and Airfoil; plot NDCG@5, Peak-IoU@0.10, and Xfer-NDCG@5 vs. L to identify optimal resolution
2. Train two independent MLP backbones on kin8nm; measure Xfer-NDCG@5 and CCC between ORACLE explanations; compare against SHAP-family baselines
3. Apply ORACLE to a tabular dataset with known feature dependence (e.g., correlated sensor readings); measure degradation in NDCG/Peak-IoU relative to independent-coordinate assumption