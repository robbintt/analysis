---
ver: rpa2
title: 'Centralized vs. Federated Learning for Educational Data Mining: A Comparative
  Study on Student Performance Prediction with SAEB Microdata'
arxiv_id: '2509.00086'
source_url: https://arxiv.org/abs/2509.00086
tags:
- data
- performance
- federated
- learning
- centralized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared centralized and federated learning for predicting
  student performance using Brazilian SAEB microdata. A centralized XGBoost model
  achieved 63.96% accuracy, while a federated DNN with FedProx reached 61.23%, demonstrating
  a minimal privacy-performance trade-off.
---

# Centralized vs. Federated Learning for Educational Data Mining: A Comparative Study on Student Performance Prediction with SAEB Microdata

## Quick Facts
- arXiv ID: 2509.00086
- Source URL: https://arxiv.org/abs/2509.00086
- Reference count: 16
- Primary result: Centralized XGBoost achieved 63.96% accuracy; federated DNN with FedProx reached 61.23% on SAEB student performance prediction

## Executive Summary
This study compares centralized and federated learning approaches for predicting student performance using Brazilian SAEB microdata. The research demonstrates that federated learning can achieve comparable accuracy to centralized models while preserving student data privacy under LGPD requirements. A centralized XGBoost model achieved 63.96% accuracy, while a federated deep neural network with FedProx reached 61.23%, showing only a minimal 2.73% performance gap. The findings validate federated learning as a viable solution for collaborative educational data mining that addresses privacy concerns without significant accuracy trade-offs.

## Method Summary
The study implemented two approaches: a centralized XGBoost model trained on aggregated SAEB microdata and a federated deep neural network using the FedProx algorithm distributed across multiple institutions. The federated framework simulated data localization by partitioning the dataset to mimic separate institutional repositories. Both models predicted student performance using demographic and socioeconomic features. The centralized approach trained on pooled data achieved 63.96% accuracy, while the federated approach, which maintained data privacy through local training and parameter aggregation, achieved 61.23% accuracy. The comparison focused on prediction accuracy as the primary metric while preserving privacy constraints.

## Key Results
- Centralized XGBoost model achieved 63.96% accuracy on SAEB dataset
- Federated DNN with FedProx achieved 61.23% accuracy, showing only 2.73% performance gap
- Federated learning demonstrates viable privacy-preserving alternative for educational data mining
- Minimal accuracy trade-off supports practical deployment under privacy regulations

## Why This Works (Mechanism)
The federated learning approach works by maintaining data privacy through local model training while still enabling collaborative learning. Each institution trains a local model on its private student data, and only model parameters (not raw data) are shared with a central server. The FedProx algorithm addresses client heterogeneity by adding a proximal term to the local objective function, improving convergence stability across institutions with different data distributions. This mechanism allows institutions to contribute to a shared predictive model without violating LGPD requirements for student data privacy.

## Foundational Learning
- **Federated Learning (FL)**: Decentralized machine learning where models train locally on client devices and parameters are aggregated centrally - needed for privacy compliance when data cannot be shared centrally
- **FedProx**: Federated learning algorithm that adds a proximal term to local objectives to handle heterogeneous client data - needed to improve convergence stability across institutions with different data distributions
- **XGBoost**: Gradient boosting framework using decision trees for classification - needed as strong baseline for comparison with federated approaches
- **LGPD**: Brazilian General Data Protection Law governing personal data usage - needed context for why privacy-preserving approaches are essential in educational settings
- **SAEB microdata**: Brazilian national assessment database containing student performance and demographic information - needed as the specific dataset for this educational data mining application
- **Performance prediction**: Classification task predicting student outcomes based on available features - needed as the specific prediction task being evaluated

## Architecture Onboarding

Component Map: Institutional data repositories -> Local FL models -> Parameter aggregation server -> Global model

Critical Path: Local data preprocessing -> Model training -> Parameter encryption -> Parameter upload -> Aggregation -> Global model update -> Model distribution

Design Tradeoffs: Privacy preservation vs. accuracy (centralized achieves 63.96% vs federated 61.23%), computational overhead of parameter communication vs data transfer costs, model convergence stability vs training time

Failure Signatures: Accuracy degradation indicates client drift or poor convergence, communication failures suggest network reliability issues, parameter divergence signals data heterogeneity problems

First Experiments: 1) Test federated model with varying numbers of institutions (2, 5, 10) to assess scalability, 2) Implement different FL algorithms (FedAvg, FedNova) to compare convergence properties, 3) Add noise injection to parameter updates to test differential privacy guarantees

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can a tree-based federated algorithm (e.g., Federated XGBoost) reduce the performance gap between centralized and federated models below the 2.73% observed with the FedProx DNN?
- Basis in paper: [explicit] The authors state that implementing a "Federated XGBoost framework remains a highly promising... research direction that could potentially close the performance gap even further."
- Why unresolved: The current study only compared a centralized XGBoost against a federated DNN, leaving the performance of a federated tree-based approach unknown.
- What evidence would resolve it: A comparative experiment using a Federated XGBoost algorithm on the same SAEB dataset.

### Open Question 2
- Question: How can Explainable AI (XAI) methods be effectively integrated into the federated framework to provide instance-level explanations without compromising student data privacy?
- Basis in paper: [explicit] The authors note the current model is a "black box" and propose integrating XAI techniques like SHAP as a primary avenue for future research.
- Why unresolved: Standard XAI techniques often require access to raw data or gradient information that may violate the privacy constraints (LGPD) established by the federated setup.
- What evidence would resolve it: A framework demonstrating local explainability generation that aggregates global insights without exposing individual student records.

### Open Question 3
- Question: Does the inclusion of longitudinal Virtual Learning Environment (VLE) engagement data improve the predictive accuracy of federated models beyond the ~61% achieved with cross-sectional socioeconomic data?
- Basis in paper: [explicit] The Future Work section suggests "Enrichment with VLE Data" is a significant next step to create more timely features for early-stage prediction.
- Why unresolved: The current study was limited to cross-sectional SAEB microdata and did not utilize dynamic interaction logs.
- What evidence would resolve it: Experimental results from a federated model trained on combined SAEB and VLE datasets.

## Limitations
- Study limited to single Brazilian educational dataset (SAEB microdata), limiting generalizability
- Comparison only between one federated algorithm (FedProx) and one baseline model (XGBoost)
- Does not address computational overhead differences between centralized and federated approaches
- No analysis of long-term model drift in federated settings

## Confidence
- High confidence in the privacy-performance trade-off characterization for this specific dataset and model combination
- Medium confidence in the generalizability of federated learning effectiveness across different educational contexts
- Medium confidence in the conclusion that federated learning is a viable solution for LGPD compliance

## Next Checks
1. Replicate the study using educational datasets from different countries and educational systems to assess generalizability of the federated learning performance
2. Test alternative federated learning algorithms (FedAvg, FedNova, etc.) and neural network architectures to establish baseline performance ranges
3. Conduct a comprehensive computational overhead analysis comparing communication costs, training time, and resource utilization between centralized and federated approaches