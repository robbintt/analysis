---
ver: rpa2
title: 'Pruning as Regularization: Sensitivity-Aware One-Shot Pruning in ASR'
arxiv_id: '2511.08092'
source_url: https://arxiv.org/abs/2511.08092
tags:
- pruning
- decoder
- encoder
- sparsity
- sensitivity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that one-shot magnitude pruning improves ASR generalization,
  not just compression. Using Whisper-small, the authors combine gradient- and Fisher-based
  sensitivity diagnostics with targeted, component-wise pruning to reveal architectural
  asymmetries.
---

# Pruning as Regularization: Sensitivity-Aware One-Shot Pruning in ASR

## Quick Facts
- **arXiv ID**: 2511.08092
- **Source URL**: https://arxiv.org/abs/2511.08092
- **Reference count**: 0
- **Primary result**: One-shot magnitude pruning improves ASR generalization; 50% decoder self-attention pruning reduces WER by 2.38% absolute on LibriSpeech test-other without fine-tuning.

## Executive Summary
This paper demonstrates that one-shot magnitude pruning can improve ASR generalization, not just compression. Using Whisper-small, the authors combine gradient- and Fisher-based sensitivity diagnostics with targeted, component-wise pruning to reveal architectural asymmetries. Decoder FFNs are pruning-sensitive, while decoder self-attention and last encoder layers contain redundancy that, when pruned, improves performance. Without fine-tuning, 50% pruning of decoder self-attention reduces WER by 2.38% absolute (20.44% relative) on LibriSpeech test-other; 50% pruning of last four encoder layers yields a 1.72% absolute (14.8% relative) improvement. Gains persist on Common Voice and TED-LIUM datasets.

## Method Summary
The authors employ a two-step sensitivity analysis: first-order gradient norms and second-order diagonal Fisher information per parameter group, followed by unstructured magnitude pruning at component granularity. Sensitivity scores S_g(m) = (1/N) Σ ||∇θ_m L||_2 / ||θ_m||_2 and S_h(m) = (1/|I_m|) Σ F_jj are computed over N samples to identify pruning targets. Pruning is applied to decoder self-attention, encoder self-attention, and FFNs at 10-50% sparsity levels. The method uses Whisper's official beam search decoding without any fine-tuning.

## Key Results
- 50% decoder self-attention pruning improves WER by 2.38% absolute (20.44% relative) on LibriSpeech test-other
- 50% pruning of last four encoder layers yields 1.72% absolute (14.8% relative) WER improvement
- At 40% sparsity, sensitivity-aware pruning preserves near-baseline accuracy while global pruning catastrophically fails
- Component-wise pruning reduces parameters from 241M to 143M with minimal WER loss

## Why This Works (Mechanism)
The paper identifies three complementary mechanisms: noise removal from overparameterized models, redundant head trimming in attention layers, and capacity reduction that improves generalization. The sensitivity-aware approach exploits architectural asymmetries—decoder FFNs are pruning-sensitive while decoder self-attention and late encoder layers contain beneficial redundancy when pruned.

## Foundational Learning
- **Sensitivity diagnostics**: Computing first-order gradient and second-order Fisher information per component to identify pruning targets. Why needed: Without sensitivity analysis, pruning could damage critical components. Quick check: Verify sensitivity scores correlate with component importance.
- **Component-wise vs. global pruning**: Applying different sparsity levels to different model components rather than uniform sparsity. Why needed: Different components have different tolerances to pruning. Quick check: Compare WER degradation between component-wise and global pruning at same total sparsity.
- **One-shot pruning**: Applying pruning once without subsequent fine-tuning. Why needed: Enables immediate deployment without retraining overhead. Quick check: Verify WER improvement persists across multiple runs.

## Architecture Onboarding
- **Component map**: Whisper-small (241M params) → 12 encoder layers + 12 decoder layers → each layer contains self-attention, cross-attention, and FFN → sensitivity analysis per component
- **Critical path**: Encoder self-attention → Encoder FFN → Decoder self-attention → Decoder cross-attention → Decoder FFN
- **Design tradeoffs**: Unstructured pruning enables fine-grained sensitivity exploitation but provides no inference speedup; structured pruning would enable acceleration but may lose regularization benefits
- **Failure signatures**: Decoder FFN pruning is highly fragile (>30% causes catastrophic WER collapse); encoder self-attention degrades above 50%; global pruning fails at 30-40% sparsity
- **First experiments**: 1) Baseline WER on LibriSpeech test-other; 2) 50% decoder self-attention pruning WER; 3) 50% last four encoder layers pruning WER

## Open Questions the Paper Calls Out
**Open Question 1**: Do the architectural asymmetries (decoder FFN fragility, decoder self-attention redundancy) generalize to other ASR model families and scales beyond Whisper-small? Only Whisper-small was evaluated; other architectures may exhibit different sensitivity patterns.

**Open Question 2**: Which of the three hypothesized mechanisms (noise removal, redundant head trimming, capacity reduction) primarily drives the generalization improvements observed from pruning? The paper shows pruning improves WER but does not isolate which mechanism causes the improvement.

**Open Question 3**: Can combining sensitivity-aware one-shot pruning with subsequent fine-tuning further amplify regularization gains or enable even higher sparsity levels? No experiments combine the one-shot pruning approach with any retraining phase.

**Open Question 4**: Would structured pruning (e.g., entire attention heads or FFN neurons) yield similar regularization benefits while enabling practical inference speedups? Only unstructured magnitude pruning was tested; structured pruning may alter the regularization dynamics.

## Limitations
- Critical implementation details like sample size N for sensitivity computation are not disclosed
- Exact parameter-to-component mappings within Whisper-small are ambiguous
- No statistical significance tests for reported WER improvements
- Results may not generalize to other ASR architectures beyond Whisper-small

## Confidence
- **High confidence**: Whisper-small with 12 encoder + 12 decoder layers, WER as primary metric, component-wise vs. global pruning comparison, Fisher and gradient sensitivity diagnostics, no fine-tuning requirement
- **Medium confidence**: Exact sensitivity computation parameters (N, batch size), parameter-to-component mappings, Common Voice and TED-LIUM subset sizes, statistical robustness of improvements
- **Low confidence**: Generalization of sensitivity-aware pruning to other ASR architectures, necessity of 50% decoder self-attention sparsity for gains, reproducibility of Common Voice/TED-LIUM results

## Next Checks
1. **Sensitivity Stability**: Run gradient and Fisher sensitivity computation across 3 different sample sizes (e.g., N=500, 1000, 2000) on a fixed subset; verify component rankings are stable across runs.
2. **Component Granularity**: Validate the exact parameter grouping within Whisper-small by printing and comparing total parameters per component before and after pruning at 50% decoder self-attention and 50% last four encoder layers.
3. **Statistical Significance**: Perform 5 independent runs of WER evaluation on LibriSpeech test-other after 50% decoder self-attention pruning; compute mean, std, and p-value vs. baseline.