---
ver: rpa2
title: 'From Theory to Throughput: CUDA-Optimized APML for Large-Batch 3D Learning'
arxiv_id: '2512.19743'
source_url: https://arxiv.org/abs/2512.19743
tags:
- point
- apml
- cuda-apml
- dense
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present CUDA-APML, a sparse CUDA implementation of APML that
  avoids the quadratic memory bottleneck of dense transport plan construction for
  point cloud supervision. By constructing row-wise and column-wise adaptive-softmax
  assignments directly in COO form, performing on-device symmetrization, and applying
  Sinkhorn normalization on the sparse support, CUDA-APML eliminates the need for
  dense NxM tensors.
---

# From Theory to Throughput: CUDA-Optimized APML for Large-Batch 3D Learning

## Quick Facts
- arXiv ID: 2512.19743
- Source URL: https://arxiv.org/abs/2512.19743
- Authors: Sasan Sharifipour; Constantino Álvarez Casado; Manuel Lage Cañellas; Miguel Bordallo López
- Reference count: 34
- Primary result: CUDA-APML achieves >99.9% GPU memory reduction over dense APML while preserving EMD accuracy on ShapeNet and MM-Fi.

## Executive Summary
This paper presents CUDA-APML, a sparse CUDA implementation of the Adaptive Probabilistic Matching Loss (APML) that avoids the quadratic memory bottleneck of dense transport plan construction for point cloud supervision. By constructing row-wise and column-wise adaptive-softmax assignments directly in COO form, performing on-device symmetrization, and applying Sinkhorn normalization on the sparse support, CUDA-APML eliminates the need for dense NxM tensors. The method preserves the accuracy trends of dense APML while enabling practical use of transport-based losses in memory-constrained settings.

## Method Summary
CUDA-APML implements APML using sparse tensor operations in CUDA. The key innovation is constructing the transport plan in COO format by first computing adaptive-softmax assignments for each point cloud separately, then symmetrizing these supports via element-wise minimum, and finally applying Sinkhorn normalization only on the stored pairs. This approach avoids materializing the full NxM transport plan, instead storing only the non-zero entries that survive the pruning threshold. The method includes a custom CUDA kernel for adaptive-softmax computation that identifies minimal distances and applies the temperature-based weighting scheme directly on-device.

## Key Results
- Achieves >99.9% reduction in peak GPU memory compared to dense APML
- Maintains comparable EMD scores: 9.5 on ShapeNet-34 and 16.1 on MM-Fi
- Shows near-linear scaling of stored support and loss-side memory in synthetic tests
- Enables practical use of transport-based losses in large-batch 3D learning scenarios

## Why This Works (Mechanism)
The method exploits the observation that transport plans are often sparse in practice due to the geometric structure of point clouds. By constructing the adaptive-softmax assignments row-wise and column-wise separately, then taking the element-wise minimum to create a valid joint support, the method captures the essential transport structure without storing all pairwise distances. Sinkhorn normalization on this sparse support produces gradients only for the stored pairs, which is mathematically valid since pruned entries contribute zero to both forward and backward passes. The CUDA implementation ensures these operations remain efficient despite the irregular memory access patterns inherent in sparse tensor operations.

## Foundational Learning
- **Adaptive-softmax temperature scaling**: Needed to balance local vs global matching; check by varying temperature and observing EMD sensitivity
- **Sinkhorn normalization**: Iterative matrix scaling to produce doubly stochastic transport plans; verify convergence on small dense examples
- **COO sparse tensor format**: Efficient storage for irregular supports; validate by comparing memory usage against dense tensors
- **Element-wise minimum for symmetrization**: Ensures joint feasibility of row/column supports; test with synthetic 2x2 examples
- **CUDA kernel design for irregular access**: Critical for performance; profile memory bandwidth utilization
- **Transport plan pruning threshold**: Controls memory-accuracy trade-off; sweep τ values and measure impact on EMD

## Architecture Onboarding
- **Component map**: Input clouds -> Adaptive-softmax kernels (row/column) -> COO construction -> Symmetrization (min) -> Sinkhorn (sparse) -> Loss computation
- **Critical path**: Distance computation → Adaptive-softmax → Support construction → Normalization → Gradient backprop
- **Design tradeoffs**: Memory vs accuracy through τ threshold; computational efficiency vs implementation complexity of custom CUDA kernels
- **Failure signatures**: Non-converging Sinkhorn iterations (check max iterations), empty supports (check τ), numerical instability (monitor log-sum-exp)
- **First experiments**: 1) Verify sparse vs dense EMD match on small ShapeNet subset 2) Profile memory usage scaling with batch size 3) Test gradient flow by checking parameter updates

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can neighborhood pruning or approximate nearest neighbor methods eliminate the remaining quadratic complexity in the adaptive-softmax scanning stage while preserving transport accuracy?
- Basis in paper: [explicit] The conclusion states: "A limitation is that the adaptive-softmax stage still scans all pairwise distances, so distance evaluation remains quadratic in the number of points. Future work will restrict the candidate support, for example via neighborhood pruning or approximate nearest neighbors."
- Why unresolved: The current CUDA-APML achieves memory efficiency but the first pass still performs O(NM) distance computations to identify minima and apply thresholding.
- What evidence would resolve it: Experiments comparing k-NN restricted support or spatial hashing against full pairwise scanning, measuring both runtime and EMD/F1-score preservation on ShapeNet and MM-Fi.

### Open Question 2
- Question: How does CUDA-APML perform on large-scale scene-level point clouds and digital twin update scenarios with potentially different sparsity characteristics?
- Basis in paper: [explicit] The conclusion identifies this as future work: "evaluate the method on larger scene-level and digital twin update settings."
- Why unresolved: Current evaluation is limited to object-scale completion (ShapeNet) and human-scale cross-modal generation (MM-Fi); scene-level data may have different density distributions affecting sparsity assumptions.
- What evidence would resolve it: Benchmarks on datasets like ScanNet, SemanticKITTI, or nuScenes showing scaling behavior and accuracy at 100K+ points per cloud.

### Open Question 3
- Question: What are the theoretical approximation bounds between sparse Sinkhorn normalization on the pruned support and the dense transport plan?
- Basis in paper: [inferred] The paper empirically shows accuracy matches "within a small tolerance" but does not provide theoretical guarantees. Section III-B states the sparsity is "based on an empirical property" without formal characterization.
- Why unresolved: Gradients are computed only on stored pairs; pruned entries contribute zero to both forward and backward passes, potentially introducing systematic bias.
- What evidence would resolve it: Theoretical analysis bounding the transport plan deviation as a function of pruning threshold τ, or empirical studies across varied τ values with convergence diagnostics.

### Open Question 4
- Question: How does the pruning threshold τ interact with point cloud density, local geometry, and convergence speed across diverse distributions?
- Basis in paper: [inferred] The paper uses a fixed τ = 10⁻⁸ without ablation. The adaptive-softmax temperature depends on local gaps (Eq. 1), suggesting optimal τ may vary with point cloud characteristics.
- Why unresolved: Different datasets and point densities may require different thresholds to balance memory savings against transport quality.
- What evidence would resolve it: Ablation study varying τ across multiple datasets with different density profiles, reporting nnz counts, memory usage, and task metrics.

## Limitations
- The adaptive-softmax stage still performs O(NM) distance computations, maintaining quadratic complexity in the number of points
- Memory savings depend on sparsity assumptions that may not hold for all point cloud distributions
- Custom CUDA implementation requires significant engineering effort and lacks open-source verification
- Experiments limited to two datasets without analysis of failure cases or robustness to noise

## Confidence
- Memory reduction claims: **High** - direct consequence of eliminating dense NxM tensor
- Computational efficiency gains: **Medium** - speedups not reported, sparse operations can be irregular
- Accuracy preservation: **Medium** - EMD scores similar but not identical, no density ablation studies
- Implementation correctness: **Medium** - custom CUDA kernels, no open-source code provided

## Next Checks
1. Benchmark wall-clock throughput and memory usage on ShapeNet-34 with batch sizes ranging from 1 to 128, comparing dense APML, sparse APML, and baseline Chamfer loss
2. Perform an ablation study on sparse APML accuracy as a function of point cloud density (e.g., 1k, 2k, 4k points per cloud), measuring EMD and Chamfer distance
3. Validate numerical stability and correctness by reproducing a subset of results using an independent sparse transport library or a reimplementation in PyTorch