---
ver: rpa2
title: 'Beyond Multiple Choice: Verifiable OpenQA for Robust Vision-Language RFT'
arxiv_id: '2511.17405'
source_url: https://arxiv.org/abs/2511.17405
tags:
- question
- answer
- mcqa
- options
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the unreliability of multiple-choice question
  answering (MCQA) for evaluating and training multimodal language models. It shows
  that MCQA metrics overestimate model capabilities and encourage answer guessing
  behavior, while reinforcement learning on MCQA data harms open-ended generalization.
---

# Beyond Multiple Choice: Verifiable OpenQA for Robust Vision-Language RFT

## Quick Facts
- arXiv ID: 2511.17405
- Source URL: https://arxiv.org/abs/2511.17405
- Authors: Yesheng Liu; Hao Li; Haiyu Xu; Baoqi Pei; Jiahao Wang; Mingxuan Zhao; Jingshu Zheng; Zheqi He; JG Yao; Bowen Qin; Xi Yang; Jiajun Zhang
- Reference count: 40
- The paper addresses the unreliability of multiple-choice question answering (MCQA) for evaluating and training multimodal language models, showing that MCQA metrics overestimate model capabilities and encourage answer guessing behavior.

## Executive Summary
This paper addresses the fundamental unreliability of multiple-choice question answering (MCQA) for evaluating and training multimodal language models. The authors demonstrate that MCQA metrics can overestimate model capabilities by up to 20 percentage points due to exploitable option signals that encourage answer guessing rather than genuine reasoning. To solve this, they propose ReVeL, a framework that rewrites MCQA questions into open-form while preserving verifiability through a hybrid approach combining rule-based and LLM-based evaluation. When applied to reinforcement fine-tuning on 20k examples, models trained with ReVeL-OpenQA data maintain MCQA accuracy while improving OpenQA accuracy by approximately six percentage points, demonstrating higher data efficiency and robustness.

## Method Summary
The ReVeL framework converts MCQA questions to open-ended format through a triage system that categorizes questions into numeric, keyword, per-option verification, and open-ended types. Category-specific prompt templates rewrite questions while rule-based verifiers handle most cases through pattern matching, synonym enumeration, and True/False lists. Only genuinely open-ended questions require LLM judging. The framework is applied in reinforcement fine-tuning using GRPO on Qwen2.5-VL models trained on 20k rewritten examples, with evaluation on four vision-language benchmarks using the hybrid verification system.

## Key Results
- Models trained on ReVeL-OpenQA matched MCQA accuracy on multiple-choice benchmarks and improved OpenQA accuracy by about six percentage points
- ReVeL evaluation revealed up to 20 percentage points of score inflation in MCQA compared to OpenQA
- Hybrid verification reduced false positive rate from 2.0% to 0.3% across 600 sampled responses
- Between 70% and 96% of questions across datasets can be evaluated through deterministic rules

## Why This Works (Mechanism)

### Mechanism 1
MCQA options provide exploitable signals that inflate accuracy metrics independent of genuine task competence. Options enable "answer guessing behaviors" and "selection heuristics" rather than reasoning from the question stem. When ground-truth options are replaced with "None of the Above," models show reasoning-choice mismatch rates up to 50% (vs. 18% in standard MCQA), indicating reasoning is post-hoc rationalization anchored to available choices.

### Mechanism 2
Categorizing questions by answer type enables hybrid verification that reduces LLM-judge reliance while maintaining accuracy. The triage system routes numeric answers (pattern matching), keyword answers (synonym enumeration), and per-option verification (True/False lists) to deterministic rule-based evaluation. Only genuinely open-ended questions require LLM judging. This reduced false positive rate from 2.0% (pure LLM judge) to 0.3% across 600 sampled responses.

### Mechanism 3
Training on rewritten OpenQA data produces more transferable reasoning than MCQA-based RFT. MCQA-based RFT optimizes for option-exploiting shortcuts, widening the MCQA-OpenQA gap. OpenQA rewards require genuine reasoning from question stem to answer. Models trained on ReVeL-OpenQA maintained MCQA accuracy while improving OpenQA accuracy by ~6 percentage points (40.4 vs 36.3 overall on Qwen2.5-VL-7B).

## Foundational Learning

- **MCQA Option Exploitation**: Why needed here: Understanding how models use options as reasoning shortcuts (vs. question stems) is essential for interpreting the paper's motivation. Quick check question: If a model answers correctly when options are present but fails without them, does it understand the concept?
- **Rule-Based vs. LLM-as-Judge Evaluation**: Why needed here: The paper's hybrid approach depends on knowing when deterministic verification suffices vs. when semantic judgment is necessary. Quick check question: For "What is 2+2?" vs. "Explain why the sky is blue," which needs an LLM judge?
- **Reinforcement Fine-Tuning (RFT) Reward Design**: Why needed here: The paper's core claim is that reward signal quality differs between MCQA and OpenQA formats, affecting what behaviors are reinforced. Quick check question: If rewards can be gamed without genuine reasoning, what happens during RL training?

## Architecture Onboarding

- **Component map**: Triage Classifier -> Rewriting Module -> Hybrid Verifier -> RFT Pipeline
- **Critical path**: Input MCQA (question + options + answer) → Triage → classify answer type → Rewrite → apply category-specific prompt → Verify → route to rule-based or LLM judge → (Training) Use verified rewards for GRPO updates
- **Design tradeoffs**: Coverage vs. precision: Aggressive rewriting may convert questions that lose nuance; conservative filtering retains more LLM-judge dependency. Cost vs. variance: Rule-based evaluation is cheaper but may miss valid answer variants; LLM judging handles nuance but introduces variance. MCQA retention vs. generalization: Training purely on OpenQA may sacrifice some MCQA benchmark performance
- **Failure signatures**: Over-restricted keywords: Valid synonyms rejected (e.g., "3" vs. "three" not enumerated). Ambiguous categorization: Numeric questions with textual units incorrectly routed. Positional memorization: If models still show letter-position bias after rewriting, conversion was incomplete. Rule-based brittleness: Symbolic representations like "1.30~40.45" may fail pattern matching
- **First 3 experiments**: Triage accuracy validation: Sample 100 questions per category, manually verify classification correctness. Hybrid vs. pure LLM judge comparison: On held-out set, compare verification accuracy, latency, and cost. Ablation on training data size: Train with 5k/10k/20k rewritten examples to measure data efficiency curve

## Open Questions the Paper Calls Out
- Can the ReVeL framework be effectively adapted for long-form generation tasks where deterministic verification is inherently difficult?
- Can an adaptive system dynamically select the optimal judging mechanism (rule-based vs. LLM) based on specific question complexity?
- To what extent does the capability of the LLM used for rewriting impact the semantic fidelity of the resulting OpenQA training data?

## Limitations
- The framework's effectiveness depends on the comprehensiveness of rule-based verification, with no validation that edge cases are covered
- Results are based on specific benchmarks and may not generalize to domains with different answer distributions
- The paper's results rely on unspecified GRPO hyperparameters and exact data composition, limiting reproducibility

## Confidence
- **High Confidence**: The empirical finding that OpenQA-trained models maintain MCQA performance while improving OpenQA accuracy (~6 percentage points) is well-supported by the ablation studies and benchmark results
- **Medium Confidence**: The claim that MCQA options cause "answer guessing behaviors" is supported by NOTA mismatch rates (50% vs 18%) but relies on the assumption that option removal isolates reasoning ability
- **Low Confidence**: The assertion that ReVeL improves "data efficiency" is based on a single comparison (20k examples) without establishing the full learning curve

## Next Checks
1. **Cross-domain validation**: Test ReVeL on benchmarks outside the original four (e.g., science, humanities) to verify generalization of the 6-point OpenQA improvement claim
2. **Robustness to adversarial rewriting**: Generate synthetic MCQA questions designed to exploit rule-based verifiers (e.g., numeric answers with unusual formats) and measure ReVeL's verification accuracy degradation
3. **Human evaluation of hybrid judgment**: Conduct blind human assessment of 200 randomly sampled ReVeL-verified answers to establish ground-truth accuracy of the hybrid system vs. pure LLM judging