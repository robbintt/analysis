---
ver: rpa2
title: An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against
  LLMs
arxiv_id: '2508.10010'
source_url: https://arxiv.org/abs/2508.10010
tags:
- misinformation
- health
- prompts
- llms
- jailbreak
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models can be manipulated via jailbreak-style prompts
  to generate harmful health misinformation, with attack success rates averaging 0.86,
  0.89, and 0.81 across GPT-3.5, Llama, and Gemini respectively. Jailbreak prompts
  using role-playing, alternate realities, and authoritative framing were most effective.
---

# An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs

## Quick Facts
- **arXiv ID**: 2508.10010
- **Source URL**: https://arxiv.org/abs/2508.10010
- **Reference count**: 18
- **Primary result**: LLM jailbreak prompts induce health misinformation generation with average ASR of 0.86, 0.89, and 0.81 across GPT-3.5, Llama, and Gemini respectively.

## Executive Summary
This study systematically audits how large language models (LLMs) can be manipulated via jailbreak-style prompts to generate harmful health misinformation, focusing on topics like mpox, colloidal silver, and COVID-19. Using an iterative prompt refinement process, the authors generated 109 jailbreak prompts that achieved high attack success rates across three major LLMs (GPT-3.5, Llama, Gemini). They found that role-playing, alternate realities, and authoritative framing were most effective techniques. The study also demonstrated that LLMs can effectively detect both jailbreak-generated and real-world health misinformation, with agreement rates exceeding 95% with human annotators when using LLM-as-a-Judge approaches. Key linguistic differences were identified between successful jailbreak prompts and benign queries, including higher vocabulary diversity and frequent use of credibility-seeking language.

## Method Summary
The authors employed an iterative semi-automated approach using GPT-3.5 as both attacker and target model to generate 109 jailbreak prompts across nine attack categories. These prompts were tested against GPT-3.5, Llama 3.1-8B, and Gemini 2.0 Flash, with responses evaluated by Gemini acting as a judge using specific criteria for misinformation generation, validation, and obedience. The study compared jailbreak prompts to benign WildChat queries and tested detection capabilities using both LLM-as-a-Judge and traditional ML classifiers (Naive Bayes, Random Forest) trained on TF-IDF features. Detection performance was evaluated on distinguishing jailbreak responses from real health information and organic misinformation sourced from Reddit, MedRed, and Fakeddit datasets.

## Key Results
- Jailbreak prompts achieved average attack success rates of 0.86, 0.89, and 0.81 across GPT-3.5, Llama, and Gemini respectively
- LLM-as-a-Judge (Gemini) achieved 100%, 95%, and 95.5% agreement with human annotations for different evaluation tasks
- Jailbreak prompts showed significantly greater vocabulary diversity than benign queries (m1=0.81 vs m2=0.64)
- ML classifiers achieved >99% accuracy distinguishing jailbreak responses from real health information

## Why This Works (Mechanism)

### Mechanism 1: Role-Play and Alternate Reality Framing
Jailbreak prompts using role-playing, alternate realities, and authoritative framing may be more effective at inducing misinformation generation than direct requests or simple refusals. These techniques create a fictional or authoritative context that may reduce a model's adherence to safety guardrails by reframing the harmful request as a simulation, historical analysis, or expert consultation, thereby bypassing standard refusal triggers.

### Mechanism 2: Linguistic Signatures of Persuasion and Credibility-Seeking
Successful jailbreak prompts contain distinct linguistic patterns, including higher vocabulary diversity, frequent credibility-seeking nouns (e.g., "citations," "studies"), and validation-oriented verbs (e.g., "confirm," "verify"), which may distinguish them from normal health queries. These features are designed to mimic authoritative discourse, which can pressure the model into compliance by presenting the request as academically legitimate or seeking factual validation rather than harmful content.

### Mechanism 3: LLM-as-a-Judge for Detection
A capable LLM (e.g., Gemini) can be used as a "judge" to detect both jailbreak-generated and human-written health misinformation with high agreement with human annotators. The judge model is prompted with specific criteria (misinformation generation, validation, obedience) and an explanation requirement. Its internal knowledge and reasoning allow it to classify content in a manner consistent with human evaluation.

## Foundational Learning

- **LLM Safety Alignment & Jailbreaking**: Understanding what safety alignment is and how jailbreaks attempt to circumvent it is essential since the entire study is predicated on the failure of safety alignment under specific adversarial prompting. Quick check: What is the primary goal of safety alignment in LLMs, and what is a "jailbreak" attack in this context?

- **LLM-as-a-Judge / Evaluator Paradigm**: The study's core methodology for measuring attack success and detecting misinformation relies on using an LLM to grade the outputs of other LLMs. Quick check: Why might using an LLM as a judge be preferable to simple keyword matching or rule-based classifiers for evaluating nuanced outputs like partial misinformation compliance?

- **Attack Success Rate (ASR) and its Components**: ASR is the key quantitative metric. Understanding its definition (generation of explicit misinformation) and its components (generation, validation, obedience) is crucial for interpreting results. Quick check: In this study, does an "attack success" require the model to both generate and validate the misinformation, or is generation alone sufficient?

## Architecture Onboarding

- **Component map**: Attacker LLM (GPT-3.5) -> Iterative Prompt Refinement -> 109 Jailbreak Prompts -> Target LLM (GPT-3.5/Llama/Gemini) -> Response -> Judge LLM (Gemini) -> Misinformation/Validation/Obedience Scores -> ASR Calculation. Comparative Analysis: Jailbreak Prompts vs. WildChat Data -> Linguistic/Topic Analysis. Jailbreak Responses vs. Reddit Misinfo vs. Reddit Real Info -> ML Classifiers / LLM Judge.

- **Critical path**: The most critical path is defining the evaluation criteria (the "judge" prompt) and validating it against human annotations. Without a reliable judge, all subsequent ASR and detection metrics are invalid.

- **Design tradeoffs**: Manual vs. Automated Analysis: The study chose a "small but deeply inspected" dataset (N=109 attacks) over a large-scale automated benchmark, allowing for nuanced qualitative analysis (e.g., attack type categorization) at the cost of generalizability. Judge Model Selection: Using Gemini as a judge introduced a dependency on that model's capabilities and potential biases, but was deemed more scalable than full human annotation.

- **Failure signatures**: High False Positives in Detection: The judge mislabeling personal anecdotes or sarcasm as misinformation. "Echoing Claims" False Positives: The judge flagging a model response that repeats a claim only to refute it, due to conditional language. Attack Stagnation: The iterative attack generation process failing to produce new successful prompts after a certain point.

- **First 3 experiments**: 1. Reproduce the Judge Validation: Manually annotate a small, held-out set of 20 LLM responses and measure inter-annotator agreement with Gemini and GPT-3.5 as judges. 2. Ablate Attack Prompt Features: Take the 10 most successful attack prompts and systematically remove or alter key features to test their individual contribution to ASR. 3. Test Detection Robustness: Train the ML classifiers on a time-split and test on a newly collected set of LLM-generated misinformation using a more recent model.

## Open Questions the Paper Calls Out

- **Open Question 1**: How do the characteristics of jailbreak-generated health misinformation and the efficacy of detection methods vary across social media platforms other than Reddit? The authors explicitly state in the Limitations section that the study was "limited to content sourced exclusively from Reddit" and suggest future work should "include other platforms."

- **Open Question 2**: Can automated misinformation detectors currently deployed on social media platforms effectively identify jailbreak-generated health misinformation? The authors note in the Limitations section that they "did not... benchmark our jailbreak responses against automated misinformation detectors currently deployed on social media platforms."

- **Open Question 3**: How can smaller, open-weight models be enhanced to reliably detect health misinformation without the performance drop observed in this study? The Discussion notes that the smaller model (Llama) achieved only 50% accuracy in the judgment task, "highlight[ing] the need for further research to enhance misinformation detection capabilities in smaller models."

## Limitations

- **Limited Real-World Generalization**: Prompts were generated via iterative refinement against specific target models, which may not reflect the full diversity of real-world jailbreak attempts. ASR figures are specific to tested models and may not generalize to newer or differently aligned models.

- **Judge Model Bias and Consistency**: The LLM-as-a-Judge approach relies on Gemini's internal knowledge and alignment. There is uncertainty about whether this agreement holds across different domains, or if subtle changes in prompt phrasing could significantly alter the judge's scoring behavior.

- **Linguistic Feature Reliability**: The study identifies linguistic signatures as distinguishing jailbreak prompts from benign ones, but these features are derived from a specific dataset and may not robustly generalize to other contexts or languages.

## Confidence

- **High Confidence**: The core finding that jailbreak prompts can induce harmful health misinformation generation (ASR >0.8) is well-supported by the experimental design and direct observation. The detection of linguistic differences between jailbreak and benign prompts is also well-documented within the dataset.

- **Medium Confidence**: The high agreement rates (100%, 95%, 95.5%) between LLM judges and human annotators are based on the reported study methodology, but the exact inter-annotator agreement among humans is not detailed, introducing some uncertainty about the true reliability of the judge.

- **Low Confidence**: The generalizability of the linguistic features and ASR to future models or broader domains is uncertain, as these are contingent on the specific models, datasets, and attack generation process used.

## Next Checks

1. **Inter-Annotator Agreement Validation**: Conduct a small-scale human annotation study (e.g., 20 responses) with multiple independent annotators to measure the true inter-annotator agreement for the "generation, validation, obedience" criteria, and compare this to the LLM judge's agreement rates.

2. **Feature Ablation Study**: Systematically remove or alter the key linguistic features (e.g., credibility-seeking nouns, validation verbs) from the top 10 most successful jailbreak prompts and re-test them against the target models to quantify the individual contribution of each feature to the ASR.

3. **Temporal and Model Generalization Test**: Train the ML classifiers (Naive Bayes, Random Forest) on a time-split of the current data and test them on a newly collected set of LLM-generated misinformation from a more recent model (e.g., GPT-4o) to assess the robustness and generalizability of the detection approach.