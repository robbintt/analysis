---
ver: rpa2
title: 'Word Overuse and Alignment in Large Language Models: The Influence of Learning
  from Human Feedback'
arxiv_id: '2508.01930'
source_url: https://arxiv.org/abs/2508.01930
tags:
- lexical
- llms
- words
- language
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the role of Learning from Human Feedback
  (LHF) in shaping lexical choices of Large Language Models (LLMs), particularly the
  overuse of terms like "delve" and "intricate." The authors propose a method to detect
  LHF-induced lexical preferences by comparing outputs from Llama models trained with
  and without LHF. They then experimentally validate that human evaluators prefer
  texts containing these overrepresented words.
---

# Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback

## Quick Facts
- arXiv ID: 2508.01930
- Source URL: https://arxiv.org/abs/2508.01930
- Authors: Tom S. Juzek; Zina B. Ward
- Reference count: 40
- Primary result: Human evaluators significantly prefer texts containing lexical items overrepresented in LHF-trained models (52.4% vs. 47.6%; χ² = 9.4, p < 0.01)

## Executive Summary
This study investigates how Learning from Human Feedback (LHF) shapes lexical choices in Large Language Models, particularly the overuse of terms like "delve" and "intricate." The authors develop a method to detect LHF-induced lexical preferences by comparing outputs from Llama models trained with and without LHF. They validate their findings through human preference experiments, demonstrating that evaluators systematically favor texts containing these overrepresented words. The results suggest LHF contributes to lexical overuse and highlight the need for transparency in AI alignment research.

## Method Summary
The study compares Llama 3.2-3B Base and Instruct models to identify words with significantly higher frequency in the LHF-trained version. Using 9,853 PubMed abstracts from 2020, the authors generate continuations with both models, clean outputs with GPT-4o, and compute frequency differences using spaCy POS tagging. They calculate an LHF-Score for each word based on its relative frequency increase. To validate, they generate 500 text variants per abstract, filter for length and AI-word contamination, compute LHF-Scores, and conduct a human preference experiment with 400 participants from the Global South, analyzing results with chi-square tests.

## Key Results
- Human evaluators significantly prefer text variants containing high LHF-Score words (52.4% vs. 47.6%; χ² = 9.4, p < 0.01)
- "Nuanced" showed a negative preference trend despite high LHF-Score, suggesting some LHF-words trigger backlash
- The study identifies "radar" as a noise artifact in the LHF-Score calculation, likely due to corpus-specific factors

## Why This Works (Mechanism)

### Mechanism 1: LHF Amplifies Specific Lexical Items
LHF optimizes model outputs against human-derived reward signals. If evaluators systematically favor texts containing "sophisticated" vocabulary like "delve" or "intricate," the optimization loop upweights these tokens, creating a feedback loop resulting in lexical overuse relative to base distributions.

### Mechanism 2: Evaluators Use Lexical Proxies for Quality
Human evaluators often skim content and associate words like "intricate" or "underscore" with academic rigor or higher quality. This cognitive shortcut results in higher ratings for texts containing these words, reinforcing their generation in subsequent model iterations.

### Mechanism 3: LHF Creates Demographic Misalignment
LHF datasets are often annotated by specific populations (e.g., Global South workers). If these groups use or prefer words like "delve" more frequently than target users (e.g., Global North academics), the model effectively learns a dialect intelligible to users but "misaligned" with their stylistic expectations.

## Foundational Learning

**Direct Preference Optimization (DPO) / RLHF**: These training procedures optimize policy based on human preferences. Understanding that DPO directly optimizes policy based on human preferences (without a separate reward model) is crucial for grasping how lexical biases get "baked in."

Quick check: Does the paper argue that lexical overuse stems from base pre-training data or the post-hoc alignment step?

**LHF-Score (Lexical Preference Score)**: This metric quantifies LHF influence by weighting words by their relative frequency increase in Instruct vs. Base models.

Quick check: In the formula, why is a token weighted by its *percentage increase* rather than its absolute frequency?

**Chi-Square (χ²) Test**: Used to validate that human preference for high LHF-Score text is statistically significant and not random chance.

Quick check: What does a p-value < 0.01 in the preference experiment (52.4% vs 47.6%) tell us about the strength of the lexical bias signal?

## Architecture Onboarding

**Component map**: PubMed 2020 abstracts -> Llama 3.2-3B Base/Instruct generation -> GPT-4o cleaning -> spaCy POS tagging -> Frequency comparison -> LHF-Score calculation -> Human A/B testing

**Critical path**:
1. Generate continuations with Base and Instruct models
2. Identify tokens with statistically significant usage increases in Instruct
3. Construct text pairs with high vs. low LHF-Scores
4. Validate with human preference data

**Design tradeoffs**:
- Model Size: Used 3B model for efficiency; larger models (90B) might yield different lexical profiles
- Metric: LHF-Score uses relative increase (%), making it sensitive to rare words that may be corpus artifacts
- Cleaning: Reliance on GPT-4o for cleaning outputs introduces dependency on another LLM which might have its own biases

**Failure signatures**:
- Noise in Score: "Radar" appeared as a top item likely due to specific abstracts in the sample
- Backfire Words: "Nuanced" appeared in high-scoring texts but was *dispreferred* by humans (46.6% preference)

**First 3 experiments**:
1. **Baseline Validation**: Generate 1,000 completions for random topics using Base vs. Instruct to verify if "delve" and "intricate" frequency gaps hold across domains
2. **Score Ablation**: Re-run human preference experiment after removing top-scoring "noisy" items (like "radar") or "backfire" items (like "nuanced") from LHF-Score calculation
3. **Demographic Flip**: Run preference test with a demographic distinct from LHF workforce (e.g., domain experts in Global North) to quantify "misalignment" gap

## Open Questions the Paper Calls Out

**Open Question 1**: Do specific demographic factors (age, dialect, geography) or the mechanics of the evaluation task (e.g., skimming behavior) primarily drive human evaluators' preferences for specific lexical items?

**Open Question 2**: Is the observed link between Learning from Human Feedback (LHF) and lexical overuse consistent across different model families beyond Meta's Llama?

**Open Question 3**: Does LHF induce similar patterns of lexical overuse in languages other than English?

## Limitations

- The study cannot definitively isolate LHF effects from instruction tuning (SFT), as both contribute to lexical shifts
- The LHF-Score metric's sensitivity to rare words creates noise artifacts, as demonstrated by "radar" appearing as a top item
- The demographic homogeneity of validation participants (Global South) raises questions about generalizability to other user populations

## Confidence

**High Confidence**: The empirical observation that human evaluators prefer texts containing specific lexical items (52.4% vs. 47.6%; χ² = 9.4, p < 0.01) is well-supported by experimental data

**Medium Confidence**: The claim that LHF amplifies lexical items is supported by comparing Base vs. Instruct models, but the mechanism could involve instruction tuning rather than preference optimization specifically

**Low Confidence**: The hypothesis that LHF induces misalignment with end-user preferences due to demographic differences in the LHF workforce is speculative and requires direct validation with target user populations

## Next Checks

1. **Score Ablation Validation**: Re-run human preference experiment after removing top-scoring "noisy" items (like "radar") or "backfire" items (like "nuanced") from LHF-Score calculation to isolate clean preference signal

2. **Cross-Domain Replication**: Generate 1,000 completions for random topics using Base vs. Instruct models to verify if "delve" and "intricate" frequency gaps hold across domains outside scientific abstracts

3. **Demographic Flip Test**: Run preference test with a demographic distinct from LHF workforce (e.g., domain experts in Global North) to quantify the "misalignment" gap and test whether lexical preferences differ systematically from target user groups