---
ver: rpa2
title: 'ChatNekoHacker: Real-Time Fan Engagement with Conversational Agents'
arxiv_id: '2504.13793'
source_url: https://arxiv.org/abs/2504.13793
tags:
- agents
- conversational
- live
- were
- interest
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ChatNekoHacker is a real-time conversational agent system that
  strengthens fan engagement for musicians. It integrates Amazon Bedrock Agents for
  autonomous dialogue, Unity for immersive 3D livestream sets, and VOICEVOX for high
  quality Japanese text-to-speech, enabling two virtual personas to represent the
  music duo Neko Hacker.
---

# ChatNekoHacker: Real-Time Fan Engagement with Conversational Agents

## Quick Facts
- **arXiv ID:** 2504.13793
- **Source URL:** https://arxiv.org/abs/2504.13793
- **Reference count:** 5
- **Primary result:** One-hour YouTube Live with 30 participants showed that perceived fun was the dominant predictor (p = 0.01, R² = 0.56) of fan interest in the music duo Neko Hacker.

## Executive Summary
ChatNekoHacker is a real-time conversational agent system designed to enhance fan engagement for musicians during livestreams. It integrates Amazon Bedrock Agents for autonomous dialogue, Unity for immersive 3D livestream sets, and VOICEVOX for high-quality Japanese text-to-speech, enabling two virtual personas to represent the music duo Neko Hacker. In a one-hour YouTube Live with 30 participants, regression analysis showed that agent interaction significantly elevated fan interest, with perceived fun as the dominant predictor (p = 0.01, R² = 0.56). Participants also expressed stronger intentions to listen to the duo's music and attend future concerts. Free-response comments highlighted the need for broader response variety, lower latency, and tighter fact-checking to curb misinformation. These findings underscore that entertaining, interactive broadcasts are pivotal to cultivating fandom and offer actionable insights for deploying conversational agents in entertainment.

## Method Summary
The system uses Amazon Bedrock Agents with a knowledge base of summarized social media posts and Wikipedia activity overviews, integrated with VOICEVOX TTS and Unity 3D visualization. Social media posts are classified into 15 categories, summarized, and indexed for retrieval. Two personas (Neko-Chan and Hacker-Chan) are created using prompt engineering for distinct personalities and Kansai dialect. The system ingests YouTube Live comments, generates responses via the agent, synthesizes speech with VOICEVOX, and drives Unity avatar lip-sync. A post-broadcast survey with 10 Likert-scale items assessed fan interest, with regression analysis identifying "Fun" as the dominant predictor.

## Key Results
- Perceived fun was the only significant predictor of fan interest (p = 0.01, R² = 0.56).
- 83% of participants reported positive fan interest; 60% expressed increased intention to listen to the duo's music.
- System integration of social media knowledge base, web search, and TTS enabled real-time conversational engagement.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Perceived enjoyment ("Fun") is the primary statistical driver of fan interest, outweighing information utility or perceived realism.
- **Mechanism:** Regression analysis of the user study identified "Fun" as the only significant predictor (coefficient 0.59, p=0.01) for increased interest in the artist. Utility and realism showed weak, statistically insignificant coefficients (p=0.13 and p=1.0 respectively).
- **Core assumption:** Survey respondents accurately distinguish between "Fun" (enjoyment) and "Useful" (information gain) as distinct constructs during a short (1-hour) exposure.
- **Evidence anchors:**
  - [abstract] "perceived fun as the dominant predictor (p = 0.01)"
  - [Section 3] "The analysis revealed that 'Fun' was the only factor contributing significantly to 'Interest'..."
  - [corpus] Related work on "Animating Language Practice" supports engagement via stylized agents, though specific statistical dominance of fun over utility is not confirmed in the corpus.
- **Break condition:** If the target audience seeks technical documentation or factual learning rather than entertainment, this mechanism likely fails.

### Mechanism 2
- **Claim:** Persona consistency relies on structured knowledge indexing (RAG) rather than fine-tuning alone.
- **Mechanism:** Instead of raw ingestion, social media posts are first classified by an LLM into 15 categories, summarized, and then indexed. This filtering reduces noise, allowing the agent to retrieve high-relevance context (e.g., specific song creation stories) to maintain the "Kansai dialect" persona.
- **Core assumption:** The 15 predefined categories sufficiently capture the semantic space of the artist's historical activities and personality.
- **Evidence anchors:**
  - [Section 2.2] "...LLM was used to classify the Social Media Posts into 15 different categories... and the summary results were indexed."
  - [Section 3] Hacker-Chan successfully references specific song concepts ("Spaceship") consistent with indexed posts.
  - [corpus] Corpus papers (e.g., Character-LLM) suggest fine-tuning is resource-intensive; this approach offers a lighter-weight alternative via summarization.
- **Break condition:** If the artist's personality is defined by nuance that fails to fit into the 15 categories, the retrieved context will be generic, breaking the immersion.

### Mechanism 3
- **Claim:** Real-time interactivity creates a pathway to commercial action (e.g., merchandise sales).
- **Mechanism:** The system bridges the gap between chat and e-commerce. An anecdotal observation noted a viewer commenting on an out-of-stock item, which was subsequently purchased after restocking, implying the agent's presence maintained or reactivated purchasing intent during the live stream.
- **Core assumption:** The conversation directly influenced the timing or decision to purchase, rather than the purchase occurring coincidentally.
- **Evidence anchors:**
  - [Section 4] "...a viewer commented regarding an out of stock item... following the subsequent restocking, the item was purchased."
  - [abstract] "Participants also expressed stronger intentions to listen... and attend future concerts."
  - [corpus] Weak support in corpus for direct commercial conversion; evidence is primarily anecdotal in this paper.
- **Break condition:** If the Action group integration (web search/external API) fails to update inventory status in real-time, the agent cannot facilitate the transaction.

## Foundational Learning

- **Concept:** Retrieval-Augmented Generation (RAG) with Summarization
  - **Why needed here:** The system does not fine-tune the LLM on artist data. Instead, it relies on a Knowledge Base. Understanding how to structure data (summarization/classification) before indexing is critical to reducing hallucinations.
  - **Quick check question:** How does summarizing social media posts before indexing them into a vector database affect retrieval precision compared to indexing raw text?

- **Concept:** Regression Analysis (Least Squares Method)
  - **Why needed here:** To validate the system's success, one must understand the statistical results provided (R², p-value, coefficients) to distinguish between what *felt* like it worked (e.g., "Reality") and what *actually* drove the outcome ("Fun").
  - **Quick check question:** In a regression model with R² = 0.56, what does the p-value of 0.01 for "Fun" vs. 1.0 for "Reality" tell us about which variable to optimize?

- **Concept:** Latency in Streaming Architectures
  - **Why needed here:** The paper highlights latency as a key user complaint. Understanding the additive latency of YouTube comment extraction → LLM inference → TTS synthesis → Unity animation is necessary for "Real-Time" performance.
  - **Quick check question:** Which component in the stack (LLM, TTS, or Network) is most likely the bottleneck for "conversational" turn-taking speeds (<1.0s ideal)?

## Architecture Onboarding

- **Component map:** YouTube Live comments → Amazon Bedrock Agents → Knowledge Base (Summarized social media) → VOICEVOX TTS → Unity 3D (Lip-sync & Audio)

- **Critical path:**
  1. **Ingest:** Unity polls YouTube Live comments.
  2. **Orchestrate:** Comment sent to Amazon Bedrock Agent.
  3. **Retrieve:** Agent queries Knowledge Base (Summaries of Artist Posts).
  4. **Generate:** Agent formulates response in specific persona (Kansai dialect).
  5. **Synthesize:** Text sent to VOICEVOX → Audio returned.
  6. **Render:** Unity plays audio and drives avatar lip-sync.

- **Design tradeoffs:**
  - **Pre-processing vs. Context Window:** The system chooses to *summarize* posts into 15 categories before indexing. This lowers token cost and retrieval noise but risks losing specific conversational nuance (detail loss).
  - **Autonomous vs. Scripted:** Using Bedrock Agents allows dynamic responses but introduces the risk of misinformation (hallucination), whereas a script-based bot would be safe but boring.

- **Failure signatures:**
  - **Hallucination:** Agent invents song titles or facts not in the Knowledge Base (Paper notes risk of "misinformation").
  - **Latency lag:** Gap between viewer comment and avatar vocalization exceeds ~3 seconds, breaking the illusion of conversation.
  - **Persona drift:** Agent loses the "Kansai dialect" or specific tone if prompt engineering constraints are overwhelmed by user prompt injection.

- **First 3 experiments:**
  1. **Latency Benchmark:** Measure the end-to-end round-trip time (Comment posted → Audio starts) to identify if the bottleneck is LLM inference or TTS generation.
  2. **Knowledge Base Ablation:** Test retrieval quality by asking the agent about a specific obscure post; compare "Raw Indexing" vs. "Summarized Indexing" for factual accuracy.
  3. **Regression Validation:** Run a small A/B test manipulating only the "Fun" element (e.g., humorous prompts vs. neutral prompts) to validate the paper's finding that Fun ∝ Interest.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the dominance of "perceived fun" as a predictor of fan interest generalize to larger audiences and different musical genres?
- **Basis in paper:** [explicit] The authors state that "due to the small sample size of 30 respondents, caution must be exercised when generalizing the results."
- **Why unresolved:** The current study is limited to a specific duo (Neko Hacker) and a small sample size, making it difficult to ascertain if the statistical significance of "fun" over "usefulness" or "reality" applies broadly to the music industry.
- **What evidence would resolve it:** Replication of the regression analysis across multiple live streams featuring diverse artists with a significantly larger participant pool (N > 100).

### Open Question 2
- **Question:** How can real-time conversational agents effectively mitigate factual inaccuracies and chronological inconsistencies without introducing latency that degrades the user experience?
- **Basis in paper:** [explicit] The discussion notes that some responses included "inaccuracies or chronologically inconsistent information" and identifies the need for "tighter fact-checking" and "lower latency."
- **Why unresolved:** There is a technical trade-off between the computational cost of rigorous fact-checking (e.g., complex RAG verification) and the need for real-time, low-latency responses during a live stream.
- **What evidence would resolve it:** A system design that integrates automated citation or verification steps, evaluated on both response latency (ms) and factual error rate compared to the baseline.

### Open Question 3
- **Question:** Does implementing dynamic interplay between multiple agents significantly improve fan engagement compared to independent agent responses?
- **Basis in paper:** [explicit] The authors identify "improving the interplay between the two conversational agents" as a necessary step for future work.
- **Why unresolved:** While the system supports two personas, the paper does not isolate the specific impact of agent-to-agent interaction (versus agent-to-user interaction) on the "sense of unity" or "fun" metrics.
- **What evidence would resolve it:** An A/B test comparing user ratings of "Fun" and "Reality" in conditions where agents acknowledge each other versus conditions where they operate in isolation.

### Open Question 4
- **Question:** Can tailoring conversational content to individual fan preferences increase the intention to purchase or attend events beyond the baseline levels observed?
- **Basis in paper:** [explicit] The conclusion suggests that future agents "could learn individual fan preferences and interests to provide tailored conversational experiences."
- **Why unresolved:** The current implementation uses a broadcast model; it is unknown if personalization is technically feasible in a live stream setting or if it meaningfully alters behavioral intent.
- **What evidence would resolve it:** A prototype implementation that adapts topics based on user comment history, followed by a survey measuring changes in the "ListenMore" or "JoinMore" scores.

## Limitations
- Commercial conversion claim is observational, not experimentally validated.
- Regression results based on a single one-hour session with 30 participants; insufficient for generalization.
- Paper does not report on hallucination rates or misinformation frequency during the live session.

## Confidence
- **High:** The technical architecture description (Bedrock Agents + VOICEVOX + Unity) is clearly specified and reproducible.
- **Medium:** The statistical dominance of "Fun" as a predictor is supported by the reported regression coefficients and p-values, though the small sample size limits confidence in effect size.
- **Low:** The commercial impact claim (Section 4 anecdote) lacks experimental validation and could be coincidental timing.

## Next Checks
1. **Commercial conversion experiment:** Design an A/B test comparing merchandise sales during streams with and without agent interaction, controlling for restocking timing and viewer count.
2. **Hallucination audit:** Implement logging of all agent responses with knowledge base retrieval sources, then conduct post-session fact-checking to quantify misinformation frequency.
3. **Cross-demographic replication:** Repeat the one-hour livestream experiment with different fan segments (age groups, music genres) to validate the regression finding that Fun consistently predicts Interest.