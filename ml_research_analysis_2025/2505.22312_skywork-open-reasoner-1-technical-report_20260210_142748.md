---
ver: rpa2
title: Skywork Open Reasoner 1 Technical Report
arxiv_id: '2505.22312'
source_url: https://arxiv.org/abs/2505.22312
tags:
- training
- entropy
- performance
- experiments
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Skywork-OR1 introduces an effective and scalable reinforcement
  learning (RL) implementation for long Chain-of-Thought (CoT) models. Building on
  the DeepSeek-R1-Distill model series, the approach employs a Multi-stage Adaptive
  entropy scheduling for GRPO In Convergence (MAGIC) pipeline, which includes data
  mixture with stringent filtering, multi-stage training, high-temperature sampling,
  and adaptive entropy control without KL loss.
---

# Skywork Open Reasoner 1 Technical Report

## Quick Facts
- arXiv ID: 2505.22312
- Source URL: https://arxiv.org/abs/2505.22312
- Reference count: 39
- Key outcome: Introduces an effective and scalable RL implementation for long CoT models, achieving significant performance improvements on AIME and LiveCodeBench benchmarks.

## Executive Summary
Skywork-OR1 presents a reinforcement learning approach for enhancing long Chain-of-Thought reasoning in language models, built on the DeepSeek-R1-Distill model series. The method employs a Multi-stage Adaptive entropy scheduling for GRPO In Convergence (MAGIC) pipeline, incorporating data mixture with filtering, multi-stage training, high-temperature sampling, and adaptive entropy control without KL loss. The framework demonstrates substantial performance gains, improving accuracy across AIME24, AIME25, and LiveCodeBench benchmarks by 15.0% for the 32B model and 13.9% for the 7B model. All training resources, including model weights, code, and datasets, are fully open-sourced to support community research.

## Method Summary
The MAGIC pipeline implements reinforcement learning fine-tuning of DeepSeek-R1-Distill-Qwen-7B/32B models for mathematical reasoning and coding tasks. The approach uses multi-stage training with progressively increasing context lengths (8K→16K→32K for 7B-Math, 16K→24K for 32B), high-temperature sampling (τ=1.0), on-policy updates (DR=DT, Nreuse=1), and adaptive entropy control with target entropy of 0.2. The training employs token-level policy loss without length normalization, rejects groups with zero advantages, and omits KL loss. Data consists of filtered NuminaMath-1.5 subsets, DeepScaleR, STILL-3, Omni-MATH, and AIME pre-2024 for math (~105K after filtering), plus LeetCode and TACO for code (13.7K total). Verifiers include Math-Verify v0.6.0 and custom code sandbox with AST validation and 50GB memory cap.

## Key Results
- Skywork-OR1-32B surpasses DeepSeek-R1 and Qwen3-32B on AIME benchmarks
- Skywork-OR1-7B and Skywork-OR1-Math-7B demonstrate competitive reasoning capabilities among similarly sized models
- Comprehensive ablation studies validate each component's effectiveness
- Entropy collapse mitigation is identified as critical for improved test performance

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Entropy Control Prevents Premature Policy Convergence
The framework maintains policy entropy above a target threshold (0.2) during RL training through a feedback loop that dynamically adjusts the entropy loss coefficient. This prevents premature convergence to suboptimal deterministic solutions and preserves exploration capability throughout training. The relationship between sustained entropy and test performance is monotonic—higher sustained entropy enables better exploration and ultimately higher accuracy.

### Mechanism 2: Multi-Stage Training Improves Token Efficiency While Preserving Scaling
Training divides into stages with progressively increasing context lengths (8K→16K→32K), reducing computational cost in early stages without sacrificing final performance. Shorter contexts in early stages reduce rollout time—the dominant cost factor—and encourage concise reasoning. Later stages with longer contexts allow elaboration while retaining efficiency gains.

### Mechanism 3: On-Policy Updates Outperform Off-Policy Data Reuse for Sustained Learning
Pure on-policy updates (one SGD step per rollout batch) lead to slower entropy decay and better final test performance compared to off-policy updates with data reuse. Reusing rollout data introduces off-policy samples that accelerate convergence to narrow solutions, while on-policy updates ensure each gradient step uses fresh samples aligned with the current policy.

## Foundational Learning

Concept: Policy Entropy in Reinforcement Learning
- Why needed here: The MAGIC framework revolves around monitoring and controlling entropy to prevent premature convergence
- Quick check question: If a policy's entropy approaches zero, what does this imply about its action distribution and exploration ability?

Concept: Group Relative Policy Optimization (GRPO)
- Why needed here: GRPO is the base algorithm modified by MAGIC; it uses group-normalized rewards to estimate advantages without value function approximation
- Quick check question: In GRPO, how is the advantage computed for each response within a group of M responses for the same prompt?

Concept: On-Policy vs. Off-Policy Updates
- Why needed here: The paper demonstrates that data reuse accelerates entropy collapse; understanding this distinction is critical for hyperparameter choices
- Quick check question: If you reuse a rollout buffer 4 times (Nreuse=4), why does this introduce off-policy data?

## Architecture Onboarding

Component map: Data Pipeline (Source selection → Preprocessing → Model-aware difficulty estimation → Quality assessment) → Training Pipeline (High-temperature rollout → Group advantage computation → Token-level policy loss → Adaptive entropy control → Multi-stage progression) → Verifiers (Math-Verify / custom sandbox)

Critical path: Quality filtering → Difficulty estimation → Stage I (short context) → Enable adaptive entropy control → Monitor entropy vs. target → Stage transition on plateau

Design tradeoffs:
- KL Loss: Omitted; hinders late-stage gains
- Advantage Mask: Not used; penalizing truncation improves efficiency without harming scaling
- Temperature: High (1.0) preferred despite slower early progress

Failure signatures:
- Entropy collapses to near zero within ~100 steps → Check if temperature too low or Nreuse > 1
- Training accuracy rises but non-truncated accuracy falls → Expected early-stage response shortening
- Early plateau in Stage I → Increase batch size or audit data quality

First 3 experiments:
1. Reproduce baseline: DR=64, DT=64, Nreuse=1, gs=16, T=16K, τ=1.0. Monitor entropy on AIME24; expect gradual decline with steady improvement
2. Ablate temperature: Compare τ=0.6 vs τ=1.0. Expect τ=0.6 to show rapid collapse and plateau
3. Ablate on-policy: Compare Nreuse=1 vs Nreuse=4 with fixed compute. Expect Nreuse=4 to converge faster but to lower final accuracy

## Open Questions the Paper Calls Out

Open Question 1: How can stable entropy control be achieved in off-policy training settings with large N_SGD, and why does adaptive entropy control remain unstable when multiple gradient steps are performed?
- Basis: Section 4.5 notes instability when N_SGD is large due to entropy loss being computed over entire vocabulary
- Why unresolved: Paper identifies the instability but only speculates on the cause without systematic investigation of alternative mechanisms

Open Question 2: How should code verifiers handle problems where the same input can yield multiple valid outputs?
- Basis: Section 7.2 states current sandbox doesn't handle non-deterministic or open-ended problems
- Why unresolved: Current rule-based verification framework assumes deterministic input-output mappings

Open Question 3: Is there a principled method for determining the optimal higher-clip ratio across different tasks and domains?
- Basis: Section 4.5 shows clip ratio sensitivity but provides no systematic approach for tuning
- Why unresolved: Paper demonstrates sensitivity but offers no systematic approach beyond trial-and-error

## Limitations

- The adaptive entropy control mechanism relies on an arbitrary target entropy threshold of 0.2
- Multi-stage training benefits depend on specific context length choices that may not generalize
- The on-policy vs. off-policy comparison is conducted within a narrow hyperparameter space

## Confidence

- High Confidence: Empirical performance improvements on AIME and LiveCodeBench benchmarks are well-documented with clear before/after comparisons
- Medium Confidence: Adaptive entropy control mechanism is validated through controlled ablations, but causal link between sustained entropy and reasoning quality could benefit from additional analysis
- Medium Confidence: Multi-stage training efficiency claims are supported by token-level analysis, but skill transfer across context lengths warrants further investigation

## Next Checks

1. Apply the adaptive entropy control mechanism to a different reasoning task domain (e.g., scientific reasoning or logical inference) to verify whether the target entropy of 0.2 generalizes or requires task-specific tuning

2. Vary the context length progression (e.g., 4K→8K→16K or 16K→32K→64K) to determine whether efficiency gains stem from the multi-stage structure itself or the specific length choices made in this work

3. Conduct a time-to-target-accuracy comparison between on-policy and off-policy updates under identical wall-clock constraints to quantify the practical tradeoff between convergence speed and final performance