---
ver: rpa2
title: 'Online Convex Optimization with Heavy Tails: Old Algorithms, New Regrets,
  and Applications'
arxiv_id: '2508.07473'
source_url: https://arxiv.org/abs/2508.07473
tags:
- theorem
- have
- which
- optimization
- stochastic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper examines the performance of classical online convex\
  \ optimization (OCO) algorithms under heavy-tailed noise, where stochastic gradients\
  \ only have finite p-th central moments for some p \u2208 (1,2]. The main result\
  \ is that three standard OCO algorithms\u2014Online Gradient Descent (OGD), Dual\
  \ Averaging (DA), and AdaGrad\u2014can achieve optimal in-expectation regret bounds\
  \ without any algorithmic modification, provided the feasible set is bounded."
---

# Online Convex Optimization with Heavy Tails: Old Algorithms, New Regrets, and Applications

## Quick Facts
- **arXiv ID:** 2508.07473
- **Source URL:** https://arxiv.org/abs/2508.07473
- **Reference count:** 40
- **One-line primary result:** Standard OCO algorithms (OGD, Dual Averaging, AdaGrad) achieve optimal heavy-tailed regret bounds without modification, provided the feasible set is bounded.

## Executive Summary
This paper establishes that classical Online Convex Optimization (OCO) algorithms can achieve optimal regret bounds in the presence of heavy-tailed noise (where gradients have finite p-th central moments for p ∈ (1,2]) without any algorithmic modification, as long as the feasible set is bounded. The key insight is that the bounded domain assumption can be leveraged to handle heavy-tailed noise through refined analysis of the algorithms' standard regret bounds. Notably, AdaGrad can achieve these bounds without requiring knowledge of problem-dependent parameters like the Lipschitz constant, noise level, or tail index. These results have important implications for stochastic optimization and finding stationary points in nonconvex optimization under heavy-tailed noise.

## Method Summary
The paper analyzes three standard OCO algorithms (Online Gradient Descent, Dual Averaging, and AdaGrad) under heavy-tailed noise conditions where stochastic gradients have finite p-th central moments but potentially infinite variance. The analysis leverages the bounded diameter of the feasible set to control noise terms through Young's inequality, decomposing gradients into true gradients and noise components. AdaGrad is shown to achieve optimal bounds without prior knowledge of problem parameters by implicitly adapting to the unknown tail index through its accumulation of gradient norms. The theoretical results are then connected to convergence guarantees for nonconvex optimization through the Online-to-Nonconvex Conversion framework.

## Key Results
- OGD, Dual Averaging, and AdaGrad achieve regret bounds of the form GD√T + σDT^{1/p} without algorithmic modification
- AdaGrad achieves these bounds without knowing the Lipschitz constant G, noise level σ, or tail index p
- These bounds are optimal in all parameters and recover the standard √T rate when p=2
- First optimal convergence rates for stochastic convex and nonconvex optimization under heavy-tailed noise without gradient clipping
- First convergence results for Hölder smooth nonconvex optimization under heavy tails

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Standard algorithms achieve optimal heavy-tailed regret if the feasible set is bounded, without gradient clipping.
- **Mechanism:** The standard regret analysis relies on E[‖g_t‖²] < ∞, which fails for heavy tails (p < 2). This paper uses a tighter "dual" inequality (Eq. 2) introducing a cross-term ⟨g_t, x_t - x_{t+1}⟩. By decomposing the gradient g_t = ∇ℓ_t + ε_t and applying Young's inequality, the noise term is bounded by η^{p-1}‖ε_t‖^p D^{2-p}. The bounded diameter D is essential to control the movement ‖x_t - x_{t+1}‖, ensuring the noise contribution scales as T^{1/p} rather than exploding.
- **Core assumption:** The feasible set X has a finite diameter D (Bounded Domain Assumption).
- **Evidence anchors:**
  - [abstract] "provided the feasible set is bounded... achieve regret bounds of the form GD√T + σDT^{1/p}"
  - [section 3.1] "A less well known analysis... leverage the boundness property of X... [to handle] ⟨g_t, x_t - x_{t+1}⟩"
  - [corpus] "Accelerated stochastic first-order method..." confirms prior reliance on clipping/normalization for similar problems.
- **Break condition:** Unbounded domains (where ‖x_t - x_{t+1}‖ cannot be bounded by D) or noise with p ≤ 1 (where only the mean exists).

### Mechanism 2
- **Claim:** AdaGrad achieves optimal heavy-tailed regret without prior knowledge of the Lipschitz constant G, noise level σ, or tail index p.
- **Mechanism:** AdaGrad guarantees a path-wise regret bounded by D√(∑‖g_t‖²). In the heavy-tailed setting (p ≤ 2), the ℓ₂-norm of the noise is bounded by the ℓ_p-norm (i.e., √(∑‖ε‖²) ≤ (∑‖ε‖^p)^{1/p}). This allows the algorithm to implicitly adapt the stepsize to the unknown tail index, maintaining the optimal GD√T + σDT^{1/p} bound.
- **Core assumption:** The relationship ‖x‖₂ ≤ ‖x‖_p holds for p ∈ [1, 2].
- **Evidence anchors:**
  - [section 3.3] "AdaGrad... achieve this result without knowing any of the Lipschitz parameter G, noise level sigma, and tail index p."
  - [section 3.3] "observe that √(∑‖g_t‖²) ≲ ... + (∑‖ε_t‖^p)^{1/p}"
- **Break condition:** If noise statistics are adversarial or non-i.i.d. in a way that violates the martingale difference sequence properties used for the expectation.

### Mechanism 3
- **Claim:** These OCO bounds translate directly to convergence guarantees for nonsmooth nonconvex optimization (finding stationary points) under heavy-tailed noise.
- **Mechanism:** The paper utilizes the "Online-to-Nonconvex Conversion" (O2NC) framework. This reduces finding a (δ, ε)-stationary point to an online learning problem with K-shifting regret. The heavy-tailed regret bounds established for OGD/AdaGrad are plugged into this meta-algorithm.
- **Core assumption:** The objective function F is Lipschitz and well-behaved (Assumption 2); the Online-to-Nonconvex conversion framework applies.
- **Evidence anchors:**
  - [section 4.2] "Theorem 4 provides a new and the first theoretical guarantee for O2NC under heavy tails."
  - [abstract] "first provable and optimal convergence result for nonsmooth nonconvex optimization under heavy-tailed noise without gradient clipping."
- **Break condition:** Objectives that are not Lipschitz or where the gradient sampling fails to capture the subgradient structure defined in the O2NC framework.

## Foundational Learning

- **Concept:** Finite p-th Central Moment (p ∈ (1, 2])
  - **Why needed here:** This defines the "heavy-tailed" condition. Unlike standard analysis requiring finite variance (p=2), this setting allows infinite variance, requiring different bounding techniques (e.g., bounding ‖ε‖₂ via ‖ε‖_p).
  - **Quick check question:** If p=1.5, is the variance of the gradient noise finite?

- **Concept:** Bounded Domain Diameter (D)
  - **Why needed here:** The core mathematical trick relies on bounding ‖x_t - x_{t+1}‖ ≤ D to neutralize the lack of higher-order moments in the noise.
  - **Quick check question:** Can we use this analysis for standard unconstrained Deep Learning training?

- **Concept:** Martingale Difference Sequences
  - **Why needed here:** To take expectations of the regret bounds, the analysis relies on the stochastic noise ε_t having conditional expectation zero (E[ε_t | F_{t-1}] = 0) and independent moments.
  - **Quick check question:** Does the noise at step t depend on the noise at step t-1?

## Architecture Onboarding

- **Component map:** Stochastic Gradient Oracle → Standard Optimizers (OGD/DA/AdaGrad) → Projection Operator → Bounded Domain
- **Critical path:** The projection step is no longer just a regularization detail; it is the causal enabler of the heavy-tailed guarantee. Without Π_X, the term ‖x_t - x_{t+1}‖ cannot be bounded by a constant D, and the Young's inequality decomposition fails.
- **Design tradeoffs:**
  - **Bounded vs. Unbounded:** The paper trades the practical flexibility of unbounded domains (standard in DL) for provable convergence without clipping.
  - **AdaGrad vs. OGD:** AdaGrad removes the need to tune hyperparameters (G, σ, p) but adds complexity in the adaptive stepsize calculation.
  - **Gradient Clipping:** The paper proves clipping is *unnecessary* on bounded domains, saving computational overhead and hyperparameter tuning, but necessitates the domain constraint.
- **Failure signatures:**
  - **Divergence on Unbounded Sets:** If the diameter D is effectively infinite (e.g., no projection), the algorithm may diverge or fail to achieve the stated regret bounds.
  - **Parameter Mismatch in OGD:** If using OGD/DA with an incorrect estimate of p or σ (unlike AdaGrad), the fixed stepsize η_t may be suboptimal, leading to slower convergence than T^{1/p}.
- **First 3 experiments:**
  1. **Bounded Linear Regression:** Implement OGD on a synthetic linear regression task with Pareto-distributed gradient noise (p=1.5) and a hard constraint ball X. Verify regret scales as T^{0.66}.
  2. **AdaGrad Adaptivity Test:** Compare AdaGrad and OGD on the same heavy-tailed task. Perturb G and σ in OGD's stepsize to show AdaGrad matches or outperforms without this knowledge.
  3. **Constraint Ablation:** Run the same experiment without the projection constraint. Observe if the regret bound breaks or diverges, contrasting with the bounded case.

## Open Questions the Paper Calls Out

- **Question:** Can classical OCO algorithms achieve optimal regret under heavy-tailed noise without relying on the bounded domain assumption?
  - **Basis in paper:** [explicit] The authors state in Section 6: "The main limitation of our work is that all the proof crucially relies on the bounded domain assumption... Finding a weaker sufficient condition... is a direction worth studying in the future."
  - **Why unresolved:** The current analysis requires the bounded diameter D to control gradient terms via inequalities like ‖x_t - x_{t+1}‖ ≤ D. Unbounded domains present different theoretical challenges.
  - **What evidence would resolve it:** A theoretical proof establishing regret bounds for OGD or AdaGrad on unbounded domains with heavy-tailed noise without gradient clipping.

- **Question:** Is the regret bound O(T^{2-p}) for Online Strongly Convex Optimization with heavy tails tight, or can it be improved?
  - **Basis in paper:** [explicit] In Appendix A.2, the authors note: "we suspect Theorem 8 is not tight in T for p ∈ (1,2)... Therefore, we conjecture that a way to obtain a better regret bound than T^{2-p} exists."
  - **Why unresolved:** The current upper bound converts to a convergence rate worse than the known lower bound 1/T^{2-2/p}, suggesting a discrepancy.
  - **What evidence would resolve it:** An analysis proving a tighter upper bound or a specific lower bound demonstrating that T^{2-p} is optimal for this setting.

- **Question:** Can the gap between upper and lower bounds for nonsmooth nonconvex optimization be closed for general accuracy ε or the noiseless case (σ=0)?
  - **Basis in paper:** [explicit] Section 4.2.3 states: "However, for any general ε > 0 or the case σ = 0, there is still a gap between the upper and lower bounds. Closing this gap could be an interesting direction for the future."
  - **Why unresolved:** Theorem 6 provides matching bounds only for small ε and σ > 0.
  - **What evidence would resolve it:** A modified lower bound proof or an improved upper bound analysis that aligns for all ε values.

## Limitations

- The theoretical results critically depend on the bounded domain assumption, limiting applicability to unconstrained optimization problems like deep learning.
- The analysis assumes martingale difference sequences for noise, which may not hold in all adversarial or non-i.i.d. settings.
- The paper does not provide empirical validation of the theoretical claims on real-world problems.

## Confidence

- **High Confidence:** The regret bounds for OGD and AdaGrad under bounded domains with heavy-tailed noise (Section 3). The core analysis using Young's inequality and the bounded diameter is mathematically sound.
- **Medium Confidence:** The translation of OCO results to nonconvex optimization convergence via the Online-to-Nonconvex Conversion framework (Section 4). This relies on the correctness of the O2NC framework itself.
- **Medium Confidence:** The claim that AdaGrad achieves optimal bounds without knowing G, σ, or p. While the mathematical argument is provided, empirical validation would strengthen this claim.

## Next Checks

1. **Bounded Domain Necessity Test:** Implement OGD on a heavy-tailed task with and without projection onto a bounded domain. Demonstrate that the T^{1/p} regret term only manifests when the constraint is enforced.
2. **AdaGrad Adaptivity Verification:** Run AdaGrad and OGD on the same heavy-tailed problem while systematically varying the true noise parameters (G, σ, p) away from the OGD-tuned values. Show AdaGrad maintains optimal performance while OGD degrades.
3. **Convergence Rate Measurement:** For a nonsmooth nonconvex objective, use the heavy-tailed OCO bounds to compute the rate of finding a (δ, ε)-stationary point. Empirically verify the rate matches the theoretical prediction (e.g., T^{(p-1)/p} for δ = ε).