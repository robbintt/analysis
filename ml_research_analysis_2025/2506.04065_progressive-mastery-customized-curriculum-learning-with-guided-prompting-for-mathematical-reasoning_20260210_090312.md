---
ver: rpa2
title: 'Progressive Mastery: Customized Curriculum Learning with Guided Prompting
  for Mathematical Reasoning'
arxiv_id: '2506.04065'
source_url: https://arxiv.org/abs/2506.04065
tags:
- training
- samples
- learning
- difficulty
- curriculum
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the limitations of post-training large language
  models (LLMs) for mathematical reasoning, specifically inefficient sample utilization
  and inflexible processing of difficult samples. The authors propose Customized Curriculum
  Learning (CCL), a framework with two key innovations: model-adaptive difficulty
  definition that customizes curriculum datasets based on each model''s individual
  capabilities, and "Guided Prompting" that dynamically reduces sample difficulty
  through strategic hints.'
---

# Progressive Mastery: Customized Curriculum Learning with Guided Prompting for Mathematical Reasoning

## Quick Facts
- arXiv ID: 2506.04065
- Source URL: https://arxiv.org/abs/2506.04065
- Authors: Muling Wu; Qi Qian; Wenhao Liu; Xiaohua Wang; Zisu Huang; Di Liang; LI Miao; Shihan Dou; Changze Lv; Zhenghua Wang; Zhibo Xu; Lina Chen; Tianlong Li; Xiaoqing Zheng; Xuanjing Huang
- Reference count: 19
- Primary result: CCL framework improves mathematical reasoning performance through model-adaptive difficulty calibration and guided prompting, outperforming uniform training by 1.04%-13.80% across benchmarks

## Executive Summary
This paper addresses fundamental limitations in post-training large language models for mathematical reasoning, specifically inefficient sample utilization and inflexible processing of difficult samples. The authors propose Customized Curriculum Learning (CCL), a framework that customizes curriculum datasets based on each model's individual capabilities and dynamically reduces sample difficulty through strategic hints. The framework combines model-adaptive difficulty definition with "Guided Prompting" that decomposes reference solutions into steps, converting intractable generation tasks into manageable completion tasks. Comprehensive experiments on both supervised fine-tuning and reinforcement learning demonstrate that CCL significantly outperforms uniform training approaches across five mathematical reasoning benchmarks, with improvements ranging from 1.04% to 13.80% depending on model size and training paradigm.

## Method Summary
CCL addresses inefficient sample utilization in mathematical reasoning by profiling each model's capabilities on the training data to define difficulty levels, then adapting the most challenging samples through "Guided Prompting" - adding partial solution steps as hints. The framework employs a three-stage training approach (easy → medium → hard) with a "Curriculum Review" strategy that reintroduces easier samples during later stages to prevent catastrophic forgetting. For SFT, the model is trained sequentially on partitioned datasets with hyperparameters lr=5e-6/1e-5 for 1.5B/7B models. For GRPO, the framework uses group relative policy optimization with lr=3e-6, num_generations=7, and beta=0.04. Both paradigms use bf16 precision and DeepSpeed Zero-2 across 8×A100 GPUs, with difficulty determined by sampling 16 responses per question at temperature 0.7 to compute accuracy-based rankings.

## Key Results
- CCL improved performance by 1.04% and 4.96% for 1.5B and 7B models respectively under SFT settings
- CCL improved performance by 13.80% and 2.44% for 1.5B and 7B models respectively under GRPO settings
- Model-adaptive difficulty definition outperformed predefined dataset difficulty labels, with CCL consistently achieving higher accuracy across all five benchmarks (MATH500, OlympiadBench, Minerva Math, AIME24, AMC23)
- "Curriculum Review" strategy prevented catastrophic forgetting, outperforming naive curriculum approaches by 2-5 percentage points

## Why This Works (Mechanism)

### Mechanism 1: Model-Adaptive Difficulty Calibration
Predefined dataset difficulty labels are inconsistent with a specific model's actual competence, potentially misaligning the curriculum. The framework profiles the base model by sampling N responses per question (temperature 0.7) and defines difficulty as the inverse of the model's accuracy on that specific sample, ranking data by "solvability" for that specific model instance. This assumes sample difficulty is relative to the model's current policy, not an intrinsic property of the text. Evidence shows models do not consistently perform worse as predefined difficulty levels increase (e.g., performance on Level 5 > Level 4).

### Mechanism 2: Scaffolding via Guided Prompting
Discarding samples that are "too hard" wastes information, but training on them directly degrades performance; partial solutions bridge this gap. "Guided Prompting" decomposes reference solutions into steps and appends a prefix of the solution steps to the input for samples where accuracy is near zero, converting an intractable generation task into a manageable completion task. This assumes the reference solution steps represent a valid reasoning path that the model can recognize and complete if given a starting point. The transformation converts (Q, A) → [Q; P_i] where P_i is a step-prefix.

### Mechanism 3: Anti-Forgetting Staged Training
Sequential training on easy-to-hard data improves mastery, but naive staging causes catastrophic forgetting of early concepts. The framework proceeds in stages (D_1 → D_2 → D_3) with a "Curriculum Review" strategy that reintroduces a portion of easier samples from previous stages during later hard-stage training. This assumes the model requires concurrent exposure to foundational (easy) and complex (hard) tasks to consolidate reasoning skills. Evidence shows that "Curriculum Review" outperforms "Naive Curriculum" (no review), confirming the forgetting risk.

## Foundational Learning

- **Concept: Curriculum Learning**
  - Why needed here: This is the core pedagogical principle (Bengio et al.) underlying the entire CCL framework
  - Quick check question: Why might a model trained on random data struggle to converge compared to one trained on sorted data?

- **Concept: Reinforcement Learning (GRPO)**
  - Why needed here: The paper validates CCL not just on SFT but on Group Relative Policy Optimization (GRPO), requiring an understanding of policy gradients and reward models
  - Quick check question: How does GRPO estimate the baseline (advantage) differently than PPO? (Hint: It uses group mean/std rather than a critic model)

- **Concept: Catastrophic Forgetting**
  - Why needed here: A central failure mode addressed by the "Curriculum Review" ablation, where learning new hard tasks degrades performance on old easy ones
  - Quick check question: What happens to accuracy on simple addition problems if you exclusively train a model on calculus?

## Architecture Onboarding

- **Component map:** Profiler → Curriculum Builder → Adapter → Trainer
- **Critical path:** 1. Run profiling on entire training set using base model 2. Generate curriculum datasets (partitioning and hint-augmentation) 3. Execute Stage 1 training (Easy) → save checkpoint 4. Load Stage 1, execute Stage 2 (Med + Review), and so on
- **Design tradeoffs:**
  - Inference Cost vs. Curriculum Precision: The paper uses 16 responses per sample for profiling, which is computationally expensive but reduces noise in difficulty estimation
  - Hint Ratio (α): Trade-off between giving the answer away (too easy) and failing to help (too hard)
  - Static vs. Dynamic Ranking: Difficulty is defined once at the start rather than re-evaluated after every epoch
- **Failure signatures:**
  - Stage Collapse: Loss diverges in Stage 3, indicating "Guided Prompting" hints are insufficient or data is still too difficult
  - Format Drift: In RL experiments, the model drops required <answer> tags if format reward is not weighted correctly
- **First 3 experiments:**
  1. Sanity Check Profile: Profile the model on a subset. Ensure ACC_i isn't uniformly 0 or 100. Verify Level 4 vs Level 5 anomalies exist
  2. SFT Curriculum Ablation: Train 1.5B model on Uniform vs. Naive Curriculum vs. Curriculum Review. Isolate the impact of the mixing strategy
  3. Hint Effectiveness: For the "Hard" subset only, compare training with 0% hints vs. 20% hints vs. 50% hints to tune the α parameter

## Open Questions the Paper Calls Out

- Can CCL be effectively generalized to complex domains beyond mathematical reasoning, such as code generation or natural language inference? The authors see "great potential in extending the CCL framework to other domains such as logical reasoning, code generation, and natural language inference," but the current study exclusively validates on mathematical benchmarks.

- Is the CCL framework compatible with reinforcement learning algorithms other than GRPO, such as PPO? The paper notes that "combining CCL with other post-training strategies—like PPO and broader reinforcement learning techniques—remains an open direction," as it restricts RL experiments to GRPO.

- Does "Guided Prompting" introduce an over-reliance on partial solutions, impairing the model's ability to solve difficult problems independently during inference? The evaluation reports final accuracy but does not isolate the model's capability to generate initial reasoning steps for hardest samples without the hints provided during training.

## Limitations
- The framework depends critically on solution decomposition into meaningful hints, but the paper doesn't specify an automated method for this crucial step
- The computational cost of profiling (16 responses × full dataset) scales poorly for larger models or datasets
- The adaptive difficulty concept is theoretically sound but practical implementation details for step decomposition and hint generation remain underspecified

## Confidence
- **High Confidence:** The core curriculum learning mechanism (model-adaptive difficulty + staged training) and its superiority over uniform training
- **Medium Confidence:** The specific numerical improvements (1.04%, 4.96%, 13.80%, 2.44%) depend on exact implementation details that aren't fully specified
- **Low Confidence:** The scalability and generalization of the framework beyond the specific Qwen2.5-MATH models and MATH dataset used in experiments

## Next Checks
1. **Solution Decomposition Verification:** Implement and test multiple automated methods for splitting reference solutions into step prefixes (sentence splitting, token-based chunking, semantic parsing) to determine which produces the most effective hints
2. **Dynamic Difficulty Recalibration:** Modify the framework to periodically re-profile model accuracy during training (every 2-3 epochs) rather than using static initial calibration, measuring impact on final performance
3. **Cross-Dataset Generalization:** Apply CCL to a non-MATH mathematical reasoning dataset (e.g., GSM8K or MATH subset) with the same model architectures to verify the framework's generalizability beyond the original training data