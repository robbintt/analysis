---
ver: rpa2
title: 'One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy
  Optimization for Order Dispatch on Ride-Sharing Platforms'
arxiv_id: '2507.15351'
source_url: https://arxiv.org/abs/2507.15351
tags:
- grpo
- order
- time
- policy
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the order dispatch problem in ride-sharing\
  \ platforms using Autonomous Vehicles (AVs), where the goal is to efficiently assign\
  \ incoming orders to a fleet of homogeneous AVs in real-time. The authors propose\
  \ two Multi-Agent Reinforcement Learning (MARL) methods\u2014GRPO (Group Relative\
  \ Policy Optimization) and OSPO (One-Step Policy Optimization)\u2014that bypass\
  \ value function estimation, leveraging the homogeneous property of AV fleets."
---

# One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms

## Quick Facts
- **arXiv ID**: 2507.15351
- **Source URL**: https://arxiv.org/abs/2507.15351
- **Reference count**: 31
- **Primary result**: OSPO achieves lowest GPU utilization among all methods while optimizing pickup times and served orders in ride-sharing dispatch

## Executive Summary
This paper introduces two Multi-Agent Reinforcement Learning methods, GRPO and OSPO, for order dispatch in ride-sharing platforms with homogeneous Autonomous Vehicles. The key innovation is eliminating value function estimation by replacing critic networks with group-averaged rewards. GRPO uses reward-to-go normalization across agents, while OSPO further simplifies to single-step rewards. Experiments on Manhattan ride-hailing data show both methods outperform baselines, with OSPO achieving superior efficiency (3.8GB GPU memory) and performance by avoiding bounded-horizon bias.

## Method Summary
The authors model order dispatch as a multi-agent MDP with 1,000 homogeneous AVs. Each agent outputs assignment probabilities for available orders, which are converted to feasible global assignments via bipartite matching. GRPO replaces PPO's V-value baseline with group-averaged reward-to-go, eliminating critic estimation errors. OSPO simplifies further by showing optimal policies can be trained using only one-step group rewards. Both methods use PPO-style clipped objectives with KL regularization anchored to the best checkpoint. The shared MLP policy network processes [vehicle, order] state pairs and outputs assignment scores.

## Key Results
- OSPO achieves the lowest GPU utilization (3.8GB) among all compared methods
- Both GRPO and OSPO outperform existing MARL baselines in optimizing pickup times and served order count
- OSPO outperforms GRPO due to elimination of bounded-horizon bias in reward estimation
- Methods maintain high efficiency with simple MLP architecture (20K parameters vs 117K-16M in baselines)

## Why This Works (Mechanism)

### Mechanism 1
Replacing critic networks with group-averaged rewards reduces estimation bias in MARL. GRPO substitutes PPO's V-value baseline with the empirical mean of reward-to-go across homogeneous agents. Since all agents share the same policy and reward structure, the group average approximates the expected value without requiring a separate critic network. This works because agent homogeneity ensures residual terms remain small across the fleet. Break condition: If agents become heterogeneous (mixed AV/human fleets), the residual grows and group averaging no longer approximates individual V-values accurately.

### Mechanism 2
Single-step rewards suffice for policy optimization under strong homogeneity. OSPO derives that the advantage function reduces to one-step reward minus group mean when homogeneity holds. The cumulative reward distribution across agents remains stationary enough that one-step rewards provide sufficient signal. Break condition: If episode boundaries create end-of-horizon effects where agents receiving late orders get truncated reward-to-go, the bounded horizon bias re-emerges, affecting GRPO but not OSPO.

### Mechanism 3
Bipartite matching converts decentralized policy outputs into globally feasible assignments. Each agent outputs probabilities for accepting orders, and the system solves a bipartite matching optimization to maximize total assignment score under constraints. This ensures one-to-one assignment feasibility. Break condition: If order bundling (multiple orders per vehicle) is required beyond capacity-1 matching, the bipartite formulation generalizes to hypergraph matching with exponential complexity.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO) and the role of baselines**
  - Why needed here: GRPO/OSPO modify PPO's advantage function; understanding why baselines reduce variance without introducing bias is essential for evaluating whether group-mean substitution is valid.
  - Quick check question: If you replaced PPO's V(s) baseline with a constant, would the policy gradient remain unbiased? What about using the empirical mean of sampled returns?

- **Concept: Value function approximation errors in MARL**
  - Why needed here: The paper's core motivation is eliminating critic networks; you need to understand why V/Q-estimation degrades in multi-agent settings (non-stationarity, credit assignment) to judge if bypassing them is principled or just faster.
  - Quick check question: In a CTDE framework, why does adding more agents typically increase variance in centralized critic estimates, even with shared parameters?

- **Concept: Bipartite matching / Hungarian algorithm**
  - Why needed here: The action selection mechanism requires solving assignment problems at each timestep; understanding complexity (O(n³)) and constraint handling is critical for scaling analysis.
  - Quick check question: Given 1000 vehicles and 500 orders, what is the worst-case complexity of finding the optimal assignment? How would you handle the case where |orders| > |vehicles|?

## Architecture Onboarding

- **Component map**: State observations -> Shared MLP policy -> Assignment probabilities -> Bipartite matching -> Feasible assignments -> Environment execution -> Group advantage calculation -> PPO update with KL regularization -> Checkpoint comparison

- **Critical path**: 1) Environment state → individual agent observations 2) Shared policy network → assignment probabilities per (agent, order) pair 3) Bipartite matching → feasible global assignment 4) Execute assignment → observe rewards, next states 5) Compute group advantage 6) PPO-style gradient update with clipped surrogate objective 7) Checkpoint comparison → update reference policy if current policy improves

- **Design tradeoffs**: Simple MLP vs. GNN/attention architectures (20K vs 117K-16M parameters); one-step (OSPO) vs. reward-to-go (GRPO); KL regularization with best checkpoint vs. initial policy anchoring. Assumes spatial relationships captured implicitly through order features rather than explicit graph structure.

- **Failure signatures**: Homogeneity collapse (monitor standard deviation of cumulative rewards per agent); matching infeasibility (check entropy of output distributions); reference policy staleness (verify checkpoint improvement frequency).

- **First 3 experiments**: 1) Baseline sanity check: Run OSPO with random policy to establish lower bound; compare against myopic greedy assignment. 2) Homogeneity ablation: Add artificial heterogeneity (10% different reward weights) and measure performance gap. 3) Scaling stress test: Double vehicle count (2000) and order volume proportionally; measure GPU memory and performance scaling.

## Open Questions the Paper Calls Out
- **Open Question 1**: How does OSPO performance degrade in mixed-fleet environments featuring heterogeneous agents (AVs and human-driven cars)? The core principle relies on homogeneous V-value assumption.
- **Open Question 2**: Can OSPO be effectively adapted to general MARL domains with sparse rewards rather than dense, immediate rewards found in order dispatch? The method relies on one-step rewards for advantage estimation.
- **Open Question 3**: How sensitive is GRPO/OSPO training stability to the choice of KL regularization reference policy in non-stationary environments? Anchoring to historical "best" policy might restrict adaptation to new demand patterns.

## Limitations
- Only evaluated on homogeneous AV fleets in Manhattan geography, not heterogeneous vehicle types or multi-region dispatch
- Critical hyperparameters (reward weights, CLIP bounds, KL coefficient, BSC decay) are unspecified, preventing exact replication
- Theoretical analysis assumes perfect homogeneity and stationary reward distributions, not time-varying demand patterns or vehicle-specific constraints

## Confidence

- **High confidence**: Fundamental claim that bypassing critic networks reduces estimation bias in MARL for homogeneous fleets is well-supported by derivation and experiments
- **Medium confidence**: One-step reward sufficiency claim (OSPO outperforming GRPO) is demonstrated empirically but lacks theoretical guarantees for non-stationary or heterogeneous environments
- **Low confidence**: Long-term stability of KL-regularized checkpoint strategy and its effects on exploration-exploitation tradeoffs are not analyzed; 30-minute episodes don't address policy degradation over longer periods

## Next Checks

1. **Heterogeneity stress test**: Introduce controlled heterogeneity (10-20% of vehicles with different reward weights or capacity constraints) and measure performance degradation relative to baseline methods.

2. **Critic comparison ablation**: Implement a shared critic network version of GRPO and directly compare value estimation variance, training stability, and final performance across varying fleet sizes (500, 1000, 2000 vehicles).

3. **Temporal generalization test**: Train policies on one time window (e.g., weekday morning rush) and evaluate on disjoint periods (weekend evenings, special events). Measure adaptation speed when using KL-regularized checkpoint versus retraining from scratch.