---
ver: rpa2
title: Hebbian Learning with Global Direction
arxiv_id: '2601.21367'
source_url: https://arxiv.org/abs/2601.21367
tags:
- learning
- hebbian
- global
- local
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the scalability limitations of Hebbian learning
  in deep neural networks by proposing a novel framework that integrates local Hebbian
  updates with global task gradients. The method combines Oja's rule with competitive
  learning for local updates and uses the sign of the global gradient as a modulatory
  signal to guide the direction of synaptic changes.
---

# Hebbian Learning with Global Direction

## Quick Facts
- arXiv ID: 2601.21367
- Source URL: https://arxiv.org/abs/2601.21367
- Reference count: 0
- This paper proposes a framework combining local Hebbian updates with global gradient sign modulation, achieving competitive accuracy on ImageNet (within 4% of backpropagation on ResNet-50) and demonstrating scalability to extremely deep networks (1202 layers).

## Executive Summary
This paper addresses the scalability limitations of pure Hebbian learning by introducing a framework that combines local synaptic plasticity with global task signals. The method uses Oja's rule with competitive learning for local updates, while the sign of the backpropagated gradient serves as a modulatory signal to guide learning direction. This hybrid approach achieves performance competitive with standard backpropagation while maintaining the biological plausibility of local learning rules. The framework demonstrates strong scalability, maintaining effectiveness on extremely deep networks up to 1202 layers.

## Method Summary
The Global Hebbian Learning (GHL) framework integrates local Hebbian updates with global task gradients through a three-factor learning rule. During the forward pass, activations are computed and softmax-based Winner-Take-All competition produces per-neuron modulation strengths. Local Hebbian updates follow Oja's rule: Δw_ik^Hebb = u_k(x_i - y_k w_ik), where u_k is the competition weight. The global component computes the standard backpropagated gradient but retains only the sign information. The final update combines these components: ΔW^GHL = sign(G) ⊙ |ΔW^Hebb|, where ⊙ denotes element-wise multiplication. This design preserves the biological plausibility of local learning while incorporating global task information through a low-bandwidth sign signal.

## Key Results
- Achieves 73.1% Top-1 accuracy on ImageNet ResNet-50, narrowing the gap with backpropagation to within 4%
- Maintains strong performance on extremely deep networks (1202 layers) on CIFAR-10
- Outperforms existing Hebbian methods across multiple architectures and datasets in extensive ablation studies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Global sign modulation resolves the directional ambiguity of purely local Hebbian updates.
- Mechanism: The framework computes standard backpropagated gradients but discards magnitude, retaining only sign (+1/-1). This binary signal gates whether local Hebbian weight changes are potentiated or depressed, aligning synaptic plasticity with global task objectives without requiring precise error propagation.
- Core assumption: The direction of gradient descent contains sufficient information for learning when magnitude is provided by local activity.
- Evidence anchors: The abstract states "the global component introduces a sign-based signal that guides the direction of local Hebbian plasticity updates" and section 2.3 conceptualizes global guidance as sign-based information.

### Mechanism 2
- Claim: Oja's normalization term prevents unbounded weight growth inherent to naive Hebbian updates.
- Mechanism: The update rule Δw = η·y·(x - y·w) includes a decay term proportional to y²·w. This acts as an implicit weight normalization, bounding synaptic strength while preserving Hebbian correlation learning.
- Core assumption: Postsynaptic activity squared (y²) provides sufficient statistics for weight magnitude control.
- Evidence anchors: Section 2.2 describes "the second term η·y²k·wik acts as a normalization or 'forgetting' term that stabilizes the learning process."

### Mechanism 3
- Claim: Soft Winner-Take-All competition enforces feature specialization across neurons.
- Mechanism: Applying softmax across layer activations produces uk = exp(yk/τ) / Σexp(yl/τ). Neurons with higher relative activation receive larger update magnitudes, creating implicit specialization without explicit architectural constraints.
- Core assumption: Feature diversity emerges naturally from competition given sufficient initialization variance.
- Evidence anchors: Section 2.2 states "This rule ensures that only the 'winning' neurons (those with large uk values) significantly update their weights, thereby achieving feature selectivity."

## Foundational Learning

- Concept: **Three-factor learning rule**
  - Why needed here: GHL instantiates this biological framework (pre-synaptic × post-synaptic × modulator). Understanding the distinction between local and global factors is essential for debugging update behavior.
  - Quick check question: Can you identify which component of GHL corresponds to each of the three factors?

- Concept: **Gradient sign vs. magnitude**
  - Why needed here: The method deliberately discards gradient magnitude. Intuition about why directional information alone suffices (and when it doesn't) informs hyperparameter choices.
  - Quick check question: In what scenarios would discarding gradient magnitude most harm convergence?

- Concept: **Competitive learning and WTA dynamics**
  - Why needed here: The SWTA mechanism drives representation formation. Understanding how softmax temperature affects specialization helps diagnose collapsed representations.
  - Quick check question: What happens to feature diversity if all neurons have identical activations before competition?

## Architecture Onboarding

- Component map:
  Forward pass -> SWTA competition -> Local Hebbian update magnitudes -> Backward pass (gradient signs only) -> Final update (sign × magnitude)

- Critical path:
  1. Forward pass computes activations
  2. SWTA competition produces uk for each neuron
  3. Local Hebbian update magnitudes computed per weight
  4. Backward pass computes gradient signs only
  5. Final update: W ← W + η · sign(∂L/∂W) ⊙ |ΔW(Hebb)|

- Design tradeoffs:
  - **vs. SignSGD**: GHL uses dynamic (activity-dependent) magnitudes; SignSGD uses fixed step sizes. GHL avoids SignSGD's instability but requires forward-pass Hebbian computation.
  - **Architecture flexibility**: Paper claims model-agnostic design, but Table 4 shows performance varies with depth/width combinations. Assumption: architectural sensitivity is lower than prior Hebbian methods but not eliminated.
  - **Biological plausibility vs. performance**: The global gradient sign remains a non-local signal; true locality would require alternative modulatory mechanisms.

- Failure signatures:
  - **Weight explosion**: Oja term ineffective → check learning rate scaling with input magnitude
  - **Representation collapse**: All neurons learn similar features → check softmax temperature τ and initialization variance
  - **No learning despite non-zero loss**: Sign frequently flips → possible gradient conflict; consider smoothing or momentum on sign
  - **Deep network degradation**: Performance drops sharply after certain depth → may indicate gradient sign noise accumulation (paper shows stability to 1202 layers on CIFAR-10, but ImageNet results only reported to ResNet-50)

- First 3 experiments:
  1. **Sanity check on MNIST/CIFAR-10 with small MLP**: Replicate basic GHL vs. pure Hebbian comparison. Target: >10% accuracy gap confirming global modulation benefit. Monitor weight norm stability to verify Oja mechanism.
  2. **Ablation: sign-only vs. magnitude-only vs. full GHL**: Isolate contribution of each component. Reference: Table 5 shows sign-only achieves 81.01% vs. GHL's 86.41% on DeepHebb architecture (CIFAR-10).
  3. **Temperature sweep for SWTA**: Test τ ∈ {0.1, 0.5, 1.0, 2.0} on validation set. Plot feature diversity (e.g., inter-neuron correlation) vs. accuracy to identify collapse threshold.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the GHL framework be successfully extended to modern non-convolutional architectures like Transformers?
- Basis in paper: The Discussion section explicitly lists "extending GHL to modern network architectures like Transformers" as a future direction.
- Why unresolved: The paper's experiments are restricted to CNNs (VGG, ResNet), and the specific local Hebbian rules (Oja's + SWTA) were tested primarily on spatially localized convolutional layers.
- What evidence would resolve it: Successful application of GHL to Attention layers in Vision Transformers (ViT) or NLP models, achieving performance comparable to standard backpropagation.

### Open Question 2
- Question: What alternative local plasticity rules or global signals could replace the current components to further improve performance?
- Basis in paper: The authors state a future direction is "exploring other local and global learning mechanisms" beyond the specific combination of Oja's rule and sign-based gradients used here.
- Why unresolved: The current instantiation is just one possible configuration of the three-factor rule; the optimality of the sign-based gradient proxy versus other modulatory signals remains unexplored.
- What evidence would resolve it: Ablation studies substituting the global sign with other neuromodulator analogues or replacing Oja's rule with different unsupervised objectives.

### Open Question 3
- Question: Can the reliance on the backpropagated gradient for the global sign be replaced by a strictly biologically plausible feedback mechanism?
- Basis in paper: While the paper claims biological plausibility, Method 2.3 notes that the global signal is a "proxy" derived from the backpropagated gradient because "no direct analog... exists." This retains the biological issue of error propagation.
- Why unresolved: The framework currently bridges the gap by using a precise mathematical derivative (the gradient) to determine the sign, which the brain arguably cannot compute directly.
- What evidence would resolve it: Experiments showing that random feedback weights or global reward signals can serve as the modulatory factor M without significant loss of accuracy compared to the gradient-based sign.

## Limitations
- The global gradient sign remains a non-local signal incompatible with true biological implementation
- Performance depends critically on hyperparameter tuning (softmax temperature τ and learning rate η)
- Claims about biological plausibility are overstated given reliance on backpropagation for gradient sign computation

## Confidence
- **High confidence**: The core mechanism combining Oja's rule with global sign modulation is mathematically sound and well-grounded in three-factor learning theory.
- **Medium confidence**: The reported performance improvements on CIFAR-10 and ImageNet are credible given the extensive ablation studies, though exact hyperparameter values remain unspecified.
- **Low confidence**: Claims about biological plausibility are overstated since the global gradient sign remains a non-local signal incompatible with true biological implementation.

## Next Checks
1. **Hyperparameter ablation study**: Systematically vary τ ∈ {0.1, 0.5, 1.0, 2.0} and η ∈ {0.001, 0.01, 0.1} on CIFAR-10 ResNet-20, measuring both accuracy and feature diversity metrics. Plot learning curves to identify optimal ranges and collapse thresholds.

2. **Sign-only ablation comparison**: Compare GHL against SignSGD with adaptive magnitude scaling on ImageNet ResNet-50. This isolates whether the dynamic local magnitudes provide measurable benefit over fixed step sizes in the sign-only regime.

3. **Deep network stability test**: Train GHL on CIFAR-10 with architectures of increasing depth (20→56→110→1202 layers), monitoring gradient sign consistency across layers and weight norm stability. This validates the claimed scalability and identifies depth thresholds where sign noise accumulation becomes problematic.