---
ver: rpa2
title: Let's Verify Math Questions Step by Step
arxiv_id: '2505.13903'
source_url: https://arxiv.org/abs/2505.13903
tags:
- question
- zhang
- wang
- mathematical
- chen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MathQ-Verify, a five-stage pipeline to verify
  the correctness of math questions themselves, not just their answers. The pipeline
  systematically checks for contaminated instructions, linguistic errors, atomic condition
  validity, logical consistency, and condition completeness.
---

# Let's Verify Math Questions Step by Step

## Quick Facts
- arXiv ID: 2505.13903
- Source URL: https://arxiv.org/abs/2505.13903
- Authors: Chengyu Shen; Zhen Hao Wong; Runming He; Hao Liang; Meiyi Qiang; Zimo Meng; Zhengyang Zhao; Bohan Zeng; Zhengzhou Zhu; Bin Cui; Wentao Zhang
- Reference count: 40
- Primary result: Introduces MathQ-Verify, a five-stage pipeline that improves F1 score by up to 25 percentage points over direct verification baselines for math question validation.

## Executive Summary
This paper introduces MathQ-Verify, a five-stage pipeline to verify the correctness of math questions themselves, not just their answers. The pipeline systematically checks for contaminated instructions, linguistic errors, atomic condition validity, logical consistency, and condition completeness. A new dataset, ValiMath, containing 2,147 math problems with diverse error types, is constructed and manually validated. Experiments show MathQ-Verify improves F1 score by up to 25 percentage points over direct verification baselines. Using a lightweight model voting scheme, it achieves approximately 90% precision and 63% recall, offering a scalable solution for filtering unreliable math QA data.

## Method Summary
MathQ-Verify implements a five-stage pipeline: (1) Contaminated Instruction Detection, (2) Linguistic Error Detection, (3) Atomic Condition Error Detection, (4) Cross-condition Conflict Detection, and (5) Condition Completeness Validation. Each question is processed sequentially through these stages, with validity requiring passage through all steps. The system uses LLMs (Qwen2.5-7B/72B, Llama-3.1-8B/70B, GPT-4o, DeepSeek-R1) with configurable settings. A voting mechanism aggregates outputs from multiple models to achieve tunable precision-recall trade-offs. The ValiMath dataset provides step-wise labeled examples for training and evaluation.

## Key Results
- MathQ-Verify improves F1 score by up to 25 percentage points over direct verification baselines
- Achieves approximately 90% precision and 63% recall using multi-model voting
- Stage-wise accuracy shows S1-S2 consistently above 90%, while S4-S5 show lower accuracy due to complexity
- Voting configuration (n,k) provides tunable precision-recall trade-offs, with (3,3) achieving 91.42% precision at 61.51% recall

## Why This Works (Mechanism)

### Mechanism 1: Sequential Decomposition with Early-Exit Filtering
Decomposing question verification into five ordered stages improves detection accuracy over end-to-end verification. Each question is first stripped of contaminated instructions, then checked for linguistic errors, then decomposed into atomic conditions P(qi) and goals G(qi), which are individually validated, checked for cross-condition contradictions, and finally assessed for completeness. A question must pass all stages to be marked valid. If early-stage errors (S1/S2) dominate, later stages provide marginal gains. Ablation shows removing Steps 1-2 causes the largest F1 drop (>3%).

### Mechanism 2: Atomic Condition Formalization Enables Principled Consistency Checks
Explicitly extracting atomic conditions allows formal verification of mathematical validity and inter-condition consistency. Questions are parsed into atomic conditions (e.g., "x ∈ Z", "area > 0"). Each condition is checked against domain definitions; subsets of conditions are checked for contradictions (e.g., "length = -2 cm" violates definition of length). For questions with ambiguous or implicit conditions, formalization may introduce false positives. The paper permits "commonly understood concepts or heuristic steps" with some flexibility.

### Mechanism 3: Multi-Model Voting Trades Recall for Precision
Majority voting across n models with threshold k provides tunable precision-recall trade-offs, with (n,n) configurations maximizing precision. Final decision requires ≥k of n models to agree on validity. Higher k yields more conservative predictions (fewer false positives, more false negatives). If model errors are correlated (e.g., all models share a systematic bias in detecting certain error types), voting provides minimal benefit. The paper uses heterogeneous models (reasoning vs. non-reasoning) to mitigate this.

## Foundational Learning

- Concept: **Atomic condition extraction**
  - Why needed here: The pipeline requires decomposing natural language questions into structured mathematical statements before validation. Without this, contradiction detection and completeness checks would operate on unstructured text.
  - Quick check question: Given "A triangle has sides 3, 4, and 10. Find its area," can you identify the atomic condition that violates triangle inequality?

- Concept: **Formal vs. semantic validity**
  - Why needed here: Step 3 checks formal validity (e.g., area cannot be negative), while Steps 4-5 check semantic consistency (conditions don't contradict, goals are derivable). Conflating these leads to ambiguous error categorization.
  - Quick check question: Is "Find the largest prime factor of 1" formally invalid (definition of prime) or semantically invalid (no solution exists)?

- Concept: **Precision-recall trade-offs in filtering pipelines**
  - Why needed here: The voting mechanism (Section 3.3) explicitly tunes this trade-off. Understanding when to prioritize precision (e.g., training data curation) vs. recall (e.g., benchmark coverage) is critical for deployment.
  - Quick check question: If your downstream task tolerates 10% noise but cannot miss valid questions, which (n,k) configuration should you choose?

## Architecture Onboarding

- Component map:
  Input Question → [S1: Contaminated Instruction Detection] → [S2: Linguistic Error Detection] → [S3: Atomic Condition Extraction + Validation] → [S4: Cross-Condition Conflict Detection] → [S5: Completeness Check] → Valid/Invalid Label
  Optional: Multi-Model Voting Layer aggregates outputs from n independent pipelines.

- Critical path: S1 → S2 must complete before S3-S5, which can be partially parallelized. The paper uses Qwen-2.5-7B-Instruct for S2; other stages use the primary model.

- Design tradeoffs:
  - Single model vs. voting: Single model is faster; voting adds 2-5x inference cost for 5-10% precision gains
  - Strict vs. lenient formalization: Allowing "magic steps" improves recall but may miss subtle errors
  - Early exit vs. full pipeline: Exiting at S1/S2 saves compute but forfeits granular error typing

- Failure signatures:
  - High invalid output count suggests instruction-following issues—math-specialized models like Qwen2.5-Math-7B show 3-5x more invalid outputs than general models
  - S4/S5 accuracy drops in reasoning models indicate overthinking on incomplete questions—models may attempt to solve rather than detect missing conditions

- First 3 experiments:
  1. Reproduce baseline vs. pipeline on MathClean-GSM8K using Qwen2.5-7B. Expected: ~6% F1 improvement, reduced invalid outputs
  2. Ablate each stage on ValiMath subset (n=500). Expected: S1/S2 removal causes largest drop; S5 removal may increase recall at precision cost
  3. Test (2,2) voting configuration using DeepSeek-R1-Distill-Qwen-7B + Qwen2.5-72B. Expected: ~89% precision, ~63% recall per Table 3 pattern

## Open Questions the Paper Calls Out
None

## Limitations
- The atomic condition extraction and formalization process relies heavily on LLM capabilities that are not independently validated
- The multi-stage design assumes error localization, but complex questions with multiple overlapping issues may still challenge the system
- The ValiMath dataset, while expert-annotated, is constructed through filtering rather than exhaustive manual creation, which may introduce selection biases

## Confidence
- High confidence: precision-recall trade-off achieved through voting mechanisms and staged pipeline design's ability to improve over direct baselines
- Medium confidence: atomic condition formalization's effectiveness across diverse mathematical domains, as validation is primarily internal to the pipeline
- Low confidence: generalizability of contradiction detection heuristics without explicit mathematical reasoning rules

## Next Checks
1. Conduct ablation studies removing individual stages to quantify their marginal contributions and identify failure modes
2. Test the pipeline on out-of-distribution math problems (e.g., competition mathematics) to assess robustness beyond the ValiMath domain
3. Implement controlled experiments comparing atomic condition extraction accuracy against ground truth formalizations for a subset of questions