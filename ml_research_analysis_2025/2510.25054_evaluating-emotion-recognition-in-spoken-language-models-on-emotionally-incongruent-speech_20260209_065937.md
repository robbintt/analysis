---
ver: rpa2
title: Evaluating Emotion Recognition in Spoken Language Models on Emotionally Incongruent
  Speech
arxiv_id: '2510.25054'
source_url: https://arxiv.org/abs/2510.25054
tags:
- speech
- emotion
- semantic
- audio
- slms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates how spoken language models (SLMs) perform
  emotion recognition when semantic content and speech expressiveness are incongruent.
  The authors create a dataset of synthetic speech samples where emotional cues in
  the words conflict with the emotion in the voice, covering explicit, implicit, and
  neutral semantic conditions.
---

# Evaluating Emotion Recognition in Spoken Language Models on Emotionally Incongruent Speech

## Quick Facts
- arXiv ID: 2510.25054
- Source URL: https://arxiv.org/abs/2510.25054
- Reference count: 0
- Primary result: Spoken language models overwhelmingly rely on semantic content rather than prosody for emotion recognition when semantic and acoustic cues are incongruent

## Executive Summary
This paper evaluates how spoken language models (SLMs) perform emotion recognition when semantic content and speech expressiveness are incongruent. The authors create a dataset of synthetic speech samples where emotional cues in the words conflict with the emotion in the voice, covering explicit, implicit, and neutral semantic conditions. Four SLMs (Audio Flamingo-3, DeSTA2, Qwen2Audio, SALMONN) are tested alongside a baseline acoustic SER model and human listeners. Results show that SLMs overwhelmingly rely on semantic content rather than prosody to classify emotions, with accuracy on target (audio) emotions near chance (25%) but much higher on proxy (semantic) emotions, especially when emotions are explicitly stated. In contrast, the SER model focuses on prosody and performs better on audio emotions. This indicates that SLMs prioritize textual semantics over acoustic signals, highlighting a key limitation in their audio understanding capabilities. The Emotionally Incongruent Synthetic Speech (EMIS) dataset is released for further research.

## Method Summary
The study evaluates SLMs on a synthetic dataset of 1,248 emotionally incongruent speech samples. Four emotion-rich sentences are generated per emotion category (angry, happy, sad, neutral) with explicit, implicit, and neutral semantic content, then synthesized using three TTS systems (StyleTTS2, CosyVoice2, F5-TTS) with emotional references from the ESD dataset. Four SLMs and a baseline SER model are tested with a prosody-focused prompt. Performance is measured via target accuracy (audio emotion), proxy accuracy (semantic emotion), and chi-squared tests with Cramér's V effect sizes. Human evaluation provides a behavioral baseline.

## Key Results
- SLMs achieve near-chance (25%) accuracy on target emotions but high accuracy (>80%) on proxy emotions in explicit semantic conditions
- The SER model performs better on target emotions (47-53%) and worse on proxy emotions (1-33%), validating prosody recovery is possible
- Chi-squared tests show Cramér's V = 0.08 for target emotions vs. 0.65 for proxy emotions, indicating strong semantic dominance
- SALMONN shows the lowest semantic bias (21-30% proxy accuracy in implicit/neutral conditions) while Audio Flamingo-3 shows the highest (66-92%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SLMs exhibit text-modality dominance, prioritizing semantic content over acoustic-prosodic cues when classifying emotions.
- Mechanism: The LLM backbone in SLMs processes tokenized representations where text embeddings (rich from pretraining) dominate over compressed audio projections from the speech encoder. When both modalities provide signals, the model defaults to the higher-capacity semantic pathway.
- Core assumption: The dominance pattern stems from representation imbalance rather than architectural design intent.
- Evidence anchors:
  - [abstract] "text-related representations largely dominate over acoustic representations"
  - [section 4] Chi-squared tests show Cramér's V = 0.08 for target (audio) vs. V = 0.65 for proxy (semantic) emotions
  - [corpus] Chi et al. (referenced in Section 2) found models "predominantly rely on semantic cues when text is present"
- Break condition: If audio encoders were trained with comparable parameter scaling and pretraining data to text backbones, dominance might shift; alternatively, explicit modality-balancing losses could reduce this bias.

### Mechanism 2
- Claim: Emotionally incongruent speech serves as a diagnostic probe to isolate modal contributions by forcing competition between channels.
- Mechanism: Zero-shot expressive TTS systems transfer prosodic patterns from emotional reference recordings to arbitrary text, creating controlled misalignment. When semantic content says "happy" but voice expresses sadness, model predictions reveal which signal drives decisions.
- Core assumption: TTS-generated prosody faithfully reproduces target emotions recognizable by both humans and acoustic models.
- Evidence anchors:
  - [abstract] "dataset of emotionally incongruent speech samples, a condition under which the semantic content...conveys one emotion while speech expressiveness conveys another"
  - [section 3.1] Three TTS systems generate 416 samples each (1,248 total), using ESD references ~32 seconds long
  - [corpus] Weak direct corpus support; neighboring papers focus on benchmark construction rather than incongruence methods
- Break condition: If TTS prosody quality degrades (e.g., emotional style transfer fails), proxy-target confounds emerge. Human validation (62-70% accuracy on ground truth vs. ~58% on synthetic) partially mitigates this concern.

### Mechanism 3
- Claim: Acoustic SER models provide a prosody-centric baseline that validates SLM behavior represents genuine modality preference, not dataset artifact.
- Mechanism: SER models process only spectral/prosodic features without linguistic semantics. Their higher target accuracy (~47-53%) and lower proxy accuracy (~1-33%) confirms audio emotions are recoverable; SLMs' failure to match this indicates architectural limitation rather than data defect.
- Core assumption: Finetuning SER on ESD subset generalizes to synthetic speech distribution.
- Evidence anchors:
  - [abstract] "the SER model focuses on prosody and performs better on audio emotions"
  - [section 4, Table 1] Baseline SER achieves 47-53% target accuracy vs. SLMs' 21-41% across conditions
  - [corpus] emotion2vec (referenced as [9]) provides the SER architecture foundation
- Break condition: If SER overfits to ESD-specific acoustic patterns absent in TTS output, the comparison becomes invalid. Cross-dataset SER evaluation would strengthen claims.

## Foundational Learning

- Concept: **Modality incongruence testing**
  - Why needed here: Evaluating multimodal models on aligned data (congruent speech) masks modality preferences; only conflict reveals true integration or dominance.
  - Quick check question: If a model achieves 90% accuracy on congruent emotion classification, can you conclude it "understands" prosody?

- Concept: **Effect size vs. statistical significance**
  - Why needed here: Chi-squared tests showed significant associations (p < 0.01) for both modalities, but Cramér's V revealed the practical difference (0.08 vs. 0.65).
  - Quick check question: A p-value < 0.01 guarantees meaningful model behavior change—true or false?

- Concept: **Zero-shot TTS emotional style transfer**
  - Why needed here: Generating controlled incongruent samples requires decoupling text from prosody; understanding TTS limitations prevents misattributing model behavior to data artifacts.
  - Quick check question: Why concatenate 7 utterances for reference audio rather than using a single short clip?

## Architecture Onboarding

- Component map:
  Speech Encoder -> Modality Adapter -> LLM Backbone -> TTS Pipeline
  Converts raw audio to compact representations -> Projects audio tokens into LLM embedding space -> Instruction-following text model that generates predictions -> GPT-4.5 (sentence generation) → ESD references → StyleTTS2/CosyVoice2/F5-TTS (synthesis)

- Critical path:
  1. Generate emotion-rich sentences with explicit/implicit/neutral semantics
  2. Synthesize 4 variants per sentence (one per emotion) using TTS + reference audio
  3. Validate synthetic quality via SER + human evaluation
  4. Prompt SLMs with prosody-focused instruction ("Using tone of voice only...")
  5. Compare predictions against target (audio) and proxy (semantic) labels

- Design tradeoffs:
  - Single prompt across models ensures comparability but may disadvantage models with different instruction sensitivities
  - Three TTS systems increase generalizability but introduce variance in prosody quality (human accuracy: 39-62%)
  - Four emotion classes limit granularity but enable clear 25% chance baseline

- Failure signatures:
  - Target accuracy ≈ 25% with proxy accuracy > 80% indicates text dominance (observed in all SLMs for explicit condition)
  - Systematic prediction bias toward "angry/happy" over "sad" may reflect prosodic feature salience or training distribution
  - Neutral semantic condition shows improved target accuracy for some models (DeSTA2: 29-38%), suggesting acoustic pathway activates when semantic signal weakens

- First 3 experiments:
  1. **Prompt ablation**: Remove prosody-focused instruction; test if explicit guidance reduces (or paradoxically increases) text dominance by priming semantic attention.
  2. **Encoder scaling**: Replace audio encoder with larger pretrained model (e.g., Whisper-large-v3); measure whether improved acoustic representations shift target accuracy.
  3. **Modality-balancing fine-tuning**: Train SLM on incongruent samples with loss weighted toward audio emotion; evaluate if intervention transfers to natural (non-synthetic) incongruent speech.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can training or fine-tuning strategies be developed to help SLMs better balance acoustic and semantic information rather than defaulting to text-based reasoning?
- Basis in paper: [inferred] The authors conclude that "models tend to over-rely on information present in textual semantics" and note this has "major implications," but propose no remediation approaches.
- Why unresolved: The paper identifies the problem without exploring solutions such as balanced multi-modal training objectives, data augmentation with incongruent samples, or curriculum learning strategies.
- What evidence would resolve it: Experiments demonstrating improved target emotion accuracy on incongruent samples after specific training interventions, while maintaining performance on standard congruent benchmarks.

### Open Question 2
- Question: Do these findings generalize to naturally occurring emotional incongruence in spontaneous human speech, beyond synthetic TTS-generated samples?
- Basis in paper: [inferred] The EMIS dataset uses synthetic speech from TTS systems, which may not fully capture natural emotional incongruence found in genuine sarcasm or spontaneous irony.
- Why unresolved: Validating on synthetic speech enabled controlled experiments but leaves open whether real-world emotional incongruence exhibits different acoustic-semantic patterns that SLMs process differently.
- What evidence would resolve it: Evaluation on datasets of naturally occurring incongruent speech (e.g., sarcastic speech corpora) showing whether similar semantic bias patterns emerge.

### Open Question 3
- Question: What architectural or training characteristics cause the observed differences in semantic bias across SLMs (e.g., SALMONN shows lower proxy accuracy than Audio Flamingo-3)?
- Basis in paper: [inferred] Table 1 shows substantial variation across models—SALMONN achieves 21-30% proxy accuracy in implicit/neutral conditions while Audio Flamingo-3 reaches 66-92%—but the paper does not analyze the causes.
- Why unresolved: Related work describes different training strategies (domain-agnostic captions for DeSTA2, three-stage alignment for Qwen2-Audio), but no systematic comparison links these to semantic bias levels.
- What evidence would resolve it: Ablation studies isolating training data composition, audio encoder choices, or fusion mechanisms to identify which factors reduce semantic dominance.

### Open Question 4
- Question: How do SLMs perform on detecting complex paralinguistic phenomena like irony and sarcasm where semantic-prosodic incongruence is inherent rather than artificially induced?
- Basis in paper: [explicit] The conclusion states this limitation affects "applications that depend on nuanced interpretation of human communication, such as detecting irony, sarcasm, or emotional subtleties."
- Why unresolved: The paper tests artificially created incongruence with basic emotions but does not evaluate whether the identified semantic bias extends to these ecologically valid and practically important scenarios.
- What evidence would resolve it: Benchmarking SLMs on sarcasm/irony detection datasets containing natural semantic-acoustic conflicts, comparing against the baseline SER and human performance patterns observed in this study.

## Limitations

- The findings rely on synthetic TTS-generated speech rather than naturally occurring emotionally incongruent speech, raising questions about ecological validity
- The study uses only four basic emotion categories, limiting generalizability to more nuanced emotional expressions
- The assumed clean separation between prosodic and semantic channels may not hold in natural speech where these modalities often co-vary

## Confidence

- **High confidence**: The observation that SLMs prioritize semantic content over acoustic prosody is robust, supported by clear statistical evidence (Cramér's V = 0.65 for proxy vs. 0.08 for target emotions) and replicated across four different SLMs
- **Medium confidence**: The interpretation that this behavior stems from architectural bias rather than training data limitations is reasonable but not definitively proven
- **Low confidence**: The generalizability of these findings to natural emotionally incongruent speech is uncertain, as the synthetic generation process may introduce artifacts not present in real-world scenarios

## Next Checks

1. **Natural Incongruence Validation**: Test SLMs on naturally occurring emotionally incongruent speech (e.g., sarcastic utterances, code-switching emotional contexts) to verify if the text dominance pattern persists outside synthetic conditions

2. **Cross-Modal Fine-Tuning**: Implement targeted fine-tuning on incongruent samples with modality-balancing loss weights, then evaluate whether improved performance transfers to congruent speech emotion recognition tasks

3. **Acoustic Encoder Ablation**: Systematically replace SLMs' audio encoders with progressively larger architectures (e.g., Whisper-tiny → Whisper-base → Whisper-large) to quantify the relationship between encoder capacity and target emotion accuracy