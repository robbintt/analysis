---
ver: rpa2
title: Chart Question Answering from Real-World Analytical Narratives
arxiv_id: '2507.01627'
source_url: https://arxiv.org/abs/2507.01627
tags:
- visualization
- question
- dataset
- data
- find
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new dataset for chart question answering
  (CQA) constructed from student-authored visualization notebooks. The dataset features
  real-world, multi-view charts paired with natural language questions grounded in
  analytical narratives.
---

# Chart Question Answering from Real-World Analytical Narratives

## Quick Facts
- arXiv ID: 2507.01627
- Source URL: https://arxiv.org/abs/2507.01627
- Reference count: 17
- Primary result: New CQA dataset from real analytical narratives achieves only 69.3% accuracy with GPT-4.1, highlighting challenges of authentic reasoning tasks

## Executive Summary
This paper introduces a new dataset for chart question answering constructed from student-authored visualization notebooks. Unlike existing benchmarks that rely on scraped images or synthetic data, this dataset grounds questions in authentic analytical narratives paired with multi-view charts. The methodology captures real-world reasoning workflows, including interactive visualizations, through literate notebooks. State-of-the-art multimodal large language models struggle with this more ecologically valid benchmark, achieving only 69.3% accuracy, which reveals significant limitations in their ability to reason about complex, real-world visualizations.

## Method Summary
The dataset is constructed from 22 student-authored literate visualization notebooks containing narrative, code, and interactive visualizations. Analytical narratives are segmented into chunks and used to generate QA pairs via an LLM prompted with task taxonomy descriptions. Interactive views are captured statically by enumerating discrete controls. Questions are classified into eight VLAT task categories and validated through expert review. The final dataset contains 205 validated QA pairs over 103 visualization images, with questions requiring reasoning across multiple views and charts. Models are evaluated on multiple-choice accuracy across task categories.

## Key Results
- GPT-4.1 achieves 69.3% accuracy on the dataset, significantly lower than on existing CQA benchmarks
- 36.6% of questions require reasoning across multiple visualization views or images
- Task breakdown shows high performance on "Retrieve Value" (81.9%) but low performance on "Make Comparisons" (46.2%) and "Find Anomalies" (37.9%)
- The dataset's questions reflect authentic analytical workflows grounded in narrative context

## Why This Works (Mechanism)

### Mechanism 1
Grounding questions in authentic analytical narratives produces more challenging and ecologically valid CQA benchmarks. Unlike benchmarks generating questions from isolated charts, this approach leverages literate visualization notebooks where narrative, code, and visuals are intrinsically linked. The narrative context surfaces reasoning steps that are typically implicit in standalone charts, capturing authentic analytical workflows.

### Mechanism 2
Inclusion of multi-view and interactive visualizations increases reasoning complexity and exposes MLLM limitations absent in single-chart benchmarks. The dataset captures multiple static views from interactive visualizations and includes questions requiring cross-chart reasoning, forcing models to integrate information across views as in dashboard analysis.

### Mechanism 3
Semi-automatic question generation from narrative context yields diverse, grounded questions aligned with human analytical tasks. An LLM is prompted with narrative segments and task taxonomy descriptions to generate QA pairs, quotes, and classifications. This language-first approach avoids error-prone chart parsing and leverages narrative richness.

## Foundational Learning

- **Literate Visualization Notebooks**: Source material combining narrative, code, datasets, and visualizations. Why needed: Understanding their structure is crucial to grasp how the dataset is constructed and why questions are grounded. Quick check: How does a literate visualization notebook differ from a standalone chart image or a dashboard screenshot?

- **Analytical Task Taxonomy (VLAT)**: Eight categories classifying question types (e.g., Retrieve Value, Find Extremum). Why needed: Knowing these helps interpret model performance breakdowns and design targeted improvements. Quick check: What distinguishes "Make Comparisons" from "Find Extremum" in the context of chart-based reasoning?

- **Ecological Validity in Benchmark Design**: The dataset better reflects real-world analytical workflows. Why needed: Understanding this concept is key to evaluating the dataset's value proposition compared to synthetic benchmarks. Quick check: Why might a question generated from an analytical narrative be more "ecologically valid" than one templated from chart data alone?

## Architecture Onboarding

- **Component map**: Notebook Selection → Narrative & Visualization Extraction → LLM QA Generation → Human Validation → Dataset Finalization → Model Evaluation
- **Critical path**: From notebook selection through human validation to final dataset creation and model evaluation
- **Design tradeoffs**: Language-first generation avoids chart parsing errors but risks unanswerable questions; static interactive capture is broader but incomplete; rigorous validation limits dataset size; student authorship provides ecological validity but may not generalize to professional contexts
- **Failure signatures**: Questions marked "Cannot be determined" due to missing interactive context; low model performance on interpretive tasks; task distribution imbalance
- **First 3 experiments**: (1) Baseline evaluation of GPT-4.1 and Qwen2.5-VL models to replicate findings; (2) Ablation on multi-view questions to quantify difficulty added by cross-view reasoning; (3) Error analysis on "Cannot be determined" cases to reveal calibration issues

## Open Questions the Paper Calls Out

- How would performance gaps change if the dataset were curated for balanced task distribution across all eight analytical categories?
- To what extent does LLM-generated question reliance introduce artifacts allowing models to "game" tasks using linguistic priors rather than visual reasoning?
- How can static screenshot capture for interactive visualizations be augmented to preserve tooltip-dependent reasoning task data?

## Limitations
- Task distribution is imbalanced, heavily favoring "Retrieve Value" and "Find Extremum" tasks
- Static capture of interactive visualizations may omit critical information accessible only through interaction (e.g., tooltips)
- Dataset size is limited due to rigorous human validation, affecting statistical power for rare task categories

## Confidence

- **High confidence**: Dataset construction methodology is clearly specified and reproducible; 69.3% accuracy finding is robust with clear evaluation protocol
- **Medium confidence**: Ecological validity claims rely on untested assumptions about student notebook representativeness; static interactive captures may be insufficient
- **Medium confidence**: Task taxonomy application is methodologically sound but small sample sizes per task limit reliability of per-task performance breakdowns

## Next Checks

1. Conduct a survey of visualization researchers and practitioners to assess whether dataset questions align with real-world analytical reasoning they encounter
2. Implement an interactive chart capture method to compare completeness and difficulty of questions from interactive vs. static views
3. Perform targeted study on "Cannot be determined" questions where models select incorrect answers, analyzing whether this reflects true model limitations or gaps in static visualization capture