---
ver: rpa2
title: 'Veri-R1: Toward Precise and Faithful Claim Verification via Online Reinforcement
  Learning'
arxiv_id: '2510.01932'
source_url: https://arxiv.org/abs/2510.01932
tags:
- claim
- evidence
- arxiv
- verification
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Veri-R1, an online reinforcement learning
  framework for claim verification that iteratively retrieves evidence and reasons
  before producing a final judgment. Unlike offline methods that assume provided evidence,
  Veri-R1 interacts dynamically with a search engine, enabling more realistic verification.
---

# Veri-R1: Toward Precise and Faithful Claim Verification via Online Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.01932
- Source URL: https://arxiv.org/abs/2510.01932
- Reference count: 19
- Primary result: Online RL framework Veri-R1 achieves up to 30% gains in joint accuracy and 150% improvement in evidence scores over offline RL and supervised baselines.

## Executive Summary
This paper introduces Veri-R1, an online reinforcement learning framework for claim verification that iteratively retrieves evidence and reasons before producing a final judgment. Unlike offline methods that assume provided evidence, Veri-R1 interacts dynamically with a search engine, enabling more realistic verification. A task-specific reward function jointly optimizes label accuracy, evidence precision, and format compliance, while a validity weight prevents shortcut reasoning. Experiments show up to 30% gains in joint accuracy and 150% improvement in evidence scores, with online RL outperforming offline RL and supervised fine-tuning, and even surpassing larger-scale baselines. Ablation studies confirm the impact of reward components. The work demonstrates the effectiveness of online RL for precise, faithful claim verification.

## Method Summary
Veri-R1 addresses online claim verification by integrating iterative retrieval with LLM reasoning in an online RL loop. The model interacts with a search engine to retrieve evidence in up to three turns, then reasons to produce a final judgment (SUPPORT/REFUTE/NEI). Training uses the GRPO algorithm built on Verl and Search-R1 frameworks, with a reward function that combines label accuracy, evidence precision (IoU), format compliance, and a validity weight that prevents shortcut reasoning. The framework is trained on filtered FEVEROUS and EX-FEVER data and evaluated on multiple datasets, demonstrating superior performance over offline RL and supervised baselines.

## Key Results
- Up to 30% improvement in joint accuracy (label + evidence) over offline RL and supervised fine-tuning.
- 150% improvement in evidence score (IoU between predicted and gold evidence).
- Online RL consistently outperforms offline RL and SFT across multiple datasets.
- Larger models (7B+) show higher verification accuracy but lower label accuracy and NEI recall due to overconfidence.

## Why This Works (Mechanism)
Veri-R1's effectiveness stems from its online RL approach, which allows the model to dynamically interact with a search engine and iteratively refine its evidence retrieval and reasoning. The validity weight in the reward function prevents shortcut reasoning by penalizing cases where the model gets the correct label without proper evidence coverage. This ensures both precision and faithfulness in the verification process.

## Foundational Learning
- **Online Reinforcement Learning**: Needed to enable dynamic interaction with search engines during training, reflecting real-world verification scenarios. Quick check: Model improves performance over time through interaction-based feedback.
- **Reward Shaping**: Critical for guiding the model toward both accurate labels and precise evidence. Quick check: Reward components (label, evidence, format, validity) jointly optimize verification quality.
- **Iterative Retrieval**: Allows the model to refine evidence collection over multiple search turns. Quick check: Performance improves with additional search turns up to a limit (3 turns).

## Architecture Onboarding
- **Component Map**: Claim -> LLM (reasoning) -> Search Engine (retrieval) -> Evidence Pool -> LLM (final judgment) -> Reward Function
- **Critical Path**: Claim → Iterative retrieval (up to 3 turns) → Evidence aggregation → Final judgment (SUPPORT/REFUTE/NEI)
- **Design Tradeoffs**: Online RL enables realistic verification but requires careful reward shaping to prevent shortcut reasoning; offline RL is simpler but less effective.
- **Failure Signatures**: Shortcut reasoning (correct label without proper evidence), format violations, poor NEI recall in larger models.
- **First Experiments**: 1) Train with and without validity weight to observe shortcut reasoning. 2) Compare online vs. offline RL performance. 3) Test evidence precision with different IoU thresholds.

## Open Questions the Paper Calls Out
- Does Veri-R1's performance generalize to real-world verification scenarios involving live, dynamic web corpora rather than fixed, local datasets?
- Can the observed overconfidence in larger models (leading to poor "Not Enough Info" recall) be mitigated through explicit uncertainty-aware reward shaping?
- Would integrating explicit falsification strategies into the search query generation improve performance on REFUTE claims?

## Limitations
- Performance on REFUTE claims remains challenging due to the need for specific counter-evidence retrieval.
- Larger models show overconfidence, avoiding NEI answers and reducing recall for this label.
- The framework currently uses a static offline corpus, not reflecting the volatility and noise of live web data.

## Confidence
- **Online RL effectiveness**: High confidence (consistent gains across datasets and ablations)
- **Joint accuracy improvements**: Medium confidence (exact IoU computation may vary)
- **Validity weight importance**: High confidence (clear ablation results)

## Next Checks
1. Reconstruct the training corpus using the same FEVEROUS/EX-FEVER split and simulate the GPT-4o filtering offline; verify ~70% retention and compare evidence distribution before/after filtering.
2. Implement the exact reward function (including IoU calculation and validity weight schedule) and run a small-scale pilot to confirm format reward converges to 1.0 within first few epochs.
3. Test the online RL loop with a held-out sample: ensure hit rate monitoring enforces validity weight correctly and that evidence cover rate does not drop after ~60 training steps.