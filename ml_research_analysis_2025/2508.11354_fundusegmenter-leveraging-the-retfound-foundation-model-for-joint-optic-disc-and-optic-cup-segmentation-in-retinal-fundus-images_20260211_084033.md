---
ver: rpa2
title: 'FunduSegmenter: Leveraging the RETFound Foundation Model for Joint Optic Disc
  and Optic Cup Segmentation in Retinal Fundus Images'
arxiv_id: '2508.11354'
source_url: https://arxiv.org/abs/2508.11354
tags:
- segmentation
- domain
- images
- retfound
- best
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces FunduSegmenter, the first adaptation of the
  RETFound foundation model for joint optic disc (OD) and optic cup (OC) segmentation
  in retinal fundus images. The model integrates novel modules including a Pre-adapter,
  Decoder, Post-adapter, skip connections with CBAM, and a ViT block adapter.
---

# FunduSegmenter: Leveraging the RETFound Foundation Model for Joint Optic Disc and Optic Cup Segmentation in Retinal Fundus Images

## Quick Facts
- arXiv ID: 2508.11354
- Source URL: https://arxiv.org/abs/2508.11354
- Reference count: 40
- Primary result: Achieves 90.51% DSC on REFUGE, outperforming baselines including nnU-Net (82.91%), DUNet (89.17%), and TransUNet (87.91%)

## Executive Summary
FunduSegmenter introduces a novel approach to joint optic disc and optic cup segmentation in retinal fundus images by leveraging the RETFound foundation model through parameter-efficient adaptation. The method integrates four key modules—Pre-adapter, ViT block adapter, skip connections with CBAM, and Post-adapter—to adapt the frozen RETFound encoder for pixel-level segmentation tasks. This architecture achieves state-of-the-art performance on internal validation while demonstrating strong generalization capabilities across multiple datasets and conditions.

## Method Summary
FunduSegmenter adapts the frozen RETFound ViT-large encoder through four novel modules: a Pre-adapter CNN that handles variable input sizes, ViT block adapters (bottleneck: 1024→128→1024) inserted at each transformer block, skip connections from intermediate layers with CBAM attention, and a Post-adapter with progressive upsampling. The model uses a Segmenter decoder with two transformer blocks and is trained with Dice and Cross-Entropy loss using AdamW optimizer. Total tunable parameters are 35.57M, with the Pre-adapter contributing only 0.04M parameters. The approach enables segmentation without fine-tuning the foundation model weights while maintaining strong performance.

## Key Results
- Internal verification on REFUGE: 90.51% average DSC (OD: 92.88%, OC: 88.14%), significantly outperforming baselines
- External verification: Maintains strong performance on Drishti-GS (84.59% DSC) and GoDARTS (93.20% DSC) datasets
- Domain generalization: Performs well on GoDARTS even without OD center cropping (93.20% DSC), improving from 92.75% with cropping
- OC segmentation: Results trail specialized methods like TVConv, identifying a key area for future improvement

## Why This Works (Mechanism)

### Mechanism 1
Frozen foundation model encoders can provide transferable spatial representations for medical image segmentation when paired with appropriate adaptation modules. RETFound's ViT-large encoder, pre-trained via masked autoencoding on 904,170 unlabeled fundus images, learns general retinal representations that remain useful when frozen. The ViT block adapter (bottleneck architecture: 1024→128→1024 channels) placed between layer normalization and multi-head attention enables task-specific adaptation without modifying the frozen weights.

### Mechanism 2
Progressive upsampling with learned convolutions stabilizes boundary delineation for structures with ambiguous edges. The Post-adapter replaces Segmenter's direct bilinear upsampling with 4 cascaded modules (each: basic CNN block + 2× bicubic upsampling), progressively reconstructing 14×14 mask maps to 224×224. This allows learned refinement at each scale rather than one-shot interpolation.

### Mechanism 3
Multi-scale skip connections from intermediate transformer layers improve small structure segmentation. Feature maps from RETFound layers 6, 12, 18, 24 are extracted, projected to 64 channels, upsampled to different scales, processed through CBAM attention, and concatenated with Post-adapter features. This injects multi-resolution spatial information lost in the final encoder output.

## Foundational Learning

- **Self-Supervised Learning with Masked Autoencoders (MAE)**: RETFound's pre-training creates transferable representations by masking 75% of image patches and forcing reconstruction from limited context. Quick check: Can you explain why masking patches rather than entire images encourages learning of semantic structure?

- **Vision Transformer (ViT) Patch Embeddings**: FunduSegmenter operates on patch-level features (16×16 patches). Understanding how spatial information is encoded in sequence format is essential for the decoder design. Quick check: How does a ViT represent spatial relationships when processing patches as a sequence?

- **Adapter Modules for Parameter-Efficient Fine-Tuning**: The ViT block adapter enables segmentation adaptation without full fine-tuning. The bottleneck design (1024→128→1024) is a standard adapter pattern. Quick check: Why use a bottleneck rather than direct linear projection for adaptation?

## Architecture Onboarding

- **Component map**: Input (variable size) → Pre-adapter (CNN, 0.04M params) → RETFound encoder (frozen ViT-large, 24 blocks) [ViT block adapters inserted at each block] → Skip connections from layers 6,12,18,24 → Segmenter decoder (2 transformer blocks) → Post-adapter (4-stage progressive upsampling) + skip connection fusion → Output masks

- **Critical path**: Load pre-trained RETFound weights (fundus version) → Initialize Pre-adapter with weighted residual connection (initial scalar weights = 0.1) → Insert ViT block adapters between LN and MHA in all 24 transformer blocks → Initialize Segmenter decoder class tokens with truncated normal distribution → Build Post-adapter with 4 CNN+upsampling modules → Total tunable parameters: 35.57M

- **Design tradeoffs**: Input size 256×256 chosen for fair baseline comparison; 224×224 performed better but limited comparison. Full freezing vs. partial freezing: Ablation showed full freezing + adapter outperformed unfreezing last 2 layers. Pre-adapter vs. position interpolation: Pre-adapter supports up to 592×592 input with 11s/epoch vs. 32s/epoch for position interpolation.

- **Failure signatures**: Delayed abrupt convergence (training loss plateaus for tens of thousands of epochs then suddenly converges). OC under-segmentation (OC results consistently lower than OD across all methods). Domain shift on small training sets (external verification with Drishti-GS as source shows degraded performance).

- **First 3 experiments**: 1) Baseline validation: Replicate RETFound + Segmenter decoder (2 layers) on REFUGE to establish baseline convergence behavior. 2) Ablation sequence: Add modules one at a time (Post-adapter → Pre-adapter → Skip connections → ViT adapter) on Drishti-GS to reproduce the improvement trajectory. 3) Cross-dataset generalization: Train on REFUGE (400 training images), test on GoDARTS external set without OD center cropping.

## Open Questions the Paper Calls Out

- Can specific architectural refinements close the performance gap in Optic Cup (OC) segmentation compared to layout-specific methods like TVConv? The current FunduSegmenter architecture lacks the translation-variant priors that allow TVConv to excel at segmenting the small, variable OC structure.

- Can the proposed adaptation modules (Pre-adapter, ViT block adapter) generalize effectively to complex segmentation tasks like retinal vessel analysis? The model was validated solely on OD and OC segmentation; retinal vessels present different challenges involving thin, branching structures.

- Does the proposed adapter architecture transfer successfully to non-retinal medical imaging domains when fine-tuning other self-supervised foundation models? The modules were validated exclusively on the RETFound encoder (trained on retinal images); their ability to interface with different ViT-based encoders remains unproven.

## Limitations
- Code and trained weights not publicly available at time of review, preventing exact reproduction
- Domain generalization claims based on single external dataset without systematic cross-dataset ablation
- OC segmentation consistently weaker than OD across all methods, suggesting inherent difficulty
- Limited direct corpus validation for frozen-encoder segmentation with ViT-to-decoder skip connections

## Confidence

- **High confidence** in conceptual novelty of combining frozen RETFound with parameter-efficient adapters for medical image segmentation
- **Medium confidence** in quantitative performance improvements on internal verification, pending code release for exact replication
- **Low confidence** in absolute domain generalization performance, given limited external validation and potential dataset-specific effects

## Next Checks
1. Replicate the ablation study (Table 11) on Drishti-GS to verify incremental performance gains from Post-adapter, Pre-adapter, skip connections, and ViT block adapter
2. Train and evaluate on additional external dataset (e.g., RIM-ONE or IDRiD) to test domain generalization beyond GoDARTS
3. Perform head-to-head comparison with TVConv on same REFUGE training set to verify relative performance differences, particularly for OC segmentation