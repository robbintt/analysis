---
ver: rpa2
title: 'From Scores to Preferences: Redefining MOS Benchmarking for Speech Quality
  Reward Modeling'
arxiv_id: '2510.00743'
source_url: https://arxiv.org/abs/2510.00743
tags:
- speech
- reward
- quality
- audio
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating synthetic speech
  quality by proposing MOS-RMBench, a unified benchmark that reformulates diverse
  MOS datasets into a preference-comparison framework. The core method involves transforming
  subjective MOS scores into relative preferences between audio pairs, then training
  three types of reward models (scalar, semi-scalar, and generative) on this reformulated
  data.
---

# From Scores to Preferences: Redefining MOS Benchmarking for Speech Quality Reward Modeling

## Quick Facts
- arXiv ID: 2510.00743
- Source URL: https://arxiv.org/abs/2510.00743
- Reference count: 39
- Primary result: Scalar reward models achieve 80.04% pairwise accuracy, with MOS-aware generative models improving fine-grained discrimination by over 3% on challenging pairs with small MOS differences.

## Executive Summary
This paper addresses the challenge of evaluating synthetic speech quality by proposing MOS-RMBench, a unified benchmark that reformulates diverse MOS datasets into a preference-comparison framework. The core method involves transforming subjective MOS scores into relative preferences between audio pairs, then training three types of reward models (scalar, semi-scalar, and generative) on this reformulated data. The authors also introduce a MOS-aware generative reward model that incorporates MOS differences into the reward function to improve fine-grained discrimination. The primary result is that scalar reward models achieve the highest overall accuracy (80.04%) in pairwise speech quality assessment, with MOS-aware generative models showing consistent improvements (over 3% gain) on challenging pairs with small MOS differences, particularly narrowing the performance gap on the most difficult cases.

## Method Summary
The method reformulates six diverse MOS datasets (BVCC, NISQA, SingMOS, SOMOS, TMHINT-QI, VMC'23) into a unified preference-based framework. Samples are grouped by content or speaker/system, then pairs are constructed where the higher-MOS sample becomes "chosen" and lower-MOS becomes "rejected." Three reward model paradigms are trained: Classic scalar with Bradley-Terry or MSE loss, Semi-scalar with LLM critiques, and Generative RM with SFT + RL. The MOS-aware variant adds cosine-scaled MOS-difference rewards to improve discrimination on difficult pairs. All models use Qwen2-Audio as base, trained 1 epoch with BT loss showing best performance.

## Key Results
- Scalar reward models achieve highest overall pairwise accuracy (80.04%) on MOS-RMBench test sets
- BT loss outperforms MSE loss by over 4% accuracy, especially on out-of-domain VMC'23 dataset
- MOS-aware generative reward models improve accuracy by over 3% on pairs with particularly similar speech quality
- All models struggle on pairs with MOS differences ≤0.5, showing error rates exceeding 40%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting absolute MOS scores to pairwise preference labels reduces cross-dataset inconsistency and improves generalization.
- Mechanism: Samples are grouped by shared content or speaker/system, then pairs are constructed where the higher-MOS sample becomes "chosen" and lower-MOS becomes "rejected." This transforms heterogeneous absolute scales into a unified ordinal comparison task.
- Core assumption: Relative quality judgments are more stable across annotation protocols than absolute scores; listeners agree more on which sample is better than on exact numerical ratings.
- Evidence anchors:
  - [Section 3.2]: "by converting absolute MOS values into relative preferences, the strategy reduces sensitivity to dataset-specific scoring standards and annotation biases"
  - [Section 5.1]: BT loss shows >4% gain over MSE on overall accuracy and larger gains on OOD VMC'23 dataset
  - [Corpus]: JSQA paper notes MOS exhibits "high levels of inherent variance due to subjective perception variability"
- Break condition: If different datasets use fundamentally different quality dimensions (e.g., naturalness vs. intelligibility), pairwise comparisons may still be inconsistent across domains.

### Mechanism 2
- Claim: Bradley-Terry (BT) loss outperforms MSE loss for training scalar reward models by focusing on relative ordering rather than absolute score regression.
- Mechanism: BT loss directly maximizes the probability that chosen samples receive higher scores than rejected samples; MSE loss penalizes deviation from absolute MOS values, which may be inconsistently scaled across datasets.
- Core assumption: The ordinal relationship between samples is more informative and transferable than precise score magnitudes.
- Evidence anchors:
  - [Section 5.1]: "BT loss consistently outperforms MSE loss, achieving over a 4% gain in overall accuracy"
  - [Section 5.1]: "The advantage of BT loss over MSE loss is even more pronounced on the OOD VMC'23 dataset"
  - [Corpus]: No direct corpus comparison; corpus papers focus on MOS regression rather than preference-based training
- Break condition: If downstream applications require calibrated absolute quality scores (not just rankings), BT-only training may produce scores without meaningful scale.

### Mechanism 3
- Claim: MOS-aware reward functions improve fine-grained discrimination by adaptively scaling rewards based on pair difficulty.
- Mechanism: The reward combines accuracy reward (binary correct/incorrect) with MOS-difference reward using cosine scaling: pairs with small ΔMOS receive larger rewards when correctly ranked and smaller penalties when incorrect.
- Core assumption: Not all preference pairs are equally informative; difficult pairs (small MOS gap) deserve higher training signal when learned correctly.
- Evidence anchors:
  - [Section 5.3]: MOS-aware GRM achieves "+3% on samples with particularly similar speech quality"
  - [Figure 4]: On pairs with ΔMOS ≤ 0.5, MOS-aware GRM improves from 57.44% to 60.74% on TMHINT-QI
  - [Corpus]: No corpus papers explicitly use difficulty-weighted reward functions for speech quality
- Break condition: If MOS differences are unreliable (noisy annotations), weighting by ΔMOS may amplify annotation noise rather than true difficulty.

## Foundational Learning

- Concept: **Bradley-Terry Model**
  - Why needed here: Underlies the BT loss function used to train preference-based reward models; understanding it clarifies why relative comparisons are effective.
  - Quick check question: Given three items A, B, C with BT scores s_A=2, s_B=1, s_C=0.5, what is P(A preferred to B)?

- Concept: **Reinforcement Learning from Human Feedback (RLHF)**
  - Why needed here: GRMs are trained via GRPO/DAPO, which are RL methods; understanding policy optimization helps interpret reward shaping.
  - Quick check question: In RLHF, what role does the reward model play during policy optimization?

- Concept: **Mean Opinion Score (MOS) Limitations**
  - Why needed here: Motivates the entire approach; understanding MOS inconsistency explains why reformulation is necessary.
  - Quick check question: Why might the same audio clip receive different MOS ratings across two datasets?

## Architecture Onboarding

- Component map: Raw audio (16kHz WAV) -> Group by content/speaker -> Create preference pairs (chosen/rejected) -> Annotation with Gemini-2.5-Pro -> Three training paradigms (scalar, semi-scalar, GRM) -> Evaluation via pairwise accuracy

- Critical path:
  1. Data filtering and grouping (Section 3.2) determines pair quality
  2. Preference pair construction (higher MOS = chosen) establishes supervision signal
  3. BT loss training (Section 4.2) produces strongest scalar models
  4. MOS-aware reward addition (Section 5.3) improves fine-grained GRM performance

- Design tradeoffs:
  - Scalar vs. Generative: Scalar models achieve higher accuracy (~80%); GRMs provide interpretability through critiques
  - BT vs. MSE: BT better for cross-domain generalization; MSE may produce better-calibrated absolute scores (not tested)
  - Accuracy reward vs. MOS-aware reward: MOS-aware improves difficult pairs but requires reliable ΔMOS values

- Failure signatures:
  - All models show >40% error rate on pairs in the lowest ΔMOS percentile (Section 5.2)
  - Synthetic speech datasets (SOMOS, VMC'23) show consistently lower accuracy than human speech
  - LLM-as-a-judge (Gemini-2.5-Pro) achieves only 69.26% overall accuracy

- First 3 experiments:
  1. **Baseline replication**: Train Classic scalar RM with BT loss on MOS-RMBench training set; verify ~80% accuracy on in-domain test
  2. **Difficulty stratification**: Evaluate baseline on test pairs binned by ΔMOS; confirm high error rates on small-ΔMOS pairs
  3. **MOS-aware ablation**: Train GRM with accuracy-only reward vs. MOS-aware reward; compare accuracy on ΔMOS ≤ 0.5 subset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can reward models be improved to achieve fine-grained discrimination on speech pairs with MOS differences ≤0.5, where current models exhibit error rates exceeding 40%?
- Basis in paper: [explicit] The error analysis section states "all models are particularly prone to errors on sample pairs with small MOS differences" and "even the top-performing Classic scalar model exhibits error rates of 40% or higher" in lowest MOS difference percentiles. The MOS-aware GRM provides only modest gains (~3%) on these challenging cases.
- Why unresolved: The paper identifies this as the "key challenge" and "critical bottleneck," but the proposed MOS-aware reward only partially addresses it—accuracy on pairs with ΔMOS≤0.5 remains near or below 65% across all models.
- What evidence would resolve it: A model architecture or training objective achieving >80% accuracy on pairs with ΔMOS≤0.5 on the MOS-RMBench test sets would demonstrate substantial progress.

### Open Question 2
- Question: What architectural or training modifications can close the persistent performance gap between synthetic speech evaluation (e.g., SOMOS, VMC'23) and human speech evaluation?
- Basis in paper: [explicit] The authors note "most models perform considerably worse on synthetic speech datasets such as SOMOS and VMC'23 compared to human speech, underscoring a persistent domain gap."
- Why unresolved: The paper documents this gap but does not propose or evaluate specific interventions targeting synthetic speech; all paradigms (scalar, semi-scalar, GRM) show reduced accuracy on synthetic data.
- What evidence would resolve it: Demonstration of a single model achieving comparable accuracy (within 2-3%) on synthetic and human speech subsets, or targeted data augmentation/adapter methods evaluated on SOMOS and VMC'23.

### Open Question 3
- Question: How does reformulating absolute MOS scores into pairwise preferences affect a model's ability to learn absolute quality levels, and can preference-trained models still produce calibrated MOS predictions?
- Basis in paper: [explicit] In the limitations section: "Reformulating MOS scores into pairwise preferences also changes the original score distribution, which may affect how models learn absolute quality levels."
- Why unresolved: The benchmark exclusively evaluates pairwise accuracy; no experiments test whether preference-trained models can recover calibrated absolute scores or generalize to new datasets with different MOS distributions.
- What evidence would resolve it: A study correlating preference-trained model scores with held-out absolute MOS ratings across datasets, or comparison of score calibration between BT-loss and MSE-loss models on a held-out regression benchmark.

### Open Question 4
- Question: Can incorporating prosody, emotion, and style dimensions into the reward modeling framework improve discrimination on perceptually similar pairs or cross-domain generalization?
- Basis in paper: [explicit] The paper states MOS-RMBench "focuses primarily on perceptual quality, leaving prosody, emotion, and style unmodeled," and future work includes "incorporating multi-dimensional quality dimensions."
- Why unresolved: The current four-dimension framework (noise, distortion, continuity, naturalness) does not capture expressive or stylistic differences, which may contribute to errors on challenging pairs or synthetic speech where prosody varies.
- What evidence would resolve it: An extended annotation schema and model evaluation showing measurable accuracy gains on pairs with similar perceptual quality but differing prosody/emotion, particularly on singing voice (SingMOS) or expressive TTS datasets.

## Limitations
- The conversion from MOS scores to pairwise preferences relies on grouping samples by content/speaker, which may exclude cross-content comparisons and introduce bias toward within-group comparisons
- LLM-as-judge approach inherits potential biases and may not generalize across diverse demographic groups or annotation protocols
- Limited out-of-domain testing—VMC'23 only covers synthetic speech and multilingual samples, not telephony, streaming, or assistive technologies

## Confidence
- **High confidence**: Scalar reward models achieve superior pairwise accuracy (80.04%) compared to generative models, and BT loss outperforms MSE loss across datasets
- **Medium confidence**: MOS-aware reward functions provide consistent 3%+ improvements on difficult pairs with small MOS differences
- **Low confidence**: The claim that preference-based reformulation "reduces sensitivity to dataset-specific scoring standards" is theoretically supported but not empirically validated through ablation studies

## Next Checks
1. **Cross-content validation**: Construct and evaluate test pairs across different content/speaker groups to verify whether the grouping strategy limits the model's ability to detect quality differences that transcend content boundaries

2. **Domain robustness testing**: Test MOS-RMBench-trained models on speech quality datasets from different application domains (telephony, streaming, assistive technologies) to quantify generalization beyond the current evaluation scope

3. **Human validation study**: Conduct a small-scale human preference study comparing LLM judgments vs. human preferences on challenging pairs (ΔMOS ≤ 0.5) to verify that the MOS-aware reward function improvements reflect genuine quality distinctions rather than LLM-specific artifacts