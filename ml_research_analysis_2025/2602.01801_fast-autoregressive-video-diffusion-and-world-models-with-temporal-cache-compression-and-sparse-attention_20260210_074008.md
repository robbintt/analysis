---
ver: rpa2
title: Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression
  and Sparse Attention
arxiv_id: '2602.01801'
source_url: https://arxiv.org/abs/2602.01801
tags:
- attention
- video
- diffusion
- generation
- cache
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the growing latency and memory bottleneck\
  \ in autoregressive video diffusion models caused by expanding KV caches during\
  \ long-term streaming generation. The authors identify three sources of redundancy\u2014\
  near-duplicate keys across frames, slowly evolving semantic features, and selective\
  \ cross-attention over long prompts\u2014and propose a training-free framework to\
  \ exploit them."
---

# Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression and Sparse Attention

## Quick Facts
- arXiv ID: 2602.01801
- Source URL: https://arxiv.org/abs/2602.01801
- Reference count: 13
- Addresses the latency and memory bottleneck in autoregressive video diffusion models during long-term streaming generation

## Executive Summary
This paper addresses the growing latency and memory bottleneck in autoregressive video diffusion models caused by expanding KV caches during long-term streaming generation. The authors identify three sources of redundancy—near-duplicate keys across frames, slowly evolving semantic features, and selective cross-attention over long prompts—and propose a training-free framework to exploit them. TempCache compresses the KV cache by merging temporally corresponding keys using attention-based correspondence matching, while AnnCA and AnnSA sparsify cross- and self-attention respectively using lightweight approximate nearest neighbor search. Experiments on Rolling-Forcing and LongVie2 demonstrate up to 5–10× end-to-end speedups with near-constant GPU memory over 3000-frame rollouts, preserving PSNR, SSIM, and perceptual quality comparable to dense attention baselines, where prior methods progressively slow and degrade.

## Method Summary
The framework exploits three sources of redundancy in autoregressive video diffusion: (1) near-duplicate keys across frames due to low motion, (2) slowly evolving semantic features enabling temporal key merging, and (3) selective cross-attention over long prompts. TempCache compresses KV caches by retrieving nearest-neighbor keys across frames using FAISS, merging corresponding keys, and averaging their values with a log(group_size) bias. AnnCA prunes prompt tokens via ANN bucket overlap with latent queries, while AnnSA restricts self-attention to keys sharing semantic buckets. The method is fully training-free, uses FlashAttention-3 for dense attention, and FlashInfer for sparse kernels. Sparsification is disabled for the first 30% of denoising steps and applied to ~70% of transformer blocks.

## Key Results
- 5–10× end-to-end speedup on 3000-frame rollouts
- Near-constant GPU memory usage compared to progressive growth in dense baselines
- PSNR, SSIM, and LPIPS quality comparable to dense attention baselines
- Significant performance gains over existing methods (SVG, RadialAttention, StreamingDiff) on LongVBench and LongVGenBench

## Why This Works (Mechanism)
The method works by identifying and exploiting three sources of redundancy in autoregressive video diffusion: (1) near-duplicate keys across frames due to low motion, (2) slowly evolving semantic features enabling temporal key merging, and (3) selective cross-attention over long prompts. TempCache uses ANN to find temporally corresponding keys and merges them, reducing memory while preserving attention quality through log(group_size) bias. AnnCA and AnnSA use ANN to identify and prune redundant tokens in cross- and self-attention respectively, reducing computation. The training-free approach adapts inference-time mechanisms to pre-trained dense models without retraining.

## Foundational Learning

**Autoregressive Video Diffusion**: Sequential frame generation where each frame conditions on previously generated ones through cross-attention. Why needed: Understanding the temporal dependency structure that creates KV cache growth. Quick check: Verify that each new frame adds KV pairs proportional to previous frame count.

**Key-Value Cache**: Memory storing keys and values from previous tokens/frames to avoid recomputation during attention. Why needed: The cache grows linearly with sequence length, creating the memory bottleneck. Quick check: Confirm KV cache size scales with number of generated frames.

**Approximate Nearest Neighbor (ANN) Search**: Fast similarity search using hashing or quantization to find approximate nearest neighbors instead of exact computation. Why needed: Enables efficient identification of redundant keys and tokens for compression/sparsification. Quick check: Verify FAISS LSH or quantization configurations retrieve correct nearest neighbors.

**Temporal Key Merging**: Combining keys from corresponding positions across time steps when they are similar, reducing redundancy. Why needed: Exploits the slow evolution of semantic features across frames. Quick check: Monitor attention recall after merging to ensure quality preservation.

**Cross-Attention Sparsification**: Reducing the number of prompt tokens attended to by selecting only relevant ones. Why needed: Many prompt tokens are redundant or irrelevant for generating specific frames. Quick check: Verify that pruned tokens are indeed less relevant based on attention weights.

**Self-Attention Sparsification**: Restricting attention to semantically similar keys within buckets. Why needed: Most self-attention computation is redundant due to local coherence. Quick check: Ensure semantic buckets preserve attention quality while reducing computation.

## Architecture Onboarding

**Component Map**: Rolling-Forcing/LongVie2 -> TempCache (ANN-based KV merging) -> AnnCA (sparse cross-attention) -> AnnSA (sparse self-attention) -> FlashAttention-3/FlashInfer -> Output frames

**Critical Path**: Input prompt -> Diffusion denoising steps -> Cross-attention (AnnCA) -> Self-attention (AnnSA) -> TempCache KV compression -> Output frame

**Design Tradeoffs**: Training-free adaptation vs. potential gains from joint training; aggressive compression vs. quality preservation; sparse kernel overhead vs. computation savings

**Failure Signatures**: Quality collapse from over-merging keys (attention recall < 85%); no speedup or slower than dense on short clips; temporal artifacts from aggressive sparsification

**First Experiments**:
1. Implement TempCache with FAISS ANN and verify key merging reduces memory while maintaining attention recall above 85%
2. Add AnnCA and measure cross-attention speedup without quality degradation on single-frame generation
3. Integrate AnnSA and validate self-attention speedup with preserved semantic coherence in generated frames

## Open Questions the Paper Calls Out

**Open Question 1**: Would joint training or fine-tuning with ANN-based attention yield better quality-efficiency trade-offs than the training-free approach? The authors state this is the first use of ANN-based attention in autoregressive diffusion models in a fully training-free manner, requiring no retraining or fine-tuning. The training-free constraint may limit potential gains; the model was trained with dense attention and only adapts at inference time. Experiments fine-tuning models with ANN-based attention enabled during training, comparing against training-free baselines on the same quality and speedup metrics would resolve this.

**Open Question 2**: How can the similarity threshold for TempCache and the sparsification schedule be made adaptive rather than fixed? Figure 10(a) shows a fixed threshold trades off compression vs. recall; sparse attention is disabled for the first 30% of denoising steps without per-layer or per-timestep adaptation. Different transformer layers, diffusion timesteps, and video content may exhibit different redundancy levels, but current settings are globally applied. A study comparing fixed vs. learned/per-layer adaptive thresholds, measuring both attention recall preservation and end-to-end speedup would resolve this.

**Open Question 3**: What video content characteristics (e.g., fast motion, scene cuts, texture complexity) cause the largest quality degradation under ANN-based sparsification? Figure 7 shows RadialAttention degrades in challenging scenes and SVG-style sparsity causes temporal drift, but no systematic analysis of failure modes. The method's robustness across diverse video dynamics is not characterized, limiting deployment guidance. A controlled ablation varying motion magnitude, scene complexity, and temporal discontinuities, with per-category PSNR/LPIPS/FID breakdowns would resolve this.

## Limitations

- Exact FAISS configurations, merging thresholds, and sparsification block selection are underspecified, creating barriers to exact replication
- Scalability to multi-prompt, multi-aspect scenarios and performance relative to online-learning alternatives are not fully characterized
- Limited analysis of failure modes under extreme conditions or cross-architecture generalization

## Confidence

- **High confidence**: The core empirical findings (5-10x speedup, near-constant memory, maintained PSNR/SSIM/LPIPS) are supported by extensive benchmarking on established datasets and models. The qualitative claims about redundancy sources are plausible and grounded in video generation characteristics.
- **Medium confidence**: The effectiveness of the three individual components (TempCache, AnnCA, AnnSA) is demonstrated, but the specific impact of each and the optimality of their combined configuration (e.g., 70% sparsification rate, first 30% disabled) are not rigorously isolated. The choice of 3000-frame rollouts as the primary evaluation scale is justified but not validated at larger scales.
- **Low confidence**: The robustness of the framework to varying video content, model architectures beyond the two tested, and the sensitivity to FAISS hyperparameters are not fully explored. The absence of online learning comparisons leaves open the question of whether training-free compression is optimal.

## Next Checks

1. Reproduce the main speed/memory/quality results on Rolling-Forcing and LongVie2 using specified benchmarks; if quality drops, adjust TempCache similarity threshold and report sensitivity
2. Systematically ablate each component (TempCache, AnnCA, AnnSA) and measure their individual contributions to overall speedup and quality to identify the dominant factor
3. Evaluate the method on a held-out video model (e.g., Wan2.1-14B or HunyuanVideo) with prompts from LongVGenBench to assess generalization beyond the training models