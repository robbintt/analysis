---
ver: rpa2
title: 'Purrturbed but Stable: Human-Cat Invariant Representations Across CNNs, ViTs
  and Self-Supervised ViTs'
arxiv_id: '2511.02404'
source_url: https://arxiv.org/abs/2511.02404
tags:
- vits
- alignment
- across
- cnns
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks cross-species representational alignment
  between human and cat visual systems using frozen vision encoders. The authors curate
  a dataset of ~300k paired human-cat frames and apply a biologically informed cat-vision
  filter to simulate species-specific optics.
---

# Purrturbed but Stable: Human-Cat Invariant Representations Across CNNs, ViTs and Self-Supervised ViTs

## Quick Facts
- arXiv ID: 2511.02404
- Source URL: https://arxiv.org/abs/2511.02404
- Reference count: 40
- This paper benchmarks cross-species representational alignment between human and cat visual systems using frozen vision encoders, finding that self-supervised ViTs achieve the strongest alignment.

## Executive Summary
This paper investigates cross-species representational alignment between human and cat visual systems by curating a dataset of ~300k paired human-cat frames and applying a biologically informed cat-vision filter. The study evaluates CNNs, supervised ViTs, and self-supervised ViTs (DINO/DINOv2/v3) using layer-wise CKA, RSA, and distributional tests. Results show that DINO ViT-B/16 achieves the strongest alignment (mean CKA-RBF ≈0.814, CKA-linear ≈0.745, RSA ≈0.698), peaking at early blocks, while CNNs and Swin underperform. The findings suggest self-supervised ViTs better bridge species-specific visual statistics than widely used CNNs or windowed transformers.

## Method Summary
The study curates 191 cat POV videos to create 346,400 paired human-cat frames, applying a biologically-informed cat-vision filter that simulates feline optical properties including spectral sensitivity, spatial acuity, geometric optics, and temporal processing. Frozen pretrained encoders (CNNs, supervised ViTs, windowed transformers, and self-supervised ViTs) extract layer-wise features, which are vectorized and compared using CKA (linear/RBF), RSA with Mantel permutation tests, and distributional shift tests (MMD, Energy distance, 1-Wasserstein). Statistical significance is controlled with Benjamini-Hochberg FDR correction. The primary metric is mean CKA-RBF, with secondary metrics providing geometric and distributional insights.

## Key Results
- DINO ViT-B/16 achieves the highest overall alignment (CKA-RBF 0.8144) vs best supervised ViT (0.8057) and best CNN (0.7017)
- Self-supervised models peak at early blocks (block0 for DINO), while supervised models require deeper processing (block8-14)
- Global attention architectures (plain ViTs) outperform windowed transformers (Swin) and hierarchical CNNs for cross-species alignment
- Early-layer alignment (conv1, block1) shows lowest CKA-Linear/RSA across families, suggesting species-specific low-level features

## Why This Works (Mechanism)

### Mechanism 1: Self-Supervision Induces Early-Stage Cross-Species Invariant Features
Self-supervised ViTs (DINO) produce representations that align human and cat visual domains more effectively than supervised alternatives, with alignment emerging at earlier network layers. Token-level self-supervision objectives (DINO's student-teacher distillation without labels) learn features that capture domain-general structure rather than human-label-specific patterns, producing early-layer representations that are inherently invariant to species-specific optical transformations.

### Mechanism 2: Global Attention Architectures Support Cross-Species Geometric Alignment
Plain ViT architectures with global self-attention achieve stronger cross-species alignment than windowed transformers (Swin) or hierarchical CNNs, independent of training objective. Global attention enables arbitrary spatial relationships between tokens, supporting shape and part-whole representations that remain consistent across species-specific spatial distortions.

### Mechanism 3: Hierarchical Convergence Depth Depends on Training Objective
Self-supervised models achieve peak alignment at early blocks (block0 for DINO), while supervised models require deeper processing (block8-14) to achieve comparable alignment. Supervised training on human-centric labels forces later layers to specialize for label-relevant features that may be species-specific, requiring deeper abstraction to recover cross-species invariance.

## Foundational Learning

- **Representational Similarity Analysis (RSA) and Centered Kernel Alignment (CKA)**: Core metrics for quantifying cross-species alignment. RSA compares representational geometries via dissimilarity matrices and Spearman correlation; CKA measures kernel-based similarity with invariance to orthogonal transforms and isotropic scaling. Quick check: If two representations have CKA=0.9 but RSA=0.5, what does this indicate? (Answer: High overall similarity but divergent representational geometry.)

- **Self-Supervised Learning Paradigms (particularly DINO)**: The paper's strongest results come from DINO, which uses student-teacher distillation without labels. Quick check: Why might a model trained without labels produce more cross-species invariant features than one trained on human-annotated categories? (Answer: Supervised labels encode human-specific semantic distinctions; SSL learns from data structure/patterns that may be shared across species.)

- **Vision Transformer Architecture and Inductive Biases**: The paper compares global-attention ViTs, windowed-attention Swin, and CNNs, concluding architectural inductive biases affect cross-species alignment. Quick check: Why would windowed attention (Swin) potentially underperform global attention (plain ViT) for cross-species alignment under geometric distortion? (Answer: Geometric distortions may move semantically related content across window boundaries, breaking local processing assumptions.)

## Architecture Onboarding

- **Component map**: 
  Data pipeline (191 cat POV videos → 346,400 frame-pairs → cat-vision filter) → Frozen encoder suite (35 models) → Feature extraction (layer-wise) → Vectorization (GAP/class token) → Alignment metrics (CKA/RSA/distributional tests) → Statistical validation (BH-FDR)

- **Critical path**: Ensure paired frame integrity → Apply cat-vision filter consistently → Extract features with canonical preprocessing → Compute alignment metrics with statistical controls → Report layer-wise aggregates

- **Design tradeoffs**: Frozen vs. fine-tuned encoders (isolates inherent representation quality vs. adaptation potential); Cat-vision filter complexity vs. interpretability (computational tractability vs. physiological completeness); Metric selection (CKA-RBF as primary vs. complementary metrics for geometric comparison)

- **Failure signatures**: High CKA with low RSA (direction-similar but magnitude/relationship-different representations); Early-layer dissimilarity (species-specific low-level features); High geometric alignment with large distributional shift (magnitude differences requiring normalization); Windowed attention underperformance (visualize attention patterns under cat-vision distortion)

- **First 3 experiments**: 
  1. Reproduce DINO vs. supervised ViT alignment gap with controlled capacity (DINO ViT-S/16 vs. ViT-B/16)
  2. Ablate cat-vision filter components (spectral only, spatial only, geometric only) to identify hardest species-specific differences
  3. Fine-tuning cross-species transfer test (DINO ViT-B/16, ViT-L/16, EfficientNet-B3 on cat-behavior classification)

## Open Questions the Paper Calls Out

1. How does cross-species representational alignment scale when extending the paired protocol to species with greater optical, ecological, and phylogenetic divergence (e.g., canine, nonhuman primate, avian)? The study only examines domestic cat–human alignment.

2. Would validation against actual neural recordings from cat visual cortex (rather than simulated cat-vision filters) confirm the early-layer alignment peaks observed in DINO ViTs? The study uses a simulated cat-vision filter, not empirical neural response data.

3. How robust are the reported alignment patterns to variations in the cat-vision filter parameters (pupil aspect ratio, rod:cone weighting, spatial acuity reduction)? The paper acknowledges the filter omits key physiological details.

## Limitations

- Results are based on a single curated dataset of ~300k paired frames from 191 cat POV videos with a simplified cat-vision filter approximation
- All models are evaluated in frozen mode, preventing assessment of adaptation potential
- No single metric captures all aspects of representational alignment, with metric-specific divergences observed

## Confidence

- **High confidence**: Architectural findings (global ViTs > windowed Swin > CNNs for frozen cross-species alignment)
- **Medium confidence**: Training objective effects (DINO > supervised ViTs for cross-species invariance)
- **Low confidence**: Physiological accuracy of cat-vision filter as complete forward model

## Next Checks

1. **Controlled capacity ablation**: Test whether DINO's alignment advantage persists when comparing models with matched parameter counts (DINO ViT-S/16 vs. ViT-B/16) to isolate training objective effects from architectural capacity.

2. **Filter component ablation**: Systematically remove cat-vision filter components (spectral only, spatial only, geometric only) to identify which species-specific differences are hardest for each architecture to handle, distinguishing between optical transformation robustness and learned invariance.

3. **Cross-species transfer validation**: Fine-tune top-performing frozen encoders (DINO ViT-B/16, ViT-L/16, EfficientNet-B3) on a small cat-behavior classification task (prey detection, navigation) to assess whether high alignment translates to practical transfer performance.