---
ver: rpa2
title: 'Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA
  in Gastrointestinal Endoscopy'
arxiv_id: '2506.09958'
source_url: https://arxiv.org/abs/2506.09958
tags:
- dataset
- question
- clinical
- medical
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Kvasir-VQA-x1, a large-scale dataset for
  medical visual question answering in gastrointestinal endoscopy. The dataset expands
  upon the original Kvasir-VQA by incorporating 159,549 new question-answer pairs,
  designed to test deeper clinical reasoning through structured question merging and
  complexity stratification.
---

# Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy

## Quick Facts
- arXiv ID: 2506.09958
- Source URL: https://arxiv.org/abs/2506.09958
- Reference count: 40
- Primary result: Fine-tuned models achieved 87–90% mean accuracy on GI endoscopy VQA tasks

## Executive Summary
This paper introduces Kvasir-VQA-x1, a large-scale dataset for medical visual question answering in gastrointestinal endoscopy. The dataset expands upon the original Kvasir-VQA by incorporating 159,549 new question-answer pairs, designed to test deeper clinical reasoning through structured question merging and complexity stratification. Visual augmentations mimicking real-world imaging artifacts are included to evaluate model robustness. The dataset is structured into two evaluation tracks: one for standard VQA performance and another for robustness testing against visual perturbations. Fine-tuned models like MedGemma-ft and Qwen2.5-VL-ft achieved mean accuracies of 87% and 90%, respectively, outperforming base models and demonstrating improved handling of complex reasoning tasks.

## Method Summary
The authors fine-tuned MedGemma-4B and Qwen2.5-VL-7B using LoRA (r=16/α=64) with frozen vision encoders on the Kvasir-VQA-x1 dataset. Training used DeepSpeed ZeRO Stage 2, bfloat16 precision, batch sizes of 36-32, and a maximum sequence length of 1000 tokens. The dataset contains 159,549 QA pairs across 6,500 images, stratified into complexity levels 1-3 based on the number of atomic QA pairs merged per question. Models were evaluated on both Normal and Transformed validation sets with augmentation (RandomResizedCrop, RandomRotation, RandomAffine, ColorJitter). Evaluation metrics included NLP metrics (ROUGE, METEOR, BLEU, BLEURT, BERT-F1) plus a binary LLM adjudicator (Qwen3-30B-A3B) scoring per clinical aspect.

## Key Results
- MedGemma-ft achieved 87% mean accuracy on Normal track; Qwen2.5-VL-ft achieved 90%
- Models trained with augmentation showed robust performance across Normal and Transformed validation sets with minimal performance gaps (~0.001-0.002)
- Level 2 questions (complexity 2) demonstrated a "synthesis sweet spot" outperforming Level 1 in some models

## Why This Works (Mechanism)

### Mechanism 1: Complexity-Stratified QA Generation
Merging multiple atomic QA pairs into single questions may promote multi-step reasoning over simple recall, conditional on the quality of the merging process and clinical coherence of the combined question. The pipeline groups 1, 2, or 3 atomic QA pairs per image and uses an LLM to synthesize them into unified questions requiring integrated answers. Complexity scores (1–3) reflect the number of merged pairs.

### Mechanism 2: Augmentation-Induced Robustness
Training on weakly augmented images (crop, rotation, color jitter) may improve model invariance to real-world imaging artifacts, conditional on augmentations being representative of clinical variability. 10 augmented variants per image expose models to controlled perturbations. Models trained on transformed data show stable performance across Normal and Transformed validation sets.

### Mechanism 3: LoRA Fine-Tuning for Domain Adaptation
Low-Rank Adaptation enables efficient specialization of general VLMs to GI endoscopy, conditional on the vision encoder remaining sufficiently generic. LoRA injects trainable low-rank matrices into transformer projection layers while freezing the vision backbone. Fine-tuned models achieve significantly higher accuracy than base models.

## Foundational Learning

- **Vision-Language Models (VLMs)**: The entire pipeline assumes familiarity with how vision encoders map images to embeddings that language decoders can process. Why needed here: Explains why Qwen2.5-VL's dynamic resolution handling might preserve more clinical detail than fixed-size encoders.

- **Parameter-Efficient Fine-Tuning (LoRA)**: The paper uses LoRA with frozen vision backbones; understanding rank (r=16) and alpha (α=64) scaling is essential for reproducing results. Why needed here: What happens to gradient updates if LoRA rank is set too low for a specialized medical domain?

- **Medical VQA Evaluation Metrics**: The paper critiques BLEU/ROUGE for clinical evaluation and introduces an LLM adjudicator; you need to understand why n-gram metrics fail on semantic correctness. Why needed here: Why might a high BERT-F1 score still fail to capture a clinically dangerous hallucination?

## Architecture Onboarding

- **Component map**: Images (6,500) → QA pairs (159,549) → Complexity levels (1-3) → HuggingFace dataset → Augmentation script → Fine-tuning pipeline → Evaluation tracks (Normal/Transformed) → LLM adjudication

- **Critical path**: Load dataset via `datasets.load_dataset("SimulaMet/Kvasir-VQA-x1")` → Apply augmentation script if using Transformed track → Initialize model with LoRA config → Train with clinical instruction prompt → Evaluate on both tracks; run adjudicator for categorical accuracy

- **Design tradeoffs**: Qwen2.5-VL vs. MedGemma: Qwen's dynamic resolution helps with heterogeneous endoscopy images; MedGemma is smaller and medical-specialized but uses fixed-size SigLIP. Clean vs. Augmented training: Augmented training improves robustness without degrading clean performance, but adds preprocessing overhead. Binary adjudication scoring: Strict criterion provides clear signals but may undercredit partial reasoning.

- **Failure signatures**: Low performance on polyp size/abnormality location (vision encoders struggle with metric and spatial precision); Level 3 performance drop (error accumulation in multi-hop questions); Homogeneity bias (LLM adjudicator may favor Qwen-family models).

- **First 3 experiments**: 1) Baseline replication: Fine-tune Qwen2.5-VL on Normal track only; evaluate on both tracks to confirm robustness gap. 2) Augmentation ablation: Train identical model on Transformed track; compare Normal vs. Transformed validation scores. 3) Complexity analysis: Evaluate checkpoint performance stratified by Level 1/2/3; identify whether Level 2 "synthesis sweet spot" holds.

## Open Questions the Paper Calls Out

- **Cross-domain generalization**: The models' performance may not generalize to other medical domains (e.g., radiology or pathology) without further fine-tuning.

- **Adjudication consistency**: The binary LLM scoring is strict but may penalize partial reasoning; human-in-the-loop validation is limited to 100 samples.

- **Augmentation transferability**: Weak transforms may not capture all real-world endoscopy artifacts (specular reflections, motion blur, compression artifacts).

## Limitations

- Adjudication consistency: The binary LLM scoring may penalize partial reasoning and human validation is limited.
- Complexity scoring validity: Level 3 questions may reflect length/complexity rather than true multi-hop reasoning.
- Homogeneity bias: The Qwen-based adjudicator may favor Qwen-family models due to shared architectural lineage.

## Confidence

- **High**: LoRA fine-tuning improves VQA accuracy (87–90% vs. ~30–45% base); baseline replication should be straightforward.
- **Medium**: Augmentation improves robustness without clean performance degradation; however, exact correlation to clinical artifact types is uncertain.
- **Low**: Complexity stratification meaningfully measures reasoning depth; without explicit reasoning trace analysis, scoring may reflect length/complexity rather than true integration.

## Next Checks

1. **Human adjudication verification**: Have 3 clinicians independently score 50 Level 1, 50 Level 2, and 50 Level 3 questions to measure agreement with LLM adjudicator and validate complexity stratification.

2. **Augmentation artifact mapping**: Document which clinical artifacts (e.g., specular highlights, motion blur) appear in Transformed set; correlate model performance drops with artifact types to validate augmentation relevance.

3. **Reasoning trace analysis**: For a subset of Level 2/3 questions, capture model attention/activation patterns to verify whether correct answers require integration of multiple image regions or simply sequential retrieval.