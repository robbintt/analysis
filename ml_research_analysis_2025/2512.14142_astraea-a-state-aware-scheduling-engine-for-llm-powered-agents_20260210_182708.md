---
ver: rpa2
title: 'Astraea: A State-Aware Scheduling Engine for LLM-Powered Agents'
arxiv_id: '2512.14142'
source_url: https://arxiv.org/abs/2512.14142
tags:
- uni00000014
- uni00000013
- uni00000011
- uni00000018
- scheduling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of existing LLM serving systems
  in handling agentic workflows, which alternate between local computation and external
  API calls. The key problem is that traditional schedulers optimize per-segment response
  times, leading to poor end-to-end Job Completion Time (JCT) and head-of-line blocking
  during long I/O waits.
---

# Astraea: A State-Aware Scheduling Engine for LLM-Powered Agents

## Quick Facts
- **arXiv ID**: 2512.14142
- **Source URL**: https://arxiv.org/abs/2512.14142
- **Reference count**: 40
- **Primary result**: Stateful-MLFQ scheduling engine reduces average Job Completion Time (JCT) by up to 25.5% for LLM agentic workflows.

## Executive Summary
Astraea is a scheduling engine designed to optimize the end-to-end Job Completion Time (JCT) for LLM agentic workflows that alternate between local GPU computation and external API calls. Traditional LLM serving systems focus on per-segment response times, leading to poor global JCT due to head-of-line blocking during long I/O waits. Astraea addresses this by implementing a state-aware, hierarchical scheduling mechanism that dynamically classifies requests by their I/O and compute intensity, prioritizing I/O-heavy requests to prevent them from blocking shorter compute tasks. It also features an adaptive KV cache manager that intelligently handles agent state during I/O waits based on system memory pressure, and a service time predictor that unifies historical state with future predictions to guide scheduling decisions.

## Method Summary
Astraea is built on top of vLLM and introduces a Stateful-MLFQ scheduler with dynamic priority migration based on token cost thresholds. Requests are classified as I/O or compute intensive based on their behavior (e.g., yielding for API calls before exhausting token cost quotas). The system uses HRRN for intra-queue ordering and an aging mechanism to prevent starvation. An adaptive KV cache manager selects between Preserve, Swap, and Discard policies based on a memory waste cost function. Service time prediction combines offline profiling for prefill, oracle-based token generation length, and category-based API latency statistics. The system was evaluated on the Infercept dataset using GPT-J (6B) and Vicuna-13B models.

## Key Results
- Reduces average JCT by up to 25.5% compared to baseline methods under various load conditions.
- Demonstrates superior stability and robustness under high load across different model scales.
- Ablation studies confirm the effectiveness of the Stateful-MLFQ scheduling algorithm itself.
- Overhead analysis shows negligible computational cost (0.17ms scheduling overhead, 1.1% additional GPU memory usage).

## Why This Works (Mechanism)

### Mechanism 1: Stateful-MLFQ with Dynamic Priority Migration
The system implements a Multi-Level Feedback Queue (MLFQ) where requests are dynamically classified and prioritized based on runtime behavior. Requests start in a high-priority queue and are demoted if they exhaust a "Token Cost" quota (indicating compute-heavy) or promoted if they yield for an API call before hitting the limit (indicating I/O-heavy). This prevents long API waits from blocking short compute tasks. The core assumption is that Token Cost is a more stable metric for scheduling granularity than time slices.

### Mechanism 2: Adaptive KV Cache Management
During API waits, the KV cache manager selects a strategy (Preserve, Swap, Discard) by minimizing a "memory waste" cost function ($W$). If memory is low, it prefers Discard/Swap to free space for new requests; if high, it preserves to avoid recomputation latency. The system assumes accurate prediction of API duration ($T_{api}$) and Swap time ($T_{swap}$) to calculate the cost function correctly.

### Mechanism 3: Lifecycle-Aware Prediction
The "Service Time Predictor" combines offline profiling for prefill, a segment-level oracle for generation length, and category-based statistical averages for API latency. The paper assumes API latencies follow stable distributions based on functional categories. This prediction is crucial for classifying requests and managing KV caches effectively.

## Foundational Learning

- **Concept: Multi-Level Feedback Queue (MLFQ)**
  - Why needed: This is the structural backbone of Astraea. You must understand how processes move between queues based on "time slices" (or Token Costs) to grasp how Astraea prioritizes I/O vs. Compute.
  - Quick check: If a request consistently uses its full time slice without blocking, does it move to a higher or lower priority queue in standard MLFQ? (Astraea modifies this logic).

- **Concept: KV Cache Offloading/Streaming**
  - Why needed: Astraea manages the trade-off between keeping the KV cache in expensive GPU memory vs. swapping it to CPU. Understanding the bandwidth cost of this data movement is essential.
  - Quick check: Why is "Discarding" a KV cache expensive for latency, even if it saves memory?

- **Concept: Highest Response Ratio Next (HRRN)**
  - Why needed: Astraea uses HRRN for intra-queue sorting to prevent "starvation" of long tasks while still favoring short ones.
  - Quick check: How does the HRRN score change as a request waits longer in the queue?

## Architecture Onboarding

- **Component map**: Request Pool -> Predictor -> Scheduler (Stateful-MLFQ) -> KV Cache Manager -> Inference Engine (vLLM)
- **Critical path**: The `BuildNextBatch` function in Algorithm 1. This is where priority aging is checked, the highest priority non-empty queue is selected, and segments are sorted by HRRN score before being packed into a batch.
- **Design tradeoffs**:
  - Prediction Accuracy vs. Overhead: The paper uses an oracle for token generation and category stats for APIs. In deployment, complex predictors may introduce latency.
  - Responsiveness vs. Throughput: The "Preserve" strategy minimizes latency for one request but risks Out-Of-Memory (OOM) errors that crash the system.
- **Failure signatures**:
  - Starvation: Long compute-heavy requests stuck in $Q_{m-1}$ never run. (Mitigated by the "Aging" mechanism).
  - HoL Blocking: The system stalls waiting for a long API call if the scheduler fails to preempt or context switch effectively.
  - Cache Thrashing: Rapid swapping of KV caches due to fluctuating memory pressure, reducing throughput to near zero.
- **First 3 experiments**:
  1. **Baseline Reproduction**: Run the GPT-J/Vicuna experiments against vLLM-FCFS to reproduce the 25.5% JCT reduction (Fig 4/5).
  2. **Ablation on Cache Manager**: Isolate the scheduler by fixing the KV cache policy to "Preserve" to measure the gains from Stateful-MLFQ alone (Fig 6).
  3. **Stress Test**: Increase QPS until JCT spikes (knee of the curve) to find the breaking point of Astraea vs. baselines (Appendix C analysis).

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in a dedicated section. However, based on the discussion and limitations, the following questions are implied:

1. How does the performance of Astraea degrade under imperfect service time predictions, specifically regarding the accuracy of token generation length and API latency?
2. To what extent does the use of statistical mean latency for API calls fail to capture the impact of heavy-tailed API response distributions?
3. Can the centralized, state-aware scheduling architecture of Astraea scale effectively to distributed, multi-node environments?

## Limitations

- The system's performance is heavily dependent on the accuracy of service time predictions, particularly for API latencies which are noted as "highly unpredictable."
- The adaptive KV cache manager introduces additional complexity and state tracking, which could lead to cache thrashing under rapid memory pressure fluctuations.
- The evaluation is limited to a single-node setup with vLLM, and the generalizability to distributed environments or other LLM serving systems is not demonstrated.

## Confidence

- **High Confidence**: The core architectural contributions (Stateful-MLFQ with dynamic priority migration, adaptive KV cache manager) are clearly specified and the experimental results are robust within the tested environment.
- **Medium Confidence**: The effectiveness of the prediction components (Service Time Predictor) is inferred from the results but not directly isolated in experiments.
- **Low Confidence**: The system's behavior under extreme conditions (e.g., GPU memory exhaustion, API failure cascades, or very high QPS) is not thoroughly characterized.

## Next Checks

1. **Prediction Robustness Test**: Deploy Astraea in a staging environment with variable API latencies (e.g., by introducing artificial delays or failures). Measure the scheduler's ability to adapt to prediction errors and maintain JCT improvements.
2. **Memory Pressure Stress Test**: Gradually increase the QPS until GPU memory is exhausted. Record the frequency of cache swaps/discards and measure the resulting throughput drop. Compare this to the baseline vLLM to quantify the overhead of the adaptive manager.
3. **Portability Validation**: Attempt to integrate Astraea's scheduling logic into a different LLM serving system (e.g., FasterTransformer). Document the required modifications and benchmark the JCT performance against the original vLLM-based implementation.