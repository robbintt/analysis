---
ver: rpa2
title: OAK -- Onboarding with Actionable Knowledge
arxiv_id: '2507.02914'
source_url: https://arxiv.org/abs/2507.02914
tags:
- knowledge
- operators
- information
- online
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OAK addresses the critical issue of knowledge loss when experienced
  operators leave, particularly in high-precision manufacturing. The system combines
  knowledge graph embeddings with multimodal interfaces to capture, store, and retrieve
  expertise.
---

# OAK -- Onboarding with Actionable Knowledge

## Quick Facts
- arXiv ID: 2507.02914
- Source URL: https://arxiv.org/abs/2507.02914
- Reference count: 22
- One-line primary result: Image-based defect search achieved weighted F1-score of 0.90

## Executive Summary
OAK (Onboarding with Actionable Knowledge) addresses knowledge loss in manufacturing when experienced operators leave by capturing, storing, and retrieving expertise through a multimodal interface. The system uses knowledge graph embeddings to enable semantic similarity search across expert knowledge, allowing operators to access relevant information through text, audio, or image inputs. A proof-of-concept tablet application was developed for quality control in aluminum plate manufacturing, integrating automatic defect recognition via Xception and achieving strong image classification performance.

## Method Summary
OAK combines knowledge graph embeddings with multimodal interfaces to collect and retrieve manufacturing expertise. The system uses Rebel for triplet generation from raw text, distiluse-base-multilingual-cased-v1 for text embeddings, and Neo4j for graph database storage following a custom ontology (Defect → Category → Machine → ImageID). A tablet application serves as the front-end with AR overlay, speech/image recognition, and rating system. The architecture includes a Main Manager orchestrator, Media Database for raw files, Knowledge Extractor for triplet generation, and Knowledge Base for graph storage.

## Key Results
- Image-based defect search achieved weighted F1-score of 0.90
- Text/audio-based search yielded top-5 accuracy of 0.5 and top-10 accuracy of 0.67
- User experience evaluation scored 1.69 overall (pragmatic quality 1.42, hedonic quality 1.96)
- Top-1 accuracy for text/audio retrieval was 0.19, indicating vocabulary mismatch between expert and novice descriptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge graph embeddings enable semantic similarity search across unstructured expert knowledge.
- Mechanism: Text embedding models encode defect descriptions and user queries into continuous vector spaces. Cosine similarity between query vectors and stored context vectors retrieves the most relevant matches. The graph structure preserves entity relationships.
- Core assumption: Expert descriptions and novice queries occupy similar regions in the embedding space when semantically equivalent.
- Evidence anchors: Combines knowledge graph embeddings and multi-modal interfaces; tested four text embedding models; sorted contexts by similarity scores.
- Break condition: If domain vocabulary differs significantly between experts (technical jargon) and novices (lay descriptions), similarity scores degrade.

### Mechanism 2
- Claim: Structured triplet extraction from raw media enables actionable knowledge capture without manual curation.
- Mechanism: Rebel processes raw text (transcribed from audio/video or direct input) and generates entity-relation-entity triplets that populate the Neo4j graph database following a predefined ontology.
- Core assumption: Rebel's triplet extraction quality is sufficient for the manufacturing domain without domain-specific fine-tuning.
- Evidence anchors: Evaluated nine models; retained Rebel because it got good scores and can directly generate triplets; Knowledge Extractor processes raw media into triplet form.
- Break condition: If expert instructions contain implicit context or visual demonstrations without corresponding verbalization, triplet extraction will be incomplete.

### Mechanism 3
- Claim: LLM-augmented query interpretation bridges the gap between novice language and technical knowledge base terminology.
- Mechanism: An LLM submodule within the Main Manager interprets user queries, translates them into graph-traversable representations, and summarizes retrieved information into contextualized answers.
- Core assumption: The LLM can map natural language queries to graph traversal patterns without hallucinating non-existent entities.
- Evidence anchors: Leverages LLMs to improve query understanding and provide adapted answers; LLM submodule within Main Manager interprets and translates users' queries.
- Break condition: If queries require multi-hop reasoning across the graph without explicit relationship paths, the LLM may fail to retrieve correct information.

## Foundational Learning

- Concept: Knowledge Graph Embeddings (KGE)
  - Why needed here: Core retrieval mechanism; you must understand how entities/relations map to vectors and how similarity search returns ranked results.
  - Quick check question: Given a triplet (DefectA, belongs_to, CategoryB), can you explain how this becomes a vector and how a query "scratches on surface" might retrieve it?

- Concept: Neo4j and Cypher query language
  - Why needed here: The Knowledge Base uses Neo4j; debugging retrieval requires reading/writing graph queries.
  - Quick check question: Write a Cypher query to find all defects associated with MachineX ordered by severity.

- Concept: Multimodal fusion/fission in interface design
  - Why needed here: The interface merges text, audio, and video inputs (fusion) and outputs via the most suitable modality (fission). Industrial constraints (PPE, noise) drive these choices.
  - Quick check question: In a noisy factory environment, which output modality should fission prioritize, and how would you handle conflicting inputs (audio says "minor scratch" but image shows "deep gouge")?

## Architecture Onboarding

- Component map:
  - Main Manager -> Media Database -> Knowledge Extractor -> Knowledge Base -> Multimodal Interface
  - Main Manager also invokes LLM submodule for query interpretation

- Critical path:
  1. Expert records knowledge (video/audio/text) → Media DB stores raw file
  2. Knowledge Extractor processes transcribed text → triplets generated
  3. Triplets inserted into Neo4j following ontology
  4. Novice queries (text/audio/image) → embedding computed → similarity search against KB
  5. LLM summarizes results → Interface displays via appropriate modality (AR overlay for measurements)

- Design tradeoffs:
  - Chose distiluse-base-multilingual-cased-v1 over larger models for deployment efficiency
  - Used only open-source models to meet industrial data safety requirements
  - Fine-tuned Xception on small, unbalanced dataset achieving 0.90 F1 but uncertain generalization

- Failure signatures:
  - Low Top-1 text/audio accuracy (0.19) due to vocabulary gap between novice and expert descriptions
  - Confusion matrix shows inter-class confusion indicating insufficient training data or feature overlap
  - Users report "confident" score of only 3.83/5 suggesting interface clarity issues under real conditions

- First 3 experiments:
  1. Validate embedding quality for your domain with benchmark of 20-50 entities; measure Top-1, Top-5, Top-10 accuracy.
  2. Test triplet extraction on 10 real expert recordings; verify precision/recall manually.
  3. Stress-test multimodal fusion under shop-floor constraints; measure task completion time and error rate.

## Open Questions the Paper Calls Out
- Can incorporating novice-generated descriptions alongside expert descriptions improve text-based defect retrieval accuracy?
- Does incorporating video and other media types improve defect severity assessment accuracy compared to static image-based analysis?
- How well does the OAK system generalize to other manufacturing domains with different defect taxonomies?
- Does sustained use of the OAK system lead to measurable improvements in novice operators' independent decision-making capability over time?

## Limitations
- Small evaluation sample (12 participants) limits statistical significance of usability results
- Small, unbalanced dataset (28 defects) raises questions about classifier robustness to novel defect types
- Vocabulary gap between novice descriptions and expert terminology resulted in low top-1 accuracy (0.19)

## Confidence
- High confidence: Technical architecture (Neo4j graph, Rebel triplet extraction, multimodal interface) is well-specified and reproducible
- Medium confidence: Text/audio retrieval metrics may not reflect real-world performance due to controlled evaluation context
- Low confidence: LLM submodule's effectiveness cannot be assessed without knowing which model was used

## Next Checks
1. Create benchmark with 20-50 domain-specific entities; have experts and novices describe each; measure retrieval accuracy.
2. Process 10 real expert video/audio recordings through Rebel; manually verify triplet accuracy (precision/recall).
3. Simulate shop-floor conditions (noise, gloves, poor lighting); measure task completion time and error rate for different input modalities.