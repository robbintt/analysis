---
ver: rpa2
title: On Arbitrary Predictions from Equally Valid Models
arxiv_id: '2507.19408'
source_url: https://arxiv.org/abs/2507.19408
tags:
- predictions
- multiplicity
- predictive
- performance
- rashomon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates model multiplicity in medical imaging,
  where multiple models can perform equally well but produce conflicting predictions
  for the same patient. The authors train 50 model instances per configuration across
  five medical datasets and four architectures, varying only the random seed.
---

# On Arbitrary Predictions from Equally Valid Models

## Quick Facts
- arXiv ID: 2507.19408
- Source URL: https://arxiv.org/abs/2507.19408
- Reference count: 40
- Primary result: Model multiplicity in medical imaging leads to arbitrary predictions; ensembling with abstention strategies can eliminate this problem

## Executive Summary
This study investigates model multiplicity in medical imaging, where multiple models can perform equally well but produce conflicting predictions for the same patient. The authors train 50 model instances per configuration across five medical datasets and four architectures, varying only the random seed. They find that validation performance is an unreliable predictor of test performance, and that a substantial proportion of predictions are arbitrary when using a single model. Ensembling with abstention strategies can effectively eliminate measurable predictive multiplicity, achieving perfect agreement among ensemble members while deferring uncertain cases to expert review. Higher model capacity that improves accuracy also reduces predictive multiplicity. The results demonstrate that relying on a single model is problematic in medical settings and advocate for ensemble-based approaches to improve diagnostic reliability and reduce arbitrary predictions.

## Method Summary
The authors investigate model multiplicity by training 50 model instances per configuration across five medical datasets (chest X-rays, skin cancer, breast cancer, and two other medical imaging tasks) and four different architectures. They systematically vary only the random seed while keeping all other hyperparameters constant. The study compares single-model performance with various ensembling strategies, including majority voting and abstention thresholds. The abstention approach involves deferring cases where ensemble members disagree beyond a specified threshold (50% agreement used as default). The authors also examine the relationship between model capacity and both accuracy and predictive multiplicity.

## Key Results
- Validation performance poorly predicts test performance across multiple model instances trained with different random seeds
- A substantial proportion of individual model predictions are arbitrary when multiple equally valid models disagree
- Ensembling with abstention strategies can effectively eliminate measurable predictive multiplicity, achieving perfect agreement among ensemble members while deferring uncertain cases to expert review
- Higher model capacity that improves accuracy also reduces predictive multiplicity

## Why This Works (Mechanism)
The mechanism underlying predictive multiplicity appears to be the presence of multiple decision boundaries in the model's hypothesis space that achieve similar validation performance but diverge on test data. When training multiple instances with different random seeds, each model finds a different local optimum in the loss landscape that is equally valid according to validation metrics. These different optima correspond to different decision boundaries that agree on validation data but disagree on test data. Ensembling with abstention works by identifying regions of the input space where models disagree (high epistemic uncertainty) and deferring these cases to human experts, effectively partitioning the decision space into regions of high and low model consensus.

## Foundational Learning
The study reveals that validation performance alone is insufficient for ensuring reliable predictions in medical imaging, as it fails to capture the underlying multiplicity of equally valid models. This suggests that traditional single-model training paradigms may be inadequate for high-stakes applications where arbitrary predictions can have serious consequences. The finding that higher model capacity reduces both error rates and predictive multiplicity indicates that more expressive models can better capture the true underlying decision boundaries, reducing the number of equally valid alternatives in the hypothesis space. This has implications for how we evaluate and deploy machine learning models in clinical settings.

## Architecture Onboarding
The results suggest that when onboarding new architectures for medical imaging tasks, practitioners should consider not just accuracy metrics but also the degree of predictive multiplicity across multiple training runs. Architectures with higher capacity appear to be preferable not only for their accuracy benefits but also for their ability to reduce arbitrary predictions. The study's findings indicate that a single architecture trained multiple times may reveal important characteristics about its robustness and reliability that are not captured by traditional validation metrics. This suggests that architecture selection should include evaluation of predictive multiplicity as a key criterion, particularly for clinical applications.

## Open Questions the Paper Calls Out
- How does predictive multiplicity manifest across different medical imaging modalities beyond the five datasets studied?
- What is the optimal abstention threshold for balancing diagnostic accuracy with clinical workflow efficiency?
- How can ensemble-based approaches be efficiently deployed in resource-constrained clinical settings?
- What are the long-term impacts of deferring uncertain cases to expert review on overall diagnostic accuracy and patient outcomes?

## Limitations
- The study relies heavily on random seed variations as the primary source of model multiplicity, which may not capture all real-world sources of model disagreement in clinical practice
- The analysis focuses on five medical datasets and four architectures, which, while diverse, may not be representative of all medical imaging scenarios
- The abstention threshold of 50% agreement may not be optimal for all clinical contexts, and the study does not explore how this threshold affects diagnostic utility
- The computational cost of training 50 models per configuration may limit practical implementation in resource-constrained settings
- The study does not investigate how model multiplicity behaves with different optimization algorithms or learning rate schedules
- The clinical impact of deferring cases to expert review is not quantified in terms of diagnostic delay or resource utilization

## Confidence
- High confidence: The observation that validation performance poorly predicts test performance across multiple model instances. The finding that ensembling with abstention strategies effectively reduces predictive multiplicity. The relationship between model capacity and both accuracy and predictive multiplicity.
- Medium confidence: The generalizability of results across different medical imaging tasks and architectures. The optimal abstention threshold for clinical implementation. The practical feasibility of deploying ensemble-based approaches in real clinical workflows.

## Next Checks
1. Validate findings across additional medical imaging datasets and tasks, particularly those with different imaging modalities and diagnostic requirements, to assess generalizability beyond the five datasets studied.
2. Conduct a clinical impact analysis comparing single-model versus ensemble-with-abstention approaches on actual patient outcomes, including time-to-diagnosis and false-negative rates when cases are deferred to expert review.
3. Investigate alternative ensembling strategies beyond simple voting, such as weighted averaging or Bayesian model averaging, to determine if they can achieve similar or better performance with fewer ensemble members.