---
ver: rpa2
title: Revisiting Transformers through the Lens of Low Entropy and Dynamic Sparsity
arxiv_id: '2504.18929'
source_url: https://arxiv.org/abs/2504.18929
tags:
- distribution
- entropy
- ptgt
- arxiv
- sparsity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores how transformers compress information and exhibit
  dynamic sparsity through controlled experiments. It finds that transformers prefer
  learning lower-entropy distributions than the target, with larger models showing
  this bias more strongly, driven by the FFN module.
---

# Revisiting Transformers through the Lens of Low Entropy and Dynamic Sparsity

## Quick Facts
- arXiv ID: 2504.18929
- Source URL: https://arxiv.org/abs/2504.18929
- Reference count: 22
- Primary result: Transformers compress information and exhibit dynamic sparsity through FFN-driven low-entropy bias and residual path dominance

## Executive Summary
This paper investigates fundamental inductive biases in transformers through controlled synthetic experiments. The authors discover that transformers systematically learn lower-entropy distributions than their targets, with larger models showing stronger compression bias driven by the FFN module rather than attention. They also reveal that larger transformers increasingly bypass attention computations via residual connections and exhibit discontinuous neuron death events correlated with training instability in second-order optimizers.

## Method Summary
The authors create a fully controllable synthetic language modeling task with known target entropy, training auto-regressive transformers on a 5-character vocabulary with 5-character sequences. They enumerate all 3125 possible sequences to compute exact entropy and KL divergence, comparing full transformers against attention-only and FFN-only variants. The study uses a custom routing mechanism to analyze path preferences and systematically varies model size (d=8 to 64) and optimizers (Adam vs SGD+momentum) to isolate architectural and optimization effects on distribution learning and sparsity patterns.

## Key Results
- Transformers prefer learning lower-entropy distributions than targets, with this bias strengthening in larger models
- The FFN module, not attention, drives the low-entropy compression behavior
- Larger transformers increasingly bypass attention via residual connections and exhibit fewer active neurons
- Second-order optimizers trigger discontinuous neuron death events correlated with loss spikes

## Why This Works (Mechanism)

### Mechanism 1: FFN-Driven Low-Entropy Inductive Bias
Transformers exhibit implicit regularization toward lower-entropy distributions, strengthening with model scale. The FFN module drives this compression, behaving as if minimizing L(θ) = E[-log pθ] + αH(pθ) where α scales with model size. Larger FFNs over-compress the learned distribution below target entropy while matching sparse pattern constraints.

### Mechanism 2: Residual Path Dominance in Scaled Attention
Larger transformers route computation through residual connections rather than attention heads. When treated as routable paths, larger models assign higher weights to residuals, creating shallower computation graphs per layer and effectively bypassing attention computation.

### Mechanism 3: Second-Order Gradient-Triggered Neuron Death
Optimizers with strong second-order gradient reliance (Adam, RMSprop) trigger phase-like neuron deactivation events. Dead neurons suddenly increase, followed by loss spikes, then re-stabilization. SGD with momentum shows neither effect, suggesting the FFN selectively "discards" neurons in jumps, re-optimizing remaining weights.

## Foundational Learning

- **Concept: Entropy and KL Divergence**
  - Why needed: Central to the claim that transformers prefer lower H(pθ) than H(ptgt), using KL(ptgt||pθ) to measure distribution mismatch
  - Quick check: If H(pθ) < H(ptgt), does this guarantee KL(ptgt||pθ) > 0? Explain why.

- **Concept: FFN as Key-Value Memory**
  - Why needed: FFN layers (W1, W2) are interpreted as key-value pairs where neurons activate sparsely, essential for understanding dynamic sparsity
  - Quick check: In FFN(x) = W2·ReLU(W1·x), which matrix acts as "keys" and which as "values"? What does a "dead neuron" mean in this formulation?

- **Concept: Adam Optimizer Moments**
  - Why needed: Links second-order gradient information (Adam's β2, second moment) to training instability and neuron death
  - Quick check: In Adam, what does the second moment estimate (v_t) approximate? How does high β2 (e.g., 0.999) affect optimizer behavior?

## Architecture Onboarding

- **Component map:** Attention module (multi-head + residual) -> FFN module (W1 keys, W2 values, dh=4d) -> Residual connections (identity shortcuts)
- **Critical path:** 1) Generate controlled ptgt with known H(ptgt) 2) Train transformer variants while logging H(pθ) and KL(ptgt||pθ) 3) Monitor FFN neuron activation across all inputs 4) Compare optimizers for loss spike/neuron death correlation
- **Design tradeoffs:** Larger models compress more aggressively but deviate from targets; Adam enables faster convergence but introduces instability risk; strong residual preference may limit representational capacity
- **Failure signatures:** Loss spike + sudden dead neuron increase indicates second-order gradient-triggered sparsity jump; H(pθ) << H(ptgt) with high KL means over-compression; uniform path weights suggests model too small
- **First 3 experiments:** 1) Train identical transformers at d ∈ {8,16,32,64} on same ptgt; plot H(pθ) vs epoch 2) Run attention-only, FFN-main, and full transformer; compare final H(pθ) and KL 3) Train large transformer (d=64) with Adam vs SGD+momentum; log loss and dead neuron proportion

## Open Questions the Paper Calls Out
1. Can the Transformer's preference for learning lower-entropy distributions be formalized as a precise implicit regularization term?
2. What is the precise mechanism that triggers the jump-like formation of dynamic sparsity in FFN modules?
3. Which specific architectural components of Transformers contribute to the loss spikes associated with dynamic sparsity?
4. Does the low-entropy preference persist in large-scale language models where exact entropy calculation is computationally infeasible?

## Limitations
- Findings rely on controlled synthetic distributions with known entropy, raising generalization questions to natural language
- Routing mechanism is an experimental proxy rather than intrinsic property of standard transformers
- Second-order gradient hypothesis lacks direct external validation and causality direction is unclear

## Confidence
- **High:** Empirical observation that transformers exhibit lower entropy than targets on synthetic distributions
- **Medium:** Larger models show stronger low-entropy bias (synthetic settings only); residual path dominance depends on routing proxy
- **Low:** Second-order gradient hypothesis for neuron death and loss spikes lacks external validation

## Next Checks
1. Train same transformer architectures on real language modeling task (e.g., WikiText-2) and estimate whether learned distributions show systematic entropy compression
2. Apply routing analysis to standard transformer implementations without routing mechanism; monitor attention weight distributions and residual usage directly
3. Train attention-only transformers on synthetic task using Adam vs SGD+momentum; if loss spikes persist, confirms factors beyond FFN sparsity contribute to instability