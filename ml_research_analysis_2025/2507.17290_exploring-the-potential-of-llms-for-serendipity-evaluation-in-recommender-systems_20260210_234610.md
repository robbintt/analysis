---
ver: rpa2
title: Exploring the Potential of LLMs for Serendipity Evaluation in Recommender Systems
arxiv_id: '2507.17290'
source_url: https://arxiv.org/abs/2507.17290
tags:
- serendipity
- user
- data
- llms
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  serve as evaluators for serendipity in recommender systems. The study uses two user-study-validated
  datasets from e-commerce and movie domains and introduces a meta-evaluation framework
  (SerenEva) to measure alignment with human judgments.
---

# Exploring the Potential of LLMs for Serendipity Evaluation in Recommender Systems

## Quick Facts
- **arXiv ID:** 2507.17290
- **Source URL:** https://arxiv.org/abs/2507.17290
- **Reference count:** 40
- **Primary result:** Zero-shot and few-shot LLMs outperform conventional proxy metrics in evaluating serendipity, achieving Pearson correlation coefficients exceeding 20% with human judgments.

## Executive Summary
This paper investigates whether large language models (LLMs) can serve as evaluators for serendipity in recommender systems. The study uses two user-study-validated datasets from e-commerce and movie domains and introduces a meta-evaluation framework (SerenEva) to measure alignment with human judgments. Results show that even basic zero-shot and few-shot LLMs outperform conventional proxy metrics, with Pearson correlation coefficients exceeding 20% when compared to user studies. Incorporating auxiliary data (e.g., user curiosity, item similarity) and multi-LLM techniques further improves evaluation accuracy. The findings suggest LLMs offer a promising, cost-effective approach for serendipity evaluation, combining the accuracy of user studies with the efficiency of automated metrics.

## Method Summary
The study evaluates LLMs as serendipity assessors using two datasets: Taobao Serendipity (e-commerce) and Serendipity-2018 (movies). LLMs are prompted to rate serendipity on a 5-point scale given user history and item information. The meta-evaluation framework (SerenEva) measures alignment with human judgments using Pearson correlation, MAE, and RMSE. Experiments test zero-shot vs. few-shot prompting, auxiliary data incorporation (curiosity, popularity, demographics), and multi-LLM ensembles. Performance is benchmarked against conventional proxy metrics (SOG, SNPR, PURS, DESR).

## Key Results
- Zero-shot and few-shot LLMs outperform conventional proxy metrics in correlation with human judgments (PCC > 20% vs. ~10-15% for baselines)
- Auxiliary data improves accuracy: curiosity for Taobao (+2.03% PCC), popularity for Movies (+4.02% PCC)
- Multi-LLM ensembles reduce RMSE and increase robustness across domains
- Shorter user history (10 items) yields better performance than longer contexts (>10 items degrades correlation)

## Why This Works (Mechanism)

### Mechanism 1
LLMs outperform rigid algorithmic formulas by semantically simulating the user's "surprise" reaction rather than calculating it via fixed heuristics. Conventional metrics use weighted combinations of relevance and unexpectedness, while LLMs leverage semantic reasoning to interpret relationships between user history and recommended items, modeling the subjective experience of "pleasant surprise." Core assumption: LLMs have sufficient embedded world knowledge to distinguish "irrelevant" from "unexpected but relevant" without fine-tuning. Break condition: Performance degrades if the LLM lacks specific domain knowledge or if the serendipity definition diverges from the user's mental model.

### Mechanism 2
Incorporating auxiliary data enhances evaluation accuracy by providing domain-specific boundary conditions. Serendipity is subjective, and by injecting user traits (like curiosity) or item traits (like popularity), the LLM conditions its probability space. For instance, a curious user might rate an unexpected item higher; an unpopular movie might feel more serendipitous. Core assumption: LLMs can map explicit numerical or categorical auxiliary data into their internal representation of a "user persona." Break condition: Indiscriminate data injection fails; adding interaction "type" (click vs. purchase) hurt performance in Taobao because LLMs struggled to model the complex relationship between interaction mode and serendipity without fine-tuning.

### Mechanism 3
Multi-LLM ensembles improve robustness by averaging out individual model biases and hallucinations regarding subjective preference. Different models have different "opinions" or reasoning paths, and averaging their scores acts as a variance reduction technique, smoothing out errors when interpreting ambiguous "surprise" scenarios. Core assumption: The errors of individual LLMs are not perfectly correlated; one model's error is likely offset by another's accuracy. Break condition: Ensembling yields diminishing returns if selected models are too similar or share common failure modes in understanding specific domain semantics.

## Foundational Learning

- **Concept: Serendipity vs. Accuracy**
  - **Why needed here:** Standard recommender metrics optimize for safe, relevant predictions. This paper targets "pleasant surprise," which inherently conflicts with pure accuracy. You must understand that optimizing for serendipity often requires accepting lower immediate relevance scores.
  - **Quick check question:** If a user loves action movies and is recommended a documentary they end up loving, is that high accuracy or high serendipity?

- **Concept: Meta-Evaluation (SerenEva)**
  - **Why needed here:** The paper does not train a recommender; it evaluates *how well* an evaluator works. You need to grasp that the "ground truth" comes from expensive user studies, and the goal is to correlate LLM scores with these human scores (Pearson correlation).
  - **Quick check question:** Why is Pearson correlation a better metric for this meta-evaluation than simple accuracy?

- **Concept: Zero-Shot vs. Few-Shot Prompting**
  - **Why needed here:** The performance gain from few-shot (providing 5 examples) over zero-shot is a key result. This shows how the LLM "learns" the evaluation task dynamically during inference without weight updates.
  - **Quick check question:** How does providing 5 examples of rated interactions change the LLM's output compared to giving it no examples?

## Architecture Onboarding

- **Component map:** Data Loader -> Prompt Constructor -> LLM Evaluators -> SerenEva Calculator
- **Critical path:** The construction of the **User Profile Prompt**. Section 4 suggests that trimming history to "short-term" (Taobao) or "long-term" (Movies) is the critical lever for performance, more so than raw model size in some cases.
- **Design tradeoffs:**
  - **Context Window vs. Noise:** Including the last 50+ interactions degrades performance; shorter context windows (10 items) yield higher correlation.
  - **Cost vs. Correlation:** GPT-4 offers high correlation but at higher cost. Section 3.2 suggests Qwen2.5-7B with few-shot prompting is a cost-efficient alternative with "impressive performance."
- **Failure signatures:**
  - **Domain Mismatch:** Using LLaMA2 for Chinese e-commerce data resulted in near-zero correlation.
  - **Over-conditioning:** Adding "interaction type" (click/purchase) for Taobao reduced performance, likely because the LLM couldn't resolve the complex relationship between interaction mode and subjective serendipity.
- **First 3 experiments:**
  1. **Baseline Correlation Test:** Run zero-shot prompts for a single domain (e.g., Movies) using the "Basic Prompt" against the Serendipity-2018 dataset to establish a Pearson baseline.
  2. **Auxiliary Ablation:** Add "User Curiosity" data to the prompt for the Taobao dataset and measure the delta in correlation vs. the baseline.
  3. **Ensemble Verification:** Run the evaluation using a Multi-LLM setup (e.g., GPT-4 + Qwen) and apply the score averaging strategy to verify if RMSE decreases (indicating error smoothing).

## Open Questions the Paper Calls Out
- **Open Question 1:** How can the explainability of serendipity be integrated into LLM-based evaluation frameworks? [explicit] The conclusion explicitly states a future aim to "investigate the explainability of serendipity."
- **Open Question 2:** What advanced reasoning techniques are required to surpass the observed correlation ceiling (~21%) with human judgments? [inferred] While the paper celebrates outperforming baselines, the optimal configuration still yields a Pearson correlation of only 21.5%, leaving significant variance in human perception unexplained.
- **Open Question 3:** How can the selection of auxiliary data be automated to optimize evaluation across different recommendation domains? [inferred] The authors found that the utility of auxiliary data is domain-dependent, and indiscriminate inclusion can harm performance, but there is no proposed mechanism for dynamically selecting the most relevant auxiliary features.

## Limitations
- Performance improvements rely on implicit assumptions about LLM "reasoning" that are not fully explained mechanistically.
- The study shows correlation with human judgments but does not validate whether the LLM's "surprise" assessment aligns with actual cognitive processes of users.
- Benefit of multi-LLM ensembling is demonstrated empirically but lacks theoretical justification for why variance reduction applies to subjective preference modeling.

## Confidence
- **High:** LLMs outperform conventional proxy metrics (SOG, SNPR) in correlation with user studies (based on direct comparison with established datasets).
- **Medium:** Auxiliary data consistently improves accuracy across domains (some auxiliary data types, like interaction "type," showed domain-specific failures).
- **Medium:** Multi-LLM ensembles reduce error (empirically demonstrated, but mechanism and cost-effectiveness not fully explored).

## Next Checks
1. **Ablation Study:** Systematically test the impact of each auxiliary data type (Curiosity, Popularity, Big-Five traits) in isolation to confirm which dimensions genuinely enhance serendipity evaluation.
2. **Out-of-Domain Generalization:** Evaluate the zero-shot model on a completely different domain (e.g., news recommendations) to test the limits of LLM background knowledge for serendipity.
3. **User Study Validation:** Conduct a small-scale user study to verify that the LLM's predicted serendipity scores correlate with actual post-recommendation user satisfaction, not just with historical survey labels.