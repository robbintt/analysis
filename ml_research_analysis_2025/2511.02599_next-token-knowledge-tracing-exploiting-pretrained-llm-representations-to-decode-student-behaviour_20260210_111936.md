---
ver: rpa2
title: 'Next Token Knowledge Tracing: Exploiting Pretrained LLM Representations to
  Decode Student Behaviour'
arxiv_id: '2511.02599'
source_url: https://arxiv.org/abs/2511.02599
tags:
- knowledge
- tracing
- question
- ntkt
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Next Token Knowledge Tracing (NTKT), a novel
  method that reformulates knowledge tracing as a next-token prediction task using
  large language models. By representing student interaction histories and question
  text as natural language sequences, NTKT leverages pretrained LLM representations
  to improve predictive accuracy and generalisation in educational settings.
---

# Next Token Knowledge Tracing: Exploiting Pretrained LLM Representations to Decode Student Behaviour

## Quick Facts
- arXiv ID: 2511.02599
- Source URL: https://arxiv.org/abs/2511.02599
- Reference count: 11
- Primary result: NTKT achieves F1 85.22% and AUC 90.32% on Eedi dataset, significantly outperforming deep learning baselines

## Executive Summary
This paper introduces Next Token Knowledge Tracing (NTKT), a novel method that reformulates knowledge tracing as a next-token prediction task using large language models. By representing student interaction histories and question text as natural language sequences, NTKT leverages pretrained LLM representations to improve predictive accuracy and generalisation in educational settings. Experiments on a large-scale mathematics dataset demonstrate that NTKT significantly outperforms existing deep learning baselines, achieving an F1 score of 85.22% and an AUC of 90.32%. Ablation studies confirm that full question text is crucial for performance, while cold-start experiments show strong generalisation to unseen students and questions.

## Method Summary
NTKT reformulates knowledge tracing as a language modelling task, where student performance prediction is set up as a causal language modelling objective. The method serializes student interaction histories and target question content into XML-tagged text sequences, allowing an LLM to process them autoregressively and predict correctness tokens. To isolate the predictive signal, NTKT employs selective loss masking, computing loss only at positions containing "Correct" or "Incorrect" tokens while preserving contextual attention. The approach fine-tunes LLaMA 3.2 (1B/3B) with LoRA parameter-efficient adapters, achieving strong generalization to unseen questions and students while maintaining high predictive accuracy.

## Key Results
- NTKT achieves F1 score of 85.22% and AUC of 90.32% on the Eedi mathematics dataset
- Cold-start experiments show NTKT maintains stable performance (0.843) on both seen and unseen questions, while baselines drop significantly (0.777 → 0.732, p < 0.001)
- Ablation studies demonstrate that full question text is essential, with AUC dropping from 90.32% to 77.06% when text is removed

## Why This Works (Mechanism)

### Mechanism 1
Representing student interactions as natural language sequences enables joint modeling of behavioral patterns and question semantics through a unified language modeling objective. Student interaction histories (question text + options + correctness) are serialized into XML-tagged text sequences. The LLM processes these autoregressively, learning to predict correctness tokens conditioned on the full textual context of prior interactions and the target question. This works because sequential patterns in student learning can be effectively captured through the same objective used for language modeling.

### Mechanism 2
Selective loss masking isolates the predictive signal to correctness tokens, reducing gradient noise from non-target tokens while preserving contextual attention. Loss is computed only at positions containing "Correct" or "Incorrect" tokens (enclosed in `<cr>` tags). All other positions receive the sentinel label -100 and contribute to attention but exert no gradient pressure. This aligns supervision with the binary classification objective, allowing the model to learn useful representations of history and question context through attention mechanisms even when those tokens receive no direct supervision signal.

### Mechanism 3
Pretrained LLM semantic representations enable robust generalization to unseen questions and students by encoding question similarity in embedding space. The pretrained LLM has learned rich semantic representations of natural language questions. When encountering an unseen question, its embedding reflects similarity to training distribution questions, enabling transfer of learned behavioral patterns without memorized ID mappings. This works because semantic similarity in question text correlates with similar student response patterns—questions about similar concepts with similar wording elicit similar knowledge states.

## Foundational Learning

- **Knowledge Tracing (KT)**
  - Why needed here: The fundamental task this paper reformulates—predicting student performance on future questions based on interaction history.
  - Quick check question: Given a student's past 10 question-answer pairs, can you explain what information a KT model uses to predict Q11 correctness?

- **Causal Language Modeling (Next-Token Prediction)**
  - Why needed here: The core pretraining objective being repurposed; understanding autoregressive prediction is essential to grasp why this reformulation works.
  - Quick check question: In a decoder-only LLM, how does the prediction of token t depend on tokens 1 through t-1?

- **Parameter-Efficient Fine-Tuning (LoRA)**
  - Why needed here: The paper uses Low-Rank Adaptation to make LLM fine-tuning tractable; understanding this explains the training setup.
  - Quick check question: Why would freezing Φ₀ and learning low-rank updates ΔΦ(Θ) be more efficient than full fine-tuning?

## Architecture Onboarding

- **Component map**: Data Preparation -> Tokenization -> LLM Backbone -> LoRA Adapters -> Selective Loss Masking
- **Critical path**: Prompt template design (must correctly structure history and target with XML tags) -> Masking logic (must identify only "Correct"/"Incorrect" tokens for loss computation) -> LoRA configuration (rank/α settings directly affect capacity and convergence)
- **Design tradeoffs**: Model scale (1B vs 3B): 3B achieves higher F1/AUC (85.22%/90.32%) but requires more GPU memory; Text inclusion: Full text yields best AUC (90.32%) vs ID-only (77.06%), but increases sequence length; Sequence length cap (15,000 tokens): Long enough for most student histories but may truncate very long sequences
- **Failure signatures**: AUC drops sharply when text removed (90.32% → 77.06%) → question text is essential; Performance degrades on cold-start questions (baselines drop ~6%, NTKT stable) → ID-based models fail without memorization; Validation loss plateaus early → possible masking error or insufficient LoRA rank
- **First 3 experiments**:
  1. Reproduce baseline comparison: Train DKT, AKT, AKT-text, DTransformer on Eedi with identical splits; verify NTKT achieves F1 > 82% and AUC > 89%
  2. Ablation on input features: Run NTKT with ID-only, Concept-only, and Full Text; confirm AUC degradation follows 77.06% → 84.59% → 90.32%
  3. Cold-start validation: Hold out 10 questions and subset of students; plot F1 vs timestep to verify NTKT personalizes faster than baselines and maintains stable question cold-start performance

## Open Questions the Paper Calls Out
- How does NTKT performance generalise to non-mathematics domains and multilingual educational settings? (The paper acknowledges it only tested on mathematics in English)
- What is the magnitude of performance degradation caused by 4-bit quantisation, and can it be mitigated without increasing computational cost? (The paper used 4-bit weights but didn't quantify the impact)
- Can attention patterns in NTKT be leveraged to extract interpretable pedagogical insights about student learning trajectories? (The paper identifies this as future work)
- How does NTKT performance scale with reduced training data, particularly for low-resource educational contexts? (The paper doesn't explore minimum data thresholds)

## Limitations
- The selective loss masking mechanism lacks direct empirical validation in knowledge tracing contexts, with the assumption that attention-only learning can work remaining unverified
- The 15,000 token sequence limit may truncate long student histories, potentially losing important temporal patterns, though truncation effects are not analyzed
- The paper doesn't analyze whether semantic similarity in question text actually correlates with similar difficulty or skill requirements, risking misleading predictions

## Confidence
- **High Confidence**: The baseline comparison results showing NTKT's superior F1 (85.22%) and AUC (90.32%) over existing deep learning methods. These metrics are directly measured and the experimental setup is well-specified.
- **Medium Confidence**: The cold-start generalization results, particularly the claim that NTKT maintains stable performance on unseen questions while baselines drop significantly. While the statistical significance is reported (p < 0.001 for question cold-start), the selection criteria for held-out questions are not fully specified.
- **Medium Confidence**: The ablation study demonstrating the importance of full question text, showing degradation from 90.32% to 77.06% AUC when text is removed. This result is compelling but assumes that question semantics correlate with student response patterns.

## Next Checks
1. **Selective Masking Validation**: Conduct an ablation where all tokens receive supervision versus only correctness tokens. Measure whether attention-only learning of history representations achieves comparable validation loss on correctness tokens.
2. **Semantic Similarity Analysis**: For the cold-start questions where NTKT maintains stable performance, compute actual semantic similarity scores between held-out and training questions. Test whether higher semantic similarity correlates with smaller performance degradation.
3. **Truncation Impact Study**: Systematically vary the maximum sequence length (e.g., 5K, 10K, 15K tokens) and measure the impact on both seen and unseen question performance.