---
ver: rpa2
title: 'Where It Moves, It Matters: Referring Surgical Instrument Segmentation via
  Motion'
arxiv_id: '2601.12224'
source_url: https://arxiv.org/abs/2601.12224
tags:
- surgical
- segmentation
- video
- expressions
- motion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SurgRef introduces motion-guided referring video segmentation for
  surgical instruments by grounding natural language expressions in instrument motion
  rather than static appearance. It uses a key-frame attention module to adaptively
  select expression-relevant frames and leverages motion-centric language descriptions
  to achieve fine-grained spatial-temporal segmentation.
---

# Where It Moves, It Matters: Referring Surgical Instrument Segmentation via Motion

## Quick Facts
- **arXiv ID:** 2601.12224
- **Source URL:** https://arxiv.org/abs/2601.12224
- **Authors:** Meng Wei; Kun Yuan; Shi Li; Yue Zhou; Long Bai; Nassir Navab; Hongliang Ren; Hong Joo Lee; Tom Vercauteren; Nicolas Padoy
- **Reference count:** 14
- **Primary result:** SurgRef achieves state-of-the-art surgical referring segmentation accuracy by grounding natural language expressions in instrument motion rather than static appearance.

## Executive Summary
SurgRef introduces motion-guided referring video segmentation for surgical instruments by grounding natural language expressions in instrument motion rather than static appearance. It uses a key-frame attention module to adaptively select expression-relevant frames and leverages motion-centric language descriptions to achieve fine-grained spatial-temporal segmentation. SurgRef is trained and evaluated on the Ref-IMotion dataset, which contains 21,350 frames with motion-based referring expressions across diverse surgical procedures. Experiments show SurgRef achieves state-of-the-art accuracy, such as 89.91 J on EndoVis-IM17 and 84.48 J&F on EndoVis-IM18, with strong zero-shot generalization to unseen datasets.

## Method Summary
SurgRef combines Swin Transformer backbone for visual features with frozen RoBERTa-base for text embeddings. The Mask2Former-based decoder uses language-initialized queries to predict masks. A key-frame selection module scores each frame's relevance to the referring expression and selects the top-8 frames. Inter-frame attention across selected frames produces the final segmentation masks. The model is trained with composite loss (cross-entropy + BCE + Dice + temporal similarity) for 100k iterations on 2x NVIDIA A100 GPUs.

## Key Results
- Achieves 89.91 J on EndoVis-IM17 and 84.48 J&F on EndoVis-IM18
- Strong zero-shot generalization to unseen datasets
- Motion-aware training improves performance even on static expressions (85.17 vs 74.25 J&F on appearance-based)
- Optimal key-frame selection at T'=8 balances efficiency with accuracy

## Why This Works (Mechanism)

### Mechanism 1: Motion-Centric Language Grounding
Grounding language in instrument motion rather than static appearance improves generalization under occlusion and terminology variation. Natural language expressions are paired with spatiotemporal motion descriptors (entry trajectories, retraction patterns, tool-tissue interactions). The model learns to associate linguistic motion semantics with visual motion patterns via transformer cross-attention. Motion signatures are more consistent across institutions and procedures than instrument appearance or naming conventions.

### Mechanism 2: Key-Frame Attention Module
Adaptive frame selection improves both efficiency and segmentation accuracy by suppressing redundant frames. Frame-level object queries from the decoder are aggregated into per-frame embeddings. A lightweight MLP scores each frame for relevance to the referring expression; only top-T' frames proceed to mask prediction. Expression-relevant visual content is sparse in surgical videos; most frames are redundant for grounding.

### Mechanism 3: Language-Initialized Object Queries
Injecting text embeddings into query initialization improves cross-modal alignment compared to generic learnable queries. Queries q(0)_i = W_init · F_text + b_i replace Mask2Former's random queries, conditioning spatial reasoning on language semantics from the first decoder layer. Early fusion of language into queries reduces ambiguity in object-token binding.

## Foundational Learning

- **Concept:** Video Object Segmentation (VOS) memory mechanisms (e.g., STM, XMem)
  - **Why needed here:** SurgRef builds on temporal propagation; understanding memory-based retrieval helps explain inter-frame attention.
  - **Quick check question:** Can you explain how STM uses past frames to guide current-frame segmentation?

- **Concept:** Vision-Language Grounding (CLIP-style alignment)
  - **Why needed here:** The core task requires aligning natural language with visual regions; grounding fundamentals clarify cross-modal attention.
  - **Quick check question:** How does contrastive training align image and text embeddings?

- **Concept:** Mask2Former query-based segmentation
  - **Why needed here:** SurgRef adapts Mask2Former's decoder with language-initialized queries; understanding the baseline is essential.
  - **Quick check question:** What role do object queries play in predicting class-agnostic masks?

## Architecture Onboarding

- **Component map:** Video frames + text expression → Swin Transformer + RoBERTa-base → Language-initialized queries → Transformer decoder → Key-frame selection MLP → Inter-frame attention → Mask prediction

- **Critical path:**
  1. Extract F_image per frame (Swin), F_text per expression (RoBERTa)
  2. Initialize queries from F_text (Equation 1)
  3. Decode with cross-attention to visual features
  4. Score frames; select top T'=8
  5. Apply inter-frame attention; predict masks via dot product with F_mask

- **Design tradeoffs:**
  - T' selection: Lower T' improves efficiency but risks missing key moments; paper shows T'=8 as optimal.
  - Frozen vs. fine-tuned text encoder: Frozen RoBERTa reduces overfitting risk; may limit domain adaptation.
  - Query count N2: Too few queries limits multi-instrument scenes; too many increases compute.

- **Failure signatures:**
  - Low relevance scores (st < 0.3 across all frames): Expression may be out-of-domain or motion absent.
  - High J&F drop between motion and non-motion expressions: Model over-reliant on specific motion cues; may need more diverse training.
  - Cross-style generalization collapse (Table 4a, origin→no location: 59.66): Model memorizing spurious cues rather than motion semantics.

- **First 3 experiments:**
  1. **Ablate key-frame selection:** Compare T'=4,8,16,24 on EV-IM17; verify Table 4b curve.
  2. **Cross-dataset zero-shot:** Train on GraSP-IM, test on CholecSeg8k-IM; measure J&F gap vs in-distribution.
  3. **Motion vs. static expression holdout:** Train with motion expressions held out; test on Table 3 splits to confirm motion-training benefit for static queries.

## Open Questions the Paper Calls Out

### Open Question 1
Can SurgRef's key-frame selection module maintain efficiency and temporal coherence when applied to continuous, full-length surgical procedures rather than pre-segmented clips? The methodology evaluates on curated video clips but doesn't demonstrate application to continuous, unsegmented surgical workflows. An evaluation of inference speed and segmentation consistency on full-length surgical videos without manual clip pre-selection would resolve this.

### Open Question 2
How does the model perform when distinguishing between multiple instruments executing identical or highly similar motion trajectories simultaneously? While motion helps disambiguate static appearance, the inverse problem—distinguishing identical motions—remains untested, potentially limiting robustness in complex bimanual tasks. An ablation study on scenes where multiple tools perform the same action would measure disambiguation accuracy.

### Open Question 3
Does the reliance on LLM-generated referring expressions for the GraSP-IM subset introduce a linguistic distribution shift that impacts performance on spontaneous human speech? The paper states expressions for GraSP-IM were generated by feeding structured inputs into a large language model, potentially creating a synthetic linguistic style distinct from natural surgeon commands. A cross-domain evaluation testing the model with human-generated, spoken referring expressions would validate generalization.

## Limitations
- Dataset generalization limited to laparoscopic procedures; lacks validation on robotic surgery or open surgical procedures
- Key-frame selection mechanism not thoroughly analyzed for expression types requiring longer temporal context
- Claims about motion signature consistency across institutions lack quantitative comparison to appearance consistency

## Confidence

- **High confidence:** Motion grounding mechanism improves performance on both motion and static expressions (Table 3). T'=8 optimal key-frame selection is well-supported (Table 4b).
- **Medium confidence:** Cross-dataset generalization claims are supported but limited to similar laparoscopic procedures. Frozen text encoder assumption may not hold for domain-specific terminology.
- **Low confidence:** Claims about motion signatures being more consistent than appearance across institutions lack quantitative evidence comparing consistency across different hospitals or surgical teams.

## Next Checks

1. **Cross-procedure generalization test:** Evaluate SurgRef on robotic surgery datasets (e.g., da Vinci surgical system videos) to validate motion-grounding benefits beyond laparoscopic procedures.

2. **Motion consistency analysis:** Measure motion descriptor consistency across different surgical teams performing the same procedure to validate the core assumption about motion signatures.

3. **Dynamic T' analysis:** Implement expression-dependent key-frame selection where T' varies based on expression temporal extent rather than fixed at 8 frames.