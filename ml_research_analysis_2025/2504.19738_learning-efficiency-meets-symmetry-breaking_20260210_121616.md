---
ver: rpa2
title: Learning Efficiency Meets Symmetry Breaking
arxiv_id: '2504.19738'
source_url: https://arxiv.org/abs/2504.19738
tags:
- graph
- state
- planning
- pruning
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of symmetry reduction in learning-based
  planning, which has not been fully exploited despite the potential of Graph Neural
  Networks (GNNs) to detect symmetries through permutation invariance. The authors
  introduce a Typed Instance Learning Graph (TILG) representation that extends prior
  work by incorporating object types and static propositions, and propose two pruning
  methods: action pruning and state pruning.'
---

# Learning Efficiency Meets Symmetry Breaking

## Quick Facts
- **arXiv ID**: 2504.19738
- **Source URL**: https://arxiv.org/abs/2504.19738
- **Authors**: Yingbin Bai; Sylvie Thiebaux; Felipe Trevizan
- **Reference count**: 18
- **Primary result**: Learning-based planner Distincter achieves state-of-the-art performance on 2023 IPC Learning Track, surpassing LAMA for first time in recent literature

## Executive Summary
This paper tackles the critical challenge of symmetry reduction in learning-based planning, where traditional approaches have failed to fully exploit the potential of Graph Neural Networks (GNNs) for symmetry detection. The authors introduce a Typed Instance Learning Graph (TILG) representation that incorporates object types and static propositions, and propose two novel pruning methods: action pruning and state pruning. These techniques enable the planner Distincter to achieve breakthrough performance on the 2023 International Planning Competition Learning Track, marking the first time a learning-based approach has surpassed traditional model-based planners like LAMA in recent literature.

## Method Summary
The authors extend Typed Instance Learning Graphs (TILGs) with object types and static propositions, then propose two symmetry pruning approaches. Action pruning identifies symmetries by analyzing object involvement in action parameters without generating child states, making it computationally efficient. State pruning exploits GNN embeddings to detect symmetries across states by comparing learned representations. These methods are implemented in the Distincter planner, which leverages GNN-based learning to predict symmetries and guide search, combining the strengths of learning-based efficiency with symmetry-breaking power.

## Key Results
- Distincter achieves state-of-the-art performance on 2023 IPC Learning Track
- First learning-based planner to surpass traditional model-based planner LAMA in recent literature
- Significant coverage improvements across multiple domains with low computational overhead
- Action pruning reduces symmetry detection to action parameter analysis without state generation
- State pruning uses GNN embeddings for efficient cross-state symmetry detection

## Why This Works (Mechanism)
The approach works by leveraging GNNs' inherent permutation invariance to detect symmetries through learned representations. TILGs provide a structured representation that captures object types and static propositions, enabling more precise symmetry detection. Action pruning identifies symmetries at the action level by analyzing parameter relationships, while state pruning propagates symmetry information through GNN embeddings across the search space. This combination allows Distincter to prune symmetric branches early without expensive state generation or comparison operations.

## Foundational Learning
- **Graph Neural Networks (GNNs)**: Why needed: To learn permutation-invariant representations for symmetry detection across states. Quick check: Verify GNN embeddings capture semantic similarity between symmetric states.
- **Typed Instance Learning Graphs (TILGs)**: Why needed: To provide structured representation incorporating object types and static propositions for precise symmetry analysis. Quick check: Confirm TILGs preserve all necessary information for valid symmetry detection.
- **Symmetry Detection in Planning**: Why needed: To reduce search space by identifying and pruning equivalent states and actions. Quick check: Ensure detected symmetries are valid (preserve solution correctness).
- **Action Parameter Analysis**: Why needed: To identify symmetries without expensive state generation operations. Quick check: Verify action pruning correctly identifies all symmetric action applications.
- **Permutation Invariance**: Why needed: To ensure symmetry detection is independent of object naming/order. Quick check: Test with different object permutations yielding same results.

## Architecture Onboarding

Component Map:
TILG Representation -> Action Pruning Module -> State Pruning Module -> Distincter Planner

Critical Path:
1. Encode planning problem into TILGs
2. Apply action pruning to identify symmetric actions
3. Generate successor states using symmetry-reduced actions
4. Apply state pruning using GNN embeddings to detect symmetric states
5. Continue search with reduced state space

Design Tradeoffs:
- **Action pruning vs State pruning**: Action pruning is faster but may miss some symmetries; state pruning is more comprehensive but computationally heavier
- **GNN complexity vs detection accuracy**: More complex GNNs may detect finer symmetries but increase computational overhead
- **TILG detail level vs generalization**: More detailed TILGs may capture domain-specific symmetries better but reduce transferability

Failure Signatures:
- **False positives in symmetry detection**: Can lead to pruning valid solution paths
- **High computational overhead**: May negate benefits of symmetry reduction
- **Poor generalization**: Performance drops significantly on domains different from training data

First Experiments:
1. Test action pruning alone on domains with clear action parameter symmetries
2. Test state pruning alone on domains where GNN embeddings should capture symmetries well
3. Run full Distincter on a simple IPC domain to verify end-to-end functionality

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- GNN-based symmetry detection may not generalize well to domains with complex or dynamic object relationships
- Action pruning effectiveness depends on sufficient training data availability for identifying action parameter symmetries
- State pruning assumes symmetries can be captured through learned representations, which may not hold for all planning domains
- Computational overhead of maintaining GNN embeddings during planning remains unquantified
- Performance gains over LAMA may not be consistent across all domain types or problem sizes

## Confidence
- **High**: The empirical results showing state-of-the-art performance on the 2023 IPC Learning Track
- **Medium**: The theoretical framework for symmetry detection through TILGs and GNN embeddings
- **Low**: The generalizability of the approach to domains with dynamic or complex object relationships

## Next Checks
1. Evaluate Distincter's performance on domains with dynamic object relationships and complex state transitions to assess generalizability beyond the IPC benchmarks
2. Conduct ablation studies to quantify the individual contributions of action pruning and state pruning to overall performance improvements
3. Measure the computational overhead of maintaining GNN embeddings during planning and compare it to traditional symmetry detection methods across varying problem sizes