---
ver: rpa2
title: Benefits of Learning Rate Annealing for Tuning-Robustness in Stochastic Optimization
arxiv_id: '2503.09411'
source_url: https://arxiv.org/abs/2503.09411
tags:
- stepsize
- rate
- learning
- convex
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the sensitivity of learning rate tuning
  in stochastic gradient descent to grid search resolution, focusing on the robustness
  of different learning rate scheduling schemes. The authors demonstrate that polynomially
  decaying learning rate schedules, including the widely-used cosine annealing, provide
  significantly improved robustness compared to fixed learning rates when the learning
  rate is coarsely tuned via grid search.
---

# Benefits of Learning Rate Annealing for Tuning-Robustness in Stochastic Optimization

## Quick Facts
- **arXiv ID**: 2503.09411
- **Source URL**: https://arxiv.org/abs/2503.09411
- **Reference count**: 40
- **Primary result**: Polynomially decaying learning rate schedules achieve sublinear dependence on multiplicative stepsize misspecification (O(ρ^(1/(2p+1))/√T)) versus linear dependence (O(ρ/√T)) for fixed stepsizes in stochastic optimization.

## Executive Summary
This paper investigates how different learning rate scheduling strategies respond to coarse grid search, demonstrating that polynomially decaying schedules (including cosine annealing) provide significantly improved robustness compared to fixed learning rates. The core theoretical insight is that annealing schedules automatically adapt to multiplicative overestimation of the optimal stepsize by reducing effective learning rates over time, achieving convergence rates with sublinear dependence on the misspecification factor. Empirical validation on synthetic logistic regression and CIFAR-10 image classification confirms that cosine and linear decay schedules demonstrate greater robustness to coarse grid searches, with substantially smaller performance degradation when grid resolution decreases.

## Method Summary
The method compares SGD with various learning rate schedules under multiplicative misspecification from coarse grid search. Three schedules are tested: cosine annealing (h(u)=½(1+cos(πu))), linear decay (h(u)=1-u), and polynomial decay (h(u)=(1-u)^p). For synthetic logistic regression, 100K samples × 100 dimensions are generated with 10% label noise, trained with batch size 1000 for 1 epoch. For CIFAR-10, a Wide ResNet 28-10 is trained for 200 epochs with batch 128, Nesterov momentum 0.9, and weight decay 0.0005. Grid search uses multiplicative spacing ~2.15 across values {1, 2.2, 5}×10^i. Robustness is measured as performance degradation versus grid multiplicative factor.

## Key Results
- Polynomially decaying schedules achieve convergence rate O(ρ^(1/(2p+1))/√T) versus O(ρ/√T) for fixed stepsizes
- Cosine annealing and linear decay demonstrate significantly better robustness to coarse grid searches in both synthetic and CIFAR-10 experiments
- Performance degradation is substantially smaller for annealed schedules when grid resolution decreases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decaying learning rate schedules automatically adapt to multiplicative overestimation of the optimal stepsize by reducing the effective learning rate over time.
- Mechanism: The algorithm uses a schedule ηt = ηh((t-1)/T) where h decays polynomially to zero. When the base learning rate η is overestimated by factor ρ ≥ 1, the decay schedule ensures that only the early iterations suffer from large steps, while later iterations operate with appropriately small steps—effectively finding a "sweet spot" automatically via the suffix argument.
- Core assumption: The schedule function h is monotonically non-increasing, differentiable, p-Lipschitz, and satisfies h(u) = 0 ⇔ u = 1 (i.e., truly annealed schedules).
- Evidence anchors:
  - [abstract] "demonstrating their increased robustness to initial parameter misspecification due to a coarse grid search"
  - [section 3.2] "any suffix of iterates xk, ..., xT+1 can be viewed as a (T-k+1)-steps SGD starting at xk, effectively ignoring the large stepsizes prior to step k"
  - [corpus] Weak direct evidence; related work on annealing strategies in LLM training (arxiv:2512.13705) suggests transferability but doesn't address this robustness mechanism.
- Break condition: If h(u) does not decay to zero (e.g., constant offset schedules), the suffix argument fails and linear ρ-dependence is restored.

### Mechanism 2
- Claim: The sublinear dependence on ρ emerges from optimizing over the starting point τ of a suffix trajectory, trading off accumulated gradient noise against stepsize misspecification.
- Mechanism: The bound involves infτ[1/(ρHh(τ)) + ρQh(τ)/Qh(0)] where Hh and Qh are schedule-dependent integrals. For polynomial decay h(u) = (1-u)^p, the optimal τ satisfies Hh(τ)H'h(τ) = -Hh(0)Qh(0)/ρ², yielding ρ^(1/(2p+1)) dependence rather than linear ρ.
- Core assumption: The schedule tail behavior determines the tradeoff; polynomial schedules with higher p provide better robustness but worse base convergence rates (O(√p)).
- Evidence anchors:
  - [abstract] "convergence rate ... O(ρ^(1/(2p+1))/√T) where p is the degree of polynomial decay"
  - [Corollary 2] "E[f(xT+1) - f(x*)] = Rate^tu_h,T · ρ^(1/(2p+1)) + O(pρη^tu G²/T)"
  - [corpus] No direct corpus evidence for this specific mathematical mechanism.
- Break condition: For non-annealed schedules or schedules with h(0) approaching zero, the infimum optimization yields τ = 0 and linear ρ-dependence.

### Mechanism 3
- Claim: In smooth optimization, the same robustness holds provided the fraction τ₀ of iterations exceeding the stability threshold 1/(2β) remains small.
- Mechanism: The bound restricts optimization to τ ∈ [τ₀, 1) where ηh(⌊τT⌋/T) ≤ 1/(2β). For sufficiently small τ₀ (i.e., initial steps not too large relative to smoothness), sublinear ρ-dependence is preserved.
- Core assumption: The base learning rate satisfies ηh(0) ≤ 1/(2β), or ρ is not so large that all iterations violate the stability condition.
- Evidence anchors:
  - [Theorem 4] "τ₀ ≜ min{τ ∈ [0,1) : ηh(⌊τT⌋/T) ≤ 1/(2β)}"
  - [Corollary 5] "if ρ² ≥ (1-τ₀)^(-(2p+1))... O(ρ^(1/(2p+1))); if ρ² < (1-τ₀)^(-(2p+1))... O(1/(1-τ₀))"
  - [corpus] Weak evidence; curvature-aware optimization work (arxiv:2505.02101) touches on stability but not this specific threshold.
- Break condition: If τ₀ approaches 1 (nearly all iterations violate stability), convergence guarantees degrade to O(1/(1-τ₀)) regardless of schedule.

## Foundational Learning

- Concept: **Stochastic Gradient Descent convergence rates**
  - Why needed here: The paper builds on standard SGD convergence (O(DG/√T) for convex Lipschitz, O(βD²/T + Dσ/√T) for convex smooth) and shows how misspecification affects these rates.
  - Quick check question: What is the optimal fixed stepsize for convex Lipschitz SGD, and how does performance degrade if you use a stepsize 10× larger?

- Concept: **Learning rate scheduling / annealing**
  - Why needed here: The core contribution concerns how different scheduling strategies (fixed, cosine, polynomial decay) respond to hyperparameter misspecification.
  - Quick check question: If you use cosine annealing with η = 1.0 and h(u) = (1+cos(πu))/2, what is the effective learning rate at 75% through training?

- Concept: **Grid search and multiplicative misspecification**
  - Why needed here: The paper explicitly models coarse grid search as multiplicative overestimation ρ ≥ 1 of the optimal stepsize.
  - Quick check question: If your grid has multiplicative spacing 10^(1/3) ≈ 2.15, what is the worst-case misspecification factor for any optimal stepsize falling between grid points?

## Architecture Onboarding

- Component map:
  - η (base learning rate) → h(u) (schedule function) → ηt = η·h((t-1)/T) (per-step learning rate)
  - Schedule function h(u) → Hh(v), Qh(v) (integral quantities) → convergence bound
  - Multiplicative grid spacing → ρ (misspecification factor) → convergence rate degradation

- Critical path:
  1. Choose schedule h (e.g., cosine, polynomial with p=2)
  2. Define grid for η (multiplicative spacing determines worst-case ρ)
  3. Run SGD with ηt = η·h((t-1)/T)
  4. Performance degrades as ρ^(1/(2p+1)) rather than ρ for annealed schedules

- Design tradeoffs:
  - Higher polynomial degree p → better robustness (ρ^(1/(2p+1))) but worse base rate (O(√p))
  - Assumption: optimal p ≈ log(ρ) balances robustness and base performance
  - Cosine annealing behaves similarly to p=2 polynomial in the tail
  - Fixed stepsize + averaging can match annealed schedules when ρ≈1 but degrades faster under misspecification

- Failure signatures:
  - Linear ρ-dependence in convergence plots → likely using non-decaying schedule or schedule that doesn't reach zero
  - Catastrophic divergence in smooth optimization → τ₀ too large, need smaller base η or faster initial decay
  - Similar degradation across all schedules → grid resolution already sufficient; benefits only appear under coarse grids

- First 3 experiments:
  1. **Replicate synthetic logistic regression**: Generate 100K samples × 100 dimensions, train linear classifier with BCE loss, batch 1000, 1 epoch. Test schedules: fixed, fixed+avg, cosine, linear.
  2. **Ablate polynomial degree**: Test h(u) = (1-u)^p for p ∈ {1,2,3,4} under ρ ∈ {2,5,10,20,50}; verify ρ^(1/(2p+1)) scaling numerically.
  3. **Probe smoothness threshold**: On a β-smooth objective, systematically vary ηh(0)/β and measure τ₀; confirm degradation to O(1/(1-τ₀)) when τ₀ > (1 - ρ^(-2/(2p+1))).

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes convexity and smooth objectives, while empirical validation uses non-convex deep learning tasks where these assumptions don't strictly hold
- The specific polynomial degree p ≈ log(ρ) optimization provides meaningful guidance for practice with only Low confidence
- Potential interactions with other hyperparameters like momentum, batch size, and initialization weren't fully explored

## Confidence
- **High**: Sublinear ρ-dependence emerges mathematically from suffix argument and integral optimization for polynomially decaying schedules
- **Medium**: Empirical validation shows cosine annealing provides robustness advantage over fixed stepsize in grid search
- **Low**: The specific polynomial degree p ≈ log(ρ) optimization provides meaningful guidance for practice

## Next Checks
1. **Stress test the robustness mechanism**: Systematically vary polynomial degree p and multiplicative misspecification ρ in synthetic experiments to verify the predicted ρ^(1/(2p+1)) scaling relationship holds across orders of magnitude.

2. **Probe the smoothness threshold effect**: For β-smooth objectives, measure the actual τ₀ achieved under different base learning rates and verify that performance degradation transitions from ρ^(1/(2p+1)) to O(1/(1-τ₀)) as predicted when τ₀ approaches the critical threshold.

3. **Test schedule-agnostic initialization**: Repeat CIFAR-10 experiments with multiple random initializations to verify the robustness advantage isn't an artifact of favorable initialization or specific data split.