---
ver: rpa2
title: 'CombiGraph-Vis: A Curated Multimodal Olympiad Benchmark for Discrete Mathematical
  Reasoning'
arxiv_id: '2510.27094'
source_url: https://arxiv.org/abs/2510.27094
tags:
- solution
- problem
- answer
- your
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CombiGraph-Vis introduces a 1,135-problem benchmark targeting discrete
  mathematical reasoning across 13 domains and three formats (short-answer, multiple-choice,
  yes/no), with 35% of problems incorporating images essential to solution. The dataset
  is curated through agentic workflows with human oversight, providing verified solutions
  and hierarchical technique labels.
---

# CombiGraph-Vis: A Curated Multimodal Olympiad Benchmark for Discrete Mathematical Reasoning

## Quick Facts
- arXiv ID: 2510.27094
- Source URL: https://arxiv.org/abs/2510.27094
- Reference count: 40
- Primary result: 1,135-problem multimodal benchmark with 35% image-tagged problems showing 14-16 point accuracy gap on visual reasoning

## Executive Summary
CombiGraph-Vis introduces a comprehensive benchmark targeting discrete mathematical reasoning across 13 domains and three problem formats (short-answer, multiple-choice, yes/no). The dataset includes 1,135 Olympiad-level problems with 35% incorporating essential images, curated through agentic workflows with human oversight. Evaluations across diverse model families reveal strong performance separation (16%-78% accuracy) and persistent gaps in visual reasoning and distractor susceptibility, particularly for top-tier models. The benchmark provides fine-grained topic-level tracking and releases solutions, technique labels, and evaluation code to support research on robust multimodal reasoning.

## Method Summary
The benchmark comprises 1,135 discrete mathematics problems sourced from Iranian National Olympiad in Informatics, spanning 13 domains with 89 sub-sub-topics. Problems use three formats: short-answer (77.9%), multiple-choice (13.8%), and yes/no (8.3%), with 35.8% including essential images. Evaluation uses 8-sample chain-of-thought generation with \boxed{} answer extraction, employing Math-Verify for numerical/algebraic equivalence and LLM fallback for other types. Models tested include GPT-5 variants, GPT-4o, Gemini-2.5 variants, and Gemma-3 models. The agentic validation pipeline employs three critics (typo/clarity, logical soundness, answer match) with majority voting across three independent runs to identify and repair dataset errors.

## Key Results
- Performance separation across model families: 16%-78% average accuracy
- Visual reasoning gap: 14-16 percentage points lower accuracy on image-tagged problems for top-tier models
- Distractor susceptibility: Large gaps (up to 44.17 points) between standalone and among-choices accuracy for multiple-choice problems
- Top-tier models achieve 75-78% avg@8 accuracy overall but show systematic weaknesses in graph-theoretic subdomains and formal languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual mathematical reasoning remains a bottleneck even for top-tier models, causing 14-16 percentage point accuracy drops on image-tagged problems.
- Mechanism: Parsing and reasoning over structured visuals (graphs, grids, geometric diagrams) requires multi-step integration of spatial information with symbolic manipulation. Models that handle text-only reasoning well still fail to reliably extract structural constraints from images and propagate them through solution derivations.
- Core assumption: The performance gap reflects genuine visual reasoning deficits rather than data quality issues with image-tagged problems.
- Evidence anchors:
  - [abstract]: "gap by 14–16 points on image problems, revealing persistent visual reasoning deficits"
  - [section 3]: "For top-tier models, the gap from no-image to image conditions is typically 14–16 percentage points (e.g., 83.5% → 68.2% and 80.8% → 66.9%)"
  - [corpus]: Related benchmarks (MathVista, MATH-V) similarly target visual math but report lower baseline performance on multimodal items, consistent with this finding.
- Break condition: If image-tagged problems in this dataset are systematically harder due to topic distribution (not modality), the mechanism would not isolate visual reasoning deficits.

### Mechanism 2
- Claim: Multiple-choice distractor options steer models toward plausible-but-incorrect answers, revealing susceptibility to trap recognition over derivation-based solution.
- Mechanism: Competition-style multiple-choice problems include deliberately crafted distractors that appear correct under superficial reasoning. When models produce answers that match any provided choice (not necessarily the correct one), it indicates pattern-matching to plausible options rather than rigorous derivation.
- Core assumption: The gap between "among-choices accuracy" and "standalone accuracy" reflects distractor susceptibility rather than random noise.
- Evidence anchors:
  - [abstract]: "susceptibility to distractor choices in multiple-choice problems"
  - [section 3]: "The large Δ values indicate that models consistently produce answers that coincide with some provided choice but not necessarily the correct one... Δ = 44.17 for gemma-3-12b-it"
  - [corpus]: AIMO-2 and RIMO papers note similar challenges with answer verification but do not quantify distractor susceptibility via this metric.
- Break condition: If standalone and among-choices metrics are computed on different problem subsets (not controlled for difficulty), the Δ values could conflate difficulty differences with distractor effects.

### Mechanism 3
- Claim: Agentic validation workflows with multi-critic majority voting can systematically detect and repair dataset errors while minimizing false positives.
- Mechanism: Three specialized critics (typo/clarity, logical soundness, answer match) independently assess each problem three times. An aggregator applies majority voting to synthesize findings, then a second-phase workflow classifies error sources (pipeline vs. original source) and attempts automated repair for fixable issues.
- Core assumption: Majority voting across multiple independent runs reduces both false positives and false negatives relative to single-pass validation.
- Evidence anchors:
  - [section 2.2]: "We run these critic stages three times independently for each problem... use the Overall Error Severity taxonomy with five categories"
  - [section 2.2]: "Algorithm 1 (Appendix H)... Aggregator stage that applies majority voting"
  - [corpus]: No direct corpus comparison; agentic validation workflows for math benchmarks are not well-documented in related work.
- Break condition: If critic outputs are highly correlated (not independent), majority voting provides no benefit over single evaluation.

## Foundational Learning

- Concept: **Discrete mathematics fundamentals** (graphs, combinatorics, grids, formal languages)
  - Why needed here: The benchmark centers discrete math reasoning—13 domains include graph theory, combinatorial counting, number theory, and computational geometry. Understanding these structures is prerequisite to interpreting results and identifying capability gaps.
  - Quick check question: Can you explain why a tree with n≥2 vertices must have at least two leaves, and how this relates to degree-sum formulas?

- Concept: **Visual-to-symbolic integration**
  - Why needed here: 35% of problems require parsing images whose structure is essential to solution. This means reading graph topology, grid configurations, or geometric diagrams and translating them into symbolic constraints.
  - Quick check question: Given a 3×3 grid with numbers where adjacent cells differ by exactly 1, what constraints does this place on the possible values in the center cell?

- Concept: **Evaluation metrics for generative reasoning** (avg@k, pass@k, maj@k)
  - Why needed here: The paper reports multiple metrics (avg@8, pass@8, maj@8, all-pass@8) to capture different aspects of model reliability. Understanding what each measures is essential for interpreting performance claims.
  - Quick check question: If a model achieves 90% "among-choices accuracy" but only 75% "standalone accuracy" on the same problems, what does this indicate about its reasoning process?

## Architecture Onboarding

- Component map: Problem text → Image parsing (if present) → 8-sample CoT generation → \boxed{} answer extraction → Math-Verify validation → Metric computation
- Critical path: Load problem → Parse \boxed{} answer via regex → If numerical/algebraic, use Math-Verify; else LLM extraction → Compare to ground truth → Compute avg@8, pass@8, maj@8 metrics
- Design tradeoffs:
  - Short-answer focus: 77.9% of problems are short-answer (verifiable) vs. proof-based (harder to evaluate automatically)
  - Image inclusion at 35%: Sufficient to measure visual reasoning gap without requiring all problems to be multimodal
  - Agentic curation vs. human-only: Faster and more scalable, but may miss edge cases requiring domain expertise
- Failure signatures:
  - Large Δ values (Among-Choices − Standalone > 20 points) indicate distractor susceptibility, not just difficulty
  - Consistent 14-16 point drops on image-tagged items across model families suggest systematic visual reasoning deficit, not topic distribution artifact
  - High severity-4/5 flags in validation that resolve to minor fixes suggest over-sensitive critics (adjust threshold if false positive rate is too high)
- First 3 experiments:
  1. Modality ablation: Evaluate text-only models on image-tagged problems with image descriptions provided (to isolate visual parsing vs. reasoning gap)
  2. Distractor robustness test: For standalone MC problems, compare performance with original distractors vs. random foils vs. no distractors (answer-only)
  3. Topic-level error analysis: Compute per-topic accuracy gaps between top-tier and mid-tier models to identify which discrete math domains show largest spreads (prioritize for targeted improvement)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can reinforcement learning (RL) workflows using CombiGraph-Vis effectively train models to resist adversarial distractor choices in multiple-choice discrete math problems?
- Basis in paper: [explicit] The authors state that the large delta between "Among-Choices" and "Standalone" accuracy "provide[s] strong support for adopting our evaluation suite as an RL environment, since models can potentially learn to avoid deliberately crafted distractors."
- Why unresolved: The paper evaluates current model susceptibility but does not implement or validate an RL training loop to test if this gap can be closed.
- What evidence would resolve it: A study measuring the reduction in the "Among-Choices vs. Standalone" accuracy gap after fine-tuning models using RL on the benchmark.

### Open Question 2
- Question: To what extent is the observed 14–16 point performance drop on image-tagged problems caused by visual parsing errors versus failures in logical reasoning integration?
- Basis in paper: [explicit] The authors note that "parsing and reasoning over structured visuals—graphs, grids, geometric diagrams—remain central bottlenecks, materially impacting overall accuracy," but do not isolate the root cause.
- Why unresolved: The paper quantifies the modality gap but does not provide a fine-grained error analysis distinguishing diagram interpretation failures from discrete reasoning errors.
- What evidence would resolve it: A diagnostic evaluation isolating steps where models fail to extract correct graph/grid information from images versus failing to solve the extracted logical structure.

### Open Question 3
- Question: Does training on high-performing discrete math domains (e.g., combinatorics) facilitate transfer learning to persistent weak domains like formal languages and graph connectivity?
- Basis in paper: [inferred] The results show that "graph-theoretic subdomains... and formal languages expose larger spreads across models," while models perform relatively well in combinatorics and number reasoning.
- Why unresolved: The paper identifies these capability gaps but does not explore whether techniques learned in stronger domains transfer to these specific weak areas.
- What evidence would resolve it: Cross-domain training experiments assessing if improvements in combinatorics reasoning correlate with better performance on formal language tasks.

## Limitations
- Reliance on unreleased GPT-5 model variants for evaluation may limit reproducibility
- Agentic validation pipeline effectiveness not independently verified against human expert review
- Dataset focus on Iranian National Olympiad problems may limit generalizability to other mathematical cultures
- Performance gaps attributed to visual reasoning deficits could partially reflect topic distribution differences

## Confidence
- High confidence: The existence of a 14-16 percentage point accuracy gap between image and non-image problems is well-supported by the reported data across multiple model families. The benchmark construction methodology is transparently documented.
- Medium confidence: The attribution of performance gaps specifically to visual reasoning deficits rather than topic difficulty distributions requires further validation through controlled experiments.
- Low confidence: Claims about the specific mechanism by which distractor options influence model behavior would benefit from controlled experiments varying distractor plausibility and type.

## Next Checks
1. Modality ablation experiment: Evaluate text-only models on image-tagged problems with image descriptions provided to isolate whether the visual reasoning gap stems from parsing difficulty versus integration challenges.
2. Distractor robustness controlled study: For a subset of MC problems, systematically vary distractor options (original vs. random foils vs. no distractors) and measure changes in standalone accuracy to quantify distractor effects independently of problem difficulty.
3. Independent validation comparison: Have domain experts independently review a stratified sample of 100 problems to establish ground truth error rates and validate the agentic validation pipeline's precision and recall.