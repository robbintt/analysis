---
ver: rpa2
title: 'Usability Testing of an Explainable AI-enhanced Tool for Clinical Decision
  Support: Insights from the Reflexive Thematic Analysis'
arxiv_id: '2504.04703'
source_url: https://arxiv.org/abs/2504.04703
tags:
- clinicians
- tool
- usability
- risk
- like
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored clinicians' perspectives on AI explainability
  through a usability study involving 20 U.S. clinicians who interacted with an AI
  tool predicting postpartum depression.
---

# Usability Testing of an Explainable AI-enhanced Tool for Clinical Decision Support: Insights from the Reflexive Thematic Analysis

## Quick Facts
- arXiv ID: 2504.04703
- Source URL: https://arxiv.org/abs/2504.04704
- Reference count: 40
- One-line primary result: Usability study identified four critical explainability dimensions (understandability, trust, usability, usefulness) for AI clinical tools

## Executive Summary
This study explored clinicians' perspectives on AI explainability through a usability study involving 20 U.S. clinicians who interacted with an AI tool predicting postpartum depression. The study identified four key themes critical to AI explainability: understandability, trust, usability, and usefulness. These themes were operationalized into a formal framework for evaluating AI tools. The findings highlight the importance of involving clinicians in the design and testing phases of AI tool development to enhance trust and acceptance.

## Method Summary
The study used reflexive thematic analysis on usability testing data from 20 U.S. clinicians (MDs, nurses, midwives) interacting with an XGBoost-based postpartum depression prediction tool enhanced with SHAP explanations. Participants engaged in think-aloud protocols while using a multi-tab web application with local and global SHAP visualizations. The framework development involved 6 phases of thematic analysis with inter-rater reliability of Cohen's Kappa = 0.69 (78% agreement).

## Key Results
- Four critical explainability dimensions identified: understandability, trust, usability, and usefulness
- Chi-squared analysis showed significant association between study phase and theme salience (χ² = 14.277, p = 0.002)
- Local SHAP explanations preferred over global explanations, but substantial training required for full comprehension
- Pre-use skepticism decreased after hands-on interaction, with trust concerns more prevalent before use

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interactive local SHAP explanations improve clinician comprehension more effectively than static global explanations, but require training to interpret correctly.
- Mechanism: Local SHAP plots decompose individual predictions into additive feature contributions from a baseline, allowing clinicians to modify inputs and immediately observe how each factor shifts risk scores. This "details on demand" approach enables validation against clinical mental models.
- Core assumption: Clinicians must relate AI outputs to their existing clinical knowledge to build acceptance.
- Evidence anchors:
  - [abstract]: "clinicians generally found the tool usable and useful, they required substantial training to fully comprehend the explainability methods"
  - [section]: "they struggled in understanding the global SHAP model (beeswarm plot) but they liked the interactive local SHAP plot"
  - [corpus]: Neighbor paper "Explainability and AI Confidence in Clinical Decision Support Systems" explores related explainability effects on trust in breast cancer care
- Break condition: If feature contributions consistently contradict clinical intuition without clear rationale, comprehension efforts reverse into distrust.

### Mechanism 2
- Claim: Pre-use skepticism about AI tools decreases after hands-on interaction, with trust concerns significantly more prevalent before use than after.
- Mechanism: Clinicians validated the tool by creating hypothetical patient profiles and comparing outputs to their clinical expectations. Chi-squared analysis (χ² = 14.277, p = 0.002) showed trust appeared 21 times in pre-study interviews versus 13 times post-interaction, while usability mentions increased from 12 to 30.
- Core assumption: Assumption: Clinicians possess reliable mental models from clinical experience that serve as validation ground truth.
- Evidence anchors:
  - [abstract]: "highlight the importance of involving clinicians in the design and testing phases of AI tool development to enhance trust and acceptance"
  - [section]: "Several clinicians created scenarios by changing the feature values to assess the reliability of the model by creating a patient profile based on their experience"
  - [corpus]: Limited corpus evidence on pre/post trust calibration; neighbor papers focus on single-point trust measurement
- Break condition: If model predictions systematically deviate from clinical expectations without explainable justification, validation attempts produce rejection rather than acceptance.

### Mechanism 3
- Claim: Explainability is a multi-dimensional construct (understandability, trust, usability, usefulness) where dimensions shift in salience across interaction phases.
- Mechanism: The four dimensions operate independently but interconnect toward acceptance. Pre-study phase emphasized trust (concerns about accuracy, bias, privacy); post-interaction emphasized usability (navigation, visualization, responsiveness) and usefulness (actionability, workflow integration).
- Core assumption: Explainability is not monolithic; satisfying one dimension does not compensate for deficits in others.
- Evidence anchors:
  - [abstract]: "identified four key themes critical to AI explainability: understandability, trust, usability, and usefulness"
  - [section]: "A chi-squared test showed that there was an association between study phase and themes (χ² = 14.277, df = 3, p-value = 0.002)"
  - [corpus]: Neighbor paper "AXAI-CDSS" similarly proposes multi-component explainability for clinical decision support
- Break condition: Optimizing one dimension (e.g., high usability) while neglecting others (e.g., low usefulness due to missing recommendations) fails to achieve acceptance.

## Foundational Learning

- Concept: **SHAP (SHapley Additive exPlanation) Values**
  - Why needed here: Core explanation method; understanding that SHAP decomposes predictions into additive feature contributions from a baseline (E[f(x)]) is essential for interpreting the tool's rationale
  - Quick check question: Given f(x) = E[f(x)] + Σφᵢ, what does a positive φᵢ value indicate about feature i's contribution?

- Concept: **Local vs. Global Explanations**
  - Why needed here: The study found clinicians succeeded with local (per-patient) explanations but struggled with global (dataset-wide) explanations; this distinction drives UI design choices
  - Quick check question: When would a global explanation (e.g., beeswarm plot showing feature importance across all patients) be more valuable than a local explanation?

- Concept: **Reflexive Thematic Analysis (RTA)**
  - Why needed here: The methodology used to derive the four-dimension framework; RTA explicitly acknowledges researcher subjectivity in theme development, affecting how to interpret the framework's generalizability
  - Quick check question: How does RTA's treatment of researcher subjectivity differ from traditional content analysis approaches?

## Architecture Onboarding

- Component map:
  - **Input Layer**: Patient features from PRAMS schema (maternal age category, race, pregnancy intention, depression history, insurance type)
  - **Model Core**: XGBoost classifier trained on CDC PRAMS data with cross-validation, hyperparameter tuning; outputs probability score
  - **Explanation Engine**: SHAP explainer generating local force plots (per-patient feature contributions) and global beeswarm plots
  - **Visualization Layer**: Color-coded gauge chart (green ≤50%, orange 50-70%, red >70%) + interactive SHAP force plot
  - **Interface**: Multi-tab web application (overview, model information, calculator)

- Critical path: User enters patient data → XGBoost computes risk probability → SHAP generates local explanation → Visualization renders gauge + force plot → User interprets and validates against clinical knowledge

- Design tradeoffs:
  - **Simplified thresholds vs. scientific grounding**: Color-coded cutoffs improve immediate comprehension but Participant 16 warned novice clinicians may "decide that yellow is totally fine" without evidence-based thresholds
  - **Transparency vs. cognitive load**: Model performance metrics (ROC-AUC, sensitivity, specificity) build trust but may overwhelm; requires tiered information architecture
  - **Prediction vs. actionability**: Clinicians strongly requested "what do I do next?" recommendations, but generating patient-specific advice requires clinical protocol integration and regulatory consideration

- Failure signatures:
  - Clinicians entering edge-case profiles and receiving counterintuitive predictions → trust erosion
  - Novice users interpreting color zones as absolute truth rather than probability bands
  - Global SHAP beeswarm plots causing comprehension failure despite tooltips and interpretation notes
  - Missing features clinicians expect (e.g., additional risk factors) reducing perceived validity

- First 3 experiments:
  1. **Explanation type ablation**: Randomize clinicians to receive local-only vs. local+global explanations; measure interpretation accuracy, task time, and subjective comprehension
  2. **Threshold configuration test**: Present identical risk scores with different visualization formats (continuous probability bar vs. discrete color zones vs. numeric-only) to novice and expert clinicians; assess decision concordance with clinical guidelines
  3. **Actionability prototype**: Build a "recommendations panel" with evidence-based interventions tied to risk level; conduct think-aloud testing to evaluate whether usefulness scores increase and what clinical validation the recommendations require

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the four-dimensional explainability framework (understandability, trust, usability, usefulness) generalize to larger, diverse clinical populations?
- Basis in paper: [explicit] The authors state the identified themes "require further validation with more clinicians to fully assess the efficacy of such tools."
- Why unresolved: The current sample size (N=20) and specific focus on postpartum depression may limit generalizability to other clinical contexts.
- What evidence would resolve it: A large-scale, quantitative validation study across various medical specialties.

### Open Question 2
- Question: Does the integration of automated, patient-specific recommendations significantly enhance the tool's perceived usefulness?
- Basis in paper: [explicit] "Future work should include a feature that triggers patient-specific actions" to address clinicians' requests for actionable guidance.
- Why unresolved: The current prototype provided risk scores without treatment suggestions, creating a gap between prediction and clinical action.
- What evidence would resolve it: Comparative usability testing measuring "usefulness" scores before and after adding a recommendation module.

### Open Question 3
- Question: What specific training strategies are required to ensure clinicians can accurately interpret complex XAI visualizations like SHAP plots?
- Basis in paper: [inferred] The study found that despite tooltips, clinicians "required substantial training" and struggled to understand global model explanations.
- Why unresolved: It is unclear if current "details on demand" interfaces are sufficient for non-expert users to grasp the underlying model logic.
- What evidence would resolve it: An experimental study comparing different training protocols (e.g., interactive tutorials vs. manuals) on interpretation accuracy.

### Open Question 4
- Question: Does embedding the tool within the Electronic Medical Record (EMR) workflow improve usability and time-efficiency compared to a standalone interface?
- Basis in paper: [inferred] Participants highlighted that manual data entry was a barrier and strongly valued potential EMR integration to save time.
- Why unresolved: The study utilized a standalone web tool, which does not reflect the time constraints of actual clinical workflows.
- What evidence would resolve it: A pilot study measuring task completion time and cognitive load in an integrated EMR environment.

## Limitations
- Small sample size (n=20) limits generalizability across clinical specialties and geographic regions
- Study focuses on a single clinical domain (postpartum depression) using XGBoost, limiting framework applicability to other contexts
- Framework development through reflexive thematic analysis introduces researcher subjectivity affecting external validity

## Confidence
- **High Confidence**: The identification of four distinct dimensions (understandability, trust, usability, usefulness) and their phase-dependent salience is well-supported by the data and methodology.
- **Medium Confidence**: The mechanism that interactive local SHAP explanations improve comprehension more than static global explanations, though this requires training, is supported but needs experimental validation.
- **Low Confidence**: The framework's applicability to other clinical domains and AI models beyond XGBoost and PPD prediction remains untested.

## Next Checks
1. **A/B Testing of Explanation Types**: Conduct randomized controlled trial comparing local-only SHAP vs. local+global SHAP explanations on clinician interpretation accuracy, task completion time, and comprehension scores.
2. **Threshold Interpretation Study**: Test color-coded risk thresholds with novice vs. experienced clinicians using identical risk scores presented in different formats (continuous probability, discrete color zones, numeric-only) to assess decision-making differences.
3. **Cross-Domain Framework Validation**: Apply the four-dimension framework to usability testing of AI tools in different clinical specialties (e.g., oncology, cardiology) and with different model architectures (deep learning, logistic regression) to test generalizability.