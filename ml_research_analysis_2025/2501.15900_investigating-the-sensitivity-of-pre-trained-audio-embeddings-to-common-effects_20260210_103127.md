---
ver: rpa2
title: Investigating the Sensitivity of Pre-trained Audio Embeddings to Common Effects
arxiv_id: '2501.15900'
source_url: https://arxiv.org/abs/2501.15900
tags:
- audio
- effects
- embeddings
- effect
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the sensitivity of pre-trained audio embeddings
  to common audio effects (gain, low-pass filtering, reverberation, and bitcrushing)
  by applying parameterized audio effects and analyzing embedding responses. The authors
  propose using canonical correlation analysis (CCA) to quantify the dimensionality
  and linearizability of deformation trajectories in embedding space.
---

# Investigating the Sensitivity of Pre-trained Audio Embeddings to Common Effects

## Quick Facts
- arXiv ID: 2501.15900
- Source URL: https://arxiv.org/abs/2501.15900
- Reference count: 14
- Primary result: Pre-trained audio embeddings do not globally linearize common audio effects, and linear projection methods cannot reliably improve downstream robustness.

## Executive Summary
This paper investigates whether pre-trained audio embeddings (OpenL3, PANNs, CLAP) exhibit linear sensitivity to common audio effects (gain, low-pass filtering, reverberation, bitcrushing). Using canonical correlation analysis (CCA), the authors find that while embeddings move monotonically along certain directions as effect strength increases, the overall deformation subspace is high-dimensional, preventing global linearization. They test whether projecting out estimated deformation directions can improve downstream classification robustness, finding that linear post-hoc methods generally fail to reduce sensitivity.

## Method Summary
The study applies parameterized audio effects to samples from the IRMAS dataset and analyzes embedding responses using global and sample-wise CCA to identify deformation directions. SVD is used to assess the dimensionality of the deformation subspace. Five projection methods (global CCA, sample-wise CCA SVD, PCA variants, average displacement, LDA) are evaluated by training a logistic regressor on instrument classification before and after projection. The effects are implemented using Pedalboard/Scipy with parameter sweeps, and embeddings are extracted from OpenL3 (512-dim), PANNs (2048-dim), and CLAP (1024-dim).

## Key Results
- Embeddings exhibit high correlation with effect strength along certain directions (R² > 0.95 for most combinations), but the overall deformation subspace is high-dimensional
- There exists a monotonic direction for each sample as effect strength increases, but global linearization does not hold across samples
- Projecting out estimated deformation directions through various methods does not generally improve downstream classification robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained audio embeddings move monotonically along a dominant direction as audio effect strength increases.
- Mechanism: When a parameterized effect is applied with increasing intensity, the embedding traces a trajectory through the embedding space. Canonical Correlation Analysis (CCA) identifies a direction u that maximally correlates with the rank-transformed effect parameter.
- Core assumption: The relationship between effect strength and embedding displacement is monotonic (not necessarily linear).
- Evidence anchors:
  - [abstract] "We find that there exists a direction along which the embeddings move monotonically as the audio effect strength increases."
  - [section III] "For all combinations of audio effect and embedding and all samples, the R² coefficient is equal to 1 [for sample-wise CCA]."
  - [corpus] Limited direct corpus support; related work (Srivastava et al.) explores sensitivity metrics but not monotonic trajectory analysis.
- Break condition: Non-monotonic effects (e.g., cyclic modulation) or effects causing discontinuous embedding jumps (observed in CLAP with bitcrushing).

### Mechanism 2
- Claim: Deformation trajectories occupy a high-dimensional subspace, preventing global linearization of effects.
- Mechanism: While sample-wise CCA directions show perfect local correlation, the singular value decomposition (SVD) of these directions across samples reveals that they span many dimensions—comparable to the original embedding variance structure.
- Core assumption: If deformation were low-dimensional, sample-wise CCA directions would concentrate in a small number of singular vectors.
- Evidence anchors:
  - [abstract] "The subspace containing the displacements is generally high-dimensional. This shows that pre-trained audio embeddings do not globally linearize the effects."
  - [section III, Fig. 3] "In all cases, the comparisons exhibit a high dimensionality of the sample-wise CCA directions."
  - [corpus] No directly comparable analysis in corpus; Wang et al. (2023) assumes low-dimensional bias subspaces for dataset debiasing, which this paper contradicts.
- Break condition: If training data contained uniform effect distributions, models might learn more compact effect representations (untested hypothesis).

### Mechanism 3
- Claim: Post-hoc linear projection cannot reliably remove effect sensitivity for downstream tasks.
- Mechanism: Five projection methods (global CCA, sample-wise CCA SVD, PCA variants, average displacement, LDA) were used to remove estimated deformation directions before training a logistic regressor on instrument classification. Performance changes were inconsistent—sometimes neutral, sometimes worse, rarely improved.
- Core assumption: Sensitivity reduction requires only removing directions/subspaces; non-linear interactions are ignored.
- Evidence anchors:
  - [abstract] "Projecting out the estimated deformation directions cannot generally improve the robustness of pre-trained audio embeddings to audio effects."
  - [section IV, Fig. 4] "With sensitivity reduction, the classification performance remains sensitive to the audio effect strength... In many cases, average displacement projection improves classification performance by around 0.003 to 0.01 AUC, but it sometimes decreases the performance."
  - [corpus] Corpus lacks replication; related work focuses on robustness via data augmentation (Ramires & Serra, 2019) rather than projection.
- Break condition: None found—failure appears systematic across methods, embeddings, and effects.

## Foundational Learning

- Concept: **Canonical Correlation Analysis (CCA)**
  - Why needed here: CCA finds directions in the embedding space that maximize correlation with effect strength, quantifying how strongly embeddings encode effect parameters.
  - Quick check question: Given two variables X (embedding dimensions) and Y (effect parameter), can you identify the direction in X-space that maximizes correlation with Y?

- Concept: **Subspace Projection for Debiasing**
  - Why needed here: The paper tests whether removing estimated "effect directions" can make embeddings invariant to effects—a common debiasing technique whose assumptions this paper challenges.
  - Quick check question: If you project out direction d from embedding x, what geometric operation are you performing? (Answer: x ← x − (dᵀx)d for unit d)

- Concept: **Singular Value Decomposition (SVD) for Dimensionality Analysis**
  - Why needed here: SVD of sample-wise CCA directions reveals whether deformation trajectories cluster in a low-dimensional subspace or span the full embedding space.
  - Quick check question: If SVD singular values decay rapidly (first few ≫ rest), what does that imply about subspace dimensionality?

## Architecture Onboarding

- Component map:
  Audio Embedding Models (OpenL3/PANNs/CLAP) -> Effect Pipeline (Pedalboard/Scipy) -> Embedding Extraction -> Analysis Pipeline (CCA/SVD) -> Mitigation Pipeline (Projection Methods) -> Downstream Classifier (Logistic Regression)

- Critical path:
  1. Select embedding model and effect type
  2. Apply parameterized effect sweep to audio samples
  3. Extract embeddings for all (original, effected) pairs
  4. Run CCA to find correlation direction(s)
  5. Perform SVD on sample-wise directions to assess dimensionality
  6. Apply projection methods and evaluate on downstream task (instrument classification using IRMAS dataset)

- Design tradeoffs:
  - **Global vs. sample-wise CCA**: Global CCA finds one direction across all samples (high correlation but ignores sample-specific trajectories); sample-wise captures local behavior but requires SVD aggregation.
  - **Projection threshold (t)**: Lower t (e.g., 0.3) projects more dimensions, risking information loss; higher t preserves more but may miss relevant subspaces.
  - **PCA normalization**: Normalized PCA (ratio of displacement variance to original variance) vs. absolute PCA prioritizes different subspace characteristics.

- Failure signatures:
  - **CLAP discontinuous trajectories**: Bitcrushing and some other effects cause non-monotonic jumps—indicating potential architectural sensitivity in text-aligned embeddings.
  - **PANNs bitcrushing saturation**: Embeddings cluster at bit depths >10, suggesting insensitivity threshold.
  - **Projection harms performance**: Average displacement and non-normalized PCA sometimes reduce AUC, indicating over-removal or removal of task-relevant information.

- First 3 experiments:
  1. **Reproduce CCA correlation analysis** on a single instrument class with one effect (e.g., gain on cello): compute global R² and verify it matches reported values (>0.95 for OpenL3/CLAP, ~0.98 for PANNs).
  2. **SVD dimensionality check**: Extract sample-wise CCA directions for 50+ samples, compute SVD, and confirm that normalized singular values do not decay rapidly—indicating high-dimensional deformation.
  3. **Single projection ablation**: Apply only average displacement projection to PANNs embeddings with gain effect; measure ROC AUC change on instrument classification to confirm inconsistent improvement pattern.

## Open Questions the Paper Calls Out
- Question: Can non-linear or learned projection methods successfully reduce embedding sensitivity to audio effects where linear post-hoc projection fails?
  - Basis in paper: [explicit] The conclusion states "a linear post-processing approach, i.e. projecting out the deformation direction or subspace, may hardly improve the robustness" and the high-dimensional nature of the deformation subspace suggests linear methods are insufficient.
  - Why unresolved: The paper only tested linear projection methods (CCA, PCA, LDA, average displacement); non-linear alternatives remain unexplored despite the demonstrated inadequacy of linear approaches.
  - What evidence would resolve it: Experiments comparing linear projection against learned non-linear transformations (e.g., adversarial debiasing, contrastive learning with effect augmentation) on the same downstream classification task.

## Limitations
- Focus on single-parameter sweeps rather than real-world effect chains that combine multiple transformations
- Use of logistic regression as a simple downstream task that may not capture complex classification scenarios
- Lack of exploration into whether training-time augmentation could induce more compact effect representations

## Confidence
- High confidence in the core finding that deformation trajectories span high-dimensional subspaces
- High confidence in monotonic correlation between effect strength and embeddings, though CLAP's discontinuous trajectories require further investigation
- High confidence in systematic failure of linear projection methods to improve downstream robustness
- Medium confidence in the magnitude of occasional improvements (0.003-0.01 AUC) due to subtle interactions

## Next Checks
1. Replicate the CCA correlation analysis with different random seeds and verify R² values consistently exceed 0.95 for global CCA across all embedding-effect combinations.
2. Test projection methods on a more complex downstream task (e.g., multi-label instrument classification) to determine if linear debiasing fails systematically or only for simple classifiers.
3. Investigate whether training embeddings with augmented data containing varied effect strengths produces lower-dimensional deformation subspaces, contradicting or supporting the current findings about learned representations.