---
ver: rpa2
title: Benchmarking the rationality of AI decision making using the transitivity axiom
arxiv_id: '2502.10554'
source_url: https://arxiv.org/abs/2502.10554
tags:
- transitivity
- choice
- llama
- gamble
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated the rationality of AI decision-making by\
  \ testing whether responses from ten versions of Meta\u2019s Llama 2 and 3 models\
  \ satisfy transitivity of preference. Using Bayesian model selection, the authors\
  \ compared weak stochastic transitivity and mixture model of transitive preference\
  \ against an unconstrained benchmark."
---

# Benchmarking the rationality of AI decision making using the transitivity axiom

## Quick Facts
- **arXiv ID**: 2502.10554
- **Source URL**: https://arxiv.org/abs/2502.10554
- **Reference count**: 14
- **Key outcome**: Most Llama models satisfy transitivity of preference, with violations occurring primarily in Chat/Instruct versions when probabilities are presented as percentages with dollar-sign monetary values.

## Executive Summary
This study evaluates the rationality of AI decision-making by testing whether responses from ten versions of Meta's Llama 2 and 3 models satisfy the transitivity axiom of preference. Using Bayesian model selection, the authors compare weak stochastic transitivity and mixture models against an unconstrained benchmark. The results show that most models satisfy transitivity, though violations are observed primarily in Chat/Instruct versions, particularly Llama 3 8B Instruct. Transitivity violations occur most frequently when probabilities are presented as percentages with dollar-sign monetary values, suggesting format sensitivity in AI decision-making processes.

## Method Summary
The study employs Bayesian model selection to test transitivity of preference across ten Llama model variants. Researchers presented decision scenarios involving probabilistic outcomes and monetary values in different formats (percentages vs. fractions). The methodology compares three statistical models: weak stochastic transitivity, mixture models of transitive preference, and an unconstrained benchmark. Model fit is evaluated using Bayesian criteria to determine which framework best explains observed AI response patterns. The experimental design specifically manipulates probability presentation formats and monetary notation to identify conditions under which transitivity violations occur.

## Key Results
- Most Llama models satisfy weak stochastic transitivity, demonstrating rational decision-making patterns
- Transitivity violations occur exclusively in Chat/Instruct versions, particularly Llama 3 8B Instruct
- Violations are most prevalent when probabilities are presented as percentages with dollar-sign monetary values

## Why This Works (Mechanism)
The transitivity axiom provides a fundamental test of rational decision-making by requiring consistent preferences across choices. When an AI system prefers option A over B and B over C, transitivity demands that it also prefer A over C. Violations indicate decision-making inconsistencies that may stem from how models process numerical information or contextual cues. The study's approach leverages this axiomatic framework to benchmark AI rationality by examining how different presentation formats affect preference consistency.

## Foundational Learning
- **Weak stochastic transitivity**: A probabilistic extension of the classic transitivity axiom that allows for noise in decision-making while maintaining core consistency requirements. Understanding this concept is needed to interpret when AI responses represent violations versus probabilistic variation. Quick check: Verify that preference patterns follow the mathematical constraints defined by weak stochastic transitivity.
- **Bayesian model selection**: A statistical approach for comparing competing theoretical models based on their fit to observed data. This method is needed to determine whether transitivity or alternative models better explain AI decision patterns. Quick check: Confirm that Bayesian evidence ratios favor the most appropriate model for each case.
- **Mixture models**: Statistical frameworks that allow for combinations of different behavioral patterns (e.g., transitive and non-transitive responses). These are needed to capture potential heterogeneity in AI decision-making processes. Quick check: Ensure mixture model parameters appropriately weight different behavioral components.
- **Model fit criteria**: Statistical measures (such as Bayes factors) that quantify how well different models explain observed data. These are needed to objectively compare transitivity against alternative explanations. Quick check: Validate that model selection criteria are appropriately calibrated and interpreted.

## Architecture Onboarding
**Component Map**: Input generation -> Model response collection -> Bayesian model fitting -> Transitivity testing -> Result interpretation
**Critical Path**: The experimental design directly influences data quality, which determines the validity of Bayesian model selection, ultimately affecting conclusions about transitivity violations and AI rationality.
**Design Tradeoffs**: The study prioritizes statistical rigor through Bayesian methods but limits generalizability by focusing on only ten Llama variants and specific probability formats.
**Failure Signatures**: Transitivity violations in Chat/Instruct versions suggest fine-tuning may introduce decision-making inconsistencies, particularly with percentage-based probability presentations and dollar-sign monetary values.
**First Experiments**:
1. Test transitivity across different probability presentation formats (fractions, percentages, ratios) to identify format-specific effects
2. Compare transitivity patterns between base models and fine-tuned versions to isolate fine-tuning impacts
3. Examine transitivity across different monetary contexts (different currencies, non-monetary rewards) to assess domain specificity

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The study's conclusions are based on only ten Llama model variants, potentially limiting generalizability to other AI systems
- Observed transitivity violations may be model-specific rather than representing broader patterns in AI decision-making
- The experimental focus on specific probability formats and monetary values may not capture violations that occur in other decision contexts

## Confidence
- **High confidence**: Most Llama models satisfy weak stochastic transitivity; the statistical methodology using Bayesian model selection; Chat/Instruct versions show more violations than base models
- **Medium confidence**: Format-specific violations (percentages with dollar signs) represent meaningful insights into AI decision-making processes; generalizability to broader AI evaluation contexts
- **Low confidence**: Claims about "computational rationality" as a general concept; suggesting transitivity violations serve as a comprehensive benchmark for AI response quality across diverse applications

## Next Checks
1. Replicate transitivity testing across a broader range of AI models including different architectures (GPT, Claude, etc.) and varying parameter sizes to assess generalizability of the findings
2. Conduct controlled experiments varying multiple contextual factors simultaneously (monetary units, probability formats, decision domains) to isolate which specific combinations trigger transitivity violations
3. Implement longitudinal testing to determine whether transitivity patterns are stable over time or change with model updates, and whether fine-tuning procedures systematically affect rational decision-making consistency