---
ver: rpa2
title: 'Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language
  Models Pretraining'
arxiv_id: '2509.15556'
source_url: https://arxiv.org/abs/2509.15556
tags:
- language
- multilingual
- data
- cross-lingual
- allocation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLIMB addresses the challenge of optimal multilingual data allocation
  in LLM pretraining by modeling cross-lingual interactions through a novel interaction-aware
  language ratio. This ratio captures how effectively each language contributes to
  model performance when co-trained with others.
---

# Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining

## Quick Facts
- **arXiv ID:** 2509.15556
- **Source URL:** https://arxiv.org/abs/2509.15556
- **Reference count:** 40
- **One-line primary result:** CLIMB achieves up to 2.60% absolute accuracy improvement on multilingual benchmarks by optimizing language proportions through interaction-aware scaling laws.

## Executive Summary
This paper addresses the challenge of optimal multilingual data allocation in LLM pretraining by introducing CLIMB, a framework that models cross-lingual interactions through a novel interaction-aware language ratio. Unlike traditional uniform allocation, CLIMB captures how effectively each language contributes when co-trained with others, using this metric to optimize language proportions. The approach employs a two-step optimization procedure that first balances marginal benefits across languages, then maximizes overall effective allocation. Experiments demonstrate that CLIMB-derived allocations consistently achieve state-of-the-art multilingual performance across diverse benchmarks, outperforming baselines and even matching models trained on more data.

## Method Summary
CLIMB addresses multilingual data allocation by first fitting monolingual scaling laws per language at two token budgets, then running small-scale multilingual experiments to estimate cross-lingual transfer parameters. The framework introduces an interaction-aware ratio that transforms nominal allocation into effective allocation by accounting for transfer coefficients and saturation effects. Using these parameters, CLIMB employs a two-stage optimization: first equalizing marginal loss reduction across languages, then maximizing effective data allocation under constraints. The method requires 3×m×2 pilot experiments for m languages to fit the parametric transfer model, followed by constrained optimization to determine optimal language proportions for full-scale training.

## Key Results
- CLIMB outperforms uniform allocation baselines by up to 2.60% absolute accuracy across multilingual benchmarks
- Models trained with CLIMB achieve comparable or superior performance to publicly available models trained on more data
- Optimal language allocations shift significantly as data scale increases, validating the interaction-aware approach
- CLIMB generalizes effectively to larger model scales while maintaining performance advantages

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Modeling effective data allocation rather than nominal proportions enables accurate prediction of multilingual validation loss.
- **Mechanism:** CLIMB introduces a cross-lingual interaction-aware ratio (˜r_i) that maps nominal allocation (r_i) to effective values, accounting for co-trained language influences through transfer coefficients (α_{j→i}) and exponential saturation factors (η_i).
- **Core assumption:** The relationship between nominal and effective allocation follows a specific parametric form where transfer strength decays linearly with 1/D and contribution saturates as proportion increases.
- **Evidence anchors:** Abstract states "explicitly quantifying each language's effective allocation by capturing inter-language dependencies," while Section 2.2 defines ˜r_i including transfer strength and saturation terms.
- **Break condition:** If cross-lingual transfer is chaotic or non-stationary, the parametric fit will fail to generalize.

### Mechanism 2
- **Claim:** Decomposing optimization into direction search (balancing marginal benefits) followed by magnitude search (maximizing effective data) simplifies the non-convex allocation problem.
- **Mechanism:** Instead of solving intractable joint minimization, CLIMB first finds direction vector p where marginal loss reduction is equalized across languages, then projects this onto constraint space to maximize total effective data.
- **Core assumption:** Optimal allocation must balance marginal utility across languages, and optimal direction remains valid while scaling magnitude.
- **Evidence anchors:** Abstract describes "first equalizing marginal benefits across languages, then maximizing the magnitude," with Section 2.3 providing detailed two-step procedure derivation.
- **Break condition:** If loss landscape is highly non-monotonic, maximizing immediate effective allocation may trap models in sub-optimal plateaus.

### Mechanism 3
- **Claim:** Cross-lingual transfer strength is a function of training data scale, diminishing as total token budget increases.
- **Mechanism:** Transfer coefficient α_{j→i} is modeled as b_{ji} + k_{ji}/D, implying that as total data D grows, language-to-language boost decays toward asymptotic baseline b_{ji}.
- **Core assumption:** Benefits of cross-lingual transfer are primarily low-data phenomena and models become "self-dominated" as they scale.
- **Evidence anchors:** Section 2.2.1 states "Cross-lingual transfer consistently weakens with larger total token budgets," with Appendix A providing explicit decay function definition.
- **Break condition:** If certain languages permanently rely on transfer, the decay assumption may cause under-allocation at large scales.

## Foundational Learning

- **Concept:** Monolingual Scaling Laws (Chinchilla/Hoffmann)
  - **Why needed here:** CLIMB builds directly upon power-law relationship (L = B/D^β + E); without understanding single-language loss decrease with data, the interaction-aware extension is unintuitive.
  - **Quick check question:** Given loss L and data D, can you derive relationship between marginal data addition and loss reduction?

- **Concept:** Optimization on the Probability Simplex
  - **Why needed here:** Final output is vector of ratios summing to 1; optimization relies on Trust-Region Interior-Point methods to ensure valid probability distribution.
  - **Quick check question:** Why is standard gradient descent insufficient for finding valid probability distribution without constrained optimization or Softmax projection?

- **Concept:** Transfer Learning (Positive vs. Negative Transfer)
  - **Why needed here:** Mechanism relies on α_{j→i} capturing whether language j helps or hurts language i.
  - **Quick check question:** If language j causes catastrophic forgetting in language i, what sign would you expect coefficient α_{j→i} to have in CLIMB model?

## Architecture Onboarding

- **Component map:** Estimation Pipeline -> Interaction Model -> Solver
- **Critical path:** Accuracy of final allocation depends entirely on Estimation Pipeline; if pilot experiments are too small or noisy to capture stable transfer effects (<25B tokens), fitted parameters will mislead solver.
- **Design tradeoffs:**
  - Estimation Cost vs. Accuracy: Fitting requires 3×m×2 experiments; for large m (e.g., 16 languages), this is expensive.
  - Generality vs. Specificity: Model fits specific language pairs; adding new language requires re-running probe experiments, unlike heuristic methods using internet text statistics.
- **Failure signatures:**
  - Extrapolation Collapse: If solver outputs ratios vastly outside pilot experiment range, scaling law prediction may be invalid.
  - Negative Transfer Over-correction: If solver detects strong negative transfer, it might allocate 0% to language, causing training instability.
- **First 3 experiments:**
  1. Verify Monolingual Fit: Replicate Chinchilla fit for single language (e.g., English) on specific dataset to ensure B, β, E are recoverable.
  2. 2-Language Interaction Check: Train small bilingual model varying one language's ratio (0.1 to 0.9) while keeping total tokens constant; plot fixed language's validation loss to confirm non-linear interaction curve.
  3. Solver Validation: Implement two-step solver (Eq 7 & 8) with dummy parameters; verify it outputs valid probability distribution summing to 1.0.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can CLIMB framework explicitly incorporate model parameter count (N) into multilingual scaling law and optimization procedure?
- Basis in paper: [explicit] Authors state in Limitations section that explicitly incorporating model size (N) into allocation optimization "remains an open area for future research," as current experiments validated strategy on fixed model scales (1.2B and 7B).
- Why unresolved: Current parametric model optimizes allocation based on dataset size (D) and ratios (r) but treats model architecture/size as fixed condition rather than dynamic variable in optimization function.
- What evidence would resolve it: Joint scaling law L(N, D, r) derivation and experiments demonstrating CLIMB-optimized proportions differ significantly and yield better results when tailored to specific target model parameter counts.

### Open Question 2
- Question: How can cross-lingual interaction-aware ratio be adapted to predict transfer benefits for languages absent from training corpus?
- Basis in paper: [explicit] Conclusion and Limitations sections note approach currently considers only trained languages, and "extending this predictive capacity to languages absent from the training corpus constitutes an important direction."
- Why unresolved: Current formulation defines interaction-aware ratio ˜r_i based on known proportions r_i of co-trained languages, lacking mechanism to model transfer to languages with r_i = 0.
- What evidence would resolve it: Formulation that predicts validation loss on held-out languages (zero-shot) based on trained language mixture, validated by comparing predicted vs. actual performance on unseen languages.

### Open Question 3
- Question: Does dynamic, curriculum-based allocation strategy outperform static optimal proportion derived by CLIMB for fixed token budget?
- Basis in paper: [inferred] Figure 8 illustrates optimal language allocations shift significantly as data scale (D) increases, suggesting static ratio fixed at start might be suboptimal compared to schedule adjusting proportions as training progresses.
- Why unresolved: Paper optimizes for final target token budget (e.g., 1T) but does not explore if transitioning between optimal ratios at intermediate steps improves convergence or final performance.
- What evidence would resolve it: Experiments comparing static CLIMB allocations against "Dynamic CLIMB" schedule that updates ratios incrementally during pretraining to match shifting optimal curves observed in Figure 8.

## Limitations

- Framework's performance hinges on accurate estimation of cross-lingual transfer parameters through pilot experiments, with computational overhead scaling quadratically with language count.
- Parametric form of transfer (α_{j→i} = b_{ji} + k_{ji}/D) assumes smooth, predictable decay that may not hold for extremely low-resource languages or when transfer is dominated by specific phenomena.
- Method assumes monolingual scaling laws are stable and transferable across domains, but real-world pretraining data often contains domain shifts that violate this assumption.

## Confidence

- **High Confidence:** Two-stage optimization procedure (direction search + magnitude maximization) is mathematically sound and empirical improvements over baselines are reproducible given proper implementation of pilot experiments.
- **Medium Confidence:** Parametric modeling of cross-lingual interactions captures general trends but may miss edge cases like negative transfer or emergent transfer phenomena at specific training stages.
- **Low Confidence:** Assumption that optimal direction remains valid when scaling magnitude may fail in non-monotonic loss landscapes, particularly for languages exhibiting "grokking" behavior.

## Next Checks

1. **Ablation on Pilot Scale:** Run full CLIMB pipeline with pilot experiments at different token budgets (10B, 25B, 50B) to measure sensitivity of final allocation to pilot scale, testing whether framework's claims about scale-dependent transfer are empirically robust.

2. **Negative Transfer Stress Test:** Deliberately construct bilingual experiment where one language interferes with another (e.g., similar languages with conflicting tokenization); verify CLIMB's optimization correctly identifies and mitigates this through α_{j→i} parameters.

3. **Cross-Domain Generalization:** Apply CLIMB pipeline trained on Fineweb-2 to completely different corpus (e.g., CommonCrawl or proprietary business documents); measure degradation in scaling law fit quality and downstream performance to assess domain transferability.