---
ver: rpa2
title: Autonomous Learning with High-Dimensional Computing Architecture Similar to
  von Neumann's
arxiv_id: '2503.23608'
source_url: https://arxiv.org/abs/2503.23608
tags:
- memory
- vectors
- computing
- vector
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a von Neumann-like computing architecture for
  autonomous learning in robots, using high-dimensional vectors (e.g., 10,000 dimensions)
  to model human and animal cognition. The system includes a working memory (analogous
  to CPU) where vectors are encoded/decoded using addition, multiplication, and permutation
  operations, and a long-term associative memory (analogous to RAM) for storing vectors,
  inspired by the cerebellum's cortex structure.
---

# Autonomous Learning with High-Dimensional Computing Architecture Similar to von Neumann's

## Quick Facts
- **arXiv ID**: 2503.23608
- **Source URL**: https://arxiv.org/abs/2503.23608
- **Authors**: Pentti Kanerva
- **Reference count**: 12
- **Primary result**: Proposes von Neumann-like computing architecture using 10,000-dimensional vectors for autonomous robot learning through predictive modeling

## Executive Summary
This paper presents a computing architecture for autonomous learning that uses high-dimensional vectors to model human and animal cognition. The system features a working memory (analogous to CPU) that encodes/decodes vectors using addition, multiplication, and permutation operations, and a long-term associative memory (analogous to RAM) for storing experience. Learning occurs through real-time prediction and interaction with the environment, with the architecture aiming to achieve human-like learning efficiency and energy use.

## Method Summary
The architecture represents system state as a single "focus" vector in high-dimensional space (H ≈ 10,000). Sensory inputs and motor commands are encoded as H-dimensional vectors and integrated into the focus. The long-term associative memory, inspired by cerebellar cortex structure, stores and retrieves vectors via content addressing using sparse distributed memory principles. Operations include coordinatewise addition (majority rule), multiplication (XOR), and permutation. Learning occurs through autoassociation and sequence storage as pointer chains, enabling next-moment prediction by recalling similar past states.

## Key Results
- High-dimensional vectors (H = 10,000) provide quasi-orthogonality enabling robust superposition and decoding
- Sparse distributed memory addressed by high-dimensional vectors supports content-addressable storage with graceful degradation
- Pointer chains encoding temporal history enable next-moment prediction through sequence recall

## Why This Works (Mechanism)

### Mechanism 1: Orthogonality from High Dimensionality Enables Robust Superposition
Random vectors in sufficiently high-dimensional space are approximately orthogonal with high probability, allowing multiple vectors to be combined into a single superposed vector and later decoded. Coordinatewise operations preserve dimensionality, and decoding exploits the self-inverse property of XOR. Noise from addition is tolerated because non-orthogonal interference remains small relative to H.

### Mechanism 2: Content-Addressable Memory with Distributed Storage Supports Graceful Degradation
A sparse distributed memory addressed by high-dimensional vectors can retrieve stored patterns from noisy or partial inputs, with storage distributed across many fixed-address locations. An incoming vector activates a small subset of memory locations (< 0.1%). Storage adds the new vector to existing contents (superposition). Retrieval sums activated locations' contents and cleans up via attractor dynamics or thresholding.

### Mechanism 3: Pointer Chains Encode Temporal History for Prediction
Storing the sequence of focus vectors as a linked list (pointer chain) enables next-moment prediction: the current focus addresses memory and retrieves the previously stored successor. Each moment's focus vector F_t is stored at address F_{t-1} (or with F_{t-1} as part of the key). When F_{t-1} recurs, memory recalls F_t as prediction.

## Foundational Learning

- **Orthogonality and quasi-orthogonality in high dimensions**
  - Why needed here: Explains why random vectors can be superposed and recovered
  - Quick check question: Given two random 10,000-bit vectors, what is the expected Hamming distance?

- **Content-addressable vs. location-addressable memory**
  - Why needed here: The system retrieves by similarity, not by numeric address
  - Quick check question: How does a Hopfield network retrieve a stored pattern from a corrupted cue?

- **Binding via elementwise multiplication (Hadamard/XOR)**
  - Why needed here: Core operation for encoding variable-value pairs and structured data
  - Quick check question: If P = X ⊕ A, how do you recover A given P and X?

## Architecture Onboarding

- **Component map:**
  - Sensors → Encoder → Working Memory (HPU) → Long-term Associative Memory → Decoder → Actuators
  - Focus vector (current state) flows through this pipeline

- **Critical path:**
  1. Sense → encode → add to focus
  2. Focus addresses associative memory → retrieve prediction
  3. Compare prediction to next sensory input → compute surprise/error
  4. Update memory with new association (current focus → next focus)
  5. Extract action from focus → actuate

- **Design tradeoffs:**
  - Higher H improves orthogonality and capacity but increases compute/memory cost linearly
  - Denser activation in memory improves recall precision but reduces capacity and increases interference
  - Sparse vectors reduce storage/compute but require different operation sets

- **Failure signatures:**
  - Catastrophic forgetting: Sudden output degradation when memory capacity exceeded
  - Decoding noise accumulation: Repeated addition without cleanup produces unrecoverable vectors
  - No recurrence: Pointer-chain prediction fails if focus vectors are too distinct across time

- **First 3 experiments:**
  1. Orthogonality test: Generate 1,000 random H-dimensional vectors (H = {1,000, 5,000, 10,000}); measure pairwise cosine similarity distribution
  2. Encoding/decoding loop: Encode variable-value pairs into single superposed vector; decode each variable; measure accuracy vs. number of bound pairs
  3. Pointer-chain prediction: Implement minimal associative memory; store sequence of random vectors as pointer chain; test retrieval from noisy cue

## Open Questions the Paper Calls Out

### Open Question 1
What is an effective set of operations for sparse high-dimensional vectors that preserves the algebraic properties (associativity, commutativity, invertibility, distributivity) needed for encoding, operating on, and decoding multiple vectors from superposition?

### Open Question 2
How is the working memory functionally organized and implemented in neural circuits, given that the long-term memory (cerebellum) architecture is reasonably understood?

### Open Question 3
What programming paradigm can control a hyperdimensional computing system operating on superposed vectors, without relying on conventional programs running on traditional computers?

### Open Question 4
How should diverse sensory modalities (vision, audition, somatosensation) be encoded into a common H-dimensional vector space to enable unified processing and cross-modal association?

## Limitations
- The architecture relies on theoretical mechanisms (orthogonality preservation, sparse memory capacity, pointer-chain prediction) that lack direct empirical validation
- Specific capacity limits of the proposed associative memory and robustness of prediction in noisy environments are not quantified
- Current implementations depend on external von Neumann control rather than self-contained hyperdimensional programming

## Confidence
- **High confidence**: Theoretical foundation of high-dimensional vector operations and their biological plausibility
- **Medium confidence**: Feasibility of implementing working memory with current hardware
- **Low confidence**: Specific capacity limits of associative memory and robustness of pointer-chain prediction in realistic environments

## Next Checks
1. Systematically measure pairwise cosine similarity for random 10,000-dimensional vectors across different dimensions to verify quasi-orthogonality threshold
2. Implement associative memory with varying numbers of hard locations and measure retrieval accuracy degradation as storage density increases
3. Test pointer-chain prediction accuracy in simulated environment with controlled noise levels and sequence lengths