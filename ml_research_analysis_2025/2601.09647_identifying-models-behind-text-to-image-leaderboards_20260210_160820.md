---
ver: rpa2
title: Identifying Models Behind Text-to-Image Leaderboards
arxiv_id: '2601.09647'
source_url: https://arxiv.org/abs/2601.09647
tags:
- image
- diffusion
- prompts
- prompt
- stable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work shows that text-to-image model leaderboards can be easily
  deanonymized using embedding-based clustering. By exploiting consistent model-specific
  generation patterns, a centroid-based method achieves 91% top-1 accuracy across
  22 models and 280 prompts without training or prompt control.
---

# Identifying Models Behind Text-to-Image Leaderboards

## Quick Facts
- arXiv ID: 2601.09647
- Source URL: https://arxiv.org/abs/2601.09647
- Reference count: 40
- Models behind T2I leaderboards can be deanonymized with 91% accuracy using embedding-based clustering

## Executive Summary
This work demonstrates that text-to-image model leaderboards can be easily deanonymized using embedding-based clustering. By exploiting consistent model-specific generation patterns, a centroid-based method achieves 91% top-1 accuracy across 22 models and 280 prompts without training or prompt control. Even under strict constraints—like access to only the target model—the attack maintains strong performance (AUC > 0.92). The authors also introduce a prompt-level distinguishability metric, revealing that certain prompts enable near-perfect deanonymization. Lightweight defenses reduce accuracy but remain imperfect, highlighting fundamental vulnerabilities in current leaderboard designs.

## Method Summary
The attack uses a training-free centroid-based classification approach. For each model, k reference images are generated per prompt and embedded using a vision encoder (CLIP/ViT). Centroids are computed as the element-wise mean of these embeddings. To identify a test image's source model, its embedding is compared to all centroids via L2 distance, with the closest centroid determining the predicted model. The method requires no labeled training data and works even when only the target model is accessible (one-vs-rest setting). A distinguishability metric quantifies how vulnerable each prompt is to deanonymization by measuring the ratio of inter-model to intra-model variation in embedding space.

## Key Results
- Achieves 91% top-1 accuracy across 22 models and 280 prompts using centroid-based classification
- Maintains AUC > 0.92 in one-vs-rest setting with only target-model access
- Shows certain prompts enable near-perfect deanonymization while others are more vulnerable
- Demonstrates that lightweight defenses reduce accuracy but remain imperfect

## Why This Works (Mechanism)

### Mechanism 1: Model-Specific Clustering in Embedding Space
Different T2I models produce characteristic generation patterns that manifest as consistent stylistic, compositional, and textural differences captured by modern image encoders as tight, well-separated clusters in high-dimensional space. The clustering separability relies on embedding models capturing stylistic signatures more effectively than raw pixel features.

### Mechanism 2: Intra-Model vs. Inter-Model Variation Asymmetry
For fixed prompts, same-model generations exhibit low variability while different models diverge systematically in style, composition, or fine-grained features. This asymmetry ensures centroid-based classification works because inter-model variation dominates intra-model variation despite stochastic sampling.

### Mechanism 3: Prompt-Level Distinguishability Gradients
Prompts vary systematically in how much they expose model-specific signatures. Stylistically rich or visually constrained prompts force models into distinctive stylistic choices, producing high distinguishability, while generic or ambiguous prompts allow models to produce similar outputs, yielding lower separability.

## Foundational Learning

- **Nearest-centroid classification**: Essential for understanding the attack's core classification mechanism. Why needed: The attack classifies test images by minimum L2 distance to model centroids. Quick check: Why is computing a centroid from multiple samples more robust than using a single reference image per model?

- **Vision-language embedding spaces (CLIP-style encoders)**: Critical for grasping why clustering works in embedding space but not pixel space. Why needed: Understanding how semantic/visual similarity is encoded in high-dimensional vectors. Quick check: Why might an encoder trained on image-text pairs capture stylistic differences better than a classifier trained only on pixel-level features?

- **AUC/ROC metrics and threshold-based binary classification**: Necessary for understanding the one-vs-rest setting evaluation. Why needed: The one-vs-rest setting evaluates detection using AUC, FPR, FNR, and TPR at specific FPR thresholds. Quick check: In the one-vs-rest setting with only target-model access, what does an AUC > 0.92 indicate about the separability of that model's generations?

## Architecture Onboarding

- **Component map**: Image generation -> T2I Model API -> Generated Images (k per model) -> Vision Encoder (ViT/CLIP) -> Embedding Vector -> Element-wise mean -> Centroid vector c_i per model -> L2 distance to all centroids -> Argmin returns predicted model

- **Critical path**: 1) Generate k reference images per model for the leaderboard prompt (k=10-30 is practical) 2) Extract embeddings using a high-quality encoder 3) Compute centroid per model 4) For test image, compute distances to all centroids; return closest model

- **Design tradeoffs**:
  - Samples per model (k): More samples improve centroid stability but increase API costs. k=10 achieves near-optimal accuracy (~87% vs. ~91% at k=30)
  - Encoder quality: Modern encoders outperform older CLIP models by 3-7% accuracy
  - Defense perturbation budget (ε): Higher values reduce attack accuracy more (ε=8: 49% reduction) but introduce visible artifacts

- **Failure signatures**:
  - Low-distinguishability prompts: Attack accuracy drops; clusters overlap in embedding space
  - Similar-architecture models: Confusion matrix shows higher confusion between models from same provider/family
  - Defended images: Post-processing with ε=2 still leaves 75% attack accuracy; defense is imperfect

- **First 3 experiments**:
  1. Validate clustering hypothesis: Replicate centroid-based attack on 5 models × 50 prompts × 10 images/model. Measure top-1/top-3 accuracy vs. random baseline
  2. Establish cost-accuracy curve: Ablate k ∈ {1, 3, 5, 10, 30} to plot accuracy vs. generation cost. Confirm paper's finding that k=10 is sufficient
  3. One-vs-rest threshold analysis: For a single target model, vary α-quantile thresholds and plot ROC curve. Confirm AUC > 0.90 is achievable without access to other models

## Open Questions the Paper Calls Out

### Open Question 1
Can stronger anonymization defenses be developed that maintain visual fidelity while effectively preventing embedding-based deanonymization? The authors note their defense reduces accuracy but remains imperfect (75% accuracy even with ε=2 perturbation), creating a tension between imperceptible perturbations and effective cluster disruption.

### Open Question 2
How do model-specific signatures evolve as models undergo fine-tuning, updates, or architectural changes? The paper evaluates models at a fixed point but doesn't examine whether signatures persist across model versions, which would affect the attack's long-term viability.

### Open Question 3
Can distinguishability-guided prompt sampling enable anonymous leaderboards without compromising meaningful visual comparisons? The trade-off between security (low-distinguishability prompts) and evaluation quality (visually discriminative prompts) remains unquantified.

## Limitations

- Attack relies on a specific set of 280 prompts from the Artificial Analysis leaderboard, which are not publicly disclosed
- Performance depends heavily on encoder quality, with modern encoders achieving 91% vs 84% with older CLIP models
- Defenses reduce accuracy but remain imperfect, with 75% accuracy even with ε=2 perturbation
- One-vs-rest detection assumes the target model's generations are consistently closer to their centroid than other models' generations

## Confidence

**High confidence** in:
- Clustering hypothesis: Multiple results show consistent separability across encoders, sample sizes, and models
- Prompt-level vulnerability gradients: Distinguishability scores correlate with attack performance across diverse prompt categories
- One-vs-rest feasibility: AUC > 0.92 achieved without multi-model access in multiple experiments

**Medium confidence** in:
- Encoder superiority claims: Performance differences are shown but not explained mechanistically
- Defense limitations: Single defense method tested; alternative defenses may perform better
- Generalization to other leaderboards: Results depend on specific prompt set and model collection

**Low confidence** in:
- Long-term attack viability: Assumes current embedding spaces capture model signatures indefinitely
- Cost-accuracy tradeoff optimality: Only k up to 30 tested; higher values may improve performance

## Next Checks

1. **Cross-leaderboard validation**: Apply the attack to a different T2I leaderboard (e.g., Hugging Face Open LLM Leaderboard) with its prompt set to verify generalization beyond Artificial Analysis

2. **Encoder ablation study**: Test alternative embedding methods (e.g., DINO, MAE, or style-specific encoders) to confirm CLIP-style encoders are optimal for capturing model signatures

3. **Temporal robustness test**: Repeat the attack on generations from the same models after fine-tuning or additional training to assess whether model signatures persist or degrade over time