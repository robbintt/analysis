---
ver: rpa2
title: 'AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?'
arxiv_id: '2509.03312'
source_url: https://arxiv.org/abs/2509.03312
tags:
- agent
- arxiv
- zhang
- failure
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles the challenge of failure attribution in complex
  multi-agent LLM systems, where current models struggle to pinpoint which agent or
  step caused a failure with less than 10% accuracy. The proposed AgenTracer framework
  automatically generates annotated failure trajectories using counterfactual replay
  and programmatic fault injection, creating the large-scale TracerTraj dataset.
---

# AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?

## Quick Facts
- **arXiv ID**: 2509.03312
- **Source URL**: https://arxiv.org/abs/2509.03312
- **Reference count**: 40
- **Primary result**: AgenTracer-8B identifies failure-inducing agent and step with up to 18.18% better accuracy than large proprietary models.

## Executive Summary
AgenTracer tackles the challenge of pinpointing failure causes in complex multi-agent LLM systems, where current models achieve less than 10% accuracy. The framework automatically generates annotated failure trajectories using counterfactual replay and programmatic fault injection, creating the large-scale TracerTraj dataset. Trained on this data, AgenTracer-8B is a lightweight failure tracer using multi-granular reinforcement learning that can identify the decisive error step and agent in long agent interactions. Experiments show AgenTracer-8B achieves up to 18.18% better accuracy than giant proprietary models on the Who&When benchmark and improves off-the-shelf multi-agent systems by 4.8-14.2% through actionable feedback.

## Method Summary
AgenTracer automatically generates annotated failure trajectories using counterfactual replay and programmatic fault injection to create the TracerTraj dataset. The framework trains AgenTracer-8B, a lightweight failure tracer using multi-granular reinforcement learning with step-level Gaussian kernel and agent-level binary reward. The model identifies the decisive error step and agent in long agent interactions, achieving up to 18.18% better accuracy than large proprietary models and improving multi-agent systems by 4.8-14.2% through actionable feedback.

## Key Results
- AgenTracer-8B achieves up to 18.18% better accuracy than Gemini-2.5-Pro on the Who&When benchmark
- Improves off-the-shelf multi-agent systems by 4.8-14.2% through actionable feedback
- Successfully identifies decisive error steps in complex multi-agent interactions where existing models fail (<10% accuracy)

## Why This Works (Mechanism)

### Mechanism 1: Counterfactual Replay
- Claim: Identifies the decisive error step by finding the earliest action whose correction changes trajectory outcome from failure to success.
- Mechanism: An analyzer agent proposes corrected actions for each step t, replays the trajectory with each correction, and labels the earliest corrected step that produces a successful outcome as the decisive error.
- Core assumption: The decisive error is causally sufficient—correcting it alone allows the system to recover, assuming subsequent steps re-execute under the corrected state.
- Evidence anchors: [abstract] "counterfactual replay and programmatic fault injection"; [Section 4.1, Equation 4-5] Formal definition; [corpus] Neighbor paper "DoVer" confirms intervention-driven debugging as emerging paradigm.

### Mechanism 2: Programmatic Fault Injection
- Claim: Creates synthetic failure trajectories with known ground-truth error labels by perturbing successful trajectories.
- Mechanism: For each successful trajectory τ, select step t, apply perturbation operator Π to corrupt action at, re-execute to generate synthetic trajectory τ̃. If τ̃ fails, the injection point is the known decisive error.
- Core assumption: Synthetic failures from perturbation resemble real failure modes distributionally, and perturbations don't create unrealistic error patterns.
- Evidence anchors: [Section 4.1, Equation 9] D+ = {(τ̃, ⟨i*, t*⟩) | τ ∈ Tsucc, τ̃ = R(τ, t, Π(at)), Ω(τ̃) = 0}; [Section 4.1, Lines 9-17 of Algorithm 1] Perturbation loop.

### Mechanism 3: Multi-Granular Reward
- Claim: Multi-granular reward with step-level Gaussian kernel and agent-level binary reward provides smoother learning signal than sparse binary correctness.
- Mechanism: Step-level reward uses Gaussian decay r_step = exp(-(t̂_k - t*)² / 2σ²), giving partial credit for predictions close to true error step. Agent-level reward is binary. Format reward gates both. Combined: R = I_format × (λ × r_step + (1-λ) × r_agent).
- Core assumption: Temporal proximity to true error step is meaningful signal (near-miss predictions are better than far-off ones).
- Evidence anchors: [Section 4.2, Equation 11-12] Explicit reward formulation with λ=0.5, σ=1; [Section 4.2] "partial credit from rstep stabilizes training."

## Foundational Learning

- **Turn-based multi-agent system dynamics**: The paper models systems where only one agent acts per time step; understanding state transitions, agent scheduling μ(t), and action dependency is prerequisite for tracing causality.
  - Quick check: Can you sketch how agent A's action at step 3 could causally influence agent B's observation at step 7?

- **Reinforcement learning with GRPO (Group Relative Policy Optimization)**: AgenTracer-8B is trained via GRPO, a variant of PPO that evaluates groups of candidate predictions against each other using relative advantages.
  - Quick check: Explain why GRPO uses group-relative advantages (R_k - mean({R_j})) / std({R_j}) rather than raw rewards.

- **Counterfactual causal reasoning**: The core annotation method asks: "Had step t been corrected, would the outcome change?" This requires understanding counterfactual intervention vs. observational correlation.
  - Quick check: What's the difference between "step t is correlated with failure" and "correcting step t would prevent failure"?

## Architecture Onboarding

- **Component map**: Trajectory Collection Module -> Annotation Pipeline (counterfactual replay OR fault injection) -> TracerTraj-2.5K Dataset -> AgenTracer-8B Model -> Inference on new failed trajectories
- **Critical path**: Trajectory → Annotation (counterfactual replay OR fault injection) → TracerTraj curation → GRPO training with multi-granular reward → Inference on new failed trajectories
- **Design tradeoffs**:
  - 8B model vs. larger proprietary models: Paper claims 18.18% accuracy gain over Gemini-2.5-Pro but uses domain-specific training data. Trade-off: generalization vs. specialization.
  - Gaussian step reward (σ=1): Smoother learning but may reward predictions that are "close but wrong" in semantically meaningful ways. No ablation reported.
  - w/ G vs. w/o G evaluation: With ground truth access is easier but unrealistic. Paper shows AgenTracer-8B maintains performance better than baselines in w/o G setting.
- **Failure signatures**:
  - Surface-level attribution: Models like Qwen3-8B identify obvious errors (code execution failures) but miss root causes (wrong data retrieval earlier in trace)
  - Ground truth misleading: Paper notes some models perform worse with G access, suggesting GT can bias reasoning
  - Cascading error confusion: Case study (Figure 4) shows Coder Agent error at Step 6 was caused by Web Surfer retrieving wrong file at Step 2—early subtle errors are hardest
- **First 3 experiments**:
  1. Reproduce annotation pipeline on a single framework: Take 10 failed trajectories from MetaGPT, manually apply counterfactual replay logic, verify if identified decisive step matches human intuition.
  2. Ablate reward components: Train with only agent-level binary reward vs. full multi-granular reward; measure step-level accuracy delta.
  3. Test cross-domain transfer: Train only on TracerTraj-Code, evaluate on TracerTraj-Math held-out set to assess generalization bound.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the failure attribution framework be extended to handle distributed blame where multiple non-consecutive steps contribute cumulatively to a final error, rather than identifying a single "decisive" step?
- Basis in paper: [inferred] The Objective Formulation (Section 3) strictly defines the target as the "earliest" decisive error (Eq. 5) whose correction flips the outcome. This assumes a single point of failure, whereas complex system collapses often result from cascading errors or multiple compounding mistakes.
- Why unresolved: The dataset curation and multi-granular reward structure (Eq. 11) are explicitly designed to optimize for a single pair $(i^*, t^*)$, reinforcing the assumption that failures are locally repairable by one specific correction.
- What evidence would resolve it: An evaluation of AgenTracer on synthetic trajectories with multiple injected faults to see if it over-assigns blame to the earliest error while missing contributing factors.

### Open Question 2
- Question: To what extent does the fidelity of the TracerTraj dataset depend on the reasoning capabilities of the proprietary DeepSeek-R1 analyzer used for annotation?
- Basis in paper: [inferred] Section 4.1 relies on a specific analyzer agent ($\pi_{analyzer}$ based on DeepSeek-R1) to propose "minimally invasive" corrections for counterfactual replay. If the analyzer hallucinates a fix or fails to spot a subtle error, the resulting ground-truth label becomes noisy.
- Why unresolved: The paper does not ablate the choice of the analyzer agent or measure the noise rate in the generated annotations. It implicitly assumes the oracle is infallible enough to generate reliable training signals for the 8B student model.
- What evidence would resolve it: An ablation study generating TracerTraj variants using smaller or less capable analyzers (e.g., Qwen-32B) and comparing the subsequent training performance of the failure tracer.

### Open Question 3
- Question: How does AgenTracer-8B perform on multi-agent systems with asynchronous or concurrent execution, given its training is constrained to turn-based protocols?
- Basis in paper: [inferred] Section 3 (Preliminaries) explicitly defines the system dynamics as turn-based, stating "only one agent is active at each time step." The collected trajectories from frameworks like MetaGPT also follow sequential orchestration.
- Why unresolved: The model learns positional dependencies and causal links in a sequential context. It is unclear if the reasoning capabilities transfer to environments where agents execute actions in parallel, interleaving logs without a strict global order.
- What evidence would resolve it: Testing the pre-trained AgenTracer-8B on a benchmark of concurrent multi-agent logs (e.g., swarm robotics or asynchronous coding tasks) without fine-tuning.

## Limitations
- Counterfactual assumption validity: The "earliest decisive error" premise assumes errors are causally sufficient in isolation, but cascading failures may violate this.
- Synthetic vs. real failure distribution: Fault injection perturbations may generate error patterns that don't match real-world failure modes.
- Reward function design: Gaussian step reward assumes temporal proximity equals partial credit, but semantic distance between predictions isn't validated.

## Confidence
- **High confidence**: Annotation pipeline mechanics, dataset construction methodology, baseline comparison methodology
- **Medium confidence**: Multi-granular reward effectiveness, counterfactual replay correctness, fault injection representativeness
- **Low confidence**: Generalization to unseen agent architectures, robustness to noisy agent outputs, real-world deployment impact

## Next Checks
1. **Cross-distribution validation**: Compare synthetic failure patterns from fault injection against real failure distributions using KL divergence or domain classifier; report distribution shift metrics.
2. **Cascading failure stress test**: Create synthetic trajectories with multiple dependent errors; measure whether AgenTracer correctly identifies root vs. symptomatic errors.
3. **Zero-shot transfer experiment**: Evaluate AgenTracer-8B on multi-agent systems from frameworks not in the training corpus; measure accuracy drop and analyze failure modes.