---
ver: rpa2
title: 'Closing Gaps: An Imputation Analysis of ICU Vital Signs'
arxiv_id: '2510.24217'
source_url: https://arxiv.org/abs/2510.24217
tags:
- imputation
- methods
- data
- missingness
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work benchmarks 15 imputation methods on ICU vital signs data,
  comparing their performance across multiple missingness types and datasets. The
  study evaluates methods including traditional baselines, algorithmic approaches,
  RNNs, attention models, and generative diffusion models.
---

# Closing Gaps: An Imputation Analysis of ICU Vital Signs

## Quick Facts
- arXiv ID: 2510.24217
- Source URL: https://arxiv.org/abs/2510.24217
- Reference count: 40
- 15 imputation methods benchmarked on ICU vital signs data across multiple missingness types

## Executive Summary
This paper presents a comprehensive benchmark of 15 imputation methods for ICU vital signs time-series data, evaluating their performance across three major datasets (MIMIC-IV, eICU, HiRID) with varying missingness patterns. The study introduces the YAIB framework that standardizes comparison of imputation techniques, addressing the practical challenge of selecting appropriate methods for clinical prediction tasks where missing data is common. Attention-based methods emerge as top performers for accuracy metrics, while GRU-D and CSDI excel at root mean squared error.

## Method Summary
The benchmark evaluates 15 imputation methods including traditional baselines (Mean, KNN), algorithmic approaches (MICE, MissForest), recurrent neural networks (GRU-D), attention models (Attention Imputer, Transformer), and generative diffusion models. Experiments use three ICU datasets downsampled to 1-hour intervals, with 6 vital signs (hr, resp, o2sat, map, sbp, dbp). Missingness is simulated using four mechanisms: MCAR (missing completely at random), MAR (missing at random), MNAR (not missing at random), and Blackout (extended gaps). Methods are evaluated on reconstruction accuracy using MAE, RMSE, and JSD metrics across 30%, 50%, and 70% missingness rates.

## Key Results
- Attention-based methods achieve the best overall performance in terms of MAE and JSD metrics
- GRU-D and CSDI methods excel at RMSE, particularly for handling large errors
- Performance varies significantly by missingness type, with MAR generally showing best results
- Attention Imputer demonstrates strong generalization across different missingness patterns

## Why This Works (Mechanism)
The study addresses a critical gap in clinical machine learning where missing data in vital signs can significantly impact prediction accuracy. By systematically evaluating multiple imputation approaches across standardized missingness patterns, the research provides evidence-based guidance for method selection. The YAIB framework enables reproducible comparisons and facilitates the integration of new imputation techniques into the evaluation pipeline.

## Foundational Learning
- **Missingness mechanisms (MCAR, MAR, MNAR)**: Understanding different missing data patterns is crucial for selecting appropriate imputation strategies; quick check: verify missingness masks show expected patterns before imputation
- **Time-series imputation techniques**: Different methods handle temporal dependencies differently; quick check: compare imputed sequences against ground truth for temporal consistency
- **Evaluation metrics (MAE, RMSE, JSD)**: Multiple metrics capture different aspects of imputation quality; quick check: ensure metric calculations match paper definitions
- **Diffusion models for imputation**: Novel approach using generative models to handle missing data; quick check: verify diffusion steps produce reasonable imputations
- **Attention mechanisms in sequential data**: Captures long-range dependencies effectively; quick check: visualize attention weights for temporal patterns

## Architecture Onboarding

**Component Map:** Data preprocessing -> Amputation (missingness injection) -> Imputation method -> Evaluation metrics -> Results aggregation

**Critical Path:** 1) Data access and preprocessing 2) Missingness simulation 3) Imputation execution 4) Metric computation 5) Statistical analysis

**Design Tradeoffs:** The framework prioritizes flexibility and reproducibility over computational efficiency, allowing easy integration of new methods but requiring significant computational resources for comprehensive benchmarking.

**Failure Signatures:** High variance in results (particularly CSDI with ±4-32 standard deviations) suggests sensitivity to initialization or data splits; inconsistent missingness proportions indicate issues with amputation implementation.

**First Experiments:**
1. Run GRU-D imputation on MIMIC-IV MCAR 50% missingness with default parameters
2. Verify missingness mask statistics match expected 50% rate across all vital signs
3. Compare MAE and RMSE against paper-reported ranges for validation

## Open Questions the Paper Calls Out
None

## Limitations
- High sensitivity of some methods (particularly CSDI) to initialization and data splits
- Focus on imputation accuracy rather than downstream clinical task performance
- 1-hour downsampling may obscure important temporal patterns in higher-frequency data

## Confidence

**High confidence:** Relative performance rankings between imputation methods, benchmark framework usability, and general methodology

**Medium confidence:** Absolute performance values and standard deviation calculations, given uncertainty about replication parameters

**Low confidence:** Claims about diffusion model performance relative to other methods due to potential implementation complexity and limited comparison runs

## Next Checks

1. Run GRU-D, Attention, and CSDI imputation on MIMIC-IV MCAR 50% missingness with 5 different random seeds to verify if RMSE and MAE ranges match paper results within reasonable tolerance (±10%)

2. Check missingness mask statistics after amputation to confirm 30%, 50%, and 70% rates are accurately achieved across all datasets

3. Validate the PhysioNet credentialing process and data access pipeline by successfully downloading and preprocessing MIMIC-IV before running any imputation experiments