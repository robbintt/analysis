---
ver: rpa2
title: 'Fusion-Augmented Large Language Models: Boosting Diagnostic Trustworthiness
  via Model Consensus'
arxiv_id: '2510.16057'
source_url: https://arxiv.org/abs/2510.16057
tags:
- consensus
- multimodal
- clinical
- cases
- diagnostic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a consensus-based multi-LLM framework that
  combines outputs from ChatGPT and Claude to improve chest X-ray diagnostic accuracy.
  Using semantic similarity scoring and multimodal inputs (images plus synthetic clinical
  notes), the approach achieved a consensus accuracy of 91.3% in multimodal cases,
  outperforming individual models.
---

# Fusion-Augmented Large Language Models: Boosting Diagnostic Trustworthiness via Model Consensus

## Quick Facts
- **arXiv ID:** 2510.16057
- **Source URL:** https://arxiv.org/abs/2510.16057
- **Reference count:** 32
- **Primary result:** Achieved 91.3% consensus accuracy for multimodal chest X-ray diagnosis using BERTScore-filtered agreement between ChatGPT and Claude

## Executive Summary
This paper introduces a consensus-based multi-LLM framework that combines outputs from ChatGPT (GPT-4) and Claude (3.7-sonnet) to improve chest X-ray diagnostic accuracy. The approach uses semantic similarity scoring (BERTScore) and multimodal inputs (images plus synthetic clinical notes) to filter and validate model predictions. By requiring both models to agree on binary classification outcomes for 14 thoracic conditions, the method achieves higher accuracy than either model alone while providing interpretable results through disagreement flagging. The framework is model-agnostic, requires no fine-tuning, and demonstrates that integrating multiple LLMs with multimodal inputs can significantly boost diagnostic reliability in radiological applications.

## Method Summary
The study employs a consensus-based fusion approach using two LLMs (ChatGPT and Claude) to classify 14 thoracic conditions from chest X-rays. For multimodal cases, synthetic clinical notes are generated following a 5-part MIMIC-CXR template and combined with base64-encoded JPEG images. Both models receive identical prompts via LangChain, and their outputs are compared using BERTScore (threshold ≥95%). The method extracts image embeddings via CLIP ViT-B/32 (512-dim) and text embeddings via BioClinicalBERT (768-dim). Consensus predictions are accepted when both models agree above the similarity threshold, while disagreements are flagged for human review. The approach requires no fine-tuning and works with parallel API calls to both LLMs.

## Key Results
- Achieved 91.3% consensus accuracy in multimodal cases (image + synthetic clinical notes)
- Reached 77.6% consensus accuracy in image-only settings
- Demonstrated statistically significant improvement over individual model performance via McNemar's test
- Maintained model-agnostic design without requiring fine-tuning or specialized training

## Why This Works (Mechanism)
The method leverages ensemble learning principles by combining multiple LLM outputs, reducing individual model biases and errors. BERTScore-based semantic similarity filtering ensures only high-confidence, mutually reinforcing predictions are accepted as consensus. The multimodal approach (combining visual and textual clinical information) provides richer context for the LLMs, improving diagnostic reasoning. By flagging disagreements rather than forcing consensus, the system maintains transparency and enables human oversight where models diverge.

## Foundational Learning
- **BERTScore**: Semantic similarity metric using contextual embeddings; needed for measuring LLM output agreement; quick check: verify outputs meet ≥95% threshold
- **Base64 image encoding**: Standard format for transmitting images to LLM APIs; needed for consistent input processing; quick check: confirm successful decoding in both models
- **LangChain parallel execution**: Framework for coordinating multiple LLM calls; needed for synchronized inference; quick check: verify both models receive identical prompts
- **Synthetic clinical note generation**: Creation of structured medical text following MIMIC-CXR template; needed for multimodal input enhancement; quick check: validate note structure matches template
- **McNemar's test**: Statistical method for comparing paired classifier outputs; needed to prove significance of improvements; quick check: confirm χ² statistic meets significance threshold
- **CLIP ViT-B/32 embeddings**: Visual feature extraction for image-text alignment; needed for multimodal embedding validation; quick check: verify cosine similarity between aligned embeddings

## Architecture Onboarding
- **Component map**: CheXpert dataset -> Image preprocessing (DICOM → base64) -> Synthetic note generation -> CLIP/BioClinicalBERT embedding -> LangChain parallel inference -> BERTScore comparison -> Consensus filtering -> Output
- **Critical path**: Base64 image + synthetic note → parallel LLM inference → BERTScore computation → consensus threshold check → final prediction
- **Design tradeoffs**: No fine-tuning required (model-agnostic) vs. dependence on API availability and costs; BERTScore threshold ≥95% (high confidence) vs. potentially missing borderline cases; synthetic notes vs. real clinical documentation
- **Failure signatures**: Low agreement rate (<72%) suggests prompt formatting issues or model misalignment; BERTScore computation errors indicate binary output incompatibility; API rate limits prevent full dataset processing
- **Three first experiments**: 1) Test base64 encoding/decoding with small image subset to verify API compatibility; 2) Generate and validate synthetic clinical notes for 5-10 cases to confirm template adherence; 3) Run parallel inference on 2-3 cases to verify LangChain coordination and BERTScore computation

## Open Questions the Paper Calls Out
None

## Limitations
- Key experimental details undisclosed: specific CheXpert study IDs, synthetic note templates, and API parameters prevent exact replication
- BERTScore may be suboptimal for binary classification outputs ("0"/"1"), potentially affecting consensus accuracy
- High computational costs and API rate limits for processing large datasets with multiple parallel calls
- Synthetic clinical notes may not capture the full complexity and variability of real medical documentation

## Confidence
- Consensus methodology and fusion approach: **High confidence** (method fully specified)
- Multimodal accuracy improvement (91.3%): **Medium confidence** (method specified, but data generation not reproducible)
- Model-agnostic claims: **High confidence** (no fine-tuning required is explicitly stated)
- BERTScore suitability for binary outputs: **Low confidence** (methodological concern not addressed)

## Next Checks
1. Replicate the BERTScore computation on a small binary output subset using both free-text and exact-match metrics to verify which approach better captures LLM agreement
2. Conduct sensitivity analysis by varying the BERTScore consensus threshold (90%, 92%, 95%, 97%) to determine optimal balance between consensus accuracy and agreement rate
3. Implement the full pipeline with a small validation set (10-20 studies) using publicly available CheXpert data to verify API integration, base64 encoding, and multimodal prompt formatting before scaling to full reproduction