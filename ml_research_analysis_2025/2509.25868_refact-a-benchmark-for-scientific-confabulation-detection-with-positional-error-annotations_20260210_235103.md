---
ver: rpa2
title: 'ReFACT: A Benchmark for Scientific Confabulation Detection with Positional
  Error Annotations'
arxiv_id: '2509.25868'
source_url: https://arxiv.org/abs/2509.25868
tags:
- answer
- question
- confabulation
- factual
- masked
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReFACT, a benchmark designed to evaluate
  the detection of scientific confabulation in large language models (LLMs). ReFACT
  consists of 1,001 question-answer pairs from the Reddit community r/AskScience,
  with each answer having a factual counterpart and a confabulated version annotated
  with precise error spans and types.
---

# ReFACT: A Benchmark for Scientific Confabulation Detection with Positional Error Annotations

## Quick Facts
- **arXiv ID**: 2509.25868
- **Source URL**: https://arxiv.org/abs/2509.25868
- **Reference count**: 16
- **Key result**: Current LLMs achieve only ~54% accuracy in detecting scientific confabulation

## Executive Summary
This paper introduces ReFACT, a benchmark designed to evaluate the detection of scientific confabulation in large language models (LLMs). ReFACT consists of 1,001 question-answer pairs from the Reddit community r/AskScience, with each answer having a factual counterpart and a confabulated version annotated with precise error spans and types. The benchmark enables multi-stage evaluation: (1) confabulation detection, (2) fine-grained error localization, and (3) correction. Nine state-of-the-art LLMs were benchmarked, revealing that even the best models, including GPT-4o, achieve only about 54% accuracy in detecting confabulation. These findings highlight the challenges in reliably distinguishing factual from confabulated scientific content and question the effectiveness of LLM-as-judge evaluation paradigms. The results underscore the need for fine-grained, human-validated benchmarks to improve the detection and correction of scientific confabulation in domain-specific contexts.

## Method Summary
ReFACT evaluates LLMs on three specific capabilities regarding scientific confabulation: (1) Judgment (binary classification of factual vs. confabulated), (2) Localization (identifying error spans), and (3) Correction (recovering original entities). The benchmark uses 1,001 QA pairs sourced from r/AskScience, with each answer having a factual counterpart and a transformed version (527 via Negation, 474 via Entity Replacement). Evaluation is zero-shot using exact prompt templates provided in Appendix C. The dataset was created by generating confabulations with Gemma-2-27B-it, filtering using NLTK and a DeBERTa-v3 NLI classifier, and applying 3-person human annotation via doccano (72.56% agreement). Metrics include Accuracy/F1 for Judgment, IoU for Localization, and Exact Match for Correction.

## Key Results
- GPT-4o achieves only 54.1% accuracy in confabulation detection (Judgment task)
- Even the best models struggle with fine-grained error localization (Localization task)
- Performance drops significantly in the Comparative Judgment setting versus Independent Judgment
- Current LLMs show limited capability in correcting confabulated scientific content

## Why This Works (Mechanism)
The benchmark works by providing a controlled environment where models must distinguish between factual and confabulated scientific content with precise error annotations. The multi-stage evaluation (judgment, localization, correction) allows for granular assessment of different aspects of confabulation detection. The use of zero-shot prompting ensures fair comparison across models while the human-validated annotations provide ground truth for evaluation.

## Foundational Learning
- **Zero-shot prompting**: Evaluating models without examples or fine-tuning to assess baseline capabilities - needed to compare models fairly; quick check: verify models can follow basic instruction format
- **IoU (Intersection over Union)**: Metric for evaluating overlap between predicted and actual error spans - needed for localization task; quick check: ensure tokenization is consistent between prediction and ground truth
- **Entity Replacement vs. Negation**: Two distinct confabulation generation strategies - needed to create diverse confabulated content; quick check: verify both types are balanced in the dataset
- **Reddit filtering criteria**: Score ≥ 4 and length 500-1000 chars - needed to ensure quality content; quick check: confirm filtered content meets scientific standards
- **Human annotation agreement**: 72.56% agreement among 3 annotators - needed to validate annotation quality; quick check: verify annotation consistency on sample pairs

## Architecture Onboarding
- **Component map**: Reddit content → Gemma-2 generation → NLI filtering → Human annotation → Benchmark evaluation
- **Critical path**: Question-Answer pairs → Model inference → Answer parsing → Metric calculation
- **Design tradeoffs**: Synthetic confabulation generation vs. real-world errors; zero-shot evaluation vs. few-shot potential; isolated task evaluation vs. end-to-end assessment
- **Failure signatures**: Models may output explanatory text instead of binary answers; span alignment issues due to whitespace/punctuation differences; comparative judgment instability with high variance
- **First experiments**: 1) Run zero-shot inference with provided prompts on a subset of 10 pairs; 2) Calculate IoU scores for localization task; 3) Compare Independent vs. Comparative Judgment performance

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark relies on Reddit-sourced content (r/AskScience) which may not fully capture peer-reviewed scientific literature complexity
- Synthetic confabulation generation (Negation and Entity Replacement) may not represent the full spectrum of real-world scientific misinformation patterns
- Human annotation process involves only three annotators without comprehensive inter-annotator reliability metrics

## Confidence
- **High Confidence**: Benchmark construction methodology is clearly specified and reproducible; reported results showing LLMs achieving only ~54% accuracy are robust
- **Medium Confidence**: Conclusion that current LLMs struggle with scientific confabulation detection is well-supported
- **Low Confidence**: Claim about "questioning the effectiveness of LLM-as-judge paradigms" extends beyond immediate results

## Next Checks
1. Reproduce IoU calculation implementation to verify reported localization scores, particularly the 0.5 IoU threshold
2. Test the benchmark with at least two additional open-weight models using the same prompt templates
3. Conduct a small-scale replication study with independent annotators on a subset of 50 question-answer pairs to verify annotation reliability