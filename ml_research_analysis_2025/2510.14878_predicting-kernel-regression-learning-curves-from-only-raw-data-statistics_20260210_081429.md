---
ver: rpa2
title: Predicting kernel regression learning curves from only raw data statistics
arxiv_id: '2510.14878'
source_url: https://arxiv.org/abs/2510.14878
tags:
- kernel
- gaussian
- data
- page
- hermite
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a theoretical framework that predicts kernel
  regression learning curves from only raw data statistics, specifically the empirical
  data covariance matrix and an empirical polynomial decomposition of the target function.
  The key innovation is the Hermite Eigenstructure Ansatz (HEA), which provides an
  analytical approximation of a kernel's eigenvalues and eigenfunctions with respect
  to anisotropic data distributions using Hermite polynomials.
---

# Predicting kernel regression learning curves from only raw data statistics

## Quick Facts
- arXiv ID: 2510.14878
- Source URL: https://arxiv.org/abs/2510.14878
- Reference count: 40
- Primary result: Presents theoretical framework predicting kernel regression learning curves from raw data statistics using Hermite Eigenstructure Ansatz

## Executive Summary
This paper introduces a theoretical framework that predicts kernel regression learning curves without constructing or diagonalizing kernel matrices. The key innovation is the Hermite Eigenstructure Ansatz (HEA), which analytically approximates a kernel's eigenvalues and eigenfunctions using Hermite polynomials when data is "Gaussian enough." The framework successfully predicts test MSE vs. sample size relationships on CIFAR-5m, SVHN, and ImageNet, and demonstrates that MLPs learn Hermite polynomials in the order predicted by HEA.

## Method Summary
The framework predicts KRR learning curves by computing empirical data covariance, estimating target function coefficients in the Hermite basis, and applying the HEA to predict eigenvalues. Instead of numerical eigendecomposition, it uses analytical formulas derived from Hermite polynomials. For non-Gaussian data, it orthogonalizes the Hermite basis using Gram-Schmidt on sampled data. The method plugs these predictions into existing eigenframework equations to compute test error, avoiding explicit kernel matrix construction.

## Key Results
- HEA accurately predicts KRR learning curves on CIFAR-5m, SVHN, and ImageNet without constructing kernel matrices
- Predictions match empirical results across different kernel types (Gaussian, Laplace) and target functions
- MLPs in feature-learning regime learn Hermite polynomials in the order predicted by HEA
- Framework demonstrates end-to-end theory mapping dataset structure to model performance for nontrivial learning algorithms

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The eigenfunctions of rotation-invariant kernels on high-dimensional data are well-approximated by multivariate Hermite polynomials.
- **Mechanism:** When data is "Gaussian enough," the orthogonal basis of the kernel integral operator aligns with the orthogonal polynomials of the Gaussian measure (Hermite polynomials). Real image datasets share sufficient second-order structure with Gaussians for this approximation to hold.
- **Core assumption:** The data distribution is close enough to a Gaussian that higher-order moments do not significantly distort the kernel's spectral structure.
- **Break condition:** Simple or low-dimensional datasets (MNIST, tabular data) deviate from Gaussian, degrading HEA prediction.

### Mechanism 2
- **Claim:** Kernel eigenvalues are determined by the decay rate of the kernel's level coefficients and data covariance variances.
- **Mechanism:** The kernel is approximated as a dot-product kernel with Taylor expansion. Eigenvalues scale as c_ℓγ^ℓ where c_ℓ are kernel coefficients and γ is data variance along a direction.
- **Core assumption:** The kernel is wide relative to data spread (σ→∞), ensuring level coefficients decay fast enough (c_{ℓ+1}/c_ℓ≪1) to create well-separated eigenvalue bands.
- **Break condition:** Narrow kernels (small σ) violate fast-decay assumption, causing higher-order modes to dominate and prediction to fail.

### Mechanism 3
- **Claim:** Learning curves can be predicted analytically without diagonalizing the kernel matrix.
- **Mechanism:** Instead of solving for empirical kernel eigensystem, plug analytical HEA eigenvalues and target function's Hermite coefficients into existing eigenframework equations relating task eigenstructure to risk.
- **Core assumption:** Standard KRR eigenframework equations remain accurate predictors in finite-sample, real-data regime.
- **Break condition:** Finite-size effects or non-orthogonality in Hermite basis introduce estimation errors, degrading prediction accuracy.

## Foundational Learning

- **Concept: Mercer's Theorem & Kernel Eigenfunctions**
  - **Why needed here:** The entire paper relies on spectral view of kernels (decomposing K into eigenvalues λᵢ and orthonormal eigenfunctions φᵢ) to apply HEA.
  - **Quick check question:** Can you explain why solving a kernel regression problem is equivalent to projecting the target function onto the kernel's eigenbasis?

- **Concept: Probabilist's Hermite Polynomials**
  - **Why needed here:** These form the functional basis of predicted eigensystem. Understanding their orthogonality with respect to Gaussian measure is essential for proofs.
  - **Quick check question:** What is the orthogonality condition for normalized probabilist's Hermite polynomials hₙ(x) with respect to standard normal distribution?

- **Concept: The Neural Tangent Kernel (NTK)**
  - **Why needed here:** The paper connects kernel theory back to MLPs by treating network as kernel regression using NTK.
  - **Quick check question:** In infinite-width limit, why does NTK remain constant during training, allowing use of linear regression theory?

## Architecture Onboarding

- **Component map:** Data Covariance (Σ) -> Level Coefficients (c_ℓ) -> Hermite Basis Construction -> Target Decomposition -> Risk Calculation
- **Critical path:** The Gram-Schmidt orthogonalization of sampled Hermite polynomials. This is required because real data is not perfectly Gaussian, causing naive Hermite basis to be non-orthogonal; failure to orthogonalize leads to overestimated coefficient power.
- **Design tradeoffs:**
  - Truncation vs. Accuracy: Computing all infinite eigenmodes is impossible. Truncating at P introduces bias for slow-decaying kernels (Laplace) unless corrected via "tail-corrected ridge."
  - Complexity vs. Gaussianity: HEA works better on complex datasets (CIFAR) than simple ones (MNIST). Do not assume this architecture works for low-dimensional tabular data.
- **Failure signatures:**
  - Diverging Eigenvalues: For Laplace/ReLU kernels on low-dimensional data, predicted eigenvalues λ_α may grow super-exponentially due to c_ℓ behavior, leading to infinite trace errors.
  - Poor Spectral Overlap: If heatmap of empirical vs. predicted eigenfunctions shows off-diagonal energy, "Gaussian enough" assumption has failed.
- **First 3 experiments:**
  1. Gaussian Sanity Check: Generate synthetic Gaussian data with known covariance. Verify analytical HEA eigenvalues match numerically diagonalized kernel matrix eigenvalues (Theorem 1).
  2. Width Sweep: On fixed real dataset (CIFAR-5m), decrease kernel width σ. Observe HEA prediction degradation as narrow kernel regime violates fast-decay assumption (Figure 13).
  3. Coefficient Recovery: On non-Gaussian dataset, compare learning curve predictions using "Naive" vs. "Gram-Schmidt" coefficient estimation (Figure 9) to validate necessity of orthogonalization step.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the condition of data being "Gaussian enough" for HEA be rigorously defined and quantified?
- **Basis in paper:** Section E states, "We do not attempt to give a precise definition here... This seems like a worthwhile direction for future exploration, especially from the statistics community."
- **Why unresolved:** The paper relies on empirical heuristics (visual inspection of marginals) to determine if dataset fits HEA, lacking formal statistical measure.
- **What evidence would resolve it:** A theoretical metric (based on moment bounds or information geometry) that predicts accuracy of HEA predictions on non-Gaussian datasets.

### Open Question 2
- **Question:** What determines the scaling exponent of optimization time with respect to HEA eigenvalue in "ultra-rich" regime (ζ≫1) of feature-learning MLPs?
- **Basis in paper:** Section F notes, "What is unknown is the scaling exponent as ζ≫1, which we find to be lower than baseline 1/2... We suspect the 0.4 slope... is thus not fundamental."
- **Why unresolved:** Observed scaling exponent changes from theoretically predicted 1/2 to approximately 0.4 as richness parameter increases, unclear if this is fundamental property or hyperparameter artifact.
- **What evidence would resolve it:** Theoretical derivation of time-to-learn scaling law for MLPs in ultra-rich regime, or empirical evidence showing exponent stabilizes to constant across varied architectures.

### Open Question 3
- **Question:** Can HEA framework be extended to theoretically characterize learning for algorithms beyond KRR and feature-learning MLPs?
- **Basis in paper:** Section 7 (Discussion) states, "Theories of this sort applicable to more general algorithms may be a good end goal for learning theory."
- **Why unresolved:** Current framework serves as "proof of concept" specifically for KRR and MLPs; unknown if ansatz holds for more complex architectures like CNNs or Transformers.
- **What evidence would resolve it:** Successful application of HEA or modified ansatz to predict learning curves in Convolutional Neural Networks or self-attention layers.

### Open Question 4
- **Question:** Does Hermite eigenstructure ansatz hold for rotation-invariant kernels that are analytic but not strictly dot-product kernels?
- **Basis in paper:** Section B.3 mentions, "One could disentangle this by studying a rotation-invariant kernel that... is still analytic... Such a kernel is not among those studied here, so we leave this to future work."
- **Why unresolved:** Paper identifies divergence issues with non-analytic kernels (Laplace) and solves dot-product kernels; behavior of analytic kernels that fall between these categories is unexplored.
- **What evidence would resolve it:** Theoretical proof or empirical verification of HEA for kernel class that is analytic but not expressible solely as function of x⊤y.

## Limitations

- The framework's reliance on the "Gaussian enough" assumption limits its applicability to datasets with significant non-Gaussian structure, particularly low-dimensional or simple datasets.
- Truncation of Hermite polynomials at finite P and the tail-corrected ridge approach introduce approximation errors that are not fully quantified, potentially affecting prediction accuracy.
- The method assumes standard KRR eigenframework equations remain accurate predictors in finite-sample regimes, which may not hold for all real-world scenarios.

## Confidence

- **High confidence:** The HEA's validity for Gaussian data (Theorem 1) and its successful application to predict learning curves on CIFAR-5m, SVHN, and ImageNet datasets.
- **Medium confidence:** The extrapolation to non-Gaussian datasets like ImageNet, where framework still performs well despite theoretical requirement for Gaussian data.
- **Low confidence:** The method's performance on low-dimensional datasets (MNIST, Mushrooms) and exact behavior of truncation strategy for kernels with slow-decaying level coefficients.

## Next Checks

1. **Dimension Reduction Test**: Apply framework to progressively lower-dimensional versions of CIFAR-5m (using PCA to reduce to 10, 50, 100 dimensions) and measure prediction accuracy degradation as dimensionality decreases.

2. **Kernel Width Sweep Validation**: Systematically vary kernel width σ on CIFAR-5m and verify that predictions fail precisely when c_{ℓ+1}γ₁/c_ℓ approaches 1, confirming fast-decay assumption's critical role.

3. **Target Coefficient Recovery**: For non-Gaussian synthetic data with known target coefficients, compare learning curve predictions using naive vs. Gram-Schmidt orthogonalized coefficient estimation to quantify orthogonalization step's impact on prediction accuracy.