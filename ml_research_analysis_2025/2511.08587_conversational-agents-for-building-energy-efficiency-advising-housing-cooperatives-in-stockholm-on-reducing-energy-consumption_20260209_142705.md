---
ver: rpa2
title: Conversational Agents for Building Energy Efficiency -- Advising Housing Cooperatives
  in Stockholm on Reducing Energy Consumption
arxiv_id: '2511.08587'
source_url: https://arxiv.org/abs/2511.08587
tags:
- energy
- spara
- efficiency
- building
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SPARA, a conversational agent system designed
  to assist housing cooperatives in Stockholm with energy efficiency decisions. It
  uses a Retrieval-Augmented Generation (RAG) framework with GPT-4o to provide tailored
  advice based on email communications between energy advisors and cooperatives.
---

# Conversational Agents for Building Energy Efficiency -- Advising Housing Cooperatives in Stockholm on Reducing Energy Consumption

## Quick Facts
- arXiv ID: 2511.08587
- Source URL: https://arxiv.org/abs/2511.08587
- Reference count: 11
- Primary result: SPARA conversational agent achieves 80% precision on general energy questions and 85% accuracy on specific building queries

## Executive Summary
SPARA is a conversational agent system designed to assist housing cooperatives in Stockholm with energy efficiency decisions. It leverages a Retrieval-Augmented Generation (RAG) framework with GPT-4o to provide tailored advice based on email communications between energy advisors and cooperatives. The system achieves strong performance metrics while maintaining traceability and avoiding speculative responses. SPARA is currently in pilot testing with municipal energy experts and housing cooperatives, demonstrating its potential to bridge the gap between technical expertise and practical decision-making for non-expert cooperative board members.

## Method Summary
SPARA uses a RAG framework with GPT-4o as the core language model, incorporating external knowledge bases to ground responses in retrieved evidence rather than parametric memory alone. The system processes queries through a four-service architecture: frontend (React chatbot UI with email ingestion), backend (Socket.IO, Python email parser, PostgreSQL), messaging layer (Redis queue with workers), and language service (GPT-4o with RAG using Azure Blob Storage for documents and Azure Cosmos DB for vector embeddings). A low temperature setting reduces hallucinations, and the system is designed to decline answering when context is insufficient.

## Key Results
- 80% precision achieved on answering general energy-related questions
- 85% accuracy on specific building-level numeric queries
- Strong semantic alignment captured by embeddings (47/50 responses scored 0.8-1.0 on cosine similarity)
- High lexical overlap with reference answers (41/50 responses scored above 0.8 on Jaccard similarity)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG enables domain-specific accuracy beyond the base LM's training data.
- Mechanism: User query → vector embedding → similarity search against knowledge base → retrieved documents injected as context → LM generates response grounded in retrieved evidence rather than parametric memory alone.
- Core assumption: The knowledge base contains sufficiently relevant, accurate, and up-to-date information that matches user intent.
- Evidence anchors: [abstract] "SPARA functions as an energy efficiency advisor by leveraging the Retrieval-Augmented Generation (RAG) framework with a Language Model(LM)." [section] Page 4: "RAG addresses this limitation by incorporating external knowledge bases, enabling the model to retrieve and utilize relevant information during the query-response process."

### Mechanism 2
- Claim: Vector similarity metrics (cosine, Jaccard) provide quantifiable alignment between generated and expert reference answers.
- Mechanism: Both reference and generated responses are embedded; cosine similarity captures semantic alignment while Jaccard captures lexical overlap; high scores suggest factual and contextual correspondence.
- Core assumption: High similarity scores correlate with advice that experts would judge accurate and actionable.
- Evidence anchors: [section] Page 5: "47 out of 50 responses fell within the 0.8–1.0 range, reinforcing the strong semantic alignment captured by embeddings." [section] Page 6: "41 out of 50 responses achieved a similarity score above 0.8, suggesting a high degree of lexical overlap."

### Mechanism 3
- Claim: Constraining LM creativity via low temperature and explicit knowledge grounding reduces speculative or hallucinated responses.
- Mechanism: Temperature parameter controls output randomness; RAG forces responses to cite retrieved context; system is designed to decline when context is insufficient.
- Core assumption: Users prefer "I don't know" over plausible but incorrect guidance in high-stakes retrofit decisions.
- Evidence anchors: [section] Page 2: "We employ retrieval-augmented generation (RAG) for traceability and set a low temperature to minimize excessive creativity." [section] Page 7: "When faced with domain-specific or ambiguous questions that require expert knowledge, SPARA refrains from providing speculative responses."

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: SPARA's core architecture; without understanding RAG, you cannot debug retrieval failures or context injection issues.
  - Quick check question: If a user asks about a building not in the knowledge base, what should the system return and why?

- Concept: **Vector embeddings and similarity search**
  - Why needed here: Retrieval depends on embedding quality; poor chunking or embedding models degrade relevance.
  - Quick check question: What is the difference between cosine similarity and Jaccard similarity in evaluating response quality?

- Concept: **Conversational agent service orchestration**
  - Why needed here: SPARA uses four decoupled services; understanding message flow is essential for debugging latency or ordering issues.
  - Quick check question: What happens to query ordering when multiple Redis workers process requests in parallel?

## Architecture Onboarding

- Component map: Frontend (React chatbot UI + email ingestion) → Backend (Socket.IO, Python email parser, PostgreSQL) → Messaging (Redis queue, workers) → Language Service (GPT-4o + RAG: Azure Blob Storage for documents, Azure Cosmos DB for vector embeddings)

- Critical path:
  1. User submits query via chat or email
  2. Backend normalizes input and pushes to Redis queue
  3. Worker retrieves query, triggers Language Service
  4. RAG retrieves relevant documents via vector similarity
  5. Augmented prompt sent to GPT-4o; response returned through Backend to Frontend/email

- Design tradeoffs:
  - Single vs. multiple Redis workers: single preserves ordering; multiple enables parallelism but loses guaranteed order
  - Low temperature reduces hallucinations but may increase refusal rate
  - Knowledge base scope: broader coverage improves recall but may dilute precision

- Failure signatures:
  - High latency: check Redis queue backlog or vector search performance
  - Refusal-heavy responses: verify knowledge base contains relevant documents for query domain
  - Inconsistent numeric answers: confirm data ingestion pipeline correctly updates building-specific metrics

- First 3 experiments:
  1. Query a building ID present in knowledge base with a specific energy consumption question; verify retrieved context contains correct documents before generation
  2. Submit an out-of-scope question; confirm system declines rather than hallucinates
  3. Enable multiple Redis workers and submit sequential queries; observe whether response ordering matches query submission order

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the stability and trustworthiness of energy efficiency advice generated by Large Language Models (LLMs) be rigorously quantified?
- Basis in paper: [explicit] The abstract explicitly states that "more research is needed to evaluate this technology, particularly limitations to the stability and trustworthiness of its energy efficiency advice."
- Why unresolved: The current evaluation relies primarily on semantic similarity metrics (Cosine/Jaccard) rather than longitudinal consistency or factual verification of the generated advice.
- What evidence would resolve it: The development and application of standardized benchmarks that measure hallucination rates and output consistency over repeated, identical queries.

### Open Question 2
- Question: How can Retrieval-Augmented Generation (RAG) systems be enhanced to accurately perform temporal aggregations on building energy data?
- Basis in paper: [inferred] The results section notes that SPARA "did not provide accurate answers when the questions required SPARA to compute electricity data over multiple months in a given year."
- Why unresolved: The current architecture retrieves context effectively but lacks the internal reasoning capabilities or function-calling integration necessary to process that data across time intervals reliably.
- What evidence would resolve it: Successful system performance on a validation set of queries requiring multi-month data calculations, moving beyond single-month retrieval.

### Open Question 3
- Question: To what extent does high semantic similarity in generated text correlate with actionable decision-making quality for non-expert users?
- Basis in paper: [inferred] The paper measures success via lexical overlap and expert validation, but the ultimate goal is to aid "informed energy retrofitting and usage decisions" for BRFs lacking technical expertise.
- Why unresolved: High scores on NLP metrics (e.g., 80% precision) do not automatically guarantee that the advice is practically useful or correctly interpreted by non-expert board members.
- What evidence would resolve it: User studies measuring the rate of successful retrofitting actions or decision confidence among BRF members using the system.

## Limitations
- Evaluation relies on similarity metrics that may not directly correlate with expert judgment of answer quality in real-world advisory contexts
- Knowledge base is narrowly scoped to email communications between advisors and cooperatives, limiting generalizability
- No validation provided on critical failure mode of multi-step numeric computations where system reportedly struggles

## Confidence
- **High confidence**: The architectural description of SPARA's RAG framework and its basic functionality (80% precision on general questions, 85% accuracy on specific queries)
- **Medium confidence**: The effectiveness of low temperature settings and knowledge grounding in preventing hallucinations
- **Low confidence**: The scalability of the system to handle diverse energy efficiency scenarios beyond the specific Stockholm cooperative context

## Next Checks
1. Conduct a user study with actual housing cooperative board members to evaluate whether SPARA's responses are perceived as actionable and trustworthy compared to human advisor responses
2. Test the system's ability to handle multi-step numeric computations (e.g., "What was the total electricity consumption for building X in 2023?") to identify failure patterns in quantitative reasoning
3. Evaluate the system's performance when presented with out-of-domain questions to quantify the frequency and quality of appropriate refusals versus hallucinated responses