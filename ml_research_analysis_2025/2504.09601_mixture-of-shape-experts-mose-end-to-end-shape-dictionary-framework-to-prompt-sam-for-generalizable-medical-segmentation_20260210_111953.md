---
ver: rpa2
title: 'Mixture-of-Shape-Experts (MoSE): End-to-End Shape Dictionary Framework to
  Prompt SAM for Generalizable Medical Segmentation'
arxiv_id: '2504.09601'
source_url: https://arxiv.org/abs/2504.09601
tags:
- shape
- medical
- segmentation
- dictionary
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mixture-of-Shape-Experts (MoSE), a novel
  end-to-end shape dictionary learning framework that integrates the mixture-of-experts
  (MoE) strategy to enhance single domain generalization (SDG) in medical image segmentation.
  The method conceptualizes each dictionary atom as a "shape expert" that specializes
  in encoding distinct semantic shape information, with a gating network dynamically
  fusing these experts into a robust shape map via sparse activation guided by SAM
  encoding to prevent overfitting.
---

# Mixture-of-Shape-Experts (MoSE): End-to-End Shape Dictionary Framework to Prompt SAM for Generalizable Medical Segmentation

## Quick Facts
- arXiv ID: 2504.09601
- Source URL: https://arxiv.org/abs/2504.09601
- Reference count: 40
- Key outcome: Achieves 91.4% average Dice coefficient on unseen domains, outperforming state-of-the-art SDG methods

## Executive Summary
This paper introduces Mixture-of-Shape-Experts (MoSE), a novel end-to-end shape dictionary learning framework that enhances single domain generalization (SDG) in medical image segmentation. The method conceptualizes each dictionary atom as a "shape expert" that specializes in encoding distinct semantic shape information, with a gating network dynamically fusing these experts into a robust shape map via sparse activation guided by SAM encoding to prevent overfitting. Experiments on multiple liver CT datasets demonstrate MoSE's superior generalization performance, achieving 91.4% Dice coefficient across unseen domains while maintaining continuous improvement as dictionary size increases.

## Method Summary
MoSE integrates the mixture-of-experts strategy with end-to-end shape dictionary learning to improve SDG medical segmentation. The framework uses a learnable dictionary of n shape experts, where each expert specializes in encoding distinct semantic shape information. A gating network dynamically fuses these experts into a shape map through sparse activation (Top-K selection) based on SAM encoding. This shape map is then provided as a prompt to the Segment Anything Model (SAM), enabling bidirectional integration that leverages SAM's powerful generalization capability. All modules, including the shape dictionary, are trained end-to-end using a composite loss on both the intermediate shape map and final segmentation output.

## Key Results
- Achieves 91.4% average Dice coefficient across unseen domains, significantly outperforming state-of-the-art SDG methods and adapted SAM models
- Demonstrates superior performance in terms of Hausdorff Distance, with an average HD of 17.0 mm
- Shows continuous improvement in performance as the number of shape experts increases, overcoming traditional dictionary size trade-offs

## Why This Works (Mechanism)

### Mechanism 1: Sparse Expert Selection Prevents Dictionary Overfitting
Enabling sparse, input-dependent selection of shape experts via a gating network allows the dictionary to scale without overfitting. A lightweight CNN gating network produces pixel-wise weight maps over all shape experts based on the SAM image embedding. Top-K selection retains only the k highest-response experts per pixel, zeroing others. The final shape map is a weighted combination of these sparsely activated experts. Relevant shape priors can be composed from a subset of experts per input, and sparse activation acts as a regularizer that prevents the model from memorizing source-domain-specific features.

### Mechanism 2: Shape Map as Prompt Leverages SAM's Generalization
Providing the dynamically composed shape map as a prompt to SAM injects domain-invariant shape priors that improve segmentation on unseen domains. The shape map, normalized via sigmoid, is fed into SAM's prompt encoder, interacting with the image embedding to guide the mask decoder. This bidirectional integration uses SAM's frozen encoder for sparse coding while injecting learned priors at inference. Shape information generalizes better across domains than appearance features, and SAM's prompt interface can effectively incorporate external shape priors.

### Mechanism 3: End-to-End Supervision Aligns Dictionary with Segmentation Task
Jointly supervising both the intermediate shape map and final SAM output produces a more task-aligned dictionary than offline pre-computation. The shape map and final segmentation are both supervised against ground truth labels using a composite Dice + Cross-Entropy loss. The gating network and dictionary atoms are updated via backpropagation, enabling the dictionary to learn shape representations directly optimized for segmentation. Offline-computed dictionaries are suboptimal because they are decoupled from the downstream task and source-domain distribution.

## Foundational Learning

- **Concept: Mixture-of-Experts (MoE) with Sparse Gating**
  - Why needed here: MoSE repurposes MoE for dictionary learning, where each expert is a shape atom. Understanding sparse gating (Top-K, load balancing) is essential for interpreting why scaling experts doesn't overfit.
  - Quick check question: Can you explain why sparse gating acts as both a computational efficiency mechanism and a regularizer?

- **Concept: Dictionary Learning and Sparse Coding**
  - Why needed here: The paper frames shape priors as dictionary atoms composed via sparse coefficients. Prior SDG methods used offline dictionaries; MoSE makes them learnable.
  - Quick check question: What is the trade-off between dictionary size and generalization in traditional sparse coding, and how does MoSE address it?

- **Concept: SAM Prompt Encoder and Adapter Fine-Tuning**
  - Why needed here: MoSE uses SAM's frozen encoder for embeddings but inserts trainable adapters. Understanding prompt types (mask, point, box) and parameter-efficient fine-tuning clarifies the bidirectional integration.
  - Quick check question: How does SAM's prompt encoder differ from its image encoder, and why is a mask-like prompt suitable for shape priors?

## Architecture Onboarding

- **Component map:** Input image → SAM encoder (with adapters) → gating network → Top-K selection → shape map → SAM prompt encoder → mask decoder → final mask
- **Critical path:** Input image → SAM encoder (with adapters) → gating network → Top-K selection → shape map → SAM prompt encoder → mask decoder → final mask. Both shape map and final mask are supervised.
- **Design tradeoffs:**
  - Dictionary size (n) vs. computation: Larger n stores more shape diversity but increases gating computation. Paper uses n=1024 with k=512 as a practical upper bound.
  - Sparsity (k) vs. expressiveness: Smaller k enforces stronger regularization but may underutilize experts. Paper finds larger k beneficial when n is large.
  - Warm-up (T_warm-up) vs. early specialization: L1 penalty during warm-up prevents premature sparse lock-in; too long delays convergence.
- **Failure signatures:**
  - Expert collapse: Gating always selects the same subset; CV regularization too weak.
  - Gradient blockage: Top-K applied too early halts backprop to less-used experts.
  - Shape map misalignment: Shape map doesn't localize target; visualizable as diffuse or off-target heatmap.
- **First 3 experiments:**
  1. Reproduce the dictionary scaling curve: Train MoSE with n ∈ {128, 256, 512, 1024} on BTCV, evaluate on all target domains. Confirm continuous improvement vs. traditional dictionary baseline.
  2. Ablate Top-K and warm-up: Run grid search over k and T_warm-up; plot Dice and HD to verify sensitivity matches reported figures.
  3. Visualize shape map and expert activation: For a few test images, overlay the shape map and display per-expert activation heatmaps to validate that the gating network selects plausible experts for each anatomy.

## Open Questions the Paper Calls Out
- **Open Question 1:** How can the MoSE framework be adapted to effectively handle multi-class segmentation where distinct anatomical structures require conflicting shape priors? The current framework generates a single fused shape map prompt optimized for single-organ (liver) segmentation, and it is unclear if a single dictionary or gating mechanism can simultaneously represent divergent geometries.
- **Open Question 2:** Does the performance of MoSE scale effectively to 3D volumetric segmentation models without prohibitive memory costs? The current study utilizes a 2D slice-based approach, and implementing 3D shape experts increases the parameter count exponentially.
- **Open Question 3:** Is the reliance on consistent semantic shape priors a limitation when segmenting anatomical structures with high morphological variability, such as lesions or tumors? The method assumes shape information remains consistent across domains, but tumors vary wildly in appearance, potentially causing the gating network to fail.

## Limitations
- The gating network architecture is underspecified beyond "lightweight CNN," leaving ambiguity about optimal design choices for expert selection.
- The paper assumes shape priors generalize better than appearance features, but this is not rigorously validated through ablation studies comparing shape-only vs. appearance-based prompts.
- The dictionary scaling improvement assumes continuous benefit, yet the paper only tests up to 1024 experts without examining potential saturation or diminishing returns at larger scales.

## Confidence
- **High confidence:** The bidirectional integration of shape maps as prompts to SAM, supported by clear architectural description and baseline comparisons
- **Medium confidence:** The claim that sparse gating prevents overfitting, as the mechanism is theoretically sound but empirical evidence shows correlation rather than definitive causation
- **Low confidence:** The assertion that end-to-end supervision produces task-aligned dictionaries, as there is no direct comparison to offline dictionary learning methods

## Next Checks
1. Test MoSE with appearance-based prompts (e.g., texture features) vs. shape maps to validate the specific advantage of shape priors
2. Extend dictionary scaling experiments to 2048+ experts to identify potential performance saturation points
3. Compare MoSE against an offline dictionary learning baseline using identical shape representation capacity to isolate the benefit of end-to-end training