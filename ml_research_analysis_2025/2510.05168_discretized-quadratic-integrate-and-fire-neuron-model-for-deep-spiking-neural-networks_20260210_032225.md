---
ver: rpa2
title: Discretized Quadratic Integrate-and-Fire Neuron Model for Deep Spiking Neural
  Networks
arxiv_id: '2510.05168'
source_url: https://arxiv.org/abs/2510.05168
tags:
- neuron
- neural
- training
- spiking
- dynamics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a discretized Quadratic Integrate-and-Fire
  (QIF) neuron model tailored for deep Spiking Neural Networks (SNNs). Unlike the
  widely-used Leaky Integrate-and-Fire (LIF) neuron, which has linear dynamics, the
  QIF model exhibits richer, nonlinear behavior such as subthreshold oscillations
  and input sensitivity.
---

# Discretized Quadratic Integrate-and-Fire Neuron Model for Deep Spiking Neural Networks

## Quick Facts
- **arXiv ID:** 2510.05168
- **Source URL:** https://arxiv.org/abs/2510.05168
- **Reference count:** 40
- **Primary result:** Novel discretized QIF neuron achieves 96.86% CIFAR-10 accuracy with minimal energy overhead over LIF.

## Executive Summary
This paper introduces a discretized Quadratic Integrate-and-Fire (QIF) neuron model designed for deep Spiking Neural Networks (SNNs), addressing the stability challenges that have historically limited QIF adoption. Unlike the widely-used Leaky Integrate-and-Fire (LIF) model with linear dynamics, QIF neurons exhibit richer nonlinear behavior including subthreshold oscillations and enhanced input sensitivity. The authors derive an analytical formulation for surrogate gradient windows directly from the model's parameters, minimizing gradient mismatch and enabling stable training of deep QIF-based SNNs. Experiments on CIFAR-10, CIFAR-100, ImageNet, and CIFAR-10 DVS demonstrate significant accuracy improvements over state-of-the-art LIF-based SNNs while maintaining minimal energy overhead.

## Method Summary
The proposed method replaces standard LIF neurons with a discretized QIF model defined by the recurrence $u(t+1) = a(u(t)-u_1)(u(t)-u_2) + I(t)$, where $a=0.25$, $u_1=0$, and $u_2=0.5$ are fixed for stability. The key innovation is an analytical surrogate gradient window derived from the membrane potential's statistical distribution under Threshold-dependent Batch Normalization (tdBN). The surrogate gradient is conditionally set to 1 if the membrane potential falls within $[\mu_u - \sigma_u, \mu_u + \sigma_u]$, otherwise 0, where $\mu_u$ and $\sigma_u$ are analytically derived from the model parameters. This approach minimizes gradient mismatch during backpropagation through time. The method is evaluated using ResNet-19 and ResNet-34 architectures on multiple image classification benchmarks.

## Key Results
- Achieves 96.86% accuracy on CIFAR-10, outperforming state-of-the-art LIF-based SNNs.
- Demonstrates 80.62% accuracy on CIFAR-100 and 79.47% on ImageNet.
- Shows 80.70% accuracy on CIFAR-10 DVS event-based dataset.
- Maintains minimal energy overhead of 0.94%–3.23% compared to LIF neurons.

## Why This Works (Mechanism)

### Mechanism 1
The quadratic discretization enables richer temporal dynamics compared to LIF's linear decay. The recurrence $u(t+1) = a(u(t)-u_1)(u(t)-u_2) + I(t)$ creates distinct behavioral regions with stable and unstable fixed points, enabling subthreshold oscillations and rapid voltage inversions that facilitate spiking in response to sudden input changes.

### Mechanism 2
Training stability is achieved through analytical derivation of surrogate gradient windows that match the membrane potential distribution. By setting the gradient window width based on derived mean $\mu_u$ and variance $\sigma_u^2$ under tdBN normalization, the method minimizes gradient mismatch between the true non-differentiable spike function and its smooth approximation.

### Mechanism 3
High accuracy is maintained with minimal energy overhead despite increased computational complexity. The QIF neuron requires only one additional multiplication and addition per timestep compared to LIF, and neuronal overhead accounts for only 0.94%–3.23% of total energy consumption, which is dominated by AC operations (spikes) and MAC operations in convolution layers.

## Foundational Learning

- **Surrogate Gradient Descent**: SNNs use discrete spikes (non-differentiable Heaviside functions). Understanding how the paper approximates the derivative $\partial o(t) / \partial u(t)$ is crucial. *Quick check:* How does the surrogate gradient window width $\alpha$ in this paper differ from heuristic constants typically used in LIF training?

- **Dynamical Systems (Fixed Points & Stability)**: The paper justifies parameter choices by analyzing stability of fixed points. Understanding Fig 1 requires knowing why $|g(u^*)| < 1$ implies stability. *Quick check:* Why is the "Blue Region" in Figure 1 described as exhibiting subthreshold oscillations, and why doesn't it trigger a spike?

- **Threshold-dependent Batch Normalization (tdBN)**: The derived surrogate gradient mechanism relies entirely on the assumption that inputs are normalized specifically to threshold $u_{th}$. Without this, the analytical gradient mismatch minimization fails. *Quick check:* What specific statistical property of the pre-synaptic input $I$ is required for Theorem 4.1 to hold?

## Architecture Onboarding

- **Component map:** Input -> Conv -> tdBN -> QIF Update -> Spike Generation -> Accumulate -> Loss
- **Critical path:** Input spikes/frames → Conv + tdBN → QIF Update: Compute $f_n(t) = a(u_n(t) - u_1)(u_n(t) - u_2)$ → Membrane Potential: $u(t+1) = f_n(t)(1-o(t)) + I(t)$ → Spike Generation: $o(t+1) = \Theta(u(t+1) - u_{th})$ → Loss: Cross Entropy on accumulated output
- **Design tradeoffs:** Stability vs. Dynamics (fixed parameters ensure stability but may limit dynamic richness); Ease of Training vs. Bio-plausibility (requires tdBN complicating event-driven backpropagation)
- **Failure signatures:** Diverging Gradients (misaligned surrogate window), Silent Neurons (threshold too high or insufficient input scaling), Voltage Explosion (threshold exceeds unstable fixed point)
- **First 3 experiments:**
  1. Plot the phase portrait (Cobweb plot) of QIF discretization with zero input using parameters $a=0.25, u_1=0, u_2=0.5$ to verify convergence to 0 and match Figure 2 regions.
  2. Train a shallow 2-layer SNN on MNIST comparing "Heuristic Alpha" vs. "Analytical Alpha" to confirm convergence speed improvements.
  3. Run full ResNet-19 on CIFAR-10 using provided hyperparameters to reproduce ~96.86% accuracy and verify energy overhead calculation.

## Open Questions the Paper Calls Out

### Open Question 1
Can the discretized QIF model be adapted for event-driven backpropagation? The current method relies on tdBN which requires normalization across temporal dimensions, conflicting with event-driven processing's asynchronous nature.

### Open Question 2
How does the additional arithmetic of the QIF model impact latency on physical neuromorphic hardware? While energy overhead is low theoretically, real hardware implementations might introduce non-negligible latency costs not captured in theoretical analysis.

### Open Question 3
Does making the QIF hyperparameters (e.g., $a$, $u_1$, $u_2$) learnable improve performance? The authors manually tune parameters to ensure stability, leaving potential for dynamic, layer-wise optimization unexplored.

## Limitations
- Stability depends critically on tdBN normalization; failure of this assumption could cause divergence or unstable dynamics.
- The analytical surrogate gradient relies on strong assumptions about normal input distribution that may not hold perfectly in practice.
- Energy overhead claims are based on theoretical FLOPs and lack validation from actual hardware measurements.

## Confidence
- **High Confidence:** Improved accuracy on CIFAR-10 (96.86%) and CIFAR-10 DVS (80.70%) over LIF-based SNNs is well-supported by reported experiments.
- **Medium Confidence:** Analytical surrogate gradient derivation is sound given stated assumptions, but real-world efficacy depends critically on tdBN functioning as intended.
- **Low Confidence:** Claim of "minimal computational overhead" compared to other complex neuron models lacks direct comparisons to other nonlinear SNN neuron types in terms of both accuracy and energy.

## Next Checks
1. **Dynamic Stability Verification:** Implement 1D simulation of QIF discretization with fixed parameters and zero input. Plot phase portrait (cobweb plot) to empirically verify stable fixed point at 0 and dynamics matching described regions.
2. **Surrogate Gradient Efficacy Test:** Train shallow 2-layer SNN on MNIST comparing analytical surrogate gradient window against standard heuristic window. Measure training convergence speed and final accuracy to confirm analytical approach benefit.
3. **Energy Overhead Validation:** Using ResNet-19 CIFAR-10 model, count additional operations introduced by QIF update (1 extra multiplication and addition per neuron per timestep). Calculate total energy overhead using reported energy model to verify claimed 0.94%–3.23% figure.