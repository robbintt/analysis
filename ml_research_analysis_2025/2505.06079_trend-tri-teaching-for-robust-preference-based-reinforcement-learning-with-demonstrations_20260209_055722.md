---
ver: rpa2
title: 'TREND: Tri-teaching for Robust Preference-based Reinforcement Learning with
  Demonstrations'
arxiv_id: '2505.06079'
source_url: https://arxiv.org/abs/2505.06079
tags:
- preference
- learning
- reward
- labels
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning from noisy preference
  feedback in preference-based reinforcement learning (PbRL). The authors propose
  TREND, a novel framework that integrates few-shot expert demonstrations with a tri-teaching
  strategy for effective noise mitigation.
---

# TREND: Tri-teaching for Robust Preference-based Reinforcement Learning with Demonstrations

## Quick Facts
- **arXiv ID:** 2505.06079
- **Source URL:** https://arxiv.org/abs/2505.06079
- **Reference count:** 40
- **Primary result:** Robustly handles up to 40% noise in preference feedback using tri-teaching with demonstrations

## Executive Summary
This paper addresses the challenge of learning from noisy preference feedback in preference-based reinforcement learning (PbRL). The authors propose TREND, a novel framework that integrates few-shot expert demonstrations with a tri-teaching strategy for effective noise mitigation. The core idea is to train three reward models simultaneously, where each model identifies preference pairs with small losses as likely clean and teaches these pairs to its peer networks for updating parameters. The method also incorporates Behavior Cloning pretraining to provide a grounded anchor for the policy.

## Method Summary
TREND trains three reward models simultaneously with different initializations. For each preference pair, the models compute a Bradley-Terry cross-entropy loss. Each model selects the 60% of samples with the lowest loss and uses these to update one of its peer models in a cyclic manner (model 1 → model 2 → model 3 → model 1). The policy is initialized via Behavior Cloning on expert demonstrations and updated using a composite loss combining SAC and BC losses. The reward models provide feedback to train the policy, which learns from both online experience and expert demonstrations.

## Key Results
- Successfully handles up to 40% noise in preference feedback on Meta-world tasks
- Demonstrates significant improvement over baseline PbRL methods in noisy settings
- Shows robustness across multiple manipulation tasks (Button-Press, Drawer-Open, Hammer)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cyclic peer-teaching prevents error accumulation that occurs when a single model self-selects "clean" data.
- **Mechanism:** Three reward models are trained simultaneously. Instead of training on its own low-loss samples (self-teaching), Model A identifies small-loss samples and feeds them to Model B, B to C, and C to A. This relies on the assumption that noise patterns learned by independent models diverge, while signal converges.
- **Core assumption:** Diverse model initialization leads to divergent error patterns during early training.
- **Break condition:** If models lack diversity (e.g., identical initialization), they will collapse into a single model, negating the denoising effect.

### Mechanism 2
- **Claim:** The "small-loss" criterion acts as a proxy for label correctness, assuming the model learns valid patterns before memorizing noise.
- **Mechanism:** The architecture filters preference pairs by ranking them based on the cross-entropy loss calculated by a peer model. It assumes that "clean" data is learned faster, yielding lower loss early on, while "noisy" data contradicts the majority signal.
- **Core assumption:** The "early learning" phenomenon holds, where models fit clean labels before overfitting to random noise.
- **Break condition:** If the noise rate is extremely high (>50%) or the model capacity is excessive, the model may fit the noise immediately.

### Mechanism 3
- **Claim:** Expert demonstrations provide a "grounded" anchor for the policy, preventing collapse when the reward signal is unreliable.
- **Mechanism:** The policy is pretrained with Behavior Cloning and then updated using a composite loss. The BC loss acts as a regularizer, tethering the policy to the expert's state distribution even if the learned reward model provides contradictory gradients due to noise.
- **Core assumption:** The expert demonstrations are near-optimal and sufficient to define a baseline behavior.
- **Break condition:** If the expert demonstrations are suboptimal or significantly different from the target task, the BC regularization will conflict with the preference signal.

## Foundational Learning

- **Concept: Bradley-Terry Model**
  - **Why needed here:** Mathematical basis for converting preference labels (A > B) into scalar reward signal.
  - **Quick check question:** If a reward model predicts identical rewards for two trajectories, what is the predicted probability of preference? (Answer: 0.5/indifference)

- **Concept: Actor-Critic (SAC)**
  - **Why needed here:** The paper uses Soft Actor-Critic as the underlying RL algorithm.
  - **Quick check question:** Does the reward model update the Actor directly, or does it train the Critic which then trains the Actor?

- **Concept: Sample Selection / Co-teaching**
  - **Why needed here:** The core novelty is the "Tri-teaching" sampling strategy.
  - **Quick check question:** In standard training, you update weights based on all data. In this method, what object determines which data is used for the update?

## Architecture Onboarding

- **Component map:** Policy Network → Reward Ensemble (3 models) → Preference Buffer → Replay Buffer → Policy Network
- **Critical path:**
  1. Pretrain: Run BC on expert demos to initialize policy
  2. Ensemble Update: For a batch of preference pairs, compute loss via ψj. Select top-60% lowest loss pairs. Train ψk on selected pairs (cyclic)
  3. Policy Update: Generate rewards using ψ. Update policy using SAC loss + weighted BC loss

- **Design tradeoffs:**
  - Selection Rate (γ): High γ includes more noisy data; low γ reduces sample efficiency
  - BC Weight (λ): High λ restricts exploration to expert states; low λ risks policy drift
  - Ensemble Size: 3 is chosen to prevent ties and balance overhead

- **Failure signatures:**
  - Reward Hacking: Agent finds states where noisy preferences grant high reward but task fails
  - Model Collapse: Reward models converge to identical weights
  - BC Overfitting: Agent mimics expert perfectly but cannot correct for perturbations

- **First 3 experiments:**
  1. Sanity Check (Noise=0%): Train without noise to verify Tri-teaching logic doesn't degrade performance
  2. Ablation (Self-Teaching vs. Tri-Teaching): Replace cyclic selection with self-selection to verify peer mechanism is required
  3. Hyperparameter Sweep (γ vs. Noise): Plot success rate against noise levels while varying selection rate γ

## Open Questions the Paper Calls Out
- Can the selection rate γ be estimated dynamically during training rather than treated as a fixed hyperparameter?
- How does TREND perform when VLM-generated preference noise exceeds 40% or becomes highly correlated/adversarial rather than random?
- Does the tri-teaching strategy effectively transfer to real-world robotic manipulation where dynamics and visual noise differ from simulation?

## Limitations
- Effectiveness depends on assumption that clean labels are learned before noisy ones, which may not hold for extremely high noise rates (>50%)
- The choice of three reward models appears somewhat arbitrary and could significantly impact performance
- Expert demonstrations are assumed to be near-optimal but the paper doesn't explore what happens when expert data is suboptimal

## Confidence
- **High confidence**: Cyclic peer-teaching mechanism as a general approach for noisy label learning in RL
- **Medium confidence**: Specific hyperparameters (selection rate γ=0.6, BC weight λ=4.0) as optimal across different tasks and noise levels
- **Medium confidence**: Assumption that three models provide the right balance between diversity and computational overhead

## Next Checks
1. Test the model with noise rates exceeding 40% to determine the breaking point of the small-loss criterion
2. Vary the number of reward models (2, 4, 5) to assess whether three is truly optimal or simply sufficient
3. Evaluate performance when expert demonstrations are intentionally suboptimal to understand the BC regularization's robustness boundaries