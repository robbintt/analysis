---
ver: rpa2
title: Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision
  Language Models
arxiv_id: '2505.20021'
source_url: https://arxiv.org/abs/2505.20021
tags:
- determine
- given
- line
- which
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces the Atomic Visual Skills Dataset (AVSD) to
  evaluate vision-language models on fundamental, indivisible visual perception skills
  in 2D Euclidean geometry. The authors systematically categorized 36 atomic visual
  skills and constructed three sub-datasets: AVSD-h (5,163 handcrafted problems),
  AVSD-s (5,400 procedurally generated problems), and AVSD-c (2,625 style-augmented
  problems).'
---

# Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision Language Models

## Quick Facts
- arXiv ID: 2505.20021
- Source URL: https://arxiv.org/abs/2505.20021
- Authors: Hyunsik Chae; Seungwoo Yoon; Jaden Park; Chloe Yewon Chun; Yongin Cho; Mu Cai; Yong Jae Lee; Ernest K. Ryu
- Reference count: 40
- Key outcome: State-of-the-art VLMs achieve only 30-34% accuracy on atomic visual skills, indicating fundamental perceptual deficits despite strong performance on higher-level tasks

## Executive Summary
This paper introduces the Atomic Visual Skills Dataset (AVSD) to evaluate vision-language models on fundamental visual perception skills in 2D Euclidean geometry. The authors systematically categorized 36 indivisible visual perception skills and constructed three sub-datasets: handcrafted problems (5,163), procedurally generated problems (5,400), and style-augmented problems (2,625). Evaluation of state-of-the-art VLMs revealed they struggle with these basic tasks, achieving only 30-34% accuracy across the full dataset, with even the best models scoring below 75%. Fine-tuning on AVSD-s-train improved performance, but training on composite geometry alone did not help. These results indicate that VLMs lack robust atomic visual perception skills despite strong performance on higher-level tasks.

## Method Summary
The authors created AVSD by first categorizing 36 atomic visual skills in 2D Euclidean geometry, then constructing three sub-datasets: AVSD-h (5,163 handcrafted problems), AVSD-s (5,400 procedurally generated problems), and AVSD-c (2,625 style-augmented problems). Evaluation involved passing images and questions to VLMs, extracting answers with GPT-4o mini, and grading correctness with GPT-4o mini. For fine-tuning, they used LLaVA-Next-13B on 8 H100 GPUs for 1-2 days, following the LLaVA repository recipe on AVSD-s-train (360k synthetic problems).

## Key Results
- State-of-the-art VLMs (GPT-4o, o3, Gemini 2.5) scored below 75% on AVSD, with average accuracy of 30-34% across the full dataset
- Domain-specific models like Math-LLaVA and G-LLaVA did not outperform general VLMs on atomic visual skills
- Fine-tuning on AVSD-s-train improved performance significantly, demonstrating out-of-distribution generalization
- Training on composite geometry diagrams alone did not improve atomic perception performance
- Chain-of-thought prompting worsened performance for most models, suggesting these tasks require perception rather than reasoning

## Why This Works (Mechanism)

### Mechanism 1: Atomic Skill Isolation and Evaluation
VLMs fail on composite visual perception because they lack fundamental atomic visual skills that cannot be acquired implicitly through composite task training. Decomposing complex visual comprehension into 36 indivisible skills isolates specific perceptual deficits, preventing models from leveraging cross-skill shortcuts.

### Mechanism 2: Pre-training Data Distribution Mismatch
Atomic visual perception capabilities require targeted pre-training data, not post-hoc fine-tuning on composite tasks. Vision encoders pre-trained on natural image datasets systematically underrepresent precise geometric features, making geometric perception qualitatively distinct from natural image scene understanding.

### Mechanism 3: Perception-Reasoning Separability
Atomic visual skills are perception tasks that do not benefit from additional reasoning steps. CoT prompting adds irrelevant reasoning steps to perception tasks requiring immediate visual discrimination, explaining why CoT worsens performance while reasoning models with thinking budgets show mixed results.

## Foundational Learning

- **Atomic vs. Composite Skill Taxonomy**: Understanding the distinction between indivisible perceptual primitives and composite tasks is essential for justifying the decomposition approach. Quick check: Can you explain why training on composite geometry diagrams failed to improve atomic perception performance?

- **Vision Encoder Pre-training Paradigms**: Understanding how vision encoders are typically trained (natural images, CLIP objectives) explains why geometric features may be systematically underrepresented. Quick check: What visual features would a CLIP-trained encoder likely prioritize over precise angle or tangency detection?

- **Chain-of-Thought vs. Test-Time Scaling**: Distinguishing prompting strategies (CoT) from architectural features (reasoning models with thinking budgets) is essential for interpreting mixed results on whether "reasoning" helps perception. Quick check: Why might Gemini 2.5 Flash benefit from increased thinking budget while CoT prompting hurts LLaVA-NeXT performance?

## Architecture Onboarding

- **Component map**: AVSD-h (handcrafted, 5,163 problems) → deep evaluation with difficulty levels; AVSD-s (synthetic, 5,400 problems) → textbook-style procedural generation; AVSD-c (ControlNet-augmented, 2,625 problems) → style robustness testing. AVSD-s-train (360k problems) → fine-tuning data.

- **Critical path**: Start with AVSD-h evaluation across all 36 skills → identify specific deficit patterns (e.g., tangency, parallel, angle consistently low) → fine-tune on AVSD-s-train for targeted skills → re-evaluate on AVSD-h to confirm OOD generalization.

- **Design tradeoffs**: AVSD-s offers unlimited data generation but may not capture edge cases in AVSD-h; ControlNet augmentation enables style robustness testing but risks distorting geometric content (filtering pipeline with Canny edge similarity threshold required); difficulty categorization is subjective.

- **Failure signatures**: Models performing well on OCR/absolute position/shape but poorly on tangency/parallel/angle suggests spatial relationship deficit (not general vision failure); performance drop from AVSD-s to AVSD-c indicates style overfitting; CoT worsening performance suggests perception-specific failure mode.

- **First 3 experiments**:
  1. Evaluate target VLM on AVSD-h across all 36 skills to establish baseline and identify specific skill deficits.
  2. Fine-tune on AVSD-s-train subset corresponding to worst-performing skills only, then re-evaluate to test skill-specific transfer.
  3. Evaluate on AVSD-c to assess style robustness; if performance drops >15%, investigate whether vision encoder or projection layer is the bottleneck.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does incorporating atomic visual skill data into large-scale pre-training (rather than fine-tuning) lead to significant improvements in VLMs' geometric perception and downstream reasoning? (Basis: explicit hypothesis about pre-training integration)

- **Open Question 2**: What atomic visual skills are required for perceiving diagrams and illustrations in scientific and everyday contexts beyond mathematics? (Basis: explicit suggestion to explore broader scope beyond 2D Euclidean geometry)

- **Open Question 3**: Can atomic visual skill training improve performance on downstream composite geometry problem solving and multimodal reasoning tasks? (Basis: inferred from observation that composite training doesn't help atomic perception)

## Limitations

- The atomic skill decomposition is not fully rigorously defined - no formal proof that 36 skills are truly indivisible or collectively sufficient for composite geometric perception
- Evaluation relies heavily on GPT-4o mini for both answer extraction and grading, introducing potential variability
- Synthetic data generation process lacks complete transparency about specific algorithms and parameters used

## Confidence

- **High confidence**: Experimental observation that state-of-the-art VLMs struggle with AVSD (30-34% accuracy) is well-supported by comprehensive evaluation
- **Medium confidence**: Hypothesis that atomic skills require pre-training rather than fine-tuning is plausible but based on limited experiments
- **Low confidence**: Assertion that atomic skills are collectively sufficient for all composite visual perception tasks lacks rigorous validation

## Next Checks

1. **Cross-domain transferability test**: Evaluate whether AVSD-trained models show improved performance on composite visual reasoning tasks outside the geometry domain (e.g., visual puzzles, diagram interpretation) to validate sufficiency of atomic skill coverage.

2. **Architectural ablation study**: Compare VLMs with different vision encoder architectures (CLIP vs specialized geometric encoders) on AVSD to isolate whether perception deficits stem from encoder architecture or training data distribution.

3. **Skill irreducibility verification**: Systematically attempt to decompose each of the 36 atomic skills into sub-skills to test hypothesis of true indivisibility, documenting any skills that can be further broken down.