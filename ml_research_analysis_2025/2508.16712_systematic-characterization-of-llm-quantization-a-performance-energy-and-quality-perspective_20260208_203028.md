---
ver: rpa2
title: 'Systematic Characterization of LLM Quantization: A Performance, Energy, and
  Quality Perspective'
arxiv_id: '2508.16712'
source_url: https://arxiv.org/abs/2508.16712
tags:
- uni00000010
- energy
- quantization
- uni00000014
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents qMeter, an automated online profiling framework
  for comprehensive characterization of LLM quantization methods. The study evaluates
  11 quantization techniques across 4 model sizes (7B-70B) and two GPU architectures
  (A100, H100) under realistic serving conditions.
---

# Systematic Characterization of LLM Quantization: A Performance, Energy, and Quality Perspective

## Quick Facts
- arXiv ID: 2508.16712
- Source URL: https://arxiv.org/abs/2508.16712
- Reference count: 40
- One-line primary result: Comprehensive characterization of 11 LLM quantization methods across 4 model sizes and two GPU architectures under realistic serving conditions.

## Executive Summary
This paper presents qMeter, an automated online profiling framework for comprehensive characterization of LLM quantization methods. The study evaluates 11 quantization techniques across 4 model sizes (7B-70B) and two GPU architectures (A100, H100) under realistic serving conditions. Key findings reveal that quantization benefits are highly task- and method-dependent, with larger quantized models sometimes outperforming smaller full-precision ones. The research demonstrates strong sensitivity to workload characteristics, complex interactions with parallelism and GPU architecture, and presents three optimization case studies for capacity planning, energy-efficient scheduling, and multi-objective tuning. This represents one of the first comprehensive characterization of LLM quantization from joint performance, energy, and quality perspectives.

## Method Summary
The study uses qMeter, a custom profiling framework that orchestrates TensorRT-LLM v0.19.0 instances on NVIDIA A100/H100 GPUs. The framework performs binary search to find saturation QPS (maximum throughput meeting SLOs) for each configuration, then measures latency, energy (via NVML), and quality at mid-range load. Eleven quantization methods are evaluated across Llama-2 models (7B, 13B, 70B) and CodeLlama-34B using three workloads: ShareGPT (Chat), HumanEval (Code), and NewsQA (Summarization). The profiling captures Time to First Token (TTFT), Time per Output Token (TPOT), energy per token, and quality metrics.

## Key Results
- Quantization benefits are highly task- and method-dependent, with larger quantized models sometimes outperforming smaller full-precision ones
- Activation quantization combined with moderate tensor parallelism can reduce hardware requirements while maintaining performance
- Energy savings from quantization only become significant at higher QPS levels
- KV cache compression generally degrades latency despite memory savings
- Quality degradation is most severe on coding tasks for smaller models with aggressive quantization

## Why This Works (Mechanism)

### Mechanism 1: Cross-Model Substitution via Precision-Compute Tradeoff
A larger model at lower precision (e.g., 70B at 4-bit) can outperform a smaller model at full precision (e.g., 34B FP16) on specific efficiency metrics without sacrificing quality. Aggressive quantization reduces memory bandwidth pressure and per-token energy consumption, allowing a larger, more capable parameter set to fit within similar latency and energy budgets as a smaller, denser model.

### Mechanism 2: Parallelism-Quantization Synergy (GPU Reduction)
Activation quantization combined with moderate tensor parallelism (TP) can achieve performance comparable to FP16 at higher TP, effectively reducing hardware requirements. Activation quantization reduces memory footprint and communication volume per GPU, allowing configurations like W8A8KV8-FP at TP4 to sustain similar QPS and latency as FP16 at TP8.

### Mechanism 3: Sensitivity-Induced Bottleneck Shift (Input/Output Length)
Quantization benefits are highly dependent on sequence lengths, where short outputs degrade prefill efficiency (TTFT) and long inputs degrade decoding efficiency (TPOT). For short outputs, dequantization overhead cannot be amortized, increasing TTFT. For long inputs, quantized activation handling becomes the bottleneck, slowing down token generation.

## Foundational Learning

- **Concept: TTFT vs. TPOT (Time to First Token vs. Time per Output Token)**
  - Why needed here: The paper distinctly analyzes how quantization affects the prefill phase (TTFT) differently than the decoding phase (TPOT).
  - Quick check question: If a user asks a question requiring a 1-word answer on a quantized model, which latency metric is most likely to degrade relative to FP16, and why?

- **Concept: Weight-Only vs. Activation Quantization**
  - Why needed here: The study treats these differently; weight-only quantization is generally safer for quality but may retain memory bottlenecks.
  - Quick check question: Does compressing activations to 8-bit generally improve or degrade the energy-per-token metric compared to weight-only quantization?

- **Concept: Tensor Parallelism (TP)**
  - Why needed here: The interaction between quantization and TP is a core finding. The paper argues quantization can substitute for aggressive TP scaling.
  - Quick check question: Why might W8A8KV8-FP at TP4 be preferable to FP16 at TP8, assuming both meet latency SLOs?

## Architecture Onboarding

- **Component map:** User Profiling Plan -> Config Synthesis -> Profiling Loop (QPS Range Search) -> Database (Latency, Energy, Quality metrics)
- **Critical path:** 1) Config Synthesis defines the grid of (Model, Quantization, TP, Dataset). 2) Saturation Detection binary-searches for highest QPS meeting SLOs. 3) Profiling Loop measures Energy, Latency, and Quality. 4) Recovery handles engine crashes.
- **Design tradeoffs:**
  - KV Cache Compression: Provides memory savings but introduces latency/energy overhead under high load or high TP.
  - Aggressive Quantization (W4A8KV4): Best for Energy-per-Token but highest risk for Quality degradation on reasoning tasks.
  - Hardware Selection: A100 offers better energy efficiency at moderate loads; H100 offers better scalability/max throughput.
- **Failure signatures:**
  - Quality Collapse: Sudden drops in accuracy on coding tasks for smaller models with aggressive quantization.
  - SLO Violation: TPOT spiking unexpectedly for long-input requests using W4A8KV4.
  - Instability: Serving engines can crash under stress tests, requiring robust restart logic.
- **First 3 experiments:**
  1. Replicate Saturation Search: Run qMeter on a single 13B model (FP16 vs. W4A8) to observe QPS saturation point shift.
  2. Isolate Input/Output Sensitivity: Fix QPS and vary output length to demonstrate TTFT vs. TPOT tradeoff.
  3. Cross-Hardware Energy Check: Deploy 34B model on A100 vs. H100 at low load to validate energy efficiency claims.

## Open Questions the Paper Calls Out

- **Open Question 1:** How do performance and energy tradeoffs change when LLM quantization is combined with orthogonal inference optimizations like chunked prefill or disaggregated serving? The authors note these may interact with quantization through communication overhead.
- **Open Question 2:** Does the finding that KV cache compression harms latency hold for very long-context workloads (â‰¥8K tokens)? The study excluded such workloads, which might amplify memory savings benefits.
- **Open Question 3:** How can a runtime system dynamically select the optimal quantization configuration based on real-time load and prompt length? The paper suggests workload-aware scheduling is needed but doesn't propose an implementation.

## Limitations
- Framework Implementation Dependency: qMeter source code is not provided, requiring re-implementation of critical components.
- Model and Method Scope: Findings may not generalize to other model architectures or emerging quantization methods.
- Hardware and Load Representation: Results based on controlled conditions may not reflect real-world traffic patterns.

## Confidence

**High Confidence:** The core finding that quantization effects are highly task- and method-dependent is well-supported by extensive benchmarking across diverse datasets and multiple models.

**Medium Confidence:** The mechanism of "Cross-Model Substitution via Precision-Compute Tradeoff" is compelling but relies on specific workload efficiency assumptions that may vary with different SLO thresholds.

**Low Confidence:** The claim that A100 is more energy-efficient than H100 at low QPS is based on a single data point and may not generalize.

## Next Checks
1. **Framework Re-implementation and Saturation Search:** Re-implement qMeter saturation search logic using a 13B model (FP16 vs. W4A8) to verify QPS saturation point shift and binary search algorithm.
2. **Input/Output Length Sensitivity Isolation:** Conduct controlled experiment fixing QPS and systematically varying output length to reproduce TTFT vs. TPOT tradeoff mechanism.
3. **Cross-Hardware Energy Validation at Low Load:** Deploy 34B model on both A100 and H100 GPUs at low, fixed QPS to independently verify energy efficiency claims.