---
ver: rpa2
title: 'EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus
  and LLM-as-Judge'
arxiv_id: '2601.09142'
source_url: https://arxiv.org/abs/2601.09142
tags:
- eva-4b
- evasion
- annotation
- stage
- consensus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EvasionBench, a large-scale benchmark for
  detecting evasive answers in earnings call Q&A, addressing the lack of such datasets
  for financial transparency. The authors employ a Multi-Model Consensus (MMC) framework
  using dual frontier LLM annotation plus three-judge arbitration for disagreement
  cases, achieving 0.835 Cohen's Kappa human agreement.
---

# EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge

## Quick Facts
- arXiv ID: 2601.09142
- Source URL: https://arxiv.org/abs/2601.09142
- Authors: Shijian Ma; Yan Lin; Yi Yang
- Reference count: 24
- Primary result: 0.835 human agreement (Cohen's Kappa) with 84.9% Macro-F1 on 3-class evasion detection

## Executive Summary
This paper introduces EvasionBench, the first large-scale benchmark for detecting evasive answers in earnings call Q&A, addressing a critical gap in financial transparency research. The authors develop a Multi-Model Consensus (MMC) framework using dual frontier LLM annotation plus three-judge arbitration for disagreement cases, achieving 0.835 Cohen's Kappa human agreement. Their Eva-4B model (4B parameters) attains 84.9% Macro-F1, outperforming base model by 25 percentage points and approaching frontier LLM performance. The MMC approach shows 2.4% improvement over single-model distillation, with judge-resolved samples enhancing generalization despite higher training loss, indicating disagreement mining acts as implicit regularization.

## Method Summary
The authors created EvasionBench by filtering 22.7M raw Q&A pairs from S&P Capital IQ to 11.27M transcripts, then applying a three-stage process: (1) dual LLM annotation using Claude Opus 4.5 and Gemini 3 Flash with structured prompts, (2) three-judge arbitration for disagreements using Opus, Gemini, and GPT-5.2 with randomized presentation order, and (3) two-stage fine-tuning of Qwen3-4B-Instruct-2507 via MS-SWIFT. The final dataset contains 84K balanced samples (60K consensus + 24K judge-resolved) with a 1K gold-standard test set. The MMC framework achieves 0.835 Cohen's Kappa human agreement and enables Eva-4B to reach 84.9% Macro-F1 on the financial evasion detection task.

## Key Results
- Eva-4B model achieves 84.9% Macro-F1, outperforming base model by 25 percentage points and approaching frontier LLM performance
- MMC framework shows 2.4% improvement over single-model distillation with 0.835 Cohen's Kappa human agreement
- Judge-resolved samples improve generalization despite higher training loss (0.421 vs 0.393), indicating disagreement mining acts as implicit regularization
- Consensus→Full training (3.5% Macro-F1 improvement) validates the two-stage curriculum approach

## Why This Works (Mechanism)

### Mechanism 1: Disagreement Mining as Implicit Regularization
- Claim: Samples where frontier LLMs disagree provide boundary cases that improve generalization, functioning as implicit regularization.
- Mechanism: When two strong models conflict, the arbitrated label captures genuinely ambiguous instances. Training on these harder cases prevents overfitting to "easy" consensus examples, forcing the model to learn more robust decision boundaries rather than surface patterns.
- Core assumption: Disagreement correlates with task difficulty and informational value, not merely annotation noise.
- Evidence anchors: [abstract] "Judge-resolved samples improving generalization despite higher training loss (0.421 vs 0.393) - evidence that disagreement mining acts as implicit regularization"; [section 8.3] "Consensus→Full: +3.5 pp Macro-F1" improvement when adding judge-resolved samples.

### Mechanism 2: Heterogeneous Model Consensus Reduces Systematic Bias
- Claim: Combining multiple frontier LLMs with distinct labeling tendencies produces more reliable annotations than any single model.
- Mechanism: Each model exhibits systematic bias (Opus: 53.3% direct, Gemini: 23.5% fully evasive, GPT-5.2: 56.7% intermediate). Consensus requirement filters out model-specific priors, retaining only instances where different "perspectives" converge. Arbitration with position randomization further mitigates primacy bias.
- Core assumption: Model biases are sufficiently uncorrelated that consensus identifies ground truth rather than shared error modes.
- Evidence anchors: [section 5.4] Figure 2 shows "systematic differences in judge tendencies" across three models; [section 9] "Randomization increased Opus's win rate from 63.5% to 68.6% (+5.1%), demonstrating substantial position bias."

### Mechanism 3: Progressive Difficulty Curriculum via Two-Stage Training
- Claim: Training first on consensus (easy) samples, then judge-resolved (hard) samples produces better final performance than joint training.
- Mechanism: Stage 1 establishes stable base representations on unambiguous examples (loss converges to 0.007). Stage 2 refines decision boundaries on edge cases where the model must discriminate finer distinctions. This curriculum prevents early training instability from ambiguous labels while still incorporating their regularization benefit.
- Core assumption: Consensus samples are genuinely "easier" and provide cleaner gradient signals; judge-resolved samples require stable representations to process beneficially.
- Evidence anchors: [section 7.2] "Stage 1: Consensus Training... Stage 2: Judge-Refined Training" on disagreement cases; [section 8.3] Figure 5 shows dramatic loss difference: Eva-4B (Full) reaches 0.007 vs. Opus Only plateaus at 0.56.

## Foundational Learning

- Concept: **Gricean Maxims (Relation)**
  - Why needed here: The evasion taxonomy operationalizes violation of Grice's Relation maxim—whether the response is relevant to the question asked. Understanding this pragmatic framework clarifies why "intermediate" responses are hardest: they appear cooperative while partially violating relevance.
  - Quick check question: Can you explain why hedging ("I think it's possible") represents partial Relation violation rather than complete evasion?

- Concept: **Cohen's Kappa for Inter-Annotator Agreement**
  - Why needed here: The paper claims 0.835 Kappa as validation of annotation quality. Understanding that this measures agreement beyond chance—and that "almost perfect" requires κ > 0.80—helps assess whether the benchmark is reliable enough for training.
  - Quick check question: Why is Kappa preferred over raw accuracy when measuring annotator agreement on a 3-class balanced dataset?

- Concept: **LLM-as-Judge Position Bias**
  - Why needed here: The paper's anti-bias mechanism (randomized presentation order) produced a 5.1% win rate shift. Understanding primacy effects in LLM evaluation is critical for implementing similar consensus frameworks without introducing systematic artifacts.
  - Quick check question: If Model A's prediction always appears first in judge prompts, what systematic error would you expect in arbitration outcomes?

## Architecture Onboarding

- Component map:
Raw Transcripts (22.7M Q&A pairs)
    ↓ [3-stage filtering: extraction → quality → length ≥500 chars]
Filtered Data (11.27M pairs)
    ↓ [Stage I: Dual LLM annotation - Opus 4.5 + Gemini 3 Flash]
Consensus Set (60K) + Disagreement Set
    ↓ [Stage II/III: Three-judge arbitration with position randomization]
Training Data (84K balanced: 60K consensus + 24K arbitrated)
    ↓ [Two-stage fine-tuning on Qwen3-4B]

- Critical path:
  1. Annotation quality depends on heterogeneous model selection (distinct biases cancel out)
  2. Position randomization in arbitration (without this, 5.1% bias contaminates labels)
  3. Stage 1 → Stage 2 curriculum order (reversing or mixing degrades the 0.007 loss convergence)

- Design tradeoffs:
  - **Three-class vs. binary**: Paper pilot showed five-class κ < 0.5; three-class restores reliability but loses granularity between "partial answer" subtypes
  - **MMC vs. single-model distillation**: +2.4% improvement but 3× annotation compute cost
  - **4B model vs. frontier LLM inference**: 84.9% vs. 84.4-84.6% performance but fraction of inference cost—acceptable for deployment, marginal for research

- Failure signatures:
  - Training loss plateau at ~0.5 in Stage 2 → indicates noisy single-model labels (switch to multi-judge)
  - Systematic over-prediction of "intermediate" → hedging language triggering false signals (review prompt engineering)
  - 95%+ errors in adjacent classes → expected (ordinal nature); if >20% direct→fully_evasive errors, taxonomy may be mis-specified

- First 3 experiments:
  1. **Validate MMC on your domain**: Sample 100 examples, run dual-LLM annotation, measure disagreement rate (paper: 16.1%). If <10%, models may be too similar; if >30%, task may be underspecified.
  2. **Position bias audit**: Run arbitration twice (fixed vs. random position) on 500 disagreement samples. If win rate shifts >3%, position bias is significant and randomization is required.
  3. **Curriculum ablation**: Train variants—(a) consensus only, (b) consensus + judge-resolved joint, (c) two-stage sequential. Compare Macro-F1 on held-out set. Expect +2-4 pp from (c) vs. (a), with (b) intermediate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Multi-Model Consensus (MMC) framework and evasion taxonomy transfer effectively to adversarial Q&A domains outside of finance, such as political interviews or legal depositions?
- Basis in paper: [explicit] The Introduction explicitly states that while data is domain-specific, the authors "hypothesize that both our annotation framework and taxonomy could transfer to other adversarial Q&A settings... though cross-domain validation remains future work."
- Why unresolved: EvasionBench is constructed exclusively from S&P Capital IQ earnings call transcripts (2002–2022), and the Eva-4B model is fine-tuned solely on this financial domain data.
- What evidence would resolve it: Experiments applying the MMC annotation pipeline and Eva-4B (zero-shot or fine-tuned) to datasets of political debates or legal transcripts, benchmarking performance against domain-specific baselines.

### Open Question 2
- Question: How does the temporal evolution of language patterns and evasion strategies affect model longevity, given the 20-year data span?
- Basis in paper: [explicit] The Limitations section notes: "Language patterns and evasion strategies may evolve, requiring periodic updates."
- Why unresolved: The paper reports aggregate performance across the 2002–2022 timespan but does not analyze concept drift or performance degradation on recent versus older data slices.
- What evidence would resolve it: A temporal ablation study training on data from 2002–2012 and testing on 2013–2022 (and vice versa) to measure performance decay and identify shifts in evasion tactics over time.

### Open Question 3
- Question: To what extent does the 100-sample inter-annotator agreement (IAA) verification represent the true reliability of the full Gold-1K evaluation set?
- Basis in paper: [explicit] The Limitations section states: "single-annotator labeling with 100-sample IAA verification represents a limitation. Larger-scale human annotation would strengthen validity."
- Why unresolved: 90% of the Gold-1K test set relies on single-annotator labeling (validated by a separate 100-sample subset), potentially leaving unobserved noise in the ground truth used for leaderboard comparisons.
- What evidence would resolve it: A follow-up study where a second expert annotator labels the remaining 900 samples, allowing for a comparison of model benchmark rankings against the fully double-annotated ground truth.

### Open Question 4
- Question: Does the "disagreement mining" phenomenon—where judge-resolved samples improve generalization despite higher training loss—generalize to other subjective NLP tasks?
- Basis in paper: [inferred] The abstract and discussion highlight that judge-resolved samples enhance generalization, suggesting "disagreement mining acts as implicit regularization," but the paper restricts validation to the financial evasion task.
- Why unresolved: It is unclear if the benefit of training on difficult "boundary cases" (identified by LLM disagreement) is a general property of LLM-as-judge pipelines or an artifact of the specific evasion taxonomy.
- What evidence would resolve it: Applying the two-stage MMC training pipeline (Consensus vs. Judge-Refined) to other subjective benchmarks (e.g., stance detection or hallucination grading) to observe if the regularization effect persists.

## Limitations
- Temporal evolution of language patterns and evasion strategies may require periodic updates to maintain model effectiveness
- Single-annotator labeling with limited 100-sample IAA verification represents a constraint on validation robustness
- Domain-specific construction limits immediate generalizability to other adversarial Q&A settings without cross-domain validation

## Confidence
- **High confidence**: Macro-F1 scores and human Kappa agreement (0.835) are well-documented and reproducible
- **Medium confidence**: The disagreement mining regularization effect is supported by loss curves but lacks ablation studies
- **Medium confidence**: Model consensus reliability depends on untested assumptions about LLM bias independence

## Next Checks
1. **Bias Correlation Analysis**: Compute pairwise agreement matrices between frontier LLMs on a held-out set to quantify bias correlation. If correlation >0.7 for any pair, consensus reliability is compromised.
2. **Curriculum Necessity Test**: Train ablations with (a) consensus only, (b) joint training on all samples, (c) reversed curriculum (judge-resolved first). Measure performance delta and loss convergence patterns.
3. **Domain Transfer Validation**: Evaluate Eva-4B on earnings calls from industries underrepresented in training (e.g., technology, healthcare) to assess whether performance generalizes beyond the benchmark's S&P 500 focus.