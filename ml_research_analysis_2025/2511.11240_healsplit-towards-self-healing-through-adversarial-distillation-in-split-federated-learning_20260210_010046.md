---
ver: rpa2
title: 'HealSplit: Towards Self-Healing through Adversarial Distillation in Split
  Federated Learning'
arxiv_id: '2511.11240'
source_url: https://arxiv.org/abs/2511.11240
tags:
- data
- learning
- healsplit
- poisoning
- smashed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HealSplit introduces the first unified defense framework for Split
  Federated Learning (SFL), protecting against five types of poisoning attacks (label,
  data, smashed data, weight, and multi-vector poisoning). The method employs topology-aware
  detection using Personalized PageRank on k-nearest neighbor graphs to identify poisoned
  samples via topological anomaly scoring, followed by GAN-based generation of semantically
  consistent substitutes validated by a consistency student.
---

# HealSplit: Towards Self-Healing through Adversarial Distillation in Split Federated Learning

## Quick Facts
- **arXiv ID**: 2511.11240
- **Source URL**: https://arxiv.org/abs/2511.11240
- **Reference count**: 17
- **Primary result**: First unified defense framework for SFL against five poisoning attacks with >92% accuracy across all attack scenarios

## Executive Summary
HealSplit introduces a unified defense framework for Split Federated Learning that protects against five types of poisoning attacks through a combination of topology-aware detection and GAN-based recovery with adversarial distillation. The method employs Personalized PageRank on k-nearest neighbor graphs to identify poisoned samples via topological anomaly scoring, followed by semantically consistent substitute generation and validation. Extensive experiments demonstrate HealSplit consistently outperforms ten state-of-the-art defenses across multiple benchmark datasets while maintaining performance under adaptive attacks and system variations.

## Method Summary
HealSplit operates through a three-stage defense pipeline: (1) topology-aware detection using TAS computed via PPR on KNN graphs to identify poisoned samples, (2) GAN-based generative recovery to create semantically consistent substitutes for detected anomalies, and (3) adversarial multi-teacher distillation with momentum-adaptive weight balancing to validate substitutes and train a robust student model. The framework integrates seamlessly with SFL systems, processing smashed data centrally to detect and recover from label, data, smashed data, weight, and multi-vector poisoning attacks simultaneously.

## Key Results
- Achieves over 92% accuracy across all attack scenarios on four benchmark datasets
- Consistently outperforms ten state-of-the-art defenses in both attack and clean settings
- Maintains stable performance under adaptive attacks targeting the detection mechanism
- Reduces server-side gradient variance through momentum-adaptive optimization

## Why This Works (Mechanism)
HealSplit leverages topological properties of smashed data distributions to detect poisoning anomalies that would be indistinguishable in input space. By computing Personalized PageRank-based Topological Anomaly Scores on k-nearest neighbor graphs, the method identifies samples that are locally dense but globally isolated, a signature of poisoning attacks. The GAN-based recovery generates semantically consistent substitutes validated through a consistency student trained via adversarial multi-teacher distillation, ensuring the substitutes maintain class integrity while removing adversarial influence.

## Foundational Learning

- **Concept: Split Federated Learning (SFL)**
  - Why needed here: This is the paradigm HealSplit is designed for, where the model splits between client and server, transmitting "smashed data" (intermediate activations) and labels
  - Quick check question: Can you explain why traditional FL defenses that rely on full model updates might fail in SFL?

- **Concept: Data Poisoning Attacks**
  - Why needed here: The paper addresses five specific types; understanding distinctions between Label Poisoning, Data Poisoning, Smashed Data Poisoning, etc., is crucial for grasping the "unified" nature
  - Quick check question: How does smashing data poisoning differ from traditional input data poisoning?

- **Concept: Knowledge Distillation**
  - Why needed here: The core of HealSplit's recovery and validation system is an adversarial multi-teacher distillation framework, training a "student" to mimic "teacher" outputs
  - Quick check question: In this paper, what is the key difference between the role of the "Vanilla Teacher" and the "Anomaly-Influence Debiasing Teacher"?

## Architecture Onboarding

- **Component map**: SFL System (clients, server-side model) -> Topology Detection Module (TAS computation via PPR on KNN graphs) -> GAN Recovery Module (clean smashed data training) -> Multi-Teacher Distillation Framework (Vanilla Teacher + AD Teacher + Student)

- **Critical path**: Smashed Data -> Topology Detection (TAS) -> GAN Generation -> Student Validation -> Substitute into Training Pipeline. The multi-teacher distillation runs in parallel to produce a robust student model that guides validation.

- **Design tradeoffs**: The paper balances clean performance vs. robustness (Eq. 1), introduces computational overhead for GAN and multi-teacher training but claims to reduce gradient variance on server side (Theorem 1). Momentum-adaptive optimization (Eq. 14-15) is a key tradeoff lever, balancing influence of two teachers dynamically.

- **Failure signatures**:
  - Adaptive Attack: Attacker crafts poisoning to be topologically indistinguishable from clean data, causing detection failure
  - GAN Mode Collapse/Instability: GAN cannot produce diverse, high-quality smashed data, introducing new noise
  - Imbalanced Teacher Learning: Momentum-adaptive optimization fails, causing one teacher to dominate

- **First 3 experiments**:
  1. Reproduce Main Results (Table 2): Run HealSplit against five attack types on baseline dataset and compare accuracy against FedAvg, DnC, and FLTrust
  2. Ablation Study (Table 3): Disable each of three main components (Vanilla Teacher, AD Teacher, Distillation) one by one to confirm individual contribution
  3. Adaptive Attack Evaluation (Figure 8): Implement adaptive attack minimizing TAS divergence and evaluate performance degradation compared to baselines

## Open Questions the Paper Calls Out

- **Open Question 1**: How does HealSplit's computational and communication overhead scale with increasing numbers of clients and higher-dimensional smashed data?
  - Basis in paper: [inferred] Experiments only evaluate 10–30 clients (Fig. 3), and no analysis of training time, memory, or communication cost is provided despite the framework's multiple components

- **Open Question 2**: Can stronger adaptive attacks that simultaneously manipulate TAS, gradient interactions, and GAN recovery degrade HealSplit's effectiveness more severely than the current TAS-evasion attack?
  - Basis in paper: [explicit] The adaptive attack experiment (Fig. 8) shows noticeable performance drop, and the paper notes that "stealthy anomalies subsequently propagate to downstream stages"

- **Open Question 3**: Does accessing smashed data for topology-aware detection and GAN-based recovery introduce additional privacy leakage vectors in SFL?
  - Basis in paper: [inferred] HealSplit processes smashed data centrally for graph construction and synthesis, yet the paper does not analyze whether this central processing compromises privacy guarantees

## Limitations

- Several hyperparameters are unspecified including KNN graph construction parameters (k, α), GAN architecture details, distillation temperature τ, and momentum-adaptive optimization coefficients (m, κ)
- Claim of being the "first unified defense" against all five poisoning attacks would benefit from clearer justification of why prior methods cannot address this combination
- Theoretical claim about reduced gradient variance (Theorem 1) needs empirical validation beyond reported results

## Confidence

- **High Confidence**: The core topological detection methodology (TAS via PPR on KNN graphs) and GAN-based recovery approach are well-established techniques with compelling experimental results
- **Medium Confidence**: The adversarial distillation framework combining Vanilla and Anomaly-Influence Debiasing Teachers is novel, but claimed benefits of momentum-adaptive weight balancing would require careful implementation to verify
- **Low Confidence**: The theoretical claim about reduced gradient variance needs empirical validation as assumptions about poisoned sample distribution may not hold in practice

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary KNN graph parameters (k, α), distillation temperature τ, and momentum parameters (m, κ) to identify sensitivity and optimal configurations for different attack types

2. **Adaptive Attack Stress Test**: Implement a white-box adaptive attacker that explicitly crafts poisoned data to minimize TAS scores and maximize GIS similarity to clean samples, measuring the true robustness of the detection mechanism

3. **Computational Overhead Measurement**: Benchmark the additional computational cost of topology-aware detection, GAN generation, and multi-teacher distillation against baseline SFL implementations across different client scales (10, 50, 100 clients)