---
ver: rpa2
title: 'Forget What''s Sensitive, Remember What Matters: Token-Level Differential
  Privacy in Memory Sculpting for Continual Learning'
arxiv_id: '2509.12958'
source_url: https://arxiv.org/abs/2509.12958
tags:
- privacy
- learning
- score
- sensitivity
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses privacy risks in continual learning (CL) where
  models accumulate sensitive data over time. It proposes a token-level dynamic differential
  privacy (TDP) approach that assigns privacy budgets to individual tokens based on
  their sensitivity, derived from model uncertainty and contextual distinctiveness.
---

# Forget What's Sensitive, Remember What Matters: Token-Level Differential Privacy in Memory Sculpting for Continual Learning

## Quick Facts
- **arXiv ID:** 2509.12958
- **Source URL:** https://arxiv.org/abs/2509.12958
- **Reference count:** 19
- **Primary result:** PeCL achieves Avg accuracy of 0.535, Last accuracy of 0.573, and lowest forgetting (BWT = -0.093) on six-task datasets while maintaining robust privacy guarantees.

## Executive Summary
This paper addresses privacy risks in continual learning (CL) where models accumulate sensitive data over time. It proposes a token-level dynamic differential privacy (TDP) approach that assigns privacy budgets to individual tokens based on their sensitivity, derived from model uncertainty and contextual distinctiveness. This is combined with a privacy-guided memory sculpting module that selectively forgets sensitive information while preserving task-invariant knowledge. Experiments show PeCL outperforms state-of-the-art baselines, achieving strong accuracy metrics while maintaining privacy guarantees.

## Method Summary
PeCL is a continual learning framework that integrates token-level differential privacy with privacy-guided memory sculpting. It uses LoRA adapters to isolate task-specific updates and computes per-token sensitivity scores based on predictive uncertainty and contextual distinctiveness across tasks. The framework dynamically allocates privacy budgets to tokens, applying Gaussian noise to high-sensitivity tokens while preserving low-sensitivity ones. Memory sculpting modulates regularization strength inversely proportional to task sensitivity, allowing selective forgetting of sensitive information while retaining general knowledge. An unlearning loss term further suppresses memorization of high-sensitivity tokens.

## Key Results
- PeCL achieves Avg accuracy of 0.535 and Last accuracy of 0.573 on six-task benchmark
- Lowest forgetting among all methods with BWT = -0.093
- Outperforms state-of-the-art baselines including Parameter Efficient CL and Adaptive DP-Noise approaches
- Maintains robust privacy guarantees through token-level dynamic allocation

## Why This Works (Mechanism)

### Mechanism 1: Sensitivity-Adaptive Noise Injection
Allocating privacy budgets at the token level preserves model utility while maintaining differential privacy guarantees. The model calculates a fused sensitivity score for each token based on predictive uncertainty and contextual discriminativeness, mapping this to a dynamic privacy budget. High-sensitivity tokens receive stronger noise, while low-sensitivity tokens receive less noise. This assumes high model uncertainty and high task-specific concentration are reliable proxies for sensitive information requiring stronger protection.

### Mechanism 2: Privacy-Guided Stability Modulation
Dynamic regularization strength allows selective retention of general knowledge while permitting forgetting of sensitive parameters. The framework computes task-specific importance scores and modulates regularization loss inversely proportional to average token sensitivity. High-sensitivity tasks receive weaker regularization to facilitate adaptation, while low-sensitivity tasks are strictly regularized to preserve knowledge. This assumes task-level average sensitivity is a valid signal for parameter constraint strength.

### Mechanism 3: Targeted Gradient Suppression
Explicitly penalizing high-sensitivity tokens prevents model memorization of private entities. An unlearning loss term scales the cross-entropy loss for tokens exceeding a sensitivity threshold, discouraging reinforcement of weights associated with predicting sensitive tokens. This assumes suppressing gradient signals for high-sensitivity tokens effectively unlearns private data without harming learning of non-sensitive grammar or structure.

## Foundational Learning

- **Concept: Differential Privacy (DP) Gaussian Mechanism**
  - **Why needed here:** You must understand how adding noise calibrated to sensitivity bounds the probability of revealing any single training sample.
  - **Quick check question:** If you clip a gradient/embedding to norm $C$ and add noise $\sigma$, how does increasing $\sigma$ affect privacy ($\epsilon$) vs. utility?

- **Concept: Elastic Weight Consolidation (EWC) / Regularization**
  - **Why needed here:** The Memory Regularization module builds on the concept of penalizing changes to "important" weights to prevent catastrophic forgetting.
  - **Quick check question:** Why does standard EWC struggle in this privacy setup compared to the proposed dynamic regularization?

- **Concept: LoRA (Low-Rank Adaptation)**
  - **Why needed here:** The framework uses LoRA adapters to isolate task-specific updates and compute importance scores.
  - **Quick check question:** Where are the sensitivity-based noise and regularization applied in relation to the frozen LLM backbone vs. the LoRA adapters?

## Architecture Onboarding

- **Component map:** Tokenizer -> Sensitivity Scorer -> TDP Layer -> LoRA Backbone -> PMS Module -> Loss Aggregator
- **Critical path:** Input -> Sensitivity Score -> Noise Injection (TDP) -> LLM Forward Pass. Backward: Compute $L_{task}$ -> Calculate Unlearning Term -> Calculate Regularization Term -> Update LoRA.
- **Design tradeoffs:** $\alpha$ controls trust between model surprise and corpus frequency; $\theta$ balances catching private tokens vs killing utility; TDP adds embedding overhead, PMS adds gradient computation overhead.
- **Failure signatures:** Accuracy collapse + low privacy leakage indicates noise scale too high; high privacy leakage + high accuracy suggests sensitivity scores failing to identify actual PII; catastrophic forgetting indicates dynamic regularization incorrectly calculated.
- **First 3 experiments:**
  1. **Sanity Check (Ablation):** Run baseline without TDP vs TDP-only to verify token-level dynamic allocation drives accuracy gain.
  2. **Hyperparameter Sweep:** Tune $\alpha$ (0.3, 0.5, 0.7) on single task pair to observe uncertainty vs context trade-off.
  3. **Attack Simulation:** Perform membership inference attack on high-sensitivity tokens to verify unlearning loss reduces memorization vs noise injection alone.

## Open Questions the Paper Calls Out
None

## Limitations
- **Sensitivity scoring methodology uncertainty:** Proxies (model uncertainty, contextual distinctiveness) may not reliably capture true privacy risks, potentially failing to protect sensitive data or unnecessarily degrading utility.
- **Dynamic regularization design vulnerability:** Relaxing regularization to forget sensitive information may simultaneously degrade retention of task-relevant general knowledge.
- **Evaluation scope limitation:** Experiments use only 6 classification tasks with 3,000 samples each, not testing robustness across diverse privacy threat models or longer task sequences.

## Confidence
- **High confidence:** Token-level DP mechanism implementation and basic experimental results are reproducible given provided hyperparameters.
- **Medium confidence:** Claims about superior privacy-utility trade-offs rely on sensitivity scoring mechanism working as intended, which has uncertainties.
- **Low confidence:** Unlearning loss mechanism's effectiveness in actually preventing memorization of sensitive tokens versus just reducing task performance is not independently verified.

## Next Checks
1. **Sensitivity scoring validation:** Run controlled experiments where synthetic PII is embedded in different contexts (high vs low uncertainty, task-specific vs general) to verify scoring mechanism correctly identifies sensitive tokens.
2. **Membership inference attack benchmark:** Implement membership inference attack targeting tokens identified as high-sensitivity to verify unlearning mechanism actually reduces memorization rather than just reducing accuracy.
3. **Mixed-content task stress test:** Design tasks containing both sensitive information and critical general knowledge to evaluate whether dynamic regularization can successfully forget sensitive content while preserving general knowledge.