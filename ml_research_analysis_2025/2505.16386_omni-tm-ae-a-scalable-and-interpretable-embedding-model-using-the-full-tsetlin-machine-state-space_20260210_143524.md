---
ver: rpa2
title: 'Omni TM-AE: A Scalable and Interpretable Embedding Model Using the Full Tsetlin
  Machine State Space'
arxiv_id: '2505.16386'
source_url: https://arxiv.org/abs/2505.16386
tags:
- embedding
- state
- tm-ae
- literals
- clause
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Omni TM-AE, a scalable and interpretable
  embedding model based on the Tsetlin Machine (TM) that fully utilizes the complete
  state space of the TM, including excluded literals, to construct reusable embeddings
  in a single training phase. The approach aggregates signed state values from both
  original features and their negations to form embeddings, overcoming scalability
  limitations of prior TM-based methods.
---

# Omni TM-AE: A Scalable and Interpretable Embedding Model Using the Full Tsetlin Machine State Space

## Quick Facts
- **arXiv ID:** 2505.16386
- **Source URL:** https://arxiv.org/abs/2505.16386
- **Reference count:** 10
- **Primary result:** Achieves competitive performance with mainstream models across semantic similarity (Spearman 0.546, Kendall 0.393 average), sentiment classification (accuracy 0.830), and clustering tasks (ARI 0.1567, NMI 0.3698) while maintaining interpretability.

## Executive Summary
This paper introduces Omni TM-AE, a scalable and interpretable embedding model based on the Tsetlin Machine (TM) that fully utilizes the complete state space of the TM, including excluded literals, to construct reusable embeddings in a single training phase. The approach aggregates signed state values from both original features and their negations to form embeddings, overcoming scalability limitations of prior TM-based methods. Experimental results show Omni TM-AE achieves competitive performance with mainstream models across semantic similarity (Spearman 0.546, Kendall 0.393 average), sentiment classification (accuracy 0.830), and clustering tasks (ARI 0.1567, NMI 0.3698), while maintaining interpretability through transparent logical expressions.

## Method Summary
Omni TM-AE constructs embeddings by training a Coalesced Tsetlin Machine (CoTM) AutoEncoder for each target word, then extracting the full state matrix including literals below the inclusion threshold N. The embedding for each word is computed by aggregating signed state values across all positive-weighted clauses: for each feature xi, the component is calculated as the floor of the net influence (sum of states for xi minus sum of states for its negation) divided by the number of positive clauses. This single-phase training process enables reusable embeddings without requiring additional training phases to establish relationships between words.

## Key Results
- Semantic similarity: Spearman correlation 0.546 and Kendall 0.393 on benchmark datasets
- Sentiment classification: Accuracy of 0.830 on IMDb dataset
- Clustering: NMI of 0.3698 and ARI of 0.1567 on standard clustering tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Utilizing literals below the inclusion threshold N captures latent semantic information typically discarded during standard clause formation.
- **Mechanism:** The model aggregates the integer state values of all Tsetlin Automata associated with a target word, rather than just those exceeding the threshold N that form the active clause. These "excluded" states retain graded evidence of feature association.
- **Core assumption:** The "excluded" literals represent meaningful semantic proximity or distance rather than random noise.
- **Evidence anchors:** [Section 3.4] states excluded literals may contain important information; [Figure 3] visually annotates words below threshold N.
- **Break condition:** If the state space below N is primarily random initialization or unlearned noise, the signal-to-noise ratio degrades.

### Mechanism 2
- **Claim:** A single-phase training process allows for reusable embeddings by decoupling the training of individual target words from the pairwise comparison process.
- **Mechanism:** By training tokens individually and deriving a fixed-dimension vector e ∈ Z^d immediately after the training epoch, the model eliminates the need to retrain when new tokens are introduced.
- **Core assumption:** The semantic relationship between two words can be approximated by the independent context profiles of each word without requiring joint optimization.
- **Evidence anchors:** [Abstract] claims reusable embeddings through single training phase; contrasts with prior work requiring second training phase.
- **Break condition:** If semantic similarity relies heavily on complex, non-linear feature interactions between words that are not captured by their independent context profiles.

### Mechanism 3
- **Claim:** Combining original features and their negations creates a more discriminating embedding vector than positive-only evidence.
- **Mechanism:** The embedding component ei is computed by adding the state value of a feature xi and subtracting the state value of its negation ¬xi, capturing both what context defines a word and what context excludes it.
- **Core assumption:** The state values of negated literals are sufficiently trained and distinct from original literals to provide meaningful negative weighting.
- **Evidence anchors:** [Section 3.4] formalizes the signed summation; [Appendix A.1] notes negations help overcome restricted state range in large vocabularies.
- **Break condition:** If the TM's penalty/reward mechanism does not sufficiently differentiate the states of xi vs. ¬xi, the net influence will be near zero.

## Foundational Learning

- **Concept:** Tsetlin Automata (TA) State Space
  - **Why needed here:** The "memory" is a discrete integer state (1 to 2N). Understanding N as the decision threshold is critical to understanding what "excluded literals" are.
  - **Quick check question:** Can you explain why a literal with state N-1 is "excluded" from the logical clause but still contributes to the Omni TM-AE embedding?

- **Concept:** Propositional Logic (Clauses)
  - **Why needed here:** The fundamental unit of computation is a conjunction of literals (AND-rules). The paper leverages "sub-clause" information.
  - **Quick check question:** How does the aggregation of states differ from simply reading the final logical formula (the clause)?

- **Concept:** Coalesced Tsetlin Machine (CoTM)
  - **Why needed here:** The paper uses CoTM for multi-output learning (training multiple target words/classes in parallel).
  - **Quick check question:** How does CoTM allow the system to build embeddings for multiple words simultaneously in a single pass?

## Architecture Onboarding

- **Component map:** Input Binary vector X -> CoTM Core (State Matrix + Weight Matrix) -> Aggregation Layer (signed summation of all literals) -> Output Integer vector e (The Embedding)

- **Critical path:** The transition from the Update Phase (standard TM training) to the Omni Embedding Mechanism (Section 3.4). The extraction script must pull state values for all 2d literals, not just those returned by a standard "get_clause" API call.

- **Design tradeoffs:**
  - *Vector Size:* The embedding dimension is fixed to vocabulary size d (e.g., 40,000), which is sparse and potentially high-dimensional compared to dense Word2Vec vectors.
  - *Precision:* The embeddings are integer-based (derived from state counts), not floating-point weights.

- **Failure signatures:**
  - "Flat" Embeddings: If signed sums cancel out to zero or narrow ranges, check if number_of_examples or epochs are sufficient for TA states to diverge.
  - Scalability Bottleneck: Prior methods took "months"; ensure the single-phase Omni implementation avoids re-loops over the corpus.
  - Corpus Evidence: Current evidence is limited to preprints; rely on paper's internal benchmarks for validation.

- **First 3 experiments:**
  1. State Distribution Visualization: Reproduce Figure 3 for a new word to confirm "excluded" literals show meaningful variance rather than random noise.
  2. Ablation on Negations: Compute embeddings using only original features (+xi) vs. full signed states (xi - ¬xi) to measure performance delta.
  3. Similarity Benchmark: Run Spearman correlation test on RG65 dataset to verify single-phase embeddings match reported 0.644 score.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the single-phase Omni TM-AE architecture be extended to generate dynamic, context-aware embeddings rather than static ones?
- **Basis in paper:** [inferred] The paper benchmarks static Omni TM-AE against dynamic models like BERT and ELMo, noting competitive performance, but does not propose a mechanism for handling polysemy or context-dependency.
- **Why unresolved:** The current method aggregates state values across the entire training corpus, resulting in a single fixed vector per token regardless of context.
- **What evidence would resolve it:** A modified Omni TM-AE model that produces distinct embedding vectors for the same token based on its surrounding context window.

### Open Question 2
- **Question:** How does the aggregation of state values for negated literals impact the model's ability to distinguish antonyms from synonyms?
- **Basis in paper:** [inferred] The method computes embedding components by subtracting state values of negated literals, but the paper does not isolate how this affects the geometric relationship between antonymous words.
- **Why unresolved:** Evaluation relies on semantic similarity datasets that generally treat antonyms as dissimilar, but the explicit mechanism is not analyzed in depth.
- **What evidence would resolve it:** A targeted ablation study evaluating similarity scores for specific antonym-synonym pairs with and without the negation-state subtraction component.

### Open Question 3
- **Question:** Does the "state compression" observed in large vocabularies limit the expressiveness of embeddings as the feature space expands significantly beyond 40,000 tokens?
- **Basis in paper:** [inferred] The appendix notes a "restricted state range" for original literals when vocabulary increases, suggesting a potential upper bound on granularity of information captured.
- **Why unresolved:** Experiments cap vocabulary at 40,000 tokens; it is unclear if the mechanism remains effective when scaling to 100k+ vocabularies.
- **What evidence would resolve it:** Performance metrics and state distribution analysis on datasets with significantly larger vocabularies.

## Limitations
- Scalability Verification Gap: Claims competitive performance but limited to small benchmarks (RG65: 65 pairs); lacks validation on larger benchmarks like SimLex-999.
- Reproducibility Constraints: Depends on CoTM implementation details from Glimsdal and Granmo [2021] that are not fully specified.
- State Space Assumptions: Core mechanism assumes excluded literals contain meaningful semantic information, but lacks statistical analysis proving these states are non-random.

## Confidence
- **High Confidence:** Mathematical framework for signed state aggregation is well-defined; performance claims on benchmark datasets are directly reported with clear metrics.
- **Medium Confidence:** Interpretability claim relies on logical expressions being human-readable, but paper doesn't demonstrate how these relate to embedding quality.
- **Low Confidence:** Single-phase training advantage over multi-phase methods is asserted but not empirically validated with runtime comparisons.

## Next Checks
1. **State Distribution Analysis:** Generate clause state histograms for multiple target words to verify excluded literals show meaningful variance rather than uniform noise distribution.
2. **Negation Ablation Study:** Compute embeddings with and without negation states to quantify the contribution of signed aggregation to final performance.
3. **Large-Scale Benchmark Validation:** Test Omni TM-AE on SimLex-999 (999 pairs) and MTurk-771 (771 pairs) to confirm scalability claims hold beyond small benchmark datasets.