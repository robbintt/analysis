---
ver: rpa2
title: 'HUMANLLM: Benchmarking and Reinforcing LLM Anthropomorphism via Human Cognitive
  Patterns'
arxiv_id: '2601.10198'
source_url: https://arxiv.org/abs/2601.10198
tags:
- patterns
- pattern
- character
- psychological
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HumanLLM addresses the challenge of authentic anthropomorphism
  in LLMs by modeling psychological patterns as interacting causal forces rather than
  isolated traits. It constructs a dataset of 244 patterns from ~12,000 academic papers
  and synthesizes 11,359 scenarios where 2-5 patterns interact across multi-turn conversations.
---

# HUMANLLM: Benchmarking and Reinforcing LLM Anthropomorphism via Human Cognitive Patterns

## Quick Facts
- arXiv ID: 2601.10198
- Source URL: https://arxiv.org/abs/2601.10198
- Reference count: 40
- Primary result: HumanLLM-8B outperforms Qwen3-32B on multi-pattern dynamics despite 4× fewer parameters by modeling psychological patterns as interacting causal forces

## Executive Summary
HUMANLLM addresses the challenge of authentic anthropomorphism in LLMs by modeling psychological patterns as interacting causal forces rather than isolated traits. The framework constructs a dataset of 244 patterns from ~12,000 academic papers and synthesizes 11,359 scenarios where 2-5 patterns interact across multi-turn conversations. HumanLLM-8B outperforms Qwen3-32B on multi-pattern dynamics despite 4× fewer parameters, demonstrating that authentic anthropomorphism requires cognitive modeling of psychological processes underlying human behavior rather than surface-level behavioral mimicry.

## Method Summary
HUMANLLM employs a dual-level evaluation framework with IPE (Individual Pattern Expression) and MPD (Multi-Pattern Dynamics) checklists, using ternary scoring (+1/0/-1) with GPT-5-mini judge. The training involves SFT on Qwen3-8B/32B with 76,358 samples mixed from HumanLLM, OpenThoughts-114k, and CoSER at 4:4:2 ratio. Key hyperparameters include LR 5e-6, cosine scheduler, warmup 0.03, 2 epochs, batch size 2×8, max seq 6144, AdamW, BF16, and DeepSpeed ZeRO-3 Offload.

## Key Results
- HumanLLM-8B outperforms Qwen3-32B on multi-pattern dynamics despite 4× fewer parameters
- Strong human alignment (r=0.91) achieved through dual-level checklist evaluation
- Conventional holistic metrics conflate simulation accuracy with social desirability bias
- Negative transfer observed when training with generic datasets alone

## Why This Works (Mechanism)
The framework succeeds by treating psychological patterns as interacting causal forces rather than isolated traits. By synthesizing scenarios where 2-5 patterns interact across multi-turn conversations, the model learns the complex dynamics of how different cognitive patterns influence each other in realistic social situations. The trinity expression format ([inner thoughts], (actions), plain dialogue) enables capturing the full cognitive-behavioral spectrum of human responses.

## Foundational Learning
- **Pattern Library Construction**: Synthesizing 244 patterns from academic literature using structured templates
  - *Why needed*: Provides psychological foundation for authentic anthropomorphism
  - *Quick check*: Verify pattern definitions align with original academic sources
- **Scenario Synthesis**: Generating 11,359 multi-turn dialogues with 2-5 interacting patterns
  - *Why needed*: Creates realistic social dynamics for training
  - *Quick check*: Validate semantic compatibility of pattern combinations
- **Dual-Level Evaluation**: IPE for individual patterns and MPD for multi-pattern dynamics
  - *Why needed*: Separates pattern-level accuracy from holistic social desirability bias
  - *Quick check*: Compare holistic vs. checklist scores to detect prosocial bias

## Architecture Onboarding
**Component Map**: Pattern Library -> Scenario Generator -> Conversation Synthesizer -> Training Pipeline -> Dual-Level Evaluator
**Critical Path**: Scenario synthesis with pattern interactions → SFT training → checklist evaluation
**Design Tradeoffs**: Generic data inclusion risks negative transfer; pattern-specific data ensures authenticity but limits diversity
**Failure Signatures**: 
- Negative transfer when using generic datasets alone (IPE -53%, MPD -43%)
- Social desirability bias in holistic evaluation (>20% gap with checklist scores)
- Shallow pattern expression (IPE/MPD gap >20%)

**First Experiments**:
1. Train baseline with 2 available patterns to verify methodology at small scale
2. Test checklist evaluation using GPT-4o to confirm social desirability bias persists
3. Validate negative transfer finding by removing generic data from training

## Open Questions the Paper Calls Out
None

## Limitations
- Incomplete pattern definitions (only 2 of 244 patterns fully specified)
- GPT-5-mini API access unavailable for public reproduction
- Missing 11,359 scenarios and full checklist items for 242 patterns
- Heavy reliance on synthetic data generation quality

## Confidence
- **High confidence**: Core claim about multi-pattern dynamics outperforming isolated trait simulation
- **Medium confidence**: Dual-level checklist methodology for isolating psychological accuracy from social desirability bias
- **Low confidence**: Full dataset reproducibility due to missing components and inaccessible evaluation tools

## Next Checks
1. Replicate IPE/MPD checklist evaluation using GPT-4o or Claude to verify social desirability bias persistence
2. Train baseline model using available 2 pattern definitions to establish methodology efficacy
3. Test whether removing OpenThoughts-114k and CoSER improves performance on novel pattern combinations