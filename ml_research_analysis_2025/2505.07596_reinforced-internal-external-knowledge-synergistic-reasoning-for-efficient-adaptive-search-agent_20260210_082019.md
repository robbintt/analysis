---
ver: rpa2
title: Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient
  Adaptive Search Agent
arxiv_id: '2505.07596'
source_url: https://arxiv.org/abs/2505.07596
tags:
- knowledge
- answer
- search
- should
- internal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of current search agents that
  over-rely on external retrieval and underutilize their intrinsic parametric knowledge,
  leading to redundant retrievals and potential knowledge conflicts. The proposed
  Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA) enables
  LLMs to delineate their knowledge boundaries and prioritize internal parametric
  knowledge before resorting to external retrieval.
---

# Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent

## Quick Facts
- arXiv ID: 2505.07596
- Source URL: https://arxiv.org/abs/2505.07596
- Authors: Ziyang Huang; Xiaowei Yuan; Yiming Ju; Jun Zhao; Kang Liu
- Reference count: 40
- Key outcome: IKEA achieves 45.26% EM with 0.95 RT on NQ, outperforming Search-R1 (42.92% EM, 1.21 RT) while reducing retrieval frequency.

## Executive Summary
This paper addresses the limitations of current search agents that over-rely on external retrieval and underutilize their intrinsic parametric knowledge, leading to redundant retrievals and potential knowledge conflicts. The proposed Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA) enables LLMs to delineate their knowledge boundaries and prioritize internal parametric knowledge before resorting to external retrieval. IKEA achieves this through a novel knowledge-boundary aware reward function and training dataset construction that incentivizes correct answers while minimizing unnecessary retrievals. Evaluations across multiple knowledge-intensive reasoning tasks demonstrate that IKEA significantly outperforms baseline methods, reducing retrieval frequency while improving performance, and exhibits strong generalization capabilities on out-of-distribution datasets.

## Method Summary
IKEA uses Group Relative Policy Optimization (GRPO) to train an LLM agent to adaptively decide between using internal parametric knowledge and external retrieval. The agent is trained on a balanced dataset of 4,000 "easy" questions (answerable from parameters) and 4,000 "hard" questions (requiring retrieval) constructed via knowledge probing. The reward function combines exact match rewards with a knowledge-boundary aware component that scales positively with fewer retrievals when answers are correct, and applies small negative rewards when retrieval is used but answers are wrong. The agent learns to recognize its knowledge boundaries and retrieve only when necessary, achieving better performance with fewer retrievals than baseline methods.

## Key Results
- IKEA-3B achieves 45.26% EM on NQ with 0.95 RT, outperforming Search-R1 (42.92% EM, 1.21 RT)
- Significant RT reduction across all datasets: 0.99→0.95 (NQ), 1.13→0.93 (PopQA), 1.44→1.26 (HotpotQA), 1.20→0.99 (2Wiki)
- Strong generalization: maintains RT-EM tradeoff on out-of-distribution datasets (PopQA/2Wiki) without fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reward function design shapes retrieval behavior more than model architecture
- Mechanism: The knowledge-boundary aware reward $r_{kb}$ scales positively with fewer retrievals when answers are correct ($r_{ans}=1$), and applies small negative reward when retrieval is used but answer is wrong. This creates a gradient that preferentially reinforces internal knowledge usage when sufficient.
- Core assumption: The model can reliably distinguish between questions it can answer parametrically versus those requiring external knowledge through exploration.
- Evidence anchors:
  - [abstract]: "This is achieved using a novel knowledge-boundary aware reward function... incentivizing the model to deliver accurate answers, minimize unnecessary retrievals"
  - [section 5.1]: "Without the knowledge boundary aware reward ('w/o r_kb'), both effective retrievals and response length show a consistent upward trend"
  - [corpus]: Related work (R1-Searcher++, SEM) confirms reward shaping is critical for search-efficient LLMs, though specific formulations vary

### Mechanism 2
- Claim: Balanced dataset difficulty prevents policy collapse to single-knowledge-source behavior
- Mechanism: Training with 1:1 ratio of $Q_{easy}$ (answerable from parameters) and $Q_{hard}$ (requiring retrieval) ensures the policy receives balanced gradients for both knowledge behaviors. Pure $Q_{easy}$ causes retrieval avoidance; pure $Q_{hard}$ causes over-retrieval.
- Core assumption: Knowledge probing via N-sample in-context learning accurately categorizes questions by the model's true parametric knowledge.
- Evidence anchors:
  - [section 3.2]: "we construct the training dataset with a 1:1 ratio of $Q_{easy}$ and $Q_{hard}$ questions"
  - [section 5.2]: Table 3 shows easy-only training drops RT to 0.49 and EM to 43.25%; hard-only increases RT to 1.44 but EM drops to 41.89%
  - [corpus]: Insufficient corpus evidence for this specific 1:1 ratio claim

### Mechanism 3
- Claim: Multi-turn GRPO enables emergent knowledge boundary discrimination without explicit classifiers
- Mechanism: Group-relative advantages normalize rewards across rollouts, allowing the policy to learn when retrieval adds marginal value. The model develops internal uncertainty signals that correlate with knowledge boundaries through trial-and-error exploration.
- Core assumption: Sufficient exploration during training (16 rollouts per task) allows discovery of both successful parametric and retrieval paths.
- Evidence anchors:
  - [section 3.1]: "GRPO performs multiple rollouts per task and calculate the relative reward within the group as the advantage"
  - [figure 2]: Training logs show retrieval counts initially increase then decrease, suggesting learned discrimination
  - [corpus]: Related work (MCTS-RAG, Fast/Slow Thinking) suggests multi-path exploration aids reasoning, but GRPO-specific evidence is limited

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: IKEA uses GRPO instead of PPO to avoid training a separate value model, reducing memory overhead for agent training
  - Quick check question: Can you explain why GRPO uses group-normalized rewards instead of a learned value function?

- Concept: **Knowledge Probing via In-Context Learning**
  - Why needed here: The dataset construction depends on classifying questions as $Q_{easy}$ or $Q_{hard}$ by sampling answers N times with few-shot prompting
  - Quick check question: How would you determine if a question falls within a model's parametric knowledge boundary?

- Concept: **Retrieval-Augmented Generation (RAG) Trade-offs**
  - Why needed here: IKEA explicitly addresses the latency/conflict costs of over-retrieval versus hallucination risks of under-retrieval
  - Quick check question: What are two failure modes when an LLM always retrieves versus never retrieves?

## Architecture Onboarding

- Component map:
  - Dataset construction (knowledge probing) -> Reward definition (r_ans + r_kb) -> GRPO rollout collection (16 rollouts/task) -> Policy update -> Evaluation on easy/hard subsets

- Critical path: Knowledge probing via in-context learning → 1:1 easy/hard dataset construction → GRPO training with knowledge-boundary aware rewards → Policy convergence → Adaptive retrieval behavior

- Design tradeoffs:
  - $r_{kb}^+ = 0.6$ and $r_{kb}^- = 0.05$ chosen empirically; higher $r_{kb}^+$ risks over-penalizing necessary retrieval, lower values reduce efficiency gains
  - Maximum retrievals ($RT_{max}=3$) balances multi-hop reasoning needs against latency
  - 1:1 easy/hard ratio optimizes for balanced behavior but may underrepresent real-world question distributions

- Failure signatures:
  - **Always-retrieve collapse**: Reward function missing $r_{kb}$ component or $r_{kb}^-$ too large
  - **Never-retrieve collapse**: Training on easy-only dataset or $r_{kb}^-$ absent
  - **Format instability**: Base models (IKEA-Zero) require longer convergence for tag structure learning
  - **OOD degradation**: Performance drops on PopQA/2Wiki if training data doesn't cover sufficient knowledge domains

- First 3 experiments:
  1. **Reproduce main results** (Table 1): Train IKEA-3B on NQ+HotpotQA, evaluate on all 4 datasets; verify EM improvement over Search-R1 with lower RT
  2. **Ablate reward components** (Table 2): Remove $r_{kb}$ and $r_{kb}^-$ separately; confirm RT increases (w/o $r_{kb}$) and EM drops (w/o $r_{kb}^-$)
  3. **Test dataset balance sensitivity** (Table 3): Train with easy-only and hard-only datasets; verify policy collapse to extreme retrieval behaviors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can knowledge boundary learning be made dynamic during the training process rather than relying on static dataset construction?
- Basis in paper: [explicit] The Conclusion explicitly identifies the need to "explore more dynamic knowledge boundary learning methods" to address the limitation of relying on specific dataset construction for knowledge boundary awareness.
- Why unresolved: The current IKEA implementation relies on a fixed dataset where questions are pre-labeled as "easy" or "hard" based on the initial model's parametric knowledge, which does not account for the model's evolution during training.
- What evidence would resolve it: A method that successfully updates the knowledge boundary labels or reward signals in real-time as the model learns, demonstrating maintained or improved performance without fixed pre-probing.

### Open Question 2
- Question: To what extent is IKEA's performance sensitive to the manual tuning of the reward function parameters (e.g., $r_{kb+}$, $r_{kb-}$)?
- Basis in paper: [inferred] The Limitations section notes that "the reward function parameters might require grid searching," implying a potential lack of robustness and suggesting that optimal performance relies on finding specific hyperparameters rather than a generalized rule.
- Why unresolved: The paper does not provide a sensitivity analysis or an automated mechanism for adjusting these reward coefficients, leaving the stability of the training process uncertain.
- What evidence would resolve it: A hyperparameter sensitivity analysis showing the model's performance variance across different reward settings, or the introduction of an adaptive reward scaling mechanism.

### Open Question 3
- Question: Does the static probing used to construct the training dataset introduce a distribution shift as the model updates its internal knowledge via reinforcement learning?
- Basis in paper: [inferred] The paper probes the model to label questions as $Q_{easy}$ (internal) or $Q_{hard}$ (external) before training. However, as RL updates the model weights, the model may internalize knowledge from $Q_{hard}$ questions, potentially rendering the static "knowledge-boundary aware" rewards suboptimal or incorrect later in training.
- Why unresolved: The authors assume the initial knowledge boundary delineation remains valid throughout the optimization process without verifying if the boundary shifts.
- What evidence would resolve it: An analysis measuring the accuracy of the initial "easy/hard" labels at various training checkpoints to determine if the static dataset construction conflicts with the model's evolving parametric state.

## Limitations

- The exact number of samples (N) used in knowledge probing for dataset construction is not specified, making it difficult to reproduce the easy/hard classification threshold
- Reward function parameters ($r_{kb+}$, $r_{kb-}$) are empirically chosen without systematic sensitivity analysis, suggesting potential instability
- Performance generalization claims on out-of-distribution datasets lack rigorous statistical validation and cross-domain testing

## Confidence

**High confidence (4/5)**: The core mechanism of using GRPO with knowledge-boundary aware rewards to shape retrieval behavior is well-supported by ablation studies. The empirical results showing improved EM with reduced RT across multiple datasets are robust and statistically significant.

**Medium confidence (3/5)**: The claim that balanced easy/hard dataset construction prevents policy collapse is supported by ablation experiments, but the specific 1:1 ratio recommendation is based on limited exploration. The paper doesn't test ratios between 1:1 and the extremes (easy-only/hard-only), leaving uncertainty about the optimal balance for different knowledge domains.

**Low confidence (2/5)**: The assertion that IKEA exhibits strong generalization on out-of-distribution datasets (PopQA/2Wiki) is supported by single-point comparisons but lacks rigorous statistical analysis. The paper doesn't control for potential dataset-specific factors or perform cross-validation across multiple OOD datasets.

## Next Checks

1. **Dataset construction sensitivity analysis**: Reproduce the training pipeline while varying the knowledge probing sample count (N) from 5 to 50. Measure how RT and EM change across training runs to identify the stability range for easy/hard classification and determine whether the choice of N significantly impacts final performance.

2. **Reward function hyperparameter sweep**: Systematically vary $r_{kb}^+$ and $r_{kb}^-$ across an order of magnitude (e.g., 0.3-3.0 for positive, 0.01-0.5 for negative). Identify the Pareto frontier where EM is maximized while RT is minimized, and test whether the empirically chosen values actually lie on this frontier or represent a suboptimal local optimum.

3. **Domain transfer validation**: Train IKEA on a Wikipedia-based dataset (as in the paper), then evaluate and fine-tune on a non-Wikipedia domain (e.g., scientific papers or legal documents). Measure the degradation in knowledge boundary discrimination and test whether additional fine-tuning on domain-specific easy/hard examples restores the RT-EM tradeoff curve observed in the original domain.