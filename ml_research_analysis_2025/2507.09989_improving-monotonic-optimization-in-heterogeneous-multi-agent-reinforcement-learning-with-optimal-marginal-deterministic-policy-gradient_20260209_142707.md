---
ver: rpa2
title: Improving monotonic optimization in heterogeneous multi-agent reinforcement
  learning with optimal marginal deterministic policy gradient
arxiv_id: '2507.09989'
source_url: https://arxiv.org/abs/2507.09989
tags:
- policy
- learning
- omdpg
- multi-agent
- heterogeneous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the policy updating baseline drift problem
  in heterogeneous multi-agent reinforcement learning (MARL) when combining Partial
  Parameter-sharing (ParPS) with the sequential update scheme required by HAPPO. The
  authors propose the Optimal Marginal Deterministic Policy Gradient (OMDPG) algorithm,
  which introduces two key innovations: the Optimal Marginal Q-value (OMQ) function
  that replaces sequential policy ratio calculations with optimal joint action sequences,
  and the Generalized Q Critic (GQC) that employs pessimistic uncertainty-constrained
  loss for stable Q-value estimation.'
---

# Improving monotonic optimization in heterogeneous multi-agent reinforcement learning with optimal marginal deterministic policy gradient

## Quick Facts
- **arXiv ID:** 2507.09989
- **Source URL:** https://arxiv.org/abs/2507.09989
- **Reference count:** 30
- **Primary result:** OMDPG significantly outperforms HAPPO baseline, achieving 0.89 win rate on MMM2 vs 0.45

## Executive Summary
This paper addresses the incompatibility between Partial Parameter-sharing (ParPS) and sequential update schemes in heterogeneous multi-agent reinforcement learning, particularly when combining with HAPPO. The authors propose the Optimal Marginal Deterministic Policy Gradient (OMDPG) algorithm that resolves the "policy updating baseline drift" problem through three key innovations: the Optimal Marginal Q-value (OMQ) function that replaces sequential policy ratio calculations with optimal joint action sequences, the Generalized Q Critic (GQC) that employs pessimistic uncertainty-constrained loss for stable Q-value estimation, and the Centralized Critic Grouped Actor (CCGA) architecture. Experimental results demonstrate OMDPG achieves higher win rates and faster convergence across SMAC and MAMuJoCo environments.

## Method Summary
OMDPG combines partial parameter-sharing with a sequential update scheme by introducing Optimal Marginal Q-value (OMQ) to eliminate policy updating baseline drift. The method uses a CCGA architecture where agents are grouped by capability and share network parameters within groups. The GQC employs an ensemble of critics with pessimistic uncertainty loss to handle out-of-distribution joint actions that arise when computing OMQ. The actor maximizes the OMQ value using deterministic policy gradients, while the critic is trained with both true and uncertainty-penalized targets.

## Key Results
- OMDPG reaches 0.89 winning rate on MMM2 map compared to 0.45 for HAPPO baseline
- Demonstrates significantly faster convergence across all tested environments
- Outperforms various state-of-the-art MARL baselines in both SMAC and MAMuJoCo environments
- Maintains monotonic improvement guarantee while enabling efficient parameter sharing

## Why This Works (Mechanism)

### Mechanism 1: Optimal Marginal Q-value (OMQ) for Baseline Drift Correction
OMDPG replaces the sequential policy ratio with Optimal Marginal Q-value to resolve baseline drift when combining ParPS with HAPPO's sequential scheme. Instead of comparing old vs. new policy probabilities, OMQ computes the marginal contribution of an agent's action using the global Q-function and optimal future actions, which doesn't depend on old policy parameters. This eliminates the corruption of importance sampling ratios that occurs when updating first agent's parameters affects subsequent agents' baselines.

### Mechanism 2: Pessimistic Uncertainty-constrained Loss for OOD Actions
The GQC uses an ensemble of critics with pessimistic uncertainty loss to stabilize learning against out-of-distribution joint actions. When computing OMQ, optimal future actions are likely not in the replay buffer. The uncertainty loss penalizes high variance in critic ensembles for these OOD actions, discouraging the actor from pursuing unverified high-reward estimates. Disagreement among critic heads correlates with estimation error for OOD actions.

### Mechanism 3: Centralized Critic Grouped Actor (CCGA) Architecture
CCGA groups agents by capability, allowing ParPS to accelerate learning without sacrificing heterogeneity representation. Agents in the same group share network parameters, enabling experience sharing and faster learning. The centralized critic computes Q-values for OMQ computation. This architecture balances sample efficiency (through sharing) with representational capacity (through grouping).

## Foundational Learning

- **Concept: Multi-Agent Advantage Decomposition (MAAD)**
  - Why needed: OMDPG preserves the monotonic improvement guarantee provided by MAAD while fixing the parameter-sharing conflict
  - Quick check: Can you explain why the sequential update scheme in standard HAPPO requires independent policies (NoPS) to guarantee monotonic improvement?

- **Concept: Parameter Sharing Strategies (NoPS vs. ParPS vs. FuPS)**
  - Why needed: The core problem is the incompatibility of ParPS with sequential updates
  - Quick check: In the "policy updating baseline drift" problem, why does updating the first agent in a group corrupt the baseline for the second agent in the same group?

- **Concept: Off-policy / OOD Evaluation in RL**
  - Why needed: GQC relies on evaluating actions not taken in the environment (optimal future actions)
  - Quick check: Why does the standard TD-loss fail when training on joint actions composed of "optimal" future actions rather than sampled actions?

## Architecture Onboarding

- **Component map:**
  - Environment -> Buffer D stores (s, a, r, s')
  - Critic Ensemble (C_k=5) -> Computes Q-values for GQC
  - Centralized Critic -> Takes global state + joint action
  - Grouped Actors (K groups) -> Share parameters within groups
  - Actor Networks -> Input: local observation o_i, Output: action a_i

- **Critical path:**
  1. Data Collection: Environment steps store (s, a, r, s') in Buffer D
  2. Critic Update (GQC): Sample batch, compute y_true for real actions and y_PU for OOD actions, update Critics using combined loss
  3. Actor Update (OMQ): For each group k, compute φ*_ψ(s, a_{1:i}) using minimum Q-value from critic ensemble, maximize via gradient descent

- **Design tradeoffs:**
  - Ensemble Size (C_k): Higher C_k improves uncertainty estimation but increases computational cost linearly. Paper recommends C_k=5
  - PU Weight (λ_PU): High values stabilize training but may cause excessive pessimism. Paper finds λ_PU=0.1 optimal for complex scenarios

- **Failure signatures:**
  - High Variance in Win Rate: Indicates GQC failing to stabilize Q-values (check if λ_PU is too low)
  - Stagnant Performance: Indicates OMQ not driving improvement (check if greedy future actions are too noisy or ParPS groups mismatched)

- **First 3 experiments:**
  1. Validation of Drift: Implement HAPPO with ParPS on simple SMAC map (6m2m_15m) to reproduce divergence/baseline drift
  2. Ablation on PU Loss: Run OMDPG on MMM2 with λ_PU = 0 vs. λ_PU = 0.1 to verify pessimistic constraint necessity
  3. Scalability Check: Run OMDPG on homogeneous map vs. highly heterogeneous map to verify grouping efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can OMDPG maintain its monotonic improvement guarantee and computational efficiency when scaled to environments with significantly larger agent populations?
- **Basis in paper:** The authors state in the conclusion: "In the future, we intend to explore heterogeneous MARL problems in large-scale scenarios and environments. The scalability problem is crucial for the MARL algorithm application in real-world scenarios."
- **Why unresolved:** Experimental validation was limited to relatively small environments (up to 30 agents), and ensemble of critics increases computational overhead
- **What evidence would resolve it:** Performance benchmarks in environments with >100 agents demonstrating tractable wall-clock time and memory usage

### Open Question 2
- **Question:** How can LLM techniques be integrated into the OMDPG framework to improve adaptability in complex, heterogeneous scenarios?
- **Basis in paper:** The conclusion suggests: "Future improvements and developments could focus on integrating... LLM techniques for better adaptability of heterogeneous MARL methods in complex scenarios."
- **Why unresolved:** Current work relies on standard neural network actors and critics without defining mechanism for incorporating LLMs
- **What evidence would resolve it:** Modified OMDPG architecture utilizing LLMs for high-level planning or semantic understanding, showing improved sample efficiency or zero-shot generalization

### Open Question 3
- **Question:** Is OMDPG robust to suboptimal or unknown agent groupings, or can the method be extended to learn optimal group structure dynamically?
- **Basis in paper:** The method depends on ParPS "based on agent grouping," assuming groups are manually defined by physical heterogeneity
- **Why unresolved:** If predefined grouping doesn't align with functional similarity required for effective parameter sharing, performance may degrade
- **What evidence would resolve it:** Experiments comparing fixed-group OMDPG against learnable/adaptive grouping in environments where physical and behavioral heterogeneity are decoupled

## Limitations

- OMQ computation reliability depends heavily on critic's ability to generalize to optimal future actions not present in replay buffer
- No detailed sensitivity analysis for critical hyperparameters like λ_PU across different environment complexities
- Generalizability of agent grouping strategies across heterogeneous environments not thoroughly validated
- Computational overhead from ensemble of critics (C_k=5) may limit scalability to larger agent populations

## Confidence

- **High Confidence:** Mechanism for resolving policy updating baseline drift through OMQ replacement is well-theoretically grounded and mathematically sound
- **Medium Confidence:** GQC's effectiveness in stabilizing training through pessimistic uncertainty loss is supported by ablation results but underlying assumption lacks strong empirical validation
- **Low Confidence:** Generalizability of agent grouping strategies across heterogeneous environments is not thoroughly validated

## Next Checks

1. **OOD Action Robustness Test:** Implement OMDPG on modified SMAC environment where optimal future actions are systematically perturbed. Measure how variance in critic estimates correlates with performance degradation to validate GQC's uncertainty handling.

2. **Hyperparameter Sensitivity Analysis:** Run OMDPG across grid of λ_PU values (0.01, 0.1, 0.5, 1.0) on MMM2 and Walker-6×1-Het environments. Document impact on convergence speed and final performance to establish hyperparameter stability.

3. **Grouping Strategy Ablation:** Create version of OMDPG using random agent groupings versus capability-based groupings on same heterogeneous environments. Compare win rates and learning curves to quantify importance of correct grouping decisions.