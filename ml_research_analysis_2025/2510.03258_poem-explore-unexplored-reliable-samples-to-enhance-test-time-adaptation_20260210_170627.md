---
ver: rpa2
title: 'POEM: Explore Unexplored Reliable Samples to Enhance Test-Time Adaptation'
arxiv_id: '2510.03258'
source_url: https://arxiv.org/abs/2510.03258
tags:
- samples
- poem
- reliable
- adaptation
- entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: POEM addresses the challenge of test-time adaptation (TTA) by identifying
  and utilizing "potentially reliable samples" - target samples initially classified
  as high-entropy that become low-entropy after model updates. The method iteratively
  selects these samples and updates normalization parameters, while freezing higher
  layers to preserve domain-agnostic knowledge.
---

# POEM: Explore Unexplored Reliable Samples to Enhance Test-Time Adaptation

## Quick Facts
- arXiv ID: 2510.03258
- Source URL: https://arxiv.org/abs/2510.03258
- Reference count: 40
- Key outcome: POEM achieves 66.2% accuracy on ImageNet-C with ViT-Base-LN compared to 63.0% for the best baseline

## Executive Summary
POEM addresses the challenge of test-time adaptation (TTA) by identifying and utilizing "potentially reliable samples" - target samples initially classified as high-entropy that become low-entropy after model updates. The method iteratively selects these samples and updates normalization parameters, while freezing higher layers to preserve domain-agnostic knowledge. An additional Adapt Branch network is introduced to capture target-specific information. Experiments on ImageNet-C, CIFAR100-C, and real-world datasets demonstrate POEM consistently outperforms state-of-the-art entropy-based TTA methods.

## Method Summary
POEM introduces a novel approach to test-time adaptation by identifying samples that initially appear unreliable (high entropy) but become reliable (low entropy) after model updates. The method iteratively selects these "potentially reliable samples" and updates normalization parameters while freezing higher layers to preserve domain-agnostic knowledge. An Adapt Branch network is introduced to capture target-specific information. The approach is computationally efficient at 200 FPS and robust to entropy threshold variations, demonstrating consistent improvements across multiple benchmark datasets.

## Key Results
- Achieves 66.2% accuracy on ImageNet-C with ViT-Base-LN compared to 63.0% for the best baseline
- Maintains 200 FPS computational efficiency during adaptation
- Demonstrates robustness to entropy threshold variations
- Outperforms state-of-the-art entropy-based TTA methods on multiple benchmark datasets

## Why This Works (Mechanism)
POEM leverages the observation that some initially high-entropy samples can become low-entropy after model updates, indicating they contain reliable information that becomes accessible as the model adapts. By identifying and utilizing these "potentially reliable samples," the method can adapt more effectively than approaches that only use low-entropy samples. The selective update of normalization parameters while freezing higher layers preserves domain-agnostic knowledge while allowing domain-specific adaptation.

## Foundational Learning

**Entropy-based sample selection**: Measures prediction uncertainty to identify reliable samples
- Why needed: Enables selection of samples that become reliable after adaptation
- Quick check: Verify entropy distribution changes before and after model updates

**Normalization parameter adaptation**: Updates batch normalization statistics during test-time adaptation
- Why needed: Normalization statistics shift between domains and need adjustment
- Quick check: Compare performance with frozen vs. updated normalization parameters

**Layer freezing strategy**: Preserves higher layer weights while adapting lower layers
- Why needed: Higher layers capture domain-agnostic features that should remain stable
- Quick check: Measure performance impact of freezing different layer combinations

## Architecture Onboarding

**Component map**: Input -> Encoder (frozen higher layers) -> Normalization Adapter -> Adapt Branch -> Output
**Critical path**: Sample selection → Entropy calculation → Model update → Normalization parameter update
**Design tradeoffs**: Selective adaptation vs. full model update, computational efficiency vs. adaptation quality
**Failure signatures**: Overfitting to noisy samples, failure to adapt to domain shifts, computational bottlenecks
**First experiments**: 1) Baseline entropy-based TTA comparison, 2) Ablation study on layer freezing, 3) Adapt Branch contribution analysis

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided content.

## Limitations

- Generalizability across diverse domain shifts beyond tested scenarios remains uncertain
- Reliance on entropy-based selection may struggle with ambiguous or multi-modal target distributions
- Adapt Branch contribution to overall performance gains versus standard normalization adaptation is unclear

## Confidence

High - Experimental results demonstrate consistent improvements over baselines with statistically significant margins
Medium - Claims about computational efficiency and robustness to threshold variations need broader validation
Low - Adapt Branch contribution and generalizability to non-vision tasks lack sufficient empirical backing

## Next Checks

1. Test POEM's performance on extreme domain shifts and multi-modal target distributions to evaluate robustness of the entropy-based sample selection mechanism
2. Conduct ablation studies specifically isolating the Adapt Branch's contribution to performance gains versus standard normalization-only adaptation
3. Evaluate computational efficiency across different hardware platforms and model architectures beyond the reported configuration