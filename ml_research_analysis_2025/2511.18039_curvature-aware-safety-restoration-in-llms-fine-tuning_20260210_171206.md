---
ver: rpa2
title: Curvature-Aware Safety Restoration In LLMs Fine-Tuning
arxiv_id: '2511.18039'
source_url: https://arxiv.org/abs/2511.18039
tags:
- safety
- loss
- harmful
- restoration
- curvature-aware
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies that fine-tuning large language models for
  downstream tasks causes safety alignment drift, even when using parameter-efficient
  methods like LoRA. The authors discover that the loss landscape geometry for harmful
  content remains structurally preserved across base and fine-tuned models, while
  task-specific behaviors change significantly.
---

# Curvature-Aware Safety Restoration In LLMs Fine-Tuning

## Quick Facts
- arXiv ID: 2511.18039
- Source URL: https://arxiv.org/abs/2511.18039
- Authors: Thong Bach; Thanh Nguyen-Tang; Dung Nguyen; Thao Minh Le; Truyen Tran
- Reference count: 40
- Key outcome: Fine-tuning LLMs with LoRA causes safety alignment drift, but curvature-aware restoration can reduce harmful response rates from 21.5-24.7% to 1.5-3.0% while maintaining task performance.

## Executive Summary
This paper addresses safety alignment drift that occurs when fine-tuning large language models for downstream tasks, even when using parameter-efficient methods like LoRA. The authors discover that while task-specific behaviors change significantly during fine-tuning, the loss landscape geometry for harmful content remains structurally preserved. Leveraging this insight, they develop a curvature-aware restoration method using influence functions and second-order optimization to selectively increase loss on harmful inputs while preserving task performance. The approach demonstrates significant safety improvements across three model families (Llama-2 7B, Llama-3.1 8B, Qwen 2.5 7B) with minimal impact on task capabilities.

## Method Summary
The method identifies that fine-tuning preserves harmful-content loss geometry while task performance changes. It uses L-BFGS to approximate the inverse Hessian of the retain-set loss, then computes influence updates that maximize loss on harmful content while constraining changes to task performance. The process involves: (1) fine-tuning base models with LoRA on task data, (2) constructing L-BFGS curvature pairs from mixed forget/retain datasets, (3) applying influence updates to increase harmful-content loss while maintaining task performance via a trust-region constraint, and (4) iterating for 3 steps with L2 regularization. The approach operates within the LoRA adapter space (r=32) for computational tractability.

## Key Results
- Harmful response rates drop from 21.5-24.7% to 1.5-3.0% across Llama-2 7B, Llama-3.1 8B, and Qwen 2.5 7B models
- Task performance is maintained or improved on Dolly test set with eval loss around 1.2
- Zero-shot utility on ARC-C, GSM8K, ToxiGen, and TruthfulQA shows gains or comparable performance
- Robustness to adversarial attacks with ASR below 65% compared to baseline ~78%
- Enhanced few-shot learning capabilities post-restoration

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The loss landscape geometry associated with harmful content remains structurally preserved across fine-tuning, enabling targeted safety restoration.
- **Mechanism:** Fine-tuning optimizes task-specific regions of parameter space but leaves safety-relevant regions largely undisturbed. The paper quantifies this via Pearson correlations between base and fine-tuned model losses: harmful content shows r > 0.77 across all models, while task data shows r ≈ 0.01–0.55. Structural difference metrics (based on Laplacian correlation) show 1.46% change for harmful vs. 20.37% for benign content.
- **Core assumption:** Safety behaviors occupy functionally distinct, geometrically separable regions in parameter space from task-specific behaviors.
- **Evidence anchors:**
  - [abstract] "fine-tuned models preserve the geometric structure of their loss landscapes concerning harmful content"
  - [Section 2.1, Table 1] Correlation coefficients: Llama-3 8B shows r=0.995 for harmful vs. r=0.550 for task data
  - [corpus] Related work on safety subspaces (SafeMERGE, LSSF) assumes similar functional separation but uses merging/subspace projection rather than curvature-aware navigation
- **Break condition:** If safety and task objectives are deeply entangled (e.g., creative writing with safety boundaries), the structural separation assumption degrades and method effectiveness may decline.

### Mechanism 2
- **Claim:** Influence functions with second-order optimization provide the update direction that maximally increases harmful-content loss while minimally affecting retain-set performance.
- **Mechanism:** The method solves a constrained optimization: maximize L_forget subject to L_retain ≤ L_retain(θ_tuned) + ε. The influence update Δθ = H⁻¹_retain ∇L_forget represents the steepest ascent direction for harmful loss under the Riemannian geometry defined by the retain-set Hessian. L-BFGS approximates H⁻¹_retain efficiently.
- **Core assumption:** The local quadratic approximation of retain loss (Equation 3) holds within the trust region defined by ε.
- **Evidence anchors:**
  - [Section 3.1] "This approximation can be interpreted as the unconstrained solution to Equation 4, where the trust region constraint is relaxed"
  - [Appendix C.2, Figures 8-9] Curvature-aware trajectory follows direct path through high-value regions; first-order methods oscillate at higher learning rates
  - [corpus] Gradient Surgery for Safe LLM Fine-Tuning addresses similar multi-objective conflict but uses gradient projection rather than curvature information
- **Break condition:** If Hessian approximation quality degrades (e.g., insufficient L-BFGS history, ill-conditioned curvature), update direction may increase retain loss unexpectedly.

### Mechanism 3
- **Claim:** Hybrid L-BFGS curvature construction from both forget and retain subsets improves restoration by capturing safety-task boundary geometry.
- **Mechanism:** The curvature buffer integrates gradients from three disjoint datasets: D_curv_forget (harmful subset), D_retain^(1), D_retain^(2) (task subsets). This ensures the inverse Hessian approximation spans both safety-critical and task-aligned directions. Only 10 L-BFGS pairs with filtering (⟨s_t, y_t⟩ > 10⁻⁶) are needed.
- **Core assumption:** A small number of high-quality curvature pairs sufficiently characterizes the local geometry for both objective regions.
- **Evidence anchors:**
  - [Appendix B.1] "Our empirical analysis reveals that just 10 high-quality pairs sufficiently approximate the local curvature structure"
  - [Section 4.2, Table 2] Method achieves 1.5–3.0% harmful response rate across model families
  - [corpus] Weak direct evidence; related work on Safe LoRA uses pre-defined alignment subspaces rather than data-driven curvature estimation
- **Break condition:** If harmful and retain gradients are nearly collinear (opposing objectives in same direction), curvature mixing provides limited benefit.

## Foundational Learning

- **Concept: Influence Functions**
  - Why needed here: Core mathematical tool for estimating how parameter changes affect loss on specific data subsets without explicit retraining
  - Quick check question: If you scale the influence update by 2×, what happens to the constraint L_retain ≤ L_retain(θ_tuned) + ε?

- **Concept: L-BFGS Quasi-Newton Optimization**
  - Why needed here: Provides tractable inverse Hessian approximation for models where exact Hessian is computationally infeasible
  - Quick check question: Why does L-BFGS require storing only m vector pairs rather than the full Hessian matrix?

- **Concept: Trust Region Methods**
  - Why needed here: Provides theoretical grounding for constraining updates to regions where quadratic approximation is valid
  - Quick check question: What happens to the trust radius δ when the actual-to-predicted reduction ratio ρ_t falls below 0.25?

## Architecture Onboarding

- **Component map:** Fine-tuning with LoRA (r=32, lr=2e-4, 1 epoch) -> L-BFGS curvature construction (10 pairs from 3 datasets) -> Influence update computation (H⁻¹∇L_forget) -> Parameter update with L2 regularization -> Safety evaluation (AdvBench + Llama-3 Guard + human review)

- **Critical path:**
  1. Fine-tune base model with LoRA on task data (Dolly)
  2. Partition harmful dataset into D_curv_forget (256 samples) and D_forget (50 held-out)
  3. Run L-BFGS construction over 3 datasets → 10 curvature pairs
  4. Compute influence update on D_forget, apply with step scaling + L2 regularization
  5. Repeat for 3 iterations (empirically sufficient per Appendix B.1)

- **Design tradeoffs:**
  - LoRA-only updates reduce expressiveness vs. full-model fine-tuning but make curvature estimation tractable
  - ε=0.1 constraint threshold balances safety vs. task preservation; tighter constraints preserve utility but may limit safety gains
  - 3-iteration limit prevents overcorrection but may underperform on severely degraded models

- **Failure signatures:**
  - Harmful response rate plateaus above 5% after 3 iterations → increase iterations or check D_forget quality
  - Task performance drops >5% → reduce step size η or increase L2 weight λ
  - L-BFGS curvature pairs rejected (⟨s_t, y_t⟩ < 10⁻⁶) → increase batch size or vary learning rates across curvature construction steps
  - Near-zero correlation recovery on retain set (r < 0.2 after restoration) → check data partition independence

- **First 3 experiments:**
  1. **Baseline validation:** Fine-tune Llama-3.1 8B on Dolly, measure HRR on AdvBench and task loss on Dolly test set (expect ~25% HRR, 1.2 eval loss)
  2. **Ablation on curvature sources:** Run restoration with L-BFGS built from (a) retain-only, (b) forget-only, (c) mixed data; compare HRR and task preservation
  3. **Robustness check:** Apply prefilling attack (4 non-refusal tokens) to restored model; verify ASR < 65% (baseline LoRA ~78%)

## Open Questions the Paper Calls Out

None

## Limitations

- The structural separation assumption between safety and task behaviors may not generalize to domains where safety constraints are intrinsically entangled with task objectives
- The L-BFGS approximation quality depends critically on the diversity and quantity of curvature pairs, with scaling properties for larger models unexplored
- The evaluation covers three model families but only single variants within each family, leaving uncertainty about performance at different scales

## Confidence

**High Confidence:** The empirical demonstration that fine-tuned models preserve harmful-content loss geometry while task performance changes significantly (HRR reduction from 21.5-24.7% to 1.5-3.0% across three model families). The correlation analysis showing r > 0.77 for harmful vs. r ≈ 0.01–0.55 for task data provides strong evidence for Mechanism 1.

**Medium Confidence:** The effectiveness of L-BFGS with 10 curvature pairs for inverse Hessian approximation. While the paper shows this works for the tested models, the scaling properties for larger models remain unexplored. The 3-iteration restoration limit appears sufficient but may be suboptimal for severe alignment drift.

**Low Confidence:** The robustness claims against adversarial attacks and parameter perturbations. The evaluation uses specific attack patterns (prefilling, VISAGE) but doesn't explore the full space of potential failure modes or the method's performance under extreme perturbation scenarios.

## Next Checks

1. **Entanglement Stress Test:** Evaluate the method on fine-tuning tasks where safety constraints are deeply integrated with task objectives (e.g., medical advice generation with harmful content restrictions). Measure whether structural separation degrades and HRR increases beyond acceptable thresholds.

2. **Scaling Validation:** Apply the method to larger model variants (e.g., Llama-3.1 70B) with proportional increases in curvature pair count. Monitor L-BFGS approximation quality metrics and verify whether the 10-pair approach scales effectively.

3. **Adversarial Robustness Expansion:** Test against gradient-based adversarial attacks and fine-tuning-aware poisoning strategies. Evaluate whether the curvature-aware restoration can withstand attacks specifically designed to exploit the method's reliance on loss landscape geometry.