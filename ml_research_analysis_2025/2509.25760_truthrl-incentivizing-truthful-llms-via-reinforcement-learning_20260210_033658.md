---
ver: rpa2
title: 'TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning'
arxiv_id: '2509.25760'
source_url: https://arxiv.org/abs/2509.25760
tags:
- truthrl
- accuracy
- truthfulness
- reward
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TruthRL, a reinforcement learning framework
  that directly optimizes the truthfulness of large language models. Unlike prior
  accuracy-focused approaches, TruthRL uses a ternary reward scheme distinguishing
  correct answers (+1), hallucinations (-1), and abstentions (0), implemented via
  GRPO.
---

# TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2509.25760
- **Source URL**: https://arxiv.org/abs/2509.25760
- **Reference count**: 33
- **Primary result**: TruthRL reduces hallucinations by up to 28.9% and improves truthfulness by up to 21.1% compared to vanilla RL across four knowledge-intensive benchmarks.

## Executive Summary
TruthRL introduces a reinforcement learning framework that directly optimizes the truthfulness of large language models by distinguishing correct answers (+1), hallucinations (-1), and abstentions (0) through a ternary reward scheme implemented via GRPO. Extensive experiments show TruthRL significantly reduces hallucinations while maintaining competitive accuracy, with gains consistent across different backbone models and both retrieval and non-retrieval setups. The framework demonstrates particular effectiveness on hallucination-baiting questions and scales well across different model sizes, addressing the critical problem of factual accuracy in knowledge-intensive tasks.

## Method Summary
TruthRL implements a ternary reward scheme (+1 correct, 0 abstention, -1 incorrect) using online GRPO with group-relative advantage estimation. The method employs an LLM-based verifier (Llama3.3-70B-Instruct) to classify responses, enabling the model to learn when to abstain rather than hallucinate. Training involves knowledge boundary probing through 256 response samples per question to identify out-of-knowledge questions, with group size G=64 for advantage computation. The framework is implemented using VeRL for RL optimization and Open-R1 library as base, trained on 8×H100 with DeepSpeed ZeRO-3 and FlashAttention-2, using bf16 mixed precision.

## Key Results
- Reduces hallucinations by up to 28.9% compared to vanilla RL across four benchmarks
- Improves truthfulness by up to 21.1% while maintaining competitive accuracy
- Ternary reward scheme achieves 25.6 average truthfulness vs 4.5 for binary reward on CRAG with retrieval
- Demonstrates consistent gains across various backbone models and both retrieval and non-retrieval setups

## Why This Works (Mechanism)

### Mechanism 1: Ternary Reward Signal Separation
The ternary reward (+1 correct, 0 abstention, -1 hallucination) creates differential advantage estimates under GRPO. Within sampled groups, abstention receives higher relative advantage than hallucination (e.g., Âternary(x, yabstain) = +0.5 vs. Âternary(x, yhallucinate) = -0.5), driving policy updates that prefer abstention over guessing when uncertain. This design enables models to learn when to say "I don't know" rather than fabricate answers.

### Mechanism 2: Online GRPO with Group-Based Advantage
Online reinforcement learning with group-relative advantage estimation provides more stable optimization than offline alternatives. GRPO samples G responses per question, computes advantage by normalizing rewards within each group (Âi = (ri − mean)/std), removing the need for a learned value function and enabling real-time policy updates. This allows the model to explore its current policy distribution rather than overfitting to fixed preference pairs.

### Mechanism 3: Knowledge Boundary Probing for Data Construction
Pre-training data construction via probing identifies out-of-knowledge (OOK) questions, enabling targeted abstention training. For each training question, 256 responses are sampled; if none are correct, the question is marked as OOK and set to "I don't know." This creates a curriculum where the model learns which questions are beyond its parametric knowledge, though the knowledge-enhanced ternary variant underperformed simple ternary (23.2 vs 25.6 T).

## Foundational Learning

- **Advantage Functions in RL**: GRPO's core learning signal comes from advantage estimates (relative reward differences within groups), not absolute rewards. Quick check: If all sampled responses receive identical rewards, what happens to the advantage and thus the policy update?

- **Calibration vs. Accuracy Tradeoff**: The paper's central claim is that accuracy-driven methods amplify hallucination; truthfulness requires balancing correctness with uncertainty awareness. Quick check: A model achieves 60% accuracy with 40% hallucination. Is it more truthful than one with 50% accuracy and 5% hallucination (truthfulness = Acc − Hall)?

- **LLM-as-Judge Verification**: The entire reward signal depends on a verifier (Llama3.3-70B-Instruct) classifying responses; rule-based string matching fails catastrophically. Quick check: Why would exact string matching produce a negative truthfulness score (Table 5: -3.6) despite low hallucination (3.6%)?

## Architecture Onboarding

- **Component map**: Pretrained backbone → vLLM rollout (temp=1.0, top_p=1.0) → LLM verifier (Llama3.3-70B-Instruct) → ternary reward assignment (+1/0/-1) → GRPO with group-relative advantage → policy update via Open-R1

- **Critical path**: 1) Load pretrained backbone (Qwen2.5-7B or Llama3.1-8B-Instruct). 2) Sample G responses via vLLM rollout. 3) Pass responses + ground truth to verifier → classify → assign ternary reward. 4) Compute advantage per response (group mean/std normalization). 5) Update policy via GRPO loss with KL regularization (β=0.001). 6) Repeat for ~200 steps.

- **Design tradeoffs**: Ternary vs. Knowledge-Enhanced Ternary: Simple ternary outperforms (25.6 vs 23.2 T) because OOK labels may be noisy; prefer simplicity unless you have high-confidence knowledge boundary data. Online GRPO vs. Iterative DPO: Online provides consistent gains but requires real-time inference infrastructure; DPO is cheaper but shows regression after 3–4 iterations. LLM Verifier vs. Rule-Based: LLM verifier essential for semantic equivalence; rule-based causes collapse to over-conservative abstention (Table 5).

- **Failure signatures**: Collapse to Always-Abstain: Truthfulness goes negative (e.g., -3.6) with near-zero hallucination → verifier is over-penalizing correct answers (likely rule-based). No Abstention Learning: Uncertainty rate stays near 0% despite training → reward scheme may be effectively binary (conflating abstention with error). Regressing After Iterations: Performance degrades after iteration 3 in iterative DPO → offline preference data is stale; switch to online GRPO.

- **First 3 experiments**: 1) Reproduce ternary vs. binary ablation on CRAG subset with a 3B model (faster iteration): confirm 15–20 point truthfulness gap. 2) Swap verifier models (Llama3.3-70B → Qwen2.5-72B): verify robustness per Table 6; if results diverge significantly, investigate prompt alignment. 3) Stress test on hallucination-baiting questions (CRAG comparison subset): confirm TruthRL maintains <20% hallucination vs. >35% for baselines (Table 2).

## Open Questions the Paper Calls Out

- **Integrating reasoning-quality rewards**: How can reasoning-quality rewards be effectively integrated with outcome-based ternary rewards without compromising factual accuracy? The authors state this requires "non-trivial design to balance multiple objectives" and leave exploration for future work. Heuristic reasoning reward strategies either maintained or slightly reduced outcome truthfulness despite improving reasoning scores, suggesting tension between the two objectives.

- **Performance of knowledge-enhanced variants**: Why does the simple ternary reward scheme consistently outperform knowledge-enhanced reward variants? Knowledge-enhanced rewards led to lower truthfulness scores (23.2 vs 25.6 T), but the mechanism behind this counterintuitive result is not explained.

## Limitations

- The ternary reward mechanism's effectiveness critically depends on the verifier's ability to correctly distinguish response types, with sensitivity to verifier model choice and prompt engineering remaining unclear.

- The knowledge boundary probing method assumes 256 samples are sufficient to identify out-of-knowledge questions, but this may not generalize well to models with different parametric knowledge distributions.

- The online GRPO approach requires substantial computational resources (8×H100) and real-time inference infrastructure, potentially limiting practical adoption.

## Confidence

- **High confidence**: The ternary reward signal design and its superiority over binary alternatives is well-supported by ablation studies (Table 3 shows 25.6 vs 4.5 truthfulness). The general trend of TruthRL reducing hallucinations while maintaining accuracy is consistently demonstrated across multiple benchmarks.

- **Medium confidence**: The claim that ternary rewards enable "uncertainty awareness" and knowledge boundary recognition is supported by results but relies heavily on the verifier's subjective judgment. The comparison with iterative DPO shows strong results but uses different training budgets.

- **Low confidence**: The knowledge-enhanced ternary variant underperforms simple ternary (23.2 vs 25.6), suggesting the probing mechanism may introduce noise, but the paper doesn't fully explore why.

## Next Checks

1. **Verifier Robustness Test**: Swap the LLM verifier (Llama3.3-70B → Qwen2.5-72B or another model) and measure performance degradation. If results change significantly, investigate prompt sensitivity and verifier reliability as a bottleneck.

2. **Sample Size Sensitivity**: Vary the knowledge boundary probing sample count (e.g., 64, 128, 512) to determine if 256 is optimal or if results are sensitive to this hyperparameter. This tests the assumption that current sample size is sufficient.

3. **Hallucination-Baiting Stress Test**: Create a controlled subset of questions designed to trigger hallucinations (similar to CRAG comparison subset) and measure whether TruthRL maintains its hallucination reduction advantage compared to vanilla RL and other baselines. This validates the core truthfulness claim under adversarial conditions.