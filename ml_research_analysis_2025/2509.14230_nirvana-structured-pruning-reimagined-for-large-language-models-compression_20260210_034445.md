---
ver: rpa2
title: 'NIRVANA: Structured pruning reimagined for large language models compression'
arxiv_id: '2509.14230'
source_url: https://arxiv.org/abs/2509.14230
tags:
- pruning
- data
- sparsity
- nirv
- calibration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NIRVANA, a structured pruning method for
  large language models that integrates NTK-based saliency scoring, adaptive sparsity
  allocation between attention and MLP modules, and KL-divergence-driven calibration
  data selection. NIRVANA improves upon existing structured pruning methods by aligning
  pruning decisions with model training dynamics and balancing module-specific pruning
  rates.
---

# NIRVANA: Structured pruning reimagined for large language models compression

## Quick Facts
- arXiv ID: 2509.14230
- Source URL: https://arxiv.org/abs/2509.14230
- Reference count: 40
- Primary result: Structured pruning method that improves perplexity and downstream task performance compared to baselines while providing consistent inference speedups

## Executive Summary
NIRVANA is a structured pruning method for large language models that addresses the limitations of existing approaches by integrating NTK-based saliency scoring, adaptive sparsity allocation between attention and MLP modules, and KL-divergence-driven calibration data selection. The method targets structured pruning (removing entire neurons/heads) for hardware efficiency while preserving model performance. Experiments on Llama3.1-8B demonstrate that NIRVANA achieves better perplexity and downstream task performance under various sparsity levels compared to baselines like LLM-Pruner and SliceGPT, while also providing consistent inference speedups through dimension alignment.

## Method Summary
NIRVANA computes NTK-guided saliency scores using gradients from a single backward pass on calibration data, combines them with weight magnitudes, and aggregates into structured groups (MLP neurons, attention heads). It then applies adaptive sparsity allocation with a γ parameter that prunes MLP modules more aggressively than attention modules, based on their different roles in the model. The method includes KL-divergence-based calibration data selection to improve pruning outcomes, and hardware-aware dimension alignment to ensure consistent inference speedups. The entire pipeline is designed to work with a single training pass and maintain model performance while reducing computational cost.

## Key Results
- NIRVANA achieves better perplexity and downstream task performance compared to baselines (LLM-Pruner, SliceGPT, FLAP) across multiple sparsity levels on Llama3.1-8B
- The method provides consistent inference speedups through dimension alignment, with hardware efficiency gains that correlate with FLOPs reduction
- Adaptive sparsity allocation (γ ≈ 3.36) outperforms uniform pruning across modules, with attention-only pruning causing rapid performance collapse
- KL-divergence-based calibration data selection shows linear correlation with downstream perplexity across 50 runs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NTK-guided saliency scores may preserve both zero-shot accuracy and fine-tuning capability better than magnitude-based scoring.
- Mechanism: The saliency score S(Wi,j) = |∂f/∂Wi,j · Wi,j| combines gradient sensitivity with weight magnitude. The paper argues this aligns with the Neural Tangent Kernel under Adam (approximated as SignGD), which characterizes how model predictions evolve during gradient-based training. If the NTK remains stable post-pruning, the pruned model may follow a similar optimization trajectory during fine-tuning.
- Core assumption: The SignGD kernel adequately approximates Adam's training dynamics for LLMs (paper cites Kunstner et al., 2023; Li et al., 2024 as justification). Theoretical bound in Proposition 4.1 assumes bounded gradients and non-zero weights for pruned parameters.
- Evidence anchors:
  - [abstract] "Leveraging a first-order saliency criterion derived from the Neural Tangent Kernel under Adam optimization dynamics"
  - [Section 4.1] Derives saliency from first-order Taylor expansion and connects to NTK stability bound in Proposition 4.1/E.1
  - [corpus] No direct corpus validation; related work on pruning-training dynamics alignment exists but does not evaluate this specific NTK formulation
- Break condition: If gradients are near-zero for important weights (gradient starvation), or if Adam's true dynamics diverge significantly from SignGD approximation, the NTK alignment may not correlate with fine-tuning recovery.

### Mechanism 2
- Claim: Adaptive sparsity allocation between MLP and attention modules using ratio γ may outperform uniform pruning across modules.
- Mechanism: Rather than applying the same sparsity rate to attention heads and MLP neurons, NIRVANA allocates higher pruning rates to MLP (controlled by γ ≈ 3.36 for Llama3.1-8B), motivated by prior observations that MLP stores factual knowledge more efficiently while attention heads are more critical for long-range dependencies. The paper derives γ analytically from expected output magnitude ratios.
- Core assumption: The analytical derivation of γ (Section F) assumes Gaussian weight initialization and specific variance relationships that may not hold for trained models.
- Evidence anchors:
  - [Section 4.3] Explicit formula for vAttn and vMLP with γ parameter
  - [Figure 2] Shows attention-only pruning causes rapid performance collapse while MLP-only pruning is more stable
  - [corpus] Corpus papers on adaptive/structural pruning exist but do not directly validate this specific MLP/attention balancing approach
- Break condition: If model architecture significantly differs (e.g., GQA variations, different MLP expansion ratios), the derived γ may not transfer without recalibration.

### Mechanism 3
- Claim: KL-divergence-based calibration data selection correlates with downstream perplexity, providing a proxy for data quality.
- Mechanism: Rather than using random or diversity-based calibration data selection, NIRVANA samples multiple candidate batches, prunes with each, and selects the batch minimizing KL divergence between original and pruned model outputs on a held-out set. The paper observes a roughly linear relationship between this KL value and perplexity.
- Core assumption: The correlation between KL divergence on held-out data and downstream task performance holds across domains and model scales.
- Evidence anchors:
  - [Section 4.4] Formal definition of C* selection criterion
  - [Figure 3] Shows linear correlation between KL divergence and perplexity across 50 runs
  - [corpus] Williams and Aletras (2024) and Bandari et al. (2024) cited for calibration data importance, but no direct corpus validation of KL-selection method
- Break condition: If calibration data distribution diverges significantly from the held-out evaluation distribution, the KL proxy may select unrepresentative batches.

## Foundational Learning

- Concept: **Neural Tangent Kernel (NTK)**
  - Why needed here: Core theoretical grounding for the saliency score; NTK characterizes how network outputs evolve under gradient descent in the infinite-width limit.
  - Quick check question: Can you explain why a stable NTK post-pruning would imply preserved fine-tuning dynamics?

- Concept: **Structured vs Unstructured Pruning**
  - Why needed here: NIRVANA targets structured pruning (removing neurons/heads) for hardware efficiency, unlike SparseGPT/Wanda which produce irregular patterns.
  - Quick check question: What is the trade-off between structured pruning's hardware compatibility versus unstructured pruning's accuracy preservation?

- Concept: **Transformer Module Roles (Attention vs MLP)**
  - Why needed here: The adaptive sparsity allocation relies on understanding that MLP stores factual knowledge while attention handles long-range dependencies.
  - Quick check question: Why might pruning attention heads more aggressively than MLP cause faster performance degradation?

## Architecture Onboarding

- Component map: Weight-level saliency scoring -> Structured grouping -> Adaptive sparsity allocation -> Calibration data selection -> Dimension alignment
- Critical path: Calibration data selection → gradient computation → saliency aggregation → global ranking with γ allocation → dimension alignment → zero-masking
- Design tradeoffs:
  - **KL selection accuracy vs compute cost**: More candidate batches improve selection but require more pruning trials
  - **γ tuning**: Higher γ prunes MLP more aggressively; paper uses γ≈3.36 analytically derived but empirically validates γ=3.0 as optimal
  - **Hardware alignment vs parameter efficiency**: Dimension alignment (multiples of 8) may leave some capacity unused but ensures consistent speedups
- Failure signatures:
  - **Dimension misalignment**: Latency may increase despite parameter reduction (see Table 8: Uniform-11468 vs Uniform-11472)
  - **Attention over-pruning**: Rapid perplexity collapse when attention sparsity too high (Figure 2)
  - **Poor calibration data**: High variance in perplexity outcomes; KL selection mitigates but does not eliminate
- First 3 experiments:
  1. **Baseline comparison on Llama3.1-8B at 20%/40%/50% sparsity**: Compare NIRVANA vs LLM-Pruner, SliceGPT, FLAP on WikiText2/PTB/Lambada perplexity and downstream tasks (ARC-e, Winogrande, HellaSwag)
  2. **Ablation on γ values**: Sweep γ ∈ {1.0, 2.0, 3.0, 4.0} at 50% sparsity to validate analytical γ≈3.36; monitor perplexity degradation curve
  3. **Calibration data sensitivity test**: Run KL-selection with T={5, 10, 20} trials; measure correlation between selected KL value and final perplexity to determine sufficient search budget

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can NIRVANA's saliency scoring and adaptive sparsity allocation be effectively extended to Mixture-of-Experts (MoE) architectures?
- Basis in paper: [explicit] The limitations section explicitly identifies extending the method to MoE models as a "promising direction."
- Why unresolved: The current experiments focus exclusively on standard dense Transformer architectures (Llama, Qwen, T5), whereas MoE models have sparse activation patterns that may require different pruning heuristics.
- What evidence would resolve it: Successful application of NIRVANA to an MoE model (e.g., Mixtral) demonstrating maintained performance and inference speedups comparable to dense model results.

### Open Question 2
- Question: Do higher-order dynamics beyond the first-order NTK approximation provide significant improvements in pruning accuracy?
- Basis in paper: [explicit] The limitations section notes that the approach relies on first-order approximations and suggests "future work may explore higher-order dynamics."
- Why unresolved: While first-order Taylor expansion provides a theoretical bound, it may not capture all complex dependencies within deep LLMs, potentially leaving performance gains on the table.
- What evidence would resolve it: A comparative analysis measuring the performance gap between the current Adam-based NTK saliency and a second-order (Hessian-based) approximation.

### Open Question 3
- Question: What specific statistical properties of calibration data define "quality" for structured pruning, independent of KL divergence?
- Basis in paper: [explicit] The authors state in the Limitations section that "the exact data properties that contribute to effective pruning remain unclear."
- Why unresolved: While the paper introduces a KL-divergence proxy to select data, it observes that surface-level coherence or diversity does not consistently correlate with performance, leaving the underlying mechanism unidentified.
- What evidence would resolve it: A correlation analysis linking specific activation statistics or token distributions of the calibration set to the final perplexity of the pruned model.

## Limitations

- The NTK-based saliency scoring relies on SignGD approximation to Adam dynamics, which may not hold for large-scale LLM training with adaptive learning rates and gradient clipping
- The adaptive sparsity allocation formula for γ assumes Gaussian weight initialization and specific variance relationships that may not transfer to trained models
- Hardware efficiency gains are validated only on NVIDIA A100 GPUs with Tensor Cores and may not generalize to other architectures or batch sizes

## Confidence

**High Confidence:**
- Structured pruning achieves consistent inference speedups when dimensions are properly aligned
- Adaptive sparsity allocation (γ ≈ 3.36) outperforms uniform pruning across modules
- KL-divergence selection correlates with downstream perplexity

**Medium Confidence:**
- NTK-guided saliency scoring preserves fine-tuning capability better than magnitude-based scoring
- The analytical derivation of γ provides optimal MLP/attention balance
- NIRVANA's perplexity and downstream task performance improvements versus baselines

**Low Confidence:**
- The SignGD kernel adequately approximates Adam's training dynamics for LLMs
- Calibration data selection method generalizes across domains
- Hardware efficiency gains transfer to non-Tensor Core architectures

## Next Checks

1. **Cross-Architecture Hardware Validation**: Evaluate NIRVANA-pruned models on different GPU architectures (e.g., H100, RTX 4090) and batch sizes to verify dimension alignment benefits generalize beyond A100 Tensor Cores.

2. **Multi-Domain Calibration Test**: Validate the KL-divergence selection method using calibration data from diverse domains (technical, conversational, multilingual) to test robustness when evaluation distribution differs from BookCorpus.

3. **Gradient Approximation Analysis**: Conduct ablation studies comparing NTK-guided saliency scores against exact second-order methods (e.g., Hessian-based) and alternative gradient approximations to quantify the impact of the SignGD assumption.