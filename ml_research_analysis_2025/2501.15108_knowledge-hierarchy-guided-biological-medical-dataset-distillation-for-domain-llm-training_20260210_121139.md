---
ver: rpa2
title: Knowledge Hierarchy Guided Biological-Medical Dataset Distillation for Domain
  LLM Training
arxiv_id: '2501.15108'
source_url: https://arxiv.org/abs/2501.15108
tags:
- knowledge
- hierarchy
- dataset
- arxiv
- biomedical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KAILIN, an automated framework for distilling
  high-quality biomedical training data from scientific literature. The core innovation
  is using Medical Subject Headings (MeSH) knowledge hierarchy to guide question generation
  and selection from 23 million PubMed abstracts, creating preference datasets that
  align with domain expertise.
---

# Knowledge Hierarchy Guided Biological-Medical Dataset Distillation for Domain LLM Training

## Quick Facts
- arXiv ID: 2501.15108
- Source URL: https://arxiv.org/abs/2501.15108
- Reference count: 32
- Primary result: Llama3-70B trained with KAILIN data outperforms GPT-4 with MedPrompt on PubMedQA benchmarks

## Executive Summary
This paper introduces KAILIN, an automated framework for distilling high-quality biomedical training data from scientific literature. The core innovation is using Medical Subject Headings (MeSH) knowledge hierarchy to guide question generation and selection from 23 million PubMed abstracts, creating preference datasets that align with domain expertise. The framework fine-tunes two question generators, retrieves context, evaluates alignment to MeSH hierarchy, and produces optimized question-answer pairs for training. Experimental results show that Llama3-70B models trained with KAILIN-generated datasets outperform GPT-4 with MedPrompt on PubMedQA benchmarks, despite having fewer parameters. The framework demonstrates particular effectiveness in enabling general-purpose models to surpass domain-specific models through targeted knowledge hierarchy alignment.

## Method Summary
KAILIN operates through a multi-stage pipeline that begins with retrieving abstracts from PubMed using MeSH descriptors as queries. The framework fine-tunes two question generators - one for yes/no questions and one for general questions - using PubMedQA as training data. After generating candidate questions from the retrieved abstracts, KAILIN evaluates each question-answer pair using two specialized evaluators that measure alignment with the MeSH hierarchy. The evaluation process computes cosine similarity between MeSH embeddings and generates hierarchical similarity scores, filtering out pairs that don't meet the threshold. The final output consists of high-quality question-answer pairs derived from PubMed abstracts, optimized for biomedical domain training.

## Key Results
- Llama3-70B trained on KAILIN dataset achieves 63.3% accuracy on PubMedQA, outperforming GPT-4 with MedPrompt (57.3%)
- The framework processes 23 million PubMed abstracts to generate domain-specific training data
- Models trained with KAILIN data show improved performance across different model scales (7B and 70B parameters)
- Knowledge hierarchy alignment enables general-purpose models to surpass domain-specific models

## Why This Works (Mechanism)
KAILIN's effectiveness stems from its systematic alignment of question generation with the structured MeSH knowledge hierarchy. By using MeSH descriptors to guide both retrieval and evaluation, the framework ensures that generated questions and answers reflect clinically relevant knowledge structures. The hierarchical similarity evaluation filters out irrelevant or low-quality pairs, while the fine-tuning process adapts general-purpose models to biomedical reasoning patterns encoded in the hierarchy. This approach bridges the gap between raw scientific literature and structured domain expertise, creating training data that captures both factual knowledge and hierarchical relationships.

## Foundational Learning

**Medical Subject Headings (MeSH)**: A controlled vocabulary thesaurus for indexing biomedical literature and articles for PubMed. Why needed: Provides structured knowledge hierarchy for guiding question generation and evaluation. Quick check: Verify MeSH descriptors are properly mapped to PubMed abstracts.

**PubMed Abstract Processing**: Handling large-scale scientific literature extraction and preprocessing. Why needed: Source data for generating training pairs. Quick check: Confirm abstract retrieval covers relevant biomedical domains.

**Hierarchical Similarity Metrics**: Computing semantic alignment between questions and knowledge hierarchy. Why needed: Ensures generated content aligns with domain expertise structure. Quick check: Validate similarity scores correlate with human judgment.

**Question Generation Fine-tuning**: Adapting language models to generate domain-specific questions. Why needed: Creates high-quality question-answer pairs from scientific text. Quick check: Test question generators on held-out PubMedQA samples.

**Preference Dataset Construction**: Building supervised fine-tuning datasets from generated content. Why needed: Provides training signal for domain adaptation. Quick check: Verify preference dataset quality through human evaluation.

## Architecture Onboarding

**Component Map**: PubMed abstracts -> MeSH query retrieval -> Question generation (fine-tuned) -> Context retrieval -> Evaluation (hierarchical similarity) -> Filtering -> Preference dataset -> Model fine-tuning

**Critical Path**: The core workflow flows from PubMed abstract retrieval through MeSH-guided question generation to hierarchical evaluation and dataset construction. The critical path ensures each stage maintains alignment with the knowledge hierarchy while filtering low-quality content.

**Design Tradeoffs**: The framework prioritizes quality over quantity by using strict hierarchical similarity thresholds, which may limit dataset size but improves relevance. The reliance on MeSH hierarchy provides strong structure but may miss emerging topics not yet incorporated into the ontology.

**Failure Signatures**: Poor performance may result from inadequate MeSH coverage of target domains, low-quality question generation fine-tuning, or overly strict similarity thresholds that filter too many potentially useful pairs. System failures often manifest as low diversity in generated questions or poor alignment with clinical reasoning patterns.

**Three First Experiments**:
1. Test question generators on held-out PubMedQA samples to verify fine-tuning effectiveness
2. Evaluate hierarchical similarity scoring on manually curated question-answer pairs
3. Measure dataset diversity by analyzing MeSH descriptor coverage across generated pairs

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the KAILIN-generated dataset transfer effectively to non-QA biomedical tasks?
- **Basis in paper:** [inferred] The paper evaluates performance exclusively on the PubMedQA benchmark, leaving the utility of the dataset for tasks like named entity recognition or summarization unverified.
- **Why unresolved:** Aligning question generation with knowledge hierarchies likely improves reasoning capabilities, but it is unclear if this transfers to the structural understanding required for extraction or summarization tasks.
- **What evidence would resolve it:** Evaluating models trained on KAILIN data on diverse benchmarks such as BioNLP or clinical summarization datasets (e.g., MIMIC-III).

### Open Question 2
- **Question:** Can the framework be generalized to other domains with less structured ontologies than MeSH?
- **Basis in paper:** [inferred] The methodology is tightly coupled with the Medical Subject Headings (MeSH) hierarchy.
- **Why unresolved:** The framework's success relies on MeSH's specific granularity and coverage; it is unknown if other domain taxonomies (e.g., legal or financial) would provide sufficient signal for the similarity evaluation to function.
- **What evidence would resolve it:** Adapting the framework to a distinct domain using a different ontology (e.g., EuroVoc for law) and measuring downstream task performance.

### Open Question 3
- **Question:** How does the reliance on synthetic answers impact the factual accuracy of the fine-tuned model?
- **Basis in paper:** [inferred] The "Ideal Dataset" construction uses GPT-4o and Llama3-70B to generate answers without explicit ground-truth verification.
- **Why unresolved:** While question selection is optimized via MeSH alignment, the validity of the supervised fine-tuning data depends entirely on the teacher models, risking the propagation of hallucinations.
- **What evidence would resolve it:** A quantitative assessment of factual consistency in the generated answer pairs against curated medical knowledge bases or human evaluation.

## Limitations
- Performance evaluation limited to PubMedQA benchmark without broader biomedical reasoning validation
- Framework dependency on MeSH hierarchy may not generalize to other biomedical knowledge organization systems
- Computational requirements for processing 23 million abstracts may be prohibitive for some research groups

## Confidence
- High: Technical innovation of using MeSH hierarchy for question generation and selection
- Medium: Claim that Llama3-70B outperforms GPT-4 with MedPrompt
- Medium: Assertion that KAILIN enables general-purpose models to surpass domain-specific models

## Next Checks
1. Test framework on additional biomedical reasoning benchmarks beyond PubMedQA to assess generalizability
2. Conduct ablation studies to isolate contribution of MeSH hierarchy guidance versus other components
3. Evaluate model performance on real-world clinical decision support tasks or external biomedical datasets