---
ver: rpa2
title: 'AC/DC: LLM-based Audio Comprehension via Dialogue Continuation'
arxiv_id: '2506.10312'
source_url: https://arxiv.org/abs/2506.10312
tags:
- audio
- training
- proposed
- data
- caption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of instruction-following audio
  comprehension by proposing a novel dialogue continuation training method for large
  language models (LLMs). The key innovation is to train an audio captioning model
  to generate dialogue responses rather than direct captions, where responses are
  generated by the LLM from ground-truth captions without task-specific instructions.
---

# AC/DC: LLM-based Audio Comprehension via Dialogue Continuation

## Quick Facts
- **arXiv ID:** 2506.10312
- **Source URL:** https://arxiv.org/abs/2506.10312
- **Reference count:** 0
- **Primary result:** Up to 47.7% average AQA accuracy on AudioBench benchmarks, significantly outperforming conventional caption-based training while maintaining competitive AAC performance

## Executive Summary
This paper addresses the challenge of instruction-following audio comprehension by proposing a novel dialogue continuation training method for large language models (LLMs). The key innovation is to train an audio captioning model to generate dialogue responses rather than direct captions, where responses are generated by the LLM from ground-truth captions without task-specific instructions. This approach mitigates the caption variation problem and enables the model to follow unseen instructions in a zero-shot manner. Experiments on AudioCaps, WavCaps, and Clotho datasets demonstrate that the proposed method significantly improves audio question-answering accuracy compared to conventional caption-based training, while maintaining competitive audio captioning performance.

## Method Summary
The method trains an audio comprehension model using dialogue continuation rather than direct caption generation. Ground-truth captions are fed to a frozen LLM with no task instruction, producing varied dialogue-like responses. An adapter (2-layer linear projection with 5× downsampling) is trained to predict these responses from audio embeddings while both the audio encoder and LLM remain frozen. This enables zero-shot instruction following without multitask instruction tuning or large-scale data augmentation. The approach trades some captioning accuracy for generalization, as the model learns semantic alignment rather than surface-level caption matching.

## Key Results
- Dialogue continuation training achieves 47.60% AQA accuracy vs. 32.64% for conventional caption training (A1 vs. A2)
- Proposed model maintains competitive AAC performance (46.20-46.82) compared to baselines (51.43-52.40)
- Zero-shot instruction following capability without multitask instruction tuning or explicit instruction tuning
- Consistent performance improvement across AudioBench AQA benchmarks (Clotho-AQA, AudioCaps-QA, WavCaps-QA)

## Why This Works (Mechanism)

### Mechanism 1
Training on LLM-generated dialogue responses instead of direct captions mitigates caption vocabulary overfitting and enables semantic-level audio-text alignment. Ground-truth captions are fed to the backbone LLM with no task instruction, producing varied dialogue-like responses. The adapter learns to map audio embeddings to the semantic content that would trigger such responses, rather than memorizing specific caption wordings. This works because the LLM's response distribution captures meaning beyond surface form.

### Mechanism 2
Using the same frozen LLM for both target generation and inference preserves the LLM's native instruction-following capability, which is inherited by the trained adapter. Since training targets are generated by the same LLM used in the model, and no task-specific prompts are used during training, the adapter learns to produce representations compatible with the LLM's general dialogue behavior. At inference, the frozen LLM can respond to novel instructions.

### Mechanism 3
The proposed method trades captioning accuracy for generalization. By not training on explicit caption generation, the model avoids vocabulary binding to the training caption distribution. This yields a more task-agnostic audio representation at the cost of task-specific captioning performance, enabling better instruction following on unseen tasks.

## Foundational Learning

- **Concept:** Cross-entropy loss for autoregressive language modeling
  - **Why needed here:** The adapter is trained to minimize cross-entropy between predicted response token distributions and LLM-generated targets. Understanding this loss function is essential for debugging training convergence.
  - **Quick check question:** Can you explain why cross-entropy loss is appropriate for sequence generation tasks, and what a rising loss curve during training might indicate?

- **Concept:** Frozen vs. trainable parameters in transfer learning
  - **Why needed here:** The model freezes both the audio encoder and LLM, training only the adapter. This design choice constrains what the model can learn and affects convergence speed.
  - **Quick check question:** If you observe underfitting on a new dataset, which components would you consider unfreezing first, and what risks does this introduce?

- **Concept:** Zero-shot generalization and instruction following
  - **Why needed here:** The core claim is zero-shot instruction following without multitask training. Understanding what enables this (frozen LLM, task-agnostic training) is critical for evaluating whether the approach will generalize to your use case.
  - **Quick check question:** How does training without task-specific prompts differ from conventional instruction tuning, and why might this preserve zero-shot capability?

## Architecture Onboarding

- **Component map:** Audio input -> EAT-base encoder -> 2-layer adapter (5× downsampling) -> LLM embedding dimension -> Frozen Llama-3-8B-Instruct decoder -> Autoregressive response generation

- **Critical path:** Audio input → Encoder → Adapter → Audio embeddings in LLM dimension → Prefix prompt tokens + Audio embeddings + Suffix prompt tokens → Concatenated context C → LLM generates response token by token using autoregressive decoding (beam search at inference)

- **Design tradeoffs:** LoRA vs. frozen LLM shows inconsistent benefits (A3 vs. A4 improves AQA, but B3 vs. B4 shows no clear pattern). Temperature for response generation uses 0.7 for diversity; lower temperature reduces variation, higher may produce noisy targets. Data scaling with WavCaps and Clotho showed minimal improvement, possibly due to limited learnable parameters.

- **Failure signatures:** Caption-style responses to questions indicate adapter has overfit to caption patterns. Low AQA accuracy with high AAC accuracy suggests learned surface-level caption generation but not semantic alignment. Loss plateau without convergence may indicate adapter capacity is insufficient or audio encoder features are unsuitable.

- **First 3 experiments:**
  1. Reproduce baseline vs. proposed comparison (A1 vs. A2): Train on AudioCaps with caption targets vs. dialogue continuation targets. Measure AQA accuracy on AudioBench subsets to confirm core claim.
  2. Ablate response diversity: Generate training targets with temperature ∈ {0.3, 0.7, 1.0} and measure impact on AQA accuracy and response variety. Hypothesis: lower temperature may reduce generalization.
  3. Test instruction generalization: Evaluate on held-out instruction types not in AudioBench (e.g., temporal reasoning, counting events) to probe zero-shot capability boundaries. Document failure modes for future fine-tuning decisions.

## Open Questions the Paper Calls Out

### Open Question 1
Does combining dialogue continuation training with explicit instruction-tuning enhance task-specific performance while preserving zero-shot capability? The current method achieves zero-shot capability without multitask instruction tuning, but it remains untested whether adding explicit instruction-tuning on top would yield additive benefits or interfere with learned generalization.

### Open Question 2
Does jointly finetuning the audio encoder with the adapter improve performance, particularly when scaling to larger training datasets? The authors note that adding WavCaps and Clotho data showed no significant improvement, which "could be due to the relatively few learnable parameters in our models. Finetuning the audio encoder jointly with the adapter may improve performance."

### Open Question 3
Can task-specific finetuning applied after dialogue continuation training recover competitive audio captioning performance while maintaining zero-shot instruction-following? The proposed model underperforms baselines on AAC metrics (43-46 vs. 51-52), suggesting a trade-off between generalization and task-specific performance that might be bridgeable through staged training.

## Limitations

- The approach trades captioning accuracy for generalization, potentially unsuitable for applications requiring strong captioning performance without fine-tuning
- Evaluation relies on a single frozen LLM (Llama-3-8B-Instruct) as both target generator and inference backbone, raising questions about generalization to other architectures
- Impact of data augmentation beyond training target generation is unclear, as experiments showed minimal improvement from adding WavCaps and Clotho datasets
- Response generation uses only one sample per caption, which may limit training target diversity compared to multi-sample approaches

## Confidence

- **High confidence:** Dialogue continuation training improving AQA accuracy compared to conventional caption-based training (A1 vs. A2 results, 32.64% to 47.60% improvement)
- **Medium confidence:** Zero-shot instruction-following capability without multitask instruction tuning, as this depends on the assumption that the frozen LLM's instruction-following ability remains intact
- **Medium confidence:** The approach mitigates caption vocabulary overfitting and enables semantic-level alignment, as evidence shows improved generalization but doesn't directly measure vocabulary bias
- **Low confidence:** The claim that the proposed method is built for "general audio comprehension" without vocabulary limitations, as the trade-off with AAC performance suggests task-specific constraints remain

## Next Checks

1. **Instruction Generalization Test:** Evaluate the trained model on held-out instruction types not present in AudioBench (e.g., temporal reasoning, counting events, emotion detection) to probe the boundaries of zero-shot capability. Document specific failure modes and response patterns when instructions fall outside the training distribution.

2. **Architecture Transferability Study:** Replicate the dialogue continuation training using different LLM backbones (e.g., Qwen2.5-7B-Instruct, Mistral-7B-Instruct) to test whether the approach generalizes beyond the specific Llama-3-8B-Instruct model used in the paper. Compare AQA accuracy across architectures to assess dependency on the particular LLM choice.

3. **Response Diversity Ablation:** Systematically vary the temperature parameter (0.3, 0.7, 1.0) used for generating training targets and measure impact on both AQA accuracy and response variety. Additionally, test whether generating multiple responses per caption (rather than one sample) improves generalization, controlling for total training data volume.