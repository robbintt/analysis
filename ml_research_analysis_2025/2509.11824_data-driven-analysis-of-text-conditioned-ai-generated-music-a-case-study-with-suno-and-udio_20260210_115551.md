---
ver: rpa2
title: 'Data-Driven Analysis of Text-Conditioned AI-Generated Music: A Case Study
  with Suno and Udio'
arxiv_id: '2509.11824'
source_url: https://arxiv.org/abs/2509.11824
tags:
- music
- lyrics
- tags
- udio
- clusters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents the first large-scale analysis of AI music
  generation platform usage, analyzing over 100,000 songs from Suno and Udio collected
  between May and October 2024. The research develops a novel methodology combining
  state-of-the-art text embedding models (NV-Embed v2), dimensionality reduction (UMAP),
  and clustering (HDBSCAN) to examine prompts, lyrics, and tags.
---

# Data-Driven Analysis of Text-Conditioned AI-Generated Music: A Case Study with Suno and Udio

## Quick Facts
- arXiv ID: 2509.11824
- Source URL: https://arxiv.org/abs/2509.11824
- Reference count: 31
- Key outcome: First large-scale analysis of AI music generation platform usage, analyzing over 100,000 songs from Suno and Udio, revealing prominent themes, language preferences, prompting strategies, and unique metatag usage.

## Executive Summary
This study presents the first large-scale analysis of user-generated music on AI platforms Suno and Udio, examining over 101,000 songs collected between May and October 2024. The research develops a novel methodology combining state-of-the-art text embedding models (NV-Embed v2), dimensionality reduction (UMAP), and clustering (HDBSCAN) to examine prompts, lyrics, and tags. Results reveal prominent themes in lyrics including love, religion, and celebration; language preferences with English dominating but significant non-English content; prompting strategies ranging from list-based to narrative approaches; and unique metatag usage for steering music generation. The study identifies 26 thematic categories in lyrics, analyzes 1,193 most common tags, and detects real artist name substitutions in Udio's metadata. Interactive visualizations enable exploration of the processed data. The work provides crucial insights into how AI music platforms are being used and establishes a foundation for AI music studies, while sharing code and resources to support future research.

## Method Summary
The methodology combines automated data collection with advanced NLP techniques to analyze AI-generated music. Researchers scraped metadata from 101,953 songs (81,434 Suno, 20,519 Udio) via HTTP requests, then applied FastText for English language filtering and regex for structural marker removal. NV-Embed v2 generated 4096-dimensional text embeddings, which were reduced to 5 dimensions using UMAP (with parameters n_neighbors=10-15, min_dist=0.15) before clustering with HDBSCAN (min_cluster_size=20-50, epsilon=0.25-0.32). Clusters were labeled using top TF-IDF words per cluster, followed by manual review. The analysis examined prompts, lyrics, and tags to identify themes, languages, and prompting strategies, with results visualized through interactive tools.

## Key Results
- Identified 26 thematic categories in lyrics, with prominent themes including love, religion, and celebration
- Analyzed 1,193 most common tags and detected real artist name substitutions in Udio's metadata
- Revealed prompting strategies ranging from list-based to narrative approaches, with English dominating but significant non-English content

## Why This Works (Mechanism)
The study leverages modern NLP techniques to extract meaningful patterns from large-scale music generation data. By combining state-of-the-art text embeddings (NV-Embed v2) with dimensionality reduction and clustering algorithms, the researchers can identify semantic structures in prompts, lyrics, and tags that would be impossible to detect through manual analysis alone. The methodology effectively bridges the gap between raw user input and interpretable thematic categories, enabling systematic analysis of creative behavior at scale.

## Foundational Learning
- **Text Embedding Models**: Convert text to high-dimensional vectors that capture semantic meaning, enabling mathematical comparison of different prompts and lyrics
  - *Why needed*: To transform qualitative text data into quantitative representations suitable for clustering and analysis
  - *Quick check*: Verify embeddings capture semantic similarity by comparing cosine distances between semantically related texts

- **Dimensionality Reduction (UMAP)**: Compresses high-dimensional embeddings to lower dimensions while preserving local and global structure
  - *Why needed*: To make clustering computationally feasible and visualize complex semantic relationships
  - *Quick check*: Ensure UMAP preserves local neighborhoods by examining nearest neighbor relationships before and after reduction

- **Density-Based Clustering (HDBSCAN)**: Groups data points based on density, handling varying cluster sizes and shapes while identifying outliers
  - *Why needed*: To discover natural groupings in the reduced-dimensional space without requiring pre-specified cluster numbers
  - *Quick check*: Validate cluster quality by examining silhouette scores and ensuring meaningful semantic coherence within clusters

## Architecture Onboarding
- **Component Map**: Data Scraping -> Preprocessing (Language Filter + Regex) -> Embedding (NV-Embed v2) -> Dimensionality Reduction (UMAP) -> Clustering (HDBSCAN) -> Labeling (TF-IDF + Manual Review) -> Visualization
- **Critical Path**: The embedding and clustering pipeline forms the critical path, as these steps determine the thematic structure and quality of all subsequent analysis
- **Design Tradeoffs**: 
  - Batch processing vs real-time analysis: Chosen batch processing for comprehensive dataset analysis
  - Model complexity vs interpretability: Used relatively interpretable methods (UMAP + HDBSCAN) over black-box alternatives
  - Manual vs automatic labeling: Combined automatic TF-IDF extraction with manual refinement to balance efficiency and accuracy
- **Failure Signatures**: 
  - Excessive outliers (clusters too sparse or disconnected)
  - Mixed-language contamination in clusters
  - Semantic incoherence within clusters
- **First Experiments**:
  1. Run a small-scale test (1,000 songs) through the complete pipeline to verify data access and processing
  2. Experiment with UMAP parameters to find optimal dimensionality reduction settings
  3. Test different HDBSCAN parameters to balance cluster granularity and coherence

## Open Questions the Paper Calls Out
None

## Limitations
- Methodology relies heavily on scraped metadata and automatic clustering, which may miss nuanced creative intent or contextual metadata changes over time
- Manual labeling process for the 26 thematic categories introduces subjectivity that cannot be fully replicated
- Platform API changes could affect reproducibility of the scraping component

## Confidence
- **High confidence**: Quantitative findings on tag usage (1,193 common tags identified), language distribution (English dominance with significant non-English content), and basic prompting strategies (list-based vs narrative approaches)
- **Medium confidence**: Thematic categorization into 26 lyric categories is methodologically sound but relies on manual refinement that introduces interpretive variability
- **Low confidence**: Claims about creative intent behind metatag usage and the relative novelty of "lyrics-driven" prompting strategies are inferred rather than directly measured

## Next Checks
1. **Reproduce clustering**: Apply the exact methodology (NV-Embed v2 → UMAP 5D → HDBSCAN) to a fresh sample of 10,000 songs to verify stability of thematic clusters and language distributions
2. **Validate manual labeling**: Have independent annotators apply the 26 thematic categories to a stratified sample of 500 lyrics to measure inter-annotator agreement and refine the taxonomy
3. **Test tag robustness**: Analyze whether the most common tags (1,193 identified) remain consistent across different time windows or platform updates, and whether metatag correlations with musical output features can be empirically verified