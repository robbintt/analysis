---
ver: rpa2
title: LLM-based Embedders for Prior Case Retrieval
arxiv_id: '2507.18455'
source_url: https://arxiv.org/abs/2507.18455
tags:
- retrieval
- legal
- case
- information
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates the effectiveness of LLM-based embedders
  for prior case retrieval (PCR) across four benchmark datasets spanning multiple
  jurisdictions and languages. The study addresses two key challenges in PCR: lengthy
  legal text (exceeding 512 tokens) and limited training data due to privacy concerns.'
---

# LLM-based Embedders for Prior Case Retrieval

## Quick Facts
- arXiv ID: 2507.18455
- Source URL: https://arxiv.org/abs/2507.18455
- Authors: Damith Premasiri; Tharindu Ranasinghe; Ruslan Mitkov
- Reference count: 26
- Key outcome: LLM-based embedders outperform BM25 and supervised transformer methods on PCR tasks across four datasets without requiring fine-tuning

## Executive Summary
This paper evaluates the effectiveness of LLM-based embedders for prior case retrieval (PCR) across four benchmark datasets spanning multiple jurisdictions and languages. The study addresses two key challenges in PCR: lengthy legal text (exceeding 512 tokens) and limited training data due to privacy concerns. By leveraging LLM-based embedders that support long inputs and require no training, the authors demonstrate that these models outperform both BM25 and supervised transformer-based methods in PCR tasks. Specifically, SFR-Embedding-2 R achieved the best performance across three datasets with MAP scores ranging from 0.32 to 0.47, significantly surpassing BM25 baselines. The results show that LLM-based embedders are highly effective for PCR without requiring fine-tuning, though the MTEB leaderboard rankings do not generalize to PCR tasks.

## Method Summary
The method uses LLM-based embedders (SFR-Embedding-2_R, bge-en-icl, stella_en_1.5B_v5) in an unsupervised manner to encode query cases and candidate court cases as dense vectors. Precomputed embeddings for all documents are stored, then cosine similarity between query and candidate embeddings produces relevance rankings. The approach leverages the ability of LLM-based embedders to process up to 32,000 tokens, eliminating truncation of lengthy legal documents. Evaluation uses Mean Average Precision (MAP) as the primary metric across four datasets: IL-PCR (English, India), COLIEE-2022 (English, Canada), MUSER (Chinese, China), and IRLeD (English, India). LEGAL-BERT serves as a supervised baseline trained with positive samples and 5 negatives per query.

## Key Results
- SFR-Embedding-2 R achieved the best MAP scores across three datasets: 0.47 on IL-PCR, 0.38 on COLIEE-2022, and 0.33 on IRLeD
- LLM-based embedders significantly outperformed BM25 baselines (MAP 0.16 on IL-PCR vs. 0.47 for SFR-Embedding-2_R)
- bge-en-icl, the top model on MTEB benchmark, underperformed on PCR tasks, demonstrating benchmark generalizability limitations
- LLM-based embedders achieved strong results without requiring fine-tuning or training data

## Why This Works (Mechanism)

### Mechanism 1: Extended Context Processing
LLM-based embedders capture more legal context by processing longer documents without truncation. They accept up to 32,000 tokens versus 512 for BERT-based models, enabling full case documents to be encoded as single vectors. This preserves legal context that would otherwise be lost through truncation or segmentation.

### Mechanism 2: Unsupervised Domain Generalization
Pre-trained LLM embedders generate task-agnostic sentence/document representations without fine-tuning. Cosine similarity between query and candidate embeddings directly produces relevance rankings, eliminating the need for domain-specific training data.

### Mechanism 3: Benchmark Domain Gap
General-purpose embedding benchmarks (MTEB) do not reliably predict PCR performance because legal relevance involves specialized reasoning patterns not captured in general-purpose retrieval benchmarks.

## Foundational Learning

- **Mean Average Precision (MAP)**: Primary evaluation metric for PCR; measures ranking quality across all relevant documents averaged over queries. Quick check: For a query with relevant documents at positions 1, 3, 5, 10, 20 out of 100 candidates, calculate the average precision for this single query.

- **Cosine similarity for dense retrieval**: The paper uses cosine similarity between normalized embeddings as the sole relevance scoring function. Quick check: Given two normalized embedding vectors, what is the range of cosine similarity values, and what does a score of 0.85 imply?

- **Context length limits in transformer models**: Understanding why BERT's 512-token limit forces truncation and how LLM-based embedders (32K tokens) address this. Quick check: A legal case has 7,000 words (~9,000 tokens). What information loss occurs if truncated to 512 tokens?

## Architecture Onboarding

- **Component map**: 
  - Embedding models (SFR-Embedding-2_R, bge-en-icl, stella_en_1.5B_v5) 
  -> Precomputed embedding store (one vector per query/candidate case) 
  -> Similarity computation (pairwise cosine similarity between query and all candidates) 
  -> Ranking (sort candidates by descending similarity scores) 
  -> Evaluation (MAP, Precision@k, Recall@k, F-score@k)

- **Critical path**: 
  1. Load dataset (query cases + candidate corpus)
  2. Encode all documents through chosen LLM embedder
  3. Compute cosine similarity matrix (queries × candidates)
  4. Rank candidates per query and compute MAP

- **Design tradeoffs**: 
  - Model selection: SFR-Embedding-2_R performed best on 3/4 datasets, but MTEB rankings do not transfer to PCR—empirical testing required per domain
  - Unsupervised vs. supervised: No training cost, but may underperform on highly specialized jurisdictions absent from pre-training data
  - Single similarity metric: Paper uses only cosine similarity; dot product or Euclidean distance alternatives not explored

- **Failure signatures**: 
  - MTEB top-ranked model underperforms on PCR (observed with bge-en-icl)
  - MAP scores plateau despite larger models (suggests pre-training domain mismatch)
  - High variance across jurisdictions (0.47 MAP IL-PCR vs. 0.12 MAP MUSER)

- **First 3 experiments**: 
  1. Replicate baseline comparison: Run BM25, LEGAL-BERT (supervised), and SFR-Embedding-2_R on IL-PCR to verify MAP gaps reported (0.16 → 0.47)
  2. Cross-jurisdiction probe: Evaluate whether the best model on English datasets (IL-PCR, COLIEE) transfers to Chinese (MUSER); check if multilingual embedders improve results
  3. Ablate context length: Truncate cases to 512, 1024, 2048, 4096 tokens and measure MAP degradation to quantify the value of extended context

## Open Questions the Paper Calls Out

- **Open Question 1**: Can supervised fine-tuning of LLM embedders on PCR-specific datasets yield better performance than the unsupervised zero-shot approach? The authors conclude that "LLM-based embedders should be trained in PCR tasks so that they will provide better results than the unsupervised approach," but this remains untested.

- **Open Question 2**: How would the inclusion of PCR tasks in the MTEB benchmark alter the generalizability of leaderboard rankings? The authors state "the IR community needs to incorporate PCR datasets widely into IR benchmarks" because current MTEB rankings do not reflect PCR performance.

- **Open Question 3**: Do larger LLM-based embedders with higher-dimensional representations provide significant performance gains over the models tested? The authors note they "did not conduct experiments on these models due to hardware resource limitations."

## Limitations

- **Dataset accessibility**: The paper relies on four specialized legal datasets (IL-PCR, COLIEE-2022, MUSER, IRLeD) that may have restricted access, varying availability, or require registration, creating significant barriers for independent validation and reproduction of results.

- **Missing implementation details**: Critical implementation specifics are absent, including BM25 parameter configuration (k1, b values), exact preprocessing steps for legal text, and hardware configurations beyond the GPU memory limitation mentioned for NV-EMBED.

- **Benchmark generalizability**: While demonstrating strong performance on PCR tasks, the authors acknowledge that MTEB leaderboard rankings do not transfer to PCR tasks, raising questions about how well these results generalize to other legal IR tasks.

## Confidence

- **High Confidence**: The core finding that LLM-based embedders significantly outperform BM25 baselines on PCR tasks across multiple datasets is supported by direct empirical evidence.

- **Medium Confidence**: The claim that LLM-based embedders eliminate the need for domain-specific training data, while supported by unsupervised performance, assumes pre-training corpora sufficiently cover legal domain concepts.

- **Low Confidence**: The generalizability of these results to other legal IR tasks beyond PCR, and whether the MTEB-PCR performance gap represents a fundamental limitation of general-purpose embeddings or specific characteristics of the PCR task.

## Next Checks

1. **Cross-task transferability**: Evaluate the best-performing LLM embedders (SFR-Embedding-2_R) on other legal IR tasks such as statute retrieval or legal question answering to assess whether PCR-specific advantages transfer to broader legal information retrieval contexts.

2. **Pre-training domain analysis**: Analyze the pre-training corpora of the top-performing LLM embedders to quantify legal domain coverage, particularly for jurisdiction-specific concepts, and correlate this with PCR performance to validate the unsupervised approach's assumptions.

3. **Segmentation vs. truncation comparison**: Systematically compare MAP scores when processing long legal documents through truncation vs. rhetorical role segmentation approaches, measuring the trade-off between context preservation and computational efficiency.