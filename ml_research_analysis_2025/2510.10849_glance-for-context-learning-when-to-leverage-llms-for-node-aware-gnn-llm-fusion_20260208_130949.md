---
ver: rpa2
title: 'Glance for Context: Learning When to Leverage LLMs for Node-Aware GNN-LLM
  Fusion'
arxiv_id: '2510.10849'
source_url: https://arxiv.org/abs/2510.10849
tags:
- nodes
- glance
- homophily
- routing
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "GLANCE introduces a selective, node-aware fusion framework that\
  \ invokes an LLM only when a lightweight router predicts it will improve performance.\
  \ By analyzing structural properties like local homophily and degree, the study\
  \ reveals that LLMs excel where GNNs falter\u2014particularly on heterophilous,\
  \ low-degree nodes."
---

# Glance for Context: Learning When to Leverage LLMs for Node-Aware GNN-LLM Fusion

## Quick Facts
- arXiv ID: 2510.10849
- Source URL: https://arxiv.org/abs/2510.10849
- Reference count: 40
- Primary result: Selective LLM routing based on structural features improves accuracy by up to 13% on heterophilous nodes while reducing queries by 90%

## Executive Summary
GLANCE introduces a selective fusion framework that invokes an LLM only when a lightweight router predicts it will improve performance. The router uses node-level features like GNN embeddings, estimated homophily, and uncertainty to identify cases where GNNs struggle—particularly on heterophilous, low-degree nodes. Across multiple datasets, GLANCE achieves significant accuracy gains while using far fewer LLM queries than uniform fusion approaches.

## Method Summary
GLANCE trains a frozen GNN backbone, then learns a lightweight router that selects nodes for LLM refinement based on structural features. The router estimates local homophily and uses policy gradient optimization with an advantage-based reward. For routed nodes, the method constructs multi-hop context prompts and fuses GNN and LLM embeddings through a refiner MLP. The framework uses top-K selection per batch with decaying K and a cost penalty β to balance accuracy and efficiency.

## Key Results
- Up to 13% accuracy gains on heterophilous node subgroups (0.00-0.25 homophily bins)
- 0.9% overall accuracy improvement across Cora, Pubmed, and Arxiv23
- 90% reduction in LLM queries compared to uniform fusion approaches
- Scalable to large graphs with over 2 million nodes (OGB-Products, Arxiv-Year)

## Why This Works (Mechanism)

### Mechanism 1: Structural Complementarity for Routing Prior
GNNs aggregate neighbor features assuming label smoothness, but this creates noise on heterophilous nodes where neighbors have different labels. LLMs process text independently of graph topology, avoiding this structural bias. GLANCE estimates local homophily to preferentially route structurally difficult nodes to the LLM.

### Mechanism 2: Advantage-Based Policy Optimization
The router uses REINFORCE-style policy gradient with counterfactual rewards. It computes the difference between GNN-only and LLM-refined losses minus a cost penalty, training the router to recognize nodes where the LLM provides net positive advantage without requiring backpropagation through the LLM.

### Mechanism 3: Multi-Scale Context Refinement
Instead of single long prompts, GLANCE generates separate embeddings for ego node, 1-hop, and 2-hop neighbors, then fuses them with GNN embeddings through a refiner MLP. This manages prompt length while capturing multi-scale context.

## Foundational Learning

- **Local Homophily (h_v)**: Measures fraction of neighbors sharing a node's label. Low h_v indicates heterophily where GNNs struggle. Quick check: If a node has degree 10 and 9 neighbors share its label, is it heterophilous or homophilous? (Answer: Homophilous)

- **Policy Gradient / REINFORCE**: Optimizes expected reward of discrete actions rather than continuous loss. Quick check: Why can't we backpropagate classification loss through routing decisions? (Answer: Decision is discrete/non-differentiable)

- **LLM Context Serialization**: Serializing graph neighborhoods into text for LLM processing. Quick check: Why generate three separate embeddings rather than one big prompt? (Answer: To manage prompt length and avoid "Lost-in-the-Middle" degradation)

## Architecture Onboarding

- **Component map**: Frozen GNN -> Homophily Estimator -> Router -> LLM Encoder -> Refiner MLP
- **Critical path**: Compute z_G → Compute routing features → Router selects Top-K → Construct prompts and run LLM for Top-K only → Fuse z_G and z_L in Refiner
- **Design tradeoffs**: Router overhead vs. LLM savings, β cost penalty balancing queries vs. accuracy, prompt depth vs. context window limits
- **Failure signatures**: Uniform routing (router outputs constant scores), performance collapse on homophilous nodes (router over-relies on LLM), OOM from high-degree nodes without sampling
- **First 3 experiments**: 1) Stratified baseline verification showing LLM > GNN on heterophilous nodes, 2) Routing feature ablation comparing random, uncertainty-only, and homophily-only routers, 3) Cost-accuracy curve varying K budget from 5% to 20% of nodes

## Open Questions the Paper Calls Out
- To what extent can advanced prompt engineering improve GLANCE's handling of heterophilous nodes compared to simple serialization?
- How robust is the router to noise in estimated local homophily when true labels are unavailable?
- Would dynamic threshold-based routing outperform the fixed top-k budget for efficiency and accuracy?

## Limitations
- Router architecture underspecification requires assumptions about MLP layer counts and dimensions
- Policy optimization stability concerns with sparse rewards and non-differentiable LLM queries
- Context serialization assumptions unverified for optimal sampling strategies beyond fixed neighbor limits

## Confidence
- **High**: LLMs outperform GNNs on heterophilous, low-degree nodes (well-supported stratified analysis)
- **Medium**: Advantage-based routing achieves efficiency gains while maintaining accuracy (dataset-sensitive β)
- **Low**: Scalability claims rely on compressed prompts without validating semantic signal preservation

## Next Checks
1. Router ablation with synthetic data varying homophily levels to confirm homophily feature's role
2. Cost-accuracy Pareto frontier analysis varying K and β across all datasets
3. Context window stress test evaluating performance with >2 hop neighbor sampling