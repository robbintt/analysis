---
ver: rpa2
title: 'AutoTailor: Automatic and Efficient Adaptive Model Deployment for Diverse
  Edge Devices'
arxiv_id: '2511.22355'
source_url: https://arxiv.org/abs/2511.22355
tags:
- supernet
- accuracy
- latency
- deployment
- autotailor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoTailor introduces the first fully automated framework for end-to-end
  adaptive model deployment on edge devices using SuperNets. It eliminates the need
  for manual DNN-to-SuperNet transformation and expensive hardware-aware profiling
  by leveraging graph structural consistency and cross-SubNet architectural similarity.
---

# AutoTailor: Automatic and Efficient Adaptive Model Deployment for Diverse Edge Devices

## Quick Facts
- **arXiv ID:** 2511.22355
- **Source URL:** https://arxiv.org/abs/2511.22355
- **Reference count:** 40
- **Primary result:** Fully automated framework for adaptive model deployment using SuperNets, eliminating manual DNN-to-SuperNet transformation and expensive hardware-aware profiling.

## Executive Summary
AutoTailor introduces the first fully automated framework for end-to-end adaptive model deployment on edge devices using SuperNets. It eliminates the need for manual DNN-to-SuperNet transformation and expensive hardware-aware profiling by leveraging graph structural consistency and cross-SubNet architectural similarity. The framework automatically compiles user-provided DNNs into SuperNets via computation graph-guided compilation and builds learning-free latency and accuracy predictors through enumeration of unique operators and modification sensitivity analysis. Evaluations across diverse models and edge devices show significant reductions in development effort and profiling costs while achieving competitive accuracy and latency performance.

## Method Summary
AutoTailor transforms static DNNs into adaptive SuperNets through a computation graph-guided compilation process. The framework uses TailorIR, an abstraction that decouples architecture representation from modifications, enabling automatic identification of transformable modules. For performance prediction, AutoTailor employs learning-free latency prediction via unique operator enumeration with space pruning and modification sensitivity-based accuracy prediction. The architecture optimizer uses genetic algorithms with beam search initialization to find optimal SubNets under hardware constraints. The entire pipeline is designed to minimize manual intervention while maintaining prediction accuracy and deployment efficiency.

## Key Results
- Reduces development effort by 11–27× compared to state-of-the-art methods
- Cuts profiling costs by at least 11× through learning-free predictors
- Achieves up to 15.60% absolute accuracy improvement and 60.03% latency reduction
- Demonstrates 2.2–11.6× speedup in accuracy predictor building vs. MLP-based methods
- Shows 78–98% top-5 ranking accuracy for modification sensitivity prediction

## Why This Works (Mechanism)

### Mechanism 1: Computation Graph-Guided Compilation via TailorIR
AutoTailor achieves automatic DNN-to-SuperNet transformation by exploiting graph structural consistency. The TailorIR abstraction decouples architecture representation from modifications through a three-step bottom-up parsing: operator parsing → block parsing (matching subgraphs to templated blocks) → stage division. Each TailorIR module maintains a Feature (meta and active architecture), a transform function (updates after modifications), an update function (ensures consistency), and a build function (generates executable modules). This enables automatic identification of transformable modules without manual annotation, assuming the SuperNet's largest SubNet preserves the original DNN's computation graph structure.

### Mechanism 2: Learning-Free Latency Prediction via Unique Operator Enumeration with Space Pruning
AutoTailor exploits cross-SubNet architectural similarity to build accurate latency predictors without training neural networks. Analysis shows 96–98% operator redundancy across 1,000 sampled SubNets, with unique operators converging quickly. The framework enumerates all unique operators, builds device-specific lookup tables (LUTs), and sums operator latencies for model-level prediction. Space pruning exploits modification independence (e.g., depth modifications don't change operator features) to avoid redundant enumeration. TailorIR's shape inference eliminates the need for actual inference to extract tensor shapes, while fusion-aware profiling captures kernel fusion effects.

### Mechanism 3: Modification Sensitivity-Based Accuracy Prediction
AutoTailor predicts relative SubNet accuracy ranking efficiently by decomposing accuracy into modification-level sensitivity contributions. Instead of profiling full SubNets, the framework defines modification sensitivity as the accuracy drop from applying a single modification: Δm = Acc(SuperNet) − Acc(SubNet({m})). SubNet accuracy is approximated as Acc(SubNet) ≈ Acc(SuperNet) − Σ Δm for all modifications in the SubNet. This transforms the prediction problem from model-level to modification-level, drastically reducing profiling cost while maintaining relative ranking accuracy.

## Foundational Learning

- **SuperNets and Weight Sharing**
  - Why needed here: AutoTailor's entire premise is that SuperNets enable efficient generation of 10¹³–10²² model variants through weight sharing, avoiding the linear training cost scaling of block-scaling methods.
  - Quick check question: Can you explain why training a SuperNet with 10¹³ SubNets is more efficient than training 10¹³ independent models?

- **Neural Architecture Search (NAS) and Hardware-Aware Optimization**
  - Why needed here: AutoTailor positions itself against existing NAS-like methods (LegoDNN, AdaptiveNet, OFA) and must optimize for both accuracy and device-specific latency under constraints.
  - Quick check question: Why does FLOPs-guided optimization not guarantee latency optimization on heterogeneous edge devices?

- **Operator Fusion and Kernel-Level Execution**
  - Why needed here: The latency predictor must account for operator fusion (multiple operators merged into one kernel), which breaks the additivity assumption. AutoTailor addresses this with fusion-aware profiling.
  - Quick check question: If two convolutions are fused, would summing their individual latencies over-estimate or under-estimate the actual execution time?

## Architecture Onboarding

- **Component map:** Input DNN -> TailorIR Parser -> SuperNet Tuner -> LUT Constructor + Sensitivity Analyzer -> Architecture Optimizer -> Deploy via PNNX to NCNN

- **Critical path:** Input DNN → TailorIR Parser → SuperNet Tuner (GPU server, 600 iterations) → LUT Constructor (device profiling, ~20–40 min) + Sensitivity Analyzer (GPU profiling, ~1 hour) → Architecture Optimizer (edge/CPU server) → Deploy via PNNX to NCNN

- **Design tradeoffs:**
  - **TailorIR library coverage vs. automation:** Broader module library → more models supported automatically; narrow library → manual extension required.
  - **LUT profiling time vs. prediction accuracy:** More profiling samples improve LUT coverage; AutoTailor's pruning minimizes this but may miss edge-case operators.
  - **Sensitivity approximation vs. accuracy:** Additive assumption enables cheap prediction but may misrank SubNets with strong modification interactions.
  - **SuperNet training investment vs. SubNet quality:** AutoTailor uses CompOFA (600 iterations); OFA's progressive training (3× more) or AlphaNet's self-distillation could improve accuracy at higher cost.

- **Failure signatures:**
  - **TailorIR parse failure:** Operator or block pattern not recognized → falls back to default TIR blocks; may miss optimization opportunities.
  - **LUT miss:** SubNet contains operator configuration not in LUT → latency prediction fails or requires fallback estimation.
  - **Sensitivity ranking inversion:** Non-linear modification interactions cause incorrect ranking → selected SubNet underperforms predictor expectation.
  - **Fusion mismatch:** Profiling-time fusion differs from deployment-time fusion (different backend/compiler) → latency prediction systematically biased.

- **First 3 experiments:**
  1. **Validate TailorIR parsing:** Run AutoTailor on ResNet-50 and MobileNetV3; verify that generated TailorIR SuperNet supports expected modifications (width, depth, input resolution) and that build() produces executable PyTorch modules matching original accuracy.
  2. **Profile LUT accuracy:** On Samsung A54 (Exynos 1380), build latency LUT with space pruning; measure prediction error on 1,000 random SubNets. Compare against nn-Meter/LitePred with equivalent profiling budget. Verify fusion-aware profiling reduces error.
  3. **End-to-end deployment validation:** For a fixed latency constraint (e.g., 15ms on phone big cores), run architecture optimization with AutoTailor's predictors; deploy selected SubNet via NCNN; measure actual accuracy and latency. Compare against AdaptiveNet and LegoDNN baselines on same device/constraint.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the AutoTailor framework be effectively extended to support complex tasks beyond image classification, such as object detection, natural language processing, or speech recognition? The paper explicitly states that extending to these tasks is an important direction for future work, noting that current TailorIR abstraction and fine-tuning pipeline are optimized for classification architectures.

- **Open Question 2:** Does the linear accumulation assumption in the modification-level accuracy predictor introduce significant errors in architectures with high inter-layer dependency? While the paper validates predictor effectiveness for selecting top candidates, it does not analyze scenarios where simultaneous depth and width modifications interact non-linearly to degrade accuracy more than the sum of their parts.

- **Open Question 3:** How does the reliance on templated TailorIR blocks affect the automation capability when encountering novel or highly irregular DNN architectures? The paper evaluates standard models but does not test robustness against unconventional architectures or custom user-defined layers that lack pre-defined TailorIR templates.

- **Open Question 4:** To what extent can advanced SuperNet training strategies (e.g., progressive shrinking or self-distillation) be integrated into AutoTailor to close the performance gap with state-of-the-art manual methods? The paper notes that more advanced fine-tuning could further boost performance but uses CompOFA's strategy for consistency.

## Limitations
- Parser automation depends on coverage of templated TailorIR blocks; novel architectures may require manual extension
- Latency prediction accuracy relies on operator fusion handling and may degrade across different deployment backends
- Additive sensitivity assumption may fail for SubNets with strong modification interactions, causing ranking errors
- Cross-device LUT transferability requires re-profiling for each new device, limiting generalizability

## Confidence
- **High:** SuperNet's weight-sharing efficiency advantage over block-scaling; fusion-aware profiling improves latency prediction; AutoTailor reduces development effort by 11–27×.
- **Medium:** Automatic SuperNet compilation works for supported architectures; learning-free LUT predictor achieves acceptable accuracy; modification sensitivity ranking works under tested conditions.
- **Low:** Cross-device LUT transferability without re-profiling; additive sensitivity assumption holds for all modification combinations; parser handles all DNN architectures without manual extension.

## Next Checks
1. **Cross-Device Latency Transferability:** Build LUT on Samsung A54, evaluate prediction error on Jetson Orin and Pixel 7 Pro. Quantify performance degradation without re-profiling.
2. **Modification Interaction Stress Test:** Systematically test accuracy prediction on SubNets with coupled depth-width modifications. Compare predicted vs. actual accuracy to identify ranking failures.
3. **Parser Robustness Evaluation:** Apply AutoTailor to diverse architectures (Transformer-based, asymmetric convolutions, custom blocks). Measure parse success rate and accuracy retention compared to manual SuperNet construction.