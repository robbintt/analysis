---
ver: rpa2
title: Subgoal Graph-Augmented Planning for LLM-Guided Open-World Reinforcement Learning
arxiv_id: '2511.20993'
source_url: https://arxiv.org/abs/2511.20993
tags:
- subgoal
- planning
- wood
- plan
- stone
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SGA-ACR, a framework that integrates an environment-specific
  subgoal graph and structured entity knowledge with a multi-LLM planning pipeline
  to improve LLM-guided open-world reinforcement learning. The method addresses the
  misalignment between LLM-generated plans and environment constraints through explicit
  separation of generation, critique, and refinement phases, and employs a subgoal
  tracker for bidirectional feedback.
---

# Subgoal Graph-Augmented Planning for LLM-Guided Open-World Reinforcement Learning

## Quick Facts
- arXiv ID: 2511.20993
- Source URL: https://arxiv.org/abs/2511.20993
- Reference count: 40
- Outperforms AdaRefiner by 1.6% score and 0.8 reward at 5M steps on Crafter benchmark

## Executive Summary
This paper addresses the misalignment between LLM-generated plans and environment constraints in open-world reinforcement learning by integrating environment-specific subgoal graphs with a structured multi-LLM planning pipeline. The framework extracts dependency-aware subgoal graphs and entity knowledge from environment documentation, then uses role-separated Actor-Critic-Refiner LLMs to generate, evaluate, and refine plans while maintaining alignment with executable subgoals. A bidirectional feedback mechanism via subgoal tracker provides auxiliary rewards and adaptive graph weighting to reinforce successful planning-execution alignment.

## Method Summary
The method operates in two phases: offline knowledge extraction and online planning-guided RL. Offline, an LLM extracts a dependency-aware subgoal graph (with AND/OR edges) and entity knowledge base from environment documentation. Online, a PPO agent uses a multi-LLM pipeline where Actor generates candidate plans, Critic evaluates feasibility and sets refinement flags, and Refiner modifies plans only when necessary. A subgoal tracker monitors state changes, provides extra rewards for first completions, and updates graph edge weights to create an implicit curriculum that guides future planning.

## Key Results
- Achieves 29.6% score and 13.3 reward at 5M steps, outperforming AdaRefiner (26.8%, 12.5) and Causal-aware LLMs (27.7%, 12.7)
- 13.6% score improvement over text-RAG baseline at 1M steps demonstrates value of structured knowledge
- Robust performance across different LLM scales (Qwen3-8B to 235B parameters)

## Why This Works (Mechanism)

### Mechanism 1: Structured Knowledge Grounding via Subgoal Graph
Environment-specific subgoal graphs constrain LLM planning to feasible dependency paths by encoding subgoals with preconditions, postconditions, and AND/OR dependency edges. This grounds abstract LLM knowledge in actionable constraints, reducing semantic-environmental misalignment.

### Mechanism 2: Role-Separated Multi-LLM Planning with Refinement Guard
Separating generation (Actor), evaluation (Critic), and refinement (Refiner) across role-specific LLMs with a refinement flag reduces overconfident-yet-flawed plans. The flag mechanism prevents unnecessary changes to already-optimal plans.

### Mechanism 3: Bidirectional Feedback via Adaptive Graph Weighting
Subgoal tracker establishes feedback from execution to planning through auxiliary rewards and graph weight updates. When subgoals are achieved, agents receive extra rewards and edge weights increase, creating an implicit curriculum that prioritizes higher-success subgoals.

## Foundational Learning

- **Concept: Goal-Conditioned Reinforcement Learning**
  - Why needed here: The framework augments POMDP with a goal space where each goal corresponds to an LLM-generated natural language plan
  - Quick check question: Can you explain why the value function V^π(s,g) must be conditioned on both state and goal, and how the advantage function A^π(s,a,g) guides policy updates?

- **Concept: Retrieval-Augmented Generation (RAG) with Graph Structures**
  - Why needed here: SGA-ACR uses both standard RAG (entity knowledge base) and graph-based RAG (subgoal graph with entity linking)
  - Quick check question: How does retrieving k-hop neighboring nodes from a knowledge graph differ from retrieving unstructured text chunks, and why might structured retrieval reduce hallucination?

- **Concept: Hierarchical Planning with AND/OR Dependency Graphs**
  - Why needed here: The subgoal graph uses AND-edges (all prerequisites required) and OR-edges (any single prerequisite sufficient)
  - Quick check question: Given subgoals A and B connected by an AND-edge to C, if A has success rate 0.8 and B has 0.5, what is the effective success rate of C? How would this change for an OR-edge?

## Architecture Onboarding

- **Component map:**
  Offline Stage: Background Info → LLM extraction → Subgoal Graph G(V,E) + Entity Knowledge Base K
  Online Stage - Multi-LLM Planning: Observation → Actor LLM → Critic LLM → Refiner LLM → Final Plan
  Online Stage - RL Loop: Plan → Policy π_θ → Action → Environment → Subgoal Tracker (Checker + Extra reward + Graph weight update)

- **Critical path:**
  1. Knowledge extraction quality: If the subgoal graph is incomplete or has incorrect dependencies, all downstream planning will be constrained to wrong paths
  2. Critic evaluation accuracy: If Critic fails to detect infeasible subgoals or incorrectly ranks candidates, Refiner receives garbage input
  3. Refinement flag calibration: Over-conservative flags waste Refiner calls; over-permissive flags cause over-refinement
  4. Subgoal detection precision: False positives in completion detection cause spurious rewards and incorrect weight updates

- **Design tradeoffs:**
  - Query interval (H=100 steps) vs. real-time planning: Longer intervals reduce LLM costs but may miss rapid state changes
  - k=3 candidates vs. generation diversity: More candidates increase coverage but also Critic evaluation cost
  - Extra reward α=0.2 vs. intrinsic motivation: Higher values speed subgoal achievement but risk reward hacking
  - Structured graph vs. text-RAG: Graph construction requires upfront LLM cost but enables 13.6% better score
  - Rule-based checker vs. learned detection: Rules are interpretable and require no training but may miss complex state changes

- **Failure signatures:**
  - Knowledge extraction failure: Generated subgoals don't match environment achievements
  - Over-refinement cascade: Refiner repeatedly modifies plans even when Critic ranks first plan highly
  - Reward hacking: Agent achieves same subgoal repeatedly without progressing to harder ones
  - Graph weight stagnation: Weights don't increase over training, indicating tracker isn't detecting completions
  - Model scale sensitivity: Performance drops sharply with smaller LLMs

- **First 3 experiments:**
  1. Validate offline knowledge extraction quality: Run extraction pipeline on Crafter documentation, manually verify all 22 achievements have corresponding subgoals and dependencies match game logic
  2. Ablate planning pipeline components in isolation: Test Actor-only, Actor+Critic without flag, Actor+Critic+Refiner with flag, and Full SGA-ACR configurations at 500K steps
  3. Characterize subgoal tracker sensitivity: Sweep extra reward α ∈ {0.1, 0.2, 0.5, 1.0} and plot learning curves, achieved subgoals distribution, and "stuck" behavior rates

## Open Questions the Paper Calls Out
- Can accurate subgoal graphs be learned directly from agent-environment interactions rather than extracted from pre-existing documentation?
- How does SGA-ACR generalize to environments beyond Crafter with different task structures, state spaces, or dependency complexity?
- What is the trade-off between multi-LLM planning quality and computational cost compared to single-LLM approaches in real-time applications?

## Limitations
- Reliance on detailed environmental information available in advance limits applicability to environments lacking documentation
- High inference costs due to three LLM calls per planning step may impact real-time applications
- Performance sensitivity to extraction quality when documentation is incomplete or contains errors

## Confidence
- Knowledge extraction approach: Medium - requires manual verification but framework is sound
- Multi-LLM pipeline design: High - ablation studies provide strong empirical support
- Subgoal tracker effectiveness: Medium - rule-based approach may miss complex state changes
- Generalization claims: Low - only tested in single Crafter environment

## Next Checks
1. Verify extracted subgoal graph contains all 22 Crafter achievements with correct AND/OR dependencies matching game mechanics
2. Test whether Critic refinement flag is set correctly by monitoring flag frequency and correlating with plan acceptance rates
3. Confirm subgoal tracker provides extra rewards only on first completion by checking reward distribution across repeated subgoal attempts