---
ver: rpa2
title: Augmenting Researchy Questions with Sub-question Judgments
arxiv_id: '2510.21733'
source_url: https://arxiv.org/abs/2510.21733
tags:
- documents
- researchy
- questions
- question
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces automatic relevance judgments for sub-questions
  in the Researchy Questions dataset, a collection of complex queries requiring multi-faceted
  information retrieval. The authors use Llama3.3 70B to generate graded (0-5 scale)
  relevance labels for each sub-question based on retrieved documents from ClueWeb22.
---

# Augmenting Researchy Questions with Sub-question Judgments

## Quick Facts
- arXiv ID: 2510.21733
- Source URL: https://arxiv.org/abs/2510.21733
- Reference count: 3
- Primary result: Automatic relevance judgments for 1.29M sub-questions in Researchy Questions dataset using LLM rubric scoring

## Executive Summary
This paper introduces automatic relevance judgments for sub-questions in the Researchy Questions dataset, a collection of complex queries requiring multi-faceted information retrieval. The authors use Llama3.3 70B to generate graded (0-5 scale) relevance labels for each sub-question based on retrieved documents from ClueWeb22. They first retrieve candidates using BM25, then rerank with Qwen3 0.6B Reranker, and finally apply automatic judgment using a rubric-based prompt adapted from prior work. The resulting dataset provides on average 18.87 judgments per sub-question across 1.29 million sub-questions, enabling training of retrieval models for complex information needs. This augmentation addresses the lack of relevance associations between sub-questions and documents in the original dataset, creating a valuable resource for factoid retrieval research.

## Method Summary
The methodology involves decomposing complex queries into sub-questions using GPT-4, retrieving candidate documents through a hybrid approach combining BM25 and neural reranking, and applying automatic relevance judgments using Llama3.3 70B with a rubric-based prompt. The candidate selection process pools top 20 documents from BM25 retrieval, top 20 from Qwen3 0.6B reranking, and 439,161 clicked documents from the original dataset. Each sub-question-document pair receives a graded relevance judgment on a 0-5 scale, resulting in approximately 24.3 million total predictions.

## Key Results
- Generated 24.3 million relevance judgments across 1.29 million sub-questions
- Average of 18.87 judgments per sub-question
- Distribution shows 7.35 million judgments at levels 4-5 vs. 14.1 million at levels 0-2, indicating discrimination
- Coverage limited to 48% of clicked documents due to ClueWeb22 Set B restriction

## Why This Works (Mechanism)

### Mechanism 1
Combining lexical and neural retrieval signals produces a more balanced candidate set for complex sub-questions than either method alone. BM25 retrieves top 20 candidates (lexical overlap), Qwen3 0.6B Reranker produces top 20 candidates (semantic similarity), and clicked documents from original queries are pooled together. This union strategy mitigates the bias of any single retrieval paradigm. The core assumption is that complex sub-questions require both exact term matching and semantic understanding; neither alone suffices.

### Mechanism 2
Large-scale LLM judgment with a graded rubric can proxy human relevance assessments for sub-question/document pairs at scale. Llama3.3 70B receives a rubric-based prompt (0-5 scale) adapted from CRUX, asking whether the question is "answerable" given the document context. The rubric captures gradations from "highly relevant and complete" to "not relevant." The core assumption is that LLM rubric judgments correlate sufficiently with human relevance judgments for training purposes; the rubric granularity (6 levels) is meaningful for downstream model learning.

### Mechanism 3
Decomposing complex queries into sub-questions enables fine-grained relevance supervision that would be impossible at the parent-query level. GPT-4 decomposes each parent query into multiple sub-questions. Each sub-question receives independent document judgments, creating ~18.87 labeled pairs per sub-question. This transforms coarse click signals into structured training data. The core assumption is that GPT-4 decompositions are semantically valid and cover the actual information needs users have.

## Foundational Learning

- **BM25 Lexical Retrieval**
  - Why needed here: First-stage candidate generation; relies on term frequency and inverse document frequency to rank documents by lexical overlap with the query.
  - Quick check question: Can you explain why BM25 alone might fail for semantically equivalent but lexically different paraphrases?

- **Neural Reranking**
  - Why needed here: Second-stage semantic refinement; the Qwen3 0.6B model encodes query-document meaning beyond surface terms.
  - Quick check question: What is the difference between pointwise and listwise reranking, and which does Figure 1 suggest?

- **LLM-based Evaluation / "LLM-as-Judge"**
  - Why needed here: The paper uses Llama3.3 70B as an automatic annotator; understanding prompt design, rubric calibration, and failure modes is critical.
  - Quick check question: What are two failure modes of LLM judges (e.g., length bias, position bias) and how might a rubric mitigate them?

## Architecture Onboarding

- Component map: Parent Query → GPT-4 Decomposition → Sub-questions → BM25 Retrieval (ClueWeb22 Set B) → Qwen3 0.6B Reranking (top 100) → Candidate Union (BM25 top 20 + Rerank top 20 + Clicked) → Llama3.3 70B Judgment (0-5 rubric per sub-question/doc) → Augmented Dataset (~24.3M labels)

- Critical path: The reranking and judgment steps are computationally intensive; Llama3.3 70B inference over 24M pairs is the dominant cost. Efficient batching and GPU allocation are essential.

- Design tradeoffs:
  - Using ClueWeb22 Set B (English subset) covers only 48% of clicked documents—trading coverage for index feasibility.
  - Graded (0-5) judgments provide nuance but increase prompt complexity vs. binary labels.
  - Automatic judgments enable scale but lack human validation.

- Failure signatures:
  - Sub-questions with few judgments (<5) may indicate retrieval failure or LLM refusal.
  - Skewed label distributions (e.g., mostly 0s or 5s) may indicate prompt misalignment.
  - High lexical overlap but low semantic match suggests BM25 dominates reranking.

- First 3 experiments:
  1. Validate a sample of Llama3.3 judgments against human annotations to estimate agreement and systematic bias.
  2. Train a retrieval model on the augmented labels and evaluate on held-out complex query benchmarks; compare against MS MARCO-trained baselines.
  3. Ablate the candidate source (BM25-only vs. rerank-only vs. union) to measure impact on label quality and downstream performance.

## Open Questions the Paper Calls Out

### Open Question 1
How well do Llama3.3 70B's automatic relevance judgments correlate with human judgments for sub-questions? The paper uses automatic judgments without providing validation against human annotations. This remains unresolved because the authors do not report any correlation analysis or human evaluation to verify the quality of the LLM-generated labels. What evidence would resolve it: A sample-based human annotation study comparing LLM judgments to human relevance assessments with correlation metrics (e.g., Cohen's kappa, Spearman's rho).

### Open Question 2
Can retrieval models trained on this augmented dataset outperform models trained on traditional datasets like MS MARCO for complex information needs? The paper does not include any experiments demonstrating downstream task performance improvements from training on the augmented dataset. What evidence would resolve it: Benchmark results comparing retrieval models trained on this dataset versus MS MARCO on complex query test sets.

### Open Question 3
What is the coverage loss from limiting candidate documents to the English subset of ClueWeb22 Set B (48% of clicked documents)? The methodology explicitly restricts to 48% of clicked documents for efficiency. This remains unresolved because the paper does not quantify how many potentially relevant documents were excluded by this filtering decision. What evidence would resolve it: Analysis of relevance judgments in the excluded 52% of clicked documents or retrieval experiments on the full ClueWeb22.

### Open Question 4
Does the candidate selection strategy of combining top 20 BM25 and top 20 reranked documents adequately capture relevant documents for sub-questions? The approach retrieves 40 candidates per sub-question without analyzing recall or coverage. This remains unresolved because the paper does not measure how many relevant documents fall outside the top-k cutoff or assess whether this candidate set is sufficient. What evidence would resolve it: Pooling analysis or recall estimation using external relevance judgments or expanded retrieval depth.

## Limitations
- LLM judgments lack direct human validation, with systematic bias in Llama3.3's rating patterns remaining unmeasured
- Coverage is limited to 48% of clicked documents due to ClueWeb22 Set B restriction, potentially biasing toward more easily retrievable sub-questions
- The assumption that GPT-4 decompositions faithfully represent user information needs is unverified

## Confidence

- **High** in the pipeline design (BM25 + rerank + union) and its computational feasibility
- **Medium** in the rubric's discriminative power, given lack of human-in-the-loop calibration
- **Low** in the semantic validity of GPT-4 decompositions and their alignment with real user needs

## Next Checks

1. Sample and manually validate 100 sub-question/document judgment pairs to estimate inter-annotator agreement and detect systematic LLM bias.
2. Train and evaluate a retrieval model on the augmented labels against a held-out set of complex queries; compare against MS MARCO-trained baselines to assess practical impact.
3. Perform ablation studies on the candidate set composition (BM25-only, rerank-only, union) to quantify each component's contribution to label quality and downstream performance.