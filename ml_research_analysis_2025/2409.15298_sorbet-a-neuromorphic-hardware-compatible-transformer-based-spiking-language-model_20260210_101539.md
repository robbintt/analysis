---
ver: rpa2
title: 'Sorbet: A Neuromorphic Hardware-Compatible Transformer-Based Spiking Language
  Model'
arxiv_id: '2409.15298'
source_url: https://arxiv.org/abs/2409.15298
tags:
- sorbet
- language
- softmax
- spiking
- energy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Sorbet addresses the challenge of deploying transformer-based language
  models on neuromorphic hardware by introducing PTsoftmax and BSPN to replace incompatible
  operations like softmax and layer normalization. These innovations use bit-shifting
  instead of complex operations, enabling energy-efficient inference.
---

# Sorbet: A Neuromorphic Hardware-Compatible Transformer-Based Spiking Language Model

## Quick Facts
- arXiv ID: 2409.15298
- Source URL: https://arxiv.org/abs/2409.15298
- Reference count: 40
- Key outcome: Introduces PTsoftmax and BSPN to enable transformer-based language models on neuromorphic hardware, achieving 27.16× lower energy consumption than BERT while maintaining competitive GLUE performance

## Executive Summary
Sorbet presents a novel approach to deploying transformer-based language models on neuromorphic hardware by addressing the incompatibility between standard transformer operations and spiking neural networks. The authors introduce two key innovations: PTsoftmax, a novel operator that approximates softmax using bit-shifting operations, and BSPN (Bit-shifting-based SNN Pooling Normalization), which replaces traditional layer normalization with hardware-efficient alternatives. The model achieves competitive performance on the GLUE benchmark while significantly reducing energy consumption, making it practical for deployment on resource-constrained neuromorphic devices.

## Method Summary
The Sorbet framework addresses the fundamental challenge of implementing transformer-based language models on neuromorphic hardware by replacing incompatible operations with spike-friendly alternatives. The authors develop PTsoftmax to approximate the softmax function using bit-shifting operations, eliminating the need for expensive exponential calculations that are challenging to implement efficiently in spiking neural networks. Additionally, they introduce BSPN (Bit-shifting-based SNN Pooling Normalization) to replace layer normalization with a more hardware-compatible approach. These innovations enable the entire model to be converted into an equivalent spiking neural network while maintaining competitive accuracy on downstream tasks. The framework leverages SOTA ANN-to-SNN conversion techniques and employs knowledge distillation during training to minimize accuracy loss during the conversion process.

## Key Results
- Achieves competitive performance on GLUE benchmark while reducing energy consumption by 27.16× compared to BERT
- Outperforms SpikeLM by 3.16× in energy efficiency while maintaining similar accuracy levels
- Successfully converts the entire model into an equivalent spiking neural network using SOTA ANN-to-SNN conversion techniques
- Demonstrates that the proposed PTsoftmax and BSPN operations can effectively replace incompatible transformer operations

## Why This Works (Mechanism)
Sorbet works by fundamentally redesigning the transformer architecture to be compatible with spiking neural networks. The key mechanism is replacing computationally expensive operations like softmax and layer normalization with bit-shifting-based approximations (PTsoftmax and BSPN) that can be efficiently implemented in neuromorphic hardware. These operations leverage the binary nature of spikes and bit-shifting capabilities of neuromorphic processors to achieve the same functional outcomes as their continuous counterparts while dramatically reducing computational complexity and energy consumption.

## Foundational Learning
**Spiking Neural Networks (SNNs):** Why needed - SNNs are the computational model used in neuromorphic hardware, but they operate fundamentally differently from traditional artificial neural networks. Quick check - Verify that the model can convert standard ANN activations into spike trains without losing critical information.

**Bit-shifting operations:** Why needed - Bit-shifting is computationally efficient and can be implemented with minimal hardware resources in neuromorphic systems. Quick check - Confirm that bit-shifting approximations maintain sufficient precision for language modeling tasks.

**ANN-to-SNN conversion:** Why needed - Direct training of SNNs for complex tasks like language modeling is challenging, so conversion from pre-trained ANNs is necessary. Quick check - Validate that knowledge distillation effectively preserves accuracy during conversion.

**Softmax approximation:** Why needed - Traditional softmax operations are computationally expensive and incompatible with spike-based computation. Quick check - Ensure PTsoftmax maintains the probability distribution properties needed for transformer attention.

**Layer normalization alternatives:** Why needed - Standard layer normalization requires operations that are difficult to implement efficiently in spiking hardware. Quick check - Verify that BSPN provides adequate normalization for stable training and inference.

## Architecture Onboarding
**Component map:** Input embeddings -> PTsoftmax (attention) -> BSPN (normalization) -> SNN conversion -> Spike-based inference
**Critical path:** Token embedding -> self-attention with PTsoftmax -> feed-forward network -> output projection
**Design tradeoffs:** Accuracy vs energy efficiency, computational complexity vs hardware compatibility, spike timing precision vs implementation simplicity
**Failure signatures:** Accuracy degradation during SNN conversion, instability in spike generation, increased latency due to timestep accumulation
**First experiments:** 1) Validate PTsoftmax approximation accuracy compared to standard softmax, 2) Test BSPN effectiveness on simple sequence tasks, 3) Evaluate end-to-end performance on a single GLUE task before full benchmark

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** What are the empirical latency and energy consumption results when deploying Sorbet on physical neuromorphic hardware?
- **Basis in paper:** [Explicit] The authors state in Section 6 that they "do not have access to physical neuromorphic chips" and relied on the Lava framework and Verilog synthesis for validation.
- **Why unresolved:** Simulations and RTL synthesis cannot fully capture real-world effects such as inter-core communication overhead, memory bandwidth bottlenecks, and thermal throttling on actual chips like Loihi or TrueNorth.
- **Evidence to resolve:** On-chip power measurements and inference latency metrics obtained from a physical implementation of the Sorbet model on a neuromorphic processor.

### Open Question 2
- **Question:** Can the Sorbet architecture be effectively adapted for autoregressive generative tasks?
- **Basis in paper:** [Explicit] The conclusion (Section 7) mentions adapting techniques to models like DeepSeek-R1, yet the current work only validates the model on encoder-based NLU tasks (GLUE).
- **Why unresolved:** SNNs introduce latency overhead via multiple timesteps (T=16 in this paper), which is manageable for batch processing but may become prohibitive for sequential token generation in decoding tasks.
- **Evidence to resolve:** Evaluation of a Sorbet-based decoder on a generative benchmark (e.g., Wikitext-103) measuring generation speed (tokens/sec) and perplexity.

### Open Question 3
- **Question:** How can the accuracy degradation caused by the spike generation and quantization process be minimized?
- **Basis in paper:** [Explicit] Section 5.3 states that the "accuracy drop... is mainly caused by the quantization of weight and spike generation process" and suggests this as a direction for future work.
- **Why unresolved:** While the operators (PTsoftmax, BSPN) are designed to be efficient, the conversion of full-precision activations into binary spike trains remains a lossy bottleneck that distillation struggles to fully overcome.
- **Evidence to resolve:** Demonstration of higher GLUE scores through alternative ANN-to-SNN conversion algorithms or non-binary spike encoding that maintains similar energy efficiency.

## Limitations
- Evaluation primarily focuses on GLUE benchmark, lacking validation on diverse NLP tasks
- Claims about hardware compatibility and energy efficiency based on comparisons under controlled conditions
- No testing on actual neuromorphic hardware, only simulations and RTL synthesis
- Does not address potential accuracy trade-offs when using PTsoftmax and BSPN approximations

## Confidence
- High confidence in the novelty of PTsoftmax and BSPN operations for neuromorphic hardware
- Medium confidence in the claimed energy efficiency improvements (27.16× vs BERT, 3.16× vs SpikeLM) based on reported metrics
- Medium confidence in the competitive performance on GLUE benchmark, though broader task evaluation is needed

## Next Checks
1. Evaluate Sorbet on additional NLP benchmarks beyond GLUE, including tasks with longer sequences and different complexity levels
2. Test hardware compatibility across multiple neuromorphic platforms (e.g., Loihi, SpiNNaker) to verify cross-platform performance consistency
3. Conduct ablation studies comparing PTsoftmax and BSPN against alternative spiking transformer approaches under identical hardware constraints