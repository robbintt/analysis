---
ver: rpa2
title: Intersectional Fairness in Vision-Language Models for Medical Image Disease
  Classification
arxiv_id: '2512.15249'
source_url: https://arxiv.org/abs/2512.15249
tags:
- fairness
- cmac-mmd
- subgroups
- diagnostic
- intersectional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CMAC-MMD, a method to reduce intersectional
  bias in vision-language models for medical image classification. It addresses the
  issue of models being less confident in diagnosing marginalized patient subgroups,
  leading to missed diagnoses.
---

# Intersectional Fairness in Vision-Language Models for Medical Image Disease Classification

## Quick Facts
- arXiv ID: 2512.15249
- Source URL: https://arxiv.org/abs/2512.15249
- Authors: Yupeng Zhang; Adam G. Dunn; Usman Naseem; Jinman Kim
- Reference count: 40
- Primary result: CMAC-MMD reduces intersectional missed diagnosis gap from 0.50 to 0.26 in dermatology and from 0.41 to 0.31 in glaucoma, while improving AUC from 0.94 to 0.97 and 0.71 to 0.72 respectively.

## Executive Summary
This paper addresses intersectional bias in vision-language models for medical image classification, where models show systematically lower confidence in diagnosing marginalized patient subgroups. The proposed CMAC-MMD method regularizes diagnostic certainty distributions across intersectional demographic subgroups during training, ensuring equal confidence in predictions regardless of demographics. Evaluated on skin lesion classification and glaucoma detection, CMAC-MDD substantially reduces missed diagnosis disparities while maintaining or improving overall accuracy, and importantly, does not require demographic data during inference.

## Method Summary
CMAC-MMD extends CLIP fine-tuning by adding a fairness regularization term that enforces statistical alignment of diagnostic certainty distributions across all intersectional demographic subgroups. The method computes a per-sample alignment score (margin between correct and most-competing-incorrect class similarity), then applies Maximum Mean Discrepancy (MMD) to enforce statistically indistinguishable certainty distributions across subgroups within each mini-batch. The total loss combines standard CLIP loss with the MMD-based fairness term, trained end-to-end on medical imaging datasets with demographic metadata.

## Key Results
- Reduced intersectional missed diagnosis gap (ΔTPR) from 0.50 to 0.26 in dermatology classification
- Reduced ΔTPR from 0.41 to 0.31 in glaucoma detection
- Improved AUC from 0.94 to 0.97 in dermatology and from 0.71 to 0.72 in glaucoma
- Maintained or improved overall accuracy while reducing fairness disparities
- Validated on both internal and external datasets with distribution shift

## Why This Works (Mechanism)

### Mechanism 1
Regularizing decision-level certainty distributions across intersectional subgroups reduces missed diagnosis disparities more effectively than regularizing high-dimensional embeddings. CMAC-MMD computes a scalar alignment score per sample (margin between correct and most-competing-incorrect class similarity), then applies Maximum Mean Discrepancy (MMD) to enforce statistically indistinguishable certainty distributions across all intersectional subgroups within each mini-batch. Equitable diagnostic AI requires equally confident predictions, not just equal aggregate accuracy; the full distribution shape matters, not just the mean. If mini-batches lack sufficient subgroup representation, MMD estimates become unreliable; batch sampling strategy is critical.

### Mechanism 2
Standard fine-tuning systematically creates a "diagnostic certainty gap" where models learn high-confidence negative predictions for minority subgroups. During standard ERM fine-tuning, loss minimization favors predicting the majority class with high confidence for underrepresented subgroups, reducing uncertainty but catastrophically degrading sensitivity. The certainty gap is an intrinsic property of standard fine-tuning on demographically imbalanced medical data, not an artifact of specific architectures. This mechanism may not hold if training data is perfectly balanced across intersectional subgroups (rare in practice).

### Mechanism 3
Certainty-based regularization transfers across distribution shift better than feature-level interventions. By enforcing consistency in decision certainty distributions rather than erasing demographic information from latent features, CMAC-MMD is less susceptible to covariate/prevalence shifts that cause feature-level interventions to fail upon deployment. External validation improvements reflect robustness, not overfitting to training distribution characteristics. Requires validation across more diverse domain shifts; single external dataset provides limited evidence.

## Foundational Learning

- Concept: Maximum Mean Discrepancy (MMD)
  - Why needed here: Core technical component for measuring distributional distance between subgroup certainty scores; requires understanding kernel methods and RKHS.
  - Quick check question: Can you explain why MMD compares full distributions rather than just means, and why an RBF kernel is used?

- Concept: Intersectional Fairness vs. Single-Attribute Fairness
  - Why needed here: Paper's central thesis is that evaluating fairness on single attributes masks compounding disparities at intersections.
  - Quick check question: Why might a model appear fair by gender alone but show large disparities for "elderly Black women"?

- Concept: CLIP Contrastive Learning and Image-Text Alignment
  - Why needed here: CMAC-MMD builds on CLIP's joint embedding space; alignment scores derive from cosine similarity between image and text embeddings.
  - Quick check question: In CLIP, what does the cosine similarity between an image embedding and a text embedding represent, and how does temperature τ affect the softmax distribution?

## Architecture Onboarding

- Component map: Image Encoder (ViT-B/16) -> ℓ₂-normalized image embeddings -> Alignment Score Calculation -> Group by demographic attributes -> MMD computation -> Backpropagation through combined loss

- Critical path: Image-text pair → dual encoders → alignment score per sample → group by demographic attributes → compute MMD between all subgroup pairs → backprop through combined loss

- Design tradeoffs:
  - λ_CMAC tuning: Higher values prioritize fairness but may impact accuracy; 0.5 worked across both domains
  - Batch size: Must be large enough to include multiple samples from each intersectional subgroup per batch
  - Demographic granularity: More attributes = exponentially more subgroups = smaller per-subgroup samples

- Failure signatures:
  - MMD estimates unstable if any subgroup has <10 samples per batch
  - "Leveling down": if λ_CMAC too high, model degrades all subgroups equally rather than lifting disadvantaged ones
  - External validation failure: fairness gains disappear under distribution shift

- First 3 experiments:
  1. Replicate the certainty gap analysis: Fine-tune CLIP on HAM10000 with standard ERM; plot KDE distributions of certainty scores per subgroup; verify Female 0-40 shows worst baseline performance and largest uncertainty zone occupancy.
  2. Ablation study on λ_CMAC: Train CMAC-MMD with λ ∈ {0.1, 0.3, 0.5, 0.7, 1.0}; plot accuracy-fairness frontier (AUC vs ΔTPR) to validate 0.5 as the operating point.
  3. Mini-batch sensitivity analysis: Compare MMD estimates with batch sizes {32, 64, 128, 256} on subgroup representation; identify minimum batch size where all 6 (dermatology) or 8 (ophthalmology) subgroups have ≥5 samples per batch consistently.

## Open Questions the Paper Calls Out

### Open Question 1
Do the improvements in diagnostic certainty and reduced missed diagnosis gaps translate to measurable changes in clinician behavior and patient outcomes in prospective, real-world clinical workflows? The current study is retrospective, relying on held-out test sets rather than interactive clinical environments where trust and human-AI interaction play a role.

### Open Question 2
Can the CMAC-MMD framework be effectively adapted to other high-stakes medical AI tasks beyond binary classification, such as prognostic modeling, clinical trial matching, or treatment recommendation? The study only validated the method on binary classification tasks; extending it to regression or multi-class settings requires further methodological consideration.

### Open Question 3
Does the fairness regularization remain robust when applied to more granular or continuous demographic attributes, rather than the binned categories used in this study? The binning of attributes may conceal within-group heterogeneity, and it is unknown if the method functions effectively with continuous age or finer racial/ethnic stratifications.

## Limitations
- The method requires demographic labels during training, which may not be available in all clinical settings despite not needing them during inference
- Performance validation is limited to two specific medical domains with controlled demographic annotations
- RBF kernel bandwidth for MMD and exact temperature parameter for alignment scores are unspecified, potentially affecting reproducibility

## Confidence

- High confidence: The intersectional fairness problem definition and its importance in medical AI; the mechanism by which certainty-based regularization addresses the diagnostic certainty gap
- Medium confidence: The generalizability of CMAC-MMD across distribution shifts, given only one external validation dataset; the optimal hyperparameter settings (λ_CMAC=0.5) that may not transfer to other domains
- Low confidence: Claims about privacy preservation, as the method still requires demographic labels during training which may not be available in all clinical settings

## Next Checks

1. Evaluate CMAC-MMD on a third, independently collected medical imaging dataset with different demographic distributions and disease types to test cross-domain robustness.

2. Conduct a systematic hyperparameter sweep (λ_CMAC from 0.1 to 1.0, multiple batch sizes) to characterize the fairness-accuracy tradeoff curve and identify failure modes.

3. Perform ablation studies removing the intersectional aspect - compare CMAC-MMD against single-attribute fairness methods to quantify the specific benefit of the intersectional approach.