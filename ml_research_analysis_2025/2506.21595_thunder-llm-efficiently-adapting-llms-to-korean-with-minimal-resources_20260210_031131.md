---
ver: rpa2
title: 'Thunder-LLM: Efficiently Adapting LLMs to Korean with Minimal Resources'
arxiv_id: '2506.21595'
source_url: https://arxiv.org/abs/2506.21595
tags:
- korean
- training
- data
- table
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a cost-effective approach to adapting English-based
  LLMs to Korean. The method involves collecting and preprocessing Korean data, extending
  the tokenizer with Korean tokens, and performing continual pre-training followed
  by post-training using supervised fine-tuning and direct preference optimization.
---

# Thunder-LLM: Efficiently Adapting LLMs to Korean with Minimal Resources

## Quick Facts
- **arXiv ID**: 2506.21595
- **Source URL**: https://arxiv.org/abs/2506.21595
- **Reference count**: 40
- **Primary result**: Achieved best Korean benchmark performance among similar-scale models while using significantly less data and computational resources

## Executive Summary
This paper presents a cost-effective approach to adapting English-based LLMs to Korean by collecting and preprocessing Korean data, extending the tokenizer with Korean tokens, and performing continual pre-training followed by post-training using supervised fine-tuning and direct preference optimization. The resulting models, Thunder-LLM and Thunder-LLM-Ins, demonstrate state-of-the-art performance on Korean benchmarks while maintaining comparable English capabilities. The approach uses significantly less data and computational resources compared to training from scratch, making it an efficient solution for bilingual adaptation.

## Method Summary
The methodology involves a multi-stage adaptation process that efficiently transforms existing English LLMs into bilingual Korean-English models. The approach begins with data collection and preprocessing of Korean text, followed by tokenizer extension to incorporate Korean-specific tokens. The model then undergoes continual pre-training on the extended dataset, followed by post-training that combines supervised fine-tuning and direct preference optimization. This staged approach enables effective Korean language capability development while preserving the model's original English performance, achieving superior results with minimal resource investment.

## Key Results
- Achieved best performance on Korean benchmarks among similar-scale models
- Maintained comparable English performance to the base model
- Demonstrated significant resource efficiency with reduced data and computational requirements

## Why This Works (Mechanism)
The adaptation approach succeeds through strategic integration of Korean language resources into existing LLM architectures. By extending the tokenizer with Korean-specific tokens, the model gains the ability to represent Korean text more efficiently and accurately. The continual pre-training phase allows the model to learn Korean language patterns while building on its existing English knowledge base. The post-training phase using both supervised fine-tuning and direct preference optimization fine-tunes the model's responses for Korean language tasks while maintaining alignment with human preferences and preserving English capabilities.

## Foundational Learning

**Korean language tokenization**: Breaking Korean text into subword units for efficient processing. Why needed: Korean has unique morphological structures requiring specialized tokenization. Quick check: Verify tokenizer handles Korean compound words and particles correctly.

**Continual pre-training**: Extending model knowledge through additional training on new language data. Why needed: Allows gradual adaptation without forgetting base language capabilities. Quick check: Monitor language-specific loss metrics during training.

**Supervised fine-tuning (SFT)**: Training on curated input-output pairs for task-specific behavior. Why needed: Establishes correct response patterns for Korean language tasks. Quick check: Validate outputs match reference examples.

**Direct preference optimization (DPO)**: Aligning model outputs with human preferences without explicit reward modeling. Why needed: Ensures natural, contextually appropriate Korean responses. Quick check: Compare preference alignment scores before and after DPO.

## Architecture Onboarding

**Component map**: Data Collection -> Tokenizer Extension -> Continual Pre-training -> SFT -> DPO -> Evaluation

**Critical path**: The most critical sequence is Tokenizer Extension followed by Continual Pre-training, as proper tokenization is foundational for effective language learning.

**Design tradeoffs**: The approach trades some Korean-specific optimization for resource efficiency and bilingual maintenance. This means potentially suboptimal Korean performance compared to models trained exclusively on Korean, but with the benefit of preserved English capabilities.

**Failure signatures**: Poor tokenization will manifest as degraded Korean performance and inefficient vocabulary usage. Insufficient pre-training data will show as poor generalization on Korean tasks. Inadequate SFT/DPO will result in unnatural or misaligned responses.

**First experiments**:
1. Tokenization test: Run sample Korean text through the extended tokenizer to verify proper handling of Korean linguistic features
2. Pre-training validation: Evaluate language modeling loss on held-out Korean validation set after initial pre-training phase
3. Bilingual capability check: Test model on simple tasks in both Korean and English to verify preservation of English capabilities

## Open Questions the Paper Calls Out
None

## Limitations
- Limited competitive analysis to a narrow set of similar-scale models
- Lack of independent verification of resource efficiency claims
- Evaluation focused primarily on benchmarks without extensive real-world deployment validation

## Confidence
| Claim | Confidence |
|-------|------------|
| State-of-the-art Korean performance among similar-scale models | Medium |
| Resource efficiency compared to training from scratch | Medium |
| Maintenance of English performance | Medium |
| Methodology reproducibility | High |

## Next Checks
1. Conduct ablation studies to isolate the contribution of each adaptation component (tokenizer extension, pre-training, SFT, DPO) to overall performance
2. Test the methodology on other non-English languages with similar resource constraints to evaluate generalizability
3. Perform extended evaluation using diverse real-world Korean language tasks and user feedback to complement benchmark results