---
ver: rpa2
title: 'RealTalk-CN: A Realistic Chinese Speech-Text Dialogue Benchmark With Cross-Modal
  Interaction Analysis'
arxiv_id: '2508.10015'
source_url: https://arxiv.org/abs/2508.10015
tags:
- speech
- dialogue
- dataset
- task
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RealTalk-CN introduces the first Chinese speech-text dual-modal
  TOD dataset with 5.4k dialogues (60K utterances, 150 hours) featuring paired speech-text
  annotations. The dataset captures real-world dialogue complexities including spontaneous
  speech disfluencies (repetitions, hesitations, self-corrections) and diverse speaker
  characteristics (gender, age, regional accents).
---

# RealTalk-CN: A Realistic Chinese Speech-Text Dialogue Benchmark With Cross-Modal Interaction Analysis

## Quick Facts
- arXiv ID: 2508.10015
- Source URL: https://arxiv.org/abs/2508.10015
- Reference count: 40
- Introduces first Chinese speech-text dual-modal TOD dataset with 5.4k dialogues (60K utterances, 150 hours) featuring paired speech-text annotations

## Executive Summary
RealTalk-CN presents the first comprehensive Chinese speech-text dialogue benchmark designed for task-oriented dialogue (TOD) systems. The dataset captures real-world conversational complexities through paired speech and text annotations across 5.4k dialogues totaling 150 hours of audio. It introduces a novel cross-modal chat task where users dynamically switch between speech and text modalities during conversations, simulating authentic voice assistant interactions. The benchmark addresses the gap in multi-modal dialogue datasets by incorporating spontaneous speech disfluencies and diverse speaker characteristics.

The evaluation framework assesses model performance across four distinct subsets covering multi-domain versus single-domain dialogues and colloquial versus system speech. Results demonstrate that speech disfluencies significantly impact slot-filling performance while having less effect on intent classification. The benchmark reveals critical insights about how speaker characteristics such as age and regional accents affect model performance, particularly for fine-grained semantic tasks.

## Method Summary
RealTalk-CN introduces a Chinese speech-text dialogue dataset with paired annotations for both modalities. The dataset was constructed through human-to-human conversations covering navigation, weather, restaurants, and other task-oriented domains. Speech data was collected with spontaneous disfluencies including repetitions, hesitations, and self-corrections. Text transcripts were aligned with audio recordings to create synchronized speech-text pairs. The benchmark defines a cross-modal chat task where users can switch between speech and text during a single conversation. Four evaluation subsets were created based on domain complexity (multi vs single) and speech style (colloquial vs system). Models were evaluated on intent classification, slot-filling, and end-to-end chat tasks using both pipeline (ASR + LLM) and end-to-end approaches.

## Key Results
- Speech disfluencies significantly impact slot-filling performance but less so intent classification accuracy
- Pipeline methods (Whisper + GPT-4o) outperformed end-to-end speech models on intent classification and chat tasks
- Speaker characteristics (age, regional accents) significantly affect model performance, particularly for fine-grained semantic tasks

## Why This Works (Mechanism)
The benchmark's effectiveness stems from capturing authentic conversational phenomena that occur in real-world voice assistant interactions. By incorporating speech disfluencies like repetitions, hesitations, and self-corrections, the dataset reflects natural human communication patterns that challenge traditional dialogue systems. The cross-modal chat task design forces models to handle modality switching, a common real-world scenario where users might start with voice input and switch to text or vice versa. The paired speech-text annotations enable direct comparison between modalities and facilitate the development of models that can seamlessly operate across both input types.

## Foundational Learning
- **Speech disfluencies**: Spontaneous speech phenomena like repetitions, hesitations, and self-corrections that occur naturally in human conversation; needed to create realistic dialogue data that reflects actual user behavior, quick check: identify disfluency types in sample utterances
- **Cross-modal dialogue processing**: Systems that handle both speech and text inputs within the same conversation; needed to support flexible user interaction patterns, quick check: verify modality detection in mixed-input dialogues
- **Slot-filling in TOD**: Extracting specific semantic information from dialogue utterances to fill predefined slots; needed for task completion in dialogue systems, quick check: test slot extraction accuracy on ambiguous utterances
- **Intent classification**: Determining the user's goal or purpose from dialogue input; needed as the first step in task-oriented dialogue understanding, quick check: evaluate classification accuracy across different domains
- **Speaker characteristic modeling**: Accounting for variations in performance due to speaker attributes like age and accent; needed for robust deployment across diverse user populations, quick check: measure performance variance across speaker demographics
- **Pipeline vs end-to-end architectures**: Different approaches to building dialogue systems where pipeline uses separate components versus end-to-end uses unified models; needed to understand trade-offs in system design, quick check: compare latency and accuracy trade-offs

## Architecture Onboarding
**Component Map**: User Input -> Speech Recognition (Whisper) -> Language Understanding (GPT-4o) -> Dialogue Management -> Response Generation -> Output
**Critical Path**: Speech recognition → Intent classification → Slot-filling → Response generation → Speech synthesis (for voice output)
**Design Tradeoffs**: Pipeline approaches offer modularity and easier debugging but introduce error propagation; end-to-end models provide better integration but are harder to interpret and train
**Failure Signatures**: Speech disfluencies cause slot-filling errors; speaker accent variations impact recognition accuracy; domain complexity affects intent classification; modality switching can confuse context tracking
**First Experiments**: 1) Test pipeline accuracy with clean vs disfluent speech inputs; 2) Evaluate end-to-end model performance across different speaker age groups; 3) Measure modality switching impact on dialogue context retention

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Dataset size of 5.4k dialogues is relatively small compared to established text-based dialogue datasets
- Evaluation framework focuses on TOD-specific domains, limiting applicability to open-domain conversations
- Pipeline approach relies on commercial APIs (Whisper, GPT-4o) whose internal implementations are not fully transparent

## Confidence
- **High**: Speaker characteristic effects - statistically significant performance variations across age groups and regional accents with clear patterns
- **Medium**: Cross-modal performance comparisons - pipeline approach reliance on commercial APIs makes exact reproduction challenging
- **Low**: Generalizability to open-domain conversations - limited by TOD-specific domain focus

## Next Checks
1. Conduct ablation studies removing specific disfluency types (repetitions, hesitations, self-corrections) to quantify their individual impacts on model performance, particularly for slot-filling accuracy
2. Expand speaker diversity testing by collecting additional recordings from underrepresented age groups and regional accents to verify whether observed performance gaps persist at larger scales
3. Implement cross-dataset validation by testing models trained on RealTalk-CN against established English speech-text datasets (e.g., SLURP, Schema-Guided Dialog) to assess generalization across languages and domains