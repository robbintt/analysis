---
ver: rpa2
title: 'Stable Thompson Sampling: Valid Inference via Variance Inflation'
arxiv_id: '2505.23260'
source_url: https://arxiv.org/abs/2505.23260
tags:
- thompson
- sampling
- bandit
- algorithm
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of constructing valid confidence
  intervals for model parameters when data is collected via Thompson Sampling in multi-armed
  bandit problems. The authors propose Stable Thompson Sampling, a variant that inflates
  the posterior variance by a logarithmic factor, enabling asymptotically normal estimates
  of arm means despite the non-i.i.d.
---

# Stable Thompson Sampling: Valid Inference via Variance Inflation
## Quick Facts
- arXiv ID: 2505.23260
- Source URL: https://arxiv.org/abs/2505.23260
- Reference count: 40
- Primary result: Variance-inflated Thompson Sampling enables valid confidence intervals for bandit parameters while maintaining logarithmic regret penalty

## Executive Summary
This paper addresses the fundamental challenge of constructing valid confidence intervals for model parameters when data is collected via Thompson Sampling in multi-armed bandit problems. The authors propose a modified algorithm called Stable Thompson Sampling that inflates posterior variance by a logarithmic factor, enabling asymptotically normal estimates despite the non-i.i.d. nature of adaptive sampling. The method achieves valid inference with only a logarithmic regret penalty compared to standard Thompson Sampling, making it theoretically appealing for problems requiring both exploration and statistical inference.

## Method Summary
The proposed Stable Thompson Sampling algorithm modifies standard Thompson Sampling by adding a variance inflation term to the posterior distribution. This inflation is achieved by adding a logarithmic factor to the variance of the posterior samples. The key insight is that this modification ensures the adaptive sampling process satisfies the Lai-Wei stability condition, which is necessary for valid inference in sequential decision problems. The algorithm maintains the exploration-exploitation balance of Thompson Sampling while creating conditions suitable for statistical inference about arm means.

## Key Results
- Stable Thompson Sampling yields asymptotically normal estimates of arm means
- The method satisfies the Lai-Wei stability condition required for valid inference
- Confidence intervals constructed using the stabilized algorithm achieve valid coverage across confidence levels
- Regret increases only logarithmically compared to standard Thompson Sampling

## Why This Works (Mechanism)
The mechanism relies on variance inflation to create sufficient randomness in the sampling process, preventing the algorithm from becoming too confident too quickly. By adding logarithmic variance inflation to the posterior, the algorithm maintains exploration longer than standard Thompson Sampling, which is necessary for satisfying stability conditions. This inflation ensures that the data collection process doesn't become overly deterministic, which would violate the assumptions needed for valid statistical inference.

## Foundational Learning
- Lai-Wei stability condition: A technical requirement for valid inference in sequential problems; needed to ensure the sampling process doesn't become too concentrated; quick check: verify that the spectral radius of a certain matrix remains bounded
- Asymptotic normality: The property that estimates converge to a normal distribution; needed for constructing valid confidence intervals; quick check: examine the limiting distribution of normalized estimation errors
- Variance inflation: The technique of artificially increasing posterior variance; needed to maintain exploration and satisfy stability conditions; quick check: verify that inflated variance doesn't grow too quickly
- Conjugate priors: Prior distributions that maintain analytical tractability; needed for tractable posterior updates; quick check: confirm posterior remains in the same family as prior
- Regret analysis: The study of cumulative loss compared to optimal decisions; needed to quantify the cost of exploration; quick check: bound the expected regret over time horizon

## Architecture Onboarding
Component map: Prior distribution -> Thompson Sampling update -> Variance inflation -> Posterior sampling -> Action selection -> Reward observation

Critical path: The algorithm begins with a prior distribution over arm means, updates this distribution using observed rewards via Thompson Sampling, applies logarithmic variance inflation, samples from the inflated posterior to select actions, and updates based on received rewards.

Design tradeoffs: The primary tradeoff is between inference validity and statistical efficiency. The variance inflation ensures valid confidence intervals but reduces the algorithm's ability to quickly identify optimal arms, resulting in higher regret. The logarithmic factor represents a balance between maintaining sufficient exploration for valid inference while minimizing the regret penalty.

Failure signatures: If variance inflation is insufficient, the algorithm may violate stability conditions leading to invalid confidence intervals. If inflation is excessive, exploration becomes too random, causing unnecessary regret. Non-conjugate priors or non-standard reward distributions may break the theoretical guarantees.

First experiments: 1) Compare coverage rates of confidence intervals between standard and stable Thompson Sampling across different confidence levels. 2) Measure regret trajectories for both algorithms over varying time horizons and number of arms. 3) Test stability condition satisfaction by examining the empirical distribution of sampling probabilities.

## Open Questions the Paper Calls Out
The paper leaves open questions about the method's applicability to more complex bandit settings like contextual bandits and non-conjugate models. The choice of variance inflation factor appears somewhat arbitrary, and optimal scaling for different problem scales remains unexplored. Performance in high-dimensional or non-stationary environments is also not addressed.

## Limitations
- Trades statistical efficiency for valid inference through logarithmic regret penalty
- Analysis assumes standard conjugate priors and specific problem structures
- Empirical validation remains limited to simulations without real-world testing
- Performance in high-dimensional or non-stationary environments unexplored

## Confidence
High confidence: Theoretical claims about asymptotic normality and Lai-Wei stability condition satisfaction
Medium confidence: Empirical claim about valid coverage across confidence levels based on simulations
Low confidence: Applicability to complex bandit settings like contextual bandits or non-conjugate models

## Next Checks
1) Test the method on real-world bandit problems with non-stationary reward distributions to assess robustness
2) Evaluate sensitivity of results to different choices of variance inflation factor beyond logarithmic scaling
3) Extend analysis to contextual bandit settings to determine whether stability conditions can be satisfied in more complex scenarios