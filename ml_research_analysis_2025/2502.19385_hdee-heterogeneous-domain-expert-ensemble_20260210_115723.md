---
ver: rpa2
title: 'HDEE: Heterogeneous Domain Expert Ensemble'
arxiv_id: '2502.19385'
source_url: https://arxiv.org/abs/2502.19385
tags:
- training
- domain
- domains
- expert
- trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HDEE (Heterogeneous Domain Expert Ensemble),
  a method that improves language modeling by training diverse-sized experts for different
  data domains rather than using uniform model sizes and training steps. The authors
  show that allowing model sizes and training steps to vary by domain difficulty (easy,
  moderate, difficult) yields better perplexity scores than homogeneous baselines.
---

# HDEE: Heterogeneous Domain Expert Ensemble

## Quick Facts
- arXiv ID: 2502.19385
- Source URL: https://arxiv.org/abs/2502.19385
- Authors: Oğuzhan Ersoy; Jari Kolehmainen; Gabriel Passamani Andrade
- Reference count: 13
- Heterogeneous ensembles achieve lowest perplexity in 20/21 domains

## Executive Summary
This paper introduces HDEE (Heterogeneous Domain Expert Ensemble), a method that improves language modeling by training diverse-sized experts for different data domains rather than using uniform model sizes and training steps. The authors show that allowing model sizes and training steps to vary by domain difficulty (easy, moderate, difficult) yields better perplexity scores than homogeneous baselines. Across 21 domains, heterogeneous ensembles achieved the lowest perplexity in 20 cases. For example, in the difficult Mathematics domain, HDEE reduced perplexity to 31.5 from 32.0 in the baseline. Performance gains were particularly notable in hard domains where larger models and more training steps were used. The method maintains the same compute budget as homogeneous training.

## Method Summary
HDEE uses a Branch-Train-Merge (BTM) approach to train heterogeneous domain expert ensembles. The method compares three configurations: MHo-IHo (homogeneous baseline with uniform model sizes and iterations), MHo-IHe (heterogeneous iterations), and MHe-IHo (heterogeneous model sizes). Domains are classified by difficulty using seed model perplexities and assigned appropriate model sizes or iteration counts. The Llama architecture is used with vocab size 128K and sequence length 1024. Domain experts are trained independently and ensembled using domain posterior with uniform prior. The compute budget is constrained to be equal across configurations via |FFN|·iterations constraints.

## Key Results
- Heterogeneous ensembles achieved lowest perplexity in 20 out of 21 domains
- In Mathematics domain (difficult), HDEE reduced perplexity from 32.0 to 31.5
- Performance gains were particularly notable in hard domains where larger models and more training steps were used
- Method maintains same compute budget as homogeneous training

## Why This Works (Mechanism)
Assumption: The heterogeneous approach works because it allocates computational resources based on domain difficulty, allowing easier domains to be trained efficiently with smaller models while dedicating more capacity to challenging domains. This matches model complexity to task complexity rather than using uniform resource allocation.

## Foundational Learning
- Domain difficulty classification: Needed to assign appropriate model sizes/iterations per domain. Quick check: Verify seed model perplexity correctly ranks domains.
- Compute budget balancing: Essential for fair comparison across configurations. Quick check: Confirm |FFN|·iterations constraints are satisfied.
- Branch-Train-Merge workflow: Core methodology for training heterogeneous experts. Quick check: Ensure all domain experts are properly merged during inference.
- Domain posterior ensembling: Combines expert predictions weighted by domain relevance. Quick check: Validate uniform prior implementation.

## Architecture Onboarding

**Component Map:** Pretraining -> Difficulty Classification -> BTM Training -> Domain Posterior Ensembling -> Evaluation

**Critical Path:** The critical path involves (1) seed pretraining to establish difficulty baselines, (2) domain difficulty classification, (3) BTM training with heterogeneous configurations, and (4) domain posterior ensembling for final evaluation.

**Design Tradeoffs:** The main tradeoff is between model size and training iterations within a fixed compute budget. The method chooses to allocate resources based on domain difficulty rather than using uniform allocation.

**Failure Signatures:** Catastrophic forgetting occurs when experts trained on later domains lose performance on earlier ones. Diagnose by tracking per-domain perplexity across iterations.

**First Experiments:**
1. Verify domain difficulty classification matches Table 1 assignments
2. Test compute budget balancing with small-scale models
3. Implement and validate domain posterior ensembling on a subset of domains

## Open Questions the Paper Calls Out
Unknown: No specific open questions were identified in the source material that the paper explicitly calls out for future research.

## Limitations
- Exact checkpoint selection mechanism for final evaluation is not fully specified
- Evaluation focuses primarily on perplexity without exploring other benefits or trade-offs
- SentencePiece tokenizer implementation details are referenced but not provided

## Confidence

**Major uncertainties and limitations:**
- **Medium confidence** in checkpoint selection methodology due to underspecification
- **Medium confidence** in computational efficiency claims
- **High confidence** in core finding that heterogeneous ensembles outperform homogeneous baselines

**Confidence assessment:**
- High confidence in perplexity improvements across 20/21 domains
- High confidence in benefits for difficult domains specifically
- Medium confidence in methodological details affecting exact reproduction

## Next Checks
1. Verify domain difficulty classification based on seed model perplexities matches final experiment assignments
2. Implement and test compute budget balancing mechanism to ensure |FFN|·iterations constraints
3. Reproduce perplexity improvements on at least two domains (one easy, one difficult) using specified hyperparameters