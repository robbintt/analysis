---
ver: rpa2
title: 'MuTri: Multi-view Tri-alignment for OCT to OCTA 3D Image Translation'
arxiv_id: '2504.01428'
source_url: https://arxiv.org/abs/2504.01428
tags:
- octa
- translation
- image
- learning
- codebook
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MuTri, a novel multi-view tri-alignment framework
  for translating 3D OCT images into 3D OCTA images in discrete and finite space.
  Unlike previous methods that learn the mapping in continuous space, MuTri leverages
  vector quantization (VQ) and multi-view guidance from 3D OCT, 3D OCTA, and 2D OCTA
  projection maps.
---

# MuTri: Multi-view Tri-alignment for OCT to OCTA 3D Image Translation

## Quick Facts
- arXiv ID: 2504.01428
- Source URL: https://arxiv.org/abs/2504.01428
- Reference count: 40
- Key outcome: MuTri achieves 0.0707 MAE, 34.10 dB PSNR, and 89.86% SSIM on OCTA-3M, outperforming state-of-the-art methods for 3D OCT to OCTA translation.

## Executive Summary
This paper introduces MuTri, a novel multi-view tri-alignment framework for translating 3D OCT images into 3D OCTA images in discrete and finite space. Unlike previous methods that learn the mapping in continuous space, MuTri leverages vector quantization (VQ) and multi-view guidance from 3D OCT, 3D OCTA, and 2D OCTA projection maps. The method consists of two stages: pre-training VQ-VAE models on OCT and OCTA data, then using contrastive-inspired semantic alignment and vessel structure alignment to guide codebook learning. Experimental results on OCTA-3M, OCTA-6M, and the newly collected OCTA2024 dataset show significant improvements, achieving 0.0707 MAE, 34.10 dB PSNR, and 89.86% SSIM on OCTA-3M, outperforming state-of-the-art methods. The proposed method effectively learns detailed vessel structure information and demonstrates strong potential for practical OCTA image translation applications.

## Method Summary
MuTri is a two-stage 3D medical image translation framework that maps OCT volumes to OCTA volumes using discrete vector quantization with multi-view guidance. In Stage I, separate VQ-VAE models are pre-trained on OCT and OCTA reconstruction tasks. In Stage II, a translation VQ-VAE is trained with contrastive-inspired semantic alignment (maximizing mutual information with pre-trained OCT and OCTA features) and vessel structure alignment (minimizing discrepancy between projection map structures). The discrete codebook space constrains the mapping and improves codebook utilization compared to continuous-space methods.

## Key Results
- Achieves 0.0707 MAE, 34.10 dB PSNR, and 89.86% SSIM on OCTA-3M dataset
- Outperforms state-of-the-art methods including VQ-VAE, CUT, and Palette
- Demonstrates improved codebook utilization (over 50% unique codewords used) compared to vanilla VQ-VAE
- Shows strong generalization capability across datasets with different field-of-view sizes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Constraining OCT→OCTA translation to discrete, finite codebook space reduces mapping uncertainty compared to continuous, infinite latent spaces.
- **Mechanism:** VQ-VAE discretizes continuous OCT features by mapping them to nearest codebook entries (Eq. 2: VQ_Z(z) := argmin ||z - z_k||). This forces the model to learn a bounded vocabulary of OCTA representations rather than unbounded continuous features. The finite codebook (N entries) acts as a regularizer, preventing the model from learning arbitrary mappings.
- **Core assumption:** The OCT→OCTA mapping can be decomposed into a finite set of reusable feature patterns that generalize across subjects.
- **Evidence anchors:**
  - [abstract] "MuTri leverages vector quantization (VQ)... translating 3D OCT images into 3D OCTA images in discrete and finite space. Unlike previous methods that learn the mapping in continuous space"
  - [section 1, page 2] "learn the mapping from the OCT domain to the OCTA domain in an infinite and continuous space, resulting in limited translation quality"
  - [corpus] Weak direct evidence; corpus focuses on OCT analysis/reconstruction, not discrete representation learning for translation.
- **Break condition:** If codebook size N is too small, quantization collapses to under-represented codewords (Fig. 1c shows vanilla VQ-VAE has low utilization). If N is too large, codebook remains underutilized.

### Mechanism 2
- **Claim:** Contrastive-inspired semantic alignment between translation model and pre-trained reconstruction models maximizes mutual information, improving codebook utilization.
- **Mechanism:** The translation model's features are aligned with pre-trained OCT and OCTA reconstruction models via patch-wise contrastive learning (Eq. 7). Positive pairs are same-location patches; negatives are different-location patches. Minimizing L_OCT and L_OCTA forces the translation encoder to produce features that share information with both OCT structural features and OCTA vascular features, encouraging the codebook to explore more codewords.
- **Core assumption:** Pre-trained reconstruction models encode domain-specific semantic priors that transfer to the translation task without fine-tuning.
- **Evidence anchors:**
  - [abstract] "contrastive-inspired semantic alignment is proposed to maximize the mutual information with the pre-trained models from OCT and OCTA views, to facilitate codebook learning"
  - [section 3.3, page 5] "LOCT maximize the information overlap degree between unquantized features from the translation and pre-trained OCT model"
  - [corpus] No direct corpus evidence for contrastive learning in OCT→OCTA translation; related work on contrastive learning (CUT, MCL) is cited but not empirically validated in corpus.
- **Break condition:** If pre-trained models are poorly trained (e.g., on insufficient data), their features provide noisy supervision, degrading alignment.

### Mechanism 3
- **Claim:** Vessel structure alignment via 2D OCTA projection maps enforces structural consistency and reduces overfitting to scan artifacts.
- **Mechanism:** Real OCTA projection maps contain discontinuities from scanning instability. The pre-trained OCTA model reconstructs smoother projection maps (Fig. 3). By computing patch-level cosine similarity matrices (Eq. 9) and minimizing their discrepancy (Eq. 10), the translation model learns vessel continuity patterns rather than overfitting to artifact-specific discontinuities.
- **Core assumption:** The 2D projection map provides a reliable structural prior that captures vessel topology better than the noisy 3D volume alone.
- **Evidence anchors:**
  - [abstract] "vessel structure alignment is proposed to minimize the structure discrepancy with the pre-trained models from the OCTA project map view"
  - [section 3.3, page 5-6] "the vanilla VQVAE may suffer from the overfitting problem by simply learning these specific patterns with only the supervision from real OCTA"
  - [corpus] Paper 20844 (GrInAdapt) addresses retinal vessel segmentation but does not validate projection map priors for translation.
- **Break condition:** If projection maps are heavily corrupted or misaligned with 3D volumes, structural guidance introduces noise.

## Foundational Learning

- **Concept: Vector Quantization (VQ-VAE)**
  - **Why needed here:** MuTri relies on discretizing continuous features into a finite codebook. Without understanding VQ (nearest-neighbor lookup, commitment loss, codebook learning), you cannot diagnose codebook collapse or low utilization.
  - **Quick check question:** Given a feature vector u ∈ R^d and codebook Z ∈ R^(N×d), write the quantization operation and explain why stop-gradient is needed for the commitment loss.

- **Concept: Contrastive Learning (InfoNCE)**
  - **Why needed here:** MuTri adapts contrastive learning across tasks (reconstruction vs. translation) rather than within a single task. Understanding positive/negative pair construction and temperature scaling is essential for debugging alignment losses.
  - **Quick check question:** In Eq. 7, why are patches at different spatial locations (m≠i or n≠j) used as negatives? What happens if all patches are similar?

- **Concept: 3D Medical Image Translation**
  - **Why needed here:** OCT→OCTA translation requires handling volumetric data, depth information, and domain gaps between structural (OCT) and functional (OCTA) modalities.
  - **Quick check question:** Why might a 2D translation model (e.g., Pix2Pix) fail to capture vessel depth information compared to a 3D model?

## Architecture Onboarding

- **Component map:** E_oct2octa -> Quantize -> D_oct2octa (translation VQ-VAE); E_oct -> D_oct (pre-trained OCT VQ-VAE); E_octa -> D_octa (pre-trained OCTA VQ-VAE)
- **Critical path:**
  1. Pre-train Stage 1 VQ-VAEs on OCT and OCTA reconstruction (verify reconstruction quality before proceeding)
  2. Freeze Stage 1 models; train Stage 2 VQ-VAE with multi-view alignment losses
  3. Monitor codebook utilization rate (Fig. 1c) — low utilization indicates collapse
  4. Inference: Single forward pass through E_oct2octa -> quantize -> D_oct2octa
- **Design tradeoffs:**
  - **Codebook size (N):** Larger N increases capacity but risks underutilization; smaller N constrains expressiveness. Paper uses N=8192 (implied from "1, 2, k...").
  - **Patch size (S):** Smaller patches increase contrastive granularity but raise memory/computation. Paper uses S not explicitly stated; ablation suggests sensitivity to τ and λ, not S.
  - **Temperature (τ):** Lower τ sharpens contrastive distribution but may cause gradient instability. Paper sets τ=0.1.
  - **Loss weight (λ):** Balances VQ-VAE reconstruction vs. alignment losses. Paper sets λ=0.5.
- **Failure signatures:**
  - **Codebook collapse:** Low utilization rate (vanilla VQ-VAE in Fig. 1c). Fix: Verify alignment losses are active; check gradient flow through projection heads.
  - **Blurred vessels:** Missing fine structure in OCTA output. Fix: Increase L_proj weight or verify pre-trained OCTA model reconstructs sharp projection maps.
  - **Domain gap not bridged:** Output resembles OCT, not OCTA. Fix: Check that L_OCTA aligns quantized features; verify pre-trained OCTA model is not undertrained.
- **First 3 experiments:**
  1. **Reproduce Stage 1 reconstruction quality:** Train OCT and OCTA VQ-VAEs separately; report PSNR/SSIM on held-out volumes. If reconstruction fails (<25 dB PSNR), Stage 2 guidance is unreliable.
  2. **Ablate alignment losses:** Train Stage 2 with (a) no alignment, (b) CSA only, (c) CSA only, (d) both. Compare codebook utilization and translation metrics (MAE, PSNR, SSIM). Table 3 provides reference points.
  3. **Generalization to new dataset:** Train on OCTA-3M, test on OCTA-6M without fine-tuning. Report metric degradation. This tests whether codebook and alignments transfer across field-of-view changes (3mm vs. 6mm).

## Open Questions the Paper Calls Out
- **Question:** Can the multi-view tri-alignment framework be effectively generalized to other 3D medical image translation tasks with distinct domain gaps, such as CT-to-MRI translation?
- **Basis in paper:** [explicit] The conclusion states, "We believe our solution can inspire other 3D medical translation tasks, i.e., CT-to-MRI," suggesting this as a direct extension of the work.
- **Why unresolved:** The method is currently validated exclusively on retinal OCT/OCTA data, which features specific volumetric structures and vascular contrasts that differ significantly from the soft tissue contrasts found in CT and MRI.
- **Evidence:** Applying MuTri to a public CT-MRI dataset (e.g., BraTS) and demonstrating superior performance over continuous-space methods would resolve this.

## Limitations
- Key hyperparameters (codebook size N, patch size S, projection head architecture) are not specified, limiting reproducibility
- Cross-domain generalization beyond OCT/OCTA is not validated despite claims of broader applicability
- The mechanism by which contrastive alignment improves codebook utilization lacks direct empirical validation through feature visualization

## Confidence
- **High Confidence:** The discrete VQ formulation and two-stage training approach are clearly specified and reproducible. The quantitative improvements (MAE, PSNR, SSIM) over baselines are directly reported with clear metrics.
- **Medium Confidence:** The mechanism claims (especially contrastive semantic alignment and vessel structure alignment) are supported by ablation studies but lack direct feature-level validation. The transfer learning assumption for pre-trained reconstruction models is plausible but not empirically tested.
- **Low Confidence:** Claims about preventing overfitting to scanning artifacts and improving generalization across field-of-view changes are asserted but not systematically validated with cross-domain experiments.

## Next Checks
1. **Codebook Utilization Analysis:** Measure unique codeword usage during Stage 2 training with and without alignment losses. Verify that CSA and VSA prevent collapse to a small subset of codewords, and compare utilization rates to the reported 50%+ in Figure 1c.
2. **Feature Alignment Verification:** Visualize feature distributions from translation model and pre-trained reconstruction models using t-SNE or UMAP. Confirm that contrastive alignment brings features closer in embedding space and that this correlates with improved translation quality.
3. **Cross-Dataset Generalization Test:** Train on OCTA-3M and OCTA-6M separately, then test on OCTA2024 without fine-tuning. Measure degradation in MAE, PSNR, SSIM to validate whether codebook and alignment components generalize to different acquisition parameters (256³ vs 304×304×640).