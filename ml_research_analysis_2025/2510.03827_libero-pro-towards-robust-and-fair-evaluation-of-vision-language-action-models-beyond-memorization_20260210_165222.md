---
ver: rpa2
title: 'LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models
  Beyond Memorization'
arxiv_id: '2510.03827'
source_url: https://arxiv.org/abs/2510.03827
tags:
- task
- bowl
- position
- plate
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a critical flaw in the widely used LIBERO
  benchmark for evaluating Vision-Language-Action (VLA) models: the evaluation tasks
  are nearly identical to training tasks, differing only by minor initial state perturbations.
  This setup allows models to achieve high scores through memorization rather than
  genuine task understanding or environmental perception.'
---

# LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization

## Quick Facts
- **arXiv ID:** 2510.03827
- **Source URL:** https://arxiv.org/abs/2510.03827
- **Reference count:** 5
- **Primary result:** Standard VLA models achieve >90% on LIBERO but collapse to 0.0% under LIBERO-PRO's generalized perturbations

## Executive Summary
This paper identifies a critical flaw in the widely used LIBERO benchmark for evaluating Vision-Language-Action (VLA) models: the evaluation tasks are nearly identical to training tasks, differing only by minor initial state perturbations. This setup allows models to achieve high scores through memorization rather than genuine task understanding or environmental perception. To address this, the authors introduce LIBERO-PRO, a systematic extension of LIBERO that evaluates model performance under controlled perturbations across four dimensions: object attributes, initial positions, task instructions, and environments. Extensive experiments reveal that while state-of-the-art VLA models achieve over 90% accuracy under standard LIBERO evaluation, their performance collapses to 0.0% under LIBERO-PRO's generalized setting. The findings demonstrate that these models rely heavily on memorizing action sequences and environment layouts rather than understanding tasks, even persisting with incorrect actions when objects are replaced or instructions are corrupted. The authors call for adopting more rigorous evaluation standards like LIBERO-PRO to ensure reported progress reflects genuine generalization and comprehension capabilities.

## Method Summary
LIBERO-PRO systematically extends the LIBERO benchmark by introducing controlled perturbations across four dimensions: object attributes (color/texture changes), initial positions (spatial rearrangements), task instructions (semantic paraphrases and task modifications), and environments (background substitution). The evaluation framework maintains task feasibility while probing different aspects of generalization. Models are tested on four LIBERO task suites (libero-goal, libero-spatial, libero-10, libero-object) under each perturbation type using pre-trained checkpoints from OpenVLA, pi0, and pi0.5. Success rate is measured by binary goal predicate over 50 episodes per task, comparing performance between original LIBERO and LIBERO-PRO across perturbation dimensions.

## Key Results
- State-of-the-art VLA models achieve >90% success on standard LIBERO evaluation
- Performance collapses to 0.0% under LIBERO-PRO's position perturbations for OpenVLA and pi0
- Pi0.5 maintains non-zero success (0.38) under position perturbations, revealing architecture-specific differences
- Models execute identical action trajectories under instruction corruption ("xxx", nonsense tokens)
- Objects replacement (e.g., target apple replaced with alphabet soup) still triggers same grasping actions

## Why This Works (Mechanism)

### Mechanism 1
Standard LIBERO's train-test overlap allows models to achieve inflated scores through memorization rather than genuine task understanding. Evaluation tasks mirror training tasks exactly (same instructions, objects, environments), differing only in minor initial state perturbations below perceptual threshold (dTV(p0, p′0) ≤ ε, ε ≪ 1), enabling policies to succeed via trajectory recall. Core assumption: Models store explicit action-sequence mappings indexed by visual-linguistic context rather than learning compositional task representations.

### Mechanism 2
LIBERO-PRO's four-dimensional perturbation framework systematically reveals generalization failures by breaking specific memorization pathways. Controlled perturbations across object attributes, initial positions, instructions, and environments each probe different aspects of generalization while maintaining task feasibility. Core assumption: Genuine task understanding requires binding language to object semantics, adapting actions to spatial configurations, and composing learned subskills—not just pattern matching.

### Mechanism 3
Models execute identical action trajectories regardless of instruction content or object presence, revealing instruction-agnostic visual pattern completion. The policy network appears to use language as weak conditioning signal while primarily relying on visual scene matching to retrieve cached action sequences—evidenced by unchanged outputs given nonsense tokens or missing target objects. Core assumption: VLA architectures may not implement genuine language-conditioned action generation but rather visual-to-action mapping with language as auxiliary context.

## Foundational Learning

- **Vision-Language-Action (VLA) Integration**
  - Why needed here: Understanding that VLA models claim to unify perception, language understanding, and motor control in single policy πθ(a|s, ℓ), making evaluation of genuine integration critical.
  - Quick check question: Can you explain why achieving 90% success on LIBERO does not prove a model understands the relationship between instruction "pick up the red cup" and the visual scene?

- **Distribution Shift and Out-of-Distribution (OOD) Generalization**
  - Why needed here: LIBERO-PRO tests whether models generalize beyond training distribution; understanding that small perturbations create meaningful OOD scenarios is essential for interpreting results.
  - Quick check question: Why does moving an object's initial position by 0.2 units in simulation constitute a valid generalization test rather than just noise?

- **Memorization vs. Compositional Generalization**
  - Why needed here: The paper's core thesis hinges on distinguishing rote trajectory recall from compositional reasoning (combining known objects/actions in novel configurations).
  - Quick check question: If a model trained on "pick apple" and "pick banana" fails at "pick orange" (all fruit in same positions), is this a memorization failure or compositional failure?

## Architecture Onboarding

- **Component map:**
  Standard LIBERO: [Training Tasks] → [Minor p0 perturbation] → [Evaluation]
                    ↑ Nearly identical ↓
                    
  LIBERO-PRO: [Training Tasks] → [Perturbation Engine]
                                  ↓
              ┌───────────────────┼───────────────────┐
              ↓                   ↓                   ↓
         [Object Attr]      [Position]         [Instruction]
         (color/texture)    (spatial)          (semantic/task)
              ↓                   ↓                   ↓
              └───────────────────┼───────────────────┘
                                  ↓
                           [Environment]
                           (background)
                                  ↓
                           [Evaluation]

- **Critical path:** When integrating LIBERO-PRO into your evaluation pipeline: (1) Load LIBERO-PRO perturbation configs from https://github.com/Zxy-MLlab/LIBERO-PRO, (2) Apply perturbations to simulation environment before episode initialization, (3) Run 50 episodes per task per perturbation type, (4) Compare success rates against baseline LIBERO scores—look for gaps >50% as memorization signal.

- **Design tradeoffs:**
  - Perturbation magnitude (δk): Too conservative fails to expose memorization; too aggressive makes tasks infeasible. Authors use neighborhood constraints but optimal values may vary by task complexity.
  - Task variation combination: Restricted from combining with other perturbations to avoid infeasibility—limits comprehensive stress testing but maintains interpretability.
  - Episode count (50): Sufficient for statistical significance on binary success metric but may miss rare success modes in partially-generalizing models.

- **Failure signatures:**
  - Position perturbation collapse (0% success at >0.2 displacement) → spatial reasoning deficit
  - Identical trajectories under instruction corruption → language not grounded
  - Success on object appearance change but failure on object replacement → visual similarity matching, not semantic understanding
  - Compositional task failure (can do A and B separately but not A-then-B) → lack of temporal/action composition

- **First 3 experiments:**
  1. Baseline sanity check: Replicate the 90%+ → 0% collapse on your model using LIBERO-PRO's position perturbation at 0.2 displacement on libero-object suite to confirm memorization vulnerability.
  2. Instruction corruption probe: Test your model with progressively corrupted instructions (paraphrase → synonym swap → random word substitution → nonsense tokens) to measure language grounding depth.
  3. Object generalization boundary: Incrementally replace target objects with increasingly dissimilar items (same category → different category → abstract shapes) to identify the visual similarity threshold where trajectory replay breaks down.

## Open Questions the Paper Calls Out

### Open Question 1
What specific architectural or training data characteristics allow Pi0.5 to maintain non-zero success rates (0.38) under position perturbations where other SOTA models collapse to 0.0%? The authors note that while "most models fail," Pi0.5 achieves a 0.38 success rate in the libero-goal task under position changes, compared to 0 for OpenVLA and Pi0, "revealing differences masked by standard benchmark scores." The paper identifies the performance discrepancy but does not ablate whether this resilience stems from Pi0.5's flow-matching architecture, larger pre-training data scale, or different policy representation.

### Open Question 2
How can VLA training objectives be modified to enforce semantic grounding and prevent models from executing actions when instructions are replaced with meaningless tokens? Section 5.4 demonstrates that models produce identical action trajectories even when given "meaningless input (e.g., 'xxx')" or corrupted instructions, indicating a reliance on visual memorization rather than language understanding. The current standard Supervised Fine-Tuning (SFT) paradigm appears to encourage bypassing the language encoder when visual cues are strong enough, but the paper does not propose a solution to force linguistic dependency.

### Open Question 3
Does high performance on LIBERO-PRO's controlled perturbations correlate with improved robustness in physical real-world robotic manipulation? The authors argue that standard benchmarks fail to assess "capabilities required for practical use" and introduce perturbations to simulate "real-world situations," yet all reported results remain within the LIBERO simulation environment. While LIBERO-PRO exposes memorization in simulation, it remains unverified whether the specific perturbations are the primary failure modes for these models when deployed on physical hardware.

## Limitations

- **Limited perturbation combinations:** The paper restricts perturbations from combining with other variations to avoid infeasibility, limiting comprehensive stress testing.
- **Simulation-only evaluation:** All results remain within the LIBERO simulation environment, leaving unverified whether identified memorization failures transfer to physical robot deployment.
- **Binary success metric:** Focus on success rate may miss nuanced aspects of model behavior such as partial task completion or adaptive failure modes.

## Confidence

- **High confidence:** The identification of near-identical train-test overlap in standard LIBERO (Section 3.2) is well-supported by formal definitions and empirical evidence.
- **Medium confidence:** The claim that VLA models produce identical trajectories under instruction corruption (Section 5.4) is supported by Figure 1 and textual evidence, though broader testing across more models would strengthen this.
- **Medium confidence:** The systematic perturbation framework (Section 4.1) is well-specified formally, but the exact perturbation magnitudes and sampling procedures contain implementation details not fully disclosed.

## Next Checks

1. **Magnitude sensitivity analysis:** Systematically vary perturbation δk across multiple orders of magnitude to identify the exact threshold where memorization breaks down versus genuine task infeasibility.
2. **Compositional generalization test:** Evaluate models on novel task compositions (e.g., "pick apple then place in bowl" when only trained on individual subtasks) to distinguish between memorization and compositional reasoning failures.
3. **Failure mode characterization:** Analyze not just binary success rates but trajectory similarity metrics, action distributions, and intermediate state progressions to understand whether models show any adaptive behavior or complete rigidity under perturbations.