---
ver: rpa2
title: Evader-Agnostic Team-Based Pursuit Strategies in Partially-Observable Environments
arxiv_id: '2511.05812'
source_url: https://arxiv.org/abs/2511.05812
tags:
- evader
- pursuer
- online
- team
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses pursuit-evasion in urban environments where
  two UAVs with asymmetric capabilities must intercept an unknown evader under partial
  observability. The key challenge is adapting strategies without prior knowledge
  of the evader's behavior, start/goal locations, or environment layout.
---

# Evader-Agnostic Team-Based Pursuit Strategies in Partially-Observable Environments

## Quick Facts
- arXiv ID: 2511.05812
- Source URL: https://arxiv.org/abs/2511.05812
- Reference count: 16
- Two-phase neuro-symbolic approach using level-k reasoning and online classification achieves 28% win rate improvement against unknown evaders compared to static policies.

## Executive Summary
This paper addresses pursuit-evasion in urban environments where two UAVs with asymmetric capabilities must intercept an unknown evader under partial observability. The key challenge is adapting strategies without prior knowledge of the evader's behavior, start/goal locations, or environment layout. The authors propose a two-phase neuro-symbolic approach based on bounded rationality: offline hierarchical policy training using level-k reasoning creates a library of best-response strategies, while an online classifier dynamically selects the appropriate counter-strategy based on real-time observation histories.

## Method Summary
The approach trains a hierarchy of pursuer-evader policy pairs using level-k reasoning, where each level k policy is trained against a level k-1 opponent. This creates K+1 pursuer-evader policy pairs in the offline phase. During online deployment, a classifier observes pursuer-evader interaction histories and selects the best-response policy for the current evader. The heterogeneous UAV team consists of a High-Level Pursuer (HLP) operating above obstacles with larger FOV for detection, and a Low-Level Pursuer (LLP) operating at evader altitude for interception. Information sharing via communication enables coordinated search-then-track behavior.

## Key Results
- Offline phase policies perform well against trained opponents (e.g., π^(0) pursuer wins 80% vs π^(0) evader) but poorly against untrained ones (π^(0) pursuer wins only 3% vs π^(1) evader)
- Online classification initially struggles due to policy switching mid-episode but shows promise: in mismatched scenarios, online deployment improves performance (π^(0) pursuer vs π^(1) evader: 3%→28% win rate)
- The classifier achieves 98% accuracy on static data but drops to ~28% early in online episodes
- Results suggest that with improved classifier training on policy-switching trajectories, the approach could significantly outperform static policies in unknown-evader settings

## Why This Works (Mechanism)

### Mechanism 1: Level-k Hierarchical Policy Training
Training pursuer policies against progressively more sophisticated adversarial evaders creates a reusable library of best-response strategies. Level-k reasoning assigns agents reasoning levels where level-k agents optimize against level-(k-1) opponents. The system trains π^(i)_HLP, π^(i)_LLP as responses to π^(i)_EDR, while π^(i)_EDR responds to π^(i-1)_HLP, π^(i-1)_LLP, creating a recursive hierarchy of strategic sophistication via POMDP-based deep RL. Core assumption: Evader behaviors encountered in deployment fall within or near the trained level-k distribution; opponents exhibit bounded rationality rather than arbitrarily novel strategies. Performance degrades sharply when facing opponents outside trained levels (Table I shows 3% win rate when level-0 pursuers face level-1 evader in offline mode).

### Mechanism 2: Online Opponent Classification for Adaptive Response
Real-time classification of evader sophistication enables dynamic policy switching to deploy the appropriate counter-strategy. A classifier C_t maps observation histories (h_HLP(t), h_LLP(t)) to discrete level predictions {0, 1, ..., K}. Upon classification, the system deploys the corresponding best-response policy pair trained in the offline phase. Core assumption: Observation histories contain sufficient discriminative signal to distinguish evader levels before critical decisions are made; classification latency is acceptable relative to game dynamics. Classification accuracy drops during online deployment due to policy-switching artifacts not represented in training data (Fig. 2 shows delayed convergence).

### Mechanism 3: Heterogeneous Altitude-Based Role Differentiation
Assigning complementary sensing and interception roles to UAVs at different altitudes exploits environmental structure for coverage and capture. The High-Level Pursuer (HLP) operates above obstacles with larger FOV for detection; the Low-Level Pursuer (LLP) operates at evader altitude for interception. Information sharing via communication enables coordinated search-then-track behavior. Core assumption: Urban environment contains viewability asymmetries (e.g., foliage occludes HLP but not LLP) that justify role specialization; communication is reliable. If evader exploits altitude-specific blind spots or communication fails, coordination breaks down; paper does not test these failure modes.

## Foundational Learning

- **Partially Observable Markov Decision Processes (POMDPs)**: Agents receive only observations within FOV; policies must condition on observation histories h(t) rather than full state. Quick check: Can you explain why a memoryless policy fails when the evader can move behind buildings?

- **Behavioral Game Theory / Level-k Reasoning**: Provides the theoretical framework for constructing the hierarchy of fictitious evaders and corresponding pursuer responses. Quick check: What does a level-2 agent assume about its opponent's reasoning level?

- **Deep Reinforcement Learning for Multi-Agent Systems**: Trains policies π_HLP, π_LLP, π_EDR in adversarial settings; must handle non-stationarity from co-evolving opponents. Quick check: Why does training against a fixed opponent policy risk overfitting?

## Architecture Onboarding

- **Component map**: Offline Trainer -> Policy Library -> Classifier C_t -> Online Selector -> Simulator
- **Critical path**: 1) Define level-0 evader (random or heuristic policy) 2) Train level-0 pursuers against level-0 evader → store policies 3) Train level-1 evader against level-0 pursuers → store evader policy 4) Train level-1 pursuers against level-1 evader → store policies 5) Collect trajectory data across all level matchups for classifier training 6) Train classifier on (h_HLP, h_LLP) → level label 7) Deploy online: classify → select policy → repeat
- **Design tradeoffs**: More levels (higher K) covers more sophisticated opponents but increases training cost and classifier complexity; paper only tests K=2. Classification frequency: frequent switching enables faster adaptation but introduces instability; paper notes switching artifacts hurt accuracy. Training data for classifier: including policy-switching trajectories would improve robustness but requires additional collection
- **Failure signatures**: Low win rate against mismatched levels (e.g., 3% for level-0 pursuers vs. level-1 evader): Offline policy generalization failure. Delayed classification convergence: Real-time classifier trained on non-switching histories fails to generalize. Performance drop when starting with correct policy (Table I: 80%→80% no gain, 60%→45% loss): Policy switching introduces regret even when initial guess is correct
- **First 3 experiments**: 1) Baseline replication: Train level-0 and level-1 policies; verify Table I metrics (win rate, first-seen time, FOV percentage) against reported values 2) Classifier ablation: Test classifier with training data that includes policy-switching trajectories; measure if online accuracy gap closes 3) Robustness probe: Evaluate performance against an evader with intermediate or extrapolated behavior (e.g., level-0.5 or level-2) to assess out-of-distribution generalization

## Open Questions the Paper Calls Out

Can the online classifier be trained to maintain high accuracy when pursuer policies switch mid-episode? Authors state "Ongoing efforts include extending this module to correctly classify an evader in the presence of policy switching to improve performance" and note the classifier was trained on datasets "where the pursuers do not switch policies in the middle of the episode, which occurs in the online implementation." The distribution shift caused by policy switching during deployment creates a train-test mismatch that degrades real-time classification. A classifier trained on trajectories with policy switching that achieves comparable accuracy (>90%) in both training and online deployment would resolve this.

Does the level-k approach generalize to evaders whose strategies fall outside the trained hierarchy? Results show pursuer performance drops dramatically (80%→3%, 60%→10%) when facing untrained evader levels, raising concerns about robustness to novel strategies. The approach assumes evaders can be mapped to discrete levels; real adversaries may use strategies that don't fit this taxonomy. Evaluation against evaders with qualitatively different policies (e.g., learned via different algorithms, or hybrid strategies) showing maintained win rates would resolve this.

Can the approach scale computationally to k>2 while maintaining tractable training and online inference? The paper tests "up to two levels of agents" but provides no analysis of how training time, sample complexity, or classifier performance scale with increasing k. Each additional level requires training new policies against all previous levels, potentially creating combinatorial growth in training requirements. Empirical scaling curves showing training time and performance as k increases to 4-5 levels would resolve this.

## Limitations
- Performance degrades sharply when facing opponents outside trained levels (3% win rate for level-0 pursuers vs level-1 evader)
- Online classification system struggles with policy-switching artifacts not represented in training data
- Approach untested beyond K=2 levels and lacks robustness evaluation for communication failures or evader strategies exploiting altitude-specific blind spots

## Confidence

- **High confidence**: The hierarchical level-k training framework is theoretically sound and well-established in behavioral game theory. The offline training methodology and basic pursuit-evasion task formulation are clearly specified.
- **Medium confidence**: The neuro-symbolic integration of level-k reasoning with deep RL is innovative but untested beyond the specific implementation details. Results show the approach can outperform static policies but with significant variance depending on opponent matching.
- **Low confidence**: Real-world applicability is limited by the assumption of reliable communication, the absence of evader policy robustness testing, and the lack of evaluation against novel or hybrid strategies outside the trained level distribution.

## Next Checks

1. **Cross-level generalization test**: Evaluate performance against an evader trained with intermediate (e.g., level 0.5) or extrapolated (e.g., level 2) strategies to quantify out-of-distribution robustness and identify the practical limits of the level-k hierarchy.

2. **Classifier robustness evaluation**: Retrain the classifier with trajectory data that includes policy-switching events and measure if the online accuracy gap (98% training → ~28% early online) closes significantly.

3. **Failure mode analysis**: Systematically test the impact of communication loss between HLP and LLP, and evaluate evader strategies that specifically target altitude-based blind spots to identify coordination vulnerabilities.