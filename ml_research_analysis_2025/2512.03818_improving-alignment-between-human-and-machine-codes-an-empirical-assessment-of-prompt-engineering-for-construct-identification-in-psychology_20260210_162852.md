---
ver: rpa2
title: 'Improving Alignment Between Human and Machine Codes: An Empirical Assessment
  of Prompt Engineering for Construct Identification in Psychology'
arxiv_id: '2512.03818'
source_url: https://arxiv.org/abs/2512.03818
tags:
- prompt
- performance
- baseline
- few-shot
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents an empirical framework for optimizing large\
  \ language models (LLMs) to classify psychological constructs in text. The authors\
  \ evaluate five prompting strategies\u2014human-generated prompts, automatic prompt\
  \ engineering, persona patterns, chain-of-thought reasoning, and explanations\u2014\
  across three psychological constructs (gratitude, negative core beliefs, and positive\
  \ meaning making) and two LLM models (GPT-4 and Llama-3.3)."
---

# Improving Alignment Between Human and Machine Codes: An Empirical Assessment of Prompt Engineering for Construct Identification in Psychology

## Quick Facts
- arXiv ID: 2512.03818
- Source URL: https://arxiv.org/abs/2512.03818
- Reference count: 12
- Primary result: Optimized prompt engineering achieves up to 75% improvement in F1 score for psychological construct classification

## Executive Summary
This paper presents an empirical framework for optimizing large language models (LLMs) to classify psychological constructs in text. The authors evaluate five prompting strategies—human-generated prompts, automatic prompt engineering, persona patterns, chain-of-thought reasoning, and explanations—across three psychological constructs (gratitude, negative core beliefs, and positive meaning making) and two LLM models (GPT-4 and Llama-3.3). They find that the most influential features of a prompt are the construct definition, task framing, and examples provided, while persona, chain-of-thought, and explanations offer minimal additional benefit. Across all constructs and models, the best-performing prompts combined codebook-guided empirical prompt selection with automatic prompt engineering and few-shot examples, achieving up to 75% improvement in F1 score for high-inference constructs.

## Method Summary
The authors evaluate five prompting strategies for psychological construct classification: (1) human-generated baseline prompts combining randomized construct definitions, task instructions, and inclusion/exclusion criteria; (2) automatic prompt engineering using meta-prompts to generate variants; (3) persona patterns; (4) chain-of-thought reasoning; and (5) explanations. They test these across three psychological constructs (gratitude, negative core beliefs, positive meaning making) using Reddit posts and clinical writing samples. The dataset is split 25% train, 50% development, 25% test at participant level. Performance is measured using F1 score, precision, recall, and accuracy with bootstrapped confidence intervals.

## Key Results
- Baseline prompt quality (definition and framing) is the dominant factor, with performance gaps up to 0.33 F1 points between poorly and well-constructed prompts
- Few-shot examples can compensate for poor baseline prompts, achieving gains comparable to selecting a better baseline
- Persona patterns, chain-of-thought, and explanations provide minimal systematic benefit beyond baseline optimization and few-shot examples
- Best performance achieved through combination of codebook-guided empirical selection, automatic prompt engineering, and few-shot examples

## Why This Works (Mechanism)

### Mechanism 1: Baseline Prompt Quality Dominates Classification Alignment
- Claim: Construct definition and task framing account for the largest variance in human-LLM code alignment, with performance gaps up to 0.33 F1 points between poorly and well-constructed baseline prompts.
- Mechanism: LLMs condition output probabilities on prompt tokens; the baseline prompt establishes the semantic context that determines which patterns in pretraining data become relevant. Well-specified definitions activate appropriate linguistic associations while excluding irrelevant ones.
- Core assumption: The construct's theory-driven definition maps onto representational patterns captured during pretraining.
- Evidence anchors: [abstract] "The most influential features of a prompt are the construct definition, task framing, and, to a lesser extent, the examples provided." [Results] "This difference reaches as high as 0.33 for classifications of meaning-making with Llama 3.3 (increasing F1 from 0.33 to 0.66)."

### Mechanism 2: Few-Shot Examples Compensate for Prompt Deficiency
- Claim: Adding empirically-selected examples to poorly-performing baseline prompts yields improvements comparable to selecting a better baseline, but provides marginal gains when added to already-effective prompts.
- Mechanism: Examples provide in-context demonstrations that establish decision boundaries beyond what the verbal definition captures. They supply task-specific knowledge through pattern completion rather than explicit instruction.
- Core assumption: Selected examples are representative of the target distribution and do not introduce systematic bias.
- Evidence anchors: [abstract] "Combined codebook-guided empirical prompt selection with automatic prompt engineering and few-shot examples, achieving up to 75% improvement in F1 score for high-inference constructs." [Results] "In five out of six construct–model combinations, the bottom baseline paired with few-shot examples performs as well as—or slightly better than—the top zero-shot baseline prompt."

### Mechanism 3: Additive Techniques (Persona, Chain-of-Thought, Explanations) Provide Marginal Value
- Claim: Persona patterns, chain-of-thought reasoning, and explanations do not systematically improve performance beyond the gains from baseline optimization and few-shot examples.
- Mechanism: These techniques were designed for generation tasks; in classification, they add tokens that may dilute task-relevant signals without addressing core definition/framing issues. Improvements occur only when applied to low-performing baselines.
- Core assumption: Classification tasks require different prompting strategies than generation tasks.
- Evidence anchors: [abstract] "Persona, chain-of-thought, and explanations offer minimal additional benefit." [Results] "When applied to top-performing baseline prompts, persona additions yield minimal to no improvement."

## Foundational Learning

- Concept: **Baseline Prompt Components** (definition, task instruction, inclusion/exclusion criteria)
  - Why needed here: Understanding what constitutes a "baseline" versus "additive" techniques is essential for prioritization.
  - Quick check question: Can you distinguish between modifying the construct definition versus adding a persona phrase?

- Concept: **Few-Shot vs. Zero-Shot Classification**
  - Why needed here: The paper's central recommendation combines empirical baseline selection with few-shot examples.
  - Quick check question: How many labeled examples does the paper recommend for prompt engineering (not validation)?

- Concept: **Train/Dev/Test Splits for Prompt Selection**
  - Why needed here: Preventing overfitting to a particular prompt configuration requires held-out evaluation.
  - Quick check question: What proportion of data did the authors allocate to each split?

## Architecture Onboarding

- Component map: Codebook → generates definition variants, task instruction variants, inclusion/exclusion criteria → Prompt generator → combines components into candidate prompts → Automatic prompt engineering loop → LLM generates variants, evaluated against training F1 → Example selector → samples few-shot combinations from training pool → Evaluator → computes F1, precision, recall with bootstrapped confidence intervals

- Critical path: 1. Generate ≥50 baseline variants from codebook components 2. Evaluate all on training set; select top and bottom performers 3. Run automatic prompt engineering (5 generations × 5 variants each) 4. Test few-shot combinations (50 samples) 5. Compare on development set; validate final selection on held-out test set

- Design tradeoffs: More prompt variants → better coverage but higher API costs; Larger dev set → more reliable comparisons but less training data; Chain-of-thought/explanations → longer completions, higher cost, uncertain benefit

- Failure signatures: Bottom baseline + few-shot outperforming top baseline zero-shot indicates examples matter more than prompt wording; Automatic prompt engineering reducing performance (observed in meaning-making with Llama) suggests seed quality matters; High variance across few-shot combinations (Appendix A) indicates example selection is unstable

- First 3 experiments: 1. Generate 50 baseline variants; compare top vs. bottom F1 on training data to quantify prompt wording impact 2. Add 5–10 randomly selected few-shot examples to the bottom baseline; measure whether gains approach top baseline performance 3. Apply automatic prompt engineering to the top baseline; confirm whether iteration improves or degrades performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the empirical prompt optimization framework be effectively adapted for low-resource settings with significantly fewer than 50 labeled texts?
- Basis in paper: [explicit] The authors note their recommended procedure "only apply to scenarios where a reasonable number of labelled texts (say 50 to 100) can be set aside" (Page 17).
- Why unresolved: The study utilized datasets with hundreds of labeled examples for training and validation; the stability of the optimization process with minimal data remains untested.
- What evidence would resolve it: Experiments replicating the prompt selection pipeline using training sets of diminishing sizes (e.g., n=10, 20, 30) to identify performance thresholds.

### Open Question 2
- Question: Does the effectiveness of specific prompt engineering strategies (e.g., persona patterns) interact with the demographic characteristics of the text authors?
- Basis in paper: [explicit] The authors acknowledge that because all clinical data "was generated by women," results are threatened if "persona prompts work better or worse for women’s writing compared to men" (Page 7).
- Why unresolved: The homogeneity of the sample (all women) prevented the authors from testing if prompt efficacy is moderated by author demographics.
- What evidence would resolve it: A comparative analysis of prompt performance across datasets stratified by author gender or other demographic markers.

### Open Question 3
- Question: Will the relative inefficacy of "additive techniques" (chain-of-thought, explanations) persist as model architectures and reasoning capabilities evolve?
- Basis in paper: [explicit] The authors warn that "model capabilities are certain to change over time," potentially limiting the generalizability of the finding that these techniques offer minimal benefit (Page 17).
- Why unresolved: The current study is a snapshot of specific model versions (GPT-4, Llama-3.3); it is unclear if these null results are artifacts of current model limitations.
- What evidence would resolve it: Longitudinal benchmarking of these additive techniques on successive generations of state-of-the-art LLMs.

## Limitations

- Generalizability across constructs and domains remains uncertain as the framework was tested on only three psychological constructs
- Temporal stability of results is questionable given rapid LLM evolution and model updates
- Reproducibility of automatic prompt engineering is limited, with reported cases of performance degradation

## Confidence

**High Confidence** (supported by systematic comparisons and consistent patterns across multiple experiments):
- Baseline prompt quality (definition and framing) is the dominant factor in classification alignment
- Few-shot examples can compensate for poor baseline prompts, though they add marginal value to good prompts
- Additive techniques (persona, chain-of-thought, explanations) provide minimal systematic benefit

**Medium Confidence** (supported by results but with domain-specific variations):
- The specific components of effective baseline prompts (definition variants, task instructions, inclusion/exclusion criteria)
- The relative performance of different prompting strategies across the three tested constructs

**Low Confidence** (based on limited evidence or single observations):
- Universal applicability of the recommended framework to all psychological constructs
- Long-term stability of the reported performance differences as LLMs evolve

## Next Checks

1. **Cross-Domain Validation**: Apply the framework to psychological constructs from different domains (e.g., clinical diagnosis, personality traits, cognitive processes) to test generalizability. Generate 50 baseline variants for a new construct and compare performance patterns against the three tested constructs.

2. **Temporal Stability Test**: Re-run the automatic prompt engineering experiments on current versions of GPT-4 and Llama models to assess whether the reported performance differences persist. Focus on cases where automatic prompt engineering previously degraded performance.

3. **Example Selection Stability**: Conduct systematic experiments varying the number and selection method of few-shot examples (beyond the 50 random combinations tested). Compare performance across different example selection strategies (e.g., diversity sampling vs. random sampling) and evaluate the stability of improvements across multiple runs.