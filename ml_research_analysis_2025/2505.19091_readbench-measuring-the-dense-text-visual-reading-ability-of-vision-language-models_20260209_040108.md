---
ver: rpa2
title: 'ReadBench: Measuring the Dense Text Visual Reading Ability of Vision-Language
  Models'
arxiv_id: '2505.19091'
source_url: https://arxiv.org/abs/2505.19091
tags:
- answer
- question
- text
- multimodal
- only
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ReadBench, a multimodal benchmark to evaluate
  Vision-Language Models' (VLMs) ability to read and reason about text-rich images.
  It converts text-only benchmarks into multimodal inputs by keeping prompts and questions
  textual while presenting contexts as images of text.
---

# ReadBench: Measuring the Dense Text Visual Reading Ability of Vision-Language Models

## Quick Facts
- arXiv ID: 2505.19091
- Source URL: https://arxiv.org/abs/2505.19091
- Reference count: 16
- Primary result: VLMs universally degrade on multimodal text inputs vs text-only, with performance drops scaling with context length.

## Executive Summary
This paper introduces ReadBench, a multimodal benchmark to evaluate Vision-Language Models' (VLMs) ability to read and reason about text-rich images. It converts text-only benchmarks into multimodal inputs by keeping prompts and questions textual while presenting contexts as images of text. Evaluating state-of-the-art VLMs, the authors find universal performance degradation when comparing multimodal to purely textual inputs. Performance drops are minimal for short contexts but become significant for multi-page inputs, with some models experiencing double-digit percentage decreases. The study also reveals that input resolution has negligible effects on performance, and that degradation is primarily model-specific rather than tied to universal problematic inputs. These findings highlight a gap in VLMs' capabilities for processing visually presented extensive textual content, which is critical for practical applications like visual retrieval-augmented generation.

## Method Summary
ReadBench evaluates VLMs by comparing their performance on text-only benchmarks versus the same benchmarks with context rendered as images. Five datasets are used: MMLU-Redux, MMLU-Pro, and GPQA-Diamond for short contexts (options rendered as images); BABILong and LongBench subsets for long contexts (up to 8k tokens, ~12 pages). Images are rendered on A4 pages in Arial 12pt font at 92.9 PPI, with whitespace cropped. Each sample is evaluated with both text-only and multimodal inputs, averaging over 3 runs. The key metric is the degradation in accuracy when context is presented visually rather than textually.

## Key Results
- All evaluated VLMs show performance degradation when context is presented as images rather than text.
- Degradation is minimal for short contexts but significant for multi-page inputs, with some models experiencing up to 30% relative drops.
- Input resolution (72-300 PPI) has negligible effect on performance.
- Performance drops are model-specific rather than tied to universally problematic inputs.
- The degradation gap highlights limitations in VLMs' ability to process visually presented extensive textual content.

## Why This Works (Mechanism)

### Mechanism 1: Visual Token Compression Scaling Loss
Performance degradation in VLMs scales with visual context length due to information loss during visual token compression. Vision encoders compress images into fixed-dimension token sequences, and as document length increases, the same compression budget must represent exponentially more textual content, creating a semantic bottleneck.

### Mechanism 2: Cross-Modal Grounding Gap
The modality conversion from text-to-vision introduces a grounding gap not tied to visual acuity but to semantic alignment quality. VLMs trained on natural images with incidental text may have weaker grounding for "pure text as image" inputs, as the vision encoder's output may not map to the same semantic space as direct text token embeddings.

### Mechanism 3: Resolution-Independent Attention Dispersion
VLM reading performance is bounded by attention mechanism capacity, not pixel-level feature extraction. Self-attention over visual tokens distributes compute across all tokens, and for multi-page documents, attention weight per token decreases, reducing effective signal for any specific text span.

## Foundational Learning

- **Vision-Language Model (VLM) Architecture**: Understanding the full VLM pipeline—vision encoder, projection layer, and LLM backbone—is essential to diagnose where degradation occurs. *Quick check*: Can you trace how a text-in-image input flows from pixel input through visual tokens to text output?

- **Visual Token Budget and Compression Ratio**: The multi-page degradation finding directly implicates token budget constraints. *Quick check*: If a model uses 256 visual tokens per image and you input 10 pages, how many visual tokens represent the full document?

- **Cross-Modal Embedding Alignment**: The model-specific degradation patterns suggest alignment quality varies by training. *Quick check*: What is the difference between a vision encoder trained with contrastive loss (CLIP-style) vs one fine-tuned with instruction-tuning data?

## Architecture Onboarding

- Component map: Text-only benchmark -> Image Renderer (A4, 12pt Arial, 92.9 PPI) -> VLM Input: [Text prompt] + [Rendered image(s)] -> Vision Encoder → Visual Tokens → Projection Layer -> LLM Backbone → Text Output -> Evaluation (Accuracy vs text-only baseline)

- Critical path: The comparison between multimodal and text-only baselines. All conclusions depend on measuring delta = accuracy(multimodal) - accuracy(text-only).

- Design tradeoffs: Sampling up to 35 examples per subset trades statistical robustness for evaluation cost. Capping long-context at 8k tokens balances realism with practical limits. Keeping prompts text-only mimics VisRAG use cases but may under-test fully multimodal instruction following.

- Failure signatures: Double-digit percentage drops on multi-page inputs indicate severe visual token compression issues. High variance across models on same input suggests training distribution gaps. No improvement at higher resolutions confirms bottleneck is not visual acuity.

- First 3 experiments: 1) Baseline replication: Run ReadBench on target VLM with both text-only and multimodal inputs; compute per-dataset degradation deltas. 2) Resolution ablation: Test a subset at 72, 150, and 300 PPI to confirm resolution-independence. 3) Error analysis on mismatches: Identify which questions cause multimodal/text disagreements; cluster by token length and task type.

## Open Questions the Paper Calls Out

- **Multilingual extension**: The paper explicitly states that future work should extend ReadBench to other languages beyond English, as current evaluations are limited to English benchmarks.

- **Architectural contribution isolation**: The benchmark evaluates models as black boxes and observes that degradation is "model-specific" but does not isolate whether failures stem from visual encoders, attention mechanisms, or projection layers.

- **Complex layout handling**: The paper acknowledges that ReadBench uses plain text images and does not test complex visual document layouts (tables, graphs), which may exacerbate performance degradation compared to native PDFs or documents with visual structure.

## Limitations

- Limited domain coverage to English benchmarks without testing multilingual documents, handwritten text, or highly stylized typography.
- Inference configuration ambiguity with unspecified VLM parameters like temperature, top-p sampling, or max token limits.
- No diagnostic of failure modes - does not analyze whether failures stem from visual token loss, reasoning deficits, or alignment issues in projection layer.

## Confidence

- **High confidence**: Performance degradation on multi-page inputs is well-supported by controlled experiments across five benchmarks and five VLMs.
- **Medium confidence**: The claim that resolution has negligible effects is supported by ablation studies, but does not explore extreme low-resolution scenarios.
- **Low confidence**: The conclusion that degradation is primarily model-specific rather than input-specific is based on Jaccard similarity of mismatched questions but may conflate model-specific weaknesses with task difficulty variance.

## Next Checks

1. **Attention-based error analysis**: For each VLM, extract attention weight distributions over visual tokens for correct vs incorrect multimodal predictions. Correlate token-level attention entropy with degradation magnitude to test whether attention dispersion is the primary bottleneck.

2. **Training distribution audit**: Analyze the proportion of dense text documents vs natural images with incidental text in each VLM's pre-training corpus. Train a proxy VLM on augmented text-image data and re-evaluate ReadBench to measure the impact of training distribution on cross-modal grounding.

3. **Dynamic token budget scaling**: Implement a VLM variant with adaptive visual token allocation (e.g., more tokens for longer documents). Re-run ReadBench long-context subsets to quantify how much degradation is eliminated by scaling the visual token budget proportionally to input length.