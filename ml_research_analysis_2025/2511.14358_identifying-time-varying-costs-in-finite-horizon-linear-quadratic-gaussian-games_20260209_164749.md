---
ver: rpa2
title: Identifying Time-varying Costs in Finite-horizon Linear Quadratic Gaussian
  Games
arxiv_id: '2511.14358'
source_url: https://arxiv.org/abs/2511.14358
tags:
- cost
- policy
- nash
- equilibrium
- trajectories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of identifying time-varying cost
  parameters in finite-horizon linear quadratic Gaussian (LQG) games from given Nash
  equilibrium policies. The authors characterize the set of cost parameters that yield
  a given Nash equilibrium policy using coupled Riccati equations, and propose a backpropagation
  algorithm to compute these parameters.
---

# Identifying Time-varying Costs in Finite-horizon Linear Quadratic Gaussian Games

## Quick Facts
- **arXiv ID:** 2511.14358
- **Source URL:** https://arxiv.org/abs/2511.14358
- **Reference count:** 36
- **Primary result:** Identifies time-varying cost parameters in LQG games from Nash equilibrium policies using backpropagation algorithm with error bounds.

## Executive Summary
This paper addresses the problem of identifying time-varying cost parameters in finite-horizon linear quadratic Gaussian (LQG) games from given Nash equilibrium policies. The authors develop a backpropagation algorithm that solves a constrained least-squares problem defined by coupled Riccati equations to recover the cost parameters. They derive a probabilistic error bound showing that cost identification error scales linearly with policy estimation error and degrades over time due to error accumulation. The method is validated through numerical simulations and multi-vehicle driving scenarios, demonstrating that identified costs can reproduce the original Nash equilibrium policies and trajectories.

## Method Summary
The method involves two main components: first, estimating Nash equilibrium policies from trajectory data using linear regression, and second, applying a backpropagation algorithm to identify time-varying cost parameters. The backpropagation works backward from the final time step, solving constrained least-squares problems at each step using the null space characterization derived from coupled Riccati equations. When trajectory data is available instead of explicit policies, a linear regression step estimates the feedback gains and bias terms before applying the main identification algorithm.

## Key Results
- Successfully identifies time-varying cost parameters from Nash equilibrium policies using backpropagation
- Derives probabilistic error bound showing linear scaling with policy estimation error
- Validated on numerical examples and multi-vehicle driving scenarios with reproduced trajectories
- Error accumulates over time horizon but remains bounded with sufficient demonstrations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Time-varying cost parameters can be identified from a known feedback Nash equilibrium policy by solving a constrained least-squares problem defined by vectorized Riccati equations.
- **Mechanism:** The algorithm inverts the forward game solution. Instead of solving Riccati equations forward (Costs â†’ Policy), it rearranges the coupled Riccati equations into a linear form $M_t^i \theta_t^i = 0$ via vectorization. It then backpropagates from $t=T$ to $t=0$, computing the null space of $M_t^i$ to find valid cost parameters $\theta_t^i$ at each step.
- **Core assumption:** The closed-loop system matrix $F_t = A_t - \sum B_j K_j^*$ is invertible for all time steps (Assumption 1).
- **Evidence anchors:**
  - [abstract] "We characterize the set of cost parameters... using coupled Riccati equations, and propose a backpropagation algorithm."
  - [Section II-A, Proposition 1] Shows the explicit construction of matrix $M_t^i$ and the recursive updates for $\bar{\Delta}_t^i$ and $\bar{\Omega}_t^i$.
  - [corpus] Limited direct mechanism overlap; "Optimal Modified Feedback Strategies" addresses forward game imperfections, not inverse identification.
- **Break condition:** If the active constraint set changes during the least-squares solve (violating Assumption 2), or if $F_t$ is singular, the algorithm fails to find a solution.

### Mechanism 2
- **Claim:** Identification error scales linearly with policy estimation error but accumulates probabilistically as the algorithm recedes in time.
- **Mechanism:** The error bound is derived via sensitivity analysis. Errors in the estimated policy $\tilde{K}$ distort the matrix $M_t^i$. Because the identification of costs at time $t$ depends on the costs at $t+1$, errors propagate backwards through the horizon. The probability of maintaining the error bound decreases multiplicatively: $(1-\delta)^{T-t+1}$.
- **Core assumption:** The active set of constraints in the optimization remains unchanged under perturbation (Assumption 2).
- **Evidence anchors:**
  - [abstract] "Derive a probabilistic error bound showing that cost identification error scales linearly with policy estimation error and degrades over time."
  - [Section III-B, Theorem 1] Explicitly states the probability bound degrades as $t$ decreases from $T$.
  - [corpus] Weak corpus support; neighbors focus on forward game convergence or unrelated learning methods.
- **Break condition:** If the policy estimation error is too large (violating the $O(\epsilon)$ perturbation assumptions in Lemma 3-5), the bound ceases to hold.

### Mechanism 3
- **Claim:** Feedback policies can be estimated from finite trajectory data using linear regression, provided noise is sub-Gaussian.
- **Mechanism:** The paper models the observed input as a linear function of the state plus noise ($u = -Kx - \alpha + \nu$). By collecting $n$ samples, it solves a standard least-squares problem to estimate gain $K$ and bias $\alpha$.
- **Core assumption:** Input observation noise $\nu$ is zero-mean sub-Gaussian.
- **Evidence anchors:**
  - [Section III-A, Eq. 7] Defines the linear regression formulation.
  - [Section III-A, Lemma 1] Cites standard finite-sample bounds for the regression error.
  - [corpus] "Policy Gradient with Self-Attention" offers a contrasting model-free approach, whereas this paper assumes a structured linear model.
- **Break condition:** If inputs are not measured accurately (high noise) or state samples are linearly dependent, $K$ cannot be uniquely identified.

## Foundational Learning

### Concept: Linear Quadratic Gaussian (LQG) Games
- **Why needed here:** This is the underlying dynamical model. You cannot interpret the "coupled Riccati equations" or the structure of the cost function $J_i$ without understanding how linear dynamics, quadratic costs, and Gaussian noise interact in a multi-agent setting.
- **Quick check question:** How does the control input of one agent affect the cost function of another in a general-sum LQG game?

### Concept: Feedback Nash Equilibrium
- **Why needed here:** The paper identifies costs *assuming* the observed behavior is a Feedback Nash Equilibrium. You must understand that this implies agents optimize based on the current state, and no single agent gains by deviating.
- **Quick check question:** Distinguish between open-loop and feedback Nash equilibria; why does the backpropagation algorithm rely on the feedback structure (gain matrices $K_t$)?

### Concept: Inverse Optimization / Inverse Reinforcement Learning
- **Why needed here:** The core problem is "Inverse." Standard control solves for $u$ given $Q, R$. This paper solves for $Q, R$ given $u$. Understanding the ill-posed nature (multiple solutions) of inverse problems explains why the paper uses a "set characterization" and null-space analysis.
- **Quick check question:** Why is the solution to an inverse optimal control problem typically a set of parameters rather than a unique point?

## Architecture Onboarding

### Component map:
Data Preprocessor -> Policy Estimator (Linear Regression) -> Backpropagation Engine -> Cost Validator

### Critical path:
The backward recursion (Step 3 in Algorithm 1). The calculation of $\bar{\Delta}_t$ and $\bar{\Omega}_t$ at time $t$ strictly depends on values from $t+1$. An error or singularity at $t=T$ corrupts all prior $t < T$.

### Design tradeoffs:
- **Sample size vs. Horizon:** Short horizons are easier to identify (less error accumulation per Theorem 1) but may not capture long-term intent.
- **Constraint strictness:** The selector matrix $D$ and threshold $\tau$ in Eq. (6) must be tuned to ensure physical realism (e.g., $R > 0$) without over-constraining the null space solution.

### Failure signatures:
- **Exploding Gradients/Norms:** If $F_t$ is near-singular, the inverse $F_t^{-1}$ blows up.
- **Negative Costs:** If constraints in Eq. (6) are mishandled, the solver might return negative penalties for control effort ($R \not\succ 0$), which is physically implausible.
- **Drift:** Trajectories reconstructed from identified costs diverge significantly from ground truth, typically indicating insufficient demonstration samples (see Fig. 4 comparison).

### First 3 experiments:
1. **Sanity Check (Noiseless):** Generate trajectories from a known LQG game. Feed exact policies into Algorithm 1. Verify if recovered $Q, R$ match ground truth (checking if the problem is well-posed).
2. **Sensitivity Analysis:** Add Gaussian noise to input observations. Measure the deviation in identified costs against the theoretical bound in Theorem 1. Confirm linear scaling.
3. **Singularity Stress Test:** Construct dynamics matrices $A, B$ such that $F_t$ is ill-conditioned. Observe if the regularized least squares (Eq. 6) stabilizes the solution or if the algorithm fails.

## Open Questions the Paper Calls Out
- **Question:** Can the cost identification framework be extended to handle state and input constraints in finite-horizon LQG games?
- **Question:** How can the exponential degradation of the identification confidence over the time horizon be mitigated?
- **Question:** Is it possible to identify time-varying costs when the closed-loop system matrix is singular or near-singular?

## Limitations
- The method requires the closed-loop system matrix to be invertible for all time steps (Assumption 1).
- The probabilistic error bound degrades exponentially over the time horizon.
- The active constraint set must remain unchanged under perturbation (Assumption 2).
- The framework currently applies only to unconstrained LQG games.

## Confidence
- **High Confidence:** The linear regression formulation for policy estimation from trajectories (Mechanism 3) is well-established and the proof methodology is standard.
- **Medium Confidence:** The backpropagation algorithm for cost identification (Mechanism 1) is correctly derived, but practical implementation challenges with constraint solvers and singular matrices are not fully addressed.
- **Medium Confidence:** The error bound analysis (Mechanism 2) follows standard sensitivity analysis, but the probabilistic guarantee may be overly pessimistic in practice.

## Next Checks
1. **Singularity Robustness:** Systematically test Algorithm 1 on LQG games where $F_t$ approaches singularity at various time steps. Measure algorithm failure rates and solution quality degradation.
2. **Constraint Sensitivity:** Vary the threshold parameter $\tau$ and selector matrix $D$ in the constrained least-squares problem. Document how solution uniqueness and physical plausibility of recovered costs change.
3. **Bound Validation:** Compare empirical error scaling with the theoretical bound in Theorem 1 across different noise levels and horizon lengths. Determine if the bound is tight or conservative in practice.