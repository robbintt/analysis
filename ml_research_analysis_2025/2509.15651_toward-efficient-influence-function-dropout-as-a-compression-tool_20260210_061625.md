---
ver: rpa2
title: 'Toward Efficient Influence Function: Dropout as a Compression Tool'
arxiv_id: '2509.15651'
source_url: https://arxiv.org/abs/2509.15651
tags:
- data
- influence
- function
- compression
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach that leverages dropout as
  a gradient compression mechanism to compute the influence function more efficiently.
  The method significantly reduces computational and memory overhead during both the
  influence function computation and the gradient compression process.
---

# Toward Efficient Influence Function: Dropout as a Compression Tool

## Quick Facts
- **arXiv ID:** 2509.15651
- **Source URL:** https://arxiv.org/abs/2509.15651
- **Reference count:** 40
- **Primary result:** Introduces dropout as a gradient compression mechanism for influence functions, achieving comparable or superior performance to Gaussian methods while reducing computational and memory overhead.

## Executive Summary
This paper addresses the computational challenges of influence function estimation for large-scale models by introducing dropout-based gradient compression. Traditional methods either compute expensive full Hessians or use projection-based compression with high memory costs. The authors propose randomly sampling gradient entries instead of projecting them, achieving O(nr) complexity versus O(nrd) for projection methods. Theoretical analysis shows dropout compression has tighter error bounds in high-dimensional settings, and empirical results demonstrate effectiveness on tasks like mislabeled data detection and cross-source identification across models up to 6.9B parameters.

## Method Summary
The method compresses gradients by randomly sampling r entries instead of using dense projection matrices. For each data point, gradients are computed normally, then r entries are randomly selected and stored. The influence function is computed in this compressed space using a Gauss-Newton approximation of the Hessian. The approach eliminates the need for O(rd) memory storage of projection matrices and avoids O(rd) matrix multiplications, achieving O(nr) complexity. The same random indices are used across all data points for consistency. Damping is applied per layer using λ_l = 0.1 × (n·d_l)^(-1) Σᵢ ∇_θl lᵢ^T ∇_θl lᵢ.

## Key Results
- Dropout compression achieves comparable or superior performance to Gaussian projection and LiSSA in mislabeled data detection tasks
- Theoretical error bounds show dropout has O(σ_max(H)) error versus O(d + d²σ_max(H)) for Gaussian methods in high dimensions
- Efficiency gains validated through benchmarks showing O(nr) complexity versus O(nrd) for projection methods
- Successfully scales to 6.9B parameter models while maintaining performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Randomly sampling gradient entries preserves sufficient information to estimate data influence while drastically reducing vector dimensionality.
- **Mechanism:** Instead of projecting gradients into a latent space, this method uses a binary matrix to retain only r random entries of the original gradient, avoiding dense matrix multiplications.
- **Core assumption:** The influence signal is distributed such that coordinate-wise sampling captures the necessary alignment between training and validation gradients, or that critical components of data influence are robust to random coordinate loss.
- **Evidence anchors:** Section 3.1 shows compression by retaining a subset of entries is equivalent to using a binary matrix; Section 3.2 demonstrates reduced dimensionality without additional memory overhead; related works discuss dropout for regularization but not specifically for influence compression.
- **Break condition:** If gradients are extremely dense and every coordinate is critical, random sampling may discard too much signal.

### Mechanism 2
- **Claim:** The theoretical error upper bound of dropout-compressed influence is lower than that of Gaussian projection in high-dimensional settings.
- **Mechanism:** Error analysis shows Dropout error scales as O(σ_max(H)), whereas Gaussian error scales as O(d + d²σ_max(H)). In overparameterized models where d is large, the Dropout bound is theoretically tighter.
- **Core assumption:** The dimension of parameters d exceeds the training dataset size n, and the specific theoretical bounds hold practically in non-asymptotic settings.
- **Evidence anchors:** Section 3.3 Theorem 3.2 shows the Dropout bound is O(σ_max(H)) vs Gaussian's O(d + d²σ_max(H)); abstract states theoretical error bounds show the error upper bound is smaller than Gaussian compression methods.
- **Break condition:** If the Hessian spectrum is not dominated by a few large singular values, the error may exceed practical utility.

### Mechanism 3
- **Claim:** Efficiency gains are achieved by eliminating the storage and computation of explicit projection matrices.
- **Mechanism:** Standard compression requires storing a projection matrix P ∈ R^(r×d) (O(rd) memory). Dropout compression requires only storing indices of sampled coordinates (O(r) memory) and requires no matrix-vector multiplication for the compression step itself.
- **Core assumption:** Accessing specific indices in memory is computationally cheaper than performing a projection dot product.
- **Evidence anchors:** Section 3.2 states the method reduces memory and computational costs to O(r) as only r entries of gradients are sampled and stored; Table 8 shows Dropout compression complexity as O(nr) vs Gaussian O(nrd).
- **Break condition:** If hardware cannot efficiently handle sparse indexing operations compared to dense BLAS routines, the speedup might be negated.

## Foundational Learning

- **Concept: Influence Functions & iHVP**
  - **Why needed here:** This is the target operation being optimized. You must understand that influence functions measure the effect of a training point on a test loss via the product of gradients and the inverse Hessian-vector product (iHVP).
  - **Quick check question:** If the Hessian is too large to invert explicitly (d is billion-scale), how does the proposed method handle the iHVP calculation? (Hint: It compresses the vectors before the Hessian approximation is formed/inverted).

- **Concept: Gradient Compression (Projection vs. Sampling)**
  - **Why needed here:** To distinguish this work from baselines like TRAK or PCA. You need to know that previous methods "project" (rotate/squash) data, while this method "samples" (selects) data.
  - **Quick check question:** Why does random sampling (Dropout) theoretically destroy less information than random projection according to the paper's specific error bounds?

- **Concept: The Hessian Spectrum**
  - **Why needed here:** The theoretical justification relies on properties of the Hessian matrix (eigenvalues/singular values).
  - **Quick check question:** Does the error bound for Dropout depend on the dimension d or the largest singular value σ_max(H)? Why is this significant for Large Language Models (LLMs)?

## Architecture Onboarding

- **Component map:** Pre-trained Model (θ*) -> Gradient Computer -> Dropout Compressor -> Influence Calculator
- **Critical path:**
  1. **Gradient Computation:** Compute full gradients per sample (remains bottleneck)
  2. **Index Sampling:** Generate fixed random index set (size r) shared across all data points
  3. **iHVP on Compressed Domain:** Solve inverse Hessian vector product in reduced r-dimensional space
- **Design tradeoffs:**
  - **Compression Rate (r):** Smaller r increases efficiency but introduces instability; paper notes stability improves significantly at r≥3 for ~1000-param LoRA
- **Failure signatures:**
  - **High Variance:** If r is set too low, influence scores may fluctuate significantly across runs
  - **Memory Overflow (Pre-compression):** Full gradients must be computed before dropping entries, so peak memory usage during backward pass remains high
- **First 3 experiments:**
  1. **Sanity Check (Mislabeled Detection):** Flip 20% of labels in MRPC/QNLI, verify Dropout-compressed influence ranks mislabeled points as more harmful than clean data, compare AUC against Gaussian and LiSSA
  2. **Scaling Test (Retraining):** Fine-tune Pythia-1.4B, identify top-k influential points using Dropout, retrain without these points, check if perplexity increases
  3. **Efficiency Profiling:** Benchmark time for "Compression" step specifically, compare O(nr) (Dropout) vs O(nrd) (Gaussian/FJLT) on GPU

## Open Questions the Paper Calls Out
- **Question:** How can the remaining computational bottleneck of calculating per-example gradients for large datasets be eliminated to complement the efficiency of dropout compression?
- **Question:** Can the stability of dropout compression be improved when the compression size r is extremely small relative to the parameter count?
- **Question:** Are the theoretical error bounds derived for dropout compression tight, and can they be refined to better predict empirical performance?

## Limitations
- The method does not alleviate resource requirements for gradient computation, which remains a bottleneck for large datasets
- Dropout compression can be less stable than Gaussian methods at very low compression ratios (r < 0.1%)
- Theoretical error bounds may be loose and don't fully account for practical Hessian approximation effects

## Confidence

- **High confidence:** The efficiency claims regarding computational complexity (O(nr) vs O(nrd)) are mathematically sound and empirically validated
- **Medium confidence:** The theoretical error bound comparison with Gaussian projection holds in asymptotic regime, but practical significance depends on specific model characteristics
- **Medium confidence:** Empirical results on mislabeled detection and cross-source identification are promising but tested on relatively small models (up to 6.9B parameters)

## Next Checks

1. **Hessian spectrum sensitivity test:** Systematically vary the damping parameter λ_l and measure how the condition number of the compressed Hessian affects influence score stability across different datasets and model architectures

2. **Compression ratio stability analysis:** Conduct a systematic study of influence score variance as a function of compression ratio r across multiple runs, identifying the minimum stable r for different model sizes and task types

3. **Cross-task generalization validation:** Apply the method to a non-classification task (e.g., regression or reinforcement learning) to verify that dropout compression preserves influence signal quality beyond tested classification scenarios