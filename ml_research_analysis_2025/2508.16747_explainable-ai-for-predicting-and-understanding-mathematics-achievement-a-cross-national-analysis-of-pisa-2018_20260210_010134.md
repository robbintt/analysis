---
ver: rpa2
title: 'Explainable AI for Predicting and Understanding Mathematics Achievement: A
  Cross-National Analysis of PISA 2018'
arxiv_id: '2508.16747'
source_url: https://arxiv.org/abs/2508.16747
tags:
- achievement
- pisa
- math
- students
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study applied explainable AI to PISA 2018 mathematics data
  from 67,329 students across 10 countries. It compared four models (MLR, RF, CATBoost,
  ANN) for predicting math scores using 24 predictors covering student, family, and
  school factors.
---

# Explainable AI for Predicting and Understanding Mathematics Achievement: A Cross-National Analysis of PISA 2018

## Quick Facts
- **arXiv ID:** 2508.16747
- **Source URL:** https://arxiv.org/abs/2508.16747
- **Reference count:** 11
- **Primary result:** RF and CatBoost models achieved up to 38% variance explained in PISA 2018 math scores across 10 countries, outperforming linear models and identifying SES and study time as top predictors.

## Executive Summary
This study applied explainable AI to PISA 2018 mathematics data from 67,329 students across 10 countries, comparing four predictive models (MLR, RF, CATBoost, ANN) using 24 predictors covering student, family, and school factors. Tree-based ensemble models (RF and CATBoost) consistently outperformed linear regression and neural networks, with RF achieving up to 38% variance explained and lower MAE across countries. SHAP and decision tree visualizations revealed that socio-economic status, student effort, and learning time were the most important predictors, though their relative importance varied by national context. The research demonstrates how explainable ML can provide both accurate predictions and actionable insights for educational policy.

## Method Summary
The study used PISA 2018 student questionnaire data from 10 countries, applying a 70/30 stratified train/test split by country. Data preprocessing included KNN imputation (k=5), Winsorizing outliers at the 99th percentile, and standardizing continuous variables. Four models were trained and compared: Multiple Linear Regression (MLR), Random Forest (RF), CatBoost, and Artificial Neural Networks (ANN). Hyperparameter tuning was performed via 5-fold cross-validation on the training set. Model performance was evaluated using test set R² and Mean Absolute Error. SHAP values were calculated to interpret feature importance and individual predictions, with decision tree surrogates used for visualization.

## Key Results
- Random Forest and CatBoost consistently outperformed MLR and ANN, with RF achieving up to 38% variance explained (R²) across countries
- Socio-economic status (ESCS) was the most consistent top predictor across all countries and models
- SHAP analysis revealed contextual variation: in more equitable systems like Finland, motivational factors became more important than SES
- The model captured non-linear relationships, showing diminishing returns for study time beyond 300 minutes
- Performance was stable across national contexts, with RF showing the best balance of accuracy and generalizability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Tree-based ensemble models (Random Forest, CatBoost) outperform linear regression in this context because they capture non-threshold and interaction effects inherent in educational data without manual specification.
- **Mechanism:** Unlike Multiple Linear Regression (MLR), which assumes additive linear relationships, Random Forest aggregates multiple decision trees. Each tree partitions the student feature space into high-dimensional rectangles (e.g., "Low SES" AND "Low Study Time"), allowing the model to fit complex, non-linear mappings between predictors and math scores.
- **Core assumption:** The true relationship between student factors (like SES or study time) and achievement is not purely additive or linear; it involves conditional dependencies (e.g., the effect of study time depends on the level of teacher support).
- **Evidence anchors:**
  - [Abstract]: "Non-linear models, especially RF and ANN, outperformed MLR... with RF balancing accuracy and generalizability."
  - [Page 4]: "Linear models have difficulty capturing high-dimensional interactions... This limitation has paved the way for more flexible artificial intelligence (AI) and machine learning methods."
- **Break condition:** If the underlying data relationships were truly linear and independent (no interactions), the complexity of RF would only serve to fit noise (overfitting), causing MLR to match or exceed RF performance.

### Mechanism 2
- **Claim:** SHAP (Shapley Additive Explanations) values translate complex "black-box" model outputs into actionable insights by quantifying the contribution of each feature to a specific student's predicted score.
- **Mechanism:** SHAP utilizes concepts from cooperative game theory to fairly distribute the "payout" (the prediction difference from the average) among the "players" (features). It calculates the marginal contribution of a feature (e.g., SES) across all possible combinations of other features, providing a consistent importance score for every student.
- **Core assumption:** The explanation generated by SHAP (feature attribution) accurately reflects how the model utilizes data, even if the model's internal logic is opaque.
- **Evidence anchors:**
  - [Page 5]: "SHAP... assigns each feature an importance value for individual predictions, showing how that feature increases or decreases a student’s predicted score."
  - [Page 19]: "To explore how the most influential predictors vary across countries... Figure 3 illustrates the top three predictors (ranked by SHAP importance)."
- **Break condition:** If features are highly multicollinear (e.g., Parent Education vs. SES composite), SHAP values may distribute importance arbitrarily between them, potentially misleading interpretation without correlation checks.

### Mechanism 3
- **Claim:** The predictive dominance of Socio-Economic Status (ESCS) is validated as a universal mechanism of achievement, but its relative importance fluctuates contextually based on system-level equity.
- **Mechanism:** The Random Forest algorithm prioritizes splits that reduce variance (impurity) the most. In systems with high inequality, SES becomes a primary differentiator of outcome variance. In more equitable or homogenous systems, the algorithm shifts weight to proximal behavioral factors (e.g., effort, learning time) that explain the remaining variance.
- **Core assumption:** The selected 24 predictors sufficiently capture the "input" side of the educational production function, and unmeasured variables (e.g., specific teacher quality, genetic factors) do not systematically bias the relative importance of the observed features.
- **Evidence anchors:**
  - [Page 16]: "Dominance of Socio-Economic Status... ESCS was consistently at or near the top for every model in every country."
  - [Page 17]: "In countries like Finland... motivational and school climate variables among the top predictors... In countries like Chile or Argentina... SES was more dominant."
- **Break condition:** If a country's PISA dataset has measurement error in SES or if the variance in SES is artificially restricted (truncated), the model would be forced to select secondary predictors, potentially underrepresenting the true structural impact of SES.

## Foundational Learning

- **Concept: Bias-Variance Tradeoff**
  - **Why needed here:** The paper compares MLR (high bias, low variance) against RF/ANN (low bias, high variance). Understanding this is crucial to interpreting why ANN performed inconsistently and why RF (which averages many trees to reduce variance) offered the "superior balance."
  - **Quick check question:** *Why might a complex Neural Network perform worse than a Random Forest on tabular data with only 24 features?*

- **Concept: Feature Attribution vs. Causal Inference**
  - **Why needed here:** The study explicitly warns that relationships are correlational (Page 28, 30). Learners must distinguish between a factor being "predictive" (feature importance) and being "causal" (intervention target).
  - **Quick check question:** *If "Books at Home" is a top predictor, does buying books necessarily cause a student's math score to rise? Why or why not?*

- **Concept: Stratification and Cross-Validation**
  - **Why needed here:** The methodology relies on a 70/30 split stratified by country and 5-fold CV to ensure the model isn't just memorizing specific country profiles but generalizing across national contexts.
  - **Quick check question:** *Why is it critical to stratify by country when splitting the data, rather than just taking the first 70% of rows?*

## Architecture Onboarding

- **Component map:** PISA 2018 raw data (67,329 rows × 24 features) → KNN Imputation (k=5) → Winsorization (99th percentile) → Standardization (Mean=0, SD=1) → Model Core (Random Forest/CatBoost) → Interpretability Layer (Feature Importance + SHAP + Decision Tree Surrogate)

- **Critical path:** The interaction between **Feature Selection (RFE)** and **Hyperparameter Tuning**. The paper uses Recursive Feature Elimination and Mutual Information to prune noise before tuning the RF depth/tree count. If this step is skipped, the model may overfit to irrelevant survey responses.

- **Design tradeoffs:**
  - **Accuracy vs. Stability:** The study selected RF over ANN despite ANN's theoretical capacity. RF provided consistent $R^2$ across all 10 countries, whereas ANN failed in Finland and Italy. *Guidance: Prioritize ensemble trees for cross-national tabular data.*
  - **Global vs. Local Explanation:** The architecture uses global Feature Importance for general trends but requires SHAP for specific student-level insights.

- **Failure signatures:**
  - **Regression to the Mean:** The model under-predicts very high scores and over-predicts very low scores (Page 21). *Fix: Do not use this architecture for identifying "genius" level outliers; it is optimized for the bulk of the distribution.*
  - **Imputation Artifacts:** Using KNN imputation on variables with >20% missingness (e.g., engagement items) risks fabricating patterns. *Check: Review imputation distributions vs. original distributions for these variables.*

- **First 3 experiments:**
  1. **Replicate Baseline:** Train a simple Linear Regression on the preprocessed PISA data to establish the lower bound of performance ($R^2 \approx 0.20$).
  2. **Feature Ablation:** Train the RF model removing the "ESCS" (SES) variable. Observe the drop in $R^2$ to quantify the exact predictive power of socioeconomic status in your specific subset.
  3. **SHAP Dependency Plot:** Generate a SHAP dependence plot for "Learning Time" (MMINS) in Japan vs. Finland. This visualizes the mechanism where learning time has diminishing returns or different slopes in different cultures.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do targeted interventions that successfully increase student engagement or sense of belonging result in statistically significant improvements in mathematics achievement?
- **Basis in paper:** [Explicit] The authors explicitly state in the Conclusion that "future research should explore whether interventions targeting the key factors identified... actually lead to improved outcomes," noting that their current findings are correlational.
- **Why unresolved:** The current study establishes strong predictive associations using cross-sectional PISA 2018 data but lacks the longitudinal or experimental design necessary to confirm causality between psychosocial factors and performance.
- **What evidence would resolve it:** Evidence from randomized controlled trials or longitudinal studies that measure changes in math scores following specific manipulations of engagement, teacher support, or study time.

### Open Question 2
- **Question:** What specific feature engineering or data volume thresholds are required for Artificial Neural Networks (ANNs) to consistently match the stability and generalizability of ensemble tree methods in educational tabular data?
- **Basis in paper:** [Explicit] Page 15 notes that the ANN’s performance was "variable" across countries, suggesting that purely data-driven neural approaches "might need even more data or feature engineering to consistently capture patterns in education data as effectively as ensemble trees."
- **Why unresolved:** The study benchmarks standard model architectures but does not exhaustively explore the specific architectural configurations or data preprocessing steps required to stabilize ANNs for diverse national datasets.
- **What evidence would resolve it:** A comparative analysis systematically varying ANN depth, regularization techniques, and training set sizes to identify the precise conditions under which deep learning matches the robustness of Random Forests.

### Open Question 3
- **Question:** Does the methodological simplification of using a single plausible value (PV1MATH) significantly impact the stability of SHAP-based feature importance rankings in non-linear models?
- **Basis in paper:** [Inferred] Page 8 states that the study used a single plausible value for simplicity, acknowledging this introduces "some additional uncertainty," yet the effect of this uncertainty on the interpretability of ML models remains untested.
- **Why unresolved:** PISA scores are imputed values (plausible values) representing a distribution of proficiency; relying on a single point estimate might distort the "importance" of predictors if the variance between plausible values is high for certain student subgroups.
- **What evidence would resolve it:** A sensitivity analysis replicating the Random Forest and SHAP analysis across all five plausible values to verify that the ranking of top predictors (e.g., SES, effort) remains consistent.

## Limitations
- The exact list of 24 predictor variables is unclear without Supplementary Table A1, creating uncertainty in exact replication
- KNN imputation on variables with >20% missingness may introduce artifacts, particularly for engagement variables
- The study establishes predictive relationships but cannot establish causality between factors and achievement
- ANN instability across countries suggests model architecture sensitivity to national data characteristics
- Limited to PISA 2018 data without external validation on other assessment cycles

## Confidence
- RF/CatBoost predictive superiority over MLR: **High** (robust cross-validation, consistent across 10 countries)
- SES as universal predictor with contextual variation: **Medium-High** (strong pattern but measurement sensitivity concerns)
- SHAP-based interpretability reliability: **Medium** (theoretically sound but sensitive to multicollinearity in PISA composites)

## Next Checks
1. Replicate the analysis using the raw 2018 student dataset with confirmed 24 predictor codes to verify exact performance metrics
2. Apply the trained RF model to PISA 2022 mathematics data to test temporal generalizability
3. Conduct a feature ablation study removing individual PISA index composites (ESCS, BELONG, etc.) to quantify their isolated contribution versus survey item-level predictors