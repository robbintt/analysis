---
ver: rpa2
title: Unveiling the Response of Large Vision-Language Models to Visually Absent Tokens
arxiv_id: '2509.03025'
source_url: https://arxiv.org/abs/2509.03025
tags:
- visual
- tokens
- visually
- neurons
- absent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work identifies that Large Vision-Language Models (LVLMs)\
  \ often misinterpret text inputs lacking visual grounding as if they were present\
  \ in the image, leading to hallucinations. Through systematic analysis, the authors\
  \ discover a subset of Feed-Forward Network neurons\u2014called Visual Absence-aware\
  \ (VA) neurons\u2014that consistently exhibit distinct activation patterns when\
  \ processing visually absent tokens."
---

# Unveiling the Response of Large Vision-Language Models to Visually Absent Tokens

## Quick Facts
- **arXiv ID**: 2509.03025
- **Source URL**: https://arxiv.org/abs/2509.03025
- **Reference count**: 31
- **Primary result**: Identifies and leverages Visual Absence-aware (VA) neurons to detect visually absent tokens in LVLMs, achieving up to 28.5% improvement in hallucination benchmarks through inference-time refinement.

## Executive Summary
This work addresses a critical hallucination problem in Large Vision-Language Models (LVLMs) where models misinterpret text lacking visual grounding as present in the image. The authors discover a specific subset of Feed-Forward Network (FFN) neurons—Visual Absence-aware (VA) neurons—that consistently exhibit distinct activation patterns for visually absent tokens. Using these neurons, they train a lightweight Visual Absence detector that classifies whether tokens are visually grounded. This detector guides refinement of model outputs: overriding responses to "No" in binary QA when absent tokens are detected, and replacing unsupported tokens during open-ended generation by setting their logits to negative infinity. Experiments across multiple LVLMs show significant improvements in accuracy and hallucination metrics while remaining training-free and decoding-agnostic.

## Method Summary
The method identifies VA neurons through sensitivity analysis using the Bhattacharyya Coefficient to measure activation distribution differences between visually present and absent tokens. A linear classifier is trained on these neuron activations to detect visual absence. During inference, the detector either overrides binary QA responses or guides token replacement in generation by masking hallucinated tokens' logits, forcing the model to select more visually grounded alternatives.

## Key Results
- VA detector achieves 97%+ accuracy in identifying visually absent tokens across multiple LVLM architectures
- Binary QA accuracy improves by up to 28.5% on object- and relation-hallucination benchmarks
- Open-ended generation hallucination metrics (CHAIR and GPT-score) decrease by 7.8-16.2 points
- Method generalizes across LLaVA, mPLUG-Owl2, InstructBLIP, Qwen2-VL, and Gemma3 without model-specific training

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Alignment Detection in FFNs
LVLM FFN layers contain neurons that encode cross-modal alignment information, with specific VA neurons showing significantly different activation distributions for visually present versus absent tokens. The sensitivity score (1 - Bhattacharyya Coefficient) quantifies this disentanglement, enabling reliable detection of visual absence.

### Mechanism 2: Inference-Time Logit Suppression
During generation, when the VA detector identifies an absent token, the method sets its logit to negative infinity, forcing the model to select alternative tokens from its candidate distribution. This preserves generation fluency while reducing hallucinations.

### Mechanism 3: Discrepancy Correction
The VA detector bridges the gap between internal knowledge (VA neuron activation) and external behavior (model output). When VA neurons signal absence but the model outputs otherwise, the method corrects this by overriding the response to "No" in binary QA.

## Foundational Learning

- **Gated Feed-Forward Networks (FFN) in Transformers**: Understanding that FFNs process tokens independently (unlike Attention) is crucial for grasping why specific neurons can encode static knowledge or alignment states. *Quick check: How does the independence of FFN processing per token support the identification of "visually absent" specific tokens?*

- **Bhattacharyya Coefficient (BC)**: This mathematical measure quantifies the overlap between two probability distributions and forms the basis for the sensitivity score (S_VA). *Quick check: Why is a low Bhattacharyya Coefficient (high 1-BC) desirable when selecting neurons for the VA detector?*

- **Object Hallucination vs. Relation Hallucination**: The method evaluates on both object-level (POPE) and relation-level (R-Bench) benchmarks, with the VA detector functioning differently for each. *Quick check: Does the VA detector look for the absence of the object or the absence of the relationship in the R-Bench evaluation?*

## Architecture Onboarding

- **Component map**: Backbone (LLaVA/mPLUG/Qwen) -> VA-QA Dataset -> VA Detector (Linear Classifier/MLP) -> Refinement Logic (Binary QA Override / Generation Logit Masking)

- **Critical path**: 
  1. Extract FFN activations from middle layers (8-16 for LLaVA-1.5)
  2. Compute S_VA per neuron; select neurons with score > β
  3. Train MLPClassifier on selected neuron activations
  4. Apply refinement logic during inference

- **Design tradeoffs**: 
  - β threshold selection balances signal quality vs. noise inclusion
  - Training-free approach ensures portability but limits ability to fix underlying model misalignment
  - Conservative VA detection may reduce Acc_yes in binary QA but improves overall hallucination control

- **Failure signatures**: 
  - Conservative outputs in Binary QA (decreased Acc_yes)
  - Repetition loops or shortened captions in generation due to aggressive masking
  - Detection failure when VA neurons fire due to spurious correlations rather than true absence

- **First 3 experiments**:
  1. Replicate Figure 4 to compute S_VA across layers for LLaVA-1.5 and confirm middle layers as the alignment hub
  2. Test "If absent, say No" logic on POPE-Random subset to isolate detector precision
  3. Generate caption for image with known absent object (e.g., "man in suit" where no suit exists) to verify logit masking successfully swaps unsupported tokens

## Open Questions the Paper Calls Out

1. **Closed-source LVLM Adaptation**: How can visual absence detection work when internal neuron activations are inaccessible? The method relies on white-box access to FFN activations, which may not be available in closed-source models.

2. **Mechanistic Cause of Detection-Behavior Gap**: Why do VA neurons correctly signal visual absence while the model's final output fails to reflect this knowledge? The paper demonstrates the gap exists but doesn't investigate the information flow breakdown.

3. **Generalization to Diverse Hallucination Types**: How does training on object/relation grounding affect the detector's ability to identify attribute hallucinations, temporal misunderstandings, and abstract reasoning errors? The current VA-QA dataset focuses on <subject, verb, object> triplets.

4. **Activation Weakening in Extended Generation**: Why do VA neuron activations diminish during longer generation sequences, and what architectural interventions could maintain detection quality? The paper observes this phenomenon but doesn't investigate the underlying causes.

## Limitations
- **Dataset Subjectivity**: VA-QA dataset construction requires subjective judgment to identify "distinct" image differences, raising reproducibility concerns
- **Architecture-Specific Parameters**: Optimal neuron selection and sensitivity thresholds appear model-dependent, requiring per-architecture tuning
- **Evaluation Scope**: Strong performance on controlled hallucination benchmarks but limited evaluation on abstract concepts, attributes, and naturalistic conversational contexts

## Confidence
- **High Confidence**: 
  - Existence of VA neurons with distinct activation patterns
  - Detector effectiveness across multiple LVLM architectures
  - Training-free nature and portability of the method
- **Medium Confidence**: 
  - Generalization to unseen hallucination types
  - Claim of no generation quality compromise
  - FFN middle layers as primary alignment hub
- **Low Confidence**: 
  - Long-term stability in extended generation sessions
  - Effectiveness on multilingual LVLMs
  - Robustness against adversarial inputs

## Next Checks
1. **Adversarial Robustness Test**: Construct test cases where visually present tokens are paired with semantically similar absent tokens (e.g., "cat" vs. "tiger") to measure VA detector precision and identify false positive/negative patterns.

2. **Extended Generation Coherence Analysis**: Apply refinement to produce 8-10 sentence captions for complex images, evaluating not just hallucination metrics but narrative coherence, repetition frequency, and whether rollback leads to truncated outputs.

3. **Cross-Lingual Generalization Study**: Retrain the VA detector on multilingual data and test on multilingual LVLM capabilities to determine whether VA neurons exhibit similar activation patterns across languages or require language-specific adaptation.