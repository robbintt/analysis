---
ver: rpa2
title: The Impact of Disability Disclosure on Fairness and Bias in LLM-Driven Candidate
  Selection
arxiv_id: '2506.00256'
source_url: https://arxiv.org/abs/2506.00256
tags:
- disability
- candidate
- information
- have
- candidates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study examined how disability disclosure impacts LLM-driven\
  \ candidate selection, finding that LLMs consistently favored candidates who disclosed\
  \ no disability, even when all candidates had identical qualifications. Across five\
  \ different LLMs tested on 102k samples, the preference for non-disabled candidates\
  \ was statistically significant (\u03C7\xB2 p-values < 0.0001)."
---

# The Impact of Disability Disclosure on Fairness and Bias in LLM-Driven Candidate Selection

## Quick Facts
- arXiv ID: 2506.00256
- Source URL: https://arxiv.org/abs/2506.00256
- Authors: Mahammed Kamruzzaman; Gene Louis Kim
- Reference count: 4
- LLMs consistently favored candidates who disclosed no disability, even when all candidates had identical qualifications (χ² p-values < 0.0001)

## Executive Summary
This study examines how disability disclosure impacts LLM-driven candidate selection, finding systematic bias against disabled candidates. Across five different LLMs tested on 102k samples, models consistently preferred candidates who disclosed no disability over those who disclosed having a disability or chose not to disclose. The bias persisted regardless of occupation and was accompanied by stereotypical assumptions about accommodation needs and confidence levels. The study also revealed intersectional biases where disability disclosure shifted model preferences for gender and race in model-specific ways.

## Method Summary
The study used 320 first names with race/gender labels and 16 occupations selected for <2% employment rate gaps between disabled/non-disabled workers. Three candidate profiles were created with identical qualifications but different disability disclosures: "Yes, I have a disability," "No, I do not have a disability," and "I do not want to answer." Five LLMs (GPT4o-mini, Gemma2-9B, Mistral-7B, Qwen2.5-7B, Llama3.2-3B) were tested using three prompt templates, with results averaged. Selection rates were analyzed using χ² tests across 102k samples for Experiment 1, 25.6k for Experiment 2, and 307k for Experiment 3.

## Key Results
- LLMs consistently selected non-disabled candidates more often than disabled candidates (χ² p-values < 0.0001)
- Non-disclosure of disability status resulted in lower selection rates than explicit non-disabled status across all models
- Intersectional biases emerged where disability disclosure shifted model preferences for race and gender in model-specific ways

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs associate disability disclosure with workplace limitations and accommodation needs, driving selection bias against disabled candidates.
- Mechan: When disability status is explicitly provided, models retrieve stereotypical associations from training data linking disability to "requiring accommodations," "being unable to adapt," or "having limitations in performing creative tasks" (qualitative analysis, p.4-5). These associations override merit-based evaluation despite identical qualifications being stated.
- Core assumption: The bias originates from statistical patterns in training corpora where disability is disproportionately discussed in contexts of accommodation, limitation, or challenge.
- Evidence anchors:
  - [abstract] "qualitative analysis revealed that LLMs often assumed individuals without disabilities required no accommodations and were more confident, while those with disabilities were viewed as needing accommodations or having limitations"
  - [Table 1, Exp.1] Mistral-7B response: "Hunter's disability information stating 'no, i do not have a disability' suggests he has built a career without needing accommodations. This could make him more adaptable... Additionally, his confidence in not having a disability suggests that he may be more confident and self-assured"
  - [corpus] AccessEval (arXiv:2509.22703) benchmarks disability bias across 21 closed-source LLMs, confirming systematic patterns
- Break condition: If models were fine-tuned on debiased corpora or instructed to ignore disability status for selection, the mechanism would weaken or reverse.

### Mechanism 2
- Claim: Non-disclosure ("I do not want to answer") is treated as an intermediate negative signal rather than neutral information.
- Mechan: Models assign probabilistic penalties to non-disclosure, possibly inferring hidden disability status. Llama3.2 and Mistral preferred non-disclosers over disabled candidates but still favored non-disabled candidates overall; GPT4o-mini penalized non-disclosure more than disability disclosure.
- Core assumption: Models learn that non-response correlates with sensitive attributes in real-world data distributions.
- Evidence anchors:
  - [abstract] "Even in cases where candidates chose not to disclose their disability status, the LLMs were less likely to select them compared to those who explicitly stated they did not have a disability"
  - [Results, p.4] "Llama3.2 and Mistral show a preference for candidates who opted not to disclose disability status over those who reported having a disability. In contrast, GPT4o-mini selects fewer candidates when disability information is undisclosed than candidates with disability"
  - [corpus] No direct corpus evidence on non-disclosure treatment; related work focuses on explicit demographic signals
- Break condition: Explicit instructions to treat non-disclosure as neutral and selection-irrelevant.

### Mechanism 3
- Claim: Disability disclosure interacts with race and gender cues, producing model-specific intersectional bias patterns.
- Mechan: The presence of disability information shifts racial and gender preferences. Gemma shifted from favoring Hispanic (no disability info) to White (with disability info). GPT4o-mini and Mistral favored Black candidates more when disability was present. Llama3.2 shifted from Asian to Black preference with disability disclosure.
- Core assumption: Intersectional effects emerge from correlated stereotypes in training data; models lack principled fairness constraints.
- Evidence anchors:
  - [Results, p.5-6] "disability disclosure impacts the relative preference of races in LLM hiring decisions, with effects varying by model... For the Gemma model, when no disability information is included, the model tends to select Hispanic candidates more frequently... When disability information is included, the model shifts to selecting White candidates more often"
  - [Table 3, 4] Statistical significance tests show varying χ² values across models and conditions
  - [corpus] "Who Gets Cited?" (arXiv:2508.02740) documents gender and majority bias in LLM-driven reference selection, supporting intersectional bias patterns
- Break condition: Model-specific debiasing or uniform fairness constraints across demographic intersections.

## Foundational Learning

- Concept: Chi-squared (χ²) test for categorical fairness evaluation
  - Why needed here: The paper uses χ² tests to determine whether selection rate differences across disability status groups are statistically significant (all p < 0.0001). Understanding this test is essential to interpret the strength of reported biases.
  - Quick check question: If an LLM selected 45 disabled and 55 non-disabled candidates from 100 equally-qualified pairs, would you expect a significant χ² result at α = 0.05?

- Concept: Intersectionality in algorithmic bias
  - Why needed here: Experiment 2 and 3 show that disability bias cannot be analyzed in isolation—gender and race preferences shift when disability is disclosed. Single-axis fairness metrics would miss these compounding effects.
  - Quick check question: A model is "fair" on gender alone and "fair" on disability alone. Could it still be unfair against disabled women? Explain.

- Concept: Qualitative explanation analysis for bias auditing
  - Why needed here: The paper's qualitative analysis (Table 1) reveals stereotypical reasoning ("confidence in not having a disability suggests he may be more confident") that quantitative metrics alone cannot surface.
  - Quick check question: Why might accuracy parity across groups fail to detect stereotypical reasoning in model outputs?

## Architecture Onboarding

- Component map: Candidate Profile Generator → LLM Selection Engine → Selection Output

- Critical path: Profile creation (controlling all variables except disability) → Randomized presentation (preventing ordering bias) → Selection → χ² statistical testing across 102k samples (Experiment 1).

- Design tradeoffs:
  - Occupations restricted to <2% employment rate gap between disabled/non-disabled workers to avoid job-type confounds (limits ecological validity)
  - Three-prompt averaging reduces prompt-specific artifacts but increases compute 3x
  - Binary selection (not ranking) simplifies analysis but may not reflect real multi-stage hiring

- Failure signatures:
  - Non-disclosure penalty: GPT4o-mini selected non-disclosers less often than explicit disabled candidates—unexpected pattern requiring investigation
  - Model-specific race shifts: Disability disclosure reversed racial preferences in Gemma (Hispanic→White) and Llama3.2 (Asian→Black)
  - "Diversity" justification masking bias: Models claimed diversity goals while disproportionately favoring specific demographic combinations

- First 3 experiments:
  1. Replicate Experiment 1 with a new occupation not in the original 16 (e.g., Nurse, Accountant) to test generalization. Measure selection rates and χ² across disability status.
  2. Add explicit fairness instruction to the prompt: "Evaluate candidates solely on qualifications; ignore disability status." Compare effect sizes to baseline.
  3. Test whether the non-disclosure penalty persists with a fourth option: "Prefer not to say" vs. "No disability" vs. "Yes, disability" vs. no disability field at all. Isolate whether the field's presence itself triggers bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do specific disability types (e.g., mobility impairment vs. neurodivergence vs. chronic illness) trigger different levels or patterns of bias in LLM candidate selection?
- Basis in paper: [inferred] The study used a composite disability definition listing many conditions, but did not test whether bias varies by disability type.
- Why unresolved: Candidates in this study only disclosed "Yes, I have a disability" without specifying the condition, masking potential variation in model behavior across different disabilities.
- What evidence would resolve it: Run the same experimental setup with candidates disclosing specific disability categories separately and compare selection rates across conditions.

### Open Question 2
- Question: Why do different LLMs show opposing shifts in racial preferences when disability information is present (e.g., Gemma favors White candidates more, while GPT4o-mini and Mistral favor Black candidates more)?
- Basis in paper: [inferred] The paper documents these divergent patterns across models but does not investigate the underlying causes or training data differences that might explain them.
- Why unresolved: The study focused on documenting intersectional bias existence rather than explaining model-specific mechanisms driving different preference directions.
- What evidence would resolve it: Comparative analysis of training corpora, fine-tuning procedures, or attention patterns across models when processing intersectional demographic information.

### Open Question 3
- Question: Do bias patterns persist or change for occupations with larger employment rate gaps between disabled and non-disabled workers?
- Basis in paper: [inferred] The study deliberately selected occupations with <2% employment rate gaps to control for confounding factors, leaving high-gap occupations unexplored.
- Why unresolved: It remains unclear whether the observed bias is amplified, reduced, or unchanged in job categories where real-world disability employment disparities are larger.
- What evidence would resolve it: Replicate the experimental design using occupations with high employment rate gaps (e.g., physical labor roles) and compare results.

### Open Question 4
- Question: What specific interventions (prompt engineering, fine-tuning, retrieval augmentation) can effectively reduce disability-related bias in LLM hiring systems?
- Basis in paper: [explicit] The authors state the goal to "develop strategies to mitigate biases" but the study only documents bias without testing mitigation approaches.
- Why unresolved: The paper's experiments characterize the problem but do not evaluate potential solutions or fairness-enhancing modifications.
- What evidence would resolve it: Test candidate selection with anti-bias prompting, fairness-aware fine-tuning, or modified disclosure formats to measure reduction in selection disparities.

## Limitations
- Controlled synthetic profiles limit ecological validity and may not reflect real-world hiring complexity
- Deliberate selection of occupations with <2% employment gaps creates an artificial environment
- Non-disclosure penalty in GPT4o-mini lacks clear theoretical explanation and may be an artifact

## Confidence

- **High Confidence**: The finding that LLMs consistently favor non-disabled candidates when qualifications are identical (χ² p < 0.0001 across 102k samples) is robustly supported by statistical significance and qualitative explanation analysis.
- **Medium Confidence**: The mechanism linking disability disclosure to accommodation assumptions is well-supported by qualitative analysis, though the specific training corpus sources remain unidentified.
- **Medium Confidence**: The intersectional bias patterns are statistically significant but vary substantially across models, suggesting context-dependent rather than universal effects.

## Next Checks

1. Test whether the non-disclosure penalty persists when adding a fourth option ("Prefer not to say" vs. "No disability" vs. "Yes, disability" vs. no disability field at all) to isolate whether the field's presence itself triggers bias.
2. Replicate Experiment 1 with occupations not restricted by employment gap criteria to assess ecological validity and real-world generalizability.
3. Implement explicit fairness instructions ("Evaluate candidates solely on qualifications; ignore disability status") and measure effect size changes compared to baseline to test debiasing potential.