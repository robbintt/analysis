---
ver: rpa2
title: 'AnaCP: Toward Upper-Bound Continual Learning via Analytic Contrastive Projection'
arxiv_id: '2511.13880'
source_url: https://arxiv.org/abs/2511.13880
tags:
- learning
- class
- feature
- anacp
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of catastrophic forgetting in
  class-incremental learning (CIL), where models must learn new classes sequentially
  without forgetting previously learned ones. Traditional CIL methods struggle with
  either feature adaptation or forgetting, while analytic approaches using pre-trained
  models are efficient but lack feature adaptation.
---

# AnaCP: Toward Upper-Bound Continual Learning via Analytic Contrastive Projection

## Quick Facts
- **arXiv ID**: 2511.13880
- **Source URL**: https://arxiv.org/abs/2511.13880
- **Reference count**: 40
- **Primary result**: AnaCP achieves accuracy comparable to joint training upper bound while being analytic and immune to catastrophic forgetting

## Executive Summary
AnaCP addresses catastrophic forgetting in class-incremental learning by combining analytic closed-form updates with feature adaptation. The method freezes a pre-trained model after first-session adaptation, then incrementally updates projection layers via ridge regression without revisiting past data. Unlike purely analytic methods that struggle with feature adaptation, AnaCP introduces a contrastive projection layer that analytically separates class prototypes through SVD perturbation, achieving upper-bound performance while maintaining computational efficiency.

## Method Summary
AnaCP freezes a pre-trained model after first-session adaptation (FSA) and incrementally updates a multi-head random projection layer with analytic ridge regression. For each new task, it updates class means and shared covariance, computes target prototypes through SVD perturbation of whitened class means, and updates projection weights in closed form. The method uses pseudo-replay features sampled from class-conditional Gaussians to train an ELM classifier, achieving joint-training-level accuracy while being immune to catastrophic forgetting through its analytic update mechanism.

## Key Results
- Outperforms existing baselines on five datasets with relative error reductions of 4.8% to 31.4%
- Achieves accuracy comparable to joint training upper bound across CIFAR100, ImageNet-R, CUB, TinyImageNet, and Cars
- Particularly effective when paired with strong pre-trained models (DINO-v2 vs. MoCo-v3 shows significant performance gaps)
- Ablation studies confirm contrastive projection layer contributes 3-20% accuracy improvements

## Why This Works (Mechanism)

### Mechanism 1: Analytic Closed-Form Updates via Ridge Regression
Incrementally updating Gram and cross matrices enables exact solution updates without revisiting past data. Ridge regression solution W = (X^T X + λI)^(-1) X^T Y decomposes into G (Gram) and H (cross) matrices. As new tasks arrive, G_t = G_{t-1} + X_t^T X_t and H_t = H_{t-1} + X_t^T Y_t allow computing W_t = (G_t + λI)^{-1} H_t without storing raw data. This closed-form solution is attractive because it forgoes iterative training and directly computes a global optimum. The CP layer in our method is inherently immune to CF, as it operates on a frozen PTM and updates its statistics incrementally without overwriting prior information.

### Mechanism 2: Contrastive Projection via Target Prototype Separation
Whitening class means, applying SVD, and perturbing singular values analytically reduces inter-class cosine similarity. The method whitens class means by Σ^{-1/2} to normalize variance across dimensions, applies SVD: Ĉ = USV^T, then perturbs singular values: Ŝ = S + α·Diag{δ_i} where δ_i ∈ {-1, 0, 1} determined by Lemma 4.1 to reduce pairwise cosine similarity. This condition arises from the proof of Lemma 4.1, where we analyze the derivative of the function f_i(α), defined as the sum of cosine similarities involving class i. Disabling negative repulsion reduces accuracy by 0.38% to 3.23% across datasets.

### Mechanism 3: Multi-Head Random Projection with ELM Classifier
Averaging projections from H independently initialized random projections stabilizes representations for classification. Each head h computes u^(h) = φ(xR^(h))W^(h) where R^(h) is fixed random matrix and W^(h) is learned CP weights. Final representation u = (1/H) Σ_h u^(h). ELM classifier trained on pseudo-replay features sampled from class-conditional Gaussians (shared covariance Σ). Since each RP is initialized independently, we learn a corresponding CP for each RP, all sharing the same set of target prototypes. Increasing heads from H=1 to H=5 shows marginal improvement (0.16%-0.44%).

## Foundational Learning

- **Ridge Regression (Regularized Least Squares)**: Core mathematical machinery enabling closed-form updates without gradient descent. Can you derive why (X^T X + λI)^{-1} exists even when X^T X is singular?
- **Incremental Matrix Updates (Sherman-Morrison-Woodbury intuition)**: Understanding how G_t = G_{t-1} + X_t^T X_t avoids storing raw data. Why must zero-padding be applied to H_{t-1} when new classes arrive?
- **Whitening and Mahalanobis Distance**: Explains why whitening before SVD is necessary for meaningful separation. How does whitening relate class separation to Mahalanobis distance rather than Euclidean distance?

## Architecture Onboarding

- **Component map**: Input (x) → PTM (frozen) → Input Layer (d-dim) → Random Projection Layer (D-dim, fixed R matrix, GELU activation) → Contrastive Projection Layer (d-dim, learned W via ridge regression) → [Average across H heads] → ELM Classifier (trained on pseudo-replay features)
- **Critical path**: 1) First session adaptation (FSA): Fine-tune PTM on Task 0, then freeze. 2) For each task t: Extract features → Update class means and Σ → Recompute target prototypes P via SVD perturbation → Update each CP head's Gram matrix and W → Sample pseudo-features → Retrain ELM classifier. 3) Inference: Average H head outputs → NCM or ELM prediction.
- **Design tradeoffs**: Higher D (projection dimension): better separation but O(D²) memory for Gram matrices. More heads H: marginal accuracy gain, linear memory increase. Shared vs. per-class covariance: shared reduces memory significantly with minimal accuracy drop. Pseudo-replay samples R: 20-100 samples/class sufficient.
- **Failure signatures**: Accuracy drops significantly on early tasks → check if PTM actually frozen after FSA. Large gap to joint training with weak PTM → expected; consider stronger PTM. Memory explosion → verify Gram matrices (D×D) dominate storage.
- **First 3 experiments**: 1) Ablation on CP layer: disable CP to confirm ~3-20% accuracy drop. 2) PTM strength comparison: run with DINO-v2 vs. MoCo-v3. 3) Forgetting verification: under TIL setting, plot accuracy matrix to confirm diagonal stability.

## Open Questions the Paper Calls Out
- **Can AnaCP achieve joint-training-level accuracy with weaker pre-trained models?**: Improving AnaCP's performance with weaker PTMs remains a meaningful goal. The authors state this explicitly in Limitations.
- **How can AnaCP be extended to task-incremental learning (TIL) and domain-incremental learning (DIL) settings?**: The authors believe AnaCP can be extended to TIL and DIL scenarios with appropriate adaptations.
- **How critical is First Session Adaptation (FSA) to AnaCP's performance?**: All experiments use FSA, but no ablation isolates its contribution from the CP layer itself.

## Limitations
- Limited empirical breadth: focuses on class-incremental learning with specific task splits; behavior under different incremental scenarios unexplored
- Theoretical gaps in contrastive projection: SVD perturbation mechanism lacks extensive empirical validation and may have unexplored failure modes
- Dependence on PTM quality: performance highly sensitive to pre-trained model feature quality, degrading significantly with weaker models

## Confidence
- **High Confidence**: The analytic closed-form update mechanism via ridge regression is mathematically sound and well-validated
- **Medium Confidence**: The contrastive projection layer's effectiveness is supported by ablation studies, but theoretical justification has limited corpus validation
- **Low Confidence**: Generalization to non-standard CIL scenarios and computational scaling properties for large class counts are not empirically established

## Next Checks
1. **Cross-PTM Generalization Test**: Implement AnaCP with three additional PTMs of varying quality across all five datasets to quantify PTM dependency
2. **Scaling Stress Test**: Evaluate AnaCP on synthetic benchmark with 100+ tasks and 1000+ classes to measure computational overhead and memory requirements
3. **Non-Standard Incremental Scenario**: Test AnaCP under instance-incremental learning to verify handling of gradual concept drift scenarios