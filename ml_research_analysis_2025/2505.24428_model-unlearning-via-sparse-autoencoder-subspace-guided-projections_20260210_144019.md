---
ver: rpa2
title: Model Unlearning via Sparse Autoencoder Subspace Guided Projections
arxiv_id: '2505.24428'
source_url: https://arxiv.org/abs/2505.24428
tags:
- unlearning
- forget
- knowledge
- subspace
- sspu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SAE-Guided Subspace Projection Unlearning (SSPU),
  a novel framework that leverages sparse autoencoder (SAE) features to drive targeted
  parameter updates for precise, interpretable, and robust knowledge removal from
  large language models. The method constructs a subspace from SAE-extracted latent
  dimensions to guide supervised unlearning, balancing forgetting of targeted content
  with retention of general capabilities.
---

# Model Unlearning via Sparse Autoencoder Subspace Guided Projections

## Quick Facts
- arXiv ID: 2505.24428
- Source URL: https://arxiv.org/abs/2505.24428
- Reference count: 40
- Primary result: SSPU reduces harmful knowledge accuracy by 3.22% vs. strongest baseline while improving retention by 2.88%

## Executive Summary
This paper introduces SSPU, a framework for selective unlearning from LLMs that leverages sparse autoencoder (SAE) features to construct guided subspaces for targeted parameter updates. The method addresses key limitations of prior unlearning approaches by providing interpretability through SAE feature supervision and stronger defense against adversarial attacks. Experiments on cybersecurity knowledge removal demonstrate that SSPU achieves better forgetting while preserving general capabilities compared to state-of-the-art baselines.

## Method Summary
SSPU employs a three-stage pipeline: (1) SAE feature selection using forget/retain activation scoring to identify top-K and bottom-K features at a specific layer, (2) subspace construction via QR decomposition on decoder vectors to create relevant and irrelevant subspaces, and (3) constrained optimization that updates MLP up-projection weights while projecting forget-set activations toward the irrelevant subspace. The approach combines gradient-based parameter updates with SAE-guided supervision, balancing targeted knowledge removal with retention of general capabilities.

## Key Results
- Reduces harmful knowledge accuracy by 3.22% compared to strongest baseline
- Improves retention by 2.88% on utility benchmarks (MMLU, TruthfulQA, GSM8K)
- Demonstrates greater robustness to jailbreak prompts, lowering malicious accuracy by up to 13.59%
- Layer 3 optimization yields optimal unlearning efficacy in gemma-2-2b-it model

## Why This Works (Mechanism)

### Mechanism 1: SAE-Guided Subspace Construction Enables Targeted Activation Steering
By projecting forget-set activations into a subspace orthogonal to forget-relevant features, the model's ability to represent and recall targeted knowledge is specifically reduced. The method constructs two subspaces: $U_{reg}$ from forget-relevant SAE decoder vectors and $U^\perp$ from irrelevant features, then penalizes forget activations for distance from a control vector in $U^\perp$.

### Mechanism 2: Parameter-Space Optimization Provides Robustness Against Adversarial Prompts
Unlike inference-time activation steering, SSPU modifies model weights via constrained optimization, creating a more permanent form of unlearning. These updates are constrained to the subspace defined by forget-relevant features, disrupting knowledge representation pathways more fundamentally than inference-time intervention.

### Mechanism 3: Layer-Specific Feature Selection Concentrates Intervention Efficacy
Knowledge in LLMs is not uniformly distributed across layers. The paper performs layer-wise analysis by steering top-K features at different layers and measuring accuracy drop on the forget set, finding that shallower layers yield greater accuracy reduction and focusing intervention where forget-relevant knowledge is most prominently encoded.

## Foundational Learning

**Sparse Autoencoders (SAE) for Mechanistic Interpretability**
Why needed: This is the foundational tool. The entire method relies on the SAE's ability to decompose LLM activations into sparse, human-interpretable features.
Quick check: Given an LLM's hidden state activation `x`, what are the two main outputs of a trained SAE, and which one is used to construct the unlearning subspaces in this paper?

**Subspace Projection via QR Decomposition**
Why needed: The paper's core technique involves constructing orthogonal "relevant" and "irrelevant" subspaces from SAE decoder vectors. QR decomposition turns a set of vectors into an orthonormal basis for a subspace.
Quick check: If you have a matrix `V` where each column is an SAE decoder vector, what does the `Q` matrix from its QR decomposition represent, and why is it necessary for computing orthogonal projections?

**Gradient-Based Unlearning vs. Inference-Time Steering**
Why needed: This paper positions itself as a solution to the limitations of both existing gradient-based methods (lack of interpretability) and SAE steering methods (lack of robustness).
Quick check: Contrast the two approaches: how does each method attempt to "remove" knowledge, and what is the primary advantage and disadvantage of each as cited in the paper?

## Architecture Onboarding

**Component map:**
SAE Feature Extractor -> Data-Driven Feature & Layer Selector -> Subspace Constructor -> SSPU Optimizer

**Critical path:**
1. Select a candidate layer and pre-trained SAE
2. Run the Feature & Layer Selector on forget/retain data to identify top/bottom K features and confirm layer efficacy
3. Run the Subspace Constructor to generate $U_{reg}$ and $U^\perp$
4. Initialize the SSPU Optimizer with these subspaces and run constrained fine-tuning on the target LLM
5. Evaluate on forget-set accuracy, utility benchmarks, and adversarial (jailbreak) prompts

**Design tradeoffs:**
- Feature Count (K): Larger K may capture more of the forget concept but risks including features with side effects (entanglement). Paper uses K=1024
- Subspace Regularization ($\lambda_{reg}$): Higher weight enforces stricter confinement of parameter updates to the "relevant" subspace, potentially improving retention but risking incomplete forgetting. Paper uses $1 \times 10^{-4}$
- Steering Coefficient ($\gamma$): Higher $\gamma$ forces activations further into the "irrelevant" subspace, potentially causing more aggressive degradation

**Failure signatures:**
- Catastrophic Utility Loss: Model loses general capabilities (e.g., fails MMLU, GSM8K). Likely due to SAE feature entanglement or excessive steering coefficient
- Incomplete Forgetting: Model still answers forget-set questions correctly. Could be due to suboptimal layer selection, insufficient K, or low learning rate
- Jailbreak Vulnerability: Model is robust on direct queries but succumbs to obfuscated prompts. Indicates subspace intervention didn't fundamentally alter knowledge representation pathways

**First 3 experiments:**
1. Baseline Layer Sweep: Run the Feature & Layer Selector on multiple layers for a given forget corpus. Plot accuracy drop to empirically verify the optimal layer
2. Ablation on Subspace Components: Run SSPU with only $L_{unlearn}$ (no $L_{reg}$), only $L_{reg}$ (no unlearn push), and the full method. Compare forget accuracy and utility retention
3. Jailbreak Robustness Test: Take the best SSPU model and compare its performance on four jailbreak datasets against an SAE-steering baseline and a gradient-ascent model to replicate the key robustness claim

## Open Questions the Paper Calls Out

**Open Question 1:** Can SSPU be adapted for models or domains where well-trained SAEs are unavailable or difficult to train?
The paper notes that its method relies on the availability of a well-trained sparse autoencoder, which may not be feasible for highly specialized domains or proprietary models. No fallback or alternative was explored.

**Open Question 2:** Does SSPU provide formal guarantees that capabilities orthogonal to the forget subspace remain preserved?
Although the method constrains parameter updates to a subspace identified as "relevant," it does not explicitly guarantee that unrelated capabilities outside this subspace remain entirely unaffected.

**Open Question 3:** Is there a principled method for determining optimal subspace dimensionality (K) without empirical tuning?
The dimensionality of the subspaces introduces additional hyperparameters that require empirical tuning for optimal trade-offs. K=1024 was used without systematic ablation or theoretical justification.

**Open Question 4:** How does SSPU perform when forget and retain corpora have significant semantic overlap?
The approach assumes access to both a forget corpus and a representative retain corpus, which may not always be clearly separable in real-world use cases. WMDP-Cyber and WikiText are cleanly separated in the evaluation.

## Limitations

- The method relies on well-trained SAEs, limiting applicability to domains where SAE training is difficult or unavailable
- The approach does not provide formal guarantees that unrelated capabilities outside the relevant subspace remain entirely unaffected
- The dimensionality of the subspaces (choice of K) introduces hyperparameters requiring empirical tuning without principled selection criteria

## Confidence

**High Confidence:** The technical implementation details of SSPU (subspace construction via QR decomposition, constrained optimization framework) are well-specified and reproducible based on the provided pseudocode and parameter settings.

**Medium Confidence:** The empirical claims about SSPU's superiority over baselines (3.22% better forgetting, 2.88% better retention) are supported by the reported results, but the evaluation scope is limited to specific benchmarks and may not generalize to all unlearning scenarios.

**Low Confidence:** The interpretability claims regarding SAE feature supervision are largely asserted rather than demonstrated - the paper does not provide concrete examples of how the extracted features map to human-understandable concepts or how this interpretability aids practical unlearning decisions.

## Next Checks

1. **Cross-Domain Generalization Test:** Evaluate SSPU on multiple forget domains (e.g., medical information, personal data) beyond cybersecurity to assess whether the SAE-based approach generalizes or is domain-specific.

2. **Adversarial Robustness Benchmark:** Implement a comprehensive adversarial evaluation suite that includes gradient-based attacks, representation-space optimization, and prompt engineering to stress-test the claimed robustness claims beyond the four jailbreak datasets.

3. **Computational Efficiency Analysis:** Measure and compare the total computational overhead (SAE training time, feature selection cost, fine-tuning iterations) against alternative unlearning methods to assess practical deployment feasibility.