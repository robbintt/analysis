---
ver: rpa2
title: 'Conversational Explanations: Discussing Explainable AI with Non-AI Experts'
arxiv_id: '2503.16444'
source_url: https://arxiv.org/abs/2503.16444
tags:
- explanations
- data
- participants
- emcee
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EMCEE, a conversational explanation system
  for explainable AI (XAI) that addresses the challenge of data scarcity in training
  conversational XAI systems. The core method uses a repetition penalty to enhance
  data diversity and a hallucination detector to filter out factually incorrect synthetic
  data when training on self-generated conversations.
---

# Conversational Explanations: Discussing Explainable AI with Non-AI Experts

## Quick Facts
- **arXiv ID:** 2503.16444
- **Source URL:** https://arxiv.org/abs/2503.16444
- **Reference count:** 40
- **Primary result:** EMCEE achieves 81.6% BLEU and 80.5% ROUGE improvements over baselines through conversational XAI explanations

## Executive Summary
This paper introduces EMCEE, a conversational explanation system for explainable AI (XAI) that addresses the challenge of data scarcity in training conversational XAI systems. The core method uses a repetition penalty to enhance data diversity and a hallucination detector to filter out factually incorrect synthetic data when training on self-generated conversations. Automatic evaluations show EMCEE achieves relative improvements of 81.6% in BLEU and 80.5% in ROUGE compared to baseline models. Human evaluations with 60 participants demonstrate that EMCEE significantly improves users' comprehension, acceptance, trust, and collaboration with static explanations compared to baseline models and control groups. The system effectively helps non-AI experts understand and utilize AI explanations through free-form conversations following static explanations.

## Method Summary
The EMCEE system trains a Vision Language Model (LLaVA-1.5) through iterative self-training on synthetic conversations about XAI explanations. The process involves generating synthetic conversations using a repetition penalty to promote diversity, filtering out factually incorrect turns with a hallucination detector, and fine-tuning the model via LoRA. The system handles four XAI methods (LIME, Grad-CAM, SHAP, Integrated Gradients) on ImageNet images, generating 2000 synthetic conversations per iteration. Key hyperparameters include temperature=1.2, repetition penalty=1.1, LoRA rank=128, and 5 total training iterations.

## Key Results
- EMCEE achieves 81.6% relative improvement in BLEU score and 80.5% in ROUGE compared to baseline models
- Human evaluations (N=60) show significant improvements in comprehension, acceptance, trust, and collaboration
- The repetition penalty and hallucination detector prevent model degeneration during iterative training
- Users show better understanding and engagement with static XAI explanations through conversational follow-ups

## Why This Works (Mechanism)

### Mechanism 1: Repetition Penalty for Diversity
The system modifies token selection during generation by dividing logits by a penalty factor (θ=1.1) for tokens already appearing in the context. This forces exploration of less probable tokens, preventing repetitive outputs and model collapse. Evidence shows performance drops are milder with this penalty applied.

### Mechanism 2: Hallucination Detection
A BERT-base classifier (79.5% accuracy) filters out factually incorrect synthetic turns before training. The detector identifies hallucinations in machine responses, removing them from the training dataset to prevent error propagation through iterative training cycles.

### Mechanism 3: Iterative Self-Training
The generation-filtering-finetuning loop allows the model to specialize in XAI explanations. Starting with LLaVA-1.5, the system generates synthetic conversations, filters them, and fine-tunes the model. This process repeats for 5 iterations, with performance plateauing rather than collapsing due to the guard rails.

## Foundational Learning

- **Feature Attribution (XAI Methods)**
  - Why needed: EMCEE discusses specific XAI outputs (LIME, SHAP, Grad-CAM, Integrated Gradients) that produce heatmaps or importance scores
  - Quick check: Does the system generate explanations itself or discuss pre-generated static explanations?

- **Vision Language Models (VLMs) - LLaVA-1.5**
  - Why needed: The base architecture connects CLIP vision encoder to Vicuna LLM for image+explanation processing
  - Quick check: During fine-tuning, are vision encoder weights updated or only LoRA adapter weights?

- **Synthetic Data Generation & Model Collapse**
  - Why needed: Understanding that models degenerate when training on self-generated data explains the need for repetition penalty and hallucination detection
  - Quick check: What are the two data quality issues identified as risks when training on synthetic data?

## Architecture Onboarding

- **Component map:** Input Interface -> Base Model (LLaVA-1.5) -> Generation Module -> Filtering Module -> Training Module
- **Critical path:** Prompt Engineering -> Synthetic Generation -> Hallucination Filtering -> LoRA Fine-tuning
- **Design tradeoffs:** Detector accuracy vs. data volume (79.5% accuracy allows some noise); Generic capability vs. XAI specialization
- **Failure signatures:** BLEU/ROUGE dropping after iteration 3-4; Domain confusion (discussing image not explanation); Detector drift causing refusal to answer
- **First 3 experiments:**
  1. Hallucination ablation: Retrain without detector to verify BLEU/ROUGE improvements
  2. Penalty sensitivity: Vary θ (1.0, 1.1, 1.5) to find diversity-incoherence knee
  3. Detector calibration: Run on 42 test conversations to calculate false positive rate

## Open Questions the Paper Calls Out
- Can EMCEE adapt to example-based explanation methods or NLP tasks beyond feature attribution?
- Can faithfulness hallucination detection improve contextual relevance of synthetic training data?
- How does user prior AI knowledge level moderate conversational explanation effectiveness?

## Limitations
- Hallucination detector accuracy of 79.5% introduces noise with ~20% misclassification rate
- Evaluation based on 60 participants provides limited generalizability across user populations
- Long-term stability beyond 5 training iterations remains uncertain with performance plateauing

## Confidence
- **High:** Technical implementation of repetition penalty mechanism and BLEU/ROUGE improvements
- **Medium:** Human evaluation results (N=60) showing comprehension, trust, and collaboration improvements
- **Low:** Long-term stability of iterative training approach beyond 5 rounds

## Next Checks
1. Run hallucination detector on 42 test conversations to calculate false positive rate on real vs. synthetic data
2. Manually examine 100 filtered synthetic turns to quantify detector false positive rate and identify error patterns
3. Extend iterative training to 10 rounds to observe whether performance continues plateauing or recovers from dips