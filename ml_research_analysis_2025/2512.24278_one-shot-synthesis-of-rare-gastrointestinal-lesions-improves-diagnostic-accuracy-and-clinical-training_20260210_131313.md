---
ver: rpa2
title: One-shot synthesis of rare gastrointestinal lesions improves diagnostic accuracy
  and clinical training
arxiv_id: '2512.24278'
source_url: https://arxiv.org/abs/2512.24278
tags:
- rare
- image
- endorare
- diffusion
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Rare gastrointestinal lesions are seldom encountered in clinical
  practice, creating data scarcity that hinders both reliable AI model development
  and novice clinician training. To address this, EndoRare is proposed as a one-shot,
  retraining-free generative framework that synthesizes diverse, high-fidelity lesion
  exemplars from a single reference image.
---

# One-shot synthesis of rare gastrointestinal lesions improves diagnostic accuracy and clinical training

## Quick Facts
- **arXiv ID:** 2512.24278
- **Source URL:** https://arxiv.org/abs/2512.24278
- **Reference count:** 40
- **One-line primary result:** One-shot generative framework synthesizes rare gastrointestinal lesions from a single reference image, improving both AI diagnostic accuracy and novice clinician training.

## Executive Summary
Rare gastrointestinal lesions are seldom encountered in clinical practice, creating data scarcity that hinders both reliable AI model development and novice clinician training. EndoRare is proposed as a one-shot, retraining-free generative framework that synthesizes diverse, high-fidelity lesion exemplars from a single reference image. By leveraging language-guided concept disentanglement, EndoRare separates pathognomonic lesion features from non-diagnostic attributes, encoding the former into a learnable prototype embedding while varying the latter to ensure diversity. The framework was validated across four rare pathologies: calcifying fibrous tumor, juvenile polyposis syndrome, familial adenomatous polyposis, and Peutz-Jeghers syndrome. Synthetic images were judged clinically plausible by experts and, when used for data augmentation, significantly enhanced downstream AI classifiers, improving the true positive rate at low false-positive rates. Crucially, a blinded reader study demonstrated that novice endoscopists exposed to EndoRare-generated cases achieved a 0.400 increase in recall and a 0.267 increase in precision. These results establish a practical, data-efficient pathway to bridge the rare-disease gap in both computer-aided diagnostics and clinical education.

## Method Summary
EndoRare is a three-stage pipeline: (1) Knowledge-enhanced diffusion pretraining on routine image-text pairs with LLM-extracted attributes (morphology, color, pathology, location); (2) Cross-modal visual concept learning via CLIP alignment loss; (3) One-shot synthesis using Prototype-Specific Embedding (PSE) + Tailored Lesion Embedding (TLE) + Fusion Module (FM). The diffusion backbone remains frozen; only learnable embeddings are optimized. Given a single target rare disease image, PSE is optimized via reconstruction loss, TLE is extracted using trained CLIP encoders, and the fusion module combines these via attention-weighted aggregation to guide the denoising process.

## Key Results
- Novice endoscopists exposed to EndoRare-generated cases achieved a 0.400 increase in recall and a 0.267 increase in precision in lesion detection.
- Synthetic images significantly enhanced downstream AI classifiers, improving the true positive rate at low false-positive rates.
- Expert clinicians judged synthetic images as clinically plausible across four rare pathologies: calcifying fibrous tumor, juvenile polyposis syndrome, familial adenomatous polyposis, and Peutz-Jeghers syndrome.

## Why This Works (Mechanism)

### Mechanism 1
Language-guided concept disentanglement enables synthesis diversity without compromising lesion identity. An LLM extracts structured attributes from clinical reports, separating pathognomonic features (encoded into a fixed prototype embedding) from non-diagnostic attributes (varied during generation). Cross-modal alignment loss enforces consistency between visual features and their textual counterparts. Core assumption: Clinical reports reliably capture diagnostically relevant attributes; extracted text aligns with visual features in CLIP embedding space. Evidence: [abstract] "By leveraging language-guided concept disentanglement..."; [Section 4.4] Attribute alignment term optimizes: `L_total = E[Σ_k ||v_k - e_k||² + λ||ε_t - ε_θ(z_t, t, τ_θ(y))||²]`; [corpus] PathoGen (arXiv:2601.08127) similarly uses diffusion-based lesion synthesis with attribute control. Break condition: If clinical reports lack structured attribute descriptions or contain contradictory annotations, disentanglement fails and generated images either lose identity or exhibit insufficient diversity.

### Mechanism 2
Freezing the diffusion backbone while learning only a compact prototype embedding prevents overfitting in one-shot settings. Given a single target image I^T, a learnable vector p_T (Prototype-Specific Embedding) is optimized via reconstruction loss while the diffusion model ε_θ remains frozen. This avoids instance memorization that occurs with full fine-tuning methods like DreamBooth. Core assumption: The pretrained diffusion prior on routine diseases transfers sufficiently to rare lesions; a single prototype embedding can capture identity without model weight updates. Evidence: [abstract] "one-shot, retraining-free generative framework"; [Section 4.5] "With the pretrained diffusion model ε_θ fixed, adaptation occurs only in p_T."; [Section 2.5 ablation] Adding PSE alone reduces FID by 19.05 (-6.8%) but also reduces diversity (LPIPS -0.1095), confirming PSE captures identity at cost of variation—necessitating TLE. Break condition: If the rare lesion morphology is fundamentally out-of-distribution relative to the pretrained prior (e.g., novel organ or imaging modality), frozen backbone cannot generalize regardless of prototype quality.

### Mechanism 3
Fusing prototype embedding with attribute-aligned text embeddings balances fidelity and diversity in the denoising trajectory. The Tailored Lesion Embedding r_T encodes attribute-specific visual features from the prototype image via CLIP encoders {v^k_T}. A fusion module computes similarity matrices between r_T and p_T, derives attention weights via softmax, and aggregates into fused embedding e_T. This e_T conditions the denoising process at each timestep. Core assumption: Attention-weighted aggregation preserves core morphology from p_T while allowing attribute variation from r_T; CLIP encoders reliably extract attribute-specific visual features. Evidence: [Section 4.5] "The fusion process involves computing similarity matrices between r_T and p_T..."; [Section 2.5 ablation] Full model (PSE + TLE + FM) achieves best FID (249.66) and highest CLIP-I (86.15), with LPIPS 0.4352—demonstrating the fusion recovers diversity lost by PSE alone. Break condition: If attention weighting collapses to favor only one embedding source (e.g., always attends to p_T), diversity degrades to near-zero; if r_T dominates, identity drifts and class faithfulness fails.

## Foundational Learning

- **Concept: Latent Diffusion Models (LDMs)**
  - **Why needed here:** EndoRare builds on Stable Diffusion, which operates in compressed latent space rather than pixel space. Understanding the denoising objective `E[||ε_t - ε_θ(z_t, t, τ_θ(y))||²]` is essential to grasp how conditioning embeddings steer generation.
  - **Quick check question:** Can you explain why LDMs are more computationally efficient than pixel-space diffusion, and where the VAE encoder/decoder fit in the pipeline?

- **Concept: CLIP Image-Text Alignment**
  - **Why needed here:** The framework relies on CLIP encoders to extract both text embeddings (τ_θ for attributes) and image embeddings (v^k_T for visual concepts). Cross-modal alignment is the core mechanism for disentanglement.
  - **Quick check question:** Given an image and a text description, how would you compute their CLIP similarity score, and what does it measure semantically?

- **Concept: One-Shot Personalization (without Fine-Tuning)**
  - **Why needed here:** Unlike DreamBooth or Textual Inversion which optimize model weights or text embeddings via gradient descent on multiple images, EndoRare learns only a single prototype vector from one image while freezing the backbone.
  - **Quick check question:** Why does full fine-tuning on a single image typically cause overfitting/memorization, and how does freezing the backbone mitigate this?

## Architecture Onboarding

- **Component map:**
  Routine Disease Dataset -> LLM Attribute Extraction -> Structured Prompts
  Routine Disease Dataset -> Diffusion Pretraining on Image-Text Pairs -> Frozen ε_θ
  Rare Disease Single Image I^T -> CLIP Image Encoder -> {v^k_T} (attribute embeddings)
  PSE Optimization: p_T <- Reconstruction Loss; Tailored Lesion Embedding r_T
  Fusion Module: e_T = Attention(p_T, r_T)
  Conditional Denoising: z_{t-1} <- ε_θ(z_t, t, e_T)
  VAE Decoder -> Synthetic Image

- **Critical path:**
  1. Pretrain diffusion model on routine diseases with structured prompts (Section 4.3) — this is the transferable prior.
  2. Given rare disease image, optimize p_T via reconstruction loss with frozen ε_θ (Section 4.5, Eq. 3).
  3. Extract attribute embeddings {v^k_T} using CLIP image encoders; format into text template to form r_T.
  4. Fuse p_T and r_T via attention-weighted similarity; use e_T to condition denoising (Eq. 4).

- **Design tradeoffs:**
  - **PSE-only vs. PSE+TLE+FM:** PSE alone improves fidelity but sacrifices diversity (LPIPS drops 0.1095). Adding TLE recovers diversity but requires reliable attribute extraction. FM balances both.
  - **Frozen vs. fine-tuned backbone:** Freezing prevents memorization and enables scalability (no per-disease model storage), but may underperform if rare lesions are out-of-distribution.
  - **LLM-based attribute extraction:** Enables automation but introduces failure risk if reports are unstructured or ambiguous. Assumption: Senior endoscopists re-annotate reports before LLM processing.

- **Failure signatures:**
  - **Mode collapse / low diversity:** Check LPIPS; if <0.3, TLE or FM may not be functioning correctly. Verify CLIP encoders are extracting distinct attributes.
  - **Identity drift / class unfaithfulness:** Check CLIP-I score and clinical ratings; if FID is low but CLIP-I is also low, fusion may be over-weighting r_T at expense of p_T.
  - **Memorization of training image:** If generated images are near-exact replicas of the single input, PSE may have overfitted—reduce optimization steps or add noise to p_T initialization.
  - **Attribute extraction failure:** If synthetic images show implausible morphology/color combinations, inspect LLM outputs for malformed templates.

- **First 3 experiments:**
  1. **Ablation on PSE optimization steps:** Vary the number of steps optimizing p_T on the single image. Plot FID vs. LPIPS to find the sweet spot where identity is preserved without overfitting. Compare against the paper's reported 29.65 FID reduction.
  2. **Cross-attribute swapping:** Take a CFT prototype image but swap its "location" attribute with that of JPS. Generate images and assess whether morphology remains CFT-like while background changes. This validates disentanglement quality.
  3. **Classifier augmentation benchmark:** Train a ResNet classifier on real rare disease data only vs. real + EndoRare synthetic data. Measure PR-AUC and ROC-AUC on a held-out test set. Reproduce the paper's reported PR-AUC improvement from 0.612 → 0.708.

## Open Questions the Paper Calls Out

- **Does EndoRare maintain diagnostic utility across diverse endoscopic devices and multi-center datasets?**
  - **Basis in paper:** [explicit] The authors state that "larger multi-center studies across devices and expertise levels are needed to assess durability and generalizability" beyond the current limited validation set.
  - **Why unresolved:** Current results derive from a small sample of four entities and five novice readers at a single center, limiting the understanding of device-specific biases.
  - **What evidence would resolve it:** Validation across multiple clinical sites involving different endoscope manufacturers and operator expertise levels.

- **Can formal privacy audits confirm that the retraining-free approach prevents memorization of patient data?**
  - **Basis in paper:** [explicit] The discussion notes that "mitigation, however, is not proof of absence; future releases should include formal privacy audits (nearest-neighbour and attribute-leakage analyses)."
  - **Why unresolved:** While the method avoids gradient updates on patient pixels, the risk of leaking prototypes via learned embeddings remains unquantified.
  - **What evidence would resolve it:** Results from membership inference attacks and nearest-neighbor analysis comparing synthetic outputs to the private training set.

- **How robust is the framework to low-quality or ambiguous textual descriptions in endoscopy reports?**
  - **Basis in paper:** [explicit] The authors acknowledge the method "assumes high-quality textual descriptors" and identifies "robust report parsing and attribute validation" as a necessary next step.
  - **Why unresolved:** The method relies on structured attribute extraction; its sensitivity to noisy, incomplete, or conflicting clinical text is untested.
  - **What evidence would resolve it:** Performance benchmarks using uncurated, noisy clinical reports as inputs rather than the curated reports used in the study.

## Limitations
- **Limited generalizability:** Current validation is based on a small sample of four rare entities and five novice readers at a single center, with acknowledged need for multi-center studies.
- **Privacy assumptions:** While retraining-free design mitigates memorization risk, formal privacy audits are not yet conducted to confirm absence of data leakage.
- **Quality of input text:** Method assumes high-quality textual descriptors; robustness to noisy, incomplete, or conflicting clinical text remains untested.

## Confidence
- **High confidence:** The mechanism descriptions are well-supported by equations and ablation studies in the paper; the architectural components are clearly specified.
- **Medium confidence:** Some implementation details (hyperparameters, exact LLM prompting, dataset specifics) are not fully specified, requiring reasonable assumptions.
- **Low confidence:** The long-term clinical impact and privacy guarantees are not yet established through formal audits or multi-center validation.

## Next Checks
1. **Reproduce the ablation study:** Vary PSE optimization steps and plot FID vs. LPIPS to confirm the sweet spot where identity is preserved without overfitting.
2. **Validate disentanglement:** Perform cross-attribute swapping experiments to test whether morphology remains consistent when attributes are varied.
3. **Benchmark classifier augmentation:** Train a ResNet classifier on real rare disease data only vs. real + EndoRare synthetic data, measuring PR-AUC and ROC-AUC on a held-out test set.