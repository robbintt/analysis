---
ver: rpa2
title: 'Learning What Matters: Causal Time Series Modeling for Arctic Sea Ice Prediction'
arxiv_id: '2509.09128'
source_url: https://arxiv.org/abs/2509.09128
tags:
- causal
- learning
- arctic
- pcmci
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving Arctic sea ice
  extent (SIE) prediction by moving beyond correlation-based deep learning methods
  to a causality-aware framework. The authors integrate Multivariate Granger Causality
  (MVGC) and PCMCI+ for causal feature selection, identifying ocean-atmospheric drivers
  with genuine influence on SIE dynamics.
---

# Learning What Matters: Causal Time Series Modeling for Arctic Sea Ice Prediction

## Quick Facts
- **arXiv ID:** 2509.09128
- **Source URL:** https://arxiv.org/abs/2509.09128
- **Reference count:** 8
- **Primary result:** Causal feature selection using MVGC and PCMCI+ improves Arctic sea ice extent prediction accuracy and interpretability compared to correlation-based deep learning approaches

## Executive Summary
This paper addresses the challenge of improving Arctic sea ice extent (SIE) prediction by moving beyond correlation-based deep learning methods to a causality-aware framework. The authors integrate Multivariate Granger Causality (MVGC) and PCMCI+ for causal feature selection, identifying ocean-atmospheric drivers with genuine influence on SIE dynamics. These causal features are used as inputs to a hybrid GRU-LSTM model, which leverages short- and long-term temporal dependencies for forecasting. Using 43 years of data, experiments show that models trained on causally selected features achieve improved prediction accuracy and interpretability across 1- to 6-month lead times. For instance, causal models demonstrate lower RMSE and higher R² values compared to baseline models using all variables. The approach enhances both robustness and interpretability, offering a scalable solution for complex, high-dimensional domains such as climate forecasting.

## Method Summary
The framework combines causal feature selection with deep learning for Arctic sea ice extent prediction. It uses Multivariate Granger Causality (MVGC) and PCMCI+ to identify causally influential predictors from 10 ocean-atmospheric variables (pressure, radiation, wind, humidity, temperature, rainfall, snowfall, SST, SSS) plus SIE itself. The causal features are then used as inputs to a hybrid GRU-LSTM architecture (64-unit GRU → 128-unit LSTM → 64-unit dense → output) with 21-step look-back windows. The model is trained separately for 1-6 month lead times using 43 years of ERA-5 and NSIDC data, split into training (1979-2013), daily testing (2014-2018), and monthly testing (2014-2021).

## Key Results
- Causal models (DL_GC, DL_PCMCI+, DL_DPCMCI+) outperform baseline DL_vanilla (all 10 variables) across all lead times
- DL_DPCMCI+ (transferring daily causal features to monthly models) shows highest accuracy for long-range forecasts (5-6 months)
- RMSE reductions of 5-15% and R² improvements of 0.05-0.15 compared to correlation-based approaches
- PCMCI+ identifies different causal variables for daily vs. monthly data, demonstrating temporal resolution dependency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Causal feature selection isolates genuine predictive signals while eliminating spurious correlations that degrade deep learning generalization.
- **Mechanism:** The framework uses Multivariate Granger Causality (MVGC) and PCMCI+ to filter the input space. By retaining only variables that pass statistical tests for conditional independence (PCMCI+) or temporal precedence (MVGC), the model reduces the hypothesis space, preventing the neural network from learning non-robust statistical noise.
- **Core assumption:** The causal structures identified in historical training data (1979-2013) remain stationary and valid for the test period (2014-2021).
- **Evidence anchors:**
  - "Identifies causally influential predictors, prioritizes direct causes... reducing unnecessary features."
  - "Causal discovery algorithms... aim to identify both direct and indirect drivers... allowing models to focus on features with true explanatory power."
- **Break condition:** If the underlying climate system undergoes a "regime shift" where previously minor variables become dominant drivers (non-stationarity), the fixed causal graph will fail to capture new dynamics.

### Mechanism 2
- **Claim:** A hybrid GRU-LSTM architecture captures distinct temporal dependencies, with GRU handling short-term transients and LSTM retaining long-term state.
- **Mechanism:** The architecture chains a GRU layer (64 units) followed by an LSTM layer (128 units). The GRU acts as an efficient filter for immediate fluctuations (e.g., daily weather noise), passing a cleaner representation to the LSTM, which utilizes its cell state to maintain long-term trends (e.g., seasonal melting cycles).
- **Core assumption:** Sequential processing (GRU → LSTM) is superior to parallel processing for this specific causal feature set.
- **Evidence anchors:**
  - "Hybrid GRU-LSTM models leverage the speed and efficiency of GRUs with the memory retention capabilities of LSTMs."
  - "This architecture is tailored to capture temporal dependencies at multiple scales—short-term patterns via GRUs and long-term trends via LSTMs."
- **Break condition:** If the look-back window (21 steps) is insufficient to capture the "long-term" dependencies required by the LSTM, the memory cell degrades, and the model behaves like a simple autoregressive estimator.

### Mechanism 3
- **Claim:** High-resolution causal discovery (daily data) transfers effectively to low-resolution forecasting (monthly data) by preserving fine-grained physical drivers.
- **Mechanism:** The DL_DPCMCI+ model uses causal features identified on daily data to train a monthly forecasting model. Daily resolution reveals transient causal links (e.g., rapid pressure changes) that are washed out in monthly averaging, providing stronger signals for the monthly predictor model.
- **Core assumption:** Causal links observed at high frequency aggregate meaningfully into lower-frequency predictive power.
- **Evidence anchors:**
  - "DL DPCMCI+ outperforms all other models... transfers causal features identified from daily data... effectively preserving high-resolution causal signals."
  - Shows that PCMCI+ identifies different variables for daily vs. monthly datasets (e.g., SST is causal in monthly but not daily).
- **Break condition:** If the computational cost of daily causal discovery is prohibitive, or if the daily signals are pure noise in the context of monthly trends.

## Foundational Learning

- **Concept:** Granger Causality vs. Correlation
  - **Why needed here:** Standard deep learning minimizes loss via correlation, which often captures spurious relationships (e.g., ice cream sales and shark attacks). Granger Causality specifically tests if variable X provides unique, statistically significant information about future Y that is not contained in Y's own past.
  - **Quick check question:** If variable A correlates with B but lags behind B, can it Granger-cause B? (Answer: No, causality requires temporal precedence).

- **Concept:** PCMCI+ (PC algorithm with Momentary Conditional Independence)
  - **Why needed here:** Standard Granger Causality can identify indirect paths (e.g., A → B → C implies A causes C). PCMCI+ conditions on parents to isolate direct links, removing false positives common in high-dimensional climate systems where many variables move together.
  - **Quick check question:** Why is controlling for autocorrelation critical in climate time series causal discovery?

- **Concept:** Recurrent Gating (GRU/LSTM)
  - **Why needed here:** The paper uses a hybrid model. Understanding gating (how networks decide what to forget and what to keep) is necessary to diagnose why the model might fail to capture long-term ice trends.
  - **Quick check question:** In the paper's architecture, which layer is responsible for "memory retention" over extended intervals?

## Architecture Onboarding

- **Component map:** Input (21 timesteps) → GRU (64 units, 20% dropout) → LSTM (128 units, 20% dropout) → Dense (64 units) → Output

- **Critical path:**
  1. Data Curation: Merge ERA-5 and NSIDC data
  2. Causal Discovery (The Gatekeeper): Run MVGC and PCMCI+. This is the most critical step; if the wrong variables are pruned here, the DL model cannot recover them.
  3. Windowing: Structure data into 21-lag tensors
  4. Training: Train separate models for different lead times (1-6 months)

- **Design tradeoffs:**
  - PCMCI+ vs. MVGC: PCMCI+ is stricter (fewer features, better for interpretability) but may miss complex indirect drivers that MVGC catches
  - Daily vs. Monthly: Daily models offer lower RMSE but require significantly more compute for causal discovery; Monthly models are noisier but faster
  - DL_DPCMCI+: Transferring daily causal features to monthly models adds complexity but improved long-range accuracy (5-6 months) in the study

- **Failure signatures:**
  - Spurious Overfitting: If DL_vanilla (all features) outperforms causal models at very short horizons (1-month), the model is likely memorizing short-term noise rather than physics
  - High RMSE at 6-month horizons: Indicates the LSTM memory is degrading or the causal drivers selected are not stable over long seasons
  - SST Exclusion: MVGC excluded Sea Surface Temperature (SST). If testing in a radically different climate regime where SST dominates, this model will fail

- **First 3 experiments:**
  1. Baseline vs. Causal: Train DL_vanilla (all 10 vars) vs. DL_PCMCI+ (selected vars) on the same test set to quantify the "cost of correlation"
  2. Ablation on Lags: Rerun PCMCI+ with varying maximum lags (τ) to see if the optimal lag aligns with the 21-step look-back window used in the NN
  3. Cross-Resolution Test: Implement the DL_DPCMCI+ approach—train a monthly model using only features identified in the daily causal graph—to validate if high-res signals improve low-res forecasts

## Open Questions the Paper Calls Out

- Can the framework be extended to effectively account for spatial heterogeneity and multi-scale interactions in the Arctic climate system?
- Do adaptive strategies for lag selection improve the robustness of causal discovery compared to the fixed maximum lag approach used in this study?
- Does a lead-time-adaptive hybrid modeling strategy outperform single-model approaches across all forecast horizons?
- Can algorithmic enhancements reduce the computational overhead of PCMCI+ and MVGC for large causal graphs without sacrificing accuracy?

## Limitations
- Assumes causal relationships identified from historical data (1979-2013) remain valid for future predictions, which may not hold under climate regime shifts
- PCMCI+ and MVGC are computationally demanding, limiting scalability for operational forecasting with many environmental variables
- The model relies on spatially aggregated data, ignoring potential spatial heterogeneity and multi-scale interactions in the Arctic system

## Confidence
- Causal feature selection efficacy: Medium
- Hybrid GRU-LSTM architecture advantage: Medium
- Daily-to-monthly feature transfer effectiveness: Medium
- Model generalizability to regime shifts: Low

## Next Checks
1. Perform temporal cross-validation by training on 1979-2013 and testing on 2014-2021, then repeat with different time splits to assess causal feature stability
2. Conduct ablation studies varying the look-back window size to determine optimal temporal dependencies for Arctic sea ice dynamics
3. Test model robustness by introducing synthetic regime shifts in the validation set to evaluate performance degradation when causal relationships change