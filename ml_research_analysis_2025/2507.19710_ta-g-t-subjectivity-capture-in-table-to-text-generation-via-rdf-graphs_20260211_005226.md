---
ver: rpa2
title: 'Ta-G-T: Subjectivity Capture in Table to Text Generation via RDF Graphs'
arxiv_id: '2507.19710'
source_url: https://arxiv.org/abs/2507.19710
tags:
- text
- subjectivity
- pipeline
- subjective
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the underexplored problem of generating text
  that captures both objective descriptions and subjective interpretations from structured
  tables. The authors propose Ta-G-T, a three-stage pipeline that converts tabular
  data into subjective narratives through: (1) extraction of RDF triples for structured
  data representation, (2) aggregation of sentences into coherent narratives, and
  (3) infusion of subjectivity using style transfer techniques.'
---

# Ta-G-T: Subjectivity Capture in Table to Text Generation via RDF Graphs

## Quick Facts
- arXiv ID: 2507.19710
- Source URL: https://arxiv.org/abs/2507.19710
- Reference count: 7
- Outperforms Mistral-7B and Llama-2 while demonstrating strong zero-shot generalization to new domains

## Executive Summary
This work addresses the underexplored problem of generating text that captures both objective descriptions and subjective interpretations from structured tables. The authors propose Ta-G-T, a three-stage pipeline that converts tabular data into subjective narratives through: (1) extraction of RDF triples for structured data representation, (2) aggregation of sentences into coherent narratives, and (3) infusion of subjectivity using style transfer techniques. The approach leverages smaller fine-tuned T5 models rather than large language models, achieving competitive performance with a METEOR score of 25.46% and BERTScore of 82.50%. Human evaluations show strong coherence, coverage, and accuracy, with systematic incorporation of subjective elements. The modular pipeline design enables independent optimization of components, enhances factual accuracy through RDF grounding, and supports reusability across domains.

## Method Summary
Ta-G-T is a three-stage pipeline that transforms tables into subjective narratives: (1) RDF extraction converts table rows into structured triples (subject, predicate, object) using a deterministic heuristic, (2) a T5-large model fine-tuned on WebNLG verbalizes these triples into sentences, (3) a T5-large model fine-tuned on reversed Wiki Neutrality Corpus adds evaluative language to create subjective narratives. The pipeline uses smaller fine-tuned models rather than LLMs, achieving competitive performance while maintaining factual accuracy through RDF grounding. The modular design allows independent component optimization and better error isolation.

## Key Results
- Achieves METEOR score of 25.46% and BERTScore of 82.50% on subjectivity evaluation
- Human evaluations show strong coherence, coverage, and accuracy with systematic subjective elements
- Outperforms Mistral-7B and Llama-2 while demonstrating zero-shot generalization to Ta2TS dataset without fine-tuning
- Modular design enables independent optimization and better error attribution across stages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RDF triples as intermediate representations reduce hallucinations by grounding generation in structured semantics.
- Mechanism: Tables are converted deterministically to RDF triples (subject = first column, predicate = header, object = cell value) before any neural processing. This structured representation is then verbalized by a T5 model fine-tuned on WebNLG. The explicit triple structure constrains the generation space.
- Core assumption: The star-shaped RDF schema (single subject per row) captures sufficient relational semantics for coherent text; complex multi-entity relationships may be underspecified.
- Evidence anchors:
  - [abstract] "By incorporating RDFs, our approach enhances factual accuracy while maintaining interpretability."
  - [Section 5.1] "This transformation follows a deterministic heuristic, ensuring a direct, error-free mapping from tables to RDF triples."
  - [corpus] Weak direct corpus support for RDF-in-T2T specifically; related work (Joshi et al., 2024) cited for hallucination reduction via knowledge graphs but not independently verified here.
- Break condition: Tables with nested structures, merged cells, or cross-row dependencies that cannot be captured in single-star RDF graphs will likely produce incoherent or incomplete narratives.

### Mechanism 2
- Claim: Modular three-stage pipeline enables independent component optimization and granular error attribution.
- Mechanism: Each stage (RDF extraction → aggregation → subjectivity) is trained on separate datasets (WebNLG, synthetic aggregation pairs, reversed WNC). Failures can be isolated: factual errors trace to Stage 1, fluency issues to Stage 2, tone problems to Stage 3.
- Core assumption: Errors do not compound destructively across stages; the modular decomposition aligns with natural task boundaries.
- Evidence anchors:
  - [abstract] "The modular pipeline design enables independent optimization of components."
  - [Section 5.4] "By dividing the process into stages... the pipeline allows for independent optimization of components, giving flexibility and scalability."
  - [Section 6.3] Ablation shows 61-84% drop in subjectivity score when Stage 3 removed; METEOR drops 4.17 points when Stage 2 removed.
  - [corpus] No direct corpus validation of modular vs. end-to-end tradeoffs in T2T subjectivity tasks.
- Break condition: If downstream stages require contextual signals from upstream that are lost in transformation (e.g., subjectivity needing raw numerical trends not preserved in verbalized RDF), modular separation may limit output quality.

### Mechanism 3
- Claim: Style transfer via reverse fine-tuning on Wiki Neutrality Corpus injects subjectivity while preserving factual content.
- Mechanism: T5-large is fine-tuned on reversed WNC pairs (neutral → subjective), learning to add evaluative adjectives and interpretive phrasing. Subjectivity is added after factual content is fixed, reducing risk of hallucinated interpretations.
- Core assumption: WNC's encyclopedic subjectivity patterns transfer to financial, weather, and sports domains without domain-specific fine-tuning.
- Evidence anchors:
  - [Section 5.3] "We fine-tune a T5-large model on the reverse of the WNC, where the original subjective text is treated as the target output."
  - [Table 3] Subjectivity scores drop from 14.52% → 4.32% (Finance), 12.35% → 4.85% (Sports), 24.62% → 3.97% (Weather) when Stage 3 ablated.
  - [corpus] Weak corpus support for WNC-based style transfer in technical domains; generalization claim rests on this paper's experiments alone.
- Break condition: Domain-specific evaluative conventions (e.g., "bullish" in finance, "muggy" in weather) absent from WNC will not be reliably generated; output may sound generic or mis calibrated.

## Foundational Learning

- Concept: **RDF (Resource Description Framework) triples**
  - Why needed here: Understanding the (subject, predicate, object) structure is essential for debugging Stage 1 extraction and interpreting how tabular relationships are preserved.
  - Quick check question: Given a table row [Company: Acme | Revenue: $1M | Year: 2023], what are the two RDF triples this produces?

- Concept: **Style transfer in sequence-to-sequence models**
  - Why needed here: Stage 3 applies neural style transfer (neutral → subjective) rather than generating subjective content directly; understanding this helps diagnose when factual content is accidentally altered.
  - Quick check question: What is the risk if the style transfer model's training data (WNC) contains systematic biases in what kinds of content receive subjective treatment?

- Concept: **Aggregation/co-reference resolution in NLG**
  - Why needed here: Stage 2 merges independently generated sentences; unresolved pronouns or redundant mentions indicate aggregation failures, not extraction failures.
  - Quick check question: If output reads "Acme had revenue of $1M. It grew by 10%. It had expenses of $500K," what aggregation problem is present?

## Architecture Onboarding

- Component map:
  - Table input → RDF triple extraction (deterministic) → Single-triple verbalization → Sentence aggregation → Subjectivity infusion → Final output

- Critical path: Table input → RDF triple extraction (deterministic) → Single-triple verbalization → Sentence aggregation → Subjectivity infusion → Final output. Any failure in Stage 1 propagates factually incorrect content through all subsequent stages.

- Design tradeoffs:
  - Smaller models (T5-large ~770M) vs. LLMs: Lower inference cost, but subjectivity richness lags GPT-3.5 in human evaluation (8.59 vs 9.06)
  - Modular vs. end-to-end: Better interpretability and error isolation, but potential information loss across stage boundaries
  - Zero-shot on Ta2TS vs. fine-tuning: Better generalization demonstrated, but BLEU-4 (1.63) suggests surface-form mismatch with references

- Failure signatures:
  - Low subjectivity score + high accuracy → Stage 3 under-active; check WNC fine-tuning
  - High subjectivity + low accuracy → Factual drift; inspect Stage 1 triple extraction or Stage 3 over-transfer
  - Fragmented output, low METEOR → Stage 2 aggregation failure; check co-reference resolution
  - Domain-specific vocabulary missing → WNC coverage gap; consider domain-augmented training

- First 3 experiments:
  1. **Run full pipeline on 10 tables from each domain** (Finance, Weather, Sports) with stage-level logging. Manually verify RDF triples match table content; identify systematic extraction errors.
  2. **Ablate Stage 2** by feeding Stage 1 output directly to Stage 3. Measure METEOR/ROUGE-L degradation to quantify aggregation contribution (expect ~4-5 point METEOR drop per Table 4).
  3. **Evaluate subjectivity classifier calibration** by applying the Movies-trained classifier to 20 human-labeled Ta2TS outputs. Verify scores align with human subjectivity ratings (H-mean 8.59); if mis calibrated, classifier may not transfer across domains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would integrating logical forms (e.g., lambda calculus) or semantic graphs as intermediate representations improve analytical reasoning capabilities compared to RDF triples?
- Basis in paper: [explicit] Conclusion states: "Future work could explore enhancing the pipeline with logical forms like lambda calculus for more analytical text generation, especially in complex domains like finance. Additionally, alternative intermediate representations, such as semantic graphs, may improve flexibility and handling of more intricate tabular formats."
- Why unresolved: Only RDF triples have been tested as the intermediate representation; no comparison with logical forms or semantic graphs exists.
- What evidence would resolve it: Comparative experiments on inference-heavy datasets (e.g., LogicNLG) measuring accuracy of analytical claims in generated text across different intermediate representations.

### Open Question 2
- Question: Can robust automated metrics be developed that reliably capture subjectivity nuances (evaluative depth, tone appropriateness) beyond binary classification?
- Basis in paper: [explicit] Limitations section states: "Developing robust, automated metrics for subjectivity evaluation remains an open challenge and an important direction for future work."
- Why unresolved: The current classifier only distinguishes subjective from objective sentences at 98% accuracy, without measuring quality, appropriateness, or degree of subjectivity.
- What evidence would resolve it: A new metric demonstrating high correlation with human subjectivity quality ratings across domains, validated against the current binary classifier approach.

### Open Question 3
- Question: Why does the dedicated subjectivity infusion stage underperform end-to-end fine-tuned models in subjectivity capture, and does error propagation across stages limit effectiveness?
- Basis in paper: [inferred] Human evaluation shows Ta-G-T's subjectivity score (8.59) lags behind T5-large pf-ct (8.93) and GPT-3.5 (9.06), despite having a specialized subjectivity module.
- Why unresolved: The paper does not analyze whether the modular design introduces error propagation or whether the WNC-based style transfer is suboptimal for table-derived content.
- What evidence would resolve it: Stage-wise error analysis and experiments with joint training or alternative style transfer datasets to identify the performance bottleneck.

### Open Question 4
- Question: Does the deterministic RDF extraction heuristic (first column as subject, headers as predicates) generalize to complex table structures with hierarchical headers, merged cells, or multi-dimensional layouts?
- Basis in paper: [inferred] The RDF extraction follows a fixed rule without evaluation on non-standard formats; the Ta2TS dataset contains only standard row-column tables from finance, weather, and sports domains.
- Why unresolved: Real-world tables often have nested structures that the current heuristic cannot handle, but this limitation is not addressed experimentally.
- What evidence would resolve it: Evaluation on datasets containing complex table structures, with comparison to adaptive or learned extraction methods.

## Limitations

- RDF Schema Expressiveness: The star-shaped RDF representation may inadequately capture complex relational structures like multi-entity events or nested hierarchies.
- Domain Generalization of Subjectivity Patterns: Assumption that WNC's encyclopedic subjectivity patterns transfer to finance, weather, and sports domains lacks independent validation.
- Small Model Tradeoff: T5-large models achieve competitive factual accuracy but show lower subjective richness compared to GPT-3.5 in human evaluations.

## Confidence

- **High Confidence**: Factual accuracy improvements through RDF grounding; modular pipeline benefits for error isolation; ablation study results showing component contributions
- **Medium Confidence**: Generalization claims to Ta2TS without fine-tuning; automatic subjectivity classifier calibration across domains
- **Low Confidence**: RDF schema adequacy for complex table structures; domain transferability of WNC-based subjectivity patterns

## Next Checks

1. **Test RDF Expressiveness**: Apply Ta-G-T to tables with nested structures and multi-entity relationships (e.g., sports match results with teams, scores, and player statistics). Document cases where RDF conversion fails to capture necessary relationships.
2. **Validate Subjectivity Classifier Transfer**: Collect 50 human-labeled Ta2TS outputs with domain-specific subjectivity annotations. Compare classifier scores against human ratings to establish calibration across Finance, Sports, and Weather domains.
3. **Compare End-to-End vs. Modular**: Train an end-to-end T5-large model on the same datasets and compare factual accuracy, subjectivity scores, and human evaluation ratings to assess information loss in the modular approach.