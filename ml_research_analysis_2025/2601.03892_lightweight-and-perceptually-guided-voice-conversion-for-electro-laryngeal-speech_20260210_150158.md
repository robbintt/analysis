---
ver: rpa2
title: Lightweight and perceptually-guided voice conversion for electro-laryngeal
  speech
arxiv_id: '2601.03892'
source_url: https://arxiv.org/abs/2601.03892
tags:
- speech
- wavlm
- loss
- inproc
- intelligibility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper adapts StreamVC, a lightweight voice conversion framework,
  for electro-laryngeal (EL) speech by removing pitch/energy modules and integrating
  perceptual and intelligibility-guided losses. It employs Whisper-based time alignment
  for EL-HE speech pairs and combines self-supervised pretraining with supervised
  fine-tuning.
---

# Lightweight and perceptually-guided voice conversion for electro-laryngeal speech

## Quick Facts
- **arXiv ID:** 2601.03892
- **Source URL:** https://arxiv.org/abs/2601.03892
- **Reference count:** 0
- **Primary result:** Lightweight StreamVC framework adapted for EL speech achieves naturalness MOS 3.3 and narrows gap to healthy speech with perceptual/intelligibility-guided losses.

## Executive Summary
This paper adapts the lightweight StreamVC voice conversion framework for electro-laryngeal (EL) speech by removing pitch/energy modules and integrating perceptual and intelligibility-guided losses. The approach uses Whisper-based time alignment for EL-HE speech pairs and combines self-supervised pretraining with supervised fine-tuning. The best model significantly improves intelligibility (reduces CER), raises naturalness MOS from 1.1 to 3.3, and narrows the performance gap to healthy speech across all evaluated metrics. Prosody and intelligibility remain primary bottlenecks.

## Method Summary
The method employs a two-stage training process: pretraining on 542 hours of healthy speech using mHuBERT-147 teacher model and 100 k-means units, followed by fine-tuning on the ELHE corpus (2.75 hours EL + 2.75 hours HE speech, expanded to 19,592 aligned pairs). Key adaptations include removing pitch/energy modules from StreamVC, adding perceptual losses (WavLM, HF, F0, BNF, WEO), and using FiLM conditioning. The model uses a ~30M parameter architecture with HiFi-GAN discriminator and is trained with noise augmentation (30%, SNR 3-30dB). Time alignment is achieved through fine-tuned Whisper-small extraction of WEO features and DTW matching.

## Key Results
- Best model (+WavLM+HF) significantly reduces EL speech Character Error Rate (CER)
- Naturalness MOS improves from 1.1 to 3.3, closing gap to healthy speech
- Prosody and intelligibility identified as primary bottlenecks despite performance gains

## Why This Works (Mechanism)
The lightweight convolutional architecture with perceptual-guided losses effectively learns to map EL spectral characteristics to healthy speech while maintaining content. The two-stage training allows learning robust representations from large healthy speech corpus before adapting to EL domain. Whisper-based WEO features provide phonetically meaningful alignment between EL and HE speech pairs, enabling effective many-to-many matching. The guided losses (WavLM, HF, F0, BNF, WEO) provide multi-faceted supervision that improves both perceptual quality and intelligibility beyond reconstruction loss alone.

## Foundational Learning
- **Voice Conversion Fundamentals**: Converting speaker identity while preserving content; needed for understanding EL-to-HE mapping challenge.
- **Self-supervised Speech Representations**: mHuBERT and WavLM provide rich feature spaces; quick check: verify mHuBERT layer outputs capture phonetic content.
- **Perceptual Loss Functions**: WavLM-based losses improve naturalness; quick check: compare MOS with/without WavLM loss.
- **Time Alignment Methods**: DTW with WEO features enables many-to-many matching; quick check: validate aligned pairs preserve phonetic correspondence.
- **FiLM Conditioning**: Enables adaptive feature modulation; quick check: confirm conditioning affects decoder output appropriately.
- **Adversarial Training**: HiFi-GAN discriminator improves naturalness; quick check: verify discriminator loss decreases during training.

## Architecture Onboarding

**Component Map**: mHuBERT-147 -> Encoder -> FiLM Decoder -> HiFi-GAN Discriminator

**Critical Path**: Encoder (content extraction) -> FiLM Decoder (style transfer) -> HiFi-GAN (quality enhancement)

**Design Tradeoffs**: Lightweight convolutional design (123MB) vs. potential expressiveness limitations; causal processing enables streaming but may limit long-range dependencies.

**Failure Signatures**: 
- CER increases when auxiliary losses exceed 2 (competing gradients)
- Intelligibility degrades below 5dB SNR (noise sensitivity)
- Prosody flattening in converted speech (expressiveness limitations)

**First Experiments**:
1. Train baseline StreamVC without guided losses, measure MOS and CER
2. Add single perceptual loss (WavLM), measure improvement in naturalness
3. Test WEO-based alignment quality by spot-checking aligned pairs

## Open Questions the Paper Calls Out
- Can context-aware Transformer decoders or expressive synthesis methodologies successfully resolve the remaining prosody generation and intelligibility bottlenecks in lightweight EL-to-HE voice conversion?
- What is the actual latency performance and computational overhead of the proposed system when deployed as a complete streaming pipeline on CPU-based embedded devices?
- How can the model's robustness be improved to prevent intelligibility degradation relative to raw EL input when operating in low Signal-to-Noise Ratio (SNR < 5dB) environments?

## Limitations
- EL-HE alignment artifacts from DTW may cause timing drift in converted speech
- Model sensitivity to rapid spectral changes in non-stationary noise degrades intelligibility below 5dB SNR
- Underspecified implementation details (loss weights, WavLM layers) limit exact reproduction

## Confidence
- **High**: Efficacy of perceptual-guided losses and role of WEO alignment
- **Medium**: Exact architectural choices and fine-tuning dynamics
- **Low**: Loss-weight tuning and VAD/noise augmentation specifics

## Next Checks
1. Implement ablation of auxiliary losses to confirm the 2-loss limit prevents competing gradients
2. Reproduce WEO-based alignment on ELHE corpus and validate phonetic match quality
3. Test range of WavLM layers and loss weights to confirm their impact on MOS and CER