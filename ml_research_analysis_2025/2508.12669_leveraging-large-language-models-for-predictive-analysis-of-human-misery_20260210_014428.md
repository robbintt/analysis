---
ver: rpa2
title: Leveraging Large Language Models for Predictive Analysis of Human Misery
arxiv_id: '2508.12669'
source_url: https://arxiv.org/abs/2508.12669
tags:
- misery
- language
- prompting
- llms
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study benchmarks Large Language Models (LLMs) for predicting\
  \ human-perceived misery scores from natural language descriptions. Three prompting\
  \ strategies\u2014zero-shot, few-shot, and retrieval-based using BERT embeddings\u2014\
  are evaluated on a dataset of 516 misery-scored scenarios."
---

# Leveraging Large Language Models for Predictive Analysis of Human Misery

## Quick Facts
- arXiv ID: 2508.12669
- Source URL: https://arxiv.org/abs/2508.12669
- Reference count: 22
- This study benchmarks LLMs for predicting human-perceived misery scores from text descriptions.

## Executive Summary
This study evaluates Large Language Models for predicting human-perceived misery scores from natural language descriptions using three prompting strategies: zero-shot, few-shot, and retrieval-based approaches. The research finds that few-shot and embedding-based methods significantly outperform zero-shot baselines, with retrieval-based prompting achieving the best results (MAE: 12.30, R²: 0.175). A gamified "Misery Game Show" simulation tests LLM reasoning across ordinal, binary, and scalar tasks, revealing model-specific strengths and demonstrating that feedback-driven adaptation improves calibration in scalar estimation tasks.

## Method Summary
The study uses 516 scenario-score pairs (mean=56.45, std=17.59, min=11, max=100) to benchmark LLMs including GPT-3.5-turbo, GPT-4, GPT-4-turbo, GPT-4o, and Azure ChatGPT. Three prompting strategies are evaluated: zero-shot instruction-only prompts, fixed and randomly sampled few-shot examples (k=1,2,5), and embedding-based retrieval using BERT sentence embeddings. The Misery Game Show simulation consists of four rounds testing ordinal classification, binary comparison, scalar prediction, and interval estimation, with both static and adaptive feedback modes.

## Key Results
- Few-shot approaches consistently outperform zero-shot baselines, with k=2 achieving the best balance (MAE: 12.49, R²: 0.105)
- Retrieval-based prompting using BERT embeddings achieves the highest performance (MAE: 12.30, R²: 0.175)
- GPT-4o achieves the highest accuracy (61.79%) and lowest prediction error (16.90) in the Misery Game Show
- Feedback-driven adaptation significantly improves calibration in scalar estimation tasks (average distance reduction from 23.41 to 17.82)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing labeled examples in the prompt improves misery score prediction accuracy.
- Mechanism: In-context learning allows the model to infer the mapping function from text to scalar misery values by pattern-matching against exemplars, without weight updates.
- Core assumption: The model has sufficient pre-trained emotional reasoning capacity to generalize from few examples to unseen scenarios.
- Evidence anchors:
  - [abstract] "Few-shot approaches consistently outperform zero-shot baselines, underscoring the value of contextual examples in affective prediction."
  - [section 3.5] "With just one example (k=1), MAE drops to 12.99, and further to 12.49 at k=2, with the highest Pearson correlation (0.606)."
  - [corpus] Related work (LlaMADRS, arXiv:2501.03624) confirms zero-shot prompting can guide models in interview-based depression assessment, supporting generalization capacity.
- Break condition: Adding too many fixed examples (k=5) caused R² to turn negative (–0.043), suggesting overfitting or reduced prompt diversity.

### Mechanism 2
- Claim: Semantically retrieved examples outperform randomly or statically selected few-shot examples.
- Mechanism: BERT sentence embeddings identify training examples with similar affective content, providing contextually relevant anchors that improve the model's implicit regression function.
- Core assumption: Semantic similarity in embedding space correlates with similarity in misery-score labeling patterns.
- Evidence anchors:
  - [abstract] "Retrieval-based prompting achieving the best performance (MAE: 12.30, R²: 0.175)."
  - [section 3.5] "Embedding-based prompting using BERT similarity performs comparably (MAE: 12.30, RMSE: 15.97 at k=5), with R-squared: 0.175—higher than fixed-k."
  - [corpus] Weak direct evidence; no corpus papers explicitly test retrieval-augmented affective regression.
- Break condition: Retrieval effectiveness depends on embedding quality; domain-specific affective embeddings may be required for niche contexts.

### Mechanism 3
- Claim: Corrective feedback after predictions improves downstream calibration, particularly in scalar estimation tasks.
- Mechanism: The model uses feedback signals to adjust its internal reference frame for misery intensity, reducing systematic over- or under-estimation in subsequent predictions.
- Core assumption: LLMs can maintain and apply corrective information within a session without explicit weight updates.
- Evidence anchors:
  - [abstract] "Feedback-driven adaptation improved calibration, particularly in scalar estimation tasks."
  - [section 4.3] "The average distance in Round 3 shows a significant reduction (from 23.41 to 17.82) with feedback, indicating better calibration in numerical estimation tasks."
  - [corpus] Gontier et al. (EMNLP 2023, cited in paper) found human-like feedback helps LLMs learn from mistakes, supporting this adaptation mechanism.
- Break condition: Feedback gains may not persist across sessions or different task types; the paper does not test retention.

## Foundational Learning

- Concept: Mean Absolute Error (MAE) vs. R-squared (R²)
  - Why needed here: MAE measures average prediction error in misery-score units; R² measures explained variance. Negative R² indicates the model performs worse than simply predicting the mean.
  - Quick check question: If a model achieves MAE=12 but R²=–0.04, is it useful?

- Concept: Zero-shot vs. Few-shot vs. Retrieval-Augmented Prompting
  - Why needed here: The paper benchmarks all three; understanding the trade-offs is essential for selecting the right strategy for affective tasks.
  - Quick check question: Why might retrieval-based prompting outperform fixed few-shot at higher k values?

- Concept: Embedding-Based Semantic Similarity
  - Why needed here: The retrieval mechanism relies on BERT sentence embeddings to find contextually relevant examples; understanding embedding space properties informs retrieval design.
  - Quick check question: What assumptions does retrieval-based prompting make about the relationship between semantic and affective similarity?

## Architecture Onboarding

- Component map:
  Input Layer -> Retrieval Module (optional) -> Prompt Constructor -> LLM API -> Output Parser -> Evaluation Module -> Feedback Loop (Game Show mode)

- Critical path:
  1. Scenario text enters system
  2. If retrieval enabled: encode query, retrieve k similar (scenario, score) pairs
  3. Construct prompt with examples and target
  4. Call LLM API
  5. Parse scalar output
  6. Evaluate against ground truth
  7. If feedback mode: append correction to context for next query

- Design tradeoffs:
  - Fixed few-shot (k=2) offers simplicity and good performance; retrieval adds complexity but improves R² at higher k.
  - Chain-of-Thought adds latency without measurable gain for this subjective task.
  - Feedback improves calibration but requires sequential execution; not parallelizable.

- Failure signatures:
  - Negative R² at k=5 (fixed): indicates overfitting to prompt examples or loss of diversity.
  - High variance in GPT-3.5-turbo predictions (Avg. Distance: 30.00 in Round 3): smaller models struggle with scalar calibration.
  - Non-execution of o1-preview, o1-mini, Gemini-1.5-pro under current pipeline: API or formatting incompatibilities.

- First 3 experiments:
  1. Replicate zero-shot vs. k=2 few-shot comparison on a held-out 20% split; verify MAE improvement magnitude.
  2. Implement BERT-based retrieval and compare against random-k selection; confirm R² gains hold across multiple seeds.
  3. Run a simplified Game Show loop (Rounds 1–3 only) with and without feedback; measure calibration drift over 10 iterations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do open-source and non-GPT commercial LLMs compare to the GPT-family in predicting human-perceived misery scores?
- Basis in paper: [explicit] The authors state that experiments were "conducted exclusively with GPT-based models" and recommend "Broader model comparisons—including other commercial or open-source LLMs" to fully understand affective prediction capabilities.
- Why unresolved: The current study restricts its evaluation to GPT-3.5, GPT-4, and Azure ChatGPT, leaving the performance of other prominent architectures untested.
- What evidence would resolve it: Benchmarking models such as Llama, Mistral, or Claude on the same misery dataset using the defined MAE and R² metrics.

### Open Question 2
- Question: Are the performance gains observed in few-shot and retrieval-based prompting statistically significant compared to zero-shot baselines?
- Basis in paper: [explicit] The authors note they "did not apply statistical significance tests to the reported metrics" and suggest that future work include "paired t-tests or bootstrapping" to verify the trends.
- Why unresolved: While mean error metrics (e.g., MAE) suggest improvement, the lack of confidence intervals or p-values means the consistency of these improvements across different samples is unconfirmed.
- What evidence would resolve it: Re-evaluation of the prompting strategies using rigorous statistical hypothesis testing on the results.

### Open Question 3
- Question: Does expanding the dataset beyond 516 examples improve the generalizability of misery predictions or resolve issues with fine-grained scalar estimation?
- Basis in paper: [explicit] The authors identify that the "dataset... contains only 516 examples, which may limit the generalizability" and explicitly suggest "Expanding the dataset" as a key area for future work.
- Why unresolved: The relatively small sample size may constrain the model's ability to learn the full variance of human misery, particularly for edge cases or specific semantic categories.
- What evidence would resolve it: Performance benchmarks on a larger, augmented dataset to determine if prediction error decreases and generalization improves.

## Limitations
- The study relies on a single, small-scale dataset (516 examples) of human-perceived misery scores, which may not generalize to broader affective reasoning tasks.
- The lack of cross-domain validation and absence of human baseline comparisons makes it difficult to assess whether LLMs truly capture subjective emotional reasoning.
- Prompt templates and API parameters (temperature, top-p, etc.) are not fully specified, creating reproducibility challenges.

## Confidence
- **High Confidence:** The superiority of few-shot prompting over zero-shot approaches is well-supported by consistent performance improvements across metrics.
- **Medium Confidence:** The retrieval-based prompting advantage (MAE: 12.30 vs. fixed few-shot at k=5) is demonstrated, but specific implementation details are missing.
- **Low Confidence:** Claims about LLMs' ability to "model subjective emotional reasoning" lack human baseline comparisons.

## Next Checks
1. **Cross-Dataset Generalization:** Test the best-performing few-shot and retrieval strategies on an independent affective reasoning dataset to assess domain transfer capability.
2. **Human Baseline Comparison:** Conduct a controlled experiment with human annotators predicting misery scores on the same 516 scenarios to establish whether LLM performance approaches human-level subjective reasoning.
3. **Prompt Template Ablation:** Systematically vary prompt structure while keeping model and examples constant to isolate the contribution of prompt engineering to performance gains.