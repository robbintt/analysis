---
ver: rpa2
title: 'BERnaT: Basque Encoders for Representing Natural Textual Diversity'
arxiv_id: '2512.03903'
source_url: https://arxiv.org/abs/2512.03903
tags:
- language
- standard
- diverse
- basque
- corpora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper argues that language models should capture the full
  spectrum of language variation, including dialectal, historical, and informal varieties,
  rather than relying solely on standardized text. Focusing on Basque, a morphologically
  rich low-resource language, the authors construct new corpora combining standard,
  social media, and historical sources, and pre-train the BERnaT family of encoder-only
  models in three configurations: standard, diverse, and combined.'
---

# BERnaT: Basque Encoders for Representing Natural Textual Diversity

## Quick Facts
- **arXiv ID:** 2512.03903
- **Source URL:** https://arxiv.org/abs/2512.03903
- **Reference count:** 0
- **Primary result:** Encoder models trained on both standard and diverse data outperform those trained on standard data alone, improving NLU task performance across all linguistic varieties.

## Executive Summary
This paper challenges the conventional approach of training language models exclusively on standardized text by demonstrating that models capturing the full spectrum of linguistic variation—including dialectal, historical, and informal varieties—perform better across all task types. Focusing on Basque, a morphologically rich low-resource language, the authors construct new corpora combining standard, social media, and historical sources, and pre-train the BERnaT family of encoder-only models in three configurations: standard, diverse, and combined. The evaluation framework separates NLU tasks into standard and diverse subsets to assess linguistic generalization. Results show consistent improvements across all task types when diverse data is included, with the benefits being most pronounced when fine-tuning data is scarce and larger models benefiting more from data diversity.

## Method Summary
The authors construct three corpora for Basque: a standard corpus (Latxa-v1.1, ~1.22B words), a diverse corpus combining Basque Social Media (BSM, ~188M words) and historical texts (EKC, ~21M words), and a combined corpus. They pre-train RoBERTa architectures (Medium: 51M, Base: 124M, Large: 355M parameters) from scratch using separate BPE tokenizers for each corpus. The models are trained with MLM objective, sequence length 512, and learning rates of 8e-4 (medium), 4e-4 (base), and 1e-4 (large). Fine-tuning is performed on BasqueGLUE benchmark and specific NLU tasks, with evaluation split into standard (e.g., NERC, POSud) and diverse (e.g., BEC, Vaxx, POShis) subsets.

## Key Results
- Models trained on both standard and diverse data consistently outperform those trained on standard corpora alone
- Benefits of diverse pre-training are most pronounced when fine-tuning data is scarce
- Larger models (355M parameters) benefit more from data diversity than smaller ones
- No compromise in standard benchmark accuracy when including diverse data

## Why This Works (Mechanism)
The paper demonstrates that exposure to linguistic diversity during pre-training enables models to better handle real-world language variation encountered during fine-tuning and inference. By training on non-standard varieties alongside standard text, models develop more robust representations that generalize across different linguistic registers. The mechanism appears to work through increased exposure to morphological variation and contextual diversity, which is particularly beneficial for morphologically rich languages like Basque where data scarcity makes standard-only training insufficient.

## Foundational Learning

**Morphological Richness:** Basque has extensive inflectional morphology with many word forms per lemma, making it low-resource despite having millions of speakers. This requires models to learn from fewer unique examples but many inflected forms.

**Tokenization Strategy:** Separate BPE tokenizers are trained for standard, diverse, and combined corpora to handle the different vocabularies and subword patterns in each data type effectively.

**Learning Rate Scaling:** Different learning rates are used for different model sizes (8e-4 for medium, 4e-4 for base, 1e-4 for large) to maintain stable training across scales, especially important when training on diverse corpora.

**Fine-tuning Evaluation:** Tasks are split into "standard" and "diverse" subsets to specifically measure how well models generalize to non-standard linguistic varieties rather than just measuring overall performance.

**Data Mixing Strategy:** The combined corpus includes standard data (Latxa) alongside diverse sources rather than training purely on the smaller diverse corpus, preventing overfitting to non-standard varieties.

## Architecture Onboarding

**Component Map:** Standard/Diverse/Combined Corpus -> BPE Tokenizer -> RoBERTa Model (51M/124M/355M) -> MLM Pre-training -> BasqueGLUE/Task-specific Fine-tuning -> Standard/Diverse Evaluation

**Critical Path:** The critical path is the pre-training phase where models learn representations. The choice of corpus (standard vs. diverse vs. combined) directly determines the model's ability to handle linguistic variation during fine-tuning.

**Design Tradeoffs:** Using separate tokenizers for each corpus increases preprocessing complexity but ensures optimal subword segmentation for each data type. Training on diverse data risks overfitting to non-standard varieties but the combined approach mitigates this while maintaining benefits.

**Failure Signatures:** Exploding gradients when training on diverse corpora (resolved by reducing learning rate to 1e-4 for large models), and degraded standard benchmark performance when over-fitting on diverse data (prevented by mixing standard data in the combined corpus).

**First Experiments:**
1. Train all three model sizes on standard corpus only and establish baseline performance on both standard and diverse task subsets
2. Train all three model sizes on diverse corpus only and measure performance degradation on standard tasks
3. Train all three model sizes on combined corpus and verify improvement across all task types without compromising standard performance

## Open Questions the Paper Calls Out

**Open Question 1:** Do the benefits of diverse pre-training transfer to decoder-only architectures in few-shot or zero-shot settings? The authors plan to explore larger model scales and generative architectures in these settings.

**Open Question 2:** How does pre-training on non-standard linguistic varieties impact the generation capabilities of language models? The authors intend to integrate generation-based evaluations beyond fine-tuning.

**Open Question 3:** Is the performance boost from diverse data dependent on the language being morphologically rich? The study's focus on Basque leaves generalizability to other language types uncertain.

## Limitations

- The study focuses exclusively on encoder-only (RoBERTa-based) architectures, leaving the effects on decoder-only models unexplored
- The evaluation framework is restricted to discriminative NLU tasks, not testing generation capabilities
- The benefits of diverse data might be correlated with Basque's morphological complexity, limiting generalizability

## Confidence

**High Confidence:** The core finding that diverse data improves model performance across all task types is well-supported by results and aligns with established findings in multilingual modeling.

**Medium Confidence:** The claim that larger models benefit more from data diversity is supported but could be strengthened with more extensive ablation studies across model sizes.

**Medium Confidence:** The assertion that benefits are most pronounced with scarce fine-tuning data is supported by experiments but would benefit from additional data points across different data scarcity levels.

## Next Checks

1. Verify that the "pre-train... with 1 million tokens" statement refers to steps or epochs by checking Artetxe et al., 2022 implementation details and standard RoBERTa configurations
2. Test training stability by reproducing the learning rate adjustments for diverse corpora (specifically the reduction to 1e-4 for large models) and monitoring gradient norms during training
3. Validate the evaluation framework by testing whether the "Combined" corpus actually improves both standard and diverse task performance, ensuring it's not overfitting to the smaller diverse dataset at the expense of standard benchmarks