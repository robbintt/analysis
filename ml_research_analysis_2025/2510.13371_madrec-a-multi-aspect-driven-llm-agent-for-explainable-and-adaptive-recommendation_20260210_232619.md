---
ver: rpa2
title: 'MADREC: A Multi-Aspect Driven LLM Agent for Explainable and Adaptive Recommendation'
arxiv_id: '2510.13371'
source_url: https://arxiv.org/abs/2510.13371
tags:
- user
- recommendation
- item
- aspect
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MADRec proposes a multi-aspect driven LLM agent for explainable
  and adaptive recommendation. It constructs user and item profiles by unsupervised
  extraction of multi-aspect information from reviews, applies a re-ranking strategy
  to select high-density inputs, and uses a self-feedback mechanism to dynamically
  adjust inference criteria when the ground-truth item is missing.
---

# MADREC: A Multi-Aspect Driven LLM Agent for Explainable and Adaptive Recommendation

## Quick Facts
- **arXiv ID:** 2510.13371
- **Source URL:** https://arxiv.org/abs/2510.13371
- **Authors:** Jiin Park; Misuk Kim
- **Reference count:** 14
- **Primary result:** Multi-aspect LLM agent outperforms baselines in recommendation accuracy and explanation quality across three domains

## Executive Summary
MADRec introduces a novel multi-aspect driven LLM agent that combines unsupervised extraction of user and item profiles from reviews with a re-ranking strategy and self-feedback mechanism. The system addresses key limitations in current LLM-based recommendation approaches by providing more adaptive, explainable, and accurate recommendations. Through extensive experiments across Beauty, Sports, and Toys domains, MADRec demonstrates superior performance in both direct and sequential recommendation tasks while generating high-quality, persuasive explanations.

## Method Summary
MADRec constructs user and item profiles by extracting multi-aspect information from reviews using unsupervised techniques. The system employs a re-ranking strategy to select high-density inputs that maximize information content. A self-feedback mechanism dynamically adjusts inference criteria when ground-truth items are missing, enabling adaptive recommendations. The approach integrates seamlessly with existing LLM frameworks while maintaining explainability through structured aspect extraction and ranking.

## Key Results
- Outperforms traditional and LLM-based baselines in direct recommendation (HR@10 up to 0.364)
- Superior performance in sequential recommendation tasks (HR@10 up to 0.368)
- Generates high-quality explanations with BERTScore up to 0.850 and human persuasiveness ratings of 4.08/5.0

## Why This Works (Mechanism)
The effectiveness stems from MADRec's multi-aspect profile construction, which captures rich semantic information from user reviews beyond simple preferences. The re-ranking strategy ensures the LLM receives high-quality, information-dense inputs that improve recommendation accuracy. The self-feedback mechanism provides adaptability by dynamically adjusting inference parameters based on feedback loops, particularly valuable when ground-truth items are unavailable during training or inference.

## Foundational Learning

1. **Unsupervised Multi-Aspect Extraction**
   - *Why needed:* Captures rich semantic relationships in user reviews without manual annotation
   - *Quick check:* Verify aspect coherence scores exceed baseline keyword extraction methods

2. **Re-ranking Strategy Implementation**
   - *Why needed:* Filters noise and selects most informative inputs for LLM inference
   - *Quick check:* Compare information density metrics between ranked and unranked input sets

3. **Self-Feedback Mechanism Design**
   - *Why needed:* Enables adaptive behavior when explicit feedback is unavailable
   - *Quick check:* Measure performance degradation with and without self-feedback in cold-start scenarios

## Architecture Onboarding

**Component Map:** Review Processor -> Aspect Extractor -> Re-ranker -> LLM Agent -> Feedback Loop -> Adaptive Controller

**Critical Path:** User reviews → Aspect extraction → Re-ranking → LLM inference → Explanation generation → Performance monitoring → Self-feedback adjustment

**Design Tradeoffs:**
- *Multi-aspect vs. single-aspect:* Richer profiles vs. increased computational complexity
- *Re-ranking intensity:* Better input quality vs. processing overhead
- *Feedback frequency:* More adaptive vs. potential instability in recommendations

**Failure Signatures:**
- Poor aspect extraction manifests as generic, non-specific recommendations
- Ineffective re-ranking produces noisy inputs and degraded LLM performance
- Overly aggressive self-feedback causes recommendation oscillation

**First 3 Experiments:**
1. Baseline comparison: MADRec vs. standard LLM recommender without multi-aspect extraction
2. Ablation study: Measure impact of re-ranking strategy on recommendation accuracy
3. Cold-start evaluation: Test self-feedback mechanism performance with minimal user history

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental results limited to three Amazon product domains (Beauty, Sports, Toys)
- User study sample size relatively small (20 participants) focusing on explanation persuasiveness
- Computational efficiency and real-time performance implications not addressed

## Confidence

**High confidence:**
- Claims about MADRec's ability to outperform baselines in HR@10 metrics (up to 0.364 for direct recommendation, 0.368 for sequential recommendation)

**Medium confidence:**
- Claims regarding explanation quality (BERTScore up to 0.850) and human persuasiveness ratings (average 4.08/5.0)
- Effectiveness of self-feedback mechanism and re-ranking strategy demonstrated through ablation studies

## Next Checks
1. Conduct larger-scale user study (minimum 100 participants) measuring actual recommendation acceptance rates and user satisfaction over time

2. Test MADRec's performance on additional domains beyond e-commerce (media, restaurants, services) to validate generalizability

3. Evaluate computational efficiency by measuring inference time, memory usage, and cost per recommendation compared to simpler approaches