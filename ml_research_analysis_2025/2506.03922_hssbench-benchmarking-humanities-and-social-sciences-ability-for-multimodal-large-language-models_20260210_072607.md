---
ver: rpa2
title: 'HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal
  Large Language Models'
arxiv_id: '2506.03922'
source_url: https://arxiv.org/abs/2506.03922
tags:
- data
- answer
- qwen2
- question
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HSSBench is a large-scale multimodal benchmark for evaluating Large
  Language Models on Humanities and Social Sciences (HSS) tasks across six languages.
  It contains 13,152 carefully curated samples spanning six categories and 45 types,
  with data generated through a novel multi-agent pipeline involving domain experts
  and automated agents.
---

# HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2506.03922
- Source URL: https://arxiv.org/abs/2506.03922
- Reference count: 40
- Large-scale multimodal benchmark for evaluating MLLMs on HSS tasks across 6 languages

## Executive Summary
HSSBench is a large-scale multimodal benchmark for evaluating Large Language Models on Humanities and Social Sciences (HSS) tasks across six languages. It contains 13,152 carefully curated samples spanning six categories and 45 types, with data generated through a novel multi-agent pipeline involving domain experts and automated agents. The benchmark addresses the gap in current MLLM evaluations, which focus primarily on STEM disciplines while neglecting the interdisciplinary and abstract reasoning demands of HSS domains. When evaluated across 20+ mainstream MLLMs, models showed significantly lower performance on HSSBench compared to STEM-focused benchmarks, with accuracy often below 60%, demonstrating the unique challenges HSS tasks pose for current MLLM architectures.

## Method Summary
HSSBench employs a three-stage visual question-answer generation pipeline (VGP) involving expert annotation, multi-agent question generation, and rigorous validation. The benchmark covers 13,152 samples across 6 categories in 6 UN languages, using both multiple-choice and open-ended question formats. Evaluation uses exact match for multiple-choice questions and LLM-as-a-judge for open-ended responses, with models tested using both direct and Chain-of-Thought prompting strategies.

## Key Results
- MLLMs achieve significantly lower accuracy on HSSBench compared to STEM benchmarks, often below 60%
- Economic tasks show lowest average scores across all models, indicating particular weakness in complex theory-application reasoning
- Chain-of-Thought prompting often degrades rather than improves HSS performance due to hallucination and irrelevant reasoning paths
- Expert textual annotations of images substantially outperform raw visual inputs, revealing vision-language binding failures

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Knowledge Transfer Failure
- Claim: MLLMs fail to establish mappings between HSS-specific visual content and their corresponding abstract domain knowledge
- Mechanism: Models store abstract concepts (e.g., "Business Penmanship") as textual knowledge but cannot reliably trigger these representations when processing visual inputs containing the same information, creating a dissociation between vision and language pathways for culturally-embedded content
- Core assumption: Visual feature extractors trained on general image datasets do not develop representations aligned with culturally-specific symbolic patterns
- Evidence anchors:
  - [abstract] "present unique challenges for MLLMs, particularly in linking abstract concepts with corresponding visual representations"
  - [Page 2] Figure 2 demonstrates models correctly answer "Business Penmanship" questions directly but fail when the same concept must be inferred from an image
  - [corpus] MMVU benchmark shows expert-level understanding requires cross-disciplinary connections, supporting the transfer difficulty claim
- Break condition: If models receive explicit textual descriptions of images from domain experts, performance improves (Table 6 shows expert annotations boost accuracy)

### Mechanism 2: Horizontal vs. Vertical Reasoning Mismatch
- Claim: Current MLLM architectures and training paradigms optimize for vertical (sequential logical deduction) rather than horizontal (interdisciplinary, contextual) reasoning patterns required by HSS
- Mechanism: STEM-tuned attention patterns privilege step-by-step causal chains, while HSS demands simultaneous activation of multiple knowledge domains and cultural contexts—a pattern not reinforced during training
- Core assumption: Training data composition and loss functions implicitly favor reasoning styles typical of STEM tasks
- Evidence anchors:
  - [Page 2] "STEM fields that employ 'vertical reasoning': a focused sequential process...whereas HSS disciplines feature symbol systems deeply rooted in regional cultures"
  - [Page 6-7] Economic tasks show lowest average scores across all models, suggesting complex theory-application reasoning is particularly weak
  - [corpus] Limited corpus evidence directly compares vertical vs. horizontal reasoning; related work on deception detection (arXiv:2511.16221) notes MLLMs struggle with contextual social reasoning
- Break condition: When domain knowledge is explicitly scaffolded through expert annotations, the gap narrows

### Mechanism 3: Chain-of-Thought Divergence in HSS Contexts
- Claim: Standard CoT prompting degrades rather than improves HSS performance by triggering hallucinations and irrelevant reasoning paths
- Mechanism: Extended generation creates more opportunity for culturally-ungrounded inferences; models lack the constraint of correct answer options in open-ended settings, causing reasoning to diverge from ground truth
- Core assumption: CoT effectiveness depends on the reasoning domain's alignment with training distribution
- Evidence anchors:
  - [Page 7] "COT prompts do not always yield positive effects, and certain models perform better with direct answer prompts"
  - [Page 8] "COT prompts exacerbate hallucination issues in certain models, where reasoning flaws...lead to generation of incorrect background knowledge"
  - [corpus] No direct corpus evidence on CoT effectiveness variance across domains; this appears to be a novel finding
- Break condition: Performance drop is most pronounced in open-ended questions without multiple-choice constraints

## Foundational Learning

- **Vertical vs. Horizontal Reasoning Paradigms**
  - Why needed here: The paper's central theoretical contribution distinguishes these reasoning types; understanding this distinction is essential for interpreting results and designing interventions
  - Quick check question: Can you explain why solving a calculus problem (vertical) requires different cognitive operations than interpreting a historical artifact's cultural significance (horizontal)?

- **Cross-Modal Knowledge Binding**
  - Why needed here: HSSBench's core challenge is binding visual features to abstract domain concepts; understanding current models' limitations here is prerequisite for improvement
  - Quick check question: If a model can correctly answer "What is Baroque architecture?" but cannot identify Baroque elements in an unlabeled photo, where does the binding failure occur?

- **Benchmark Design for Open-Ended Domains**
  - Why needed here: HSS lacks ground truth consensus; the paper's multi-expert validation pipeline addresses this epistemic challenge
  - Quick check question: Why does adding "None of the above" as an option significantly impact model performance, and what does this reveal about model uncertainty?

## Architecture Onboarding

- **Component map:**
  [Raw Data Sources] → [Expert Annotation Platform] → [VGP Pipeline] → [Filtered Content] → [Quality Control] → [Question Generator] → [Image Matcher + Validator] → [HSSBench 13,152 samples]

- **Critical path:** VGP (Visual Question-Answer Generation Pipeline) — the multi-agent pipeline is the innovation enabling scale. Stage I (data prep) → Stage II (construction: expert + multi-agent) → Stage III (validation: agent + expert). Bottleneck is expert validation; design choices in Stage II determine downstream quality.

- **Design tradeoffs:**
  - Multiple-choice vs. open-ended: MC provides evaluation clarity but constrains reasoning visibility; open-ended reveals model capabilities but evaluation is noisier (Table 1 shows accuracy drops from ~40% MC to ~15% open-ended)
  - Expert-only vs. agent-assisted: Expert-only ensures quality but doesn't scale; agents enable 13k+ samples but require validation overhead
  - CoT vs. direct prompts: Domain-dependent; test both (Table 10 prompts) as default

- **Failure signatures:**
  - Models correctly answer textual queries about concepts but fail visual recognition of same concepts (Figure 2 pattern)
  - CoT outputs contain plausible-but-incorrect cultural/historical claims (hallucination pattern)
  - "None of the above" addition causes coherent reasoning degradation, not just answer switching (Table 5)

- **First 3 experiments:**
  1. **Baseline establishment:** Run the 6 representative models (Table 9: InternVL3-8B, Qwen2.5-VL-7B, Qwen2.5-VL-32B, Qwen2.5-VL-72B, QVQ-72B-Preview, GPT-4.1-mini) on the 900-sample stratified subset across all 6 UN languages to confirm cross-lingual performance patterns
  2. **Visual information extraction test:** Replicate Table 6 experiment—convert images to GPT-4.1 text descriptions vs. expert annotations, measure performance delta to quantify vision-language binding failures
  3. **Confounding option sensitivity:** Apply "None of the above" intervention (Table 5) to a new domain subset to test whether models use reasoning vs. surface cue heuristics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multimodal reasoning strategies be adapted to prevent the performance degradation observed with Chain-of-Thought (CoT) prompting in HSS tasks?
- Basis in paper: [explicit] Section 3.3 states that CoT prompts "do not always yield positive effects" and can cause models to focus on irrelevant information or suffer from hallucinations, leading to worse performance than direct prompting in HSS contexts.
- Why unresolved: The paper identifies that standard vertical reasoning steps (CoT) often mislead models in horizontal reasoning tasks, but it does not propose an alternative reasoning architecture or prompting strategy that reliably improves accuracy for abstract HSS concepts.
- What evidence would resolve it: A new prompting framework or model architecture that consistently yields higher accuracy on HSSBench compared to direct prompting, specifically by reducing hallucination during the intermediate reasoning steps.

### Open Question 2
- Question: How can MLLMs improve the extraction of deep symbolic information from images to bridge the performance gap between raw visual input and expert textual descriptions?
- Basis in paper: [inferred] Section 3.3 details an experiment where replacing images with expert textual annotations improved model accuracy, suggesting that current visual encoders fail to capture critical symbolic or cultural cues (shallow vs. deep visual attributes).
- Why unresolved: While the paper demonstrates that expert text guides models better than the original images, it leaves open the technical challenge of training models to recognize these visual features autonomously without external textual crutches.
- What evidence would resolve it: Fine-tuning results on HSSBench showing that a model using only raw images can match the performance of the model provided with expert textual descriptions, particularly in "Culture" and "Art" categories.

### Open Question 3
- Question: What mechanisms can mitigate the susceptibility of MLLMs to "confusing options" or distractors in HSS multiple-choice questions?
- Basis in paper: [explicit] Section 3.3 notes that introducing the distractor "None of the above" caused a noticeable performance decline and made reasoning less coherent, indicating models struggle to assess the relative credibility of answer choices.
- Why unresolved: The paper highlights this fragility but does not explore whether this is due to a lack of confidence calibration or a failure in contrasting distinct HSS concepts, offering no solution to stabilize judgment under ambiguity.
- What evidence would resolve it: A study demonstrating that specific training techniques (e.g., contrastive learning or uncertainty-aware training) significantly reduce the performance drop when invalid distractors are introduced into the HSSBench evaluation set.

### Open Question 4
- Question: How can HSS benchmarks account for the time-sensitivity and evolving interpretations of social and economic knowledge?
- Basis in paper: [explicit] Section 4 (Limitations) acknowledges that data involving recent social events or economic knowledge is time-sensitive and current answers may be invalidated by future developments.
- Why unresolved: The authors impose static temporal constraints to ensure current correctness, but this avoids the research challenge of evaluating a model's ability to handle the dynamic nature of truth in social sciences.
- What evidence would resolve it: The development of a dynamic evaluation subset within HSSBench that requires models to identify or flag temporally bound knowledge versus enduring historical facts.

## Limitations
- Limited empirical validation of proposed failure mechanisms; theoretical constructs remain largely untested
- Heavy reliance on closed-source models (GPT-4o, GPT-4.1) with opaque training data and inference settings
- Single LLM-as-a-judge evaluation without inter-annotator agreement validation for culturally-nuanced HSS content

## Confidence
**High confidence** in benchmark construction methodology and observed performance gaps between HSS and STEM tasks.

**Medium confidence** in the three proposed failure mechanisms and CoT prompting degradation claims.

**Low confidence** in causal attribution of performance differences to specific architectural limitations and generalizability beyond tested models.

## Next Checks
1. **Mechanism isolation experiment:** Replicate the visual information extraction test (Table 6) across all 6 categories with 100 samples each, comparing expert vs. GPT-4o image descriptions to quantify the exact contribution of vision-language binding failures to overall performance drop.

2. **CoT vs. Direct prompting validation:** Conduct a controlled experiment where models solve identical HSS problems using both prompting strategies, with human expert validation of reasoning quality. Measure hallucination rates and correct reasoning path adherence across 50 open-ended samples per domain.

3. **Cross-cultural generalization test:** Evaluate HSSBench performance on models specifically trained on diverse cultural datasets (e.g., models trained on non-Western corpora) to determine whether the observed failures stem from training data bias or fundamental architectural limitations in cross-modal reasoning.