---
ver: rpa2
title: 'DES-LOC: Desynced Low Communication Adaptive Optimizers for Training Foundation
  Models'
arxiv_id: '2505.22549'
source_url: https://arxiv.org/abs/2505.22549
tags:
- local
- adam
- des-loc
- communication
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces DES-LOC, a family of adaptive optimizers\
  \ that reduce communication costs in distributed training by assigning independent\
  \ synchronization periods to model parameters and optimizer states (momenta). The\
  \ key insight is that higher-\u03B2 optimizer states evolve more slowly and can\
  \ be synchronized less frequently, reducing communication without sacrificing convergence."
---

# DES-LOC: Desynced Low Communication Adaptive Optimizers for Training Foundation Models

## Quick Facts
- arXiv ID: 2505.22549
- Source URL: https://arxiv.org/abs/2505.22549
- Authors: Alex Iacob; Lorenzo Sani; Mher Safaryan; Paris Giampouras; Samuel Horváth; Andrej Jovanovic; Meghdad Kurmanji; Preslav Aleksandrov; William F. Shen; Xinchi Qiu; Nicholas D. Lane
- Reference count: 40
- Key outcome: DES-LOC reduces communication costs in distributed training by 2× over Local Adam and 170× over DDP while maintaining convergence, enabling efficient training of billion-parameter models.

## Executive Summary
This paper introduces DES-LOC, a family of adaptive optimizers that reduce communication costs in distributed training by assigning independent synchronization periods to model parameters and optimizer states (momenta). The key insight is that higher-β optimizer states evolve more slowly and can be synchronized less frequently, reducing communication without sacrificing convergence. DES-LOC achieves significant communication reduction while maintaining convergence guarantees for both SGD with momentum and Adam variants, scaling effectively to billion-parameter models and demonstrating robustness to worker failures through eventual state synchronization.

## Method Summary
DES-LOC extends distributed optimizers by introducing independent synchronization periods for model parameters and optimizer states. Each state type (parameters, first momentum, second momentum) is synchronized at its own interval K_x, K_u, K_v, determined by the half-life of the corresponding momentum decay rate β. The method maintains convergence guarantees by bounding the drift between local and synchronized states through per-coordinate gradient clipping, while leveraging the slower evolution of higher-β states to reduce communication frequency. DES-LOC provides both Adam and ADOPT variants, with the latter using a more stable high-β₂ formulation (β₂=0.9999) that better exploits the desynchronization benefits.

## Key Results
- Achieves 2× communication reduction over Local Adam and 170× over DDP while maintaining convergence
- Scales effectively to billion-parameter models, matching baseline performance in ICL benchmarks
- Maintains stability and performance when worker count doubles mid-training
- Enables training on bandwidth-limited infrastructure without sacrificing model quality

## Why This Works (Mechanism)

### Mechanism 1: Half-life Guided Synchronization Intervals
Higher momentum decay rates (β) correspond to longer half-lives, meaning optimizer states change more slowly and can be synchronized less frequently. The half-life τ₀.₅(β) = ln(0.5)/ln(β) determines steps until 50% weight decay. With gradient clipping bounds, the maximum drift between syncs is bounded, ensuring bounded divergence from synchronized values.

### Mechanism 2: Asymmetric Importance in Convergence Bounds
Parameter synchronization frequency impacts convergence more than momentum synchronization frequency. In the convergence rate, parameter sync probability p_x appears quadratically (1/p_x²) while momentum sync p_u appears linearly. This allows setting p_u very low (or even zero) without breaking asymptotic convergence rates.

### Mechanism 3: Eventual Synchronization for Fault Tolerance
Periodic synchronization of optimizer states enables robustness to worker failures and dynamic scaling. Unlike purely local state methods, DES-LOC synchronizes all states eventually, preventing accumulation of noisy local updates while still reducing communication. This allows seamless integration of new workers and recovery from failures.

## Foundational Learning

- **Concept**: Exponential Moving Average (EMA) momentum in optimizers
  - Why needed here: DES-LOC's core insight depends on understanding how β decay rates control the effective "memory" of gradient history
  - Quick check question: If β₁=0.9, approximately what fraction of the update at step t depends on gradients from steps before t-10?

- **Concept**: Ring-AllReduce communication patterns in distributed training
  - Why needed here: The paper's communication cost analysis assumes bandwidth-optimal Ring-AllReduce
  - Quick check question: In Ring-AllReduce with M workers and payload size P, what is the total communication time given bandwidth B and latency l?

- **Concept**: Non-convex optimization convergence rates (O(1/√T))
  - Why needed here: The theoretical guarantees use standard assumptions from non-convex optimization
  - Quick check question: What does it mean for an algorithm to have O(1/√T) convergence for non-convex objectives?

## Architecture Onboarding

- **Component map**:
  ```
  DES-LOC Core (Algorithm 1)
  ├── State Management: x (params), {s_j} (N optimizer states)
  ├── Sync Scheduler: Independent periods K_x, {K_j} for each state
  ├── Gradient Processor: Per-coordinate clipping with radius ρ
  └── State Updater: Generic UPDATE_j functions (Adam: Eqs. 1-2; ADOPT: Algorithm 3)
  ```

- **Critical path**: The synchronization check at lines 13-16 (for states) and 17-20 (for parameters) determines which states get averaged via E_m[·] at each step. This is where communication actually happens—incorrect implementation here breaks the algorithm.

- **Design tradeoffs**:
  - **Communication vs Convergence**: Larger K_u, K_v → less communication but potentially slower convergence. Paper recommends K_u=3K_x, K_v=6K_x as robust defaults.
  - **Adam vs ADOPT**: Adam requires β₂ tuning and can be unstable at high β₂; ADOPT (default β₂=0.9999) is more stable but less widely tested.
  - **Homogeneous vs Heterogeneous data**: SGDM analysis handles heterogeneity; Adam analysis assumes homogeneous losses.

- **Failure signatures**:
  - **Loss spikes**: If K_x is too large (>~1500 for 135M model), perplexity explodes. Reduce K_x.
  - **Activation growth**: If using purely local states, activations and parameter norms grow unboundedly. Ensure finite K_u, K_v.
  - **No benefit from desyncing**: If β values are low (β₁≈β₂≈0.95), first and second momenta evolve at similar rates, reducing benefit of separate K_u, K_v.

- **First 3 experiments**:
  1. **Baseline reproduction**: Train 135M GPT with DDP Adam and Local Adam (K=64) on SmolLM2 mixture. Match perplexities from Table 3 hyperparameters.
  2. **Synchronization frequency ablation**: With DES-LOC-ADOPT (β₁=0.95, β₂=0.9999), vary K_x∈{16,32,256,1024,3072} while fixing K_u=K_v=256. Reproduce the sharp degradation pattern to confirm parameter sync is critical.
  3. **Communication reduction validation**: Set K_x=256, K_u=768, K_v=1536 (2× reduction vs Local Adam with K=256). Compare final perplexity and wall-clock time against Local Adam baseline.

## Open Questions the Paper Calls Out
- Can layer-wise or parameter-group-specific synchronization periods provide additional communication savings beyond the uniform Kx, Ku, Kv settings explored in this work?
- Can adaptive or automatic tuning of synchronization periods during training improve upon the fixed 3× and 6× multipliers heuristically chosen for Ku and Kv relative to Kx?
- Can DES-LOC maintain its convergence guarantees under heterogeneous data distributions for Adam variants, or does it require the homogeneous loss assumption used in the current Adam analysis?

## Limitations
- Limited exploration of the full hyperparameter space for synchronization periods across different model sizes
- No ablation on the impact of heterogeneous data distributions for Adam variants
- The clipping radius ρ=1.0 is fixed without sensitivity analysis
- Results depend on the specific training setup with limited generalization testing

## Confidence
- **High confidence**: Core claims about communication reduction (2× vs Local Adam, 170× vs DDP) and theoretical convergence guarantees
- **Medium confidence**: Empirical scaling results for billion-parameter models and worker failure robustness
- **Medium confidence**: Seamless worker addition claims based on single dynamic scaling experiment

## Next Checks
1. **Scaling robustness test**: Reproduce DES-LOC-ADOPT training on a 1B-2B parameter model with 8-16 workers, varying K_x ∈ {64, 128, 256} and measuring communication reduction vs Local Adam at matched perplexity.

2. **Fault tolerance validation**: Design an experiment where workers are randomly dropped/re-added during training (5% failure rate every 1K steps). Compare DES-LOC against FAVG+OPT and Local Adam on 1.7B model stability and final ICL benchmark performance.

3. **Hyperparameter sensitivity analysis**: Systematically vary β₁ ∈ {0.9, 0.95, 0.99} and ρ ∈ {0.1, 0.5, 1.0} for DES-LOC-Adam on 135M model, measuring optimal K_x/K_u/K_v combinations and their impact on communication reduction.