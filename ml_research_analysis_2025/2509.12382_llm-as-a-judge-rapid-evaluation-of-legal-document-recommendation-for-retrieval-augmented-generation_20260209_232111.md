---
ver: rpa2
title: 'LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for Retrieval-Augmented
  Generation'
arxiv_id: '2509.12382'
source_url: https://arxiv.org/abs/2509.12382
tags:
- metrics
- legal
- evaluation
- systems
- agreement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using LLMs as judges for evaluating legal
  document recommendation systems. The authors found that traditional inter-rater
  reliability metrics like Krippendorff's alpha can be misleading in skewed distributions
  common in AI evaluations.
---

# LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2509.12382
- Source URL: https://arxiv.org/abs/2509.12382
- Reference count: 40
- This paper demonstrates that LLM-as-a-Judge is viable for legal RAG evaluation when using Gwet's AC2 and rank correlations for judge selection and Wilcoxon Signed-Rank Test with Benjamini-Hochberg corrections for system comparison.

## Executive Summary
This paper addresses the critical challenge of evaluating legal document recommendation systems in Retrieval-Augmented Generation (RAG) applications. The authors investigate using LLMs as judges to assess RAG system outputs across multiple quality dimensions including relevance, completeness, correctness, readability, and hallucination prevention. A key contribution is demonstrating that traditional inter-rater reliability metrics like Krippendorff's alpha can be misleading under skewed rating distributions common in AI evaluations, and instead recommend Gwet's AC2 and rank correlation coefficients for more robust judge selection. The study also establishes a statistically rigorous framework using Wilcoxon Signed-Rank Test with Benjamini-Hochberg corrections for comparing RAG systems across multiple dimensions.

## Method Summary
The evaluation framework uses two legal RAG systems (System A with BM25 retrieval and open-source LLM summarizer, System B with enhanced retrieval and GPT-4 summarizer) to generate answers to 117 legal queries. Human experts rate outputs on ordinal scales (1-4) across five dimensions. GPT-4o serves as the LLM judge, using both direct and pairwise assessment approaches with majority voting over 10 runs per query. Inter-rater reliability between human and LLM judges is computed using multiple metrics including Gwet's AC2, Spearman's rank correlation, Kendall's Tau, and percent agreement. System comparisons employ Wilcoxon Signed-Rank Test with Benjamini-Hochberg correction to control false discovery rate across multiple testing dimensions.

## Key Results
- GPT-4o achieved highest inter-rater reliability with humans (Gwet's AC2 = 0.78, Spearman = 0.73, Kendall's Tau = 0.66) among tested judges
- System B significantly outperformed System A in relevance (adjusted p = 0.0358), completeness (adjusted p = 1.215e-18), and correctness (adjusted p < 0.05)
- System A excelled in hallucination prevention (adjusted p = 0.0204) and readability (adjusted p = 0.01997)
- Krippendorff's alpha was found to be misleading under skewed distributions, deflating artificially even with high observed agreement

## Why This Works (Mechanism)

### Mechanism 1: Gwet's AC2 Resists Prevalence Paradox Under Skew
- Claim: Gwet's AC2 provides stable reliability estimates when rating distributions are highly skewed, whereas Krippendorff's alpha deflates artificially.
- Mechanism: Gwet's AC2 normalizes by available chance disagreement (1 - Σp_k²), which only vanishes when all ratings collapse into one category. Krippendorff's alpha and Cohen's Kappa normalize by expected agreement that approaches 1 under skew, shrinking the denominator and depressing the coefficient even when observed agreement is high.
- Core assumption: Skewed label distributions in legal RAG evaluation reflect task difficulty, not rater failure.
- Evidence anchors: [abstract] "traditional agreement metrics like Krippendorff's alpha can be misleading in the skewed distributions typical of AI system evaluations"; [section 4.1] "Gwet's AC2 with quadratic weighting demonstrated superior performance (0.78 for GPT4o) in assessing relevance"
- Break condition: If your rating distribution is approximately uniform, Krippendorff's alpha and Gwet's AC2 converge; the advantage disappears.

### Mechanism 2: Rank Correlations Capture Ordering Consistency Beyond Absolute Agreement
- Claim: Spearman's rank correlation and Kendall's Tau reveal LLM judges' ability to preserve relative quality ordering, which absolute agreement metrics may obscure.
- Mechanism: Correlation metrics measure whether the LLM judge ranks document-query pairs in the same order as human raters, independent of whether exact ordinal scores match. This is critical when legal precedent hierarchy or relevance gradients matter more than precise Likert scores.
- Core assumption: In legal document recommendation, correctly identifying which documents are more relevant than others is at least as important as assigning correct absolute scores.
- Evidence anchors: [abstract] "Gwet's AC2 and rank correlation coefficients emerge as more robust indicators for judge selection"; [section 4.1] "Llama's higher Spearman and Kendall Tau values indicate superior performance in maintaining relative ordering" despite lower Krippendorff's alpha
- Break condition: If your evaluation requires exact score calibration, rank correlations alone are insufficient—you need agreement metrics.

### Mechanism 3: Non-Parametric Testing with FDR Control Enables Multi-Dimension Comparison
- Claim: Wilcoxon Signed-Rank Test with Benjamini-Hochberg correction detects significant system differences across multiple quality dimensions while limiting false discoveries.
- Mechanism: WSRT handles paired ordinal data without normality assumptions, appropriate for skewed evaluation scores. B-H correction controls False Discovery Rate (FDR) rather than Family-Wise Error Rate (FWER), preserving statistical power when testing multiple dimensions simultaneously.
- Core assumption: Evaluation scores across dimensions are correlated but distinct, and exploratory analysis benefits from higher sensitivity than Bonferroni provides.
- Evidence anchors: [abstract] "Wilcoxon Signed-Rank Test with Benjamini-Hochberg corrections provides the statistical rigor needed for reliable system comparisons"; [section 4.2] System B significantly outperformed System A in relevance (adjusted p = 0.0358), completeness (adjusted p = 1.215e-18), correctness (adjusted p < 0.05)
- Break condition: If sample size drops below ~30 paired observations or distributions are symmetric, paired t-test with Bonferroni may offer more power.

## Foundational Learning

- Concept: Inter-Rater Reliability (IRR) and the Prevalence Paradox
  - Why needed here: The paper's central finding is that metric choice determines which LLM judge you select; misunderstanding IRR behavior under skew leads to wrong conclusions about judge quality.
  - Quick check question: If 90% of your ratings are "relevant" and human-LLM agreement is 88%, why might Krippendorff's alpha report low reliability?

- Concept: Non-Parametric Paired Tests vs. Parametric Alternatives
  - Why needed here: Legal RAG evaluation scores are skewed; applying t-tests would violate normality assumptions and produce unreliable p-values.
  - Quick check question: When would you choose Wilcoxon Signed-Rank Test over a paired t-test for comparing two RAG systems?

- Concept: Multiple Hypothesis Testing Corrections (FDR vs. FWER)
  - Why needed here: Evaluating systems across 5+ dimensions without correction inflates false positive rates; B-H balances discovery sensitivity with error control.
  - Quick check question: If you test 10 metrics and report any with p < 0.05 as "significant," what's your actual false positive risk?

## Architecture Onboarding

- Component map: Query -> Retrieval Layer (BM25/hybrid) -> Generation Layer (LLM summarizer) -> Evaluation Layer (LLM-as-a-Judge) -> Statistical Analysis Layer (WSRT + B-H)
- Critical path:
  1. Collect paired human and LLM judge ratings on held-out query-document pairs (117 queries used in study)
  2. Compute Gwet's AC2 and Spearman/Kendall correlations to validate judge selection
  3. Deploy validated LLM judge to score System A vs. System B across all quality dimensions
  4. Apply WSRT with B-H correction to identify significant differences per dimension
  5. Report adjusted p-values and effect direction; avoid claiming "overall superiority" without dimension-level evidence
- Design tradeoffs:
  - GPT-4o judge: Highest agreement (Gwet's AC2 = 0.78, Spearman = 0.73) but higher cost vs. open-source alternatives with lower agreement
  - B-H vs. Bonferroni: B-H preserves power for exploratory analysis; Bonferroni appropriate for high-stakes regulatory comparisons
  - Pairwise vs. direct assessment: Paper uses both with majority vote over 10 runs; increases robustness at cost of compute
- Failure signatures:
  - Krippendorff's alpha shows low agreement (< 0.4) despite high percent agreement (> 70%) → suspect distribution skew, switch to Gwet's AC2
  - Adjusted p-values all non-significant despite visible score differences → check sample size adequacy and whether WSRT assumptions hold
  - LLM judge systematically favors one system regardless of query difficulty → self-preference bias; use blinded pairwise comparison
- First 3 experiments:
  1. Baseline IRR validation: Have 2+ human experts rate 50 query-document pairs; compute Gwet's AC2, Spearman, and Krippendorff's alpha between humans to establish human ceiling, then measure LLM judge against human baseline using same metrics
  2. Skew sensitivity test: Stratify evaluation set into balanced vs. skewed subsets; verify Gwet's AC2 remains stable while Krippendorff's alpha drops under skew
  3. System A/B comparison with full protocol: Score 100+ legal queries with validated LLM judge across 5 dimensions; apply WSRT with B-H correction; document which dimensions show significant differences and effect sizes

## Open Questions the Paper Calls Out

- How do LLM-as-a-Judge evaluation frameworks generalize to other high-stakes specialized domains beyond legal (e.g., medical, financial)? The study only evaluated Bloomberg Law products; different domains have distinct terminology, accuracy requirements, and regulatory constraints that may affect judge reliability differently.

- How should practitioners systematically resolve conflicting IRR metric rankings when selecting LLM judges? The authors found that "Prometheus2 8x7B exhibits a higher K-Alpha (0.43) than the Llama model (0.32)... However, for ordinal data where rank preservation is crucial, Llama's higher Spearman and Kendall Tau values indicate superior performance" but provide no decision framework for when metrics disagree.

- What is the minimum sample size required to achieve stable IRR estimates when selecting LLM judges for specialized domains? The paper uses 117 queries and defends this as "realistic and practically representative of typical industry evaluations" but acknowledges "generating large-scale, expert-curated queries poses practical challenges" without conducting power analysis.

## Limitations

- The study's findings depend heavily on the specific legal domain and relatively small evaluation set (117 queries)
- The superiority of GPT-4o as a judge may not generalize to other legal subfields or different LLM judge architectures
- The analysis assumes ordinal scales are appropriate for all dimensions, though some (like hallucination detection) may benefit from binary classification approaches

## Confidence

- High Confidence: The statistical methodology (WSRT with B-H correction) and its appropriateness for ordinal, non-normal data
- Medium Confidence: The superiority of Gwet's AC2 over Krippendorff's alpha under skew, based on theoretical foundations but limited empirical corpus support
- Medium Confidence: The recommendation to use rank correlations alongside agreement metrics, as this represents a methodological choice rather than a universal requirement

## Next Checks

1. **Distribution Sensitivity Test:** Systematically vary the label distribution (balanced vs. skewed) in synthetic datasets to empirically validate that Gwet's AC2 remains stable while Krippendorff's alpha deflates under prevalence conditions

2. **Domain Transfer Experiment:** Apply the same LLM-as-Judge methodology to a non-legal domain (e.g., medical or technical documentation) to test whether GPT-4o maintains its reliability advantage across domains

3. **Metric Substitution Analysis:** Replace ordinal scales with binary classifications for dimensions like hallucination detection and compare IRR results to assess whether different measurement scales affect judge selection and system comparison outcomes