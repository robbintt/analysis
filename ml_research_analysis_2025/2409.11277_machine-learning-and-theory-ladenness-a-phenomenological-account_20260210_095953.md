---
ver: rpa2
title: Machine Learning and Theory Ladenness -- A Phenomenological Account
arxiv_id: '2409.11277'
source_url: https://arxiv.org/abs/2409.11277
tags:
- theory
- data
- domain
- which
- scientific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that machine learning model-building practices
  in science are largely theory-indifferent, meaning they do not require explicit
  reference to domain-theory (scientific expertise of the field where ML is applied).
  The authors analyze ML models through comparison with phenomenological models, showing
  that MLMs are constructed primarily through data-fitting procedures and optimization
  tasks that are independent of theoretical considerations.
---

# Machine Learning and Theory Ladenness -- A Phenomenological Account

## Quick Facts
- arXiv ID: 2409.11277
- Source URL: https://arxiv.org/abs/2409.11277
- Reference count: 15
- Primary result: Machine learning model-building practices are largely theory-indifferent, enabling easy domain transfer but raising questions about whether they should be theory-laden.

## Executive Summary
This paper argues that machine learning model-building practices in science are fundamentally theory-indifferent, meaning they do not require explicit reference to domain-theory during construction. Through comparison with phenomenological models, the authors show that MLMs are constructed primarily through data-fitting procedures and optimization tasks independent of theoretical considerations. They introduce the concept of "theory-infection" to describe how domain-theory can be passively inherited from data sets without being necessary for model construction. This theory-indifference has important implications: it enables ML methods to transfer easily across scientific domains and shifts the debate from whether ML should be theory-laden to normative questions about whether it should be. The analysis challenges the prevailing "blanket view" in philosophy of science that all scientific activities must be theory-laden.

## Method Summary
The paper employs conceptual analysis and philosophical argumentation rather than empirical experimentation. The authors compare ML model-building practices with phenomenological modeling traditions, examining three components of ML systems (training sample, training engine, learned model) through the lens of theory-ladenness. They analyze specific examples including CNNs, the SIR epidemiological model, AlphaFold, and neuroimaging research to illustrate their claims about theory-indifference and theory-infection. The approach is primarily descriptive, establishing that ML construction can be theory-indifferent, though they note this shifts the discussion to normative questions about whether it should be.

## Key Results
- ML model construction is theory-indifferent: optimization procedures, hyperparameter selection, and weight learning require no domain-theoretic reference
- ML models are theory-infected: domain-theory is passively inherited from curated data but remains untraceable in the final model's parameters
- ML models can transfer "as-is" across scientific domains without architectural modifications
- The theory-indifference claim challenges the "blanket view" that all scientific activities must be theory-laden

## Why This Works (Mechanism)

### Mechanism 1
Weight parameter learning is theory-indifferent; optimization procedures require no domain-theoretic reference. Training engines minimize loss functions via gradient-based heuristics that exploit mathematical properties of differentiable functions, not domain semantics. The same loss function (e.g., mean squared error) applies identically across domains. Core assumption: Predictive accuracy on in-distribution data is achievable without domain semantics.

### Mechanism 2
Hyperparameter and architectural choices are guided by data modality, not domain-theory. Selection relies on engineering considerations (regression vs classification, image vs time-series format) rather than semantic interpretation of features. CNNs process any image similarly; LSTMs process any sequence. Core assumption: Data modality (not domain semantics) suffices to determine suitable architectures.

### Mechanism 3
Models are theory-infected—domain-theory passively inherited from curated data but not required for construction. Data curation embeds domain-theoretic priors (classification schemas, measurement protocols). Training scatters this information across millions of parameters lacking semantic interpretation, making theoretical priors untraceable in the final model. Core assumption: Inherited priors do not structurally constrain model-building; they are instrumental artifacts.

## Foundational Learning

- Concept: **Theory-Ladenness (philosophy of science)**
  - Why needed here: The paper argues against the "blanket view" that all scientific activities require domain-theory. Understanding the distinction between theory-informed, theory-directed, and theory-testing is prerequisite.
  - Quick check question: Can you distinguish a theory-informed model (uses domain assumptions as constraints) from a theory-indifferent model (no necessary domain reference)?

- Concept: **Phenomenological Models vs. Theoretical Models**
  - Why needed here: The comparison frames ML models as mathematically descriptive rather than explanatory. Phenomenological models correlate parameters without postulating causal structure.
  - Quick check question: Does a model that fits data without explaining mechanism count as phenomenological?

- Concept: **Optimization Landscapes and Local Minima**
  - Why needed here: The paper notes training seeks "sufficiently good local minima" rather than global optima due to non-convex loss surfaces. This grounds why heuristic strategies (e.g., SGD) are architecture-agnostic.
  - Quick check question: Why does gradient descent not require domain knowledge about what the loss function represents?

## Architecture Onboarding

- Component map: **ML System = Training Sample + Training Engine + Learned Model**
- Critical path: Data curation (domain experts, theory-laden) → Feature engineering (increasingly automated, theory-indifferent) → Hyperparameter selection (engineering-driven, theory-indifferent) → Weight optimization (mathematical, theory-indifferent) → Deployment/use (domain-theory may return for interpretation)
- Design tradeoffs:
  - **Theory-infused vs. theory-indifferent**: Infusing domain-theory (e.g., AlphaFold) may improve robustness/OOD generalization but reduces transferability and requires interdisciplinary collaboration.
  - **Automation vs. interpretability**: Automated feature learning (deep learning) increases theory-indifference but reduces parameter semantic interpretability.
  - **Proximate goal (prediction) vs. ultimate goal (explanation)**: MLMs excel at prediction; explanation requires supplementary tools/methods.
- Failure signatures:
  - Model performs well in-distribution but fails on distribution shift—suggests over-reliance on spurious correlations without domain-theoretic grounding.
  - Features learned are uninterpretable to domain experts—indicates deep theory-indifference; may limit scientific adoption.
  - Transfer to new domain yields poor accuracy despite retraining—check data modality mismatch or feature engineering assumptions.
- First 3 experiments:
  1. **Transferability test**: Train a CNN on Siim-pneumothorax (publicly available on Kaggle), then retrain the same architecture on a different medical imaging dataset (e.g., chest X-ray for tuberculosis or knee MRI). Measure predictive accuracy to test "as-is" transfer.
  2. **Feature semantics audit**: For a deep learning model, extract embeddings and attempt to correlate with domain-expert features (e.g., cortical thickness). Assess interpretability gap.
  3. **Theory-infusion intervention**: Train two models—one theory-indifferent (standard), one theory-infused (domain-constrained optimization). Compare OOD robustness. Tests normative claim about whether theory-ladenness is desirable.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is it desirable to explicitly infuse domain-theory into ML model construction, or should the process remain theory-indifferent?
- Basis in paper: [explicit] The authors state that their analysis shifts the debate "from descriptive... to normative" goals, specifically asking: "is it desirable to infuse domain-theory in ML systems?" (p. 32).
- Why unresolved: The paper establishes that ML construction *can* be theory-indifferent, but the authors explicitly refrain from arguing whether it *should* be, noting they "do not have the space to provide an argument in favour or against theory-ladenness from this normative perspective" (p. 32).
- What evidence would resolve it: A comparative analysis of scientific outcomes (e.g., novelty, reliability) from "verticalized" (theory-laden) models versus general-purpose (theory-indifferent) foundation models across multiple scientific disciplines.

### Open Question 2
- Question: Does incorporating domain-theory into ML construction improve performance on epistemic desiderata like explainability, robustness, and out-of-distribution generalizability?
- Basis in paper: [explicit] The authors identify the need to determine if "an explicit reference to domain-theory in MLM-construction should be required to address other epistemic desiderata... such as explainability, robustness and generalisability" (p. 33).
- Why unresolved: While some scholars argue theory improves robustness (e.g., Pearl 2019 cited on p. 33), others push for fully automated, theory-indifferent AI (e.g., Lu et al. 2024). The paper highlights this "conflict" but does not resolve which approach yields better non-predictive metrics.
- What evidence would resolve it: Empirical benchmarks comparing theory-constrained models against purely data-driven models specifically on out-of-distribution (OOD) samples and interpretability metrics.

### Open Question 3
- Question: Does the theory-indifference of ML model construction imply that scientific training should shift from domain-specific curricula to general-purpose data science education?
- Basis in paper: [explicit] The authors ask: "do we need ML curricula specific for a given discipline, or just one, general-purpose curriculum, that will be adequate for future ML practitioners to work in any scientific discipline?" (p. 31).
- Why unresolved: This question arises from the concept of "as-is transferability" (p. 29), but it remains open because the authors acknowledge that while *construction* is theory-indifferent, the *use* and interpretation of models still rely heavily on domain-expertise (p. 17).
- What evidence would resolve it: Studies tracking the efficacy of "general-purpose" ML practitioners versus domain-experts in successfully applying and interpreting models in specialized scientific contexts.

## Limitations
- The theory-indifference claim is primarily conceptual rather than empirically validated; no quantitative metrics or experimental results are provided.
- The paper relies heavily on illustrative examples rather than comprehensive case studies across scientific domains, limiting generalizability.
- The "theory-infection" concept lacks operational definitions or methods to detect or measure this infection.
- The normative shift from descriptive to prescriptive is briefly gestured at but not developed, leaving practical implications ambiguous.

## Confidence

- **High confidence**: The claim that ML model-building optimization procedures (gradient descent, loss minimization) are mathematically theory-indifferent and apply identically across domains.
- **Medium confidence**: The transferability claim that ML models can be "transferred as-is" across scientific domains without theoretical modification, based on limited illustrative examples.
- **Low confidence**: The theory-infection concept as a meaningful distinction from intentional theory-infusion, given lack of operational definitions or empirical validation methods.

## Next Checks
1. Conduct systematic transferability experiments across diverse scientific domains (e.g., medical imaging → materials science, genomics → astronomy) measuring performance degradation and architectural requirements.
2. Develop operational metrics for quantifying theory-indifference (e.g., percentage of model parameters that can be explained by domain theory vs. pure data-fitting) and apply them to benchmark models.
3. Design controlled experiments comparing theory-indifferent vs. theory-infused model variants on the same scientific tasks to empirically test claims about OOD robustness and explanatory power.