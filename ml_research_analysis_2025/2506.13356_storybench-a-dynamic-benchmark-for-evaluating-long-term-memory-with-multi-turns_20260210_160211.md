---
ver: rpa2
title: 'StoryBench: A Dynamic Benchmark for Evaluating Long-Term Memory with Multi
  Turns'
arxiv_id: '2506.13356'
source_url: https://arxiv.org/abs/2506.13356
tags:
- memory
- long-term
- reasoning
- arxiv
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: StoryBench is a novel benchmark for evaluating long-term memory
  (LTM) in large language models (LLMs) through dynamic, branching narrative games.
  It addresses the lack of standardized benchmarks for LTM by simulating real-world
  decision-making with multi-turn interactions, testing both knowledge retention and
  sequential reasoning.
---

# StoryBench: A Dynamic Benchmark for Evaluating Long-Term Memory with Multi Turns

## Quick Facts
- arXiv ID: 2506.13356
- Source URL: https://arxiv.org/abs/2506.13356
- Reference count: 13
- Primary result: Novel benchmark evaluates LTM in LLMs through branching narratives, exposing significant gaps in sequential reasoning and self-recovery

## Executive Summary
StoryBench introduces a dynamic benchmark for evaluating long-term memory (LTM) in large language models through interactive fiction games with branching narratives. The benchmark addresses the gap in standardized LTM evaluation by testing models' ability to maintain context and reason over extended sequences across 80+ branching paths. Through two evaluation modes—Immediate Feedback and Self Recovery—the benchmark reveals that even models with large context windows struggle with sequential reasoning and independent error correction. Results show varying performance across four models, with GPT-4o and Claude 3.5 Sonnet demonstrating stronger LTM capabilities than Doubao 1.5-pro and Deepseek-R1.

## Method Summary
StoryBench evaluates LTM through branching interactive fiction games structured as directed acyclic graphs with scene and choice nodes. The benchmark features two modes: Immediate Feedback (retry after incorrect choices with explicit error signals) and Self Recovery (no feedback, requiring independent error identification and correction). Models navigate 80+ branching paths from "The Invisible Guardian" game, with decisions classified as "easy" or "hard" based on cognitive demands. Evaluation metrics include knowledge retention (Overall Accuracy, First-Try Accuracy, Longest Consecutive Correct) and sequential reasoning (Easy/Hard Accuracy, Retry Count, ErrorCount≥9). The dataset consists of 311 scene nodes and 86 choice nodes in JSON format, with Chain-of-Thought prompting and a 9-retry threshold for Self Recovery mode.

## Key Results
- GPT-4o and Claude 3.5 Sonnet outperform Doubao 1.5-pro and Deepseek-R1 on LTM metrics across all evaluation modes
- Hard decisions show 24.73 percentage point gap (84.94% to 60.21%) for Deepseek-R1, indicating significant sequential reasoning limitations
- Self Recovery mode reveals models struggle with independent error correction, often backtracking only 1-2 steps when errors originate 5+ turns back
- Content filtering blocks occur for GPT-4o on weapon-related terms, requiring vocabulary preprocessing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Branching narratives create cascading memory dependencies that stress-test LTM beyond static retrieval.
- Mechanism: The benchmark structures storylines as directed acyclic graphs with scene nodes and choice nodes. Each decision modifies downstream state, requiring models to track latent variables across turns. Hard decisions explicitly require "recalling information from a distant context, tracking latent state changes, or performing multi-step sequential reasoning."
- Core assumption: Real-world memory demands involve interdependent decisions rather than isolated fact queries.
- Evidence anchors: [abstract] "each choice triggers cascading dependencies across multi-turn interactions"; [Section 4.2] Figure 3 illustrates four dependency patterns; [corpus] CogMem paper confirms LLMs "often lose accuracy and coherence over extended, multi-turn interactions."

### Mechanism 2
- Claim: Feedback suppression reveals true self-correction capacity that immediate feedback masks.
- Mechanism: In Self Recovery mode, incorrect choices propagate to failure endings without hints. Models must independently identify the error origin and backtrack. The paper reports that First-Try Accuracy and Longest Consecutive Correct Sequence increase for some models when feedback is removed, suggesting immediate feedback can disrupt long-horizon coherence.
- Core assumption: Self-correction requires maintaining an explicit causal model of past decisions, not just reactive adjustment.
- Evidence anchors: [abstract] "Self Recovery requiring models to independently trace back and revise earlier choices after failure"; [Section 5.2.2] "Claude 3.5 and GPT-4o never reach [error threshold], suggesting their task completions are entirely due to self-correction"; [corpus] MEMTRACK benchmark similarly evaluates state tracking without intermediate feedback.

### Mechanism 3
- Claim: Differential difficulty classification isolates memory-specific failures from reasoning failures.
- Mechanism: Decisions are labeled "hard" if they require distant recall, latent state tracking, or multi-step reasoning; otherwise "easy." The gap between Easy and Hard Accuracy reveals whether failures stem from basic comprehension or memory/reasoning limitations.
- Core assumption: Easy/Hard classification reliably captures cognitive load differences.
- Evidence anchors: [Section 3.4.2] Formal definitions of Accuracy(t)_easy and Accuracy(t)_hard; [Section 5.2.1] "most models show large gaps between Easy and Hard Accuracy, reflecting their large gaps in sequential reasoning"; [corpus] Weak direct evidence—neighbor papers do not explicitly validate difficulty classification schemes.

## Foundational Learning

- Concept: **Context window vs. Long-term memory**
  - Why needed here: The paper distinguishes passive context retention from active LTM (integrating and updating knowledge across dynamic interactions). Models with 256k token windows still fail on sequential reasoning despite capacity.
  - Quick check question: Can you explain why a model with a 200k+ token context window might still fail a 50-turn narrative task?

- Concept: **Causal dependency chains in sequential decision-making**
  - Why needed here: StoryBench's core challenge is tracking how early choices constrain later options. Failure analysis shows models "exhibit shallow search strategies, typically backtracking only one or two steps."
  - Quick check question: In a 10-turn story where turn 3 affects turn 7, what information must the model maintain, and what must it infer?

- Concept: **Multi-solution evaluation and reward ambiguity**
  - Why needed here: Unlike single-answer benchmarks, branching narratives have multiple valid paths. The dataset annotation explicitly supports this, requiring evaluation logic that doesn't over-penalyze valid alternative choices.
  - Quick check question: How would you design a scoring function that rewards valid alternative paths without inflating success rates via random guessing?

## Architecture Onboarding

- Component map:
  - Scene nodes (311 entries) -> Choice nodes (86 entries) -> Branching paths (80+)
  - JSON DAG structure: scenes contain descriptions, characters, dialogues; choices contain context and branching options
  - Evaluation modes: Immediate Feedback (retry with error signal) and Self Recovery (no feedback, 9-retry threshold)

- Critical path:
  1. Load narrative graph from JSON dataset
  2. Initialize at starting scene node
  3. Present scene description + choice options to model
  4. Parse model's selection, traverse to next node
  5. If Immediate Feedback + incorrect: retry with explicit error signal
  6. If Self Recovery + failure ending: prompt model to identify error point and restart from that choice
  7. Record decision sequence {c1, c2, ..., cT} and compute all metrics

- Design tradeoffs:
  - Single narrative source (The Invisible Guardian) vs. multi-domain: Enables controlled, coherent evaluation but limits generalizability to non-narrative tasks
  - Manual annotation vs. synthetic generation: Preserves realistic complexity but scales slowly; synthetic alternatives noted as "overly simplistic"
  - Retry threshold of 9 in Self Recovery: Prevents infinite loops but may artificially assist models that would otherwise fail indefinitely

- Failure signatures:
  - Contextual inconsistency: Decisions contradicting earlier events or character motivations
  - Shallow backtracking: Revising only 1-2 steps when error origin is 5+ turns back
  - Format mismatch: Returning option indices instead of decision point IDs
  - Content filtering blocks: GPT-4o sensitive to weapon-related terms; requires vocabulary filtering
  - Stall loops: Repeatedly selecting the same wrong option (mitigated by retry threshold)

- First 3 experiments:
  1. Baseline probe: Run Immediate Feedback mode on 5 random paths with a single model (e.g., Claude 3.5 Sonnet). Compute Overall Accuracy, Hard Accuracy, and Retry Count. This establishes ground truth for your evaluation pipeline and validates metric calculations against the paper's reported ranges (Claude: Overall 74.86%, Hard 69.38%).
  2. Ablation on feedback timing: Compare Immediate Feedback vs. Self Recovery on identical paths. Specifically test whether First-Try Accuracy and Longest Consecutive Correct increase in Self Recovery mode (as reported in Figure 8). This validates the counterintuitive feedback disruption hypothesis.
  3. Difficulty validation: Manually inspect 10 decisions labeled "hard" to confirm they require distant recall or multi-step reasoning. Measure the correlation between human-rated difficulty and model performance gap. This tests whether the Easy/Hard classification captures genuine cognitive load or prompt artifacts.

## Open Questions the Paper Calls Out

- How does StoryBench performance translate to multimodal or task-oriented contexts outside of text-based interactive fiction? [explicit] Section 6 (Limitations) states that the text-based, single-domain nature of the scenarios may limit generalizability to other knowledge-intensive or multimodal contexts.
- How does increasing the narrative length (e.g., doubling the chapters) impact the assessment of long-term dependencies? [explicit] Section 6 notes that the current dataset consists of only 6 chapters, which may not fully capture complex reasoning in more extensive narratives.
- Why does the removal of immediate feedback lead to increased First-Try Accuracy in Self Recovery mode for certain models? [inferred] Section 5.2.2 observes an "amazing trend" where First-Try Accuracy and Longest Consecutive Correct Sequence metrics improved without feedback.
- Can memory-augmented architectures (e.g., RAG, MemGPT) overcome the "Self Recovery" failures observed in standard foundation models? [inferred] Section 5.1 explicitly excludes memory-augmented models as they center on isolated retrieval, while the Conclusion calls for developing such mechanisms to solve the identified failures.

## Limitations

- Single-source narrative limits generalizability to other domains and knowledge-intensive contexts
- Manual annotation process introduces subjective bias in difficulty classification and dependency patterns
- Lack of empirical validation against other long-sequence tasks (multi-document QA, extended code generation)
- Easy/Hard classification lacks formal operational definitions and systematic validation

## Confidence

**High confidence** in the benchmark architecture and metric definitions. The DAG structure, JSON schema, and evaluation modes are precisely specified with clear implementation paths.

**Medium confidence** in the generalizability of results. While the 80+ branching paths provide statistical power, performance on a single narrative source may not extrapolate to other domains.

**Low confidence** in the difficulty classification scheme. The paper defines "hard" decisions but provides no systematic validation that human annotators consistently apply these criteria.

## Next Checks

1. **Domain transfer validation**: Evaluate the same benchmark on a different narrative source (e.g., another interactive fiction game or multi-chapter novel) to quantify performance degradation and assess generalizability claims.

2. **Difficulty classification audit**: Conduct inter-annotator reliability testing on 100+ decisions, measuring Cohen's kappa between human raters applying the hard/easy criteria. Compute correlation between human-rated difficulty and model performance gaps.

3. **Alternative task comparison**: Benchmark models on a non-narrative long-sequence task (e.g., multi-document QA with 50+ paragraphs or extended code generation) to determine whether StoryBench results reflect general LTM limitations or narrative-specific challenges.