---
ver: rpa2
title: Mathematics of Continual Learning
arxiv_id: '2504.17963'
source_url: https://arxiv.org/abs/2504.17963
tags:
- learning
- continual
- task
- tasks
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the mathematical foundations of continual learning
  by connecting it to adaptive filtering theory. The authors show that classic adaptive
  filtering methods like Least Mean Squares (LMS), Affine Projection Algorithm (APA),
  Recursive Least-Squares (RLS), and Kalman Filter (KF) can be extended and interpreted
  as continual learning methods.
---

# Mathematics of Continual Learning

## Quick Facts
- arXiv ID: 2504.17963
- Source URL: https://arxiv.org/abs/2504.17963
- Reference count: 40
- One-line primary result: Classic adaptive filtering methods (LMS, APA, RLS, KF) can be extended and interpreted as continual learning methods with provable guarantees

## Executive Summary
This paper establishes a unified mathematical framework connecting adaptive filtering theory to continual learning. The authors demonstrate that classic adaptive filtering algorithms like Least Mean Squares (LMS), Affine Projection Algorithm (APA), Recursive Least-Squares (RLS), and Kalman Filter (KF) can be extended and rigorously interpreted as continual learning methods. They prove theoretical equivalences between different continual learning approaches and show that these methods converge to minimum-norm solutions under shared parameter assumptions. The paper also demonstrates how KF combined with the Rauch-Tung-Striebel smoother provides positive backward transfer, and proposes three practical extensions for applying these techniques to modern deep learning architectures.

## Method Summary
The paper establishes theoretical connections between adaptive filtering methods and continual learning by showing that classic algorithms like APA†, ICL, and ORFit converge to the same minimum-norm solution for linear regression tasks with shared ground-truth parameters. RLS is proven equivalent to a special case of Kalman Filter, while KF combined with RTS smoother provides provable positive backward transfer through covariance reduction. The authors propose three main extensions for deep learning: linearization of nonlinear models using first-order Taylor expansions, layer-wise application to neural networks (reducing memory complexity from O(d²) to O(LD²)), and combination with pre-trained models using SVD-based low-rank projections.

## Key Results
- APA†, ICL, and ORFit converge to the same minimum-norm solution for linear models with shared ground-truth parameters
- RLS with forgetting factor β < 1 naturally implements continual learning by gradually downweighting past samples
- KF combined with Rauch-Tung-Striebel smoother provides provable positive backward transfer through covariance reduction
- The proposed layer-wise extension reduces memory complexity from O(d²) to O(LD²) for deep networks

## Why This Works (Mechanism)

### Mechanism 1
APA†, ICL, and ORFit converge to the same minimum-norm solution for linear regression tasks with shared ground-truth parameters. Each method maintains an orthogonal projection matrix P_t onto Span(X_:t)^⊥, and the update θ_t = P_t θ_{t-1} is mathematically equivalent to directly solving min ||θ||² s.t. y_:t = X^⊤_:t θ. This works because sequential projection equals one-shot projection to the intersection. The core assumption is that tasks share a common solution θ* (Assumption 1), enabling constraint feasibility in the overparameterized regime (t ≤ d).

### Mechanism 2
RLS with forgetting factor β < 1 naturally implements continual learning that gradually downweights past samples. RLS minimizes weighted least-squares with exponentially decaying weights 1/β^i, and the Woodbury identity enables recursive update of the inverse Hessian Φ_t without storing past data. When β → 0, RLS becomes equivalent to constraint-based methods. This mechanism requires no shared θ* assumption, making it more flexible for tasks with differing solutions.

### Mechanism 3
Kalman Filter combined with Rauch-Tung-Striebel smoother provides provable positive backward transfer in linear Gaussian models. KF computes forward estimates θ_{t|t} with error covariance Σ_{t|t}, and RTS runs backward, updating past estimates θ_{i|t} (i < t) using the linear task relationship θ_i = A_i θ_{i-1} + w_i. Theorem 7 proves Σ_{i|t} ⪯ Σ_{i|s} for all i ≤ s < t: covariance strictly decreases with more data, enabling improved estimates for past tasks.

## Foundational Learning

- **Orthogonal projection matrices and subspace operations**: Core mathematical structure for understanding how APA, ICL, and GP constrain updates to preserve past task performance. Quick check: Given matrix X ∈ R^{d×t} with t < d, compute the projection onto Span(X)^⊥. Do you understand why P_t = I - X(X^⊤X)^{-1}X^⊤?

- **Woodbury matrix identity for rank-k updates**: Enables O(d²) recursive updates to inverse Hessians rather than O(d³) recomputation; underlies RLS and KF efficiency. Quick check: Derive the update (Φ^{-1} + xb^{-1}x^⊤)^{-1} = Φ - Φx(b + x^⊤Φx)^{-1}x^⊤Φ. Can you identify where this appears in RLS updates?

- **Conditional distributions for jointly Gaussian vectors**: KF derivation relies on computing p(θ_t | y_{1:t}) from p(θ_t | y_{1:t-1}) via Gaussian conditioning formulas. Quick check: If [a; b] ~ N(μ, Σ) is jointly Gaussian, write down E[a|b] and Cov[a|b] in terms of the block structure of μ and Σ.

## Architecture Onboarding

- **Component map**: Projection matrix P_t or inverse Hessian Φ_t -> Model parameters θ_t -> Memory buffer (for APA variants) -> Kalman Filter states (θ_t|t, Σ_t|t)

- **Critical path**: 1) Initialize θ_0 (zero or pre-trained), P_0 = I (or Φ_0 = I/λ) 2) For each new task/sample (x_t, y_t): compute projected data x̃_t = P_{t-1} x_t 3) Update θ_t using method-specific gain formula 4) Update P_t or Φ_t incrementally (Eq. 7 or 12) 5) For KF+RTS: after forward pass, run backward smoother to improve past estimates

- **Design tradeoffs**: Memory vs. guarantees (APA†/ICL/ORFit guarantee zero forgetting for linear models but require O(d²) storage for P_t; GP with low-rank approximation reduces memory but breaks guarantees), Flexibility vs. assumptions (RLS handles differing task solutions but averages poorly when solutions are far apart; KF handles arbitrary task relationships but requires known/learned A_i), Linear vs. nonlinear (Layer-wise application scales as O(LD²) memory; linearization works locally but accumulates approximation error)

- **Failure signatures**: Gradient projection stalls when P_{t-1}^ℓ Δ_t^ℓ = 0 (feature matrix has full row rank); switch to low-rank approximation, Numerical instability with RLS on ill-conditioned features (common with pre-trained models); use SVD-based projection updates, Catastrophic forgetting returns when using β < 1 (forgetting factor) when tasks should be remembered; check β ≥ 1 setting

- **First 3 experiments**: 1) Linear regression sanity check: Generate synthetic data with known θ*; verify APA†, ICL, ORFit all converge to θ* and that P_t θ_0 = θ_t (validates Theorem 4) 2) Ablate memory buffer size b in APA: Plot MSE vs. b on p-recurring tasks; should see convergence to LMS behavior as b → 0 3) KF backward transfer test: Implement KF+RTS on linear Gaussian model; measure E[||θ_i - θ_{i|t}||²] for various t > i; should decrease with t (validates Theorem 7)

## Open Questions the Paper Calls Out

1. What are the optimal stepsizes for LMS variants when applied to settings with more than two recurring tasks? While optimal stepsizes exist for two recurring tasks, finding optimal stepsizes for more than two recurring tasks remains an open problem due to complex contraction factors that don't generalize trivially from the 2-recurring case.

2. Can rigorous non-forgetting guarantees be derived for the original nonlinear formulations of continual learning problems without relying on linearization? The paper notes that while linearized problems allow for proofs similar to linear models, deriving such guarantees for the original nonlinear problems remains challenging due to the inability of first-order Taylor expansions to capture global dynamics and loss landscapes of deep nonlinear networks.

3. How can the trade-off between the practical necessity of low-rank approximations and the theoretical guarantees of Gradient Projection (GP) be bridged? Using a low-rank approximation for the subspace projection destroys the orthogonality required to ensure the gradient update does not interfere with past knowledge, theoretically opening the door to forgetting. A modified GP algorithm or analysis that quantifies/limits the forgetting when using approximate projection matrices would resolve this tension.

## Limitations

- Theoretical results apply strictly to linear models and overparameterized regimes, with deep learning extensions lacking rigorous validation
- Numerical stability issues with pre-trained models and ill-conditioned feature matrices are acknowledged but not fully resolved
- The assumption of shared ground-truth parameters across tasks (Assumption 1) is restrictive and may not hold in many practical scenarios

## Confidence

- **High Confidence**: The equivalence proofs between APA†, ICL, and ORFit (Mechanism 1) and the RLS-KF connection (Mechanism 2) are mathematically rigorous with explicit derivations
- **Medium Confidence**: The KF+RTS backward transfer results (Mechanism 3) are theoretically sound but require careful implementation of the RTS smoother for validation
- **Low Confidence**: The proposed extensions to deep learning (linearization, layer-wise application) lack empirical validation and may suffer from approximation errors in practice

## Next Checks

1. Implement the LGM simulation with KF+RTS and verify that tr(Σ_{i|t}) decreases monotonically for past tasks i < t (Theorem 7 validation)
2. Test the layer-wise application on a small CNN using Permuted-MNIST to verify that the O(LD²) memory scaling is manageable and that positive backward transfer occurs
3. Compare low-rank vs. full-rank projection matrices in GP on a benchmark dataset to quantify the trade-off between memory efficiency and forgetting guarantees