---
ver: rpa2
title: High-Throughput LLM inference on Heterogeneous Clusters
arxiv_id: '2504.15303'
source_url: https://arxiv.org/abs/2504.15303
tags:
- uni00000013
- uni00000048
- uni00000057
- uni00000051
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses high-throughput LLM inference on heterogeneous
  clusters, tackling two challenges: optimizing deployment configurations across different
  hardware and balancing request scheduling among instances with varying processing
  capacities. The authors propose a lightweight profiling-based approach to model
  instance memory usage and processing time, enabling efficient configuration search,
  and introduce a scheduler that considers both computational power and memory utilization
  to distribute requests.'
---

# High-Throughput LLM inference on Heterogeneous Clusters

## Quick Facts
- arXiv ID: 2504.15303
- Source URL: https://arxiv.org/abs/2504.15303
- Authors: Yi Xiong; Jinqi Huang; Wenjie Huang; Xuebing Yu; Entong Li; Zhixiong Ning; Jinhua Zhou; Li Zeng; Xin Chen
- Reference count: 19
- Key outcome: 122.5% and 33.6% throughput improvements on two heterogeneous clusters

## Executive Summary
This paper addresses the challenge of achieving high-throughput LLM inference on heterogeneous GPU clusters by optimizing both deployment configurations and request scheduling. The authors propose a lightweight profiling-based approach to model instance memory usage and processing time, enabling efficient configuration search without exhaustive benchmarking. They introduce a scheduler that considers both computational power and memory utilization to distribute requests, significantly improving throughput over baseline strategies.

## Method Summary
The system profiles prefill and decode times as linear functions of batch size and sequence length, using these parameters to estimate throughput for different tensor parallelism settings via static batching simulation. The scheduler calculates a workload score for each request-instance pair, weighting processing time by an exponential factor of current KV cache utilization, and selects the assignment that minimizes the maximum workload across instances. The approach is evaluated on two heterogeneous clusters using LLaMA models, showing significant throughput improvements over Round Robin scheduling.

## Key Results
- Throughput improvements of 122.5% and 33.6% on two heterogeneous clusters compared to Round Robin scheduling
- Profiler successfully identifies optimal tensor parallelism configurations without exhaustive benchmarking
- Memory-aware scheduling prevents bottlenecks on weaker instances and improves overall system utilization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: A lightweight profiling model using linear regression can effectively rank deployment configurations for throughput without requiring exhaustive benchmarking.
- **Mechanism**: The system profiles prefill and decode times as linear functions of batch size and sequence length, then estimates total processing time for a sample workload using static batching simulation to approximate throughput of different tensor parallelism settings.
- **Core assumption**: The relationship between compute/memory usage and latency is sufficiently linear to preserve the ranking of configuration performance, even if absolute throughput estimates are inaccurate.
- **Evidence anchors**: [abstract] modeling resource amount and expected throughput, [section 3.1] linear fitting of T_prefill and T_decode parameters, [corpus] indirect support from AccelGen using profiling for heterogeneous clusters.
- **Break condition**: If the underlying inference engine uses complex dynamic batching that significantly alters latency distribution compared to the static model, the ranking of configurations may diverge from reality.

### Mechanism 2
- **Claim**: Scheduling requests based on a combined metric of processing capacity and current KV cache utilization maximizes throughput by preventing memory bottlenecks on weaker instances.
- **Mechanism**: The scheduler calculates a "workload" score for assigning a request to instance, weighting the predicted processing time by an exponential factor of the instance's current KV cache usage.
- **Core assumption**: Instances with high memory usage will soon become bottlenecks, so their effective workload score should be penalized exponentially.
- **Evidence anchors**: [abstract] scheduler that considers both computational power and memory utilization, [section 4.1] defines workload calculation and kv_usage concept, [corpus] no direct evidence for this specific exponential weighting formula.
- **Break condition**: If the output length predictor is highly inaccurate, the kv_usage term will misguide the scheduler, potentially causing queueing on fast instances or OOM errors on slow ones.

### Mechanism 3
- **Claim**: Minimizing the maximum workload across all instances yields higher system throughput than simple Round Robin or purely compute-weighted strategies.
- **Mechanism**: The mapper component selects the target instance that results in the lowest value for max(workloads of instances) after the assignment, updating workload state via hooks when requests complete.
- **Core assumption**: Throughput is bottlenecked by the slowest instance; therefore, equalizing completion times across heterogeneous hardware maximizes aggregate token flow.
- **Evidence anchors**: [section 4.1] describes Mapper logic to minimize max(workloads of instances), [section 5.2] shows Round Robin leads to extreme workload imbalance, [corpus] consistent with Hybrid Learning emphasizing dynamic scheduling for DL workloads.
- **Break condition**: In scenarios with extremely low request rates, the overhead of calculating complex workload scores might outweigh the benefits of load balancing compared to simple random selection.

## Foundational Learning

- **Concept**: **KV Cache & Paged Attention**
  - **Why needed here**: The scheduler explicitly tracks kv_usage to prevent OOM and manage throughput. You must understand that KV cache grows with sequence length to interpret the workload formulas.
  - **Quick check question**: Does the KV cache size depend more on the batch size or the sequence length? (Answer: Both, specifically total token count).

- **Concept**: **Tensor Parallelism (t_i)**
  - **Why needed here**: The paper's configuration search primarily optimizes this variable. You need to know that higher t_i means faster instance compute but fewer total instances per machine.
  - **Quick check question**: If you have 8 GPUs and set tensor parallelism to 2, how many model instances can you host? (Answer: 4).

- **Concept**: **Continuous vs. Static Batching**
  - **Why needed here**: The paper's estimation model uses static batching logic for simplicity, while the real engine uses continuous batching. Understanding the gap between these two is critical for interpreting the results.
  - **Quick check question**: Why does the paper claim its estimation is valid despite using a static batching model? (Answer: The ranking of configurations remains consistent even if absolute numbers differ).

## Architecture Onboarding

- **Component map**: Instance Profiler -> Deployment Optimizer -> Scheduler (Output Length Predictor -> Workload Calculator -> Mapper)
- **Critical path**: 1) Profile a single GPU to fit p parameters, 2) Run Algorithm 1 to find optimal t_i, 3) Deploy instances and start scheduler loop (Alg 2), continuously updating instLoads and assigning requests
- **Design tradeoffs**: Accuracy vs. Cost (linear estimation vs. precise benchmarking), Complexity vs. Generality (output length predictor can be simple or complex)
- **Failure signatures**: Rank Inversion (linear model fails to rank configurations correctly), Memory Drift (kv_usage underestimated, causing OOM errors)
- **First 3 experiments**:
  1. Validate the Profiler: Run profiling script on a single GPU and compare fitted curve against real latency data
  2. Configuration Search Verification: Run Algorithm 1 and compare estimated best configuration against brute-force benchmarks
  3. Scheduler Stress Test: Deploy two instances with vastly different compute power and verify completion times are closer with "OS" policy than "RR"

## Open Questions the Paper Calls Out
None

## Limitations
- The profiling model's validity for ranking configurations across heterogeneous clusters is not empirically isolated from the full system performance
- The exponential penalty function for memory utilization lacks rigorous justification for its specific form
- Experiments focus on LLaMA models on two specific cluster configurations, limiting generalizability to other model families or topologies
- Baselines like Round Robin are acknowledged as poor fits for heterogeneous systems, making improvements appear larger

## Confidence

- **High Confidence**: The concept that heterogeneous hardware requires different request scheduling strategies is well-established; empirical observation that Round Robin leads to workload imbalance is convincing
- **Medium Confidence**: The profiling-based configuration search is a reasonable approach for reducing benchmarking costs; claim that it preserves ranking is plausible but requires dedicated validation
- **Low Confidence**: The specific design of the memory-aware scheduling algorithm (exponential weighting, specific workload formula) lacks sufficient empirical or theoretical justification

## Next Checks

1. **Profiler Ranking Validation**: Isolate profiling component by comparing top-3 configurations recommended by linear model against top-3 found by exhaustive benchmarking, calculating overlap and throughput difference

2. **Scheduler Component Isolation**: In heterogeneous cluster, run experiments with full "OS" scheduler vs. scheduler considering only computational power (ignoring kv_usage term), measuring difference in max workload and system throughput

3. **Generalizability Test**: Replicate configuration search and scheduler evaluation on different heterogeneous cluster (mixing high-end/low-end GPUs or different GPU manufacturer), verifying if same profiling parameters and logic yield comparable throughput improvements