---
ver: rpa2
title: 'From Annotation to Adaptation: Metrics, Synthetic Data, and Aspect Extraction
  for Aspect-Based Sentiment Analysis with Large Language Models'
arxiv_id: '2503.20715'
source_url: https://arxiv.org/abs/2503.20715
tags:
- negative
- dataset
- aspect
- aspects
- novel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates Large Language Models (LLMs) for Aspect-Based
  Sentiment Analysis (ABSA), focusing on implicit aspect extraction in a novel sports
  feedback domain. A synthetic dataset with 35% implicit aspects is introduced, and
  a new evaluation metric is proposed to account for linguistic variability in aspect
  extraction.
---

# From Annotation to Adaptation: Metrics, Synthetic Data, and Aspect Extraction for Aspect-Based Sentiment Analysis with Large Language Models

## Quick Facts
- arXiv ID: 2503.20715
- Source URL: https://arxiv.org/abs/2503.20715
- Reference count: 40
- Large Language Models perform worse than traditional methods on in-domain ABSA data but outperform them on a novel dataset with implicit aspects

## Executive Summary
This study evaluates Large Language Models (LLMs) for Aspect-Based Sentiment Analysis (ABSA), focusing on implicit aspect extraction in a novel sports feedback domain. A synthetic dataset with 35% implicit aspects is introduced, and a new evaluation metric is proposed to account for linguistic variability in aspect extraction. Experiments show that LLMs without fine-tuning perform worse than traditional methods on in-domain data but outperform them on the novel dataset. In-context learning with sampled examples significantly improves performance. Fine-tuning is effective only when the training and evaluation data are similar, while mixed-dataset fine-tuning yields consistent gains across both domains. The results highlight the potential and limitations of LLMs in ABSA tasks involving implicit aspects.

## Method Summary
The study introduces a novel sports feedback dataset (480 documents, 35% implicit aspects) and a generalized precision/recall metric using Sentence-T5 embeddings with θ=0.95 threshold. LLMs (Mistral 7B Instruct, LLaMA-3 8B Instruct) are evaluated with generic in-context learning, ICL with sampling (2 examples per polarity), and LoRA fine-tuning (r=128, alpha=32, 4-bit quantization). Performance is compared against the PyABSA ensemble baseline across three configurations: Composite dataset (SemEval-14-Laptop, Restaurant, MAMS, Twitter), Novel sports dataset, and Blended mixed-dataset.

## Key Results
- LLMs without fine-tuning perform worse than traditional methods on in-domain data but outperform them on the novel dataset with implicit aspects
- In-context learning with sampled examples significantly improves LLM performance on aspect-polarity pair extraction
- Fine-tuning effectiveness is conditional on training-evaluation data similarity; cross-domain fine-tuning degrades generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context learning with sampled examples significantly improves LLM performance on aspect-polarity pair extraction
- Mechanism: Sampling training examples uniformly by polarity class provides task-specific patterns that guide the model's extraction behavior without weight updates
- Core assumption: The training subset contains representative examples of the evaluation distribution
- Evidence anchors: ICL with sampling improved F1.95 from 0.44 to 0.65 on Composite and from 0.29 to 0.51 on Novel dataset for Mistral

### Mechanism 2
- Claim: Fine-tuning effectiveness is conditional on training-evaluation data similarity; cross-domain fine-tuning degrades generalization
- Mechanism: Gradient-based adaptation specializes model weights to the training distribution's aspect vocabulary and phrasing patterns
- Core assumption: The model's parameter space encodes domain-specific priors that transfer poorly when aspect distributions shift
- Evidence anchors: When fine-tuned on Composite dataset, performance on Novel samples declined below that of ICL with sampling

### Mechanism 3
- Claim: Generalized precision/recall with similarity threshold θ = 0.95 captures valid aspect matches while excluding false positives for implicit aspect evaluation
- Mechanism: Uses embedding cosine similarity + Hungarian algorithm to find optimal pairings between detected and gold aspects
- Core assumption: Sentence-T5 embeddings provide sufficiently discriminative similarity scores
- Evidence anchors: Manual examination at θ = 0.95 found no incorrect pairings except 2% compound aspect edge cases

## Foundational Learning

- **Aspect-Based Sentiment Analysis (ABSA) Subtasks**: The paper decomposes ABSA into extraction, classification, and category detection; understanding these distinctions is required to interpret F1 results and design prompts
  - Quick check: Given "The food was great but service was slow," can you identify two aspect-polarity pairs?

- **Implicit vs. Explicit Aspects**: The novel dataset contains 35% implicit aspects (not matching document text), which is the core challenge addressed by the proposed metric and experimental design
  - Quick check: In "Some locations had volunteers that could have been useful elsewhere," what is the implicit aspect and why is it hard to extract?

- **LoRA (Low-Rank Adaptation) Fine-Tuning**: All fine-tuning experiments use 4-bit quantization + LoRA (r=128, α=32); understanding parameter-efficient adaptation explains why 96 training samples may be insufficient
  - Quick check: Why might LoRA with 96 samples cause overfitting despite low parameter count?

## Architecture Onboarding

- **Component map**: GPT-4/Gemini generation → LLM annotation drafts → Volunteer selection → Expert revision → Sentence-T5 embeddings → Cosine similarity matrix → Threshold filtering (θ=0.95) → Linear sum assignment → P/R/F1 computation → Mistral 7B / LLaMA-3 8B with LoRA fine-tuning (4-bit quantization) or ICL prompting → PyABSA baseline

- **Critical path**: Dataset creation (3-step annotation workflow) → Prompt engineering for ASPE task → ICL example sampling (2 per polarity, uniform distribution) → Fine-tuning with early stopping at validation loss inflection → Evaluation with generalized metrics

- **Design tradeoffs**: ICL vs. Fine-tuning (ICL provides better cross-domain generalization; fine-tuning excels only when train/test distributions match); Exact match vs. θ-smoothed metrics (exact match fails on implicit aspects; θ-smoothed permits linguistic variation but introduces ~2% compound aspect errors); Dataset size vs. overfitting risk (96-sample fine-tuning improved Novel but degraded Composite)

- **Failure signatures**: F1.95 < 0.40 on Novel with Composite fine-tuning → Domain mismatch; Large P/R gap → Model either over-extracts or misses implicit aspects; ASC recall drops despite extraction improvement → Aspect extraction errors cascade

- **First 3 experiments**:
  1. Replicate ICL sampling ablation: Test Mistral with 1, 2, 4 examples per polarity on Novel dataset; plot F1.95 vs. example count
  2. Threshold sensitivity analysis: Evaluate θ ∈ {0.90, 0.925, 0.95, 0.975, 1.0} on Composite + Novel merged set; measure error rate and match count tradeoff
  3. Cross-domain transfer test: Fine-tune LLaMA-3 on Novel (96 samples) with dropout=0.2, weight_decay=0.01; compare Composite F1 degradation

## Open Questions the Paper Calls Out
None

## Limitations
- The generalized metric (θ=0.95) shows ~2% compound aspect errors in manual validation, but systematic evaluation across diverse aspect types and languages is absent
- Fine-tuning results are sensitive to training set size (96 samples) and regularization parameters, with limited ablation studies on batch size, dropout, or weight decay
- The ICL sampling mechanism assumes uniform polarity distribution in training data; performance may degrade with imbalanced sentiment classes

## Confidence

- **High confidence**: ICL with sampling improves cross-domain generalization; domain mismatch causes fine-tuning degradation; exact match metrics fail on implicit aspects
- **Medium confidence**: θ=0.95 threshold balances precision-recall tradeoff; mixed-dataset fine-tuning consistently improves performance across domains
- **Low confidence**: Specific hyperparameters for LoRA fine-tuning (learning rate, rank, alpha) are optimal; sampling strategy generalizes beyond the sports feedback domain

## Next Checks

1. **Threshold sensitivity validation**: Systematically evaluate θ ∈ {0.90, 0.925, 0.95, 0.975, 1.0} across all datasets to quantify error rate vs. match completeness tradeoff
2. **Sampling strategy ablation**: Compare ICL performance using different sampling strategies (uniform polarity, random, difficulty-based) on the Novel dataset with 1, 2, 4, 8 examples per polarity
3. **Fine-tuning robustness test**: Fine-tune LLaMA-3 on Novel with increased regularization (dropout=0.2, weight_decay=0.01) and monitor Composite performance degradation vs. the paper's reported 0.45 F1.95