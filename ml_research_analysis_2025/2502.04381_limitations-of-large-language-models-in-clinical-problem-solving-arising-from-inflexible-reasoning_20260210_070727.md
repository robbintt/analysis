---
ver: rpa2
title: Limitations of Large Language Models in Clinical Problem-Solving Arising from
  Inflexible Reasoning
arxiv_id: '2502.04381'
source_url: https://arxiv.org/abs/2502.04381
tags:
- reasoning
- medical
- arxiv
- preprint
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces M-ARC, a medical abstraction and reasoning
  corpus designed to probe failure modes in LLM clinical problem-solving arising from
  inflexible reasoning patterns. The benchmark exploits the Einstellung effect by
  incorporating long-tail reasoning patterns underrepresented in medical texts to
  disrupt predictable problem-solving strategies.
---

# Limitations of Large Language Models in Clinical Problem-Solving Arising from Inflexible Reasoning

## Quick Facts
- arXiv ID: 2502.04381
- Source URL: https://arxiv.org/abs/2502.04381
- Authors: Jonathan Kim; Anna Podlasek; Kie Shidara; Feng Liu; Ahmed Alaa; Danilo Bernardo
- Reference count: 40
- Primary result: LLMs tested on M-ARC clinical benchmark achieved <50% accuracy versus 66% for physicians, revealing failures in flexible reasoning.

## Executive Summary
This paper introduces M-ARC, a medical benchmark designed to expose LLM failures in clinical reasoning through adversarial question design that targets the Einstellung effect. When evaluated against human physicians, state-of-the-art LLMs including o1 and Gemini models demonstrated poor performance with accuracies below 50% compared to human performance of 66%. The models exhibited overconfidence in their responses despite limited accuracy, with examples revealing fundamental reasoning errors and hallucinations. The findings suggest that LLM limitations in clinical reasoning stem from training biases favoring pattern matching over flexible reasoning, highlighting the need for caution when deploying these models in clinical settings.

## Method Summary
The authors developed M-ARC, a 100-question medical benchmark incorporating long-tail reasoning patterns underrepresented in medical texts to disrupt predictable problem-solving strategies. Questions follow USMLE-style format with adversarial answer choices targeting pattern-matching behaviors. Models were evaluated using temperature=0 for reproducibility, with uncertainty estimation performed through 15 runs varying patient age ±10 days to calculate sample consistency and Brier scores. Human performance was benchmarked using a panel of 5 physicians, and questions were validated through majority vote of three physicians for reasonableness.

## Key Results
- Current state-of-the-art LLMs including o1 and Gemini models achieved accuracies below 50% on M-ARC, compared to 66% for human physicians
- Models exhibited overconfidence with high sample consistency despite low accuracy, as measured by Brier scores
- Examples revealed fundamental reasoning errors including hallucinations (e.g., claiming blood pressure can be measured on forehead) and failure to recognize logical constraints (e.g., no brain → no expected EEG activity)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs exhibit rigid adherence to high-probability token completions from training data, which can be exploited through adversarial question design targeting this inductive bias.
- **Mechanism:** The autoregressive next-token prediction objective favors statistical patterns prevalent in training corpora. When familiar medical text patterns appear in questions, models complete toward high-likelihood continuations even when context negates them. M-ARC exploits this by embedding logical contradictions that require deductive override of pattern completion.
- **Core assumption:** LLM reasoning deficits stem partially from memorization of frequently-seen task patterns, leading to generalization loss for novel tasks.
- **Evidence anchors:**
  - [abstract]: "targeting LLM inductive biases toward inflexible pattern matching from their training data rather than engaging in flexible reasoning"
  - [Discussion]: "McCoy et al. hypothesized that poorer performance in these low-probability situations stem from biases inherent in the LLM training paradigm, which favors probabilistic strategies in autoregressive next-token prediction"
- **Break condition:** If models develop genuine deductive reasoning capabilities that override statistical priors.

### Mechanism 2
- **Claim:** Long-tail (low-probability) clinical reasoning patterns are underrepresented in medical training texts, causing disproportionate LLM failures on out-of-distribution scenarios.
- **Mechanism:** Medical texts and QA benchmarks concentrate on common presentations. LLMs, trained on this distribution, develop weak representations for rare scenarios. M-ARC deliberately constructs questions with unusual constraints that lie outside the training distribution, forcing models to reason from first principles rather than retrieve memorized patterns.
- **Core assumption:** Medical texts have systematic blind spots for edge cases that clinicians handle via flexible reasoning.
- **Evidence anchors:**
  - [Methods 2.1]: "M-ARC alters predictable aspects of medical problems, emphasizing 'long-tail' or low-probability reasoning patterns underrepresented in medical texts"
  - [Discussion]: "M-ARC targets this inductive bias by disrupting the predictability of familiar medical problems through incorporation of long-tail concepts which are difficult for LLMs to capture effectively"
- **Break condition:** If training data is augmented with diverse adversarial edge cases.

### Mechanism 3
- **Claim:** LLMs exhibit miscalibrated confidence—high self-consistency across repeated samples despite low accuracy—complicating safe deployment.
- **Mechanism:** Sample consistency measures agreement across multiple stochastic runs. High agreement with wrong answers indicates confident but incorrect reasoning paths. Brier scores quantify this calibration gap; lower scores indicate better alignment between confidence and accuracy.
- **Core assumption:** Sample consistency is a meaningful proxy for model uncertainty in medical domains.
- **Evidence anchors:**
  - [abstract]: "uncertainty estimation analyses indicate that LLMs exhibit overconfidence in their answers, despite their limited accuracy"
  - [Results 2.5]: "Agreement-based Brier scores and reliability plots for the top-performing models demonstrated overconfidence in their responses despite exhibiting low accuracy"
- **Break condition:** If uncertainty estimation methods successfully identify low-confidence outputs for human review.

## Foundational Learning

- **Concept:** Einstellung Effect (cognitive bias)
  - **Why needed here:** The paper deliberately imports a concept from cognitive psychology to characterize LLM failure modes. Without understanding that humans also exhibit rigid problem-solving when prior experience creates mental "sets," the adversarial design rationale is unclear.
  - **Quick check question:** If a chess master always opens with the same moves and loses to a novel opening, is this Einstellung? (Yes—familiar patterns blocking flexible adaptation.)

- **Concept:** Long-tail Distribution
  - **Why needed here:** M-ARC's core strategy is targeting underrepresented reasoning patterns. Understanding that real-world data concentrates around common cases (power-law distributions) explains why benchmarks may overestimate real-world performance.
  - **Quick check question:** Why might a model trained on common cold cases fail on a rare autoimmune presentation? (Insufficient training examples for long-tail scenarios.)

- **Concept:** Uncertainty Calibration (Brier Score)
  - **Why needed here:** The paper evaluates not just accuracy but whether model confidence matches reality. A model that is wrong with high confidence is more dangerous than one that signals uncertainty.
  - **Quick check question:** Model A achieves 70% accuracy with well-calibrated confidence. Model B achieves 75% but is always 99% confident. Which is safer for clinical use? (Model A—calibration enables appropriate human oversight.)

## Architecture Onboarding

- **Component map:** M-ARC Benchmark -> Evaluation Pipeline -> Metrics
- **Critical path:**
  1. Design adversarial questions with long-tail patterns and adversarial distractor options
  2. Validate questions via 3-physician majority vote for reasonableness
  3. Run each model with temperature=0 for deterministic outputs
  4. For uncertainty estimation: vary patient age ±10 days across 15 runs, compute inter-response agreement
  5. Calculate Brier scores from agreement-based consistency
- **Design tradeoffs:**
  - Small dataset (N=100) enables careful adversarial crafting but limits statistical power
  - Temperature=0 ensures reproducibility but may not reflect real-world usage patterns
  - Age perturbation for stochasticity is clinically neutral for adult patients but may not generalize to pediatric contexts
  - Physician baseline (N=5) provides human comparison but has high variance (±5.3% SE)
- **Failure signatures:**
  - Hallucinated medical facts (e.g., "blood pressure can be measured on forehead")
  - Tangential reasoning that ignores logical constraints (e.g., recommending brain scan for patient without brain)
  - High-confidence wrong answers (low Brier score despite low accuracy)
  - Selecting adversarial distractors that match common text patterns
- **First 3 experiments:**
  1. **Baseline replication:** Run GPT-4o and one open-source model (e.g., Meditron-7b) on M-ARC, compute accuracy and Brier scores to verify paper findings.
  2. **Ablation on adversarial strength:** Create modified questions with adversarial distractors removed; measure accuracy delta to isolate the Einstellung effect contribution.
  3. **Selective prediction pilot:** Implement a simple deferral rule (e.g., abstain if sample consistency < threshold) and measure what fraction of errors would be caught vs. unnecessary deferrals.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can selective prediction strategies that defer to human clinicians in long-tail or out-of-distribution scenarios effectively mitigate risks from LLM inflexibility in clinical settings?
- **Basis in paper:** [explicit] The authors state: "To mitigate these risks, the development of selective prediction strategies, as proposed by Goetz et al., may offer a pathway to AI deployment in clinical scenarios. In this strategy, LLMs could defer to human clinicians in long-tail or out-of-distribution scenarios."
- **Why unresolved:** The authors propose this as a potential mitigation but do not implement or test such a strategy.
- **What evidence would resolve it:** Empirical evaluation showing that LLMs with selective prediction reduce adverse outcomes compared to non-selective deployment in simulated clinical workflows.

### Open Question 2
- **Question:** Would expanding M-ARC beyond 100 questions improve its robustness and predictive validity for real-world clinical reasoning performance?
- **Basis in paper:** [explicit] The authors acknowledge: "M-ARC consists of a smaller set of 100 questions... Future work will aim to increase the size of the M-ARC dataset to improve its robustness."
- **Why unresolved:** The current dataset size limits statistical power and generalizability; the relationship between M-ARC performance and actual clinical competence remains unestablished.
- **What evidence would resolve it:** Correlation analyses between M-ARC scores and physician performance on standardized clinical assessments across larger sample sizes.

### Open Question 3
- **Question:** Can LLM training or prompting interventions reduce susceptibility to the Einstellung effect without merely causing overfitting to adversarial benchmarks?
- **Basis in paper:** [inferred] The paper demonstrates LLM vulnerability to inflexible pattern matching but does not test remediation strategies.
- **Why unresolved:** It remains unclear whether architectural changes, training modifications, or prompting techniques can improve reasoning flexibility.
- **What evidence would resolve it:** Pre-post intervention studies showing improved M-ARC performance that generalizes to new adversarial scenarios and other clinical reasoning benchmarks.

## Limitations
- Dataset size and representativeness: The M-ARC benchmark contains only 100 questions, limiting statistical power for comparing model performances, with physician baseline (N=5) having high variance (±5.3% SE)
- Generalizability of adversarial design: The Einstellung effect exploitation relies on deliberately introducing logical contradictions that may overestimate LLM failures in typical clinical practice
- Temperature setting and real-world relevance: Using temperature=0 ensures reproducibility but may not reflect how models behave with stochastic sampling in deployment

## Confidence

**High confidence:** The observation that LLMs exhibit overconfidence with poor calibration (Brier scores indicating high agreement despite low accuracy) is well-supported by the experimental design and aligns with prior work on uncertainty estimation in AI systems.

**Medium confidence:** The specific performance gaps (66% human vs. sub-50% LLM accuracy) are concerning but limited by the small sample size and potential ceiling effects in the physician baseline.

**Low confidence:** The assertion that M-ARC's adversarial design specifically exploits the Einstellung effect in a clinically meaningful way is speculative.

## Next Checks
1. **Scale validation:** Expand M-ARC to 500+ questions across diverse medical specialties and re-evaluate model performance to determine if the sub-50% accuracy pattern holds with increased statistical power.
2. **Real-world deployment study:** Deploy top-performing models (o1, GPT-4o) in a controlled clinical setting with actual patient cases that lack obvious logical contradictions to assess whether adversarial benchmark findings predict real-world performance.
3. **Training data augmentation experiment:** Fine-tune models on augmented datasets containing diverse long-tail clinical scenarios and adversarial examples, then re-evaluate on M-ARC to measure the impact of distribution shift mitigation on reasoning flexibility.