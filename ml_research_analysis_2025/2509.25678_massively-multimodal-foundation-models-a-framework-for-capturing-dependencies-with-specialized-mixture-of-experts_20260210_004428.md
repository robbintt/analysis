---
ver: rpa2
title: 'Massively Multimodal Foundation Models: A Framework for Capturing Dependencies
  with Specialized Mixture-of-Experts'
arxiv_id: '2509.25678'
source_url: https://arxiv.org/abs/2509.25678
tags:
- multimodal
- temporal
- information
- routing
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of scaling multimodal learning
  to settings with dozens of heterogeneous input streams, where each stream constitutes
  a separate modality with distinct sampling rates, noise characteristics, and temporal
  dynamics. Existing Mixture-of-Experts (MoE) architectures route tokens based on
  similarity alone, failing to account for rich temporal dependencies across modalities.
---

# Massively Multimodal Foundation Models: A Framework for Capturing Dependencies with Specialized Mixture-of-Experts

## Quick Facts
- arXiv ID: 2509.25678
- Source URL: https://arxiv.org/abs/2509.25678
- Reference count: 40
- Primary result: AUROC of 85.40% and F1 of 84.97% on MIMIC-IV

## Executive Summary
This paper addresses the challenge of scaling multimodal learning to settings with dozens of heterogeneous input streams, where each stream constitutes a separate modality with distinct sampling rates, noise characteristics, and temporal dynamics. Existing Mixture-of-Experts (MoE) architectures route tokens based on similarity alone, failing to account for rich temporal dependencies across modalities. The proposed MERGE framework explicitly quantifies temporal dependencies between modality pairs across multiple time lags using a multi-scale BATCH estimator that computes redundancy, uniqueness, and synergy (RUS) values. A dependency-aware router then dispatches tokens to specialized experts based on interaction type, enabling experts to learn generalizable dependency-processing skills. Experiments across healthcare, activity recognition, and affective computing benchmarks demonstrate substantial performance gains while providing interpretable routing patterns aligned with domain knowledge.

## Method Summary
MERGE uses a two-phase approach: first, pre-compute temporal RUS sequences via a multi-scale BATCH estimator that decomposes directed information into redundancy (shared information), uniqueness (modality-specific), and synergy (emergent from combination) across multiple time lags. These cached RUS values are then used by a RUS-aware MoE router that attends over pairwise R/S values while a GRU encodes uniqueness dynamics. The router dispatches tokens to specialized experts: high-redundancy pairs to shared experts (early fusion), high-uniqueness modalities to diverse experts (late fusion), and high-synergy pairs to cross-attention-based synergy experts (hybrid fusion). Training uses task loss plus auxiliary RUS-guided losses (JSD-based redundancy/uniqueness terms, synergy expert promotion).

## Key Results
- Achieves AUROC of 85.40% and F1 of 84.97% on MIMIC-IV
- Substantial performance gains across all six benchmarks (healthcare, activity recognition, affective computing)
- Provides interpretable routing patterns that align with domain knowledge (e.g., CXR+notes synergy, chest+hand redundancy)
- Scales efficiently with sub-linear routing complexity while maintaining performance even with noisy RUS estimates

## Why This Works (Mechanism)

### Mechanism 1: Information-Theoretic Routing Prior
Pre-computed temporal RUS sequences provide principled routing signals that improve expert specialization beyond similarity-based routing. The multi-scale BATCH estimator computes directed information at multiple time lags, decomposing it into redundancy (shared information), uniqueness (modality-specific), and synergy (emergent from combination). These cached values condition router decisions without requiring end-to-end gradient flow through RUS estimation. The core assumption is that temporal dependencies between modality pairs are stable dataset properties that transfer across tasks.

### Mechanism 2: Interaction-Type Specific Expert Assignment
Different interaction types (R/U/S) benefit from qualitatively different fusion strategies, which specialized routing enforces. High-redundancy pairs route to shared experts (early fusion); high-uniqueness modalities diversify across experts (late fusion); high-synergy pairs route to cross-attention-based synergy experts (hybrid fusion). Auxiliary losses enforce these assignments via JSD penalties and synergy-expert promotion. The mapping from interaction type to optimal fusion strategy is assumed to generalize across domains.

### Mechanism 3: Multi-Scale Temporal Context Encoding
Longer RUS sequences improve performance by capturing richer dependency trajectories. The RUS-aware router attends over pairwise R/S values while a GRU encodes uniqueness dynamics. Increasing max time lag or repeating RUS segments strengthens context modeling and reduces gradient variance. The core assumption is that temporal dependency patterns have regularity that persists across time and benefits from explicit sequence modeling.

## Foundational Learning

- **Concept: Partial Information Decomposition (PID)**
  - Why needed here: Understanding how R/U/S decompose mutual information is essential for interpreting routing decisions and debugging RUS estimates.
  - Quick check question: Given two modalities predicting a target, can you explain why redundancy requires optimizing over marginal-matching distributions Q rather than using the empirical joint directly?

- **Concept: Directed Information**
  - Why needed here: Extends PID to temporal settings by respecting causal ordering (past→present), enabling lag-specific RUS.
  - Quick check question: How does DI(τ) differ from standard mutual information I(X₁, X₂; Y), and why does this matter for capturing delayed physiological responses?

- **Concept: Sinkhorn-Knopp Algorithm**
  - Why needed here: Enforces marginal-matching constraints when computing optimal Q* for redundancy estimation.
  - Quick check question: What constraints does Sinkhorn enforce on the alignment tensor, and what happens if convergence is premature?

## Architecture Onboarding

- **Component map**: RUS Estimator (offline) -> RUS-aware router -> Modality Encoders -> 8 experts (6 feedforward + 2 cross-attention synergy experts) -> Task loss + auxiliary losses

- **Critical path**: 1) Pre-compute temporal RUS on training split (cache results) 2) Verify RUS patterns are non-degenerate (check class balance, inspect R/U/S trajectories) 3) Train MERGE with frozen RUS + auxiliary losses 4) Analyze routing distributions for interpretability

- **Design tradeoffs**: Decoupled RUS vs end-to-end (prevents task-specific bias but cannot adapt RUS to downstream needs), number of experts (paper uses 8; scaling requires rebalancing synergy expert ratio), max time lag τ (longer captures more dependencies but increases estimator variance)

- **Failure signatures**: Routing collapses to uniform distribution (check λ values in auxiliary losses), RUS sequences are near-constant (check class imbalance in RUS estimation batch), synergy experts underutilized (τ_S threshold may be too high), performance degrades with longer RUS sequences (estimator capacity insufficient)

- **First 3 experiments**: 1) RUS sanity check: compare multi-scale BATCH estimator output vs step-wise computation (should match within ~10%) 2) Ablation by interaction type: remove each auxiliary loss individually and measure performance drop 3) Routing visualization: for known high-synergy pair (e.g., insulin+furosemide), verify tokens route to synergy experts with high probability

## Open Questions the Paper Calls Out

- **Can end-to-end joint learning of temporal RUS estimates and MoE routing improve performance over the decoupled approach, or does task-specific optimization corrupt the information-theoretic properties of RUS values?** The authors intentionally decouple these components to avoid entangling RUS values with downstream objectives, but the trade-off remains unexplored.

- **How does MERGE scale to settings with hundreds of modalities where O(M²) pairwise RUS computation becomes prohibitive?** The paper demonstrates results on 4–7 modalities but doesn't evaluate scaling behavior as modality count approaches "dozens to hundreds."

- **Can cross-lagged temporal dependencies (where X₁ and X₂ have different time lags τ₁ ≠ τ₂) provide additional predictive signal beyond the synchronized lag setting currently implemented?** The framework naturally extends to cross-lagged interactions, but synchronized lags are used for efficiency.

## Limitations

- **RUS Estimator Stability**: Performance is highly sensitive to class balance during RUS computation; severe imbalance causes degenerate joint distributions that produce uninformative RUS sequences.
- **Decoupling RUS from End-to-End Optimization**: Pre-computing RUS prevents the model from adapting routing signals to downstream objectives, potentially missing task-specific optimizations.
- **Synergy Expert Design**: The rationale for using exactly two cross-attention synergy experts lacks theoretical justification and appears somewhat ad hoc.

## Confidence

- **High Confidence**: Experimental results with reported AUROC (85.40%) and F1 (84.97%) scores appear methodologically sound, with strong ablation studies and routing visualizations.
- **Medium Confidence**: The information-theoretic foundation using R/U/S decomposition is mathematically rigorous, but the assumption that cached values provide optimal routing without joint optimization is untested.
- **Low Confidence**: The synergy expert mechanism lacks theoretical justification for why cross-attention specifically captures synergistic interactions better than other fusion strategies.

## Next Checks

1. **RUS Sensitivity Analysis**: Systematically vary class balance in the RUS estimation phase and measure degradation in downstream performance to quantify the minimum class balance required for reliable RUS estimates.

2. **End-to-End RUS Optimization**: Implement a variant where RUS values are learned jointly with the task objective and compare performance to test whether the trade-off between unbiased routing signals and task-adapted optimization is worthwhile.

3. **Synergy Expert Generalization**: Replace cross-attention synergy experts with alternative architectures (e.g., concatenation + feedforward, or self-attention fusion) to test whether the specific design is critical or merely one of many viable fusion strategies.