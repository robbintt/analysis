---
ver: rpa2
title: Learning from Natural Language Feedback for Personalized Question Answering
arxiv_id: '2508.10695'
source_url: https://arxiv.org/abs/2508.10695
tags:
- feedback
- user
- personalized
- response
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VAC introduces a novel framework for personalized question answering
  that replaces scalar reward signals with natural language feedback (NLF) generated
  from user profiles and question narratives. The framework alternates between optimizing
  a feedback model to produce actionable guidance and fine-tuning a policy model to
  generate improved personalized responses without requiring feedback at inference
  time.
---

# Learning from Natural Language Feedback for Personalized Question Answering

## Quick Facts
- arXiv ID: 2508.10695
- Source URL: https://arxiv.org/abs/2508.10695
- Reference count: 40
- Primary result: VAC achieves 13.6% relative improvement over non-personalized baselines for personalized question answering

## Executive Summary
VAC introduces a novel framework for personalized question answering that replaces scalar reward signals with natural language feedback (NLF) generated from user profiles and question narratives. The framework alternates between optimizing a feedback model to produce actionable guidance and fine-tuning a policy model to generate improved personalized responses without requiring feedback at inference time. Experiments on the LaMP-QA benchmark show VAC achieves state-of-the-art performance while being 1.9× more efficient than competing personalized methods.

## Method Summary
VAC trains a feedback model and policy model iteratively, where the feedback model generates textual critiques (e.g., "include specific advice for stovetop cooking") instead of numerical scores. The policy model learns to produce revised responses directly from the input without seeing the feedback at inference time. The training alternates between: (1) training the feedback model to predict the feedback that maximizes an evaluation metric for the current policy outputs, and (2) training the policy model via supervised fine-tuning to generate the refined response directly from query and profile. This creates a co-adaptation loop where feedback becomes increasingly tailored to the policy's error distribution.

## Key Results
- 13.6% relative improvement over non-personalized baselines
- 3.6% improvement over best personalized baseline while being 1.9× more efficient
- 6.0% improvement over reinforcement learning with scalar rewards
- Human evaluation found VAC was preferred in 44% of comparisons against state-of-the-art method

## Why This Works (Mechanism)

### Mechanism 1: Actionable Guidance over Scalar Rewards
Replacing scalar rewards with natural language feedback provides higher-information supervision, enabling the policy model to learn "how" to improve rather than just "what" is good. The framework uses a Feedback Model to generate text critiques instead of a numerical score, and the Policy Model is fine-tuned via SFT to produce the revised response directly from the input, distilling the correction step into the model weights.

### Mechanism 2: Co-adaptation via Feedback Selection
Iteratively optimizing the Feedback Model to match the Policy Model's current error distribution creates a tailored learning curriculum. In each iteration, the Feedback Model is trained to predict the feedback that maximizes the task metric for the current Policy Model's outputs, forcing it to specialize in critiquing the specific types of errors the Policy Model is making at that moment.

### Mechanism 3: Inference-Time Unloading
The framework achieves efficient personalization by decoupling the "thinking" (feedback generation) from the "doing" (response generation), allowing the inference-time model to be fast and standalone. While training involves a dual-model loop, the Policy Model is optimized via SFT to generate the revised response directly from the input without seeing the feedback, internalizing the reasoning pattern.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed: The system relies on a Retriever to fetch documents from the user profile to ground the question answering
  - Quick check: Can you implement a retriever that fetches the top K documents from a user profile and formats them into the prompt?

- **Concept: Supervised Fine-Tuning (SFT)**
  - Why needed: Despite the RL-sounding name, the core optimization for both models is SFT (Cross-Entropy Loss), not PPO or standard RL gradients
  - Quick check: Can you set up a standard next-token prediction training loop where the target is the "updated response"?

- **Concept: LLM-based Evaluation (LLM-as-a-Judge)**
  - Why needed: The critical step depends on an LLM scoring responses based on "personalized aspects"
  - Quick check: Can you write a prompt that takes a question, response, and rubrics and outputs a normalized score?

## Architecture Onboarding

- **Component map:** User Query + User Profile + Question Narrative -> Contriever Retriever -> Top K docs -> Qwen 2.5 7B Feedback Model -> Text Feedback -> Qwen 2.5 7B Policy Model -> Initial Response -> Evaluator (Qwen 2.5 32B)

- **Critical path:** Generate N feedback candidates -> Selection Step (bottleneck): generate N revised responses and score them with Evaluator to find best feedback -> SFT update for Feedback Model -> SFT update for Policy Model using trained Feedback Model

- **Design tradeoffs:** SFT vs. DPO for Policy (SFT outperforms DPO), Feedback Model Size (larger models lead to better policy performance), Speed vs. Quality (1.9x faster inference than PlanPers)

- **Failure signatures:** Metric Saturation (performance plateaus after iteration 2), Random User Baseline Failure (worse than No-Personalization), Feedback Drift (performance drops if Feedback Model is frozen)

- **First 3 experiments:** Metric Correlation Check (inspect evaluator scores vs human judgment), Feedback Ablation (replace selected feedback with generic templates), Inference Speed Benchmark (measure latency vs PlanPers)

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on 32B-parameter LLM for feedback selection creates computational overhead and potential evaluation noise
- Assumes initial response is reasonable enough to benefit from textual feedback
- Iterative process shows diminishing returns with no established optimal stopping criteria

## Confidence

- **High Confidence**: VAC improves over non-personalized baselines (13.6% relative gain) and achieves better efficiency than PlanPers (1.9× speedup)
- **Medium Confidence**: Natural language feedback provides more actionable signals than scalar rewards (theoretically sound but specific ablation missing)
- **Low Confidence**: Co-adaptation mechanism evidence is weak from related work (most cited papers focus on single-step optimization)

## Next Checks
1. **Metric Reliability Test**: Manually inspect 50 samples where the 32B evaluator selected feedback and compare its scoring decisions against human judgments
2. **Feedback Content Ablation**: Run the full VAC pipeline but replace selected feedback with generic templates to isolate whether NLF content drives improvements
3. **Catastrophic Forgetting Evaluation**: After completing T=3 iterations, evaluate the final policy model on a held-out general QA benchmark to measure degradation