---
ver: rpa2
title: 'Automatic Detection of LLM-Generated Code: A Comparative Case Study of Contemporary
  Models Across Function and Class Granularities'
arxiv_id: '2409.01382'
source_url: https://arxiv.org/abs/2409.01382
tags:
- code
- detection
- lines
- claude
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines the detection of code generated by four LLM
  models (GPT-3.5, Claude 3 Haiku, Claude Haiku 4.5, and GPT-OSS) at function and
  class levels using structural software metrics. The research reveals that while
  LLM-generated code differs significantly from human-written code, detectability
  varies substantially across models and granularities.
---

# Automatic Detection of LLM-Generated Code: A Comparative Case Study of Contemporary Models Across Function and Class Granularities

## Quick Facts
- arXiv ID: 2409.01382
- Source URL: https://arxiv.org/abs/2409.01382
- Reference count: 40
- Primary result: LLM-generated code detection varies dramatically across models (GPT-3.5: AUC-0.96 vs. Claude 4.5: AUC-0.68) and granularities, with granularity effects dominating model differences by 8.6x.

## Executive Summary
This study examines the detection of code generated by four LLM models (GPT-3.5, Claude 3 Haiku, Claude Haiku 4.5, and GPT-OSS) at function and class levels using structural software metrics. The research reveals that while LLM-generated code differs significantly from human-written code, detectability varies substantially across models and granularities. GPT-3.5 shows exceptional detectability (AUC-ROC 0.96 for functions, 0.89 for classes) compared to other models (AUC-ROC 0.68-0.80), challenging assumptions about detector generalization. The study finds that granularity effects dominate model differences by a factor of 8.6, with function-level and class-level detection relying on fundamentally different structural signatures. SHAP analysis identifies Comment-to-Code Ratio as the sole universal discriminator, though its predictive magnitude varies dramatically across models. The research demonstrates that robust detection requires model-specific approaches and highlights the limitations of existing detectors trained on single models.

## Method Summary
The study uses CodeSearchNet Python subset (14,485 functions, 11,913 classes) and generates corresponding code from four LLM models using docstring prompts, retaining only snippets successfully generated by all models. Structural metrics (18 function-level, 39 class-level) are extracted via SciTools Understand and reduced using AutoSpearman feature selection. CatBoost binary classifiers are trained on 8 configurations (4 models × 2 granularities) with 30 × 10-fold cross-validation. Performance is evaluated using AUC-ROC with bootstrap CIs, and SHAP analysis provides feature interpretability. The approach compares structural metrics against commercial detectors like GPTZero.

## Key Results
- Granularity effects dominate model differences by a factor of 8.6, with negligible feature overlap between function-level and class-level detection
- GPT-3.5 achieves exceptional detectability (AUC-ROC 0.96) compared to other models (0.68-0.80), suggesting GPT-3.5 benchmarks may not generalize
- SHAP analysis identifies Comment-to-Code Ratio as the sole universal discriminator, though its predictive magnitude varies dramatically across models
- Detection models show catastrophic temporal degradation, dropping 26.3 percentage points on repositories created after December 2024

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Comment-to-Code Ratio serves as a universal discriminator across all model-granularity configurations, but its predictive magnitude varies dramatically.
- Mechanism: LLMs exhibit systematic differences in documentation density compared to human programmers. The ratio captures a stylometric property that persists across architectures, though newer models (Claude Haiku 4.5) suppress this signal more effectively than older ones (GPT-3.5, where SHAP importance reaches 3.795).
- Core assumption: Assumes that documentation behavior reflects a fundamental generation characteristic rather than prompt-specific artifacts.
- Evidence anchors:
  - [abstract] "SHAP analysis identifies the Comment-to-Code Ratio as the sole universal discriminator. However, its predictive magnitude varies drastically across models."
  - [section] Table 8 shows Comment-to-Code Ratio appearing in top-3 features for 7/8 configurations, with SHAP values ranging from 0.18 to 3.80.
  - [corpus] Limited external validation; corpus papers focus on security/quality rather than detection mechanisms.
- Break condition: If future LLMs are fine-tuned specifically to match human commenting ratios, this signal could vanish or reverse.

### Mechanism 2
- Claim: Code granularity affects detection signatures 8.6x more strongly than model architecture differences.
- Mechanism: Function-level and class-level detection rely on fundamentally disjoint feature spaces (Jaccard similarity = 0.099). Functions expose algorithmic characteristics (statement counts, nesting depth), while classes expose architectural patterns (coupling, method organization). This creates granularity-dependent inversions where models detectable at one level become harder at another.
- Core assumption: Assumes that standalone artifacts (functions/classes without dependencies) represent detection behavior in coupled contexts.
- Evidence anchors:
  - [abstract] "Granularity effects dominate model differences by a factor of 8.6, with negligible feature overlap between function-level and class-level detection."
  - [section] Table 6 shows cross-granularity Jaccard = 0.099 vs. cross-model (class-level) Jaccard = 0.855.
  - [corpus] No corpus papers examine granularity effects in detection.
- Break condition: If LLMs improve at maintaining consistent structural patterns across abstraction levels, this separation would diminish.

### Mechanism 3
- Claim: GPT-3.5's high detectability (AUC-ROC 0.96) is an artifact of extreme brevity, not representative of contemporary models (0.68-0.80).
- Mechanism: GPT-3.5 generates systematically shorter code with dramatically fewer comments (87% reduction in comment lines) compared to humans. Newer models (Claude Haiku 4.5, GPT-OSS) produce verbose, well-documented code that more closely mimics human patterns, reducing their detectability at the function level.
- Core assumption: Assumes training on GPT-3.5 data creates overfitting to an atypically loud signal that doesn't transfer.
- Evidence anchors:
  - [abstract] "GPT-3.5's exceptional detectability (AUC-ROC 0.96) is unrepresentative of contemporary models (AUC-ROC approximately between 0.68 and 0.80)."
  - [section] RQ1 analysis shows GPT-3.5 has 94% non-negligible feature differences at function level vs. Claude Haiku 4.5's 17%.
  - [corpus] Corpus papers on LLM-generated code quality don't address detection benchmarking issues.
- Break condition: If older models are deprecated and training data shifts entirely to newer architectures, GPT-3.5 benchmarks become obsolete.

## Foundational Learning

- Concept: **AUC-ROC vs. Recall Trade-off**
  - Why needed here: The paper shows GPTZero achieves moderate precision (71-86%) but catastrophically low recall (15-64%), meaning it misses most LLM-generated code. Understanding this distinction is critical for evaluating detection utility in security contexts.
  - Quick check question: If a detector has 90% precision but 20% recall, what proportion of LLM-generated code goes undetected?

- Concept: **Feature Collinearity and AutoSpearman Selection**
  - Why needed here: Software metrics are highly correlated (e.g., Lines correlates with Code Lines). The paper uses AutoSpearman to reduce 18-39 metrics to 6-12 uncorrelated features, enabling interpretable detection without redundancy.
  - Quick check question: Why would keeping highly correlated features inflate model interpretation difficulty?

- Concept: **Jaccard Similarity for Feature Overlap**
  - Why needed here: Quantifies how different configurations share discriminative features. A value of 0.099 (cross-granularity) vs. 0.855 (cross-model) provides statistical evidence that detection strategies must be granularity-specific.
  - Quick check question: If two feature sets have Jaccard similarity of 0.5, what does that mean about their intersection size relative to their union?

## Architecture Onboarding

- Component map: CodeSearchNet functions/classes -> intersection filtering -> 4 LLM generation versions per artifact -> SciTools Understand metric extraction -> AutoSpearman reduction -> CatBoost binary classifiers (8 configurations) -> SHAP interpretation -> GPTZero baseline comparison

- Critical path:
  1. Intersection filtering (all 4 LLMs must successfully generate for same artifact) -> ensures fair cross-model comparison
  2. AutoSpearman feature selection (Spearman ρ ≥ 0.7 removal -> VIF ≥ 5 removal) -> prevents multicollinearity
  3. 30 × 10-fold cross-validation with bootstrap CI -> robust performance estimates
  4. SHAP analysis on test instances -> interpretable feature importance

- Design tradeoffs:
  - Standalone artifacts only: Removes dependency complexity but limits ecological validity for coupled codebases
  - Structural metrics over text-based: Provides interpretability but may sacrifice accuracy vs. CodeBERT-style embeddings
  - Per-configuration training: Maximizes within-distribution performance but eliminates cross-model generalization

- Failure signatures:
  - Temporal degradation: 26.3pp average AUC drop on 2025 repositories (catastrophic for Claude 3 Haiku class: 0.830 -> 0.432)
  - Cross-model transfer failure: GPT-3.5-trained detectors fail on Claude due to magnitude differences in Comment-to-Code Ratio signal
  - Granularity mismatch: Function-level detector applied to class-level code will fail (Jaccard = 0.099 overlap)

- First 3 experiments:
  1. **Baseline validation**: Train CatBoost on 50% of data, evaluate on held-out test set. Confirm AUC-ROC matches reported values (GPT-3.5 function: ~0.96, others: 0.68-0.80).
  2. **Granularity ablation**: Take function-level model, apply to class-level data. Expect dramatic performance drop due to feature space mismatch.
  3. **Temporal robustness test**: Evaluate trained models on repositories created after December 2024. Quantify degradation and identify which models remain most stable.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can continuous learning or domain adaptation techniques mitigate the catastrophic temporal degradation observed in LLM-code detectors, where performance dropped 26.3 percentage points on repositories created just months after training?
- Basis in paper: [explicit] The authors evaluated trained models on a "future" dataset (repos created after December 31, 2024) and observed "catastrophic degradation: AUC-ROC dropped by an average of 26.3 percentage points." They conclude that "Practical detection systems must move beyond standard train-test splits and adopt continuous learning approaches."
- Why unresolved: The study identified the problem but did not implement or test any mitigation strategies.
- What evidence would resolve it: A longitudinal study comparing detector performance with and without continuous learning mechanisms (e.g., periodic retraining, incremental updates) across multiple time slices.

### Open Question 2
- Question: How do detection signatures change when analyzing context-dependent code with complex dependencies compared to the standalone artifacts studied here?
- Basis in paper: [explicit] The authors acknowledge: "real software systems contain both standalone and context-dependent code. Detection characteristics of highly coupled code integrated into existing projects might differ from our observed patterns... future work examining detection within complete projects with complex dependencies would complement our findings."
- Why unresolved: Standalone artifacts were deliberately chosen to control for cross-model comparison fairness, excluding code requiring substantial context.
- What evidence would resolve it: A replication study using non-standalone functions/classes from complete project contexts, comparing detection signatures against the standalone baselines.

### Open Question 3
- Question: To what extent do the granularity-dependent detection patterns generalize to languages with different paradigms (e.g., functional languages like Haskell, or low-level systems languages like Rust)?
- Basis in paper: [explicit] The authors note: "our reliance on the CodeSearchNet Python dataset may limit generalizability to proprietary codebases or languages with distinct paradigms (e.g., functional or low-level systems)."
- Why unresolved: Only Python was studied, and structural metrics may behave differently in languages with different syntactic and semantic constraints.
- What evidence would resolve it: Cross-language replication using equivalent metrics on multi-language corpora, measuring whether granularity effects (8.6× dominance) persist across paradigms.

### Open Question 4
- Question: Can a hybrid detector combining structural features with text-based deep learning representations achieve both the interpretability of structural metrics and the potential accuracy gains of neural approaches?
- Basis in paper: [explicit] The authors state: "Future work directly comparing structural feature-based detection with text-based deep learning approaches on identical datasets would provide valuable insights into the relative strengths of these complementary paradigms."
- Why unresolved: The study deliberately excluded neural baselines for interpretability, estimating 100+ GPU-hours would be needed for fair comparison.
- What evidence would resolve it: A controlled comparison on identical data splits, plus evaluation of hybrid architectures that combine both signal types.

## Limitations

- Limited ecological validity: The study uses standalone functions and classes, excluding coupled codebases where dependencies may alter detection signatures.
- Temporal fragility: Detection models show substantial performance degradation (26.3pp average AUC drop) on repositories created after December 2024.
- Cross-model generalization failure: SHAP analysis reveals model-specific detection patterns, suggesting existing detectors trained on single models cannot reliably detect outputs from other architectures.

## Confidence

- High confidence: Granularity effects dominating model differences (AUC-ROC gap: 0.16-0.28 vs. 0.03-0.12). The statistical evidence (Jaccard similarity = 0.099) and consistent cross-validation results support this claim strongly.
- Medium confidence: GPT-3.5's exceptional detectability being unrepresentative. While the AUC-ROC of 0.96 is clear, the claim about contemporary models (0.68-0.80) relies on comparison with only three newer models, limiting generalizability.
- Low confidence: Universal discriminator hypothesis for Comment-to-Code Ratio. The claim that this ratio is the "sole" universal discriminator across all configurations may be overstated given that its predictive magnitude varies dramatically and other features appear in top-3 rankings for 7/8 configurations.

## Next Checks

1. **Temporal robustness experiment**: Evaluate trained models on a time-stratified dataset spanning 2022-2025 to quantify degradation rates and identify which structural features remain stable across model generations.

2. **Coupled codebase validation**: Apply detection models to non-standalone Python projects from GitHub, measuring performance degradation when dependencies and imports are included.

3. **Cross-model transfer learning**: Train detectors on multiple LLM models simultaneously and evaluate transfer performance, measuring whether multi-model training improves generalization compared to per-model approaches.