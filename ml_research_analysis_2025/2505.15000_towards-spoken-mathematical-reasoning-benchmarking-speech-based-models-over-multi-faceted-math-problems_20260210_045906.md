---
ver: rpa2
title: 'Towards Spoken Mathematical Reasoning: Benchmarking Speech-based Models over
  Multi-faceted Math Problems'
arxiv_id: '2505.15000'
source_url: https://arxiv.org/abs/2505.15000
tags:
- reasoning
- speech
- arxiv
- mathematical
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Spoken-MQA, a benchmark designed to evaluate
  mathematical reasoning from spoken input using both cascade models (ASR + LLMs)
  and end-to-end speech LLMs. The benchmark covers diverse math problem types including
  arithmetic, contextual reasoning, and knowledge-oriented reasoning, all presented
  in unambiguous natural spoken language.
---

# Towards Spoken Mathematical Reasoning: Benchmarking Speech-based Models over Multi-faceted Math Problems

## Quick Facts
- **arXiv ID**: 2505.15000
- **Source URL**: https://arxiv.org/abs/2505.15000
- **Reference count**: 23
- **Key outcome**: Speech-based models show degraded mathematical reasoning performance compared to text-based models, with end-to-end speech LLMs particularly struggling on arithmetic tasks despite competitive performance on contextual reasoning.

## Executive Summary
This paper introduces Spoken-MQA, a benchmark designed to evaluate mathematical reasoning from spoken input using both cascade models (ASR + LLMs) and end-to-end speech LLMs. The benchmark covers diverse math problem types including arithmetic, contextual reasoning, and knowledge-oriented reasoning, all presented in unambiguous natural spoken language. Through extensive experiments, the study finds that while some speech LLMs perform competitively on contextual reasoning tasks involving basic arithmetic, they still struggle with direct arithmetic problems, suggesting a lack of genuine mathematical understanding. Current LLMs exhibit a strong bias toward symbolic mathematical expressions written in LaTeX and have difficulty interpreting verbalized mathematical expressions. Mathematical knowledge reasoning abilities are significantly degraded in current speech LLMs. Domain-specific fine-tuning improves performance on arithmetic and knowledge reasoning tasks, highlighting the need for improved natural spoken language understanding and domain-adaptive training.

## Method Summary
The study evaluates mathematical reasoning capabilities of speech-based models on the Spoken-MQA benchmark, which includes four categories: Arithmetic (human-recorded), Single-step Contextual Reasoning, Multi-step Contextual Reasoning, and Knowledge-Oriented Reasoning (TTS-synthesized). Models tested include cascade architectures (Whisper ASR + LLM) and end-to-end speech LLMs (Qwen2-Audio, Ultravox, Phi-4-multimodal). Evaluation uses zero-shot chain-of-thought reasoning with "Please reason step by step" prompts. Fine-tuning experiments used 500k samples from OpenMathInstruct-1 converted to speech via Coqui TTS, with LoRA adapters trained for 3 epochs at learning rate 4e-5.

## Key Results
- Cascade models outperform end-to-end speech LLMs on contextual reasoning tasks but suffer from ASR transcription errors on formula-heavy problems
- Current speech LLMs show 5-17% accuracy degradation when processing verbalized mathematical expressions compared to symbolic LaTeX notation
- Domain-specific fine-tuning significantly improves performance on arithmetic (38.8→56.4) and knowledge reasoning (27.2→52.2) tasks
- Models achieve competitive performance on contextual reasoning (77-87%) but fail on direct arithmetic problems (38-50%), suggesting superficial pattern matching

## Why This Works (Mechanism)

### Mechanism 1: Cascade Decoupling Reduces Error Propagation at Cost of Transcription Fidelity
- Claim: Cascade models (ASR + LLM) outperform end-to-end speech LLMs on mathematical reasoning by decoupling speech recognition from reasoning, but suffer from ASR transcription errors on mathematical symbols.
- Mechanism: ASR transcribes speech → text → LLM processes familiar text format. This preserves the LLM's trained reasoning capabilities but introduces transcription artifacts, especially for formula-heavy content.
- Core assumption: The underlying text-based LLM retains its full reasoning capacity when given accurate text input.
- Evidence anchors:
  - [abstract]: "cascade models, which transcribe speech to text before reasoning, remain effective but suffer performance degradation due to ASR transcripts, especially for mathematical symbols and formula-heavy problems"
  - [section 4.2.2, Table 3]: Qwen2.5-7B-Instruct shows 57.8% accuracy on Knowledge-oriented Reasoning with ASR transcripts vs 75.4% with ground-truth text (+17.6 improvement)
  - [corpus]: VoxEval (Cui et al., 2025) similarly evaluates knowledge understanding from spoken inputs, confirming the challenge of preserving specialized knowledge through speech-to-text pipelines.
- Break condition: When ASR transcription quality degrades significantly on domain-specific vocabulary, or when verbal ambiguity cannot be resolved without context.

### Mechanism 2: Symbolic Training Bias Impairs Verbalized Math Comprehension
- Claim: LLMs exhibit a strong preference for symbolic mathematical expressions (LaTeX) over natural spoken language verbalizations, causing systematic performance degradation on spoken math problems.
- Mechanism: Training data predominantly contains symbolic mathematical notation → model internalizes reasoning patterns tied to visual/symbolic representations → verbalized expressions lack these learned patterns.
- Core assumption: Mathematical reasoning in LLMs is partially mediated through learned symbolic pattern recognition rather than pure semantic understanding.
- Evidence anchors:
  - [abstract]: "current LLMs exhibit a strong bias toward symbolic mathematical expressions written in LaTeX and have difficulty interpreting verbalized mathematical expressions"
  - [section 4.2.2, Table 3]: All tested LLMs showed lower accuracy with verbalized text compared to ground-truth LaTeX; Qwen2.5-Math-7B-Instruct showed 72.4% (ASR) → 83.8% (verbalized) → 89.0% (ground-truth)
  - [corpus]: MATH-Perturb benchmarks LLM robustness against problem perturbations, supporting the hypothesis that model performance is sensitive to input format variations.
- Break condition: When training incorporates substantial verbalized mathematical expressions alongside symbolic notation.

### Mechanism 3: Domain-Specific Fine-Tuning Recovers Degraded Reasoning Capabilities
- Claim: Speech LLMs lose mathematical reasoning capabilities during general-purpose training, but domain-specific fine-tuning can partially recover this performance.
- Mechanism: General speech LLM training emphasizes conversational/audio understanding → mathematical reasoning capabilities degrade or fail to transfer across modalities → targeted fine-tuning re-aligns speech representations with mathematical reasoning patterns.
- Core assumption: Mathematical reasoning representations exist in the base LLM but are not properly aligned with speech encoder outputs without domain-specific training.
- Evidence anchors:
  - [abstract]: "mathematical knowledge reasoning abilities are significantly degraded in current speech LLMs"
  - [section 4.2.3]: "FT-Phi-4-multimodal-instruct, fine-tuned on a 500k-sample speech instruction dataset in the mathematical domain, shows substantial performance gains in the Arithmetic (38.8→56.4) and Knowledge-oriented Reasoning (27.2→52.2) categories"
  - [corpus]: Related work on Audio-Reasoner (Xie et al., 2025) demonstrates that targeted training on audio reasoning tasks improves reasoning capabilities, supporting domain-adaptive training effectiveness.
- Break condition: When fine-tuning data quality is poor, contains ambiguous verbalizations, or lacks coverage of target mathematical domains.

## Foundational Learning

- **Concept: Speech LLM Architecture (Encoder-Projector-LLM)**
  - Why needed here: Understanding how speech signals are transformed into embeddings and aligned with the LLM's embedding space is essential for diagnosing where reasoning degradation occurs.
  - Quick check question: Can you explain why an end-to-end speech LLM might lose capabilities that the base text LLM possesses?

- **Concept: Verbal Ambiguity in Mathematical Expressions**
  - Why needed here: The paper identifies that 32% of sampled MATH problems are ambiguous when verbalized (e.g., "|z-w|" vs "|z|-w"). Understanding this is critical for constructing unambiguous spoken math benchmarks.
  - Quick check question: How would you verbally distinguish "x squared plus y squared" from "the quantity x plus y, squared" without visual notation?

- **Concept: Chain-of-Thought (CoT) Reasoning Evaluation**
  - Why needed here: All models were evaluated in zero-shot CoT setting with "Please reason step by step" prompts. Understanding CoT is necessary to interpret whether models are genuinely reasoning or performing shallow pattern matching.
  - Quick check question: What evidence would distinguish genuine multi-step reasoning from surface-level pattern matching on contextual reasoning tasks?

## Architecture Onboarding

- **Component map:**
  ```
  Spoken Input → [Speech Encoder: Whisper-large-v2/v3] → [Projection Module/Adapter] → [Base LLM: Llama-3.1-8B / Qwen2-7B / Phi-4-Mini] → Text Output (reasoning + answer)
  ```
  
  Alternative cascade path:
  ```
  Spoken Input → [ASR: Whisper] → Text Transcript → [Text LLM] → Output
  ```

- **Critical path:** Speech encoder quality → projection alignment fidelity → base LLM reasoning capability. The paper shows this chain is weakest at the projection/alignment stage for mathematical reasoning, as demonstrated by Ultravox matching cascade performance on Contextual Reasoning but failing on Arithmetic.

- **Design tradeoffs:**
  - **End-to-end vs. Cascade**: End-to-end preserves paralinguistic cues and reduces latency but sacrifices reasoning fidelity; Cascade preserves LLM reasoning but loses audio nuance and adds ASR error propagation.
  - **TTS vs. Human Speech**: Paper used Coqui TTS for scalability but acknowledges synthesized speech lacks natural variability, potentially biasing evaluation.
  - **Verbalization Strategy**: GPT-4o verbalization + human filtering ensures unambiguous problems but may not reflect natural spoken math patterns.

- **Failure signatures:**
  - **Arithmetic Gap**: Strong contextual reasoning performance (77-87%) but poor direct arithmetic (38-50%) suggests superficial pattern matching rather than genuine computation.
  - **Symbolic Bias**: Consistent 5-17% accuracy drop from ground-truth text to verbalized text on Knowledge-oriented Reasoning indicates reliance on LaTeX patterns.
  - **ASR Error Propagation**: Multi-step reasoning and formula-heavy problems show largest degradation from ground-truth to ASR transcripts.

- **First 3 experiments:**
  1. **Baseline establishment**: Run Whisper-Large-V3 + target LLM on all Spoken-MQA categories with both ASR transcripts and ground-truth text to quantify transcription error impact specific to your model combination.
  2. **Verbalization robustness test**: Create paired problems (symbolic LaTeX vs. verbalized text) for a subset of Knowledge-oriented problems to measure symbolic bias magnitude in your system.
  3. **Domain adaptation pilot**: Fine-tune projection module only (freeze speech encoder and LLM) on 10-50k verbalized math samples from OpenMathInstruct-1 to isolate whether alignment training alone recovers reasoning capabilities.

## Open Questions the Paper Calls Out

- **Open Question 1**: Do speech LLMs that perform well on contextual math reasoning genuinely comprehend arithmetic logic, or are they exploiting superficial pattern matching?
  - Basis in paper: [explicit] The authors state: "Investigating whether these models genuinely comprehend the logic behind arithmetic operations or merely exploit shallow correlations remains an open direction for future research."
  - Why unresolved: Models like Ultravox and Phi-4 achieve competitive Contextual Reasoning scores but fail on direct Arithmetic problems involving similar numbers, suggesting potential pattern matching without true understanding.
  - What evidence would resolve it: Probing experiments that test whether models can generalize arithmetic reasoning to novel problem structures, or adversarial tests that disrupt surface-level cues while preserving mathematical logic.

- **Open Question 2**: How can the strong symbolic bias of LLMs toward LaTeX mathematical expressions be reduced to improve performance on verbalized natural language math input?
  - Basis in paper: [explicit] The authors highlight "the need for further research to reduce the symbolic bias of LLMs and enhance the model for more robust natural spoken language understanding."
  - Why unresolved: Experiments show all models perform worse on verbalized text compared to ground-truth symbolic text, even though semantic content is preserved (e.g., 11-17% accuracy drops on Knowledge-Oriented Reasoning).
  - What evidence would resolve it: Training interventions specifically targeting verbalized math understanding, followed by evaluation on controlled comparisons between symbolic and verbalized forms of identical problems.

- **Open Question 3**: Would evaluation on human-spoken math problems yield different performance patterns compared to the TTS-generated speech used in Spoken-MQA?
  - Basis in paper: [explicit] In the Limitations section: "Future work is needed for more human-spoken data across all subsets to better reflect authentic speech patterns and to ensure a more realistic and comprehensive evaluation of speech-based mathematical reasoning."
  - Why unresolved: TTS-generated speech lacks natural variability, prosodic variation, disfluencies, and accent diversity that characterize real speech, potentially biasing evaluations.
  - What evidence would resolve it: Collecting human-spoken versions of benchmark subsets and comparing model performance against TTS equivalents to quantify any systematic differences.

- **Open Question 4**: What specific pretraining or cross-modal alignment strategies enable end-to-end speech LLMs to retain the mathematical reasoning capabilities of their underlying text LLMs?
  - Basis in paper: [inferred] The authors note Ultravox performs comparably to its cascade counterpart despite being end-to-end, while other speech LLMs significantly underperform. The training methodology behind Ultravox remains undisclosed.
  - Why unresolved: Most speech LLMs show degraded reasoning abilities, with training typically focused on general conversation rather than domain-specific mathematical reasoning.
  - What evidence would resolve it: Ablation studies comparing different training strategies (e.g., speech-text alignment objectives, mathematical instruction tuning, curriculum design) on downstream math reasoning performance.

## Limitations

- Benchmark coverage gaps due to filtering out 32% of ambiguous problems when verbalized, potentially overestimating model capabilities on clean, well-structured problems
- Synthetic speech limitations using TTS instead of natural human speech, lacking prosodic variation and disfluencies
- Evaluation methodology constraints with fixed zero-shot CoT prompts, making it difficult to distinguish genuine reasoning from pattern matching

## Confidence

**High Confidence (80-95%)**:
- Cascade models outperform end-to-end speech LLMs on contextual reasoning tasks while suffering ASR-induced degradation on formula-heavy problems
- Current speech LLMs show significant performance gaps compared to text LLMs on mathematical reasoning tasks
- Domain-specific fine-tuning improves performance on arithmetic and knowledge reasoning tasks

**Medium Confidence (60-80%)**:
- LLMs exhibit systematic bias toward symbolic mathematical expressions over verbalized forms
- Verbal ambiguity affects approximately 32% of mathematical problems when converted to spoken language
- Performance degradation patterns are consistent across different model architectures

**Low Confidence (30-60%)**:
- Claims about "lack of genuine mathematical understanding" in speech LLMs are difficult to validate without controlled experiments isolating reasoning mechanisms
- The relative importance of projection alignment versus speech encoder quality in end-to-end model performance

## Next Checks

**Validation Check 1: Natural Speech Robustness Test**
Replace TTS synthesis with human-recorded speech samples for a subset of Knowledge-oriented problems, measuring performance degradation. This validates whether TTS-induced artifacts systematically bias results and whether models can handle natural speech variability including disfluencies and varied pacing.

**Validation Check 2: Symbolic vs. Verbalized Input Ablation**
Create controlled pairs of problems (LaTeX notation vs. verbalized text) within the same mathematical content and measure absolute performance gaps for each model. This isolates whether performance differences stem from symbolic bias or general comprehension difficulties.

**Validation Check 3: Fine-tuning Data Quality Assessment**
Perform error analysis on models fine-tuned with domain-specific data, categorizing failures by problem type (arithmetic vs. knowledge reasoning) and error mode (transcription, reasoning, ambiguity). This validates whether observed improvements reflect genuine capability gains or overfitting to training distribution patterns.