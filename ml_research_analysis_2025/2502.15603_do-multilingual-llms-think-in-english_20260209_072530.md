---
ver: rpa2
title: Do Multilingual LLMs Think In English?
arxiv_id: '2502.15603'
source_url: https://arxiv.org/abs/2502.15603
tags:
- english
- language
- llms
- shows
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether multilingual large language models
  (LLMs) process semantic decisions in an English-centric representation space, regardless
  of the input and output languages. The authors use logit lens analysis, activation
  steering, and causal tracing to study internal representations across four open-source
  models (Llama-3.1-70B, Gemma-2-27b, Aya-23-35B, and Mixtral-8x22B) prompted in French,
  German, Dutch, and Mandarin.
---

# Do Multilingual LLMs Think In English?

## Quick Facts
- arXiv ID: 2502.15603
- Source URL: https://arxiv.org/abs/2502.15603
- Authors: Lisa Schut; Yarin Gal; Sebastian Farquhar
- Reference count: 40
- Primary result: Multilingual LLMs often process semantic decisions through English representations regardless of input/output languages

## Executive Summary
This paper investigates whether multilingual large language models rely on an English-centric conceptual space when processing non-English inputs. Through logit lens analysis, activation steering, and causal tracing across four open-source models (Llama-3.1-70B, Gemma-2-27b, Aya-23-35B, and Mixtral-8x22B), the authors find evidence that lexical words are often routed through English representations before translation. Steering interventions are more effective when using English-derived vectors, and interpolation between language representations favors English output. The degree of English-centricity varies by model, correlating with multilingual pretraining diversity and model size.

## Method Summary
The paper employs three mechanistic interpretability methods: (1) Logit lens - applying unembedding at intermediate layers to decode hidden states into tokens, (2) Activation steering - adding computed vectors to residual streams to influence generation, and (3) Causal tracing - corrupting tokens with Gaussian noise to identify where factual knowledge is most sensitive. The experiments use four models across five languages (English, French, German, Dutch, Mandarin) with datasets including LLM-Insight (72 target words × 10 prompts × 10 sentences) and City Facts translated to German, Dutch, and French.

## Key Results
- Lexical words are often routed through English representations before translation, visible through logit lens analysis
- Activation steering is more effective using English-derived vectors than target-language vectors
- Factual knowledge is stored in shared representation locations across languages, but English gravitation occurs during interpolation
- English-centricity varies by model, with smaller or less multilingual models showing stronger bias

## Why This Works (Mechanism)

### Mechanism 1: English-Mediated Semantic Routing
Multilingual LLMs may process semantically loaded words by first mapping them to English-aligned representations before translating to the target output language. During forward passes, intermediate layers decode to English tokens for lexical concepts even when final output must be non-English, suggesting a "think in English, translate later" pipeline. This routing is specific to semantic concepts rather than syntax, as non-lexical words show different behavior.

### Mechanism 2: Superior Efficacy of English Control Vectors
Activation steering interventions are more effective when steering vectors are derived from English sentences compared to target-language vectors. If the internal "concept space" is optimized for English, activation directions for concepts are likely more distinct or robust when computed in English, suggesting English vectors align better with the model's internal causal mechanisms for generating concepts.

### Mechanism 3: Shared Factual Geometry with English Gravitation
Facts are stored as language-agnostic vectors plus a language component, where the decoding path has lower energy state or higher probability for English outputs. When interpolating between language representations, the model preserves accuracy but gravitates toward English output, suggesting the "default" path out of shared semantic space is English.

## Foundational Learning

- **The Logit Lens**: A diagnostic tool that applies the final unembedding matrix to earlier layers to see what the model is "thinking" before committing to final tokens. Why needed: Primary method to detect English routing. Quick check: If layer 20 decodes to "cat" but layer 30 decodes to "gato", what does this imply about processing path?

- **Activation Steering (Residual Stream)**: Adding a vector (calculated as `h(concept_A) - h(concept_B)`) to the residual stream shifts model behavior without retraining. Why needed: Proves causality of English-centric representations. Quick check: Why would English-derived "love" vector work better than French-derived one if model "thinks" in English?

- **Language-Agnostic vs. Language-Specific Representations**: The paper argues against purely universal concept space, suggesting syntax might be language-specific while semantics is English-centric. Why needed: Core to understanding the model's internal organization. Quick check: Are determiners routed through English space the same way nouns are?

## Architecture Onboarding

- **Component map**: Input Embeddings -> Detokenization/Processing Layers -> Factual Storage -> Unembedding/LM Head
- **Critical path**: English Pivot occurs in middle layers. Information flows [Input Language] -> [English Semantic Representation] -> [Target Language Unembedding]. Interventions most effective if targeting "English Semantic Representation" phase.
- **Design tradeoffs**: High-diversity pretraining (Aya) shows less English routing but potentially higher steering complexity; smaller models (Gemma-2) show stronger English routing possibly due to limited capacity for distinct language spaces.
- **Failure signatures**: Language confusion (defaults to English when target language signal is weak); steering collapse (non-English vectors cause incoherent output); tokenization misalignment across languages complicating logit lens analysis.
- **First 3 experiments**:
  1. Logit Lens Sanity Check: Run logit lens on Llama-3.1 with Dutch prompt, verify English nouns appear 5-10 layers before final output.
  2. Steering Vector Transfer: Calculate "sentiment" vector using English data, apply to French generation task, compare success rate against French-derived vector.
  3. Interpolation Bias: Take hidden states for fact in German and English, interpolate (alpha=0.5), check if output is disproportionately English.

## Open Questions the Paper Calls Out

### Open Question 1
Does task choice (open-generation vs. single-token retrieval) fundamentally alter whether a model operates in English-centric or language-agnostic space? The paper notes divergent findings compared to prior work may stem from difference between single-token tasks and open-generation setting used here. Evidence would require comparative study across both task types.

### Open Question 2
How can interpretability methods be adapted to robustly handle varying tokenization lengths across languages? Tokenization variance is identified as key complication for cross-lingual comparisons. Evidence would require development of position-invariant interpretability techniques.

### Open Question 3
Would Sparse Autoencoders (SAEs) trained on multilingual data reveal different structural hierarchy than logit lens? SAEs were excluded because available pre-trained versions are predominantly English-biased. Evidence would require analysis using SAEs trained specifically on balanced multilingual corpora.

## Limitations
- Core methodology assumes one-to-one correspondence between semantic concepts and token representations may not hold across languages
- Steering vector validity oversimplifies complex, non-linear semantic relationships
- Fact localization results conflate retrieval mechanisms with storage organization
- Limited model and language coverage (four models, five languages) may not generalize to typologically diverse languages

## Confidence
- **High Confidence**: Steering vectors derived from English are more effective than target-language vectors; causal tracing methodology for identifying fact-sensitive layers; English gravitation effect during interpolation
- **Medium Confidence**: Logit lens analysis showing English routing of lexical words; correlation between model size, pretraining diversity, and English-centricity
- **Low Confidence**: Claims about language-agnostic fact storage; adequacy of addressing alternative explanations for English gravitation

## Next Checks
1. Conduct quantitative analysis of embedding space geometry by measuring cosine similarity between hidden states of semantically equivalent words across languages versus similarity between intermediate English decodings and target tokens.
2. Design steering experiments that control for semantic similarity between languages by creating steering vectors using semantically related words across languages and testing whether English advantage persists for words without direct English equivalents.
3. Fine-tune a model showing strong English-centricity (e.g., Llama-3.1-70B) with balanced multilingual objectives and re-run steering and interpolation tests to determine if English bias is a fixed architectural constraint or learnable preference.