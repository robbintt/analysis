---
ver: rpa2
title: 'Mellum: Production-Grade in-IDE Contextual Code Completion with Multi-File
  Project Understanding'
arxiv_id: '2510.05788'
source_url: https://arxiv.org/abs/2510.05788
tags:
- code
- completion
- jetbrains
- context
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mellum presents a production-ready 4B-parameter code completion
  model optimized for in-IDE use. It adopts a Llama-style architecture and is trained
  on ~4T permissively licensed tokens with multi-stage fine-tuning including fill-in-the-middle
  and project context awareness.
---

# Mellum: Production-Grade in-IDE Contextual Code Completion with Multi-File Project Understanding

## Quick Facts
- arXiv ID: 2510.05788
- Source URL: https://arxiv.org/abs/2510.05788
- Authors: Nikita Pavlichenko; Iurii Nazarov; Ivan Dolgov; Ekaterina Garanina; Dmitry Ustalov; Ivan Bondyrev; Kseniia Lysaniuk; Evgeniia Vu; Kirill Chekmenev; Joseph Shtok; Yaroslav Golubev; Anton Semenkin; Uladzislau Sazanovich
- Reference count: 40
- Primary result: 4B-parameter model achieves KK score of 0.69 on custom JetComplete benchmarks

## Executive Summary
Mellum is a production-ready code completion model designed for in-IDE use, achieving strong performance with a compact 4B-parameter architecture. The model employs a Llama-style transformer trained on approximately 4 trillion tokens, with multi-stage fine-tuning including fill-in-the-middle (FIM) and project context awareness. Optimized for low latency (targeting 500ms response time) and practical deployment constraints, Mellum demonstrates measurable improvements over larger baselines on custom benchmarks and in live JetBrains IDE experiments.

## Method Summary
Mellum employs a three-stage training pipeline: pre-training on ~4T permissively licensed tokens, supervised fine-tuning (SFT) with semantic FIM splits and project context retrieval, and direct preference optimization (DPO) to suppress verbose or stub-like outputs. The architecture uses a 4B-parameter Llama-style transformer with 8K context window and a custom 49K BPE tokenizer. Context retrieval uses IoU-based strategies for efficiency, while FIM training focuses on syntactic boundaries to improve stopping behavior. The model is evaluated using custom JetComplete benchmarks and online IDE experiments.

## Key Results
- KK score of 0.69 on JetComplete benchmark, outperforming larger baselines (0.62)
- Strong online performance in JetBrains IDEs with 90% requests served under 500ms
- Effective suppression of verbose or stub-like code through DPO alignment
- Robust multi-language support with consistent performance across Python, Java, and Kotlin

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Constraining fill-in-the-middle (FIM) training splits to syntactic boundaries improves the model's ability to stop generation at semantically appropriate points.
- **Mechanism**: Rather than splitting code at random character spans, the system uses a parser (via Code Engine) to define the "middle" segment as a complete logical unit (e.g., a function body). This trains the model to predict a coherent block and emit a stop token upon closure, preventing "jarring" over-generation.
- **Core assumption**: The distribution of syntactic blocks in training data generalizes well to incomplete code fragments typed by users in an IDE.
- **Evidence anchors**:
  - [section 3.3.1] Describes selecting "proper meaningful middle segments like function or loop bodies" rather than random slices to mitigate the "jarring effect."
  - [figure 3] Shows fine-tuned Mellum models generating fewer lines (closer to ground truth) compared to base models that over-generate.
  - [corpus] Related work *SpareCodeSearch* discusses efficient context retrieval, supporting the need for precise boundary detection, though it does not validate the specific FIM split strategy.
- **Break condition**: If the target language lacks a robust parser for the Code Engine, or if user code is frequently syntactically broken (invalid AST), the boundary detection may fail, leading to training noise.

### Mechanism 2
- **Claim**: Retrieving project context using n-gram overlap (IoU) provides sufficient signal for multi-file completion while minimizing latency compared to dense embedding retrieval.
- **Mechanism**: The context engine ranks code chunks from related files based on Intersection over Union (IoU) of token sets (lines or BPE tokens). This explicitly prioritizes lexically similar code (e.g., variable usages or API calls) over purely semantic similarity, conditioning the model on tangible project dependencies.
- **Core assumption**: Lexical overlap correlates strongly with the information required to complete the current cursor position better than, or sufficiently well compared to, semantic vector similarity.
- **Evidence anchors**:
  - [section 3.3.2] Details the "IoU Strategy" and "RAG Strategy" using `IoU_BPE` scoring against a cursor context window.
  - [abstract] Mentions "editor-critical capabilities such as context packing" as a key factor.
  - [corpus] The *SpareCodeSearch* paper validates the difficulty of GPU-heavy retrieval, indirectly supporting heuristic-based efficient retrieval, but does not confirm IoU's superiority over embeddings.
- **Break condition**: In projects with highly abstracted or polymorphic code where implementation details differ significantly from interface definitions, pure lexical overlap may fail to retrieve the correct implementation logic.

### Mechanism 3
- **Claim**: Direct Preference Optimization (DPO) specifically suppresses "correct but useless" generations (e.g., TODO comments or stubs) better than standard supervised fine-tuning (SFT).
- **Mechanism**: By constructing a preference dataset where "good" samples are functional code and "bad" samples are verbose or stub code (labeled via LLM-as-a-Judge), the model learns to align its outputs with utility rather than just syntactic probability.
- **Core assumption**: The "LLM-as-a-Judge" proxy accurately reflects human developer preferences for code readability and utility in an IDE setting.
- **Evidence anchors**:
  - [section 3.4] Describes using DPO to align against "verbose, hard-to-read code" and "NotImplementedError stubs."
  - [table 1] Shows KK score lifting from 0.63 (SFT) to 0.69 (DPO), indicating improved human-perceived quality.
  - [corpus] Weak external signal; related papers focus on benchmarking rather than the specific DPO alignment mechanism for code stubs.
- **Break condition**: If the reward model (or judge) exhibits bias toward specific formatting styles over functional correctness, the model may learn to generate aesthetically pleasing but logically hollow code.

## Foundational Learning

- **Concept: Fill-in-the-Middle (FIM)**
  - **Why needed here**: Mellum is not a chat bot; it is an infilling engine. Understanding the `prefix-suffix-middle` transformation is required to grasp how the model predicts code that fits *between* existing lines.
  - **Quick check question**: Given `prefix: def add(a,b):` and `suffix: return result`, what is the likely `middle` the model should predict?

- **Concept: Syntactic Awareness (Tree-sitter/AST)**
  - **Why needed here**: The paper relies on parsing code to find "meaningful" boundaries for training and evaluation. Without this, the model treats code as unstructured text.
  - **Quick check question**: Why would a random text split fail to teach a model when to stop generating a Python function?

- **Concept: Intersection over Union (IoU) in NLP**
  - **Why needed here**: This is the core retrieval heuristic. Unlike RAG using vector embeddings, this uses set theory on tokens.
  - **Quick check question**: If File A contains `import os` and File B contains `import os; import sys`, how would IoU rank their similarity to a query containing `import sys`?

## Architecture Onboarding

- **Component map**: IDE Cursor Context (Prefix/Suffix) + Project Files -> Code Engine (IoU/Path Distance strategies) -> Packed Context Window -> Llama-style 4B Transformer -> Completion Output

- **Critical path**: The **SFT Data Preparation (Code Engine)** is the most critical node. If the "Better FIM" segmentation logic (splitting at syntactic boundaries) is flawed, the model fails to learn stopping behavior, rendering it annoying for users regardless of inference speed.

- **Design tradeoffs**:
  - **4B vs 7B+ Parameters**: The paper explicitly trades raw parameter count (and potentially raw benchmark scores on HumanEval) for **latency** (500ms constraint) and **cost** (fitting in 80GB VRAM).
  - **IoU vs Semantic Search**: Trades retrieval quality/depth for **speed** and simplicity (no GPU needed for embedding inference during retrieval).
  - **8K Context Window**: Limits performance on RepoBench (which assumes >8K) but optimizes for interactive inference throughput.

- **Failure signatures**:
  - **Over-generation**: Model continues writing code past the logical end of the function (Failure in Stopping Behavior/DPO).
  - **Stubbing**: Model generates `raise NotImplementedError` instead of logic (Failure in DPO alignment).
  - **Context Blindness**: Model ignores imported functions from other files (Failure in IoU retrieval or context packing).

- **First 3 experiments**:
  1. **Ablation on FIM Splits**: Train two small models—one with random FIM splits, one with syntactic splits (Code Engine)—and compare the mean generation length vs. ground truth on the JetComplete benchmark.
  2. **Retrieval Stress Test**: Measure completion accuracy (KK score) in a "clean room" repo vs. a massive monorepo to validate the latency and relevance of the IoU strategy at scale.
  3. **Latency Profiling**: Profile end-to-end inference on an H100 with concurrent requests to verify if the 4B model maintains the <500ms latency goal when context windows are fully packed (8K tokens).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating real user interaction data into the DPO stage significantly improve model behavior and UX compared to LLM-as-a-Judge generated preference data?
- Basis in paper: [explicit] The authors state: "In the future, user data can be incorporated into the DPO stage to further improve the model behavior and UX."
- Why unresolved: The current DPO approach relies on LLM-as-a-Judge scoring, which may not fully capture user preferences in real IDE usage scenarios.
- What evidence would resolve it: A/B testing comparing DPO models trained on LLMaaJ preferences versus those trained on actual user acceptance/rejection telemetry, measured via online RoCC and AR metrics.

### Open Question 2
- Question: What is the optimal balance between context window size and latency for production IDE code completion, and would extending Mellum's 8K context yield measurable quality improvements?
- Basis in paper: [inferred] The paper notes Mellum's modest performance on RepoBench may be partly due to its 8K context limit, while RepoBench assumes contexts larger than 8K tokens.
- Why unresolved: The trade-off between extended context (potentially better completions) and latency constraints (90% of requests served within 500ms) remains unexplored.
- What evidence would resolve it: Experiments varying context window sizes (8K, 16K, 32K) while measuring both completion quality on repository-level benchmarks and latency distribution under production load.

### Open Question 3
- Question: Would semantic embedding-based context retrieval (e.g., dense retrievers) outperform the computationally efficient IoU-based methods for project context collection?
- Basis in paper: [inferred] The authors note they chose IoU similarity because it "requires minimal computational overhead, unlike more expensive approaches such as cosine distance over semantic embeddings."
- Why unresolved: The trade-off assumes embeddings are too expensive, but the quality gap was not empirically quantified.
- What evidence would resolve it: Controlled comparison of IoU-based vs embedding-based retrieval on JetComplete benchmark, measuring both KK score improvements and latency/throughput impact.

### Open Question 4
- Question: Why does multi-lingual DPO training outperform language-specific DPO even on single-language tasks (e.g., Python), and is this pattern consistent across all supported languages?
- Basis in paper: [inferred] Table 2 shows Mellum-4b-dpo-all achieves 0.68 KK score on Python versus 0.64 for Mellum-4b-dpo-python, despite the latter being specifically trained on Python data.
- Why unresolved: The paper notes this phenomenon but does not investigate the underlying mechanism (cross-language transfer, data diversity effects, or regularization benefits).
- What evidence would resolve it: Ablation studies isolating training data composition, analysis of attention patterns across language contexts, and experiments with controlled mixing ratios to identify the causal factor.

## Limitations

- **Data provenance and license compliance**: Exact mixing ratios, deduplication strategy, and license verification process are unspecified, creating uncertainty about potential legal or ethical risks.
- **Retrieval strategy trade-offs**: Lack of comparative validation between IoU and embedding-based retrieval leaves the quality-cost trade-off uncertain.
- **DPO alignment reliability**: Dependence on LLM-as-a-Judge without disclosed prompts or scoring rubric makes it difficult to assess alignment quality.

## Confidence

- **High confidence**: Architectural choices (4B parameters, 8K context, syntactic FIM splits) are internally consistent and well-justified by latency and deployment constraints. The ablation showing DPO’s impact on KK score (0.63→0.69) is directly reported and reproducible.
- **Medium confidence**: The claim that IoU retrieval is “sufficient” for project context is plausible given the latency requirements, but lacks comparative validation. Similarly, the assertion that syntactic FIM splits improve stopping behavior is supported by qualitative figures but not rigorously quantified.
- **Low confidence**: The reliability of LLM-as-a-Judge for preference labeling and the generalizability of custom JetComplete metrics to broader developer populations are weakly supported.

## Next Checks

1. **Ablation on FIM Splits**: Train two small models—one with random FIM splits, one with syntactic splits (Code Engine)—and compare the mean generation length vs. ground truth on the JetComplete benchmark to quantify the impact of boundary-aware splitting on stopping behavior.

2. **Retrieval Strategy Comparison**: Implement a lightweight embedding-based retriever (e.g., using FAISS) and benchmark its retrieval quality (precision@k) and latency against the IoU strategy on a diverse set of multi-file completion tasks.

3. **DPO Alignment Robustness**: Conduct a human study where developers rate completions from SFT-only and DPO-aligned models on a held-out set, focusing on utility vs. verbosity, to validate whether LLM-as-a-Judge preferences align with human preferences.