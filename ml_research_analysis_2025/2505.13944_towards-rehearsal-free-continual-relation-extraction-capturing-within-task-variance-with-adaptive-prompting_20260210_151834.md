---
ver: rpa2
title: 'Towards Rehearsal-Free Continual Relation Extraction: Capturing Within-Task
  Variance with Adaptive Prompting'
arxiv_id: '2505.13944'
source_url: https://arxiv.org/abs/2505.13944
tags:
- prompt
- relation
- task
- each
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles catastrophic forgetting in continual relation
  extraction without relying on memory buffers. The authors propose WAVE++, which
  leverages task-specific prompt pools inspired by the connection between prefix-tuning
  and mixture-of-experts.
---

# Towards Rehearsal-Free Continual Relation Extraction: Capturing Within-Task Variance with Adaptive Prompting

## Quick Facts
- arXiv ID: 2505.13944
- Source URL: https://arxiv.org/abs/2505.13944
- Reference count: 11
- Achieves up to 2% improvement in accuracy over state-of-the-art methods on FewRel and TACRED datasets

## Executive Summary
This paper addresses catastrophic forgetting in continual relation extraction without relying on memory buffers. The authors propose WAVE++, a framework that uses task-specific prompt pools to capture within-task variance and cross-task diversity. By assigning dedicated prompt pools to each task and employing a training-free cascade voting mechanism for task prediction, WAVE++ eliminates the need for parameter sharing that typically leads to forgetting. The approach also incorporates label descriptions to provide richer relational context and uses generative models to consolidate prior knowledge in shared parameters, achieving significant improvements over existing prompt-based and rehearsal-based methods.

## Method Summary
WAVE++ tackles continual relation extraction by assigning a dedicated prompt pool to each task, allowing for flexible, input-dependent expert selection through a query-key matching mechanism. The framework employs a training-free cascade voting mechanism based on Mahalanobis distance to infer task identity, eliminating the need for a trainable MLP classifier. Additionally, WAVE++ incorporates LLM-generated label descriptions with a contrastive loss to stabilize training and improve generalization. Generative models are used to consolidate prior knowledge in shared parameters, eliminating the need for explicit data storage. The method is evaluated on FewRel and TACRED datasets, demonstrating superior performance compared to state-of-the-art approaches.

## Key Results
- Achieves up to 2% improvement in accuracy over state-of-the-art prompt-based and rehearsal-based methods
- Demonstrates robust resistance to catastrophic forgetting without using memory buffers
- Outperforms existing methods in capturing within-task variance through task-specific prompt pools

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Task-specific prompt pools improve within-task variance capture and prevent cross-task interference
- **Mechanism:** Treats prompt tokens as experts in a Mixture of Experts framework, allowing flexible, input-dependent expert selection
- **Core assumption:** Input features are sufficiently distinct to select correct experts within a task
- **Evidence anchors:** Abstract mentions leveraging task-specific prompt pools; Section 4.1 discusses prompt pool construction
- **Break condition:** Noisy query-key selection may fail with extremely fine-grained tasks or scarce data

### Mechanism 2
- **Claim:** Training-free cascade voting improves Task Identity Inference and robustness to distribution shifts
- **Mechanism:** Uses Mahalanobis distance between input embedding and stored Gaussian parameters for task prediction
- **Core assumption:** Relation embeddings form class-conditional distributions approximated by multivariate Gaussian distributions
- **Evidence anchors:** Abstract references training-free cascade voting; Section 4.3 describes the mechanism
- **Break condition:** Distance-based voting may misclassify if embeddings don't cluster neatly into Gaussian shapes

### Mechanism 3
- **Claim:** LLM-generated label descriptions with contrastive loss stabilize training and improve generalization
- **Mechanism:** Minimizes contrastive loss pulling input representations closer to correct relation label descriptions
- **Core assumption:** LLM-generated descriptions provide meaningful, noise-reduced signals for relation space geometry
- **Evidence anchors:** Abstract notes incorporation of label descriptions; Section 4.2 discusses contrastive loss
- **Break condition:** Noisy or ambiguous descriptions might introduce conflicting signals

## Foundational Learning

- **Concept: Prefix-Tuning vs. Prompt-Tuning**
  - **Why needed here:** WAVE++ relies on prefix-tuning (injecting learned vectors into Key/Value matrices) rather than simple prompt-tuning
  - **Quick check question:** Does the prompt modify self-attention computation (Key/Value pairs) or just input token sequence?

- **Concept: Sparse Mixture of Experts (SMoE)**
  - **Why needed here:** WAVE++ frames prompt selection as an SMoE problem with sparsely activating experts
  - **Quick check question:** How does TopK gating differ from standard dense attention and relate to query-key lookup?

- **Concept: Catastrophic Forgetting in Continual Learning (CL)**
  - **Why needed here:** Understanding why updating shared parameters degrades previous knowledge
  - **Quick check question:** Why does storing mean and covariance of previous task embeddings help mitigate forgetting better than raw samples?

## Architecture Onboarding

- **Component map:** Frozen BERT -> Task-Specific Prompt Pools -> Prefix-Tuning -> Classifier (MLP) -> Cascade Voting
- **Critical path:**
  1. Training: Input → Select Top-K Prompts from current task pool → Forward Pass with Prefix-Tuning → Compute Loss → Update Current Pool & Classifier
  2. Inference: Input → Cascade Voting (Compare against stored μ, Σ of all tasks) → Select identified Task Pool → Relation Classification
- **Design tradeoffs:**
  - Memory vs. Isolation: Storing full prompt pool per task increases memory linearly but guarantees isolation
  - Statistical Assumption vs. Complexity: Gaussian assumptions simplify model but may fail on non-Gaussian distributions
- **Failure signatures:**
  - Voting Deadlock: Inconsistent results between base model and first prompt pool with no majority
  - Description Mismatch: Contrastive loss forcing embeddings into contradictory regions
  - Forgetting in Classifier: Shared MLP classifier drift despite prompt isolation
- **First 3 experiments:**
  1. Prompt Pool Ablation: Compare single prompt vs. pool of M prompts per task
  2. Task Identity Inference Validation: Compare Cascade Voting against MLP-based predictor
  3. Label Description Sensitivity: Vary number of LLM-generated descriptions (D=1, 3, 5, 7)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can replacing constant-function prefix experts with more complex architectures improve expressiveness without negating efficiency gains?
- **Basis:** Authors suggest exploring more sophisticated expert architectures in the Conclusion
- **Why unresolved:** Complex functions could capture more variance but risk increasing computational overhead
- **What evidence would resolve it:** Experiments integrating lightweight trainable modules (e.g., adapters or LoRA) as experts

### Open Question 2
- **Question:** How can the design and size of task-specific prompt pools be theoretically optimized?
- **Basis:** Method "heavily depends on design of prompt pools and experts" according to Conclusion
- **Why unresolved:** Current hyperparameters are tuned empirically rather than determined by relation task complexity
- **What evidence would resolve it:** Theoretical framework linking optimal number of experts to data variance within specific tasks

### Open Question 3
- **Question:** How robust is cascade voting if relation embeddings deviate from assumed multivariate Gaussian distribution?
- **Basis:** Section 4.3 explicitly assumes class-conditional distributions follow multivariate Gaussian distribution
- **Why unresolved:** Real-world embedding spaces are often non-Gaussian, multimodal, or sparse
- **What evidence would resolve it:** Ablation studies using non-parametric density estimation compared to Gaussian assumption

## Limitations

- Hyperparameter sensitivity: Critical values for prompt pool loss weight, contrastive loss weight, learning rate, and pool size are not explicitly specified
- Label description quality: Exact number of LLM-generated descriptions used in main results is ambiguous
- Gaussian assumption validity: Cascade voting mechanism may fail if relation embeddings don't follow multivariate Gaussian distributions

## Confidence

- **High confidence:** Core mechanism of task-specific prompt pools preventing cross-task interference (supported by ablation results)
- **Medium confidence:** Training-free cascade voting superiority over MLP classifiers (novel approach but limited comparative evidence)
- **Medium confidence:** Label descriptions providing meaningful semantic context (theoretically sound but sensitive to description quality)

## Next Checks

1. **Prompt Pool Size Sensitivity:** Systematically vary prompt pool size (M=4, 8, 16) and selection count to determine optimal trade-off between memory usage and within-task variance capture

2. **Cascade Voting Robustness:** Compare Cascade Voting accuracy against standard MLP-based task predictor across different dataset splits to quantify reduction in task identity misclassification

3. **Description Quantity Impact:** Vary number of LLM-generated descriptions (D=1, 3, 5, 7) per relation to determine performance saturation and test with noisy descriptions for robustness assessment