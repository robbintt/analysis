---
ver: rpa2
title: Natural language processing for African languages
arxiv_id: '2507.00297'
source_url: https://arxiv.org/abs/2507.00297
tags:
- languages
- language
- african
- https
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Natural language processing for African languages

## Quick Facts
- arXiv ID: 2507.00297
- Source URL: https://arxiv.org/abs/2507.00297
- Authors: David Ifeoluwa Adelani
- Reference count: 0
- Key outcome: None

## Executive Summary
This thesis presents a comprehensive study on building Natural Language Processing (NLP) resources and models for African languages. It introduces new datasets, improves existing models through techniques like multilingual adaptive fine-tuning (MAFT), and demonstrates the effectiveness of high-quality, in-domain data over larger noisy datasets. The work focuses on overcoming challenges unique to African languages, including orthography issues and limited data availability, through practical methods like vocabulary pruning and targeted data collection.

## Method Summary
The research employs a combination of data collection, model fine-tuning, and evaluation across multiple African languages. For Named Entity Recognition (NER), it compares CNN-BiLSTM-CRF models with XLM-R-base, training on subsets of 500, 1K, 2K, and 4K sentences per language from the MasakhaNER dataset. For Machine Translation (MT), it fine-tunes large pre-trained models like M2M-100 on small in-domain datasets (e.g., 2.5k sentence pairs) and introduces MAFT for joint adaptation across multiple languages. Vocabulary pruning is applied to reduce model size by removing non-target script tokens.

## Key Results
- MAFT enables a single model to perform competitively across multiple African languages, reducing the need for separate models per language.
- Fine-tuning massive pre-trained models on small, high-quality in-domain data (e.g., 2.5k sentences) outperforms training from scratch on larger, noisy out-of-domain datasets.
- Vocabulary pruning (removing non-Latin/non-Ge'ez tokens) reduces model size by ~50% with minimal performance loss for African languages.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning a massive multilingual PLM on a curated aggregation of monolingual texts from multiple low-resource languages (Multilingual Adaptive Fine-Tuning - MAFT) creates a single, high-performing model that rivals individual language models.
- **Mechanism:** Jointly adapting the model allows for positive transfer between related languages (e.g., Bantu family) while the base pre-trained weights (from e.g., XLM-R) provide a strong linguistic foundation, preventing overfitting to small datasets.
- **Core assumption:** The languages included in the adaptation share enough structural similarities (or are supported well enough by the base model) to benefit from joint parameter updates.
- **Evidence anchors:**
  - [section]: "We propose multilingual adaptive fine-tuning (MAFT)... to provide a single model for cross-lingual transfer learning for African languages." (Page 63)
  - [section]: "MAFT is competitive to performing LAFT... with the benefit of having a single model instead of a separate model for each of the target languages." (Page 64)
  - [corpus]: AfroXLMR models trained on "Aggregation of texts from 17 African languages".
- **Break condition:** If the target language is linguistically isolated or uses a script entirely unsupported by the base model's tokenizer (e.g., needing full script embedding from scratch), MAFT may fail without vocabulary expansion.

### Mechanism 2
- **Claim:** Leveraging massive pre-trained models (like M2M-100 or mT5) and fine-tuning them on tiny amounts (as little as 2k sentences) of high-quality, in-domain data significantly outperforms training from scratch on larger, noisy out-of-domain datasets.
- **Mechanism:** The pre-trained model provides robust syntactic and semantic priors; the small in-domain dataset shifts the specific vocabulary and stylistic distribution without catastrophic forgetting or overfitting to noise.
- **Core assumption:** High-quality data refers to correct orthography (e.g., diacritics in Yorùbá) and domain relevance (e.g., news vs. religious texts).
- **Evidence anchors:**
  - [section]: "We show that having a targeted collection of translations is surprisingly effective... fine-tuning M2M-100 on 2.5k sentence pairs of in-domain data... is sufficient to outperform the bilingual Transformer baselines." (Page 170)
  - [section]: "Models trained on JW300... generalize poorly to new domains." (Page 142, Chapter 9 intro)
  - [corpus]: "MAFAND-MT" (African News) vs "JW300" (Religious). "MENYO-20k" (Yorùbá News).
- **Break condition:** If the small in-domain dataset contains systemic annotation errors or inconsistent tokenization, the model will quickly overfit to these specific errors.

### Mechanism 3
- **Claim:** Removing vocabulary tokens corresponding to non-target scripts (e.g., non-Latin/non-Ge'ez) before adaptation reduces model size by ~50% with minimal performance loss for the specific African languages.
- **Mechanism:** Pruning the embedding layer removes parameters dedicated to languages/scripts irrelevant to the target domain, making the model more efficient without altering the transformer layers that capture generalizable linguistic patterns.
- **Core assumption:** The remaining vocabulary (Latin + specific extended characters like Ge'ez) covers the vast majority of the target language's tokens.
- **Evidence anchors:**
  - [section]: "We remove vocabulary tokens from the embedding layer that corresponds to non-Latin and non-Ge’ez... effectively reducing the model size by 50%." (Page 63)
  - [section]: "XLM-R-base-v70k (140M)... Average F1 reduces by (-1.6)... they are still better than XLM-R-miniLM." (Page 74)
  - [corpus]: AfroXLMR-small (vocabulary reduced version).
- **Break condition:** If the target language heavily borrows from languages using the removed scripts (e.g., Arabic loanwords in Swahili/Hausa without transliteration), performance may degrade more than expected.

## Foundational Learning

- **Concept:** **Domain Shift**
  - **Why needed here:** A central finding is that models trained on massive religious corpora (JW300) fail on news tasks due to vocabulary and style mismatch.
  - **Quick check question:** If your training data is 90% religious text, what domain will your model struggle to translate to? (Answer: News/General conversation).
- **Concept:** **Orthography/Noise Sensitivity**
  - **Why needed here:** Many African languages use diacritics (Yorùbá, Igbo). Pre-trained models often ignore them, and "unclean" data (stripping diacritics) drastically hurts performance for native speakers, even if BLEU scores look similar.
  - **Quick check question:** Does removing tone marks in Yorùbá affect the *fluency* rating by native speakers, even if *adequacy* is preserved? (Answer: Yes, fluency drops significantly).
- **Concept:** **Transfer Learning vs. Training from Scratch**
  - **Why needed here:** With limited data (<100MB monolingual), training models from scratch is ineffective. Fine-tuning existing PLMs is the only viable path.
  - **Quick check question:** You have 5,000 sentences of parallel data. Should you train a Transformer from scratch or fine-tune M2M-100? (Answer: Fine-tune M2M-100).

## Architecture Onboarding

- **Component map:** Base Model (XLM-RoBERTa/M2M-100/mT5) -> Adapter (MAFT) -> Tokenizer (Modified SentencePiece/BPE) -> Data Pipeline (Web Crawler -> Orthography Check -> Clean Corpus) -> Evaluation (MasakhaNER/MAFAND-MT/FLORES)
- **Critical path:**
  1. **Data Curation:** Cleaning the monolingual/parallel data (fixing diacritics, removing noise) is the highest leverage activity.
  2. **Vocabulary/Tokenization:** Ensure the tokenizer supports specific characters (e.g., ẹ, ọ, ƒ) or prune standard vocabularies.
  3. **MAFT/Adaptation:** Run Multilingual Adaptive Fine-Tuning on the clean aggregate corpus.
  4. **Task Fine-tuning:** Fine-tune on the specific small task dataset (e.g., NER labels).
- **Design tradeoffs:**
  - **Quality vs. Quantity:** Using 2k sentences of high-quality news text outperforms 100k sentences of noisy JW300 text for translation. **Assumption:** This holds for specific domains; religious data is better for religious tasks.
  - **Model Size vs. Coverage:** Pruning vocabulary reduces size but makes the model useless for non-African scripts.
- **Failure signatures:**
  - **Zero-shot Failure:** XLM-R performs well on languages seen during pre-training (e.g., Swahili) but fails on unseen low-resource languages unless adapted (LAFT/MAFT).
  - **Diacritic Loss:** Models fine-tuned on text with missing tone marks will produce uninterpretable output for native speakers (e.g., "owo" means hand, money, broom depending on marks).
  - **Domain Hallucination:** A model trained on religious text might insert biblical terms into a news article about politics.
- **First 3 experiments:**
  1. **Baseline Establishment:** Fine-tune XLM-R-base on a small clean news dataset (2k samples) and evaluate against a from-scratch Transformer trained on JW300. Confirm the transfer learning advantage.
  2. **Vocabulary Analysis:** Tokenize a sample corpus (e.g., Yorùbá) with the standard XLM-R tokenizer. Count the "UNK" tokens or broken words caused by missing diacritics/characters to justify vocabulary modification.
  3. **MAFT Validation:** Take a base multilingual model, perform MAFT on a curated mix of 3-5 related African languages, and measure the performance delta on a held-out language from the same family.

## Open Questions the Paper Calls Out
None

## Limitations
- The success of MAFT relies on shared linguistic features within language families, which may not apply to linguistically isolated languages or those using unsupported scripts.
- The emphasis on domain shift is critical but tied to specific corpora (JW300 for religious, MAFAND-MT for news); performance in other domains remains untested.
- While NER and MT are thoroughly explored, other NLP tasks (e.g., sentiment analysis, question answering) are less covered.

## Confidence
- **High Confidence:** The core claim that fine-tuning massive pre-trained models on small, high-quality, in-domain datasets significantly outperforms training from scratch or using noisy out-of-domain data.
- **Medium Confidence:** The effectiveness of MAFT for creating a single, high-performing model for multiple African languages.
- **Medium Confidence:** The benefit of vocabulary pruning (removing non-target scripts) for reducing model size with minimal performance loss.

## Next Checks
1. **Domain Transfer Validation:** Take a model trained on religious text (e.g., JW300) and evaluate its performance on a held-out news dataset (e.g., MAFAND-MT) for the same language. Quantify the performance drop and identify the specific types of errors (e.g., vocabulary mismatch, style inconsistencies) to validate the domain shift hypothesis.

2. **MAFT Cross-Family Generalization:** Apply MAFT to a set of linguistically diverse African languages (e.g., one from each of Bantu, Semitic, and Niger-Congo families) and evaluate on a held-out language from a fourth, unrelated family (e.g., Khoisan). Measure whether the joint adaptation provides any benefit or if it fails due to lack of shared linguistic features.

3. **Orthography Sensitivity Test:** Train two models for a diacritic-sensitive language (e.g., Yorùbá): one on clean text with diacritics and one on "unclean" text with diacritics removed. Evaluate both on a native speaker fluency test (e.g., Likert scale ratings) in addition to standard metrics (e.g., BLEU, F1) to quantify the real-world impact of orthography preservation.