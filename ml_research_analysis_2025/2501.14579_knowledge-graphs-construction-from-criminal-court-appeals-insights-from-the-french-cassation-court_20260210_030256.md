---
ver: rpa2
title: 'Knowledge Graphs Construction from Criminal Court Appeals: Insights from the
  French Cassation Court'
arxiv_id: '2501.14579'
source_url: https://arxiv.org/abs/2501.14579
tags:
- knowledge
- ontology
- data
- such
- legal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for constructing knowledge graphs
  from criminal court appeals in the French Cassation Court, addressing the challenge
  of accurately representing unstructured legal data. The framework combines a domain-specific
  ontology with generative AI to automate knowledge extraction from court decisions,
  enabling scalable and reliable analysis of judicial practices.
---

# Knowledge Graphs Construction from Criminal Court Appeals: Insights from the French Cassation Court

## Quick Facts
- arXiv ID: 2501.14579
- Source URL: https://arxiv.org/abs/2501.14579
- Authors: Alexander V. Belikov; Sacha Raoult
- Reference count: 22
- Primary result: 93% precision and 89% recall in extracting RDF triples from French criminal court appeals using LLM-guided ontology

## Executive Summary
This paper presents a framework for constructing knowledge graphs from unstructured French criminal court appeals by combining domain-specific ontologies with generative AI. The authors demonstrate that providing an explicit criminal law ontology within LLM prompts significantly improves extraction accuracy compared to schema-free approaches. Using GPT-4o mini, the system achieves high precision and recall on a ground truth dataset, enabling scalable analysis of judicial decision patterns across 2,820 appeals from the French Cassation Court.

## Method Summary
The methodology employs iterative prompt engineering with large language models to generate RDF triples based on a tailored criminal law ontology. Court decisions are parsed from PDFs, then processed through a langchain pipeline where the LLM converts unstructured text into structured triples using the provided ontology as a constraint. The output undergoes RDF syntax validation before storage in a triple store. The ontology was developed semi-automatically with LLM assistance, validated through expert review, and refined based on edge cases identified in the source documents.

## Key Results
- Achieved 93% precision and 89% recall on ground truth dataset of 20 annotated appeals
- Generated average of 30 triples per appeal document
- Outperformed Property Graph approach (50-60% accuracy) by using RDF triples with explicit ontology constraints
- Cost-effective at approximately $2.50 per 1,000 documents using GPT-4o mini
- Revealed negligible correlation between punishment severity and appeal outcomes in analyzed dataset

## Why This Works (Mechanism)

### Mechanism 1: Ontology-Guided Extraction
Providing an explicit domain ontology within the LLM prompt significantly improves extraction accuracy compared to schema-free approaches. The ontology constrains the LLM's output space, forcing it to map unstructured legal text to predefined graph structures rather than inventing inconsistent entity types. This relies on the LLM's instruction-following capabilities to adhere to the provided Turtle/RDF schema syntax without extensive fine-tuning.

### Mechanism 2: Unified Pipeline Replacement
Generative LLMs effectively replace multi-stage NLP pipelines (NER, relation extraction, linking) for legal documents. Instead of training separate models for entity recognition and relation extraction, a single LLM performs these steps implicitly via prompt instructions, converting raw text directly into serialized RDF triples. This assumes general-purpose LLMs possess sufficient latent knowledge of legal syntax and logic to perform these tasks without task-specific architectural modifications.

### Mechanism 3: Iterative Schema Refinement
The ontology development process benefits from a semi-automated, iterative feedback loop with the LLM. The authors used the LLM to suggest new types and relations based on edge cases in source text, validating these against expert expectations. This allows the schema to evolve dynamically to fit the data rather than forcing data into a rigid, pre-conceived schema, assuming the LLM can reliably identify structurally relevant edge cases rather than hallucinating unnecessary complexity.

## Foundational Learning

- **RDF Triples (Subject-Predicate-Object):** The paper explicitly chooses RDF triple format over Property Graphs for final output. Understanding this standard is required to query the resulting dataset via SPARQL or Fuseki. Quick check: How does the strict "Subject-Predicate-Object" structure of RDF differ from the flexible node-property schema of a Property Graph?

- **Precision vs. Recall in Extraction:** The paper reports 93% precision and 89% recall. High precision implies the extracted graph is trustworthy (few false positives), critical for legal analysis, even if some data is missed (lower recall). Quick check: If the system lowers its confidence threshold to capture more criminal entities (increasing recall), what is the likely trade-off regarding the reliability of the generated graph?

- **Context Window Constraints:** The authors note a tradeoff between ontology granularity and cost/context capacity. If the prompt (ontology + document) is too large, the model may "forget" instructions. Quick check: Why might adding more detailed classes to the ontology actually degrade the quality of extraction for very long court documents?

## Architecture Onboarding

- **Component map:** PDF Court Decisions → pymupdf parsing → langchain orchestrator → GPT-4o mini query (ontology + text prompt) → RDF triple output → rdflib validation → Fuseki triple store

- **Critical path:** The prompt engineering step. The prompt must contain the ontology, formatting rules, and specific constraints (e.g., ISO 8601 dates). Failure here results in malformed Turtle syntax that rdflib cannot parse.

- **Design tradeoffs:** RDF vs. Property Graph: RDF was chosen despite being less intuitive to query because it offered >90% accuracy versus 50-60% for Property Graphs. Model Size: GPT-4o mini was chosen for cost efficiency, requiring specific prompt tuning to handle format constraints that larger models might manage natively.

- **Failure signatures:**
  - Format hallucination: LLM generates text that looks like Turtle but contains non-standard predicates or data types
  - Schema projection: Model incorrectly maps "Court Location" to "Crime Location" due to ambiguous ontology definitions
  - Low-quality output: Smaller models (e.g., 7B parameters) resulted in unparseable or nonsensical triples

- **First 3 experiments:**
  1. Baseline Validation: Run provided code on 5 sample PDFs without ontology in prompt to quantify drop in triple validity and observe "drifting" entity types
  2. Format Stress Test: Modify prompt to remove explicit instruction to validate outputs (e.g., ISO dates) and measure rate of rdflib parsing failures
  3. Comparative Extraction: Attempt to reproduce 50-60% accuracy result using Property Graph approach on same ground-truth 20 appeals to understand structural limitations

## Open Questions the Paper Calls Out
- Can domain-specific pretraining or fine-tuning enable smaller, open-source models to match or exceed GPT-4o mini's extraction performance in this legal context?
- How does the constructed knowledge graph perform in downstream applications such as retrieval-augmented generation or predictive analysis of judicial decisions?
- Can storing the ontology in a vector store mitigate context window limitations without compromising the accuracy of triple extraction?

## Limitations
- Performance on non-French legal documents or civil cases remains untested
- Relies heavily on availability of domain-specific ontologies and expert validation
- Cost model assumes access to GPT-4o mini which may not be available in all deployment contexts

## Confidence
- **High Confidence:** RDF triple extraction methodology and its superiority over Property Graph approaches (93% vs 50-60% accuracy) are well-validated through direct comparison
- **Medium Confidence:** Ontology development process is described in detail but full specification not provided, making independent replication challenging
- **Low Confidence:** Scalability claims based on cost estimates rather than actual large-scale deployment data; performance on civil/administrative appeals is speculative

## Next Checks
1. Apply the same extraction framework to civil court decisions using the same criminal ontology and measure degradation in precision/recall to quantify domain dependence
2. Systematically compare extraction quality across different LLM models while tracking both accuracy metrics and per-document costs to establish optimal scaling points
3. Process appeals from multiple years (2021-2023) to determine if extraction quality degrades over time or if legal terminology/structures evolve in ways that affect ontology coverage