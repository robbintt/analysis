---
ver: rpa2
title: Understanding Counting Mechanisms in Large Language and Vision-Language Models
arxiv_id: '2511.17699'
source_url: https://arxiv.org/abs/2511.17699
tags:
- uni00000013
- uni00000011
- layer
- uni00000014
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how large language and vision-language
  models internally represent and compute numerical information during counting tasks.
  The authors develop a specialized mechanistic interpretability tool called CountScope
  to analyze latent numerical representations through causal mediation and activation
  patching experiments.
---

# Understanding Counting Mechanisms in Large Language and Vision-Language Models

## Quick Facts
- **arXiv ID**: 2511.17699
- **Source URL**: https://arxiv.org/abs/2511.17699
- **Reference count**: 40
- **Key outcome**: This study investigates how large language and vision-language models internally represent and compute numerical information during counting tasks, developing CountScope to analyze latent numerical representations through causal mediation and activation patching experiments.

## Executive Summary
This study investigates the internal mechanisms of counting in large language models (LLMs) and vision-language models (LVLMs) through a novel mechanistic interpretability framework called CountScope. The authors demonstrate that both model types maintain a latent internal counter mechanism that updates incrementally with each item, primarily stored in the final token or visual region. Layerwise analyses reveal that numerical representations emerge progressively, with lower layers encoding small counts and higher layers representing larger ones. The models show transferable counting mechanisms across contexts and rely on structural separators as shortcuts for tracking item counts, providing causal evidence that counting is a structured, layerwise process.

## Method Summary
The CountScope framework analyzes counting mechanisms through synthetic datasets (textual lists of 1-9 items and visual images of 1-9 geometric shapes) using activation patching and causal mediation analysis. The method involves running forward passes to collect activations at each layer, then using minimal target contexts with placeholder tokens to decode latent counts through online/offline patching (zero, mean, interchange). Causal Influence scores quantify the importance of different tokens and layers by measuring probability changes when patching source activations into target contexts. The approach is applied across multiple model families including Qwen2.5, Llama3 (LLMs), and Qwen2.5-VL, InternVL3.5 (LVLMs).

## Key Results
- Numerical representations emerge progressively across layers, with small counts (1-3) encoded early and larger counts (7-9) requiring deeper layers (15+)
- Models maintain a latent internal counter stored primarily in the final token/region, behaving in a Markov-like manner that updates incrementally
- Structural separators (commas) act as positional shortcuts, carrying more count information than item tokens themselves
- LVLMs show vision encoder bottlenecks, struggling with counts beyond 5 despite higher-resolution inputs improving some models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Numerical representations emerge progressively across layers, with small counts encoded early and larger counts requiring deeper layers.
- **Mechanism**: Each layer incrementally refines the count representation. Lower layers can linearly separate counts 1-3, while distinguishing counts 7-9 requires layers 15+. This creates a depth-dependent capacity ceiling.
- **Core assumption**: The layer hierarchy builds representations through sequential transformation rather than parallel computation.
- **Evidence anchors**: [abstract] "Layerwise analyses reveal a progressive emergence of numerical representations, with lower layers encoding small counts and higher layers representing larger ones." [section 3] "PCA projections reveal a consistent trend: representations of smaller numbers become linearly separable in early layers, whereas larger numbers are distinguished only in deeper layers."
- **Break condition**: If early layers could decode all counts equally well, the mechanism fails. Also breaks if deeper layers show no improvement for larger numbers.

### Mechanism 2
- **Claim**: Models maintain a latent internal counter that updates incrementally and is stored primarily in the final token (text) or final visual region (images).
- **Mechanism**: Each item token encodes its running position in the sequence. The counter behaves in a Markov-like manner—it only needs the previous count state, not a full sum over all items. Patching the final item transfers the total count.
- **Core assumption**: Counting relies on incremental state updates rather than global aggregation after seeing all items.
- **Evidence anchors**: [abstract] "identify an internal counter mechanism that updates with each item, stored mainly in the final token or region and transferable between contexts." [section 4.3.2] "the internal counter behaves as if it simply continues from the end of the source list rather than restarting... operates in a memoryless manner... consistent with a Markov assumption."
- **Break condition**: If patching the final token fails to transfer the count, or if the model requires all items (not just the last state) for accurate prediction.

### Mechanism 3
- **Claim**: Structural separators (commas) act as shortcuts that carry more positional information than the items themselves.
- **Mechanism**: Each separator token encodes its position in the sequence (e.g., the third comma reflects count 4). Models rely on these structural cues to track counts rather than computing from semantic content alone.
- **Core assumption**: Models exploit regularities in input formatting as computational shortcuts.
- **Evidence anchors**: [abstract] "Models rely on structural cues such as separators in text, which act as shortcuts for tracking item counts." [section 4.7] "separators alone contain sufficient information about their position in the list... separators provide even more predictive signals than the elements."
- **Break condition**: If removing or altering separators has no effect on counting accuracy, or if item tokens alone are sufficient for accurate counting.

## Foundational Learning

- **Concept**: Causal Mediation Analysis (CMA) and Activation Patching
  - **Why needed here**: The entire methodology depends on causal intervention—modifying activations and measuring output changes—to identify which components are necessary for counting behavior.
  - **Quick check question**: Can you explain the difference between zero patching, mean patching, and interchange patching, and when each is appropriate?

- **Concept**: Transformer Layer Representations
  - **Why needed here**: The paper's central finding is that numerical information emerges progressively across layers; understanding residual stream, attention heads, and layer-wise transformation is essential.
  - **Quick check question**: How does information flow from layer L to layer L+1 in a decoder-only transformer, and what does it mean for a representation to "emerge" at a particular layer?

- **Concept**: Markov Chains / Memoryless Processes
  - **Why needed here**: The internal counter is described as Markov-like; the next state depends only on the current state, not the full history.
  - **Quick check question**: If a process is Markov, what information do you need to predict the next state? How would you test if a model's internal counter satisfies this property?

## Architecture Onboarding

- **Component map**: Input tokenizer -> embedding layer -> N transformer layers (attention + MLP per layer) -> final layer norm -> output logits
- **Critical path**: 1. Identify counting task (text list or visual objects) 2. Run forward pass and collect activations at each layer for each token/patch 3. Apply CountScope: patch activations from source into minimal target context 4. Decode predictions and compute Causal Influence (CI) scores 5. Layerwise analysis: patch only up to layer L to observe where counts become decodable
- **Design tradeoffs**: Online vs. offline patching (online modifies during forward pass, offline substitutes post-hoc); patch granularity (token-level vs. attention head-level); LVLM resolution (higher resolution improves some models but degrades others)
- **Failure signatures**: Counts >5 show degraded performance in LVLMs (vision encoder bottleneck); removing separators causes accuracy collapse (monotypic: 0.75 drop, polytypic: 0.97 drop); question-last format reduces mechanism visibility (CI scores near 0 for continued counting hypothesis)
- **First 3 experiments**: 1. Replicate final-token patching experiment: create two text contexts with different counts, patch final token activation from source to target, verify prediction follows source count 2. Layerwise decoding test: for a single 9-item list, use CountScope to decode count at layers 5, 10, 15, 20, 25 3. Separator ablation: replace all commas with uniform separator token, measure accuracy drop, then patch first-separator activation to all separator positions

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the counting mechanism operate under chain-of-thought decoding or other "system-2" reasoning setups?
- **Basis in paper**: [explicit] The authors state: "A full circuit-level description is still missing, and counting under system-2 setups such as chain-of-thought decoding is not yet understood."
- **Why unresolved**: The current study focuses on direct counting tasks without intermediate reasoning steps. Chain-of-thought prompting may invoke different mechanisms or distributed processing across tokens.
- **What evidence would resolve it**: Apply CountScope to models generating chain-of-thought responses for counting tasks, tracking whether latent counters emerge in reasoning tokens versus final output tokens.

### Open Question 2
- **Question**: What is the complete circuit-level description of the counting mechanism, including which attention heads and MLP sublayers causally mediate numerical updates?
- **Basis in paper**: [explicit] The authors explicitly note: "A full circuit-level description is still missing."
- **Why unresolved**: The current analysis operates at the level of token/region activations and layer-wide interventions, not at the granularity of individual attention heads or feed-forward components.
- **What evidence would resolve it**: Head-level and neuron-level activation patching experiments, combined with attention pattern analysis, to identify the minimal causal circuit for counting.

### Open Question 3
- **Question**: Why do background visual regions frequently carry stronger count signals than foreground object patches in LVLMs?
- **Basis in paper**: [inferred] The paper reports this finding as "surprising" and notes it varies with image layout, but does not provide a mechanistic explanation for why background regions encode count information.
- **Why unresolved**: Global spatial attention in vision encoders could distribute information across all patches, but the specific computational reason for background dominance remains unclear.
- **What evidence would resolve it**: Systematic manipulation of background properties (size, texture, uniformity) combined with attention visualization to determine whether background patches serve as aggregation hubs or incidental carriers of global information.

## Limitations
- The synthetic datasets (1-9 items, uniform backgrounds, non-overlapping shapes) create ideal conditions that diverge from real-world counting scenarios
- The separator shortcut mechanism may represent overfitting to synthetic task structure rather than robust numerical reasoning
- LVLMs show clear vision encoder bottlenecks with counts beyond 5, suggesting fundamental architectural limitations not fully explored

## Confidence

**High Confidence (8/10):**
- Layerwise emergence of numerical representations is well-supported by PCA projections and causal mediation analysis
- Final token as primary count storage is strongly validated through activation patching experiments
- Separator shortcut mechanism is clearly demonstrated through ablation studies

**Medium Confidence (6/10):**
- The Markov-like counter assumption is plausible but not exhaustively tested
- Cross-context transferability of counting mechanisms is suggested but based on limited experimental variations

**Low Confidence (4/10):**
- Generalization to real-world counting tasks is uncertain given synthetic dataset constraints
- The claim that counting is "structured" in both LLMs and LVLMs may overstate similarities

## Next Checks

1. **Real-World Robustness Test**: Apply CountScope to natural images with complex backgrounds, occlusions, and varying object sizes. Compare counting accuracy and activation patterns against synthetic datasets to identify breaking points in the proposed mechanisms.

2. **Alternative Separator Structures**: Test counting performance with different separator types (semicolons, periods, no separators, or variable-length gaps) to determine if the shortcut mechanism is specific to commas or represents a broader positional encoding strategy.

3. **Multi-Step Numerical Reasoning**: Extend beyond simple counting to addition and comparison tasks. Apply the same layerwise and activation patching analyses to verify whether the incremental counter mechanism scales to more complex numerical operations or reveals fundamental limitations.