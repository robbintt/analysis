---
ver: rpa2
title: 'Patient-Centered Summarization Framework for AI Clinical Summarization: A
  Mixed-Methods Design'
arxiv_id: '2510.27535'
source_url: https://arxiv.org/abs/2510.27535
tags:
- patient
- patients
- clinical
- summaries
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed a patient-centered clinical summarization
  framework using mixed-methods involving patient and clinician input, and evaluated
  open-source LLMs against this standard. Patients emphasized lifestyle, social support,
  stressors, and values, while clinicians prioritized concise functional and psychosocial
  context.
---

# Patient-Centered Summarization Framework for AI Clinical Summarization: A Mixed-Methods Design

## Quick Facts
- arXiv ID: 2510.27535
- Source URL: https://arxiv.org/abs/2510.27535
- Reference count: 32
- Primary result: Open-source LLMs (Llama-3.1-8B, Mistral-8B) achieve ROUGE-L of 0.206 and BERTScore of 0.683 on patient-centered clinical summarization, but miss patient values in 49% of summaries.

## Executive Summary
This study developed a patient-centered clinical summarization framework using mixed-methods involving patient and clinician input, and evaluated open-source LLMs against this standard. Patients emphasized lifestyle, social support, stressors, and values, while clinicians prioritized concise functional and psychosocial context. Five LLMs (Llama-3.1-8B, Mistral-8B, etc.) were tested on 72 atrial fibrillation consultations using zero- and few-shot prompting, achieving best ROUGE-L of 0.206 and BERTScore of 0.683 (Llama-3.1-8B with three-shot). Human evaluation showed models matched experts in completeness and fluency but lagged in correctness and patient-centeredness. Current LLMs struggle to capture nuanced patient values and factual accuracy compared to human-generated summaries.

## Method Summary
The study used a mixed-methods design with Patient and Public Involvement (PPIE) groups to identify essential themes from patient and clinician interviews, creating a 5-domain annotation framework (lifestyle, support systems, stressors, preferences/values, sources of meaning). Eight clinician annotators generated 88 gold-standard Patient-Centered Summaries (PCS) from 72 atrial fibrillation consultation transcripts. Five open-source LLMs (Llama-3.2-3B, Llama-3.1-8B, Mistral-8B, Gemma-3-4B, Qwen3-8B) were evaluated using zero-shot and few-shot prompting with Background/Issues/Plan structure, tested for ROUGE-L, BERTScore, and human evaluation on correctness, completeness, conciseness, patient-centeredness, and fluency.

## Key Results
- Best zero-shot performance: Llama-3.2-3B (ROUGE-L 0.189, BERTScore 0.655)
- Best few-shot performance: Llama-3.1-8B (ROUGE-L 0.206, BERTScore 0.683 with three-shot examples)
- Models matched human summaries in completeness (3.4 vs 3.6) and fluency (3.8 vs 4.1) but lagged in correctness (-1.8 vs 4.1) and patient-centeredness (0.8 vs 3.5)
- 49% of AI-generated summaries contained zero patient-centered domains
- Hallucinations affected correctness scores, with models inventing medications and procedures

## Why This Works (Mechanism)

### Mechanism 1
A mixed-methods framework (PPIE) translates abstract patient values into concrete annotation guidelines, creating a "Gold Standard" that captures non-biological priorities often missed in standard clinical notes. Semi-structured interviews identify thematic domains (e.g., lifestyle, social support) which are operationalized into guidelines, then used by clinicians to generate reference summaries containing both biomedical facts and patient values. The assumption is that priorities from a small PPIE group (18 participants) generalize to the broader population and can be consistently applied by annotators.

### Mechanism 2
In-context learning via few-shot prompting improves semantic alignment (BERTScore) and lexical overlap (ROUGE-L) for open-source LLMs compared to zero-shot prompting. Providing 1–3 examples restricts the model's output distribution space, helping it infer the specific "patient-centered" style and structure required, rather than relying solely on pre-training bias toward clinical documentation. The assumption is that improvement reflects genuine alignment with the PCS framework, not just superficial formatting mimicry.

### Mechanism 3
General-purpose open-source LLMs fail to capture patient-centeredness because their training distribution prioritizes biomedical/clinical utility over personal values, leading to the omission of low-probability "lifestyle/meaning" tokens. LLMs predict tokens based on pre-training data (internet text, clinical notes). Since standard clinical notes rarely document "sources of meaning" or "social support," the model assigns low probability to these concepts, resulting in high rates of omission (49% of summaries had zero patient-centered domains). The failure is intrinsic to the model weights/training data bias, not solely a limitation of prompt engineering.

## Foundational Learning

- **Concept: Patient and Public Involvement (PPIE)**
  - Why needed: This is the foundational step that defines the "Gold Standard." Without understanding that PPIE bridges the gap between "what doctors write" and "what patients value," the evaluation criteria (PCS) make no sense.
  - Quick check: Can you distinguish between a summary that is "biologically accurate" vs. one that is "patient-centered"?

- **Concept: Hallucination vs. Omission in Clinical NLP**
  - Why needed: The paper highlights two distinct failure modes. Hallucination affects "Correctness" (inventing meds), while omission affects "Patient-Centeredness" (ignoring lifestyle).
  - Quick check: If a model invents a medication not in the transcript, which evaluation domain is penalized?

- **Concept: Zero-shot vs. Few-shot Prompting**
  - Why needed: Understanding this distinction is critical to interpreting the results. The study shows that while few-shot helps, it does not close the gap with human performance.
  - Quick check: Does providing examples in a prompt (few-shot) guarantee the model will adopt the *values* expressed in those examples, or just the format?

## Architecture Onboarding

- **Component map:** Raw Transcripts → PPIE Interviews → Annotation Guidelines → Gold Standard PCS → [Transcript + Prompt] → Open-Source LLM → Generated Summary → Evaluation (Quantitative Metrics + Qualitative Human Review)

- **Critical path:** The **Definition Layer**. If the annotation guidelines derived from PPIE are flawed or inconsistent, the "Gold Standard" is noisy, making the evaluation of LLMs unreliable.

- **Design tradeoffs:**
  - Privacy vs. Performance: The study prioritized **privacy** by using open-source models (Llama/Mistral) to avoid sending sensitive data to external APIs, trading off the potentially higher performance of proprietary models (e.g., GPT-4).
  - Qualitative Depth vs. Scale: The study uses a small, curated dataset (N=72) to enable deep, blinded human evaluation, sacrificing the statistical power of larger automated benchmarks.

- **Failure signatures:**
  - Hallucination: Model includes specific medical details (e.g., "inventing medications") not found in the source transcript (Scored -3 to -5 on Correctness).
  - Semantic Drift: Summary is fluent and grammatically correct but contains zero "patient-centered" domains (lifestyle, values), focusing entirely on biomedical issues.
  - Metric Disconnection: High BERTScore (semantic similarity) but low human rating for "Patient-Centeredness," indicating the model captured the *topic* but missed the *nuance*.

- **First 3 experiments:**
  1. **Framework Validation:** Validate the 5-domain PCS framework on a different clinical condition (e.g., diabetes or oncology) to test generalizability beyond Atrial Fibrillation.
  2. **Fine-tuning vs. Prompting:** Fine-tune a smaller model (e.g., Llama-3.2-3B) on the 88 Gold Standard PCS to see if "patient-centeredness" can be learned via weight updates rather than just in-context learning.
  3. **Proprietary Gap Analysis:** Compare the best open-source performer (Llama-3.1-8B) against a leading proprietary model (e.g., GPT-4o) on the same 72 transcripts to quantify the "privacy premium" (performance loss from using local models).

## Open Questions the Paper Calls Out

- **Question:** Does the UK-derived patient-centered framework require modification to accurately capture patient values in non-NHS healthcare contexts?
  - Basis: The authors state that "prioritization of patient concerns can be shaped by cultural and systemic contexts" and suggest "future research should focus on validating and adapting this framework across diverse settings."
  - Why unresolved: The framework was defined using UK participants but tested on US data; the potential mismatch in priorities (e.g., financial stressors in the US vs. UK) was not empirically tested.
  - What evidence would resolve it: A comparative study identifying distinct value domains in US-based Patient and Public Involvement groups and testing whether the current model misses US-specific patient-centered themes.

- **Question:** Can fine-tuning open-source LLMs on Patient-Centered Summary (PCS) data reduce hallucination rates and improve the capture of nuanced patient values?
  - Basis: The authors identify the "exclusion of... the use of fine-tuning" as a limitation and attribute poor performance to models being trained on general-purpose data rather than "personal, emotional, and value-based content."
  - Why unresolved: It is unknown if PCS-specific training data can overcome the "biologically centered" bias inherent in general LLMs observed in the study.
  - What evidence would resolve it: Comparing zero-shot performance against a model fine-tuned on the gold-standard PCS to measure improvements in correctness and patient-centeredness scores.

- **Question:** Does involving patients directly in the annotation loop result in summaries that are rated higher in patient-centeredness than clinician-only annotations?
  - Basis: The authors state that "an ideal standard would involve... obtaining direct patient feedback to verify whether the PCS accurately reflect their priorities and experiences."
  - Why unresolved: The gold-standard summaries were generated by clinicians interpreting patient needs, potentially missing nuances that only the patient themselves would validate.
  - What evidence would resolve it: A randomized trial comparing clinician-only annotated summaries against those reviewed and corrected by the corresponding patient.

## Limitations
- The 5-domain PCS framework was derived from a UK-based PPIE group (N=18) focused on atrial fibrillation, limiting generalizability to other conditions and cultural contexts.
- The 72 transcripts used for evaluation may not capture the full spectrum of patient-clinician interactions, particularly those involving diverse patient populations or more complex clinical scenarios.
- While the 5-domain qualitative rubric is comprehensive, it does not explicitly measure for factual hallucinations beyond the "Correctness" domain, which could underestimate safety risks.

## Confidence
- **High Confidence:** The mechanism that few-shot prompting improves LLM performance on this task is well-supported by the empirical results (ROUGE-L and BERTScore improvements across all models).
- **Medium Confidence:** The claim that open-source LLMs fail to capture patient-centeredness due to training data bias is plausible but relies on inference from the omission patterns rather than direct evidence of the training corpus.
- **Low Confidence:** The generalizability of the PCS framework to other clinical domains and populations is stated as a future direction but is not validated in this study.

## Next Checks
1. **Cross-Condition Framework Validation:** Apply the 5-domain PCS framework to a different clinical condition (e.g., diabetes or oncology) and assess inter-annotator agreement and patient acceptance.
2. **Fine-tuning Experiment:** Fine-tune a smaller open-source model (e.g., Llama-3.2-3B) on the 88 Gold Standard PCS to determine if "patient-centeredness" can be learned through weight updates rather than just in-context learning.
3. **Proprietary Model Benchmark:** Compare the best open-source performer (Llama-3.1-8B) against a leading proprietary model (e.g., GPT-4o) on the same 72 transcripts to quantify the performance gap and the "privacy premium."