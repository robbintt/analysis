---
ver: rpa2
title: Comparative Analysis of OpenAI GPT-4o and DeepSeek R1 for Scientific Text Categorization
  Using Prompt Engineering
arxiv_id: '2503.02032'
source_url: https://arxiv.org/abs/2503.02032
tags:
- deepseek
- relationship
- gpt-4o
- categories
- scientific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares GPT-4o and DeepSeek R1 for scientific text
  categorization using prompt engineering. We compile a dataset of 10 cleaned scientific
  articles and introduce a 17-category relationship taxonomy.
---

# Comparative Analysis of OpenAI GPT-4o and DeepSeek R1 for Scientific Text Categorization Using Prompt Engineering

## Quick Facts
- arXiv ID: 2503.02032
- Source URL: https://arxiv.org/abs/2503.02032
- Reference count: 12
- Key outcome: GPT-4o categorized 1654/1823 sentences, DeepSeek R1 categorized 1738/1823, with 44.71% category agreement between models

## Executive Summary
This study compares GPT-4o and DeepSeek R1 for sentence-level classification of scientific text into 17 semantic relationship categories using prompt engineering. The authors compile a dataset of 10 cleaned scientific articles and introduce a taxonomy for categorizing sentences based on relationships between entities. While DeepSeek R1 achieved broader coverage and better handled mathematical symbols, both models showed significant differences in classification patterns, with highest agreement in Representation & Symbol (85.71%) and Limitation & Restriction (85.11%), and lowest in Time-Based Relationship (13.04%).

## Method Summary
The methodology processes 10 arXiv papers (1823 sentences) through both GPT-4o and DeepSeek R1 using paragraph-level prompting. Each model receives a structured prompt containing 17 relationship categories with definitions and examples, then categorizes sentences while extracting Entity A and Entity B. Output parsing uses regex and fuzzy matching to align results between models. The study compares coverage rates and agreement metrics across categories, Entity A, and Entity B dimensions.

## Key Results
- GPT-4o categorized 1654 sentences, DeepSeek R1 categorized 1738 sentences (84 more)
- Category agreement between models was 44.71%
- Highest agreement occurred in Representation & Symbol (85.71%) and Limitation & Restriction (85.11%)
- Entity A agreement was 37.36%, while Entity B agreement was only 22.44%
- DeepSeek R1 handled mathematical symbols better and provided more complete coverage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Paragraph-level batch processing preserves contextual coherence while reducing API overhead in scientific text categorization.
- Mechanism: Processing complete paragraphs instead of isolated sentences maintains surrounding context that disambiguates entity relationships. The prompt includes all 17 categories with definitions and examples once per paragraph, reducing token costs while keeping logical flow intact.
- Core assumption: Sentences within scientific paragraphs contain interdependent semantic information that improves categorization accuracy.
- Evidence anchors:
  - [section 4.2]: "processing one paragraph at a time... required for logical flow and contextual coherence... significantly reduces the total number of API calls while maintaining accuracy"
  - [corpus]: AutoP2C paper (arXiv:2504.20115) similarly processes multimodal academic content with context awareness.
- Break condition: If sentences are highly self-contained or paragraphs contain unrelated multi-topic content, batch processing may introduce noise rather than signal.

### Mechanism 2
- Claim: Category semantic distinctness predicts inter-model agreement rates.
- Mechanism: Categories with clearer semantic boundaries (e.g., "Representation & Symbol" at 85.71% agreement) have fewer overlaps with alternatives, reducing classification ambiguity. Categories involving temporal or causal reasoning show lower agreement because they require more interpretive judgment.
- Core assumption: The 17-category taxonomy is sufficiently non-overlapping for some categories but not others.
- Evidence anchors:
  - [section 5.4]: "highest agreement occurs in Representation & Symbol (85.71%) and Limitation & Restriction (85.11%)... Cause & Effect (35.20%), Time-Based Relationship (13.04%) exhibit low agreement"
  - [corpus]: No direct corpus evidence on category distinctness; related work on semantic taxonomies (Maia and Lima) cited but not empirically validated for LLM agreement.
- Break condition: If categories are refined with hierarchical structuring (as suggested in Conclusion), agreement patterns may shift substantially.

### Mechanism 3
- Claim: DeepSeek R1's reinforcement learning training may produce higher coverage but divergent classification heuristics compared to GPT-4o.
- Mechanism: DeepSeek R1 categorized 84 more sentences than GPT-4o and handled mathematical symbols better, suggesting different uncertainty thresholds. However, it applies broader categories (e.g., Category & Type instead of Part-Whole) and classifies causal relationships as Interaction & Influence.
- Core assumption: RL-based reasoning training produces different classification boundary decisions than GPT-4o's training approach.
- Evidence anchors:
  - [section 5.2]: "DeepSeek R1 provided more complete coverage, categorizing 84 more sentences... more capable of assigning categories to sentences containing mathematical symbols"
  - [section 5.4]: "GPT-4o frequently assigns Interaction & Influence, while DeepSeek R1 tends to classify similar cases as Formation & Emergence"
  - [corpus]: DeepSeek's WEIRD Behavior paper (arXiv:2512.09772) examines cultural alignment differences in DeepSeek, supporting divergent training hypothesis.
- Break condition: If prompts are optimized specifically for each model's tendencies, agreement rates may improve but at the cost of comparability.

## Foundational Learning

- **Semantic Relationship Taxonomies**:
  - Why needed here: The study relies on a 17-category relationship schema derived from prior work (Maia and Lima). Understanding how Part-Whole, Cause-Effect, and Interaction categories differ is essential for interpreting disagreement patterns.
  - Quick check question: Given the sentence "Budget constraints limit research progress," which two relationship categories might plausibly apply, and why might models disagree?

- **Prompt Engineering for Structured Output**:
  - Why needed here: The classification task requires consistent JSON-formatted output with Sentence, Category, Entity A, and Entity B. Output parsing failed when models added formatting artifacts.
  - Quick check question: What prompt constraints would reduce "fluff text" and inconsistent formatting in model responses?

- **Fuzzy Matching for Entity Alignment**:
  - Why needed here: Entity B agreement was only 22.44%, and fuzzy matching was required to align sentences with minor phrasing differences between model outputs.
  - Quick check question: If GPT-4o extracts "COVID-19" as Entity A and DeepSeek extracts "data gathered during COVID-19," what fuzzy matching threshold would consider these equivalent?

## Architecture Onboarding

- **Component map**:
  Data Collection -> Preprocessing -> Prompt Construction -> Model Inference -> Output Parsing -> Alignment -> Analysis

- **Critical path**: Output parsing → entity alignment. Formatting inconsistencies (GPT-4o's `***`, `—`, `**` symbols; DeepSeek's retained citations) required custom regex rules. Without clean parsing, agreement metrics are unreliable.

- **Design tradeoffs**:
  - Paragraph vs. sentence processing: Reduces API calls but may conflate unrelated sentences within paragraphs.
  - 17 vs. fewer categories: Higher granularity captures more relationship types but increases inter-category confusion (Cause & Effect vs. Interaction & Influence).
  - Single-prompt vs. multi-stage classification: Current approach includes all categories in one prompt; hierarchical prompting might improve precision.

- **Failure signatures**:
  - Low Entity B agreement (22.44%): Models struggle with implicit entities, multi-clause sentences, or domain-specific terms.
  - "N/A" assignments: GPT-4o left 169 sentences uncategorized vs. DeepSeek's 85.
  - Format drift: Numbered vs. compact outputs required post-hoc normalization.

- **First 3 experiments**:
  1. **Category reduction test**: Collapse overlapping categories (e.g., merge Cause & Effect with Interaction & Influence) and measure agreement improvement.
  2. **Mathematical symbol handling**: Create a held-out test set of sentences with LaTeX notation; compare categorization rates with/without equation removal.
  3. **Model-specific prompt tuning**: Optimize separate prompts for GPT-4o and DeepSeek R1 based on observed classification biases, then re-measure agreement to distinguish prompt vs. model effects.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would scaling to a substantially larger dataset produce significantly different inter-model agreement rates, or is the ~45% category agreement consistent across scientific domains?
- Basis in paper: [explicit] "a broader study with an expanded dataset is necessary to draw more definitive conclusions"
- Why unresolved: The study used only 10 curated papers; generalizability to broader scientific literature remains unknown
- What evidence would resolve it: Running identical methodology on 100+ papers across more domains and comparing agreement statistics

### Open Question 2
- Question: Would hierarchical structuring or refinement of the 17-category relationship taxonomy improve inter-model agreement, particularly for low-agreement categories like Time-Based (13.04%) and Ownership & Control (23.33%)?
- Basis in paper: [explicit] "Some categories showed frequent misclassification, suggesting that adjustments or hierarchical structuring could improve consistency"
- Why unresolved: Current flat taxonomy produced low agreement in several categories; unclear whether definitions, overlaps, or structure cause misclassification
- What evidence would resolve it: Testing modified or hierarchical taxonomies with both models and measuring agreement changes per category

### Open Question 3
- Question: Which model's categorizations align more closely with human expert judgments on scientific text classification?
- Basis in paper: [explicit] "A more comprehensive evaluation, involving multiple experts assigning scores to model responses, would provide a deeper understanding"
- Why unresolved: Human evaluation was "conducted in a limited manner, primarily to verify major discrepancies"
- What evidence would resolve it: Multiple domain experts independently categorizing sentences with inter-rater reliability metrics, compared against model outputs

### Open Question 4
- Question: What specific linguistic or structural features predict the Entity B extraction disagreement (only 22.44% agreement)?
- Basis in paper: [inferred] "A deeper analysis is required to find the specific cases where misalignment occurs most often"
- Why unresolved: The paper documents the low Entity B agreement but does not systematically characterize what causes models to extract different secondary entities
- What evidence would resolve it: Correlating Entity B disagreement with sentence features such as clause count, implicit references, passive voice, and domain-specific terminology

## Limitations
- Small sample size (10 papers, 1823 sentences) limits generalizability
- No ground truth labels—all agreement metrics are relative to model outputs
- 17-category taxonomy lacks empirical validation for distinctness
- Web interface usage introduces unknown API parameter variability

## Confidence
- **High confidence**: Comparative coverage metrics and basic agreement calculations
- **Medium confidence**: Claims about DeepSeek R1's superior handling of mathematical symbols
- **Low confidence**: Causal claims about category distinctness and RL training effects

## Next Checks
1. **Ground truth establishment**: Manually label a subset (50-100 sentences) to establish reference categories, then measure model accuracy against human judgment rather than inter-model agreement.
2. **Category refinement experiment**: Test hierarchical prompting where ambiguous categories (Cause-Effect vs. Interaction-Influence) are first distinguished by broader relationship types before fine-grained classification.
3. **Prompt parameter sensitivity**: Systematically vary temperature, max tokens, and system prompt for both models while keeping category definitions constant to isolate model vs. configuration effects on agreement rates.