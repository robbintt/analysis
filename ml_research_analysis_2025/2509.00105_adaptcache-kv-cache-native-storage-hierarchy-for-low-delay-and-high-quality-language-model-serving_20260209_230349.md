---
ver: rpa2
title: 'AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and High-Quality
  Language Model Serving'
arxiv_id: '2509.00105'
source_url: https://arxiv.org/abs/2509.00105
tags:
- cache
- compression
- quality
- adaptcache
- storage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AdaptCache introduces a lossy KV cache compression system for LLM
  serving that adaptively chooses compression algorithms, rates, and storage placement
  to maximize DRAM hits and minimize loading delays while maintaining generation quality.
  It uses a marginal utility gain framework to decide optimal compression per KV cache
  entry, addressing the challenge of large KV caches exceeding memory capacity when
  serving multi-turn conversations.
---

# AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and High-Quality Language Model Serving

## Quick Facts
- arXiv ID: 2509.00105
- Source URL: https://arxiv.org/abs/2509.00105
- Reference count: 16
- Key outcome: AdaptCache achieves 1.43-2.4× faster TTFT and 6-55% higher quality vs static compression across summarization, QA, and coding tasks using Llama-3.1-8B-Instruct

## Executive Summary
AdaptCache addresses the challenge of large KV caches exceeding memory capacity in multi-turn conversations by introducing a lossy compression system that adaptively selects compression algorithms, rates, and storage placement. The system maximizes DRAM hits while minimizing loading delays and maintaining generation quality through a marginal utility gain framework. This approach enables efficient LLM serving by optimizing the KV cache storage hierarchy based on access patterns and quality requirements.

## Method Summary
AdaptCache implements a KV cache compression system that dynamically chooses compression algorithms and storage placement for each cache entry. The system uses a marginal utility gain framework to evaluate the trade-off between compression quality loss and memory savings, making decisions based on access frequency and quality impact. The approach involves monitoring KV cache access patterns, applying adaptive compression algorithms (including quantization and pruning), and placing compressed entries in appropriate memory tiers to optimize DRAM hit rates.

## Key Results
- Achieves 1.43-2.4× faster time-to-first-token (TTFT) compared to static compression baselines
- Delivers 6-55% higher quality across summarization, QA, and coding tasks
- Maintains generation quality within 5% degradation threshold while maximizing DRAM hits

## Why This Works (Mechanism)
The system works by treating KV cache compression as an optimization problem where each entry's placement and compression level is determined by its marginal utility gain. By analyzing access patterns and quality impact, AdaptCache can make informed decisions about which entries to compress more aggressively and where to store them. The marginal utility gain framework quantifies the trade-off between memory savings and quality loss, enabling data-driven compression decisions that optimize for both speed and quality.

## Foundational Learning

**KV Cache Compression**: Needed because KV caches grow linearly with sequence length, often exceeding memory capacity. Quick check: Understand how transformer attention mechanisms generate and use KV caches.

**Marginal Utility Gain Framework**: Required to make optimal compression decisions by quantifying quality vs. memory trade-offs. Quick check: Verify understanding of utility functions and optimization in caching systems.

**Storage Hierarchy Management**: Essential for balancing DRAM access speed with capacity constraints. Quick check: Know the performance characteristics of different memory tiers (DRAM, SSD, etc.).

## Architecture Onboarding

**Component Map**: Request Handler -> KV Cache Monitor -> Compression Decision Engine -> Storage Tiers -> Model Inference

**Critical Path**: Input request → KV cache access → compression decision → compressed data retrieval → model inference → output generation

**Design Tradeoffs**: The system trades computational overhead for memory efficiency and speed. More sophisticated compression algorithms provide better compression ratios but require more processing power. The marginal utility framework adds decision-making overhead but enables optimal cache management.

**Failure Signatures**: High DRAM miss rates indicate poor compression decisions or insufficient memory capacity. Quality degradation beyond acceptable thresholds suggests overly aggressive compression. Increased TTFT points to inefficient storage placement or compression algorithm selection.

**First 3 Experiments**:
1. Measure baseline KV cache size and access patterns for target workload
2. Test individual compression algorithms (quantization, pruning) on representative KV cache entries
3. Evaluate marginal utility gain calculations for different compression levels and storage placements

## Open Questions the Paper Calls Out
None

## Limitations
- Adaptation framework effectiveness for larger models (70B+) and different architectures remains uncertain
- Quality degradation thresholds may not generalize across all domains and tasks
- Computational overhead and energy efficiency impact not fully characterized, particularly for edge deployments

## Confidence
**High Confidence**: TTFT improvements and quality gains are well-supported by controlled experiments
**Medium Confidence**: Marginal utility gain framework is theoretically sound but depends on accurate utility estimation
**Low Confidence**: Claims about extreme memory pressure scenarios and larger model performance lack empirical validation

## Next Checks
1. Test AdaptCache with diverse model sizes (7B, 34B, 70B) and architectures to validate scalability
2. Evaluate system performance under varying request patterns and memory constraints
3. Measure computational overhead and energy consumption of the adaptation framework, especially for edge devices