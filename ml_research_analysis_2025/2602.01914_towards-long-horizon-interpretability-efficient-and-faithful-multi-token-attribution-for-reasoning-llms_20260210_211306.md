---
ver: rpa2
title: 'Towards Long-Horizon Interpretability: Efficient and Faithful Multi-Token
  Attribution for Reasoning LLMs'
arxiv_id: '2602.01914'
source_url: https://arxiv.org/abs/2602.01914
tags:
- attribution
- reasoning
- tokens
- input
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of attributing importance to
  input tokens for long and multi-token outputs generated by reasoning language models.
  The key issues are efficiency bottlenecks when explaining multi-token targets and
  faithfulness degradation as intermediate reasoning tokens absorb attribution mass.
---

# Towards Long-Horizon Interpretability: Efficient and Faithful Multi-Token Attribution for Reasoning LLMs

## Quick Facts
- **arXiv ID**: 2602.01914
- **Source URL**: https://arxiv.org/abs/2602.01914
- **Authors**: Wenbo Pan, Zhichao Liu, Xianlong Wang, Haining Yu, Xiaohua Jia
- **Reference count**: 40
- **Key outcome**: FlashTrace achieves over 130x speedup versus baselines while maintaining superior faithfulness across long-context retrieval, mathematical reasoning, and multi-hop QA tasks, with a single recursive hop redistributing attribution mass from intermediate reasoning tokens back to source inputs.

## Executive Summary
This paper addresses the challenge of attributing importance to input tokens for long and multi-token outputs generated by reasoning language models. The key issues are efficiency bottlenecks when explaining multi-token targets and faithfulness degradation as intermediate reasoning tokens absorb attribution mass. The proposed method, FlashTrace, introduces span-wise aggregation to efficiently attribute importance to multi-token spans in a single pass and employs recursive attribution to trace importance through reasoning chains back to source inputs. Experiments demonstrate substantial efficiency gains and improved faithfulness metrics across multiple reasoning tasks.

## Method Summary
FlashTrace builds on the ALTI/IFR proximity framework to solve two problems in multi-token attribution for reasoning LLMs: efficiency bottlenecks and faithfulness degradation. The method employs span-wise aggregation to compute attribution over multi-token targets in a single pass using L1 proximity: Prox(c,t) = max(0, -||t-c||₁ + ||t||₁). This pre-aggregates attention weights over target spans, reducing complexity from O(M·N·D) to O(N·D). Recursive attribution traces output→reasoning→input across K hops (K=1 default), with final aggregation weighted by flow ratios ρ_k representing the fraction of importance remaining in reasoning tokens. The approach is evaluated on RULER benchmark tasks with Qwen-3 8B Instruct and LLaMA-3.1-8B-It models.

## Key Results
- FlashTrace achieves over 130x speedup compared to IFR/AttnLRP baselines on efficiency benchmarks
- Single-hop recursive attribution significantly improves faithfulness by redistributing importance from reasoning tokens back to input tokens
- The method maintains superior performance across long-context retrieval (RULER NIAH), mathematical reasoning (MATH), and multi-hop QA (MoreHopQA) tasks

## Why This Works (Mechanism)

### Mechanism 1: Span-wise Aggregation for Linear-Time Multi-Token Attribution
The method exploits linearity of attention by pre-aggregating attention weights over target spans. Since transformed value vector v_j depends only on source token j (not target position i), the contribution from token j to span S becomes: C_{j→S} = v_j · Σ_{i∈S} w_i · α_{i,j}. This algebraic reordering allows computing v_j once per source token, reducing complexity from O(M·N·D) to O(N·D).

### Mechanism 2: Recursive Attribution for Recovering Input Importance
Recursive attribution redistributes importance mass from intermediate reasoning tokens back to original input tokens that direct attribution misses. Hop k+1 uses reasoning token scores w^{(k)}_T as weights for a new target span, attributing "why were these reasoning tokens important" backward through earlier context. Final importance aggregates across hops with decay factors based on reasoning mass ratios.

### Mechanism 3: Proximity-Based Component Decomposition
L1-norm proximity measures component contribution robustly in transformer anisotropic embedding space. For each source component (attention head outputs, residual stream, MLP output), the method computes proximity scores based on magnitude decrease if component removed. Scores normalize per-layer and accumulate across layers.

## Foundational Learning

- **Attention Mechanism Decomposition (QKV, attention weights, output projection)**: Why needed here - span-wise aggregation requires understanding that v_j = x_j W_V W_O is target-independent, enabling the key optimization. Quick check: For a single attention head, given attention matrix A and value projection W_V, can you write the expression for the contribution of token j to token i?

- **Attribution Faithfulness vs. Plausibility**: Why needed here - the paper targets faithfulness (attribution reflects true causality) not plausibility (attribution looks reasonable). Understanding perturbation metrics (RISE, MAS) is essential to evaluate claims. Quick check: If an attribution method highlights tokens that "look right" to humans but don't affect model output when removed, is it faithful? Is it plausible?

- **Recursive/Chain Attribution**: Why needed here - the core innovation is treating attribution as a flow problem through intermediate reasoning states, not direct input-output mapping. Quick check: Why does standard attribution fail when a model answers based on intermediate reasoning rather than directly on input?

## Architecture Onboarding

- **Component map**: Forward pass cache -> Span-wise attribution module -> Recursive controller -> Final aggregator
- **Critical path**: Forward pass → Hop 0 (output → context) → Extract reasoning weights → Hop k (weighted reasoning → context) → Aggregate across hops
- **Design tradeoffs**: Hop count K (K=1 default for efficiency), attention cache storage vs. recomputation, token-level vs. sentence-level span granularity
- **Failure signatures**: OOM on long contexts with full attention cache, attribution mass trapped in reasoning tokens, near-zero or exploding proximity scores
- **First 3 experiments**: 1) Reproduce efficiency curve (Figure 4a) comparing FlashTrace vs. IFR/AttnLRP on RULER samples with varying input length, 2) Single-hop ablation (Table 7) comparing K=0 vs. K=1 on MoreHopQA, 3) Recovery rate sanity check (Table 1, NIAH mq_q2) on 10 RULER NIAH samples

## Open Questions the Paper Calls Out

- **Open Question 1**: How does FlashTrace perform on hallucinated or logically flawed reasoning chains, given the current methodology explicitly filters for correct model outputs? The paper filters incorrect predictions as "reasoning" noise, but interpretability is most needed for debugging failures.

- **Open Question 2**: How can noise accumulation in deeper recursive hops be mitigated to enable faithful attribution through very long reasoning chains? Appendix H notes diminishing returns with higher K likely due to noise accumulation, restricting recursion depth to K=1 or K=2.

- **Open Question 3**: Does the empirical finding that "reasoning chain dependencies are often resolved within one step" generalize to the dense, multi-thousand-token reasoning traces produced by frontier models? The paper's evaluation uses reasoning traces of limited length relative to the "thousands" described in the motivation.

## Limitations
- The 130x speedup benchmark is specific to NVIDIA RTX 4090 hardware with particular sequence lengths
- Faithfulness evaluation relies on RISE and MAS perturbation metrics which can be sensitive to implementation details
- Recursive attribution assumes linear importance flow through reasoning chains, which may not hold for all reasoning patterns
- Performance claims are limited to retrieval, mathematical reasoning, and multi-hop QA tasks without testing on fundamentally different reasoning structures

## Confidence

**High confidence** (mechanism validated through controlled experiments and ablation):
- Span-wise aggregation reduces multi-token attribution complexity from O(M·N·D) to O(N·D)
- Recursive attribution redistributes importance from reasoning tokens back to input tokens
- Single-hop recursion (K=1) provides substantial faithfulness improvement without significant noise

**Medium confidence** (claims supported by experiments but with measurement uncertainties):
- 130x speedup claim relative to IFR/AttnLRP baselines
- 0.2 MAS reduction represents "substantial improvement" in faithfulness
- Performance improvements generalize across the three task categories tested

**Low confidence** (claims with limited experimental support or unexplored assumptions):
- Recursive attribution effectiveness beyond K=1 hops (Table 7 shows slight degradation)
- Performance on tasks with fundamentally different reasoning structures
- Memory-complexity tradeoff of attention caching vs. recomputation

## Next Checks

1. **Controlled ablation of recursive hops**: Run the MoreHopQA experiment with K=0, K=1, and K=2 hops on identical samples, measuring both MAS faithfulness and ρ flow ratios at each hop.

2. **Efficiency benchmarking with attention cache disabled**: Reproduce the speed comparison (Figure 4a) with and without pre-computed attention maps, measuring actual wall-clock time and memory usage across sequence lengths 1k-16k tokens.

3. **Faithfulness sensitivity to perturbation parameters**: Run RISE and MAS evaluations on RULER NIAH with varying perturbation schedules (5%, 10%, 20% per step) and step counts (10, 20, 30 steps).