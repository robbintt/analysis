---
ver: rpa2
title: A High-Capacity and Secure Disambiguation Algorithm for Neural Linguistic Steganography
arxiv_id: '2510.02332'
source_url: https://arxiv.org/abs/2510.02332
tags:
- capacity
- look-ahead
- security
- sync
- secure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of tokenization ambiguity in
  neural linguistic steganography, which can lead to catastrophic decoding failures.
  The authors propose a method called look-ahead Sync that resolves ambiguity while
  preserving statistical undetectability.
---

# A High-Capacity and Secure Disambiguation Algorithm for Neural Linguistic Steganography

## Quick Facts
- arXiv ID: 2510.02332
- Source URL: https://arxiv.org/abs/2510.02332
- Reference count: 34
- Primary result: Look-ahead Sync resolves tokenization ambiguity with zero-KL security while improving embedding capacity by >160% in English and 25% in Chinese over SyncPool

## Executive Summary
This paper addresses tokenization ambiguity in neural linguistic steganography, where subword tokenizers allow one visible string to map to multiple token sequences, breaking sender-receiver synchronization. The authors propose look-ahead Sync, which performs minimal synchronized sampling only on truly indistinguishable token sequences while strategically preserving all other discernible paths to maximize embedding capacity. Experiments on Llama 3 and Qwen 2.5 show consistent approach to theoretical capacity upper bounds with significant improvements over state-of-the-art methods.

## Method Summary
The method implements a three-phase iterative loop: (1) PARTITIONBYPREFIX groups candidates by visible prefix; (2) Inter-group entropy encoding via iMEC/Discop selects one pool; (3) LOOKAHEAD resolves intra-pool ambiguity via synchronized sampling from prefix set plus single LLM forward pass. The synchronized sampling uses a CSPRNG seeded with shared key K. The algorithm partitions candidates into "Prefix Set" (exact matches to visible string) and "Partial Set" (longer continuations), preserving partials while only synchronizing the prefix subset.

## Key Results
- Look-ahead Sync consistently approaches theoretical capacity upper bound defined by visible string distribution
- Outperforms SyncPool method by over 160% in English and 25% in Chinese benchmarks
- Maintains zero-KL security with KL divergence scores of 0.00
- Shows low Jensen-Shannon Divergence (JSD) indicating minimal synchronization loss

## Why This Works (Mechanism)

### Mechanism 1
If the algorithm preserves "partial" candidates (longer sequences sharing a visible prefix) rather than synchronizing the entire ambiguous pool, embedding capacity significantly increases compared to coarse-grained synchronization. During look-ahead resolution, the algorithm partitions the selected intra-group candidates into a "Prefix Set" (exact matches to visible string) and a "Partial Set" (longer continuations). It carries the Partial Set forward to the next state unchanged, preserving their probability mass for future embedding, while only synchronizing the Prefix Set. The entropy contained in these partial continuations is recoverable in subsequent steps without compromising the statistical distribution.

### Mechanism 2
If a synchronized sampler (CSPRNG) is restricted to selecting from only the "Prefix Set," the system maintains computational zero-KL security while minimizing the entropy consumed for synchronization. A shared secret key K seeds a CSPRNG which selects a representative s_sync from the indistinguishable prefix candidates. This ensures the sender and receiver generate identical contexts for the next LLM forward pass without embedding payload bits in this specific choice, thereby preserving the original probability distribution P_t. The output of the CSPRNG is computationally indistinguishable from true randomness to a PPT adversary.

### Mechanism 3
If the algorithm recursively expands the synchronized representative, it resolves ambiguity iteratively, approaching the theoretical capacity upper bound defined by the visible string distribution. After selecting s_sync, the algorithm performs a forward LLM pass to generate new candidates. These are merged with the preserved partials to form the new state. This repeats, "unrolling" the ambiguity only when necessary, aligning the final visible string distribution P_embed with the base model P_Vend. The "Linguistic Smoothness Hypothesis" holds—i.e., semantically equivalent token sequences yield statistically similar future conditional distributions, making the synchronization loss (mutual information) negligible.

## Foundational Learning

**Tokenization Ambiguity (Non-injective Detokenization)**
- Why needed here: This is the fundamental problem the paper solves. Subword tokenizers allow one visible string ("mistrust") to map to multiple token sequences ([mistrust] vs [mis, trust]), which breaks the synchronization between sender and receiver in autoregressive generation.
- Quick check question: If a receiver re-tokenizes a steganographic text and gets a different token sequence than the sender, why does decoding fail?

**Computational Zero-KL Security**
- Why needed here: This is the security standard. It requires the stegotext distribution to be indistinguishable from the covertext distribution to any computationally bounded adversary, ensuring no statistical artifacts reveal the message.
- Quick check question: Why does "pruning" ambiguous tokens (as done in MWIS) inherently violate the Zero-KL requirement?

**Entropy Coding (Arithmetic/Huffman)**
- Why needed here: The "Inter-Group Entropy Coding" step uses this to map secret bits to candidate pools based on probability mass. Understanding how bit consumption relates to Shannon entropy is vital for grasping capacity bounds.
- Quick check question: How does the size of the candidate pool (k) influence the efficiency of entropy coding in this architecture?

## Architecture Onboarding

**Component map:**
LLM -> Partitioning Module -> Entropy Encoder -> Sync Sampler -> State Manager

**Critical path:**
1. PARTITION: Group raw tokens by visible string
2. EMBED: Use Entropy Encoder to pick a group (payload embedded here)
3. RESOLVE: Check selected group for internal ambiguity
4. SYNC/LOOK-AHEAD: If ambiguous, sync sample the prefix -> LLM call -> merge with partials
5. LOOP: Update state; repeat until termination

**Design tradeoffs:**
- Capacity vs. Computation: The "Look-ahead" requires additional LLM forward passes (Tok/Call < 1.0). You trade generation speed (inference cost) for higher BPT (capacity)
- Security vs. Complexity: Maintaining strict zero-KL requires the overhead of the CSPRNG and state management, which is more complex than simple pruning but provably secure

**Failure signatures:**
- Desynchronization: If the receiver's tokenizer version differs from the sender's, or if the partial set is incorrectly pruned, the state (S_t) diverges, leading to catastrophic decoding failure (garbage output)
- Capacity Drop: If the top-k setting is too low, ambiguity pools shrink, and the overhead of the sync mechanism may not justify the capacity gain relative to baseline

**First 3 experiments:**
1. Verify Security (KL Divergence): Run the pipeline with random payloads and compare the output distribution against the raw model distribution. Confirm KL ≈ 0 (or effectively 0.00 as per Table I/II)
2. Capacity Benchmarking: Compare BPT of Look-ahead Sync vs. SyncPool across different top-k values (16, 32, 128). Verify the >160% improvement claim on English text
3. JSD Analysis: Isolate instances of tokenization ambiguity (e.g., [mis, trust] vs [mistrust]) and measure the Jensen-Shannon Divergence of their next-token predictions to validate the "Linguistic Smoothness Hypothesis"

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can adaptive look-ahead strategies that dynamically trigger expansion based on prefix-set entropy heuristics further close the gap to the theoretical capacity upper bound?
- Basis in paper: The Discussion section states: "A clear avenue for future research to mitigate this loss lies in the development of adaptive look-ahead strategies. A more sophisticated implementation could employ a heuristic, such as the entropy of the prefix set, H(Sprefix), to dynamically guide its operations."
- Why unresolved: The current Look-ahead Sync algorithm uses a fixed single-path expansion strategy for all ambiguous prefix sets, regardless of potential capacity gain. The authors acknowledge this results in a residual gap to the theoretical capacity limit due to synchronization loss.
- What evidence would resolve it: Experiments comparing fixed versus adaptive look-ahead triggering across varying entropy thresholds, measuring BPT improvement against the theoretical upper bound and computational cost.

**Open Question 2**
- Question: Would multi-path expansion for high-entropy prefix sets yield significant capacity gains, and what is the computational trade-off?
- Basis in paper: The Discussion section proposes: "perform a more resource-intensive multi-path expansion for particularly high-entropy prefix sets" as an approach to close the capacity gap.
- Why unresolved: The current algorithm deliberately selects a single representative path via SyncSample to avoid combinatorial explosion. The information-theoretic cost of this pruning (synchronization loss) is quantified but not experimentally tested against multi-path alternatives.
- What evidence would resolve it: Comparative experiments implementing bounded multi-path expansion for high-entropy cases, reporting capacity gains relative to increased LLM forward passes.

**Open Question 3**
- Question: How robust is the Linguistic Smoothness Hypothesis across diverse model architectures, languages, and domain-specific texts?
- Basis in paper: Section IV-C introduces the "Linguistic Smoothness Hypothesis, which states that a model's semantic understanding yields statistically similar conditional distributions for such semantically equivalent sequences." Empirical validation is limited to Llama3-8b on IMDB and Qwen2.5-7b on Douban with 1,000 ambiguity instances per model.
- Why unresolved: The hypothesis underpins the algorithm's efficiency but is tested on only two models and two datasets. If certain domains or architectures exhibit higher divergence among tokenization-equivalent sequences, synchronization loss would increase.
- What evidence would resolve it: Systematic evaluation of JSD distributions across additional models (e.g., Mistral, GPT), languages with different morphology, and specialized domains (legal, medical, code).

## Limitations

- Model-specific generalization: Performance gains evaluated only on Llama3-8b and Qwen2.5-7b; "Linguistic Smoothness Hypothesis" may not hold uniformly across all model architectures
- Ambiguity frequency dependence: Capacity improvements rely on presence of tokenization ambiguity; computational overhead may not justify benefits in low-ambiguity settings
- CSPRNG security assumptions: Computational zero-KL security proof depends on cryptographically secure CSPRNG, but implementation details remain unspecified

## Confidence

**High confidence**: The core mechanism of partitioning candidates into Prefix Set and Partial Set, then preserving partials while synchronizing only the prefix subset, is well-specified and theoretically sound. The capacity improvements over SyncPool (160% in English, 25% in Chinese) are directly measurable and reproducible given the methodology.

**Medium confidence**: The security proof's reliance on the computational indistinguishability of CSPRNG output is standard cryptography, but practical implementation details remain unspecified. The capacity gains are well-demonstrated but may vary with different ambiguity frequencies across domains.

**Low confidence**: The generalizability of the "Linguistic Smoothness Hypothesis" across different model families and tokenization schemes has not been established. The computational overhead trade-off in low-ambiguity scenarios requires further empirical validation.

## Next Checks

1. **Cross-model validation**: Implement look-ahead Sync on a third, structurally different LLM (e.g., GPT-2 or a model with SentencePiece tokenization) and measure whether the capacity improvements and zero-KL security claims hold consistently.

2. **Ambiguity frequency analysis**: Quantify the frequency of tokenization ambiguity in various corpora (news, technical writing, dialogue) to establish when look-ahead Sync's overhead is justified versus when simpler methods suffice.

3. **Practical security audit**: Implement the complete CSPRNG-based synchronization protocol with explicit key exchange and evaluate vulnerability to known-plaintext and chosen-plaintext attacks in realistic threat models.