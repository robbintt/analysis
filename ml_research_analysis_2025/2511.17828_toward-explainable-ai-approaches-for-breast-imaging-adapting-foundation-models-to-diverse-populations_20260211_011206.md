---
ver: rpa2
title: 'Toward explainable AI approaches for breast imaging: adapting foundation models
  to diverse populations'
arxiv_id: '2511.17828'
source_url: https://arxiv.org/abs/2511.17828
tags:
- breast
- density
- images
- imaging
- multi-modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of automating BI-RADS breast
  density classification using foundation models. BiomedCLIP was fine-tuned using
  multi-modality mammographic data (synthesized 2D, digital mammography, and digital
  breast tomosynthesis) from 96,995 images.
---

# Toward explainable AI approaches for breast imaging: adapting foundation models to diverse populations

## Quick Facts
- arXiv ID: 2511.17828
- Source URL: https://arxiv.org/abs/2511.17828
- Reference count: 9
- Primary result: BiomedCLIP fine-tuned for BI-RADS breast density classification achieves AUC >0.84 across all categories with strong generalization to external datasets

## Executive Summary
This study addresses the challenge of automating BI-RADS breast density classification using foundation models. BiomedCLIP was fine-tuned using multi-modality mammographic data (synthesized 2D, digital mammography, and digital breast tomosynthesis) from 96,995 images. Two approaches were compared: single-modality (s2D only) and multi-modality training. Both achieved similar accuracy (multi-modality: 0.74, single-modality: 0.73), with multi-modality offering broader applicability and consistently higher AUC values above 0.84 across BI-RADS categories. External validation on RSNA and EMBED datasets demonstrated strong generalization (AUC range: 0.80-0.93). GradCAM visualizations confirmed consistent, clinically relevant attention patterns, demonstrating interpretability and robustness to imaging variations.

## Method Summary
The study fine-tuned BiomedCLIP, a biomedical vision-language foundation model pretrained on 15M PubMed Central image-text pairs, for BI-RADS breast density classification. The model was trained using weighted contrastive learning with class weights computed as inverse frequency to address severe imbalance (only 3,000 D-density images available). Training used 19,000 undersampled images from 96,995 total, with patient-level stratification preventing data leakage. Two approaches were compared: single-modality (s2D only) and multi-modality (s2D, DM, DBT). Images were preprocessed to 224×224 pixels with artifact removal and breast region cropping. Text prompts were standardized to four BI-RADS descriptions. GradCAM visualizations were generated for interpretability, and zero-shot classification was performed on external RSNA and EMBED datasets.

## Key Results
- Multi-modality training achieved comparable accuracy to single-modality (0.74 vs 0.73) while offering broader applicability
- Both approaches maintained AUC values above 0.84 across all BI-RADS categories
- External validation on RSNA and EMBED datasets showed strong generalization (AUC range: 0.80-0.93)
- GradCAM visualizations confirmed clinically relevant attention patterns focused on breast tissue rather than artifacts

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Alignment of Vision-Language Embeddings
Weighted contrastive learning enables joint image-text representation learning that maps mammographic images to BI-RADS density categories. BiomedCLIP's dual encoders project inputs to a shared embedding space where matching image-text pairs form positives (cosine similarity maximized) while mismatched pairs serve as negatives (cosine similarity minimized). Class weights computed as inverse frequency address severe imbalance. Core assumption: pretrained BiomedCLIP embeddings contain transferable biomedical features that can be realigned to breast density patterns through domain-specific contrastive fine-tuning.

### Mechanism 2: Multi-modality Training Induces Cross-domain Invariance
Training across multiple acquisition modalities (s2D, DM, DBT) and scanner systems forces the model to learn fundamental tissue density features rather than modality-specific artifacts. Exposure to varied imaging protocols and vendor-specific characteristics develops invariance to acquisition variations, enabling zero-shot generalization to external datasets containing imaging artifacts not seen during training.

### Mechanism 3: GradCAM Attention as Clinical Plausibility Check
Gradient-weighted activation maps provide post-hoc evidence that model predictions rely on clinically relevant tissue regions rather than confounding artifacts. GradCAM computes gradients flowing into the final convolutional layer, weights feature maps by gradient importance, and generates saliency overlays. Fatty breasts show diffuse attention while dense breasts show concentrated attention on fibroglandular tissue.

## Foundational Learning

- **Contrastive Language-Image Pre-training (CLIP)**: BiomedCLIP extends CLIP to biomedical domains; understanding dual-encoder architecture and contrastive loss is essential for adaptation. Quick check: Can you explain how contrastive loss encourages matching image-text pairs to cluster together while pushing mismatched pairs apart in embedding space?

- **BI-RADS Density Classification System**: Four categories (A: fatty, B: scattered, C: heterogeneous, D: extremely dense) with subjective boundaries causing inter-reader variability. Quick check: Why does the paper note that misclassifications occur predominantly between adjacent classes?

- **Class Imbalance Strategies**: Severe imbalance (D-class is minority with only 3,000 images vs 6,000+ for others) required undersampling and weighted loss. Quick check: How does inverse-frequency weighting change the loss contribution of minority vs majority class samples during training?

## Architecture Onboarding

- **Component map**: Vision Encoder (BiomedCLIP) -> Text Encoder (BiomedCLIP) -> Shared Embedding Space -> Weighted Contrastive Loss -> GradCAM Module

- **Critical path**: 1) Preprocess images: Remove burned-in annotations, crop to breast region, resize to 224×224, normalize contrast 2) Format text prompts: Simplify radiology reports to four standardized descriptions 3) Stratified group splitting: Patient-level separation prevents data leakage 4) Fine-tune across 5 folds with weighted contrastive loss 5) Validate on internal folds + external datasets in zero-shot setting

- **Design tradeoffs**: Single-modality vs Multi-modality (nearly identical accuracy, but multi-modality enables deployment across imaging systems); Undersampling ratio (trading overall data volume for balanced class representation); Assumption: Zero-shot external validation vs fine-tuning on target data

- **Failure signatures**: Adjacent class confusion (errors cluster at A→B, B→C, C→D boundaries); Artifact attention (GradCAM highlights paddle marks, text overlays, or implants instead of tissue); Scanner-specific overfitting (performance drops on vendor systems not represented in training)

- **First 3 experiments**: 1) Baseline reproduction: Train single-modality s2D model with reported undersampling; verify accuracy ≈0.73 and AUC distribution matches Table 2 2) Class weighting ablation: Compare weighted vs unweighted contrastive loss to isolate improvement from imbalance handling 3) External validation sanity check: Run zero-shot inference on RSNA/EMBED; verify GradCAM focuses on breast tissue, not imaging artifacts

## Open Questions the Paper Calls Out
- Can the adapted foundation model framework be effectively extended to lesion detection and broader diagnostic tasks beyond density classification?
- Do clinicians agree that GradCAM attention patterns reflect clinically meaningful features for density assessment?
- How does model performance vary across demographic subgroups (age, race, breast cancer risk factors) not analyzed in the current study?

## Limitations
- BiomedCLIP adaptation relies on contrastive learning that may not fully capture nuanced visual-textual relationships specific to mammographic density
- Class imbalance handling through undersampling reduces total training data from 96,995 to 19,000 images, potentially limiting model robustness
- External validation uses zero-shot classification without fine-tuning, which may underestimate performance compared to domain-adapted models

## Confidence
- **High Confidence**: Multi-modality training achieves comparable accuracy to single-modality while enabling broader deployment across imaging systems (AUC range: 0.80-0.93 across external datasets)
- **Medium Confidence**: Weighted contrastive learning effectively handles severe class imbalance while maintaining per-class AUC >0.84
- **Medium Confidence**: GradCAM visualizations reflect clinically relevant attention patterns on breast tissue rather than imaging artifacts

## Next Checks
1. **Feature Attribution Validation**: Conduct controlled experiments removing specific tissue regions to verify GradCAM-identified regions are causally necessary for correct classification decisions

2. **Generalization Stress Test**: Fine-tune the multi-modality model on external RSNA/EMBED datasets and compare performance gains against zero-shot results to quantify true generalization limits

3. **Modality Correlation Analysis**: Analyze whether density classes correlate with specific imaging modalities or scanner vendors in training data to rule out spurious modality-density associations