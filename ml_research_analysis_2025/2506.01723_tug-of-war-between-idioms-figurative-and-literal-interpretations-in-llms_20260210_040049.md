---
ver: rpa2
title: Tug-of-war between idioms' figurative and literal interpretations in LLMs
arxiv_id: '2506.01723'
source_url: https://arxiv.org/abs/2506.01723
tags:
- layers
- idiom
- interpretation
- figurative
- literal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how large language models (LLMs) process
  idiomatic expressions, which have figurative meanings that diverge from their literal
  interpretations. Using causal tracing methods, the researchers analyze a pretrained
  transformer model's mechanisms for idiom comprehension.
---

# Tug-of-war between idioms' figurative and literal interpretations in LLMs
## Quick Facts
- arXiv ID: 2506.01723
- Source URL: https://arxiv.org/abs/2506.01723
- Reference count: 40
- Primary result: Causal tracing reveals how transformers resolve idiom ambiguity through early figurative retrieval and contextual disambiguation in middle layers

## Executive Summary
This study investigates how large language models process idiomatic expressions using causal tracing methods on a pretrained transformer. The researchers uncover three key mechanisms for idiom comprehension: early layers retrieve figurative meanings while suppressing literal ones, contextual disambiguation occurs immediately with refinement in later layers, and competing pathways carry both interpretations with an intermediate pathway favoring figurative meaning. The findings provide mechanistic evidence for how autoregressive transformers handle the tug-of-war between figurative and literal interpretations of idioms.

## Method Summary
The researchers employed causal tracing methods to analyze a pretrained transformer model's mechanisms for idiom comprehension. They systematically identified which components (attention heads and MLPs) were critical for processing idiomatic expressions versus literal interpretations. By examining activation patterns across different layers and measuring the impact of component ablations, they mapped out the model's internal processing pathways for handling figurative language.

## Key Results
- Early layers and specific attention heads retrieve figurative interpretations while suppressing literal ones
- Contextual disambiguation occurs immediately, with later layers refining interpretation when context conflicts with retrieved meaning
- Competing pathways carry both interpretations - an intermediate pathway prioritizes figurative meaning while a parallel direct route favors literal interpretation

## Why This Works (Mechanism)
The model uses a multi-layered approach to idiom processing where early components establish the figurative interpretation foundation, middle layers handle contextual disambiguation, and competing pathways allow for both interpretations to be represented before final selection. This architecture enables efficient handling of idiomatic expressions while maintaining the ability to process literal meanings when appropriate.

## Foundational Learning
- Causal tracing methodology: why needed - to identify which model components actually cause specific behaviors; quick check - can trace specific idioms to their processing components
- Transformer attention mechanisms: why needed - attention heads enable selective information retrieval across tokens; quick check - can observe attention patterns for idiom tokens
- Layer-wise processing in transformers: why needed - different layers perform different computational functions; quick check - can analyze how processing changes across depth

## Architecture Onboarding
- Component map: Input -> Early attention heads/MLPs (figurative retrieval) -> Middle layers (contextual disambiguation) -> Later layers (refinement) -> Output
- Critical path: Early figurative retrieval pathway (essential for establishing meaning) -> Middle contextual refinement (crucial for disambiguation) -> Final interpretation
- Design tradeoffs: Model balances efficiency (early retrieval) with accuracy (contextual refinement) but maintains competing pathways that may increase computational cost
- Failure signatures: Inability to suppress literal interpretation when context strongly favors figurative meaning; failure to update interpretation when context changes
- First experiments: 1) Test if early component ablation eliminates figurative interpretation capability, 2) Measure impact of middle layer disablement on contextual disambiguation accuracy, 3) Compare performance on idioms with strong vs weak contextual cues

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis based on a single pretrained transformer model without specifying architecture, limiting generalizability
- Causal tracing methodology cannot definitively prove causal relationships between identified mechanisms and final outputs
- Study focuses on interpretation rather than generation of idioms, leaving questions about bidirectional processing

## Confidence
- Early figurative retrieval mechanism: Medium-High - supported by causal tracing but suppression remains indirect
- Contextual disambiguation findings: Medium - relies on middle layer analysis without comprehensive cross-layer interaction examination
- Competing pathways hypothesis: Medium - plausible but needs more direct evidence of dynamic interaction

## Next Checks
1. Replicate causal tracing analysis across multiple transformer architectures (BERT, RoBERTa, GPT variants) to test generalizability
2. Conduct ablation studies systematically disabling identified components to measure direct causal impact on idiom interpretation accuracy
3. Extend analysis to idiom generation tasks to determine if same mechanisms operate bidirectionally or if distinct processes are involved