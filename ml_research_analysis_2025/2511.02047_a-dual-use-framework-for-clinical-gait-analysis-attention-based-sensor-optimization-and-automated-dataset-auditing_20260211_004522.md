---
ver: rpa2
title: 'A Dual-Use Framework for Clinical Gait Analysis: Attention-Based Sensor Optimization
  and Automated Dataset Auditing'
arxiv_id: '2511.02047'
source_url: https://arxiv.org/abs/2511.02047
tags:
- sensor
- clinical
- gait
- attention
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a dual-use framework for clinical gait analysis
  using wearable sensors and attention-based deep learning. The method addresses both
  sensor optimization and automated dataset auditing.
---

# A Dual-Use Framework for Clinical Gait Analysis: Attention-Based Sensor Optimization and Automated Dataset Auditing

## Quick Facts
- arXiv ID: 2511.02047
- Source URL: https://arxiv.org/abs/2511.02047
- Reference count: 36
- Primary result: Dual-use framework for sensor optimization and dataset auditing in clinical gait analysis using attention-based deep learning

## Executive Summary
This paper introduces a dual-use framework that combines multi-stream neural networks with sensor-level attention to address both sensor optimization and automated dataset auditing in clinical gait analysis. The framework is applied to a multi-cohort gait dataset for Parkinson's disease, osteoarthritis, and stroke asymmetry detection. By analyzing attention weights across sensor streams, the method not only identifies task-specific sensor importance but also uncovers hidden biases in the dataset, such as a severe laterality confound where over 70% of attention was assigned to the right foot sensor. This discovery was attributed to right-side pathology bias in the dataset rather than clinical findings, demonstrating the framework's potential for responsible AI development in digital health.

## Method Summary
The framework employs a multi-stream 1D-CNN architecture with sensor-level attention to classify gait patterns from wearable IMU data. Each of four sensor locations (Head, Lower Back, Left Foot, Right Foot) has its own convolutional branch with 3 layers (32→64→128 filters, kernel_size=15, padding=7), followed by ReLU, MaxPool, and AdaptiveAvgPool to produce 128-dimensional vectors. A sensor-level attention module (single linear layer → softmax) generates 4 attention weights that sum to 1, which are used to combine the sensor representations. The combined context vector is classified using a 2-layer fully connected network with ReLU and Dropout(0.5). Models are trained separately for four binary classification tasks using weighted BCE loss with patient-level 70/15/15 splits, Adam optimizer (lr=1e-4), batch_size=32, 50 epochs, and early stopping on validation loss.

## Key Results
- Strong classification performance across all tasks: OA ROC-AUC 0.990, CVA 0.950
- Attention mechanism quantitatively uncovered dataset bias: over 70% attention assigned to right foot sensor for OA and CVA tasks
- Right-foot dominance reflected dataset confound (e.g., 15/0 right-sided OA, 47/2/0 right-dominant CVA)
- Proposed task-specific sensor configurations (e.g., HE+RF for PD) based on attention weights

## Why This Works (Mechanism)
The framework leverages sensor-level attention to simultaneously optimize sensor selection and audit dataset biases. By learning attention weights that sum to 1 across sensor streams, the model identifies which sensors contribute most to task performance. When these weights are systematically skewed toward one sensor (e.g., right foot), it reveals potential biases in the dataset rather than genuine clinical insights. This dual capability stems from the attention mechanism's ability to quantify feature importance while the multi-stream architecture preserves sensor-specific information.

## Foundational Learning
**Multi-stream CNN with attention**: Why needed - to handle multi-sensor time-series data while learning sensor importance. Quick check - verify each sensor branch produces distinct feature representations.
**Sensor-level attention**: Why needed - to quantify task-specific sensor importance and detect biases. Quick check - attention weights should sum to 1 and reflect known clinical or data patterns.
**Patient-level data splitting**: Why needed - to prevent data leakage when multiple trials exist per patient. Quick check - ensure no patient ID appears across train/val/test splits.
**Imbalance-robust metrics**: Why needed - clinical datasets often have class imbalance. Quick check - ROC-AUC and PR-AUC should be reported alongside accuracy metrics.

## Architecture Onboarding
**Component map**: IMU sensors -> 4 sensor branches (Conv1D layers) -> 128-dim vectors -> Attention module (linear→softmax) -> Context vector -> Classifier (FC layers) -> Binary output
**Critical path**: Sensor data → multi-stream CNN → attention aggregation → classification
**Design tradeoffs**: Multi-stream preserves sensor-specific features but increases parameters; attention adds interpretability but may be unstable with small datasets
**Failure signatures**: Attention weights collapsing to one sensor may indicate dataset bias or insufficient task complexity; poor generalization suggests overfitting to dataset-specific patterns
**First experiments**: 1) Verify attention weights sum to 1 and produce stable distributions across runs. 2) Test classification performance with all sensors vs reduced sensor sets based on attention. 3) Apply framework to an independent gait dataset to validate bias detection capability.

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Unknown random seed affects reproducibility of exact performance metrics and attention distributions
- Sequence length handling for variable-duration trials not explicitly specified
- Dataset biases (right-sided pathology dominance) may limit generalizability to other cohorts
- Computational cost for large-scale auditing not evaluated

## Confidence
- **High confidence** in core methodology: Multi-stream CNN with sensor-level attention is well-defined
- **Medium confidence** in data bias discovery: Attention reveals real confound but interpretation requires domain validation
- **Medium confidence** in sensor optimization claims: Attention-based recommendations need prospective validation

## Next Checks
1. Reproduce patient-level splits with fixed random seed to achieve exact test set class distributions
2. Compute bootstrap CIs for attention weights across multiple random seeds to confirm stability
3. Apply trained model to an external gait dataset to assess generalizability of attention patterns and classification performance