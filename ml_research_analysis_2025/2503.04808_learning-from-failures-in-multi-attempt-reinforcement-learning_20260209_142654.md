---
ver: rpa2
title: Learning from Failures in Multi-Attempt Reinforcement Learning
arxiv_id: '2503.04808'
source_url: https://arxiv.org/abs/2503.04808
tags:
- multi-attempt
- attempts
- task
- answer
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a multi-attempt reinforcement learning framework
  for enhancing large language models' reasoning capabilities. Instead of single-turn
  question-answering, the model is given multiple attempts with feedback after incorrect
  responses, encouraging iterative refinement.
---

# Learning from Failures in Multi-Attempt Reinforcement Learning

## Quick Facts
- arXiv ID: 2503.04808
- Source URL: https://arxiv.org/abs/2503.04808
- Authors: Stephen Chung; Wenyu Du; Jie Fu
- Reference count: 20
- Primary result: Multi-attempt RL improves math reasoning accuracy from 45.6% to 52.5% when multiple evaluation attempts are allowed

## Executive Summary
This paper proposes a multi-attempt reinforcement learning framework for enhancing large language models' reasoning capabilities. Instead of single-turn question-answering, the model is given multiple attempts with feedback after incorrect responses, encouraging iterative refinement. Experiments with a 1.5B Qwen model show that multi-attempt training significantly improves performance when multiple evaluation attempts are allowed (45.6% → 52.5% accuracy), while single-turn training shows minimal improvement (42.3% → 43.2%). Even under standard single-attempt evaluation, the multi-attempt model slightly outperforms its single-turn counterpart (45.4% vs 43.5% average accuracy across five math benchmarks). The approach enables models to learn self-correction and refine solutions based on feedback, suggesting that multi-turn RL environments can foster better reasoning capabilities compared to single-turn settings.

## Method Summary
The method uses Proximal Policy Optimization (PPO) to train a 1.5B Qwen model on math problems with multiple attempts. For each question, the number of attempts N is randomly sampled from {1,...,5}. The model generates responses iteratively, receiving feedback ("Your answer is wrong. Try alternatives and refine your answer. You have X attempt(s) left.") after each incorrect attempt. The reward function provides +1 for correct answers, -0.5 for well-formed but incorrect answers, and -1 for invalid formats, with no discounting across attempts. Training uses 8K math questions over 160 episodes (1.28M samples total). The approach is compared against standard single-turn RL training on the same data and model architecture.

## Key Results
- Multi-attempt training improves accuracy from 45.6% to 52.5% when 4-7 evaluation attempts are allowed
- Single-turn training shows minimal improvement (42.3% → 43.2%) under multi-attempt evaluation
- Multi-attempt model slightly outperforms single-turn counterpart even under standard single-attempt evaluation (45.4% vs 43.5% average accuracy across five math benchmarks)
- Performance gains scale with the number of allowed evaluation attempts, demonstrating the value of iterative refinement

## Why This Works (Mechanism)

### Mechanism 1
Multi-attempt training creates explicit learning pressure for self-refinement behaviors that single-turn training cannot provide. The reward structure (+1 for correct in any attempt, -0.5 for well-formed but wrong, -1 otherwise) with no discount per attempt means the model receives strong gradient signal to refine incorrect answers. Repeating the same failed answer yields no additional reward, creating selection pressure toward correction strategies.

### Mechanism 2
Random sampling of attempt count (N ~ Uniform{1,...,M}) teaches context-dependent reasoning strategies. When N=1, the model learns conservative, high-confidence responses. When N is large, early attempts can be exploratory. This variation prevents overfitting to a single reasoning mode and encourages meta-learning about resource allocation.

### Mechanism 3
Multi-turn dialogue structure enables learning from self-generated failure examples as implicit contrastive signal. Previous failed attempts remain in the context window, providing the model with concrete examples of what didn't work. This functions as implicit negative examples without requiring external data curation.

## Foundational Learning

- **Proximal Policy Optimization (PPO)**: Why needed here: The paper uses PPO as the core RL algorithm; understanding clip ratios, advantage estimation, and KL penalties is necessary to modify the training pipeline. Quick check question: Can you explain why the paper uses a small KL divergence coefficient (0.01) and what would happen if it were increased significantly?

- **Sparse vs. Dense Reward Signals**: Why needed here: The paper explicitly frames single-turn tasks as having sparse rewards; understanding reward shaping helps evaluate whether the multi-attempt reward design is optimal. Quick check question: Why does the paper use no discount factor (γ=1) rather than discounting rewards by attempt number? What behavior would discounting encourage?

- **Context-Dependent Policy**: Why needed here: The model must condition its behavior on remaining attempts; understanding how policies can be conditioned on context is essential for debugging failure cases. Quick check question: How does the model know how many attempts remain, and what happens if this signal is removed from the prompt?

## Architecture Onboarding

- **Component map**: Math question dataset D -> PPO policy πθ -> reward function (+1/-0.5/-1) -> Environment with attempt counter N

- **Critical path**: 1. Sample question x₀ and attempt count N from dataset 2. Generate response a₀ ~ πθ(·|x₀) 3. Extract answer, compare to ground truth 4. If incorrect and N>1: append feedback to context, decrement N, return to step 2 5. If correct or N≤1: compute rewards for all turns, update policy via PPO

- **Design tradeoffs**: M=5 max attempts balances training time vs. refinement capability; no reward discounting encourages exploration but removes time-pressure; uniform attempt sampling promotes diverse behaviors but could add noise

- **Failure signatures**: Policy collapse to always outputting format-valid but wrong answers; repetition without refinement; over-correction abandoning correct reasoning paths; single-attempt capability degradation

- **First 3 experiments**: 1. Ablation on M: Train with M∈{2,3,5,7} to find optimal attempt budget 2. Feedback format variation: Compare binary vs. detailed vs. no feedback 3. Transfer evaluation: Test refinement capability on held-out domains like code generation

## Open Questions the Paper Calls Out
The paper identifies several directions for future exploration, including incorporating more nuanced and detailed feedback, introducing auxiliary tasks, and fostering different capabilities through enhanced task environments. The authors specifically mention that their approach facilitates the emergence of self-refinement and may enable the "Aha Moment" to emerge more easily through explicit training for self-correction.

## Limitations
- Performance gains rely on ground-truth verifiable feedback, which may not generalize to domains where verification is ambiguous or expensive
- Multi-attempt training requires substantially more compute and memory than single-turn training due to maintaining multiple turns of dialogue history
- Random sampling of attempt counts could introduce high-variance training dynamics that aren't fully characterized

## Confidence

**High Confidence** in the core empirical finding that multi-attempt training improves performance when multiple evaluation attempts are allowed.

**Medium Confidence** in the claimed mechanism that multi-attempt training creates selection pressure for self-refinement behaviors.

**Low Confidence** in the hypothesis that random attempt count sampling teaches context-dependent reasoning strategies.

## Next Checks
1. **Ablation on Feedback Informativeness**: Compare "Your answer is wrong" vs. no feedback vs. detailed error hints to isolate whether the multi-attempt benefit comes from having any feedback loop or specifically from learning self-correction capabilities.

2. **Single-Attempt Capability Degradation Test**: After multi-attempt training, fine-tune the model on single-turn data to test whether the multi-attempt capability comes at the cost of degraded single-turn performance, or if both capabilities can coexist.

3. **Generalization to Non-Mathematical Domains**: Evaluate the multi-attempt trained model on code generation or text editing tasks where self-correction is valuable but ground-truth verification is more complex, to assess whether the learned refinement behaviors transfer beyond math problems.