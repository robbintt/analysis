---
ver: rpa2
title: Exploring LLMs for Predicting Tutor Strategy and Student Outcomes in Dialogues
arxiv_id: '2507.06910'
source_url: https://arxiv.org/abs/2507.06910
tags:
- tutor
- student
- giving
- move
- moves
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the use of large language models (LLMs)
  for predicting tutor strategies and student outcomes in educational dialogues. The
  authors evaluate GPT-4o and Llama 3 on two math tutoring datasets, comparing their
  performance against traditional baselines like logistic regression, Markov chains,
  and LSTMs.
---

# Exploring LLMs for Predicting Tutor Strategy and Student Outcomes in Dialogues

## Quick Facts
- arXiv ID: 2507.06910
- Source URL: https://arxiv.org/abs/2507.06910
- Reference count: 18
- Primary result: LLMs predict student outcomes better than baselines, especially with tutor move annotations, but struggle to predict future tutor moves (max F1 49% MathDial, 27% AlgebraNation)

## Executive Summary
This paper investigates whether large language models can predict tutor strategies and student outcomes in educational dialogues. The authors evaluate GPT-4o and Llama 3 on two math tutoring datasets (MathDial and AlgebraNation), comparing against traditional baselines like logistic regression, Markov chains, and LSTMs. The study examines four prediction tasks: future tutor moves, current tutor move classification, future dialogue success, and next student turn success. Results show that while LLMs outperform baselines in predicting student outcomes—particularly when incorporating tutor move annotations—predicting future tutor moves remains challenging even for state-of-the-art models. The analysis reveals that certain tutor strategies (confirmatory feedback, giving instruction, giving explanation) positively correlate with student success, while directive "telling" moves negatively impact outcomes.

## Method Summary
The study uses two math tutoring datasets: MathDial (2,484 dialogues with 4 single-label tutor moves) and AlgebraNation (2,318 forum posts with 16 multi-label moves). Four tasks are evaluated: predicting future tutor moves, classifying current moves, predicting future dialogue success, and predicting next student turn success. Models include fine-tuned Llama 3.2 3B with LoRA (treating labels as text tokens), zero-shot GPT-4o prompting (returning JSON), and baselines (Markov Chain, Logistic Regression, LSTM on move sequences). Two input conditions are tested: dialogue only vs. dialogue plus prior move labels. Evaluation uses accuracy and weighted F1, with exact-match accuracy for multi-label tasks.

## Key Results
- LLMs outperform traditional baselines in predicting student outcomes, especially when incorporating tutor move annotations
- Predicting future tutor moves remains difficult, with F1 scores reaching only 49% for MathDial and 27% for AlgebraNation
- Specific tutor strategies (confirmatory feedback, giving instruction, giving explanation) positively correlate with student success, while "telling" moves negatively impact outcomes
- Including previous move labels improves outcome prediction for Llama 3, showing tutor moves complement dialogue text

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tutor move annotations provide structured pedagogical signals that complement raw dialogue text for predicting student outcomes.
- Mechanism: Move labels act as compact, high-level summaries of tutor intent, reducing the inference burden on models by explicitly encoding pedagogical strategy that may be linguistically ambiguous in text alone.
- Core assumption: The annotated move taxonomy captures pedagogically relevant distinctions that correlate with student learning trajectories.
- Evidence anchors:
  - [abstract] "LLMs outperform baselines, especially when incorporating tutor move annotations"
  - [section 3.4] "previous move labels improve performance for Llama 3, showing that tutor moves complement dialogue text to infer student outcomes"
  - [corpus] Related work on training LLM tutors also highlights the importance of pedagogical strategy, suggesting move-level supervision is a recurring effective signal.
- Break condition: If move annotations are noisy, inconsistently applied, or taxonomies differ across contexts, the signal degrades.

### Mechanism 2
- Claim: Instructionally supportive tutor moves (confirmatory feedback, giving instruction, giving explanation) positively correlate with student success, while directive "telling" moves negatively correlate.
- Mechanism: These supportive moves scaffold student reasoning—reinforcing correct steps, clarifying procedures, or deepening conceptual understanding—whereas "telling" short-circuits student cognitive engagement.
- Core assumption: Correlations in the logistic regression coefficients reflect predictive relationships between move types and outcomes.
- Evidence anchors:
  - [section 3.4] "confirmatory feedback, giving instruction, and giving explanation have the greatest positive impact on success" (AlgebraNation); "telling has a negative impact on success" (MathDial)
  - [section 3.4, Tables 8-9] Chi-squared analysis shows statistical significance for these features
  - [corpus] Evidence is limited in immediate neighbors; assumption: these patterns align with broader tutoring literature.
- Break condition: If student prior knowledge, problem difficulty, or dialogue context modulates these effects, the correlation may not generalize.

### Mechanism 3
- Claim: LLMs leverage textual context and model capacity to improve over sequence-only baselines, but predicting future tutor moves remains fundamentally difficult due to strategy variability.
- Mechanism: LLMs can encode dialogue history and linguistic nuances beyond move sequences, capturing tutor reasoning patterns; however, tutor decisions are influenced by factors not fully observable in text.
- Core assumption: The gap between LLM performance and perfect prediction reflects inherent unpredictability in human tutor behavior, not solely model limitations.
- Evidence anchors:
  - [abstract] "even state-of-the-art LLMs struggle to predict future tutor strategy"
  - [section 3.3] "predicting the next tutor move proves to be more difficult than classifying the current move" with F1 of 49% (MathDial) and 27% (AlgebraNation); LLMs outperform baselines but performance remains low
  - [corpus] Related work using GRUs/RoBERTa also found future move prediction challenging, suggesting the task difficulty is consistent across architectures.
- Break condition: If future work incorporates richer context (student state modeling, multimodal cues, or reinforcement learning over dialogue policies), predictability may increase.

## Foundational Learning

- Concept: Tutor Moves as Categorical Pedagogical Actions
  - Why needed here: The entire framework depends on understanding that dialogues are annotated with discrete "moves" representing tutor strategy.
  - Quick check question: Given a tutor utterance "Can you explain why you added these numbers?", would you classify this as "probing" or "telling"?

- Concept: Sequence Modeling for Dialogue (Markov, LSTM, Transformer)
  - Why needed here: The baselines and LLM methods differ in how they capture dependencies across turns.
  - Quick check question: Why might a second-order Markov chain fail to capture long-range dependencies that an LSTM or LLM could model?

- Concept: Multi-Label vs. Single-Label Classification
  - Why needed here: MathDial uses single-label moves per turn; AlgebraNation uses multi-label (a tutor turn can have multiple moves).
  - Quick check question: If a tutor turn is annotated as both "giving_instruction" and "confirmatory_feedback," how should accuracy be computed—per-label or exact match?

## Architecture Onboarding

- Component map:
  Input layer (dialogue history + optional move labels) -> Model variants (Llama 3 fine-tuned, GPT-4o zero-shot, baselines) -> Output layer (predicted tutor moves or student outcomes) -> Evaluation (accuracy and weighted F1)

- Critical path:
  1. Parse dialogue datasets into turn-level sequences with move annotations and outcome labels
  2. For Llama 3 fine-tuning: Format data as input-target pairs; apply LoRA with hyperparameters (rank 8, lr 1e-4)
  3. For GPT-4o: Construct prompts with annotation schema definitions; set temperature=0, JSON output
  4. Run baselines on move sequences only
  5. Compute metrics and analyze confusion matrices/misclassification patterns

- Design tradeoffs:
  - Fine-tuned Llama 3 vs. GPT-4o: Fine-tuning adapts to dataset-specific label distributions but requires training infrastructure; GPT-4o generalizes zero-shot but may confuse similar labels
  - Including vs. excluding move annotations: Moves improve outcome prediction but may not be available at inference time in real applications
  - Exact match vs. per-label metrics: Exact match is stricter and penalizes partial correctness; useful for multi-label tasks like AlgebraNation

- Failure signatures:
  - Low F1 with high accuracy: Model is predicting majority class (check label distributions)
  - Confusion between similar moves (probing vs. focus in MathDial; giving_instruction vs. giving_explanation in AlgebraNation): Indicates taxonomic ambiguity
  - Llama 3 skew toward "giving_instruction" in future prediction: Model may be overfitting to frequent class

- First 3 experiments:
  1. Replicate future tutor move prediction on MathDial using Llama 3 with and without previous move labels; compare F1 scores to verify the 49% ceiling
  2. Ablate input context length (e.g., last 3 turns vs. full dialogue) to measure how much history is needed for outcome prediction
  3. Test GPT-4o on AlgebraNation with a simplified 5-class move taxonomy (collapsing similar categories) to assess whether label granularity affects confusion patterns

## Open Questions the Paper Calls Out

- Can reinforcement learning guided by student outcomes effectively suggest optimal tutor moves during a dialogue?
- Can advanced techniques like in-context learning or reinforcement learning significantly improve the accuracy of future tutor move prediction?
- Does incorporating sequential modeling of student behavior enhance the prediction of tutor strategy and student outcomes?

## Limitations
- Dataset accessibility is a major limitation, particularly for AlgebraNation which requires author contact or institutional access
- The study relies on pre-existing move annotations whose quality and consistency across datasets are not fully characterized
- The predictive models do not account for contextual factors like tutor expertise, student prior knowledge, or problem difficulty
- Correlation analyses cannot establish causation between specific tutor moves and student success

## Confidence

- High confidence: LLMs outperform traditional baselines in predicting student outcomes, particularly when incorporating tutor move annotations
- Medium confidence: Predicting future tutor moves remains fundamentally difficult even for state-of-the-art LLMs
- Medium confidence: Identified correlations between specific tutor moves and positive student outcomes, and the negative impact of "telling" moves

## Next Checks
1. Obtain and preprocess both MathDial and AlgebraNation datasets following the reported train/test splits and move annotation schemas to verify the reported F1 scores and accuracy metrics
2. Systematically test outcome prediction with and without tutor move labels across different input context lengths to quantify the contribution of structured pedagogical signals versus raw dialogue text
3. Collapse similar move categories (e.g., probing/focus in MathDial; giving_instruction/giving_explanation in AlgebraNation) and re-run classification experiments to measure the impact of label granularity on model performance and confusion patterns