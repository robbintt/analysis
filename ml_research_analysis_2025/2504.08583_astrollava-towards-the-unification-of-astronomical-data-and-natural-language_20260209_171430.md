---
ver: rpa2
title: 'AstroLLaVA: towards the unification of astronomical data and natural language'
arxiv_id: '2504.08583'
source_url: https://arxiv.org/abs/2504.08583
tags:
- astronomical
- image
- language
- arxiv
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AstroLLaVA fine-tunes LLaVA on a curated dataset of ~30k astronomical
  images and text captions to enable vision-language interactions with astronomical
  imagery. A two-stage fine-tuning approach adapts the model to both image captioning
  and visual question answering in the astronomy domain.
---

# AstroLLaVA: towards the unification of astronomical data and natural language

## Quick Facts
- arXiv ID: 2504.08583
- Source URL: https://arxiv.org/abs/2504.08583
- Authors: Sharaf Zaman; Michael J. Smith; Pranav Khetarpal; Rishabh Chakrabarty; Michele Ginolfi; Marc Huertas-Company; Maja Jabłońska; Sandor Kruk; Matthieu Le Lain; Sergio José Rodríguez Méndez; Dimitrios Tanoglidis
- Reference count: 22
- Primary result: Domain-adapted vision-language model achieves 0.597 cosine similarity on galaxy descriptions, marginally outperforming baseline models

## Executive Summary
AstroLLaVA fine-tunes LLaVA on a curated dataset of ~30k astronomical images and text captions to enable vision-language interactions with astronomical imagery. A two-stage fine-tuning approach adapts the model to both image captioning and visual question answering in the astronomy domain. When evaluated on the Galaxy10 DECaLS dataset, AstroLLaVA achieved a 0.597 cosine similarity between generated galaxy descriptions and ground truth labels, marginally outperforming LLaVA 1.5 7B (0.594) and LLaVA 1.5 13B (0.591). This suggests domain adaptation improves astronomical knowledge, though performance is bottlenecked by the shared CLIP vision encoder rather than language model capacity. The model, code, and dataset are publicly released under an MIT license.

## Method Summary
AstroLLaVA employs a two-stage fine-tuning process on LLaVA 1.5 architecture. Stage 1 trains only projection layers while freezing both CLIP vision encoder and LLaMA-7B, learning to map astronomical visual features into the language model's embedding space. Stage 2 performs end-to-end instruction tuning on synthetic QA pairs generated by GPT-4 from image captions. The training uses ~29.8k image-text pairs scraped from APOD, ESO, and HST, with synthetic conversations created to simulate astronomical dialogue. The model is evaluated on Galaxy10 DECaLS dataset using cosine similarity between generated descriptions and ground truth labels.

## Key Results
- AstroLLaVA achieved 0.597 cosine similarity on Galaxy10 DECaLS, marginally outperforming LLaVA 1.5 7B (0.594) and LLaVA 1.5 13B (0.591)
- Performance bottleneck appears to be the shared CLIP vision encoder rather than language model capacity, as 7B and 13B variants show similar results
- Synthetic GPT-4 generated conversations successfully enabled scalable QA training data creation without human annotation
- Model trained on outreach imagery shows domain adaptation benefits but may have limitations for scientific-grade analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage fine-tuning preserves general visual-language capabilities while adapting to astronomical domain knowledge.
- Mechanism: Stage 1 trains only the projection layers (freezing both CLIP vision encoder and LLaMA-7B), learning to map astronomical visual features into the language model's embedding space. Stage 2 then performs end-to-end instruction tuning on synthetic QA pairs, allowing the full model to specialize in astronomical dialogue while retaining pretrained knowledge.
- Core assumption: Freezing pretrained components initially prevents catastrophic forgetting of general visual-linguistic representations.
- Evidence anchors:
  - [abstract] "Our two-stage fine-tuning process adapts the model to both image captioning and visual question answering in the astronomy domain."
  - [Section 3] "This two-stage process helps preserve the strong general capabilities of the pre-trained components while adapting them specifically to astronomical data and dialogue."
  - [corpus] Weak direct evidence; corpus papers focus on benchmarking rather than training mechanics.

### Mechanism 2
- Claim: Synthetic conversation generation from captions enables scalable QA training data creation without requiring human annotation.
- Mechanism: GPT-4 generates 3-5 question-answer pairs per caption by simulating a dialogue between an "AstroLLaVA oracle" and "human inquisitor," focusing on visual aspects while avoiding information not present in the image (e.g., redshift measurements).
- Core assumption: Text captions adequately represent the visual content of astronomical images for QA generation purposes.
- Evidence anchors:
  - [Section 2] "We use unimodal text GPT-4 to generate synthetic conversations... we do not include our scraped images in our prompting routine, and simply use the text captions as an approximation of our imagery."
  - [Figure 2] Prompt explicitly instructs: "Focus on the visual aspects of the image such as size, position... that can be inferred without the text information."
  - [corpus] No corpus papers evaluate synthetic QA generation quality.

### Mechanism 3
- Claim: The shared CLIP vision encoder creates a performance bottleneck independent of language model scale.
- Mechanism: All compared models (AstroLLaVA 7B, LLaVA 1.5 7B, LLaVA 1.5 13B) use the same CLIP ViT-L/14 encoder pretrained on natural images at 336×336 resolution. The encoder's embeddings fundamentally limit what visual information reaches the language model, regardless of LLM capacity.
- Core assumption: CLIP's pretrained representations are suboptimal for astronomical imagery characteristics (different noise profiles, intensity distributions, and feature scales).
- Evidence anchors:
  - [Section 4] "Since all these models use the same vision encoder to extract features from the galaxy images, they are fundamentally limited by the information captured by these CLIP embeddings: this is the case even with AstroLLaVA's specialised astronomical training and LLaVA-13B's additional parameters."
  - [Section 4] "This suggests that the performance bottleneck could be the visual encoder rather than language modelling capacity or domain adaptation."
  - [corpus] Radio Astronomy VLM paper confirms VLMs "struggle with scientific imaging, especially on unfamiliar and potentially previously unseen data distributions."

## Foundational Learning

- Concept: Vision-Language Model (VLM) architecture
  - Why needed here: AstroLLaVA inherits LLaVA's design where a vision encoder (CLIP) and language model (LLaMA) communicate via learned projection layers—understanding this separation is essential for debugging which component limits performance.
  - Quick check question: Can you explain why changing the language model from 7B to 13B parameters showed no improvement in the evaluation?

- Concept: Cosine similarity for semantic evaluation
  - Why needed here: The paper evaluates model outputs using cosine similarity between sentence embeddings (all-MiniLM-L6-v2), not exact string matching—this captures semantic similarity between generated descriptions and ground-truth labels.
  - Quick check question: Why might cosine similarity of 0.597 represent only marginal improvement over 0.594 despite domain-specific training?

- Concept: Instruction tuning vs. pretraining
  - Why needed here: AstroLLaVA's second training stage uses instruction tuning on QA pairs rather than raw caption prediction—this teaches the model conversational behavior, not just description generation.
  - Quick check question: What would happen if you skipped stage 1 and went directly to end-to-end instruction tuning?

## Architecture Onboarding

- Component map:
  - Image → CLIP ViT-L/14 (336×336) → Visual embeddings → Projection layers → Language embedding space → LLaMA-7B + Tokenized prompt → Autoregressive text generation

- Critical path:
  1. Image → CLIP encoder → visual embeddings
  2. Visual embeddings → projection layers → language embedding space
  3. Projected visual embeddings + tokenized prompt → LLaMA
  4. LLaMA → autoregressive text generation

- Design tradeoffs:
  - **Outreach vs. scientific data**: Current dataset (APOD, ESO, HST outreach images) prioritizes public engagement over research-grade analysis; the paper explicitly notes this may limit galaxy classification performance
  - **Synthetic vs. human QA**: GPT-4 generated conversations scale easily but may lack domain expert nuance and propagate caption errors
  - **CLIP vs. domain-specific encoder**: Keeping CLIP frozen preserves general capabilities but constrains astronomical feature extraction

- Failure signatures:
  - **Hallucinated measurements**: Model may generate redshift values or object names it cannot actually infer from images (prompt engineering attempts to prevent this)
  - **Outreach-style responses on scientific data**: Model trained on promotional images may describe galaxy morphology in layperson terms rather than technical classification
  - **Vision encoder saturation**: Similar performance across 7B and 13B variants indicates visual features, not language capacity, limit quality

- First 3 experiments:
  1. **Ablate projection-only training**: Skip stage 1, train end-to-end from initialization on astronomical data to measure catastrophic forgetting of general capabilities
  2. **Evaluate on research-grade imagery**: Test AstroLLaVA on raw telescope data (not outreach-processed images) to quantify domain gap
  3. **Swap vision encoder**: Replace CLIP with an astronomy-pretrained vision model (if available) or train CLIP from scratch on astronomical images to test the encoder bottleneck hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does replacing the general-purpose CLIP vision encoder with a domain-specific pre-trained encoder significantly improve performance on scientific astronomical tasks?
- Basis in paper: [explicit] The authors state that the similar performance across model sizes suggests "the performance bottleneck could be the visual encoder rather than language modelling capacity" due to the shared CLIP backbone.
- Why unresolved: The current study only fine-tunes projection layers and the LLM, leaving the CLIP encoder frozen, which limits the capture of specialized astronomical visual features.
- What evidence would resolve it: A comparative evaluation where AstroLLaVA is trained using an astronomy-specialized vision encoder versus the standard CLIP ViT-L/14.

### Open Question 2
- Question: To what degree does fine-tuning on outreach-focused imagery transfer to quantitative scientific tasks like galaxy morphology classification?
- Basis in paper: [explicit] The authors note the compiled dataset (APOD, ESO, HST) may be "too focussed on outreach to provide a strong fine-tuning signal for the galaxy classification task."
- Why unresolved: The current evaluation relies on semantic similarity metrics rather than strict classification accuracy, potentially masking the model's limitations in scientific precision.
- What evidence would resolve it: Benchmarking the model's zero-shot performance on standardized, labeled scientific datasets (e.g., SDSS) to measure the domain gap between outreach and research data.

### Open Question 3
- Question: Does training on synthetic conversations generated from text-only captions increase the rate of visual hallucinations?
- Basis in paper: [inferred] The methodology uses GPT-4 to generate conversations based solely on text captions, explicitly stating, "we do not include our scraped images in our prompting routine."
- Why unresolved: The model is trained to align visual inputs with conversational responses that were generated without access to the specific visual context, potentially decoupling description from observation.
- What evidence would resolve it: A qualitative and quantitative error analysis specifically assessing "hallucination" (describing features not present in the image) on a held-out test set.

## Limitations

- Training dataset consists primarily of outreach imagery rather than research-grade astronomical data, potentially limiting scientific application performance
- Synthetic QA pairs generated by GPT-4 may propagate caption errors or omit key visual features that human annotators would catch
- Evaluation metric (cosine similarity) measures semantic similarity but does not assess quantitative measurement accuracy expected in scientific contexts

## Confidence

**High Confidence**: The two-stage fine-tuning approach and the observation that CLIP vision encoder limitations bottleneck performance are well-supported by the experimental results and architectural design. The 0.597 vs 0.594 cosine similarity improvement is directly measurable and statistically meaningful.

**Medium Confidence**: The claim that synthetic GPT-4 generated conversations adequately substitute for human-annotated QA pairs is reasonable given the scale requirements, but lacks validation through human evaluation studies. The assumption that outreach imagery captures sufficient astronomical visual features for scientific tasks is plausible but untested on research-grade data.

**Low Confidence**: The assertion that the model can reliably avoid hallucinating astronomical measurements (redshifts, object names) is based on prompt engineering rather than empirical validation, and the paper does not provide systematic hallucination analysis.

## Next Checks

1. **Research-grade data evaluation**: Test AstroLLaVA on raw telescope imagery (not outreach-processed) to quantify the domain gap between training and scientific use cases, measuring both performance degradation and changes in response style.

2. **Human evaluation of synthetic QA quality**: Conduct blind studies where domain experts assess the accuracy, completeness, and scientific appropriateness of GPT-4 generated QA pairs compared to human-annotated alternatives.

3. **Vision encoder ablation study**: Replace CLIP with an astronomy-pretrained vision model (or train CLIP from scratch on astronomical images) to empirically test whether the observed performance ceiling is indeed caused by the vision encoder rather than projection layers or language model limitations.