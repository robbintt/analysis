---
ver: rpa2
title: 'Forget by Uncertainty: Orthogonal Entropy Unlearning for Quantized Neural
  Networks'
arxiv_id: '2602.00567'
source_url: https://arxiv.org/abs/2602.00567
tags:
- unlearning
- forgetting
- data
- gradient
- quantized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses machine unlearning in quantized neural networks
  (QNNs) where existing methods face challenges: they induce forgetting by training
  models to memorize incorrect labels (confusing forgetting with misremembering) and
  use scalar gradient reweighting that cannot resolve directional conflicts between
  gradients. The authors propose OEU (Orthogonal Entropy Unlearning), a framework
  with two key innovations: (1) Entropy-guided unlearning that maximizes prediction
  uncertainty on forgotten data to achieve genuine forgetting rather than confident
  misprediction, and (2) Gradient orthogonal projection that eliminates interference
  by projecting forgetting gradients onto the orthogonal complement of retain gradients.'
---

# Forget by Uncertainty: Orthogonal Entropy Unlearning for Quantized Neural Networks

## Quick Facts
- arXiv ID: 2602.00567
- Source URL: https://arxiv.org/abs/2602.00567
- Reference count: 30
- The paper introduces OEU (Orthogonal Entropy Unlearning), a framework that outperforms state-of-the-art QNN unlearning methods by achieving superior forgetting effectiveness while preserving retain accuracy, with average gaps from retrained baseline significantly lower than Q-MUL.

## Executive Summary
This paper addresses machine unlearning in quantized neural networks (QNNs), where existing methods induce forgetting by training models to memorize incorrect labels and use scalar gradient reweighting that cannot resolve directional conflicts between gradients. The authors propose OEU (Orthogonal Entropy Unlearning), which combines entropy-guided unlearning that maximizes prediction uncertainty on forgotten data with gradient orthogonal projection that eliminates interference by projecting forgetting gradients onto the orthogonal complement of retain gradients. Extensive experiments on CIFAR-10, CIFAR-100, SVHN, and Tiny-ImageNet datasets with ResNet-18 and MobileNetV2 architectures demonstrate that OEU consistently outperforms existing methods, achieving average gaps from the retrained baseline significantly lower than Q-MUL.

## Method Summary
OEU consists of two key innovations: (1) Entropy-guided unlearning that maximizes prediction uncertainty on forgotten data by driving output distributions toward uniform through negative entropy loss, achieving genuine forgetting rather than confident misprediction; (2) Gradient orthogonal projection that eliminates interference by projecting forgetting gradients onto the orthogonal complement of retain gradients, providing theoretical guarantees for utility preservation under first-order approximation. The framework uses layer-wise normalization before projection to handle STE-induced gradient errors that vary across layers, improving over global projection. Training involves computing entropy loss on forget set and cross-entropy retain loss on retain set, then combining projected forget gradients with retain gradients for parameter updates.

## Key Results
- OEU consistently outperforms existing methods across multiple datasets and architectures
- Achieves average gaps from retrained baseline significantly lower than Q-MUL (e.g., 1.98% vs 3.90% on CIFAR-100 with 30% forgetting ratio)
- Superior forgetting effectiveness while preserving retain accuracy compared to label manipulation methods
- Layer-wise normalized projection shows significant improvement over global projection (AG 1.98%→5.40%)

## Why This Works (Mechanism)

### Mechanism 1
Maximizing prediction entropy on forgotten data achieves genuine forgetting without class-specific biases, unlike label manipulation methods that merely replace correct memorization with incorrect memorization. The entropy loss drives the output distribution toward uniform, yielding maximum entropy and zero KL divergence from the ideal "forgotten" state. Core assumption: uniform distribution represents the correct "forgotten" state—a model with no knowledge should have equal probability across all classes. Evidence: Theorem 4.2 shows EGU achieves D_KL(p* || U) = 0, while Theorem 4.1 shows label manipulation achieves D_KL(p* || U) = log K.

### Mechanism 2
Projecting forgetting gradients onto the orthogonal complement of retain gradients mathematically guarantees retain loss preservation under first-order approximation. The orthogonal projection ensures parameter updates along the projected gradient do not increase retain loss locally. Core assumption: first-order Taylor approximation holds sufficiently; gradients from STE in quantized networks are meaningful despite discretization. Evidence: Theorem 4.3 proves L_r(θ') ≈ L_r(θ); Theorem 4.4 proves the projected gradient maintains descent direction. Break condition: If gradients are highly non-linear, the first-order guarantee weakens.

### Mechanism 3
Layer-wise normalization before projection handles STE-induced gradient errors that vary across layers, improving over global projection. Normalizing per-layer gradients before computing layer-wise orthogonal projection allows each layer to independently resolve gradient conflicts. Core assumption: per-layer treatment captures layer-specific gradient dynamics better than a single global projection direction. Evidence: Ablation shows global projection degrades AG from 1.98% to 5.40%, with FA dropping from 66.41% to 57.99%. Break condition: If layer gradients are highly correlated across layers, layer-wise projection may introduce unnecessary fragmentation.

## Foundational Learning

- **Machine Unlearning Fundamentals**: OEU targets approximate unlearning (not exact retraining), evaluated via Forget Accuracy, Retain Accuracy, Test Accuracy, MIA, and Average Gap from retrained baseline. Quick check: Can you explain why the "Retrain" model serves as the gold standard, and why lower Average Gap is better?

- **Quantization-Aware Training (QAT) and STE**: QNNs use discrete weights; backpropagation requires the Straight-Through Estimator to approximate gradients through non-differentiable quantization operations. Quick check: What does STE assume about gradient flow through the quantization function?

- **Gradient Geometry (Orthogonality, Projection)**: The core GOP mechanism relies on vector projection onto orthogonal complements; understanding dot products and orthogonal subspaces is essential. Quick check: If two gradients have cosine similarity of -0.5, what does orthogonal projection achieve that scalar weighting cannot?

## Architecture Onboarding

- **Component map**: Entropy Loss Module -> Cross-Entropy Retain Loss -> Gradient Orthogonal Projection -> Parameter Update
- **Critical path**: 1) Forward pass on forget batch → compute entropy loss → backprop to get g_f; 2) Forward pass on retain batch → compute cross-entropy loss → backprop to get g_r; 3) Per-layer: normalize gradients, compute orthogonal projection, rescale to original magnitude; 4) Update parameters with combined (projected forget + retain) gradient
- **Design tradeoffs**: α ∈ [0,1] controls orthogonality strength (α=1 maximizes retain protection but may slow forgetting); layer-wise vs. global projection (layer-wise captures finer-grained dynamics but adds computational overhead); entropy vs. label manipulation (entropy avoids class bias but may be slower to converge)
- **Failure signatures**: Over-forgetting (FA far below Retrain, RA degraded) indicates gradient projection too weak; under-forgetting (FA too high, MIA high) indicates entropy objective not converging; catastrophic retain loss indicates projection failing; near-zero projected gradients indicate forget and retain gradients nearly collinear
- **First 3 experiments**: 1) Sanity check: Reproduce CIFAR-10 10% forgetting with ResNet-18 4-bit; verify AG ~0.58% and compare FA/RA gaps against Table 1; 2) Ablation: Remove GOP (set α=0) and observe AG degradation; remove EGU (use random labels) and observe FA/MIA degradation per Table 2; 3) Hyperparameter sweep: Test α ∈ {0.5, 0.75, 1.0} on CIFAR-100 30% forgetting to characterize tradeoff between forgetting aggressiveness and retain protection

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text. However, based on the limitations section, key unresolved questions include whether gradient orthogonal projection maintains its utility preservation guarantees beyond first-order approximation, how OEU performs under extremely aggressive quantization regimes like binary/ternary quantization, and how the framework handles sequential or continual unlearning requests over multiple rounds.

## Limitations

- Missing critical hyperparameters: exact β weight in the combined loss, numerical stability ε for layer normalization, and precise learning rates per dataset
- Limited exploration of edge cases where forget and retain gradients are nearly collinear
- No ablation on the orthogonality strength α across datasets beyond the single CIFAR-100 example

## Confidence

- **High**: The entropy-guided unlearning mechanism and its mathematical foundation (Theorems 4.1-4.2)
- **High**: The orthogonal projection theory and its first-order approximation guarantee (Theorems 4.3-4.4)
- **Medium**: Layer-wise normalization improves over global projection, though corpus validation is weak
- **Low**: STE gradient reliability in 2-bit quantization settings is not empirically validated

## Next Checks

1. Reproduce the CIFAR-100 30% forgetting experiment with ResNet-18 4-bit quantization to verify AG of 1.98% and compare against the reported Q-MUL baseline of 3.90%
2. Implement ablation removing GOP (α=0) and EGU (random labels) to confirm Table 2's performance degradation patterns
3. Sweep α ∈ {0.5, 0.75, 1.0} on CIFAR-100 to quantify the tradeoff between forgetting effectiveness and retain accuracy preservation