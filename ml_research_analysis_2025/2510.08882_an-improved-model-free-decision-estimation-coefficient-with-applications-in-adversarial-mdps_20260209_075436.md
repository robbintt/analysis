---
ver: rpa2
title: An Improved Model-Free Decision-Estimation Coefficient with Applications in
  Adversarial MDPs
arxiv_id: '2510.08882'
source_url: https://arxiv.org/abs/2510.08882
tags:
- assumption
- lemma
- setting
- learning
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Dig-DEC, a new model-free DEC approach that\
  \ improves upon optimistic DEC by removing the optimism principle and relying purely\
  \ on information gain for exploration. The key contribution is a general framework\
  \ for decision making in \u03A6-restricted environments that handles both stochastic\
  \ and hybrid MDPs (with stochastic transitions and adversarial rewards)."
---

# An Improved Model-Free Decision-Estimation Coefficient with Applications in Adversarial MDPs

## Quick Facts
- **arXiv ID**: 2510.08882
- **Source URL**: https://arxiv.org/abs/2510.08882
- **Reference count**: 40
- **Primary result**: Introduces Dig-DEC, a model-free DEC approach that removes optimism and achieves improved regret bounds (T^(2/3) → √T) for Bellman-complete MDPs while handling adversarial environments

## Executive Summary
This paper introduces Dig-DEC, a novel model-free decision-making framework that improves upon optimistic DEC by replacing the optimism principle with pure information gain for exploration. The key innovation is a general framework for decision-making in Φ-restricted environments that can handle both stochastic and hybrid MDPs (with stochastic transitions and adversarial rewards). By partitioning the model space into infosets based on shared value functions, Dig-DEC achieves strictly better performance than optimistic DEC in certain cases and resolves the open problem of model-free learning in hybrid MDPs with bandit feedback. The framework provides sublinear regret bounds and demonstrates that removing optimism is crucial for handling adversarial environments.

## Method Summary
The method maintains a distribution ρ over infosets (Φ) rather than individual models, where each infoset contains models that induce the same Q*-function. At each round, the algorithm solves a saddle-point problem to get policy and model distributions, executes the sampled policy, and updates the posterior using information gain (KL divergence). Two variants exist: a batching approach for average error minimization and a two-timescale bi-level learning procedure for squared error minimization in Bellman-complete MDPs. The key insight is that exploration can be driven purely by information gain rather than optimistic value estimates, enabling the framework to handle adversarial reward structures.

## Key Results
- Dig-DEC achieves strictly better regret bounds than optimistic DEC, with improvement factors that can be arbitrarily large in special cases
- For average estimation error minimization, improves regret from T^(3/4)/T^(5/6) to T^(2/3)/T^(7/9)
- For squared error minimization in Bellman-complete MDPs, improves regret from T^(2/3) to √T, matching optimism-based approaches for the first time
- Provides first sublinear regret bounds for model-free learning in hybrid MDPs with bandit feedback under general transition structures
- Removes the optimism principle, enabling handling of adversarial reward environments

## Why This Works (Mechanism)

### Mechanism 1: Dual Information Gain for Exploration
Replacing optimism with a dual information gain term allows exploration without explicit reward estimation, enabling adversarial environment handling. Dig-DEC adds two information-gain terms: KL(ν, ρ) for regularization and E_π~p E_M~ν E_o~M(·|π)[KL(ν(·|π,o), ν)] for information acquisition. This replaces the optimism term V_ϕ(π_ϕ) that requires knowing optimal values.

### Mechanism 2: Φ-Restricted Environment Partitioning
Partitioning the model space into infosets based on shared value functions reduces estimation complexity from log|M| to log|Φ|. Instead of estimating full models M ∈ M, the learner partitions M×Π into disjoint infosets ϕ ∈ Φ where each infoset contains models that induce the same Q*-function. The algorithm maintains a distribution ρ over infosets rather than individual models.

### Mechanism 3: Two-Timescale Posterior Update for Squared Error
A bi-level learning procedure with a biased top-layer loss achieves constant Est error, improving T^(2/3) to √T regret in Bellman-complete MDPs. The update maintains both a distribution ρ over infosets and auxiliary distributions q(·|ϕ) over reference functions, exploiting the self-bounding property of squared error.

## Foundational Learning

- **Concept: Decision-Estimation Coefficient (DEC)**
  - Why needed here: Dig-DEC is the central complexity measure; understanding how DEC trades off regret vs estimation error is essential for grasping why removing optimism works
  - Quick check question: Can you explain why DEC characterizes both lower and upper bounds on regret in DMSO problems?

- **Concept: KL Divergence and Information Gain**
  - Why needed here: The entire Dig-DEC mechanism relies on using KL divergence as an information-gain measure to replace optimism; the posterior update and concentration arguments depend on information-theoretic bounds
  - Quick check question: What is the relationship between KL divergence and expected log-likelihood ratio, and why does Pinsker's inequality matter for the analysis?

- **Concept: Mirror Descent and Bregman Divergence**
  - Why needed here: The analysis in Equation (5) and Appendix E uses mirror descent with Bregman divergences to prove regret bounds; understanding this optimization framework is necessary to follow the proof structure
  - Quick check question: How does the first-order optimality condition (Lemma 18) connect the Bregman divergence to the saddle-point optimization in Eq. (3)?

## Architecture Onboarding

- **Component map**: Initialize ρ_1 over Φ → Solve min_p max_ν AIR^Φ,D_η(p, ν; ρ_t) → Sample π_t ~ p_t → Execute π_t, observe o_t → Update ρ_{t+1} via POSTERIORUPDATE → (For squared error) Update q_{t+1} via exponential weights

- **Critical path**: 1) Initialize ρ_1 uniformly over Φ; 2) Each round: Solve min_p max_ν AIR^Φ,D_η(p, ν; ρ_t) → get (p_t, ν_t); 3) Execute π_t ~ p_t, observe o_t; 4) Update ρ_{t+1} via POSTERIORUPDATE based on divergence choice; 5) For squared error: Also update auxiliary q_{t+1} via exponential weights

- **Design tradeoffs**: D_av (average error) vs D_sq (squared error): Average error gives O(T^(1/3) log|Φ|) Est but requires batching; squared error gives O(log²|Φ|) Est but requires Bellman completeness. Batching parameter τ: Algorithm 4's τ = T^(1/3) trades off statistical efficiency vs adaptivity. Learning rate η: Must be tuned jointly with DEC complexity.

- **Failure signatures**: Vacuous bounds if dig-dec_Φ ≈ 1 for all η; Estimation blowup if Est exceeds O(log|Φ|) or O(T^(1/3) log|Φ|); Numerical instability with very small probabilities in KL computations.

- **First 3 experiments**: 1) Tabular MDP baseline: 3-state, 2-action MDP with known transitions; verify regret matches O(H√(d log|Φ| T)); compare optimistic E2D vs Dig-DEC on 3-arm bandit example. 2) Linear MDP with adversarial rewards: Test hybrid setting with linear Q* structure; use known linear reward features; measure regret against changing reward functions. 3) Ablation on batching vs bi-level: For Bellman-complete MDPs, compare Algorithm 2 (batching, D_av) vs Algorithm 3 (bi-level, D_sq); expected: bi-level achieves √T vs batching's T^(2/3).

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the Dig-DEC framework be extended to handle hybrid low-rank MDPs with unknown reward features? The current partition scheme under Assumption 3 forces polynomial dependence on the number of feature mappings, while alternative two-phase approaches that avoid this design remain difficult to integrate into the DEC framework.

- **Open Question 2**: Can Assumption 4 (linear reward with known features) be relaxed while maintaining sublinear regret? The current analysis relies on the linear structure of rewards and known features to construct the estimation functions; removing this breaks the connection between reward observations and value function updates.

- **Open Question 3**: What are the fundamental limits of model-free learning in terms of lower bounds? While the paper provides improved upper bounds, matching or approaching existing lower bounds in some cases, a systematic characterization of model-free lower bounds analogous to the DEC framework's lower bounds remains unexplored.

- **Open Question 4**: Can high-probability regret bounds be achieved in the hybrid setting? The proof technique that enables high-probability bounds relies on properties that do not extend when the reward structure is adversarial.

## Limitations
- The framework requires careful construction of the Φ partition structure, which is theoretically specified but lacks practical implementation guidance
- The two-timescale bi-level learning procedure introduces significant complexity that could be numerically unstable in practice
- The saddle-point solver for the core optimization problem is not specified in implementation detail
- The approach may not scale well to very large state/action spaces due to the need to maintain distributions over infosets

## Confidence
- **High confidence**: The theoretical framework for Φ-restricted environments and the DIG-DEC construction are mathematically sound based on KL divergence properties and information theory
- **Medium confidence**: The improved regret bounds depend critically on Bellman completeness and the self-bounding property in Assumption 6, which may not hold in all practical settings
- **Low confidence**: The practical implementation details are sparse - the saddle-point solver, the posterior update algorithms, and the construction of Φ are all theoretically specified but lack implementation guidance

## Next Checks
1. **Construct Φ for a concrete MDP**: Implement the infoset partitioning for a 4-state, 3-action tabular MDP with linear Q*-function structure. Verify that the partition captures the shared-value structure and that DIG-DEC achieves better regret than optimistic approaches on this instance.
2. **Test Bellman completeness requirement**: Create a linear MDP where the Bellman completeness assumption fails. Measure whether Algorithm 3 still achieves √T regret or if it degrades to the baseline T^(2/3) rate.
3. **Ablate the information gain term**: Modify DIG-DEC to remove the information acquisition term while keeping the regularization KL term. Compare regret on a hybrid MDP with adversarial rewards to isolate the contribution of pure information-gain exploration versus regularization.