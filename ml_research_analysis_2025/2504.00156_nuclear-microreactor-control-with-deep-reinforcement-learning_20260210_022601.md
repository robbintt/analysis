---
ver: rpa2
title: Nuclear Microreactor Control with Deep Reinforcement Learning
arxiv_id: '2504.00156'
source_url: https://arxiv.org/abs/2504.00156
tags:
- control
- nuclear
- power
- reactor
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores deep reinforcement learning (RL) for drum-based
  control in nuclear microreactors, addressing the need for efficient load-following
  control in compact, autonomous systems. Using a point kinetics model with thermal
  and xenon feedback, the study compares single-agent RL, multi-agent RL (MARL), and
  PID controllers.
---

# Nuclear Microreactor Control with Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2504.00156
- Source URL: https://arxiv.org/abs/2504.00156
- Reference count: 40
- Primary result: RL controllers achieved similar or superior performance to PID in drum-based microreactor control, with MARL enabling independent drum control while maintaining symmetry.

## Executive Summary
This paper explores deep reinforcement learning (RL) for drum-based control in nuclear microreactors, addressing the need for efficient load-following control in compact, autonomous systems. Using a point kinetics model with thermal and xenon feedback, the study compares single-agent RL, multi-agent RL (MARL), and PID controllers. RL controllers achieved similar or superior performance to PID, with single-agent RL reducing tracking error in short transients and maintaining within 1% error in extended scenarios despite training only on short profiles. MARL enabled independent drum control while preserving symmetry, a capability PID and single-agent RL lacked. RL controllers also demonstrated greater robustness to Gaussian noise in power measurements. These findings highlight RL's potential for autonomous nuclear reactor control, with MARL offering unique advantages for multi-drum systems. The work lays the groundwork for future integration into high-fidelity simulations and experimental validation.

## Method Summary
The authors implemented a point kinetics model with thermal and xenon feedback to simulate a nuclear microreactor with drum control. They trained and compared three control approaches: a single-agent RL controller, a multi-agent RL (MARL) controller, and a baseline PID controller. The RL agents were trained using proximal policy optimization (PPO) on short transient load-following scenarios. Performance was evaluated across various transient profiles, including short step changes and extended complex power demands, with added Gaussian noise to power measurements to test robustness.

## Key Results
- RL controllers achieved similar or superior performance to PID in tracking power demand profiles
- Single-agent RL reduced tracking error in short transients and maintained within 1% error in extended scenarios despite training only on short profiles
- MARL enabled independent drum control while preserving symmetry, a capability PID and single-agent RL lacked
- RL controllers demonstrated greater robustness to Gaussian noise in power measurements

## Why This Works (Mechanism)
The RL controllers learned optimal drum actuation policies through trial-and-error in simulation, capturing the complex, nonlinear dynamics of nuclear reactor control that traditional PID controllers approximate with fixed parameters. The MARL approach allowed each drum to develop specialized policies while maintaining system-wide symmetry through shared observation spaces and coordinated action spaces.

## Foundational Learning
- **Point Kinetics Modeling**: Simplified neutron population dynamics using a single effective group
  - Why needed: Reduces computational complexity while capturing core dynamics
  - Quick check: Verify reactivity insertions match expected power response curves
- **Deep Reinforcement Learning**: Agents learn control policies through interaction with simulated environment
  - Why needed: Handles complex, nonlinear control problems beyond traditional control theory
  - Quick check: Monitor training reward curves for convergence
- **Proximal Policy Optimization (PPO)**: Stable RL algorithm that limits policy updates
  - Why needed: Prevents destructive policy updates during training
  - Quick check: Verify KL divergence stays within acceptable bounds
- **Multi-Agent Coordination**: Multiple RL agents controlling different drums simultaneously
  - Why needed: Enables independent control while maintaining system symmetry
  - Quick check: Verify power distribution remains symmetric across drum pairs
- **Xenon Feedback Dynamics**: Incorporated xenon-135 poisoning effects on reactivity
  - Why needed: Critical for accurate long-term transient behavior
  - Quick check: Verify xenon buildup/decay matches analytical predictions
- **Gaussian Noise Injection**: Added measurement uncertainty to test robustness
  - Why needed: Evaluates controller performance under realistic sensor conditions
  - Quick check: Monitor controller response to varying noise levels

## Architecture Onboarding

**Component Map:**
Point Kinetics Model -> RL Agents (Single/Agent MARL) -> Drum Actuators -> Power Output -> Observation Space

**Critical Path:**
Power demand profile -> Observation space (power, xenon, temperature) -> RL policy network -> Drum control commands -> Reactivity insertion -> Power output

**Design Tradeoffs:**
- Simplified point kinetics vs. full spatial resolution: computational efficiency vs. accuracy
- Synchronous vs. asynchronous drum control: training simplicity vs. real-world applicability
- Gaussian noise vs. real sensor noise: controlled testing vs. realistic conditions
- Short training profiles vs. extended validation: training efficiency vs. generalization

**Failure Signatures:**
- Power overshoot/undershoot beyond safety limits
- Xenon oscillations not damped
- Drum control commands exceeding physical limits
- Asymmetric power distribution in MARL controller
- Reward function divergence during training

**First 3 Experiments:**
1. Implement single-agent RL controller in point kinetics simulator with basic load-following profile
2. Add Gaussian noise to power measurements and evaluate robustness compared to PID
3. Deploy MARL controller and verify symmetric power distribution across drum pairs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can RL controllers effectively manage spatial power distributions within high-fidelity multiphysics simulations?
- Basis in paper: [explicit] The authors state that future studies must validate RL controllers within high-fidelity multiphysics simulations to address the lack of spatial resolution inherent in the point kinetics model used here.
- Why unresolved: The point kinetics model assumes a single effective neutron population, preventing the analysis of local power tilts, radial asymmetries, or spatial oscillations that might occur in a real core.
- What evidence would resolve it: Successful demonstration of MARL maintaining power symmetry and following load profiles in a 2D/3D coupled neutronics and thermal-hydraulics simulation.

### Open Question 2
- Question: What are the capabilities and limits of RL controllers in accident scenarios involving super-prompt criticality?
- Basis in paper: [explicit] The paper explicitly states that understanding RL performance in accident scenarios is crucial for it to be seriously considered for reactor control, a scenario currently handled by independent safety systems.
- Why unresolved: This work focused on operational load-following and assumed emergency control rods would handle shutdowns, excluding accident dynamics from the RL agent's training and testing scope.
- What evidence would resolve it: Benchmarking RL agent behavior and stability during simulated accident transients that require rapid, high-worth reactivity insertions.

### Open Question 3
- Question: How does asynchronous drum actuation affect MARL policy performance compared to the idealized synchronous timesteps used in training?
- Basis in paper: [explicit] The authors note that real-world actions are not perfectly simultaneous and suggest further study is needed on the effects of actions taken at varying times and observations that are agnostic to absolute time.
- Why unresolved: The current framework utilized exact 1-second timesteps and simultaneous action application for all eight drum agents, which may not reflect hardware latencies.
- What evidence would resolve it: Evaluation of trained policies in an environment that introduces stochastic delays in actuation and observation availability.

## Limitations
- Simplified point kinetics model lacks spatial resolution and full multiphysics coupling
- Training and validation conducted entirely in simulation without experimental verification
- Gaussian noise model may not represent real sensor noise characteristics in reactor environments
- Study focuses only on drum-based control, limiting generalizability to other reactor designs

## Confidence

**High confidence** in the comparative performance metrics between RL and PID controllers within the simulation framework, given the clear methodology and consistent results across multiple scenarios.

**Medium confidence** in the robustness claims to measurement noise, as the noise model is simplified and may not capture all real-world conditions.

**Medium confidence** in the scalability and practical applicability of MARL for larger multi-drum systems, as the study only demonstrates this for two-drum configurations.

## Next Checks

1. Implement the trained RL controllers in a high-fidelity, spatially-dependent reactor simulator (e.g., MOOSE-based tools) to assess performance degradation due to model mismatch.

2. Conduct hardware-in-the-loop testing using a scaled experimental facility or a high-fidelity simulator coupled with actual control hardware to validate noise robustness and real-time performance.

3. Perform sensitivity analysis on RL controller performance with respect to variations in fuel composition, temperature coefficients, and xenon dynamics to evaluate robustness across different microreactor designs.