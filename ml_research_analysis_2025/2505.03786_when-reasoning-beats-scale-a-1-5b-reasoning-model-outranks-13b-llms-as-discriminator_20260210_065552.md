---
ver: rpa2
title: 'When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator'
arxiv_id: '2505.03786'
source_url: https://arxiv.org/abs/2505.03786
tags:
- reasoning
- table
- performance
- planning
- discrimination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks a distilled 1.5B parameter reasoning model
  (DeepSeek-R1) against several state-of-the-art non-reasoning LLMs within a generator-discriminator
  LLM planning framework for the text-to-SQL task. A novel method is introduced to
  extract soft scores from the chain-of-thought outputs of reasoning models, enabling
  fine-grained ranking of candidates.
---

# When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator
## Quick Facts
- arXiv ID: 2505.03786
- Source URL: https://arxiv.org/abs/2505.03786
- Reference count: 40
- Primary result: A distilled 1.5B reasoning model (DeepSeek-R1) outperforms 13B parameter non-reasoning LLMs as a discriminator in text-to-SQL tasks

## Executive Summary
This study demonstrates that a 1.5B parameter reasoning model (DeepSeek-R1) can outperform significantly larger non-reasoning LLMs as a discriminator within a generator-discriminator LLM planning framework for text-to-SQL tasks. The key innovation is a method for extracting soft scores from chain-of-thought outputs, enabling fine-grained ranking of candidate SQL queries. Despite having far fewer parameters, the distilled reasoning model achieves up to 87% higher F1 and 3.7% better discrimination accuracy than larger models, while finding generation more challenging than discrimination. The work establishes reasoning models as superior discriminators in agentic frameworks, challenging the assumption that scale always wins.

## Method Summary
The study evaluates a generator-discriminator LLM planning framework for text-to-SQL using the Spider dataset. The discriminator uses DeepSeek-R1-Distill-Qwen-1.5B with a novel JSON-based soft score extraction method from chain-of-thought outputs. Candidates are generated using non-reasoning LLMs (CodeLlama, TinyLlama, DeepSeek-Coder) with max_length=300, temperature=0.2, and num_return_sequences=5. Two re-ranking settings are tested: naive discrimination and enhanced with executability checking. Intrinsic evaluation uses oracle-generated outputs, while end-to-end evaluation measures execution accuracy. Token budget experiments sweep from 256 to 2048 tokens to identify performance saturation points.

## Key Results
- DeepSeek-R1-1.5B achieves 87% higher F1 and 3.7% better discrimination accuracy than CodeLlama-7B
- Same reasoning model achieves 3.7% higher execution accuracy than CodeLlama-13B despite 8.7x fewer parameters
- Reasoning models exhibit inverted capability asymmetry: 56.0% execution accuracy as discriminator vs. only 7.2% as generator
- Performance plateaus at 1024 tokens, with diminishing returns and increased redundancy beyond this threshold
- Adding schema context degrades performance by 4.3% and increases failure rates by 215%

## Why This Works (Mechanism)

### Mechanism 1: Soft Score Extraction from Structured CoT Outputs Enables Fine-Grained Ranking
Reasoning models can produce discriminative scores comparable to logit-based methods by parsing structured JSON outputs from chain-of-thought reasoning. The model is prompted to output a final verdict in JSON format (`{"correct": true/false}`), and the logit associated with the key value is extracted and softmax-normalized to produce a probability score. This converts an unstructured reasoning trace into a scalar suitable for ranking.

### Mechanism 2: Reasoning Models Exhibit Inverted Capability Asymmetry (Discrimination ≫ Generation)
Reasoning models are comparatively stronger at discrimination than generation, reversing the pattern observed in non-reasoning LLMs. The deliberative CoT process decomposes evaluation into verifiable substeps, improving judgment accuracy. For generation, however, the open-ended CoT can branch into unproductive paths, compounding errors and degrading output quality.

### Mechanism 3: Reasoning Capacity Saturates Beyond Moderate Compute Budgets
Increasing test-time compute (token limits) improves discrimination only up to a threshold (~1024 tokens), after which returns diminish and redundancy increases. At low token limits, the model cannot complete its reasoning trace, causing high failure rates. As budget increases, reasoning completes and performance peaks. Beyond the threshold, additional tokens produce repetitive, low-information content without improving decisions.

## Foundational Learning

- **Generator-Discriminator Planning Framework**
  - Why needed here: The paper's entire evaluation sits within this three-stage architecture (generation → discrimination → planning/ranking). Understanding the roles clarifies why reasoning models excel at one role but not the other.
  - Quick check question: In a re-ranking planning strategy, which component is responsible for assigning scores to candidate solutions?

- **Chain-of-Thought (CoT) Reasoning**
  - Why needed here: The mechanism distinguishing reasoning models from non-reasoning LLMs is the internal deliberation process that produces intermediate reasoning traces before the final output.
  - Quick check question: What is the primary challenge in extracting soft scores from CoT outputs compared to direct logit extraction from non-reasoning models?

- **Test-Time Compute Scaling**
  - Why needed here: The paper explicitly investigates how varying inference-time token budgets affects discrimination performance, revealing diminishing returns.
  - Quick check question: At approximately what token budget does Distill-R1's discrimination performance plateau in the text-to-SQL task?

## Architecture Onboarding

- **Component map**: Generator LLM -> Discriminator LLM -> Soft Score Extractor -> Executability Checker (optional) -> Re-ranking Planner

- **Critical path**: 1. Generator produces N candidates (num_return_sequences=5). 2. Each candidate is passed to the discriminator with the natural language question. 3. Discriminator outputs CoT + JSON verdict; soft score is extracted. 4. (Optional) Executability check filters non-runnable queries. 5. Planner ranks and returns the highest-scoring executable query.

- **Design tradeoffs**:
  - Binary vs. logit-based soft scoring: End-to-end impact is minimal (<1.5% accuracy difference), but soft scores provide better pairwise discrimination.
  - Naive vs. executability-enhanced discrimination: Executability checks improve hard-problem performance (18.2% relative gain on extra-hard queries) but add latency.
  - Token budget: 1024 tokens recommended as default; lower budgets cause failures, higher budgets add redundancy without gains.

- **Failure signatures**:
  - Malformed JSON output from discriminator → falls back to score −0.5, potentially misranking valid candidates.
  - Extremely low token budget (<256) → >94% failure rate, near-random discrimination.
  - Schema augmentation in prompt → failure rate increases by 215%, likely due to distraction or context dilution.

- **First 3 experiments**:
  1. Reproduce the intrinsic discrimination evaluation: Use the 400-example Spider subset, compare Distill-R1 vs. CodeLlama-7B/13B on F1, Hit@1, and pairwise accuracy metrics.
  2. Sweep token budgets: Run discrimination at 256, 512, 1024, 1536, 2048 tokens to verify the 1024-token saturation point and observe reasoning quality degradation.
  3. Test the generator-discriminator swap: Run Distill-R1 as generator (greedy decoding) and compare execution accuracy against TinyLlama-1.1B and DeepSeek-Coder-1.3B to confirm the asymmetry claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can targeted fine-tuning using the extracted soft scores further improve the reasoning model's discrimination performance?
- Basis in paper: The authors state in the Discussion: "it is potentially possible to fine-tune such models (with soft score as the cost function) and adjust their reasoning approaches to improve performance even further. We plan to explore this in future studies."
- Why unresolved: The current study relies on off-the-shelf distilled models without updating model weights based on the specific discrimination feedback loop proposed.
- What evidence would resolve it: Comparative performance metrics showing a significant accuracy increase after fine-tuning the reasoning model specifically on the soft-score objective derived from its CoT outputs.

### Open Question 2
- Question: Does the superior performance of reasoning models as discriminators generalize to tasks beyond text-to-SQL?
- Basis in paper: The authors note in the Limitations section that "experiments are conducted within the specific context of text-to-SQL generation... which may not directly generalize to other domains or reasoning tasks."
- Why unresolved: The text-to-SQL task relies on specific logical constraints and syntax; it remains unclear if the findings hold for open-ended reasoning or multi-modal tasks.
- What evidence would resolve it: Benchmarking the same generator-discriminator framework with DeepSeek-R1 on diverse domains like mathematical reasoning (GSM8K) or code generation (HumanEval).

### Open Question 3
- Question: Why does the addition of database schema context degrade the discrimination performance of reasoning models?
- Basis in paper: Section 4.1.3 shows that adding schema context results in a 4.3% drop in accuracy and a 215% increase in failure rates, contradicting standard prompting intuition.
- Why unresolved: The authors identify increased redundancy and "circular reasoning" but do not determine if this is a fundamental limitation of current context windows or a prompt-engineering failure.
- What evidence would resolve it: An ablation study varying context formats (e.g., RAG vs. full schema) or analyzing attention mechanisms to identify where the reasoning trace diverges due to context overload.

## Limitations
- JSON Output Dependency: The soft score extraction method critically depends on the reasoning model producing valid JSON with the expected key structure, with fallback to -0.5 when parsing fails.
- Limited Generalization: The study focuses exclusively on text-to-SQL tasks within the Spider dataset, limiting applicability to other domains.
- Parameter Size Constraints: Only a 1.5B parameter reasoning model is evaluated against non-reasoning models ranging from 1.1B to 13B parameters.

## Confidence
- High Confidence: The core finding that reasoning models outperform non-reasoning LLMs of similar or larger size as discriminators (3.7-87% accuracy improvements) is well-supported by multiple evaluation metrics and statistical comparisons across different difficulty levels.
- Medium Confidence: The claim about inverted capability asymmetry (reasoning models excel at discrimination but underperform at generation) is supported by the specific experimental setup but requires further validation across different task types and model sizes.
- Medium Confidence: The saturation of reasoning capacity beyond 1024 tokens is demonstrated through controlled experiments, but the exact threshold may vary with task complexity and model architecture.

## Next Checks
1. Cross-Domain Validation: Apply the discriminator methodology to a non-SQL task (e.g., code generation, mathematical reasoning, or multi-hop QA) to test whether the inverted capability asymmetry holds beyond text-to-SQL.

2. Model Size Scaling Study: Evaluate reasoning models across a broader parameter range (e.g., 7B, 14B, 32B) to determine whether the discrimination advantage persists at larger scales and whether the generation-underperformance pattern remains consistent.

3. Robustness Testing: Systematically measure the failure rate of JSON output parsing across different prompt complexities and schema contexts to quantify the reliability ceiling of the soft score extraction method.