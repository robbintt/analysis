---
ver: rpa2
title: Complexity of Faceted Explanations in Propositional Abduction
arxiv_id: '2507.14962'
source_url: https://arxiv.org/abs/2507.14962
tags:
- lemma
- complexity
- explanations
- acet
- abduction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces facets to propositional abduction, enabling
  fine-grained reasoning between relevant and necessary propositions. A facet is a
  proposition that belongs to some but not all explanations, capturing variability
  in explanations.
---

# Complexity of Faceted Explanations in Propositional Abduction

## Quick Facts
- **arXiv ID**: 2507.14962
- **Source URL**: https://arxiv.org/abs/2507.14962
- **Reference count**: 40
- **Primary result**: First comprehensive complexity analysis of faceted abduction reasoning across Post's lattice, showing facets are often tractable while diversity is generally harder

## Executive Summary
This paper introduces facets to propositional abduction as a way to reason about variability in explanations without full enumeration. A facet is a proposition that belongs to some but not all explanations, providing intermediate granularity between relevant and necessary propositions. The authors systematically analyze the computational complexity of deciding facets across Post's lattice of Boolean constraint languages, showing that while facets are often tractable (P-time for dualHorn, 2-affine), diversity between explanations is generally harder (NP-hard). The work answers an open question about relevance complexity and provides theoretical foundations for explainable AI applications in diagnosis and planning.

## Method Summary
The paper employs theoretical complexity analysis using Post's lattice classification of Boolean constraint languages. For tractable cases like 2-affine and dualHorn languages, the authors provide polynomial-time algorithms involving variable clustering and equivalence class identification. For harder cases, they establish NP-hardness and Σ₂ᴾ-completeness through many-one reductions from known hard problems. The methodology centers on defining facets as propositions that are relevant but not necessary, then analyzing the decision problem IsFacet(Γ) across different constraint languages Γ.

## Key Results
- Facets provide a tractable middle ground between relevance (finding one explanation) and necessity (finding all explanations)
- IsFacet problem is in P for dualHorn and 2-affine languages using variable clustering algorithms
- IsFacet becomes NP-hard for implicative languages and Σ₂ᴾ-hard for general Horn clauses
- Diversity calculation (finding explanations with minimum distance) is generally harder than facet detection
- The facet classification framework also determines complexity of relevance questions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Intermediate reasoning via "facets" offers granular understanding of solution variability without computational cost of full enumeration
- **Mechanism**: Defines literal x as facet if (1) Relevance: ∃ explanation E₁ where x ∈ E₁, and (2) Dispensability: ∃ explanation E₂ where x ∉ E₂
- **Core assumption**: User requires insight into solution flexibility rather than single solution or exhaustive enumeration
- **Evidence anchors**: Abstract mentions facets "maintain favorable complexity," Section 3 defines facets formally, related work shows counting is computationally expensive
- **Break condition**: If minimality not enforced, non-minimal explanations may falsely qualify as facets

### Mechanism 2
- **Claim**: Complexity is tractable (P) for specific constraint languages because IsFacet reduces to checking variable equivalence classes
- **Mechanism**: For 2-affine, identifies clusters of dependent variables; x is facet if in necessary equivalence class but can be swapped for another variable while maintaining consistency
- **Core assumption**: KB conforms to constraint language closed under specific algebraic operations where SAT(Γ⁺) is in P
- **Evidence anchors**: Lemma 3.8 proves IsFacet(2-affine) ∈ P, Section 3.1 describes clustering approach
- **Break condition**: If constraint language includes general Horn clauses or arbitrary formulas, mechanism fails (becomes NP-hard)

### Mechanism 3
- **Claim**: Measuring diversity (distance between explanations) is computationally harder than identifying facets
- **Mechanism**: Diversity check Div-ABD asks for two explanations with symmetric difference ≥ k; requires solving constrained satisfaction problems mapping to NP-hard Positive 2-SAT variants
- **Core assumption**: System attempts to use same solver logic for diversity as facet detection
- **Evidence anchors**: Abstract states diversity "generally harder than facets," Section 4 proves NP-hardness via reduction
- **Break condition**: If distance threshold k is small or fixed, complexity may drop, but general k resists P-time solutions

## Foundational Learning

- **Concept**: **Post's Lattice & Constraint Languages**
  - **Why needed here**: Entire complexity classification depends on identifying which clone or constraint language KB belongs to
  - **Quick check question**: Can you distinguish between Horn clause (at most one positive literal) and dualHorn clause (at most one negative literal)?

- **Concept**: **Abductive Reasoning (Propositional)**
  - **Why needed here**: Base formalism; must understand Abduction ≠ Deduction
  - **Quick check question**: Given KB = {a → b} and M = {b}, is E = {a} an explanation? What about E = {b}?

- **Concept**: **Symmetric Difference (Δ)**
  - **Why needed here**: Used to define distance between explanations in diversity section
  - **Quick check question**: If E₁ = {a, b} and E₂ = {b, c}, what is E₁ Δ E₂? (Answer: {a, c}, distance 2)

## Architecture Onboarding

- **Component map**: Parser for propositional formulas -> Constraint language classifier -> Solver Core (P-time or NP-oracle) -> Query Engine (Relevance/Necessity check) -> Facet identification -> Diversity Module
- **Critical path**: Classification of input language Γ is single point of failure; misidentification causes incorrect results
- **Design tradeoffs**: Facets faster than enumeration but less information; restricting to dualHorn/2-affine guarantees speed but limits expressiveness
- **Failure signatures**: Performance cliff when moving from dualHorn to Horn inputs; false positives identifying necessary variables as facets
- **First 3 experiments**:
  1. Verify tractability: Generate random 2-affine/dualHorn KBs, run P-time IsFacet algorithm, verify polynomial scaling
  2. Hardness validation: Construct implicative language instances, compare facet vs diversity runtime to confirm P vs NP-hard gap
  3. Reduction testing: Implement reduction from Lemma 3.15 to verify complexity lower bounds hold

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Is IsFacet(Γ) tractable, NP-complete, or NP-intermediate for constraint languages of even-length affine linear equations (co-clones IL and IL1)?
- **Basis**: Paper identifies these as "two open cases" and "interesting future research questions"
- **Why unresolved**: Standard algebraic techniques covered almost all cases but failed for these specific affine languages
- **Evidence**: Formal complexity proof determining if these instances reside in P, are NP-complete, or fall into distinct complexity class

### Open Question 2
- **Question**: What is full complexity classification for diverse abduction problem (Div-ABD) when restricted to affine constraint languages?
- **Basis**: Conclusion suggests classifying Div-ABD for affine languages is "possible way forward"
- **Why unresolved**: Paper establishes general hardness but doesn't provide complete classification for affine fragment
- **Evidence**: Systematic classification of Div-ABD complexity across affine co-clones in Post's lattice

### Open Question 3
- **Question**: Can complexity results extend to Abductive Logic Programming (ALP) utilizing stable model semantics?
- **Basis**: Conclusion explicitly asks about extending results to stable models in ALP
- **Why unresolved**: Current work assumes propositional formula KB; ALP involves logic program rules with different structural constraints
- **Evidence**: Complexity analysis of IsFacet within ALP framework determining if trichotomy results hold for stable models

## Limitations
- Complexity classification relies heavily on Post's lattice theory which may not map to practical mixed constraint implementations
- Tractability results assume idealized constraint solvers not readily available in practice
- Complexity jump from tractable to intractable cases is abrupt, making practical implementation challenging with mixed constraint types

## Confidence

- **High confidence**: Theoretical framework for defining facets and relationship to relevance/necessity
- **Medium confidence**: P-time algorithms for 2-affine and dualHorn languages requiring specialized solvers
- **Medium confidence**: NP-hardness proofs for diversity and certain constraint languages, though reductions appear sound

## Next Checks

1. Implement and benchmark P-time IsFacet algorithm for 2-affine languages against randomly generated instances to verify polynomial scaling
2. Construct concrete examples demonstrating hardness gap between facet detection (tractable) and diversity calculation (NP-hard) for implicative languages
3. Test robustness of facet definition by running on non-minimal explanations to confirm necessity of minimality constraint