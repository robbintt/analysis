---
ver: rpa2
title: 3D Gaussian Splat Vulnerabilities
arxiv_id: '2506.00280'
source_url: https://arxiv.org/abs/2506.00280
tags:
- adversarial
- attack
- scene
- gaussian
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a new class of adversarial attacks on 3D
  Gaussian Splatting (3DGS), a rapidly adopted method for real-time 3D scene rendering
  in safety-critical applications like autonomous driving. The authors propose two
  attacks: CLOAK, which embeds view-dependent adversarial textures in training data
  using spherical harmonics so objects appear benign from some angles but adversarial
  from others, and DAGGER, which directly perturbs 3D Gaussian parameters to mislead
  object detectors without access to training images.'
---

# 3D Gaussian Splat Vulnerabilities

## Quick Facts
- arXiv ID: 2506.00280
- Source URL: https://arxiv.org/abs/2506.00280
- Reference count: 9
- 3D Gaussian Splatting attacks can mislead object detectors with view-dependent adversarial textures and direct parameter perturbations

## Executive Summary
This paper identifies a new class of adversarial attacks on 3D Gaussian Splatting (3DGS), a rapidly adopted method for real-time 3D scene rendering in safety-critical applications like autonomous driving. The authors propose two attacks: CLOAK, which embeds view-dependent adversarial textures in training data using spherical harmonics so objects appear benign from some angles but adversarial from others, and DAGGER, which directly perturbs 3D Gaussian parameters to mislead object detectors without access to training images. In experiments, CLOAK reduced YOLOv8 detection confidence for a car at adversarial viewpoints from 110/110 to 32/110, with 78/80 missed detections for overhead "road" views. DAGGER caused Faster R-CNN to misclassify cars as people with >70% confidence in just 11 PGD iterations. These findings expose significant security risks in 3DGS pipelines and motivate new defenses.

## Method Summary
The paper presents two novel adversarial attacks on 3D Gaussian Splatting systems. CLOAK uses spherical harmonics to embed view-dependent adversarial textures during training - objects appear normal from most viewpoints but adversarial from specific angles. DAGGER directly optimizes 3D Gaussian parameters using projected gradient descent to fool downstream object detectors, requiring only access to the scene file rather than training data. Both attacks leverage the differentiable nature of 3DGS rendering to compute gradients that can be backpropagated to individual Gaussian attributes.

## Key Results
- CLOAK reduced YOLOv8 car detection confidence from 110/110 to 32/110 across adversarial viewpoints
- CLOAK achieved 78/80 missed detections for overhead "road" views
- DAGGER caused Faster R-CNN to misclassify cars as people with >70% confidence in just 11 PGD iterations

## Why This Works (Mechanism)

### Mechanism 1: View-Dependent Spherical Harmonics Encoding (CLOAK)
- Claim: Adversarial textures can be concealed within 3DGS scenes and revealed only from specific viewpoints.
- Mechanism: Spherical Harmonics (SH) coefficients define how a Gaussian's color varies with incident viewing direction rather than storing fixed RGB values. By poisoning training images at targeted camera poses C*, the optimization process learns SH coefficients that render benign appearances from non-targeted views and adversarial appearances from attacked views.
- Core assumption: The downstream observer (human or detector) will not exhaustively sample all viewpoints during initial inspection.
- Break condition: If a defense multi-view consistency checks across densely sampled viewpoints, the concealment fails.

### Mechanism 2: Differentiable Rendering Enables Gradient-Based Perturbation (DAGGER)
- Claim: White-box adversaries can directly optimize Gaussian parameters to fool downstream detectors without training data access.
- Mechanism: 3DGS uses differentiable rasterization, meaning gradients flow from rendered 2D output back through renderer to individual Gaussian attributes (position, SH coefficients, scale, rotation, opacity). An attacker with scene file access can apply PGD: render scene, compute detection loss, backpropagate to Gaussian parameters, project perturbations within ε-ball, iterate.
- Core assumption: The attacker has read/write access to the stored 3DGS scene file and knows the downstream model architecture.
- Break condition: If the scene file is encrypted, signed, or access-controlled, the attack surface collapses.

### Mechanism 3: Multi-Stage Detector Vulnerability via Feature Space Manipulation
- Claim: Perturbed 3DGS renders can cause high-confidence misclassifications in two-stage detectors with few iterations.
- Mechanism: Faster R-CNN and similar detectors have region proposal and classification stages. By optimizing rendered appearance directly against the classifier's loss function, the attacker can shift feature representations across class boundaries. The paper achieved >70% confidence misclassification (car→person) in 11 PGD iterations.
- Core assumption: The rendered perturbations survive the detector's preprocessing and remain in-distribution enough to produce confident (but wrong) outputs.
- Break condition: If the detector employs adversarial training or input certification, the perturbation budget may be insufficient.

## Foundational Learning

- Concept: Spherical Harmonics
  - Why needed here: SH coefficients encode view-dependent radiance; understanding this is essential to see how CLOAK hides textures and how DAGGER manipulates color.
  - Quick check question: Given SH coefficients for a Gaussian, can you predict its rendered color from an arbitrary viewpoint?

- Concept: Projected Gradient Descent (PGD)
  - Why needed here: DAGGER generalizes PGD from 2D images to 3D Gaussian parameter space.
  - Quick check question: What constraint does the projection operator Π enforce after each gradient step?

- Concept: Differentiable Rendering
  - Why needed here: Explains why gradients can flow from 2D detector loss back to 3D Gaussian attributes.
  - Quick check question: In a differentiable rasterizer, what operations must remain smooth to permit backpropagation?

## Architecture Onboarding

- Component map: Training images with poses -> 3DGS training -> CLOAK-perturbed scene OR Pre-trained 3DGS scene -> DAGGER PGD optimization -> Misled detector
- Critical path:
  1. For CLOAK: Identify target poses C* → generate adversarial renders → replace training images → train 3DGS normally
  2. For DAGGER: Load scene → select target Gaussians Θ → render → compute loss L(M(R(G)), y) → PGD update → repeat
- Design tradeoffs:
  - CLOAK requires training data access but leaves no gradient trace in final scene
  - DAGGER requires scene file access but achieves targeted misclassification faster (11 iterations vs. full retraining)
  - Both assume the downstream model M is known or transferable
- Failure signatures:
  - CLOAK: Adversarial textures visible from unintended diagonal views (imperfect SH optimization)
  - DAGGER: Perturbations exceed ε-budget, producing visible artifacts or rendering failures
  - Both: Detector still fires with low confidence rather than missing entirely
- First 3 experiments:
  1. Reproduce CLOAK on a single object (car) with two adversarial viewpoints; measure YOLOv8 confidence drop across view trajectory.
  2. Implement DAGGER on a small scene; verify Faster R-CNN misclassification within 15 PGD iterations and log per-iteration confidence.
  3. Test transferability: Apply DAGGER-perturbed scene to a different detector (e.g., SSD) to assess if attack generalizes beyond the white-box target.

## Open Questions the Paper Calls Out
The paper explicitly calls for new defenses to secure 3DGS-based systems and releases its methods openly to support future research. While the authors demonstrate significant vulnerabilities, they do not propose or evaluate any defensive strategies, leaving open questions about how to detect or mitigate these attacks in practice.

## Limitations
- CLOAK assumes observers won't exhaustively sample all viewpoints, but real systems may employ multi-view consistency checking
- DAGGER requires scene file access, which may not hold for secured systems with encrypted or access-controlled assets
- The paper lacks evaluation of attack transferability across different detector architectures and robustness to input preprocessing

## Confidence
- High confidence: Core mechanisms are technically sound with well-established principles
- Medium confidence: Specific attack formulations and parameter choices may require tuning for different scenes
- Low confidence: Limited evaluation of transferability, defensive robustness, and real-world safety implications

## Next Checks
1. Implement a multi-view consistency check that samples 10-20 viewpoints across a scene and verifies object appearance consistency. Test whether CLOAK attacks remain effective under this defense.
2. Apply DAGGER-perturbed scenes to three different object detectors (YOLOv8, Faster R-CNN, SSD) and measure attack success rates to determine cross-detector transferability.
3. Systematically vary the ε constraint in DAGGER (ε = 1.0, 2.5, 5.0, 10.0) and measure the relationship between perturbation magnitude, visual artifacts, and attack success to identify the minimum budget required for >70% misclassification confidence.