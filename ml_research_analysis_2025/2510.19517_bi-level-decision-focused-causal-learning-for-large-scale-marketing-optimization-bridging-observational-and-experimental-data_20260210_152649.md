---
ver: rpa2
title: 'Bi-Level Decision-Focused Causal Learning for Large-Scale Marketing Optimization:
  Bridging Observational and Experimental Data'
arxiv_id: '2510.19517'
source_url: https://arxiv.org/abs/2510.19517
tags:
- data
- marketing
- decision
- optimization
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the problem of resource allocation in online
  marketing, where the goal is to maximize revenue while respecting budget constraints.
  The authors address two main challenges: the prediction-decision misalignment, where
  improved prediction accuracy doesn''t always lead to better decisions, and the bias-variance
  dilemma, where observational data is abundant but biased, while experimental data
  is unbiased but scarce.'
---

# Bi-Level Decision-Focused Causal Learning for Large-Scale Marketing Optimization: Bridging Observational and Experimental Data

## Quick Facts
- arXiv ID: 2510.19517
- Source URL: https://arxiv.org/abs/2510.19517
- Reference count: 40
- Primary result: Bi-DFCL achieves 1.67-1.70% improvement in EOM on marketing datasets by combining biased observational data with scarce-but-unbiased experimental data through bi-level optimization

## Executive Summary
This paper addresses the challenge of resource allocation in online marketing where the goal is to maximize revenue under budget constraints. The authors tackle two key problems: the prediction-decision misalignment where better prediction accuracy doesn't always lead to better decisions, and the bias-variance dilemma where observational data is abundant but biased while experimental data is unbiased but scarce. Their solution, Bi-Level Decision-Focused Causal Learning (Bi-DFCL), uses a bi-level optimization framework that leverages experimental data to guide training from biased observational data, achieving an optimal bias-variance tradeoff. The method has been evaluated on public benchmarks and industrial marketing datasets, as well as large-scale online A/B tests at Meituan, demonstrating statistically significant improvements over state-of-the-art methods and is currently deployed in several real-world marketing scenarios.

## Method Summary
Bi-DFCL addresses the resource allocation problem by formulating it as a bi-level optimization task. The upper level optimizes decision quality using scarce-but-unbiased RCT data, while the lower level optimizes prediction accuracy using abundant-but-biased observational data. A bridge network dynamically generates counterfactual pseudo-labels for observational data using signals learned from RCT, allowing the target model to benefit from observational data's low variance while being corrected by experimental data's low bias. The method employs implicit differentiation via conjugate gradient to efficiently compute meta-gradients, avoiding the computational complexity of explicit differentiation through optimization trajectories. Two primal surrogate losses (PPL and PIFD) are proposed to better align with real-world budget constraints compared to dual formulations.

## Key Results
- Bi-DFCL achieves 1.67-1.70% improvement in Expected Outcome Metric (EOM) over state-of-the-art methods on Marketing Data I
- The framework shows statistically significant improvements in large-scale online A/B tests at Meituan
- Bi-DFCL with implicit differentiation is 20-23% faster (265-295 min vs 345-428 min) than without on Marketing Data II
- Cross-validated experiments show Bi-DFCL maintains performance advantages across different OBS:RCT ratios (5:1 to 100:1)

## Why This Works (Mechanism)

### Mechanism 1
Bi-level optimization achieves better bias-variance tradeoff than training on either data source alone. The upper level optimizes decision loss on scarce-but-unbiased RCT data while the lower level optimizes prediction loss on abundant-but-biased observational data. A Bridge Network generates counterfactual pseudo-labels for observational data using signals learned from RCT, allowing the target model to benefit from observational data's low variance while being corrected by RCT's low bias. Core assumption: RCT data satisfies strong ignorability; observational data contains learnable bias structure that can be partially corrected.

### Mechanism 2
Primal surrogate losses (PPL/PIFD) align better with real-world budget constraints than dual losses. Rather than optimizing over all possible budgets (dual approach), primal losses directly target the specific budget B through softmax relaxation of the discrete allocation decision. The PIFD variant uses finite-difference gradient estimation to avoid relaxation artifacts. Core assumption: The Lagrangian relaxation algorithm provides a sufficiently good approximation to the NP-hard MCKP; budget B is known or can be reasonably estimated.

### Mechanism 3
Implicit differentiation through the lower-level optimum provides more stable meta-gradients than explicit differentiation through optimization trajectory. Instead of unrolling k gradient steps (which can suffer from vanishing gradients and path-dependence), the method uses the implicit function theorem to compute ∂θ*/∂φ directly from the optimality condition. Conjugate gradient solves the linear system without explicit Hessian inversion. Core assumption: The lower-level loss is sufficiently smooth that the Hessian is well-conditioned locally; conjugate gradient converges within a small number of iterations.

## Foundational Learning

- **Bi-level optimization**: Nested optimization framework where upper-level gradients flow through lower-level solutions. Quick check: Can you explain why computing ∂L_upper/∂φ requires differentiating through θ*(φ), and why this is non-trivial?
- **Treatment effect estimation & counterfactuals**: Framework addresses missing counterfactual problem where only one outcome is observed per user-treatment pair. Quick check: Why can't we directly compute the decision loss LDL(θ) from observational data alone?
- **Lagrangian duality for constrained optimization**: MCKP resource allocation uses Lagrangian relaxation; dual variable λ* controls budget satisfaction. Quick check: How does binary search on λ relate to satisfying the budget constraint?

## Architecture Onboarding

- **Component map**: OBS Data -> Lower Level: L_PL on OBS -> Target Network F_θ -> Decision Quality <- Upper Level: L_DL on RCT <- Bridge Network G_φ <- RCT Data
- **Critical path**: 1. Pretrain Teacher F_ψ on RCT with MSE loss and freeze 2. For every k-th batch: compute θ* via k assumed GD steps on L_PL with current pseudo-labels 3. Use CG to compute Jacobian ∂θ*/∂φ from optimality condition 4. Compute decision loss gradient on RCT, backprop through Bridge Network 5. Update F_θ on every batch using refined pseudo-labels
- **Design tradeoffs**: k (assumed GD steps): Higher k = more accurate θ* but slower; paper uses k=5; n_cg (CG iterations): Higher = better Jacobian but O(n) per iteration; paper uses n_cg=50; OBS:RCT ratio: Paper shows 10:1 to 100:1 effective
- **Failure signatures**: Decision loss not decreasing: Check if RCT batch size is too small (<1000) for stable gradient; Training divergence: Bi-level can be unstable; add warm-start (20 epochs) before enabling upper-level updates; Memory issues: CG requires Hessian-vector products; if OOM, reduce batch size or use gradient checkpointing
- **First 3 experiments**: 1. Sanity check: Run TSM-SL on RCT-only as baseline; confirm your AUCC/EOM computation matches Table 2 values (±0.01) 2. Ablation on k: Compare k∈{1,5,10} with fixed n_cg=50 on validation set; expect plateau around k=5 3. Data ratio sensitivity: Vary OBS:RCT from 5:1 to 50:1 on Marketing Data I; plot EOM improvement vs. ratio to find diminishing returns point

## Open Questions the Paper Calls Out
The paper explicitly states that future work includes further improving computational efficiency and applying Bi-DFCL to other decision-making domains beyond marketing optimization.

## Limitations
- Computational efficiency remains a challenge, with Bi-DFCL requiring 6-7× training time compared to two-stage methods
- Performance depends on the quality of RCT data and the assumption of strong ignorability, which may be violated in practice
- The framework's effectiveness for domains beyond marketing optimization remains untested

## Confidence

- **High confidence**: The decision-focused causal learning framework addresses a real and important problem in marketing optimization; the mathematical formulation of bi-level optimization and implicit differentiation is sound
- **Medium confidence**: The specific implementation choices (k=5 assumed GD steps, n_cg=50 CG iterations, OBS:RCT ratios) appear reasonable based on ablation studies, but optimal hyperparameters may vary by dataset
- **Low confidence**: The long-term stability of models deployed in production at Meituan hasn't been validated beyond initial deployment metrics

## Next Checks

1. **Robustness to RCT scarcity**: Systematically vary RCT proportion from 1% to 20% of OBS data and measure performance degradation to identify minimum effective RCT sample size
2. **Generalization across budget levels**: Test model performance across a wider range of budget constraints (B values) not seen during training to evaluate out-of-distribution generalization
3. **Comparison with simpler ensemble methods**: Benchmark against straightforward approaches like weighted averaging of OBS-only and RCT-only models to quantify the value of the complex bi-level optimization