---
ver: rpa2
title: 'Cross-Lingual Stability of LLM Judges Under Controlled Generation: Evidence
  from Finno-Ugric Languages'
arxiv_id: '2602.02287'
source_url: https://arxiv.org/abs/2602.02287
tags:
- language
- across
- coherence
- judge
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates cross-lingual evaluation stability of large
  language models (LLMs) by controlling dialogue generation parameters across Estonian,
  Finnish, and Hungarian while varying only the target language. Using synthetic customer-support
  dialogues, the study tests whether LLM-as-a-judge scoring produces stable model
  rankings across these morphologically rich Finno-Ugric languages.
---

# Cross-Lingual Stability of LLM Judges Under Controlled Generation: Evidence from Finno-Ugric Languages

## Quick Facts
- arXiv ID: 2602.02287
- Source URL: https://arxiv.org/abs/2602.02287
- Reference count: 40
- Primary result: Zero-shot LLM-as-a-judge rankings are unstable for discourse-level metrics (coherence) across morphologically rich Finno-Ugric languages, despite controlled generation

## Executive Summary
This study investigates whether LLM-as-a-judge can reliably compare models across languages by controlling generation parameters while varying only the target language. Using synthetic customer-support dialogues in Estonian, Finnish, and Hungarian, the research tests cross-lingual evaluation stability. While surface-level metrics (lexical diversity, similarity) maintain stability, pragmatic judgments (coherence, instruction-following) show systematic rank inversions and near-zero correlations, indicating that judge scoring behavior differs across languages rather than reflecting true model performance differences.

## Method Summary
The study generates 10K synthetic customer-support dialogues per language (Estonian, Finnish, Hungarian) using parametrized templates with identical parameters across languages. Six different generator models create dialogues varying by industry, problem type, channel, agent experience, and message length. GPT-5-mini serves as the zero-shot LLM judge using English meta-prompts. Stability is measured through Kendall's τ correlation of per-language model rankings, with semantic similarity (multilingual-e5-large-instruct) verifying content equivalence. Human annotations for Estonian (N=100) provide ground truth reference.

## Key Results
- Surface-level metrics (Grammar, Readability, Fluency) maintain high cross-language stability (τ≥0.70) with minimal rank inversions
- Pragmatic metrics (Coherence, Label Recovery Accuracy) exhibit near-zero or negative correlations across Finno-Ugric language pairs (τ≈0 for Estonian-Hungarian)
- Semantic similarity scores remain consistent (.89–.94) across all models and languages, confirming comparable underlying content quality
- Meta-prompt language does not explain instability; native-language prompts produce nearly identical scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Controlled generation isolates evaluation transfer failures from genuine content variation
- Mechanism: By holding generation parameters constant across Estonian, Finnish, and Hungarian while only varying target language, any ranking instability in judge outputs can be attributed to the evaluation process rather than model performance differences. Semantic similarity scores remaining consistent (.89–.94 across all models and languages) confirms comparable underlying content quality.
- Core assumption: The parametrized templates produce semantically equivalent dialogues across languages despite surface-level morphological differences.
- Evidence anchors:
  - [abstract] "Using synthetic customer-support dialogues generated with identical parameters across Estonian, Finnish, and Hungarian... Because generation is controlled, these inconsistencies reflect how judge scoring behaves differently across languages rather than true model differences."
  - [Section 3.1] "Semantic similarity scores remain remarkably consistent (.89–.94 across all models and languages), confirming that underlying content quality is comparable despite surface variation."
  - [corpus] M-Prometheus paper addresses multilingual LLM judges but lacks controlled generation methodology—this paper's contribution is the diagnostic isolation of judge behavior from content variance.
- Break condition: If semantic similarity diverges significantly across languages, content variance cannot be ruled out as the source of ranking instability.

### Mechanism 2
- Claim: Surface-level and pragmatic evaluation dimensions transfer differently across morphologically rich languages
- Mechanism: Surface-level metrics (Grammar, Readability, Fluency) operate on local linguistic features that have relatively stable cross-lingual representations in LLMs, achieving τ≥0.70. Pragmatic judgments (Coherence, Label Recovery Accuracy) require discourse-level reasoning that depends on language-specific pragmatic norms, resulting in near-zero or negative correlations (τ=−0.06 for Estonian-Hungarian Coherence).
- Core assumption: The judge's internal discourse-level assessment logic differs systematically from surface-level processing across languages.
- Evidence anchors:
  - [abstract] "Surface-level metrics (lexical diversity, surface and semantic similarity) maintain cross-language stability, but pragmatic judgments (coherence, instruction-following) exhibit systematic rank inversions and near-zero correlations (τ≈0)."
  - [Section 3.4] "Surface-level metrics (G, R, F) exhibit high cross-language stability (τ≥.70) with minimal rank inversions (1–3 per pair). However, Coherence shows systematic breakdown: near-zero or negative correlations across Finno-Ugric language pairs."
  - [corpus] RULERS paper identifies rubric instability as a failure mode but focuses on English—this paper extends to cross-lingual instability specifically for discourse-level assessment.
- Break condition: If surface metrics also destabilize, the issue is general cross-lingual judge failure rather than discourse-specific transfer breakdown.

### Mechanism 3
- Claim: Meta-prompt language does not explain cross-lingual judge instability
- Mechanism: The judge's evaluation behavior is driven by its internal representation of the target language rather than the language of instructions. Sensitivity analysis using native-language (Estonian) meta-prompts produced nearly identical scores to English meta-prompts (maximum variance <0.05), ruling out instruction-language bias.
- Core assumption: The judge model has sufficient multilingual capacity to process instructions in either language equivalently.
- Evidence anchors:
  - [Section 3.5] "Scores produced by the native-language prompt are nearly identical to those produced by the English meta-prompt... Results suggest that the judge's evaluation behavior is driven by its internal representation of the target language rather than the language of the instructions."
  - [Table 12] Shows minimal variance between English and Estonian meta-prompts across all metrics and models.
  - [corpus] Middle-Layer Representation Alignment paper addresses cross-lingual transfer in fine-tuned LLMs but focuses on representation alignment rather than evaluation behavior—limited direct relevance to judge instability mechanisms.
- Break condition: If native-language prompts significantly change scoring patterns, prompt engineering could address the instability.

## Foundational Learning

- Concept: **Morphological richness and its impact on LLM evaluation**
  - Why needed here: Finno-Ugric languages (Estonian, Finnish, Hungarian) are morphologically complex with extensive case systems and agglutinative morphology. This affects surface metrics like TTR/MATTR and may contribute to judge instability.
  - Quick check question: Can you explain why MATTR scores differ systematically across Estonian (.48–.80), Finnish (.45–.70), and Hungarian (.49–.76) even with identical generation parameters?

- Concept: **Kendall's τ for ranking stability**
  - Why needed here: The paper uses Kendall's τ correlation coefficient to quantify how well model rankings are preserved across languages. Understanding this metric is essential for interpreting the stability results.
  - Quick check question: If τ=0 for Coherence between Estonian and Hungarian, what does this mean for the reliability of cross-lingual model comparison?

- Concept: **LLM-as-a-judge paradigm and its failure modes**
  - Why needed here: The paper evaluates whether zero-shot LLM judges can reliably assess discourse quality across languages. Understanding inherent judge limitations helps contextualize the findings.
  - Quick check question: Why might a judge achieve ceiling effects (C≈2.98–3.00) in English but show near-zero correlations across Finno-Ugric languages?

## Architecture Onboarding

- Component map:
  1. Dialogue Generator: Parametrized templates → 10K conversations/language → single API calls for end-to-end generation
  2. Surface Metrics Pipeline: TTR/MATTR (lemmatized via EstNLTK/Stanza), Self-BLEU, Intra-model semantic similarity (multilingual-e5-large-instruct)
  3. LLM Judge: GPT-5-mini with zero-shot English meta-prompts → scores for G/R/C/F/LRA
  4. Stability Analysis: Per-language model rankings → Kendall's τ with 95% bootstrap CIs → permutation test for inversion significance

- Critical path: Generate controlled dialogues → verify semantic consistency via automatic metrics → apply LLM judge → compute cross-language ranking correlations → identify stability breakdown in pragmatic dimensions

- Design tradeoffs:
  - Synthetic dialogues enable controlled comparison but may lack naturalistic variation (stylistic homogeneity, translation-like phrasing noted by annotators)
  - Single-judge design (GPT-5-mini) limits generalizability; ablation study (Appendix G) shows minimal judge-to-judge variance for this task
  - Human calibration restricted to Estonian (N=100) provides reference point but not multilingual ground truth

- Failure signatures:
  - **Semantic similarity divergence** (<0.85): Content quality varies too much for valid comparison
  - **Surface metric instability** (τ<0.5): Judge transfer failure extends beyond discourse level
  - **Ceiling effects in target language** (mean>2.95): Insufficient score variance for meaningful ranking
  - **Significant rank inversions** (p<0.05 via permutation): Systematic evaluation breakdown

- First 3 experiments:
  1. **Replicate stability diagnostic on your target language family**: Generate 100 dialogues with identical parameters across 2-3 related languages, compute surface metrics and semantic similarity to verify content equivalence.
  2. **Calibrate judge against human annotations**: Collect N=100 native speaker annotations for coherence/fluency in your target language, correlate with LLM judge scores to establish baseline alignment.
  3. **Test prompt-language sensitivity**: Compare English vs. native-language meta-prompts on a subset (N=50) to determine if instruction language affects scoring in your context.

## Open Questions the Paper Calls Out

- **Do cross-lingual ranking instabilities persist when evaluating natural, non-synthetic dialogues?**
  - Basis in paper: [explicit] The authors state that "Validation on natural customer support scenarios is needed to confirm ranking instabilities persist in operational settings."
  - Why unresolved: The current study relies on synthetic data generated via parametrized templates, which may exhibit stylistic homogeneity or specific artifacts not found in authentic human interactions.
  - What evidence would resolve it: Replicating the controlled evaluation protocol using human-authored customer support logs across the same languages to observe if the τ≈0 correlations for coherence remain.

- **Do these evaluation instabilities generalize to linguistically distant or non-Finno-Ugric languages?**
  - Basis in paper: [explicit] The limitations section notes that "our findings may not hold for... linguistically distant languages."
  - Why unresolved: The study is restricted to three related morphologically rich languages (Estonian, Finnish, Hungarian); it remains unclear if the coherence breakdown is a universal failure mode or specific to this language family.
  - What evidence would resolve it: Applying the same controlled generation and judge evaluation pipeline to typologically distinct language families (e.g., isolating languages like Chinese or analytic languages like English variants).

- **Can language-specific calibration methods restore ranking stability for discourse-level metrics?**
  - Basis in paper: [inferred] The paper concludes by "motivating language-specific calibration against targeted human baselines," but does not test specific calibration techniques.
  - Why unresolved: While the paper identifies that zero-shot transfer fails, it does not determine if providing few-shot examples or regression-based calibration can fix the rank inversions observed in coherence scoring.
  - What evidence would resolve it: Experiments testing calibrated judge scores (using the small N=100 human baselines) to see if they significantly improve Kendall's τ rank correlation compared to zero-shot scores.

## Limitations

- The controlled generation approach assumes semantic equivalence across morphologically rich languages, but lacks human verification for Finnish and Hungarian
- Findings are specific to Finno-Ugric languages and customer-support dialogues, limiting generalizability to other language families or domains
- Single-judge design (GPT-5-mini) and restricted human calibration (Estonian only) may not capture broader judge behavior

## Confidence

- **High confidence**: Surface-level metric stability (τ≥0.70) and semantic similarity consistency across languages
- **Medium confidence**: Judge scoring behavior being driven by target language representation rather than meta-prompt language
- **Low confidence**: Generalizability to other language families, domains, or judge models beyond GPT-5-mini

## Next Checks

1. **Human verification for target languages**: Collect N=100 native speaker annotations for Finnish and Hungarian to establish cross-lingual ground truth for coherence and fluency judgments
2. **Judge model ablation study**: Repeat evaluation using 2-3 different LLM judges to confirm instability patterns are not specific to GPT-5-mini
3. **Cross-family language comparison**: Test the same controlled generation methodology on a non-Finno-Ugric language pair (e.g., English-German) to identify whether morphological complexity drives the observed instability