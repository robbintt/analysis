---
ver: rpa2
title: 'ESPnet-SDS: Unified Toolkit and Demo for Spoken Dialogue Systems'
arxiv_id: '2503.08533'
source_url: https://arxiv.org/abs/2503.08533
tags:
- dialogue
- systems
- spoken
- system
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The ESPnet-SDS toolkit provides a unified, open-source framework
  for building web interfaces to evaluate and compare both cascaded and end-to-end
  spoken dialogue systems. It features a modular design that supports various ASR,
  LLM, TTS, and VAD components, along with on-the-fly evaluation metrics such as latency,
  intelligibility, audio quality, coherence, and relevance.
---

# ESPnet-SDS: Unified Toolkit and Demo for Spoken Dialogue Systems

## Quick Facts
- arXiv ID: 2503.08533
- Source URL: https://arxiv.org/abs/2503.08533
- Reference count: 30
- Unified open-source framework for comparing cascaded and end-to-end spoken dialogue systems with on-the-fly evaluation

## Executive Summary
ESPnet-SDS is a unified, open-source toolkit that provides web interfaces to evaluate and compare both cascaded and end-to-end spoken dialogue systems. The toolkit features a modular design supporting various ASR, LLM, TTS, and VAD components, along with automated metrics for latency, intelligibility, audio quality, coherence, and relevance. Using the Switchboard dataset, experiments revealed that current E2E systems like Mini-Omni generate less diverse and lower-quality audio responses compared to cascaded pipelines, with Mini-Omni's TTS quality (UTMOS 2.88) significantly lower than LJSpeech VITS (4.06).

## Method Summary
The toolkit employs modular wrapper classes that standardize input/output protocols for ASR, LLM, TTS, and VAD components, unified under an `ESPnet_SDS_Model_Interface` for orchestration. A Gradio frontend provides a plug-and-play architecture allowing users to select from various cascaded and E2E dialogue systems. The system computes automated metrics (WER, UTMOS, VERT, BERT similarity) during interactions and includes human-in-the-loop feedback collection. The framework was evaluated on the Switchboard Eval 2000 dataset using a template-based recipe structure with pre-trained models including OWSM, LLaMA, and Mini-Omni.

## Key Results
- Current E2E systems like Mini-Omni generate less diverse and lower-quality audio responses compared to cascaded pipelines
- Mini-Omni's TTS quality (UTMOS 2.88) was significantly lower than LJSpeech VITS (4.06)
- The toolkit enables researchers to easily compare systems and gain actionable insights into their strengths and limitations

## Why This Works (Mechanism)

### Mechanism 1
A modular wrapper architecture reduces integration friction, allowing for direct comparison of disparate Spoken Dialogue Systems (SDS). The system employs abstract wrapper classes (`AbsASR`, `AbsTTS`, `AbsE2E`) that standardize input/output protocols for components like Whisper, LLaMA, or Mini-Omni. These are unified under an `ESPnet_SDS_Model_Interface` which handles the logic flow, allowing the Gradio frontend to swap backends without changing UI code.

### Mechanism 2
On-the-fly automated metrics enable immediate diagnosis of quality degradation in End-to-End (E2E) models compared to cascaded pipelines. The toolkit computes specific metrics (UTMOS for audio quality, VERT for diversity, WER for intelligibility) during interaction, revealing that while E2E models maintain intelligibility, they suffer from "template-like" outputs and lower audio quality scores (UTMOS 2.88) compared to specialized TTS modules (VITS 4.06).

### Mechanism 3
Human-in-the-loop feedback collection captures subjective "naturalness" and "relevance" that automated metrics like BERT similarity miss. The interface includes a UI component to collect user ratings on a 4-point scale, identifying that while systems might score well on coherence (BERT similarity), they lack backchanneling and natural turn-taking (gap/overlap statistics).

## Foundational Learning

### Concept: Cascaded vs. End-to-End (E2E) Architectures
**Why needed here:** The toolkit's primary function is to contrast these two designs. You must understand that Cascaded systems chain distinct modules (ASR -> LLM -> TTS) while E2E systems process audio-to-audio directly (often using audio tokens) to appreciate the trade-offs in latency vs. quality.
**Quick check question:** Does a cascaded system typically require a separate VAD module for turn-taking, or is that handled by the LLM? (Answer: It usually requires a distinct VAD, as implied in Section 4.1).

### Concept: Subjective Audio Evaluation (MOS/UTMOS)
**Why needed here:** The paper relies heavily on UTMOS scores to claim E2E inferiority. You need to know that MOS (Mean Opinion Score) is a subjective 1-5 rating, and UTMOS is a model trained to predict this score without human reference.
**Quick check question:** Why is a UTMOS score of 2.88 (Mini-Omni) considered a significant failure compared to 4.06 (VITS) in a production environment?

### Concept: Diversity Metrics (VERT/BLEU)
**Why needed here:** The paper identifies "template-like" responses in Mini-Omni using these metrics. Understanding that high Self-BLEU implies repetition helps diagnose why the E2E model felt "unnatural" despite being coherent.
**Quick check question:** If a model has high VERT score (24.1 for Mini-Omni vs 5.7 for LLaMA), does it generate more or less diverse responses? (Answer: Higher VERT indicates lower diversity in this context).

## Architecture Onboarding

### Component map:
- **Wrappers (`espnet2/sds`):** Python classes that abstract external models (e.g., `Whisper_ASR`, `HuggingFace_LLM`)
- **Interface (`ESPnet_SDS_Model_Interface`):** The orchestration layer connecting VAD, dialogue logic, and audio I/O
- **Template (`egs2/TEMPLATE/sds1`):** The recipe directory containing `App.py` (Gradio UI) and `Run.sh` (Launch script)
- **Metrics (`pyscripts/utils/dialog_eval`):** Standalone scripts for calculating WER, UTMOS, and latency

### Critical path:
1. User speaks -> **WebRTC VAD** detects silence
2. Audio chunk passed to **Model Interface**
3. Interface routes audio to **ASR** (Cascaded) OR **Audio Encoder** (E2E)
4. Text/Audio Latents processed by **LLM** (Cascaded) OR **LLM Backbone** (E2E)
5. Output synthesized by **TTS** (Cascaded) OR **Audio Decoder** (E2E)
6. **Metrics** computed in parallel; results displayed on UI

### Design tradeoffs:
- **Latency vs. Quality:** Specialized TTS (VITS) outperforms E2E audio generation but implies E2E could theoretically be faster due to non-cascaded architecture
- **Control vs. Simplicity:** Cascaded systems allow swapping individual modules, whereas E2E requires retraining or finding new checkpoints for holistic improvements

### Failure signatures:
- **"Template-like" Responses:** High Self-BLEU score; user feels the AI is repetitive (observed in Mini-Omni)
- **Latency Spikes:** Gaps in conversation significantly longer than human-human baselines
- **Hallucination in TTS:** Intelligibility metrics (WER) degrading if TTS model produces phonemes not matching the text

### First 3 experiments:
1. **Baseline Comparison:** Run the provided `Run.sh` recipe to launch the demo. Interact with the Cascaded (OWSM + LLaMA + VITS) pipeline and record the average latency. Switch to Mini-Omni (E2E) and compare the UTMOS score and latency difference.
2. **Module Ablation:** In the Cascaded pipeline, swap the `LLaMA 3.2-1B` with `SmolLM v2` using the config arguments. Observe the change in *BERT Similarity* and *DialoGPT Perplexity* to quantify the drop in coherence.
3. **Diversity Stress Test:** Engage the Mini-Omni model in a multi-turn conversation about a repetitive topic. Check the UI logs for the *Self-BLEU* metric to confirm the paper's finding of high repetition (>75%).

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How robust are the ESPnet-SDS evaluation metrics and supported systems in low-resource language settings, noisy environments, and multi-speaker scenarios?
**Basis in paper:** Section 9 (Limitations) explicitly states that "additional investigation is needed to assess its performance" in these specific conditions.
**Why unresolved:** The current analysis is restricted to the relatively clean, English-only Switchboard dataset.
**What evidence would resolve it:** Benchmarks of ASR, LLM, and TTS modules using the toolkit on noisy corpora (e.g., CHiME) or datasets in low-resource languages.

### Open Question 2
**Question:** What architectural or training modifications are required to improve the response diversity of end-to-end dialogue systems?
**Basis in paper:** Section 7 notes that the E2E system Mini-Omni "exhibits significant overlap in its responses, often generating template-like outputs" despite high coherence.
**Why unresolved:** The paper identifies the low diversity (high Self-BLEU) as a deficiency of current E2E models but does not propose a solution.
**What evidence would resolve it:** An E2E model evaluated via the toolkit demonstrating higher VERT scores and lower Self-BLEU scores without sacrificing coherence.

### Open Question 3
**Question:** Can end-to-end spoken dialogue systems achieve audio quality comparable to specialized cascaded TTS pipelines?
**Basis in paper:** Section 7 and Table 4 highlight that Mini-Omni's audio quality (UTMOS 2.88) is significantly lower than cascaded models like LJSpeech VITS (4.06).
**Why unresolved:** The paper establishes a clear performance gap, leaving the feasibility of closing it within E2E architectures as an open challenge.
**What evidence would resolve it:** An E2E system achieving UTMOS and DNS scores statistically equivalent to state-of-the-art cascaded TTS systems when evaluated through the unified interface.

## Limitations
- The evaluation framework assumes proxy metrics adequately capture quality differences between cascaded and E2E systems
- Latency measurements include both model processing time and UI overhead without clear separation
- The paper doesn't establish statistical significance of quality differences across the full Switchboard dataset

## Confidence

- **High confidence:** The modular wrapper architecture successfully enables comparison of disparate systems
- **Medium confidence:** E2E systems generate less diverse responses than cascaded pipelines
- **Medium confidence:** Human-in-the-loop feedback captures quality aspects missed by automated metrics

## Next Checks

1. **Statistical validation of quality differences:** Run a full statistical significance test (e.g., paired t-test) across all 40 Switchboard conversations to confirm that the UTMOS difference between Mini-Omni and VITS is not due to random variation.

2. **Latency decomposition analysis:** Instrument the toolkit to separately measure model inference time versus UI/communication overhead, then re-compare cascaded versus E2E systems using only the model processing component.

3. **Cross-model diversity assessment:** Test at least three different E2E models (not just Mini-Omni) against the cascaded baseline to determine if the "template-like" behavior is a systematic limitation of current E2E approaches or specific to the tested model.