---
ver: rpa2
title: Multi-Modal 3D Mesh Reconstruction from Images and Text
arxiv_id: '2503.07190'
source_url: https://arxiv.org/abs/2503.07190
tags:
- images
- reconstruction
- object
- input
- mesh
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a language-guided few-shot 3D mesh reconstruction
  method that uses images and text prompts to reconstruct 3D objects without requiring
  pre-existing 3D models. The approach combines GroundingDINO and Segment Anything
  Model (SAM) for text-driven object segmentation, VGGSfM for sparse point cloud reconstruction,
  and SuGaR for mesh generation using Gaussian Splatting.
---

# Multi-Modal 3D Mesh Reconstruction from Images and Text

## Quick Facts
- arXiv ID: 2503.07190
- Source URL: https://arxiv.org/abs/2503.07190
- Reference count: 36
- One-line primary result: Few-shot 3D mesh reconstruction from images and text prompts without pre-existing 3D models, converging at ~15 images with optimal viewing angles at 45 degrees.

## Executive Summary
This paper introduces a language-guided few-shot 3D mesh reconstruction method that combines images and text prompts to reconstruct 3D objects without requiring pre-existing 3D models. The approach integrates text-driven object segmentation, sparse point cloud reconstruction, and Gaussian Splatting-based mesh generation to create textured 3D meshes from multi-view RGB images. Experiments demonstrate that the method achieves geometric accuracy (IoU up to 82.91%) and high texture quality (SSIM up to 0.967) with as few as 15 input images, making it suitable for robotics applications where pre-existing 3D models are unavailable.

## Method Summary
The method processes input images and text prompts through a pipeline that begins with LangSAM (GroundingDINO + SAM) for text-guided object segmentation with 50-pixel padding. VGGSfM performs sparse point cloud reconstruction using a differentiable end-to-end approach that eliminates chaining pairwise matches. SuGaR then reconstructs the mesh using Gaussian Splatting, followed by PyMeshLab post-processing to remove artifacts. The approach is evaluated on the THU MVS dataset using Chamfer Distance and IoU for geometric accuracy, and PSNR, SSIM, and LPIPS for texture quality, showing convergence at around 15 images with optimal viewing angles at 45 degrees for most objects.

## Key Results
- Method converges at approximately 15 input images for optimal reconstruction quality
- 45-degree viewing angle achieves highest geometric accuracy (IoU 82.91%)
- Top-down views at 30 degrees yield best texture quality (SSIM 0.967, LPIPS 0.0373)
- Runtime scales linearly with image count, with mesh extraction consistently taking 4-5 minutes
- Approach works without pre-existing 3D models, enabling zero-shot reconstruction for robotics applications

## Why This Works (Mechanism)

### Mechanism 1: Language-Guided Object Isolation
Text prompts enable zero-shot object selection through GroundingDINO translating natural language to visual regions, which condition SAM to produce binary segmentation masks with 50-pixel padding. This preserves boundary features critical for mesh generation, particularly for semi-transparent objects. The approach assumes unambiguous text queries and sufficient VLM grounding capability.

### Mechanism 2: Differentiable Multi-View Feature Aggregation
VGGSfM's end-to-end differentiable structure-from-motion jointly estimates camera parameters and reconstructs sparse point clouds by tracking 2D correspondences across all views simultaneously. This global optimization eliminates compounding errors from chained pairwise matches, assuming sufficient image overlap (Δφ ≤ 30°) and adequate viewing angles.

### Mechanism 3: Surface-Aligned Gaussian Mesh Extraction
SuGaR optimizes 3D Gaussians aligned to surfaces, enabling both photorealistic rendering and mesh extraction. The COLMAP-format sparse reconstruction initializes Gaussian positions, which are refined through differentiable rendering. The method assumes adequate geometric initialization from sparse point clouds and consistent lighting in input images.

## Foundational Learning

- **Concept: Structure-from-Motion (SfM) Fundamentals**
  - Why needed here: Essential for debugging sparse reconstruction failures and selecting appropriate input image configurations
  - Quick check question: Given two images of an object from different viewpoints, can you sketch how corresponding 2D points triangulate to a 3D point?

- **Concept: Gaussian Splatting Representation**
  - Why needed here: Core mesh generation relies on optimizing 3D Gaussians; understanding their parameters is necessary for interpreting reconstruction artifacts
  - Quick check question: How does a 3D Gaussian differ from a point cloud vertex in terms of rendering and surface extraction?

- **Concept: Vision-Language Grounding**
  - Why needed here: Pipeline depends on text-to-region grounding; understanding grounding failures helps diagnose segmentation errors
  - Quick check question: Why might "blue cup" fail to ground in an image containing a teal mug under warm lighting?

## Architecture Onboarding

- **Component map:** Input Layer (RGB images + text prompt) -> Segmentation (LangSAM) -> Sparse Reconstruction (VGGSfM) -> Dense Reconstruction (SuGaR) -> Post-Processing (PyMeshLab) -> Output (Final 3D mesh)

- **Critical path:** Segmentation quality → sparse point density → Gaussian initialization quality → final mesh fidelity. Errors compound downstream; poor segmentation cannot be recovered later.

- **Design tradeoffs:**
  - Image count vs. runtime: Linear scaling; 15 images identified as convergence point, 36 provides best quality but doubles runtime vs. 18
  - Viewing angle: 45° optimal for geometry (IoU 82.91%), 30° optimal for texture (SSIM 0.967)
  - Overlap vs. coverage: Dense overlap (Δφ=10°) with fewer images limits coverage; sparse overlap (Δφ=30°) with 12 images balances both

- **Failure signatures:**
  - Incomplete mesh: Insufficient angular coverage; increase image count or reduce Δφ
  - Scale drift: No reference object; integrate metric scale calibration
  - Texture artifacts: Extreme viewing angles; prefer θ=30-45°
  - VRAM overflow at 36+ images: VGGSfM downscaling; expect quality drop

- **First 3 experiments:**
  1. Baseline reconstruction test: 15 images at θ=45° with Δφ=24° on a single object; verify pipeline execution end-to-end and measure CD/IoU against ground truth if available.
  2. Ablation on image count: Test 4, 8, 12, 15, 18, 24, 36 images at fixed θ=45°, Δφ=10°; plot convergence curve for geometric metrics to validate the 15-image threshold.
  3. Viewing angle sweep: Compare θ∈{30°, 45°, 75°} with fixed 36 images; measure texture metrics (PSNR, SSIM, LPIPS) to confirm angle-dependent quality tradeoff.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the integration of a reference object effectively resolve the scale estimation challenge identified in the reconstruction pipeline? The current method reconstructs object geometry and texture but does not inherently recover absolute metric scale, necessary for robotic interaction.

- **Open Question 2:** What efficient alternatives to the current sparse reconstruction module can mitigate the computational bottleneck to enable real-time application? The current dependency on VGGSfM causes runtime to increase linearly with image count, taking up to 22 minutes for 36 images.

- **Open Question 3:** Does the observed optimal viewing angle of 45 degrees and convergence at 15 images generalize to objects with complex geometry or non-Lambertian surfaces? The experimental evaluation is limited to two specific figurines, and it's unstated if these imaging heuristics apply to the broader class of objects mentioned.

## Limitations
- Scale estimation remains unresolved without reference objects, limiting metric accuracy for robotics applications
- Computational bottleneck in sparse reconstruction prevents real-time application with current VGGSfM implementation
- Limited evaluation on only two object categories (cat and dog figurines) restricts generalizability to diverse object geometries and materials

## Confidence
- **High:** The core pipeline combining LangSAM, VGGSfM, and SuGaR is well-defined and supported by the paper
- **Medium:** Reported convergence at 15 images and optimal viewing angles are supported by experiments but may depend on specific dataset characteristics
- **Low:** Exact post-processing steps in PyMeshLab and impact of unspecified hyperparameters introduce uncertainty in reproducing results

## Next Checks
1. **Dataset Generalization:** Validate the pipeline on a different multi-view dataset (e.g., DTU or Tanks and Temples) with varied object categories to test robustness beyond figurines
2. **Hyperparameter Sensitivity:** Perform ablation studies on SuGaR and PyMeshLab parameters to quantify their impact on CD, IoU, and texture metrics
3. **Real-World Application Test:** Apply the method to a robotics-relevant scenario (e.g., cluttered scene with target object) to assess performance in the intended application domain and identify failure modes