---
ver: rpa2
title: Comparison of Unsupervised Metrics for Evaluating Judicial Decision Extraction
arxiv_id: '2510.01792'
source_url: https://arxiv.org/abs/2510.01792
tags:
- legal
- metrics
- block
- expert
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated 16 unsupervised metrics for extracting seven\
  \ semantic blocks from 1,000 anonymized Russian judicial decisions, validated against\
  \ 7,168 expert reviews on a 1\u20135 Likert scale. The metrics span document-based,\
  \ semantic, structural, pseudo-ground truth, and legal-specific categories, operating\
  \ without pre-annotated ground truth."
---

# Comparison of Unsupervised Metrics for Evaluating Judicial Decision Extraction

## Quick Facts
- arXiv ID: 2510.01792
- Source URL: https://arxiv.org/abs/2510.01792
- Reference count: 40
- 16 unsupervised metrics evaluated against 7,168 expert reviews on 1,000 Russian judicial decisions

## Executive Summary
This study systematically evaluates 16 unsupervised metrics for extracting seven semantic blocks from Russian judicial decisions, validated against 7,168 expert reviews on a 1–5 Likert scale. The metrics span document-based, semantic, structural, pseudo-ground truth, and legal-specific categories, operating without pre-annotated ground truth. Bootstrapped correlations, Lin's concordance correlation coefficient (CCC), and mean absolute error (MAE) revealed that Term Frequency Coherence (Pearson r = 0.540, Lin CCC = 0.512, MAE = 0.127) and Coverage Ratio/Block Completeness (Pearson r = 0.513, Lin CCC = 0.443, MAE = 0.139) best aligned with expert ratings, while Legal Term Density (Pearson r = -0.479, Lin CCC = -0.079, MAE = 0.394) showed strong negative correlations. The LLM Evaluation Score (mean = 0.849, Pearson r = 0.382, Lin CCC = 0.325, MAE = 0.197) showed moderate alignment, but its performance suggests limited specialization for legal texts. These findings highlight that unsupervised metrics, including LLM-based approaches, enable scalable screening but cannot fully replace human judgment in high-stakes legal contexts.

## Method Summary
The study evaluated 16 unsupervised metrics using 1,000 anonymized Russian judicial decisions from the sud-resh-benchmark dataset, with 7,168 expert reviews providing ground truth ratings. Metrics were computed using Python 3.12 with spaCy for Russian NLP, SentenceTransformer for embeddings, and NER for entity extraction. Expert ratings were compared against metric outputs using Pearson correlation, Spearman correlation, Kendall's tau, Lin's concordance correlation coefficient, and mean absolute error. Bootstrapping with 1,000 resamples provided confidence intervals. The evaluation considered document-level and block-level analyses, with metrics categorized into document-based, semantic, structural, pseudo-ground truth, and legal-specific types.

## Key Results
- Term Frequency Coherence achieved the strongest positive correlation (Pearson r = 0.540) with expert ratings
- Coverage Ratio and Block Completeness showed the second-best alignment (Pearson r = 0.513)
- Legal Term Density demonstrated the strongest negative correlation (Pearson r = -0.479)
- LLM Evaluation Score showed moderate alignment (Pearson r = 0.382) using gpt-4.1-mini via g4f
- Lin's CCC values were consistently lower than Pearson correlations, indicating systematic scaling issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TF-IDF vector alignment between source documents and extracted blocks provides a scalable proxy for extraction quality, as measured by human expert agreement.
- Mechanism: **Term Frequency Coherence** computes the cosine similarity between the TF-IDF vector of the full judicial decision and the TF-IDF vector of the combined extracted blocks. The core idea is that if the extracted blocks maintain a similar term distribution to the source, they are more likely to contain the key information a human would judge as important.
- Core assumption: The most frequent and distinctive terms in a legal document (as identified by TF-IDF) are the primary indicators of content that human experts value and use to assess quality.
- Evidence anchors:
  - [abstract] Term Frequency Coherence (Pearson r = 0.540, Lin CCC = 0.512, MAE = 0.127) best aligned with expert ratings.
  - [section 4, Results] At the document level (n= 995 evaluations),Term Frequency Coherence... achieved the highest positive correlation with expert ratings (Pearsonr= 0.540,p= 8.30×10 −40... Lin CCC= 0.512; MAE0.127).
  - [corpus] This approach is a standard and foundational technique in information retrieval [18, 20], but its application as a primary evaluation metric for legal extraction quality is validated by this paper's strong correlations.
- Break condition: This mechanism will fail or weaken if expert judgments are based on qualities not captured by term frequency (e.g., logical coherence, syntactic correctness) or if the key information is expressed through rare but crucial synonyms not weighted heavily by TF-IDF.

### Mechanism 2
- Claim: Capturing key legal entities and citations from the source text in the extracted blocks is a strong, positive indicator of extraction quality.
- Mechanism: **Coverage Ratio** and its structural equivalent **Block Completeness** measure the proportion of key terms from the source document that are present in the extracted text. These key terms are defined as the top-50 TF-IDF terms, which in legal documents often correspond to case-specific entities (plaintiffs, defendants, organizations) and legal concepts. A higher ratio means more critical information is preserved.
- Core assumption: The top N terms by TF-IDF score serve as an effective proxy for the "key information" a human expert would expect to find in a good extraction.
- Evidence anchors:
  - [abstract] Coverage Ratio/Block Completeness (Pearson r = 0.513, Lin CCC = 0.443, MAE = 0.139) best aligned with expert ratings.
  - [section 4, Results] Coverage Ratio and Block Completeness (both mean= 0.685... Pearsonr= 0.513, p= 1.50×10 −35... Lin CCC= 0.443) ... emphasizing the importance of capturing key terms.
  - [corpus] Citation and entity preservation is a well-known challenge in legal NLP [15, 21], and these metrics provide a direct, unsupervised measure of this important fidelity.
- Break condition: This metric is identical for Coverage Ratio and Block Completeness in the implementation. It fails if key information is not term-based (e.g., the *absence* of a party is the key fact) or if paraphrasing makes the same concept unrecognizable as the same term.

### Mechanism 3
- Claim: Simply measuring the density of domain-specific jargon in extracted blocks is a negative indicator of quality, as perceived by human experts.
- Mechanism: **Legal Term Density** calculates the proportion of legal-specific entities, patterns, and top terms within a single block. The negative correlation suggests that high density does not equate to high quality; an extraction could be full of legal jargon but miss the narrative or factual core of the case, which experts value more.
- Core assumption: A quality extraction balances legal terminology with factual and narrative content. An overabundance of jargon may signal an extraction that is too narrow or misses the broader context.
- Evidence anchors:
  - [abstract] Legal Term Density (Pearson r = -0.479, Lin CCC = -0.079, MAE = 0.394) showed strong negative correlations.
  - [section 4, Results] Legal Term Density... demonstrated the strongest negative correlation (Pearsonr=−0.479,p= 1.38×10 −30), implying that higher concentrations of legal terminology might compromise clarity or relevance from the experts’ perspective.
  - [corpus] This finding is novel and not universally assumed in legal NLP. Corpus neighbors on "Evaluation of Large Language Models in Legal Applications" [80195] discuss general challenges but do not report on the negative correlation of jargon density with quality, highlighting this paper's specific contribution.
- Break condition: This correlation might be specific to the extraction task (segmenting a document) versus a summarization or generation task, where high legal term density might be more desirable. It could also be an artifact of this specific dataset.

## Foundational Learning

- Concept: **TF-IDF (Term Frequency-Inverse Document Frequency)**
  - Why needed here: This is the core statistical method underlying the two best-performing metrics, Term Frequency Coherence and Coverage Ratio. Understanding how it weights terms is essential to understanding why these metrics work.
  - Quick check question: In a corpus of 1,000 diverse court cases, would the term "court" have a high or low TF-IDF weight, and why?

- Concept: **Cosine Similarity**
  - Why needed here: This is the distance metric used to compare the TF-IDF and embedding vectors. It measures the angular distance between two vectors, representing the similarity of their content regardless of document length.
  - Quick check question: If Document A is a short excerpt from Document B, would you expect their cosine similarity (using TF-IDF vectors) to be high or low?

- Concept: **Lin's Concordance Correlation Coefficient (CCC)**
  - Why needed here: The paper uses CCC to evaluate the metrics, not just Pearson correlation. CCC measures both the correlation and the agreement (how close the data is to the 45-degree line), which is crucial for knowing if a metric score (e.g., 0.8) can be trusted to represent an expert rating of 0.8.
  - Quick check question: If a new metric has a Pearson r of 0.9 with expert ratings but a Lin CCC of 0.2, what does that tell you about the metric's scores?

## Architecture Onboarding

- Component map: Input JSON (source_text + 7 semantic_blocks) -> Preprocessing (spaCy ru_core_news_sm) -> Feature Extraction (TF-IDF, SentenceTransformer, NER, regex) -> Metric Computation (16 calculators) -> Evaluation (correlations, CCC, MAE vs expert ratings)
- Critical path: The primary success path depends on the **TF-IDF vectorizer**. Ensure the `custom_analyzer` correctly lemmatizes Russian text and removes stopwords, as the quality of the `key_terms` directly determines the `Coverage Ratio` and `Term Frequency Coherence` scores, which are the most reliable indicators.
- Design tradeoffs: The paper explicitly notes a tradeoff between **generalizability** and **domain specificity**. It uses general-purpose models (all-MiniLM-L6-v2) for embeddings, which enables cross-linguistic application but, as shown by the poor performance of `Raw Cosine Similarity`, fails to capture legal nuances. A legal-specific BERT model would likely improve semantic metrics but would require more compute and domain-specific fine-tuning data.
- Failure signatures:
  - **Flatlining Metric:** A metric showing zero variance (e.g., `Neutrality Bias` and `Monotonicity Score` both had a constant mean and no correlation). This indicates the metric is not sensitive enough for the dataset.
  - **Strong Negative Correlation:** A metric that consistently scores high when experts score low (e.g., `Legal Term Density`). This is a strong signal that the metric's objective is misaligned with the goal of extraction quality.
  - **High MAE with Low CCC:** A metric has a decent correlation but a high Mean Absolute Error and low CCC. This indicates the metric's scores are systematically biased or scaled incorrectly compared to human ratings (e.g., `LLM Evaluation Score`).
- First 3 experiments:
  1. **Reproduce the top metrics:** Implement `Coverage Ratio` and `Term Frequency Coherence` as defined in the paper (section 2.4.1). Validate your implementation by checking if the computed scores on a sample of the provided `sud-resh-benchmark` dataset fall within the reported ranges (means of ~0.685 and ~0.720).
  2. **Test domain adaptation:** Replace the general-purpose `all-MiniLM-L6-v2` SentenceTransformer model with a multilingual or Russian-legal-specific model (e.g., a `rubert` variant). Compute `Raw Cosine Similarity` with both models on a subset of documents and compare their correlations with the provided expert ratings.
  3. **Analyze failure modes:** Select 10-20 documents where the best metric (`Term Frequency Coherence`) has a high score but low expert agreement. Manually inspect these cases to hypothesize what textual qualities are being captured by TF-IDF but penalized by human experts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can domain-adapted embeddings (e.g., Russian Legal-BERT) significantly improve the correlation and Lin's concordance correlation coefficient (CCC) of semantic metrics like Raw Cosine Similarity compared to general-purpose models?
- Basis in paper: [explicit] The conclusion states that "future iterations should incorporate domain-adapted embeddings (e.g., Legal-BERT trained on Russian jurisprudence)" to enhance semantic fidelity, as the current general-purpose model (all-MiniLM-L6-v2) underperformed.
- Why unresolved: The study identified that general-purpose vectorization failed to capture Russian legal nuances, resulting in low correlations (Pearson $r = 0.207$), but did not test alternative models.
- What evidence would resolve it: A re-evaluation of the dataset using legal-domain embeddings showing significantly higher Pearson correlations and Lin CCC values for semantic metrics.

### Open Question 2
- Question: Does fine-tuning Large Language Models (LLMs) with legal-specific rubrics improve the LLM Evaluation Score's alignment with expert judgment beyond a Lin CCC of 0.325?
- Basis in paper: [explicit] The authors note that the LLM evaluator's performance suggests "limited specialization for legal texts" and explicitly call for "fine-tuned LLMs prompted with legal-specific rubrics" in future work.
- Why unresolved: The specific LLM used (gpt-4.1-mini via g4f) provided only moderate alignment with experts, potentially missing argumentative coherence.
- What evidence would resolve it: Comparative benchmarking where a legal-specialized LLM evaluates the extractions, resulting in higher concordance (CCC) and lower Mean Absolute Error (MAE) against the 7,168 expert reviews.

### Open Question 3
- Question: Can a legal-domain sentiment analysis model prevent the Neutrality Bias metric from collapsing to a constant value and successfully detect bias in judicial texts?
- Basis in paper: [inferred] Section 4 reports that Neutrality Bias remained constant (mean = 0.500) with zero variance, and Section 2.3 explicitly attributes this limitation to the model's "lack of legal-domain training" leading to misclassification.
- Why unresolved: The current conversational sentiment model failed to distinguish actual bias from neutral legal terminology, rendering the metric ineffective.
- What evidence would resolve it: Applying a sentiment model trained on legal data to the dataset, resulting in non-zero variance in the Neutrality Bias metric and a measurable correlation with expert fairness assessments.

## Limitations
- Results are context-dependent and may not generalize across legal systems beyond Russian judicial decisions
- Strong negative correlation of Legal Term Density may be specific to document segmentation rather than summarization tasks
- LLM Evaluation Score shows only moderate alignment, suggesting current general-purpose LLMs lack specialization for Russian legal text evaluation
- TF-IDF assumption that key information equals frequent terms may fail when context or logical structure matters more than term frequency

## Confidence
- High confidence in Term Frequency Coherence and Coverage Ratio mechanisms, as they align with established IR principles and show the strongest correlations
- Medium confidence in Legal Term Density interpretation, as the negative correlation is counterintuitive and may reflect task-specific biases rather than universal principles
- Medium confidence in LLM Evaluation Score results, given API access uncertainties and potential variability in prompt implementation
- Low confidence in generalizability across legal systems, as the study focuses exclusively on Russian judicial decisions

## Next Checks
1. **Cross-jurisdiction validation:** Apply the top three metrics (Term Frequency Coherence, Coverage Ratio, LLM Evaluation Score) to judicial decisions from another legal system (e.g., US or EU courts) and compare correlation patterns with expert ratings.
2. **Task-specific sensitivity analysis:** Test whether Legal Term Density's negative correlation holds when metrics are applied to legal summarization tasks rather than document segmentation, controlling for document length and complexity.
3. **Domain adaptation experiment:** Replace the general-purpose SentenceTransformer with a Russian-legal-specific embedding model and measure changes in semantic metric performance, particularly for Raw Cosine Similarity and Sentence Embedding Similarity.