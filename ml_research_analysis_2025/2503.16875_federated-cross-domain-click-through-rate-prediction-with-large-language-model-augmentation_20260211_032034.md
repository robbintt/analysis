---
ver: rpa2
title: Federated Cross-Domain Click-Through Rate Prediction With Large Language Model
  Augmentation
arxiv_id: '2503.16875'
source_url: https://arxiv.org/abs/2503.16875
tags:
- user
- domain
- cross-domain
- item
- privacy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes FedCCTR-LM, a federated learning framework
  for cross-domain click-through rate prediction that addresses data sparsity and
  privacy constraints through large language model (LLM) augmentation. The framework
  integrates three core components: PrivAugNet for privacy-preserving LLM-based data
  augmentation, IDST-CL for independent domain-specific transformer learning with
  contrastive objectives, and AdaLDP for adaptive differential privacy.'
---

# Federated Cross-Domain Click-Through Rate Prediction With Large Language Model Augmentation

## Quick Facts
- **arXiv ID**: 2503.16875
- **Source URL**: https://arxiv.org/abs/2503.16875
- **Reference count**: 40
- **Primary result**: FedCCTR-LM achieves up to 10.45% improvement in NDCG@10 and 6.84% in MRR@10 over state-of-the-art baselines while maintaining strong privacy guarantees through adaptive noise calibration.

## Executive Summary
This paper proposes FedCCTR-LM, a federated learning framework for cross-domain click-through rate prediction that addresses data sparsity and privacy constraints through large language model (LLM) augmentation. The framework integrates three core components: PrivAugNet for privacy-preserving LLM-based data augmentation, IDST-CL for independent domain-specific transformer learning with contrastive objectives, and AdaLDP for adaptive differential privacy. Experiments on Amazon datasets (Book&Movie and Food&Kitchen) demonstrate significant performance improvements over state-of-the-art baselines while maintaining rigorous privacy guarantees.

## Method Summary
FedCCTR-LM operates in a federated learning setting where multiple domains collaborate to improve CTR prediction without sharing raw data. The framework consists of PrivAugNet, which uses an LLM to generate semantically coherent user/item features and interaction sequences to mitigate data sparsity; IDST-CL, which employs independent domain-specific transformers with contrastive objectives to disentangle shared and domain-specific preferences; and AdaLDP, which implements adaptive differential privacy through dynamic noise injection calibrated by privacy budget tracking. The model is trained via FedAvg aggregation where clients contribute gradients under strict privacy constraints.

## Key Results
- FedCCTR-LM achieves up to 10.45% improvement in NDCG@10 and 6.84% in MRR@10 over state-of-the-art baselines
- PrivAugNet augmentation increases average sequence length from 13.33 to 15.41 while reducing sparsity
- AdaLDP outperforms static LDP by achieving higher NDCG@10 by round 200 through adaptive noise decay
- Optimal privacy budget range is ε∈[1,3], with performance degrading significantly below 0.6 or above 3

## Why This Works (Mechanism)

### Mechanism 1: LLM-Augmented Representation Enrichment
LLM-based augmentation reduces data sparsity effects by generating semantically coherent item/user features and expanding sparse interaction sequences. PrivAugNet uses structured prompts to guide an LLM through three stages: item feature augmentation harmonizes attributes across domains, user profile augmentation infers demographic signals from cross-domain histories, and interaction sequence expansion generates positive/negative candidates. These augmented signals are fused with original data for training. Core assumption: LLM outputs preserve semantic relationships relevant to user preferences. Evidence: Table 3 shows sequence length increase and sparsity reduction; ablation shows w/o PrivAugNet drops NDCG@10 by 7.39% (Book) and 10.23% (Movie).

### Mechanism 2: Contrastive Disentanglement of Domain-Specific vs. Shared Preferences
Contrastive learning objectives (IDRA + CDRD) enable knowledge transfer across domains while preserving domain-specific personalization. IDRA minimizes cosine similarity margin between augmented and original sequence representations within each domain, reducing LLM-introduced noise. CDRD simultaneously pulls cross-domain representation h_M closer to both domain-specific representations while pushing them apart, enforcing disentanglement. Core assumption: User preferences decompose into domain-invariant and domain-specific components. Evidence: IDRA loss aligns augmented/original sequences; CDRD loss uses temperature-scaled softmax to align h_M with both domains while separating h_A/h_B. C2DSR and Tri-CDR provide precedent for contrastive cross-domain alignment.

### Mechanism 3: Adaptive Noise Injection via Privacy Budget Tracking
Dynamically decaying noise (σ_t) with cumulative privacy accounting maintains differential privacy guarantees while allowing more informative gradients as training progresses. AdaLDP clips gradients to threshold θ, adds Gaussian noise, then decays σ_t via factor R (0.997 optimal). Privacy cost per round is tracked using Rényi Differential Privacy, accumulating ε_t(α) and stopping client participation when budget ε_0 is exhausted. Core assumption: Gradient sensitivity decreases as training progresses. Evidence: Figure 8 shows AdaLDP achieves higher NDCG@10 than static LDP by round 200; RDP-based budget tracking with subsampling amplification. Related work uses static DP, suggesting this is a novel contribution.

## Foundational Learning

- **Federated Learning (FL) Fundamentals**: Understanding FedAvg, local vs. global updates, and communication-efficiency tradeoffs is essential. Quick check: Can you explain why gradient aggregation preserves privacy better than raw data sharing, and what attack vectors remain?

- **Differential Privacy (DP) and Rényi DP (RDP)**: AdaLDP uses RDP for privacy accounting. You need to understand (ε,δ)-DP guarantees, sensitivity, noise calibration, and how RDP enables tighter composition across rounds. Quick check: Given gradient sensitivity Δ=2θ, how would you compute the per-round RDP cost ε_t(α) for noise scale σ_t?

- **Contrastive Learning Objectives**: IDRA and CDRD use margin-based and softmax-based contrastive losses. Understanding representation alignment, negative sampling, and temperature scaling is critical for debugging λ hyperparameters. Quick check: Why does CDRD push h_A and h_B apart while pulling both toward h_M? What failure mode would occur if h_A/h_B were collapsed together?

## Architecture Onboarding

- **Component map**: Raw Local Data (user u_k) → [PrivAugNet] → LLM API → Augmented sequences, features → [IDST-CL] → Embedding Layer → Transformer^A, Transformer^B, Transformer^M → IDRA, CDRD → MLP^A, MLP^B → CTR predictions → [AdaLDP] → Gradient clipping, Gaussian noise, RDP tracking → Server: FedAvg aggregation → Broadcast Θ^G_t

- **Critical path**: Data augmentation quality (PrivAugNet) → representation disentanglement (IDST-CL) → privacy-preserving gradient transmission (AdaLDP). Failures cascade: poor augmentation introduces noise that IDRA cannot fully correct; mis-tuned contrastive losses fail to disentangle domains; excessive noise destroys gradient signal.

- **Design tradeoffs**:
  1. Privacy vs. utility: Higher ε improves accuracy but weakens guarantees; optimal range ε∈[1,3]
  2. Augmentation richness vs. noise: More candidates increase diversity but risk hallucinations; optimal temp≈0.4, |C|≈10
  3. Domain disentanglement vs. transfer: Higher λ₂ enforces stronger separation but may fragment shared features; optimal λ₂≈0.5

- **Failure signatures**:
  - Stagnant loss with high noise: AdaLDP decay rate R too low (e.g., 0.991) or initial σ₀ too high → reduce σ₀ or increase R
  - Domain representations overlap in t-SNE: λ₂ too low (CDRD under-regularized) or transformer capacity insufficient → increase λ₂ or add transformer layers
  - Client dropout mid-training: Privacy budget ε₀ exhausted too quickly → increase ε₀ or reduce sampling ratio ρ
  - Augmentation hurts performance: LLM temperature too high → reduce temp to 0.4; or IDRA weight λ₁ too low → increase λ₁ to 0.3-0.4

- **First 3 experiments**:
  1. Baseline sanity check: Run FedCCTR-LM without any augmentation (raw data only) to establish lower bound; verify ablation numbers in Table 6 reproduce
  2. Hyperparameter sweep for PrivAugNet: Fix λ₁=λ₂=0.5, sweep LLM temperature {0, 0.4, 0.8, 1.2} and candidate size |C|={5, 10, 20} on a single domain pair (Book&Movie) to find optimal augmentation settings
  3. Privacy-utility curve: Fix optimal augmentation and contrastive settings, sweep privacy budget ε∈{0.1, 0.3, 0.6, 1, 3, 10} with AdaLDP, plot NDCG@10 vs. ε to characterize tradeoff; compare against static LDP baseline to validate adaptive noise benefit

## Open Questions the Paper Calls Out

- How can visual and textual multimodal content be effectively integrated into the PrivAugNet module to further enrich representations? The Conclusion explicitly states future research aims to "explore multimodal enhancement within the federated framework, leveraging visual and textual content."

- How can FedCCTR-LM be adapted to operate effectively in dynamic, real-time federated environments? The Conclusion identifies extending FedCCTR-LM to "dynamic, real-time federated environments" as necessary for broader applications.

- Is it computationally feasible to execute the PrivAugNet augmentation using Llama-2-13b on resource-constrained edge devices without relying on external APIs? Algorithm 1 dictates LLM-based data augmentation during "Client-Side Execution," while Section 5.1 reveals use of a computationally heavy 13-billion parameter model.

## Limitations

- The framework relies heavily on specific hyperparameter tuning and LLM augmentation quality, with performance gains dependent on careful calibration of λ hyperparameters and prompt templates
- The adaptive differential privacy mechanism (AdaLDP) lacks direct validation against static baselines in the main text, though Appendix A provides theoretical support
- The generalizability of the three-stage LLM augmentation process and specific prompt templates to domains beyond e-commerce remains untested and potentially problematic given task-specific nature of prompts

## Confidence

- **High Confidence**: Empirical performance improvements (NDCG@10 up to 10.45%, MRR@10 up to 6.84%) and ablation results showing PrivAugNet contribution are well-supported by experimental data; privacy accounting methodology using Rényi DP is theoretically sound
- **Medium Confidence**: Mechanism explanations for contrastive disentanglement and adaptive noise decay are plausible but rely on assumptions about domain preference decomposition and gradient sensitivity patterns requiring further validation across diverse datasets
- **Low Confidence**: Generalizability of three-stage LLM augmentation process and specific prompt templates to domains beyond e-commerce (e.g., healthcare, finance) remains untested and potentially problematic

## Next Checks

1. Cross-domain generalization test: Apply FedCCTR-LM to non-Amazon datasets (e.g., MovieLens + IMDb) to verify that the three-stage augmentation and contrastive objectives generalize beyond e-commerce domains

2. Dynamic privacy budget sensitivity: Systematically vary the privacy budget ε from 0.1 to 10 on multiple dataset pairs to characterize the full privacy-utility tradeoff curve and validate the adaptive noise benefits claimed in Figure 11

3. Ablation of LLM augmentation quality: Implement a controlled experiment comparing LLM-augmented features against heuristic synthetic data generation to isolate whether the LLM's semantic understanding (versus simple data expansion) drives the performance gains observed in Table 6