---
ver: rpa2
title: 'SemEval-2025 Task 9: The Food Hazard Detection Challenge'
arxiv_id: '2503.19800'
source_url: https://arxiv.org/abs/2503.19800
tags:
- uni00000013
- uni00000011
- uni00000048
- uni00000003
- uni00000044
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The Food Hazard Detection Challenge explored text-based food hazard
  prediction with long-tail class distributions, dividing the task into two subtasks:
  coarse-grained category prediction and fine-grained vector detection. The study
  found that large language model-generated synthetic data effectively addressed class
  imbalance, while fine-tuned encoder-only, encoder-decoder, and decoder-only systems
  achieved comparable performance.'
---

# SemEval-2025 Task 9: The Food Hazard Detection Challenge

## Quick Facts
- arXiv ID: 2503.19800
- Source URL: https://arxiv.org/abs/2503.19800
- Reference count: 12
- Primary result: Large language model-generated synthetic data effectively addressed class imbalance in food hazard detection, with no single transformer architecture consistently outperforming others

## Executive Summary
The Food Hazard Detection Challenge explored text-based food hazard prediction with long-tail class distributions, dividing the task into two subtasks: coarse-grained category prediction and fine-grained vector detection. The study found that large language model-generated synthetic data effectively addressed class imbalance, while fine-tuned encoder-only, encoder-decoder, and decoder-only systems achieved comparable performance. Performance metrics ranged from 0.1426 to 0.8223 in the category task and 0.0049 to 0.5473 in the vector task, with ensemble strategies and richer feature sets (both title and text) improving results. No single transformer architecture consistently outperformed others, and multi-task learning did not generally improve performance.

## Method Summary
The challenge involved two text classification subtasks on food-incident reports using 6,644 English food recall texts with long-tail class distributions. Subtask 1 required predicting HAZARD-CATEGORY (10 classes) and PRODUCT-CATEGORY (22 classes), while Subtask 2 required predicting specific HAZARD (128 classes) and PRODUCT (1,142 classes). Baselines included TF-IDF with Logistic Regression, fine-tuned bert-base-uncased, and CICLe with GPT-4 plus conformal prediction. Key approaches for addressing class imbalance included LLM-generated synthetic data through paraphrasing underrepresented classes, ensemble methods combining DeBERTa-v3-large and RoBERTa-large with focal loss and class weights, and using both title and text features together.

## Key Results
- Performance ranged from 0.1426 to 0.8223 in the category task and 0.0049 to 0.5473 in the vector task
- LLM-generated synthetic data effectively addressed class imbalance in the long-tail distribution
- Ensemble strategies combining different transformer architectures improved results over single models
- Multi-task learning did not generally improve performance compared to single-task approaches

## Why This Works (Mechanism)
The effectiveness of LLM-generated synthetic data in addressing class imbalance stems from the ability to generate diverse paraphrases of underrepresented classes, effectively expanding the training data for rare categories. Transformer architectures, regardless of type (encoder-only, encoder-decoder, or decoder-only), can capture complex semantic relationships in food hazard descriptions when fine-tuned appropriately. The combination of title and text features provides complementary information - titles offer concise summaries while full texts provide detailed context. Ensemble methods improve robustness by aggregating predictions from different model architectures, each potentially capturing different aspects of the classification task.

## Foundational Learning
**Text Classification Fundamentals**: Understanding document categorization into predefined classes is essential for food hazard detection. Quick check: Can you explain the difference between binary, multi-class, and multi-label classification?

**Class Imbalance Handling**: Techniques like synthetic data generation, focal loss, and class weighting are crucial when some categories have few training examples. Quick check: What happens to model performance when rare classes are ignored during training?

**Transformer Architecture Basics**: Encoder-only (BERT), encoder-decoder (T5), and decoder-only (GPT) models each have different strengths for sequence processing. Quick check: Can you identify which transformer component processes input tokens in BERT vs GPT?

**Macro F1 Score**: This metric equally weights performance across all classes, making it sensitive to performance on rare categories. Quick check: How does macro F1 differ from micro F1 in evaluating imbalanced datasets?

**Hierarchical Classification**: Predicting coarse categories before fine-grained classes can improve accuracy by reducing search space. Quick check: Why might predicting product category first help narrow down specific product identification?

## Architecture Onboarding

**Component Map**: Raw Text -> Text Concatenation (TITLE + TEXT) -> Transformer Model (BERT/DeBERTa/RoBERTa) -> Classification Head -> Predicted Categories/Vectors

**Critical Path**: Text preprocessing → Model fine-tuning → Class imbalance mitigation → Ensemble prediction → Macro F1 evaluation

**Design Tradeoffs**: Single-task vs multi-task learning (multi-task didn't improve performance generally), encoder-only vs encoder-decoder vs decoder-only architectures (comparable performance), synthetic data vs real data (synthetic helped rare classes), ensemble vs single model (ensemble improved results)

**Failure Signatures**: Poor performance on rare classes (<5 samples) with near-zero F1 scores, dramatic performance drop in fine-grained vector task (ST2) compared to category task (ST1), instances with zero accuracy across all systems indicating extreme difficulty or missing information

**First Experiments**:
1. Fine-tune bert-base-uncased on ST1 using provided stratified split and verify ~0.667 baseline on validation set
2. Generate synthetic data for the rarest classes (<5 samples) and measure impact on per-class F1 for low-frequency categories
3. Compare ensemble predictions (DeBERTa-v3-large + RoBERTa-large) against single-model performance using soft voting

## Open Questions the Paper Calls Out

**Open Question 1**: Can explainability techniques systematically improve performance on the fine-grained vector classification task (ST2)? The paper suggests this is under-explored, with ST2 scores (0.0049-0.5473) remaining substantially lower than ST1 (0.1426-0.8223) despite no participating systems incorporating explainability methods.

**Open Question 2**: Under what conditions does multi-task learning provide measurable benefits over single-task learning for food hazard detection? The paper reports inconsistent findings with three out of four top systems using single-task approaches, yet one system found multi-turn prompts outperformed single-turn, concluding that multi-task learning did not generally improve performance.

**Open Question 3**: What textual or structural features characterize instances never correctly classified by any participating system? The difficulty analysis notes a spike at zero accuracy for all categories/vectors, indicating extremely difficult or information-missing samples, with instance difficulty scores included in the released dataset.

## Limitations
- Incomplete specification of experimental details and individual team methodologies prevents verification of which techniques drove reported performance ranges
- Extreme class imbalance inherent to the dataset makes it difficult to achieve high performance on rare categories regardless of approach
- Lack of ablation studies prevents understanding which components were most effective in top-performing systems

## Confidence

**High confidence**: Dataset structure (6,644 texts, 10 hazard categories, 22 product categories, 128 hazard classes, 1,142 product classes), evaluation metric (macro F1 with combined score formula), and finding that no single architecture consistently outperformed others

**Medium confidence**: Claim that LLM-generated synthetic data effectively addressed class imbalance is supported by performance improvements but lacks detailed methodology for verification

**Low confidence**: Specific performance thresholds (0.667 baseline for ST1, 0.82 max for ST1, 0.55 max for ST2) cannot be independently validated without exact implementations and hyperparameters

## Next Checks
1. Recreate the ST1 baseline using bert-base-uncased with provided train/validation/test splits and verify the ~0.667 macro F1 score on the validation set
2. Implement LLM-based synthetic data generation for the rarest classes (<5 samples) and measure the impact on per-class F1 scores for low-frequency categories
3. Compare ensemble predictions (DeBERTa-v3-large + RoBERTa-large) against single-model performance to verify that soft voting provides measurable improvement over individual models