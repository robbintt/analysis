---
ver: rpa2
title: 'A Customer Journey in the Land of Oz: Leveraging the Wizard of Oz Technique
  to Model Emotions in Customer Service Interactions'
arxiv_id: '2511.21909'
source_url: https://arxiv.org/abs/2511.21909
tags:
- emotion
- valence
- emotions
- customer
- participants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EmoWOZ-CS, a new corpus of customer service
  dialogues collected through a controlled Wizard of Oz experiment. Participants engaged
  in written conversations with what they believed to be autonomous agents, while
  wizards actively steered interactions toward predetermined affective trajectories.
---

# A Customer Journey in the Land of Oz: Leveraging the Wizard of Oz Technique to Model Emotions in Customer Service Interactions

## Quick Facts
- **arXiv ID:** 2511.21909
- **Source URL:** https://arxiv.org/abs/2511.21909
- **Reference count:** 40
- **Primary result:** EmoWOZ-CS corpus of 2,148 bilingual dialogues with emotion annotations and operator strategy labels enables controlled emotion modeling in customer service contexts.

## Executive Summary
This paper introduces EmoWOZ-CS, a new corpus of customer service dialogues collected through a controlled Wizard of Oz experiment. Participants engaged in written conversations with what they believed to be autonomous agents, while wizards actively steered interactions toward predetermined affective trajectories. The resulting dataset contains 2,148 bilingual dialogues annotated with discrete emotions, dimensional valence-arousal-dominance ratings, operator response strategies, and participant profiling data. Temporal analysis confirms successful conversation-level steering, with negative targets showing the clearest divergence in valence progression. Emotion annotation agreement is moderate for multilabel emotions and valence, lower for arousal and dominance.

## Method Summary
The EmoWOZ-CS corpus was collected through a Wizard of Oz experiment where participants believed they were interacting with autonomous agents while wizards actively steered conversations toward predetermined valence targets (positive/neutral/negative). The dataset contains 2,148 bilingual (Dutch/English) dialogues across four domains, with participant turns annotated for 11 discrete emotions plus neutral, dimensional VAD scores, and operator response strategies. The corpus underwent post-processing including external emotion annotation, wizard strategy self-labeling, translation post-editing, and anonymization. Machine learning benchmarks were conducted using fine-tuned encoder models (RobBERT, ModernBERT) for detection and inference tasks, weighted fusion experiments, and zero-shot LLM evaluation.

## Key Results
- Operator response strategies show clear associations with subsequent participant emotions, with suboptimal responses increasing negative affect
- Temporal analysis confirms successful conversation-level steering toward negative valence targets
- Machine learning benchmarks demonstrate challenging forward-looking emotion inference, with zero-shot prompting redistributing probability toward non-neutral emotions
- Emotion annotation agreement is moderate for multilabel emotions and valence, lower for arousal and dominance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Operator response strategies causally influence subsequent participant emotional states, with systematic differences between strategy types.
- **Mechanism:** Suboptimal responses (miscomprehension, non-collaborative replies, irony) increase negative affect (anger, annoyance, disappointment, desire, confusion), while objective strategies (explanation, help online/offline) often elicit neutrality or gratitude. Some affective strategies (cheerfulness, gratitude) foster positive reciprocity, whereas others (apology, empathy) may leave residual negative states.
- **Core assumption:** Participant emotional responses in the WOZ setting generalize to real customer service interactions.
- **Evidence anchors:** Heatmap analysis shows normalized emotion distributions following each strategy; suboptimal techniques yield >55% negative affect collectively.

### Mechanism 2
- **Claim:** The Wizard of Oz paradigm with operator-steered valence trajectories enables controlled collection of in-domain emotional dialogue data that would otherwise be inaccessible.
- **Mechanism:** Participants interact believing the agent is autonomous, producing naturalistic responses, while wizards actively steer conversations toward predetermined affective endpoints (positive/neutral/negative). This combines experimental control with ecological validity in task structure and language use.
- **Core assumption:** Deception does not fundamentally alter emotional expression patterns in customer service contexts.
- **Evidence anchors:** 73.7% of participants reported unawareness or only slight suspicion of the human operator; only 5% were certain.

### Mechanism 3
- **Claim:** Forward-looking emotion inference from conversational context is fundamentally harder than same-turn detection, with zero-shot LLMs showing a characteristic redistribution toward non-neutral predictions.
- **Mechanism:** Predicting future emotional states from prior operator and customer turns lacks access to the target utterance's textual cues. Supervised fusion models achieve ~53-55% emotion accuracy (vs. ~74% same-turn), while zero-shot LLMs redistribute probability mass away from the dominant neutral class toward negative emotions despite lower overall accuracy.
- **Core assumption:** Textual context alone contains sufficient signal to anticipate upcoming emotional shifts; self-reported labels represent ground truth.
- **Evidence anchors:** Baseline comparison shows p(0) detection at 74.4% vs. p(1) inference at 53.2%; zero-shot yields 15.9% emotion accuracy but higher per-class F1 for anger/annoyance than fusion models.

## Foundational Learning

- **Concept:** Valence-Arousal-Dominance (VAD) dimensional emotion representation
  - **Why needed here:** Annotations use both discrete emotion categories and continuous VAD scores; understanding these dimensions is essential for interpreting agreement differences (valence moderate, arousal lower, dominance weakest) and dimensional benchmarks.
  - **Quick check question:** Can you explain why dominance might show lower inter-annotator agreement than valence in text-based customer service interactions?

- **Concept:** Wizard of Oz experimental paradigm in NLP
  - **Why needed here:** The entire corpus design relies on WOZ methodology; practitioners must understand the trade-offs between experimental control and ecological validity, deception protocols, and wizard training requirements.
  - **Quick check question:** What are the ethical considerations when using deception in WOZ experiments for emotion research?

- **Concept:** Multi-label vs. multi-class annotation schemes
  - **Why needed here:** Emotions are annotated in a multi-label setup, but ML experiments reduce to multi-class using inverse-frequency precedence; understanding this transformation is critical for interpreting benchmark results and class imbalance effects.
  - **Quick check question:** Why might inverse-frequency precedence for label reduction preserve rare emotions but potentially introduce noise?

## Architecture Onboarding

- **Component map:** Dialogue layer (2,148 bilingual conversations) -> Annotation layer (emotions, VAD scores, strategies) -> Metadata layer (demographics, personality traits, WOZ awareness) -> Parallel corpus (English translations) -> Benchmark layer (pre-formatted splits)

- **Critical path:** Data collection (WOZ experiment with trained wizards) -> Post-processing (external annotation, translation, anonymization) -> ML pipeline (fine-tuned encoders, fusion experiments, zero-shot evaluation)

- **Design tradeoffs:** Controlled vs. naturalistic (WOZ provides scenario grounding but may not capture full unpredictability), Annotation depth vs. agreement (rich multi-label scheme yields moderate agreement), Expression vs. experience (ground truth relies on third-party annotations)

- **Failure signatures:** Class imbalance (neutral dominates ~50%), Fusion asymmetry (gains driven by single superior component), Zero-shot disconnect (LLM redistributes toward negative emotions unexpressed in ground truth)

- **First 3 experiments:**
  1. **Replicate detection baseline:** Fine-tune encoder-only model (RobBERT for Dutch, ModernBERT for English) on same-turn emotion detection using participant-level 5-fold splits; verify ~74% accuracy ceiling.
  2. **Test fusion contribution:** Implement weighted late fusion of other-to-future (p(1)) and self-to-future (p(2b)) predictions; ablate each branch to confirm asymmetric contribution patterns.
  3. **Evaluate zero-shot shift:** Prompt GPT-4 or comparable LLM with same context windows; analyze per-class F1 distribution to confirm redistribution away from neutral toward anger/annoyance/fear categories.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can emotion inference models move beyond short-window late fusion to effectively integrate longer discourse context, role dynamics, and causal structure across turns?
- **Basis in paper:** "For future work, we propose replacing short-window late fusion with long-context, role-aware models that retrieve salient episodes across the dialogue and attach causal, turn-level attributions to operator strategies."
- **Why unresolved:** The ablation analysis reveals that fusion gains stem predominantly from a single stronger branch rather than genuine complementarity between prediction strategies, with F1-macro and F1-weighted scores often deteriorating despite accuracy gains.
- **What evidence would resolve it:** A model architecture that demonstrates consistent improvements across accuracy, F1-macro, and F1-weighted when combining multiple contextual sources, with ablation studies showing each component contributes meaningfully.

### Open Question 2
- **Question:** How should perspectivist supervision frameworks pair self-reports with third-party judgments to model valid disagreement without causing label collapse?
- **Basis in paper:** "supervision should be perspectivist: first-party self-reports can be explicitly paired with third-party judgments to model valid disagreement and prevent label collapse, an idea supported by recent surveys and evidence showing misalignment between third-party readings and authors' private emotional states."
- **Why unresolved:** The paper documents significant divergence between self-reports and third-party labels (average Krippendorff's α of 0.2065), with emotions like admiration, fear, and relief showing high participant-only reporting (>0.70), while the current detection models rely solely on expression-focused ground truth.
- **What evidence would resolve it:** A training paradigm that explicitly models disagreement as signal, with evaluation protocols that distinguish between predicting expressed emotion versus experienced emotion.

### Open Question 3
- **Question:** What explains the zero-shot LLM's redistribution of probability away from neutral toward negative emotions, and can this sensitivity be harnessed while maintaining overall accuracy?
- **Basis in paper:** "instruction-only zero-shot underperforms on multiclass emotions, but shows sensitivity to subtle negative affect, hinting at a mismatch between unexpressed internal states and observable emotion expression in text."
- **Why unresolved:** Zero-shot achieves better per-class F1 for anger (0.24/0.27), annoyance (0.16/0.17), and fear (0.05/0.09) than supervised fusion, suggesting sensitivity to latent affect, yet overall accuracy remains low (0.16–0.21).
- **What evidence would resolve it:** Analysis correlating zero-shot predictions with self-reported emotions rather than third-party labels, potentially revealing alignment with experienced but unexpressed affect.

### Open Question 4
- **Question:** Can introducing multiple, evolving target valence points throughout an interaction enable finer-grained tracking and influencing of emotional shifts within smaller conversational segments?
- **Basis in paper:** "future research could explore tracking and influencing emotional shifts within smaller conversational segments by introducing multiple, evolving target valence points throughout the interaction."
- **Why unresolved:** The current WOZ design uses only conversation-level steering with a single end-valence target, which successfully produces negative trajectories but shows similar distributions for positive and neutral targets.
- **What evidence would resolve it:** A WOZ experiment with segment-level valence targets demonstrating distinct trajectory divergence across multiple intermediate points, with analysis of operator strategies that trigger specific within-conversation emotion transitions.

## Limitations
- The corpus represents a controlled experimental setting that may not fully capture the unpredictability of real-world customer service interactions.
- The Wizard of Oz paradigm relies on deception that could influence participant behavior even when detection rates are low.
- The dataset exhibits significant class imbalance, with neutral emotions comprising approximately 50% of annotations.

## Confidence
- **High confidence:** Operator strategy-emotion associations (Section 4.4.2) - supported by clear statistical patterns across 10,858 participant messages
- **Medium confidence:** Controlled emotion steering effectiveness (Section 4.2.1) - temporal divergence patterns are evident but may be influenced by scenario structure
- **Medium confidence:** ML benchmark performance (Section 5) - results follow expected patterns for imbalanced multi-label classification

## Next Checks
1. **Validate real-world generalizability:** Deploy trained emotion detection models on actual customer service logs to assess performance degradation and identify domain gaps between experimental and production data.
2. **Test deception sensitivity:** Conduct a follow-up study with participants who were aware of the WOZ setup from the beginning to measure differences in emotional expression patterns and strategy effectiveness.
3. **Evaluate zero-shot robustness:** Replace GPT-5 nano with currently available large language models (GPT-4, Claude, LLaMA) to verify whether the characteristic redistribution toward non-neutral emotions persists across model families.