---
ver: rpa2
title: Evaluating the Effectiveness of Direct Preference Optimization for Personalizing
  German Automatic Text Simplifications for Persons with Intellectual Disabilities
arxiv_id: '2507.01479'
source_url: https://arxiv.org/abs/2507.01479
tags:
- text
- group
- target
- preference
- pairs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored personalizing LLM-based automatic text simplification
  for persons with intellectual disabilities using direct preference optimization
  (DPO). A lightweight web application was developed to collect preference annotations
  from both target group participants and text simplification experts.
---

# Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities

## Quick Facts
- **arXiv ID:** 2507.01479
- **Source URL:** https://arxiv.org/abs/2507.01479
- **Reference count:** 40
- **Primary result:** DPO improved readability but reduced semantic preservation; expert preferences yielded better alignment than target group data due to higher annotation consistency.

## Executive Summary
This study explored personalizing German automatic text simplification (ATS) for persons with intellectual disabilities using direct preference optimization (DPO). The authors developed HF4ATS, a dataset of over 6,000 preference pairs annotated by both target users and experts, and used it to post-train three German ATS models. While DPO improved text readability (lower WSTF4 scores), it also reduced semantic preservation (BERTScore). Expert-trained models showed higher win rates and better alignment with human preferences compared to target group models, likely due to higher annotation consistency. The findings suggest that DPO may be suboptimal for modeling diverse subjective preferences without explicit reward modeling.

## Method Summary
The authors curated HF4ATS from DEplain-APA, filtering sentence pairs using cosine similarity (>0.5) and ROUGE (<0.8) thresholds to create 3,600 training pairs. They fine-tuned three German LLMs (DiscoLeo-Llama-3-8B, Llama-3.1-8B, LeoLM-Mistral-7B) using LoRA (rank 16, alpha 32) with AdamW optimizer. Preference data (3,037 pairs) was collected via web application from target group participants and experts. DPO post-training used β=0.1, with checkpoints selected by highest win rate on dev set. Models were evaluated using WSTF4, SARI, BERTScore, and human preference tests.

## Key Results
- DPO improved readability (lower WSTF4 scores) across all models, especially for expert-trained versions
- Expert-trained DPO models achieved win rates >60%, while target-trained models hovered around 50%
- BERTScore dropped significantly during DPO (0.90 → 0.78-0.88), indicating semantic drift
- Target group preferences showed high variability (Intra-AA near zero), limiting DPO effectiveness

## Why This Works (Mechanism)

### Mechanism 1: Implicit Reward Alignment Improves Readability
DPO post-training improves text readability by aligning policy outputs with preference-annotated simplifications through implicit reward optimization. DPO bypasses explicit reward modeling by reparameterizing the reward function through log-odds differences between preferred and dispreferred outputs, directly shifting probability mass toward human-preferred simplifications.

### Mechanism 2: Preference Consistency Enables Effective Alignment
DPO effectiveness depends on annotation consistency; expert preferences produce better alignment than target group preferences due to higher inter-annotator agreement. DPO assumes preferences follow a Bradley-Terry distribution; inconsistent preferences violate this assumption, reducing the signal-to-noise ratio in the implicit reward.

### Mechanism 3: Semantic Drift Under Contrastive Optimization
DPO post-training reduces semantic preservation (BERTScore) even while improving readability, suggesting a trade-off in the optimization objective. The contrastive loss optimizes for preference ranking without explicit semantic constraints; preferred simplifications may sacrifice information content for simplicity.

## Foundational Learning

- **Bradley-Terry Preference Modeling**: Models the probability that one item is preferred over another as a sigmoid of reward difference. Needed to understand why DPO requires consistent preferences—if preferences don't follow this distribution, implicit reward estimates become unreliable.
  - Quick check question: Given two simplifications with rewards 2.0 and 0.5, what is P(preferred | Bradley-Terry)?

- **Implicit Reward in DPO**: DPO estimates rewards directly from policy log-probabilities without training a separate reward model. Needed because the paper relies on implicit reward margins to explain why expert preferences work better than target preferences.
  - Quick check question: Why does β control deviation from the reference model in the DPO loss?

- **Preference Consistency Metrics (Intra-AA/Inter-AA)**: Cohen's Kappa for self-consistency; Krippendorff's Alpha for group consistency. Needed to interpret Table 1 and understand why target group annotations fail to produce successful personalization.
  - Quick check question: If Inter-AA = 0.02, what does this imply about group-level preference stability?

## Architecture Onboarding

- **Component map**: DEplain-APA -> SFT LoRA fine-tuning -> Preference pair generation -> Web app annotation -> DPO post-training -> Checkpoint selection -> Evaluation
- **Critical path**: SFT checkpoint quality → Preference pair diversity → Annotation consistency → DPO convergence → Win rate evaluation. Paper shows expert annotations succeed (win rates 60%+) while target annotations fail (~50%) due to consistency gaps.
- **Design tradeoffs**:
  - Full-prompt vs. completion-only tuning: Full-prompt improves instruction following when prompt:completion ratio >5
  - Showing complex text to annotators: Experts with complex text (ea02, ea04) showed different preferences than those without
  - Data filtering thresholds: Semantic similarity <0.5 and ROUGE >0.8 removed low-quality pairs, but may exclude valid simplification strategies
- **Failure signatures**:
  - Win rate oscillating around 50% → Preference inconsistency too high for DPO
  - BERTScore dropping significantly → Over-simplification without semantic constraint
  - Intra-AA near zero (target group) → Annotations may not reflect stable preferences
- **First 3 experiments**:
  1. Replicate SFT checkpoint selection with one model (e.g., Llama-3.1-8B) on filtered DEplain-APA; validate SARI and WSTF4 on held-out set
  2. Collect small preference dataset (100 pairs) from simulated "expert" annotators with controlled consistency; train DPO and measure win rate on held-out pairs
  3. Ablate β parameter (0.05, 0.1, 0.2) to observe trade-off between preference alignment and semantic drift

## Open Questions the Paper Calls Out

### Open Question 1
Can alternative alignment methods like Kahneman-Tversky Optimization (KTO) successfully personalize ATS models for persons with intellectual disabilities while reducing the cognitive load of the annotation process compared to DPO? The authors explicitly propose exploring KTO in future work, noting that it removes the need for pairwise comparisons and could better incorporate cognitive biases like loss aversion.

### Open Question 2
Do sophisticated, explicit reward models (such as those used in PPO) outperform implicit reward methods (DPO) in capturing the complex utility structures of users with intellectual disabilities? The authors explicitly ask in the conclusion whether "sophisticated reward models are still needed to actively capture the complex utility structures underlying persons' choices."

### Open Question 3
How can adaptive and accessible HCI tools be designed to elicit more consistent preference feedback from users with cognitive disabilities? The conclusion identifies the need for "improved HCI approaches for eliciting preferences, including adaptive and accessible tools that support users in providing consistent feedback."

## Limitations
- Preference consistency as DPO success factor is based on proxy metrics without direct testing of how preference noise propagates through the Bradley-Terry likelihood
- Semantic preservation trade-off relies on BERTScore interpretation without qualitative analysis of actual semantic loss
- Generalization to real users is limited as the study used simulated "target group" participants rather than actual persons with intellectual disabilities

## Confidence

**High Confidence:**
- SFT phase methodology and checkpoint selection process
- Preference collection methodology (web application, annotation protocols)
- Basic DPO implementation using LoRA and standard hyperparameters

**Medium Confidence:**
- Attribution of DPO success to preference consistency differences
- Interpretation of BERTScore drops as semantic drift
- Win rate comparisons between expert and target-trained models

**Low Confidence:**
- Generalization of target group preference patterns to actual users with intellectual disabilities
- Causal relationship between preference consistency and DPO effectiveness without explicit reward modeling
- Semantic preservation claims based solely on BERTScore without qualitative analysis

## Next Checks
1. **Ablation of Preference Noise**: Train DPO models on preference subsets with varying Inter-AA thresholds (0.1, 0.2, 0.3+) to directly test the consistency-effectiveness relationship. Measure win rates and semantic preservation across these ablations.

2. **Alternative Semantic Metrics**: Supplement BERTScore with human evaluation of semantic preservation on a held-out test set. Compare how different preference sources (expert vs. target) affect the readability-semantics trade-off.

3. **KTO Comparison**: Implement the KTO algorithm as an alternative to DPO to test whether explicit reward modeling can handle the noisy target group preferences better than implicit reward alignment.