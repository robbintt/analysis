---
ver: rpa2
title: Do LLMs Judge Distantly Supervised Named Entity Labels Well? Constructing the
  JudgeWEL Dataset
arxiv_id: '2601.00411'
source_url: https://arxiv.org/abs/2601.00411
tags:
- entity
- luxembourgish
- dataset
- data
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents judgeWEL, a dataset for named entity recognition
  (NER) in Luxembourgish, constructed using Wikipedia and Wikidata as sources of distant
  supervision. The authors propose a pipeline that combines automatic entity linking
  with LLM-based quality control to filter out noisy annotations.
---

# Do LLMs Judge Distantly Supervised Named Entity Labels Well? Constructing the JudgeWEL Dataset

## Quick Facts
- **arXiv ID:** 2601.00411
- **Source URL:** https://arxiv.org/abs/2601.00411
- **Reference count:** 0
- **Primary result:** judgeWEL dataset with 28,866 Luxembourgish NER sentences built using Wikipedia/Wikidata distant supervision and LLM-based quality filtering

## Executive Summary
This paper introduces judgeWEL, a large-scale named entity recognition dataset for Luxembourgish constructed using Wikipedia and Wikidata as sources of distant supervision. The authors develop a pipeline that combines automatic entity linking with LLM-based quality control to filter out noisy annotations. The resulting dataset contains five entity types and is approximately five times larger than existing Luxembourgish NER resources. The study demonstrates that proprietary LLMs, particularly GPT-5-mini, effectively filter high-quality annotated sentences with agreement approaching human inter-annotator levels, and that models trained on judgeWEL achieve strong performance on downstream NER tasks.

## Method Summary
The method involves three main stages: distant supervision from Wikipedia/Wikidata, automatic annotation refinement, and LLM-based quality filtering. Wikipedia hyperlinks are mapped to Wikidata entities using the P31 property to determine entity types (PER, ORG, LOC, DATE, MISC). The pipeline includes sentence selection, entity linking, and initial annotation, followed by refinement using a fine-tuned LuxGPT-NER model and regex patterns for dates. Finally, GPT-5-mini acts as a judge to filter low-quality annotations through binary keep/discard decisions on batches of sentences, producing the final judgeWEL dataset with 28,866 sentences split 80/10/10 for training, validation, and testing.

## Key Results
- judgeWEL dataset contains 28,866 sentences, approximately 5x larger than existing Luxembourgish NER resources
- GPT-5-mini achieves Cohen's κ=0.62 in filtering quality, approaching human inter-annotator agreement (κ=0.66)
- Open-weight models show near-zero or negative correlation (κ=-0.29 to -0.05), highlighting proprietary LLM advantage
- Downstream models trained on judgeWEL achieve high F1 scores (~0.91-0.92), demonstrating dataset reliability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structured knowledge bases (Wikipedia + Wikidata) can generate initial NER annotations with minimal human intervention.
- **Mechanism:** Wikipedia internal hyperlinks identify entity spans → Wikidata API retrieves entity attributes (P31 "instance of" property) → attribute matching maps to predefined entity types (PER, ORG, LOC, DATE) → BIO encoding applied.
- **Core assumption:** Wikipedia hyperlinks point to valid entity pages and Wikidata contains accurate type classifications for those entities.
- **Evidence anchors:**
  - [abstract] "leveraging Wikipedia and Wikidata as structured sources of weak supervision. By exploiting internal links within Wikipedia articles, we infer entity types based on their corresponding Wikidata entries"
  - [Section 3.2] Details the Wikidata property matching (P31 = Q5 for PER, organizational QIDs for ORG, etc.)
  - [corpus] Related DS-NER work (DynClean, Towards DS-NER) confirms distant supervision introduces noise but remains scalable
- **Break condition:** Target language has sparse Wikipedia coverage or inconsistent Wikidata entity typing.

### Mechanism 2
- **Claim:** Proprietary LLMs can approximate human judgment in validating annotation quality, approaching inter-annotator agreement levels.
- **Mechanism:** LLM receives tokenized sentence + BIO tags → prompted to verify correctness (missing labels, incorrect spans, valid negative examples) → outputs binary keep/discard decision.
- **Core assumption:** LLMs can understand Luxembourgish text sufficiently to evaluate annotation consistency even without explicit training support.
- **Evidence anchors:**
  - [abstract] "proprietary LLMs, especially GPT-5-mini, are effective at filtering high-quality annotated sentences, with agreement approaching human inter-annotator levels"
  - [Section 4.1, Table 2] GPT-5-mini achieves κ=0.62 vs human κ=0.66; open-weight models show near-zero or negative correlation
  - [corpus] Limited direct corpus evidence on LLM-as-judge for NER; related work focuses on synthetic data generation
- **Break condition:** LLM lacks sufficient representation of target language; smaller open-weight instruction-tuned models fail (Gemma-3-27B κ=-0.29, LLaMA-3.3-8B κ=-0.05).

### Mechanism 3
- **Claim:** Multi-stage annotation refinement combining distant supervision, model-based prediction, and pattern matching catches different error types before LLM filtering.
- **Mechanism:** Initial distant supervision labels → LuxGPT-NER re-evaluates O-tagged tokens → regex patterns capture date expressions → LLM filters remaining noise.
- **Core assumption:** Each stage addresses a distinct failure mode of prior stages (missed entities, incomplete date tagging).
- **Evidence anchors:**
  - [Section 3.4] "Using a LuxGPT model fine-tuned... we re-evaluate all entities initially labelled with O-tags"
  - [Section 3.4] "apply a regular expression–based script that searches for date patterns"
  - [corpus] DS-NER noise handling approaches (DynClean, Constraint PU Learning) use training dynamics rather than LLMs for cleaning
- **Break condition:** Compounding errors if early-stage noise overwhelms subsequent refinement capacity.

## Foundational Learning

- **BIO Tagging Scheme (Begin-Inside-Outside):**
  - Why needed here: The entire annotation pipeline outputs BIO-encoded labels; understanding token-level boundary marking is essential for debugging.
  - Quick check question: Given ["De", "Jhempi", "Kniddel", "schafft"], what BIO tags indicate a person named "Jhempi Kniddel"?

- **Cohen's Kappa (Inter-annotator Agreement):**
  - Why needed here: The paper's central claim hinges on LLM κ approaching human κ; without this metric, the quality argument is unintelligible.
  - Quick check question: If two annotators agree 83% of the time (414/500), why might κ=0.66 be more informative than raw accuracy?

- **Distant Supervision:**
  - Why needed here: The baseline data generation method; understanding its inherent noise characteristics explains why filtering is necessary.
  - Quick check question: Why does aligning text with Wikidata entities produce false negatives (missed entities) even when the entity appears in the text?

## Architecture Onboarding

- **Component map:**
Wikipedia XML dump → WikiExtractor (JSON) → Sentence splitting (German mode) → Entity Linking (Wikidata API, P31 property matching) → Sentence Selection (skip intro, next 5, filter short/redundant) → Annotation Refinement (LuxGPT-NER + date regex) → LLM-as-Judge (batch of 20, CSV output: ID, keep/discard) → judgeWEL dataset (28,866 sentences, 80/10/10 split)

- **Critical path:** Entity linking accuracy determines upper bound on annotation quality. LLM judge selection (GPT-5-mini over open-weight alternatives) determines filtering precision. Do not swap to smaller open models without re-validating κ.

- **Design tradeoffs:**
  - **Reliability vs coverage:** Pipeline accepts loss of valid samples (74,710 → 28,866) for higher consistency.
  - **Cost efficiency:** GPT-5-mini (~$25 for 74k sentences) vs GPT-5 (~$180) with identical output quality.
  - **Proprietary vs reproducible:** Best κ from closed models; open-weight alternatives (Mistral-Medium-3.1 κ=0.45) offer transparency but lower accuracy.

- **Failure signatures:**
  - Generative LLMs (LuxGPT-NER, Aya, Llama) produce malformed BIO sequences—token merging, boundary omission—on downstream NER tasks.
  - Cross-domain transfer (judgeWEL → RTL-NER) causes LLM F1 drop (Llama: 0.92 → 0.02), while encoder models remain stable.
  - Small open-weight judges produce negative κ, indicating worse-than-random filtering.

- **First 3 experiments:**
  1. **Replicate LLM-as-Judge evaluation:** Run GPT-5-mini and Mistral-Medium-3.1 on the 500-sentence human-annotated subset; verify κ values match reported 0.62 and 0.45.
  2. **Ablate annotation refinement:** Skip LuxGPT-NER and regex stages; measure final dataset size and downstream F1 to quantify each stage's contribution.
  3. **Cross-domain stress test:** Train LuxemBERT on judgeWEL, evaluate on RTL-NER with original GPE/LOC split (not merged); identify which entity types transfer poorly.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can this LLM-as-judge methodology generalize effectively to under-represented languages with sparse or inconsistent Wikipedia coverage, or does the approach fundamentally require a sufficiently populated Wikipedia edition?
- **Basis in paper:** [explicit] The authors state in the Limitations section: "the methodology depends on the existence of a sufficiently populated Wikipedia edition and reliable entity linking to Wikidata. For languages with extremely sparse or inconsistent Wikipedia coverage, this strategy may yield limited or biased samples." They also mention in the Conclusion that "Future work will focus on extending this methodology to other under-represented languages with similar data profiles."
- **Why unresolved:** The pipeline was only tested on Luxembourgish, which has approximately 77k Wikipedia articles. It remains untested whether languages with significantly smaller Wikipedia footprints can still benefit from this approach, or whether alternative base data sources would be needed.
- **What evidence would resolve it:** Applying the same pipeline to languages with varying Wikipedia coverage sizes (e.g., languages with <10k articles, <1k articles) and comparing the quality and quantity of resulting datasets against human-annotated benchmarks for those languages.

### Open Question 2
- **Question:** How stable and reproducible are LLM-as-judge evaluations across different prompts, temperature settings, and API versions, and what is the magnitude of variance introduced by these factors?
- **Basis in paper:** [explicit] The authors note in the Limitations that "LLMs can approximate human judgement, their evaluations are still probabilistic and subject to variation across prompts, temperature settings, and API versions."
- **Why unresolved:** The paper reports results from a single prompt configuration and does not systematically test how sensitive the judge evaluations are to prompt wording, sampling parameters, or model versioning. This raises concerns about reproducibility and long-term reliability of the pipeline.
- **What evidence would resolve it:** A systematic ablation study varying prompt formulations (e.g., different instruction phrasings, example formats), temperature settings (0.0, 0.3, 0.7, 1.0), and testing across multiple API versions or model checkpoints to quantify variance in agreement scores with human annotators.

### Open Question 3
- **Question:** Can the coarse-grained five-entity-type schema be reliably extended to finer-grained entity subtypes (e.g., distinguishing organizations from institutions, temporal from numerical expressions) while maintaining LLM judge agreement rates comparable to human inter-annotator levels?
- **Basis in paper:** [explicit] The authors acknowledge in the Limitations that "the dataset focuses on five coarse-grained entity categories, which may not capture finer semantic distinctions (e.g., between organisations and institutions or between temporal and numerical expressions)." In the Conclusion, they propose future work on "refining label granularity to capture additional entity subtypes and relational information."
- **Why unresolved:** The current schema merges GPE and LOC, excludes many potential subtypes, and LLM judge performance was only evaluated on the simplified five-category system. Finer distinctions typically yield lower inter-annotator agreement even among humans, and it is unknown whether LLMs would maintain κ ≈ 0.62 with more nuanced categories.
- **What evidence would resolve it:** Extending the annotation schema to include 8-12 entity types with finer distinctions, re-running the LLM-as-judge evaluation on a new human-annotated sample, and comparing κ values to the current 0.62 baseline and to human inter-annotator agreement on the same expanded schema.

### Open Question 4
- **Question:** What is the optimal cost-quality trade-off point for human-in-the-loop validation, and does limited human verification at critical pipeline stages significantly improve precision compared to fully automatic LLM-filtering?
- **Basis in paper:** [explicit] The authors propose in the Conclusion that "Incorporating limited human-in-the-loop validation and cross-lingual alignment could further enhance both precision and transferability." In the Limitations, they note that "Manual validation or iterative human-in-the-loop refinement will be essential for tasks requiring high precision, such as information extraction in sensitive or policy-related domains."
- **Why unresolved:** Only 500 sentences received human verification (used primarily for LLM evaluation), and the current pipeline accepts losing valid samples for higher consistency. The paper does not test whether strategic human intervention (e.g., at low-confidence LLM decisions, or for specific entity types like ORG which has fewer samples) would yield better overall dataset quality.
- **What evidence would resolve it:** Experiments comparing multiple pipeline variants with human verification applied at different stages (e.g., reviewing only sentences where LLM confidence is below a threshold, reviewing only rare entity types, or reviewing a random percentage) and measuring downstream NER model performance on held-out human-annotated test sets.

## Limitations
- Heavy reliance on proprietary LLM technology (GPT-5-mini) creates reproducibility challenges since open-weight alternatives showed poor performance (κ ≤ 0.45)
- Entity type definitions for ORG and LOC remain incompletely specified, requiring manual QID curation from Wikidata
- Lacks ablation experiments showing the marginal contribution of each annotation refinement stage beyond overall dataset quality

## Confidence
- **High Confidence:** Dataset size claim (28,866 sentences, 5x larger than baseline) and basic distant supervision pipeline mechanics are well-supported by verifiable implementation details.
- **Medium Confidence:** LLM-as-Judge effectiveness claims are credible given κ=0.62 vs human κ=0.66, but depend on proprietary model access. The downstream model performance improvements are plausible but need variance reporting.
- **Low Confidence:** Cross-domain transfer claims are weakened by the merged GPE/LOC evaluation approach, and the exact impact of individual annotation refinement stages remains unclear without ablation studies.

## Next Checks
1. **Replicate LLM-as-Judge evaluation:** Run GPT-5-mini and Mistral-Medium-3.1 on the 500-sentence human-annotated subset to verify reported κ values of 0.62 and 0.45.
2. **Ablate annotation refinement stages:** Skip LuxGPT-NER and regex filtering to measure their contribution to final dataset quality and size.
3. **Validate ORG/LOC entity type coverage:** Sample unmatched hyperlinks during Wikidata querying to identify missing QIDs and expand the entity type allowlists.