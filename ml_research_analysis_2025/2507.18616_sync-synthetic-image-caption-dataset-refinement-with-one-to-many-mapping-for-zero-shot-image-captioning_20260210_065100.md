---
ver: rpa2
title: 'SynC: Synthetic Image Caption Dataset Refinement with One-to-many Mapping
  for Zero-shot Image Captioning'
arxiv_id: '2507.18616'
source_url: https://arxiv.org/abs/2507.18616
tags:
- image
- sync
- synthetic
- caption
- captioning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SynC addresses the challenge of semantic misalignment in synthetic
  image-caption datasets used for zero-shot image captioning. While text-to-image
  models generate training data, they often produce images that poorly represent their
  captions, introducing noise that hinders model performance.
---

# SynC: Synthetic Image Caption Dataset Refinement with One-to-many Mapping for Zero-shot Image Captioning

## Quick Facts
- arXiv ID: 2507.18616
- Source URL: https://arxiv.org/abs/2507.18616
- Reference count: 40
- Primary result: Achieves state-of-the-art zero-shot image captioning performance with +8.2 CIDEr improvement on MS-COCO

## Executive Summary
SynC addresses the challenge of semantic misalignment in synthetic image-caption datasets used for zero-shot image captioning. While text-to-image models generate training data, they often produce images that poorly represent their captions, introducing noise that hinders model performance. Existing pruning methods, designed for textual noise in web data, are ill-suited for synthetic data where captions are clean but images are misaligned. SynC introduces a novel refinement framework that leverages a one-to-many mapping strategy to reassign captions to the most semantically aligned images already present within the synthetic image pool, achieving significant performance improvements across multiple benchmarks.

## Method Summary
SynC refines synthetic image-caption datasets by leveraging a one-to-many mapping strategy that reassigns captions to semantically aligned images within a pre-generated pool. Instead of conventional filtering or regeneration, SynC uses text-to-image retrieval to find multiple relevant candidate images for each caption, then employs a cycle-consistency-inspired alignment scorer to select the best image by verifying its ability to retrieve the original caption via image-to-text retrieval. The method preserves top τ=0.9 fraction of pairs by alignment score, balancing data quality and coverage while requiring only 2.3 GPU hours to refine the full synthetic dataset.

## Key Results
- Achieves +8.2 CIDEr improvement over baseline on MS-COCO using ViT-B/32
- Demonstrates state-of-the-art performance in cross-domain scenarios
- Consistently improves zero-shot image captioning models across MS-COCO, Flickr30k, and NoCaps benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: One-to-many mapping enables caption-to-image realignment within existing synthetic pools, bypassing expensive regeneration.
- Mechanism: Instead of binding each caption C_i to its originally generated image I^syn_i, SynC uses T2I retrieval to select K candidate images from the pre-generated pool that best match the caption embedding, then scores each candidate.
- Core assumption: The pre-generated image pool contains semantically relevant images for most captions, even if not the originally paired one.
- Evidence anchors:
  - [abstract]: "Instead of conventional filtering or regeneration, SynC focuses on reassigning captions to the most semantically aligned images already present within the synthetic image pool."
  - [Section 3.2]: Eq. (2–3) define S_T2I(C_i) = {I^syn_r}_{r∈R_i} where R_i retrieves top-K via cosine similarity between E_I(I^syn_j) and E_T(C_i).
  - [corpus]: Related work on structured captions and prompt adherence addresses caption quality, not image-caption realignment—no direct corpus support for this specific mechanism.
- Break condition: If the pre-generated pool lacks semantic coverage for caption distribution, retrieval returns low-similarity candidates, and scoring cannot recover alignment.

### Mechanism 2
- Claim: Cycle-consistency-inspired alignment scoring via I2T→text comparison more reliably detects fine-grained misalignment than direct CLIPScore.
- Mechanism: For each candidate image I^syn, perform I2T retrieval to get top-K_r captions from corpus C, then compute max SBERT similarity between retrieved captions and original caption C. This proxies whether I^syn "remembers" C via the reverse retrieval path.
- Core assumption: Unimodal text encoders (SBERT) better capture sentence-level semantics for alignment verification than cross-modal CLIP embeddings, which may prioritize global similarity over compositional fidelity.
- Evidence anchors:
  - [Section 3.3]: Eq. (5–6) define f_ret(I^syn, C) = max_{r∈R̂(I^syn)} ⟨E_S(C_r), E_S(C)⟩ where R̂ uses I2T retrieval with VLM encoders.
  - [Table 8]: f_ret + S_T2I achieves 112.0 CIDEr vs. 108.2 for f_SigLIP2 + S_T2I, confirming scoring function contribution.
  - [corpus]: No corpus papers directly validate cycle-consistency for synthetic dataset refinement; mechanism evidence is internal to this work.
- Break condition: If I2T retrieval fails to surface semantically proximate captions (e.g., rare vocabulary), f_ret underestimates alignment for valid pairs.

### Mechanism 3
- Claim: Upper-tail preservation (τ=0.9) retains most data while filtering lowest-alignment pairs, improving training signal quality.
- Mechanism: After scoring all N pairs, sort by alignment score descending and preserve top τ fraction. Setting τ=0.9 removes worst 10% without excessive data loss.
- Core assumption: Misaligned pairs concentrate in lower score distribution; removing them reduces gradient noise without sacrificing coverage.
- Evidence anchors:
  - [Section 3.3, Eq. (9)]: SynC(D_syn, τ) = D*_syn preserves ⌊N·τ⌋ top-scored pairs.
  - [Figure 4]: τ=0.9 yields best CIDEr (112.0); even τ=0.3 (30% data) outperforms baseline (103.8), indicating quality over quantity.
  - [corpus]: Corpus papers do not address preservation ratio strategies for synthetic caption datasets.
- Break condition: If misalignment is uniformly distributed across scores, thresholding provides limited benefit; if τ too aggressive, removes valid tail-domain data.

## Foundational Learning

- Concept: Cross-modal retrieval (T2I and I2T)
  - Why needed here: SynC relies on bidirectional retrieval—T2I for candidate selection, I2T for alignment scoring. Understanding embedding spaces, similarity metrics, and top-K retrieval is essential.
  - Quick check question: Given a caption "a dog on a skateboard," would T2I retrieval return images where "dog" dominates over "skateboard," and how would I2T retrieval differ?

- Concept: Cycle consistency in vision-language learning
  - Why needed here: f_ret is explicitly "cycle-consistency-inspired"—verifying that an image retrieves its source caption. This concept appears in self-training and unpaired data settings.
  - Quick check question: If I2T retrieval from image I returns captions C' that are semantically similar but not identical to original caption C, should f_ret still score high? Why or why not?

- Concept: Synthetic dataset noise characteristics
  - Why needed here: SynC targets a specific noise pattern—clean captions, noisy images—unlike web-scraped data where text is noisy. Recognizing this distinction guides method selection.
  - Quick check question: For a dataset with well-formed captions but images missing objects, would caption-rewriting methods (e.g., LaCLIP) help? What about image-filtering methods?

## Architecture Onboarding

- Component map: D_syn -> S_T2I (T2I retrieval) -> f_ret (I2T→SBERT) -> Filter (τ=0.9) -> D*_syn
- Critical path:
  1. Embed all synthetic images with E_I (SigLIP2) and all captions with E_T (SigLIP2) and E_S (SBERT) — precompute once
  2. For each caption: retrieve K candidates (T2I), score each via f_ret (I2T→SBERT), select argmax
  3. Sort pairs by alignment score, preserve top τ
- Design tradeoffs:
  - K (candidate count): Higher K improves recall of good matches but increases retrieval cost; paper finds K=15 optimal
  - K_r (retrieved captions for scoring): Higher K_r may introduce noise; K_r=2 balances precision and robustness
  - τ (preservation ratio): Lower τ removes more noise but risks data loss; τ=0.9 empirically best
  - Encoder choice: SigLIP2 outperforms CLIP; ViT-B/16 offers better efficiency than ViT-L/16 with similar gains
- Failure signatures:
  - Low alignment scores across all pairs → corpus coverage issue or encoder mismatch
  - Performance degradation vs. baseline → τ too aggressive or scoring function misconfigured
  - High variance across runs → retrieval instability; check embedding normalization and similarity thresholds
- First 3 experiments:
  1. Replicate baseline comparison: Train PCM-Net on DSynthImgCap vs. SynC-refined version; expect +8 CIDEr on MS-COCO
  2. Ablate selection strategy: Compare S_T2I vs. S_One with same scorer; quantify one-to-many contribution
  3. Ablate scoring function: Compare f_ret vs. f_SigLIP2 with same selection; verify cycle-consistency benefit

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the SynC framework be effectively adapted to improve performance in other vision-and-language downstream tasks, such as visual question answering or segmentation?
- **Basis in paper:** [explicit] The conclusion explicitly states that future work will "investigate extending SynC to other vision-and-language downstream tasks (e.g., segmentation, visual question answering)."
- **Why unresolved:** The current study validates SynC exclusively on Zero-shot Image Captioning (ZIC) benchmarks; its utility for tasks requiring different semantic granularities remains untested.
- **What evidence would resolve it:** Empirical results showing that models trained on SynC-refined synthetic datasets achieve improved mIoU for segmentation or accuracy for VQA compared to baselines.

### Open Question 2
- **Question:** Does SynC yield diminishing returns when applied to datasets generated by state-of-the-art text-to-image models with higher inherent semantic fidelity?
- **Basis in paper:** [explicit] The conclusion notes the intent to "employ advanced image generative models" in future work, while the experiments primarily utilize Stable Diffusion v1.4.
- **Why unresolved:** It is unclear if the "semantic misalignment" problem is as prevalent in newer models (e.g., DALL-E 3, SDXL), or if their improved prompt adherence renders the retrieval-based refinement redundant.
- **What evidence would resolve it:** A comparative analysis of SynC's performance gains when refining images from SD v1.4 versus a newer generative model using the same text corpus.

### Open Question 3
- **Question:** How does SynC perform when the pre-generated image pool lacks sufficient visual diversity to provide a suitable candidate for a specific caption?
- **Basis in paper:** [inferred] SynC relies on a "one-to-many" mapping strategy to retrieve candidates from a fixed pool. The method assumes that a better alignment exists within the pool, but may struggle if the initial generation failed to produce any relevant visual concepts for a prompt.
- **Why unresolved:** The paper evaluates aggregate performance improvements but does not analyze failure cases where the "best" retrieved image still suffers from low alignment due to pool limitations.
- **What evidence would resolve it:** An analysis of alignment score distributions for "hard" captions where top-k retrieved candidates all exhibit low similarity scores, potentially necessitating a regeneration step rather than reassignment.

## Limitations
- Effectiveness depends heavily on semantic coverage of pre-generated image pool
- Limited validation on truly out-of-distribution domains (medical imaging, satellite imagery)
- Computational efficiency claim specific to tested dataset size and hardware configuration

## Confidence

- **High Confidence**: The core mechanism of one-to-many mapping for caption-image realignment is well-supported by ablation studies showing S_T2I consistently outperforms S_One across all benchmarks. The computational efficiency gains are clearly demonstrated.
- **Medium Confidence**: The superiority of f_ret over unimodal scoring functions is supported by internal comparisons, but the theoretical justification for why cycle-consistency better captures fine-grained misalignment than direct CLIPScore comparison remains partially heuristic.
- **Medium Confidence**: The τ=0.9 preservation ratio is empirically optimal for tested datasets, but the sensitivity analysis is limited to a narrow range. The claim that this ratio balances quality and coverage needs broader validation.

## Next Checks
1. **Domain Transfer Test**: Apply SynC to a synthetic dataset from a structurally different domain (e.g., medical or technical imagery) and evaluate whether the same τ=0.9 and K=15 parameters remain optimal, or if domain-specific tuning is required.

2. **Error Analysis**: Systematically analyze pairs rejected by SynC (bottom 10%) to determine whether they represent true misalignments or valid but rare compositional structures. This would clarify whether the method introduces bias against certain caption types.

3. **Scalability Benchmark**: Measure retrieval and scoring time as dataset size scales from 100K to 10M pairs, identifying at what scale the current implementation becomes computationally prohibitive and what architectural modifications would be needed.