---
ver: rpa2
title: 'FinerWeb-10BT: Refining Web Data with LLM-Based Line-Level Filtering'
arxiv_id: '2501.07314'
source_url: https://arxiv.org/abs/2501.07314
tags:
- data
- quality
- training
- lines
- filtering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an LLM-based line-level filtering method
  to improve data quality for training large language models (LLMs). Using GPT-4o
  mini to label a 20,000-document sample from FineWeb, the authors create descriptive
  labels for low-quality lines, which are then grouped into nine categories and used
  to train a DeBERTa-v3 classifier.
---

# FinerWeb-10BT: Refining Web Data with LLM-Based Line-Level Filtering

## Quick Facts
- arXiv ID: 2501.07314
- Source URL: https://arxiv.org/abs/2501.07314
- Reference count: 3
- Primary result: LLM-based line-level filtering improves GPT-2 model accuracy on HellaSwag by ~0.1 while reducing training steps by 32% and data by up to 25%

## Executive Summary
This paper introduces an LLM-based line-level filtering method to improve data quality for training large language models. Using GPT-4o mini to label a 20,000-document sample from FineWeb, the authors create descriptive labels for low-quality lines, which are then grouped into nine categories and used to train a DeBERTa-v3 classifier. The filtered data is evaluated by training GPT-2 models, showing that models trained on the filtered dataset achieve higher accuracy on the HellaSwag benchmark and reach their performance targets faster, even with up to 25% less data. The results demonstrate that LLM-based line-level filtering significantly improves data quality and training efficiency.

## Method Summary
The method uses GPT-4o mini to evaluate each line in a 20,000-document sample from FineWeb, labeling lines as "Clean" or with descriptive labels (e.g., "copyright notice," "incomplete sentence"). These labels are grouped into nine main categories, then used to train a DeBERTa-v3-base classifier that predicts quality scores for each line. The classifier is applied to a 10B-token subset of FineWeb, removing lines below a quality threshold while preserving document structure. The resulting dataset is used to train GPT-2 models, which show improved performance on HellaSwag compared to models trained on unfiltered data.

## Key Results
- GPT-2 models trained on filtered data achieve ~0.1 higher accuracy on HellaSwag
- Models reach performance targets 32% faster (approximately 6,000 steps earlier)
- Up to 25% data reduction possible while maintaining or improving performance
- Micro F1 scores of 0.80-0.81 and precision of 0.86-0.88 for the Clean class

## Why This Works (Mechanism)

### Mechanism 1: Data-Driven Label Taxonomy Generation
Allowing an LLM to generate its own descriptive labels for low-quality content captures more nuanced quality issues than predefined heuristic categories. GPT-4o mini evaluates each line and either assigns "Clean" or generates a descriptive label, producing 547 unique labels that are consolidated into nine categories. This data-driven approach captures subtle quality distinctions that fixed heuristics might miss.

### Mechanism 2: Line-Level Granular Filtering with Classifier Scaling
Removing low-quality lines while preserving document structure retains more training signal than document-level filtering. A DeBERTa-v3 classifier trained on labeled lines assigns quality scores to each line in the full dataset, enabling granular removal of noise while keeping complete documents intact. This preserves valuable content that document-level filtering would discard.

### Mechanism 3: Efficiency Gains from Data Quality Concentration
Higher-quality data enables models to reach target performance with fewer training steps and less data. By removing low-quality lines (navigation elements, formatting artifacts, boilerplate), each training batch contains more informative tokens per step. The model learns signal faster because it is not wasting capacity on noise, achieving the same performance in fewer steps.

## Foundational Learning

- **Line-level vs. Document-level Filtering**
  - Why needed here: Understanding granularity determines how much data is preserved. Document-level filtering is efficient but wasteful; line-level preserves more content.
  - Quick check question: If a 100-line document has 5 lines of navigation text, does document-level filtering keep or discard the entire document?

- **Classifier Calibration (Platt Scaling)**
  - Why needed here: Raw classifier probabilities can be overconfident due to class imbalance (86% Clean). Calibration adjusts scores for reliable thresholding.
  - Quick check question: Why might a classifier predict 0.99 probability for a line that is actually low-quality if the training data is 86% positive class?

- **HellaSwag as a Proxy for Pretraining Quality**
  - Why needed here: Evaluating pretraining data quality requires training models to convergence, which is expensive. HellaSwag provides a lightweight downstream signal.
  - Quick check question: What are the limitations of using a single benchmark (commonsense reasoning) to evaluate general-purpose training data quality?

## Architecture Onboarding

- **Component map:** GPT-4o mini labeling module -> o1-preview label aggregation -> DeBERTa-v3 classifier training -> Platt scaling calibration -> quality score inference -> threshold-based filtering engine

- **Critical path:** Labeling quality -> classifier generalization -> calibration accuracy -> threshold selection -> downstream performance

- **Design tradeoffs:**
  - Higher threshold (0.9, -25% data): Slightly better HellaSwag performance but more aggressive data loss; risk of removing useful diversity
  - Lower threshold (0.5, -8% data): More conservative; preserves more data but may retain more noise
  - Batch size (15 lines): Provides context for labeling but increases prompt length and cost

- **Failure signatures:**
  - Classifier over-predicts "Clean" on low-quality lines -> filtered dataset is not meaningfully cleaner
  - Classifier over-flagging specific domains (e.g., technical content as "programming code") -> loss of valuable domain coverage
  - LLM labeler bias (e.g., flagging "shut up" as toxic) -> inherited bias in downstream classifier

- **First 3 experiments:**
  1. Baseline replication: Train GPT-2 on original FineWeb-10BT vs. filtered versions (0.5 and 0.9 thresholds); evaluate on HellaSwag to confirm ~0.1 accuracy improvement and 32% training speedup
  2. Threshold sweep: Test additional thresholds (0.7, 0.8, 0.95) to identify the point where data reduction hurts performance
  3. Ablation on label categories: Remove only specific categories (e.g., just "Navigation & Interface Elements" or just "Programming Code") to measure category-specific impact on downstream performance

## Open Questions the Paper Calls Out

- **Does the efficacy of LLM-based line-level filtering scale to state-of-the-art model architectures and sizes beyond GPT-2 (124M parameters)?**
  - The study only validates the method on a small model, leaving the impact on larger, more complex architectures unknown.

- **Is the observed performance improvement caused by the specific removal of low-quality lines, or simply by the reduction of dataset size?**
  - Without a control group of random data removal, it is unclear if the filtering logic or the data compression provides the benefit.

- **Can this filtering pipeline be effectively adapted for low-resource languages given the performance gap in the labeling LLM?**
  - The labeling LLM (GPT-4o mini) is English-centric; its ability to generate accurate quality labels for low-resource languages is unverified.

## Limitations

- LLM labeler reliability depends entirely on GPT-4o mini's judgment, with manual corrections needed but frequency unspecified
- Single dataset evaluation on FineWeb-10BT without validation on other web corpora or multilingual distributions
- Downstream benchmark scope limited to HellaSwag (commonsense reasoning), potentially missing domain-specific weaknesses

## Confidence

**High Confidence (9/10):** Technical implementation of line-level filtering, classifier training, and calibration is well-specified and reproducible. The 32% training speedup and 0.1 HellaSwag accuracy improvement are directly measurable from reported experiments.

**Medium Confidence (6/10):** Claim that LLM-generated labels capture more nuanced quality issues than predefined heuristics. While the mechanism is sound, there's no quantitative comparison against a heuristic baseline on the same dataset.

**Low Confidence (4/10):** Assertion that removing 25% of lines improves overall model quality. The paper only tests up to 25% removal and does not explore the point where aggressive filtering begins to harm downstream performance.

## Next Checks

1. **Cross-Dataset Generalization Test:** Apply the FinerWeb classifier to a different web corpus (e.g., C4 or RedPajama) and measure classification performance and downstream training efficiency. This validates whether the approach generalizes beyond FineWeb's distribution.

2. **Threshold Sensitivity Analysis:** Systematically test thresholds from 0.3 to 0.95 in 0.05 increments, measuring HellaSwag accuracy and training steps for each. Identify the exact point where data reduction begins to harm performance, and test whether the optimal threshold differs by content domain.

3. **Ablation on Label Categories:** Create filtered datasets that remove only specific categories (e.g., only "Navigation & Interface Elements" vs. only "Programming Code") and train GPT-2 models on each. This isolates which quality issues most impact downstream performance and whether some categories are beneficial to retain.