---
ver: rpa2
title: Few-shot Learning on AMS Circuits and Its Application to Parasitic Capacitance
  Prediction
arxiv_id: '2507.06538'
source_url: https://arxiv.org/abs/2507.06538
tags:
- graph
- link
- node
- prediction
- subgraph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CircuitGPS is a few-shot learning framework for parasitic effect
  prediction in AMS circuits. It represents circuit netlists as heterogeneous graphs
  and uses a small-hop subgraph sampling technique to convert target links or nodes
  into subgraphs.
---

# Few-shot Learning on AMS Circuits and Its Application to Parasitic Capacitance Prediction

## Quick Facts
- **arXiv ID**: 2507.06538
- **Source URL**: https://arxiv.org/abs/2507.06538
- **Reference count**: 31
- **Primary result**: CircuitGPS achieves at least 20% improvement in coupling existence accuracy and at least 0.067 reduction in MAE for capacitance estimation compared to existing methods.

## Executive Summary
CircuitGPS is a few-shot learning framework for parasitic effect prediction in AMS circuits. It represents circuit netlists as heterogeneous graphs and uses small-hop subgraph sampling to convert target links or nodes into subgraphs. The method employs a hybrid graph Transformer with a low-cost positional encoding (DSPD) to learn subgraph embeddings. CircuitGPS is pre-trained on link prediction and fine-tuned on edge regression tasks. The approach demonstrates strong generalization, enabling zero-shot learning on diverse AMS designs.

## Method Summary
CircuitGPS addresses few-shot parasitic effect prediction by converting the problem into graph classification through 1-hop enclosing subgraph sampling around target links. The framework uses a hybrid architecture combining parallel MPNNs (GatedGCN) with global attention mechanisms (Transformer/Performer). DSPD positional encoding provides structural coordinates relative to anchor nodes. The two-stage training strategy first pre-trains on binary link existence prediction, then fine-tunes on capacitance regression. Critical design choices include excluding circuit statistics during pre-training to improve generalization and using heterogeneous graph representations that capture different node types (Nets, Pins, Devices).

## Key Results
- At least 20% improvement in coupling existence accuracy compared to existing methods
- At least 0.067 reduction in MAE for capacitance estimation
- Demonstrates zero-shot learning capability on diverse AMS designs
- Validates predicted capacitance through SPICE simulations with 14.5% mean absolute percentage error

## Why This Works (Mechanism)

### Mechanism 1
Decoupling target links from the global circuit structure via enclosing subgraph sampling enables few-shot generalization across diverse AMS designs. The method extracts local "enclosing subgraphs" around target node pairs, forcing the model to learn generalizable topological motifs rather than memorizing global positions. This works because parasitic effects are primarily determined by local topology within 1-2 hops rather than distant global structures.

### Mechanism 2
The Double-anchor Shortest Path Distance (DSPD) positional encoding provides a portable structural coordinate system that improves link prediction accuracy over global encodings. DSPD encodes node positions relative to two anchor nodes (target link endpoints), creating a consistent input space for the Transformer regardless of global graph size. This is critical for zero-shot transfer because the relative distance to coupling endpoints is more predictive of coupling than absolute graph spectral properties.

### Mechanism 3
A two-stage training process (pre-training on binary link existence, fine-tuning on regression) facilitates convergence and accuracy in low-data regimes. By first learning to predict if a coupling exists (simpler binary task), the model develops robust structural embeddings that can be efficiently adapted to the harder regression task of predicting capacitance magnitude with limited data. This works because structural features that imply link existence are highly correlated with features determining capacitance magnitude.

## Foundational Learning

- **Concept**: Heterogeneous Graphs (HINs)
  - **Why needed here**: AMS circuits contain distinct node types (Nets, Pins, Devices) and edge types that determine how information must be aggregated. Treating different node types identically would lose critical circuit semantics.
  - **Quick check**: Can you explain why treating a "Device" node identically to a "Net" node would lose critical circuit semantics?

- **Concept**: Message Passing Neural Networks (MPNN) vs. Transformers
  - **Why needed here**: The paper uses a "Hybrid" architecture. Understanding the bias of MPNNs (locality) vs. Transformers (global attention) is required to interpret the ablation study.
  - **Quick check**: Does the paper suggest that complex Global Attention is always better than simple local message passing? (Hint: Check Observation 2).

- **Concept**: Inductive vs. Transductive Learning
  - **Why needed here**: The core value proposition is "Zero-shot" learning on unseen designs, requiring an inductive approach rather than transductive learning.
  - **Quick check**: Why does subgraph sampling enable the model to predict links on a circuit design it has never seen during training?

## Architecture Onboarding

- **Component map**: Input (Heterogeneous Circuit Graph) -> Sampler (1-hop Enclosing Subgraph) -> Encoder (DSPD + Node Type Embedding) -> Backbone (Parallel MPNN + Global Attention) -> Head (Task-specific MLP)
- **Critical path**: The Subgraph Sampler and DSPD Encoder are the most brittle components. If sampling hop count is too low or DSPD is misconfigured, the downstream Transformer will receive garbage input regardless of its size.
- **Design tradeoffs**: The paper notes that classic MPNNs can match or outperform Transformers on this data while being cheaper to train. Using detailed original node features during pre-training can hurt generalization by acting as noise.
- **Failure signatures**: High training accuracy but near-random test accuracy on new circuits suggests the model learned layout-specific coordinates rather than structural patterns. Large errors on edge regression likely indicate mismatched normalization of circuit statistics between training and inference datasets.
- **First 3 experiments**:
  1. Train the link predictor on a single small circuit and verify it can achieve near-perfect training accuracy.
  2. Run link prediction with vs. without original node features to verify that removing them improves zero-shot generalization.
  3. Compare a pure MPNN model against the full Hybrid Transformer on the capacitance regression task to validate if the Transformer overhead is justified.

## Open Questions the Paper Calls Out
None

## Limitations
- The reliance on small-hop subgraph sampling may fail when parasitic effects depend on long-range global interactions rather than local topology.
- DSPD positional encoding assumes shortest-path distances are meaningful features, which may not hold for weighted or non-Euclidean circuit representations.
- The two-stage training strategy's effectiveness depends critically on the similarity between link existence distribution and high-capacitance link distribution, with potential negative transfer if these differ significantly.

## Confidence

| Claim | Confidence |
|-------|------------|
| CircuitGPS achieves at least 20% improvement in coupling existence accuracy | High |
| CircuitGPS achieves at least 0.067 reduction in MAE for capacitance estimation | High |
| The method enables zero-shot learning on diverse AMS designs | Medium |
| DSPD positional encoding is superior to other encodings | Medium |

## Next Checks

1. Test CircuitGPS on circuit designs where parasitic effects are known to depend on long-range global routing (e.g., clock distribution networks) to validate whether local subgraph sampling remains effective.

2. Perform an ablation study comparing DSPD encoding against Laplacian PE and Random Walk SE on a held-out test set with varying circuit topologies to quantify the positional encoding advantage.

3. Evaluate the two-stage training strategy on datasets where the distribution of existing links differs significantly from high-capacitance links to measure potential negative transfer effects.