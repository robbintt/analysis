---
ver: rpa2
title: 'SpaRC-AD: A Baseline for Radar-Camera Fusion in End-to-End Autonomous Driving'
arxiv_id: '2508.10567'
source_url: https://arxiv.org/abs/2508.10567
tags:
- radar
- driving
- fusion
- autonomous
- end-to-end
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SpaRC-AD addresses the challenge of radar integration in end-to-end
  autonomous driving, which remains unexplored despite radar's advantages in adverse
  weather, long-range detection, and precise velocity estimation. The core method
  extends sparse representation paradigms to planning-oriented autonomous driving
  through query-based camera-radar fusion.
---

# SpaRC-AD: A Baseline for Radar-Camera Fusion in End-to-End Autonomous Driving

## Quick Facts
- **arXiv ID:** 2508.10567
- **Source URL:** https://arxiv.org/abs/2508.10567
- **Reference count:** 40
- **Primary result:** SpaRC-AD achieves 4.8% mAP improvement in 3D detection and 8.3% AMOTA improvement in multi-object tracking through radar-camera fusion

## Executive Summary
SpaRC-AD addresses the challenge of integrating radar sensing into end-to-end autonomous driving systems. The framework extends sparse representation paradigms to planning-oriented driving through query-based camera-radar fusion, processing multi-view images and multi-sweep radar point clouds. By leveraging radar's advantages in adverse weather, long-range detection, and precise velocity estimation, SpaRC-AD demonstrates significant improvements over vision-only baselines across multiple perception, prediction, and planning tasks.

## Method Summary
SpaRC-AD processes multi-view images and multi-sweep radar point clouds using sparse feature encoding, unified sparse fusion with adaptive radar aggregation, and parallel motion planning for trajectory generation. The method employs a range-adaptive attention mechanism that penalizes attention scores based on the Euclidean distance between query positions and radar key points, forcing the model to prioritize radar features physically close to objects. The framework uses a two-stage training pipeline: first optimizing detection losses, then end-to-end fine-tuning including planning and motion forecasting.

## Key Results
- 3D detection: +4.8% mAP improvement over vision-only baselines
- Multi-object tracking: +8.3% AMOTA improvement
- Online mapping: +1.8% mAP improvement
- Motion prediction: -4.0% mADE improvement
- Trajectory planning: -0.1m L2 error and -9% TPC reduction

## Why This Works (Mechanism)

### Mechanism 1: Range-Adaptive Spatial Attention
The architecture implements a range-adaptive aggregation layer that penalizes attention scores based on Euclidean distance between query positions and radar key points. This forces the model to prioritize radar features physically close to the object being queried. Core assumption: radar points maintain sufficient spatial correlation with object positions to serve as reliable geometric priors for vision features.

### Mechanism 2: Doppler-Enhanced Motion Modeling
The model encodes Doppler velocity directly into radar point features, which are aggregated into detection queries and propagated to the motion planner. This provides immediate kinematic state estimates rather than inferring velocity from positional changes over time. Core assumption: Doppler velocity corresponds accurately to object's radial velocity relative to ego-vehicle.

### Mechanism 3: Unified Sparse Query Optimization
The model uses a Sparse Fusion Transformer that processes multi-modal inputs into sparse instance queries, iteratively refining planning trajectory without intermediate dense rasterization. Core assumption: sparse query set is sufficient to capture critical driving scene elements without losing fine-grained spatial context.

## Foundational Learning

- **Sparse Representation (Point Clouds & Queries):** Understanding how transformers handle sets of objects (set-to-set interactions) rather than 2D grids. *Quick check:* Can you explain how a transformer attends to a sparse set of 3D points without a fixed grid structure?
- **Radar Signal Processing (RCS & Doppler):** Understanding radar's sparsity, noise, and measurement of radial velocity. *Quick check:* If a car crosses perpendicular to ego-vehicle, what would its Doppler velocity reading approximate?
- **Imitation Learning & End-to-End Planning:** Grasping behavior cloning basics—learning "what to do" from data rather than explicit rules. *Quick check:* In end-to-end frameworks, if perception fails to detect an object, can the planner explicitly "know" it missed something?

## Architecture Onboarding

- **Component map:** Multi-view Images (ResNet Backbone) -> Multi-sweep Radar (Point Transformer) -> Sparse Frustum Fusion -> Sparse Fusion Decoder (Range-adaptive attention) -> Detection/Tracking Head, Online Mapping Head, Motion Prediction Head -> Parallel Motion Planner
- **Critical path:** The Sparse Fusion Decoder with Range Adaptive Radar Aggregation (Equation 1) is the core. Performance gains hinge on this attention mechanism's ability to associate radar points with correct visual queries.
- **Design tradeoffs:** Sparse vs. Dense (efficiency vs. context), Radar-only vs. Fusion (geometry/velocity vs. semantics)
- **Failure signatures:** Ghost Objects (multipath reflections), Velocity Drift (occluded radar returns), Sparse Map Errors (missing lane dividers)
- **First 3 experiments:**
  1. Proximity Ablation: Run with α=0 to quantify spatial proximity's specific value
  2. Velocity Input Masking: Zero out Doppler channel to isolate physical velocity vs. learned visual velocity
  3. Weather Robustness: Evaluate breakdown in "Night" and "Rain" splits to see if collision rate actually drops

## Open Questions the Paper Calls Out
- Can extending perception range beyond 50m to utilize radar's 150m+ capabilities further improve long-horizon trajectory prediction and planning safety?
- How does utilizing raw radar tensor representations compare to the current pre-processed sparse point cloud approach in end-to-end driving tasks?
- To what extent do closed-loop results in Bench2Drive simulation transfer to real-world environments given simulator limitations?

## Limitations
- Performance gains rely heavily on undisclosed hyperparameters for the range-adaptive attention mechanism
- Sparse query representation may fail to capture complex background context crucial for safe planning
- Limited evaluation of radar's distinct advantage in long-range detection due to 50m perception range constraint

## Confidence
- **High Confidence:** Doppler velocity for direct motion estimation (well-supported by experimental velocity error reduction)
- **Medium Confidence:** Range-Adaptive Spatial Attention (promising but limited by missing implementation details)
- **Medium Confidence:** Unified sparse query optimization (demonstrates efficiency but tradeoff with BEV context not fully characterized)

## Next Checks
1. Conduct proximity ablation study systematically varying α from 0 to full range
2. Perform detailed breakdown of performance in "Night" and "Rain" weather scenarios
3. Design scenarios with significant perpendicular motion to evaluate tangential motion handling