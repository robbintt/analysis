---
ver: rpa2
title: 'Breaking SafetyCore: Exploring the Risks of On-Device AI Deployment'
arxiv_id: '2509.06371'
source_url: https://arxiv.org/abs/2509.06371
tags:
- safetycore
- inputs
- loss
- input
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates how on-device AI models, exemplified by
  Google's SafetyCore Android service, can be extracted and manipulated to bypass
  security protections. The authors reverse-engineered SafetyCore's AI model using
  static and dynamic analysis techniques, converted the extracted model to a usable
  format, and successfully generated adversarial examples that cause the model to
  misclassify sensitive content.
---

# Breaking SafetyCore: Exploring the Risks of On-Device AI Deployment

## Quick Facts
- arXiv ID: 2509.06371
- Source URL: https://arxiv.org/abs/2509.06371
- Reference count: 30
- Key outcome: Demonstrates extraction and adversarial attack on Google's SafetyCore Android service, bypassing content filtering through model theft and gradient-based perturbations

## Executive Summary
This paper demonstrates how on-device AI models, exemplified by Google's SafetyCore Android service, can be extracted and manipulated to bypass security protections. The authors reverse-engineered SafetyCore's AI model using static and dynamic analysis techniques, converted the extracted model to a usable format, and successfully generated adversarial examples that cause the model to misclassify sensitive content. Using a Projected Gradient Descent attack, they showed that images could be perturbed to either trigger false positives (enabling unwanted blurring) or false negatives (bypassing blurring protection). The attack required less than 30 lines of code and worked across all devices using the same model, effectively rendering SafetyCore's protection mechanism ineffective.

## Method Summary
The researchers extracted SafetyCore's TFLite model from Android device storage by identifying the TFL3 magic value signature, then converted it to a PyTorch-compatible format using the REOM tool to enable gradient computation. They applied Projected Gradient Descent (PGD) attacks on the dequantized proxy model using binary cross-entropy loss to generate adversarial perturbations. The attack pipeline involved static extraction of the model file, format conversion with dequantization, and iterative gradient-based image manipulation to create perturbations that either triggered false positives or bypassed content filtering, with success measured by the model's misclassification of sensitive content.

## Key Results
- Successfully extracted SafetyCore TFLite model from device storage using static analysis
- Generated adversarial examples that bypass content filtering with less than 30 lines of code
- Attack worked across all devices using the same model version, demonstrating universal vulnerability

## Why This Works (Mechanism)

### Mechanism 1: Model Serialization Extraction
On-device AI models stored as serialized files (TFLite format) can be extracted via static analysis when they retain standard file signatures like the "TFL3" magic value. The model logic is separated from the inference engine, allowing isolation and copying if found in storage. This works because AI models are data files describing operation graphs rather than compiled binary code, making them easier to steal than traditional software.

### Mechanism 2: Quantized Model Conversion
Quantized model parameters (int8) can be converted into differentiable proxy models (float32) to enable gradient-based attacks. By extracting quantization parameters (scale and zero point) from the model file, attackers can mathematically reconstruct an approximation of the original floating-point weights, allowing gradient calculation required for advanced adversarial attacks.

### Mechanism 3: Projected Gradient Descent Attacks
PGD can generate adversarial perturbations that bypass content filters by iteratively maximizing the model's loss function. The attack exploits the correlation-based nature of neural networks by inferring the loss function from output layer activations, then computing gradients to adjust pixels in directions that maximize loss, creating imperceptible perturbations that cause misclassification.

## Foundational Learning

- **Concept: Model Serialization vs. Compilation**
  - *Why needed here:* To understand why AI models are easier to reverse-engineer than traditional software
  - *Quick check:* Why does the separation of the model file (e.g., `.tflite`) from the inference engine make the model easier to steal than a compiled C++ binary?

- **Concept: Quantization and Precision**
  - *Why needed here:* To grasp how models shrink to fit on devices and why this doesn't protect them
  - *Quick check:* If a model uses 8-bit integers for weights, does this prevent an attacker from calculating gradients for an attack? (Hint: Consider the "proxy model")

- **Concept: Gradients and the Loss Function**
  - *Why needed here:* To understand *how* the image is manipulated
  - *Quick check:* In a classification task, does the attacker want to minimize or maximize the loss function to force a misclassification?

## Architecture Onboarding

- **Component map:** Android device storage -> SafetyCore TFLite model file -> LiteRT/TensorFlow Lite interpreter -> Attack pipeline (extraction → conversion → dequantization → gradient computation → image perturbation)
- **Critical path:** Extraction of the TFL3 file is the initial dependency; without the model file, the attack cannot start
- **Design tradeoffs:** Standardization vs. Security (standard inference engines optimize performance but provide known parsing tools), On-Device vs. Cloud (enhances privacy but grants attacker physical access)
- **Failure signatures:** Extraction failure (no TFL3 signature implies encryption/remote loading), conversion failure (proxy model outputs NaN implies quantization parameters incorrect), attack failure (visible distortion yet still detected implies epsilon constraint too loose)
- **First 3 experiments:** 1) Scan target directory for files with FlatBuffers or AI magic values, 2) Load extracted model into visualization tool to identify input/output layers, 3) Convert single layer to float32 and test backpropagation for gradient flow

## Open Questions the Paper Calls Out
- Can the methodology be adapted for on-device models processing text or audio data modalities?
- Is it feasible to perform model inversion attacks on multi-label classification models with high intra-class diversity?
- Do adversarial examples generated against SafetyCore transfer effectively to future model updates or different architectures?

## Limitations
- Attack relies on model being stored as recognizable serialized file in accessible device storage
- Effectiveness may degrade due to JPEG compression removing subtle adversarial noise patterns
- Generalizability across different model architectures and tasks is not fully explored

## Confidence
- **High Confidence:** Core extraction mechanism using static analysis to locate TFLite files by magic values
- **Medium Confidence:** Dequantization process and proxy model construction depends on quantization metadata availability
- **Low Confidence:** Universal applicability claim across all devices assumes identical model versions and quantization parameters

## Next Checks
1. Extract SafetyCore model from multiple Android devices running different OS versions to verify consistent attack success
2. Systematically test adversarial examples through various compression pipelines to quantify failure rates
3. Implement and test common defense mechanisms to evaluate potential hardening options for SafetyCore