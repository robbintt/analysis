---
ver: rpa2
title: Actionable and diverse counterfactual explanations incorporating domain knowledge
  and causal constraints
arxiv_id: '2511.20236'
source_url: https://arxiv.org/abs/2511.20236
tags:
- counterfactual
- counterfactuals
- plausibility
- page
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of generating counterfactual
  explanations that are both actionable and domain-relevant by incorporating feature
  dependencies and causal constraints. The DANCE method learns linear and nonlinear
  relationships from data or integrates expert-provided dependency graphs, ensuring
  plausibility and real-world feasibility of counterfactuals.
---

# Actionable and diverse counterfactual explanations incorporating domain knowledge and causal constraints

## Quick Facts
- **arXiv ID:** 2511.20236
- **Source URL:** https://arxiv.org/abs/2511.20236
- **Reference count:** 40
- **Primary result:** DANCE method consistently outperforms state-of-the-art approaches across 140 public datasets by incorporating domain knowledge and causal constraints into counterfactual explanations

## Executive Summary
This paper presents DANCE, a novel method for generating counterfactual explanations that are both actionable and domain-relevant. DANCE addresses the challenge of producing realistic counterfactuals by incorporating feature dependencies and causal constraints through directed acyclic graphs (DAGs). The method learns or integrates expert-provided dependency graphs and optimizes a custom loss function using Tree-structured Parzen Estimator (TPE) to balance plausibility, diversity, and sparsity. Extensive evaluation on 140 public datasets demonstrates consistent outperformance across key metrics compared to existing state-of-the-art methods.

## Method Summary
DANCE generates counterfactual explanations by first learning or incorporating a directed acyclic graph (DAG) that represents feature relationships and dependencies. The method then uses a Tree-structured Parzen Estimator (TPE) to optimize a multi-objective loss function that balances plausibility (ensuring consistency with the DAG constraints), diversity (generating varied explanations), sparsity (minimizing feature changes), and proximity (staying close to the original instance). The system can learn linear and nonlinear relationships from data using algorithms like DirectLiNGAM or NOTEARS, or integrate expert-provided dependency graphs. During optimization, changes to features propagate through the DAG to maintain consistency with learned relationships.

## Key Results
- Consistent outperformance across 140 public datasets on key counterfactual metrics
- Achieves superior balance between plausibility, diversity, and sparsity compared to state-of-the-art methods
- Demonstrates effectiveness on both linear and nonlinear dependency structures
- Shows robustness across diverse tabular datasets from OpenML

## Why This Works (Mechanism)

### Mechanism 1: Graph-Constrained Plausibility Enforcement
The system structures feature dependencies as a directed acyclic graph (DAG), allowing counterfactual explanations to respect predefined relationships and improve real-world feasibility. During search, a plausibility loss component penalizes candidates that deviate from values predicted by their parent nodes in the graph, as defined by learned weights or conditional probability tables.

### Mechanism 2: Multi-Objective Optimization via Tree-structured Parzen Estimator (TPE)
A custom loss function balancing plausibility, diversity, and sparsity is effectively optimized using TPE, a Bayesian optimization method suitable for non-convex and structured search spaces. The TPE algorithm searches for counterfactual candidates that minimize this combined loss, navigating trade-offs between competing objectives.

### Mechanism 3: Cascading Feature Modification via Topological Sort
Changes propagate through the DAG using topological order, ensuring modifications to upstream features correctly influence downstream dependent features. This maintains consistency with the learned causal structure by calculating implied changes for all descendants when a feature is modified.

## Foundational Learning

- **Concept: Counterfactual Explanations**
  - Why needed here: These are the core output of the DANCE system - minimal changes to input features that alter a model's prediction
  - Quick check question: If a model predicts a loan will be denied, what would a counterfactual explanation suggest?

- **Concept: Directed Acyclic Graphs (DAGs) for Causal Modeling**
  - Why needed here: DANCE relies on DAGs to represent relationships between features and enforce plausibility constraints on counterfactuals
  - Quick check question: How does a DAG represent the relationship "an increase in temperature causes an increase in ice cream sales"?

- **Concept: Bayesian Optimization & Tree-structured Parzen Estimator (TPE)**
  - Why needed here: The paper uses TPE, a specific type of Bayesian optimization, to find the best counterfactual candidates by minimizing its custom loss function
  - Quick check question: Why would you choose a non-gradient-based optimizer like TPE for a problem with a non-convex loss function component like sparsity?

## Architecture Onboarding

- **Component map:** Relationship Learner -> Candidate Generator -> Counterfactual Optimizer
- **Critical path:** Input Data + Expert Rules -> Relationship Graph (DAG) -> Plausibility Loss -> Optimization Loop -> Final Counterfactual
- **Design tradeoffs:** Enforcing strong plausibility constraints via the DAG reduces the search space, potentially leading to less diverse or less sparse counterfactuals
- **Failure signatures:**
  - Unrealistic Counterfactuals: Check if DAG was learned correctly or if plausibility weight is too low
  - Overly Sparse or Diverse Counterfactuals: Check relative weights of loss components
  - Optimization Failure: Could be due to overly constrained search space or poor hyperparameter tuning
- **First 3 experiments:**
  1. Ablation Study on Plausibility: Run DANCE with plausibility component turned on and off, compare results on plausibility, diversity, and sparsity
  2. Sensitivity to Graph Quality: Provide corrupted or incomplete DAG and evaluate counterfactual quality
  3. Weight Tuning on a Single Domain: Systematically vary loss function weights on Wine dataset to observe tradeoffs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the inherent trade-off between plausibility and the reduction in diversity and sparsity be mitigated?
- Basis in paper: Section 5.2 states that the plausibility component "visibly negatively impacted" diversity and sparsity because the relationship graph constrains the explorable feature space
- Why unresolved: The paper identifies this as a consequence of the constrained search space but does not propose a mechanism to decouple these competing objectives
- What evidence would resolve it: A modification to the loss function or search algorithm that maintains high plausibility scores without statistically significant degradation in diversity

### Open Question 2
- Question: How robust is the method to errors in the automatically learned or expert-provided relationship graph?
- Basis in paper: The method relies on DirectLiNGAM or NOTEARS to discover a DAG, assuming the resulting constraints accurately reflect reality
- Why unresolved: The evaluation assumes the graph structure is correct; the sensitivity of counterfactual quality to structural errors is not quantified
- What evidence would resolve it: An analysis of DANCE performance when the input graph is systematically perturbed or contains varying levels of structural noise

### Open Question 3
- Question: How does the required discretization of continuous variables impact the accuracy of the conditional probability estimation?
- Basis in paper: Section 4.1 notes that continuous variables are first discretized for conditional dependencies
- Why unresolved: Discretization is treated as a necessary preprocessing step, but the potential loss of information is not evaluated
- What evidence would resolve it: A comparative evaluation using current discretized approach versus continuous density estimation method

## Limitations

- **DAG Dependency:** The quality of generated counterfactuals directly depends on whether the learned or expert-provided graph correctly captures true feature dependencies
- **Acyclic Constraint:** The method currently supports only acyclic relationships, potentially missing important cyclic dependencies in real-world domains
- **Discretization Impact:** The required discretization of continuous variables for conditional probability estimation may lead to loss of information or precision

## Confidence

- **High Confidence:** The multi-objective optimization framework using TPE is well-established in the literature and the mathematical formulation of the loss function is clearly specified
- **Medium Confidence:** The empirical evaluation shows consistent outperformance across 140 datasets, but specific hyperparameter settings are not fully disclosed
- **Low Confidence:** The claim that DANCE produces "actionable" counterfactuals relies heavily on the assumption that the provided DAG accurately represents real-world constraints

## Next Checks

1. **DAG Sensitivity Analysis:** Systematically vary the quality of the dependency graph (e.g., by randomly removing edges) and measure the impact on counterfactual plausibility and validity across multiple datasets
2. **Cross-Dataset Weight Transferability:** Train DANCE with optimal weights on one dataset and evaluate performance on unseen datasets to assess whether the method generalizes or requires per-dataset tuning
3. **Human Evaluation of Actionability:** Conduct user studies where domain experts rate the practicality and real-world feasibility of DANCE-generated counterfactuals versus baseline methods on a representative sample of explanations