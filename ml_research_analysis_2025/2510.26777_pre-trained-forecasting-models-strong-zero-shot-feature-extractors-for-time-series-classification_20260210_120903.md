---
ver: rpa2
title: 'Pre-trained Forecasting Models: Strong Zero-Shot Feature Extractors for Time
  Series Classification'
arxiv_id: '2510.26777'
source_url: https://arxiv.org/abs/2510.26777
tags:
- accuracy
- average
- mean
- overall
- last
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether pre-trained time series forecasting
  models can serve as effective zero-shot feature extractors for time series classification.
  The authors evaluate diverse forecasting models as frozen feature extractors on
  standard classification benchmarks, analyzing representation extraction strategies
  and proposing two model-agnostic embedding augmentations that incorporate absolute
  statistics and differencing.
---

# Pre-trained Forecasting Models: Strong Zero-Shot Feature Extractors for Time Series Classification

## Quick Facts
- arXiv ID: 2510.26777
- Source URL: https://arxiv.org/abs/2510.26777
- Authors: Andreas Auer; Daniel Klotz; Sebastinan Böck; Sepp Hochreiter
- Reference count: 40
- Primary result: Top forecasting models achieve zero-shot classification accuracy on par with or exceeding state-of-the-art task-specific models

## Executive Summary
This paper investigates whether pre-trained time series forecasting models can serve as effective zero-shot feature extractors for time series classification. The authors evaluate diverse forecasting models as frozen feature extractors on standard classification benchmarks, analyzing representation extraction strategies and proposing two model-agnostic embedding augmentations that incorporate absolute statistics and differencing. Results show that top-performing forecasting models achieve classification accuracy on par with or exceeding state-of-the-art classification-specific models, even without fine-tuning. A positive correlation is observed between forecasting and classification performance, suggesting that forecasting objectives may provide a powerful route toward general-purpose time series foundation models. The findings challenge the assumption that task-specific pre-training is necessary for optimal performance.

## Method Summary
The method extracts embeddings from frozen pre-trained forecasting models (TiRex, Chronos, TimesFM, Moirai) by passing time series through the model and collecting hidden states from all layers, then applying mean pooling across the time dimension and concatenating layer representations. For multivariate data, each variate is processed independently and embeddings are concatenated. Two augmentation strategies are proposed: absolute sample statistics (mean, std, min, max computed over 8 patches) and first-order differencing. A Random Forest classifier (n_estimators=300) is trained on the final embedding vector. The approach is evaluated on UCR (127 univariate) and UEA (30 multivariate) archives with datasets filtered to max length 2048.

## Key Results
- Top forecasting models (TiRex, TimesFM) achieve classification accuracy matching or exceeding state-of-the-art classification-specific models without fine-tuning
- A positive correlation exists between forecasting performance (CRPS) and classification accuracy across models
- Proposed augmentation strategies (absolute statistics and differencing) consistently improve classification performance by recovering signal lost during instance normalization

## Why This Works (Mechanism)

### Mechanism 1
Forecasting pre-training objectives induce representations that transfer effectively to classification tasks because predicting future time steps requires learning robust temporal dynamics, trends, and state transitions that overlap with discriminative features needed for classification.

### Mechanism 2
Multi-layer aggregation preserves hierarchical features lost when using only the final layer, as deeper layers specialize to the pre-training objective while earlier layers retain generic feature detectors useful for classification.

### Mechanism 3
Absolute statistics and differencing augmentations recover signal lost during instance normalization, as forecasting models normalize inputs to predict relative changes, stripping absolute scale and baseline information that classification tasks often rely on.

## Foundational Learning

- **Frozen Feature Extraction (Zero-Shot)**: Critical to understand that the pre-trained model remains frozen while only the classifier is trained; updating forecasting model weights defeats the zero-shot evaluation premise.
- **Instance Normalization (RevIN)**: Forecasting models normalize inputs (mean 0, std 1) to handle non-stationarity; understanding this explains why absolute statistics augmentation is necessary.
- **Channel Independence (Univariate Strategy)**: Forecasting models trained on univariate data are applied to multivariate data by processing each channel independently and concatenating results.

## Architecture Onboarding

- **Component map**: Time Series -> Frozen Forecasting Model -> All Layer Hidden States -> Mean Pooling + Layer Concatenation -> Absolute Stats Augmentation + Differencing -> Random Forest Classifier
- **Critical path**: Input Time Series (T x V) → Loop over V variates → Pass through Frozen Backbone → Extract all decoder/encoder block hidden states → Mean Pool across time for each layer → Concatenate layer vectors → Compute Absolute Stats and Diff-series embeddings → Concatenate all → Train Random Forest
- **Design tradeoffs**: Concatenation preserves specific channel info but creates high-dimensional embeddings; Random Forest captures non-linear separability better than Linear/KNN but may overfit on small datasets
- **Failure signatures**: Loss of absolute scale causes failure on amplitude-defined classes (100% accuracy on synthetic baseline test); memory overflow on high-variate data; stationarity mismatch requires differencing augmentation
- **First 3 experiments**: 1) Baseline extraction with "Last Layer + Last Token" vs "Mean Pool + Layer Concat" validation; 2) Augmentation ablation (No Aug, Stats Only, Stats+Diff) to test data reliance on absolute scale; 3) Classifier sensitivity test between Random Forest and Linear Probe to assess embedding richness

## Open Questions the Paper Calls Out

- **Question**: Do the representations learned by pre-trained forecasting models transfer effectively to unsupervised tasks such as anomaly detection?
- **Question**: What is the performance gap between zero-shot feature extraction and end-to-end fine-tuning for these forecasting models?
- **Question**: Why do some high-performing forecasting models (e.g., Chronos) underperform in classification relative to their forecasting accuracy?

## Limitations

- The study exclusively evaluates supervised Time Series Classification, leaving transfer to other tasks like anomaly detection unexplored
- Zero-shot evaluation does not reveal whether fine-tuning could provide significant performance improvements over the frozen approach
- The positive correlation between forecasting and classification performance shows considerable noise, with some high-forecasting models underperforming in classification

## Confidence

- **High Confidence**: Core empirical finding that top forecasting models match or exceed classification-specific models on UCR/UEA benchmarks
- **Medium Confidence**: Proposed augmentation strategies show consistent improvements but necessity may vary by dataset characteristics
- **Medium Confidence**: Positive correlation between forecasting and classification performance is observed but based on limited models and datasets

## Next Checks

1. Evaluate the same pipeline on non-standard time series datasets (medical, financial, or sensor data) to test robustness beyond UCR/UEA benchmarks
2. Compare zero-shot feature extraction against fine-tuning the forecasting model on classification tasks to quantify the trade-off between efficiency and performance
3. Systematically test whether absolute statistics and differencing augmentations remain beneficial for datasets with different stationarity properties or amplitude-based class boundaries