---
ver: rpa2
title: 'Subtract the Corruption: Training-Data-Free Corrective Machine Unlearning
  using Task Arithmetic'
arxiv_id: '2511.18660'
source_url: https://arxiv.org/abs/2511.18660
tags:
- proxy
- corruption
- training
- task
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses corrective machine unlearning (CMU) in a source-free
  setting, where original training data is unavailable and only a proxy dataset of
  corrupted samples is accessible. Existing CMU methods typically rely on identified
  corrupted training samples, making them ineffective or inapplicable here.
---

# Subtract the Corruption: Training-Data-Free Corrective Machine Unlearning using Task Arithmetic

## Quick Facts
- arXiv ID: 2511.18660
- Source URL: https://arxiv.org/abs/2511.18660
- Reference count: 40
- The paper proposes CUTS, a task arithmetic-based method for source-free corrective machine unlearning that recovers large fractions of utility under label noise and nearly eliminates backdoor attacks without requiring clean training data.

## Executive Summary
The paper addresses corrective machine unlearning in source-free settings where original training data is unavailable and only a proxy dataset with corruption is accessible. Existing CMU methods fail in this setting because they rely on identified corrupted training samples. The authors propose Corrective Unlearning in Task Space (CUTS), which treats clean and corruption signals as distinct tasks and removes corruption by subtracting a calibrated multiple of the corruption task vector from model weights. Across multiple architectures and corruption types, CUTS recovers substantial utility under label noise and eliminates backdoor triggers with minimal performance degradation, outperforming state-of-the-art specialized CMU methods in the source-free setting.

## Method Summary
CUTS operates by fine-tuning a corrupted model on a proxy dataset containing only corrupted samples, extracting the resulting weight changes as a proxy task vector representing corruption, then subtracting a calibrated multiple of this vector from the original corrupted weights. The scaling coefficient α is determined using self-agreement metrics for label noise or attack success rate suppression for backdoors, all evaluated on the proxy dataset without requiring clean validation data. The method handles symmetric/asymmetric label noise and backdoor triggers by treating each corruption type as a distinct task in weight space, leveraging pre-trained weight disentanglement to isolate and remove corruption effects while preserving clean task performance.

## Key Results
- CUTS achieves 69.4% average recovery rate on CLIP under 40% symmetric label noise, outperforming baselines
- For backdoor triggers, CUTS reduces ASR from 100% to <1% while maintaining utility above 95%
- On Clothing1M with unknown noise, CUTS achieves 71.7% accuracy, outperforming SAP (68.9%) and MCU (70.3%)
- Pre-trained models show substantially higher recovery rates than randomly initialized networks, confirming the importance of weight disentanglement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Corruption learned during training can be approximated as a distinct task vector that, when subtracted from model weights, reduces corruption effects while preserving clean task performance.
- Mechanism: Training on mixed clean/corrupted data yields a weight vector τ_mix ≈ τ_clean + τ_corruption. By fine-tuning on a proxy dataset containing only corrupted samples, the model moves further along the corruption direction, producing τ_proxy. Subtracting a scaled version of (τ_proxy - τ_mix) from the original weights approximates removing the corruption component.
- Core assumption: Clean and corruption task vectors are approximately linearly additive and directionally distinct in weight space (weight disentanglement holds).
- Evidence anchors:
  - [abstract] "CUTS treats the clean and the corruption signal as distinct tasks... compute the difference between the corrupted and fine-tuned weights as a proxy task vector, and subtract a calibrated multiple of this vector to cancel the corruption."
  - [Section 4] "τ_mix can be decomposed to the summation of two directionally distinct task vectors τ_s and τ_c associated with T_s and T_c."
  - [corpus] MCU paper notes task arithmetic methods "suffer from weight entanglement" — supporting that disentanglement is a key enabling condition, not guaranteed.
- Break condition: Pre-trained priors are weak or absent; randomly initialized networks show degraded recovery rates (Fig. 4), suggesting disentanglement fails without strong pre-training.

### Mechanism 2
- Claim: Fine-tuning a corrupted model on corrupted proxy samples amplifies corruption-specific weight updates more than clean-signal updates, making the resulting task vector a usable proxy for the corruption direction.
- Mechanism: The corrupted model θ_mix already encodes corruption patterns. Further training on D_proxy (containing only corrupted samples) reinforces these patterns (e.g., backdoor trigger associations, noisy label mappings), yielding weight changes dominated by corruption rather than clean signal.
- Core assumption: The corruption pattern in the proxy dataset is sufficiently similar to the original corruption to activate the same weight-space direction.
- Evidence anchors:
  - [Section 4] "This fine-tuning amplifies the corruption patterns already embedded in θ_mix, producing a model f(·; θ_proxy) that more distinctly reflects corruption-induced behavior."
  - [Section 5.2, PCA trajectory] For poison triggers, "triggered inputs are mapped to a compact cluster... As α increases, this triggered cluster dissolves and the samples migrate into their true class clusters."
  - [corpus] Related work on test-time adaptation (Panda) uses augmentation at inference time but operates differently — no direct corpus support for proxy amplification mechanism.
- Break condition: Corruption lacks regularity (e.g., fully random per-sample label noise with no shared structure); proxy samples may not activate consistent weight directions.

### Mechanism 3
- Claim: The optimal scaling coefficient α* can be estimated using only the proxy dataset via self-agreement metrics (for label noise) or attack success rate suppression (for backdoors), without requiring clean validation data.
- Mechanism: For label noise, kNN self-agreement on penultimate features measures cluster separation — higher agreement indicates better class structure recovery. For backdoors, ASR on triggered proxy samples directly measures residual vulnerability. These proxy-evaluated metrics guide α selection.
- Core assumption: Improved self-agreement or reduced ASR on proxy samples correlates with genuine correction on clean test data.
- Evidence anchors:
  - [Section 4, Eq. 10-12] Explicit formulations for α* estimation using ASR (poison) and SA(α) with coverage penalty (label noise).
  - [Section 5.1] CUTS achieves 69.4% average recovery rate on CLIP under label noise and 90.3 PUS for poison triggers, outperforming baselines.
  - [corpus] No direct corpus evidence for this specific α estimation technique; appears novel to this work.
- Break condition: Proxy dataset is too small or unrepresentative; coverage penalty may reject valid corrections if classes are missing from proxy.

## Foundational Learning

- Concept: Task Arithmetic
  - Why needed here: The entire CUTS method relies on treating model weight differences as task vectors that can be added/subtracted to compose or remove behaviors.
  - Quick check question: Given pre-trained weights θ_0 and fine-tuned weights θ_task, what is the task vector and how would you remove that task's influence?

- Concept: Label Noise Types (Symmetric vs. Asymmetric)
  - Why needed here: CUTS handles these differently in α estimation; symmetric noise lacks structure (random flips) while asymmetric has systematic class-to-class mappings.
  - Quick check question: If 20% of "cat" labels are flipped to "dog" but all other classes are clean, is this symmetric or asymmetric noise?

- Concept: Backdoor/Poison Attacks
  - Why needed here: Backdoors create learnable trigger patterns that CUTS targets; understanding that triggers are generalizable features (not per-sample memorization) is critical.
  - Quick check question: Why might fine-tuning on clean data (CF baseline) fail to remove a backdoor trigger?

## Architecture Onboarding

- Component map:
  - θ_mix: Corrupted model weights (input to CUTS)
  - D_proxy: Proxy dataset with corruption applied (not original training data)
  - θ_proxy: Weights after fine-tuning θ_mix on D_proxy
  - τ_p = θ_proxy - θ_mix: Proxy task vector approximating corruption direction
  - α scaling: Calibration factor controlling how much of τ_p to subtract
  - θ_u = θ_mix - α*τ_p: Corrected model weights

- Critical path: Fine-tune → Extract τ_p → Grid search α → Apply correction → Evaluate on test set (requires held-out clean test data for final validation, though α selection is proxy-only)

- Design tradeoffs:
  - Pre-trained vs. random init: Pre-trained models yield higher recovery (disentanglement stronger) but may not be available for all domains
  - Proxy size: Larger proxy → higher τ_p norm → smaller α needed, but diminishing returns (Table 11)
  - BatchNorm handling: BN parameters excluded from task vector to avoid destabilizing running statistics

- Failure signatures:
  - Mix-τr baseline shows random direction subtraction has no effect (ASR remains 100%, UT unchanged) — indicates direction matters
  - SAP baseline underperforms CF in source-free setting — methods relying on clean sample identification fail without access to original data
  - Potion collapses at high corruption rates — specialized methods may be unstable outside their design assumptions

- First 3 experiments:
  1. Reproduce CLIP + CIFAR10 symmetric label noise (η=40%): Train Mix model, create 2% proxy with random labels, run CUTS, verify RR > 80%
  2. Ablate pre-training: Compare ResNet18 random init vs. ImageNet pretrained on same corruption — confirm pretrained RR substantially higher
  3. Test poison trigger (η=10%): Verify ASR drops below 1% while UT remains > 95%; compare against CF baseline which should fail (ASR stays 100%)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can CUTS be extended to a zero-shot setting where the proxy dataset is generated directly from the trained model using data-free inversion methods, eliminating the need for any external data?
- Basis in paper: [explicit] "One possible extension of our work can be a zero-shot version of CMU, where the proxy set is generated directly from the trained models using data-free inversion methods [9, 70]."
- Why unresolved: The current method still requires a proxy dataset from the input domain. Data-free inversion may not preserve corruption-specific features needed for accurate task vector estimation.
- What evidence would resolve it: Experiments applying CUTS to proxy sets generated via methods like DeepInversion, comparing recovery rates against externally-sourced proxy datasets.

### Open Question 2
- Question: Would layer-wise tuning of the scaling coefficient α improve correction performance compared to the current single global coefficient approach?
- Basis in paper: [explicit] "Scaling coefficient α can also be tuned layer-wise instead of a single global coefficient."
- Why unresolved: Different layers may encode corruption and clean signals at different intensities, but layer-wise tuning increases hyperparameter search complexity and may require clean validation data.
- What evidence would resolve it: Ablation studies comparing global vs. layer-wise α selection across architectures and corruption types, measuring both recovery rates and computational overhead.

### Open Question 3
- Question: Why does CUTS show substantially lower recovery rates on randomly initialized networks compared to pre-trained models, and can this gap be closed?
- Basis in paper: [inferred] Figure 4 and Table 2 show markedly lower recovery rates for randomly initialized ResNets versus ImageNet-pretrained models. The authors attribute this to weaker weight disentanglement but do not propose solutions.
- Why unresolved: Task arithmetic effectiveness is tied to pre-trained priors, but the underlying mechanisms determining when corruption and clean task directions become disentangled remain unclear.
- What evidence would resolve it: Analysis of weight space geometry across training regimes, or hybrid approaches that inject beneficial structure (e.g., auxiliary pre-training, regularization) before CMU.

### Open Question 4
- Question: Can CUTS handle complex, unknown, or mixed corruption types where the practitioner cannot accurately specify the corruption kernel for the proxy dataset?
- Basis in paper: [inferred] The method assumes the corruption type is identified and the proxy dataset faithfully reflects it. For Clothing1M, the authors note "the original dataset may involve a more complex noise transition matrix, we assume it is unknown and use symmetric noise."
- Why unresolved: Real-world corruptions may be heterogeneous, instance-dependent, or unidentified, making proxy construction ambiguous.
- What evidence would resolve it: Experiments with mismatched or heterogeneous proxy corruptions, or methods that automatically infer corruption characteristics from the model itself.

## Limitations
- CUTS performance degrades significantly on randomly initialized networks compared to pre-trained models, limiting applicability to domains without strong pre-training data
- The method requires proxy datasets with corruption patterns matching the original training corruption, which may be unknown or unidentifiable in real-world settings
- Layer-wise weight disentanglement is assumed but not empirically validated across diverse architectures and corruption types

## Confidence
- **High Confidence**: Basic CUTS mechanism (fine-tune corrupted model on proxy → extract task vector → subtract calibrated multiple) is well-supported by controlled experiments across multiple architectures and datasets.
- **Medium Confidence**: The claim that CUTS generalizes to arbitrary corruption types without architectural modification is supported but requires broader validation.
- **Low Confidence**: The assertion that CUTS works "without any additional modification" for completely unseen corruption types has not been rigorously tested.

## Next Checks
1. **Architecture Stress Test**: Apply CUTS to a BERT model corrupted with token-level label noise and verify weight disentanglement holds.
2. **Corruption Generalization**: Evaluate CUTS on corrupted data with mismatched corruption patterns between original training and proxy dataset.
3. **Scale Validation**: Test CUTS on ImageNet-scale models and datasets to verify computational efficiency claims at larger scale.