---
ver: rpa2
title: Attribution-Guided Decoding
arxiv_id: '2509.26307'
source_url: https://arxiv.org/abs/2509.26307
tags:
- header
- uni00000014
- uni00000012
- attribution
- uni00000004
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Attribution-Guided Decoding (AGD) is a decoding strategy that steers
  LLM generation by selecting output tokens based on their attribution to a user-defined
  Region of Interest (ROI), such as an instruction or knowledge source. Rather than
  manipulating model activations, AGD evaluates a small set of high-probability token
  candidates and chooses the one with the highest relevance score to the ROI, using
  post-hoc attribution methods like Layer-wise Relevance Propagation.
---

# Attribution-Guided Decoding

## Quick Facts
- **arXiv ID:** 2509.26307
- **Source URL:** https://arxiv.org/abs/2509.26307
- **Reference count:** 40
- **Primary result:** Attribution-Guided Decoding (AGD) improves instruction-following success rates on Llama 3.1 from 66.0% to 79.1% by steering token selection via relevance to user-defined Regions of Interest (ROI).

## Executive Summary
Attribution-Guided Decoding (AGD) is a decoding strategy that improves LLM adherence to user instructions and factual accuracy by selecting output tokens based on their attribution to a user-defined Region of Interest (ROI), such as an instruction or knowledge source. Rather than manipulating model activations, AGD evaluates a small set of high-probability token candidates and chooses the one with the highest relevance score to the ROI, using post-hoc attribution methods like Layer-wise Relevance Propagation (LRP). Experiments on instruction following and knowledge-intensive tasks show AGD significantly improves adherence and factual accuracy—for example, boosting instruction-following success rates on Llama 3.1 from 66.0% to 79.1%—while an entropy-gated variant preserves output quality and reduces computational cost by intervening only when the model is uncertain.

## Method Summary
AGD modifies the standard autoregressive decoding loop by adding an attribution-based selection step. At each timestep, the model generates a candidate set (typically top-5 tokens with probability above 0.05). For each candidate, Layer-wise Relevance Propagation (LRP) backpropagates the logit value to the input, and the algorithm sums the relevance scores over the user-defined ROI (e.g., instruction tokens). The candidate with the highest summed relevance score is selected for output. An entropy-gated variant applies this selection only when the output distribution's Shannon entropy exceeds a threshold (1.734, the 80th percentile on IHEval), otherwise defaulting to greedy decoding to preserve fluency.

## Key Results
- AGD significantly improves instruction-following success rates (IHEval PLA/ILA) across multiple models, with Llama 3.1 improving from 66.0% to 79.1%.
- The entropy-gated variant preserves output quality while reducing computational cost by intervening only on high-entropy (uncertain) steps.
- AGD improves factual accuracy on knowledge-intensive tasks (TriviaQA, Natural Questions, HotPotQA) by steering toward tokens more relevant to the question context.
- Negative relevance scores enable AGD to suppress forbidden words by penalizing candidates with negative attribution to exclusion instructions.

## Why This Works (Mechanism)

### Mechanism 1: Selection via Attribution Maximization
- **Claim:** If an attribution method (specifically LRP) faithfully quantifies the contribution of an input region to a logit, then selecting tokens that maximize this attribution score increases adherence to that region.
- **Mechanism:** At each step $t$, the model generates a candidate set $C_t$ (top-$k$ filtered). For each candidate $c$, Layer-wise Relevance Propagation (LRP) backpropagates the logit value to the input. The algorithm sums the relevance scores over the user-defined Region of Interest (ROI) (e.g., instruction tokens) and selects the candidate with the highest score (Eq. 3).
- **Core assumption:** The relevance score $S(c, R)$ is a faithful proxy for the semantic dependency between the candidate token and the ROI.
- **Evidence anchors:**
  - [abstract] "selecting the one with the highest relevance score to the ROI, using post-hoc attribution methods"
  - [section 3.2] Eq. 2 defines the score summation; Eq. 3 defines the argmax selection.
  - [corpus] Neighbor paper "Attribution-guided Pruning..." supports the general utility of attribution for model analysis, though not explicitly for decoding.
- **Break condition:** If the attribution method produces noisy or unfaithful relevance maps (e.g., pure gradient methods without LRP's normalization), the selected token may appear relevant but be semantically incoherent.

### Mechanism 2: Entropy-Gated Intervention
- **Claim:** Limiting attribution-guided selection to high-uncertainty (high-entropy) steps preserves output fluency while maintaining constraint satisfaction.
- **Mechanism:** The method calculates the Shannon entropy $H(p_t)$ of the output distribution. If $H(p_t) < \tau$ (model is confident), it defaults to greedy decoding. If $H(p_t) \ge \tau$ (model is uncertain), it applies AGD.
- **Core assumption:** "Critical forks" (high entropy) are the primary points where a model might deviate from instructions; low-entropy steps are "safe" and primarily drive fluency.
- **Evidence anchors:**
  - [abstract] "entropy-gated variant preserves output quality... by intervening only when the model is uncertain."
  - [section 3.4] Eq. 5 formalizes the conditional selection rule.
  - [corpus] "Optimizing Decoding Paths in Masked Diffusion Models..." links uncertainty quantification to output quality, supporting the general strategy of targeting uncertain steps.
- **Break condition:** If the threshold $\tau$ is set too low, the model intervenes too often, degrading fluency (as seen in the ablation of standard AGD); if set too high, it misses critical constraint violations.

### Mechanism 3: Negative Relevance for Exclusion
- **Claim:** Negative attribution scores allow the model to actively suppress candidate tokens associated with negative constraints (e.g., "forbidden words").
- **Mechanism:** When a forbidden word appears in the candidate set, it often exhibits negative relevance scores with respect to the exclusion instruction. Because the selection rule is $\text{argmax}$ (seeking the highest value), these negative scores push the selection toward alternative tokens with positive or neutral scores.
- **Core assumption:** The model internally represents "negative constraints" as inhibitory signals that manifest as negative relevance during backpropagation.
- **Evidence anchors:**
  - [section 4.3] Figure 2c and corresponding text: "When a forbidden word appears as a candidate, it exhibits a negative attribution signal... a penalty observed on average across all layers."
- **Break condition:** If the attribution method only produces positive values (e.g., standard Grad-CAM without sign preservation), this avoidance mechanism fails.

## Foundational Learning

- **Concept:** Layer-wise Relevance Propagation (LRP)
  - **Why needed here:** AGD relies on AttnLRP to handle the non-linearities of Transformers (Attention, LayerNorm) more faithfully than simple Input $\times$ Gradient methods, which can be noisy.
  - **Quick check question:** Can you explain why a simple gradient might fail to attribute relevance correctly in a deep Transformer layer compared to LRP?

- **Concept:** Region of Interest (ROI)
  - **Why needed here:** The effectiveness of AGD depends entirely on defining the correct subset of input tokens or internal components (heads) to sum relevance over.
  - **Quick check question:** If you wanted to improve factuality in a closed-book setting, would you define the ROI over the input tokens or specific attention heads?

- **Concept:** Entropy in Autoregressive Decoding
  - **Why needed here:** Understanding the relationship between output probability distribution entropy and model uncertainty is required to tune the entropy-gating mechanism ($\tau$).
  - **Quick check question:** Does high entropy at a decoding step typically indicate the model is confident in a single token or torn between multiple plausible continuations?

## Architecture Onboarding

- **Component map:** Standard Forward Pass -> Top-$k$ Candidate Generation -> Attribution Backward Pass (LRP) -> ROI Aggregator -> Entropy Gate -> Token Selection
- **Critical path:** The backward pass (Attribution) is the primary computational bottleneck. Unlike standard decoding, this requires $k$ backward passes per generated token (unless entropy-gated), increasing latency significantly.
- **Design tradeoffs:**
  - **Candidate Set Size ($k$):** Larger $k$ increases the chance of finding a compliant token but linearly increases compute cost.
  - **Threshold ($\tau$):** Lower thresholds improve instruction adherence but risk "run-on" sentences and quality degradation (as seen in the "riddle" example).
  - **Attribution Method:** LRP is more robust but computationally heavier than Input $\times$ Gradient (I×G), which is faster but noisier.
- **Failure signatures:**
  - **Fluency Collapse:** Generating grammatically correct but semantically repetitive or "run-on" text (often fixed by entropy gating).
  - **False Positives:** Selecting a token with high attribution but low probability, resulting in incoherent text (mitigated by $\pi_{min}$).
- **First 3 experiments:**
  1. **Implement the ROI definition:** Set up the system to separate the "Instruction" (System Prompt) from the "Task" (User Prompt) and sum relevance only over the former.
  2. **Visualize the "Negative" Signal:** Replicate the forbidden word experiment (Section 4.3) to verify that your attribution implementation correctly generates negative scores for excluded concepts.
  3. **Tune the Entropy Threshold:** Run a sweep on $\tau$ (e.g., 1.0 to 3.0) on a validation set to find the inflection point where adherence gains outweight fluency loss.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can defining Regions of Interest (ROI) over specific, monosemantic circuits within the model architecture yield more granular control than defining ROIs over input spans or attention heads?
- **Basis in paper:** [explicit] The Conclusion states: "Moreover, the ROI concept could be extended... to more monosemantic structures, such as specific, functionally-identified circuits within the model, enabling more granular control."
- **Why unresolved:** The paper demonstrates AGD using input embeddings and attention heads, but these are broad structural components; the efficacy of targeting specific functional sub-networks (circuits) remains unexplored.
- **What evidence would resolve it:** Experiments applying AGD to specific, mapped mechanistic circuits (e.g., induction heads or refusal circuits) and measuring the precision of behavioral changes compared to standard head-level guidance.

### Open Question 2
- **Question:** Can efficient attribution proxies be developed to replace backward-pass methods like LRP, thereby reducing inference latency while maintaining steering fidelity?
- **Basis in paper:** [explicit] The Conclusion identifies the computational cost of multiple backward passes as a limitation and suggests: "Future work could focus on developing more efficient attribution proxies to mitigate these costs."
- **Why unresolved:** While entropy-gating reduces the *frequency* of intervention, the *cost* of the intervention itself (backward pass) remains high, limiting real-time application.
- **What evidence would resolve it:** A comparative study of AGD performance using approximate attribution methods (e.g., learned surrogate models or linear approximations) versus the current AttnLRP implementation, measuring the latency/quality trade-off.

### Open Question 3
- **Question:** Can AGD be effectively hybridized with "interventionist" techniques to overcome the limitation where the model fails to propose the desired target token in the candidate set?
- **Basis in paper:** [explicit] The Conclusion lists as a primary limitation: "AGD's primary limitation is inherent to its design as a selection mechanism: it cannot generate a desired token if it is not proposed by the model."
- **Why unresolved:** AGD is purely "selectionist" (choosing from existing options); if the model's probability distribution excludes the correct answer entirely, AGD currently fails.
- **What evidence would resolve it:** Experiments combining AGD with logit-biasing or activation steering to force low-probability target tokens into the top-k candidate set, followed by AGD selection, evaluated on complex constraint tasks.

### Open Question 4
- **Question:** Is the 80th percentile entropy threshold ($\tau=1.734$) universally robust across different model architectures and domains, or does it require per-model tuning?
- **Basis in paper:** [inferred] The authors fixed the entropy threshold based on the 80th percentile of token-level entropy on IHEval (Llama 3.1), but experimental results show distinct entropy distributions and quality trade-offs for Gemma and Qwen.
- **Why unresolved:** While the authors argue for the "critical fork" hypothesis, the fixed threshold may be suboptimal for models with different uncertainty calibrations or entropy profiles.
- **What evidence would resolve it:** A hyperparameter sweep of $\tau$ across the three tested models (Llama, Qwen, Gemma) to determine if the optimal intervention point varies significantly from the 80th percentile heuristic.

## Limitations

- **Faithfulness of Attribution:** The core mechanism relies on LRP as a faithful measure of semantic relevance, but this assumption is not empirically validated within the paper.
- **Computational Overhead:** While entropy-gating reduces intervention frequency, the backward-pass attribution remains computationally expensive, with no concrete performance metrics provided.
- **Token Availability:** AGD cannot select tokens that are not proposed in the top-$k$ candidate set, limiting its effectiveness when the model's distribution excludes the desired answer.

## Confidence

- **High Confidence:** The empirical results showing improved instruction-following (IHEval) and factuality (QA tasks) are robust and directly support the paper's primary claims.
- **Medium Confidence:** The mechanism of using negative relevance to suppress forbidden words is well-demonstrated, but the broader claim that LRP attribution is a faithful proxy for semantic dependency is supported by indirect evidence but not rigorously proven.
- **Low Confidence:** The paper's discussion of the computational efficiency of entropy-gated AGD is vague, lacking concrete performance metrics.

## Next Checks

1. **Validate Attribution Faithfulness:** Design an experiment to test whether the tokens selected by AGD (those with the highest LRP score to the ROI) are actually the tokens that, when ablated, cause the greatest decrease in the logit value for the ROI. This would provide direct evidence that LRP is a faithful attribution method for this specific use case.

2. **Systematic Entropy Threshold Sweep:** Instead of relying on a single, hand-tuned value of τ=1.734, conduct a comprehensive sweep over a range of entropy thresholds (e.g., 0.5 to 4.0) on a held-out validation set. Plot the tradeoff curve between instruction adherence and output quality (e.g., perplexity or human evaluation) to identify the Pareto-optimal operating point.

3. **Quantify Computational Overhead:** Implement a benchmark to measure the wall-clock inference time of AGD versus standard greedy decoding. Report the average time per token and the total time for a representative sample of prompts. This will provide a concrete understanding of the practical cost of the method.