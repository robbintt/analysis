---
ver: rpa2
title: 'Z1: Efficient Test-time Scaling with Code'
arxiv_id: '2504.00810'
source_url: https://arxiv.org/abs/2504.00810
tags:
- reasoning
- thinking
- energy
- tokens
- test-time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Z1 achieves efficient test-time scaling in language reasoning by
  fine-tuning code-related trajectory data with a shifted thinking window that adapts
  reasoning intensity to problem complexity. Fine-tuned on Z1-Code-Reasoning-107K,
  Z1-7B matches R1-Distill-Qwen-7B performance while using only ~30% of the average
  thinking tokens.
---

# Z1: Efficient Test-time Scaling with Code
## Quick Facts
- arXiv ID: 2504.00810
- Source URL: https://arxiv.org/abs/2504.00810
- Reference count: 40
- Achieves efficient test-time scaling in language reasoning by adapting reasoning intensity to problem complexity

## Executive Summary
Z1 introduces an efficient test-time scaling approach for language reasoning by fine-tuning code-related trajectory data with a shifted thinking window mechanism. This method adapts reasoning intensity based on problem complexity, achieving performance parity with R1-Distill-Qwen-7B while using only ~30% of the average thinking tokens. The model demonstrates strong generalization from code-only trajectories to broader reasoning tasks, achieving 47.5% accuracy on GPQA Diamond.

## Method Summary
Z1 employs a shifted thinking window approach that dynamically adjusts reasoning intensity based on problem complexity during test-time inference. The model is fine-tuned on Z1-Code-Reasoning-107K, a proprietary dataset of code-related trajectory data using the longest-greedy sampling strategy. This approach enables the model to apply weak reasoning for simple problems and strong reasoning for complex ones, significantly reducing overthinking and computational overhead while maintaining performance.

## Key Results
- Z1-7B matches R1-Distill-Qwen-7B performance while using only ~30% of the average thinking tokens
- Ablation studies show longer training trajectories and larger training sample sizes are critical for effective reasoning
- Achieves 47.5% accuracy on GPQA Diamond, demonstrating generalization from code-only trajectories to broader reasoning tasks

## Why This Works (Mechanism)
The shifted thinking window mechanism works by adapting the reasoning intensity to match problem complexity during test-time inference. For simple problems, the model applies minimal reasoning steps, while for complex problems, it allocates more extensive reasoning. This dynamic allocation prevents overthinking on easy problems while ensuring sufficient reasoning depth for difficult ones, leading to computational efficiency without sacrificing accuracy.

## Foundational Learning
- **Test-time scaling**: Adjusting reasoning depth during inference based on problem difficulty; needed to balance efficiency and accuracy; quick check: compare token usage vs performance across difficulty levels
- **Trajectory data fine-tuning**: Using step-by-step reasoning sequences for training; needed to teach the model reasoning patterns; quick check: verify training data contains diverse reasoning paths
- **Longest-greedy sampling**: Selecting the most extended reasoning sequences during training; needed to capture comprehensive reasoning strategies; quick check: confirm sampling strategy preserves essential reasoning steps

## Architecture Onboarding
- **Component map**: Input Problem -> Shifted Thinking Window -> Adaptive Reasoning Steps -> Output Solution
- **Critical path**: Problem → Complexity Assessment → Reasoning Intensity Allocation → Solution Generation
- **Design tradeoffs**: Efficiency vs accuracy balance, generalization from code to non-code tasks, proprietary dataset limitations
- **Failure signatures**: Overthinking on simple problems, under-reasoning on complex problems, poor generalization to non-code domains
- **First experiments**: 1) Benchmark Z1-7B against R1-Distill-Qwen-7B on varied reasoning tasks, 2) Test generalization on mathematics and science benchmarks, 3) Perform ablation studies on thinking window parameters

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation beyond code-related tasks and single non-code benchmark (GPQA Diamond)
- Proprietary training dataset prevents independent verification of data quality and distribution effects
- Claims of performance parity require clarification on statistical significance across all benchmarks

## Confidence
- High Confidence: Technical implementation of shifted thinking window mechanism is well-described
- Medium Confidence: Efficiency improvements and performance parity are supported but need broader validation
- Medium Confidence: Generalization claims demonstrated but require testing on additional non-code domains

## Next Checks
1. Evaluate Z1-7B on diverse reasoning benchmarks including mathematics, science, and commonsense reasoning tasks
2. Conduct ablation studies varying token budget allocation between simple and complex problems
3. Perform statistical significance testing comparing Z1-7B and R1-Distill-Qwen-7B across all benchmarks