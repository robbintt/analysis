---
ver: rpa2
title: 'IndiCASA: A Dataset and Bias Evaluation Framework in LLMs Using Contrastive
  Embedding Similarity in the Indian Context'
arxiv_id: '2510.02742'
source_url: https://arxiv.org/abs/2510.02742
tags:
- bias
- dataset
- stereotype
- loss
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes IndiCASA, a novel dataset and evaluation framework
  for detecting societal biases in LLMs within the Indian cultural context. The framework
  uses contrastive learning to fine-tune an encoder that distinguishes between stereotypical
  and anti-stereotypical sentences.
---

# IndiCASA: A Dataset and Bias Evaluation Framework in LLMs Using Contrastive Embedding Similarity in the Indian Context

## Quick Facts
- **arXiv ID**: 2510.02742
- **Source URL**: https://arxiv.org/abs/2510.02742
- **Reference count**: 40
- **Key outcome**: Novel dataset and evaluation framework for detecting societal biases in LLMs within the Indian cultural context using contrastive embedding similarity

## Executive Summary
This paper introduces IndiCASA, a comprehensive framework for detecting and evaluating societal biases in Large Language Models (LLMs) within the Indian cultural context. The framework combines a novel dataset of 2,575 human-validated sentences across five bias categories (disability, religion, gender, profession, and caste) with a contrastive learning approach to fine-tune an encoder that can distinguish between stereotypical and anti-stereotypical sentences. Evaluations across multiple open-weight LLMs reveal that all models exhibit some degree of stereotypical bias, with disability-related biases being notably persistent while religion bias was generally lower, likely due to global debiasing efforts. The method effectively captures nuanced biases and provides a robust framework for assessing LLM fairness in culturally diverse contexts.

## Method Summary
IndiCASA employs a two-pronged approach: dataset creation and model evaluation. The IndiCASA dataset is constructed through systematic data collection from diverse sources, followed by rigorous human validation to ensure quality and relevance to the Indian context. For evaluation, the framework uses contrastive learning to fine-tune an encoder that learns to distinguish between stereotypical and anti-stereotypical sentences. This fine-tuned encoder then serves as a bias detector, measuring the embedding similarity between generated text and known stereotypical or anti-stereotypical patterns. The framework evaluates multiple open-weight LLMs across five bias categories, providing a comprehensive assessment of cultural biases in language models.

## Key Results
- All evaluated LLMs exhibit some degree of stereotypical bias across the five tested categories
- Disability-related biases were notably persistent across models
- Religion bias was generally lower, likely due to global debiasing efforts
- The contrastive learning approach effectively captures nuanced biases specific to the Indian cultural context

## Why This Works (Mechanism)
The framework works by leveraging contrastive learning to create a specialized bias detector that can effectively distinguish between stereotypical and anti-stereotypical patterns in text. By fine-tuning an encoder on carefully curated contrasting pairs from the IndiCASA dataset, the model learns to recognize subtle linguistic cues and contextual patterns that indicate bias. The embedding similarity measurement then provides a quantitative assessment of how closely generated text aligns with stereotypical patterns, enabling systematic bias evaluation across different LLMs and bias categories.

## Foundational Learning

**Contrastive Learning**
- *Why needed*: Enables the model to learn meaningful representations by contrasting similar and dissimilar examples
- *Quick check*: Verify that the model can correctly classify known stereotypical vs. anti-stereotypical sentence pairs

**Cultural Bias Detection**
- *Why needed*: Addresses the need for culturally-specific bias evaluation beyond Western-centric approaches
- *Quick check*: Ensure the framework captures biases relevant to Indian cultural contexts and societal norms

**Embedding Similarity Metrics**
- *Why needed*: Provides quantitative measurement of bias through vector space analysis
- *Quick check*: Confirm that embedding distances correlate with human judgments of bias

## Architecture Onboarding

**Component Map**
Dataset Creation -> Contrastive Fine-tuning -> Embedding Similarity Measurement -> Bias Evaluation

**Critical Path**
1. Data collection and human validation
- Assumption: Human validators are representative of the Indian cultural context
2. Contrastive learning fine-tuning of encoder
- Assumption: The encoder can generalize from limited training pairs to broader bias patterns
3. Embedding similarity computation and bias scoring
- Assumption: Cosine similarity is an appropriate metric for measuring bias in this context

**Design Tradeoffs**
- Smaller, high-quality human-validated dataset vs. larger automated datasets
- Focus on Indian cultural context vs. broader global applicability
- Open-weight models only vs. including proprietary models

**Failure Signatures**
- Poor performance on categories with limited training examples
- Inability to detect subtle or nuanced biases
- Overfitting to specific linguistic patterns in training data

**First Experiments**
1. Evaluate encoder performance on held-out stereotypical vs. anti-stereotypical pairs
2. Compare bias scores across different LLMs for the same prompt
3. Test framework sensitivity by introducing known biased text samples

## Open Questions the Paper Calls Out

None

## Limitations

- Dataset size of 2,575 sentences may not capture the full spectrum of societal biases in the Indian context
- Focus on five specific bias categories means other important cultural dimensions may be overlooked
- Methodology's generalizability to other cultural contexts or bias types remains untested
- Evaluation limited to open-weight LLMs, potentially missing biases in proprietary models
- Assumption: Human validation process is consistent and unbiased
- Unknown: Whether the framework would perform similarly with larger or differently constructed datasets

## Confidence

- **High**: Dataset creation methodology and human validation process are clearly described and reproducible
- **Medium**: Observed patterns of bias across different categories (particularly the lower religion bias and persistent disability bias) are likely valid within the Indian context
- **Low**: Generalizability of the contrastive learning approach to other cultural contexts and bias types

## Next Checks

1. Test the IndiCASA framework on additional cultural contexts beyond India to evaluate cross-cultural applicability
2. Expand the dataset size and include additional bias categories relevant to the Indian context (e.g., linguistic, regional, or economic biases)
3. Evaluate proprietary LLMs alongside open-weight models to provide a more comprehensive assessment of bias in commercially deployed systems