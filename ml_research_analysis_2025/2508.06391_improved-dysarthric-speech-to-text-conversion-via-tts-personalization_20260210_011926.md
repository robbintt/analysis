---
ver: rpa2
title: Improved Dysarthric Speech to Text Conversion via TTS Personalization
arxiv_id: '2508.06391'
source_url: https://arxiv.org/abs/2508.06391
tags:
- speech
- dysarthric
- data
- train-dys
- speaker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a case study on developing a personalized speech-to-text
  system for a Hungarian speaker with severe dysarthria, using synthetic speech augmentation.
  The authors fine-tuned an ASR model using synthetic dysarthric speech generated
  via a personalized TTS system trained on premorbidity recordings and speaker embedding
  interpolation.
---

# Improved Dysarthric Speech to Text Conversion via TTS Personalization

## Quick Facts
- arXiv ID: 2508.06391
- Source URL: https://arxiv.org/abs/2508.06391
- Reference count: 28
- Monolingual FastConformer Hu ASR model outperformed Whisper-turbo when fine-tuned on dysarthric speech

## Executive Summary
This paper presents a case study on developing a personalized speech-to-text system for a Hungarian speaker with severe dysarthria. The approach leverages synthetic speech augmentation, where a personalized TTS system is trained on premorbidity recordings to generate controlled dysarthric speech with varying severity levels. The authors fine-tuned an ASR model using both real and synthetic dysarthric speech, achieving a dramatic reduction in character error rate from 36-51% (zero-shot) to 7.3%. The synthetic data contributed an 18% relative CER reduction, demonstrating the value of TTS-based augmentation for dysarthric speech recognition.

## Method Summary
The authors created a personalized TTS system by training on 13 hours of premorbidity lecture recordings from the target speaker. To generate dysarthric speech, they employed speaker embedding interpolation, creating datasets with different severity levels by adjusting interpolation weights. The ASR model (FastConformer Hu) was fine-tuned on both real dysarthric speech (2.6 hours) and synthetic data generated through the TTS system. The approach used vector arithmetic in the speaker embedding space to control the severity of dysarthria in synthetic speech, with specific weight vectors manually selected to create datasets with different impairment levels.

## Key Results
- Character error rate reduced from 36-51% (zero-shot) to 7.3% after fine-tuning
- Synthetic data contributed an 18% relative reduction in CER
- Monolingual FastConformer Hu ASR model significantly outperformed Whisper-turbo when fine-tuned on the same data
- Fine-tuning on both real and synthetic dysarthric speech was essential for achieving optimal performance

## Why This Works (Mechanism)
The approach works by creating a bridge between typical speech and dysarthric speech through speaker embedding interpolation. By training a TTS system on premorbidity recordings and then interpolating between the typical speaker embedding and a dysarthria-affected embedding, the system can generate synthetic speech that maintains the speaker's identity while exhibiting controlled levels of dysarthria. This synthetic data augmentation addresses the fundamental challenge in dysarthric ASR: the lack of sufficient training data for rare speech patterns.

## Foundational Learning

**Speaker Embedding Interpolation**: Vector arithmetic in embedding space to create new speaker characteristics
- Why needed: To generate synthetic dysarthric speech that maintains speaker identity while controlling severity
- Quick check: Verify interpolated embeddings produce perceptually distinct severity levels

**TTS Personalization**: Training TTS systems on individual speaker's typical speech
- Why needed: To maintain speaker identity when generating synthetic dysarthric speech
- Quick check: Ensure synthetic speech retains target speaker's voice characteristics

**ASR Fine-tuning**: Adapting pre-trained models to specific speech patterns
- Why needed: To improve recognition accuracy for dysarthric speech patterns
- Quick check: Monitor CER improvement during fine-tuning process

## Architecture Onboarding

**Component Map**: Premorbidity recordings -> TTS Training -> Speaker Embedding Interpolation -> Synthetic Speech Generation -> ASR Fine-tuning -> Improved Recognition

**Critical Path**: The most critical sequence is: premorbidity recordings → TTS training → synthetic speech generation → ASR fine-tuning → evaluation. Any failure in the TTS system or embedding interpolation will directly impact the quality of synthetic data and thus ASR performance.

**Design Tradeoffs**: The system trades computational complexity (training personalized TTS) for improved recognition accuracy. Using synthetic data reduces the dependency on collecting large amounts of real dysarthric speech, which is time-consuming and requires speaker cooperation.

**Failure Signatures**: Poor TTS quality will manifest as synthetic speech that doesn't match the speaker's voice or exhibits unnatural dysarthria patterns. Incorrect interpolation weights will produce either too mild or too severe dysarthria that doesn't match the target speaker's actual condition.

**3 First Experiments**:
1. Test TTS system with held-out premorbidity samples to verify speaker identity preservation
2. Generate small synthetic datasets with different interpolation weights and conduct listening tests
3. Fine-tune ASR on synthetic data alone before adding real dysarthric speech to assess augmentation value

## Open Questions the Paper Calls Out

**Open Question 1**: What is the minimum amount of premorbidity speech data required to effectively maintain speaker identity and dysarthria severity control in the TTS personalization process?

**Open Question 2**: Does the proposed TTS-based augmentation method generalize to speakers with different types of dysarthria (e.g., ataxic, hypokinetic) or is it optimized primarily for the single speaker's specific pathology?

**Open Question 3**: Can the optimal interpolation weights for speaker embeddings (controlling severity) be determined automatically, or are they strictly dependent on empirical trial-and-error?

## Limitations
- Single-speaker case study limits generalizability to other dysarthric speakers with different speech patterns or underlying conditions
- Only one type of synthetic speech generation (TTS-based) was evaluated, leaving alternative approaches unexplored
- Evaluation relies on a single Hungarian ASR model, making cross-language generalization unclear

## Confidence

**High confidence**: Technical methodology for synthetic speech generation and fine-tuning procedures follow established practices
**Medium confidence**: Reported CER improvements are substantial but limited by single-case nature and small real speech dataset
**Low confidence**: Generalizability to other speakers, languages, or dysarthria severities due to narrow evaluation scope

## Next Checks

1. Replicate the approach with a multi-speaker dysarthric speech corpus to assess generalization across different speakers and dysarthria severities
2. Compare TTS-based synthetic speech augmentation against voice conversion approaches to determine which method yields better ASR performance for dysarthric speech
3. Conduct a longitudinal evaluation with the target speaker to assess system performance stability and real-world usability over extended periods of use