---
ver: rpa2
title: Towards Self-Supervised Foundation Models for Critical Care Time Series
arxiv_id: '2509.19885'
source_url: https://arxiv.org/abs/2509.19885
tags:
- datasets
- data
- learning
- mimic-iii
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work demonstrates the feasibility of self-supervised foundation\
  \ models for critical care time series by introducing a pre-trained model based\
  \ on the Bi-Axial Transformer (BAT) architecture, trained on pooled electronic health\
  \ record datasets. The model is fine-tuned for mortality prediction on an unseen\
  \ dataset, outperforming supervised baselines\u2014especially on small datasets\
  \ (<5,000 samples)\u2014with improvements in AUC-PR and AUC-ROC."
---

# Towards Self-Supervised Foundation Models for Critical Care Time Series

## Quick Facts
- arXiv ID: 2509.19885
- Source URL: https://arxiv.org/abs/2509.19885
- Reference count: 40
- Key outcome: Self-supervised pre-training on pooled EHR datasets improves low-data performance for mortality prediction, with head-only fine-tuning matching full-model fine-tuning.

## Executive Summary
This work demonstrates the feasibility of self-supervised foundation models for critical care time series by introducing a pre-trained model based on the Bi-Axial Transformer (BAT) architecture, trained on pooled electronic health record datasets. The model is fine-tuned for mortality prediction on an unseen dataset, outperforming supervised baselines—especially on small datasets (<5,000 samples)—with improvements in AUC-PR and AUC-ROC. Notably, fine-tuning only the classification head achieves performance close to full-model fine-tuning, indicating that the learned representations are both informative and transferable. The results highlight the potential of self-supervised pre-training to support robust clinical applications in resource-limited settings, while emphasizing the need for large, diverse pre-training datasets to achieve strong generalization across different clinical environments.

## Method Summary
The method involves pre-training a Bi-Axial Transformer on pooled EHR datasets using a self-supervised forecasting objective, then fine-tuning the model for binary mortality prediction on a held-out dataset. The architecture handles irregular, sparse time series through bi-axial attention and explicit missingness indicators. Pre-training uses dynamic sampling of observation (≥12h) and forecasting (2h) windows with masked MSE loss. Fine-tuning compares head-only versus full-model parameter updates, with evaluation on varying training set sizes (100-9,506 samples) using AUC-PR and AUC-ROC metrics.

## Key Results
- Pre-trained models outperform supervised baselines on mortality prediction, particularly with <5,000 training samples
- Head-only fine-tuning achieves performance close to full-model fine-tuning, indicating strong learned representations
- Larger and more diverse pre-training datasets (e.g., eICU+MIMIC-IV at 273K samples) yield better transfer performance
- Models trained on single datasets show 1.0-4.8 point AUC-ROC drops when tested on other datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Forecasting-based self-supervised pre-training generates transferable representations for critical care time series.
- Mechanism: The model learns to predict values in a forecasting window given an observation window, which requires capturing temporal dependencies, sensor identity distributions, and physiological patterns that generalize across patient populations and institutions.
- Core assumption: Predicting future clinical measurements necessitates learning underlying physiological relationships that are relevant to downstream tasks like mortality prediction.
- Evidence anchors:
  - [abstract] "trained on pooled electronic health record datasets... demonstrate effective transfer learning by fine-tuning the model on a dataset distinct from the training sources for mortality prediction"
  - [section 2.1] "We hypothesize that combining and pretraining on additional datasets {D2, ...DK}, in a self-supervised fashion will result in a model that learns richer, more generalized representations of sensor identities and their measurement distributions"
  - [corpus] PatchFormer demonstrates hierarchical masked reconstruction for cross-domain transfer in time series, supporting the broader principle that self-supervised objectives create transferable representations.
- Break condition: If the forecasting task can be solved using dataset-specific artifacts or non-physiological patterns, representations will not transfer to new clinical environments.

### Mechanism 2
- Claim: Bi-axial attention with explicit missingness handling enables effective learning from sparse, irregular clinical time series.
- Mechanism: BAT applies separate self-attention along temporal and feature axes while incorporating missingness indicators directly into input embeddings, allowing the model to learn from both observed values and patterns of missingness (which may be clinically informative).
- Core assumption: Missingness patterns in clinical data are not purely random but carry predictive signal ("informative missingness").
- Evidence anchors:
  - [section 2.2] "BAT attends to both the temporal and clinical feature axes through axial attention mechanisms, and explicitly accounts for missing values... Investigation into model attention maps revealed evidence of BAT learning from informative missingness"
  - [section A.1] "BAT incorporates missingness indicators directly into its input embeddings"
  - [corpus] Limited direct corpus validation for BAT-specific missingness handling; neighboring papers focus on different architectural approaches.
- Break condition: If missingness in a given deployment is random rather than informative, or if sparsity exceeds training distribution, the architectural advantage degrades.

### Mechanism 3
- Claim: Pre-trained representations enable strong low-data performance via minimal fine-tuning (head-only), reducing overfitting risk.
- Mechanism: Self-supervised pre-training produces embeddings already aligned with clinical structure; fine-tuning only the classification head leverages these representations while minimizing parameter updates, which is critical when labeled data is scarce.
- Core assumption: The forecasting pre-training task teaches representations relevant to mortality prediction without explicit mortality labels.
- Evidence anchors:
  - [abstract] "fine-tuning only the classification head achieves performance close to full-model fine-tuning, indicating that the learned representations are both informative and transferable"
  - [section 3.1/Table 1] On MIMIC-III with 5,000 samples: head-only fine-tuning achieves 38.99 AUC-PR vs. 41.89 for full fine-tuning vs. 36.30 for scratch baseline
  - [section 4] "This effect was most pronounced for the model pretrained on the largest dataset (277K samples; MIMIC-IV + eICU)"
  - [corpus] Insufficient corpus evidence directly comparing head-only vs. full fine-tuning in healthcare foundation models.
- Break condition: With sufficient labeled data (≥3,000–5,000 samples depending on dataset), training from scratch may match or exceed fine-tuned performance, as seen when BAT-scratch outperformed head-only on MIMIC-IV and eICU with larger training sets.

### Mechanism 4
- Claim: Pre-training dataset size and diversity are critical for cross-institution generalization.
- Mechanism: Larger, more diverse pre-training corpora expose the model to broader distributions of clinical practices, patient populations, and measurement patterns, creating representations that transfer more robustly.
- Core assumption: Distribution shift across institutions is the primary barrier to transfer; exposure to multiple datasets mitigates this.
- Evidence anchors:
  - [section 4] "large and diverse pre-training datasets are crucial for learning representations that generalize and transfer effectively across datasets"
  - [section 4/Limitations] "To reach a training set size that is competitive with other foundation models, future work will need to incorporate additional datasets"
  - [section A.5/Figure 5] Models trained on single datasets show 1.0–4.8 point AUC-ROC drops when tested on other datasets
  - [corpus] Neighboring papers emphasize domain-specific requirements in healthcare but don't provide comparative scale evidence.
- Break condition: If pre-training datasets share systematic biases or lack diversity relevant to the target deployment, scale alone won't improve transfer.

## Foundational Learning

- **Concept: Self-Supervised Learning with Forecasting Objective**
  - Why needed here: Critical care EHR data has limited labels; forecasting creates a supervision signal from the data itself by predicting future values from past observations.
  - Quick check question: Given an observation window of 12 hours, can the model predict the next 2 hours of measurements with masked MSE loss?

- **Concept: Transfer Learning Across Clinical Institutions**
  - Why needed here: Models trained on one hospital's EHR often fail on others due to distribution shift; pre-training on pooled multi-institution data aims to learn institution-agnostic representations.
  - Quick check question: Does the model pre-trained on eICU+MIMIC-IV and fine-tuned on MIMIC-III outperform a model trained from scratch on MIMIC-III alone?

- **Concept: Axial Attention for High-Dimensional Time Series**
  - Why needed here: Full self-attention on D×T tensors has O(D²T²) complexity; axial attention reduces this by attending separately along each dimension.
  - Quick check question: Can you identify whether attention is being applied along the time axis (capturing temporal dynamics) vs. the feature axis (capturing sensor correlations)?

## Architecture Onboarding

- **Component map:**
  Input embedding -> Bi-axial attention layers -> Static feature fusion -> Prediction heads (classification or forecasting)

- **Critical path:**
  1. Harmonize datasets via YAIB-cohorts (ensure 52 consistent features across MIMIC-III/IV, eICU)
  2. Preprocess: forward-fill → mean imputation → standardize using training split statistics only
  3. Pre-training: Dynamic sampling of observation (≥12h) and forecasting (2h) windows; optimize masked MSE
  4. Fine-tuning: Load pre-trained weights, replace head, train on mortality labels; compare head-only vs. full

- **Design tradeoffs:**
  - Head-only vs. full fine-tuning: Head-only is faster and regularizes better on small data; full fine-tuning may win with ≥5,000 labeled samples
  - Pre-training dataset combination: eICU+MIMIC-IV (273k) > MIMIC-III+eICU (227k) > MIMIC-III+MIMIC-IV (100k) for transfer quality
  - Model scale: 0.86M–1.13M parameters; larger isn't always better if pre-training data is limited

- **Failure signatures:**
  - AUC-PR near positive class prevalence (~11.9% for MIMIC-III): Model not learning meaningful signal
  - Large gap between head-only and full fine-tuning on small data: Pre-trained representations insufficient
  - Strong in-distribution but weak out-of-distribution performance: Overfitting to pre-training distribution
  - Training loss decreasing but validation loss increasing early: Overfitting in fine-tuning; reduce learning rate or freeze more layers

- **First 3 experiments:**
  1. **Reproduce main result**: Pre-train on eICU+MIMIC-IV, fine-tune head-only on MIMIC-III with 1,000 samples; target AUC-PR ~34% (vs. ~28% scratch baseline)
  2. **Data scaling ablation**: Fine-tune on MIMIC-III with [100, 500, 1000, 3000, 5000, 9506] samples; plot AUC-PR/AUC-ROC curves for head-only, full, and scratch to identify crossover points
  3. **Generalization test**: Pre-train on two of three datasets, hold out the third for fine-tuning; correlate t-SNE distribution overlap with transfer performance gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does pre-training on non-clinical time-series domains (e.g., weather, electricity consumption) improve or degrade performance on critical care tasks compared to domain-specific pre-training?
- Basis in paper: [explicit] "Expanding the training data may therefore also require incorporating time-series datasets from other domains... This would allow us to assess whether exposure to broader time-series distributions improves model performance, or if domain-specific data is necessary given the sparse and irregular nature of critical care records."
- Why unresolved: The authors only experimented with pooling ICU datasets; they did not test cross-domain pre-training despite raising it as a key limitation of available critical care data.
- What evidence would resolve it: Pre-train BAT on non-clinical time-series corpora (weather, electricity), fine-tune on mortality prediction, and compare against clinically pre-trained baselines using identical evaluation protocols.

### Open Question 2
- Question: Do the learned representations transfer effectively to clinical tasks beyond binary mortality prediction, such as length-of-stay forecasting, phenotype classification, or real-time sepsis detection?
- Basis in paper: [inferred] The paper evaluates transfer learning only on mortality prediction. Since BAT was originally shown effective for sepsis classification and the pre-training objective is forecasting, the breadth of transfer to other clinically relevant tasks remains untested.
- Why unresolved: No experiments were conducted on downstream tasks other than mortality; the authors state their goal is to support "generalizable and robust clinical applications" without demonstrating that generalization.
- What evidence would resolve it: Fine-tune the pre-trained BAT model on multiple downstream ICU tasks within the YAIB framework and report performance relative to supervised baselines across tasks.

### Open Question 3
- Question: At what scale of pre-training data do the benefits of self-supervised pre-training saturate or diminish for critical care time-series?
- Basis in paper: [inferred] The authors show that their largest pre-trained model (277K samples) yields the strongest transfer, while smaller pre-training datasets (100K–227K) are outperformed by supervised baselines given sufficient labeled fine-tuning data. The relationship between pre-training scale and downstream utility remains unclear.
- Why unresolved: The study tests only three pre-training dataset sizes and does not systematically investigate scaling laws or the point at which additional pre-training data yields diminishing returns.
- What evidence would resolve it: Train a series of BAT models on incrementally larger pooled datasets (e.g., 50K to 1M+ samples) and evaluate fine-tuning performance across a fixed set of downstream dataset sizes to characterize the scaling curve.

## Limitations

- The paper lacks ablation studies isolating the impact of each architectural choice (bi-axial attention, missingness indicators, static feature fusion)
- Pre-training scale (273K samples) is still orders of magnitude smaller than non-healthcare foundation models
- Evaluation was limited to three North American ICU datasets, raising questions about cross-continental generalization

## Confidence

- **High confidence**: Self-supervised pre-training improves low-data performance compared to training from scratch
- **Medium confidence**: Bi-axial attention with missingness handling is essential for the observed improvements
- **Medium confidence**: Pre-training dataset size correlates with transfer quality
- **Low confidence**: The model will generalize to non-ICU settings or non-EHR time series

## Next Checks

1. **Ablation study**: Remove bi-axial attention (use standard Transformer), remove missingness indicators, and remove static feature fusion; measure impact on transfer performance across datasets
2. **Cross-continental transfer**: Fine-tune on a European ICU dataset (e.g., from MIMIC-ES or a UK ICU dataset) to test generalization beyond North American institutions
3. **Downstream task diversity**: Evaluate the pre-trained model on other critical care tasks (e.g., length-of-stay prediction, sepsis detection) to assess representation versatility