---
ver: rpa2
title: 'Document Intelligence in the Era of Large Language Models: A Survey'
arxiv_id: '2510.13366'
source_url: https://arxiv.org/abs/2510.13366
tags:
- document
- pages
- linguistics
- language
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a comprehensive overview of Document AI's
  evolution, focusing on how large language models have transformed the field. It
  systematically categorizes recent research into multimodal and multilingual approaches
  for document understanding and generation tasks.
---

# Document Intelligence in the Era of Large Language Models: A Survey

## Quick Facts
- **arXiv ID:** 2510.13366
- **Source URL:** https://arxiv.org/abs/2510.13366
- **Reference count:** 40
- **Primary result:** Comprehensive survey categorizing Document AI research into multimodal and multilingual approaches, highlighting LLM-driven advancements and future directions

## Executive Summary
This survey systematically examines how large language models have transformed Document AI, organizing recent research into multimodal and multilingual approaches for document understanding and generation. The authors analyze key advancements in integrating textual, visual, and layout modalities through prompt engineering and unified encoding strategies. The paper identifies critical challenges in cross-lingual generalization, structural comprehension, and cross-modal alignment while proposing future research directions including collaborative document agents and foundation models. The survey emphasizes the need for domain-aware datasets, enhanced alignment techniques, and robust evaluation frameworks to advance document-specific AI systems.

## Method Summary
The survey employs a systematic literature review methodology, categorizing Document AI research from 2020-2024 into multimodal and multilingual approaches. The authors analyze 40 key references across document understanding and generation tasks, focusing on how LLMs have addressed traditional challenges in document intelligence. The methodology includes taxonomy development, comparative analysis of architectural approaches, and identification of open research problems through critical evaluation of current limitations and performance bottlenecks.

## Key Results
- LLMs have fundamentally transformed Document AI by enabling unified encoding strategies for multimodal document processing
- Retrieval-augmented generation paradigms significantly enhance context-aware document intelligence capabilities
- Cross-lingual generalization remains challenging due to the "curse of multilinguality" and data scarcity in low-resource languages

## Why This Works (Mechanism)
Document AI's effectiveness with LLMs stems from their ability to process and integrate multiple modalities through unified encoding and prompt engineering. The mechanism leverages pre-trained transformer architectures that can simultaneously understand textual content, visual elements, and spatial layout through cross-attention mechanisms. Retrieval augmentation enhances this by providing contextual grounding, while fine-tuning on domain-specific data enables adaptation to specialized document formats. The success relies on LLMs' inherent ability to capture long-range dependencies and semantic relationships across heterogeneous document elements.

## Foundational Learning
- **Multimodal document representation** - Combines text, layout, and visual features into unified embeddings; needed to capture document semantics holistically; quick check: benchmark performance on multimodal QA tasks
- **Cross-lingual alignment** - Maps representations across different languages while preserving document structure; needed to enable multilingual document understanding; quick check: evaluate zero-shot transfer across language pairs
- **Layout-aware processing** - Incorporates spatial relationships and formatting into model reasoning; needed for accurate interpretation of visually rich documents; quick check: test table and form understanding accuracy
- **Retrieval augmentation** - Integrates external knowledge through RAG frameworks; needed to enhance context and factual accuracy; quick check: measure improvement in answer consistency
- **Foundation model pretraining** - Trains document-specific models on curated multimodal data; needed to overcome limitations of general-purpose LLMs; quick check: compare performance against fine-tuned generic models

## Architecture Onboarding

**Component Map:** Document Input -> Multimodal Encoder -> Cross-Modal Fusion -> Task-Specific Head -> Output Generation

**Critical Path:** The most critical processing sequence involves accurate document parsing (OCR + layout detection), followed by multimodal fusion that aligns visual and textual representations, culminating in task-specific reasoning that leverages both modalities effectively.

**Design Tradeoffs:** End-to-end models offer better alignment but require extensive training data, while modular approaches with separate OCR and vision components are more practical but suffer from error accumulation. Unified architectures reduce complexity but may underperform specialized systems on domain-specific tasks.

**Failure Signatures:** Common failure modes include layout misinterpretation leading to text segmentation errors, cross-lingual interference causing semantic drift, and retrieval hallucination where augmented content contradicts document context. Visual component failures typically manifest as incorrect bounding box detection or OCR errors propagating through the pipeline.

**First Experiments:**
1. Implement a baseline multimodal document QA system using OCR + layout parsing + text-only LLM
2. Add visual feature extraction and cross-modal attention to measure multimodal improvement
3. Conduct ablation study removing each modality to quantify individual contributions

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can collaborative DocAgent frameworks be developed to handle complex, dynamic layouts and multimodal information without relying solely on visual components?
- **Basis in paper:** [Explicit] Section 6.1 identifies "Complex Layout Handling" and "Enhanced Reasoning and Generalization" as critical future research directions for DocAgents. The authors note that current frameworks "still encounter significant challenges in comprehending visually rich documents."
- **Why unresolved:** Current LLM-based agents struggle to interpret structured data like tables and dynamic layouts, often failing to integrate domain-specific knowledge with visual reasoning.
- **What evidence would resolve it:** The successful demonstration of a multi-agent system that can autonomously generate or interpret documents adhering to specific layout templates and natural language instructions, outperforming single-model baselines on complex layout benchmarks.

### Open Question 2
- **Question:** What specific data curation and alignment techniques are required to train document-specific foundation models that avoid the misalignment issues caused by noisy web-text pairs?
- **Basis in paper:** [Explicit] Section 6.2 states that utilizing "noisy web-text pairs and frozen encoder-decoder architectures can lead to misalignment across different modalities," and suggests future work should prioritize "Cross-Modal Datasets" and "Enhanced Alignment Techniques."
- **Why unresolved:** Generic pre-training data lacks the precise alignment between text, layout, and visual elements necessary for high-fidelity document understanding, leading to hallucinations or structural errors.
- **What evidence would resolve it:** The creation of a high-quality paired dataset that captures text, visual elements, and layout dynamics, coupled with a model trained on it that shows statistically significant improvements in cross-modal alignment metrics over models trained on general web data.

### Open Question 3
- **Question:** How can effective embedding and representation methods be designed for multimodal retrieval augmentation to ensure consistency between textual and visual evidence?
- **Basis in paper:** [Explicit] Section 5.2 concludes that "effective embedding and representation methods, and comprehensive evaluation approaches remain open research directions" in the context of multimodal retrieval augmentation.
- **Why unresolved:** Current methods often rely on heuristic parsing (OCR, object detection) which leads to error accumulation, or end-to-end models that struggle to align visual and textual RAG outputs efficiently.
- **What evidence would resolve it:** The proposal of a unified embedding space that can index text, tables, and images simultaneously, validated by superior performance on visual document QA tasks (e.g., VisDoMBench) compared to text-only or heuristic-based RAG pipelines.

### Open Question 4
- **Question:** How can the "curse of multilinguality" be mitigated in document understanding models to ensure robust cross-lingual generalization for low-resource languages?
- **Basis in paper:** [Explicit] Section 4.2 discusses the "curse of multilinguality" where training on large-scale multilingual documents introduces language interference. The section concludes that "challenges in representation and effective cross-lingual generalization remain open research directions."
- **Why unresolved:** Models often overfit to high-resource languages, and data scarcity in low-resource languages restricts the ability of LLMs to capture complex document context and structure.
- **What evidence would resolve it:** A training strategy (e.g., utilizing language-specific neurons or synthetic code-mixing) that enables a model to perform document tasks (like KIE or Classification) on low-resource languages with performance parity to high-resource languages, without catastrophic forgetting.

## Limitations
- The survey focuses primarily on English-language literature, potentially missing important developments in other linguistic contexts
- Empirical validation of proposed approaches is limited, with most analysis based on theoretical frameworks and existing literature
- The rapidly evolving nature of LLM-based document AI means some discussed approaches may become outdated quickly

## Confidence
- **High confidence:** Categorization of document understanding and generation tasks using LLMs
- **Medium confidence:** Assessment of cross-lingual generalization challenges
- **Medium confidence:** Proposed future directions and their feasibility
- **Low confidence:** Completeness of evaluation frameworks discussion

## Next Checks
1. Verify the claimed comprehensiveness by comparing the survey's coverage against a systematic literature review protocol for Document AI publications from 2020-2024
2. Replicate the analysis of cross-modal alignment techniques by implementing and benchmarking at least three different approaches mentioned in the survey
3. Test the practical applicability of retrieval-augmented document intelligence by conducting a user study comparing traditional document processing systems with LLM-enhanced variants across multiple domains