---
ver: rpa2
title: 'The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques
  for Reasoning VLMs'
arxiv_id: '2507.07562'
source_url: https://arxiv.org/abs/2507.07562
tags:
- reasoning
- training
- arxiv
- accuracy
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the synergy between long chain-of-thought
  supervised fine-tuning (SFT) and reinforcement learning (RL) for enhancing reasoning
  capabilities in large vision-language models (VLMs). The study systematically explores
  how these two post-training techniques affect model performance across varying question
  difficulties.
---

# The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs

## Quick Facts
- arXiv ID: 2507.07562
- Source URL: https://arxiv.org/abs/2507.07562
- Authors: Jierun Chen; Tiezheng Yu; Haoli Bai; Lewei Yao; Jiannan Wu; Kaican Li; Fei Mi; Chaofan Tao; Lei Zhu; Manyi Zhang; Xiaohui Li; Lu Hou; Lifeng Shang; Qun Liu
- Reference count: 9
- This paper investigates the synergy between long chain-of-thought supervised fine-tuning (SFT) and reinforcement learning (RL) for enhancing reasoning capabilities in large vision-language models (VLMs).

## Executive Summary
This paper investigates the synergy between long chain-of-thought supervised fine-tuning (SFT) and reinforcement learning (RL) for enhancing reasoning capabilities in large vision-language models (VLMs). The study systematically explores how these two post-training techniques affect model performance across varying question difficulties. Key findings include: SFT improves performance on harder questions through structured reasoning but degrades accuracy on simpler ones due to verbosity, while RL provides consistent gains across all difficulty levels through concise responses. The paper explores various strategies to combine SFT and RL including two-stage, interleaved, progressive training, data mixing, and model merging, but finds that none produce true synergy - instead creating trade-offs in accuracy, reasoning style, and response length. This "synergy dilemma" highlights the need for more adaptive approaches that can balance structured reasoning with generalization across difficulty levels. The work suggests future research should focus on model-compatible dataset construction and adaptive frameworks for difficulty-aware reasoning mode selection.

## Method Summary
The study uses Qwen2.5-VL-7B-Instruct as the base model and systematically evaluates SFT on long reasoning traces versus RL with GRPO. SFT datasets include s1-Gemini2 (1k textual samples from Gemini 2.0 flash), s1.1-R1 (1k textual samples from DeepSeek-R1), and Eureka-Distill (34k multimodal samples from MM-Eureka queries). The RL pipeline uses Verl framework with GRPO algorithm, accuracy and format rewards, and KL regularization (β=0.005). Various combination strategies are tested including two-stage (SFT→RL), interleaved (SFT for hard, RL for rest), progressive (fading external traces), data mixing (243k blended samples), and model merging (Linear/TIES/SLERP). The paper evaluates performance across multiple benchmarks with questions stratified by difficulty based on pass rates across 16 runs.

## Key Results
- SFT improves performance on harder questions through structured reasoning but degrades accuracy on simpler ones due to verbosity injection
- RL provides consistent gains across all difficulty levels through concise responses and KL regularization preventing policy collapse
- No combination strategy (two-stage, interleaved, progressive, data mixing, model merging) produces true synergy - all result in interpolation between parent model performances
- Progressive training is closest to synergy but still sacrifices performance on some benchmarks like MathVerse and MMStar

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Long-CoT SFT improves hard questions but degrades easy ones through verbosity injection.
- Mechanism: SFT on long reasoning traces introduces "reasoning pivotal tokens" (structural words like "first/second," logical connectors like "however/because," meta-thinking like "wait/hmm"). These tokens correlate with performance gains on complex queries requiring multi-step decomposition, but cause overthinking on simple perception-focused questions.
- Core assumption: Performance gains scale with reasoning trace quality, not just length—low-quality verbose traces (e.g., s1-Gemini2) degrade performance despite 8-15× longer responses.
- Evidence anchors:
  - [abstract] "SFT improves performance on difficult questions by in-depth, structured reasoning, but introduces verbosity and degrades performance on simpler ones."
  - [Section 2.1, Tab. 1] s1.1-R1 (1k textual samples) outperforms Eureka-Distill (34k multimodal samples), showing reasoning quality dominates data scale.
  - [corpus] QFFT paper (arXiv:2506.12860) confirms "overthinking generates redundant reasoning steps for simple questions."
- Break condition: When benchmark contains >50% simple perception questions (like MathVista/MMMU), SFT's verbosity penalty outweighs gains.

### Mechanism 2
- Claim: RL with GRPO and KL regularization yields steady cross-difficulty gains through constrained self-exploration.
- Mechanism: GRPO normalizes advantages across sampled outputs, while KL term (β=0.005) prevents policy drift from reference model. Including easy questions (even with zero advantage after normalization) maintains performance via KL loss contribution, preventing catastrophic forgetting.
- Core assumption: Reward signal quality and stability matter more than unconstrained exploration.
- Evidence anchors:
  - [Section 2.2, Tab. 2] KL=0.005 achieves 55.1% avg accuracy vs 52.9% without KL; reward collapses at step 300 without KL.
  - [Section 2.2, Tab. 3] Removing easiest questions drops MathStar from 66.5% to 64.8%.
  - [corpus] Weak direct corpus support for this specific KL mechanism; most related work assumes KL removal is acceptable.
- Break condition: If generation length N < 16k tokens post-SFT, clip ratio spikes to 20% causing training instability (Fig. 11).

### Mechanism 3
- Claim: SFT and RL exhibit fundamental incompatibility—combination strategies produce interpolation, not synergy.
- Mechanism: SFT causes overfitting to verbose reasoning patterns that RL cannot undo. Interleaved training balances but doesn't exceed standalone RL. Progressive training approaches RL performance but sacrifices some benchmarks. Model merging (Linear/SLERP) interpolates accuracy monotonically rather than finding Pareto improvements.
- Core assumption: SFT-induced distribution shift creates a local optimum that RL cannot escape without losing SFT benefits.
- Evidence anchors:
  - [abstract] "Combining them through two-staged, interleaved, or progressive training strategies... fails to produce additive benefits."
  - [Section 3.1] Two-stage SFT→RL stagnates at 51.4% (same as SFT alone); interleaved achieves intermediate performance.
  - [corpus] AceReason-Nemotron 1.1 claims SFT-RL synergy in LLMs, suggesting domain (text-only vs. VLM) is a boundary condition.
- Break condition: Synergy may emerge with adaptive difficulty-aware frameworks (paper's proposal, not validated).

## Foundational Learning
- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: RL backbone that eliminates value model, uses group-normalized advantages for stability.
  - Quick check question: Can you explain why normalizing rewards across sampled outputs (rather than absolute rewards) reduces variance?
- Concept: **KL Divergence Regularization**
  - Why needed here: Critical for preventing policy collapse in RL; controls deviation from reference model.
  - Quick check question: What happens to training dynamics if β=0 during RL fine-tuning?
- Concept: **Chain-of-Thought (CoT) Distillation**
  - Why needed here: SFT relies on reasoning traces from stronger models; quality determines effectiveness.
  - Quick check question: Why might textual-only CoT data transfer to multimodal reasoning tasks?

## Architecture Onboarding
- Component map:
  - SFT Pipeline: LLaMa-Factory → Qwen2.5-VL-7B backbone (frozen ViT + MLP connector) → long-CoT data (s1.1-R1 or Eureka-Distill) → learning rate 1e-5, batch 32
  - RL Pipeline: Verl framework → GRPO algorithm → Eureka-Distill queries → accuracy (0.9) + format (0.1) rewards → KL coefficient 0.005, rollout 8, max length 4k
  - Combination Strategies: Two-stage (SFT→RL), interleaved (SFT for hard, RL for rest), progressive (fade external traces), data mixing (243k blended samples), model merging (Linear/TIES/SLERP)

- Critical path:
  1. Baseline Qwen2.5-VL-7B evaluation → categorize questions by difficulty (pass rate across 16 runs)
  2. SFT on high-quality traces (epoch selection via checkpoint validation)
  3. RL with KL term + easy questions retained
  4. If combining: progressive training is closest to synergy (54.9% vs RL's 55.1%)

- Design tradeoffs:
  - SFT: +hard questions, -easy questions, 10× inference cost (verbosity)
  - RL: +all difficulties (modest), concise responses, less structured reasoning
  - Progressive: near-RL accuracy with some SFT reasoning patterns, but benchmark-specific tradeoffs

- Failure signatures:
  - SFT quality failure: s1-Gemini2 (verbose but low-quality) → accuracy drops across all benchmarks
  - RL instability: KL=0 → reward collapse at step 300, entropy drops, length fluctuations
  - Two-stage overfitting: SFT→RL → clip ratio 20% if max length <16k, no accuracy improvement
  - Model incompatibility: TIES merging → erratic performance; Linear/SLERP only interpolate

- First 3 experiments:
  1. **Difficulty stratification analysis**: Run baseline model 16× on target benchmark, bucket questions by pass rate (L1: ≥75%, L5: <12.5%), measure SFT vs. RL gains per level.
  2. **KL ablation with response monitoring**: Train RL with β∈{0, 0.005, 0.01}, track reward curve, entropy, and response length over 500 steps.
  3. **Progressive training pilot**: Implement fading external traces (full→prefix→none) for zero-pass-rate questions, compare against pure RL on MathVision (hard) and MathStar (easy).

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can adaptive frameworks be developed that dynamically select reasoning modes (structured/verbose vs. concise) based on the inferred difficulty of the input question?
- Basis in paper: [explicit] Section 5 suggests future research should focus on "developing adaptive frameworks capable of accurately identifying problem difficulty, selecting optimal reasoning modes."
- Why unresolved: Current hybrid strategies (interleaved, progressive) force a single behavior or trade-off; the paper demonstrates that static applications of SFT hurt easy questions while RL helps them, and vice-versa for hard questions.
- What evidence would resolve it: A model architecture or routing mechanism that matches SFT-level accuracy on L5 (hardest) questions while retaining RL-level efficiency and accuracy on L1 (easiest) questions.

### Open Question 2
- Question: How can model-compatible or self-distilled long-CoT datasets be constructed to mitigate the data-model incompatibility and catastrophic forgetting observed in SFT?
- Basis in paper: [explicit] Section 5 prioritizes "constructing model-compatible or self-distilled long-CoT datasets to mitigate data-model incompatibility."
- Why unresolved: The paper shows that external distillation (e.g., from DeepSeek-R1) causes "verbosity and overthinking" on simple tasks, while data mixing fails to preserve the strengths of the parent models.
- What evidence would resolve it: A dataset generation pipeline using prompt engineering or in-context learning that produces reasoning traces the base VLM can adopt without the 10x increase in response length or degradation on simple benchmarks.

### Open Question 3
- Question: Why does progressive training, designed to bridge the train-inference mismatch, fail to achieve true synergy and instead result in trade-offs on benchmarks like MathVerse and MMStar?
- Basis in paper: [inferred] Section 3.1 notes that while progressive training approaches RL performance, it "sacrifices performance on MathVerse and MMStar" despite theoretically smoothing the transition from external traces to self-generation.
- Why unresolved: The paper establishes that progressive training is superior to two-stage/interleaved methods but does not explain the mechanism causing the specific performance drops on generalist benchmarks (MMStar) versus math benchmarks.
- What evidence would resolve it: Ablation studies on the fading schedules of external traces that isolate the interference between "slow-thinking" reasoning patterns and the general visual perception required for MMStar.

## Limitations
- The study focuses on a single base model (Qwen2.5-VL-7B-Instruct), making it unclear whether findings extend to larger or differently architected VLMs.
- The sample size of reasoning datasets (1k-34k samples) may limit generalizability across different domains and task complexities.
- Difficulty stratification relies on pass rates across 16 runs, which may not capture the full complexity of reasoning challenges.

## Confidence
**High Confidence**: The core finding that SFT degrades easy question performance while improving hard questions, and that RL provides consistent gains across all difficulties. The mechanism of verbosity-induced overthinking and KL regularization preventing policy collapse is well-supported by ablation studies.

**Medium Confidence**: The claim that combination strategies only interpolate rather than synergize. While the empirical results support this, the search space of combination strategies may not be exhaustive, and the progressive training approach comes close to RL performance.

**Low Confidence**: The assertion that this represents a fundamental incompatibility rather than a limitation of current combination methods. The paper proposes adaptive frameworks as future work, suggesting the authors acknowledge this uncertainty.

## Next Checks
1. **Cross-model validation**: Test the synergy dilemma hypothesis on additional VLM architectures (e.g., LLaVA-NeXT, InternVL) to determine if findings are architecture-dependent.

2. **Dataset quality analysis**: Systematically vary the quality-to-quantity ratio in SFT datasets (e.g., 100 high-quality vs. 10,000 moderate-quality traces) to precisely map the relationship between reasoning trace quality and performance impact.

3. **Adaptive difficulty detection**: Implement and evaluate the proposed adaptive framework that switches between reasoning modes based on difficulty detection, measuring whether this approach achieves true synergy by maximizing accuracy across all difficulty levels simultaneously.