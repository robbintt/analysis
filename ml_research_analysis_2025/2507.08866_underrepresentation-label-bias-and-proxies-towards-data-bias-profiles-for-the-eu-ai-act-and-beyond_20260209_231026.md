---
ver: rpa2
title: 'Underrepresentation, Label Bias, and Proxies: Towards Data Bias Profiles for
  the EU AI Act and Beyond'
arxiv_id: '2507.08866'
source_url: https://arxiv.org/abs/2507.08866
tags:
- bias
- data
- fairness
- label
- underrepresentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study examines three types of data bias\u2014underrepresentation,\
  \ label bias, and proxies\u2014and their combined effects on algorithmic discrimination.\
  \ Through extensive experiments across diverse datasets, models, and fairness metrics,\
  \ the research challenges the conventional emphasis on underrepresentation, finding\
  \ that label bias and proxy effects are often more critical drivers of discrimination."
---

# Underrepresentation, Label Bias, and Proxies: Towards Data Bias Profiles for the EU AI Act and Beyond

## Quick Facts
- arXiv ID: 2507.08866
- Source URL: https://arxiv.org/abs/2507.08866
- Reference count: 35
- Primary result: Data bias profile (DBP) metrics predict discriminatory outcomes and guide targeted fairness interventions

## Executive Summary
This study examines three types of data bias—underrepresentation, label bias, and proxies—and their combined effects on algorithmic discrimination. Through extensive experiments across diverse datasets, models, and fairness metrics, the research challenges the conventional emphasis on underrepresentation, finding that label bias and proxy effects are often more critical drivers of discrimination. The authors introduce the Data Bias Profile (DBP), a quantitative framework for detecting and documenting specific data biases without requiring external unbiased test sets. DBP effectively predicts the risk of discriminatory outcomes and informs targeted fairness interventions. The findings highlight the need for careful label curation and proxy management, while recommending the use of DBP for systematic bias assessment and documentation to support compliance with anti-discrimination regulations and guide the development of fairer AI systems.

## Method Summary
The study proposes a Data Bias Profile (DBP) framework to detect and quantify three types of data bias: underrepresentation (RD), label bias (SD), and proxy strength (sAUC). The method involves injecting controlled biases into datasets and measuring their impact on fairness metrics (DP, EO, PQP). Experiments use 7 datasets with binary protected attributes, testing logistic regression, random forest, and SVM models. Bias injection includes subsampling for underrepresentation, label flipping for measurement bias, and feature manipulation for proxies. The DBP metrics are calculated without external unbiased data, using auxiliary classifiers to predict the protected attribute from features and measure separation in target predictions.

## Key Results
- Label bias and proxy effects, not underrepresentation, are the primary drivers of algorithmic unfairness
- DBP metrics effectively predict the risk of discriminatory outcomes and the utility of fairness-enhancing interventions
- Proxy reduction interventions significantly improve fairness when high proxy strength is detected by DBP
- Controlled label flipping experiments demonstrate that measurement bias has a stronger impact on fairness than demographic imbalance

## Why This Works (Mechanism)
The DBP framework works by quantifying three distinct bias signals that each independently contribute to algorithmic discrimination. Representation Difference (RD) measures demographic imbalance, Separation Difference (SD) captures how well protected groups are separated in the target space (indicating label bias), and sAUC measures the strength of proxy features that allow inference of protected attributes. By decomposing bias into these components, DBP reveals that discrimination often stems from measurement bias in labels or proxy features rather than simple underrepresentation. This allows practitioners to diagnose the root cause and apply targeted interventions like label correction or proxy removal rather than generic approaches.

## Foundational Learning
- **Fairness Metrics (Equal Opportunity, Demographic Parity)**: These metrics measure different aspects of fairness - EO focuses on equal true positive rates between groups while DP measures equal acceptance rates. Understanding their differences is crucial for interpreting which type of bias is most problematic.
  - Quick check: Which metric focuses on the difference in true positive rates between groups? (Answer: Equal Opportunity)

- **Proxy Variables**: A proxy is a non-sensitive feature that allows a model to infer a protected attribute. For example, zip code can serve as a proxy for race. Removing the protected attribute column is often ineffective because other features can reconstruct it.
  - Quick check: Why is removing the protected attribute column from a dataset often ineffective for fairness? (Answer: Because other features can serve as proxies, allowing the model to reconstruct the protected attribute)

- **Label Bias (Measurement Bias)**: Labels in datasets are not always objective ground truth and can reflect historical societal biases. For instance, arrest data may reflect biased policing rather than actual crime rates.
  - Quick check: In a dataset used to predict loan defaults, how might label bias manifest? (Answer: Historically marginalized groups might be mislabeled as higher risk due to biased lending practices, not actual default risk)

## Architecture Onboarding

The Data Bias Profile (DBP) consists of three main analytical components:

Representation Analyzer -> Separation Analyzer -> Proxy Analyzer

1. **Representation Analyzer**: Calculates RD = |Advantaged| - |Disadvantaged| / |Total| to measure underrepresentation
2. **Separation Analyzer**: Trains a classifier on the data and calculates SD, a composite metric derived from within-group and cross-group AUC differences, to measure label bias
3. **Proxy Analyzer**: Trains a classifier to predict the protected attribute from non-protected features and measures sAUC to quantify proxy strength

The critical path focuses on detecting high SD and sAUC values, which indicate label bias and proxy effects respectively. When these metrics are elevated, targeted interventions like proxy removal or label correction should be prioritized over generic approaches like reweighting for underrepresentation.

Design tradeoffs include: simplicity vs. completeness (only measuring three bias types), detectability vs. measurability (choosing metrics computable without external data), and generalizability (findings based on tabular data may not extend to all domains).

Failure signatures include: observing high underrepresentation with low unfairness (check for strong label bias/proxies), fairness interventions failing despite addressing representation (re-profile for other bias types), and high variance in metrics for extremely small minority groups.

First 3 experiments:
1. Replicate key finding by varying underrepresentation factor u on Adult dataset and observing minimal effect on Equal Opportunity compared to baseline
2. Isolate label bias effect by injecting controlled label bias (flip positive labels with factor f=0.5) and measuring resulting drop in fairness
3. Test DBP-guided mitigation by computing DBP for Adult dataset, applying proxy removal intervention, and showing fairness improvement

## Open Questions the Paper Calls Out
- How can definitive, quantitative thresholds be established for DBP metrics (RD, SD, sAUC) to objectively distinguish between mild and excessive data bias?
- How does the inclusion of additional data bias types—specifically feature bias, omitted variable bias, and concept drift—affect the predictive accuracy of the Data Bias Profile?
- Can the Data Bias Profile be natively adapted to handle multi-group protected attributes without resorting to one-vs-all problem transformations?
- How can Data Bias Profiles be utilized to prescribe the most effective fairness-enhancing intervention for a specific dataset?

## Limitations
- The DBP framework is presented as a preliminary construct without extensive validation across real-world deployment scenarios
- Controlled label flipping experiments may not fully capture complex real-world measurement bias
- Focus on single-feature proxies may underestimate the impact of multi-feature proxy effects
- Findings are primarily based on tabular data and may not generalize to other data modalities

## Confidence
- Core finding (label bias > underrepresentation): Medium
- DBP framework effectiveness: Medium
- Experimental methodology: High
- Generalizability to real-world scenarios: Low

## Next Checks
1. Validate DBP framework on a new dataset with known bias characteristics not used in the original study
2. Test the framework's effectiveness in detecting and mitigating bias in a deployed ML system with real-world impact
3. Examine whether the relative importance of bias types holds for non-tabular data (e.g., text, audio) and complex proxy relationships