---
ver: rpa2
title: 'Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process of
  Fast and Slow Thinking'
arxiv_id: '2501.01306'
source_url: https://arxiv.org/abs/2501.01306
tags:
- reward
- thinking
- system
- switch
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of hallucination in large language
  models (LLMs) by proposing HaluSearch, a framework that explicitly models text generation
  as a deliberate reasoning process using tree search algorithms. The key innovation
  is a dynamic system switch mechanism that adaptively alternates between fast and
  slow thinking modes at both the instance and step levels.
---

# Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process of Fast and Slow Thinking

## Quick Facts
- **arXiv ID**: 2501.01306
- **Source URL**: https://arxiv.org/abs/2501.01306
- **Reference count**: 40
- **Primary result**: HaluSearch significantly outperforms baseline methods in mitigating hallucinations and improving response quality on both English and Chinese datasets.

## Executive Summary
This paper addresses the problem of hallucination in large language models (LLMs) by proposing HaluSearch, a framework that explicitly models text generation as a deliberate reasoning process using tree search algorithms. The key innovation is a dynamic system switch mechanism that adaptively alternates between fast and slow thinking modes at both the instance and step levels. The approach frames generation as step-by-step reasoning with MCTS, guided by a self-evaluation reward model. Extensive experiments on both English and Chinese datasets show HaluSearch significantly outperforms baseline methods in mitigating hallucinations and improving response quality.

## Method Summary
HaluSearch reframes text generation as a Markov Decision Process (MDP) solved through Monte Carlo Tree Search (MCTS). The framework employs a hierarchical system switch mechanism that routes queries between fast thinking (direct generation) and slow thinking (MCTS-guided reasoning). A policy model generates candidate responses, while a self-evaluation reward model scores responses for hallucination risk. The switch model determines when to employ each system based on query complexity and generation uncertainty. The method is trained using synthetic data generated by GPT-4 to provide ground truth scores and critiques.

## Key Results
- HaluSearch achieves superior performance over baseline methods on multiple hallucination benchmarks including HaluEval-QA, TruthfulQA, and ChineseFactEval
- The critique-based reward model shows better performance than direct generative reward modeling in Table 2
- The hierarchical switching mechanism successfully balances latency and accuracy trade-offs

## Why This Works (Mechanism)

### Mechanism 1: Tree Search Decoupling of Generation and Selection
- Replacing auto-regressive token prediction with search-guided sentence expansion reduces error accumulation, conditional on the model possessing underlying knowledge
- The framework treats text generation as a Markov Decision Process (MDP) using MCTS with Upper Confidence Bound (UCT) algorithm to balance exploitation and exploration
- Core assumption: The base LLM acts as a valid world model containing correct facts that are merely inaccessible via standard decoding

### Mechanism 2: Critique-Based Reward Modeling
- Explicitly generating a critique before a score improves the precision of the reward signal compared to direct scalar generation
- The reward model is trained to map Response → (Critique, Score), forcing the model to articulate reasoning for penalties
- Core assumption: GPT-4-generated critiques transfer effectively to smaller open-source models for self-evaluation

### Mechanism 3: Hierarchical System Switching
- Decoupling thinking mode into instance-level (query complexity) and step-level (generation uncertainty) switching optimizes latency/accuracy trade-off
- A binary Switch Model acts as a router, rejecting System 2 for simple queries and triggering MCTS only when high hallucination risk is detected
- Core assumption: The Switch Model can reliably predict hallucination risk based on current context without full output generation

## Foundational Learning

- **Concept: Monte Carlo Tree Search (MCTS)**
  - Why needed: This is the engine of the "Slow Thinking" System 2. You must understand how UCT explores the sample space versus exploiting known rewards to grasp how HaluSearch navigates generation paths.
  - Quick check: How does the UCT formula in Section 3.2 differ from standard greedy decoding in handling uncertainty?

- **Concept: Dual Process Theory (System 1 / System 2)**
  - Why needed: The paper borrows cognitive science framing to structure its architecture. Understanding that System 1 is "fast/intuitive" (Direct Gen) and System 2 is "slow/deliberate" (MCTS) is essential for understanding the "Switch" component.
  - Quick check: In the context of this paper, does System 2 imply a larger model or a more complex decoding process?

- **Concept: Reward Modeling (Generative vs. Discriminative)**
  - Why needed: The system relies on a "self-evaluation reward model" to guide the tree search. Distinguishing between a model that outputs a score directly vs. one that generates a critique first is key to the performance gains in Table 2.
  - Quick check: Why might generating a text critique before a numerical score improve the reliability of the reward signal?

## Architecture Onboarding

- **Component map**: Input Query → Instance Switch (Fast vs. Slow?) → [If Slow] → MCTS Loop: Select Node → Step Switch (Expand 1 or K nodes?) → Expansion → Evaluation (Rollout + Reward) → Backprop → Final Output

- **Critical path**: The query flows through the instance switch to determine thinking mode, then through MCTS (if selected) where the step switch determines expansion strategy, followed by expansion, evaluation via reward model, backpropagation, and final output generation.

- **Design tradeoffs**: Latency vs. Accuracy adjusted by switch threshold γ; Critique-based rewards are more accurate but computationally heavier than direct generative scoring.

- **Failure signatures**: Reward Hacking (policy generates plausible but incorrect sentences that RM fails to penalize), Search Collapse (UCT weight too low causing exploitation trap), Switch Failure (high latency on simple questions).

- **First 3 experiments**:
  1. Replicate Table 2 on held-out set comparing "Generative RM" vs. "Generative RM + Critic" correlation with human ground truth
  2. Replicate Figure 2, plotting Time vs. Accuracy by varying threshold γ (3, 4, 5) to find optimal operating point
  3. Test ablation on rollouts by changing number of rollouts m in Section 4.5 to see if performance saturates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can HaluSearch effectively mitigate hallucinations in open-ended, long-form generation tasks outside of question answering?
- Basis: Experimental evaluation is conducted exclusively on question-answering datasets (e.g., HaluEval-QA, TruthfulQA), framing reasoning strictly as sentence-level steps toward a specific answer
- Why unresolved: Efficacy of sentence-level MCTS reward signal has not been validated on tasks requiring creative or extensive generation where "correctness" is less discrete
- What evidence would resolve it: Evaluation results on long-form generation benchmarks (e.g., summarization or essay writing) showing sustained hallucination reduction

### Open Question 2
- Question: Is it possible to train a highly accurate self-evaluation reward model without relying on GPT-4 for data synthesis?
- Basis: Section 3.3 states "To obtain diverse reward data... we employ GPT-4 to give a reward score to these completed responses," relying on a stronger model for ground truth
- Why unresolved: The paper demonstrates a method for self-evaluation, but training of that evaluator is currently dependent on a superior proprietary model (GPT-4)
- What evidence would resolve it: Experiments using human-annotated rewards or weaker models to synthesize data, achieving comparable performance to GPT-4 distilled version

### Open Question 3
- Question: Does the performance gain from slow thinking diminish as the parameter size and inherent capability of the policy model increase?
- Basis: Section 4.5 notes that performance gains decrease as expanded nodes/simulations increase due to "inherent limitations of the internal knowledge of the policy model"
- Why unresolved: While the method works for 7B/8B models, it's unclear if scaling inference-time compute via MCTS provides significant value over simply using a much larger base model with stronger internal knowledge
- What evidence would resolve it: Comparative analysis of HaluSearch applied to models of varying sizes (e.g., 8B vs. 70B) to measure marginal utility of tree search

## Limitations
- The paper relies on GPT-4 for synthetic training data, creating dependency on a proprietary model for the reward model training process
- Computational cost of MCTS remains significant even with the switching mechanism, potentially limiting practical deployment
- Performance gains diminish as policy model size increases, suggesting the approach may be most valuable for mid-sized models rather than state-of-the-art systems

## Confidence

- **High Confidence**: The core architectural design of using MCTS for deliberate reasoning is well-founded and empirical improvements over baselines are statistically significant across multiple datasets
- **Medium Confidence**: The specific implementation details for the hierarchical switching mechanism, particularly the step-level threshold calibration, lack sufficient experimental validation
- **Low Confidence**: The paper claims general applicability but results are primarily demonstrated on English and Chinese QA tasks, with limited exploration of other domains where hallucination patterns differ significantly

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically vary the switch threshold γ from 1 to 5 and measure impact on both latency and hallucination rate to identify optimal operating points for different latency budgets

2. **Cross-Domain Generalization**: Test HaluSearch on non-QA tasks like creative writing, code generation, or medical diagnosis to verify framework's effectiveness across domains with different hallucination patterns

3. **Resource-Constrained Evaluation**: Implement HaluSearch on edge devices or with reduced computation budgets to validate claimed latency improvements hold under practical deployment constraints