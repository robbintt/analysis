---
ver: rpa2
title: Dynamic Sparse Training of Diagonally Sparse Networks
arxiv_id: '2506.11449'
source_url: https://arxiv.org/abs/2506.11449
tags:
- sparse
- sparsity
- training
- dynadiag
- diagonal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DynaDiag, a dynamic sparse training method
  that enforces diagonal sparsity patterns to achieve both high accuracy and practical
  GPU acceleration. Unlike unstructured sparsity, which lacks hardware speedup, DynaDiag
  maintains sparse forward and backward passes with a structured diagonal pattern.
---

# Dynamic Sparse Training of Diagonally Sparse Networks

## Quick Facts
- arXiv ID: 2506.11449
- Source URL: https://arxiv.org/abs/2506.11449
- Authors: Abhishek Tyagi; Arjun Iyer; William H Renninger; Christopher Kanan; Yuhao Zhu
- Reference count: 40
- Primary result: Achieves up to 3.13× inference and 1.59× training speedup on GPUs at 90% sparsity while maintaining accuracy comparable to unstructured sparsity methods.

## Executive Summary
This paper introduces DynaDiag, a dynamic sparse training method that enforces diagonal sparsity patterns to achieve both high accuracy and practical GPU acceleration. Unlike unstructured sparsity which lacks hardware speedup, DynaDiag maintains sparse forward and backward passes with a structured diagonal pattern. The method uses a differentiable TopK-based approach to dynamically select and optimize diagonal positions during training, converting diagonal matrices to GPU-efficient BCSR format for acceleration.

## Method Summary
DynaDiag implements a dynamic sparse training framework where diagonal sparsity masks are learned during training rather than pre-defined. The method introduces a learnable importance vector α that parameterizes diagonal selection through a differentiable TopK mechanism. Selected diagonals are placed into sparse weight matrices which are then converted to BCSR format for efficient GPU computation. The approach maintains sparse operations throughout both forward and backward passes while preserving theoretical properties like universal approximation.

## Key Results
- Achieves up to 3.13× inference speedup and 1.59× training speedup on GPUs at 90% sparsity
- Maintains accuracy comparable to unstructured methods like RigL across vision and language tasks
- Demonstrates robustness at extreme sparsity levels and benefits from LoRA-FA fine-tuning to surpass unstructured sparsity performance
- Shows theoretical preservation of universal approximation properties and full input-output coverage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Differentiable TopK-based selection learns optimal diagonal placements dynamically during training.
- Mechanism: A learnable importance vector $\alpha$ parameterizes each possible diagonal. A temperature-controlled softmax-based TopK function selects the K highest-scoring diagonals, allowing gradients to flow back through the selection process and update $\alpha$. This enables the network to "search" for the most beneficial diagonal positions for the specific task.
- Core assumption: The gradient signal from the loss function provides a reliable proxy for the "importance" of a diagonal's contribution to the task.
- Evidence anchors:
  - [abstract]: "...uses a differentiable TopK-based approach to dynamically select and optimize diagonal positions during training..."
  - [section 3.2]: "To determine which diagonals contribute the most to the final matrix WK, we introduce a learnable vector of importance weights α... We use a TopK function to select the K most significant diagonals..."
  - [corpus]: Weak or missing. Related work on dynamic sparse training (e.g., RigL, CHT) uses gradient or magnitude-based heuristics, but a directly learnable diagonal selection via differentiable TopK is the specific contribution here.
- Break condition: The TopK approximation becomes too crude (e.g., K is very small), or the temperature schedule prevents convergence to a stable set of diagonals, making the optimization process unstable.

### Mechanism 2
- Claim: Diagonal sparsity patterns preserve network expressivity by ensuring full input-output coverage and rank potential.
- Mechanism: Unlike block sparsity where a row or column can be entirely zeroed out, each selected diagonal is guaranteed to have a non-zero entry in every row and column it intersects. This ensures no neuron is completely disconnected and maintains the network's ability to propagate information and gradients.
- Core assumption: Universal approximation properties, which require every input to potentially influence every output, are sufficiently preserved by this connectivity.
- Evidence anchors:
  - [abstract]: "Theoretical analysis supports that diagonal sparsity preserves universal approximation properties and full input-output coverage..."
  - [section B, Theorem 2]: "A feed-forward neural network consisting of layers employing diagonal sparsity masks... retains the universal approximation property."
  - [corpus]: Weak or missing. The cited corpus neighbors discuss small-world networks or structured sparsity generally, but do not provide specific theoretical proof regarding the universal approximation properties of diagonal sparsity.
- Break condition: A conflicting or more restrictive sparsity pattern is introduced that violates full input-output coverage (e.g., non-diagonal structured masks), or the theory's assumptions about depth/width are not met.

### Mechanism 3
- Claim: Diagonal-to-BCSR conversion translates a theoretically sound but hard-to-accelerate pattern into a GPU-efficient format.
- Mechanism: The unstructured diagonals are reordered and grouped into contiguous, dense blocks in memory using a Block Compressed Sparse Row (BCSR) format. This clustering allows standard sparse matrix multiplication kernels (like those optimized for Tensor Cores) to operate on dense sub-blocks, significantly reducing memory access overhead and improving computational throughput.
- Core assumption: The overhead of converting the diagonal format to BCSR and managing the index structures is lower than the computational gains from using the accelerated kernel.
- Evidence anchors:
  - [abstract]: "...converting diagonal matrices to GPU-efficient BCSR format for acceleration."
  - [section 3.3]: "Although finding the optimal block-minimizing permutation from diagonals to BCSR is NP-hard, various heuristics have been developed to cluster non-zero values... Our approach yields training acceleration by leveraging the same diagonal-to-BCSR conversion for WK and W^T K."
  - [corpus]: Weak or missing. Related papers discuss general structured sparsity for acceleration but do not detail the specific diagonal-to-BCSR conversion heuristic or its performance analysis as described.
- Break condition: The cost of the online conversion or the index structure storage negates the kernel speedup, especially at lower sparsities where there is less opportunity for acceleration.

## Foundational Learning

- Concept: **Dynamic Sparse Training (DST)**
  - Why needed here: DynaDiag is a DST method. You must understand the core principle of starting sparse and dynamically updating the mask during training, differentiating it from static sparse training and pruning.
  - Quick check question: How does a DST method update a network's sparse mask during training, and how does this differ from pruning?

- Concept: **Block Compressed Sparse Row (BCSR) Format**
  - Why needed here: This is the key data structure for GPU acceleration. Understanding how it stores sparse matrices in dense blocks to enable efficient vectorized operations is crucial for the hardware implementation.
  - Quick check question: In BCSR, how are non-zero elements stored to optimize for parallel access on a GPU, and what information must be stored alongside the values?

- Concept: **Differentiable TopK**
  - Why needed here: The core algorithm uses a differentiable TopK to learn diagonal placements. You need to understand how a normally non-differentiable discrete operation (like TopK) can be made smooth for gradient-based optimization.
  - Quick check question: Why is a standard TopK operation non-differentiable, and what is the general strategy to create a differentiable approximation for use in a neural network?

## Architecture Onboarding

- Component map: Input tensor -> Sparse Layer -> alpha -> TopK Selector -> mask -> Matrix Constructor -> sparse W -> Diagonal-to-BCSR Converter -> BCSR W -> Custom CUDA Kernel -> Output tensor
- Critical path:
  1. Forward Pass: Input tensor -> Sparse Layer -> `alpha` -> TopK Selector -> `mask` -> Matrix Constructor -> `sparse W` -> Diagonal-to-BCSR Converter -> `BCSR W` -> Custom CUDA Kernel (with input tensor) -> Output tensor.
  2. Backward Pass: Gradients flow from the loss back through the Custom CUDA Kernel (computing sparse gradients) -> Matrix Constructor -> TopK Selector (updating `alpha` and `V`). The transpose of the weight matrix also uses the BCSR conversion for efficient gradient computation.
- Design tradeoffs:
  - **Sparsity vs. Speedup**: Higher sparsity leads to greater potential speedups but risks accuracy degradation. The paper notes speedup benefits are most pronounced at >80% sparsity (Fig. 4).
  - **Block Size for BCSR**: Smaller blocks are more flexible for fitting diagonals but have more overhead. Larger blocks are faster but may include more zeros, wasting computation. The optimal size is hardware-dependent.
  - **Temperature Schedule**: A faster decay in temperature focuses exploitation earlier but may get stuck in a suboptimal mask. A slower decay encourages exploration but takes longer to converge to a stable structure.
- Failure signatures:
  - **Slow Convergence**: Training loss plateaus or decreases very slowly. This may indicate the temperature schedule is too aggressive or the learning rate for α is too low.
  - **No Speedup**: Wall-clock time does not decrease with sparsity. This indicates a problem in the CUDA kernel implementation or excessive overhead in the Python-side BCSR conversion.
  - **Layer Collapse**: Model accuracy drops to chance level. This could mean the TopK selector is choosing a degenerate mask (e.g., all zeros) due to a bug or poor hyperparameters.
- First 3 experiments:
  1. **Sanity Check on Small Task**: Implement the DynaDiag layer and train a small MLP on MNIST or CIFAR-10. Verify that loss decreases and accuracy is comparable to a dense baseline, confirming the core algorithm works.
  2. **Kernel Micro-Benchmark**: Isolate the custom CUDA kernel. Benchmark matrix multiplication of a single diagonal-sparse matrix (converted to BCSR) against a dense baseline across different sizes and sparsity levels. This validates the claimed speedup independent of the training loop.
  3. **Ablation on Temperature Schedule**: Run the full training (e.g., on CIFAR-10) with different temperature schedules (Constant, Linear, Cosine) while keeping other hyperparameters fixed. Analyze the resulting accuracy and stability of the learned mask to understand the importance of this hyperparameter.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can DynaDiag be effectively extended to CNN architectures given the channel-wise overhead of searching for distinct diagonal patterns?
- Basis in paper: [explicit] The authors state "DynaDiag faces scalability challenges with CNNs due to the overhead of searching for distinct diagonal patterns across each channel."
- Why unresolved: The current formulation optimizes diagonal placement per weight matrix, but convolutions require handling multiple channels with spatial dimensions, creating a combinatorial search problem not addressed in this work.
- What evidence would resolve it: A modified DynaDiag implementation for CNNs (e.g., ResNet, ConvNeXt) showing comparable accuracy-speedup trade-offs to those demonstrated for transformers.

### Open Question 2
- Question: What is the minimum number of active diagonals required to preserve model expressivity at extreme sparsity levels approaching 99.99% or higher?
- Basis in paper: [explicit] The authors note they "aim to extend our method to networks with extremely sparse weight matrices—e.g., at sparsity levels approaching 99.9999% or more—while still retaining more than a single active diagonal."
- Why unresolved: Lemma 1 only guarantees input-output coverage with k≥1 diagonals, but expressivity at extreme sparsities remains empirically uncharacterized beyond 99.99%.
- What evidence would resolve it: Systematic ablation studies varying diagonal count at extreme sparsities, with theoretical bounds on expressivity preservation.

### Open Question 3
- Question: Can Triton-based GPU kernels provide further speedup improvements over the current custom CUDA implementation?
- Basis in paper: [explicit] The authors acknowledge that "our method's performance could be further improved through optimized GPU implementations of diagonal sparsity using frameworks like Triton."
- Why unresolved: The current implementation uses custom CUDA with BCSR conversion; Triton may enable better kernel fusion, memory coalescing, or tensor core utilization patterns specific to diagonal sparsity.
- What evidence would resolve it: Benchmarks comparing Triton-optimized diagonal sparse kernels against current CUDA kernels across sparsity levels and matrix sizes.

### Open Question 4
- Question: How does the performance gap between DynaDiag and unstructured sparsity methods (e.g., RigL) evolve with model scale beyond the ViT-H/GPT-2-Medium sizes tested?
- Basis in paper: [inferred] The authors demonstrate results on ViT-H/14 and GPT-2-Medium but note the gap with RigL varies by sparsity level, suggesting scale-dependent behavior that remains unexplored for larger LLMs.
- Why unresolved: Scaling laws for structured vs. unstructured sparsity are not established, and it is unclear whether diagonal sparsity's expressivity limitations become more or less pronounced at billion-parameter scales.
- What evidence would resolve it: Experiments training LLMs (e.g., LLaMA-7B, 13B) from scratch with DynaDiag, comparing against unstructured DST baselines.

## Limitations
- The BCSR conversion relies on heuristics for an NP-hard optimization problem, potentially limiting the achievable speedup in practice
- The temperature-controlled TopK approximation may converge to suboptimal diagonal selections, particularly at extreme sparsity levels
- The proposed diagonal sparsity pattern may not be optimal for all network architectures

## Confidence

**High Confidence**: The hardware acceleration claims are well-supported by established GPU kernel optimization principles and the use of BCSR format. The universal approximation property preservation has theoretical backing from the paper's own analysis.

**Medium Confidence**: The dynamic diagonal selection mechanism works as described, but the quality of the learned mask relative to other DST methods needs more extensive benchmarking. The LoRA-FA fine-tuning results show promise but lack ablation studies.

**Low Confidence**: The paper's claims about the method's superiority over unstructured sparsity at extreme sparsities (>90%) are based on limited experimental validation. The theoretical analysis of approximation properties assumes certain depth/width conditions that may not hold in practical settings.

## Next Checks
1. Benchmark DynaDiag against unstructured DST methods (RigL, SET) on larger-scale tasks (e.g., ImageNet, LLMs) to validate relative performance claims across a wider range of sparsity levels.

2. Conduct an ablation study on the BCSR conversion heuristic: compare different clustering strategies and measure their impact on both accuracy and speedup to identify the most effective approach.

3. Test the method's robustness to initialization and hyperparameter sensitivity by running multiple trials with different random seeds and analyzing the variance in final accuracy and learned diagonal patterns.