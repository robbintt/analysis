---
ver: rpa2
title: Deontically Constrained Policy Improvement in Reinforcement Learning Agents
arxiv_id: '2506.06959'
source_url: https://arxiv.org/abs/2506.06959
tags:
- policy
- action
- algorithm
- stit
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a policy improvement algorithm for reinforcement\
  \ learning agents subject to deontic constraints expressed in probabilistic computation\
  \ tree logic (PCTL). The algorithm maximizes the agent\u2019s mission utility while\
  \ ensuring compliance with normative constraints by integrating PCTL model checking\
  \ into each policy update step."
---

# Deontically Constrained Policy Improvement in Reinforcement Learning Agents

## Quick Facts
- **arXiv ID**: 2506.06959
- **Source URL**: https://arxiv.org/abs/2506.06959
- **Reference count**: 37
- **Primary result**: Presents policy improvement algorithm for RL agents subject to deontic constraints expressed in PCTL, converging to locally optimal feasible policies

## Executive Summary
This paper introduces a policy improvement algorithm for reinforcement learning agents that must satisfy deontic (normative) constraints expressed in probabilistic computation tree logic (PCTL) while maximizing mission utility. The algorithm integrates PCTL model checking into each policy update step, computing reachability probabilities to identify valid actions that maintain constraint satisfaction. By balancing exploration and exploitation through epsilon-greedy selection, the method converges to a locally optimal policy that satisfies the PCTL formula. Experimental results on sample MDPs demonstrate that controlled exploration enables escape from local optima to reach globally optimal solutions.

## Method Summary
The algorithm finds policies that maximize expected discounted reward while satisfying PCTL constraints through iterative refinement. At each iteration, it evaluates the current policy, computes reachability probabilities using Prob0/Prob1 algorithms and linear systems, determines valid actions that maintain constraint satisfaction, and updates the policy via epsilon-greedy selection from the feasible action set. The method handles constrained reachability efficiently and uses exploration to escape local optima, though it guarantees only local optimality rather than global optimality.

## Key Results
- Controlled exploration (epsilon > 0) enables escape from local optima to reach globally optimal solutions
- Epsilon = 0 converges to local optima, as demonstrated on MDP2 grid navigation
- The algorithm maintains constraint satisfaction through PCTL probability vectors and action pruning
- Experimental results show 0.05-0.15s runtime on small MDPs

## Why This Works (Mechanism)

### Mechanism 1: Feasibility Filtering via PCTL Probability Vectors
The algorithm maintains constraint satisfaction by restricting policy updates to actions that preserve a sufficient probability of satisfying the PCTL formula. At each iteration t, it computes a vector x where x_s represents the probability of satisfying the temporal logic formula from state s. Before selecting a new action, it calculates the expected satisfaction probability Y(s, a) for available actions. Actions are pruned from the candidate set C(s) if Y(s, a) falls below the threshold λ. The core assumption is that the probability of satisfying the PCTL formula can be accurately modeled as a linear system within the Markov chain induced by the current policy.

### Mechanism 2: Bi-Level Value Maximization (Explicit vs. Implicit)
The agent maximizes explicit mission utility while implicitly adhering to a deontic utility derived from stit (seeing-to-it-that) logic. The mission is encoded in standard reward R, while the norm is encoded in PCTL. The algorithm performs constrained maximization of V^π(s) over the set of policies that act optimally regarding the implicit deontic constraints. The core assumption is that deontic obligations can be derived from the dominance relations of expected utility in the stit model translation.

### Mechanism 3: Escape from Local Optima via Epsilon-Greedy Exploration
Controlled randomness allows the policy to escape local utility peaks that strict greedy improvement would trap it in. Standard policy improvement (with epsilon=0) converges provably to a local optimum. By introducing epsilon > 0, the algorithm occasionally accepts a "worse" valid action (lower Q-value). This stochasticity allows the state distribution to shift sufficiently to discover a globally superior, constraint-satisfying policy trajectory. The core assumption is that the MDP structure permits traversal from the basin of attraction of a local optimum to that of a global optimum via single-step deviations.

## Foundational Learning

- **Probabilistic Computation Tree Logic (PCTL)**: The language used to define deontic constraints (e.g., "probability of reaching goal while avoiding hazard ≥ 0.9"). Understanding the syntax (Until, Eventually) is required to define the "valid action" sets. Quick check: Can you distinguish between the path formula φ₁ U φ₂ and the state formula P≥λ(φ)?

- **Markov Decision Processes (MDPs) & Value Iteration**: The "mission" is a standard MDP reward maximization problem. The algorithm builds upon standard Bellman optimality and Q-functions. Quick check: How does adding a constraint change the standard Bellman update equation? (Hint: It restricts the argmax domain).

- **Stit Logic & Expected Act Utilitarianism (EAU)**: Provides the theoretical justification for why a constraint is an "obligation." It translates the MDP into a "stit model" where optimal strategies correspond to "duties." Quick check: In this framework, is an action "optimal" because it follows the rule, or is the rule defined by what optimal actions achieve?

## Architecture Onboarding

- **Component map**: MDP inputs -> PCTL Solver (computes Prob0/Prob1 and probability vector x_s) -> Constraint Filter (computes valid action set C(s)) -> Q-Selector (evaluates Q(s,a)) -> Policy Updater (selects a* via epsilon-greedy) -> Updated policy

- **Critical path**: The loop is dominated by the PCTL probability calculation (solving linear equations for x_s) at every policy evaluation step, which is more expensive than standard value iteration alone.

- **Design tradeoffs**:
  - Constraint Complexity: Current implementation handles constrained reachability efficiently; complex nested PCTL formulas require recursive abstraction, increasing computational overhead
  - Optimality: Trades guaranteed global optimality for guaranteed feasibility (local optimality). Global optimality is heuristic (dependent on epsilon)

- **Failure signatures**:
  - Empty Candidate Set: If C(s) = ∅, the constraint is unsatisfiable from state s. Check if lambda is too strict or if the initial policy was poorly chosen
  - Cyclic Policies: With high epsilon, the policy may never stabilize
  - Sub-optimal Convergence: With epsilon=0, the agent stops improving despite clear better paths

- **First 3 experiments**:
  1. Validation on MDP1: Run with epsilon=0 on 4-state MDP1 to verify convergence to known global optimum
  2. Local Optima Stress Test: Run on MDP2 with epsilon=0 to confirm getting stuck in suboptimal "North" vs "West" choice
  3. Exploration Sweep: Run grid search over epsilon ∈ [0.1, 0.5] on MDP2 and plot frequency of reaching global optimum

## Open Questions the Paper Calls Out

### Open Question 1
Can the constrained policy improvement framework be extended to guarantee convergence to globally optimal policies rather than local optima? The algorithm provably converges only to locally optimal policies. The epsilon-greedy exploration helps empirically but amounts to "little more than a brute force approach" asymptotically. Evidence to resolve: A modified algorithm with provable global optimality guarantees, or a proof that the constrained optimization problem structure admits efficient global optimization.

### Open Question 2
Can the methods be extended to handle partially observable MDPs (POMDPs)? The current formulation assumes full state observability for PCTL model checking and policy evaluation. POMDPs require belief-state reasoning. Evidence to resolve: An algorithm that maintains belief distributions, performs PCTL model checking over belief MDPs, and converges under partial observability.

### Open Question 3
Can normative conflicts between multiple deontic constraints be detected and resolved within this framework? The current approach handles single PCTL constraints. Multiple constraints may be mutually unsatisfiable or define an empty feasible policy space. Evidence to resolve: A method for detecting unsatisfiable constraint combinations and mechanisms for prioritization or trade-offs between conflicting obligations.

### Open Question 4
Do the impossibility results for encoding LTL objectives as Markovian rewards extend to PCTL formulas? The paper states "strictly speaking it is not known whether these results extend to PCTL formulas (though we suspect they do)." LTL and PCTL have incomparable expressiveness, so existing LTL impossibility proofs do not directly transfer. Evidence to resolve: A formal proof showing PCTL formulas cannot be captured by scalar reward functions, or a constructive counterexample demonstrating successful translation.

## Limitations
- Computational overhead of solving linear systems for reachability probabilities at every iteration scales poorly with MDP size
- Limited experimental validation on only two small MDPs (4 and 6 states) makes scalability assessment difficult
- Epsilon-greedy exploration provides heuristic global optimization but lacks theoretical guarantees beyond local optimality

## Confidence

- **High Confidence**: The basic PCTL model checking mechanism (Prob0/Prob1, linear system solving) is well-established in the literature and correctly applied
- **Medium Confidence**: The policy improvement algorithm correctly maintains feasibility through action pruning, though the interaction between exploration noise and constraint satisfaction could be more rigorously analyzed
- **Medium Confidence**: The empirical demonstration that epsilon > 0 escapes local optima is convincing for the tested MDPs, but generalization to larger problems is uncertain
- **Low Confidence**: The theoretical claim that satisfying the PCTL formula corresponds to acting optimally according to an implicit deontic utility lacks sufficient justification in the paper

## Next Checks

1. **Scalability Test**: Implement the algorithm on MDPs with 50-100 states to measure how computation time scales with state space size and PCTL formula complexity. Track whether the linear system solving remains tractable.

2. **Constraint Tightness Analysis**: Systematically vary lambda from 0.1 to 0.9 on MDP2 to identify the threshold where C(s) becomes empty for certain states. This would reveal the practical limits of constraint strictness.

3. **Global Optimality Bound**: For MDP1, exhaustively enumerate all feasible policies (those satisfying P≥0.4(F s3)) and compare their utilities against the algorithm's output across different epsilon values. This would quantify how close the heuristic exploration gets to the true global optimum.