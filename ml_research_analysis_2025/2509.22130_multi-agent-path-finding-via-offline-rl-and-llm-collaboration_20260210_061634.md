---
ver: rpa2
title: Multi-Agent Path Finding via Offline RL and LLM Collaboration
arxiv_id: '2509.22130'
source_url: https://arxiv.org/abs/2509.22130
tags:
- agents
- arxiv
- learning
- multi-agent
- environments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses Multi-Agent Path Finding (MAPF), a critical
  problem in robotics and logistics characterized by combinatorial complexity and
  partial observability. The authors propose a novel decentralized approach using
  Decision Transformer (DT) in an offline reinforcement learning setup, which significantly
  reduces training time from weeks to hours.
---

# Multi-Agent Path Finding via Offline RL and LLM Collaboration

## Quick Facts
- arXiv ID: 2509.22130
- Source URL: https://arxiv.org/abs/2509.22130
- Reference count: 10
- Key outcome: Novel decentralized MAPF approach combining offline RL via Decision Transformer with GPT-4o guidance achieves competitive performance in static scenarios and superior makespan/collision rates in dynamic environments.

## Executive Summary
This paper addresses the Multi-Agent Path Finding (MAPF) problem using a novel decentralized approach that combines offline reinforcement learning via Decision Transformer with LLM guidance from GPT-4o. The authors demonstrate that Decision Transformer can efficiently learn from expert trajectories generated by ODrM*, reducing training time from weeks to hours while achieving competitive success rates and sum-of-costs. For dynamic scenarios where agent goals change during execution, the integration of GPT-4o provides rapid real-time adjustments, directly navigating agents toward newly designated goals and achieving lower makespan and collision rates compared to DT alone. The method handles long-horizon credit assignment in sparse-reward environments and adapts to environmental changes without complex communication modules.

## Method Summary
The approach uses Decision Transformer trained offline on expert trajectories generated by ODrM* across diverse grid configurations. Each agent maintains a 10×10 local field of view with 4 observation channels (neighboring agents, own goal, neighbors' goals, obstacles). The DT policy learns return-conditioned behavior from trajectory chunks of length 50. For dynamic scenarios, GPT-4o intervenes for 5 timesteps after goal changes using Chain-of-Thought prompting with in-context examples, then reverts to DT control. The system is evaluated on static Pogema Random/Maze maps and dynamic goal-change scenarios across multiple grid sizes.

## Key Results
- DT training reduced from weeks to ~3 hours while achieving competitive CSR and sum-of-costs vs. PRIMAL, DCC, SCRIMP baselines
- DT+GPT-4o collaboration achieves better makespan and collision rates in dynamic scenarios with 25% and 50% goal changes
- Offline RL approach handles long-horizon credit assignment effectively in sparse-reward MAPF environments
- Decentralized architecture eliminates need for complex communication modules between agents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Offline RL via Decision Transformer enables efficient credit assignment in sparse-reward MAPF scenarios
- Mechanism: DT frames RL as sequence modeling over trajectories, using transformer attention to capture long-range dependencies and associate delayed terminal rewards with earlier actions without explicit value propagation
- Core assumption: Expert trajectories from ODrM* provide sufficient coverage to avoid severe distributional shift at test time
- Evidence anchors: Training time reduction from weeks to hours; transformer architecture addresses credit assignment in long-horizon scenarios
- Break condition: Significant deviation from training grid distributions causes policy failure due to out-of-distribution states

### Mechanism 2
- Claim: Brief LLM intervention during environmental changes reduces makespan and collision rates by providing global situational awareness
- Mechanism: GPT-4o receives full environment state during goal changes and generates coordinated actions via CoT prompting, bypassing DT's exploration phase near old goals
- Core assumption: GPT-4o's spatial reasoning over grid coordinates is reliable enough to produce collision-free guidance for 5 timesteps
- Evidence anchors: DT requires several timesteps to comprehend goal changes; GPT-4o intervention ensures rapid real-time adjustments
- Break condition: LLM hallucination or incorrect spatial reasoning produces invalid actions requiring fallback validation

### Mechanism 3
- Claim: Decentralized DT agents achieve competitive performance without complex inter-agent communication modules
- Mechanism: 10×10 local observation with goal-direction encoding provides implicit coordination signals; DT learns cooperative behaviors from expert demonstrations without explicit message passing
- Core assumption: Local FOV combined with goal encoding is sufficient for agents to anticipate and avoid conflicts without real-time communication
- Evidence anchors: Decentralized approach eliminates reliance on complex communication modules; improves performance in sparse, delayed-reward scenarios
- Break condition: Dense multi-agent scenarios with limited FOV may cause conflicts agents cannot anticipate

## Foundational Learning

- **Concept: Decision Transformer (DT) Architecture**
  - Why needed here: DT is the core policy backbone; understanding return-conditioned sequence modeling explains how offline RL avoids iterative value estimation
  - Quick check question: Given a trajectory segment with return-to-go decreasing over time, how does DT infer the next action?

- **Concept: Offline RL vs. Online RL**
  - Why needed here: The paper's primary efficiency claim rests on offline training; understanding distributional shift risk is critical for evaluating generalization
  - Quick check question: What happens if the test-time state distribution diverges from the offline dataset? How does the paper attempt to mitigate this?

- **Concept: Dec-POMDP Formulation**
  - Why needed here: MAPF is formalized as a Dec-POMDP; understanding partial observability, joint vs. individual policies, and the credit assignment problem is essential
  - Quick check question: In a Dec-POMDP, why is credit assignment harder when rewards are sparse and delayed?

## Architecture Onboarding

- **Component map:** ODrM* -> Expert Trajectory Generator -> DT with Conv Encoder -> Execution Loop; GPT-4o Intervention Module -> Execution Loop
- **Critical path:** Dataset quality/diversity -> Observation encoding fidelity -> Prompt engineering for GPT-4o -> Intervention timing
- **Design tradeoffs:** Offline data coverage vs. distributional shift; LLM intervention frequency vs. latency/consistency; FOV size vs. partial observability constraints
- **Failure signatures:** DT oscillation/failure to reach goals within episode limit; GPT-4o suggests invalid actions; collision rate spikes in dense environments
- **First 3 experiments:**
  1. Train DT on collected expert trajectories; evaluate on static Pogema Random/Maze maps; compare CSR and sum-of-costs vs. benchmarks
  2. In 20×20, 40×40, 80×80 grids with 25%/50% goal changes, compare DT-only vs. DT+GPT-4o; measure success rate, makespan, collision rate
  3. Vary GPT-4o intervention window (1, 3, 5, 10 timesteps) after goal change; identify minimum effective intervention length

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the framework be effectively adapted to utilize Visual Language Models (VLMs) with visual inputs instead of textual grid encodings? The authors explicitly list this as a promising future direction.

- **Open Question 2:** How does the inference latency of the LLM component impact the viability of this approach for real-time robotic deployment? The paper focuses on training efficiency and episode success metrics rather than wall-clock decision times.

- **Open Question 3:** What specific failure modes arise from LLM hallucinations, and how can the system mitigate incorrect strategic guidance? The paper acknowledges hallucination risk but does not isolate failure cases caused by erroneous LLM advice.

## Limitations

- **Distributional Shift Risk:** Performance depends critically on expert trajectory coverage; no quantitative analysis of generalization gaps to unseen configurations
- **LLM Intervention Reliability:** Acknowledges hallucination risk but provides no empirical quantification of failure rates or fallback mechanisms
- **Local FOV Constraints:** 10×10 field of view may cause performance degradation in dense multi-agent scenarios where limited lookahead prevents conflict anticipation

## Confidence

- **High Confidence:** Offline RL training efficiency (weeks→hours), baseline DT performance on static scenarios, core architectural design
- **Medium Confidence:** DT+GPT-4o collaboration benefits in dynamic scenarios, LLM intervention effectiveness
- **Low Confidence:** Generalization across diverse environments, LLM failure rate quantification, dense scenario performance

## Next Checks

1. **Distributional Shift Analysis:** Systematically evaluate DT performance degradation when testing on configurations outside training distribution; measure success rate and sum-of-costs across incremental deviations

2. **LLM Error Quantification:** Instrument GPT-4o interventions to log invalid action rates and spatial reasoning failures; compare performance with and without output validation/parsing guards

3. **FOV Sensitivity Study:** Evaluate performance across different observation window sizes (5×5, 10×10, 15×15) in dense multi-agent scenarios to quantify tradeoff between local observability and computational constraints