---
ver: rpa2
title: 'MK2 at PBIG Competition: A Prompt Generation Solution'
arxiv_id: '2507.08335'
source_url: https://arxiv.org/abs/2507.08335
tags:
- patent
- product
- evaluation
- prompt
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes MK2, a prompt-centric pipeline for converting
  patents into commercially viable product ideas. MK2 uses iterative prompt optimization
  via Gemini 2.5 and GPT-4.1 generation, with an Elo-based internal evaluation loop
  using Qwen3-8B.
---

# MK2 at PBIG Competition: A Prompt Generation Solution

## Quick Facts
- arXiv ID: 2507.08335
- Source URL: https://arxiv.org/abs/2507.08335
- Reference count: 32
- Primary result: Lightweight prompt engineering pipeline won 25/36 tests on PBIG automatic leaderboard

## Executive Summary
MK2 is a prompt-centric pipeline for converting patents into commercially viable product ideas. It uses iterative prompt optimization via Gemini 2.5 and GPT-4.1 generation, with an Elo-based internal evaluation loop using Qwen3-8B. No training data or manual feature engineering is used. Across three domains, two evaluator types, and six criteria, MK2 topped the automatic leaderboard and won 25 of 36 tests.

## Method Summary
MK2 implements a prompt optimization loop where Gemini 2.5 drafts and iteratively edits prompts by analyzing underperforming outputs and grafting useful fragments. GPT-4.1 generates final ideas, while Qwen3-8B runs internal Elo evaluation with 50% position swapping to mitigate bias. A GPT-4.1-mini pre-filter screens candidates before full evaluation. The pipeline adapts from NLP to CS and Materials Chemistry domains through terminology injection.

## Key Results
- Won 25 of 36 tests across three domains, two evaluator types, and six criteria
- Topped the automatic leaderboard in all three domains (NLP, CS, Materials Chemistry)
- Strongest performance in NLP and CS domains, with notable gaps in Materials Chemistry under human evaluation

## Why This Works (Mechanism)

### Mechanism 1: Iterative Prompt Grafting
The system treats prompt optimization as a combinatorial search problem where Gemini 2.5 extracts effective components from underperforming outputs and integrates them into the current best prompt. This assumes the meta-model can correctly identify which specific fragments in weak outputs are actually high-quality. If the meta-model cannot distinguish between noise and signal, the prompt may accumulate contradictory instructions.

### Mechanism 2: Internal Elo-Based Judge Loop
A lightweight internal evaluation loop uses Qwen3-8B to approximate expensive human/GPT-4 evaluation through pairwise comparisons and Elo ratings. This assumes Qwen3-8B has high correlation with official evaluation metrics. If the judge model exhibits distinct biases not present in official evaluation, the loop optimizes for the wrong objective.

### Mechanism 3: Constraint Proximity for Length Control
Placing character limit instructions at the end of the user prompt (near generation start) improves adherence compared to constraints buried in the system prompt. This exploits LLMs' recency bias. If outputs require complex reasoning that forces extensive intermediate text generation, recency-based constraints may still be ignored.

## Foundational Learning

- **Concept: Elo Rating System**
  - Why needed: The system uses Elo to rank prompts and ideas dynamically for debugging the Judge Loop
  - Quick check: If Prompt A beats Prompt B, does Prompt A necessarily become top-ranked, or does it depend on K-factor and previous score?

- **Concept: LLM-as-a-Judge (Position Bias)**
  - Why needed: The paper explicitly swaps output positions to mitigate bias in interpretation
  - Quick check: If Option A is presented before Option B to an LLM judge and it consistently picks A, is A better or does the model just prefer the first option?

- **Concept: Metaprompting / Prompt Synthesis**
  - Why needed: The core innovation uses Gemini to write prompts for GPT-4.1
  - Quick check: Can you structure a prompt that asks an LLM to output another prompt object (e.g., JSON) rather than raw text?

## Architecture Onboarding

- **Component map:** Patent JSON -> Gemini 2.5 (Prompt Generator) -> GPT-4.1 (Executor) -> Qwen3-8B (Evaluator) -> Elo ratings -> Filter (GPT-4.1-mini pre-screening)
- **Critical path:** The Prompt Generator loop - if "grafting" logic fails to improve the prompt, the rest of the pipeline executes with suboptimal instructions
- **Design tradeoffs:** GPT-4.1 chosen over o1/DeepSeek-R1 for faster iteration speed; Qwen3-8B chosen for internal loop to save costs despite potential correlation drop
- **Failure signatures:** Domain drift (high automatic scores but low human scores in Materials Chemistry) or length violation (if outputs are consistently truncated)
- **First 3 experiments:** 1) Sanity Check: Run final prompt with constraint at start vs. end on 10 samples, 2) Judge Validation: Compare Qwen3-8B rankings against human/GPT-4 ground truth on 20 patent ideas, 3) Grafting Ablation: Run prompt generator with grafting enabled vs. disabled

## Open Questions the Paper Calls Out

1. Does the Materials Chemistry performance gap stem primarily from base LLMs' inherent lack of scientific reasoning capabilities or from inadequate adaptation of NLP-centric prompt structures to physical sciences?
2. Can manual steps in the prompt optimization loop be fully automated without sacrificing the quality of the "grafting" process?
3. How can automated LLM-based evaluators be calibrated to better align with human experts when assessing technical feasibility and scientific rigor in specialized domains?

## Limitations
- The Elo correlation claim between Qwen3-8B and GPT-4.1 is based on internal data without independent verification
- The "grafting" mechanism's effectiveness depends heavily on Gemini 2.5's ability to correctly identify useful fragments
- Domain adaptation relies on unspecified terminology injection for CS and MC domains
- Materials Chemistry underperformance under human evaluation suggests automatic scoring may not capture domain-specific scientific validity

## Confidence

- **High Confidence:** Core prompt engineering approach and overall pipeline architecture are well-documented and reproducible
- **Medium Confidence:** Elo-based internal evaluation loop is methodologically sound, but correlation with human judgment needs verification
- **Low Confidence:** Iterative prompt grafting mechanism's actual contribution vs. random improvements is unclear without ablation studies

## Next Checks

1. Judge Correlation Validation: Compare Qwen3-8B pairwise rankings on 20 patent ideas against ground truth human/GPT-4 evaluation to verify claimed high correlation
2. Grafting Mechanism Ablation: Run prompt iteration with grafting enabled vs. disabled (using random prompt mutations) to determine if iterative improvement is statistically significant
3. Length Constraint Verification: Test final prompt with constraint placement at beginning vs. end of user prompt on 10 samples to measure character count adherence differences