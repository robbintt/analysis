---
ver: rpa2
title: Subjective Question Generation and Answer Evaluation using NLP
arxiv_id: '2512.17289'
source_url: https://arxiv.org/abs/2512.17289
tags:
- question
- evaluation
- generation
- answer
- subjective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study tackled the challenge of subjective question generation
  and answer evaluation using NLP. Synthetic datasets of 6,978 question samples and
  4,523 answer-evaluation samples were created to train fine-tuned LLM models.
---

# Subjective Question Generation and Answer Evaluation using NLP

## Quick Facts
- arXiv ID: 2512.17289
- Source URL: https://arxiv.org/abs/2512.17289
- Reference count: 24
- The study tackled subjective question generation and answer evaluation using NLP, achieving viable automated methods with Mistral 7B excelling in question generation and GPT-3.5 leading in answer evaluation consistency.

## Executive Summary
This paper addresses the challenge of automating subjective question generation and answer evaluation using natural language processing. The authors developed a system that generates questions at higher cognitive levels (Analysis, Synthesis, Evaluation) from text passages and evaluates subjective student answers with multi-dimensional scoring. Using synthetic datasets created via GPT-4 and fine-tuning 7B-parameter models with QLoRA and 4-bit quantization, the approach demonstrates that smaller open-source models can perform these subjective tasks with reasonable accuracy. GPT-3.5 emerged as the most consistent performer overall, while Mistral 7B showed strong peak performance in question generation.

## Method Summary
The authors created synthetic datasets of 6,978 question samples and 4,523 answer-evaluation samples using GPT-4, targeting Bloom's Taxonomy upper levels. They fine-tuned LLaMA 7B, Falcon 7B, and Mistral 7B using QLoRA (4-bit NormalFloat quantization + LoRA adapters with r=8, α=16) on a single RTX A6000 GPU. Models were trained with lit-gpt using AdamW optimizer and CosineAnnealingLR scheduler. Evaluation used GPT-4 to rank anonymized model outputs 1-4, with GPT-3.5 serving as the baseline. The fine-tuning process required approximately 3.5-5 hours per task.

## Key Results
- Mistral 7B achieved the highest performance in question generation (65.78% top rank) but was less consistent than GPT-3.5
- GPT-3.5 was the most consistent model overall, ranking in top-2 positions 97% of the time for questions and 96% for answer evaluation
- For answer evaluation, GPT-3.5 ranked highest (58.22% top rank) with Mistral 7B close behind (40.44% top rank)
- Falcon 7B performed poorly, ranking last in 87.56% of question generation tasks

## Why This Works (Mechanism)

### Mechanism 1: QLoRA Fine-tuning Efficiency
4-bit NormalFloat quantization compresses base models while LoRA adapters (r=8, α=16) add only ~21M trainable parameters to 7B models. This enables memory-efficient fine-tuning while preserving pre-trained knowledge.

### Mechanism 2: Synthetic Data Generation
GPT-4 generates context passages and questions targeting Bloom's Taxonomy upper levels (Analysis, Synthesis, Evaluation) from manually curated subject hierarchies. This substitutes for human-curated datasets at scale.

### Mechanism 3: GPT-4 Evaluation Proxy
Since subjective tasks lack fixed references for BLEU/ROUGE, GPT-4 rankings serve as a scalable proxy for human evaluation, justified by Sottana et al.'s finding that GPT-4 aligns with human judgments.

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**: Only ~21M of 7B parameters are trainable during fine-tuning, enabling memory-efficient adaptation while preserving base model knowledge.
  - Quick check: If LoRA rank r=8 and you're adapting a 4096-dimension weight matrix, how many total trainable parameters does that layer contribute?

- **Bloom's Taxonomy (Cognitive Levels)**: Questions target Analysis, Synthesis, and Evaluation levels rather than recall, affecting prompt design and evaluation criteria.
  - Quick check: Which Bloom's level would require generating a question asking students to "critique the author's argument"?

- **Quantization-Aware Fine-Tuning**: 4-bit NF4 quantization with bfloat16 adapters enables hardware efficiency while maintaining model performance.
  - Quick check: Why does NF4 quantization use an information-theoretically optimal distribution for normally-distributed weights compared to uniform 4-bit quantization?

## Architecture Onboarding

- **Component map**: GPT-4 API → synthetic samples → 80/15/5 split → LLaMA/Falcon/Mistral 7B (frozen, 4-bit quantized) → LoRA adapters (Q/K/V/Projection/MLP/Head) → lit-gpt training → GPT-4 ranking
- **Critical path**: 1) Define subject/topic hierarchy → 2) Generate synthetic data via GPT-4 → 3) Format as instruction prompts → 4) Load quantized model + LoRA → 5) Train with cross-entropy loss → 6) Evaluate via GPT-4 ranking
- **Design tradeoffs**: Mistral wins on peak question generation quality (65.78% top rank) vs. GPT-3.5's consistency (97% top-2). Synthetic data is faster but risks GPT-4 biases. GPT-4 evaluation is scalable but may not reflect human preferences.
- **Failure signatures**: Falcon 7B gibberish (87.56% last place), LLaMA 7B adding irrelevant details (79.56% third place), Mistral underperforming on answer evaluation vs. question generation.
- **First 3 experiments**: 1) Fine-tune Mistral 7B with paper's QLoRA config and verify training loss matches Figure 3. 2) Train with LoRA on attention layers only vs. full coverage and compare question quality. 3) Generate 100 questions with both GPT-4 and Mistral; have human educators blind-rank to validate GPT-4 evaluation alignment.

## Open Questions the Paper Calls Out

1. **Can larger models match GPT-3.5 performance?**: The study used only 7B models due to resource constraints, leaving unexplored whether larger architectures (13B, 70B+) could perform on par with or exceed GPT-3.5 in answer evaluation.

2. **Synthetic vs. human data quality**: The authors suggest using human-curated datasets instead of GPT-4-generated synthetic data might yield higher quality subjective assessments, but this comparison was not tested.

3. **Root cause of LLaMA-2 and Falcon underperformance**: The paper observed poor performance from these models but did not isolate whether architectural differences or optimization issues were responsible.

4. **GPT-4 ranking correlation with human evaluation**: While GPT-4 evaluation is justified by external literature, the study did not independently verify how well GPT-4's rankings align with human expert evaluation in this specific educational domain.

## Limitations

- Synthetic datasets generated by GPT-4 may contain biases from its training data that don't align with actual educational needs
- GPT-4 rankings as the primary evaluation metric lack independent human expert validation
- Study limited to 7B-parameter models, so performance patterns may not generalize to larger or smaller architectures
- Answer evaluation proved more challenging than question generation, revealing fundamental limitations in smaller models' reasoning capabilities

## Confidence

**High Confidence**: Technical implementation of QLoRA fine-tuning, experimental methodology (80/15/5 splits, GPT-4 ranking), and hardware requirements are well-specified and reproducible.

**Medium Confidence**: Synthetic data generation approach and GPT-4 evaluation methodology are reasonable but lack independent validation. Performance differences between models are meaningful but may be influenced by architecture-specific factors.

**Low Confidence**: Claims about educational effectiveness and pedagogical quality are limited by absence of human expert validation. Study demonstrates technical feasibility but not actual educational standards compliance.

## Next Checks

1. **Human Expert Validation**: Conduct blind evaluations of 100 generated questions and 100 answer evaluations using qualified educators to compare human rankings with GPT-4 rankings and identify systematic biases.

2. **Cross-Model Generalization**: Test QLoRA fine-tuning on 13B and 30B parameter models to determine if performance patterns persist across model scales and reveal whether Mistral 7B's superiority is size-specific or architecture-specific.

3. **Bias Analysis**: Analyze synthetic datasets for generation patterns by clustering questions by Bloom's level, subject matter, and difficulty, then compare distributions against actual educational datasets to identify systematic gaps.