---
ver: rpa2
title: Image-Text Relation Prediction for Multilingual Tweets
arxiv_id: '2505.05040'
source_url: https://arxiv.org/abs/2505.05040
tags:
- text
- data
- wang
- zhang
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores how multilingual vision-language models handle
  image-text relation prediction for tweets in different languages. The authors address
  limitations in prior work by creating a balanced evaluation dataset from Latvian
  Twitter posts with manual English translations.
---

# Image-Text Relation Prediction for Multilingual Tweets

## Quick Facts
- arXiv ID: 2505.05040
- Source URL: https://arxiv.org/abs/2505.05040
- Reference count: 3
- Key outcome: Llama 3.2 11B and LLaVa-NeXT 13B outperform previous baselines on image-text relation prediction for Latvian tweets, with accuracy up to 33.83% on English translations.

## Executive Summary
This paper evaluates multilingual vision-language models on image-text relation prediction for tweets in Latvian and English. The authors address limitations in prior work by creating a balanced evaluation dataset from Latvian Twitter posts with manual English translations. Five open-source vision-language models are tested on classifying four types of image-text relationships in tweets. Results show that larger models (11B-13B parameters) outperform smaller ones, though improvements remain modest. The study reveals significant variation in language sensitivity across models, with some performing better on English translations than original Latvian text. Manual translations yield higher evaluation quality than automatic ones, emphasizing the importance of high-quality data for multilingual VLM assessment.

## Method Summary
The study evaluates five open-source vision-language models (Llama 3.2 11B, LLaVa-NeXT 7B/13B, Qwen2-VL 7B, and Phi 3.5 4B) on image-text relation prediction for Latvian tweets. The task involves classifying four types of relationships between images and text in tweets, which can be decomposed into two binary questions about whether the image adds meaning and whether the text is represented in the image. Models are evaluated zero-shot using English instruction prompts (Latvian prompts produced gibberish outputs), with results averaged over 10 random seeds. The evaluation uses a balanced subset of 350 tweets from the Latvian Twitter Eater Corpus (LTEC), supplemented with manual English translations and the TIRT dataset for cross-dataset validation.

## Key Results
- Llama 3.2 11B achieves 33.07% accuracy on Latvian tweets and 33.83% on English translations, outperforming previous baselines
- Larger models (11B-13B parameters) perform better than smaller models (4B-7B), but improvements remain modest
- Language sensitivity varies significantly across models, with LLaVa-NeXT 13B and Phi 3.5 showing preference for English translations over Latvian text
- Manual translations outperform automatic ones (52.63-63.49 BLEU), highlighting the importance of high-quality data for evaluation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Larger vision-language models (11B-13B parameters) achieve higher accuracy on image-text relation prediction than smaller models (4B-7B), but improvements remain modest.
- **Mechanism:** Increased parameter capacity may enable richer cross-modal representations, allowing the model to better reason about semantic relationships between visual and textual content. The paper reports Llama 3.2 11B at 33.07% and LLaVa-NeXT 13B at 28.91% versus smaller models like Qwen2-VL 7B at 15.71%.
- **Core assumption:** Scale contributes to cross-modal reasoning; causal relationship not isolated from architecture differences.
- **Evidence anchors:**
  - [abstract] "Results show that larger models perform better"
  - [Section 5, Table 3] Shows 11B-13B models outperforming 4B-7B models on classification accuracy
  - [corpus] SigLIP 2 paper notes multilingual VLMs trained on "vast corpora" still show language coverage limitations
- **Break condition:** If smaller models with specialized fine-tuning match or exceed larger zero-shot models, scale is not the primary driver.

### Mechanism 2
- **Claim:** Some VLMs exhibit language sensitivity, performing better on English-translated text than original lower-resource language text (Latvian).
- **Mechanism:** Training data imbalance toward English may cause models to develop stronger semantic grounding in English, even for visual tasks. LLaVa-NeXT 13B improved from 19.43% (Latvian) to 28.91% (English); Phi 3.5 improved from 18.14% to 25.14%.
- **Core assumption:** Performance gap stems from training distribution, not inherent language limitations.
- **Evidence anchors:**
  - [abstract] "sensitivity to input language... varies significantly across models"
  - [Section 5] "LLaV A-NeXT 13B and Phi 3.5 4B... seem to prefer the English translation rather than the original Latvian text"
  - [corpus] LinguaMark benchmark confirms LMMs show "biased and unfair outputs across languages" due to limited linguistic coverage
- **Break condition:** If models with balanced multilingual pretraining show no language sensitivity, training data imbalance is confirmed as cause.

### Mechanism 3
- **Claim:** Manual translations yield higher evaluation quality than automatic translations, reducing error propagation in multilingual VLM assessment.
- **Mechanism:** Automatic MT introduces noise (52.63-63.49 BLEU for Latvian-English), which compounds with VLM classification errors. Manual translation minimizes this noise source.
- **Core assumption:** Classification errors are partially attributable to translation quality rather than VLM capability alone.
- **Evidence anchors:**
  - [abstract] "Manual translations outperform automatic ones, highlighting the importance of high-quality data for evaluation"
  - [Section 4.2, Table 2] Shows MT systems achieve 52.63-63.49 BLEU, 67.94-75.56 ChrF—far from perfect
  - [corpus] No direct corpus evidence on translation-quality impact on VLM evaluation; this mechanism is understudied in neighbors
- **Break condition:** If VLMs trained end-to-end on multilingual data perform equally well on original and back-translated text, translation noise is mitigated.

## Foundational Learning

- **Concept: Vision-Language Model (VLM) Architecture**
  - **Why needed here:** The paper evaluates five VLMs with different architectures; understanding how vision encoders connect to language models explains performance variations.
  - **Quick check question:** Can you explain how CLIP-style vision encoders interface with autoregressive language model decoders?

- **Concept: Zero-Shot Classification**
  - **Why needed here:** All experiments use zero-shot prompting; the model receives no task-specific training examples, relying entirely on pretrained knowledge.
  - **Quick check question:** What is the difference between zero-shot, few-shot (in-context learning), and fine-tuning evaluation paradigms?

- **Concept: Class-Balanced Evaluation**
  - **Why needed here:** The paper explicitly rebalances the dataset from 84.73% dominance by two classes to enable fairer assessment; imbalanced metrics can be misleading.
  - **Quick check question:** Why would accuracy be misleading on a dataset where one class comprises 48% of samples?

## Architecture Onboarding

- **Component map:** Image + tweet text (Latvian or English) + instruction prompt (English only) -> VLM backbone -> Binary answers to Q1 and Q2 -> 4-class prediction
- **Critical path:**
  1. Data preparation: Extract balanced 350-tweet subset from LTEC
  2. Manual translation: Human translator converts Latvian to English
  3. Prompt engineering: English instruction prompt (Latvian prompts failed); Llama 3.2 required specific formatting suffix
  4. Zero-shot inference: Each model processes image-text pairs with 10 different seeds
  5. Aggregated reporting: Mean accuracy ± standard deviation
- **Design tradeoffs:**
  - Model size vs. hardware: Authors limited to models runnable on single RTX 3090 (24GB VRAM), excluding 70B+ models
  - Balanced subset vs. full data: Reduced from 812 to 350 tweets for class balance, sacrificing statistical power
  - Manual vs. automatic translation: Higher quality but not scalable to larger datasets
- **Failure signatures:**
  - Latvian instruction prompts produced "gibberish word salad, repetitions, empty strings" across all models
  - High variance across seeds for LLaVa-NeXT models (±8.03 for 7B) indicates instability
  - Qwen2-VL showed zero variance across seeds (0.00), suggesting deterministic or stuck behavior
  - Domain mismatch: Llama 3.2 was best on LTEC (food tweets) but worst on TIRT (general tweets)
- **First 3 experiments:**
  1. **Reproduce baseline comparison:** Run all five models on the balanced LTEC subset with English prompts, Latvian text input; verify you can replicate Table 3 accuracy ranges (15-33%).
  2. **Ablate language sensitivity:** Compare accuracy when input text is Latvian vs. manual English translation vs. automatic MT output for a single model (recommend LLaVa-NeXT 13B).
  3. **Test prompt robustness:** Vary the instruction prompt formatting (remove Llama 3.2's required suffix, try alternative phrasings) to measure sensitivity to prompt design across models.

## Open Questions the Paper Calls Out

- **Question:** Can in-context learning or fine-tuning substantially improve image-text relation prediction accuracy beyond the current ~33% baseline?
  - **Basis in paper:** [explicit] Authors state: "To further improve classification results, the two obvious directions to explore would be in-context learning... or fine-tuning the model checkpoints on the image-text relation task. Both are currently out of scope in our case."
  - **Why unresolved:** Current experiments only use zero-shot classification; ICL requires multi-image processing capability that not all evaluated models support.
  - **What evidence would resolve it:** Experiments comparing zero-shot, few-shot (ICL), and fine-tuned performance on the same benchmark using models that support multiple input images.

- **Question:** Why do models exhibit inconsistent sensitivity to input language, with some performing equally well on Latvian and English while others show large gaps?
  - **Basis in paper:** [inferred] Results show LLaVA-NeXT 7B, Llama 3.2 11B, and Qwen2-VL 7B are language-insensitive, while LLaVA-NeXT 13B and Phi 3.5 perform "far better" on English input.
  - **Why unresolved:** The paper does not analyze architectural or training differences that might explain why larger versions of the same model family (LLaVA-NeXT 7B vs 13B) show opposite language sensitivity patterns.
  - **What evidence would resolve it:** Ablation studies comparing training data composition, tokenization strategies, and vision-language alignment mechanisms across these models.

- **Question:** What causes the dramatic performance inversion where Llama 3.2 11B excels on LTEC but performs worst on TIRT, while Qwen2-VL 7B shows the opposite pattern?
  - **Basis in paper:** [explicit] Authors note that Llama 3.2 11B "was overwhelmingly the highest performer on the LTEC data, but lowest on the TIRT data, while Qwen2-VL 7B scored lowest on LTEC, but was competitive on TIRT."
  - **Why unresolved:** The paper observes this domain sensitivity but does not investigate whether it stems from training data domains, the food-specific vocabulary in LTEC, or differences in the class distributions between datasets.
  - **What evidence would resolve it:** Controlled experiments varying domain, vocabulary, and class distribution while holding other factors constant.

## Limitations

- Limited evaluation dataset (350 tweets) may not capture full variability of multilingual image-text relationships
- High variance in model performance (±8.03 for LLaVa-NeXT 7B) suggests instability that may not generalize
- English-only instruction prompts introduce a potential confound between language sensitivity and instruction-following ability
- Hardware constraints limited evaluation to models with up to 13B parameters, leaving open whether larger models would show different patterns

## Confidence

- **High Confidence:** Scale-performance relationship (larger models perform better), manual translation superiority (higher quality evaluation data), class balancing necessity (prevents accuracy distortion)
- **Medium Confidence:** Language sensitivity patterns (observed across multiple models but with high variance), domain generalization limitations (Llama 3.2 performed poorly on TIRT despite success on LTEC)
- **Low Confidence:** Specific accuracy values (±5-10% variance across seeds), causal attribution of performance differences to training data distribution vs. architecture differences

## Next Checks

1. **Stability validation:** Replicate the evaluation across 20+ random seeds to confirm whether the observed high variance (±8.03) in LLaVa-NeXT models is consistent and to establish confidence intervals for all reported accuracy values.
2. **Scaling experiment:** Test a 34B or 70B parameter model on the same evaluation set to determine if the scale-performance relationship continues beyond the 13B parameter range explored in this study.
3. **Instruction language ablation:** Create parallel evaluations where the instruction prompt is provided in Latvian (with improved prompt engineering to avoid gibberish outputs) versus English, to isolate whether performance differences stem from language comprehension or instruction-following capabilities.