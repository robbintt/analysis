---
ver: rpa2
title: 'AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology'
arxiv_id: '2506.18156'
source_url: https://arxiv.org/abs/2506.18156
tags:
- moral
- cognitive
- llms
- human
- framing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Large language models exhibit human-like cognitive tendencies
  across four psychological frameworks: thematic apperception, framing bias, moral
  foundations, and cognitive dissonance. In TAT experiments, models generated coherent
  narratives with varied emotional and moral tones.'
---

# AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology

## Quick Facts
- arXiv ID: 2506.18156
- Source URL: https://arxiv.org/abs/2506.18156
- Reference count: 40
- Primary result: LLMs exhibit human-like cognitive tendencies across TAT, framing bias, moral foundations, and cognitive dissonance frameworks, shaped by training and alignment methods.

## Executive Summary
This study evaluates large language models across four cognitive psychology frameworks to measure human-like cognitive patterns. The research finds that models generate coherent narratives with varied emotional and moral tones in TAT experiments, show optimism bias in framing tasks, exhibit elevated sensitivity to liberty/oppression themes in moral foundations analysis, and demonstrate rationalization rather than direct contradiction in cognitive dissonance scenarios. These findings suggest that LLMs mirror human cognitive patterns shaped by training and alignment methods, with implications for AI transparency and ethical deployment.

## Method Summary
The study employed gpt-4o and gpt-4o-mini to generate test materials across four frameworks: TAT (30 images), Framing Bias (230 question pairs across 46 categories), Moral Foundations Theory (360 situation-based questions), and Cognitive Dissonance (200 scenario variations). Five LLMs were evaluated: gpt-4o, QvQ-72B, LLaMA-3.3-70B, Mixtral-8x22B, and DeepSeek-V3. Automated scoring was performed using LLaMA-3.1-405B with manual verification for TAT annotations. Human baseline data came from 55 participants answering 60 MFT questions.

## Key Results
- Models generated coherent TAT narratives with varied emotional and moral tones across eight SCORS-G dimensions
- Framing bias tests showed strong preference for positive entailments even under negative framing, aligning with prospect theory patterns
- Moral foundations analysis revealed elevated Liberty/Oppression sensitivity, likely influenced by alignment training
- Cognitive dissonance evaluation indicated low contradiction rates but high rationalization complexity

## Why This Works (Mechanism)

### Mechanism 1: RLHF Embeds Moral Priors
- Claim: Alignment training (RLHF) appears to elevate model scores on specific moral foundations—particularly Liberty/Oppression—above human baselines.
- Mechanism: Human raters in RLHF pipelines preferentially reward outputs demonstrating fairness, anti-oppression, and ethical behavior. This creates a learned prior where models generate responses that reflect "socially desirable" moral reasoning rather than replicating the distribution of human moral intuitions.
- Core assumption: RLHF raters consistently favor Liberty/Oppression and Fairness concerns over other foundations like Loyalty or Authority.
- Evidence anchors: [abstract] "behaviors mirror human cognitive tendencies yet are shaped by their training data and alignment methods"; [section 5.3] "One possible explanation for these elevated Liberty/Oppression scores is the role of Reinforcement Learning with Human Feedback (RLHF)..."
- Break condition: If base (non-RLHF) models showed similar Liberty/Oppression elevation, this mechanism would be invalidated.

### Mechanism 2: Framing Triggers Learned Response Patterns
- Claim: Models exhibit optimism bias, producing positive entailments regardless of framing polarity, analogous to human risk-aversion in prospect theory.
- Mechanism: Linguistic framing (gain vs. loss wording) activates different learned associations from training data. Models appear to prefer "safe" positive interpretations, maintaining coherence even when questions are negatively framed—treating positive entailment as a default strategy.
- Core assumption: Training corpora contain more positively-framed, affirmational language, creating a statistical prior toward positive outputs.
- Evidence anchors: [abstract] "show susceptibility to positive framing"; [section 5.2] "models are more inclined to produce positive entailments, even when a question is negatively framed..."
- Break condition: If contradiction rates were symmetrical across positive/negative framing, the optimism bias claim would weaken.

### Mechanism 3: Rationalization Masks Dissonance
- Claim: When models encounter contradictory prompts, they reduce perceived inconsistency through elaborate rationalization rather than direct position reversals.
- Mechanism: RLHF favors verbose, justified responses. When faced with dissonant scenarios, models generate high-complexity rationalizations (scores 2.2–2.4 on 0–3 scale) while maintaining low direct contradiction scores (<1.5 on 0–4 scale). This produces an appearance of coherence despite underlying inconsistencies.
- Core assumption: Human raters prefer elaborated justifications over terse admissions of error or uncertainty.
- Evidence anchors: [abstract] "demonstrate self-contradictions tempered by rationalization"; [section 5.4] "Rationalization Complexity tends to be fairly high (scores around or above 2 on a 0–3 scale)..."
- Break condition: If models trained with explicit conciseness rewards showed lower rationalization but similar contradiction scores, this mechanism would need revision.

## Foundational Learning

- **Moral Foundations Theory (Haidt, 2008)**:
  - Why needed here: The paper evaluates models across six moral dimensions. Understanding that human morality is multi-faceted—rather than single-axis (liberal/conservative)—is essential for interpreting why models score high on Liberty but low on Loyalty.
  - Quick check question: Why might an LLM aligned via RLHF score higher on Liberty/Oppression than on Loyalty/Betrayal?

- **Framing Effects & Prospect Theory (Tversky & Kahneman, 1981)**:
  - Why needed here: Interpreting framing bias results requires understanding that equivalent information presented differently (gain vs. loss) systematically alters human—and apparently model—decisions.
  - Quick check question: Would a purely rational agent give different answers to "20% return" vs. "80% loss" on the same investment?

- **Cognitive Dissonance (Festinger, 1959)**:
  - Why needed here: The paper's dissonance evaluation measures how models handle conflicting beliefs. The theory predicts rationalization as a tension-reduction strategy—which the paper observes in LLMs.
  - Quick check question: When confronted with evidence contradicting a prior statement, would a model more likely (a) reverse its position, (b) deny the contradiction, or (c) generate a nuanced justification reconciling both?

## Architecture Onboarding

- **Component map**: gpt-4o/mini -> test material generation -> model inference (gpt-4o, QvQ-72B, LLaMA-3.3-70B, Mixtral-8x22B, DeepSeek-V3) -> LLaMA-3.1-405B scoring -> human verification (TAT only) -> OpenAI O1 synthesis

- **Critical path**: 1. Curate/generate stimuli (30 TAT images, 230 framing pairs, 360 MFT questions, 220 dissonance scenarios) 2. Collect responses from subject models (no parameter modifications) 3. Apply LLaMA-405B automated scoring with rubrics 4. Manual verification of annotations (required for TAT psychological markers) 5. Aggregate scores, compare across models, validate against human baseline (55 participants, 60-question subset for MFT)

- **Design tradeoffs**: Automated vs. human scoring (full human evaluation was infeasible; LLaMA-405B scoring requires manual spot-checks); situational vs. self-report MFT (original MFQ uses self-reflective statements meaningless for LLMs; paper designed 360 situation-based questions to elicit reasoning); breadth vs. depth (four frameworks provide coverage but limit statistical power for any single paradigm); English-only (results may not generalize; language-dependent effects unexplored)

- **Failure signatures**: Refusal responses ("I am an AI model...") functioning as rationalization-by-avoidance (0.76–9.78% of framing responses); SCORS-G score variance (e.g., gpt-4o's Affective Quality dropping to 1 on specific images); absence of long-term planning indicators in Identity/Coherence scores; missing base-model comparison (financial constraints prevented testing non-RLHF variants)

- **First 3 experiments**: 1. Base vs. instruction-tuned comparison: Test identical architectures (e.g., LLaMA base vs. instruct) on all four frameworks to isolate RLHF's contribution to moral scoring and rationalization behavior 2. Refusal rate analysis: Systematically categorize when models deploy "AI disclaimer" responses and whether this correlates with specific prompt types (moral/controversial/ambiguous) 3. Rationalization ablation: Add explicit constraints ("Answer in ≤50 words without justification") to dissonance prompts and measure impact on contradiction scores vs. perceived coherence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the elevated "Liberty/Oppression" preference in LLMs result from explicit alignment training (RLHF) or the emulation of socially desirable human responses?
- Basis in paper: [explicit] The authors state in Section 5.3 that the precise cause of this behavior "is not conclusively revealed by our experiment and remains an open question."
- Why unresolved: The study identifies high scores in this dimension but cannot distinguish between internalized moral reasoning and mimicry of safety-aligned training data.
- What evidence would resolve it: Ablation studies comparing base models against instruction-tuned variants to isolate the effect of alignment objectives on moral foundation scores.

### Open Question 2
- Question: How do LLMs reconcile abstract ethical themes (derived from MFT) with specific decisions in targeted moral dilemmas?
- Basis in paper: [explicit] Section 6 suggests future work should "combine broad moral theories like MFT with targeted single-dilemma probes (e.g., trolley problems)" to understand this reconciliation.
- Why unresolved: The current research utilized broad, situationally grounded questionnaires rather than specific, high-stakes trade-off scenarios.
- What evidence would resolve it: Comparative experiments analyzing the correlation between a model's MFT profile and its choices in constrained, action-based ethical dilemmas.

### Open Question 3
- Question: Are the observed cognitive biases and "human-like" patterns consistent across different natural languages?
- Basis in paper: [inferred] Section 7 (Limitations) notes that "all tests were conducted in English, and potential language-dependent differences were not explored."
- Why unresolved: It remains unclear if the observed cognitive coherence and biases are generalizable or are artifacts of English-specific training data dominance.
- What evidence would resolve it: Replicating the TAT, Framing, and MFT evaluation protocols across a diverse set of non-English languages.

## Limitations
- Automated annotation via LLaMA-3.1-405B introduces potential scorer bias; manual verification was limited to TAT annotations
- Study did not test base (non-RLHF) model variants, leaving alignment's specific contribution unclear
- Results confined to English-language models; may not generalize across linguistic contexts
- Weak corpus-level evidence for optimism bias; relies primarily on model outputs rather than training data analysis

## Confidence
- **High confidence**: TAT narrative generation patterns; general framing bias tendency toward positive entailment; observed rationalization complexity in dissonance scenarios
- **Medium confidence**: Specific elevation of Liberty/Oppression scores due to RLHF; claim that framing effects mirror prospect theory; rationalization masking dissonance
- **Low confidence**: Claims about moral reasoning differences across models without base model controls; assertion that dissonance handling parallels human cognitive dissonance without direct comparison; broad generalization of findings across all LLMs based on five model families

## Next Checks
1. **Base model comparison**: Test identical architectures (e.g., LLaMA base vs. instruct) on all four frameworks to isolate RLHF's contribution to moral scoring and rationalization behavior
2. **Training corpus analysis**: Quantify positive-to-negative sentiment ratios and framing patterns in training data to test the optimism bias mechanism directly
3. **Human scorer replication**: Have human annotators score a subset of framing and dissonance responses to validate automated annotation consistency and detect scorer bias