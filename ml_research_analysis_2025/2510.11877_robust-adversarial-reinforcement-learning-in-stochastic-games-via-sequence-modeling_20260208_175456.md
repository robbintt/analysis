---
ver: rpa2
title: Robust Adversarial Reinforcement Learning in Stochastic Games via Sequence
  Modeling
arxiv_id: '2510.11877'
source_url: https://arxiv.org/abs/2510.11877
tags:
- transformer
- stochastic
- decision
- learning
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CART, a method to improve adversarial robustness
  of Decision Transformer in stochastic games. The core idea is to estimate the value
  of a state-action pair by taking the minimum over adversary actions of the expected
  maximum value over possible next states, accounting for transition probabilities.
---

# Robust Adversarial Reinforcement Learning in Stochastic Games via Sequence Modeling

## Quick Facts
- arXiv ID: 2510.11877
- Source URL: https://arxiv.org/abs/2510.11877
- Reference count: 22
- Key outcome: CART improves adversarial robustness of Decision Transformer in stochastic games by incorporating expected maximum value over transitions, achieving worst-case returns of 8.0 vs 5.7 (ARDT) and 6 (DT) on illustrative game.

## Executive Summary
This paper introduces CART (Conditional Adversarial Reinforcement Transformer), a method that improves adversarial robustness of Decision Transformer in stochastic games by explicitly modeling probabilistic state transitions. The core innovation is formulating stage-game payoffs as expected maximum values over subsequent states weighted by transition probabilities, rather than using deterministic next-state values as previous methods did. By conditioning the transformer on NashQ values derived from these stage games, CART generates policies that are simultaneously less exploitable by adversaries and more accurate in minimax value estimation across a suite of synthetic stochastic games.

## Method Summary
CART addresses stochastic over-optimism in adversarial RL by learning three interconnected value functions: Q̄(s,a,ā) estimates stage-game payoff as expected maximum value over subsequent states, Q(s,a) computes the NashQ value via expectile regression for minimization, and V(s') estimates value via expectile regression for maximization. These are learned through alternating optimization on offline data collected from uniform behavioral policies. The Decision Transformer is then trained with conditioning signals set to the NashQ values, enabling robust policy generation that accounts for both adversarial responses and transition uncertainty.

## Key Results
- Achieves worst-case return of 8.0 on illustrative 2-stage game vs 5.7 (ARDT) and 6 (DT)
- Provides more accurate minimax value estimation compared to baselines
- Demonstrates improved robustness to optimal adversaries across synthetic stochastic games
- Explicitly incorporates stochastic state transitions rather than assuming deterministic transitions

## Why This Works (Mechanism)

### Mechanism 1: Expected Maximum Value Payoff Formulation
CART addresses stochastic transition over-optimism by formulating stage-game payoffs as expected maximum values over subsequent states, weighted by transition probabilities. At each stage, instead of taking a single next-state value (as ARDT does deterministically), CART computes Q̄(s,a,ā) = E_{s′∼T(·|s,a)}[r + V(s′)], where V(s′) = max_{a′} Q(s′,a′). This integrates over all possible transitions rather than treating observed transitions as certain. The core assumption is that the offline dataset contains sufficient coverage of transition outcomes to estimate expected values accurately. Break condition: if transition coverage in D is sparse for certain (s,a) pairs, expected value estimates become unreliable.

### Mechanism 2: Minimax NashQ Conditioning for Worst-Case Awareness
Conditioning the Decision Transformer on NashQ values (min over adversary actions of the expected payoff) produces policies robust to optimal adversaries. The conditioning signal z = Q^{CART}(s,a) = min_{ā} Q̄(s,a,ā) represents the worst-case expected return given the protagonist's action. By training π_θ(a_t | τ_{0:t-1}, s_t, z) with z set to this minimax value, the transformer learns to generate actions that achieve robust returns even under adversarial response. The core assumption is that the adversary observes protagonist actions before responding (sequential stage game structure) and offline data contains adversary action coverage. Break condition: if adversary action space is large and offline data has sparse ā coverage, min_{ā} estimation via expectile regression may fail.

### Mechanism 3: Expectile Regression for In-Sample Min/Max Approximation
Expectile regression enables differentiable approximation of min and max operations over in-sample actions without requiring exhaustive coverage. The asymmetric loss L_α^{ER}(u) = E[|α−1(u>0)|·u²] with α→0 approximates minimum (emphasizing lower tails) and α→1 approximates maximum. This allows joint Q-learning with minimization/maximization via gradient descent, avoiding the need to enumerate all ā or a′ values. The core assumption is that the expectile parameter α can be tuned to approximate the true min/max sufficiently. Break condition: if α is not properly tuned, expectile regression may converge to non-extremal values.

## Foundational Learning

- **Concept: Stochastic Games (Markov Games)**
  - Why needed here: CART operates on two-player zero-sum stochastic games where state transitions T(·|s_t,a_t,ā_t) are probabilistic, not deterministic. Understanding the difference between MDPs, deterministic games, and stochastic games is essential to grasp why ARDT fails (assumes deterministic T).
  - Quick check question: Given state s and actions (a, ā), if there are two possible next states s′_1 and s′_2 with probabilities 0.9 and 0.1, would ARDT's value estimate correctly account for both? (Answer: No—ARDT would use the max over observed s′ values, potentially overestimating.)

- **Concept: Decision Transformer and Return-Conditioned Sequence Modeling**
  - Why needed here: CART builds on DT, which reframes RL as autoregressive sequence modeling: given trajectory tokens (R̂_1, s_1, a_1, R̂_2, s_2, a_2, ...), the transformer predicts a_t conditioned on return-to-go R̂_t. CART modifies the conditioning signal from return-to-go to NashQ.
  - Quick check question: In standard DT, what happens if you condition on an unrealistically high target return z during inference? (Answer: DT may still attempt to generate actions, but performance degrades if such returns were never seen in training data.)

- **Concept: Minimax and Nash Equilibrium in Zero-Sum Games**
  - Why needed here: The NashQ value is derived from solving a stage game where the protagonist maximizes and adversary minimizes. Understanding min_{ā} max_a Q(s,a,ā) (or equivalently max-min in zero-sum) is critical for interpreting the conditioning signal.
  - Quick check question: In a stage game with payoffs Q(s,a_0,ā_0)=5, Q(s,a_0,ā_1)=3, Q(s,a_1,ā_0)=4, Q(s,a_1,ā_1)=4, what is the minimax value for action a_0? (Answer: min{5,3} = 3.)

## Architecture Onboarding

- **Component map:** Q̄-function network → V-function network → Q^{CART}-function network → Decision Transformer
- **Critical path:**
  1. Collect offline dataset D with uniform behavioral policy covering all trajectories
  2. Initialize Q̄, V, Q networks
  3. Train terminal state values via MSE
  4. **Loop:** (a) Update Q̄ via Eq. 5 → (b) Update Q via Eq. 6 → (c) Update V via Eq. 7
  5. Relabel trajectories in D with z = Q^{CART}(s,a)
  6. Train Decision Transformer on relabeled data via L_{DT} (Eq. 1)
  7. At inference: condition on high z, autoregressively sample actions

- **Design tradeoffs:**
  - Conservatism vs. optimality: Higher conservatism (lower α in Eq. 6) improves worst-case robustness but may sacrifice expected return under weak adversaries
  - Offline coverage vs. overfitting: Uniform behavioral policy ensures coverage but may require large D; limited D leads to inaccurate E_{s′} estimates
  - Expressiveness vs. stability: Three separate function approximators (Q̄, V, Q) provide flexibility but introduce optimization interdependence

- **Failure signatures:**
  - Over-optimistic value estimates: If Q̄ converges before V is accurate, expected values will be wrong; check if L(Q̄) stagnates while L(V) is still decreasing
  - Collapsed expectile: If α→0 or α→1 is too extreme, gradients may vanish for non-tail samples; monitor gradient norms across different Q-value percentiles
  - Poor NashQ transfer to DT: If Q^{CART} values are accurate but DT fails to achieve them, check sequence modeling capacity or tokenization

- **First 3 experiments:**
  1. Two-stage stochastic game validation: Implement the Figure 4 game (90%/10% transition split with payoffs 5,5,0 vs 15,1,2); verify CART achieves ~8 worst-case return vs ARDT's ~5.7 and DT's ~6 (Table in Figure 1)
  2. Ablation on expectile parameter α: Sweep α ∈ {0.1, 0.3, 0.5, 0.7, 0.9} for L(Q) and L(V); plot worst-case return vs α to confirm α→0 for min and α→1 for max is necessary
  3. Transition coverage sensitivity: Reduce dataset size from 10^5 trajectories to 10^3, 10^4; measure degradation in worst-case return to identify minimum viable coverage threshold

## Open Questions the Paper Calls Out
None

## Limitations
- **Transition coverage dependence:** CART's effectiveness critically relies on sufficient offline coverage of all transition outcomes, which is assumed but not empirically validated across varying dataset sizes
- **Expectile regression hyperparameter sensitivity:** The choice of expectile parameters (α→0 for min, α→1 for max) is treated as given without empirical justification or ablation studies
- **Game-specific evaluation:** All experiments are conducted on synthetic 2-stage and 3-stage games with small action spaces; generalization to larger, more complex stochastic games remains unverified

## Confidence
- **High confidence:** The core mechanism of incorporating expected maximum value over transitions (Q̄ formulation) is theoretically sound and directly addresses the stochastic over-optimism identified in ARDT
- **Medium confidence:** The expectile regression approach for differentiable min/max approximation is reasonable given prior work but lacks thorough ablation studies
- **Low confidence:** The scalability claims to larger stochastic games are unsupported by current experimental scope, and offline-only evaluation prevents assessment of online robustness transfer

## Next Checks
1. Dataset coverage ablation: Systematically reduce trajectory count from 10^5 to 10^3, measuring degradation in worst-case return to identify minimum viable coverage threshold
2. Expectile parameter sweep: Test CART with α ∈ {0.1, 0.3, 0.5, 0.7, 0.9} for both L(Q) and L(V), plotting worst-case return vs α to verify theoretical choices
3. Deterministic transition baseline: Evaluate CART on the same games with deterministic transitions to confirm the method's overhead is justified only when stochasticity is present