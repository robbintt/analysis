---
ver: rpa2
title: 'Chengyu-Bench: Benchmarking Large Language Models for Chinese Idiom Understanding
  and Use'
arxiv_id: '2506.18105'
source_url: https://arxiv.org/abs/2506.18105
tags:
- idiom
- idioms
- chinese
- cloze
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Chengyu-Bench, a comprehensive benchmark
  designed to evaluate large language models'' understanding and usage of Chinese
  idioms. The benchmark features three tasks: Evaluative Connotation (classifying
  idioms as positive or negative), Appropriateness (detecting incorrect idiom usage
  in context), and Open Cloze (filling blanks in longer passages without options).'
---

# Chengyu-Bench: Benchmarking Large Language Models for Chinese Idiom Understanding and Use

## Quick Facts
- arXiv ID: 2506.18105
- Source URL: https://arxiv.org/abs/2506.18105
- Reference count: 17
- Benchmark reveals LLMs excel at sentiment classification (>95%) but struggle with contextual usage (40-85% accuracy)

## Executive Summary
This paper introduces Chengyu-Bench, a comprehensive benchmark designed to evaluate large language models' understanding and usage of Chinese idioms. The benchmark features three tasks: Evaluative Connotation (classifying idioms as positive or negative), Appropriateness (detecting incorrect idiom usage in context), and Open Cloze (filling blanks in longer passages without options). Chengyu-Bench comprises 2,937 human-verified examples covering 1,765 common idioms sourced from diverse corpora. Evaluation of leading LLMs reveals high accuracy on Evaluative Connotation (>95%) but significantly lower performance on Appropriateness (~85%) and Open Cloze (~40% top-1 accuracy). Error analysis indicates most mistakes stem from fundamental misunderstandings of idiom meanings, highlighting that while LLMs can gauge idiom sentiment reliably, they struggle with cultural and contextual nuances essential for proper usage.

## Method Summary
Chengyu-Bench was constructed through a systematic process of collecting Chinese idioms from diverse corpora, followed by human verification to ensure quality. The benchmark includes three distinct tasks designed to test different aspects of idiom understanding: sentiment classification, contextual appropriateness detection, and cloze-style completion. The dataset covers 1,765 common idioms across 2,937 examples, providing comprehensive coverage of the Chinese idiom landscape. Evaluation was conducted on leading LLMs across all three tasks, with performance metrics calculated for each task type. Error analysis was performed to identify patterns in model failures and understand the nature of LLM limitations in idiom comprehension.

## Key Results
- LLMs achieve >95% accuracy on Evaluative Connotation task for classifying idiom sentiment
- Performance drops to ~85% accuracy on Appropriateness task for detecting incorrect idiom usage
- Open Cloze task shows the lowest performance at ~40% top-1 accuracy for filling blanks in passages
- Most errors stem from fundamental misunderstandings of idiom meanings rather than surface-level mistakes

## Why This Works (Mechanism)
The benchmark effectively captures the multi-dimensional nature of Chinese idiom understanding by testing both surface-level sentiment recognition and deeper contextual comprehension. The human-verified examples ensure high-quality evaluation data that reflects real-world idiom usage patterns. The three-task structure reveals a performance gradient that mirrors the complexity of language understanding, from simple classification to complex contextual reasoning.

## Foundational Learning
- Chinese idiom structure and semantics - needed to understand why idioms pose unique challenges for LLMs; quick check: can identify idiom components and their literal vs figurative meanings
- Cultural context in language understanding - needed to grasp why idioms require cultural knowledge beyond vocabulary; quick check: can explain idiom origins and cultural significance
- Chinese linguistic features - needed to understand task-specific challenges; quick check: can identify tonal patterns and syntactic structures unique to Chinese

## Architecture Onboarding
- **Component map:** Data Collection -> Human Verification -> Task Construction -> Model Evaluation -> Error Analysis
- **Critical path:** Human-verified dataset creation is the foundation for all subsequent evaluation and analysis
- **Design tradeoffs:** Comprehensive coverage vs. dataset size; task difficulty vs. model capability assessment
- **Failure signatures:** Consistent errors across tasks indicate fundamental idiom misunderstanding rather than task-specific limitations
- **3 first experiments:**
  1. Test model performance on idiom sentiment vs. non-idiom sentiment to isolate idiom-specific challenges
  2. Compare performance on literal vs. figurative idiom interpretations
  3. Evaluate transfer learning from one task to another to identify task relationships

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset size of 2,937 examples may limit generalizability across all possible idiom contexts
- Benchmark focuses on three specific tasks, potentially missing other important dimensions of idiom understanding
- Performance gap between tasks may reflect dataset characteristics rather than true model limitations

## Confidence
- **High confidence** in benchmark construction methodology and human verification process
- **Medium confidence** in absolute performance numbers due to limited model and task scope
- **Medium confidence** in error analysis conclusions due to reliance on human interpretation

## Next Checks
1. Expand evaluation to include additional Chinese idiom understanding tasks such as idiom derivation and creative usage generation
2. Conduct cross-validation with a larger, independently collected idiom dataset to verify performance pattern generalizability
3. Perform ablation studies to isolate whether performance differences stem from task difficulty versus dataset characteristics