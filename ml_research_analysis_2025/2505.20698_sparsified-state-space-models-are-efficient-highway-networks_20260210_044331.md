---
ver: rpa2
title: Sparsified State-Space Models are Efficient Highway Networks
arxiv_id: '2505.20698'
source_url: https://arxiv.org/abs/2505.20698
tags:
- simba
- mamba
- pruning
- token
- ssms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces Simba, a hierarchical token pruning method\
  \ that sparsifies pre-trained state-space models (SSMs) to improve efficiency while\
  \ preserving or enhancing performance. By leveraging the redundancy in SSM token\
  \ states\u2014particularly in upper layers that capture global context\u2014Simba\
  \ progressively prunes less important tokens, creating a trapezoidal-shaped network\
  \ where upper layers act as highways to facilitate long-range information flow."
---

# Sparsified State-Space Models are Efficient Highway Networks

## Quick Facts
- **arXiv ID**: 2505.20698
- **Source URL**: https://arxiv.org/abs/2505.20698
- **Reference count**: 19
- **Primary result**: Token pruning method that sparsifies SSMs to improve efficiency while preserving or enhancing performance

## Executive Summary
Simba introduces a hierarchical token pruning method that sparsifies pre-trained state-space models (SSMs) to improve efficiency while preserving or enhancing performance. By leveraging the redundancy in SSM token states—particularly in upper layers that capture global context—Simba progressively prunes less important tokens, creating a trapezoidal-shaped network where upper layers act as highways to facilitate long-range information flow. The pruning criterion measures each token's global impact on the final output by reformulating SSM equations to accumulate local recurrence effects.

## Method Summary
Simba implements a training-free token pruning method that operates on pre-trained SSMs by progressively removing less important tokens at each layer. The method uses a linear pruning schedule where upper layers prune more aggressively than lower layers, ultimately retaining approximately 10% of tokens at the final layer. Token importance is computed using a global influence score based on the accumulated effect of each token on the final output, calculated through reformulations of the SSM recurrence equations. This creates a trapezoidal network structure that acts as an implicit highway network, improving long-range information flow while achieving significant computational efficiency gains.

## Key Results
- Simba consistently outperforms Mamba and Pythia models with equivalent computational budgets on 6 NLP benchmarks
- Achieves up to 62.5% accuracy improvement and better perplexity across various context lengths
- Demonstrates robustness in length extrapolation beyond the pre-trained context limit (up to 2k tokens)
- Enhances information flow from earlier tokens while maintaining or improving overall performance

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Token Redundancy Exploitation
- Claim: Tokens in SSMs exhibit hierarchical redundancy, with upper layers being more redundant than lower layers due to global vs. local information encoding.
- Mechanism: Lower layers encode local patterns (high-diagonal attention patterns), upper layers encode global context (lower-triangular patterns). Similar positions in upper layers compress similar global information, creating redundancy measurable via cosine similarity between adjacent tokens.
- Core assumption: SSMs process information hierarchically—local→global across layers—and redundant global representations can be safely pruned without information loss.
- Evidence anchors:
  - [abstract] "upper layers of SSMs tend to be more redundant as they encode global information, while lower layers encode local information"
  - [section 3.1] Figure 2 shows cosine similarity between adjacent tokens increases with layer depth
  - [corpus] Limited direct corpus validation; related work on contextual flow in SSMs (arxiv:2510.06640) but no direct redundancy analysis

### Mechanism 2: Global Impact Token Importance Scoring
- Claim: Token importance can be measured by accumulated influence on the final output via reformulated SSM recurrence equations.
- Mechanism: For token xt at position t, compute ∆yT(t) = CT(∏k=t+1T Āk)B̄txt, which isolates that token's contribution to final output yT. Max-pooling across channels yields importance score s(t). Tokens with lowest scores are pruned.
- Core assumption: Linear recurrence structure allows tractable decomposition of individual token contributions; max-pooling normalizes cross-channel importance.
- Evidence anchors:
  - [section 3.2] Equation 4 derivation shows exact influence computation
  - [section 4.4] Ablation shows influence-based pruning outperforms uniform/random baselines on ARC-Challenge
  - [corpus] No direct corpus validation of SSM-specific pruning criteria; Transformer token merging exists but uses attention-based metrics

### Mechanism 3: Highway Network Formation via Sparsified Recurrence
- Claim: Sparsified upper layers create implicit highway connections that improve long-range information flow and length extrapolation.
- Mechanism: Dense recurrence attenuates earlier token information through successive state updates. Pruned upper layers skip redundant updates, preserving earlier token contributions—effectively creating skip-like connections without explicit architectural changes. This "trapezoidal" network enables information to flow from early positions to final output more directly.
- Core assumption: Dense recurrence operations are the bottleneck for long-range dependency; reducing recurrence steps preserves information without explicit skip connections.
- Evidence anchors:
  - [section 4.3] Figure 5 shows Simba maintains flatter influence curves in upper layers (early tokens retain influence), while Mamba shows steep decay
  - [section 4.2] Simba continues improving perplexity beyond 2k pre-trained context limit; Mamba deteriorates
  - [corpus] Highway Networks (Srivastava et al. 2017) and LSTM gates are cited as conceptual ancestors, but no corpus validation of implicit highway formation via pruning

## Foundational Learning

- Concept: **State-Space Models (SSMs) and Linear Recurrence**
  - Why needed here: Simba operates on SSM architectures (specifically Mamba), which replace self-attention with linear recurrence: ht = Āht-1 + B̄xt. Understanding this is essential to grasp why token pruning affects information flow differently than in Transformers.
  - Quick check question: If you remove token t from an SSM, does it affect the hidden state at position t+1? What about Transformers?

- Concept: **Token Pruning vs. Weight Pruning**
  - Why needed here: Simba uses token pruning (removing sequence elements), not weight pruning (removing parameters). This yields actual speedups since computation scales with active tokens, unlike unstructured weight sparsity on GPUs.
  - Quick check question: Why does token pruning provide inference speedups while magnitude-based weight pruning often doesn't?

- Concept: **Highway Networks and Long-Term Dependencies**
  - Why needed here: The paper frames sparsified upper layers as implicit highways, connecting to classical RNN solutions for vanishing gradients. This contextualizes why pruning can improve, not just preserve, performance.
  - Quick check question: How do highway connections in RNNs address the vanishing gradient problem? How might sparse upper layers achieve similar effects?

## Architecture Onboarding

- Component map:
  Input tokens (x1...xT) -> Layer 1: Full token set -> compute importance scores s(t) -> prune lowest-N% -> Layer 2: Remaining tokens -> compute scores -> prune additional tokens -> ... (progressive pruning following linear schedule) -> Layer L: ~10% original tokens (default) -> output

- Critical path:
  1. Forward pass through layer with full tokens
  2. Compute influence scores s(t) = max(CT · ∏k=t+1T Āk · B̄t · xt)
  3. Rank tokens by s(t), prune bottom-k% (k follows linear schedule: higher pruning in upper layers)
  4. Pass remaining token indices to next layer
  5. Repeat for all layers

- Design tradeoffs:
  - **Final-layer retention ratio**: 10% (default) vs. 70% (moderate). Lower = more efficiency but riskier for tasks needing all context.
  - **Pruning schedule**: Linear (used) vs. uniform. Linear preserves early-layer local information; uniform may over-prune early.
  - **Score aggregation**: Max-pooling (used) vs. L2-norm. Max slightly better per experiments.
  - **Bias handling**: Excluding bias in Āk computation improves score consistency across channels.

- Failure signatures:
  - **Catastropic perplexity increase**: Likely over-pruned for task; increase final-layer retention ratio.
  - **No speedup**: Pruning may not be implemented efficiently; ensure pruned tokens are actually skipped in computation, not just masked.
  - **Length extrapolation failure**: Highway effect not forming; check if upper layers have sufficient sparsity (should see trapezoidal shape in token counts).
  - **Random-pruning-level performance**: Influence score computation may have bugs; verify Eq. 4 implementation against gradient check.

- First 3 experiments:
  1. **Reproduce redundancy analysis** (Figure 2): Measure cosine similarity between adjacent token states across layers on PG-19. Verify hierarchical pattern exists in your target SSM.
  2. **Ablate pruning criteria**: Compare influence-based vs. uniform vs. random pruning on ARC-Challenge (Figure 6). Use Mamba-2.8b, test final-layer retention ratios [10%, 30%, 50%, 70%].
  3. **Validate highway effect**: Reproduce Figure 5 information flow visualization. Plot normalized influence s(t)/||yT||² vs. relative position for layers 1, 13, 26, 38, 51, 64. Simba should show flatter curves in upper layers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a sophisticated fine-tuning scheme tailored for sparse SSMs further reduce the distribution shift caused by token pruning and improve performance gains?
- Basis in paper: [explicit] The limitations section states: "token pruning incurs distribution shifts from the original models, and further fine-tuning could reduce this misalignment... a more sophisticated fine-tuning scheme tailored for sparse SSMs could be investigated."
- Why unresolved: The paper only demonstrates simple fine-tuning on small models with a basic loss design combining sparsified and dense forwarding losses.
- What evidence would resolve it: Experiments comparing various fine-tuning strategies (e.g., knowledge distillation, progressive pruning schedules, specialized sparse-aware losses) across multiple model scales showing perplexity and benchmark improvements.

### Open Question 2
- Question: Would performing token pruning at specific layers rather than at every layer further enhance inference speedups while maintaining accuracy?
- Basis in paper: [explicit] Appendix D.1 states: "We believe that these gains could be further enhanced by leveraging existing techniques, such as performing token pruning at specific layers rather than at every layer. This represents a promising direction for future exploration."
- Why unresolved: Simba currently prunes tokens at every layer with a linear schedule; layer-selective pruning remains unexplored.
- What evidence would resolve it: Ablation studies varying which layers perform pruning, measuring both inference time and task accuracy to identify optimal pruning layer configurations.

### Open Question 3
- Question: What mechanisms cause moderate pruning to improve performance over dense models in smaller SSMs, and can this benefit be systematically extended to larger models?
- Basis in paper: [explicit] Appendix D.3 states: "We believe that further exploration in this direction will be essential to fully understand the underlying mechanisms and to optimize the use of token pruning across a broader range of settings." The authors observe gains are more evident for smaller models with smaller state sizes.
- Why unresolved: The authors hypothesize limited state capacity causes forgetting in dense recurrence, but the exact mechanism and why it doesn't scale to larger models remains unclear.
- What evidence would resolve it: Controlled experiments varying state size independently from model scale, plus analysis of information retention metrics across different pruning ratios.

## Limitations

- **Generalizability Across Architectures**: The method's effectiveness on other SSM variants (H3, S4, RWKV) or hybrid architectures remains untested.
- **Task-Specific Sensitivity**: The fixed 10% final-layer retention ratio may be suboptimal for tasks requiring precise positional information or highly structured input patterns.
- **Theoretical Foundation Gaps**: Lacks rigorous theoretical analysis of the proposed mechanisms, particularly the hierarchical redundancy exploitation and highway network formation.

## Confidence

- **High Confidence**: Simba achieves consistent efficiency gains across all tested NLP benchmarks; influence-based token importance scoring outperforms random and uniform baselines; superior length extrapolation beyond pre-trained context limits.
- **Medium Confidence**: Hierarchical token redundancy exists and is exploitable in SSMs; trapezoidal network structure is essential for observed performance gains; 10% final-layer retention ratio represents optimal balance across tasks.
- **Low Confidence**: Exact mechanism by which sparse upper layers create highway-like effects; generalizability to non-NLP domains or longer sequence lengths (>2048); robustness to adversarial or highly structured input patterns.

## Next Checks

1. **Architecture Transferability Study**: Apply Simba to at least two additional SSM architectures (e.g., H3 and RWKV) on the same benchmark suite. Compare performance degradation/gains relative to Mamba implementation to quantify architecture dependence. Measure whether the hierarchical redundancy pattern holds across architectures.

2. **Theoretical Analysis of Highway Formation**: Formalize the relationship between token pruning and information flow preservation. Derive conditions under which sparsified recurrence approximates highway connections, and identify the threshold where pruning begins to degrade information propagation. Validate with synthetic sequence patterns where exact information flow can be traced.

3. **Adaptive Pruning Ratio Optimization**: Develop and evaluate a task-adaptive method for determining optimal final-layer retention ratios. Test whether performance can be further improved by task-specific tuning versus the fixed 10% ratio. Measure sensitivity across the full spectrum of task complexities in the benchmark suite.