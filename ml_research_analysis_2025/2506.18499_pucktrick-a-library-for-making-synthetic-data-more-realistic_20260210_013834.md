---
ver: rpa2
title: 'PuckTrick: A Library for Making Synthetic Data More Realistic'
arxiv_id: '2506.18499'
source_url: https://arxiv.org/abs/2506.18499
tags:
- data
- synthetic
- dataset
- datasets
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Pucktrick is a Python library that systematically introduces controlled\
  \ errors\u2014such as missing values, noise, outliers, and label misclassification\u2014\
  into synthetic datasets to better mimic real-world imperfections. This enhances\
  \ machine learning model robustness by exposing models to realistic data inconsistencies\
  \ during training."
---

# PuckTrick: A Library for Making Synthetic Data More Realistic

## Quick Facts
- arXiv ID: 2506.18499
- Source URL: https://arxiv.org/abs/2506.18499
- Reference count: 24
- Primary result: Controlled error injection into synthetic data improves ML model robustness, especially for tree-based models

## Executive Summary
PuckTrick is a Python library designed to introduce realistic imperfections—missing values, noise, outliers, and label misclassification—into synthetic datasets. The goal is to bridge the gap between artificially clean synthetic data and real-world data, improving model generalization. Experiments using financial datasets showed that models trained on Pucktrick-contaminated synthetic data consistently outperformed those trained on error-free synthetic data, with the largest gains for tree-based and linear models like SVMs and Extra Trees.

## Method Summary
The method involves generating synthetic data using GReaT (distilgpt2, batch_size=32, 50 epochs), then contaminating it with PuckTrick at a 30% error rate across five error types: duplicates, label flips, missing values, noise, and outliers. Ten ML models (SVM, ET, RF, KN, LDA, MLP, LR, NB, DT, QDA) were trained via PyCaret on each contaminated variant and evaluated on a held-out real test set. The approach tests whether exposure to imperfect synthetic data improves robustness on real-world data.

## Key Results
- Models trained on Pucktrick-contaminated synthetic data outperformed those trained on error-free synthetic data
- Tree-based models (Extra Trees, Random Forest) showed the most significant improvement, especially with missing values and outliers
- SVM models consistently benefited from noise injection, suggesting noise acts as regularization
- Label misclassification had highly model-dependent effects with no universal winner

## Why This Works (Mechanism)

### Mechanism 1
Controlled noise injection into synthetic training data can improve model generalization by acting as an implicit regularizer. Noise perturbs feature values within statistically bounded ranges, forcing models to learn more robust decision boundaries rather than overfitting to artifactually clean distributions. This parallels data augmentation in computer vision but operates at the feature-value level.

### Mechanism 2
Tree-based ensemble models (Extra Trees, Random Forest) are inherently more resilient to missing values and outliers because their splitting logic partitions feature space without requiring complete records. Decision trees handle missingness by surrogate splits or available-feature routing; outliers are isolated into individual leaves without propagating through the ensemble.

### Mechanism 3
Label misclassification contamination has highly model-dependent effects, requiring complementary correction strategies rather than standalone contamination. Flipped labels introduce noise into the loss surface; linear models like LDA and tree ensembles show inconsistent benefits across years, suggesting interaction between model capacity, label noise rate, and underlying data structure.

## Foundational Learning

- **Concept: Distribution Shift and Covariate Shift**
  - Why needed here: The core problem PuckTrick addresses is that synthetic data is too clean, creating a mismatch between training and deployment distributions. Understanding that contaminating synthetic data is essentially reducing covariate shift helps contextualize why this improves generalization.
  - Quick check question: If your synthetic data has no missing values but your deployment data has 15% missingness, what type of shift is this, and would PuckTrick's missing module help?

- **Concept: Regularization via Noise Injection**
  - Why needed here: The paper explicitly frames noise as regularization. Understanding that adding noise to inputs or features during training prevents overfitting to spurious patterns is essential for interpreting the SVM results.
  - Quick check question: Why might adding Gaussian noise to continuous features during training improve test-time accuracy, even though the model never sees the exact same noisy input twice?

- **Concept: Tree-based Model Robustness Mechanisms**
  - Why needed here: Extra Trees consistently benefited from contamination. Understanding that trees can handle missing values via surrogate splits and outliers via leaf isolation explains why contamination helps rather than hurts these models.
  - Quick check question: If a decision tree encounters a missing value at a split point during inference, what are two strategies it might use to continue prediction?

## Architecture Onboarding

- **Component map:**
  ```
  PuckTrick Library
  ├── Modes
  │   ├── New (inject errors into clean dataset)
  │   └── Extended (add errors to already-contaminated data, targeting untouched rows)
  ├── Modules (5 error types)
  │   ├── duplicate – row duplication (random or attribute-targeted)
  │   ├── label – target variable flipping (binary swap or multi-class reassignment)
  │   ├── missing – null insertion at specified column/percentage
  │   ├── noise – value perturbation (continuous: normal dist within min/max; categorical: sampling)
  │   └── outlier – extreme value injection (3σ bounds for numeric; new categories for categorical)
  └── Data Type Support: Categorical, Numerical, Boolean, DateTime
  ```

- **Critical path:**
  1. Generate or obtain synthetic dataset (paper used GReaT with distilgpt2)
  2. Select contamination mode: `new` for clean data, `extended` for incremental corruption
  3. Choose error type(s) and target percentage (paper tested 30% fixed rate)
  4. Apply per-feature or dataset-wide contamination
  5. Train model on contaminated synthetic data
  6. Evaluate on held-out real test set to measure generalization improvement

- **Design tradeoffs:**
  - Error rate selection: Paper fixed at 30% for consistency; lower rates may not provide sufficient regularization, higher rates risk overwhelming signal. No automated rate tuning guidance provided.
  - Feature-level vs. dataset-level: Feature-level allows targeted stress-testing of specific attributes; dataset-level simulates systemic data quality issues.
  - Sequential contamination: Extended mode enables layered errors but does not track which rows have which error types, making ablation studies harder.

- **Failure signatures:**
  - Label contamination showing inconsistent gains across model types indicates label noise requires model-specific handling
  - Neural networks (MLP) showed variable results (93.4% best-or-equal in 2014, 85.25% in 2015) suggesting sensitivity to contamination type
  - QDA showed poor performance (3.28% improvement in 2016) indicating some architectures are not robust to these perturbations

- **First 3 experiments:**
  1. **Baseline comparison:** Train your chosen model architecture on clean synthetic data vs. PuckTrick-contaminated synthetic data (noise at 15-30%) and compare accuracy on real held-out test data. This validates whether contamination helps your specific use case.
  2. **Error type ablation:** Create separate contaminated datasets for each error type (missing, noise, outlier, label) at the same percentage. Train identical models on each to identify which contamination type provides the largest generalization benefit for your architecture.
  3. **Error rate sensitivity:** For the highest-performing error type from experiment 2, test error rates at 10%, 20%, 30%, 40% to find the regularization peak before performance degrades.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can synthetic data tools simulate structural schema changes, such as feature splitting or merging, between the training phase and the inference phase?
  - Basis in paper: [explicit] The authors state they are "exploring methods to modify dataset schemas rather than only altering data values" to address structural mismatches.
  - Why unresolved: The current Pucktrick library only modifies data values within an existing structure; it lacks the capability to simulate schema evolution or structural drift.
  - What evidence would resolve it: The implementation of schema-altering functions within the library and a subsequent evaluation of model adaptability to structural changes.

- **Open Question 2:** What is the minimum set of essential features required to maintain high classifier accuracy when training on imperfect datasets contaminated by Pucktrick?
  - Basis in paper: [explicit] The conclusion notes that future work aims to "help users identify the minimum set of essential features required to improve classifier accuracy... with imperfect datasets."
  - Why unresolved: The current experiments utilize a fixed set of features (top 20) and do not analyze the interaction between feature selection, error types, and model performance.
  - What evidence would resolve it: Ablation studies showing performance curves as features are incrementally removed from datasets containing specific controlled errors.

- **Open Question 3:** How does the simultaneous application of multiple error types (e.g., injecting noise alongside missing values) impact model robustness compared to single-error contamination?
  - Basis in paper: [inferred] The methodology states the pipeline constructs a "separate contaminated dataset for each introduced error type," implying that interactions between error types were not studied.
  - Why unresolved: Real-world data often suffers from compound imperfections, but the current study isolates variables to attribute effects to specific error categories.
  - What evidence would resolve it: Comparative benchmarks evaluating models trained on datasets containing various combinations of overlapping errors versus single-error datasets.

## Limitations
- The 30% fixed contamination rate across all error types is arbitrary; optimal rates likely vary by model architecture, data domain, and error type, but the paper provides no sensitivity analysis beyond this single setting
- Cross-year model performance variability suggests contamination benefits may depend on underlying data characteristics that are not characterized
- No ablation studies examining which error types provide the most benefit for specific model families, limiting practical guidance for contamination strategy selection

## Confidence

- **High Confidence:** Tree-based models (Extra Trees, Random Forest) show consistent robustness to missing values and outliers due to their inherent handling mechanisms (surrogate splits, leaf isolation)
- **Medium Confidence:** Noise injection acts as regularization for linear models and SVMs, improving generalization by preventing overfitting to clean synthetic distributions
- **Low Confidence:** Label misclassification benefits are highly model-dependent without clear systematic patterns; no single contamination strategy emerges as universally optimal

## Next Checks

1. Conduct error rate sensitivity analysis (10%, 20%, 30%, 40%) for each contamination type to identify optimal regularization levels per model architecture
2. Perform ablation studies comparing each error type's contribution to overall performance improvement for tree-based vs. linear vs. neural models
3. Test contamination strategies on non-financial tabular datasets to validate generalization of findings across domains and data distributions