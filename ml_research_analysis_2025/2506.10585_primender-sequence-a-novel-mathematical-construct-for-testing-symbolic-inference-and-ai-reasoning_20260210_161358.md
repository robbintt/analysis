---
ver: rpa2
title: 'Primender Sequence: A Novel Mathematical Construct for Testing Symbolic Inference
  and AI Reasoning'
arxiv_id: '2506.10585'
source_url: https://arxiv.org/abs/2506.10585
tags:
- primender
- sequence
- numbers
- prime
- symbolic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Primender sequence, a novel mathematical
  construct combining classical primality with modular digit-based rules. It serves
  as a benchmark for evaluating symbolic reasoning in large language models (LLMs).
---

# Primender Sequence: A Novel Mathematical Construct for Testing Symbolic Inference and AI Reasoning

## Quick Facts
- arXiv ID: 2506.10585
- Source URL: https://arxiv.org/abs/2506.10585
- Reference count: 26
- Primary result: Only ChatGPT o3 successfully inferred the Primender rule with 5.16% error rate on 100,000 generated terms

## Executive Summary
This paper introduces the Primender sequence, a novel mathematical construct combining classical primality with modular digit-based rules. It serves as a benchmark for evaluating symbolic reasoning in large language models (LLMs). The study tests a hypothesis about the sequence and assesses LLM performance across rule inference, hypothesis evaluation, and sequence generation. Only ChatGPT o3 successfully inferred the correct rule and validated the hypothesis, achieving a 5.16% error rate in generating 100,000 terms. Most models failed at rule inference despite strong hypothesis evaluation. The results highlight current LLM limitations in symbolic pattern generalization and underscore the need for interpretable, rule-based benchmarks in AI evaluation.

## Method Summary
The study benchmarks LLMs on three symbolic reasoning tasks: (1) Infer the generative rule from the first 100 terms of the Primender sequence; (2) Evaluate a specific hypothesis about the sequence; (3) Generate the next 100,000 terms. The ground truth logic defines a number as Primender if it is prime, ends with a prime digit (2,3,5,7), or ends with any prime suffix. Models were evaluated using exact prompt text, with error rates calculated by comparing generated sequences against reference lists using unordered frequency comparison.

## Key Results
- ChatGPT o3 was the only model to successfully infer the correct Primender rule
- Most models achieved strong hypothesis evaluation but failed rule inference
- Sequence generation error rates ranged from 5.16% (o3) to 79.43% (Copilot)
- Models asking clarifying questions before solving demonstrated superior performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A hybrid rule combining classical primality with modular digit-based conditions creates a deterministic yet non-trivial sequence that resists simple pattern extrapolation.
- Mechanism: The Primender sequence requires satisfying at least one of multiple overlapping conditions: being prime, ending with a prime unit digit (2,3,5,7), or having any-length prime suffix. This forces LLMs to consider multiple simultaneous hypotheses rather than relying on single-rule heuristics.
- Core assumption: Models capable of symbolic reasoning should identify the disjunctive rule structure, whereas pattern-matching approaches will overfit to surface regularities.
- Evidence anchors: [abstract] "a number n is included in the sequence if it is prime or ends with a prime number of unit digit or any length"

### Mechanism 2
- Claim: The maximum delta value of 5 between consecutive Primender numbers emerges from structural constraints imposed by the inclusion rules.
- Mechanism: Within any 6 consecutive integers, at least one must satisfy the inclusion criteria—either by primality or by digit-based rules. This makes large gaps impossible.
- Core assumption: The proof-by-contradiction holds universally, not just for the first million terms.
- Evidence anchors: [Section III.C] "An interesting pattern that we can note is that Delta does not ever cross the value 5... This is contrast with Prime gaps."

### Mechanism 3
- Claim: Models that ask clarifying questions before solving achieve superior symbolic inference compared to immediate-answer models.
- Mechanism: ChatGPT o3 first clarified 3 points before inference, building semantic context that enabled correct rule identification with only 5.16% error rate on 100,000 generated terms.
- Core assumption: The questioning behavior causally improves reasoning, not merely correlates with model capability.
- Evidence anchors: [abstract] "Only ChatGPT o3 successfully inferred the correct rule and validated the hypothesis, achieving a 5.16% error rate"

## Foundational Learning

- Concept: Modular arithmetic and digit-based primality
  - Why needed here: Understanding `n mod 10 ∈ {2,3,5,7}` and extended suffix conditions requires comfort with modulo operations and their relationship to digit extraction.
  - Quick check question: Given n = 1247, what are the values of n mod 10, n mod 100, and which Primender conditions does it satisfy?

- Concept: Prime gaps and Bertrand's postulate
  - Why needed here: The maximum delta proof relies on understanding that prime gaps are bounded differently than Primender gaps, and that small ranges guarantee prime presence.
  - Quick check question: Why can't the gap between consecutive primes have a fixed upper bound, but the Primender sequence can?

- Concept: Disjunctive rule inference
  - Why needed here: LLMs must recognize that satisfying ANY ONE of multiple conditions qualifies a number—not ALL conditions.
  - Quick check question: Does 111 belong to the Primender sequence? Justify using the relevant condition(s).

## Architecture Onboarding

- Component map: `is_primender(n)` -> `largest_prime_leq(n)` -> `sequence_generator(n_terms)` -> `delta_computer(seq)` -> `hypothesis_validator(seq)`

- Critical path: Rule inference accuracy → Hypothesis evaluation → Sequence generation with low error rate. The paper shows most models achieve hypothesis evaluation but fail rule inference and generation—indicating these are progressively harder tasks requiring deeper understanding.

- Design tradeoffs:
  - Speed vs. depth: Faster models (Copilot: 10 sec) produced higher error rates (79.43%) than slower reasoning models (ChatGPT o3: 4 min, 5.16% error)
  - Single-prompt vs. multi-turn: Models requiring iterative prompting (LLaMA, Gemini 2.5 Pro) failed to produce complete outputs, but ChatGPT o3's clarifying questions improved quality

- Failure signatures:
  - High generation error rate despite correct hypothesis evaluation (Copilot: 79.43%, Grok: 79.48%)
  - Inability to generate downloadable output (LLaMA 3.3, Gemini variants)
  - Incorrect hypothesis evaluation (Gemini 2.5 Flash: "said our hypothesis was wrong")

- First 3 experiments:
  1. Replicate the Primender checker using the provided Python code on first 1000 terms; verify maximum delta is 5.
  2. Test hypothesis `if PEn - LPn = 1 then Delta = 1` on first 10,000 terms using the validation script provided in Section IV.
  3. Extend the rule to include 4-digit prime suffixes (n mod 10000 is prime); measure if maximum delta changes and whether this simplifies or complicates LLM inference.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Primender framework be adapted to evaluate multimodal reasoning capabilities in AI models?
- Basis in paper: [explicit] The conclusion states future work could include "exploring multimodal reasoning" and "expanding the Primender framework to new sequences."
- Why unresolved: The current study focused exclusively on text-based symbolic inference using structured prompts and integer sequences.
- What evidence would resolve it: A study applying visual or auditory representations of the Primender sequence properties to assess model performance.

### Open Question 2
- Question: Is there a formal mathematical proof explaining why $PEn - LPn$ values never end in the digit 7?
- Basis in paper: [inferred] Section IV.B notes that analysis showed $PEn-LPn$ "didn’t assume the values 7, 17, 27, 37 and so on," labeling it a structural artifact, but provides no formal proof for this specific observation.
- Why unresolved: The paper verifies the hypothesis computationally and proves the maximum delta is 5, but leaves the "absence of values ending in 7" as an observed artifact.
- What evidence would resolve it: A deductive proof demonstrating why the difference between a Primender number and the largest prime less than or equal to it cannot end in 7.

### Open Question 3
- Question: How can LLM architectures be modified to successfully deduce logical rules for novel sequences like Primender without relying on prior training data?
- Basis in paper: [explicit] The conclusion states that "current state of artificial or generative intelligence is not sufficient for true symbolic inferences" and models fail at "deducing logical rules on new sequences involving puzzle like pattern inference."
- Why unresolved: Most models failed rule inference (8 out of 9), suggesting current attention mechanisms or reasoning pathways are insufficient for novel symbolic logic.
- What evidence would resolve it: An architectural modification or training methodology that enables diverse LLMs to achieve high rule inference accuracy on the Primender sequence or similar novel constructs.

## Limitations

- The study lacks documentation of ChatGPT o3's clarifying questions and answers, making reproducibility uncertain
- Extreme performance gaps between models may reflect implementation details rather than fundamental architectural differences
- Most models failed to provide complete 100,000-term outputs, limiting comparative analysis

## Confidence

- **High Confidence**: The mathematical structure of the Primender sequence (maximum delta = 5) and its formal definition are well-specified and verifiable through provided code
- **Medium Confidence**: The comparative performance rankings across models are reproducible if identical prompts and evaluation scripts are used
- **Low Confidence**: The causal mechanism linking clarifying questions to superior performance requires validation through controlled ablation studies

## Next Checks

1. **Ablation Study on Questioning**: Run ChatGPT o3 on the same task with and without the opportunity to ask clarifying questions. Compare rule inference accuracy and error rates to determine if the questioning behavior is truly causal rather than merely correlated with capability.

2. **Cross-Domain Rule Transfer**: Test whether models that successfully infer the Primender rule can transfer this symbolic reasoning to structurally similar sequences (e.g., "Fibender" combining Fibonacci numbers with modular digit rules). This would validate whether the skill generalizes beyond the specific benchmark.

3. **Extreme Range Verification**: Compute the Primender sequence for values exceeding 10^9 to empirically verify the maximum delta = 5 claim holds universally, not just within the tested range. This would confirm the structural proof's validity across all scales.